{
  "article_text": [
    "today s state - of - the - art perceptual models  @xcite have mostly tackled detecting and recognizing individual objects _ in isolation_. however , understanding a visual scene often goes beyond recognizing individual objects .",
    "take a look at the two images in fig .",
    "[ fig : intro_figure ] .",
    "even a perfect object detector would struggle to perceive the subtle difference between a man feeding a horse and a man standing by a horse .",
    "the rich semantic relationships between these objects have been largely untapped by these models . as indicated by a series of previous works  @xcite , one crucial step towards a deeper understanding of visual scenes is building a structured representation that captures objects and their semantic relationships .",
    "such representation not only offers contextual cues for fundamental recognition tasks  @xcite but also provide values in a larger variety of high - level visual tasks  @xcite .    the recent success of deep learning - based recognition models  @xcite has resurged interest in examining the detailed structures of a visual scene , especially in the form of object relationships  @xcite . scene graph , proposed by johnson  @xcite , offers a platform to explicitly model objects and their relationships . in short , a _ scene graph _ is a visually - grounded graph over the object instances in an image , where the edges depict their pairwise relationships ( see example in fig .",
    "[ fig : intro_figure ] ) .",
    "the value of scene graph representation has been proven in a wide range of visual tasks , such as semantic image retrieval  @xcite , 3d scene synthesis  @xcite , and visual question answering  @xcite .",
    "anderson recently proposed spice  @xcite as an enhanced automated caption evaluation metric defined over scene graphs .",
    "however , these models that use scene graphs either rely on ground - truth annotations  @xcite , synthetic images  @xcite , or extract a scene graph from text domain  @xcite . to truly take advantage of such rich structure , it is crucial to devise a model that automatically generates scene graphs from images .",
    "[ fig : intro_figure ]    in this work , we address the problem of scene graph generation , where the goal is to generate a visually - grounded scene graph from an image . in a generated scene graph ,",
    "an object instance is characterized by a bounding box with an object category label , and a relationship is characterized by a directed edge between two bounding boxes ( i.e. , object and subject ) with a relationship predicate ( red nodes in fig .",
    "[ fig : intro_figure ] ) . the major challenge of generating scene graphs is reasoning about relationships .",
    "much effort has been expended on localizing and recognizing semantic relationships in images  @xcite .",
    "most methods have focused on making _",
    "local _ predictions of object relationships  @xcite , which essentially simplify the scene graph generation problem into independently predicting relationships between pairs of objects . however , by doing _ local",
    "_ predictions these models ignore surrounding context , whereas joint reasoning with contextual information can often resolve ambiguity due to local predictions in isolation .    to capture this intuition",
    ", we propose a novel end - to - end model that learns to generate image - grounded scene graphs ( fig .",
    "[ fig : pipeline ] ) .",
    "the model takes an image as input and outputs a scene graph that consists of object categories , their bounding boxes , and semantic relationships between pairs of objects .",
    "our major contribution is that instead of inferring each component of a scene graph in isolation , the model passes messages containing contextual information between a pair of bipartite sub - graphs of the scene graph , and iteratively refines its predictions using rnns .",
    "we evaluate our model on the visual genome scene graph dataset  @xcite , which contains human - annotated scene graphs on 108,077 images . on average , each image",
    "is annotated with 13.5 objects and 15 pairwise object relationships .",
    "we show that relationship prediction in scene graphs can be significantly improved by our model .",
    "furthermore , we also apply our model to the nyu depth v2 dataset  @xcite , establishing new state - of - the - art results in reasoning about spatial relations , such as horizontal and vertical supports .    in summary , we propose an end - to - end model that generates visually - grounded scene graphs from images .",
    "the model uses a novel inference formulation that iteratively refines its prediction by passing contextual messages along the topological structure of a scene graph .",
    "we demonstrate its use for generating semantic scene graphs from the visual genome scene graph dataset as well as predicting support relations using the nyu depth v2 dataset  @xcite .",
    "* scene understanding and relationship prediction .",
    "* visual scene understanding often harnesses the statistical patterns of object co - occurrence  @xcite as well as spatial layout  @xcite .",
    "a series of contextual models based on surrounding pixels and regions have also been developed for perceptual tasks  @xcite .",
    "recent works  @xcite exploits more complex structures for relationship prediction .",
    "however , these works focus on image - level predictions without detailed visual grounding . physical relationships , such as support and stability , have been studied in  @xcite .",
    "lu @xcite directly tackled the semantic relationship detection by combining visual inputs with language priors to cope with the long - tail distribution of real - world relationships .",
    "however , their method predicts each relationship independently .",
    "we show that our model outperforms theirs with joint inference",
    ".    * visual scene representation .",
    "* one of the most popular ways of representing a visual scene is through text descriptions  @xcite .",
    "although text - based representation has been shown to be helpful for scene classification and retrieval , its power is often limited by ambiguity and lack of expressiveness . in comparison ,",
    "scene graphs  @xcite offer explicit grounding of visual concepts , avoiding referential uncertainty in text - based representation .",
    "scene graphs have been used in many downstream tasks such as image retrieval  @xcite , 3d scene synthesis  @xcite and understanding  @xcite , visual question answering  @xcite , and automatic caption evaluation  @xcite . however , previous work on scene graphs shied away from the graph generation problem by either using ground - truth annotations  @xcite , or extracting the graphs from other modalities  @xcite .",
    "our work addresses the problem of generating scene graphs directly from images",
    ".    * graph inference . * conditional random fields ( crf ) have been used extensively in graph inference .",
    "johnson used crf to infer scene graph grounding distributions for image retrieval  @xcite .",
    "yatskar  @xcite proposed situation - driven object and action prediction using a deep crf model .",
    "our work is closely related to crfasrnn  @xcite and graph - lstm  @xcite in that we also formulate the graph inference problem using an rnn - based model .",
    "a key difference is that they focus on node inference while treating edges as pairwise constraints , whereas we enable edge predictions using a novel primal - dual graph inference scheme .",
    "we also share the same spirit as structural rnn  @xcite .",
    "a crucial distinction is that our model iteratively refines its predictions through message passing , whereas the structural rnn model only makes one - time predictions along the temporal dimension , and thus can not refine its past predictions .",
    "a _ scene graph _ , as defined by johnson  @xcite , is a structured representation of an image , where nodes in a scene graph correspond to object bounding boxes with their object categories , and edges correspond to their pairwise relationships between objects .",
    "the task of _ scene graph generation _ is to generate a visually - grounded scene graph that most accurately correlates with an image .",
    "intuitively , individual predictions of objects and relationships can benefit from their surrounding context .",
    "for instance , knowing `` a horse is on grass field '' is likely to increase the chance of detecting a person and predicting the relationship of `` man riding horse '' . to capture this intuition , we propose a joint inference framework to enable contextual information to propagate through the scene graph topology via a message passing scheme .",
    "however , inference on a densely connected graph can be very expensive .",
    "as shown in previous work  @xcite and  @xcite , dense graph inference can be approximated by mean field in conditional random fields ( crf ) .",
    "our approach is inspired by zeng  @xcite , which designs fully differentiable layers to enable end - to - end learning with recurrent neural networks ( rnn ) .",
    "yet their model relies on purpose - built rnn layers . to achieve greater flexibility in a more principled training framework",
    ", we use a generic rnn unit instead , in particular a gated recurrent unit ( gru )  @xcite . at each iteration",
    ", each gru takes its previous hidden state and an incoming message as input , and produces a new hidden state as output .",
    "each node and edge in the scene graph maintains its internal state in its corresponding gru unit , where all nodes share the same gru weights ( node grus ) , and all edges share the other set of gru weights ( edge grus ) .",
    "this setup allows the model to pass messages ( an aggregation of gru hidden states ) among the gru units along the scene graph topology .",
    "we also propose a message pooling function that learns to dynamically aggregate the hidden states of the grus into messages .",
    "we further observe that the unique structure of scene graphs forms a bipartite structure of message passing channels . since messages only pass along the topological structure of a scene graph , the set of edge grus and the set of node grus form a bipartite graph , where no message is passed inside each set .",
    "inspired by this observation , we formulate two disjoint sub - graphs that are essentially the dual graph to each other .",
    "the primal graph defines channels for messages to pass from edge grus to node grus .",
    "the dual graph defines channels for messages to pass from node grus to edge grus . with such primal - dual formulation",
    ", we can therefore improve inference efficiency by iteratively passing messages between these sub - graphs instead of through a densely connected graph .",
    "[ fig : architecture ] gives an overview of our model .",
    "we first lay out the mathematical formulation of our scene graph generation problem . to generate a visually grounded scene graph",
    ", we need to obtain an initial set of object bounding boxes .",
    "these bounding boxes can be either from ground - truth human annotation or algorithmically generated . in practice , we use the region proposal network ( rpn )  @xcite to automatically generate a set of object bounding box proposals @xmath0 from an image @xmath1 as the base input to the inference procedure ( fig .",
    "[ fig : architecture](a ) ) .    for each object box proposal",
    ", we need to infer two types of object - centric variables : 1 ) an object class label , and 2 ) four bounding box offsets relative to the proposal box coordinates , which are used for refining the proposal boxes .",
    "in addition , we need to infer a relationship - centric variable between every pair of proposal boxes , which denotes the predicate type of the relationship between the corresponding object pair . given a set of object classes @xmath2 ( including background ) and a set of relationship types @xmath3 ( including none relationship ) , we denote the set of all variables to be @xmath4 , where @xmath5 is the number of proposal boxes , @xmath6 is the class label of the @xmath7-th proposal box , @xmath8 is the bounding box offsets relative to the @xmath7-th proposal box coordinates , and @xmath9 is the relationship predicate between the @xmath7-th and the @xmath10-th proposal boxes .    at the high level ,",
    "the inference task is to classify objects , predict their bounding box offsets , and classify relationship predicates between each pair of objects .",
    "formally , we formulate the scene graph generation problem as finding the optimal @xmath11 that maximizes the following probability function given the image @xmath1 and box proposals @xmath0 :    @xmath12    in the next subsection , we introduce a way to approximate the inference procedure using an iterative message passing scheme modeled with gated recurrent units  @xcite .",
    "we use mean field to perform approximate inference .",
    "we denote the probability of each variable @xmath13 as @xmath14 , and assume that the probability only depends on the current state of each node and edge at each iteration .",
    "in contrast to zeng  @xcite , we use a generic rnn module to compute the hidden states .",
    "in particular , we choose gated recurrent units  @xcite due to its simplicity and effectiveness .",
    "we use the hidden state of the corresponding gru , a high - dimensional vector , to represent the current state of each node and each edge . as all the nodes ( edges )",
    "share the same update rule , we share the same set of parameters among all the node grus , and the other set of parameters among all the edge grus ( fig .  [",
    "fig : architecture ] ) .",
    "we denote the current hidden state of node @xmath7 as @xmath15 and the current hidden state of edge @xmath16 as @xmath17 .",
    "then the mean field distribution can be formulated as @xmath18 where @xmath19 is the visual feature of the @xmath7-th node , and @xmath20 is the visual feature of the edge from the @xmath7-th node to the @xmath10-th node . in the first iteration ,",
    "the gru units take the visual features @xmath21 and @xmath22 as input ( fig .",
    "[ fig : architecture](a ) ) .",
    "we use the visual feature of the proposal box as the visual feature @xmath19 for the @xmath7-th node .",
    "we use the visual feature of the union box over the proposal boxes @xmath23 as the visual feature @xmath24 for edge @xmath25 .",
    "these visual features are extracted by a roi - pooling layer  @xcite from the image . in later iterations ,",
    "the inputs are the aggregated messages from other gru units of the previous step .",
    "we talk about how the messages are aggregated and passed in the next subsection .",
    "[ sec : rnn ] offers a generic formulation for solving graph inference problem using rnns .",
    "however , we observe that we can further improve the inference efficiency by leveraging the unique bipartite structure of a scene graph . in the scene graph topology ,",
    "the neighbors of the edge grus are node grus , and vice versa . passing messages along this",
    "structure forms two disjoint sub - graphs that are the dual graph to each other .",
    "specifically , we have a node - centric primal graph , in which each node gru gets messages from its inbound and outbound edge grus . in the edge - centric dual graph",
    ", each edge gru gets messages from its subject node gru and object node gru ( fig .  [",
    "fig : architecture](b ) ) .",
    "we can therefore improve inference efficiency by iteratively passing messages between these two sub - graphs instead of through a densely connected graph  ( fig .",
    "[ fig : architecture](c ) ) .",
    "as each gru receives multiple incoming messages , we need an aggregation function that can fuse information from all messages into a meaningful representation",
    ". a nave approach would be standard pooling methods such as average- or max - pooling .",
    "however , we found that it is more effective to learn adaptive weights that can modulate the influences of incoming messages and only keep the relevant information .",
    "we introduce a _ message pooling _ function that computes the weight factors for each incoming message and fuse the messages using a weighted sum .",
    "we provide an empirical analysis of different message pooling functions in sec .",
    "[ sec : experiment ] .",
    "formally , given the current gru hidden states of nodes and edges @xmath15 and @xmath17 , we denote the messages to update the @xmath7-th node as @xmath26 , which is computed by a function of its own hidden state @xmath15 , and the hidden states of its outbound edge grus @xmath27 and inbound edge grus @xmath28 .",
    "similarly , we denote the message to update the edge from the @xmath7-th node to the @xmath10-th node as @xmath29 , which is computed by a function of its own hidden state @xmath27 , the hidden states of its subject node gru @xmath15 and its object node gru @xmath30 . to be more specific , @xmath26 and @xmath31",
    "are computed by the following two adaptively weighted message pooling functions :    @xmath32 ) h_{i\\rightarrow j } + \\sum_{j : j \\rightarrow i } \\sigma(\\mathbf{v}_2^t[h_i , h_{j\\rightarrow i } ] ) h_{j\\rightarrow i } \\label{eq : vert}\\\\      & m_{i\\rightarrow j }   = \\sigma(\\mathbf{w}_1^t[h_i , h_{i\\rightarrow j } ] ) h_{i } + \\sigma(\\mathbf{w}_2^t[h_j , h_{i\\rightarrow j } ] ) h_{j } \\label{eq : edge}\\end{aligned}\\ ] ]    where @xmath33 $ ] denotes a concatenation of vectors , and @xmath34 denotes a sigmoid function . @xmath35 , @xmath36 and @xmath37 , @xmath38 are learnable parameters .",
    "these two equations describe the primal - dual update rules , as shown in ( b ) of fig .",
    "[ fig : architecture ] .",
    "our final output layers follow closely with the faster r - cnn setup  @xcite .",
    "we use a softmax layer to produce the final scores for the object class as well as relationship predicate .",
    "we use an fc layer to regress to the bounding box offsets for each object class separately .",
    "we use cross - entropy to compute the loss on the object class and the relationship predicate .",
    "we use l1 function to compute the loss of the bounding box offsets .",
    "we use an ms coco - pretrained vgg-16 network to extract visual features from images .",
    "we freeze the weights of all convolution layers , and only finetune the fully connected layers , including the grus .",
    "the node grus and the edge grus have both 512-dimensional input and output . during training",
    ", we first use nms to select at most 2,000 boxes from all proposed boxes @xmath0 , and then randomly select 256 boxes as the object proposals . due to the quadratic number of edges , we randomly sub - sample 512 edges for each image at training time .",
    "edges without predicate labels are assigned to `` none '' class . at test time",
    ", we use nms to select at most 50 boxes from the object proposals with an iou threshold of 0.3 .",
    "we make predictions on all edges except the self - connections at the test time .",
    "we evaluate our model on generating scene graphs from images .",
    "we compare our model against a recently proposed model on visual relationship prediction  @xcite .",
    "our goal is to analyze our model in datasets with both sparse and dense relationship annotations .",
    "we use the visual genome scene graph dataset  @xcite in our main experiment .",
    "we also evaluate our model on the support relation inference task in the nyu depth v2 dataset .",
    "the key difference between these two datasets is that scene graph annotation is very sparse in visual genome : among all possible pairing of objects , only 5% of them are labeled with a relationship predicate .",
    "the nyu depth v2 dataset , on the other hand , exhaustively annotates the support of every labeled object .",
    "our experiments show that our model outperforms the baseline model  @xcite , and can generalize to other types of relationships , in particular support relations  @xcite , without any architecture change .",
    "* visual genome * the visual genome dataset  @xcite is a super set of the visual relationship dataset used in  @xcite .",
    "it contains 108,077 images annotated with on average 13.5 objects and 15 relationships per image .",
    "the object categories and relationship predicates are annotated with an open vocabulary .",
    "there are 75,729 unique object categories , and 40,480 unique relationship predicates .",
    "as the annotations follow an extremely long - tail distribution , we use the most frequent 150 object categories and 50 predicates for evaluation . as a result",
    ", each image has a scene graph of around 12 objects and 7 relationships .",
    "we use 70% of the images for training and the remaining 30% for testing .    *",
    "nyu depth v2 * we also evaluate our model on the support relation graphs from the nyu depth v2 dataset  @xcite .",
    "the dataset contains 1,449 rgbd images captured in 27 indoor scenes .",
    "each image is annotated with instance segmentation , region class labels , and support relations between regions .",
    "we use the standard split , with 795 images used for training and 654 images for testing .",
    "[ [ setup ] ] setup + + + + +    given an image , the scene graph generation task is to localize a set of objects , classify their category labels , and predict relationships between each pair of the objects .",
    "we evaluate our model on the visual genome dataset .",
    "we analyze our model in three setups below .    1 .   the * predicate classification * ( predcls ) task is to predict the predicates of all pairwise relationships of a set of localized objects .",
    "this task examines the model s performance on predicate classification in isolation from other factors . 2 .   the * scene graph classification * ( sgcls ) task is to predict the predicate as well as the object categories of the subject and the object in every pairwise relationship given a set of localized objects .",
    "the * scene graph generation * ( sggen ) task is to simultaneously detect a set of objects and predict the predicate between each pair of the detected objects .",
    "an object is considered to be correctly detected if it has at least 0.5 iou overlap with the ground - truth box .",
    "we adopted the image - wise recall evaluation metrics , r@50 and r@100 , that are used in lu  @xcite for all the three setups .",
    "the r@@xmath39 metric measures the fraction of ground - truth relationship triplets ` ( subject- predicate - object ) ` that appear among the top @xmath39 most confident triplet predictions in an image .",
    "the choice of this metric is , as explained in  @xcite , due to the sparsity of the relationship annotations in visual genome  metrics like map would falsely penalize positive predictions on unlabeled relationships .",
    "we also report per - type recall@5 of classifying individual predicate .",
    "this metric measures the fraction of the time the correct predicate is among the top 5 most confident predictions of each labeled relationship triplet .",
    "as shown in table  [ table : pred_type ] , many predicates have very similar semantic meanings , for example , ` on ` vs. ` over ` and ` hanging from ` vs. ` attached to ` .",
    "the less frequent predicates would be overshadowed by the more frequent ones during training .",
    "we use the recall metric to alleviate such an effect .        [",
    "fig : train_iter ]      we evaluate our final model and a number of baseline models .",
    "one of the key components in our primal - dual formulation is the message pooling functions that use learnt weighted sum to aggregate hidden states of nodes and edges into messages ( see eq .",
    "[ eq : vert ] and eq .",
    "[ eq : edge ] ) . in order to demonstrate its effectiveness ,",
    "we evaluate variants of our model with standard pooling methods .",
    "the first is to use average - pooling ( * avg .",
    "pool * ) instead of the learnt weighted sum to aggregate the hidden states .",
    "the second is similar to the first one , but uses max - pooling ( * max  pool * ) .",
    "we also evaluate our models against a relationship detection model proposed by lu  @xcite .",
    "their model consists of two components  a vision module that makes predictions from images , and a language module that captures language priors .",
    "we compare with their vision module , which uses the same inputs as ours ; their language module is orthogonal to our model , and can be added independently .",
    "note that this model is equivalent to our final model without any message passing .",
    ".evaluation results of the scene graph generation task on the visual genome dataset  @xcite .",
    "we compare a few variations of our model against a visual relationship detection module proposed by lu  @xcite ( sec .",
    "[ sec : models ] ) . [ cols=\"<,<,^,^,^,^,^\",options=\"header \" , ]     [ table : nyu_eval ]      we then evaluate on the nyu depth v2 dataset  @xcite with densely labeled support relations .",
    "we show that our model can generalize to other type of relationships and is effective on both sparsely and densely labeled relationships .",
    "[ [ setup-1 ] ] setup + + + + +    the nyu depth v2 dataset contains three types of support relationships : an object can be supported by an object from behind , by an object from below , or supported by a hidden object .",
    "each object is also labeled with one of the four structure classes : ` { floor , structure , furniture , prop } ` .",
    "we define the support graph generation task as to predicting both th support relation type between objects and the structure class of each object .",
    "we take the smallest bounding box that encloses an object segmentation mask as its object region .",
    "we assume ground - truth object locations in this task .",
    "we compare our final model with two previous models  @xcite on the support graph generation task . following the metric used in previous work , we report two types of support relation accuracies  @xcite : type - aware and type - agnostic .",
    "we also report the performance with r@50 and r@100 measurements of the predicate classification task introduced in sec .",
    "[ sec : vg_eval ] .",
    "note that both @xcite and @xcite use rgbd images , whereas our model uses only rgb images .",
    "[ [ results-1 ] ] results + + + + + + +    our model outperforms previous work , achieving new state - of - the - art performance using only rgb images .",
    "our results show that having contextual information further improves support relation prediction , even compared to purpose - built models  @xcite that used rgbd images .",
    "[ fig : nyu ] shows some sample predictions using our final model .",
    "incorrect predictions typically occur in ambiguous supports , e.g. , books in shelves can be mistaken as being supported from behind ( row 1 , column 2 ) .",
    "another failure mode is due to geometric structures that have weak visual features .",
    "as shown in row 2 , column 1 , the ceiling at the top left corner of the image is predicted as supported from behind instead of supported below by the wall , but the boundary between the ceiling and the wall is nearly invisible .",
    "such visual uncertainty may be resolved by having additional depth information .",
    ": support from below , @xmath40 : support from behind .",
    "red arrows are incorrect predictions .",
    "we also color code structure classes : ground is in blue , structure is in green , furniture is in yellow , prop is in red .",
    "purple indicates missing structure class .",
    "note that the segmentation masks are only shown for visualization purpose .",
    "our model uses object bounding boxes as input instead of these masks ]",
    "we addressed the problem of automatically generating a visually grounded scene graph from an image by a novel end - to - end model .",
    "our model performs iterative message passing between the primal and dual sub - graph along the topological structure of a scene graph . this way , it improves the quality of node and edge predictions by incorporating informative contextual cues .",
    "our model can be considered a more generic framework for graph generation problem . in this work",
    ", we have demonstrated its effectiveness in predicting visual genome scene graphs as well as support relations in indoor scenes .",
    "a possible future direction would be to explore its capability in other structured prediction problems in vision and other problem domains .",
    "we would like to thank ranjay krishna , judy hoffman , junyoung gwak , and anonymous reviewers for useful comments .",
    "this research is partially supported by a yahoo labs macro award , and an onr muri award .                    c.  desai , d.  ramanan , and c.  fowlkes .",
    "discriminative models for static human - object interactions . in _",
    "2010 ieee computer society conference on computer vision and pattern recognition - workshops_. ieee , 2010 .        c.  galleguillos , a.  rabinovich , and s.  belongie .",
    "object categorization using co - occurrence , location and appearance . in _ computer vision and pattern recognition , 2008 .",
    "cvpr 2008 .",
    "ieee conference on_. ieee , 2008 .",
    "j.  johnson , r.  krishna , m.  stark , l.  j. li , d.  a. shamma , m.  s. bernstein , and l.  fei - fei .",
    "image retrieval using scene graphs . in _",
    "ieee conference on computer vision and pattern recognition ( cvpr ) _ , 2015 .",
    "r.  krishna , y.  zhu , o.  groth , j.  johnson , k.  hata , j.  kravitz , s.  chen , y.  kalantidis , l .- j .",
    "li , d.  a. shamma , m.  bernstein , and l.  fei - fei .",
    "visual genome : connecting language and vision using crowdsourced dense image annotations . in _ arxiv _ , 2016 .",
    "d.  lin , s.  fidler , and r.  urtasun .",
    "holistic scene understanding for 3d object detection with rgbd cameras . in _ proceedings of the ieee international conference on computer vision _ , pages 14171424 , 2013 .",
    "v.  ramanathan , c.  li , j.  deng , w.  han , z.  li , k.  gu , y.  song , s.  bengio , c.  rossenberg , and l.  fei - fei .",
    "learning semantic relationships for better action retrieval in images . in _",
    "2015 ieee conference on computer vision and pattern recognition ( cvpr)_. ieee , 2015 .",
    "r.  salakhutdinov , a.  torralba , and j.  tenenbaum .",
    "learning to share visual appearance for multiclass object detection . in _",
    "computer vision and pattern recognition ( cvpr ) , 2011 ieee conference on_. ieee , 2011 .",
    "b.  yao and l.  fei - fei .",
    "modeling mutual context of object and human pose in human - object interaction activities . in _",
    "computer vision and pattern recognition ( cvpr ) , 2010 ieee conference on_. ieee , 2010 .",
    "s.  zheng , s.  jayasumana , b.  romera - paredes , v.  vineet , z.  su , d.  du , c.  huang , and p.  torr .",
    "conditional random fields as recurrent neural networks . in _ international conference on computer vision ( iccv ) _ , 2015 ."
  ],
  "abstract_text": [
    "<S> understanding a visual scene goes beyond recognizing individual objects in isolation . </S>",
    "<S> relationships between objects also constitute rich semantic information about the scene . in this work , we explicitly model the objects and their relationships using scene graphs , a visually - grounded graphical structure of an image . </S>",
    "<S> we propose a novel end - to - end model that generates such structured scene representation from an input image . </S>",
    "<S> the model solves the scene graph inference problem using standard rnns and learns to iteratively improves its predictions via message passing . </S>",
    "<S> our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships . </S>",
    "<S> the experiments show that our model significantly outperforms previous methods on generating scene graphs using visual genome dataset and inferring support relations with nyu depth v2 dataset . </S>"
  ]
}