{
  "article_text": [
    "co - expression networks have been frequently used to reverse engineer the whole - genome interactions between complex multicellular organisms by ascertaining common regulation and thus common functions .",
    "a gene co - expression network is represented as an undirected graph with nodes being genes and edges representing significant inter - gene interactions .",
    "such a network can be constructed by computing linear ( e.g. @xcite ) or non - linear ( e.g. @xcite @xcite @xcite ) co - expression measures between paired gene expression profiles across multiple samples . as the first formal and wide - spread correlation measure @xcite @xcite , pearson s correlation coefficient ( pcc ) , or alias pearson s @xmath4 , is one widely used technique in co - expression network construction @xcite .",
    "however , all - pairs pcc computation of gene expression profiles is not computationally trivial for genome - wide association study with large number of gene expression profiles across a large population of samples , especially when coupled with permutation tests for statistical inference .",
    "the importance of all - pairs pcc computation and its considerable computing demand motivated us to investigate its acceleration on parallel / high - performance computing architectures .",
    "pcc statistically measures the strength of linear association between pairs of continuous random variables , but does not apply to non - linear relationship .",
    "thus , we must ensure the linearity between paired data prior to the application of pcc . given two random variables @xmath5 and @xmath6 of @xmath7 dimensions each ,",
    "the pcc between them is defined as @xmath8 - \\bar{u})(v[k ] - \\bar{v})}{\\sqrt{\\sum_{k=0}^{l-1}(u[k ] - \\bar{u})^2 \\sum_{k=0}^{l-1}(v[k]-\\bar{v})^2 } } \\label{equation : pearsonr}\\ ] ] in equation ( [ equation : pearsonr ] ) , @xmath9 $ ] is the @xmath10-th element of @xmath5 , while @xmath11 is the mean of @xmath5 and equal to @xmath12 $ ] .",
    "notations are likewise defined for @xmath6 .",
    "given a variable pair , the sequential implementation of equation ( [ equation : pearsonr ] ) has a linear time complexity proportional to @xmath7 .",
    "moreover , it is known that the absolute value of the nominator is always less than or equal to the denominator @xcite .",
    "thus , @xmath13 always varies in @xmath14 $ ] .",
    "concretely , indicates no linear relationship , positive association and negative association .",
    "although pcc is widely used in science and engineering , the acceleration of its computation using parallel / high - performance computing architectures has not yet been extensively investigated in the literature .",
    "@xcite used the cuda - enabled gpu to accelerate all - pairs computation of pcc and computed pairwise pcc using the following standard reformulation : @xmath15\\cdot v[k ] } - l\\cdot \\bar{u}\\cdot \\bar{v}}{\\sqrt{(\\sum_{k=0}^{l-1 } { u[k]^2 } - l\\cdot \\bar{u}^2 ) ( \\sum_{k=0}^{l-1 } { v[k]^2 } - l\\cdot \\bar{v}^2})}\\ ] ] this work was extended by @xcite to support gpu clusters , which adopted a master - slave model to manage workload distribution over multiple gpus .",
    "@xcite adopted a hybrid cpu - gpu coprocessing model and reformulated each variable @xmath5 to a new representation @xmath16 , which is defined as @xmath17= \\frac{u[k ] - \\bar{u}}{|u[k ] - \\bar{u}| } \\label{equation : pearson_w}\\ ] ] for each @xmath10 ( @xmath18 ) in order to employ general matrix - matrix multiplication ( gemm ) parallelization that has been well studied in parallel computing .",
    "note that due to the commutative nature of pairwise pcc computation , this gemm approach will cause a waste of half horsepower .",
    "similarly , wang _ et al .",
    "_ @xcite also employed a parallel gemm approach to accelerate the all - pairs computation of a given dataset @xmath19 , but on a single xeon phi ( phi ) .",
    "note that our approach is different from @xcite , because ours accelerates the overall computation over @xmath19 on a cluster of phis .    in this paper ,",
    "we present lightpcc , the first parallel and distributed algorithm to harness phi clusters to accelerate all - pairs pcc computation . to achieve high speed ,",
    "our algorithm explores instruction - level parallelism within simd vector processing units per phi , thread - level parallelism over many cores per phi , and accelerator - level parallelism across a cluster of phis .",
    "moreover , we have investigated a general framework for symmetric all - pairs computation to facilitate balanced workload distribution within and between processing elements ( pes ) , by pioneering to build a reversible and bijective relationship between job identifier and coordinate space in a job matrix . using both artificial and real gene expression datasets ,",
    "we have compared lightpcc to two cpu - based counterparts : a sequential c++ implementation in alglib ( http://www.alglib.net ) and an implementation based on a parallel gemm routine in intel math kernel library ( mkl ) .",
    "our experimental results showed that by using one 5110p phi and 16 phis , lightpcc is able to run up to @xmath0 and @xmath1 faster than alglib , and up to @xmath2 and @xmath3 faster than singled - threaded mkl , respectively .",
    "in addition , lightpcc demonstrated good parallel scalability with respect to varied number of phis .",
    "a phi coprocessor is a many - core shared - memory computer @xcite , which runs a specialized linux operating system and provides full cache coherency over the entire chip .",
    "the phi is comprised of a set of processor cores , and each core offers four - way simultaneous multithreading , i.e. 4 hardware threads per core .",
    "while offering scalar processing , each core also includes a newly - designed vpu which features a 512-bit wide simd instruction set architecture ( isa ) .",
    "each vector register can be split to either 16 32-bit - wide lanes or 8 64-bit - wide lanes .",
    "the phi does not provide support for legacy simd isas such as the sse series .",
    "as for caching , each core locally has separate l1 instruction and data caches of size 32 kb each , and a 512 kb l2 cache .",
    "moreover , all l2 caches across the entire chip are interconnected , through a bidirectional ring bus , to form a unified shared l2 cache of over 30 mb .",
    "in addition , there are two usage models for invoking phis : offload model and native model .",
    "the offload model relies on compiler pragmas / directives to offload highly - parallel parts of an application to the phi , while the native model treats a phi as a symmetric multi - processing computer . as of today",
    ", phis have been used to accelerate important computational problems in diverse research fields such as bioinformatics @xcite @xcite and machine learning @xcite @xcite .",
    "in the case of all - pairs computation , significantly more computation can be further reduced than pure pairwise computation .",
    "for instance , given a @xmath7-dimensional variable @xmath5 , the values of @xmath20 - \\bar{u})$ ] and @xmath21 - \\bar{u})^2}$ ] could be repeatedly calculated up to @xmath22 times in the case of literal computing using equation ( [ equation : pearsonr ] ) . since these two values are only dependent on @xmath5 , they can be computed once beforehand .",
    "we define @xmath23 to denote a set of @xmath24 @xmath7-dimensional variables and compute the new representation @xmath25 of @xmath26 as @xmath27 = \\frac{x_i[k ] - \\bar{x_i}}{\\sqrt{\\sum_{k=0}^{l-1}(x_i[k ] - \\bar{x_i } ) } }      \\label{equation : u}\\ ] ] in this way , the pcc between @xmath26 and @xmath28 is computed as @xmath29\\cdot u_j[k ] \\label{equation : pearson2}\\ ] ] from equation ( [ equation : pearson2 ] ) , we can see that if organizing all members of @xmath30 to form a @xmath31 matrix @xmath32 with @xmath25 being row @xmath33 of @xmath32 , we can realize the all - pairs computation over @xmath30 by multiplying matrix @xmath32 by its transpose , i.e. @xmath34 via a gemm algorithm . note that because @xmath35 is symmetric , direct application of a gemm algorithm will cause a waste of half compute power as noted before .    as mentioned above ,",
    "wang _ et al .",
    "_ @xcite also proposed a reformulation in order to benefit from parallel gemm algorithms ( refer to equation ( [ equation : pearson_w ] ) ) .",
    "this reformulation computes the pairwise pcc between @xmath26 and @xmath28 as @xmath36\\cdot u_j[k]}{\\sqrt{\\sum_{k=0}^{l-1 } u_i[k]^2 \\sum_{k=0}^{l-1}u_j[k]^2}}\\ ] ] using this equation , though a gemm algorithm can be used to compute the nominator , the denominator has to be additionally computed .",
    "we consider the @xmath37 job matrix to be a 2-dimensional coordinate space on the cartesian plane , and define the left - top corner to be the origin , the horizontal @xmath38-axis ( corresponding to columns ) in left - to - right direction and the vertical @xmath39-axis ( corresponding to rows ) in top - to - bottom direction .      for non - symmetric all - pairs computation ( non - commutative pairwise computation ) , the workload distribution over pes ( e.g. threads , processes , cores and etc .",
    ") would be relatively straightforward .",
    "this is because coordinates in the 2-dimensional matrix corresponds to distinct jobs .",
    "specifically , given a coordinate @xmath40 ( @xmath41 ) , we can compute its unique job identifier @xmath42 as @xmath43 .",
    "reversely , given a job identifier @xmath42 , we can compute its unique coordinate as @xmath44 in the job matrix .      unlike non - symmetric all - pairs computation , it suffices by only computing the upper - triangle ( or lower - triangle ) of the job matrix for symmetric all - pairs computation ( commutative pairwise computation ) . in this case",
    ", balanced workload distribution could be more complex than non - symmetric all - pairs computation . for workload distribution ,",
    "one approach @xcite is to allocate a separate job array , totally of @xmath45 elements if the major diagonal is counted in , with element @xmath33 ( @xmath46 ) storing the coordinate of the @xmath33-th job and then let each pe access this array to obtain the coordinates of the jobs assigned to it .",
    "the major drawback of this approach is the extra memory consumed by the job array , since the memory overhead can be huge for large @xmath24 .",
    "another approach @xcite is to use a policy designed for parallel matrix - matrix multiplication by discarding the redundant computing part , but could incur unbalanced workload distribution .",
    "in addition , some approaches ( e.g. @xcite ) use master - slave computing model .    in this paper , we propose a general framework for workload balancing in symmetric all - pairs computation .",
    "this framework works by assigning each job a unique identifier and then building a bijective relationship between a job identifier @xmath47 and its corresponding coordinate @xmath40 .",
    "this mapping is called direct bijective mapping in our context .",
    "while facilitating balanced workload distribution , this mapping merely relies on bijective functions , which is a prominent feature distinguished from existing methods . to the best of our knowledge , in the literature",
    "bijective functions have not ever been proposed for workload balancing in symmetric all - pairs computation . in @xcite",
    ", the authors used a very similar job numbering approach to ours in this study , but did not derive a bijective function for symmetric all - pairs computation .",
    "our framework can be applied to cases with identical ( e.g. our study ) or varied workload per job ( e.g. using a shared integer counter to realize dynamic workload distribution via remote memory access operations in mpi and unified parallel c ( upc ) programming models ) and is also particularly useful for parallel computing architectures with hardware schedulers such as gpus and fpgas . in the following , without loss of generality , we will interpret our framework relative to the upper triangle of the job matrix by counting in the major diagonal .",
    "nonetheless , this framework can be easily adapted to the cases excluding the major diagonal .",
    "given a job @xmath48 ) in the upper triangle , we compute its integer job identifier @xmath47 as @xmath49 for @xmath24 variables . in this equation , @xmath50 is the total number of cells preceding row @xmath39 in the upper triangle and is computed as @xmath51 where @xmath39 varies in @xmath52 $ ] and there are two boundary cases needing to be paid attention to : one is when @xmath53 and the other is when @xmath54 .",
    "when @xmath53 , @xmath55 because no cell in the upper triangle appears before row @xmath56 ; and when @xmath54 , @xmath57 because all cells in the upper triangle are included .",
    "in this way , we have defined equation ( [ equation : j ] ) based on our job numbering policy , i.e. all job identifiers vary in and jobs are sequentially numbered left - to - right and top - to - bottom in the upper triangle ( see fig .",
    "[ fig : direct ] for an example ) .",
    "reversely , given a job identifier @xmath58",
    "( @xmath59 ) , we need to compute the coordinate @xmath40 in order to locate the corresponding variable pair . as per our definition , we have @xmath60 it needs to be stressed that there is surely an integer value @xmath39 satisfying these two inequalities based our job numbering policy mentioned above . by solving @xmath61 , we get @xmath62 this is because ( @xmath33 ) @xmath63 and thus @xmath64 has two distinct @xmath39 solutions theoretically , and ( @xmath65 ) all @xmath66 values are to the left of the symmetric axis @xmath67 , meaning strictly monotonically decreasing as a function of @xmath39 . meanwhile , by solving @xmath68 , we get @xmath69 similarly , this is because ( @xmath33 ) @xmath70 and thus @xmath71 has two distinct @xmath39 solutions theoretically , and ( @xmath65 ) all @xmath66 values are to the left of the symmetric axis @xmath72 , meaning strictly monotonically decreasing as a function of @xmath39 .    in this case , by defining @xmath73 , @xmath74 and @xmath75 , we can reformulate equations ( [ equation : inequality_y_hi ] ) and ( [ equation : inequality_y_low ] ) to be @xmath76 . in this case , because @xmath77 , we know that @xmath78 $ ] is a sub - range of @xmath79 and thereby have @xmath80 . as mentioned above , as a function of integer @xmath39 , equation ( [ equation : inequality_j ] ) definitely has @xmath39 solutions as per our definition , meaning that at least one integer exists in @xmath78 $ ] , which satisfies equation ( [ equation : inequality_j ] ) .",
    "meanwhile , it is known that there always exists one and only one integer in @xmath79 ( can be easily proved ) and this integer equals @xmath81 , regardless of the value of @xmath82 . since @xmath78 $ ] is a sub - range of @xmath83 , we can conclude that equation ( [ equation : inequality_j ] ) has a unique solution @xmath39 that is computed as @xmath84 having got @xmath39 , we can compute the coordinate @xmath38 as @xmath85 based on equation ( [ equation : j ] ) .",
    "besides this theoretical proof , we also wrote a computer program to test its correctness .",
    "tiled computation is a frequently used technique in various applications accelerated by accelerators such as cell / bes @xcite , gpus @xcite and phis @xcite .",
    "this technique partitions a matrix into a non - overlapping set of equal - sized @xmath86 tiles . in our case , we partition the job matrix and produce a tile matrix of size @xmath87 tiles , where @xmath88 equals @xmath89 . in this way , all jobs in the upper triangle of the job matrix are still fully covered by the upper triangle of the tile matrix . by treating a tile as a unit",
    ", we can assign a unique identifier to each tile in the upper triangle of the tile matrix and then build bijective functions between tile identifiers and tile coordinates in the tile matrix , similarly as we do for the job matrix .",
    "as mentioned above , we have proposed a bijective mapping between job identifier and coordinate space . because the tile matrix has an identical structure with the original job matrix",
    ", we can directly apply our aforementioned bijective mapping to the tile matrix . in this case ,",
    "given a coordinate @xmath90 ( @xmath91 ) in the upper triangle of the tile matrix , we can compute a unique tile identifier @xmath92 as @xmath93 where @xmath94 is defined similar to equation ( [ equation : f ] ) as @xmath95 likewise , given a tile identifier @xmath96 ( @xmath97 ) , we can reversely compute its unique vertical coordinate @xmath98 as @xmath99 and subsequently its unique horizontal coordinate @xmath100 as @xmath101      having got the coordinate @xmath90 of a tile , we can determine the coordinate range of all jobs per tile , relative to the original job matrix .",
    "more specifically , the vertical coordinate @xmath39 lies in @xmath102 and the horizontal coordinate @xmath38 in @xmath103 .",
    "consequently , the computation of a tile can be completed by looping over the coordinate ranges of both @xmath39 and @xmath38 .",
    "note that the jobs whose coordinate @xmath104 do not need to be computed since they lie beyond the upper triangle of the job matrix . for all - pairs pcc computation , every job has the same amount of computation . in this case , the ideal load balancing policy is supposed to distributing identical number of jobs onto each pe .",
    "herein , a thread on the phi is referred to as a pe . from section [ sec : xeon_phi ] , we know that each core on the phi has four hardware threads and a two - level private l1/l2 cache hierarchy with caches being connected via a bidirectional ring bus to provide coherent caching on the entire chip .",
    "this non - uniform cache access reminds us that we should keep the active hardware threads per core sharing as much data as possible with the intention to improve caching performance . in these regards ,",
    "our tiled computation schedules a tile to @xmath105 threads and lets these @xmath105 threads to compute the tile in parallel .",
    "note that we must guarantee that the number of threads is a multiple of @xmath105 within the parallel region of our phi kernel .",
    "algorithm [ alg : phi_tiled ] shows the pseudocode of the phi kernel of our tiled computation . due to the limited amount of device memory , we are not able to entirely reside the resulting @xmath37 correlation matrix @xmath35 in the phi for large @xmath24 values . to address this problem ,",
    "we partition the title identifier range @xmath106 into a set of equal - sized non - overlapping sub - ranges and adopt a multi - pass execution model by letting one pass compute one sub - range , denoted by @xmath107 for simplicity . to match this execution model , a result buffer @xmath108 of size @xmath109 elements is allocated on the phi and used to store the results of the @xmath110 tiles . for each tile ,",
    "its @xmath111 results are consecutively placed in @xmath108 .",
    "once a pass finishes , we transfer @xmath108 to the host , then extract the results per tile and finally store them in the resulting matrix @xmath35 allocated on the host .    to benefit from the wide 512-bit simd vector instructions , we have aligned each @xmath7-dimensional variable in @xmath30 to 64-byte memory boundary on the phi . in algorithm",
    "[ alg : phi_tiled ] , compiler directives are used to hint to the compiler to auto - vectorize the inner - most loop ( lines 18 and 20 in algorithm [ alg : phi_tiled ] ) . alternatively , we also manually vectorized the loop using simd instrinsic functions .",
    "the simd instrinsic functions used are _",
    "mm512_setzero_ps / pd , _",
    "mm512_load_ps / pd , _",
    "mm512_fmadd_ps / pd , _ mm512_mask3_fmadd_ps / pd , and _",
    "mm512_reduce_add_ps / pd for single / double precision floating point .",
    "interestingly , our manual - vectorization did not demonstrate obvious / significant performance advantage to auto - vectorization through our evaluations . more specifically",
    ", our manual - vectorization does run faster than auto - vectorization , but only by a tiny margin , on a single phi . considering that auto - vectorization is more portable than hard - coded simd intrinsic functions ,",
    "we have used auto - vectorization all through our implementations .      as mentioned above ,",
    "we rely on multiple passes of kernel execution to complete all - pairs computation .",
    "conventionally , having completed one pass , we transfer the newly computed results to the host , and do not initiate a new kernel execution until having completed the processing of the new results . in this way , the co - processor will be kept idle , while we transfer and process the results on the host side .",
    "a better solution would be to employ asynchronous kernel execution , which enables concurrent execution of host - side tasks and device - side kernel execution .",
    "fortunately , the offload model provides the signal and wait clauses to support for asynchronous data transfer and kernel execution .",
    "more specifically , the signal clause enables asynchronous data transfer in # pragma offload_transfer directives and asynchronous computation in # pragma offload directives .",
    "the wait clause blocks the current execution until an asynchronous data transfer or computation has completed .",
    "note that the signal and wait clauses are associated with each other via a unique value . in our implementation ,",
    "we have used a double - buffering approach to facilitate asynchronous computation .",
    "algorithm [ alg : async ] gives the pseudocode of our asynchronous implementation .      on phi clusters ,",
    "two distributed computing models can be used to develop parallel and distributed algorithms .",
    "one model is mpi offload model , which launches mpi processes just as an ordinary cpu cluster does .",
    "the difference is that one or more phi coprocessors will be associated to a parental mpi process and this parental process will utilize offload pragmas / directives to interact with the affiliated phis . in this model ,",
    "communications between phis have to be explicitly managed by their parental processes and it is not a necessity for phis to be aware of the existence of remote communications between mpi processes .",
    "the other model is symmetric model , which treats a phi as a regular computer interconnected to form a compute cluster .",
    "one advantage of symmetric model to mpi offload model is that symmetric model allows for the execution of existing mpi programs designed for cpu clusters to be directly executed on phi clusters , with no need of re - programming the code .",
    "nonetheless , considering different architectural features between cpus and phis , some amount of efforts may have to be devoted to performance tuning on phi clusters .    in lightpcc , we used mpi offload model with the requirement of one - to - one correspondence between mpi processes and phis .",
    "this pairing is straightforward for the cases launching one mpi process into one node .",
    "however , it would become more complex when a node has multiple phis available and multiple processes running .",
    "this is because multiple processes launched into the same node have no idea about which phi should be associated to themselves . to address this problem ,",
    "we have used the registration - based management mechanism used by @xcite for paring mpi processing and phis .",
    "our distributed implementation is also based on tiled computation on the phi .",
    "given @xmath112 mpi processes , we evenly distribute tiles onto the @xmath112 processes with the @xmath33-th ( @xmath113 ) process assigned to compute the tiles whose identifiers are in @xmath114 . within each process",
    ", we adopt the same asynchronous control workflow with the computation for single phis ( see algorithm [ alg : async ] ) and execute the same tiled computation kernel on the affiliated phi in each pass ( see algorithm [ alg : phi_tiled ] ) .",
    "note that the initialization of variables @xmath115 and @xmath116 ( lines 2@xmath1173 in algorithm [ alg : async ] ) must be changed accordingly for each process . concretely , @xmath118 should initialize @xmath115 to be @xmath119 , and @xmath116 to be @xmath120 .      as mentioned above",
    ", we reformulate the computation of pcc by transforming each original variable @xmath26 to a new representation @xmath25 based on equation ( [ equation : u ] ) . this variable transformation for input set",
    "@xmath19 only needs to be done once beforehand , and is also embarrassingly parallel since the transformation per variable is mutually independent . on the other hand ,",
    "each variable requires identical amount of computation since these variables have the same dimension . in these regards , we parallelize the variable transformation by evenly distributing variables onto all threads on the phi .    algorithm [ alg : var_transform ] gives the pseudocode of the phi kernel of our variable transformation . in algorithm",
    "[ alg : var_transform ] , for each variable @xmath26 the transformation consists of three steps .",
    "step 1 ( lines 7@xmath11713 ) computes the mean of all elements in @xmath26 and requires @xmath7 unit arithmetic operations .",
    "step 2 ( lines 14@xmath11720 ) computes the variance of all elements and takes @xmath121 unit arithmetic operations if considering a fused multiply - add operation as a unit one .",
    "step 3 ( lines 21@xmath11725 ) finishes the transformation of @xmath26 to @xmath25 and also needs @xmath121 unit arithmetic operations .",
    "therefore , the total computational cost of variable transformation can be estimated as @xmath122 unit arithmetic operations .",
    "on the other hand , for symmetric all - pairs computation using equation ( [ equation : pearson2 ] ) , its computational cost can be estimated to be @xmath123 unit arithmetic operations .",
    "consequently , the overall computational cost of our method can be estimated to be @xmath124 unit arithmetic operations .",
    "we evaluated lightpcc from three perspectives : ( @xmath33 ) performance comparison with the sequential alglib ( version 3.10.0 ) , ( @xmath65 ) performance comparison with an implementation based on the cblas_dgemm gemm routine in mkl , which first transforms variables based on equation ( [ equation : u ] ) ( refer to algorithm [ alg : var_transform ] ) and then applies gemm based on equation ( [ equation : pearson2 ] ) , and ( @xmath125 ) parallel scalability evaluation on a phi cluster , using a set of artificial and real gene expression datasets .",
    "all tests are conducted on 8 compute nodes in cyence hpc cluster ( iowa state university ) , where each node has two intel e5 - 2650 8-core 2.0 ghz cpus , two 5110p phis ( each has 60 cores and 8 gb memory ) and 128 gb memory .",
    "every program is compiled by intel c++ compiler v15.0.1 with option -fast enabled .",
    "meanwhile , when two processes run in a node , we used the environment variable i_mpi_pin_processor_list to guide intel mpi runtime system to pin two processes per node to distinct cpus ( recall that a node has two cpus ) .    for lightpcc , we set the tile size to @xmath126 ( i.e. @xmath127 ) and configure each core to run four hardware threads associated with the compact openmp thread affinity mode ( i.e. 236 actives threads on 59 cores ) .",
    "in this implementation , we schedule one tile to a core at a time , and let all four threads per core compute the same tile in parallel , with one thread processing one column . in this way",
    ", the four threads per core will access the same row variable , thereby improving data sharing .",
    "however , even though a 5110p phi executes four hardware threads per core in order @xcite , the four threads on each core are actually scheduled individually and independently by the operating system .",
    "hence , we can not guarantee that the four hardware threads per core always work on the same tile at any instant . in this regard ,",
    "we have introduced a software - based centralized barrier @xcite , which is implemented using the atomic intrinsic function _ _",
    "sync_fetch_and_sub , in order to synchronize the four hardware threads per core each time they finish their computation on a tile ( note that cores are configured to have independent software barriers ) .",
    "unfortunately , we observed slight performance decrease after using software barriers through our evaluations . in this regard , we have decided not to use software barriers both in our implementation and following tests . in addition , for each evaluated program , we used double - precision floating point for fair comparison and averaged its five runs to get the runtime .",
    "we first evaluated the performance of lightpcc , alglib and the implementation using mkl ( refer to as mkl for short in the following ) using three artificial gene expression datasets by randomly generating gene expression values in @xmath128 $ ] .",
    "this is reasonable because the runtime of pcc computation is merely subject to @xmath24 and @xmath7 and independent of specific values .",
    "they are randomly generated by setting @xmath24 to 16,000 ( 16k ) , 32,000 ( 32k ) or 64,000 ( 64k ) and @xmath7 to 5,000 ( 5k ) .",
    "table [ tab : artificial_alglib ] shows the performance comparison between lightpcc and alglib .",
    "compared to alglib , lightpcc runs @xmath129 , @xmath130 and @xmath0 faster using one phi and @xmath131 , @xmath132 , and @xmath1 faster using 16 phis for @xmath24=16k , @xmath24=32k and @xmath24=64k , respectively .",
    "moreover , the speedup gradually increases as @xmath24 grows larger .",
    "it is worth mentioning that many applications require determining the statistical significance of pairwise correlation . for this purpose ,",
    "permutation test is a frequently used approach for statistical inference .",
    "however , this approach needs to repeatedly permute vector variables at random and compute pairwise correlation from the random data , where the more iterations ( typically iterations ) are conducted , the more precise statistical results ( e.g. @xmath133-value ) can be expected ( except for the cases of complete permutation tests that rarely happen ) . in this case",
    ", the runtime with a specified number of permutation tests can be roughly inferred from the runtime per iteration and the number of iterations conducted .",
    ".comparison with alglib on artificial data [ cols= \" < ,",
    "< , < , < , < , < , < \" , ]     on this real dataset , compared to alglib , lightpcc runs @xmath134 faster on a single phi , @xmath135 faster on 2 phis , @xmath136 faster on 4 phis , @xmath137 faster on 8 phis , and @xmath138 faster on 16 phis ( see table [ tab : human ] ) . in comparison to single - threaded mkl , lightpcc runs @xmath139 faster on a single phi and @xmath140 faster on 16 phis . when it comes to 16-threaded mkl",
    ", lightpcc is not able to outperform the former until @xmath141 phis are used , similar to the assessment using artificial datasets . for this case ,",
    "our algorithm reaches a maximum speedup of 4.3 with 16 phis .",
    "furthermore , we evaluated mkl on a single phi as well .",
    "the experimental result showed that it took 11.6 seconds for phi - based mkl to finish the computation , resulting in a speedup of 2.77 over our algorithm on one phi .",
    "as for parallel scalability , lightpcc also demonstrates good performance ( refer to figure [ fig : real ] ) , where compared to the execution on one phi , the speedup is 1.6 on 2 phis , 3.2 on 4 phis , 6.2 on 8 phis and 11.9 on 16 phis , respectively .",
    "pcc is a correlation measure investigating linear relationship between continuous random variables and has been widely used in bioinformatics . for instance , one popular application is to compute pairwise correlation between gene expression profiles and then build a gene co - expression network to identify common regulation and thus common functions . in addition , pcc can be applied to some computational problems ( e.g. feature selection @xcite and correlation clustering @xcite ) in machine learning as well .    in this paper ,",
    "we have presented lightpcc , the first parallel and distributed all - pairs pcc computation algorithm on phi clusters .",
    "it is written in c++ template classes and harnesses three levels of parallelism ( i.e. simd - instruction - level parallelism , thread - level parallelism and accelerator - level parallelism ) to achieve high performance .",
    "furthermore , we have proposed a general framework for workload balancing in symmetric all - pairs computation .",
    "this framework assigns unique identifiers to jobs in the upper triangle of the job matrix and builds bijective functions between job identifier and coordinate space .",
    "we have evaluated the performance of lightpcc using a set of gene expression profiles and further compared it to a sequential c++ implementation in alglib and an implementation using the cblas_dgemm gemm routine in mkl , both of which run on the cpu .",
    "performance evaluation showed that lightpcc runs up to @xmath0 faster than alglib on one 5110p phi and up to @xmath1 faster on 16 phis , with a corresponding speedup of up to 6.8 on one phi and up to 71.4 on 16 phis over single - threaded mkl .",
    "besides , lightpcc yielded good parallel scalability with respect to varied number of phis .",
    "as part of our future work , we plan to apply this work to construct genome - wide gene expression network ( e.g. from conventional microarray data @xcite , emerging rna - seq data @xcite @xcite or diverse genomic data @xcite ) and integrate it with statistical and graph analysis methods to identify critical pathways .",
    "in addition , our current implementation does not distribute pcc computation onto cpu cores .",
    "therefore , we expect to further boost its performance by employing an alternative cpu - phi coprocessing model that enables concurrent workload distribution onto both cpus and phis .",
    "this research is supported in part by us national science foundation under iis-1416259 and an intel parallel computing center award .",
    "_ conflict of interest _ : none declared .",
    "a.  j. butte and i.  s. kohane , `` unsupervised knowledge discovery in medical databases using relevance networks . '' in _ proceedings of the amia symposium_.1em plus 0.5em minus 0.4emamerican medical informatics association , 1999 , p. 711 .",
    "a.  a. margolin , i.  nemenman , k.  basso , c.  wiggins , g.  stolovitzky , r.  d. favera , and a.  califano , `` aracne : an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context , '' _ bmc bioinformatics _",
    ", vol .  7 , no .",
    "suppl 1 , p.  s7 , 2006 .",
    "chang , a.  h. desoky , m.  ouyang , and e.  c. rouchka , `` compute pairwise manhattan distance and pearson correlation coefficient of data points with gpu , '' in _ acis international conference on software engineering , artificial intelligences , networking and parallel / distributed computing_.1em plus 0.5em minus 0.4emieee , 2009 , pp .",
    "501506 .",
    "e.  kijsipongse , c.  ngamphiw , s.  tongsima _ et  al .",
    "_ , `` efficient large pearson correlation matrix computing using hybrid mpi / cuda , '' in _ international joint conference on computer science and software engineering_.1em plus 0.5em minus 0.4emieee , 2011 , pp . 237241 .    y.  wang , h.  du , m.  xia , l.  ren , m.  xu , t.  xie , g.  gong , n.  xu , h.  yang , and y.  he , `` a hybrid cpu - gpu accelerated framework for fast mapping of high - resolution human brain connectome , '' _ plos one _ , vol .  8 , no .  5 , p. e62789 , 2013 .",
    "y.  wang , m.  j. anderson , j.  d. cohen , a.  heinecke , k.  li , n.  satish , n.  sundaram , n.  b. turk - browne , and t.  l. willke , `` full correlation matrix analysis of fmri data on intel xeon phi coprocessors , '' in _ proceedings of the international conference for high performance computing , networking , storage and analysis_.1em plus 0.5em minus 0.4emacm , 2015 .      y.  liu , t .-",
    "tran , f.  lauenroth , and b.  schmidt , `` swaphi - ls : smith - waterman algorithm on xeon phi coprocessors for long dna sequences , '' in _ ieee international conference on cluster computing_.1em plus 0.5em minus 0.4emieee , 2014 , pp .",
    "257265 .",
    "s.  misra , k.  pamnany , and s.  aluru , `` parallel mutual information based construction of genome - scale networks on the intel xeon phi coprocessor , '' _ ieee / acm transactions on computational biology and bioinformatics _ , vol .",
    "12 , no .  5 , pp . 10081020 , 2015 .",
    "l.  jin , z.  wang , r.  gu , c.  yuan , and y.  huang , `` training large scale deep neural networks on the intel xeon phi many - core coprocessor , '' in _ 2014 ieee international parallel & distributed processing symposium workshops_.1em plus 0.5em minus 0.4emieee , 2014 , pp .",
    "16221630 .",
    "a.  viebke and s.  pllana , `` the potential of the intel xeon phi for supervised deep learning , '' in _ 17th ieee international conference on high performance computing and communications_.1em plus 0.5em minus 0.4emieee , 2015 , pp .",
    "758765 .",
    "y.  liu , b.  schmidt , and d.  l. maskell , `` msa - cuda : multiple sequence alignment on graphics processing units with cuda , '' in _ ieee international conference on application - specific systems , architectures and processors_.1em plus 0.5em minus 0.4emieee , 2009 , pp .",
    "121128 .",
    "t.  kiefer , p.  b. volk , and w.  lehner , `` pairwise element computation with mapreduce , '' in _ proceedings of the 19th acm international symposium on high performance distributed computing_.1em plus 0.5em minus 0.4emacm , 2010 , pp .",
    "826833 .",
    "q.  zhu , a.  k. wong , a.  krishnan , m.  r. aure , a.  tadych , r.  zhang , d.  c. corney , c.  s. greene , l.  a. bongo , v.  n. kristensen _",
    "_ , `` targeted exploration and analysis of large cross - platform human transcriptomic compendia , '' _ nature methods _ , vol .  12 , no .  3 , pp .",
    "211214 , 2015 .",
    "x.  pan , d.  papailiopoulos , s.  oymak , b.  recht , k.  ramchandran , and m.  i. jordan , `` parallel correlation clustering on big graphs , '' in _ advances in neural information processing systems _ , 2015 , pp .",
    "f.  zhao , y.  qu , j.  liu , h.  liu , l.  zhang , y.  feng , h.  wang , j.  gan , r.  lu , and d.  mu , `` microarray profiling and co - expression network analysis of lncrnas and mrnas in neonatal rats following hypoxic - ischemic brain damage , '' _ scientific reports _ , vol .  5 , 2015 .",
    "b.  issac , n.  f. tsinoremas , and e.  capobianco , `` abstract b1 - 04 : co - expression profiling and transcriptional network evidences from rna - seq data reveal specific molecular subtype features in breast cancer , '' _ cancer research _",
    ", vol .  75 , no . 22 supplement 2 , pp .",
    "b104 , 2015 .",
    "b.  wang , a.  m. mezlini , f.  demir , m.  fiume , z.  tu , m.  brudno , b.  haibe - kains , and a.  goldenberg , `` similarity network fusion for aggregating data types on a genomic scale , '' _ nature methods _ , vol .",
    "11 , no .  3 , pp .",
    "333337 , 2014 ."
  ],
  "abstract_text": [
    "<S> co - expression network is a critical technique for the identification of inter - gene interactions , which usually relies on all - pairs correlation ( or similar measure ) computation between gene expression profiles across multiple samples . </S>",
    "<S> pearson s correlation coefficient ( pcc ) is one widely used technique for gene co - expression network construction . </S>",
    "<S> however , all - pairs pcc computation is computationally demanding for large numbers of gene expression profiles , thus motivating our acceleration of its execution using high - performance computing . in this paper , we present lightpcc , the first parallel and distributed all - pairs pcc computation on intel xeon phi ( phi ) clusters . </S>",
    "<S> it achieves high speed by exploring the simd - instruction - level and thread - level parallelism within phis as well as accelerator - level parallelism among multiple phis . to facilitate balanced workload distribution , </S>",
    "<S> we have proposed a general framework for symmetric all - pairs computation by building bijective functions between job identifier and coordinate space for the first time . </S>",
    "<S> we have evaluated lightpcc and compared it to two cpu - based counterparts : a sequential c++ implementation in alglib and an implementation based on a parallel general matrix - matrix multiplication routine in intel math kernel library ( mkl ) ( all use double precision ) , using a set of gene expression datasets . </S>",
    "<S> performance evaluation revealed that with one 5110p phi and 16 phis , lightpcc runs up to @xmath0 and @xmath1 faster than alglib , and up to @xmath2 and @xmath3 faster than single - threaded mkl , respectively . </S>",
    "<S> in addition , lightpcc demonstrated good parallel scalability in terms of number of phis . </S>",
    "<S> source code of lightpcc is publicly available at http://lightpcc.sourceforge.net .    </S>",
    "<S> pearson s correlation coefficient ; co - expression network ; all - pairs computation ; intel xeon phi cluster </S>"
  ]
}