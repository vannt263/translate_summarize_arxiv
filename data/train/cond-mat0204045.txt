{
  "article_text": [
    "in the manifold of structures which are used as information carriers in nature , culture and engineering , linear strings consisting of sequences of letters play a central role .",
    "this may be demonstrated by the following examples : the proteins and polynucleotids , the main information carriers in living systems are sequences of amino acids and/or nucleotides  @xcite .",
    "further most of the messages transporting information between human informational systems have the form of strings of letters .",
    "examples are books or letters , music , computer programs etc . by using the methods of symbolic dynamics any trajectory of a dynamic system",
    "may be mapped to a string of letters on certain alphabet  @xcite@xcite .",
    "hence each sequence can be interpreted as the trajectory of a discrete dynamic system .",
    "this work is devoted to the investigation of strings of the type characterized above , i.e.  to objects ( documents , programs etc . ) which may be mapped to strings of letters .",
    "the main aim of the investigation is the analysis of long range correlations in information carrying strings . for several reasons we expect the existence of long range structures in these sequences  @xcite . especially we expect correlations which range from the beginning of a string up to its end .",
    "let us discuss some of the reasons for this behavior :    1 .",
    "predictability : we know from our every - day experience and from scientific research that the identification of the first hundred or thousand letters of the string tells us already a lot about the continuation .",
    "often we make a decision in a book shop after reading just one page . for example",
    ", if we find there several times the word _ love _ and _ tennis _ we expect to find them on the other pages again and again , but if we find first words like _ file _ and _ program _ we expect to remain in quite another field .",
    "listening to the beginning of a bach praeludium where the general theme is worked out we expect to hear it again in many variations up to the very end .",
    "such expectations are only justified if there exist indeed long range correlations .",
    "this is the scientific expression of our intuitive expectations which are based on the experience that texts and music have certain inherent predictability .",
    "syntactical limitations : another heuristic reason to expect long range correlations is the exponential explosion of the variety of possible subwords with increasing length for uncorrelated strings .",
    "uncorrelated sequences generated on an alphabet of @xmath0 letters have a variety of @xmath1 different subwords of length @xmath2 .",
    "a subword is here any combination of letters including the space , punctuation marks etc .",
    "strings of this type may be generated by stochastic processes of bernoulli  type or by fully developed chaotic dynamics using alphabets of @xmath0 letters .",
    "for @xmath3 the number @xmath4 is extremely large .",
    "in other words we need very sharp restrictions to select a meaningful subset out of it .",
    "long range correlations provide such a selection criterion . denoting the selected subset by @xmath5 we expect @xmath6 bounds of this kind are given by syntax and semantics .",
    "the syntactical rules do not allow for an arbitrary concatenation of words to sentences , most of them are forbidden .",
    "furthermore we know that texts ( and pieces of music ) are formed by keywords ( motifs ) which are the raw material for the generation of a text ( a piece of music ) .",
    "in fact all these rules lead to slower growth with @xmath2 . +",
    "a rather sharp restriction on the growth with @xmath2 corresponds to the power law @xmath7 symbolic strings generated by the logistic map in the feigenbaum  point have this scaling property  @xcite .",
    "the conjecture that several natural objects have this type of scaling has been made earlier  @xcite .",
    "we will show here that pieces of music possibly belong to this class .",
    "another growth law which is much faster is the stretched exponential law @xmath8 this scaling which is typically for intermittent processes was observed for several texts  @xcite .",
    "we mention that in the limit @xmath9 this law corresponds to the scaling in eq .",
    "( [ scaling ] ) .",
    "the reduction due to the scaling rule  ( [ scaling_rule ] ) is not as strong as in the scaling rule  ( [ scaling ] ) , however , the reduction factor corresponding to rule  ( [ scaling_rule ] ) is still enormous for large @xmath2 .",
    "3 .   evolution : the third reason to expect such behavior is the rather general idea that evolution operates in regions where long relaxation times , @xmath10-noise and other long range correlations are essential  @xcite .",
    "in the following section we will study entropy - like quantities as a measure of the long range correlations in sequences . in order to describe the structure of a given string of length @xmath11 using an alphabet of @xmath0 letters @xmath12 we introduce the following notations  @xcite :    let @xmath13 be the letters of a given substring of length @xmath14 . furthermore let @xmath15 be the probability to find in a string a block of length @xmath2 ( subword of length @xmath2 ) with the letters @xmath16 .",
    "the probability of having a pair with @xmath17 arbitrary letters in between we denote by @xmath18 we introduce the following quantities :    1 .",
    "the mutual information , also called transinformation  @xcite @xmath19 2 .",
    "the entropy per subword of length @xmath2 @xmath20 3 .   the uncertainty of the letter following a block of length @xmath2 @xmath21 4 .",
    "the entropy of the source ( related to the kolmogorov ",
    "sinai entropy ) @xmath22    in an earlier paper @xcite we assumed the scaling behavior @xmath23 related assumptions have been made independently by several authors  @xcite . from that point of view the long range order of strings",
    "may be well characterized by the asymptotic for large @xmath2 .",
    "chaotic and stochastic strings of the standard type have the property @xmath24 .",
    "special cases are bernoulli processes or fully developed chaos with @xmath25 in the following we write all entropy measures in units of @xmath26 . for markov processes with memory",
    "@xmath27 the uncertainty decreases during the first @xmath27 steps and remains then constant @xmath28 on the other hand any string with a finite period @xmath29 is characterized by @xmath30 this means that any new letter added to a string does not increase the complexity of the sequence .",
    "consequently we find for periodic strings @xmath31 and @xmath32 .",
    "of special interest to our further considerations are systems which are neither periodic nor chaotic and which are characterized by @xmath33 presently there are not enough data available to estimate the limit entropy @xmath34 for homogeneous texts ( written by one author ) .",
    "we follow in this respect the seminal work by claude e. shannon who concluded in his pioneering paper : `` from this analysis it appears that , in ordinary literary english , the long range statistical effects ( up to 100 letters ) reduce the entropy  '' .",
    "shannon gave an estimate of 40 bits for the entropy of @xmath35 letters , i.e. about 0.4 bit per letter .",
    "transforming the bits to @xmath0units ( which we use throughout this paper ) we get @xmath36 and we find for the entropy per letter the value @xmath37 . according to several general investigations  @xcite it is not likely that the uncertainty ( entropy per letter ) decreases still further for larger @xmath2n . based on these considerations we assume in the following that the limit entropy ( in @xmath0units ) is in the region @xmath38 since a reliable estimate seems to be impossible at present we simply neglect the contribution @xmath39 to the block entropy in eq .",
    "( [ ebeling_scaling ] ) . in this way we obtain for the intermediate region @xmath40 the formula which will be the basis for our further investigations @xmath41 special cases are :    * logarithmic tails @xmath42 * power law tails @xmath43    the working hypothesis developed earlier  @xcite is that strings characterized by eqs .",
    "( [ logtails ] ) or ( [ powertails ] ) being on the borderline between order and chaos might be prototypes of information carrying sequences .    following a relation derived by mcmillan and khinchin the @xmath2letter entropy and the mean number of subwords of length @xmath2 are related  @xcite @xmath44 in this way a logarithmic entropy scaling corresponds to a power law of the numbers of subwords and a power law scaling of the entropy corresponds to a stretched exponential growth of the number of subwords .    the mutual information ( transinformation ) defined by eq .",
    "( [ transinformation ] ) is not monotonically decreasing with increasing @xmath2 .",
    "we define here long range tails as any non exponential decay or increase of the averaged transinformation @xmath45 .",
    "periodic strings show correlations of infinite long range .",
    "periodicity with the period @xmath29 implies that all conditional probabilities @xmath46 for @xmath47 are also periodic .",
    "this leads to a periodic behavior of the transinformation .",
    "therefore sequences with long range correlations show a fluctuating behavior at large scales  @xcite .",
    "printed texts in natural languages and music written in the language of notes are examples of information  carrying strings .",
    "other examples , which we do not consider here , are biosequences , where some evidence for the existence of long range correlations was found  @xcite . originally texts or pieces of music were generated by the writer or composer as a dynamical process in real time .",
    "today we find in books or albums the frozen in results of this process in the form of a symbolic sequence .",
    "certainly texts or pieces of music are symbolic sequences of high complexity . in contrast to other dynamical processes , writing and composing have developed during a long way of evolution being intended for communication between human beings . in spite of all these difficulties",
    "let us now follow the largely simplifying assumption due to shannon , mandelbrot and others that texts and pieces of music may be considered as the results of a stationary random process .",
    "although this assumption is still controversial we will take it here as a basis for the further analysis . in our analysis",
    "we considered the following sequences :         + the last symbol @xmath51 stands for the empty space and # for any number . in the case of music",
    "we encoded the notes for @xmath52 octaves beginning with the low @xmath53 and ending with the high @xmath54 on an alphabet with 32 symbols .",
    "the white keys on the piano forte where encoded by the 18 symbols +       + the pause was encoded by the score `` _ '' and holding a tone by the ``  '' . in order to get a better statistics we also used compressed alphabets consisting of 3 letters @xmath55 , @xmath56 , @xmath11 only .",
    "the letter @xmath55 codes for vowels in the case of texts or for a move down in the case of music , the letter @xmath56 stands for consonants or move ups , the letter @xmath11 codes for all other symbols , e.g.  pauses ( spaces ) , holding the tone and punctuation marks . for the analysis of the mutual information we have to count here the frequencies of pairs .",
    "since the number of different pairs is @xmath57 we have a rather good statistics if the length @xmath11 of the string satisfies the inequality @xmath58 . as shown by several authors",
    "@xcite the transinformation is a reliable measure of the correlations of letters at the distance @xmath2 .",
    "every peak at @xmath2 in the transinformation corresponds to a strong positive correlation . in fig .",
    "[ mutual_beethoven_bach_mozart ] we show the pair correlations of the beethoven sonata and of music by bach and mozart  @xcite@xcite .",
    "the peaks show that there exist strong correlations between two notes at characteristic distances .",
    "the interpretation of these peaks must be left to specialists .",
    "we further notice some similarity in the correlation structure of bach s and beethoven s music and a distinct different structure of mozart s music .",
    "a more detailed study of the differences between composers will be given in a separate paper  @xcite .    in figs .",
    "[ mutual_moby_grimm ] and  [ trans_moby ] the mutual information calculated for moby dick and for grimm s tales ( @xmath59 ) are drawn .",
    "the results show that there are well expressed correlations in the range @xmath60 which are followed by a long slowly decaying tail .",
    "the results for the transinformation @xmath61 become meaningless if the values are smaller than the level of natural fluctuations due to the finite length @xmath11 of the text which is  @xcite @xmath62 although the fluctuation level decays with @xmath63 it has even for the rather long text moby dick with @xmath64 a value of about 0.00012 .",
    "this means , as seen from fig .",
    "[ trans_moby ] that any conclusions suggesting long range correlations beyond @xmath65 are rather problematic",
    ". however , the range of studies of @xmath61 may still be extended by using length corrections @xcite .",
    "an alternative method is based on studies of the dependence of @xmath61 on @xmath63 ( which presumably is linear ) and by extrapolations @xmath66 @xcite .    as we see from figs .",
    "[ mutual_moby_grimm ] and  [ trans_moby ] the mutual information decreases monotonously up to @xmath67 and converges into the fluctuation level .",
    "there are no well expressed correlation peaks . evidently long texts are in this respect less correlated than dna sequences where well expressed long range pair correlations have been found  @xcite .    the results obtained so far base only on the statistical distributions of pairs . in this case",
    "one can reach a rather good statistics by counting the probability of pairs .",
    "let us study now the distribution of words of length @xmath68 .",
    "due to the fact that the number of different words of length @xmath2 using an alphabet consisting of 32 letters is @xmath69 there are for @xmath70 already @xmath71 words with approximately @xmath72 letters .",
    "this is much more than all texts stored in libraries which have been estimated to consist of about @xmath73 letters  @xcite .",
    "therefore we could not do a statistic analysis if there were no additional constraints due to grammar and semantics .",
    "according to an earlier estimate  @xcite we expect that the growth law scales as @xmath74 with @xmath75 for texts .",
    "we have done the analysis with two long texts , namely moby dick and grimm s tales .",
    "[ rank_moby ] and  [ rank_grimm ] show the rank ordered distribution of subwords of length @xmath76 .",
    "the structures of both distributions are similar but the lists of words are quite different .",
    "for example among the most frequent subwords of length @xmath77 in the case of moby dick are `` _",
    "_ '' , and `` _ the_quarter_deck _ '' . for grimm",
    "s tales rather frequent subwords of length @xmath78 are e.g. `` _ _ if_i_could_but_shudder . _",
    "_ '' and `` _ _ princess,_open_the_door _ _ '' .",
    "the shapes of the subword distributions are distinctly not zipf  like , they do not follow a power law .",
    "the distribution tends to form a fermi  like plateau .",
    "this tendency is based on the theorem of asymptotic equipartition derived by mcmillan  @xcite and khinchin  @xcite .",
    "this theorem proves that for @xmath79 the asymptotic form of the distribution is rectangular , i.e. @xmath80 the effects due to finite @xmath2 and to the finite text length @xmath11 tend to smooth the edges of its distribution . since our texts are rather long the finite size effects do not have too much influence to the distributions .",
    "much more difficult is the analysis of relative short pieces of music . here",
    "additional problems arise due to the short sample .",
    "for example beethoven s sonata consists of only 4,920 letters ( notes ) .",
    "the importance of length corrections for estimating the frequencies of words was considered by several authors  @xcite . for a deeper analysis of this problem",
    "we refer to  @xcite .",
    "the method we used here was found by generalizing a method proposed in @xcite . according to this method the unknown distribution function of the words",
    "is guessed by a comparison of expected ( generated ) and observed distributions . instead of the simple rectangular distributions in eq .",
    "( [ rectangular ] ) we applied a more realistic expression .",
    "for given word length @xmath2 we guess the true ( not normalized ) distribution , i.e. the distribution for @xmath81 , in the form @xmath82 with @xmath83 this distribution has 7 free parameters , one of them is fixed by the condition that the total number of words is given by the size of the text . for a string of length @xmath11 the total number of @xmath2words is @xmath84 .",
    "each of the parameters has a simple meaning as : +       the authors thank h.  herzel and a.  schmitt for fruitful discussions , c.  frmmel for the permission to compare in fig .",
    "[ mutual_beethoven_bach_mozart ] with an unpublished result on mozart s music and the _ project gutenberg etext , illinois benedictine college , lisle _ for providing the ascii  text of `` moby dick '' and `` grimm s fairy tales '' .",
    "albrecht ,  k .- f .",
    ", ebeling ,  w. , frmmel ,  c. & frmmel ,  chr . [ 1994 ] `` entropy , mutual information and subword distribution for information - carrying strings : pieces of music '' , _ preprint _ itp-302 ( humboldt  university , berlin ) ."
  ],
  "abstract_text": [
    "<S> we investigate correlations in information carriers , e.g. texts and pieces of music , which are represented by strings of letters . for information carrying strings generated by one source ( i.e. a novel or a piece of music ) we find correlations on many length scales . </S>",
    "<S> the word distribution , the higher order entropies and the transinformation are calculated . </S>",
    "<S> the analogy to strings generated through symbolic dynamics by nonlinear systems in critical states is discussed .    2 </S>"
  ]
}