{
  "article_text": [
    "principal component analysis ( pca ) , also known as the karhunen - love transform or the hotelling transform , is a well - known method in machine learning , signal processing and statistics  @xcite .",
    "pca essentially builds an orthogonal transform to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables .",
    "pca has been used for manifold description and dimensionality reduction in a wide range of applications because of its simplicity , energy compaction , intuitive interpretation , and invertibility .",
    "nevertheless , pca is hampered by data exhibiting nonlinear relations . in this paper , we present a nonlinear generalization of pca that , unlike other alternatives , keeps all the above mentioned appealing properties of pca .      in recent years",
    ", several dimensionality reduction methods have been proposed to deal with manifolds that can not be linearly described ( see  @xcite for a comprehensive review ) : the approaches proposed range from local methods  @xcite , to kernel - based and spectral decompositions  @xcite , neural networks  @xcite , and projection pursuit methods  @xcite . however , despite the advantages of nonlinear methods , classical pca still remains the most widely used dimensionality reduction technique in real applications .",
    "this is because pca : 1 ) is easy to apply , 2 ) involves solving a convex problem , for which efficient solvers exist , 3 ) identifies features which are easily interpretable in terms of original variables , and 4 ) has a straightforward inverse and out - of - sample extension .    the above properties , which are the base of the success of pca , are not always present in the new nonlinear dimensionality reduction methods due either to complex formulations , to the introduction of a number of non - intuitive free parameters to be tuned , to their high computational cost , to their non - invertibility or , in some cases , to strong assumptions about the manifold .",
    "more plausibly , the limited adoption of nonlinear methods in daily practice has to do with the lack of feature and model interpretability . in this regard ,",
    "the usefulness of data description methods is tied to the following properties :    1 .",
    "_ invertibility of the transform .",
    "_ it allows both characterizing the transformed domain and evaluating the quality of the transform . on the one hand",
    ", inverting the data back to the input domain is important to understand the features in physically meaningful units , while analyzing the results in the transformed domain is typically more complicated ( if not impossible ) . on the other hand ,",
    "invertible transforms like pca allow the assessment of the dimensionality reduction errors as simple reconstruction distortion .",
    "_ geometrical interpretation of the manifold .",
    "_ understanding the system that generated the data is the ultimate goal of manifold learning .",
    "inverting the transform is just one step towards knowledge extraction .",
    "geometrical interpretation and analytical characterization of the manifolds give us further insight into the problem .",
    "ideally , one would like to compute geometric properties from the learned model , such as the curvature and torsion of the manifold , or the metric induced by the data .",
    "this geometrical characterization allows to understand the latent parameters governing the system .",
    "it is worth noting that both properties are scarcely achieved in the manifold learning literature .",
    "for instance , _ spectral _ methods do not generally yield intuitive mappings between the original and the intrinsic curvilinear coordinates of the low dimensional manifold . even though a metric can be derived from particular kernel functions  @xcite ,",
    "the interpretation of the transformation is hidden behind an implicit mapping function , and solving the pre - image problem is generally not straightforward  @xcite . in such cases ,",
    "the application of ( indirect ) evaluation techniques has become a relevant issue for methods leading to non - invertible transforms  @xcite .",
    "one could argue that direct and inverse transforms can be alternatively derived from mixtures of local models  @xcite .",
    "however , the effect of these local alignment operations in the metric is not trivial . in the same way , explicit geometric descriptions of the manifold , such as the computation of curvatures , is not obvious from other invertible transforms , as autoencoders or deep networks  @xcite .    in this paper",
    ", we introduce the principal polynomial analysis ( ppa ) , which is a _ nonlinear generalization of pca _ that still shares all its important properties .",
    "ppa is computationally easy as it only relies on matrix inversion and multiplication , and it is robust since it reduces to a series of marginal ( univariate ) regressions .",
    "ppa implements a volume - preserving and invertible map .",
    "not only the features are easy to interpret in the input space but , additionally , the analytical nature of ppa allows to compute classical geometrical descriptors such as curvature , torsion and the induced metric at any point of the manifold .",
    "applying the learned transform to new samples is also as straightforward as in pca .",
    "preliminary versions of ppa were presented in  @xcite , and applied to remote sensing in  @xcite .",
    "however , those conference papers did not study the analytical properties of ppa ( volume preservation , invertibility , and model geometry ) , nor compared with approaches that follow similar logic like nl - pca .",
    "the proposed ppa method can be motivated by considering the conditional mean of the data .",
    "in essence , pca is optimal for dimensionality reduction in a mean square error ( mse ) sense _ if and only if _ the conditional mean in each principal component is constant along the considered dimension .",
    "hereafter , we will refer to this as the _ conditional mean independence _ assumption .",
    "unfortunately , this symmetry requirement does not apply in general , as many datasets live in non - gaussian and/or curved manifolds .",
    "see for instance the data in fig .",
    "[ fig2 ] ( left ) : the dimensions have a nonlinear relation even after pca rotation ( center ) .",
    "in this situation , the mean of the second principal component given the first principal component can be easily expressed with a parabolic function ( red line ) .",
    "for data manifolds lacking the required symmetry , nonlinear modifications of pca should remove the residual nonlinear dependence .",
    "following the previous intuition , ppa aims to remove the condition mean . left panel in fig .",
    "[ fig2 ] shows the input @xmath0 data distribution , where we highlight a point of interest , @xmath1 .",
    "ppa is a sequential algorithm ( as pca ) that transforms one dimension at each step in the sequence .",
    "the procedure in each step consists of two operations .",
    "the _ first operation _ looks for the best vector for data projection .",
    "even though different possibilities will be considered later ( section 2.3 ) , a convenient choice for this operation is the leading eigenvector of pca .",
    "figure  [ fig2][middle ] shows the data after this projection : although the linear dependencies have been removed , there are still relations between the first and the second data dimensions .",
    "the _ second operation _ consists in subtracting the conditional mean to every sample .",
    "the conditional mean is estimated by fitting a curve predicting the residual using the projections estimated by the first operation .",
    "this step , composed of the two operations above , describes the @xmath2-dimensional data along _ one _ curvilinear dimension through ( 1 ) a projection score onto certain leading vector , and ( 2 ) a curve depending on the projection score .",
    "ppa differs from pca in this second operation because it bends the straight line into a curve , thus capturing part of the nonlinear relations between the leading direction and the orthogonal subspace .",
    "since this example is two - dimensional , ppa ends after one step .",
    "however , when there are more dimensions , the two - operations are repeated for the remaining dimensions . at the first step ,",
    "the @xmath3-dimensional information still to be described is the departure from the curve in the subspace orthogonal to the leading vector .",
    "this data of reduced dimension is the input for the next step in the sequence",
    ". the last ppa dimension will be the @xmath4 residual which , in this example , corresponds to the residuals in the second dimension .",
    "the paper is organized as follows .",
    "section 2 formalizes the forward ppa transform and analytically proves that ppa generalizes pca and improves its performance in dimensionality reduction .",
    "the objective function of ppa , its restrictions , and its computational cost are then analyzed .",
    "section 3 studies the main properties of ppa : jacobian , volume preservation , invertibility , and metric . in section 4",
    "we discuss the differences between ppa and related work . in section 5",
    ", we check the generalization of mahalanobis distance using the ppa metric , and its ability to characterize the manifold geometry ( curvature and torsion ) . finally , we report results on standard databases for dimensionality and redundancy reduction .",
    "section 6 concludes the paper .",
    "additionally , the appendix details a step - by - step example of the forward transform .",
    "in this section , we start by reviewing the pca formulation as a deflationary ( or sequential ) method that addresses one dimension at a time . this is convenient since it allows to introduce ppa as the generalization that uses polynomials instead of straight lines in the sequence .      given a @xmath2-dimensional centered random variable @xmath1 , the pca transform , @xmath5 , maps data from the input domain , @xmath6 , to a response domain , @xmath7 .",
    "pca can be actually seen as a sequential mapping ( or a set of concatenated @xmath8 transforms ) .",
    "each transform in the sequence explains a single dimension of the input data by computing a single component of the response : @xmath9 and hence the pca transformation can be expressed as : @xmath10 . here",
    "vectors , @xmath11 , and transforms , @xmath12 , refer to the @xmath13-th step of the sequence .",
    "each of these elementary transforms , @xmath14 , acts only on part of the dimensions of the output of the previous transform : the residual , @xmath15 .",
    "subscript @xmath16 refers to the input data so @xmath17 .",
    "this sequential ( deflationary ) interpretation , which is also applicable to ppa as we will see later , is convenient to derive most of the properties of ppa in section 3 .    in pca ,",
    "each transform @xmath14 : ( 1 ) @xmath18 , which is the projection of the data coming from the previous step , @xmath15 , onto the unit norm vector @xmath19 ; and ( 2 ) @xmath20 , which are the residual data for the next step , obtained by projecting @xmath15 in the complement space : @xmath21 where @xmath22 is a @xmath23 matrix containing the remaining set of vectors . in pca , @xmath19 is the vector that maximizes the variance of the projected data : @xmath24 \\ } } , \\label{eq : orthoerror}\\ ] ] where @xmath25 represents the set of possible unit norm vectors .",
    "@xmath26 can be any matrix that spans the subspace orthonormal to @xmath19 , and its rows contain @xmath27 orthonormal vectors .",
    "accordingly , @xmath19 and @xmath28 fulfil : @xmath29 which will be referred to as the _ orthonormality relations _ of @xmath19 and @xmath28 in the discussion below .    in the @xmath13-th step of the sequence ,",
    "the data yet to be explained is @xmath30 .",
    "therefore , truncating the pca expansion at dimension @xmath13 implies ignoring the information contained in @xmath11 so that the dimensionality reduction error is : @xmath31 = \\mathbb{e } [ \\|{{\\bf x}}_{p } \\|_2 ^ 2 ] .",
    "\\label{pcaerror}\\ ] ] pca is the optimal linear solution for dimensionality reduction in mse terms since eq",
    ".   implies minimizing the dimensionality reduction error in eq .   due to the orthonormal nature of the projection vectors @xmath19 and @xmath28 .",
    "ppa removes the conditional mean in order to reduce the reconstruction error of pca in eq .  . when the data fulfill the _ conditional mean independence _",
    "requirement , the conditional mean at every point in the @xmath19 direction is zero . in this case",
    ", the data vector goes through the means in the subspace spanned by @xmath28 , resulting in a small pca truncation error .",
    "however , this is not true in general ( cf .  fig .",
    "[ fig2 ] ) and then the conditional mean @xmath32 \\neq 0 $ ] . in order to remove the conditional mean @xmath33 from @xmath11",
    ", ppa modifies the elementary pca transforms in eq .   by subtracting an estimation of the conditional mean , @xmath34 : @xmath35 assuming for now that the leading vector , @xmath19 , is computed in the same way as in pca",
    ", ppa only differs from pca in the second operation of each transform @xmath14 ( cf .",
    ". however , this suffices to ensure the superiority of ppa over pca",
    ". we will refer to this particular choice of @xmath19 as the _ pca - based solution _ of ppa . in section 2.3 , we consider more general solutions to optimize the objective function at the cost of facing a non - convex problem . in any case , and independently of the method used to choose @xmath19 , the truncation error in ppa is : @xmath36 .",
    "\\label{ppaerror}\\ ] ]    [ [ estimation - of - the - conditional - mean - at - step - p . ] ] estimation of the conditional mean at step @xmath13 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the conditional mean can be estimated with any regression method @xmath37 . in this work",
    ", we propose to estimate the conditional mean at each step of the sequence using a polynomial function with coefficients @xmath38 and degree @xmath39 .",
    "hence , the estimation problem becomes :    @xmath40    which , in matrix notation is @xmath41 where @xmath42 , and @xmath43^\\top$ ] .",
    "note that when considering @xmath44 input examples , we may stack them column - wise in a matrix @xmath45 . in the above",
    "mentioned _ pca - based solution _ , the @xmath13-th step of the ppa sequence starts by computing pca on @xmath46 .",
    "then , we use the first eigenvector of the sample covariance as leading vector @xmath19 , and the remaining eigenvectors as @xmath28 . these eigenvectors are orthonormal ; if a different strategy is used to find @xmath19 , then @xmath28 can be chosen to be any orthonormal complement of @xmath19 ( see section 2.3 ) . from the projections of the @xmath44 samples onto the leading vector ( i.e. from the @xmath44 coefficients @xmath47 with @xmath48 ) , we build the vandermonde matrix @xmath49 , by stacking the @xmath44 column vectors @xmath50 , with @xmath48 .    then , the least squares solution for the matrix @xmath51 of coefficients of the polynomial is : @xmath52 where @xmath53 stands for the pseudoinverse operation .",
    "hence , the estimation of the conditional mean for all the samples , column - wise stacked in matrix @xmath54 , is : @xmath55 and the residuals for the next step are , @xmath56 .",
    "summarizing , the extra elements with respect to pca are a single matrix inversion in eq .   and the matrix product in eq .  .",
    "also note that the estimation of the proposed polynomial is much simpler than fitting a polynomial depending on a natural parameter such as the orthogonal projection on the curve , as one would do according to the classical principal curve definition  @xcite . since the proposed objective function in eq",
    "does not estimate distortions orthogonal to the curve but rather those orthogonal to the leading vector @xmath19 , the computation of the projections is straightforward and decoupled from the computation of @xmath51 . the proposed estimation in eq .",
    "consists of @xmath27 separate univariate problems only : this means that ppa needs to fit @xmath27 one - dimensional polynomials depending solely on the ( easy - to - compute ) projection parameter @xmath18 . since lots of samples @xmath57 are typically available , the estimation of such polynomials is usually robust .",
    "the convenience of this decoupling is illustrated in the step - by - step example presented in the appendix .",
    "since we compute @xmath51 using least squares , we obtain three important properties :    the ppa error does not depend on the particular selection of the basis @xmath28 if it satisfies the orthonormality relations in eq .  . +    * proof : * using different basis @xmath58 in the subspace orthogonal to @xmath19 is equivalent to applying an arbitrary @xmath59 rotation matrix , @xmath60 , to the difference vectors expressed in this subspace in eq .  : @xmath61 $ ] .",
    "since @xmath62 , the error is independent of this rotation , and hence independent of the basis .",
    "the ppa error is equal to or smaller than the pca error .",
    "+    * proof : * the ppa eqs .   and",
    "reduce to pca eq .   in the restricted case of @xmath63 .",
    "since , in general , ppa allows for @xmath64 , this implies that @xmath65 .",
    "even though the superiority of ppa over pca in mse terms is clearer when taking @xmath19 as in pca , this property holds in general .",
    "if a better choice for @xmath19 is available , it would reduce the error while having no negative impact in the cost function , since it is independent from the basis @xmath28 chosen ( see _ property 1 _ above ) .",
    "ppa reduces to pca when using first degree polynomials ( i.e. straight lines ) .",
    "+    * proof : * in this particular situation ( @xmath66 ) , the first eigenvector of @xmath46 is the best direction to project onto  @xcite .",
    "additionally , when using first degree polynomials , @xmath67 is very simple and @xmath68 can be computed analytically . plugging this particular @xmath68 into eq .  , it is easy to see that @xmath63 since the data is centered and @xmath18 is decorrelated of @xmath69 . therefore , when using straight lines @xmath51 vanishes and ppa reduces to pca .      [ cols=\"^,^,^ \" , ]",
    "this paper has been partially supported by the spanish mineco under project tin2012 - 38102-c03 - 01 and by the swiss nsf under project pz00p2 - 136827 .",
    "figure  [ con_cucharita ] presents a step - by - step example to illustrate how the sequence of ppa curvilinear components and projections are computed on a manifold of well - defined geometry : an helix embedded in a 3d space corrupted with additive gaussian noise which is a usual test case in principal curves  @xcite .",
    "data ( in gray ) were sampled from the same helix as in section  [ helice ] and noise with standard deviation 0.3 . since @xmath70 ,",
    "ppa consists of a sequence of two transforms ( see eq .  ):",
    "@xmath71 ( first row in fig .",
    "[ con_cucharita ] ) and @xmath72 ( second row ) .",
    "a representative sample is highlighted throughout the transform .    in this example",
    "we use the pca - based solution .",
    "therefore , the leading vector @xmath73 is the first eigenvector ( biggest eigenvalue ) of the covariance matrix of @xmath74 . in the example",
    ", @xmath73 ( or pc1 , in orange ) , and the vectors pc2 and pc3 ( in green and blue respectively ) constitute the basis @xmath75 .",
    "the first ppa component , @xmath76 , is the projection of the data onto the first leading vector , @xmath77 , in eq .",
    "( orange dots and the circle for the highlighted sample ) . the conditional mean , @xmath78 , is shown decomposed in two subspaces in the top center panel . we will call @xmath79 the conditional mean in the subspace spanned by @xmath73 and pc2 ( green dots ) , and",
    "let @xmath80 be the conditional mean in the subspace spanned by @xmath73 and pc3 ( blue dots ) .",
    "it is obvious the strong non - linear dependence of the conditional mean with @xmath76 , i.e. given the value of @xmath76 ( @xmath73 axis -black line- ) it is easy to predict the value of the data in the orthogonal subspaces ( blue and green dots ) using a non - linear function .",
    "fitting the first ppa polynomial in @xmath81 dimensions with regard to the parameter @xmath76 is equivalent to fitting the polynomials in the @xmath0 subspaces in the center plot ( simple univariate regressions ) .",
    "the polynomials in the @xmath0 subspaces have the coefficients @xmath82 $ ] , and equivalently , @xmath83 ; which are the rows of the matrix @xmath84 .",
    "polynomial coefficients are easy to fit by constructing the vandermonde matrix of degree @xmath85 using @xmath76 , @xmath86^\\top$ ] and applying eq .  .",
    "this ensures the best fitting in least squares terms .",
    "then , we estimate @xmath79 ( and correspondingly @xmath80 ) using @xmath76 and the weights , eq .  :",
    "@xmath87 in the top center panel , the estimated conditional mean , @xmath88^\\top$ ] , is represented by the curve ( red ) , while the curve projected in the bottom plane ( green ) and the curve projected in the vertical plane ( blue ) represent the conditional means in the respective subspaces ( @xmath89 and @xmath90 ) .",
    "once the polynomial has been fitted , we can remove @xmath91 from each sample ( second line in eq .  ) obtaining the residuals ( departures from the conditional mean ) represented in the top right plot ( yellow dots ) .    summarizing the process in the top row ,",
    "the transform @xmath71 , the first principal polynomial ( red curve ) accounts for the first curvilinear dimension of the data .",
    "after @xmath71 , we have @xmath92 dimensions yet to be explained : @xmath93 , at the top right and bottom left plots . the second row of fig .",
    "[ con_cucharita ] reproduces the same steps in the reduced dimension residual : projection onto the first pc in the bottom left plot ( orange dots ) , fitting the polynomial ( in this case , the best cross - validation solution was a second order polynomial , represented by the curve ( red ) in the bottom center plot , and removing the conditional mean so that the residuals ( yellow dots ) are aligned , and projected in the orthogonal subspace .",
    "j.  arenas - garca , k.  petersen , g.  camps - valls , and l.k .",
    "kernel multivariate analysis framework for supervised subspace learning : a tutorial on linear and kernel multivariate methods .",
    ", 30(4):1629 , 2013 .",
    "l.  ronan , r.  pienaar , g.  williams , e.t .",
    "bullmore , t.j .",
    "crow , n.  roberts , p.b .",
    "jones , j.  suckling , p.c .",
    "intrinsic curvature : a marker of millimeter - scale tangential cortico - cortical connectivity ?",
    ", 21(5 ) : 351366 , 2011 ."
  ],
  "abstract_text": [
    "<S> this paper presents a new framework for manifold learning based on a sequence of principal polynomials that capture the possibly nonlinear nature of the data . the proposed principal polynomial analysis ( ppa ) </S>",
    "<S> generalizes pca by modeling the directions of maximal variance by means of curves , instead of straight lines . </S>",
    "<S> contrarily to previous approaches , ppa reduces to performing simple univariate regressions , which makes it computationally feasible and robust . </S>",
    "<S> moreover , ppa shows a number of interesting analytical properties . </S>",
    "<S> _ first _ , ppa is a volume - preserving map , which in turn guarantees the existence of the inverse . </S>",
    "<S> _ second _ , such an inverse can be obtained in closed form . </S>",
    "<S> invertibility is an important advantage over other learning methods , because it permits to understand the identified features in the input domain where the data has physical meaning . </S>",
    "<S> moreover , it allows to evaluate the performance of dimensionality reduction in sensible ( input - domain ) units . </S>",
    "<S> volume preservation also allows an easy computation of information theoretic quantities , such as the reduction in multi - information after the transform . </S>",
    "<S> _ third _ , the analytical nature of ppa leads to a clear geometrical interpretation of the manifold : it allows the computation of frenet - serret frames ( local features ) and of generalized curvatures at any point of the space . and _ fourth _ </S>",
    "<S> , the analytical jacobian allows the computation of the metric induced by the data , thus generalizing the mahalanobis distance . </S>",
    "<S> these properties are demonstrated theoretically and illustrated experimentally . </S>",
    "<S> the performance of ppa is evaluated in dimensionality and redundancy reduction , in both synthetic and real datasets from the uci repository . </S>"
  ]
}