{
  "article_text": [
    "sentiment analysis has become popular in recent years .",
    "web services , such as socialmention.com , may even score microblog postings on identi.ca and twitter for sentiment in real - time .",
    "one approach to sentiment analysis starts with labeled texts and uses supervised machine learning trained on the labeled text data to classify the polarity of new texts @xcite .",
    "another approach creates a sentiment lexicon and scores the text based on some function that describes how the words and phrases of the text matches the lexicon .",
    "this approach is , e.g. , at the core of the _ sentistrength _ algorithm @xcite .",
    "it is unclear how the best way is to build a sentiment lexicon .",
    "there exist several word lists labeled with emotional valence , e.g. , anew @xcite , general inquirer , opinionfinder @xcite , sentiwordnet and wordnet - affect as well as the word list included in the sentistrength software @xcite .",
    "these word lists differ by the words they include , e.g. , some do not include strong obscene words and internet slang acronyms , such as `` wtf '' and `` lol ''",
    ". the inclusion of such terms could be important for reaching good performance when working with short informal text found in internet fora and microblogs .",
    "word lists may also differ in whether the words are scored with sentiment strength or just positive / negative polarity .",
    "i have begun to construct a new word list with sentiment strength and the inclusion of internet slang and obscene words .",
    "although we have used it for sentiment analysis on twitter data @xcite we have not yet validated it .",
    "data sets with manually labeled texts can evaluate the performance of the different sentiment analysis methods .",
    "researchers increasingly use amazon mechanical turk ( amt ) for creating labeled language data , see , e.g. , @xcite . here",
    "i take advantage of this approach .",
    "my new word list was initially set up in 2009 for tweets downloaded for online sentiment analysis in relation to the united nation climate conference ( cop15 ) .",
    "since then it has been extended .",
    "the version termed afinn-96 distributed on the internet has 1468 different words , including a few phrases .",
    "the newest version has 2477 unique words , including 15 phrases that were not used for this study .",
    "as sentistrength it uses a scoring range from @xmath0 ( very negative ) to @xmath1 ( very positive ) . for ease of labeling",
    "i only scored for valence , leaving out , e.g. , subjectivity / objectivity , arousal and dominance .",
    "the words were scored manually by the author .    the word list initiated from a set of obscene words",
    "@xcite as well as a few positive words .",
    "it was gradually extended by examining twitter postings collected for cop15 particularly the postings which scored high on sentiment using the list as it grew .",
    "i included words from the public domain _ _ original balanced affective word list _ _ by greg siegle .",
    "later i added internet slang by browsing the urban dictionary including acronyms such as wtf , lol and rofl .",
    "the most recent additions come from the large word list by steven j. derose , _ the compass derose guide to emotion words_. the words of derose are categorized but not scored for valence with numerical values . together with the derose words i browsed wiktionary and the synonyms it provided to further enhance the list . in some cases i used twitter to determine in which contexts the word appeared .",
    "i also used the microsoft web n - gram similarity web service ( `` clustering words based on context similarity '' ) to discover relevant words .",
    "i do not distinguish between word categories so to avoid ambiguities i excluded words such as patient , firm , mean , power and frank .",
    "words such as `` surprise''with high arousal but with variable sentiment  were not included in the word list .    most of the positive words were labeled with + 2 and most of the negative words with 2 , see the histogram in figure  [ fig : hist ] .",
    "i typically rated strong obscene words , e.g. , as listed in @xcite , with either 4 or 5 .",
    "the word list have a bias towards negative words ( 1598 , corresponding to 65% ) compared to positive words ( 878 ) .",
    "a single phrase was labeled with valence 0 .",
    "the bias corresponds closely to the bias found in the opinionfinder sentiment lexicon ( 4911 ( 64% ) negative and 2718 positive words ) .",
    "r0.5     i compared the score of each word with mean valence of anew .",
    "figure  [ fig : anew ] shows a scatter plot for this comparison yielding a spearman s rank correlation on 0.81 when words are directly matched and including words only in the intersection of the two word lists .",
    "i also tried to match entries in anew and my word list by applying porter word stemming ( on both word lists ) and wordnet lemmatization ( on my word list ) as implemented in nltk @xcite .",
    "the results did not change significantly .",
    "r0.5     when splitting the anew at valence 5 and my list at valence 0 i find a few discrepancies : aggressive , mischief , ennui , hard , silly , alert , mischiefs , noisy .",
    "word stemming generates a few further discrepancies , e.g. , alien / alienation , affection / affected , profit / profiteer .",
    "apart from anew i also examined general inquirer and the opinionfinder word lists .",
    "as these word lists report polarity i associated words with positive sentiment with the valence + 1 and negative with 1 .",
    "i furthermore obtained the sentiment strength from sentistrength via its web service and converted its positive and negative sentiments to one single value by selecting the one with the numerical largest value and zeroing the sentiment if the positive and negative sentiment magnitudes were equal .",
    "for evaluating and comparing the word list with anew , general inquirer , opinionfinder and sentistrength a data set of 1,000 tweets labeled with amt was applied .",
    "these labeled tweets were collected by alan mislove for the _ twittermood_/``pulse of a nation '' study @xcite .",
    "each tweet was rated ten times to get a more reliable estimate of the human - perceived mood , and each rating was a sentiment strength with an integer between 1 ( negative ) and 9 ( positive ) .",
    "the average over the ten values represented the canonical `` ground truth '' for this study .",
    "the tweets were not used during the construction of the word list .    to compute a sentiment score of a tweet i identified words and found the valence for each word by lookup in the sentiment lexicons .",
    "the sum of the valences of the words divided by the number of words represented the combined sentiment strength for a tweet .",
    "i also tried a few other weighting schemes : the sum of valence without normalization of words , normalizing the sum with the number of words with non - zero valence , choosing the most extreme valence among the words and quantisizing the tweet valences to + 1 , 0 and 1 .",
    "for anew i also applied a version with match using the nltk wordnet lemmatizer .",
    "r0.5     my word tokenization identified 15,768 words in total among the 1,000 tweets with 4,095 unique words .",
    "422 of these 4,095 words hit my 2,477 word sized list , while the corresponding number for anew was 398 of its 1034 words .",
    "of the 3392 words in general inquirer i labeled with non - zero sentiment 358 were found in our twitter corpus and for opinionfinder this number was 562 from a total of 6442 .",
    "r.5    [ cols=\"^,^,^,^,^,^ \" , ]     i found my list to have a higher correlation ( pearson correlation : 0.564 , spearman s rank correlation : 0.596 , see the scatter plot in figure  [ fig : tweetscatter ] ) with the labeling from the amt than anew had ( pearson : 0.525 , spearman : 0.544 ) . in my application of the general inquirer word list",
    "it did not perform well having a considerable lower amt correlation than my list and anew ( pearson : 0.374 , spearman : 0.422 ) .",
    "opinionfinder with its 90% larger lexicon performed better than general inquirer but not as good as my list and anew ( pearson : 0.458 , spearman : 0.491 ) .",
    "the sentistrength analyzer showed superior performance with a pearson correlation on 0.610 and spearman on 0.616 , see table  [ tab : corrmatrix ] .",
    "i saw little effect of the different tweet sentiment scoring approaches : for anew 4 different pearson correlations were in the range 0.5220.526 . for my list",
    "i observed correlations in the range 0.5430.581 with the extreme scoring as the lowest and sum scoring without normalization the highest .",
    "with quantization of the tweet scores to + 1 , 0 and 1 the correlation only dropped to 0.548 .",
    "for the spearman correlation the sum scoring with normalization for the number of words appeared as the one with the highest value ( 0.596 ) .",
    "r.5     figure  [ fig : evolution ] plots the evolution of the performance of the word list on the twitter as the word list is extended from 5 words to the full set of 2477 words .    to examine whether the difference in performance between the application of anew and my list is due to a different lexicon or a different scoring i looked on the intersection between the two word lists . with a direct match",
    "this intersection consisted of 299 words .",
    "building two new sentiment lexicons with these 299 words , one with the valences from my list , the other with valences from anew , and applying them on the twitter data i found that the pearson correlations were 0.49 and 0.52 to anew s advantage .",
    "on the simple word list approach for sentiment analysis i found my list performing slightly ahead of anew .",
    "however the more elaborate sentiment analysis in sentistrength showed the overall best performance with a correlation to amt labels on 0.610 .",
    "this figure is close to the correlations reported in the evaluation of the sentistrength algorithm on 1,041 myspace comments ( 0.60 and 0.56 ) @xcite .    even though general inquirer and opinionfinder have the largest word lists i found i could not make them perform as good as sentistrength , my list and anew for sentiment strength detection in microblog posting .",
    "the two former lists both score words on polarity rather than strength and it could explain the difference in performance .    is the difference between my list and anew due to better scoring or more words ?",
    "the analysis of the intersection between the two word list indicated that the anew scoring is better . the slightly better performance of my list with the entire lexicon may be due to its inclusion of internet slang and obscene words .",
    "newer methods , e.g. , as implemented in sentistrength , use a range of techniques : detection of negation , handling of emoticons and spelling variations @xcite . the present application of my list used none of these approaches and might have benefited .",
    "however , the sentistrength evaluation showed that valence switching at negation and emoticon detection might not necessarily increase the performance of sentiment analyzers ( tables 4 and 5 in @xcite ) .",
    "the evolution of the performance ( figure  [ fig : evolution ] ) suggests that the addition of words to my list might still improve its performance slightly .",
    "although my list comes slightly ahead of anew in twitter sentiment analysis , anew is still preferable for scientific psycholinguistic studies as the scoring has been validated across several persons .",
    "also note that anew s standard deviation was not used in the scoring .",
    "it might have improved its performance .",
    "i am grateful to alan mislove and sune lehmann for providing the 1,000 tweets with the amazon mechanical turk labels and to steven j. derose and greg siegle for providing their word lists .",
    "mislove , lehmann and daniela balslev also provided input to the article .",
    "i thank the danish strategic research councils for generous support to the ` responsible business in the blogosphere ' project .",
    "thelwall , m. , buckley , k. , paltoglou , g. , cai , d. , kappas , a. : sentiment strength detection in short informal text .",
    "journal of the american society for information science and technology * 61*(12 ) ( 2010 ) 25442558    bradley , m.m .",
    ", lang , p.j . : affective norms for english words ( anew ) : instruction manual and affective ratings .",
    "technical report c-1 , the center for research in psychophysiology , university of florida ( 1999 )    wilson , t. , wiebe , j. , hoffmann , p. : recognizing contextual polarity in phrase - level sentiment analysis . in : proceedings of the conference on human language technology and empirical methods in natural language processing , stroudsburg , pa ,",
    "usa , association for computational linguistics ( 2005 )    hansen , l.k .",
    ", arvidsson , a. , nielsen , f. . ,",
    "colleoni , e. , etter , m. : good friends , bad news  affect and virality in twitter . accepted for the 2011 international workshop on social computing , network , and services ( socialcomnet 2011 ) ( 2011 )    akkaya , c. , conrad , a. , wiebe , j. , mihalcea , r.",
    ": for subjectivity word sense disambiguation . in : proceedings of the naacl hlt 2010",
    "workshop on creating , speech and language data with amazon s mechanical turk , association for computational linguistics ( 2010 ) 195203"
  ],
  "abstract_text": [
    "<S> sentiment analysis of microblogs such as twitter has recently gained a fair amount of attention . </S>",
    "<S> one of the simplest sentiment analysis approaches compares the words of a posting against a labeled word list , where each word has been scored for valence ,  a `` sentiment lexicon '' or `` affective word lists '' . </S>",
    "<S> there exist several affective word lists , e.g. , anew ( affective norms for english words ) developed before the advent of microblogging and sentiment analysis . </S>",
    "<S> i wanted to examine how well anew and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs . </S>",
    "<S> i used manually labeled postings from twitter scored for sentiment . using a simple word matching </S>",
    "<S> i show that the new word list may perform better than anew , though not as good as the more elaborate approach found in sentistrength . </S>"
  ]
}