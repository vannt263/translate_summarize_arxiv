{
  "article_text": [
    "the traveling salesman problem ( tsp ) is one of the most fundamental problems in combinatorial optimization . given a graph ,",
    "the goal is to find a hamiltonian cycle of minimum or maximum weight .",
    "we consider finding hamiltonian cycles of maximum weight ( ) .",
    "an instance of  is a complete graph @xmath5 with edge weights @xmath6 .",
    "the goal is to find a hamiltonian cycle of maximum weight .",
    "the weight of a hamiltonian cycle ( or , more general , of a subset of @xmath7 ) is the sum of the weights of its edges .",
    "if @xmath8 is undirected , we speak of ( symmetric tsp ) .",
    "if @xmath8 is directed , we have  ( asymmetric tsp ) .    both  and  are np - hard and apx - hard .",
    "thus , we are in need of approximation algorithms . the currently best approximation algorithms for  and  achieve approximation ratios of @xmath9 and @xmath10 , respectively  @xcite .",
    "cycle covers are an important tool for designing approximation algorithms for the tsp .",
    "a cycle cover of a graph is a set of vertex - disjoint cycles such that every vertex is part of exactly one cycle .",
    "hamiltonian cycles are special cases of cycle covers that consist of just one cycle .",
    "thus , the weight of a maximum - weight cycle cover is an upper bound for the weight of a maximum - weight hamiltonian cycle .",
    "in contrast to hamiltonian cycles , cycle covers of minimum or maximum weight can be computed efficiently using matching algorithms  @xcite .      in many optimization problems , there is more than one objective function .",
    "consider buying a car : we might want to buy a cheap , fast car with a good gas mileage .",
    "how do we decide which car suits us best ?",
    "with multiple criteria involved , there is no natural notion of a best choice .",
    "instead , we have to be content with a trade - off .",
    "the aim of multi - criteria optimization is to cope with this problem . to transfer the concept of an optimal solution to multi - criteria optimization problems , the notion of _ pareto curves _ was introduced ( cf .",
    "ehrgott  @xcite ) .",
    "a pareto curve is a set of solutions that can be considered optimal .",
    "more formally , a @xmath3-criteria optimization problem consists of instances @xmath11 , solutions @xmath12 for every instance @xmath13 , and @xmath3 objective functions @xmath14 that map @xmath13 and @xmath15 to @xmath16 . throughout this paper ,",
    "our aim is to maximize the objective functions .",
    "we say that a solution @xmath15 _ dominates _ another solution @xmath17 if @xmath18 for all @xmath19 = \\{1 , \\ldots , k\\}$ ] and @xmath20 for at least one @xmath21 .",
    "this means that @xmath22 is strictly preferable to @xmath23 .",
    "a _ pareto curve _ ( also known as _",
    "pareto set _ or _ efficient set _ ) for an instance contains all solutions of that instance that are not dominated by another solution .",
    "unfortunately , pareto curves can not be computed efficiently in many cases : first , they are often of exponential size .",
    "second , because of straightforward reductions from knapsack problems , they are np - hard to compute even for otherwise easy problems .",
    "thus , we have to be content with approximate pareto curves .    for simpler notation ,",
    "let @xmath24 .",
    "we will omit the instance @xmath25 if it is clear from the context .",
    "inequalities are meant component - wise . a set @xmath26 of solutions is called an _ @xmath27 approximate pareto curve _ for @xmath13 if the following holds : for every solution @xmath17 , there exists a @xmath28 with @xmath29 .",
    "we have @xmath30 , and a @xmath31 approximate pareto curve is a pareto curve .",
    "( this is not precisely true if there are several solutions whose objective values agree .",
    "however , in our case this is inconsequential , and we will not elaborate on this for the sake of clarity . )",
    "an algorithm is called an _",
    "@xmath27 approximation algorithm _ if , given the instance @xmath25 , it computes an @xmath27 approximate pareto curve .",
    "it is called a randomized @xmath27 approximation algorithm if its success probability is at least @xmath32 .",
    "this success probability can be amplified to @xmath33 by executing the algorithm @xmath34 times and taking the union of all sets of solutions .",
    "( we can also remove solutions from this union that are dominated by other solutions in the union , but this is not required by the definition of an approximate pareto curve . )    papadimitriou and yannakakis  @xcite showed that @xmath35 approximate pareto curves of size polynomial in the instance size and @xmath36 exist .",
    "the technical requirement for the existence is that the objective values of solutions in @xmath12 are bounded from above by @xmath37 for some polynomial @xmath38 , where @xmath39 is the size of @xmath25 .",
    "this is fulfilled in most natural optimization problems and in particular in our case .",
    "a _ fully polynomial time approximation scheme _ ( fptas ) for a multi - criteria optimization problem computes @xmath35 approximate pareto curves in time polynomial in the size of the instance and @xmath36 for all @xmath2 .",
    "papadimitriou and yannakakis  @xcite , based on a result of mulmuley et al .",
    "@xcite , showed that multi - criteria minimum - weight matching admits a _ randomized fptas _ , i.  e.",
    ", the algorithm succeeds in computing a @xmath35 approximate pareto curve with constant probability .",
    "this randomized fptas yields also a randomized fptas for the multi - criteria maximum - weight cycle cover problem  @xcite , which we will use in the following .",
    "manthey and ram  @xcite designed randomized approximation algorithms for several variants of multi - criteria min - tsp . however , they leave it as an open problem to design any approximation algorithm for .",
    "we devise the first approximation algorithm for multi - criteria max - tsp . for @xmath3-criteria",
    ", we achieve an approximation ratio of @xmath1 for arbitrarily small @xmath2 . for @xmath3-criteria ,",
    "we achieve @xmath4 .",
    "our algorithm is randomized .",
    "its running - time is polynomial in the input size and @xmath36 and exponential in the number @xmath3 of criteria .",
    "however , the number of different objective functions is usually a small constant .",
    "the main ingredient for our algorithm is a decomposition technique for cycle covers and a reduction from @xmath3-criteria instances to @xmath40-criteria instances .",
    "a straight - forward @xmath32 approximation for mono - criterion max - atsp is the following : first , we compute a maximum - weight cycle cover @xmath41 . then we remove the lightest edge of each cycle , thus losing at most half of @xmath41 s weight . in this way",
    ", we obtain a collection of paths . finally , we add edges to connect the paths to get a hamiltonian cycle . for max - stsp ,",
    "the same approach yields a @xmath10 approximation since the length of every cycle is at least three .",
    "unfortunately , this does not generalize to multi - criteria  for which `` lightest edge '' is usually not well defined : if we break an edge that has little weight with respect to one objective , we might lose a lot of weight with respect to another objective . based on this observation , the basic idea behind our algorithm and its analysis is the following case distinction :    light - weight edges : : :    if all edges of our cycle cover contribute only little to its weight ,    then removing one edge does not decrease the overall weight by too    much .",
    "now we choose the edges to be removed such that no objective    loses too much of its weight .",
    "heavy - weight edges : : :    if there is one edge that is very heavy with respect to at least one    objective , then we take only this edge from the cycle cover . in this    way",
    ", we have enough weight for one objective , and we proceed    recursively on the remaining graph with @xmath42 objectives .    in this way , the approximation ratio for @xmath3-criteria  depends on two questions : first , how well can we decompose a cycle cover consisting solely of light - weight edges ?",
    "second , how well can @xmath40-criteria  be approximated ?",
    "we deal with the first question in section  [ sec : decomp ] . in section",
    "[ sec : alg ] , we present and analyze our approximation algorithms , which also gives an answer to the second question .",
    "finally , we give evidence that the analysis of the approximation ratios is tight and point out some ideas that might lead to better approximation ratios ( section  [ sec : remarks ] ) .",
    "let @xmath43 $ ] , and let @xmath41 be a cycle cover .",
    "we call a collection @xmath44 of paths an _ @xmath27-decomposition of @xmath41 _ if @xmath45 .",
    "( remember that all inequalities are meant component - wise . ) in the following , our aim is to find @xmath27-decompositions of cycle covers consisting solely of light - weight edges , that is , @xmath46 for all @xmath47 .    of course",
    ", not every cycle cover possesses an @xmath27-decomposition for every @xmath27 . for instance , a single directed cycle of length two , where each edge has a weight of @xmath31 shows that @xmath48 is best possible for a single objective function in directed graphs . on the other hand , by removing the lightest edge of every cycle , we obtain a @xmath32-decomposition .    for undirected graphs and @xmath49 , @xmath50 is optimal : we can find a @xmath10-decomposition by removing the lightest edge of every cycle , and a single cycle of length three , where each edge weight is @xmath31 , shows that this is tight .",
    "more general , we define @xmath51 $ ] to be the maximum number such that every directed cycle cover @xmath41 with @xmath52 for all @xmath47 possesses an @xmath53-decomposition .",
    "analogously , @xmath54 $ ] is the maximum number such that every undirected cycle cover @xmath41 with @xmath55 possesses an @xmath56-decomposition .",
    "we have @xmath57 and @xmath58 , as we have already argued above .",
    "we also have @xmath59 and @xmath60 as well as @xmath61 .      in this section",
    ", we investigate for which values of @xmath27 such @xmath27-decompositions exist . in the subsequent section ,",
    "we show how to actually find good decompositions .",
    "we have already dealt with @xmath62 and @xmath63 .",
    "thus , @xmath64 remains to be considered in the following theorems .",
    "in particular , only @xmath64 is needed for the analysis of our algorithms .",
    "let us first normalize our cycle covers to make the proofs in the following a bit easier",
    ". for directed cycle covers @xmath41 , we can restrict ourselves to cycles of length two : if we have a cycle @xmath65 of length @xmath66 with edges @xmath67 , we replace it by @xmath68 cycles @xmath69 for @xmath70 .",
    "if @xmath66 is odd , then we add a edge @xmath71 with @xmath72 and add the cycle @xmath73 .",
    "( strictly speaking , edges are 2-tuples of vertices , and we can not simply reconnect them .",
    "what we mean is that we remove the edges of the cycle and create new edges with the same names and weights together with appropriate new vertices . )",
    "we do this for all cycles of length at least three and call the resulting cycle cover @xmath74 . now any @xmath27-decomposition @xmath75 of the new cycle cover @xmath74 yields an @xmath27-decomposition @xmath76 of the original cycle cover @xmath41 by removing the newly added edges @xmath71 : in @xmath41 , we have to remove at least one edge of the cycle @xmath65 to obtain a decomposition . in @xmath74",
    ", we have to remove at least @xmath68 edges of @xmath65 , thus at least one . furthermore , if @xmath77 for every @xmath47 , then also @xmath78 for every @xmath79 since we kept all edge weights .",
    "this also shows @xmath80 .",
    "we are interested in @xmath27-decompositions that work for all cycle covers with @xmath3 objective functions .",
    "thus in particular , we have to be able to decompose @xmath74 .",
    "the consequence is that if every directed cycle cover that consists solely of cycles of length two possesses an @xmath27-decomposition , then every directed cycle cover does so",
    ".    for undirected cycle covers , we can restrict ourselves to cycles of length three : we replace a cycle @xmath81 by @xmath82 cycles @xmath83 for @xmath84",
    ". if @xmath66 is not divisible by three , then we add one or two edges @xmath85 to form a cycle of length three with the remaining edge(s ) . again , every @xmath27-decomposition of the new cycle cover yields an @xmath27-decomposition of the original cycle cover .    in the remainder of this section",
    ", we assume that all directed cycle covers consist solely of cycles of length two and all undirected cycle covers consist solely of cycles of length three .",
    "both theorems are proved using the probabilistic method .      for the proof of theorem  [ thm : alphau ] below ,",
    "we use hoeffding s inequality  ( * ? ? ?",
    "* theorem 2 ) , which we state here in a slightly modified version .",
    "[ lem : hoeffding ] let @xmath86 be independent random variables , where @xmath87 assumes values in @xmath88 $ ] .",
    "let @xmath89 . then @xmath90    [ thm : alphau ] for all @xmath64 , we have @xmath91 .",
    "let @xmath41 be any cycle cover and @xmath14 be @xmath3 objective functions .",
    "first , we scale the edge weight such that @xmath92 for all @xmath21 .",
    "thus , @xmath93 for all edges @xmath94 of @xmath41 since the weight of any edge is at most a @xmath95 fraction of the total weight .",
    "second , we can assume that @xmath41 consists solely of cycles of length three .",
    "let @xmath96 be the cycles of @xmath41 and let @xmath97 be the three edges of @xmath98 .",
    "we perform the following random experiment : we remove one edge of every cycle independently and uniformly at random to obtain a decomposition @xmath76 .",
    "fix any @xmath19 $ ] .",
    "let @xmath87 be the weight with respect to @xmath99 of the path in @xmath76 that consists of the two edges of @xmath98 .",
    "then @xmath100 . let @xmath101",
    ". then @xmath102 .",
    "every @xmath87 assumes values between @xmath103 and @xmath104 . since the weight of each edge is at most @xmath31 , we have @xmath105 . since the sum of all edge weights is @xmath3 ,",
    "we have @xmath106 let us estimate the probability of the event that @xmath107 , which corresponds to @xmath108 .",
    "if @xmath109 , then , by a union bound , we have @xmath110 .",
    "thus , @xmath111 , which implies the existence of a @xmath95-decomposition .",
    "by hoeffding s inequality , @xmath112 we have @xmath113 , @xmath114 , and @xmath115 .",
    "thus , for @xmath116 , and also for all larger values of @xmath3 , we have @xmath117 , which implies the existence of a @xmath95-decomposition for @xmath118 . the cases @xmath119 and @xmath120 remain to be considered since @xmath121 and @xmath122 .",
    "the bound for @xmath123 follows from lemma  [ lem : alpha2u ] below , which does not require @xmath124 .",
    "let us show @xmath125 .",
    "this is done in a constructive way .",
    "first , we choose from every cycle @xmath98 the edge @xmath126 that maximizes @xmath127 and put it into @xmath75 .",
    "the set @xmath75 will become a subset of @xmath76 . then @xmath128 .",
    "but we can also have some weight with respect to @xmath129 or @xmath130 .",
    "let @xmath131 and @xmath132 .",
    "if @xmath133 , then @xmath99 does not need any further attention .",
    "let @xmath134 .",
    "we have @xmath135 for @xmath136 , and @xmath74 consists solely of paths of length two .",
    "of every such path , we can choose at most one edge for inclusion in @xmath76 .",
    "( choosing both would create a cycle . )",
    "let @xmath137 be the two edges of @xmath98 with @xmath138 .",
    "now we proceed by considering only @xmath130 .",
    "let @xmath139 be initially empty sets .",
    "for all @xmath140 , if @xmath141 , then we put ( the heavier edge ) @xmath142 into @xmath143 and @xmath144 into @xmath145 .",
    "if @xmath146 , then we put @xmath142 into @xmath145 and @xmath144 into @xmath143 .    both @xmath147 and @xmath148 are decompositions of @xmath41 .",
    "we claim that at least one has a weight of at least @xmath31 with respect to all three objectives .",
    "since @xmath128 , this holds for both with respect to @xmath127 . furthermore , @xmath149 since @xmath150 for all edges .",
    "we have @xmath151 .",
    "thus , @xmath152 .",
    "this implies @xmath153 and @xmath154 .",
    "hence , with respect to @xmath130 and @xmath127 , both @xmath147 and @xmath147 will do .",
    "the first objective @xmath129 remains to be considered .",
    "we have @xmath155 . choosing either @xmath156 or @xmath157 results in @xmath158 .    for undirected graphs and @xmath119",
    ", we do not need the assumption that the weight of each edge is at most @xmath123 times the weight of the cycle cover .",
    "lemma  [ lem : alpha2u ] below immediately yields a @xmath159 approximation for bi - criteria : first , we compute a pareto curve of cycle covers .",
    "second , we decompose each cycle cover to obtain a collection of paths , which we then connect to form hamiltonian cycles .",
    "the following lemma can also be generalized to arbitrary @xmath3 ( lemma  [ lem : kmo ] ) .",
    "[ lem : alpha2u ] for every undirected cycle cover @xmath41 with edge weights @xmath160 , there exists a collection @xmath44 of paths with @xmath161 .",
    "let @xmath65 be a cycle of @xmath41 consisting of edges @xmath162 .",
    "since we have three edges , there exists one edge @xmath163 that is neither the maximum - weight edge with respect to @xmath129 nor the maximum - weight edge with respect to @xmath130 .",
    "we remove this edge .",
    "thus , we have removed at most half of @xmath65 s weight with respect to either objective .",
    "consequently , we have kept at least half of @xmath65 s weight , which proves @xmath164 .      for directed cycle covers ,",
    "our aim is again to show that the probability of having not enough weight in one component is less than @xmath95 .",
    "hoeffding s inequality works only for @xmath165 .",
    "we use a different approach , which immediately gives us the desired result for @xmath166 , and which can be tweaked to work also for small @xmath3 .",
    "[ thm : alphad ] for all @xmath64 , we have @xmath167 .",
    "as argued above , we can restrict ourselves to cycle covers consisting solely of cycles of length two .",
    "we scale the edge weights to achieve @xmath168 for all @xmath19 $ ] .",
    "this implies @xmath93 for all edges @xmath47 .    of every cycle ,",
    "we randomly choose one of the two edges and put it into @xmath76 .",
    "fix any @xmath19 $ ] .",
    "our aim is to show that @xmath169 , which would prove the existence of an @xmath53-decomposition .",
    "let @xmath96 be the cycles of @xmath41 with @xmath170 .",
    "let @xmath171 and @xmath172 .",
    "we assume @xmath173 for all @xmath174 $ ] .",
    "let @xmath175 .",
    "then , no matter which edges we choose , we obtain a weight of at least @xmath176 . hence ,",
    "if @xmath177 , we are done .",
    "otherwise , we have @xmath178 and replace @xmath179 by @xmath180 and @xmath181 by @xmath182 . then we only need additional weight @xmath183 , and our new goal is to prove @xmath184 .",
    "this boils down to the following random experiment : we have numbers @xmath185 $ ] with @xmath186 .",
    "then we choose a set @xmath187 $ ] uniformly at random .",
    "for such an @xmath11 , we define ( by abusing notation ) @xmath188 .",
    "we have to show @xmath189 .    to this aim ,",
    "let @xmath190 $ ] with @xmath191 be pairwise disjoint sets with @xmath192 .",
    "such sets exist : we select arbitrary elements for @xmath193 until @xmath194 .",
    "this can always be done since @xmath195 for all @xmath196",
    ". then we continue with @xmath197 , @xmath198 , and so on .",
    "if we have already @xmath199 such sets , then @xmath200 since @xmath64 .",
    "thus , at least weight @xmath201 is left , which suffices for @xmath202 .",
    "the sets @xmath203 do not necessarily form a partition of @xmath204 $ ] .",
    "let @xmath205 \\setminus ( c_1 \\cup \\ldots \\cup c_z)$ ] .",
    "we will have to consider @xmath74 once in the end of the proof .",
    "now consider any @xmath206 $ ] .",
    "we say that @xmath207 if @xmath208 for some @xmath209 . here , @xmath210 denotes the symmetric difference of sets .",
    "the relation @xmath211 is an equivalence relation that partitions all subsets of @xmath204 $ ] into @xmath212 equivalence classes , each of cardinality @xmath213 .",
    "let @xmath214 = \\{j \\subseteq [ m ] \\mid j \\sim i\\}$ ] .",
    "[ lem : atmosttwo ] for every @xmath187 $ ] , there are at most two sets @xmath215 $ ] with @xmath216 .    without loss of generality",
    "assume that @xmath217 } w(j)$ ] . if @xmath218",
    ", then there is nothing to show . otherwise , consider any @xmath219 $ ] with @xmath220 : @xmath221 we conclude that the only possibility for other sets @xmath215 $ ] with @xmath216 is @xmath222 for some @xmath66 .",
    "we prove that there is at most one such set by contradiction .",
    "so assume that there are @xmath223 and @xmath224 with @xmath225",
    ". then @xmath226 and @xmath227 .",
    "thus , @xmath228 a contradiction .",
    "a consequence of lemma  [ lem : atmosttwo ] is @xmath229 .",
    "this is less than @xmath95 for @xmath166 .",
    "the cases @xmath230 need special treatment .",
    "let us treat @xmath231 first . here",
    "@xmath232 , which is almost good enough . to prove @xmath233",
    ", we only have to find a set @xmath11 such that at most one set @xmath215 $ ] has @xmath234 .",
    "we claim that @xmath235 is such a set : of course @xmath236 .",
    "but for any other @xmath237 $ ] , we have @xmath238 for some @xmath209 .",
    "the latter equality holds since the sets @xmath203 are disjoint .",
    "thus @xmath239 .    to finish the proof",
    ", we consider the case @xmath240 .",
    "for this purpose , we consider @xmath11 and @xmath241 \\setminus i$ ] simultaneously .",
    "the classes @xmath214 $ ] and @xmath242 $ ] are disjoint : @xmath214 = [ \\overline i]$ ] would imply @xmath243 . then @xmath244 .",
    "since @xmath3 is odd , we have @xmath245 .",
    "thus , since @xmath246 , there must exist an @xmath66 with @xmath247 which contradicts @xmath248 .",
    "we show that the number of sets @xmath215 \\cup [ \\overline i]$ ] with @xmath249 is at most two .",
    "this would prove the result for @xmath240 since this would improve the bound to @xmath250 .    if we had more than two sets @xmath215 \\cup [ \\overline i]$ ] with @xmath216 , we can assume that we have two such sets in @xmath214 $ ] .",
    "( we can not have more than two such @xmath251 due to lemma  [ lem : atmosttwo ] . )",
    "we assume that these two sets are @xmath11 and @xmath252 .",
    "now consider any @xmath215 $ ] .",
    "since @xmath3 is odd and @xmath253 is even , we have @xmath254 thus , all sets @xmath215 $ ] have a weight of less than @xmath255 .",
    "this implies @xmath256 for all @xmath257 $ ] .",
    "thus , if @xmath214 $ ] contains two sets whose weight is less than @xmath183 , then @xmath242 $ ] contains no such set .      to conclude this section ,",
    "let us discuss some improvements of the results of this section .",
    "first , as a generalization of lemma  [ lem : alpha2u ] , cycle covers without cycles of length at most @xmath3 can be @xmath32-decomposed .",
    "this , however , does not immediately yield an approximation algorithm since finding maximum - weight cycle covers where each cycle must have a length of at least @xmath3 is np- and apx - hard for @xmath258 in directed graphs and for @xmath259  @xcite .",
    "[ lem : kmo ] let @xmath260 , and let @xmath41 be an arbitrary cycle cover such that the length of every cycle is at least @xmath253 . then there exists a collection @xmath44 of paths with @xmath161 .",
    "the proof is similar to the proof of lemma  [ lem : alpha2u ] .",
    "let @xmath65 be any cycle of @xmath41 .",
    "for each @xmath19 $ ] , we choose one edge of @xmath65 that maximizes @xmath99 for inclusion in @xmath76 .",
    "since @xmath65 has at least @xmath253 edges , this leaves us ( at least ) one edge for removal .",
    "figure  [ fig : lowerdecomp ] shows that theorems  [ thm : alphau ] and  [ thm : alphad ] , respectively , are tight for @xmath119 . due to these limitations for @xmath119 , proving larger values for @xmath56 or @xmath53",
    "does not immediately yield better approximation ratios ( see section  [ sec : remarks ] ) .",
    "however , for larger values of @xmath3 , hoeffding s inequality yields the existence of @xmath261-decompositions . together with a different technique for heavy - weight cycle covers , this might lead to improved approximation algorithms for larger values of @xmath3 .",
    "[ lem : suffk ] we have @xmath262 .",
    "let @xmath263 for some sufficiently large constants @xmath65 and @xmath264 .",
    "since @xmath59 , we can restrict ourselves to directed graphs . using the notation of theorem  [ thm : alphau ]",
    ", we have to show that @xmath109 , where @xmath101 and @xmath87 assumes values in the interval @xmath88 $ ] , @xmath265 , @xmath266 , and @xmath267 .",
    "we use hoeffding s inequality and plug in @xmath268 : @xmath269      while we know that decompositions exist due to the previous section , we have to find them efficiently in order to use them in our approximation algorithm",
    ". we present a deterministic algorithm and a faster randomized algorithm for finding decompositions .",
    "( algorithm  [ alg : decomp ] ) is a deterministic algorithm for finding a decomposition .",
    "the idea behind this algorithm is as follows : first , we scale the weights such that @xmath270",
    ". then @xmath271 for all edges @xmath47 .",
    "second , we normalize all cycle covers such that they consist solely of cycles of length two ( in case of directed graphs ) or three ( in case of undirected graphs ) .",
    "third , we combine very light cycles as long as possible .",
    "more precisely , if there are two cycles @xmath65 and @xmath272 such that @xmath273 and @xmath274 , we combine them to one cycle @xmath275 with @xmath276 . the requirements for an @xmath27-decomposition to exist are still fulfilled",
    "furthermore , any @xmath27-decomposition of @xmath74 immediately yields an @xmath27-decomposition of @xmath41 .",
    "@xmath277 cycle cover @xmath41 , edge weights @xmath278 , @xmath64 , @xmath77 for all @xmath47 a collection @xmath76 of paths [ line : wprime]obtain @xmath279 from @xmath278 by scaling each component such that @xmath280 for all @xmath21 normalize @xmath41 to @xmath74 as described in the text such that @xmath74 consists solely of cycles of length three ( undirected ) or two ( directed ) combine @xmath65 and @xmath272 to @xmath275 with @xmath281 replace @xmath65 and @xmath272 by @xmath275 in @xmath74 [ line : endnormal ] try all possible combinations of decompositions choose one @xmath75 that maximizes @xmath282 } w'_i(p)$ ] translate @xmath283 back to obtain a decomposition @xmath44 return @xmath76    the proof of the following lemma follows immediately from the existence of decompositions ( theorems  [ thm : alphau ] and  [ thm : alphad ] ) .",
    "[ lem : finddet ] let @xmath64 .",
    "let @xmath41 be an undirected cycle cover and @xmath14 be edge weights such that @xmath55 .",
    "then @xmath284 returns a collection @xmath76 of paths with @xmath285 .",
    "let @xmath41 be a directed cycle cover and @xmath14 be edge weights such that @xmath52 .",
    "then @xmath286 returns a collection @xmath76 of paths with @xmath287 .",
    "let us also estimate the running - time of .",
    "the normalization in lines  [ line : wprime ] to  [ line : endnormal ] can be implemented to run in linear time . due to the normalization",
    ", the weight of every cycle is at least @xmath32 with respect to at least one @xmath288 .",
    "thus , we have at most @xmath289 cycles in @xmath74 in the undirected case and at most @xmath290 cycles in @xmath74 in the directed case . in either case",
    ", we have @xmath291 cycles .",
    "all of these cycles are of length two or of length three .",
    "thus , we find an optimal decomposition , which in particular is an @xmath56 or @xmath53-decomposition , in time linear in the input size and exponential in @xmath3 .      by exploiting the probabilistic argument of the previous section , we can find a decomposition much faster with a randomized algorithm .",
    "( algorithm  [ alg : randecomp ] ) does this : we choose the edges to be deleted uniformly at random for every cycle .",
    "the probability that we obtain a decomposition as required is positive and bounded from below by a constant .",
    "furthermore , as the proofs of theorems  [ thm : alphau ] and  [ thm : alphad ] show , this probability tends to one as @xmath3 increases . for @xmath166 , it is at least approximately @xmath292 for undirected cycle covers and at least @xmath293 for directed cycle covers . for @xmath294 , we just use our deterministic algorithm , which has linear running - time for constant @xmath3 .",
    "the following lemma follows from the considerations above .",
    "@xmath295 cycle cover @xmath41 , edge weights @xmath296 , @xmath64 , @xmath77 for all @xmath47 a collection @xmath76 of paths with @xmath297 randomly choose one edge of every cycle of @xmath41 remove the chosen edges to obtain @xmath76 @xmath277    [ lem : findrand ] let @xmath64 .",
    "let @xmath41 be an undirected cycle cover and @xmath14 be edge weights such that @xmath55 .",
    "then @xmath298 returns a collection @xmath76 of paths with @xmath285 .",
    "let @xmath41 be a directed cycle cover and @xmath14 be edge weights such that @xmath52 .",
    "then @xmath299 returns a collection @xmath76 of paths with @xmath287 .",
    "the expected running - time of  is @xmath300 .",
    "based on the idea sketched in section  [ sec : idea ] , we can now present our approximation algorithms for multi - criteria  and .",
    "however , in particular for , some additional work has to be done if heavy edges are present .",
    "we first present our algorithm for  ( algorithm  [ alg : maxatsp ] ) since it is a bit easier to analyze .",
    "@xmath301 directed complete graph @xmath5 , @xmath260 , edge weights @xmath302 , @xmath2 approximate pareto curve  for @xmath3-criteria max - tsp compute a @xmath10 approximation @xmath303 compute a @xmath35 approximate pareto curve @xmath304 of cycle covers @xmath305 [ line : decide ] @xmath306 add edges to @xmath76 to form a hamiltonian cycle @xmath307 add @xmath307 to @xmath303 let @xmath308 be an edge with @xmath309 obtain @xmath310 from @xmath8 by contracting the paths of @xmath311 obtain @xmath279 from @xmath278 by removing the @xmath21th objective @xmath312 form a hamiltonian cycle from @xmath313 plus @xmath311 ; add it to @xmath303 form a hamiltonian cycle from @xmath313 plus @xmath314 ; add it to @xmath303    first of all , we compute a @xmath35 approximate pareto curve @xmath304 of cycle covers .",
    "then , for every cycle cover @xmath315 , we decide whether it is a light - weight cycle cover or a heavy - weight cycle cover ( line  [ line : decide ] ) .",
    "if @xmath41 has only light - weight edges , we decompose it to obtain a collection @xmath76 of paths .",
    "then we add edges to @xmath76 to obtain a hamiltonian cycle @xmath307 , which we then add to @xmath303 .",
    "if @xmath41 contains a heavy - weight edge , then there exists an edge @xmath316 and an @xmath21 with @xmath317 .",
    "we pick one such edge .",
    "then we iterate over all possible vertices @xmath318 ( including equalities and including @xmath319 and @xmath320 ) .",
    "we denote by @xmath311 the graph with vertices @xmath319 , @xmath320 , @xmath321 , @xmath322 , @xmath65 , @xmath264 and edges @xmath323 , @xmath324 , @xmath325 , and @xmath326 .",
    "we call @xmath311 _ legal _ if it can be extended to a hamiltonian cycle : @xmath311 is legal if and only if it consists of one or two vertex - disjoint directed paths",
    ". figure  [ fig : pabcde ] shows the different possibilities .    for every legal @xmath311 ,",
    "we contract the paths as follows : we remove all outgoing edges of @xmath321 and @xmath65 , all incoming edges of @xmath322 and @xmath264 , and all edges incident to @xmath319 or @xmath320",
    ". then we identify @xmath321 and @xmath322 as well as @xmath65 and @xmath264 .",
    "if @xmath311 consists of a single path , then we remove all vertices except the two endpoints of this path , and we identify these two endpoints .    in this way , we obtain a slightly smaller instance @xmath310 . then , for every @xmath21 , we remove the @xmath21th objective to obtain @xmath279 , and recurse on @xmath310 with only @xmath42 objectives @xmath279 .",
    "this yields approximate pareto curves @xmath327 of hamiltonian cycles of @xmath310 .",
    "now consider any @xmath328 .",
    "we expand the contracted paths to obtain @xmath307 .",
    "then we construct two tours : first , we just add @xmath311 to @xmath307 , which yields a hamiltonian cycle by construction .",
    "second , we observe that no edge in @xmath307 is incident to @xmath319 or @xmath320 .",
    "we add the edge @xmath314 to @xmath307 as well as some more edges such that we obtain a hamiltonian cycle .",
    "we put the hamiltonian cycles thus constructed into @xmath303 .",
    "we have not yet discussed the success probability .",
    "randomness is needed for computing the approximate pareto curves of cycle covers and the recursive calls of  with @xmath42 objectives .",
    "let @xmath39 be the size of the instance at hand , and let @xmath329 is a polynomial that bounds the size of a @xmath35 approximate pareto curve from above .",
    "then we need at most @xmath330 recursive calls of . in total , the number of calls of randomized algorithms is bounded by some polynomial @xmath331 .",
    "we amplify the success probabilities of these calls such that the probability is at least @xmath332 .",
    "thus , the probability that one such call is not successful is at most @xmath333 by a union bound .",
    "hence , the success probability of the algorithm is at least @xmath32 .    instead of",
    ", we can also use its randomized counterpart .",
    "we modify such that the running - time is guaranteed to be polynomial and that there is only a small probability that  errs .",
    "furthermore , we have to make the error probabilities of the cycle cover computation as well as the recursive calls of  slightly smaller to maintain an overall success probability of at least @xmath32 .",
    "overall , the running - time of  is polynomial in the input size and @xmath36 , which can be seen by induction on @xmath3 : we have a polynomial time approximation algorithm for @xmath49 . for @xmath0 , the approximate pareto curve of cycle",
    "covers can be computed in polynomial time , yielding a polynomial number of cycle covers .",
    "all further computations can also be implemented to run in polynomial time since  for @xmath42 runs in polynomial time by induction hypothesis .",
    "[ thm : aalg ]  is a randomized @xmath4 approximation for multi - criteria .",
    "its running - time is polynomial in the input size and @xmath36 .",
    "we have already discussed the error probabilities and the running - time .",
    "thus , it remains to consider the approximation ratio , and we can assume in the following , that all randomized computations are successful .",
    "we prove the theorem by induction on @xmath3 .",
    "for @xmath49 , this follows since mono - criterion  can be approximated with a factor @xmath334 .",
    "now assume that the theorem holds for @xmath42 .",
    "we have to prove that , for every hamiltonian cycle @xmath335 , there exists a hamiltonian cycle @xmath336 with @xmath337 .",
    "since every hamiltonian cycle is in particular a cycle cover , there exists a @xmath315 with @xmath338 .",
    "now we distinguish two cases .",
    "the first case is that @xmath41 consists solely of light - weight edges , i.  e. , @xmath339 , then  returns a collection @xmath76 of paths with @xmath340 , which yields a hamiltonian cycle @xmath307 with @xmath341 as claimed .",
    "the second case is that @xmath41 contains at least one heavy - weight edge @xmath316 .",
    "let @xmath323 , @xmath324 , @xmath342 , and @xmath343 be the edges in @xmath335 that are incident to @xmath319 or @xmath320 .",
    "( we may have some equalities among the vertices as shown in figure  [ fig : pabcde ] . )",
    "note that @xmath335 does not necessarily contain the edge @xmath94 .",
    "we consider the corresponding @xmath311 and divide the second case into two subcases .",
    "the first subcase is that there exists a @xmath344 $ ] with @xmath345 , i.  e. , at least a @xmath346 fraction of the @xmath196th objective is concentrated in @xmath311 .",
    "( we can have @xmath347 , but this is not necessarily the case . ) let @xmath348 $ ] be the set of such @xmath196 .",
    "we fix one @xmath349 arbitrarily and consider the graph @xmath310 obtained by removing the @xmath196th objective and contracting the paths @xmath350 and @xmath351 . a fraction of @xmath352 of the weight of @xmath335 is left in @xmath310 with respect to all objectives but those in @xmath251 .",
    "thus , @xmath310 contains a hamiltonian cycle @xmath353 with @xmath354 for all @xmath355 \\setminus j$ ] .",
    "since @xmath40-criteria  can be approximated with a factor of @xmath1 by assumption , @xmath327 contains a hamiltonian cycle @xmath313 with @xmath356 for all @xmath355 \\setminus j$ ] .",
    "together with @xmath311 , which contributes enough weight to the objectives in @xmath251 , we obtain a hamiltonian cycle @xmath307 with @xmath357 , which is as claimed .",
    "the second subcase is that @xmath358 for all @xmath344 $ ] .",
    "thus , at least a fraction of @xmath359 of the weight of @xmath335 is outside of @xmath311 .",
    "we consider the case with the @xmath21th objective removed .",
    "then , with the same argument as in the first subcase , we obtain a hamiltonian cycle @xmath313 of @xmath310 with @xmath360 for all @xmath355 \\setminus \\{i\\}$ ] . to obtain a hamiltonian cycle of @xmath8",
    ", we take the edge @xmath316 and connect its endpoints appropriately .",
    "( for instance , if @xmath361 are distinct , then we add the path @xmath362 and the edge @xmath363 . )",
    "this yields enough weight for the @xmath21th objective in order to obtain a hamiltonian cycle @xmath307 with @xmath364 since @xmath365 .",
    "works of course also for undirected graphs , for which it achieves an approximation ratio of @xmath4 .",
    "but we can do better for undirected graphs .",
    "our algorithm  for undirected graphs ( algorithm  [ alg : maxstsp ] ) starts by computing an approximate pareto curve of cycle covers just as  did .",
    "then we consider each cycle cover @xmath41 separately .",
    "if @xmath41 consists solely of light - weight edges , then we can decompose @xmath41 using .",
    "if @xmath41 contains one or more heavy - weight edges , then some more work has to be done than in the case of directed graphs .",
    "the reason is that we can not simply contract paths  this would make the new graph @xmath310 ( and the edge weights @xmath279 ) asymmetric .",
    "@xmath366 undirected complete graph @xmath5 , @xmath64 , edge weights @xmath302 , @xmath2 approximate pareto curve  for @xmath3-criteria max - tsp compute a @xmath35 approximate pareto curve @xmath304 of cycle covers @xmath305 @xmath367 add edges to @xmath76 to form a hamiltonian cycle @xmath307 add @xmath307 to @xmath303 @xmath306 add edges to @xmath76 to form a hamiltonian cycle @xmath307 add @xmath307 to @xmath303 let @xmath19 $ ] and @xmath368 with @xmath369 @xmath370 obtain @xmath279 from @xmath278 by removing the @xmath196th objective set @xmath371 for all edges @xmath372 incident to @xmath373 @xmath374 remove all edges @xmath372 from @xmath307 with @xmath375 to obtain @xmath313 add @xmath376 to @xmath303    so assume that a cycle cover @xmath315 contains a heavy - weight edge @xmath377 .",
    "let @xmath19 $ ] be such that @xmath378 .",
    "in a first attempt , we remove the @xmath21th objective to obtain @xmath279",
    ". then we set @xmath371 for all edges @xmath372 incident to @xmath319 or @xmath320 .",
    "we recurse with @xmath42 objectives on @xmath8 with edge weights @xmath279 .",
    "this yields a tour @xmath313 on @xmath8 .",
    "now we remove all edges incident to @xmath319 or @xmath320 of @xmath313 and add new edges including @xmath94 . in this way , we get enough weight with respect to objective @xmath21 .",
    "unfortunately , there is a problem if there is an objective @xmath196 and an edge @xmath372 incident to @xmath319 or @xmath320 such that @xmath372 contains almost all weight with respect to @xmath379 : we can not guarantee that this edge @xmath372 is included in @xmath307 without further modifying @xmath313 . to cope with this problem , we do the following : in addition to @xmath319 and @xmath320 , we set the weight of all edges incident to the other vertex of @xmath372 to @xmath182 .",
    "then we recurse .",
    "unfortunately , there may be another objective @xmath380 that now causes problems . to solve the whole problem , we iterate over all @xmath381 and over all additional vertices @xmath382 .",
    "let @xmath383 .",
    "we remove one objective @xmath19 $ ] to obtain @xmath279 , set the weight of all edges incident to @xmath373 to @xmath182 , and recurse with @xmath42 objectives . although the time needed to do this",
    "is exponential in @xmath3 , we maintain polynomial running - time for fixed  @xmath3 .",
    "as in the case of directed graphs , we can make the success probability of every randomized computation small enough to maintain a success probability of at least @xmath32 .",
    "the base case is now @xmath384 : in this case , every cycle cover possesses a @xmath32 decomposition , and we do not have to care about heavy - weight edges .",
    "overall , we obtain the following result .",
    "[ thm : salg ]  is a randomized @xmath1 approximation for multi - criteria .",
    "its running - time is polynomial in the input size and @xmath36 .",
    "we have already dealt with error probabilities and running - time .",
    "thus , we can assume that all randomized computations are successful in the following .",
    "what remains to be analyzed is the approximation ratio . as in the proof of theorem  [ thm : aalg ] , the proof is by induction on @xmath3 .",
    "the base case is @xmath384 .",
    "let @xmath335 be an arbitrary hamiltonian cycle .",
    "then there is a @xmath315 with @xmath338 . from @xmath41",
    ", we obtain a hamiltonian cycle @xmath307 with @xmath385 by decomposition and lemma  [ lem : alpha2u ] .",
    "let us analyze  for @xmath258 objectives . by the induction hypothesis",
    ", we can assume that  is a @xmath386 approximation for @xmath40-criteria .",
    "let @xmath335 be any hamiltonian cycle .",
    "we have to show that @xmath303 contains a hamiltonian cycle @xmath307 with @xmath387 .",
    "there is a @xmath388 with @xmath338 .",
    "we have to distinguish two cases .",
    "first , if @xmath41 consists solely of light - weight edges , i.  e. , @xmath389 for all @xmath94 , then we obtain a hamiltonian cycle @xmath307 from @xmath41 with @xmath390 .",
    "second , let @xmath47 and @xmath19 $ ] with @xmath369 .",
    "we construct sets @xmath391 $ ] , @xmath392 , and @xmath393 in phases as follows ( we do not actually construct these sets , but only need them for the analysis ) : initially , @xmath394 and @xmath395 . in every phase , we consider the set @xmath396 of all edges of @xmath335 that have exactly one endpoint in @xmath373 .",
    "we always have @xmath397 by construction .",
    "let @xmath398 \\mid j \\notin i , w_j(x ' ) \\geq w_j(\\hat h)/k\\}$ ] . if @xmath399 is empty , then we are done . otherwise , add @xmath399 to @xmath11 , add @xmath396 to @xmath25 , and add all new endpoints of vertices in @xmath396 to @xmath373 .",
    "we add at least one element to @xmath11 in every phase .",
    "thus , @xmath400 and @xmath401 since @xmath402 .",
    "let @xmath403 , and let @xmath404 be the weight of edges of @xmath335 that have exactly one endpoint in @xmath373 .",
    "let @xmath405 . by construction , we have @xmath406 for all @xmath407",
    "otherwise , we would have added more edges to @xmath25 .",
    "we distinguish two subcases .",
    "the first subcase is that @xmath408 . then @xmath409 and @xmath410 .",
    "consider the set @xmath411 and the edge weights @xmath279 used to obtain it .",
    "we have @xmath412 for @xmath413 . by the induction hypothesis",
    ", there is an @xmath414 with @xmath415 for @xmath413 .",
    "we remove all edges incident to @xmath319 or @xmath320 to obtain @xmath313 . since the weight of all these edges has been set to @xmath182 , we have @xmath416 .",
    "there exists a set @xmath417 such that @xmath418 and @xmath419 is a hamiltonian cycle . for this cycle , which is in @xmath303",
    ", we have @xmath420 and , for @xmath413 , @xmath421    the second subcase is that @xmath11 is not empty .",
    "let @xmath422 , and let @xmath373 .",
    "we consider @xmath423 .",
    "let @xmath424 , @xmath425 , and @xmath426 be as defined above . by the induction hypothesis ,",
    "the set @xmath423 contains a hamiltonian cycle cover @xmath307 with @xmath427 for @xmath428 .",
    "we remove all edges incident to @xmath373 from @xmath307 to obtain @xmath313 with @xmath416 . by construction @xmath429",
    "is a collection of paths .",
    "we add edges to @xmath25 to obtain @xmath430 such that @xmath376 is a hamiltonian cycle .",
    "let us estimate the weight of @xmath376 . for all @xmath431 , we have @xmath432 . for all @xmath433",
    ", we have @xmath434 which completes the proof .",
    "the analysis of the approximation ratios of our algorithms is essentially optimal : our approach can at best lead to approximation ratios of @xmath435 for some @xmath436 .",
    "the reason is as follows : assume that @xmath40-criteria  can be approximated with a factor of @xmath437 .",
    "if we have a @xmath3-criteria instance , we have to set the threshold for heavy - weight edges somewhere .",
    "assume for the moment that this threshold @xmath438 be arbitrary .",
    "then the ratio for @xmath3-criteria  is @xmath439 .",
    "choosing @xmath440 maximizes this ratio .",
    "thus , if @xmath441 for some @xmath442 , then @xmath443 . we conclude that the denominator of the approximation ratio increases by at least @xmath31 if we go from @xmath42 to @xmath3 .    for undirected graphs , we have obtained a ratio of roughly @xmath95 , which is optimal since @xmath444 implies @xmath445 .",
    "similarly , for directed graphs , we have a ratio of @xmath446 , which is also optimal since @xmath447 implies @xmath448 .    due to the existence of @xmath261-decompositions , we conjecture that both @xmath3-criteria  and @xmath3-criteria  can in fact be approximated with factors of @xmath261 .",
    "this , however , requires a different approach or at least a new technique for heavy - weight edges .",
    "christos  h. papadimitriou and mihalis yannakakis . on the approximability of trade - offs and",
    "optimal access of web sources . in _ proc . of the 41st ann .",
    "ieee symp . on foundations of computer science ( focs ) _ , pages 8692 .",
    "ieee computer society , 2000 ."
  ],
  "abstract_text": [
    "<S> we present randomized approximation algorithms for multi - criteria . for  with @xmath0 objective functions , we obtain an approximation ratio of @xmath1 for arbitrarily small @xmath2 . for  with @xmath3 objective functions , </S>",
    "<S> we obtain an approximation ratio of @xmath4 . </S>"
  ]
}