{
  "article_text": [
    "[ cols=\"^,^,^,^ \" , ]     it has been suggested  @xcite that the best performance of boosting algorithm is achieved by adaboost using decision tree  @xcite or cart  @xcite . some of the confusions about generalization ( test )",
    "error based on the margin theory has recently been clarified by reyzin and schapire  @xcite . in table  ( [ tb : comparison ] ) , we compare the our algorithms with adaboost and arc - gv using decision tree . for a fair comparison , we show the improvement of adaorboost , arc - gv using cart , and adaboost using cart over those using decision stump .",
    "table  ( [ tb : comparison ] ) shows the error ratio . as we can see",
    ", the improvement of adaorboost is comparable to arc - gv using cart , but is worse than ada - cart .",
    "however , each cart , after tree pruning , has around 16 leaf nodes with the tree depth being around 7 .",
    "therefore , the complexity of cart is much bigger than that of orboost and andboost .",
    "this is particularly an issue for applications in vision as the training data is massive with each data sample having thousands or even millions of features .",
    "the good performance of ada - cart is achieved using an average of @xmath0 levels of tree .",
    "this greatly limits its usage in many vision applications and leaves the decision stump classifiers still being currently widely used  @xcite .",
    "first , we demonstrate it on the weizmann horse dataset  @xcite .",
    "we use 328 images and use 126 for training and 214 for testing .",
    "each input image comes with a label map in which the pixels on the horse body and background are labeled as @xmath1 and @xmath2 respectively .",
    "given a test image , our task is to classify all the pixels into horse or background . in training , we take image patch of size @xmath3 centered on every pixel as training samples .",
    "the background and horse body image patches are the negatives and positives respectively . for each image patch , we compute around @xmath4 features such as the mean , variance , and haar responses of the original as well as gabor filtered .",
    "we implement a cascade approach  @xcite and implement several versions .",
    "one uses ada - stump and others use ada - or and ada - andor .",
    "each cascade node selects and fuses 100 weak classifiers .",
    "all the algorithms use an identical set of features and bootstrapping procedure .",
    "( [ fig : detection].a ) shows the precision and recall curve of the algorithms on the training and test images .",
    "we observe similar result as that for the uci repository datasets .",
    "ada - andor improves the results over ada - stump by a considerable amount .",
    "the differences between the training and test errors are nearly the same in this cascade setting as well .",
    "the f - value of the results by ada - andor is around 0.8 which is better than the number 0.66 reported in  @xcite which uses low and middle level information .",
    "next , we show the ada - andor algorithm for pedestrian detection on dataset reported in  @xcite .",
    "we use 8 level of cascade with different choices of weak classifiers for adaboost .",
    "( [ fig : detection]b ) shows the results by ada - stump , ada - ada , ada - or , ada - and , and ada - andor .",
    "the conclusion is nearly the same as before .",
    "ada - andor achieves the best result among all with ada - and being on the second place . though we are not specifically addressing the pedestrian detection problem here",
    ", the result is nevertheless close to that by the well - known hog pedestrian detector  @xcite .",
    "however , we only use a set of generic haar features without tuning the system specifically for the pedestrian detection task .",
    "many of the classification problems in machine learning and computer vision can be understood as performing logic operations combining ` and ' , ` or ' , and ` not ' . in this paper , we have introduced layered logic classifiers .",
    "we show that adaboost can not solve the ` xor ' problem using decision stump type of weak classifiers .",
    "we propose an orboost and andboost algorithms to study the ` or ' and ` and ' operations respectively .",
    "we demonstrate that the combined algorithm of two layers , ada - andor , greatly outperformed ada - stump which is widely used in the literature .",
    "the improvement is significant in most the cases .",
    "we demonstrate the effectiveness of ada - andor on traditional machine learning datasets , as well as challenging vision applications .",
    "though decision tree based adaboost algorithm is shown to produce smaller test error , its complexity in training often limits its usage .",
    "the orboost and andboost algorithm only increases the time complexity slightly than decision stump , but they significantly reduce the test error .",
    "the ada - andor algorithm is useful for a wide variety of applications in machine learning and computer vision ."
  ],
  "abstract_text": [
    "<S> designing effective and efficient classifier for pattern analysis is a key problem in machine learning and computer vision . </S>",
    "<S> many the solutions to the problem require to perform logic operations such as ` and ' , ` or ' , and ` not ' . </S>",
    "<S> classification and regression tree ( cart ) include these operations explicitly . </S>",
    "<S> other methods such as neural networks , svm , and boosting learn / compute a weighted sum on features ( weak classifiers ) , which weakly perform the and and or operations . </S>",
    "<S> however , it is hard for these classifiers to deal with the xor pattern directly . in this paper </S>",
    "<S> , we propose layered logic classifiers for patterns of complicated distributions by combining the ` and ' , ` or ' , and ` not ' operations . </S>",
    "<S> the proposed algorithm is very general and easy to implement . </S>",
    "<S> we test the classifiers on several typical datasets from the irvine repository and two challenging vision applications , object segmentation and pedestrian detection . </S>",
    "<S> we observe significant improvements on all the datasets over the widely used decision stump based adaboost algorithm . </S>",
    "<S> the resulting classifiers have much less training complexity than decision tree based adaboost , and can be applied in a wide range of domains . </S>"
  ]
}