{
  "article_text": [
    "for two decades , single - fibre multi - object spectrographs dominated galaxy redshift surveys @xcite .",
    "more than two million galaxies now have accurate redshift measurments .",
    "these surveys have taught us a great deal about large - scale structure and how the bolometric properties of galaxies evolve with cosmic time . in recent years , broadband photometric surveys have revealed there is much to be learnt from the spatially resolved properties of large galaxy samples @xcite in particular , how these properties vary with the large - scale environment @xcite .",
    "what is missing from these surveys is knowledge of the kinematics , ages and metallicity of prominent stellar populations across each galalxy , plus the emission line intensities and kinematics that provide insight on the star formation , dynamics and chemical state of the galaxy .",
    "integral field and fabry - perot spectrographs provide the necessary 3d ( 2d spatial , 1d spectral ) information but these are typically designed as single - target instruments ( e.g. , hifi & pmas - * ? ? ?",
    "* ; * ? ? ?",
    "* ) and modest survey samples @xcite .",
    "recent integral field surveys have managed to observe of order 200@xmath1300 galaxies over a long observing campaign ( e.g. , sauron - @xcite ; atlas3d - @xcite ; califa - @xcite ) .    with a view to obtaining integral field data on thousands of galaxies ,",
    "the sami concept was born @xcite .",
    "this multi - object instrument uses a new kind of fibre bundle  the hexabundle @xcite  in order to achieve spatially resolved 3d spectroscopy of up to 13 galaxies at a time over a 1 degree diameter field",
    ". the sami galaxy survey will observe more than 3000 galaxies in a 3  yr campaign .",
    "this instrument is proposed to be extended to 50@xmath1100 bundles in its next incarnation with a view to obtaining data on a far larger sample of objects @xcite .",
    "one challenge of the hexabundle technique is its irregular fibre format of close - packed circular fibres ( figure  [ layout ] ) which for many applications must be reformatted to give well - formed data for uniform analysis .",
    "indeed the same issues are faced by any imaging system if none integer spaxel shifts are employed in dithering . in this paper",
    "we present the resampling solution adopted , along with other processing steps required to remove the instrumental signature , for the sami galaxy survey .    the code to perform the datacube generation described in this paper is available from the astrophysics source code library as project asci:1407.006 .",
    "the motivation for the sami instrument is described by @xcite with technical specification provided in @xcite and updated in @xcite .",
    "the first explorations of its scientific capabilities are provided by @xcite .",
    "briefly , the sami system uses lightly fused fibre bundles to create self - contained fibre integral - field units .",
    "each of the 13 sami fibre bundles contains a close packed array of 61 optical fibres with individual fibre - core diameters of 16 ( figure  [ layout ] ) .",
    "the confinement of the bundle by a circular outer form arranges the fibres into 4 concentric rings around the central fibre , rather than a hexagonal packing @xcite .",
    "the fibre bundles populate the 1@xmath2 diameter focal plane of the @xmath3 triplet corrector at the anglo - australian telescope ( aat ) via a plug - plate system .",
    "each fibre - bundle has a fill factor of 73% over a fov of @xmath015diameter .",
    "the 13 ifu bundles , each of 61 fibres , and 26 independent blank - sky fibres for sky - subtraction , feed the aaomega spectrograph @xcite . using the 580v and 1000r gratings",
    ", the dual beam aaomega spectrograph provides wavelength coverage in two bands , 3700 - 5700  at a spectral resolution of r@xmath01730 and 6250 - 7350  at r@xmath04500 .",
    "the sami galaxy survey will observe more than 3000 galaxies in a 3  year campaign as described in @xcite .",
    "the fibre core arrangement is shown for one of the sami fibre ifus .",
    "a quality control image of a bundle , taken during production of a hexabundle , is shown along with a graphical fibre mapping the none uniform fibre illumination pattern in the image is a product of the back - illumination used for the photograph and is not representative of the final bundle transmission . a number of defocused dust artefacts on the laboratory camera are also seen .",
    "each fibre has a core diameter of 105@xmath4 m , with the cladding thinned to 110@xmath51@xmath4 m over the first 50 mm of each fibre .",
    "this allows a tight fibre packing , creating a semi - regular ifu array with a filling factor of 73%.,title=\"fig:\",width=264 ]   the fibre core arrangement is shown for one of the sami fibre ifus .",
    "a quality control image of a bundle , taken during production of a hexabundle , is shown along with a graphical fibre mapping the none uniform fibre illumination pattern in the image is a product of the back - illumination used for the photograph and is not representative of the final bundle transmission . a number of defocused dust artefacts on the laboratory camera are also seen .",
    "each fibre has a core diameter of 105@xmath4 m , with the cladding thinned to 110@xmath51@xmath4 m over the first 50 mm of each fibre .",
    "this allows a tight fibre packing , creating a semi - regular ifu array with a filling factor of 73%.,title=\"fig:\",width=302 ]",
    "a detailed description of the fibre spectroscopy data reduction software ` 2dfdr ` , whose usage is common to data taken in all modes of the aaomega spectrograph , is presented by @xcite who provide detail of aaomega data processing for the gama survey program @xcite . `",
    "2dfdr ` carries out all the reduction steps up to the point of generating row - stacked spectra ( rss ) which are wavelength calibrated and sky subtracted with the basic instrumental signatures removed .",
    "the rss frames are 2d images with one row per fibre spectrum ( along with the associated variance information and source details in image and binary table extensions ) for each observation with the sami ifus . the description of how these are flux calibrated and converted into datacubes are given in sections [ fluxcal ] and [ drizzle ] below . here",
    "we will describe the steps required to produce the rss frames within the ` 2dfdr ` package .",
    "the first stage is to subtract bias and dark frames to correct a number of errant ccd pixels .",
    "both arms of the dual - beam aaomega spectrograph suffers from a number of extended regions of bad columns whose charge transfer inefficiency effects associated with hot pixels can be compensated for by bias / dark correction .",
    "an overscan correction is also applied , subtracting the bias level in each frame . after this , each frame is divided by a _ detector flat _ that is generated by averaging ( typically @xmath6 ) fibre flats for which the spectrograph has been defocussed so that the illumination is relatively uniform .",
    "these frames are then filtered to remove large - scale variations , leaving only smaller - scale pixel - to - pixel flat field variations .",
    "charge spots due to cosmic rays are removed from each individual science frame using a tuned implementation of the lacosmic routine @xcite .",
    "an optimisation of the parameters was performed to ensure high confidence in cosmic - ray rejection with minimal impact on fibre spectra .",
    "the next stage is to trace the fibre locations across the detector ( generating a so - called _ tramline map _ giving the pixel - by - pixel [ x , y ] location of each fibre ) .",
    "this is a crucial step as good extraction of 1d spectra from the 2d data frame is contingent on accurately mapping the fibre positions and profiles across the detector .",
    "this is performed using a fibre flat field frame taken using a quartz - halogen lamp that illuminates a white - spot on the aat dome .",
    "the fibre intensity is traced in a two stage process .",
    "first the fibre peaks are identified and fitted approximately using a quadratic fit to the 3 pixels around each peak ( this gives positions accurate at the @xmath7 pixel level ) .",
    "then as a second stage ( newly implemented for the sami pipeline ) we implement an algorithm that assumes a gaussian fibre profile ( a good approximation to sami fibres in aaomega ) and fits five gaussians ( the central one and two either side ) to precisely determine both the centre and width ( to be used later for optimal extraction ) of the fibre profile .",
    "when fitting we integrate the gaussian model across each pixel .",
    "a robust 4th order polynominal is then fit to both the tramline and width of each fibre as a function of spectral pixel .",
    "the multiple gaussians are required as the close packing of fibres causes the wings of their light profiles to overlap at the @xmath8 percent level so that adjacent fibres influence the measured position and centre of the fibre in question @xcite .",
    "our approach allows two systematic effects present in the fibre traces to be addressed .",
    "the first is that the aaomega ccds , 2@xmath94k e2v detectors , are manufactured using a lithography mask of 1024@xmath9512 pixels .",
    "there are errors in the relative positioning of the mask on different parts of the detector , causing discontinuties in the fibre traces between pixel 1024 and 1025 ( in the spectral direction ) . adding a step when fitting the fibre trace allows us to accurately measure these lithography alignment errors",
    ", we find the typical uncertainty on an individual fibre is @xmath10 pixels . because there are @xmath11 fibres per lithography block",
    ", the mean error per block can be derived at @xmath12 pixel precision .",
    "the worst lithography errors were found to be @xmath13 pixels , but most are substantially less than that @xmath14 pixels . a small region of a fibre flat field is shown in figure  [ fig : litho_im ] . the residual image ( the difference between the input and model flat field data ) without correction for lithography errors ( middle panel ) shows clear discontinuities that are removed after the fibre traces are corrected for this error ( lower panel ) . in figure  [ fig : litho_errors ] we plot the lithography errors measured for each fibre and the median values in each block for a typical fibre flat field from the aaomega red arm .",
    "+   +         a second systematic discovered was a slow shift in position of the fibre traces of approximately 0.02 - 0.03 pixels per hour .",
    "investigation showed this to be due to a time - varying gravitational torque on the aaomega cameras caused by the slow boiling off of the liquid nitrogen in the dewers attached to the cameras .",
    "comparison of tram - line maps to the residuals from extracted data frames suggests that our typical total tramline map error is @xmath15 pixels , and never worse than @xmath16 pixels .    after measuring the tramline map and fibre profile widths",
    ", the next stage is extraction of the flux from the 2d image to generate a 1d spectrum for each fibre .",
    "an optimal extraction @xcite is performed to fit the flux amplitudes perpendicular to the dispersion axis .",
    "gaussian profiles are fit , holding the centre and width constant ( based on the tramline and fibre width maps measured above ) and fitting all 819 fibres simultaneously . at the same time",
    "a b - spline is fit to model the smooth scattered light .",
    "this is typically 8th order , so that a total of 827 parameters are fit at once for each ccd column ( 4096 pixels ) .",
    "to enforce smoothness on the scattered light model , the fit is done in two passes . on the first pass all 819 fibres plus 8 scattered light parameters are fit ( as outlined in * ? ? ?",
    ". then the scattered light model is smoothed across columns and the resulting 2d image is subtracted from the data frame .",
    "the second pass is then done on the subtracted frame , fitting only the 819 fibre amplitudes , without any scattered light model .",
    "an example extraction fit is shown in figure  [ fig : optex ] .     between fibres 126 and 127",
    "is due to the separation between individual slitlets each containing 63 fibres in the sami slit.,width=340 ]    following extraction , the 1d spectra are divided by an extracted and normalised 1d fibre flat field spectrum , which removes residual fibre - to - fibre variations in spectral response .",
    "these do not correct for total throughput as the illumination is not sufficiently uniform across the flat field and so no transmission calibration between the fibres is possible at this stage @xcite .",
    "wavelength calibration is performed using standard arc - lamps ( cuar ) .",
    "emission lines are identified in extracted 1d spectra and matched to line - lists with a 3@xmath17 order polynomial for each fibre solution .",
    "a secondary wavelength calibration is performed in the red arm by measuring the positions of several sky emission lines and fitting a quadratic to the residuals relative to their known wavelengths .",
    "this modification corrects for small shifts introduces due to due to difference in the feed angle of sky and calibration illumination .",
    "it is found to improve sky subtraction accuracy while not significantly modifying the wavelength solution .",
    "this correction can not be applied in the blue arm , where only the 5577-sky line is in the spectral range observed .",
    "the relative throughput between all fibres is established via measurement of several isolated night sky emission lines .",
    "the requirements for relative transmission calibration are outlined by @xcite . in occasional cases",
    "the sky lines are affected by cosmic rays and other artefacts . to minimise the effect of this issue",
    ", we use the median throughput value for each fibre across all observations of a field on a single night    the 26 individual sky fibres within the sami spectrograph slit are configured to blank sky positions in each observation field . once throughput calibrated , a master sky spectrum is generated by stacking the individual sky fibres with each frame before subtracting the sky .",
    "the principal components analysis technique @xcite can not be used due to the common spectral features in most sami galaxy survey targets , a consequence of the limited redshift range for the survey and the coherence of spectral structure within each galaxy .",
    "the individual fibre spectrum processing with ` 2dfdr ` is now complete and the row - stacked - spectra ( rss ) frames are passed to an external processing suite to flux calibrate and correct for differential atmospheric refraction ( dar ) and dispersion before aligning and mosaicking individual objects to produce datacubes .",
    "the flux calibration for each set of galaxy observations has two parts .",
    "first , a spectrophotometric standard star  typically observed on the same night as the galaxy observations  is used to correct for the large - scale wavelength dependence of the instrumental transmission . a secondary standard star  observed simultaneously with the galaxies  is then used to correct the telluric absorption bands .",
    "a full analysis of the flux calibration accuracy achieved for the sami galaxy survey early data release is given by @xcite .",
    "below we briefly describe the principles adopted .      in principle , each stage in the flux calibration can be performed by multiplying the observed galaxy spectra by the ratio of the true flux to the observed flux of a standard star . for ifu observations with a filling factor less than 100 per cent",
    ", we must also account for the light that falls between the fibres .",
    "crucially , the fraction of light lost in this manner is a function of wavelength , as a result of atmospheric dispersion and the dependence of seeing on wavelength .",
    "we make the correction by fitting the observed stellar flux across the hexabundle with a model psf , including the atmospheric effects .",
    "the model psf as a function of @xmath18 , @xmath19 position ( in arcseconds ) and wavelength , @xmath20 , takes the form of a @xcite profile : @xmath21 where @xmath22 , @xmath23 and @xmath24 give the position and size at an arbitrary reference wavelength @xmath25 . the dependence on wavelength",
    "is given by @xmath26 where @xmath27 is the zenith distance and @xmath28 is the parallactic angle .",
    "the refractive index of air as a function of wavelength , @xmath29 , is given by equations 13 of @xcite , which are in turn functions of temperature , pressure and water vapour pressure .",
    "to constrain the free parameters , the observed wavelength range in each ccd is divided into 20 chunks of 100 pixels each , having discarded the 24 pixels at the beginning and end of each ccd .",
    "the observed counts in each fibre are summed within each of these wavelength ranges , and the model psf is fitted to this summed data .",
    "the data from both ccds are fitted simultaneously , in order to have a wide wavelength range to constrain the atmospheric dispersion effects . during this fit",
    ", the parameters @xmath22 , @xmath23 , @xmath24 , @xmath30 and @xmath27 are allowed to vary along with the total flux in each wavelength range , while @xmath28 and the atmospheric parameters are fixed to their measured values were allowed as a free parameter of the model . ] .",
    "for sami observations under typical observing conditions the uncorrected atmospheric dispersion between 3900  and 7270  is of the order 1 , @xmath060% of a fibre core diameter . after correction",
    "the mean offset between these two wavelengths , averaged over a series of observations , is found to be 014 . at less than 10% of the fibre core diameter , this is deemed to be at the limit of measurement .    with the psf parameters set , the final step in extracting the spectrum is to fit for the overall flux , i.e.  the scaling of the psf , in each wavelength pixel .",
    "a uniform background level is also fit in each wavelength pixel , to allow for residual errors in the sky subtraction .",
    "each wavelength pixel is fit independently .",
    "the result is a spectrum recording the total number of ccd counts that would have been observed , if the filling factor of the ifu was 100 per cent .",
    "the spectrum for the primary standard ( typically multiple standard stars are observed , at a range of airmasses ) is then corrected for atmospheric extinction using the default siding spring observatory ( sso ) extinction curve ( scaled for airmass ) and rebinned to match the wavelength sampling of the reference spectrum , given in units of ergs@xmath31@xmath32@xmath31 . the ratio of the observed and reference spectra is then used to infer the wavelength - dependent scaling needed to calibrate the data in an absolute sense , or , equivalently , the instrumental response function , @xmath33 .    to minimise the effect of noise in the standard star observations , and of small - scale mismatches between the observed and template spectra , the measured ratio is smoothed by fitting a spline function .",
    "approximately eight spline knots are used within each arm , although extra knots are inserted around 5500  where @xmath33 shows a sharp turnover due to the dichroic cut off , and knots that lie within telluric bands are removed .    after smoothing , multiple observations of each standard star from a given night",
    "are combined to produce the final calibration .",
    "the agreement between different observations is very good in terms of the shape of @xmath33 , but there is some variation in the overall scaling with a standard deviation of 11.1% found in the available year-1 data set .",
    "this variation is driven by a combination of changes in atmospheric transmission and a weak degeneracy between the scaling and the fwhm in fitting the psf . to remove the effect of any outliers with discrepant normalisations ,",
    "the individual measurements of @xmath33 in each arm are re - scaled such that their value in the centre of the wavelength range is equal to the median of these values across the set of observations .",
    "the re - scaled measurements of @xmath33 are then combined using a simple mean .",
    "the response function is then applied to each science frame after correction with the airmass - scaled sso extinction curve .",
    "details of the accuracy of the calibration process , with sami datacubes referenced to ancillary data ( e.g. , sdss photometry and spectroscopy ) are given for the sami galaxy survey early data release by @xcite .",
    "the fraunhofer b - band , a telluric absorption feature due to atmospheric o@xmath34 at 6867 , falls within the red portion of the sami galaxy survey spectra and requires correction . because the telluric band strength is variable and strongly correlated with airmass , the correction is best derived from data contemporary with the science observations .",
    "for every science observation , one of the thirteen sami bundles is allocated to a secondary standard star .",
    "these secondary standards are colour selected to be f sub dwarfs , which have relatively flat and featureless spectral shapes , allowing simple modelling with either synthetic or empirical template spectra . the total spectrum for each secondary standard is extracted using the process described in  [ starspec ] , after the rss data have been flux calibrated as per  [ sec : prifluxcal ] .",
    "the secondary standards are used to correct for telluric absorption . as a byproduct of this process",
    ", the secondaries also provide per - dataframe information about the seeing , including an empirical measurement of dar .",
    "this information is used in the later alignment and drizzling stages .",
    "the spectrum of a f sub dwarf is sufficiently smooth around the regions of telluric absorption ( 68506960  and 71307360 ) that it can be modelled with a simple straight line fit .",
    "the fit is performed using all data outside of the telluric regions and redwards of h@xmath35 . the telluric absorption is then given by the ratio of the extracted spectrum to the straight line fit ; all galaxy spectra in the frame are divided by this ratio , with the correction applied only in the vicinity of the telluric feature .",
    "the flux calibration procedure assumes no change in atmospheric conditions ( other than for the telluric absorption features ) between the observations of the spectrophotometric standard star and the galaxy field . in practice ,",
    "some variation can occur , which to first order produces a wavelength - independent scaling of the observed flux . to correct for this scaling",
    ", the spectrum of the secondary standard star is extracted from the combined datacubes using the same procedure as described in section  [ sec : prifluxcal ] , and converted to a @xmath36-band magnitude by integrating across the sdss filter curve @xcite .",
    "this measurement is compared to the available photometry to find the appropriate scaling , which was then applied to all objects in the field .",
    "the sami hexabundle format imposes two restrictions on image reconstruction which are shown graphically in figure  [ layout ] . while the positions of the individual fibre cores that make up each ifu are well defined ( @xmath37 m relative errors , @xmath01% of a fibre diameter ) , they are on an irregular grid .",
    "secondly , each hexabundle ifu has a 73% fill - factor for a single observation .",
    "the full image profile is recovered through a conventional series of dithered observation with the telescope offset to fill in the gaps in target coverage .",
    "dithering generates a series of misaligned data frames which possess a well registered but non - common geometry . in principle",
    ", one can retain the data in this native format , but the exigencies of scientific analysis typically dictate that data should be rebinned , often to a common cartesian output grid .",
    "this facilitates straightforward stacking of dithered frames to allow outlier rejection and accrual of s / n as well as easy visualisation of resulting data products , and significantly simplifies many subsequent analysis steps .",
    "a simple option for resampling is to perform a nearest - neighbour interpolation of the hexabundle input data onto a regular cartesian grid .",
    "an immediate effect of such an operation is to effectively convolve the intrinsic resolution of the spatial data with a kernel corresponding to the chosen output spaxel size , blurring the image .",
    "secondly , such a resampling introduces a complex covariance between output spaxels which overlap more than one input fibre core .",
    "both issues can be minimised by selecting a fine pitch for the output spaxel grid , but this produces a bloated data format , containing significant redundant information , and ultimately reduces s /",
    "n due to the excessive oversampling . the problem is well documented , and a solution clearly defined by @xcite through their introduction of the _ drizzle _ algorithm , initially conceived to resample high - resolution imagining from @xmath38/wpfc2 , which sub - critically samples the psf from @xmath38 .    for a detailed description of the drizzle algorithm ,",
    "the reader is directed to @xcite .",
    "briefly , considering each input fibre core in turn , and for any given input geometry , one can calculate the overlap area of the input fibre core with each element of a predefined regular grid of output spaxels .",
    "the fractional area of an input fibre core covering each output spaxel dictates how the flux should be redistributed to each output spaxel ( figure  [ drizzleexample ] ) .",
    "this fractional area provides a _ weight _ for each output spaxel which represents the relative exposure of each output spaxel .",
    "output spaxels that do not sit within any input fibre cores will be assigned a weight of zero .",
    "output spaxels that fall on the border between multiple input cores will have a weighted contribution from each core , but the total weight for such spaxels will not exceed unity ( since by definition , an output element can not be completely inside one input element if it is also part of other input elements ) . to place",
    "dithered data onto the regularised grid , one merely recalculates the overlaps after perturbing the baseline position ( by the known telescope offset ) of the input fibre cores relative to the initial reference position .",
    "figure  [ weight ] presents an example of the relative weight mappings for three observation that make up part of a dither set .    for isolated output spaxels ,",
    "this process conserves total flux but does introduce covariance , an issue we return to in  [ neglectingcovar ] . in this regime , the associated error information for each spaxel can be redistributed such that the global signal - to - noise is preserved , i.e. , the input variance is simply weighted by the square of the weight map .",
    "the weight maps derived from the drizzle resampling trace the effective exposure time for each output spaxel . by design , not all output spaxels have the same effective exposure time .",
    "those that are not completely covered by one or more ifu fibre cores will have a reduced intensity proportional to the fractional area of the output spaxel covered by input fibre cores .",
    "depending on the output spaxel size chosen , some spaxels may have an effective exposure time of near zero even though they reside within the physical boundary of the ifu bundle .",
    "for this reason , a reconstructed image of a source within the drizzle resampled datacube will exhibit an intensity structure dominated by the relative weights of its component spaxels , largely obscuring the underlying source structure .    to overcome this effect , the resampled datacubes ( and their associated variance arrays )",
    "are stored with the weight map normalisation pre - applied ( i.e. , mosaic cubes are divided by their weight map in the default data product ) .",
    "this normalises the effective exposure time across the datacube and recovers the underlying source structure .",
    "this data format means most users may safely ignore the weight maps for most applications .",
    "provided the weight map is propagated alongside each data frame , the process of conversion between the two formats is reversible and deterministic .",
    "examples of three weight maps for a three - point dithered observation are shown in figure  [ weight ] and the imprint of variations in relative exposure across a resampled observation is shown in figure  [ weightedimages ] .",
    "the effect of the weight map is illustrated with three mosaics of a sami galaxy .",
    "the two upper rows each show a single dither position observation for the galaxy .",
    "the bottom row shows the reconstructed image after seven dither positions are combined .",
    "the first column shows the reconstructed image after division by the weight map , showing a constant effective exposure time for all spaxels .",
    "the second column shows the raw output from the drizzle process where the intensity distribution is heavily modulated by the different effective exposure times of each output spaxel due to the partial coverage by input fibre core .",
    "the third column shows the weight map , the effective exposure time for each spaxel.,width=321 ]    as dithered observations are combined , the relative weighting of all spaxels will approach uniformity across the mosaic and the relative exposure structure will be removed from the datacube .",
    "the practical implication of this is : + _ local properties _ of a source , such as surface brightness of emission features or light profile fitting , should use the default data format with the weighting already applied to the datacubes .",
    "+ _ global properties _ of a source , such as integrated h@xmath35 flux or continuum intensity , should pay strict attention to the weight map values to avoid erroneously including additional signal , particularly from the edges of the mosaic , which are highly scaled up from low relative exposure times with respect to the central regions of the cube .",
    "when combining data , cubes are first multiplied by their weight maps , the data and individual weight maps summed , and the summed data finally divided by the new weight map to preserve the correct flux calibration .      the relative offsets between each dithered dataset must be known before they can be drizzled to a common output spaxel grid and combined . in principle",
    ", these offsets can be obtained from telescope pointing and offset information available in each rss frame header . in practise this information does not provide the required level of accuracy , particularly when data are taken with multiple source acquisitions ( for example over multiple nights ) , a process which can affect the base pointing position .",
    "a number of methods were explored for alignment , largely treating each ifu individually .",
    "these included a simple centroid fitting , cross correlation of reconstructed ifs images and alignment using sdss imaging data as a reference frame .",
    "it was found that all work remarkably well for most source types , but fail significantly for some classes of objects , such as galaxies with disturbed morphologies , interacting pairs or low surface brightness systems .    in order to overcome this limitation , and to recover all the observed targets in a uniform manner",
    ", an alignment technique has been developed to simultaneously estimate the dither patterns for all ifus in a given observation .",
    "the principle treats each exposure as an image of the sky , and estimates the best - fitting coordinate transformation to align each rss frame to a reference rss frame ( typically the first observation in a sequence ) . in this way , even if one ( or more ) ifus can not be used for the estimate of the coordinate transformation ( for example due to a disturbed morphology system which provides unstable alignment results ) , the best - fitting solution allows us to recover the dither pattern for the entire frame .",
    "this goal is achieved in three steps .",
    "first , a 2d gaussian is fitted to an intensity map for each ifu ( obtained by collapsing the cube along the wavelength axis ) in order to recover the centroid ( i.e. , peak signal ) positions .",
    "assuming that the absolute position of the peak emission on the sky does not vary between exposures , the centroid coordinates can be used to estimate the best - fitting coordinate transformation necessary to align all ifus on a reference frame .",
    "second , the best - fitting coordinate transformation is computed using a python implementation of the irafgeomap task .",
    "we allow for a combination of shifts in the @xmath18 and @xmath19 directions , a rotation and a common plate - scale change in the @xmath18 and @xmath19 directions .",
    "specifically , the coordinate transformation has the following functional form : @xmath39 where @xmath40 and @xmath41 are the centroid positions in the reference frame , @xmath42 and @xmath43 are the centroid positions of the plate to be aligned , @xmath44 and @xmath45 are the rigid shifts in the x and y directions , @xmath46 is the magnification factor and @xmath47 is the rotation angle .",
    "a 2@xmath48 clipping technique is applied to remove those ifus ( 3 on average ) for which the 2d gaussian fit is unstable .",
    "the mean values of shift , magnification and rotation angle found for our data are @xmath035@xmath4 m , @xmath010@xmath49 and @xmath00.014degrees , respectively .",
    "third , we use an implementation of the iraf geoxytran task to apply the coordinate transformation to the central fiber of each ifu and determine its position on the reference plate . for each ifu",
    "the relative offset between the two exposures is then given by the difference in the positions of the central fibers .",
    "while the rotation term introduces a significant translation in base [ @xmath18,@xmath19 ] position between observations , the magnitude is sufficiently small that no accounting is made for rotation of fibre positions within a bundle ( the correction for internal rotation would be typically of the order 0.2@xmath4 m , less than 1% of a fibre core diameter and smaller than the relative positional uncertainty of fibre cores within each bundle ) .    in data analysed to date ,",
    "the typical rms for the final dither solution is @xmath012@xmath51 m @xmath58@xmath51 m ( i.e. , 1/9@xmath52 of the fibre size ) .",
    "these values are found to be consistent with those obtained with the single ifu methods for stable cases indicating the technique is providing a high accuracy and stable solution for all ifus across a given sami observation .",
    "once relative alignment has been determined , dithered observation data sets can be combined .",
    "we start by assuming @xmath53 datacubes are available from a dithered set .",
    "each datacube @xmath54 is an array of [ @xmath18,@xmath19 ] spaxels ( which for compactness we shall denote by @xmath55 , with the assumption that @xmath55 is defined in a reference frame with all cubes aligned ) .",
    "we also assume , as is the default for sami data , that the datacubes are stored with division by the weight map pre - applied (  [ weightcubes ] ) .",
    "each cube also has @xmath20 wavelength elements that , due to atmospheric dispersion , are offset with respect one and other .",
    "this misalignment is corrected by recomputing the drizzle mapping solution as a function of wavelength at regular intervals .",
    "the interval is chosen such that the accumulated dispersion misalignment is never more than 10@xmath56 of a spaxel .",
    "datacubes need not be a common size in [ @xmath18,@xmath19 ] , although the spaxel scale of the cubes to be combined must be constant and the region of union between cubes well defined .",
    "each cube has an associated variance map @xmath57 and map of the relative spaxel weights @xmath58 .",
    "we define an output datacube , @xmath59 , and its associated variance array , @xmath60 and weight map , @xmath61 , such that    @xmath62    the default values of @xmath54 must be multiplied by their weight maps in order to restore the true relative exposure time structure to the input data before summation . after summation , the output cube is divided by the new weight map to remove the relative exposure time structure from the final data product .",
    "not applying the weight map would leave the cube scaled for differences in the effective exposure times for each spaxel and would not correctly propagate observed flux . for many spaxels",
    "the weight will eventually exceed unity as multiple exposures are summed together .",
    "this merely indicates that the spaxel is fully sampled at a higher signal - to - noise than would be achieved for unit exposure time , due to multiple overlapping observations .",
    "while these relative exposure values must be carefully propagated in the image header , they present few problems .",
    "arbitrary renormalization of the weight map is permissible , provided the effective exposure time of the associated datacube is also renormalised by the same factor .",
    "this preserves the correct surface brightness for the datacube .",
    "cosmic rays and other defects will remain in the reduced datacubes at some level .",
    "additionally , bad pixels from the ccd will contain no data and hence will be missing / flagged in the datacubes . clipping flagged values",
    "is as simple as removing them from summation and reducing the corresponding value of @xmath63 for each @xmath64 in eq .",
    "the key is accurate flagging .    to remove outliers from the summation of pixel values a clipped mean",
    "is preferable because it simplifies variance propagation . to generate the clipping flags",
    ", we work on each spaxel and spectral pixel of the output cube @xmath64 independently , noting that there will be @xmath63 intensity values at each output pixel , one from each input datacube . note also that some input values will have weight @xmath65=0 due to the nature of the dithered observation .",
    "we generate the working vector , @xmath66 , and its associated variance array such that    @xmath67\\end{aligned}\\ ] ]    at this point a simple sigma clipping rejection , with a modest threshold , will fail due to the finite fibre footprints which sample different parts of each source due to the dithered observation stratergy .",
    "put simply , in comparing the dithered input spectra at each spaxel , one is not directly comparing like - with - like .",
    "for all real sources , which lack spatial discontinuities on the scale of sami data due to the seeing profile , the first order difference will simply be intensity .",
    "there will be no appreciable change in spectral shape across the output spaxels .",
    "therefore , if each input spectrum is first normalised to unity prior to construction of the vector @xmath66 the sigma - clipping outlier rejection flags only significantly deviant pixels ( a 5@xmath48-clipping threshold is used ) . with the errant pixels now cleanly identified , the remaining good pixels are used , without normalisation , in the summation equations of eq .",
    "[ sum ] . in this manner",
    ", highly discrepant data points are removed from the summation , while still retaining the correct intensity information from the dithered data set .",
    "the default data product presented above generates a variance array to track signal - to - noise for each input fibre spectrum as it is added to the dithered output mosaic .",
    "this simple data product fails to account for the significant correlation introduced between adjacent spaxels . by definition",
    "the covariance matrix is given by @xmath68 \\right ) \\cdot \\left(x_j-\\mathbb{e}[x_j]\\right)\\right ] , \\sigma(i , j)=\\mathbb{e}\\left [ \\left ( x_i - \\mathbb{e}[x_i ] \\right ) \\cdot \\left(x_j-\\mathbb{e}[x_j]\\right)\\right],\\end{aligned}\\ ] ] where @xmath69 is the expectation value of the data in spaxel @xmath70 .",
    "consider two adjacent output spaxels of the data cube , @xmath70 & @xmath71 which sit beneath a common input fibre - core with @xmath72 that contributes a fraction of flux to each output spaxel @xmath73 & @xmath74 .",
    "then @xmath75 and hence the variance term for spaxel @xmath70 is @xmath76 and the covariance between output spaxels is @xmath77 .    in order to test this error propagation model , a simulated dataset is generated .",
    "a dithered sequence of noise - free image frames are generated and drizzled onto the standard output grid .",
    "the input data is then duplicated and a gaussian random noise field of known distribution is added .",
    "this second data set is also drizzled onto the standard output grid .",
    "if the associated noise properties of the image are correctly propagated by the error model , then a histogram of the difference between pixel intensity values for the noise - free and noisy data , scaled by the error array , will be a gaussian distribution centred on zero with unit width .",
    "this is confirmed to be the case , indicating accurate propagation of variance information from rss frames to sami datacubes .",
    "this analysis confirms the correct propagation of the statistical errors recorded during the data reduction process .",
    "underlying systematic variations in the data cubes , due principally to variations in observing conditions during survey operations , are assessed by @xcite .    in essence , the drizzle process creates a number of identical copies of each input spectrum , each copy scaled by a value @xmath73 . since duplication and scaling of a spectrum",
    "must retain the intrinsic noise properties , the associated error array is simply scaled by @xmath73 to maintain the s / n in each copy of the spectrum .",
    "the noise properties of each output spaxel are correctly traced , but this simple model has made no accounting for the covariance between output spaxels , it merely correctly retains the noise properties of individual input spectra .",
    "neglecting covariance between spaxels makes the assumption @xmath78 for @xmath79 . with this assumption ,",
    "on summation of the output error values for a dithered data set we no longer recover the input error , @xmath80 .",
    "as the @xmath73 ( with @xmath81 ) sum to unity , @xmath82 , the quadrature sum is generally less than unity , @xmath83 .",
    "the s / n is correctly modelled within each individual output spectrum , but the noise level is underestimated across the full mosaic due to the lost covariance information . while a crude fix for this approximation would be to rescale all the @xmath84 by the factor @xmath85 , a method for tracking the covariance",
    "is considered below .",
    "the drizzle resampling methodology introduces unavoidable covariance between the output spaxels . for a detailed description of the problem ,",
    "see  7.1 of @xcite . in principle",
    "the covariance information can be tracked for each output pixel .",
    "indeed , the diagonal elements of the full covariance matrix ( i.e. the variance ) are already tracked in full .",
    "for an input fibre core diameter of 16 , and an output spaxel grid pitch of 05 , there are @xmath86 output spaxels with non - zero covariance for each input fibre core ( although only 4  9 of these have significant covariance depending on the specific input / output geometry ) . on mosaicing dithered data of this format each output spaxel has a non - zero covariance only within a @xmath87 spaxel grid of adjacent spaxels when adopting the default sami galaxy survey observing strategy ( a seven point dithered mosaic with a dither pitch of 072 , to be considered in ",
    "[ optimaldither ] , onto 05 spaxels with a 50% drizzle drop size reduction to be presented in  [ dropsize ] ) . explicit evaluation of the covariance between spaxels confirms there is no covariance outside of the @xmath87 spaxel grid .",
    "however , even assuming these limited overlaps , providing the full covariance information in the output mosaic files would require a significant and largely unwarranted increase in the data volume and processing time for each observation .",
    "when calculating and retaining the full covariance information , a datacube of @xmath88 mb is generated , requiring a runtime of @xmath89 minutes on a standard dual core desktop machine  a data volume and processing time that are prohibitively high for a large galaxy survey such as the sami galaxy survey .",
    "both data volume and processing time can be significantly reduced by noting that the structure of the @xmath87 covariance maps varies slowly with wavelength . by sampling at regular intervals in wavelength space and then interpolating between the measured values along the wavelength axis",
    "the full covariance matrix can be recovered with only minimal loss of information .",
    "the covariance cubes generated in this way are stored normalised to the variance ( i.e. , the central pixel of each 5@xmath95 covariance map has value 1.0 ) and require scaling by the variance to recover the correct magnitude .",
    "further more , the covariance maps are generated from the overlap fractions of the input fibre footprints , and hence apply the flux and variance values before normalisation for relative exposure time with the weight maps .",
    "as an example , consider the datacbe , @xmath90 $ ] , and its associated variance and weight arrays , @xmath91 $ ] and @xmath92 $ ] .",
    "two adjacent spaxels , @xmath93 and @xmath94 , will be covariant .",
    "the datacube contains the normalised spaxel values @xmath95 $ ] and @xmath96 $ ] , each of which has an input flux and variance , prior to normalisation by the weight map , given by @xmath97 & = c[x_n , y_n,\\lambda ] \\times w[x_n , y_n,\\lambda]\\\\   v'[x_n , y_n,\\lambda ] & = v[x_n , y_n,\\lambda ] \\times w[x_n , y_n,\\lambda]^2\\end{aligned}\\ ] ] the covariance of spaxel @xmath93 with spaxel @xmath94 is contained within the covariance matrix , @xmath98 , such that the covariance between @xmath99 $ ] and @xmath100 $ ] is given by @xmath101 \\times v[x_a , y_a,\\lambda ] \\times w[x_a , y_a,\\lambda]^2 % covar[x_a , y_a , x_b , y_b,\\lambda ] \\times v'[x_n , y_n,\\lambda]\\end{aligned}\\ ] ] in addition to sampling at regular wavelength intervals , it is critical to finely sample the full covariance matrix ( in the wavelength direction ) either side of wavelength slices at which the applied atmospheric dispersion correction changes .",
    "the correction shifts the on - sky positions of the input fibre cores relative to their position on the ccd , and therefore alters the covariance between output spaxels . as a compromise between adequately sampling the full covariance matrix , and minimising the processing time required to produce a datacube and the volume required to store that cube we opted to sample the covariance matrix at 100 pixel intervals along the wavelength axis , and additionally over a region of @xmath102 pixels at each wavelength slice the applied atmospheric dispersion correction is updated .",
    "this results in data processing times of @xmath103 minutes and datacube volumes of @xmath104 mb .",
    "the full covariance matrix is then recovered by interpolating between wavelength slices with covariance information , with the full covariance matrix produced in this way recovering @xmath105 per cent of the true covariance ( figure  [ covarhist ] ) .",
    "the sami datacubes contain an additional ` covar ` extension which contains the covariance information sampled in this fashion .",
    "the ` .fits ` header of this extension records the wavelength slices ( in pixels ) at which the covariance was sampled , with the header item , ` covarlocn ` giving the wavelength slice of the @xmath106 element along the wavelength axis of the sparsely sampled covariance matrix .",
    "an example implementation of the simple interpolation necessary to recover the full data is found in appendix  [ pseudocode ] .",
    "the effectiveness of this covariance compression model is assessed by comparing the full covariance matrix to the modelled data on a spaxel by spaxel basis .",
    "each spaxel is assessed across the 5@xmath95 covariance grid , normalised to unity centred on the spaxel ( i.e. , for unit variance ) . the difference between the true covariance matrix and the modelled data",
    "is then summed , both while retaining the sign of differences and also as an absolute sum of differences .",
    "a cumulative histogram of these sums is then generated ( figure  [ covarhist ] ) .",
    "it is found that for 85% of spaxels the missing covariance signal amounts to @xmath107% the variance associated with each spaxel for the simple summation . when considering the summed absolute differences the missing covariance signal remains @xmath108% of the variance for 80% of spaxels .",
    "this level of accuracy is considered to be sufficient for most sami galaxy survey purposes . for specific individual cases in which higher accuracy is required ,",
    "individual galaxy datacubes will be reformed with either finer sampling or full covariance array generation .",
    "the difference between the true covariance matrix and the modelled matrix is presented as the cumulative histogram of spaxels .",
    "the upper figure presents the cumulative histogram summing all under - estimates and over - estimates in the difference between the covariance model and the full matrix .",
    "the lower histogram considers the sum of absolute differences.,width=264 ]",
    "for a given fibre geometry within each sami ifu , and for a given number of dithered observations , it is necessary to determine the optimal dither pattern which provides the _ most uniform coverage _ of the output image map .",
    "in essence we wish to provide all parts of the combined output image with a uniform effective exposure time , i.e. , we wish to have a uniform weight map .",
    "the metric developed to optimise the alignment is to minimise the ratio of the standard deviation of flux weights in the output cube relative to the median flux weight .",
    "the minimisation is restricted to a circular region which completely encompasses the _ central _ position of the dither pattern ( typically the first observations ) .",
    "uniform coverage is trivially achieved by increasing the number of dither positions available .",
    "this is of course limited by finite observation time , ccd readout overheads , and the sensitivity requirement of achieving sky - limited dither exposures , all of which restricts the number of independent frames .",
    "analysis reveals that while the optimal dither strategy lies within an extended plateau of parameter space ( i.e. , there are many essentially optimal strategies ) it is surprisingly simple to select pathologically bad strategies that provide highly sub - optimal coverage and structure with a wide range of effective exposures times ( or even image holes ) across the final mosaic .    for the fibre arrangement of the sami hexabundles , a seven - point hexagonal close - packed strategy , with a radial dither offset of 45% of the fibre core diameter ( i.e. , 072 for the 16 sami fibres )",
    "was selected as the default sami observation mode ( figure  [ samifootprint ] ) .",
    "the impact of the rotation angle for this pattern was explored , noting that each sami hexabundle has an accurately known but distinct _",
    "lattice _ structure .",
    "the effect was found to be negligible due to the high fill - factor of the sami ifs , the optimisation being dominated by local coverage around each fibre - core . for a system with lower fill - factor , the orientation of the offset pattern relative to the fibre - core lattice would likely be significant .",
    "the most unexpected outcome was the realisation that the dither pitch should be less than half the fibre - core diameter in order to prevent the appearance of significant coverage holes at the centre of each core position .",
    "a further complication is the imprint of the dither strategy on the resolution of stacked datacubes .",
    "this issue is addressed in the next section .",
    "the main motivation for the original drizzle algorithm @xcite was to provide a means of recovering resolution in under - sampled @xmath38/wfpc2 images and avoid introducing a convolution with the selected output pixel size which would further degrade the under - sampled psf . using a number of sub - pixel dithered observations of the input image , @xcite demonstrate that not only can one avoid significant degradation of the input image , but also that a significant fraction of the intrinsic resolution of the imaging system can be recovered despite the sub - critical sampling of each individual component frame .",
    "this is achieved by reducing the effective size of each input spaxel while scaling the flux to account for the reduced area ( figure  [ drizzleexample ] ) .",
    "this reduction in the drizzle _ drop - size _ results in a smaller footprint for each input spaxels on the output grid .",
    "each additional frame is drizzled onto the output grid but shifted by a fraction of the input spaxel true size and , with careful selection of the drop - size reduction , the process has been shown to result in high - fidelity images which faithfully represent the input image profile while minimising image degradation due to pixelisation by the observing system .",
    "drizzle is now routinely applied to imaging data from numerous sources and the soundness of the resampling has been confirmed a number of times ( e.g. , * ? ? ? * and references therein ) .",
    "however , in such cases the transformation has always been for a continuously sampled image .",
    "below we explore the effects of drizzling with a reduced drop - size on the 73% fill factor and under - sampled sami data ( 16 fibres sampling with typical seeing of @xmath020 ) .      in order to test the impact of the sami sampling on recovery of an image psf",
    ", a series of model observations were created .",
    "noiseless stellar profiles , modelled in the first instance as a 2d symmetric gaussian profile , were generated assuming a seven - point hexagonal close - packed dither strategy .",
    "the model data were generated as rss frames and then mosaiced into datacubes using the sami drizzle code .",
    "the rss spectra were generated via numerical integration of the 2d psf across the face of a model sami hexabundle using measured core position properties .      in all cases",
    "a seven - point hexagonal dither was considered , a choice driven largely by the need to reach a final integration time total in individually sky limited observations .",
    "a number of different drizzle drop sizes are also explored for resampling the simulated rss data to construct the datacube .",
    "it is apparent from figure  [ drizdropsize ] that using the full drop size generates a broadened profile with a reduced peak intensity .",
    "the data are degraded by an amount consistent with the smoothing by the fibre diameter .",
    "on reducing the drop size from 16 to 08 resolution is recovered although not to the full input image resolution . the image degradation , an inevitable consequence of the subcritical sampling of the input image by the fibre footprint , is dependant on the input image fwhm . for the median",
    "seeing expected in survey quality sami data , this image blur results in an increase of the image fwhm of @xmath002 in simulated data . reducing the drizzle drop size below 50 percent of the fibre size does result in a more compact image , but the incomplete sampling due to the limited fill factor of the resulting seven - point dither pattern introduces artificial structure to the image .",
    "a limited quantity of observational data is available with which to confirm the simulated observations .",
    "each sami galaxy survey plate contains a single calibration star .",
    "these standard stars are analysed in the same manner as the simulated data , with the observational seeing assessed in each individual rss frame via forward modelling of the input stars and the final output image seeing measured directly from each stars dither combined datacube .",
    "datacubes are generated with an output spaxel size of 05 , with a drizzle drop size of 08 , a 50% reduction . at the time of writing 242 individual rss frames",
    "were available from 36 individual sami galaxy survey fields .",
    "this includes observations over a wide range of observational conditions including some periods of poor seeing . data taken in the poorest conditions will not comprise part of the final sami galaxy survey , but is included in this analysis for completeness .",
    "figure  [ drizdropsize ] compares the final recovered seeing for each star to the median of the input seeing for each data set .",
    "a chromatic term is visible between the red and blue datacubes , with the blue data typically 02 poorer .",
    "this is consistent with the typical wavelength dependence of seeing over the two wavelength ranges .",
    "a minor aat tracking error ( identified through analysis of the year one sami data ) is found to introduce a 10% ellipticity in the year-1 sami data set .",
    "remedial work is underway at the aat to rectify the issues .",
    "fwhm measurements are taken as the geometric mean of a 2d elliptical gaussian fit to the data .",
    "fitting residuals are improved slightly , leading to a modest reduction in fwhm , if a 2d moffat profile @xcite is fitted but for consistency with the simulated test data the gaussian form is presented .",
    "the measured cube fwhm values are found to lie above the 1:1 correlation , as expected .",
    "the departure from this correlation is not as marked as in the clean simulated data , largely due to the averaging effect for datasets with a marked seeing variation .",
    "in early sami data products no explicit resolution matching is performed beyond the simple rejection of markedly discrepant data sets .",
    "the mean and standard deviation of input seeing values are found to be 253@xmath5060 in the blue and 225@xmath5043 in the red .",
    "the significant scatter is due to the inclusion of the full year-1 data set in this analysis . for the resulting cubed data the corresponding values are 261@xmath5062 and 240@xmath5064 .",
    "this analysis indicates a degradation of the input of not more than 02 due to the alignment and cubing process when performed with the 50% drizzle drop size reduction . in the context of the input fibre diameter of 16 and typical aat seeing ,",
    "this is deemed satisfactory . as the sami galaxy survey progresses the median seeing of survey data",
    "will be improved via judicious flagging and rejection of poor seeing data .",
    "reduction of the drizzle drop size requires a rescaling of the output cube values in order to preserve the flux calibration .",
    "consider a linear reduction in the drop - size by a scale factor @xmath109 ( i.e. , @xmath109=0.5 for a drop size of 08 scaled down from the sami fibre core size of 16 ) .",
    "we perform the drizzle resampling of the input flux rss frame onto an output spaxel grid with the input fibre geometry set to incorporate the reduction factor .",
    "this produces the usual set of output data product , @xmath110 & @xmath111 .",
    "consider also the intermediate datacube prior to division by the weight map , @xmath112 .",
    "if the total flux in the output cubes is calculated for a data set with and without the scale factor @xmath109 clearly the values will differ by a factor of @xmath113 .",
    "this is a consequence of the smaller fibre foot prints covering fewer output spaxels and so less flux is distributed . in order to preserve the flux we find    @xmath114    however , because the default sami data products are provided with the data and variance cubes divided by the weight map , in order to remove relative exposure time variations across the mosaic , the @xmath113 scaling is effectively removed from the output datacube ( the scaling being tracked in the weight array , @xmath115 ) .",
    "this means that for most applications , users need not concern themselves with the weight maps .",
    "upper plot ) the recovered resolution , with and without a reduction of the drizzle drop size is shown for simulated data .",
    "simulated observations are generated with a range of gaussian psf fwhm .",
    "the full fibre diameter and a drizzle drop size of 08 diameter ( a 50% reduction ) are used to resample data onto an output spaxel size of 05 .",
    "a diagonal line marks the 1:1 correlation between input and output fwhm , while the curve corresponding to the quadrature sum of the input fwhm and the 16 fibre diameter . dashed vertical and horizontal lines mark this fibre diameter .",
    "lower plot ) for the available standard stars taken from year one of the sami galaxy survey science observations , the median input seeing for each observing block ( 4 - 8 dithered frames ) is compared with the seeing recovered from fitting to the final datacube of each star.,title=\"fig:\",width=340 ]   upper plot ) the recovered resolution , with and without a reduction of the drizzle drop size is shown for simulated data .",
    "simulated observations are generated with a range of gaussian psf fwhm .",
    "the full fibre diameter and a drizzle drop size of 08 diameter ( a 50% reduction ) are used to resample data onto an output spaxel size of 05 .",
    "a diagonal line marks the 1:1 correlation between input and output fwhm , while the curve corresponding to the quadrature sum of the input fwhm and the 16 fibre diameter .",
    "dashed vertical and horizontal lines mark this fibre diameter .",
    "lower plot ) for the available standard stars taken from year one of the sami galaxy survey science observations , the median input seeing for each observing block ( 4 - 8 dithered frames ) is compared with the seeing recovered from fitting to the final datacube of each star.,title=\"fig:\",width=340 ]",
    "to generate the data products necessary to realise the key science goals of the sami galaxy survey we have devised a scheme to generate cartesian gridded datacubes from the irregularly sampled sami observational data products .",
    "adapting established image processing techniques , we have demonstrated sami data can be accurately regularised , accounting for complications such as atmospheric refraction and dispersion , flux calibration and incomplete sampling of the focal plane .",
    "image degradation due to the sparse sampling and reconstruction is limited to that inherent in the use of an input fibre core comparable in size to the natural image seeing scale .",
    "photometric error information derived during data processing is accurately propagated as well as a compact representation of the complex covariance inevitable in the data .",
    "using these techniques , the sami galaxy survey is exploring high - multiplex integral - field spectroscopy .",
    "the sami galaxy survey is based on observation made at the anglo - australian telescope .",
    "the sydney - aao multi- object integral - field spectrograph ( sami ) was developed jointly by the university of sydney and the australian astronomical observatory .",
    "the sami input catalogue is based on data taken from the sloan digital sky survey , the gama survey and the vst atlas survey .",
    "the sami galaxy survey is funded by the australian research council centre of excellence for all - sky astrophysics ( caastro , ce1100010200 ) and other participating institutions .",
    "the sami galaxy survey website is http://sami-survey.org .",
    "we thank the dedicated staff at the anglo - australian telescope ( aat ) whose support in interfacing sami with aaomega is invaluable .",
    "mso & jta acknowledge the funding support from the australian research council through a super science fellowship ( arc fs110200023 & fs110200013 ) .",
    "smc acknowledges the support of an arc future fellowship ( ft100100457 ) .",
    "isk is the recipient of a john stocker postdoctoral fellowship from the science and industry endowment fund ( australia ) .",
    "lc acknowledges support under the australian research councils discovery projects funding scheme ( dp130100664 ) .",
    "cjw acknowledges support through the marie curie career integration grant 303912 .",
    "this research made use of astropy , a community - developed core python package for astronomy ( astropy collaboration , 2013 ) +",
    "presented below is a pseudo - code example of the covariance recovery process .",
    "while the spectral datacube is not required in the reconstitution of the covariance matrix , four other components are : the spectral variance cube , the weight map , the compressed covariance cube , and , the ` .fits ` header from the covariance cube extension .",
    "the procedures for regeneration of the covariance data is as follows :    * the covariance cube ` .fits ` header is checked for the keyword ` covarmod = optimal ` . * a full size array is created to store the reconstructed covariance . *",
    "the number of slices used to store the compressed covariance data is recovered from the covariance cube ` .fits ` header via the ` covar_n ` keyword . *",
    "the indices for the wavelength slices at which the covariance map has been stored in the compressed form are extracted from the covariance cube fits header via the ` covarlocn ` keywords - where ` n ` is the index of the slice in the reduced array .",
    "* running through each wavelength plane of the full size covariance array in turn , the array is populated with the corresponding slice from the compressed covariance array starting from element ` 0 ` of the compressed array , and moving to the next element of that array at the wavelength sliced indicated by ` covarlocn ` . * with the full size covariance array is now populated with the normalised data from the compressed array , one now loops over each wavelength slice in turn and multiplies the normalised mapping by the variance value , with the weight map normalisation removed , for each output spaxel .    .... # python based pseudo - code to regenerate covariance array .",
    "# input are : #                   the compressed covariance array #                   its fits header #                   the full variance array #                   the full weight map   # the full reconstructed covariance array is returned .",
    "def reconstruct_covariance(var_array , covar_array_red , weight_array , covar_header ) :      # reconstruct the full covariance array from the reduced covariance      # information stored in a standard cube           if covar_header['covarmod ' ] !",
    "= ' optimal ' :          raise exception('this cube does not contain covariance information in the optimal format ' )           # create an empty full covariance cube      # this has dimensions of [ wavelength , covariance scale , covariance scale , cube size , cube size ]      covar_array_full = np.zeros([2048,5,5,50,50 ] )           # populate the full covariance cube with covariance maps from reduced array      n_covar = covar_header['covar_n ' ]      for i in range(n_covar ) :          slice = covar_header['covarloc'+str(i+1 ) ]          covar_array_full[slice , : , : , : , : ] = covar_array_red[i , : , : , : , : ]            # fill values between calculated covariance map slices with       # the last calculated value      lowest_point = np.min(np.where((covar_array_full[1:,2,2,25,25 ] !",
    "= 0.0 ) &                                     ( np.isfinite(covar_array_full[1:,2,2,25,25])))[0 ] ) + 1      for i in range(2048 ) :          if np.sum(np.abs(covar_array_full[i , : , : , : , : ] ) ) = = 0 :              if i < lowest_point :                  covar_array_full[i , : , : , : , : ] = covar_array_full[lowest_point , : , : , : , : ]              else :                  covar_array_full[i , : , : , : , : ] = covar_array_full[i-1 , : , : , : , : ]                          # for each wavelength slice at each spaxel ,      # scale the normalised covariance maps by the appropriate variance removing the      # after removing the relative exposure time weight map normalisation .",
    "for i in range(2048 ) :          for x in range(50 ) :              for y in range(50 ) :          covar_array_full[i,:,:,x , y ] = covar_array_full[i,:,:,x , y ] * var_array[i , x , y ] * ( weight_array[i , x , y]**2 )                                              return covar_array_full ...."
  ],
  "abstract_text": [
    "<S> we present a methodology for the regularisation and combination of sparse sampled and irregularly gridded observations from fibre - optic multi - object integral - field spectroscopy . </S>",
    "<S> the approach minimises interpolation and retains image resolution on combining sub - pixel dithered data . </S>",
    "<S> we discuss the methodology in the context of the sydney - aao multi - object integral - field spectrograph ( sami ) galaxy survey underway at the anglo - australian telescope . </S>",
    "<S> the sami instrument uses 13 fibre bundles to perform high - multiplex integral - field spectroscopy across a one degree diameter field of view . </S>",
    "<S> the sami galaxy survey is targeting @xmath03000 galaxies drawn from the full range of galaxy environments . </S>",
    "<S> we demonstrate the subcritical sampling of the seeing and incomplete fill factor for the integral - field bundles results in only a 10% degradation in the final image resolution recovered . </S>",
    "<S> we also implement a new methodology for tracking covariance between elements of the resulting datacubes which retains 90% of the covariance information while incurring only a modest increase in the survey data volume .    </S>",
    "<S> [ firstpage ]    instrumentation : spectrographs , methods : data analysis , techniques : imaging spectroscopy </S>"
  ]
}