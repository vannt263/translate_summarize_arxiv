{
  "article_text": [
    "high - energy physics has advanced over the years by the use of higher energy and higher intensity particle beams , more capable detectors and larger volumes of data .",
    "the tevatron collider at fermilab is used to study fundamental properties of matter by colliding protons and anti - protons at very high energy .",
    "the fermilab tevatron run ii project has increased the intensity and energy of the proton and anti - proton beams .",
    "the collider detector at fermilab ( cdf ) detector is a large general purpose cylindrical detector used to measure charged and neutral particles that result from the proton - anti - proton collision .",
    "the cdf detector has been upgraded to take advantage of the improvements in the accelerator  @xcite .",
    "computing systems were also upgraded for processing larger volumes of data collected in run ii .",
    "the type of computing required for cdf data production can be characterized as loosely - coupled parallel processing  @xcite .",
    "the data consists of a group of  events \" , where each event is the result of a collision of a proton and an anti - proton .",
    "a hardware and software trigger system is used to store and save as many of the most interesting collisions as possible .",
    "each event is independent in the sense that it can be processed through the offline code without use of information from any other event .",
    "events of a similar type are collected into files of a data stream .",
    "data is logged in parallel to eight data streams for final storage into a mass storage system .",
    "each file is processed through an event reconstruction program that transforms digitized electronic signals from the cdf sub - detectors into information that can be used for physics analysis .",
    "the quantities calculated include particle trajectories and momentum , vertex position , energy deposition , and particle identities .",
    "the cdf production farm is a collection of dual cpu pcs running linux , interconnected with 100 mbit and gigabit ethernet .",
    "this farm is used to perform compute and network intensive tasks in a cost - effective manner and is an early model for such computing .",
    "historically , fermilab has used clusters of processors to provide large computing power with dedicated processors ( motorola 68030 )  @xcite or commercial unix workstations @xcite .",
    "commodity personal computers replaced unix workstations in the late 1990s . the challenge in building and operating",
    "such a system is in managing the large flow of data through the computing units .",
    "this paper will describe the hardware integration and software for operation of the cdf production farm .",
    "the first section will describe the requirements and design goals of the system .",
    "next , the design of the farm , hardware and software , will be given .",
    "the software system will be described in the next section .",
    "next , the performance and experiences with the system , including prototypes , will be described .",
    "finally , conclusions and general directions for the future are given .",
    "to achieve the physics goals of the cdf experiment at the fermilab tevatron , the production computing system is required to process the data collected by the experiment in a timely fashion .",
    "the cdf production farm is required to reconstruct the raw data with only a short delay that allows for the determination and availability of calibrations or other necessary inputs to the production executable .",
    "in addition the farm is expected to reprocess data and to process special data .    to accomplish rapid data processing through the farms , adequate capacity in network and cpu is required . in 2001 through 2004",
    "the cdf experiment collects a maximum of 75 events / second at a peak throughput of 20 mbyte / sec .",
    "the event processing requires 2 - 5 cpu seconds on a pentium iii 1 ghz pc .",
    "the exact number depends on the type of event , the version of the reconstruction code , and the environment of the collision .",
    "these numbers lead to requirements of the equivalent of 190 - 375 pentium iii 1 ghz cpus , assuming 100% utilization of the cpus .",
    "the output of event reconstruction is split into many physics data - sets .",
    "the splitting operation is required to place similar physics data together on disk or tape files , allowing faster and more efficient physics analysis .",
    "the output event size is currently approximately the same as the input .",
    "each event is written 1.2 times on average because some events are written to more than one output data set . therefore the system output capacity is also required to be approximately 20 mbyte / sec .",
    "in addition to providing sufficient data flow and cpu capacity for processing of data , the production farm operation is required to be easily manageable , fault - tolerant , scalable , with good monitoring and diagnostics .",
    "hardware and software options were explored to meet the requirements for the system .",
    "these include large symmetric multiprocessing ( smp ) systems , commercial unix workstations , alternative network configurations .",
    "prototype systems were built and tested before the final design was chosen and production systems built .",
    "the cdf data production farm is constructed using cost - effective dual cpu pc s .",
    "the farm consists of a large number of pcs that run the cpu - intensive codes ( workers ) , pcs that buffer data into and out of the farm ( readers and writers ) and pcs providing various services ( servers ) .",
    "the hardware architecture of the cdf production farm is shown in fig .",
    "[ fig : farm ] .",
    "it has two server nodes cdffarm1 and cdffarm2 .",
    "cdffarm1 is a sgi o2000 machine that host a batch submission system and a database server .",
    "cdffarm2 is a dual pentium server running control daemons for resource management and job submission .",
    "these two servers have recently been replaced by a dell 6650 machine ( cdffarm0 ) .",
    "monitoring and control interfaces for farm operation includes a java server to the control daemons and and a web server for monitoring .",
    "the disk space is a `` dfarm '' file system @xcite , which is a distributed logical file system using a collection of ide hard - disks of all dual pentium nodes .",
    "the dfarm server is hosted on cdffarm1 .",
    "the job scheduling on the production farm is controlled by a batch management system called fbsng developed by the computing division at fermilab @xcite .",
    "the cdf data handling group has well - defined interfaces and operation @xcite to provide input data for the farm and to write output to a mass storage system ( enstore ) @xcite .",
    ".farm nodes added over the years ( @xmath0 xeon cpu is scaled by 1.35 to the equivalent pentium iii cpu performance for cdf data reconstruction ) .",
    "the total in use in the summer of 2004 is 192 nodes ( 570 ghz ) .",
    "[ tab : workers ] [ cols=\"^,^,^,<\",options=\"header \" , ]     raw data from the experiment is first written to tape in the enstore mass storage system .",
    "raw data are streamed into eight data - sets listed in table  [ tab : rawdata ] .",
    "these tapes are cataloged in the cdf data file catalog ( dfc ) @xcite as a set of tables in an oracle database ( accessed via cdfora1 in fig .",
    "[ fig : farm ] ) . after the data",
    "is written to tape and properly cataloged , and once the necessary calibration constants exist , the data is available for reconstruction on the farms .",
    "the production farm is logically a long pipeline with the constraint that files must be handled in order .",
    "the input is fetched directly from enstore tapes and the outputs are written to output tapes .",
    "the data flow is illustrated in fig .",
    "[ fig : flow ] for the files moving through dfarm storage controlled by four production daemons .",
    "the daemons communicate with the resource manager daemon and the internal database to schedule job submission .",
    "the internal database is a mysql @xcite system used for task control , file - tracking , and process and file history .",
    "the dfc records are fetched at the beginning of staging input data .",
    "output files written to tapes are recorded in the dfc .",
    "job log files and other logs and files are collected to the user accessible fcdflnx3 node .",
    "operation status is monitored by a web server fnpcc .",
    "the operation daemons are configured specifically for production of a input `` data - set '' . for raw data ,",
    "each data stream is a data - set .",
    "the input files are sent to worker nodes for reconstruction .",
    "each worker node ( dual - cpu ) is configured to run two reconstruction jobs independently .",
    "an input file is approximately 1 gbyte in size and is expected to run for about 5 hours on a pentium iii 1 ghz machine .",
    "the output is split into multiple files , with each file corresponding to a data - set defined by the event type in the trigger system .",
    "an event may satisfy several trigger patterns and is consequently written to multiple data - sets that are consistent with that event s triggers .",
    "each data - set is a self - contained sample for physics analysis .",
    "the total number of output data - sets is 43 for the eight data streams used in the most recent trigger table .",
    "the cdf farm processing system ( fps ) is the software that manages , controls and monitors the cdf farm .",
    "it has been designed to be flexible and allows configuration for production of data - sets operated independently in parallel farmlets .",
    "a farmlet contains a subset of the farm resources specified for the input data - set , the executable and the output configuration for concatenation .",
    "since a farmlet is an independent entity , it is treated as such , that is , its execution is handled by its own daemons taking care of consecutive processing in production and its records are written in the internal database . the task control by fps for a farmlet is illustrated in fig .",
    "[ fig : bookkeeping ] .",
    "the daemons of the fps farmlets are :    * * stager * is a daemon that is responsible for finding and delivering data from tapes based on user selection for a set of data files or run range in the data - set .",
    "jobs are typically submitted one `` file - set '' at a time .",
    "a file - set is a collection of files with a typical size of 10 gbyte .",
    "the stager fetches dfc records for input and checks that proper calibration constants are available .",
    "the staging jobs are submitted to the input i / o nodes and the file - sets are copied to their scratch area , and afterward to dfarm .",
    "* * dispatcher * submits jobs through the batch manager to the worker nodes and controls their execution .",
    "it looks for the staged input file , which is then copied into the worker scratch area .",
    "the binary tarball ( an archive of files created with the unix tar utility ) containing the executable , complete libraries , and control parameter files are also copied .",
    "this allows the reconstruction program to run locally on the worker nodes and the output files , of various sizes from 5 mbyte to 1 gbyte , are written locally . at the end of the job the output files",
    "are then copied back to dfarm . in case of abnormal system failure ,",
    "job recovery is performed and the job is resubmitted . * * collector * gathers any histogram files , log files and any additional relevant files to a place where members of the collaboration can easily access them for the need of validation or monitoring purposes . * * concatenator * writes the output data that is produced to the selected device ( typically the enstore tape ) in a timely organized fashion .",
    "it checks the internal database records for a list of files to be concatenated into larger files with a target file size of 1 gbyte .",
    "it performs a similar task as the dispatcher , with concatenation jobs submitted to output nodes .",
    "the output nodes collect files corresponding to a file - set size ( @xmath1 gbyte ) from dfarm to the local scratch area , execute a merging program to read events in the input files in increasing order of run and section numbers .",
    "it has a single output truncated into 1 gbyte files .",
    "these files are directly copied to tapes and dfc records are written .",
    "since all of the farmlets share the same sets of computers and data storage of the farm , the resource management is a vital function of fps for distribution and prioritization of cpu and dfarm space among the farmlets .",
    "the additional daemons are :    * * resource manager * controls and grants allocations for network transfers , disk allocations , cpu and tape access based on a sharing algorithm that grants resources to each individual farmlet and shares resources based on priorities .",
    "this management of resources is needed in order to prevent congestion either on the network or on the computers themselves and to use certain resources more effectively . * * dfarm inventory manager * controls usage of the distributed disk cache on the worker nodes that serves as a front - end cache between the tape pool and the farm . * * fstatus * is a daemon that checks periodically whether all of the services that are needed for the proper functioning of the cdf production farm are available and to check the status of each computer in the farm .",
    "errors are recognized by this daemon and are reported either to the internal database which can be viewed on the web or through the user interfaces in real time .",
    "errors can also be sent directly to a pager with a copy to an e - mail address that is registered as the primary recipient of these messages .",
    "the fps framework is primarily coded in python @xcite .",
    "it runs on one of the server computers ( cdffarm2 ) and depends on the kernel services provided by cdffarm1 , namely the fbsng batch system , the fipc ( farm interprocess communication ) between the daemons and dfarm server governing available disk space on the worker nodes .",
    "daemons have many interfacing components that allow them to communicate with the other needed parts of the offline architecture of the cdf experiment . those include mainly the dfc ( data file catalog ) and the calibration database .      with hundreds of files being processed at the same time it is important to track the status of each file in the farm .",
    "file - tracking is an important part of fps and the bookkeeping is based on a mysql database .",
    "the database stores information about each individual file , process and the history of earlier processing .",
    "three tables are implemented for each farmlet : for stage - in of input files ; reconstruction and output files ; and the concatenation . the processing steps tracked by the book - keeping and records in each table",
    "are illustrated in fig .",
    "[ fig : bookkeeping ] .",
    "once a file is successfully processed , its records are copied over to the corresponding history tables .",
    "the file status is used in order to control the flow of data and to make sure that files are not skipped or processed more than once .",
    "the mysql database also includes detailed information about the status of each file at every point as it passes through the system .",
    "this information is available through a web interface to the collaboration in real time .",
    "this database server was designed to serve thousands of simultaneous connections . for our application",
    "it is a perfect match .",
    "emphasis is put on automatic error recovery and minimal human interaction in the course of processing . with the help of information that is stored in the internal database ,",
    "the system is able in most cases to recover and return to the previously known state from which it can safely continue to operate .",
    "the daemons checking the file history in the database are not instrumented to detect an abnormal failure for a job in process or a file lost to network or hardware problems .",
    "the concatenator often has to wait for output file in order to combine files in order .",
    "this bottleneck can be a serious problem and is a major consideration for relaxing strict ordering of files to improve overall system performance .",
    "the fps system status is shown in real time on a web page that gives the status of data processing , flow of data , and other useful information about the farm and data processing .",
    "the web page is hosted on a dual pentium node ( fnpcc on fig .",
    "[ fig : farm ] ) connected to the farm switch .",
    "the web interface was coded in the php language @xcite and rrdtool @xcite for efficient storage and display of time series plots .",
    "the structural elements in the schema include output from fps modules , a parser layer that transforms data into a format suitable for rrdtool , a rrdtool cache that stores this data in a compact way , and finally the web access to rrd files and queries from mysql for real time display of file - tracking information .",
    "the java fps control interface was designed for platform independent access to production farm control using an internet browser .",
    "information transfer between the client and server over the network is done using iiop ( internet inter - orb protocol ) which is part of corba@xcite .",
    "it has proved to be stable , and there have been no problems with short term disconnections and reconnections .",
    "an xml processor@xcite is used to generate and interpret the internal representation of data .",
    "abstract internal representation of data is important to cope with changes in the fps system .",
    "a java programming language , java web start technology @xcite was used for implementation of a platform independent client .",
    "the cdf experiment collected data samples in the tevatron run ii commissioning run in october , 2000 and the beginning of proton - antiproton collisions in april , 2001 .",
    "these events were processed through the cdf production farms .",
    "the events collected during the commissioning run were processed through two versions of the reconstruction code .",
    "the early 2001 data , taken under various beam and detector conditions , consist of about 7.6 million events and these were processed with one or two versions of the reconstruction code .",
    "this processing experience gave some confidence that the farm had the capacity to handle the volume of data coming from the detector and also uncovered many operational problems that had to be solved .",
    "beginning in june , 2001 , both the tevatron and the cdf detector ran well and began to provide significant samples for offline reconstruction .",
    "this early data was written in 4 streams and the output of the farms was split into 7 output data - sets .",
    "the cdf experiment wrote data at a peak rate of 20 mbyte / sec , which met the design goal .",
    "the farms were able to reconstruct data at the same peak rate .",
    "the output systems of the farm were adjusted to increase their capacity to handle the large output of the farms .",
    "more staging disk was added to provide a larger buffer and additional tape - drives were added .",
    "beginning in early 2002 the cdf detector and accelerator had reached a point where data was being recorded in the 8 final data streams defined for run 2 and the output was split into the final physics data - sets ( approximately 50 different data - sets ) .",
    "data was processed as quickly as possible and was normally run through the farms within a few days of having the final calibrations .",
    "approximately 500 million events were collected and processed during this period .",
    "upgrades were made to the farm with the addition of new nodes for processing as well as improved i / o capability .",
    "these improvements helped to maintain the processing capability as well as to provide some capability to catch up when calibrations were not ready in time or when the data taking rate was high or when reprocessing was necessary .    a major reprocessing of all data collected from the beginning of 2002 was begun in the fall of 2003 using the version of cdf code 5.1.1 .",
    "the output of this processing was later reprocessed with improved calibration ( version 5.3.1 ) of calorimetry and tracking leading to higher efficiencies .",
    "the reprocessing was launched in march , 2004 and the production farm operated at full capacity for a six week period . the main characteristics and performance of the farm",
    "is described in the following sections .",
    "the cpu speed and data through - put rate are the factors that determine the data reconstruction capacity of the production farm .",
    "the computing time required for an event depends on the event characteristics determined by the event trigger in different data streams .",
    "in addition , the intensity of the proton and antiproton beams matters .",
    "more intense beams lead to multiple events per beam crossing which in turn lead to more cpu time per event .",
    "inefficiency in utilizing cpu comes from the file transfer of the executable and data files to and from the worker scratch area .",
    "the event size and cpu time varies for different raw data streams . in fig .",
    "[ fig : cpuevt ] the cpu time per event is illustrated for reconstruction of cdf software version 5.3.1 .",
    "the cpu time on a dual pentium iii 1 ghz machine varies from 1 to 10 sec depending on the beam intensity and event size .",
    "the input data files are staged from enstore tapes .",
    "the rate of staging data depends on how fast the link to enstore movers is established .",
    "once a mover is allocated , staging a file - set of 10 gbyte takes about 20 minutes .",
    "the data transmission rate varies file by file , the commonly observed rate is around 10 mbyte / sec .",
    "output of concatenated files are copied to tapes .",
    "the effectiveness in staging data to a tape is a concern because of the limited dfarm space and output bandwidth . a concatenation job on the output node collects files of a data - set with close to 10 gbyte at a speed that may reach the maximum ide disk transfer speed of 40 mbyte / sec .",
    "it takes an average 10 minutes to copy all the files requested .",
    "the concatenation program reads the numerous small files and writes output that is split into into 1 gbyte files . on a pentium 2.6 ghz node",
    "the cpu time is about 24 minutes for processing 10 gbyte .",
    "the job continues by copying the output to enstore ( encp ) at an average rate of close to 20 mbyte / sec .",
    "the encp takes about 10 minutes for writing 10 gbyte .",
    "further delay may be caused by having more than one job accessing the same hard disk in dfarm , or waiting to write to the same physical tape .",
    "the output of reprocessing does not require concatenation , ( one - to - one processing with output file size of @xmath2 mbyte ) .",
    "therefore the operation has one fewer step .",
    "after the files are collected to output nodes , they are copied to enstore tapes . on average",
    "the stage - out takes 25 minutes for writing a file - set of 10 gbyte to enstore .",
    "the tape writing is limited to one mover per data - set at a time , to ensure that files are written sequentially on tape .",
    "a tape is restricted to files of the same data - set .",
    "the instantaneous tape writing rate is 30 mbyte / sec .",
    "however , the average rate drops to below 20 mbyte / sec because of latency in establishing connection to the mass storage system ( this includes mounting and positioning the tape establishing the end - to - end communication ) . running only one data - set on",
    "the farm limits the capability of the farm . running a mix of jobs from different data - sets in parallel increases the through - put of the farm by increasing the output data rate .    a concatenation job ( 25 min for 10 gbyte ) spends less than half its time accessing an enstore mover ( 10 min ) . when two jobs are running for the same data - set",
    "there is idle time while the mover is waiting for files .",
    "the observed data transmission rate is 9 mbyte / sec per data - set .",
    "the idle time for the mover is eliminated by adding one more job ( three concatenation for a data - set ) , and the data transmission rate increases to 15 mbyte / sec .",
    "the data reprocessing was performed with the revised cdf software version 5.3.1 . to maximize the farm efficiency",
    "the data reprocessing was performed on five farmlets with each farmlet processing one data - set .",
    "the tapes were loaded one data - set at a time , therefore farm cpu usage came in waves shared by a couple data - sets at a time .",
    "the cpu usage for the week of march 18 is shown in fig .",
    "[ fig : week_cpu ] . a lag in cpu utilization",
    "was observed when the farm switched to a new data - set , seen as the dips in cpu in fig .",
    "[ fig : week_cpu].a , because of lack of input files .",
    "file - sets are distributed almost in sequence on a tape the lag at the beginning of staging in a data - set is because the files requested are stored on the same tape , causing all the stage - in jobs to wait for one tape .",
    "overall the stage - in is effective in feeding data files to dfarm .",
    "the cpu usage varies for data - sets .",
    "the `` minimum bias '' data - set has smaller file sizes and the cpu per event is about 40% less than the average . when this data - set was processed , the stage - in rate was not able to keep up with the cpu consumption .",
    "the production farm operation is efficient . the cpu usage for the month of april 2004 is shown in fig .",
    "[ fig : month_cpu ] .",
    "each shaded area seen is one data - set being processed .",
    "the output data logging rate is shown in fig .",
    "[ fig : stat531 ] for the number of files , number of events , and total file size written to enstore tapes .",
    "compressed outputs were also created for selected data - sets . therefore the total events in output was increased by about 25% . the event size was reduced and resulted to a net reduction in storage by about 20% . on average",
    "we had a through - put of over 2 tbyte ( 10 million events ) per day to the enstore storage .",
    "the data logging lasted two extra weeks for a large b physics data - set that accounted for about 20% of the total cdf data .",
    "it was the latest data - set processed and the tape logging rate was saturated at about 800 gbyte per day .      the production farm uses cron jobs to check the online database for newly acquired data .",
    "timely processing is critical for detector monitoring . the express stream -",
    "a processing is used for monitoring data quality and beam - line calibrations are performed using stream - g data .",
    "data - sets of stream - b and stream - g are used for additional calibrations .",
    "full data processing is then carried out after final calibration constants are available .",
    "the load on the farm for new data , at a logging rate of 10 pb@xmath3 per week , is less than half of the cpu capacity .",
    "the raw - data volume collected and processed are shown in fig .",
    "[ fig : dual - line3 ] . in february 2004",
    "one of the major detector components was unstable .",
    "raw data processing was held except for detector studies .",
    "meanwhile the farm was put in use for 5.3.1 reprocessing .",
    "raw - data processing resumed in early may 2004 .",
    "data collected after february 2004 were processed with preliminary calibrations .",
    "later it was reprocessed with refined calibrations .",
    "the reprocessing was started in october 2004 .",
    "the cdf production pc farms have been successfully prototyped , installed , integrated , commissioned and operated for many years .",
    "they have been successful in providing the computing processing capacity required for the cdf experiment in run ii .",
    "the success of this system has in turn enabled successful analysis of the wealth of new data being collected by the cdf experiment at fermilab . the system has been modified and enhanced during the years of its operation to adjust to new requirements and to enable new capabilities .",
    "the system will continue to be modified in the future to continue to serve the cdf collaboration as required .",
    "some of the modifications are simple upgrades of components with more capable ( faster , more capacious ) replacements .",
    "other modifications will affect the architecture of the system and quite likely will embrace distributed processing and the grid in some way .",
    "these developments will allow cdf to continue to process and analyze data through the end of the life of the experiment ."
  ],
  "abstract_text": [
    "<S> the data production farm for the cdf experiment is designed and constructed to meet the needs of the run ii data collection at a maximum rate of 20 mbyte / sec during the run . </S>",
    "<S> the system is composed of a large cluster of personal computers ( pcs ) with a high - speed network interconnect and a custom design control system for the flow of data and the scheduling of tasks on this pc farm . </S>",
    "<S> the farm explores and exploits advances in computing and communication technology . </S>",
    "<S> the data processing has achieved a stable production rate of approximately 2 tbyte per day . </S>",
    "<S> the software and hardware of the cdf production farms has been successful in providing large computing and data throughput capacity to the experiment .    </S>",
    "<S> pacs : 07.05-t . </S>",
    "<S> keywords : computer system ; data processing </S>"
  ]
}