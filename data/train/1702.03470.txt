{
  "article_text": [
    "recently , many researchers @xcite showed the capabilities of deep learning for natural language processing tasks such as word embedding .",
    "word embedding is the task of representing each term with a low - dimensional ( typically less than 1000 ) numerical vector .",
    "distributed representation of words showed better performance than traditional approaches for tasks such as word analogy @xcite .",
    "some words are entities , i.e. name of an organization , person , movie , etc .",
    "on the other hand , some terms and phrases have a page or definition in a knowledge base such as wikipedia , which are called concepts .",
    "for example , there is a page in wikipedia for `` data mining '' or `` computer science '' concepts .",
    "both concepts and entities are valuable resources to get semantic and better making sense of a text . in this paper",
    ", we used deep learning to represent wikipedia concepts and entities with numerical vectors .",
    "we make the following contributions :    * wide coverage of words and concepts : about 1.7 million wikipedia concepts and near 2 million english words were embedded in this research , which is one of the highest number of concepts embedding currently exists , to the best of our knowledge .",
    "the concept and words vectors are also publicly available for research purposes .",
    "we also used one of the latest versions of wikipedia english dump to learn words embedding . over time , each term may appear in different contexts , and as a result , it may have different embeddings so this is why we used one of the recent versions of the wikipedia . * unambiguous word embedding : existing word embedding approaches suffer from the problem of ambiguity .",
    "for example , top nine similar terms to amazon based on pre - trained google s vectors in word2vec @xcite and glove @xcite models are in table [ hqv : table2 ] .",
    "word2vec and glove are the two first pioneer approaches for word embedding . in a document , amazon may refer to the name of a jungle and not the name of a company . in the process of embedding ,",
    "all different meaning of a word amazon is embedded in a single vector .",
    "producing distinct embedding for each sense of the ambiguous terms could lead to better representation of documents .",
    "one way to achieve this , is using unambiguous resources such as wikipedia and learning the embedding separately for each entity and concept .",
    "* we compared the quality versus the size of the corpus on the quality of trained vectors .",
    "we demonstrated that much smaller corpora with more accurate textual content is better than a very large text corpora with less accuracy in the content for the concept and phrase embedding .",
    "* we studied the impact of fine tuning weights of network by pre - trained word vectors from a very large text corpora in tasks of phrase analogy and phrase similarity .",
    "fine tuning is the task of initializing the weights of the network by pre - trained vectors instead of random initialization . *",
    "proposing different approaches for wikipedia concept embedding and comparing results with the state - of - the - art methods on the standard datasets .",
    "word2vec and glove are two pioneer approaches for word embedding . recently",
    ", other methods have been introduced that try to both improve the performance and quality of the word embedding @xcite by using multilingual correlation . a method based on word2vec",
    "is proposed by mikolov et al . for phrase embedding .",
    "@xcite . in the first step",
    ", they find the words that appear more frequently to gather than separately , and then they replace them with a single token . finally , the vector for phrases is learned in the same way as single word embedding .",
    "one of the features of this approach is that both words and phrases are in the same vector space .",
    "graph embedding methods @xcite using deep neural networks are similar to the goals of this paper .",
    "graph representation has been used for information management in many real world problems .",
    "extracting deep information from these graphs is important and challenging .",
    "one solution is using graph embedding methods .",
    "the word embedding methods use linear sequences of words to learn a word representation . for graph embedding , the first step is converting the graph structure to an extensive collection of linear sequences . a uniform sampling method named as truncated random walk was presented in @xcite . in the second step ,",
    "a word embedding method such as word2vec is used to learn the representation for each graph vertex .",
    "wikipedia is also can be represented by a graph , and the links are the inter citation between wikipedia s pages , called anchors .",
    "a graph embedding method for wikipedia using a similarity inspired by the hits algorithm @xcite was presented by sajadi et al . @xcite .",
    "the output of this approach for each wikipedia concept is a fixed length list of similar wikipedia pages and their similarity , which represents the dimension name of the corresponding wikipedia concepts .",
    "the difference between this method and deep learning based methods is that each dimension of a concept embedding is meaningful and understandable by the human",
    ".    a wikipedia concept similarity index based on in - links and out - links of a page was proposed by milne and witten @xcite . in their similarity method ,",
    "two wikipedia pages are more similar to each other if they share more common in and out links .",
    "this method is used to compare the result of concept similarity task with the proposed approaches .",
    "the idea of using anchor texts inside wikipedia for learning phrase vectors is being used in some other researches @xcite as well . in this research , we proposed different methods to use anchor texts and evaluated the results in standard tasks .",
    "we also compared the performance of the proposed methods with top notch methods .",
    "from this point on , we describe how we trained our word embedding . at first",
    "we describe the steps for preparing the wikipedia dataset and then describe different methods we used to train words and concepts vectors .",
    "[ [ preparing - wikipedia - dataset ] ] preparing wikipedia dataset : + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this research , the wikipedia english text used , is from wikipedia dump may 01 , 2016 . in the first step , we developed a toolkit using several open source python libraries ( described in appendix a ) to extract all pages in english wikipedia , and as a result 16,527,332 pages were extracted .",
    "not all of these pages are valuable , so we pruned the list by the several rules ( check appendix b for more information ) .",
    "as a result of pruning , 5,001,168 unique wikipedia pages , pointed at by the anchors , were extracted . for the next step ,",
    "the plain text of all these pages were extracted in such a way that anchors belonging to the pruned list of wikipedia pages were replaced ( using developed toolkit ) with their wikipedia page i d ( the redirects were also handled ) , and for other anchors , the surface form of them was substituted .",
    "we merged the plain text of all pages in a single text file in which , each line is a clean text of a wikipedia page .",
    "this dataset contains 2.1b tokens .",
    "[ [ convec ] ] convec : + + + + + + + +    the wikipedia dataset obtained as a result of previous steps was used for training a skip - gram model @xcite with negative sampling instead of hierarchical softmax .",
    "we called this approach as convec .",
    "the skip - gram model is a type of artificial neural network , which contains three layers : input , projection , and output layer .",
    "each word in the dataset is input to this network , and the output is predicting the surrounding words within a fixed window size ( we used a window size of 10 ) .",
    "skip - gram has been shown to give a better result in comparison to the bag of words ( cbow ) model @xcite .",
    "cbow gets the surrounding words of a word and tries to predict the word ( the reverse of the skip - gram model ) .    as a result of running the skip - gram model on the wikipedia dataset",
    ", we got 3,274,884 unique word embeddings , of which 1,707,205 are wikipedia concepts ( words and anchors with a frequency of appearance in wikipedia pages less than five are not considered ) .",
    "the procedure of training both words and concepts in the same model result in concepts and words belonging to the same vector space .",
    "this feature enables not only finding similar concepts to a concept but also finding similar words to that concept .    [",
    "[ convec - fine - tuned ] ] convec fine tuned : + + + + + + + + + + + + + + + + + + +    in image datasets , it is customary to fine - tune the weights of a neural network with pre - trained vectors over a large dataset .",
    "fine tuning is the task of initializing the weights of the network by pre - trained vectors instead of random initialization .",
    "we tried to investigate the impact of fine tuning the weights for textual datasets as well . in this case",
    ", we tried to fine - tune the vectors with glove 6b dataset trained on wikipedia and gigaword datasets @xcite .",
    "the weights of the the skip - gram model initialized with glove 6b pre - trained word vectors and then the training continued with the wikipedia dataset prepared in the previous step .",
    "we called the concept vectors trained based on this method convec fine tuned .",
    "[ [ convec - heuristic ] ] convec heuristic : + + + + + + + + + + + + + + + + + +    we hypothesize that the quality of concept vectors can improve with the size of training data .",
    "the sample data is the anchor text inside each wikipedia page .",
    "based on this assumption , we experimented with a heuristic to increase the number of anchor texts in each wikipedia page .",
    "it is a wikipedia policy that there is no self - link ( anchor ) in a page .",
    "it means that no page links to itself . on the other hand , it is common that the title of the page is repeated inside the page .",
    "the heuristic is to convert all exact mentions of the title of a wikipedia page to anchor text with a link to that page . by using this heuristic ,",
    "18,301,475 new anchors were added to the wikipedia dataset .",
    "this method called convec heuristic .",
    "[ [ convec - only - anchors ] ] convec only anchors : + + + + + + + + + + + + + + + + + + + + +    the other experiment is to study the importance and role of the not anchored words in wikipedia pages in improving the quality of phrase embeddings . in that case , all the text in a page , except anchor texts were removed and then the same skip - gram model with negative sampling and the window size of 10 is used to learn phrase embeddings .",
    "this approach ( convec only anchors ) is similar to convec except that the corpus only contains anchor texts .",
    "[ [ section ] ]    an approach called doc2vec was introduced by mikolov et al.@xcite for document embedding . in this embedding ,",
    "the vector representation is for the entire document instead of a single term or a phrase .",
    "based on the vector embeddings of two documents , one can check their similarity by comparing their vector similarity ( e.g. using cosine distance ) .",
    "we tried to embed a whole wikipedia page ( concept ) with its content using doc2vec and then consider the resulting vector as the concept vector .",
    "the results of this experiment were far worse than the other approaches so we decided not to compare it with other methods .",
    "the reason is mostly related to the length of wikipedia pages . as the size of a document increases the doc2vec approach for document embedding results in a lower performance .",
    "phrase analogy and phrase similarity tasks are used to evaluate the different embedding of wikipedia concepts . in the following , detail results of this comparison are provided .",
    "[ [ phrase - analogy - task ] ] phrase analogy task : + + + + + + + + + + + + + + + + + + + +    to evaluate the quality of the concept vectors , we used the phrase analogy dataset in @xcite which contains 3,218 questions",
    ". the phrase analogy task involves questions like `` _ _ word1 _ _ is to _ word2 _ as _ word3 _ is to _ word4 _ '' . the last word ( word4 )",
    "is the missing word .",
    "each approach is allowed to suggest the one and only one word for the missing word ( word4 ) .",
    "the accuracy is calculated based upon the number of correct answers . in word",
    "embedding , the answer is finding the closest word vector to the eq .",
    "@xmath0 is the vector representation of the corresponding word .",
    "@xmath1    @xmath0 is the vector representation of the corresponding word .",
    "the cosine is similarity used for majoring the similarity between vectors in each side of the above equation .    .comparing the results of three different versions of convec ( trained on wikipedia 2.1b tokens ) with google freebase pre - trained vectors over google 100b tokens news dataset in the phrase analogy task .",
    "the accuracy ( all ) , shows the coverage and performance of each approach for answering questions . the accuracy for common questions ( accuracy ( commons ) ) , is for fair comparison of each approach .",
    "# phrases shows the number of top frequent words of each approach that are used to calculate the accuracy . # found is the number of questions that all 4 words of them are present in the approach dictionary . [",
    "cols=\"<,<,^,^,^,^ \" , ]     in order to calculate the accuracy in the phrase analogy , all four words of a question should be present in the dataset . if a word is missing from a question , the question is not included in the accuracy calculation .",
    "based on this assumption , the accuracy is calculated using the eq .",
    "[ eq2 ] .",
    "@xmath2    we compared the quality of three different versions of convec with google freebase dataset pre - trained over google 100b token news dataset . the skip - gram model with negative sampling",
    "is used to train the vectors in google freebase .",
    "the vectors in this dataset have 1000 dimensions in length . for preparing the embedding for phrases , they used a statistical approach to find words that appear more together than separately and then considered them as a single token .",
    "in the next step , they replaced these tokens with their corresponding freebase i d .",
    "freebase is a knowledge base containing millions of entities and concepts , mostly extracted from wikipedia pages .    in order to have a fair comparison",
    ", we reported the accuracy of each approach in two ways in table [ hqv : table3 ] .",
    "the first accuracy is to compare the coverage and performance of each approach over the all questions in the test dataset ( accuracy all ) .",
    "the second accuracy is to compare the methods over only common questions ( accuracy commons ) .",
    "each approach tries to answer as much as possible to the 3,218 questions inside the phrase analogy dataset in _",
    "accuracy for all _ scenario . for top 30.000 frequent phrases ,",
    "google freebase were able to answer more questions , but for top 3,000,000 frequent phrases convec was able to answer more questions with higher accuracy .",
    "fine tuning of the vectors does not have impact on the coverage of convec this is why the number of found is similar to the base model .",
    "this is mainly because we used the wikipedia i d of a page instead of its surface name .",
    "the heuristic version of convec has more coverage to answering questions in comparison with the base convec model .",
    "the accuracy of the heuristic convec is somehow similar to the base convec for top 300,000 phrases , but it will drop down for top 3,000,000 .",
    "it seems that this approach is efficient to increase the coverage without significant sacrificing the accuracy , but probably it needs to be more conservative by adding more regulations and restrictions in the process of adding new anchor texts .",
    "only common questions between each method are used to compare the _ accuracy for commons _ scenario .",
    "the results in the last column of table [ hqv : table3 ] show that the fine - tuning of vectors does not have a significant impact on the quality of the vectors embedding .",
    "the result of convec heuristic for common questions , argue that this heuristic does not have a significant impact on the quality of base convec model and it just improved the coverage ( added more concepts to the list of concept vectors ) .",
    "the most important message of the third column of table [ hqv : table3 ] is that even very small dataset ( wikipedia 2.1 b tokens ) is able to produce good vectors embedding in comparison with the google freebase dataset ( 100b tokens ) and consequently , the quality of the training corpus is more important than its size .",
    "[ [ phrase - similarity - task ] ] phrase similarity task : + + + + + + + + + + + + + + + + + + + + + + +    the next experiment is evaluating vector quality in the phrase similarity datasets ( check table [ hqv : table4 ] ) . in these datasets ,",
    "each row consists of two words with their relatedness assigned by the human .",
    "the spearman s correlation is used for comparing the result of different approaches with the human evaluated results .",
    "these datasets contain words and not the wikipedia concepts .",
    "we replaced all the words in these datasets with their corresponding wikipedia pages if their surface form and the wikipedia concept match .",
    "we used the simple but effective most frequent sense disambiguation method to disambiguate words that may correspond to several wikipedia concept .",
    "this method of assigning words to concepts is not error prone but this error is considered for all approaches .",
    "wikipedia miner @xcite is a well - known approach to find the similarity between two wikipedia pages based on their input and output links .",
    "results show that our approach for learning concepts embedding can embed the wikipedia link structure properly since its results is similar to the structural based similarity approach of wikipedia miner ( see table [ hqv : table4 ] ) .",
    "the average correlation for the heuristic based approach is less than the other approaches , but average of not - found entries in this approach is much less than the others .",
    "it shows that using the heuristic can increase the coverage of the wikipedia concepts .    to have a fair comparison between different approaches",
    ", we extracted all common entries of all datasets and then re - calculated the correlation ( table [ hqv : table5 ] ) .",
    "we also compared the results with another structural based similarity approach called hitsim @xcite .",
    "the comparable result of our approach to structural based methods is another proof that we could embed the wikipedia link structure properly .",
    "the result of heuristic based approach is slightly better than our base model .",
    "this shows that without sacrificing the accuracy , we could increase the coverage .",
    "this means that with the proposed heuristic , we have a vector representation of more wikipedia pages .",
    "results for only anchors version of convec ( see the last column of table [ hqv : table5 ] ) show that in some datasets this approach is better than other approaches , but the average result is less than the other approaches .",
    "this shows it is better to learn wikipedia s concepts vector in the context of other words ( words that are not anchored ) and as a result to have the same vector space for both concepts and words .",
    "in this paper , several approaches for embedding wikipedia concepts are introduced .",
    "we demonstrated the higher importance of the quality of the corpus than its quantity ( size ) and argued the idea of the larger corpus will not always lead to a better word embedding .",
    "although the proposed approaches only use inter wikipedia links ( anchors ) , they have a performance as good as or even higher than the state of the arts approaches for concept analogy and concept similarity tasks . in contrary",
    "to word embedding , wikipedia concepts embedding is not ambiguous , so there is a different vector for concepts with similar surface form but different mentions .",
    "this feature is important for many nlp tasks such as named entity recognition , text similarity , and document clustering or classification . in the future",
    ", we plan to use multiple resources such as infoboxes , multilingual version of a wikipedia page , categories and syntactical features of a page to improve the quality of wikipedia concepts embedding",
    ".    10    s.  cao , w.  lu , and q.  xu .",
    "deep neural networks for learning graph representations . in _ proceedings of the thirtieth aaai conference on artificial intelligence _ , pages 11451152 .",
    "aaai press , 2016 .",
    "m.  faruqui and c.  dyer .",
    "community evaluation and exchange of word vectors at wordvectors.org . in _ proceedings of acl : system demonstrations _ , 2014 .",
    "m.  faruqui and c.  dyer .",
    "improving vector space word representations using multilingual correlation .",
    "association for computational linguistics , 2014 .",
    "l.  finkelstein , e.  gabrilovich , y.  matias , e.  rivlin , z.  solan , g.  wolfman , and e.  ruppin .",
    "placing search in context : the concept revisited . in _ proceedings of the 10th international conference on world wide web _ , www 01 , pages 406414 , new york , ny , usa , 2001 .",
    "g.  halawi , g.  dror , e.  gabrilovich , and y.  koren .",
    "large - scale learning of word relatedness with constraints . in _ proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining _ , pages 14061414 .",
    "acm , 2012 .",
    "f.  hill , r.  reichart , and a.  korhonen .",
    "simlex-999 : evaluating semantic models with ( genuine ) similarity estimation . , 2016 .",
    "j.  m. kleinberg .",
    "authoritative sources in a hyperlinked environment . , 46(5):604632 , 1999 .",
    "q.  v. le and t.  mikolov . distributed representations of sentences and documents . in _ icml _ ,",
    "volume  14 , pages 11881196 , 2014 .",
    "t.  luong , r.  socher , and c.  d. manning .",
    "better word representations with recursive neural networks for morphology . in _ conll _ , pages 104113 , 2013 .",
    "t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems _ ,",
    "pages 31113119 , 2013 .",
    "g.  a. miller and w.  g. charles .",
    "contextual correlates of semantic similarity . , 6(1):128 , 1991 .",
    "d.  milne and i.  h. witten .",
    "learning to link with wikipedia .",
    "in _ proceedings of the 17th acm conference on information and knowledge management _ , cikm 08 , pages 509518 , new york , ny , usa , 2008 .",
    "j.  pennington , r.  socher , and c.  d. manning .",
    "glove : global vectors for word representation . in",
    "_ emnlp _ , volume  14 , pages 153243 , 2014 .",
    "b.  perozzi , r.  al - rfou , and s.  skiena .",
    "deepwalk : online learning of social representations . in _ proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining _ , pages 701710 .",
    "acm , 2014 .",
    "m.  radovanovi , a.  nanopoulos , and m.  ivanovi .",
    "hubs in space : popular nearest neighbors in high - dimensional data .",
    ", 11(sep):24872531 , 2010 .",
    "a.  sajadi , e.  milios , v.  keelj , and j.  c. janssen .",
    "domain - specific semantic relatedness from wikipedia structure : a case study in biomedical text . in _ international conference on intelligent text processing and computational linguistics _ , pages 347360 .",
    "springer , 2015 .",
    "tsai and d.  roth .",
    "cross - lingual wikification using multilingual embeddings . in _ proceedings of naacl - hlt _ ,",
    "pages 589598 , 2016 .",
    "[ [ section-1 ] ]    the following libraries are used to extract and prepare the wikipedia corpus :    * wikiextractor : www.github.com/attardi/wikiextractor * mwparserfromhell : www.github.com/earwig/mwparserfromhell * wikipedia 1.4.0 : www.pypi.python.org/pypi/wikipedia    [ [ section-2 ] ]    the following libraries are used for word2vec and doc2vec implementation and evaluation :    * gensim : www.pypi.python.org/pypi/gensim * eval - word - vectors @xcite : www.github.com/mfaruqui/eval-word-vectors",
    "list of rules that are used to prune useless pages from wikipedia corpus :    * having < ns0:redirect > tag in their xml file . * there is category: in the first part of age name .",
    "* there is file: in the first part of page name .",
    "* there is template: in the first part of page name .",
    "* anchors having (disambiguation) in their page name .",
    "anchors having may refer to: or may also refer to in their text file .",
    "* there is portal: in the first part of page name .",
    "* there is draft: in the first part of page name .",
    "* there is mediawiki: in the first part of page name .",
    "* there is list of in the first part of the page name .",
    "* there is wikipedia : in the first part of page name . * there is timedtext: in the first part of page name .",
    "* there is help: in the first part of page name .",
    "* there is book: in the first part of page name .",
    "* there is module: in the first part of page name .",
    "* there is topic: in the first part of page name ."
  ],
  "abstract_text": [
    "<S> using deep learning for different machine learning tasks such as image classification and word embedding has recently gained many attentions . </S>",
    "<S> its appealing performance reported across specific natural language processing ( nlp ) tasks in comparison with other approaches is the reason for its popularity . </S>",
    "<S> word embedding is the task of mapping words or phrases to a low dimensional numerical vector . in this paper , we use deep learning to embed wikipedia concepts and entities . the english version of wikipedia contains more than five million pages , which suggest its capability to cover many english entities , phrases , and concepts . </S>",
    "<S> each wikipedia page is considered as a concept . </S>",
    "<S> some concepts correspond to entities , such as a person s name , an organization or a place . </S>",
    "<S> contrary to word embedding , wikipedia concepts embedding is not ambiguous , so there are different vectors for concepts with similar surface form but different mentions . </S>",
    "<S> we proposed several approaches and evaluated their performance based on concept analogy and concept similarity tasks . </S>",
    "<S> the results show that proposed approaches have the performance comparable and in some cases even higher than the state - of - the - art methods .    </S>",
    "<S> , concept embedding , vector representation </S>"
  ]
}