{
  "article_text": [
    "recent developments in the machine learning community have brought innovative ideas and methods to solve problems in data mining and predictive analytics with important applications in the business world such as credit risk , fraud detection , survey direct response , customer segmentation , etc .. among such methodologies ensemble learning has been an active research area within the last two decades , where it is believed that combining the predictions of different models or base models , e.g. , by weighted averaging , can bring an improvement in the prediction error in comparison to the error provided by the best individual model or base model .",
    "alternative ways that we will be using throughout the paper to refer to base models are base learner , level-0 learner or level-0 models .",
    "the setting of the approximation for regression we are considering is in the context of supervised learning : given an historical dataset @xmath0 with @xmath1 training samples @xmath2 , @xmath3 , where @xmath4 , and @xmath5 is a stochastic error , the task is to approximate the unknown function @xmath6 by @xmath7 , so we fit a model @xmath8 to the training data by minimizing the square error    @xmath9    interested readers can consult @xcite and @xcite for a deeper discussion about this topic .",
    "the aim of this work is to approximate the unknown function @xmath10 using a linear combination of base learners , or learning algorithms .",
    "so , instead of a single estimator @xmath7 we have a collection of them ; @xmath11 .",
    "we then learn each individual @xmath12 model separately using the error function in equation ( 1 ) on the training samples .",
    "once this is achieved , the outputs of the learned models are combined resulting in an approximating ensemble function @xmath13 :    @xmath14    the above function may be used to make predictions of @xmath15 for a new instance @xmath16 .",
    "there are different ways to determine the @xmath17 weight vector . in section 2.3",
    "we introduce the way we are going to calculate it in this paper .    the expected error of @xmath18 , or @xmath19",
    "@xmath20 @xmath21 @xmath22 , is defined as @xmath23 $ ] .",
    "the @xmath24 can be used to measure the generalization error of a predictor on new instances , and can be decomposed as the sum of two prediction errors @xcite , the variance , @xmath25 , or spread of @xmath18 around its average prediction , @xmath26 $ ] , and the square of the bias , @xmath27 , or amount by which the average prediction of @xmath18 differs from @xmath28 .    @xmath29 + [ \\bar{f}(\\boldsymbol{x}_0 ) - f^*(\\boldsymbol{x}_0 ) ] ^2 $ ]    this is known as the bias - variance dilemma .",
    "the variance term can be further decomposed for an ensemble as follows @xcite : @xmath30 =     e[{(\\tilde{f}(\\boldsymbol{x}_0)-e(\\tilde{f}(\\boldsymbol{x}_0 ) ) ) } ^2 ] = \\\\",
    "\\sum_{j=1}^{j}\\alpha_j^2(e[f_j^2(\\boldsymbol{x}_0)]-e^2[f_j(\\boldsymbol{x}_0 ) ] ) + \\\\   2\\sum_{j",
    "< k}\\alpha_j\\alpha_k(e[f_j(\\boldsymbol{x}_0)f_k(\\boldsymbol{x}_0 ) ] - e[f_j(\\boldsymbol{x}_0)]e[f_k(\\boldsymbol{x}_0 ) ] ) \\end{split}\\ ] ]    the expectation operator is taken with respect to the historical , or training dataset @xmath0 .",
    "the first sum contains the lower limit of the ensemble variance , which is the weighted mean of the variance of ensemble members .",
    "the second sum contains the cross terms of the ensemble members , which disappears if the models are completely uncorrelated @xcite .    according to the principle of the above bias - variance trade - off , an ensemble consisting of diverse models with much disagreement is more likely to have a good generalization performance @xcite .",
    "so , how to generate diverse models is very important .",
    "for example , for neural networks , @xcite suggest to initialize different weights for each neural network models , training neural networks with different training subsets , etc .. in our work , the main driver to achieve that purpose will be to inject a diversity mechanism into the cross - validation procedure , which will generate new partitions on which corresponding ensembles will be trained .",
    "we will then select a partition , or ensemble where the correlation between their base components satisfies a minimum correlation criterion , which is intended to have an effect in second sum of equation ( 2 ) to lower the variance .",
    "the aim is to establish a trade - off between bias and variance when training any predictor or classifier with good generalization , i.e. , with good performance when applying it on new instances , not present in the training mechanism .",
    "it is generally accepted that combining different classifiers can reduce the variance while not affecting the bias @xcite , @xcite .",
    "it should be mentioned briefly that another approach for improving the generalization error is based on statistical learning theory @xcite , which has its origins with @xcite in @xmath31 @xmath32 @xmath33 or pac learning .",
    "the goal of pac learning is to understand how large a dataset needs to be in order to give good generalization , we will not use this approach in our work .    according to @xcite ,",
    "most of the research focuses on one specific approach to build the ensemble , e.g. , sub - sampling from the training set or manipulating the induction algorithm , and further investigation is necessary to achieve gains by combining several approaches .",
    "this observation of using different ideas along with the potential gain we can achieve in prediction accuracy by a joint design of ensemble generation , and integration steps , have motivated us to investigate the performance of standard stacking approaches , and develop new extensions to stacking for regression .    in this work we present methodology that we have developed within the framework of ensemble learning through stacked generalization for regression . the main idea behind stacking",
    "is to generate a meta - training set from the model predictions on validation sets with the aim to integrate them through a learning mechanism , e.g. by weighted average .",
    "we propose to use a set of simple base learners , and consider the selection of the best base learner as a benchmark to compare the performance of the stacking approaches .",
    "our empirical evaluation shows that the oracle composed of best base learners performs better than the best of the standard stacking approaches used in this work . in this paper",
    "we will refer to any of the standard stacking approaches as level-1 learner , level-1 ensemble , or base ensemble as well .",
    "we propose a couple of extensions to the standard stacking approach .",
    "the first extension considers combining standard stackings into an ensemble of level-1 learners , using a two - step level-2 ensemble learning , i.e. , we are making the ensemble grow . the two - step level-2 ensemble learning scheme introduced here",
    "provides a way to measure the diversity of the base ensembles , or level-1 learners through computing pairwise correlations between its components , which will be exploited in a second extension with the aim to improve the prediction accuracy of the two - step level-2 ensemble .    the second extension to standard stacking is composed of two parts .",
    "the initial part consists of an extra diversity mechanism injected in the data generation which creates different cross - validation partitions from current cross - validation partition , on which new level-2 ensembles are trained .",
    "the final part , it is an algorithm based on computed level-1 learner correlation criterion , and ranking based - criterion , applied after two - step level-2 ensemble learning , which selects the most appropriate ensemble of standard stacking approaches , and/or corresponding data partition on which to make the final prediction . we show in this work that the latter extension performs better than the best of the standard stacking approaches , or level-1 ensembles , and it is as good as the best base model by cross - validation .",
    "we also show that such extension performs better than two of state - of - the - art ensemble learning algorithms , and it is as good as a third state - of - the - art method .    in section 2 , we summarize the state - of - the - art , the stacking framework , and survey some recent approaches of stacking regression .    in section 3",
    ", we introduce a set of base models , and level-1 learning which consists of standard stacking approaches , each composed of different combinations of base models .",
    "in section 4 , we present our first extension to standard stacking , level-2 learning , a mixture of ensembles of level-1 learners .",
    "we introduce two different learning schemes we have developed : two - step ensemble learning , and all - at - once ensemble learning .",
    "the former has much richer structure than the latter , which will be exploited in the algorithm presented later in section 5 .    in section 5",
    ", we introduce the second extension to stacking , which initially consists of building a diversity mechanism to generate different @xmath34 partitions by perturbing the current @xmath34 partition ; and finally a partition selection is done , using either a computed level-1 learner correlation - based criterion , or a ranking - based criterion within a max - min rule - based heuristic algorithm . at the end",
    ", we present such algorithm for systematic ensemble learning based on generating the partitions in an orderly way .    in section 6",
    ", the experimental setup is described for the comparison of the different ensemble approaches , the oracle of databases , and three state of the art ensemble learning algorithms .",
    "section 7 presents and discusses the experimental results , and section 8 concludes .",
    "our results match in error performance the ones obtained with the oracle of databases .",
    "in addition to that our results performed better than two of the state - of - the - art ensemble algorithms for regression , and it also matches the ones obtained with a third state - of - the - art ensemble method for regression .",
    "we review some background information about ensemble learning in section 2.1 , followed by the framework of cross - validation in section 2.2 , and in section 2.3 , we present stacking introduced by @xcite .",
    "we then describe in section 2.4 recent stacking approaches of incremental learning , model integration , and we review as well some recent studies for regression , less referred in literature .",
    "typically , ensemble learning consists of three phases , the first is ensemble generation which generates a set of base models .",
    "second is ensemble pruning where the ensemble is pruned by eliminating some of the base models generated earlier .",
    "finally , in the integration step a strategy is used to obtain the prediction for new cases .",
    "most of the work done in ensemble generation is about generating a single learning algorithm known as homogeneous learning . to ensure a level of diversity amongst the base models ,",
    "one of the following strategies are used @xcite : varying the learning parameters of the learning algorithm , varying the training data employed such as bagging @xcite , and boosting @xcite , both use re - sampling techniques to obtain different training sets for each of the classifiers .",
    "varying the feature set employed , such as random subspace @xcite , which is a feature selection algorithm , where each base learner corresponds to a different subset of attributes from the feature space . in randomize outputs",
    "approach , rather than present different regressors with different samples of input data , each regressor is presented with the same training data , but with output values for each instance perturbed by a randomization process @xcite , @xcite .",
    "another approach , known as heterogeneous learning , is to generate models using different learning algorithms where each model is different from each other .",
    "the conventional approach where each base model has been built with a different learning algorithm but the same training data is called @xmath35 , whereas a @xmath36-@xmath35 heterogeneous ensemble set where the base models are built from more than one learning algorithm , but each base model is not required to be built with a distinct learning algorithm @xcite .",
    "the base models in this last approach can be generated from homogeneous and/or heterogeneous learning .    in pruning step",
    "the objective is to reduce the size of the ensemble with the aim to improve the prediction accuracy @xcite .",
    "it has been shown that given a set of @xmath1 base models it is possible that an ensemble performs better if only uses a subset of such models instead of all of them .",
    "@xcite reduces the size using a measure of diversity and accuracy , where the diversity is measured with the positive correlation of base models in their prediction errors .",
    "they used homogeneous learning for stacked regression .",
    "the integration approach can be performed in two ways , either by combining the predictions of different models , or selecting one of the base models to make the final prediction @xcite .",
    "integration can be classified as @xmath37 or @xmath38 .",
    "it is static when for any new instance the combination of predictions is done globally in the same way , based on the whole instance set ; whereas for different new instances the dynamic approach combines the predictions based on different localized regions of the instance space . for the static case",
    "the simplest way to combine is to average the predictions of the base models , which is referred to as the unweighted averaging @xcite . among the weighted averaging techniques we have one based on the performance of individual ensemble members @xcite , or in the case of artificial neural networks @xmath39 to penalize correlations @xcite , and @xcite where the base models produced are negatively correlated .",
    "the simplest static selection approach is to choose the model with minimum cross - validation overall training error referred to as selection by cross - validation @xcite , o also as select best @xcite .",
    "it is very common to use these methods to benchmark ensemble approaches .",
    "cross - validation @xmath40 is a resampling technique often used in data mining for model selection and estimation of the prediction error in regression and classification problems . in order to measure the performance of one model the historical set @xmath0",
    "is split in two parts , the training @xmath41 and the validation @xmath42 .",
    "the training part @xmath41 is used to fit a model which is then used to predict the data on the validation set @xmath42 .",
    "because we know in advance the actual values for the predicted variable in the validation set @xmath42 , then a performance measure such as the @xmath24 , defined below lines for cross - validation , is used to estimate the accuracy achieved by the model .",
    "we should point out that estimating the prediction error must be done on data points on @xmath42 which are outside of the data @xmath41 that was used for model estimation to prevent from having _",
    "overfitting_. overfitting the training data happens when the model fits so well the training data that results in poor generalization when it is used to predict new instances not present in the fitting phase .",
    "the general @xmath43-fold cross - validation procedure is as follows :    \\1 .",
    "randomly split the data in @xmath43 almost equal parts @xmath44 .",
    "\\2 . for each @xmath45th",
    "fold define @xmath46 and @xmath47 to be the validation and training sets for the @xmath45th fold of a @xmath43-fold cross - validation .",
    "invoke the learning algorithm on the training set @xmath48 to estimate a model @xmath49 and calculate its prediction error on the @xmath46 fold .",
    "repeat the above procedure for @xmath50 .",
    "let @xmath51 be the @xmath52-th observation in @xmath46 and be @xmath53 the prediction of the estimated model on @xmath51 .",
    "the cross - validation estimate of mean square error is :    @xmath54 we will be using the cross - validated mean square error normalized by the sample variance for model performance , which is defined as    @xmath55      stacking is the abbreviation used to refer to stacked generalization @xcite , it has been used for regression tasks @xcite , and classification purposes @xcite .",
    "further development has been done by @xcite . for more recent extensions of stacking see @xcite . in stacking the general idea",
    "is to have level-0 models which consist of a set of @xmath56 different predictors whose task is to generate predictions based on different partitions of a dataset .",
    "those @xmath56 level-0 models are the learned models on a training dataset @xmath57 corresponding to @xmath56 learning algorithms .    given a dataset @xmath58 , where @xmath59 is the actual value and",
    "@xmath51 is a vector representing the attribute values of the @xmath52th instance , randomly split the data into @xmath43 almost equal parts to get @xmath46 and @xmath48 the validation and training sets for the @xmath45th fold of a @xmath43-fold cross - validation .",
    "these datasets constitute the @xmath60 @xmath61 .",
    "it should be noted that the validation subsets @xmath46 , @xmath62 are mutually exclusive and that their union gives the whole dataset @xmath57 .    for each instance @xmath51 in @xmath46 , the validation set for the @xmath45th cross - validation fold ,",
    "let @xmath63 denote @xmath64-th model fitted on training subset @xmath48 and @xmath65 denote the prediction of the estimated model @xmath66 on @xmath51 . at the end of the @xmath43-fold @xmath34 procedure ,",
    "the data set assembled from the outputs of the @xmath56 models along the @xmath43 validation subsets is    @xmath67    the @xmath68 @xmath61 consists of @xmath69 where + @xmath70 .",
    "then , using some learning algorithm that we call the @xmath68 @xmath71 to derive from these level-1 data a model @xmath7 for @xmath72 for prediction of new data @xmath73 .",
    "the @xmath68 @xmath74 is    @xmath75    the @xmath17 vector is chosen by minimizing    @xmath76    this prediction is expected to be more accurate than , e.g. the one we get with the best level-1 model selected from cross - validation .",
    "@xcite used as level-0 learners either linear regression with different number of variables , or different collections of shrinkage parameters for ridge regression .",
    "then , he used the level-0 models outputs , level-1 data , as input for the level-1 generalizer to minimize the least squares criterion under the constraints @xmath77 , @xmath78 , with the additional constraint @xmath79 .",
    "breiman concluded that both were better than the corresponding single best model as selected by cross - validation . stacking improved subset selection more than it improved ridge regression though , which breiman suggested was due to a greater instability of subset selection .",
    "he also stacked together subset and ridge regressions , which results in a predictor more accurate than either ridge , or variable selection .",
    "the key enabling concept in all ensemble based systems is diversity @xcite .",
    "little research , however , has been devoted to constructing ensembles with different base classifiers @xcite ; heterogeneous ensembles are obtained when more than one learning algorithm is used . in @xcite , they use heterogeneous base classifiers as base learners , and propose two extensions for stacking in classification problems .",
    "one extension is based on an extended set of meta - level features , and the other uses multi - response model trees to learn the meta - level .",
    "they showed , that the latter performs better than existing methods of stacking approaches , and selecting the best classifier from the ensemble by cross - validation .",
    "in heterogeneous learning the generated base models are expected to be diverse .",
    "however , the problem is the lack of control on the diversity of the ensemble components during the generation phase @xcite .",
    "one approach , commonly followed in the literature , combines two approaches : using different induction algorithms , e.g. , heterogeneous models , mixed with the use of different parameter sets , e.g. different neighbourhood sizes , see @xcite and @xcite . another way to overcome",
    "the lack of control in diversity is through ensemble pruning ; @xcite embedded the ensemble integration phase in the ensemble selection one .",
    "@xcite extended the technique of stacked regression to prune an ensemble of base learners by taking into account the accuracy , and diversity .",
    "the diversity is measured by the positive correlation of base learners in their prediction errors .",
    "the above examples show that an important part of the problems at the integration phase can be solved by a joint design of the generation , pruning , when appropriate , and integration phases @xcite .    in this work",
    "we propose a set of base learners diverse and accurate , and combine them using stacking into what we already called , standard stacking approach , or level-1 learning .",
    "we generate different standard stackings based on different combination of base models .",
    "we also combine such standard stackings into an ensemble of level-1 learners through a level-2 ensemble learning giving an extra level of smoothing in the predictions , which is a way to control the diversity of such ensembles .",
    "there has been considerably less attention given to the area of homogeneity in the area of stacked regression problems @xcite .",
    "for example , among the best state - of - the art stacking for regression methods is wmetacomb @xcite , which integrates two different integration methods using homogeneous learning .",
    "the method uses a weighted average to combine stacked regression , and a dynamic stacking method , @xmath80 ; the weights are determined based on the error performance of both methods .",
    "tests conducted on 30 regression data sets resulted , on one side , that wmetacomb never looses against stacked regression , where it wins 5 , and draws the remaining 25 ; on the other side , it looses 3 against @xmath81 , where it wins 13 and draws 14 .",
    "a state of the art method using heterogeneous learning for stacking regression is cocktail ensemble learning @xcite .",
    "they used an error - ambiguity decomposition to analyse the optimal linear combination of two ensembles , and extended it to multiple ensembles via pairwise combinations .",
    "they claim that such hybrid ensemble is superior to the base ensemble , simple averaging , and selecting the best ensemble .",
    "it should be noted that our initial work based on standard stacking differs from the literature in two aspects .",
    "first , we used a particular set of level-0 models , that we will introduce in section 3.1 , and second , in section 3.2 we introduce three standard stacking approaches each of them composed of a different combination of base learners .",
    "the @xmath82 learning algorithms we consider in this work are three versions of linear regression , and a fourth learning algorithm which can be seen as a type of non - linear regression .",
    "let @xmath11 be the set of @xmath56 learned base learners of corresponding @xmath56 different learning algorithms trained on a dataset @xmath57 .",
    "next , we will define the prediction models that we use as first level-0 learners .",
    "it is worth to mention that the methodology developed in this work can work with any number @xmath56 of different base learners .",
    "for the learned level-0 learner of constant regression @xmath83 , or naive regression , we just consider the simple average value of the dependent variable in the training set as the predictor , i.e. , @xmath84 .",
    "second , when we formulate a learned linear regression model @xmath85 on a training dataset , we have in total @xmath86 terms , corresponding to @xmath87 single terms plus the constant term , i.e. , @xmath88 , , where @xmath73 and @xmath89 are both vectors .",
    "third , we formulate a learned quadratic polynomial regression model @xmath90 on a training dataset , where we include @xmath87 single terms corresponding to @xmath87 independent variables , @xmath87 quadratic terms corresponding to the squared independent variables , as well as @xmath91 cross terms plus one term for the constant term . let @xmath92 be the matrix representation of the quadratic polynomial , where @xmath73 and @xmath93 are both vectors , and @xmath94 is a symmetric matrix .",
    "we are going to introduce a fourth learned level-0 learner trained on a training dataset known as radial basis function networks @xmath95 . in next section",
    ", we will denote @xmath96 same as @xmath97 , i.e. @xmath98 . in the same way we define @xmath99 as @xmath100 , @xmath101 as @xmath102 , and @xmath103 as @xmath104 .",
    "the radial basis function approach is a very active research discipline particularly in neural networks and penalized splines @xcite .",
    "we will be considering radial basis function networks as one type of radial basis model , which can be considered as an statistical model for non - parametric regression , to be introduced shortly .",
    "_ radial basis function networks _    given the set of @xmath1 training points @xmath105 where @xmath106 and @xmath107 . in order to obtain a radial basis function approximation of the set @xmath108 , a model @xmath7 of the form @xmath109    where @xmath110 and , @xmath111 is the radial distance between @xmath112 and @xmath51 , is fitted to the data by minimizing the error function in equation ( 1 ) .    in the above model",
    "the points @xmath51 are considered as the centers of the radial basis functions . in real applications to localize the radial basis functions on the input vector @xmath51",
    "can be very expensive , so usually a reduced number of centers @xmath113 where @xmath114 are estimated first . in our work",
    "an optimal number of centers is calculated by splitting the historical data in training and validation parts .",
    "this will give us the number of radial basis functions of the model .",
    "other parameter that is estimated in this part is the spread parameter @xmath115 which play a role in the gaussian and multi - quadratic models .",
    "we use @xmath45-means clustering for estimation of centers and @xmath115 s .",
    "once we estimate these set of parameters , we will estimate the weights @xmath116 s assuming a fixed number of radial basis functions , and a constant spread for each function . in this way",
    "the estimation of weights is linear and can be estimated by standard least squares optimization of the error function as in equation ( 1 ) .",
    "this is an advantage of this kind of models @xcite where we are avoiding to optimize together all these parameters , which would require a non - linear optimization of the weights where the error function is non - convex , and with many local minima @xcite .",
    "another advantage to do it this way is that the interpretation of the model , from the statistical point of view , is very simple as a linear combination of non - linear transformation of the input variables .",
    "nevertheless , this two parameter learning approach is a very active research subject .",
    "there are six basis functions , which are recognized as having useful properties for @xmath117 models @xcite : inverse multi - quadratic , gaussian , multi - quadratic , thin plate spline , cubic and linear .",
    "for example , the gaussian is defined as @xmath118 for @xmath119 and @xmath120 , and is considered a local function in the sense that @xmath121 as @xmath122 .",
    "the same property is exhibited by inverse multi - quadratic function .",
    "on the other hand , the last four are global radial basis functions , which all have the property @xmath123 as @xmath122 , e.g. this is valid for thin plate spline , which is defined as @xmath124",
    ". for formulations of the other functions please see @xcite .    among the group of local radial basis functions , we are interested on gaussian radial basis function as the main replacement of the quadratic regression model because , we think , it will provide a better non - linear behaviour than the quadratic regression model .",
    "we introduce and define in section 3.2.1 , the standard stacking approaches proposed in this work based on different combinations of level-0 learners .",
    "the basic methodology is described next :    1 .",
    "we estimate first learned constant regression @xmath96 , linear regression @xmath99 , and quadratic regression @xmath101 on the training sets of a @xmath43-fold @xmath34 .",
    "the first one is just the average value taken on the dependent variable values .",
    "the last two are calculated from ridge regression by minimizing the penalized sum of squares using tikhonov regularization .",
    "it should be mention that the we used a fixed value for the regularization parameter in the ridge regression estimate , we do not optimize such regularization parameter using cross - validation .",
    "we then compute the predictions of each based model on the validation folds of a @xmath43-fold @xmath34 .",
    "+ then , we define a level-1 @xmath71 , or learning ensemble @xmath125 as a linear combination of base learners in set @xmath126 : @xmath127 where we estimate the weight vector @xmath17 by @xmath43-fold cross - validation @xmath40 based on a stacked generalization procedure to combine the individual model predictions + for the determination of the weights , we followed what we found in literature , which can be made by optimizing last equation in section 2.3 , which we reproduce here for convenience .",
    "optimal weight learning from @xmath34 .",
    "* given a dataset @xmath58 do @xmath43-fold @xmath34 .",
    "+ @xmath128 + @xmath129 + in the above equation , the @xmath68 @xmath61 consists of @xmath69 where + @xmath70 , @xmath130 , so @xmath131 are predictions of level-0 learners on @xmath51 , for @xmath132 .",
    ", we will use a level-1 generalizer , or ensemble learner for predictions of a `` new '' data @xmath73 on the testing dataset .",
    "@xmath133 + where the optimal @xmath134 weight vectors of models @xmath135 are estimated based on @xmath57 the whole historical data , and the weight vector @xmath17 is estimated using equation ( 3 ) for a given @xmath34 .",
    "we call the learning , in equation ( 3 ) and equation ( 4 ) , to determine the weight vector @xmath17 , one - step learning , or @xmath136 learning , and denote the level-1 generalizer @xmath137 in equation ( 4 ) as @xmath138 for the set of base models in @xmath139}. so , the approximation @xmath140 for a new @xmath73 based on optimal weight vector @xmath141 for a given @xmath34 is    @xmath142    as a result of the fourth learned base learner @xmath143 , @xmath117 , we consider two additional standard approaches @xmath144 , and @xmath145 , such that for a prediction of a new observation @xmath112 based on corresponding optimal weights @xmath146 and @xmath147 we have    @xmath148    @xmath149    so , the ensembles @xmath150 , and @xmath151 are linear combinations of base learners in the sets of @xmath152 } , and @xmath153 } , respectively . for the generation of the above ensemble learners we are keeping @xmath96 in each of possible combination of the the generated ensembles .",
    "we have found empirical evidence that for a few databases the @xmath96 model has a higher weight than any of the other models , giving the corresponding ensembles better prediction error than any of the individual models .",
    "therefore , all possible combinations of the remaining three level-0 learners taken two at a time makes a total of three level-1 learners : @xmath154 , @xmath150 and @xmath151 by keeping present @xmath96 in each ensemble .",
    "the motivation behind the level-2 ensemble learning we are proposing is to see whether combining , or mixing a set of level-1 learners using stacked generalization , we can improve the prediction error in comparison to the level-1 learning , particularly against the best level-1 learner among the standard stacking approaches . in the next two sections ,",
    "we propose , as our first extension to standard stacking , two different level-2 learning approaches based on stacked generalization : the first approach , presented in section 4.1 , is based on two - step optimization , and the other , which is presented in section 4.2 , we call it all - at - once optimization .",
    "it is worth to mention that , the iterative nature of two - step learning will be advantageous over the all - at - once optimization to improve prediction error as we will discuss later at the end of this section .",
    "we will introduce our scheme of level-2 learning through iterative optimization based on optimal weights . given the new predictions by the level-1 learners , @xmath155 , @xmath156 and @xmath157",
    ", we can ask ourselves how to combine them ? we suggest to apply the same @xmath158fold @xmath34 procedure on this new set of level-1 learners , instead of the ensemble composed of @xmath159 , @xmath160 , @xmath161 and @xmath162 , to determine the corresponding set of weights @xmath163 , @xmath164 , and @xmath165 , for @xmath154 , @xmath150 and @xmath151 respectively .",
    "the optimal weight vector @xmath166 is obtained through an optimization process , level-2 @xmath71 , resulting in corresponding set of optimal weights @xmath167 , @xmath168 and @xmath169 .",
    "once we get the optimal weight vector @xmath166 , we can now combine the predictions of trained level-1 learners @xmath170 , @xmath171 and @xmath172 optimally as @xmath173 to get the approximation of level-2 learner .",
    "it should be pointed out that if we expand , e.g. , the equation for @xmath174 according to    @xmath175 @xmath176 @xmath177    we get the following expression for @xmath174 in terms of the models @xmath96 , @xmath99 , @xmath101 and @xmath103 :    @xmath178 where    @xmath179",
    "@xmath180    @xmath181    @xmath182    analysing any @xmath183 s weight , e.g. @xmath184 corresponding to model @xmath99 , we can observe that it is additively combining the @xmath17 s weights of model @xmath99 , @xmath185 and @xmath186 , which are present in ensembles @xmath154 and @xmath150 from first step of learning , through the extra weights @xmath163 and @xmath164 that were gotten in the second step of learning for such ensembles .",
    "therefore , the set of weight vectors @xmath17 and @xmath187 were obtained running two separated but iterative optimizations which were called first step and second step of learning respectively .",
    "we will denote this two - step optimization using the @xmath187 s weights as follows :    * given a dataset @xmath58 do @xmath43-fold @xmath34 .",
    "+ @xmath188 + @xmath189 @xmath190    in the above equation , the @xmath191 - 2 data consists of @xmath69 where + @xmath70 , @xmath192 , and @xmath193 on @xmath51 are predictions of level-1 learners for @xmath132 .",
    "we call this two - step learning , or @xmath194 learning , and denote the approximation , or level-2 ensemble learner based on weight vector @xmath187 for predictions of a `` new '' data @xmath112 on the testing dataset as    @xmath195    where the optimal @xmath196 weight vectors of models @xmath197 are estimated using an optimization similar to equation ( 3 ) for a given @xmath34 , where we replace @xmath198 . on the other hand ,",
    "the weight vector @xmath187 is estimated using equation ( 5 ) for a given @xmath34 .",
    "we can as well run both optimizations at once where we optimize simultaneously both weight vectors @xmath17 and @xmath187 which now will be denoted as @xmath199 and @xmath200 respectively to emphasize that the optimized values are different from the iterative optimization .",
    "therefore , given    @xmath201    @xmath202    @xmath203    @xmath204    we can formulate such optimization via stacked generalization as follows :    @xmath205    @xmath206 @xmath207 @xmath208 @xmath209 @xmath210 @xmath211    where    @xmath212 , @xmath213 and @xmath214 are the set of weights for the set of level-0 learners @xmath215 in the learning ensemble @xmath216 .",
    "@xmath217 , @xmath218 and @xmath219 are the set of weights for the set of level-0 learners @xmath220 in the learning ensemble @xmath221 .",
    "@xmath222 , @xmath223 and @xmath224 are the set of weights of the level-0 learners @xmath225 in the learning ensemble @xmath226 .    and @xmath227 , @xmath228 and @xmath229 are the set of weights for the level-1 learners @xmath154 , @xmath150 and @xmath151 in learning ensemble @xmath230 .",
    "this optimization , as we already said , optimize all parameters at once . , and we call it @xmath231 learning , and denote the approximation , or level-2 ensemble learner based on weight vector @xmath183 for predictions of a `` new '' data @xmath112 on the testing dataset as    @xmath232    where the optimal @xmath134 weight vectors of models @xmath233 are estimated based on @xmath57 the whole historical data",
    ".    there is one more question that we should ask ourselves at this point .",
    "is the two - step learning better than all - at - once learning ?",
    "this question could be answered in two ways .",
    "first by noting that according to the equivalence that we just showed for both methodologies , in terms of the corresponding gamma weights , we would expect to get very close results in terms of the prediction error for any database considered .",
    "second , we found empirical evidence by running both methodologies across the database considered where we actually got very close results in terms of their corresponding error performances .",
    "however , the iterative nature of the two - step ensemble learning and its richness in structure in terms of of the pairwise correlation between each pair of level-1 ensemble predictions in the mixture , will be exploited and bring some insight about how to make two - step ensemble learning better than all - at - once optimization to improve error performance , as we show in detail in next section , particularly section 5.2 .",
    "in stacked generalization the components of the ensemble , i.e. the level-0 learners , must be as diverse as possible , meaning that they provide predictions which are different from each other , otherwise if all of them provide the same predictions there will not be any improvements when we combine them .    in section 5.1 , we present in detail our second extension to standard stacking approach .",
    "as a first part of such extension , we propose to inject a diversity mechanism by perturbing the original @xmath34 partition to generate new @xmath34 partitions on which we can train new generated level-2 learners using stacked generalization , and two - step ensemble learning .",
    "particularly , in section 5.1.1 , we introduce a combinatorial mechanism to generate all possible @xmath34 partitions forming an exhaustive search space , as well as a systematic way to generate @xmath34 partitions to shorten significantly the huge combinatorial search space given by the exhaustive search . in section 5.1.2 ,",
    "we formulate two - step ensemble learning based on a new generated partition .    in section 5.2 , we introduce , as a final part of the second extension , methodology we have developed for this work , that allows us to quantify the quality of a partition , and select one on which to make the final prediction . in section 5.2.1",
    ", we exploit the structure of two - step level-2 ensemble learning to measure its diversity in terms of its base ensembles , or level-1 learners ; introducing the concept of computed level-1 ensemble correlation in their prediction errors",
    ". this will provide us with relevant diversity information about the quality of the different @xmath34 partitions . in section 5.2.2",
    ", we introduce a decision matrix to support the selection of a partition . in section 5.2.3",
    ", we present a correlation - based criterion , and a partition ranking - based criterion to help us make a more informed decision by ranking such partitions . in section 5.2.4",
    ", we introduce heuristic rule - based algorithm @xmath41 , that uses the above two criteria , to select the best @xmath34 partition that contains a minimum correlation rank , with the aim to improve the prediction accuracy of the level-2 ensemble corresponding to the original @xmath34 .        in our initial @xmath43-fold @xmath34",
    "was obtained by removing one of the @xmath43 subsets , which results in a set of partitions , that will be represented by @xmath234 . in order to generate different subsets of learning datasets",
    "we now take out two subsets instead of only one fold out of @xmath43 .",
    "the total number of possibly different partitions sets that can be generated this way out of a @xmath43-fold @xmath34 partition is prohibitively large , with a total of @xmath235 new partitions in addition to the original @xmath234 partition .",
    "so , we will generate only @xmath236 new different @xmath34 partitions in an orderly way .",
    "it will provide us with a systematic mechanism to explore a reduced set of alternatives , and to test the algorithms proposed in this paper in a computationally efficient way .",
    "for some @xmath237 in @xmath238 , the partition set for a new systematic generated partition will be denoted as @xmath239 .",
    "please see section 6.1 for details of the implementation of the new partitions .",
    "given the partition set of corresponding generated partition @xmath239 , we have the following formulation of two - step ensemble learning :    * given a dataset @xmath58 do @xmath43-fold @xmath239 .",
    "+ @xmath240 + @xmath241 @xmath190    we denote the level-2 ensemble learner based on weight vector @xmath242 , for predictions of a `` new '' data @xmath112 on hold - out dataset , given @xmath239 as    @xmath243    equation ( 7 ) and equation ( 8) , are equivalent to equation ( 5 ) and equation ( 6 ) respectively . in equation",
    "( 8) we denote with an index @xmath237 the dependency on a given @xmath239 partition of level-1 learners @xmath244 , with @xmath132 , and the level-2 learner @xmath245 as well .",
    "the reason of this is that , the @xmath246 weight vectors of level-1 learners present in equation ( 7 ) is estimated using an optimization similar to equation ( 3 ) , but in the context of a given @xmath247 partition , and the weight vector @xmath187 in equation ( 8) is estimated as well for a given @xmath239 by solving equation ( 7 ) . in section 6.1 , we explain in a little bit more detail the two - step ensemble learning .      in section 5.2.1 , we will introduce some concepts to help us define a level-1 ensemble correlation - based criterion , which will provide relevant information about the quality of a cross - validation partition . in section 5.2.2 ,",
    "we introduce a decision matrix which will facilitate the way we make the selection of an appropriate partition . in section 5.2.3",
    ", we present a correlation - based criterion , and a partition ranking - based criterion that can be used to rank partitions . in section 5.2.4 ,",
    "both criteria are employed by algorithm @xmath41 , which is a max - min rule - based systematic learning algorithm , to select best @xmath34 partition , that contains a minimum correlation rank .",
    "next , we will introduce some notation that we will use in the description of the selection criteria in the next two sections .    in the framework of two - step level-2 learning represented by equation ( 8) ,",
    "let us look at the level-2 data which are input for the optimization process in equation ( 7 ) , i.e. , the actual values @xmath59 along with predictions @xmath248 , for @xmath249 .",
    "the actual values @xmath59 along with the above predictions collected from each of the @xmath43 validation folds of @xmath247 are used to define the error matrix as    @xmath250    where each element in @xmath251 is defined as @xmath252 @xmath253 and @xmath254 with @xmath255    from the error matrix @xmath251 , the sample variance - covariance matrix between columns of size @xmath256 : @xmath257 is calculated as well as its sample correlation matrix between columns @xmath258 .    in our prediction problem matrix @xmath259 is symmetric , and the diagonal elements are equal to one , as it is the correlation between the predictions of level-1 learners @xmath260 and @xmath261 .",
    "we are interested in the @xmath262 elements above the diagonal of such matrix , and we put them in a row - wise fashion into the following row vector @xmath263 : @xmath264    in the last equation we are representing correlation vector @xmath265 by an alternative vector @xmath266 containing @xmath267 criteria , or elements , where @xmath268 .",
    "each criterion @xmath269 applied on an alternative @xmath266 and represented for element @xmath270 in vector @xmath266 with @xmath271 , measures the performance of alternative @xmath266 on criterion @xmath269 , and corresponds to an error correlation function between a pair of ensemble , or level-1 learner predictors , @xmath260 and @xmath261 with @xmath272 in vector @xmath263 .",
    "so , when we talk about partition @xmath239 we will be referring to alternative @xmath266 , and vice - versa .",
    "it should be emphasized that it is here where we exploit the structure of two - step level-2 ensemble algorithm as opposed to all - at - once level-2 learning , which lacks such structure by construction . at the end of the first step of two - step level-2 learning , or level-1 learning , and for a given @xmath43-fold @xmath247 partition",
    ", we can compute the error correlations @xmath273 , before the optimization in equation ( 7 ) , between the prediction errors of level-1 learner approximations @xmath274 and @xmath275 where @xmath276 , and @xmath46 with @xmath277 are validation folds .",
    "this correlation information provides relevant information about the quality of the @xmath43-fold @xmath247 partition as we can see in vector @xmath263 from equation ( 9 ) , and it is the basis for algorithm @xmath41 that we present in the next sections .      in this section",
    "we build a decision matrix which will facilitate the way we make the selection of an appropriate level-2 learner .",
    "the matrix will have as rows alternatives corresponding to the different generated @xmath34 partitions .    in the general case ,",
    "if we have @xmath278 different alternatives , @xmath266 with @xmath279 , which are associated to partitions @xmath239 with @xmath280 , and @xmath281 criteria @xmath282 , @xmath283 we can build the following decision matrix @xmath284 :    @xmath285    where @xmath270 is the performance of alternative @xmath266 on criteria @xmath269 , for @xmath271 .",
    "we can determine the minimum correlation set @xmath286 as the line vector of minimums column - wise .",
    "we will refer alternatively to the decision matrix @xmath284 as the set of alternatives , i.e. , we will also use the notation : @xmath287 .    from now on ,",
    "when using a set @xmath284 of alternatives we will be making reference to the systematic generation of alternatives , @xmath266 with @xmath288 , where @xmath113 , the number of new alternatives , is equal to the number of @xmath43 of folds by design of systematic learning .",
    ", i.e. , @xmath236 .",
    "it should be emphasized that each alternative @xmath266 in @xmath284 is related to a corresponding @xmath43-fold @xmath239 partition , as vector @xmath266 is equal to vector @xmath265 according to equation ( 9 ) .    on the other hand , in equation ( 8) , level-2 learning , we make reference to a level-2 generalizer , or learner @xmath245 for a @xmath239 , for some @xmath237 in @xmath289 .",
    "we use @xmath245 for prediction of a new instance @xmath73 . in section 5.2.3",
    ", we present in detail two rule - based criteria used for selection of a @xmath247 partition .",
    "these two rules are employed within algorithm @xmath41 in section 5.2.4 to choose one alternative @xmath266 for some @xmath237 in @xmath290 .",
    "once we have such alternative we can use its corresponding level-2 learner @xmath245 , to make predictions of new instances @xmath73 .",
    "one of such rules is a correlation based criterion between predictions of level-1 learners , while the other rule is based on a rank - based criterion of such correlations .",
    "in @xcite the authors use an uncorrelation maximization algorithm , which is based on a correlation based criterion , to remove ensemble learners based on a threshold @xmath291 with the objective of determining the best subset of ensemble learners .",
    "their algorithm is based on the principle of model diversity , where the correlation between the predictors should be as small as possible @xcite . in this section",
    "we will introduce a computed level-1 ensemble correlation criterion , and a rank based criterion , not to prune an ensemble , but to handle additional , and diverse @xmath34 partitions , rank them and select one .",
    "the selected partition has to contain a minimum correlation rank as we will explain in algorithm @xmath41 .",
    "one of its advantages is that they do not depend on extra parameter values like @xmath292 .",
    "the basis of the correlation criterion to select a particular @xmath247 partition , is to compute for a two - step level-2 ensemble , @xmath293 , or corresponding partition @xmath239 , the error correlation between its level-1 learner predictions @xmath294 at the end of level-1 learning step , for @xmath295 , represented by alternative @xmath266 in @xmath284 .",
    "the correlations , measured at this extra level of smoothing in the predictions , gives a way to control the diversity of its base ensembles , or level-1 learner components , and therefore be able to quantify the quality of a partition using a correlation based criterion , and a correlation rank based criterion .",
    "in next section we introduce algorithm @xmath41 , where we select the best alternative by grouping alternatives into different segments .",
    "for the first segment of alternatives we use a median based selection rule , @xmath296 rule , and in case we are not able to do a selection due to potential outliers in such alternative , then we go to next segment where we use @xmath297 rule for such selection instead .",
    "if we determine that there are potential outliers in such alternatives , then we go to next segment and apply @xmath297 rule again , until we eventually select one with no potential outliers , or if we ran out of alternatives then we keep the original alternative @xmath298 corresponding to original partition @xmath299 .",
    "we will describe some of the inputs required for algorithm @xmath41 .",
    "first , @xmath286 is the set of minimum correlations collected across all @xmath300-th criteria , @xmath269 in @xmath284 ; @xmath286 is then used for @xmath296 rule to make a selection , as we will describe in next section .",
    "algorithm @xmath41 also requires , a set @xmath301 of rank vectors of alternatives , and a set @xmath302 of scores of alternatives , which we define below , and are used by the @xmath297 rule in algorithm @xmath41 to make a selection .",
    "these inputs are generated before hand by the average ranking method , which we describe next briefly .",
    "the average ranking method @xmath303 @xcite selects one criterion , @xmath269 of @xmath284 , which contains a set of alternative s performances : @xmath304 , for @xmath305 .",
    "then , it computes a set of rankings on them to get @xmath306 , where @xmath307 , returns the rank of @xmath308 in a set of alternatives @xmath309 , @xmath310 , @xmath311 according to the @xmath269 criterion .",
    "rank value @xmath312 means the best criterion value , and rank value @xmath278 means the worst criterion value .",
    "it repeats this procedure for criterion @xmath269 , with @xmath313 .",
    "the ranking for an alternative @xmath308 is then given by the rank vector @xmath314    once the rank vector @xmath315 is computed for each alternative @xmath308 in @xmath284 , its score is calculated by adding the ranks of @xmath308 for each criterion .",
    "@xmath316    let @xmath301 be the set of rank vectors :    @xmath317    and @xmath302 the set of alternative scores @xmath318    next , we give the specification of algorithm @xmath41 in terms of its inputs , and outputs :    @xmath319 \\leftarrow s(\\textit{m+1},q,\\boldsymbol{g},g^{min},\\boldsymbol{t},\\boldsymbol{ar } ) \\ ] ]    so , algorithm @xmath41 returns best alternative .",
    "then , we can predict with corresponding best level-2 learner @xmath320 for a new instance @xmath73 : @xmath321 .      in a high level description , the main objective of algorithm",
    "@xmath41 is to consider alternatives in @xmath284 containing a minimum rank in @xmath301 , and select one of them by using one of two selection rules . in order to do",
    "that we proceed at different segments , we first find all rank vectors of alternatives in @xmath301 with @xmath322 , or @xmath323 , in at least one of its coordinate elements . using a selection rule at @xmath324 , @xmath296 selection rule ,",
    "an alternative is selected as long as such alternative is not disregarded when applying two rules used to detect potential outliers in it . otherwise , all alternatives with @xmath322 are eliminated from @xmath301 , and we then proceed to next segment , @xmath325 , by finding all alternatives with a minimum rank , @xmath326 , and put them in updated set @xmath301 .",
    "a second selection rule , @xmath327 is used at this segment to select an alternative with minimum score as long as such alternative is not disregarded if outliers are not detected in it .",
    "otherwise , the disregarded alternative is eliminated from set @xmath301 , and repeat the same steps at same @xmath328 ; applying @xmath327 rule on remaining alternatives in @xmath301 .",
    "if we run out of elements in @xmath301 , then we go to next segment by increasing @xmath328 by 1 , finding all alternatives with next minimum rank , and applying @xmath327 rule until either an alternative is selected with no potential outliers at @xmath328 , or either we reach the maximum number of segments @xmath113 , or there is not any alternative left in @xmath301 . in any of these two last cases is true then we keep original partition for final prediction .",
    "it is worth to mention that at @xmath323 we applied @xmath296 rule as opposed to had applied @xmath297 , because the former rule is more robust than the latter rule based on empirical evidence through the databases considered in this work .",
    "in addition to that , we believe that at @xmath324 we have an extreme case where the rank vectors have at least one of its element with minimum rank , @xmath322 across all rank vector available , and the @xmath296 rule , takes into account extra information , where it selects from @xmath284 a compromised alternative containing the median of the minimum correlation values , which corresponds to one of the rank vectors with @xmath322 . on the other hand , at the upper segments , @xmath329 , we do not have that extreme case any more , and there is not need to use the more specialized @xmath296 rule that takes into account correlation information explicitly .",
    "next , we describe in more detail algorithm @xmath41 , focusing more in the selection rules , and two rules designed to help us detect an alternative with potential outliers .",
    "first rule , checks if an alternative contains two rank values with a minimum value at @xmath328 , the second rule detects if such alternative has a maximum rank value equal to @xmath278 .",
    "+ in the first part , labelled @xmath330 @xmath331 , lines 1 - 14 , where @xmath323 , the objective is to apply @xmath296 rule on @xmath284 in order to get an alternative @xmath332 , or center alternative , containing the median value in set @xmath286 , with no potential outliers in it .",
    "we mark criterion @xmath333 , which contains such minimum median value , where @xmath334 . in line 7 and",
    "line 8 , we apply @xmath335 and @xmath336 rules on @xmath337 to help us find minimum and maximum rank values in it , respectively .",
    "those values will help us to determine whether such alternative has any potential outliers .",
    "on one hand , @xmath335 rule finds a criterion @xmath338 in @xmath339 with minimum rank : @xmath340 . on the other hand ,",
    "@xmath336 rule finds a criterion @xmath341 in @xmath337 with maximum rank : @xmath342 . in line 10 , we check if the center rank vector has two minimum rank values , i.e. , @xmath343 , or if its maximum rank value @xmath342 is equal to @xmath278 . if any or both of these conditions are true then we go to second part of the algorithm , as we have found an alternative with potential outliers",
    ". otherwise , if both conditions are false then we select @xmath332 as @xmath344 , and go to line 36 , and return alternative @xmath344 , whose corresponding two - step level-2 ensemble , @xmath320 , will be used to make predictions of new instances @xmath73 .",
    "+ in the second part , line 15 to line 35 , labelled @xmath345 @xmath346 @xmath331 , we first go to next segment by increasing @xmath328 by 1 , and collect all rank vectors from @xmath301 with @xmath326 into @xmath347 . the objective is to find a best alternative @xmath348 with minimum @xmath349 score , or corresponding best rank vector @xmath350 from @xmath347 , that has minimum rank , i.e. , @xmath351 , with no potential outliers in it .",
    "so , we apply again the same @xmath335 and @xmath336 rules , but now on @xmath350 . in line 26 , we check whether the best rank vector has two minimum rank values at @xmath328 , or if its maximum rank value is equal to @xmath278 . if any or both of these conditions are true then we go on in second part of the algorithm .",
    "otherwise , if both conditions are false then we select @xmath332 as @xmath344 , and go to line 36 , and return alternative @xmath344 , whose corresponding two - step level-2 ensemble , @xmath320 , will be used to make predictions of new instances @xmath73 .",
    "@xmath352 , } \\textit{q } \\text { } \\text{[number of criteria ] }      $ ]    @xmath353     + @xmath354    @xmath355    [ find a minimum correlation alternative based on median rule ]    [ find criterion @xmath338 with minimum rank , @xmath356 , in center rank vector excluding rank of @xmath357 from it ]    [ find @xmath341 with maximum rank ]    [ unmark alternative that has potential outliers ]    [ update @xmath284 and @xmath301 eliminating alternatives with @xmath358 +   + @xmath359 @xmath360    @xmath361 [ find best rank vector ] .",
    "[ find criterion @xmath338 with minimum rank , @xmath362 , in best rank vector excluding rank of @xmath363 ]    [ find @xmath341 with maximum rank ]    [ unmark alternative that has potential outliers ]    @xmath359    [ return best alternative , whose level-2 learner @xmath320 will be used for prediction ] +",
    "in our initial @xmath43-fold @xmath34 we partitioned the original historical dataset @xmath57 in the following disjoint sub - samples @xmath46 with @xmath62 .",
    "then , we built the learning or training subsets @xmath48 by where @xmath364 , and we denoted @xmath365 as the validation folds .    the above results in a set of partitions @xmath366 , where @xmath48 are the training subsets , and @xmath46 the validation folds , with @xmath277 .",
    "making an abuse of notation , such set will be represented as :    @xmath367    in order to generate different subsets of learning datasets we now take out two folds instead of only one fold .",
    "the total number of possibly different partitions sets that can be generated this way out of a @xmath43-fold @xmath34 partition is prohibitively large , with a total of @xmath235 new partitions in addition to the original @xmath234 partition .",
    "the method proposed next generates only @xmath236 new different @xmath34 partitions in an orderly way",
    ". it will provide us with a systematic mechanism to explore a reduced set of alternatives , and to test the algorithms proposed in this paper in a computationally efficient way .",
    "it should be noted that for the systematic procedure the number of partitions @xmath113 coincides with @xmath43 , the number of folds , i.e. , @xmath236 .",
    "for some @xmath237 in @xmath238 , and e.g. , @xmath368 , the partition set for a new systematic generated @xmath239 partition is    @xmath369    where @xmath370 are the training subsets , and @xmath46 the validation folds , for @xmath371 .",
    "let @xmath372 denote @xmath64-th level-1 learner estimated on training subset @xmath370",
    ".    given the partition @xmath239 , we have the formulation of two - step ensemble learning given by equation ( 7 ) .",
    "we reproduce below equation ( 8) for convenience , which represents the level-2 ensemble learner based on weight vector @xmath242 , for predictions of a `` new '' data @xmath112 on hold - out dataset , given @xmath239 as    @xmath243    in order to summarize the method of two - step level-2 learning from top to bottom , let us emphasized that the weight vector @xmath373 for @xmath245 or , level-2 learner , present in equation ( 8) is estimated based on training datasets of partition @xmath239 using @xmath191 - 2 data as input to equation ( 7 ) : predictions of level-1 learners @xmath374 along with actual values @xmath59 , where @xmath276 , and @xmath46 for @xmath277 are validation folds ..    the optimal weight vector @xmath196 , for @xmath132 , corresponding to a parameter for each level-1 learner approximation , @xmath274 with @xmath132 in equation ( 7 ) , was estimated previously by solving an optimization problem similar to equation ( 3 ) , but in the context of a @xmath43-fold @xmath239 partition , using level-1 data as input to such optimization : predictions of level-0 learners @xmath375 , along with actual values @xmath59 , where @xmath276 , and @xmath46 for @xmath277 are validation folds .. for @xmath376 the index @xmath377 takes values in set @xmath378 corresponding to base learners in @xmath154",
    ". on the other hand , for @xmath379 , index @xmath380 takes values across set @xmath381 corresponding to base learners in @xmath150 , and for @xmath382 , index @xmath383 takes values in set @xmath384 corresponding to base learners in @xmath151 .      in order to evaluate empirically max - min systematic two - step level-2 ensemble learning , @xmath385",
    ", we consider four simple base models already introduced in section 3.1 : constant regression , @xmath96 , linear regression , @xmath99 , quadratic regression , @xmath101 and radial basis networks , @xmath103 , and take the best model in terms of error performance for each database to build an oracle , which we called @xmath386 .",
    "we also consider three standard stacking approaches , or level-1 learners , @xmath154 , a linear combination of base learners in set @xmath387 , @xmath150 , a linear combination of base learners in set @xmath388 , and @xmath151 , a linear combination of base learners in @xmath389 , and compare @xmath386 versus the best performer , @xmath390 .",
    "we are also be comparing @xmath385 versus @xmath390 , and against @xmath386 .",
    "the outputs , or predictions of our base models @xmath96 , @xmath99 , @xmath101 , and @xmath103 are represented in the next tables , and figures , by the acronyms @xmath97 , @xmath391 , @xmath392 and @xmath117 respectively .",
    "these acronyms represent common simple models , which were already introduced in section 3.1 .",
    "so , @xmath154 is a combination of @xmath393 , @xmath150 is a combination of @xmath394 , and @xmath151 is a combination of @xmath395 .",
    "in addition to that , we also compared our results versus @xmath396 package @xcite implemented in @xmath397 , a statistical programming software tool @xcite .",
    "we also use two regression tree ensemble algorithms which are provided in @xmath398 @xcite @xcite , a data mining software tool .",
    "we use the @xmath399 @xcite interface to call these two regression tree algorithms from @xmath397 .",
    "one is known as @xmath400 and the other @xmath401@xmath402@xmath400 @xcite , @xcite .",
    "we use the @xmath398 default parameters for @xmath400 , and @xmath401@xmath402@xmath400 , and the @xmath403 default parameters for the generation of their corresponding predictions .",
    "the empirical study uses thirty databases which are popular in the literature , most of them from the uci repository of machine learning databases and domain theories @xcite . for each data",
    "set , we split it randomly in two parts , where the first part has @xmath404 percent of the observations used for training , and the remaining @xmath405 percent of the observations is used for testing .",
    "we conduct a @xmath43-fold @xmath34 partition with @xmath368 on the training part to fit the models in our approach , and measure the performance on the testing part . for the base models of the oracle @xmath386 , and the three state - of - the art ensemble algorithms against we are comparing our results ,",
    "we pass the whole training part to each method to fit its parameters , and then we measure their performance on the test part for comparison purposes .",
    "then , a non - parametric two - sided wilcoxon signed rank test @xcite is applied , as suggested in @xcite , to compare two approaches over all data sets with a significance level of 0.05 .",
    "the null hypothesis is that the average difference between both approaches is zero .",
    "the @xmath406-value is compared against the @xmath407 confidence level to determine the null hypothesis must be accepted or rejected .",
    "alternatively , we compute the @xmath408 confidence interval around a point estimate to decide whether the zero average difference falls within the interval to accept or reject the null hypothesis .",
    "we present in section 7.1 the results regarding standard stacking level-1 learning , and in section 7.2 the second extension to standard stacking approach : systematic level-2 learning . while in section 7.3",
    ", we compare systematic level-2 ensemble learning against three of the state of the art ensemble learning algorithms for regression .      in this section , we present different results , among them the comparison of each of the level-1 learners versus an oracle of databases composed of the best level-0 learner for each database .",
    "in addition to that , we select the best performer among the level-1 learners versus the database oracle . in table 1 , please see appendix a for all table results , we present the @xmath409 for the thirty databases considered in this work .",
    "if we look at table 1 , in the first column we have the database names .",
    "next , we have split such table in two parts , in the first half we have the @xmath410 for the level-0 learners : @xmath411 , @xmath412 , @xmath413 and @xmath117 . on the other hand , in the second half of the table",
    ", we have the @xmath410 for the level-1 ensemble @xmath154 , linear combination of base learners in @xmath393 , followed by the other two level-1 ensembles @xmath150 , linear combination of base learners in @xmath394 , and @xmath151 , linear combination of base learners in @xmath395 .",
    "the ensemble weights are determined using the optimal way as we described in section 3.2 .",
    "finally , in the last two columns are the number of records , and the total number of variables included in each database , independent variables plus dependent variable .",
    "it should be mentioned that we will build a database oracle based on the best level-0 model performer in each database , which we will compare later , in terms of error performance , against the best level-1 learner , as we will describe shortly .",
    "we will first focus our analysis on the level-0 learners , first half of the table .",
    "we can observe from table 1 that the prediction performance of the level-0 learner @xmath391 , as measured by the @xmath409 , is the best , indicated as underlined bold error , in ten out of thirty databases among the four models , @xmath97 , @xmath391 , @xmath392 and @xmath117 .",
    "the model @xmath413 is the best in thirteen out of the thirty databases . while the model @xmath117 is the best in seven of such databases . here ,",
    "we build @xmath386 the oracle which consists of the best level-0 model performer in terms of the @xmath409 for each database .",
    "the average error performances across all databases , in terms of @xmath410 , are @xmath414 , @xmath415 , @xmath416 , @xmath417 for @xmath97 , @xmath391 , @xmath392 and @xmath117 level-0 learners respectively . among the level-0 learners",
    "the @xmath391 model has the best error performance with a @xmath415 @xmath409 across all databases .    in the second part of table 1 ,",
    "the level-1 learner @xmath154 is the best , among the level-1 learners , in ten out of thirty databases , while @xmath150 is the best in seven of those databases , and @xmath151 is the best in eleven databases .",
    "in addition to that , there are two ties , where for @xmath418 and @xmath419 , the @xmath410 of @xmath154 are tied with the corresponding errors for @xmath151 and @xmath150 respectively . in relation to",
    "the level-1 learners their average error performances across all databases , are @xmath420 , @xmath421 , and @xmath422 for @xmath154 , @xmath150 and @xmath151 respectively .",
    "then , the best level-0 learner performer is @xmath154 with a @xmath409 of @xmath420 .",
    "next , we will compare the performance results between the database oracle and each of the level-1 learners to determine which level-1 learner performs better in average versus the oracle .    in fig . 1",
    ", we visualize the difference of the @xmath409 s between the oracle , @xmath386 , or best performer among the models @xmath97 , @xmath391 , @xmath392 and @xmath117 on the testing part , and the ensemble @xmath154 for each of the thirty databases considered .",
    "the differences are sorted by its absolute values . a positive difference , right part of the figure , means that accuracy is in favour of @xmath154 , while a negative difference means that the accuracy is against it .",
    "we will use this convention for all figures where we present the differences in performance between models or ensembles . by looking at the positive values in fig . 1",
    ", we see that the ensemble @xmath154 has better error performance than the oracle in the following databases : @xmath423 , @xmath424 , @xmath425 , @xmath426 , @xmath427 , @xmath428 , @xmath429 , @xmath430 , @xmath431 , @xmath432 , @xmath433 , @xmath434 and @xmath435 .",
    "it is worth to mention that the best performer in each database is not know in advance , as we are using the hold - out sample as testing dataset .",
    "the result of two - sided non - parametric wilcoxon signed - rank test to evaluate the performance of the oracle @xmath386 , and the ensemble @xmath154 gives a @xmath406-value of @xmath436 .",
    "we reject @xmath437 , the null hypothesis that there was not significant difference in accuracy , in terms of @xmath409 , between the database oracle @xmath386 , and the ensemble @xmath154 , i.e. @xmath437 stated that the average difference between their corresponding @xmath409s is zero .",
    "the @xmath408 confidence interval is @xmath438 , with a point estimate of the average difference is @xmath439 , which lead us to conclude a better performance of @xmath386 vs @xmath154 as the zero average difference is to the right of such confidence interval .",
    "1 * normalized mean square error difference + results of \\{@xmath386 } vs \\{@xmath154 }    on the other hand , for the ensembles @xmath150 and @xmath151 we do not present the result figures of each of them versus the oracle @xmath386 , or best performer among the models @xmath97 , @xmath391 , @xmath392 and @xmath117 on the testing part across the thirty databases considered .",
    "however , the result of two - sided non - parametric wilcoxon signed - rank tests to evaluate the performance of the oracle @xmath386 , and the ensemble @xmath150 gives a @xmath406-value of @xmath440 .",
    "the @xmath408 confidence interval is @xmath441 with a point estimate of the average difference of @xmath442 .",
    "we reject the null hypothesis ho , and therefore @xmath386 outperforms @xmath150 in prediction accuracy .",
    "the result of two - sided non - parametric wilcoxon signed - rank tests to evaluate the performance of the oracle @xmath386 , and the ensemble @xmath151 gives a @xmath406-value of @xmath443 .",
    "the @xmath408 confidence interval is @xmath444 , with a point estimate of the average difference of @xmath445 .",
    "we reject ho , and @xmath386 outperforms @xmath151 in prediction accuracy , as the average difference between both models being different than zero is significant . on the other hand , the level-1 learner which performed better , in terms of @xmath406-value , against the database oracle",
    "@xmath386 is @xmath154 , so we select it as the best level-1 learner , and it will be denoted as @xmath390 from now on .      in section 7.2.1 , we first present the results of applying the algorithm @xmath41 from section 5.2.4 to select the best max - min rule - based level-2 learner , or corresponding partition with the aim of improving the predictions from the original level-2 learner . in section 7.2.2 ,",
    "we compare the performance results of this best level-2 learner against the best level-1 learner , then in section 7.2.3 , we compare the performance of best level-2 learner against the oracle of best models .      in order to improve the two - step level-2 learning ,",
    "we have introduced the methodology of systematic two - step level-2 learning for the mixing of level-1 learners",
    ". we will illustrate algorithm @xmath41 from section 5.2.4 by using it to select the @xmath34 partition , and applying it on each of the thirty databases considered .",
    "we will present the corresponding @xmath409 s of systematic two - step level-2 learning @xmath446 , for @xmath447 , with @xmath368 .    in table 2",
    ", we show the corresponding results , where in the first column is as usual the database name , followed by the next six columns corresponding to the normalized mean square errors @xmath448 of the ensembles @xmath449 , @xmath450 , @xmath451 , @xmath452 , @xmath453 and @xmath454 .",
    "the next column @xmath331 refers to the selection rule that was employed to select the @xmath34 partition , among @xmath234 , @xmath455 , @xmath456 , @xmath457 , @xmath458 and @xmath459 , on which we are going to make the final prediction , partition @xmath460 . for such selection",
    "we used the two max - min rule - based based criteria described in algorithm @xmath41 .",
    "the selected @xmath460 is indicated as underlined error in table 2 for each of the databases considered .",
    "if we look again at table 2 , where we have the original two - step level-2 learner , @xmath461 , versus systematic two - step level-2 learners , @xmath462 , we see that we have improved , indicated as bold errors , in fifteen databases : @xmath463 , @xmath464 , @xmath465 , @xmath466 , @xmath467 , @xmath468 , @xmath430 , @xmath469 .",
    "@xmath470 , _ autompg _ , @xmath431 , @xmath424 , @xmath432 , @xmath434 and @xmath419 .",
    "the corresponding selected best max - min level-2 learners for such databases by the algorithm @xmath41 are @xmath471 , @xmath472 , @xmath471,@xmath473 , @xmath473 , @xmath472 , @xmath474 , @xmath472 , @xmath472 , @xmath473 , @xmath473 , @xmath472 , @xmath473 , @xmath475 and @xmath472 , respectively . on the other hand , for nine databases the algorithm @xmath41 selected @xmath174 , which is exactly the original level-2 learner , @xmath174 . in the remaining six databases we lost a bit of accuracy where , e.g. , in @xmath476 , the algorithm selected max - min level-2 learner @xmath477 with an error of @xmath478 , the other five databases are @xmath479 , @xmath480 , @xmath481 , @xmath482 and @xmath483 .    in table 3 , we present the @xmath409 s of @xmath484 corresponding to the best max - min level-2 learners selected by algorithm @xmath41 , which are represented in table 2 as underlined errors .",
    "now , we compare them against the corresponding @xmath409 s of @xmath485 , the best level-1 learner . in order to mark in table 3 the ensembles with best error performance between the best level-1 learning and systematic level-2 learning for each database , we highlight theirs corresponding @xmath409 s in bold .    as we can see from table 3 ,",
    "the @xmath409 s of @xmath486 are the best , indicated as bold errors under its corresponding column , against the corresponding @xmath409 of @xmath390 , in nineteen out of thirty databases .",
    "the best level-1 learner @xmath390 is the best in eight databases . while for three databases the error performance is the same between them .      in fig .",
    "2 , we are visualizing the @xmath409 s differences of max - min systematic level-2 learning , and level-1 learning for the thirty databases considered .",
    "we can clearly see that the best max - min level-2 learner @xmath320 is better than the best level-1 learner @xmath390 in the following nineteen databases : @xmath487 , @xmath483 , @xmath479 , @xmath430 , @xmath469 , @xmath432 , @xmath464 , @xmath434 , @xmath488 , @xmath470 , @xmath463 , @xmath481 , @xmath480 , @xmath482 , @xmath431 , @xmath424 , @xmath465 , @xmath435 , and _",
    "solarflares_. the result of @xmath408 two - sided non - parametric wilcoxon signed - rank test to evaluate the performance of the best level-1 ensemble , @xmath390 , and the ensemble @xmath320 gives a test statistics @xmath489 with @xmath406-value of @xmath490 .",
    "so , we do not reject ho .",
    "the @xmath408 confidence interval is @xmath491 , and the point estimate of the average difference is @xmath492 .",
    "however , the @xmath493 two - sided non - parametric wilcoxon signed - rank test is significant to reject @xmath437 with a @xmath406-value of @xmath490 .",
    "the @xmath493 confidence interval is ( 0.00083 , 0.020 ) with a point estimate of @xmath492 , where the zero average difference corresponding to the null hypothesis is outside and to the left of such interval .",
    "therefore , the ensemble @xmath320 outperforms @xmath390 at a @xmath493 significance level .",
    "2 * normalized mean square error difference + results of \\{@xmath390 } vs \\{@xmath486 }    based on the above results , table 3 and fig .",
    "2 , we see that max - min systematic level-2 learning with @xmath320 is a step in the right direction being in average more accurate and robust than the best level-1 learning @xmath390 .",
    "therefore , we are achieving our goal of improving the performance error of best level-1 ensemble , @xmath390 , through the two - step level-2 learning and systematization proposed in this work .      in fig .",
    "3 , we present the @xmath409 s difference between the best performer , or database oracle , among the models @xmath97 , @xmath391 , @xmath392 and @xmath117 on the testing part , and the best max - min level-2 learner @xmath320 for each of the thirty databases considered . by looking at the positive value part of such figure , we see that @xmath320 has better error performance than the database oracle , @xmath386 , in eleven out of thirty databases : @xmath424 , @xmath494 , @xmath432 , @xmath466 , @xmath434 , @xmath482 , @xmath429 , @xmath431 , @xmath464 , @xmath435 and @xmath427 . the result of @xmath408 two - sided non - parametric wilcoxon signed - rank test to evaluate the performance of the oracle @xmath495 , and the ensemble @xmath320 gives a @xmath406-value of @xmath496 .",
    "so , we do not reject @xmath437 , as there is not significant accuracy difference between the oracle @xmath386 , and the two - step systematic ensemble learning @xmath320 .",
    "the @xmath408 confidence interval is @xmath497 , and the point estimate of the average difference is @xmath498 .        *",
    "3 * normalized mean square error difference + results of \\{@xmath386 } vs \\{@xmath486 }      for the three state - of - the art ensemble algorithms , @xmath396 , @xmath400 , and @xmath499 , we pass to each of them the same training and test datasets we used in our methods , to fit its parameters , and to measure their performance for comparison purposes versus our results in this paper .    the above three methods are well regarded state - of - the - art ensemble learning algorithms for regression problems .",
    "the two - step level-2 systematic ensemble learning presented in this paper constitutes a new state - of - the - art ensemble learning algorithm , as we have tested it against each of the three ensemble learning algorithms just mentioned , and we have gotten excellent results .",
    "our ensemble learning algorithm performed better than @xmath396 and @xmath500 in terms of error performance , and it is as good as @xmath501 . in this section we present results that support our above claims .",
    "the @xmath396 package @xcite , @xmath502 , is available in the @xmath397 statistical software @xcite , @xmath503 .",
    "@xmath396 solves the penalized residual sum of square , that can perform lasso penalized regression , ridge regression , and the elastic net .",
    "we use the version 3.01 of @xmath397 software , and version 1.9 - 3 of @xmath396 package to get the results presented in this paper .",
    "the lasso penalty encourages sparsity , so this penalty performs a sort of variable selection as some of the coefficients are shrunken to zero , while the ridge penalty encourages highly correlated variables to have similar coefficients .",
    "the elastic net is a linear combination of the two penalties through an @xmath136 parameter , with @xmath504 .",
    "note that elastic net becomes the lasso when @xmath505 , and the ridge regression when @xmath506 .",
    "it has also a tuning parameter @xmath507 , which is a complexity parameter . in our experiments",
    ", we used the default parameter for @xmath136 , which is @xmath505 . for the @xmath508 parameter we estimated it using cross - validation with its default parameter , and we take the minimum of @xmath508 values as an estimate .    in fig .",
    "4 , we present the @xmath409 s difference on the testing part between the @xmath396 method , and the best max - min level-2 learner @xmath320 for each of the thirty databases considered .        * fig .",
    "4 * normalized mean square error difference + results of \\{@xmath403 } vs \\{@xmath486 }    the result of @xmath408 two - sided non - parametric wilcoxon signed - rank test to evaluate the performance of @xmath396 , and the ensemble @xmath509 gives a @xmath406-value of @xmath510 .",
    "so , we reject @xmath437 in favour of @xmath320 , as there is a significant accuracy difference between @xmath396 , and the two - step systematic ensemble learning @xmath320 .",
    "the @xmath408 confidence interval is @xmath511 , and the point estimate of the average difference is @xmath512 .",
    "we use a regression tree algorithm @xmath500 @xcite , which is based on an earlier @xmath513 regression tree @xcite .",
    "@xmath400 algorithm is available in @xmath398 data mining software @xcite @xcite , @xmath514 .",
    "although we install @xmath398 version 3.6.1 , we will only need to interact with it from @xmath397 .",
    "we use the @xmath399 @xcite interface version 0.4 - 13 to call this regression tree algorithm from @xmath397 .",
    "we use the @xmath399 default parameters for @xmath500 for the generation of their corresponding predictions .",
    "@xmath500 is a regression tree algorithm , which produces a regression tree such that each leaf node consists of a linear model for combining the numerical features .",
    "@xmath500 uses a tree pruning mechanism , which is a sort of variable selection , and @xmath500 can perform non - linear regression with the partitions provided by the internal nodes and is thus more powerful than linear regression .    in fig . 5 , we present the @xmath409 s difference on the testing part between @xmath400 method , and the best max - min level-2 learner @xmath509 for each of the thirty databases considered .        * fig . 5",
    "* normalized mean square error difference + results of \\{@xmath400 } vs \\{@xmath486 }    the result of @xmath408 two - sided non - parametric wilcoxon signed - rank test to evaluate the performance of @xmath500 , and the ensemble @xmath509 gives a @xmath406-value of @xmath515 .",
    "so , we reject @xmath437 in favour of @xmath509 , as there is significant accuracy difference between @xmath400 , and the two - step systematic ensemble learning @xmath509 .",
    "the @xmath408 confidence interval is @xmath516 , and the point estimate of the average difference is @xmath517 .",
    "in this part , we consider multiple @xmath500 trees combined by the popular bootstrap aggregation ( bagging ) method ( breiman 1996 ) , which is available in @xmath398 , and is called @xmath518 @xcite .",
    "we use again the @xmath399 @xcite interface to call this regression tree algorithm from @xmath397 .",
    "we use the @xmath399 default parameters for @xmath518 for the generation of their corresponding predictions .    in fig .",
    "6 , we present the @xmath409 s difference on the testing part between @xmath499 method , and the best max - min level-2 learner @xmath509 for each of the thirty databases considered .",
    "the result of @xmath408 two - sided non - parametric wilcoxon signed - rank test to evaluate the performance of @xmath400 , and the ensemble @xmath509 gives a @xmath406-value of @xmath519 .",
    "so , we do not reject @xmath437 , as there is not significant accuracy difference between @xmath518 , and the two - step systematic ensemble learning @xmath509 .",
    "the @xmath408 confidence interval is @xmath520 , and the point estimate of the average difference is @xmath521 .        *",
    "6 * normalized mean square error difference + results of \\{@xmath518 } vs \\{@xmath486 }      it should be emphasized that the iterative nature of two - step level-2 learning and its richness in structure in terms of correlation between each pair of ensembles was exploited in section 5.1 , through the generation of @xmath34 partitions , and in section 5.2 , to select a partition on which to make the final prediction .",
    "a difference to what is developed for @xmath403 @xcite , and the default parameter @xmath522 used in this work , where the diversity comes by using lasso penalized regression as variable selection mechanism , @xmath400 @xcite whose diversity comes by pruning a regression tree , and @xmath401@xmath402@xmath400 @xcite , which in addition to pruning a regression tree , creates more diversity by generating different training sub - samples through bagging ; the diversity in our approach , which uses combinations of standard stacking approaches , comes through the generation of different partitions on which such ensembles are trained .",
    "in addition to that , our approach controls the diversity of the different level-2 ensembles generated , or corresponding partitions , through a computed correlation criterion built on top of two - step ensemble learning , which provides an extra level of smoothing for the base ensembles in each partition .",
    "we have empirically tested three standard stacking approaches , or level-1 ensembles , each composed of a combination of simple based models , for heterogeneous stacking for regression .",
    "we have shown that the best of such ensembles performed not as good as the best base model from the ensemble by cross - validation .",
    "we have proposed two extensions to standard stacking approach .",
    "the first is to create an ensemble composed of different combinations of such standard approaches , through a two - step level-2 ensemble learning .",
    "we mentioned that the richness of structure in such two - step level-2 ensemble learning provided the basis to compute the base ensemble pairwise correlations , which will help us on improving the prediction accuracy in the second extension .",
    "the second extension was built to systematically generate different partitions from the current partition , and correspondingly two - step level-2 ensembles ; that along a partition correlation - based criterion , or a partition ranking - based criterion within an heuristic algorithm allowed us to select the best 2-level ensemble , and show that it performed better than the best of the standard stacking approaches , and is as good as the best of the base models from the standard stacking approaches by cross - validation .",
    "we also compared our results of best systematic level-2 ensemble learning versus three state of the art ensemble learning algorithms for regression , and found that our results performed better than two of those ensemble methods for regression , and it is as good as the third of such algorithms in terms of error performance .",
    "as a future research to do , it is worth to mention , that the systematic way to generate @xmath34 partitions introduced in section 5.1.1 will allow us to rank them , and reduce the size of the original @xmath34 partition by eliminating the least relevant fold , and apply iteratively max - min rule - based algorithm @xmath41 .",
    "we are currently investigating different stopping criterion that could prevent us from iterating the algorithm one more time , if after the current iteration it could detect that we may not get better prediction accuracy going into the next iteration after pruning the current partition .",
    "* table 1 * normalized mean square errors + level-0 learners : \\{@xmath97 } , \\{@xmath391},\\{@xmath392 } , \\{@xmath117 } and level-1 learners : \\{@xmath154 } , \\{@xmath150 } , \\{@xmath151 }    * table 2 * normalized mean square errors + systematic generation of training sub - samples + first iteration of algorithm + results for \\{@xmath461 } , \\{@xmath473 } , \\{@xmath475 } , \\{@xmath472 } , \\{@xmath474 } and \\{@xmath471 }          peter  j bentley and jonathan  p wakefield . finding acceptable solutions in the pareto - optimal range using multiobjective genetic algorithms . in _ soft computing in engineering design and manufacturing _ , pages 231240 .",
    "springer , 1998 .",
    "r.  caruana , a.  niculescu - mizil , g.  crew , and a.  ksikes .",
    "ensemble selection from libraries of models . in _ proceedings of the twenty - first international conference on machine learning _",
    ", page  18 .",
    "acm , 2004 .",
    "jerome friedman , trevor hastie , and robert tibshirani .",
    "regularization paths for generalized linear models via coordinate descent . _ journal of statistical software _",
    ", 330 ( 1):0 122 , 2010 . url http://www.jstatsoft.org",
    "/ v33/i01/.                            mp  perrone and ln  cooper . when networks disagree : ensemble methods for hybrid neural networks .",
    "r. j. mammone ( ed . )",
    "neural networks for speech and imageprocessing_. new york : chapman and hall , 1993 .",
    "lean yu , kin  keung lai , shouyang wang , and wei huang .",
    "a bias - variance - complexity trade - off framework for complex system modeling . in _ computational science and its applications - iccsa 2006 _ , pages 518527 .",
    "springer , 2006 ."
  ],
  "abstract_text": [
    "<S> the motivation of this work is to improve the performance of standard stacking approaches or ensembles , which are composed of simple , heterogeneous base models , through the integration of the generation and selection stages for regression problems . </S>",
    "<S> we propose two extensions to the standard stacking approach . in the first extension </S>",
    "<S> we combine a set of standard stacking approaches into an ensemble of ensembles using a two - step ensemble learning in the regression setting . </S>",
    "<S> the second extension consists of two parts . in the initial part </S>",
    "<S> a diversity mechanism is injected into the original training data set , systematically generating different training subsets or partitions , and corresponding ensembles of ensembles . in the final part after measuring the quality of the different partitions or ensembles , a max - min rule - based selection algorithm is used to select the most appropriate ensemble / partition on which to make the final prediction . </S>",
    "<S> we show , based on experiments over a broad range of data sets , that the second extension performs better than the best of the standard stacking approaches , and is as good as the oracle of databases , which has the best base model selected by cross - validation for each data set . </S>",
    "<S> in addition to that , the second extension performs better than two state - of - the - art ensemble methods for regression , and it is as good as a third state - of - the - art ensemble method .    </S>",
    "<S> * keywords : * ensemble learning , stacked generalization , systematic cross - validation , ensemble selection , regression , max - min based rules . </S>"
  ]
}