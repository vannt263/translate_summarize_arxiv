{
  "article_text": [
    "let @xmath0 denote the set of all proper lower semi - continuous convex functions from @xmath1 to @xmath2 , and @xmath3 denote the set of dc functions on @xmath1 .",
    "we are interested in the following nonconvex optimization problem : @xmath4 where @xmath5 is convex , @xmath6 is a nonempty closed convex subset in @xmath1 , and @xmath7 with @xmath8 and @xmath9 ( @xmath10 ) belongs to @xmath11 .",
    "we refer to @xmath12 as dc constraints .",
    "let us denote by @xmath13 the feasible set of and @xmath14 the set of interior points of @xmath15 .",
    "problems of the form have been studied by many researchers in theory and applications ( see , e.g. , @xcite and the references quoted therein ) . however , the methods for solving that exploit dc structures are usually global optimization techniques",
    ". these approaches are not applicable to problems with a high dimension . in this paper , we are interested in finding local minimizers only .",
    "the class of dc functions is sufficiently rich to deal with many practical problems .",
    "it is well - known @xcite that the set of dc functions defined on a compact convex set of @xmath1 is dense in the set of continuous functions on this set .",
    "therefore , in principle , every continuous function can be approximated by a dc function with any desired precision .",
    "moreover , every @xmath16-function defined on a compact set is a dc function @xcite that includes the smooth cases of .",
    "many practical problems can be reformulated in the form of ( see , e.g. , @xcite ) .",
    "although dc representations are available for important function classes , finding such a representation for an arbitrary dc function is still a hard problem .",
    "this paper investigates the relation between scp methods @xcite and dc programming @xcite .",
    "both families of methods address the local solution of nonconvex optimization problems via an iteration based on convex subproblems .",
    "dc programming algorithms ( dca ) for solving have been introduced by pham @xcite .",
    "the original dca is supposed to solve convex constrained dc programs . to handle dc constraints , penalty functions",
    "have been used @xcite and then dca is applied to the penalized problem for a fixed penalty parameter .",
    "yuillie and rangarajan in @xcite proposed a method for solving smooth dc programs that is called the concave - convex procedure ( cccp ) , a variant of dca applied to smooth dc programs @xcite .",
    "the authors in @xcite further investigated the global convergence of the cccp method .",
    "dca as well as cccp have been widely applied in many practical problems ( see , e.g. @xcite ) .",
    "it is well - known that the use of penalty functions in dc programming with dc constraints introduces conservatism and might lead to excessively short steps .",
    "one particular variant of dc programming that keeps the dc constraints in the problem was considered in @xcite .",
    "this again leads to possible conservatism or even to infeasibility of the subproblems ( which might be overcome by relaxation techniques ) .",
    "these methods have not become very popular due to these problems and their combination with exact penalties was never properly investigated .",
    "it is the aim of this paper to improve and investigate the numerical behaviour of these algorithms and show that they can be interpreted as a special case of scp methods .      in @xcite , a generic algorithm framework for solving nonlinear optimization problems with partially convex structure",
    "was proposed which is called _ sequential convex programming _ ( scp ) .",
    "the main idea of scp methods is to convexify the nonconvex part and preserve the remaining convexity in the resulting subproblems at each iteration . under mild assumptions",
    ", the local convergence of the scp methods was proved .",
    "the rate of local convergence is linear .    to the family of scp methods",
    "belong such classical algorithms as the constrained or unconstrained gauss - newton methods as well as sequential linear programming ( slp ) or sequential quadratic programming ( sqp ) with convex subproblems @xcite .",
    "all these methods are based on linearization of nonconvex constraints or objective functions , and are widely used in applications of nonlinear optimization , in particular , in parameter estimation ( constrained gauss - newton @xcite and nonlinear model predictive control @xcite ) .    when dc constraints are treated within an scp framework",
    ", it is possible to only linearize the concave parts .",
    "this can be interpreted as a special case of scp , which offers a favourable feature : namely that globalization strategies like line search or trust - region methods are not needed and full scp steps can always be taken . when feasibility of the subproblems becomes an issue , which is always the case for nonlinear equality constraints",
    ", we propose to relax the subproblems using an exact @xmath17-penalty function and investigate the behaviour of this relaxed scp algorithm .",
    "we show through an example that it can lead to less conservative convex subproblems than the standard approach of using unconstrained dc programming with penalty functions .      throughout this paper ,",
    "we use @xmath18 for the set of @xmath19-nonnegative vectors and @xmath20 ( resp . ,",
    "@xmath20 ) for the set of nonnegative ( resp . , positive ) numbers .",
    "a function @xmath21 is called @xmath22-convex on a convex subset @xmath23 of @xmath1 with @xmath24 if for all @xmath25 and @xmath26 $ ] the inequality @xmath27 holds .",
    "if @xmath28 then @xmath29 is convex . otherwise , @xmath29 is strongly convex with the parameter @xmath30 .",
    "let us assume that @xmath29 is a dc function such that @xmath31 , then it is trivial to see that @xmath32 for any given @xmath33 .",
    "therefore , without loss of generality , we can find a dc decomposition @xmath34 of @xmath29 such that @xmath35 and @xmath36 are strongly convex .",
    "we also use the notation @xmath37 for the domain of a convex function @xmath29 . for @xmath38 , the symbol @xmath39 denotes the exact subdifferential of @xmath29 at @xmath40 , i.e. , @xmath41 .",
    "a convex function @xmath29 is said to be subdifferentiable at @xmath38 if @xmath42 .",
    "a vector @xmath43 is called a subgradient of @xmath29 at @xmath40 .",
    "suppose that @xmath44 is an arbitrary dc decomposition of @xmath45 .",
    "let us define @xmath46 $ ] the lagrange function of problem .",
    "the generalized f. john condition of is expressed as follows @xcite : @xmath47 + n_{\\omega}(x),\\\\ 0 \\neq ( \\lambda_0 , \\lambda ) \\geq 0 , ~u(x)-v(x ) \\leq 0 , ~\\lambda^t[u(x ) - v(x ) ] = 0 , \\end{cases}\\ ] ] where @xmath39 , @xmath48 and @xmath49 ( @xmath10 ) are the subdifferentials of @xmath29 , @xmath50 and @xmath51 at @xmath40 , respectively .",
    "the multivalued mapping @xmath52 is the normal cone of @xmath6 at @xmath40 defined by : @xmath53 note that the first line of includes implicitly that @xmath54 .",
    "if @xmath55 satisfies then @xmath56 is called a stationary point and @xmath57 is the corresponding multiplier of .    since problem is nonconvex , a stationary point might not be a local minimizer .",
    "however , we will show later that under the calmness constraint qualification , the first order necessary condition for still holds .",
    "we consider the following parametric optimization problem : @xmath58 where the perturbation ( or parameter ) @xmath59 belongs to a neighborhood @xmath60 of the origin .",
    "it is trivial that @xmath61 .",
    "let @xmath56 solve .",
    "problem is said to be calm at @xmath56 ( in the sense of clarke s calmness constraint qualification @xcite ) if there exist a neighborhood @xmath62 of the origin , @xmath63 of @xmath56 and a positive number @xmath64 such that for all @xmath65 and @xmath66 that are feasible for [ eq : para_nlp ] , one has @xmath67 .",
    "the characterizations of calmness have been investigated in the literature ( see , e.g. , @xcite ) .",
    "the optimality conditions for dc programs with dc constraints have been studied in @xcite .",
    "if @xmath51 @xmath68 is continuously differentiable on @xmath1 then , under the calmness of at a local solution @xmath56 , without loss of generality , we can assume that the multiplier @xmath69 .",
    "thus the f. john condition collapses to the ( generalized ) kkt condition . with @xmath70 ,",
    "the point @xmath71 satisfying is called a kkt point .",
    "in particular , if @xmath29 , @xmath50 and @xmath51 @xmath68 are continuously differentiable on @xmath1 , and @xmath6 is the whole space , then the condition collapses to the classical kkt condition in smooth nonlinear optimization @xcite . under the mangasarian - fromowitz constraint qualification , the first order necessary condition corresponding to holds for .",
    "the following theorem shows that the first order necessary condition for problem still holds .",
    "[ th : fonc ] suppose that @xmath72 and @xmath44 is a dc decomposition of @xmath45 such that @xmath73 is continuously differentiable on @xmath1 .",
    "let @xmath56 be a local minimum of such that is calm at @xmath56 .",
    "then there exists a multiplier @xmath74 such that @xmath75 is a solution to the kkt system .",
    "note that if a function @xmath76 is continuously differentiable ( resp .",
    ", convex ) then the clarke subdifferential coincides with its gradient ( resp .",
    ", its convex subdifferential ) @xcite[proposition 2.2.7 ] .",
    "since @xmath77 is convex and continuously differentiable on @xmath1 , it implies that @xmath78 for all @xmath10 .",
    "on the other hand , since @xmath50 is subdifferentiable on @xmath1 , we have @xmath79 , where @xmath80 is the clarke subdifferential of @xmath9 ( @xmath81 ) @xcite .",
    "applying proposition 6.4.4 in @xcite we obtain the conclusion of the theorem .",
    "@xmath82    the rest of the paper is organized as follows .",
    "section  [ sec : examples ] presents two motivating examples .",
    "a variant of the scp algorithm for solving is presented in section  [ sec : scp_using_inner_approx ] .",
    "then its global convergence is investigated in section  [ sec : convergence ] .",
    "a relaxation technique is proposed in section  [ sec : scp_penalty ] to handle possibly inconsistent linearizations .",
    "computational tests are performed in the last section to demonstrate the behaviour of the class of algorithms .",
    "there are many practical problems that can be conveniently reformulated in the form of such as mathematical programs with complementarity constraints , bridge location problems , design centering problems , location problems , packing problems , optimization over efficient sets , trust - region subproblems in sqp algorithms , and nonconvex quadratically constrained quadratic programming problems ( see , e.g. , @xcite ) . for motivation",
    ", we present here two examples .",
    "the first example originates from optimal control of a bilinear system and the second one is a mathematical programming problem with complementarity constraints .",
    "the optimization problem resulting from nmpc of a bilinear dynamic system has the following form : @xmath83 + \\frac{1}{2}x_{h_p}^tw_ex_{h_p } \\\\",
    "\\textrm{s.t . } & x_{k+1 } = ax_k + b[x_k , u_k ] + cu_k , ~ k=0,\\dots , h_p-1 , \\tag{nmpc}\\\\ & x_0 = x_{\\textrm{init}},\\notag\\\\ & \\underline{x}_k \\leq x_k \\leq \\bar{x}_k , ~k=0,\\dots , h_p , \\notag\\\\   & \\underline{u}_k \\leq u_k \\leq \\bar{u}_k , ~k=0,\\dots , h_p-1 , \\notag\\\\ & x_{h_p}^tw_ex_{h_p } \\leq r_f , \\end{array}\\right.\\ ] ] where @xmath84 , @xmath85 , @xmath86 are the weighting matrices ; @xmath87 are given consistent matrices ; @xmath88 is a given initial state ; @xmath89 , @xmath90 , @xmath91 , @xmath92 are lower and upper bounds on the variables @xmath93 and @xmath94 , respectively ; @xmath95 is the radius of the terminal region ; and @xmath96 $ ] denotes a bilinear form of @xmath93 and @xmath94 .",
    "introducing a new variable @xmath97 with @xmath98 , the objective function of can be rewritten as @xmath99 , where @xmath100 is a symmetric positive semidefinite matrix with @xmath101 , @xmath102 and @xmath103 on the diagonal block .",
    "it is known that a given bilinear form is always associated with a quadratic form .",
    "therefore , the discrete time bilinear dynamic system @xmath104 + cu_k$ ]  ( @xmath105 ) can be reformulated as : @xmath106 where @xmath107 , @xmath108 is a given symmetric indefinite matrix , @xmath109 and @xmath110 ( @xmath10 ) .",
    "any symmetric indefinite matrix @xmath108 can be decomposed in such a form @xmath111 , where @xmath112 and @xmath113 are two symmetric positive semidefinite matrices ( e.g. , using spectral decomposition ) . using two different dc decompositions of @xmath108 and choosing @xmath114 , @xmath115 , @xmath116 , @xmath117 , @xmath118 , @xmath119 , @xmath120 , @xmath121 such that @xmath122 , @xmath123 , respectively ,",
    "the equality constraints can be rewritten as @xmath124 - [ w^tp_i^2w + ( q_i^2)^tw + r_i^2 ] \\leq 0,\\\\ [ ( w^t\\tilde p_i^2w + ( \\tilde q^2_i)^tw + \\tilde r^2_i ] - [ w^t\\tilde p_i^1w + ( \\tilde q_i^1)^tw + \\tilde r_i^1 ] \\leq 0 ,   \\end{cases}\\ ] ] for all @xmath10 .",
    "hence , problem is reformulated in the form of .",
    "note that @xmath125 ( @xmath126 ) is a possible choice in the formula .",
    "mathematical programs with equilibrium constraints ( mpec ) have been studied widely and have many applications in economic models , shape optimization , transportation , network design , and data mining . in this example , we particularly consider the following mathematical programming problem with complementary constraints : @xmath127 where @xmath128 is convex , @xmath129 is a nonempty closed convex set , @xmath130 , and @xmath131 , @xmath15 are two given matrices of consistent dimensions .    theory and methods for have been developed intensively in recent years ( see , e.g. , @xcite and the references quoted therein ) .",
    "the main difficulty of this problem is the complementarity constraints in the two last lines of .",
    "these constraints lead to nonconvexity and loss of constraint qualification of the problem .    introducing a slack variable @xmath132 ,",
    "the complementarity constraint can be reformulated as : @xmath133 since @xmath134 and @xmath135 , the constraint @xmath136 is equivalent to @xmath137 . using the expression @xmath138 , we can rewrite the condition @xmath137 as a dc constraint : @xmath139 where @xmath140 and @xmath141 that are convex .",
    "problem is now reformulated in the form of .    for an mpec problem , by using the kkt condition for the equilibrium constraint ( low level problem ) , we can transform this problem to the form ( mpcc ) ( see @xcite ) .",
    "then , by the same technique as before , we obtain a dc formulation for the equilibrium constraint .",
    "in this section , we present an algorithm for solving problem which we might call _ sequential convex programming with dc constraints_. let us assume that @xmath44 is a dc decomposition of @xmath45 , i.e. , @xmath142 for a given point @xmath143 , we take an arbitrary matrix @xmath144 , where the multivalued mapping @xmath145 with @xmath146 ( @xmath81 ) is the subdifferential of the convex function @xmath51 at @xmath147",
    ". we will refer to @xmath148 as a subgradient matrix of @xmath73 at @xmath147 .",
    "consider the following convex problem : @xmath149 since [ eq : convex_subprob ] is convex , under the slater constraint qualification @xmath150 where @xmath151 is the set of relative interior points of the convex set @xmath6 , any global solution @xmath152 of [ eq : convex_subprob ] is characterized as a kkt point of [ eq : convex_subprob ] . in the following algorithm , we assume that the convex subproblem [ eq : convex_subprob ] is solvable for given @xmath147 and @xmath148 .    a generic framework of the _ sequential convex programming algorithm with dc constraints _ ( scp - dc ) can be described as follows :    ' '' ''       -0.3cm[alg : a1 ]     -0.4 cm    ' '' ''    * initialization : * take an initial point @xmath153 in @xmath6 . set @xmath154 . + * iteration @xmath155 : * for a given @xmath147 , execute the three steps below :    * _ step 1 _ : compute a subgradient matrix @xmath144 . *",
    "_ step 2 _ : solve the convex subproblem [ eq : convex_subprob ] to get a solution @xmath152 and the corresponding multiplier @xmath156 . *",
    "_ step 3 _ : if @xmath157 with a given tolerance @xmath158 , then stop .",
    "otherwise , increase @xmath155 by @xmath159 and go back to step 1 .",
    "-0.3 cm    ' '' ''    at step 1 of algorithm [ alg : a1 ] , a subgradient matrix @xmath148 of @xmath73 at @xmath147 must be computed .",
    "if @xmath51 ( @xmath81 ) has a simple form , @xmath148 can be computed explicitly .",
    "otherwise , a convex problem needs to be solved . if @xmath73 is differentiable at @xmath147 then @xmath160 is identical to the jacobian matrix of @xmath73 at @xmath147 , i.e. @xmath161 .    the cost of finding an initial point @xmath162 depends on the structure of @xmath6 .",
    "it can be computed explicitly if @xmath6 is simple .",
    "otherwise , a convex problem should be solved .",
    "the projection methods ( onto @xmath6 ) can be also used in this case .",
    "[ re : quadratic case ] if the objective function @xmath29 of is linear ( resp . , quadratic ) then :    * if the function @xmath163 is linear then subproblem [ eq : convex_subprob ] is linear ( resp . ,",
    "quadratic ) . *",
    "if the function @xmath163 is quadratic then [ eq : convex_subprob ] is a quadratically constrained linear ( resp .",
    ", quadratic ) programming problem .",
    "this problem can be reformulated as a second order cone programming or semidefinite programming problem @xcite .",
    "dc decomposition of the function @xmath45 plays a crucial role in algorithm [ alg : a1 ] . a suitable dc decomposition",
    "may ensure that the convex subproblem [ eq : convex_subprob ] is solvable .",
    "moreover , it might make [ eq : convex_subprob ] easy to solve , and help algorithm [ alg : a1 ] to reach a kkt point of ( e.g. , @xmath163 and @xmath73 have small strongly convex parameters ) .",
    "the following small example shows the behaviour of algorithm [ alg : a1 ] using two different dc decompositions .",
    "@xmath164\\times [ -2 , 2 ] .   \\end{array}\\right.\\ ] ] the constraint @xmath165 is a dc constraint .",
    "hence , for a given tolerance @xmath166 and a starting point @xmath167 , if we choose @xmath168 with @xmath169 and @xmath170 for the dc decomposition of @xmath45 ( case 1 ) then algorithm [ alg : a1 ] converges to the global solution after @xmath171 iterations .",
    "if we choose @xmath172 and @xmath173 ( case 2 ) then it converges to the global solution after @xmath174 iterations .",
    "note that , in the first case , @xmath163 and @xmath73 are only convex , while @xmath163 is strongly convex with the parameter @xmath175 and @xmath73 is convex in the second case .",
    "the convergence behaviour is illustrated in figure [ fig : f1 ] . here",
    ", the left figure corresponds to case 1 and the right one corresponds to case 2 .     using different dc decompositions.,scaledwidth=98.0%,height=162 ]",
    "the following lemma shows that if algorithm [ alg : a1 ] terminates after some iterations then @xmath147 is a stationary point of .",
    "[ le : kkt_point ] suppose that @xmath147 is a solution of [ eq : convex_subprob ] ; then it is a stationary point of the original problem .",
    "suppose that @xmath147 is a solution of [ eq : convex_subprob ] corresponding to the multiplier @xmath176 then @xmath177 is a solution of its kkt system , i.e. , @xmath143 , @xmath178^t\\lambda^k + n_{\\omega}(x^k)$ ] , @xmath179 , @xmath180 and @xmath181 , which implies that @xmath143 , @xmath182^t\\lambda^k + n_{\\omega}(x^k)$ ] , @xmath183 , @xmath180 and @xmath184^t\\lambda^k = 0 $ ] .",
    "the last five relations mean that @xmath177 satisfies .",
    "thus @xmath147 is a stationary point of corresponding to the multiplier @xmath176 . @xmath82",
    "the next lemma gives us a key property to prove the global convergence of algorithm [ alg : a1 ] .",
    "[ le : descent_dir ] suppose that @xmath29 , @xmath50 and @xmath51 @xmath185 are @xmath22 , @xmath186 and @xmath187 - convex , respectively .",
    "then the sequence @xmath188 generated by algorithm [ alg : a1 ] satisfies @xmath189\\label{eq : descent_dir}\\\\[-1.5ex ] & & + \\frac{1}{2}\\sum_{i=1}^m\\rho^{v_i}\\lambda^{k+1}_i{\\|x^k - x^{k-1}\\|}^2.\\nonumber\\end{aligned}\\ ] ]    since @xmath152 is a solution of [ eq : convex_subprob ] corresponding to the multiplier @xmath156 , the kkt condition of [ eq : convex_subprob ] is expressed as follows : @xmath190^t\\lambda^{k+1 } + n_{\\omega}(x^{k+1}),\\\\ 0 \\geq u(x^{k+1 } ) - v(x^k ) - \\xi^k(x^{k+1}-x^k ) ,   ~ \\lambda^{k+1}\\geq 0 , \\\\ 0 = ( \\lambda^{k+1})^t[u(x^{k+1 } ) - v(x^k ) - \\xi^k(x^{k+1}-x^k ) ] .",
    "\\end{cases}\\ ] ] from the first line of , we have @xmath191(y - x^{k+1 } ) \\geq 0 , ~\\forall y\\in \\omega,\\ ] ] for all vectors @xmath192 and matrices @xmath193 .",
    "since @xmath29 and @xmath50 @xmath185 are strongly convex on @xmath6 , it holds that @xmath194 where @xmath195 . combining , and , and noting that @xmath196",
    ", we obtain @xmath197   \\nonumber\\\\ & & \\geq ( \\xi_f^{k+1})^t(y - x^{k+1 } ) + ( \\lambda^{k+1})^t(\\xi_u^{k+1 } - \\xi^k)(y - x^{k+1 } ) \\nonumber\\\\ [ -1.5ex]\\label{eq : term_24}\\\\[-1.5ex ] & & + \\frac{1}{2}[\\rho^f + \\sum_{i=1}^m\\rho^{u_i}\\lambda^{k+1}_i]{\\|y - x^{k+1}\\|}^2 \\nonumber\\\\ & & \\geq \\frac{1}{2}[\\rho^f + \\sum_{i=1}^m\\rho^{u_i}\\lambda^{k+1}_i]{\\|y - x^{k+1}\\|}^2 , ~\\forall y\\in\\omega.\\nonumber\\end{aligned}\\ ] ] substituting @xmath198 into and after a simple rearrangement , we get @xmath199 \\nonumber\\\\ & & -   f(x^{k+1 } ) - ( \\lambda^{k+1})^t[u(x^{k+1 } ) - v(x^k ) - \\xi^k(x^{k+1}-x^k ) ] \\label{eq : term_24a}\\\\ & & \\geq \\frac{1}{2}[\\rho^f + \\sum_{i=1}^m\\rho^{u_i}\\lambda^{k+1}_i]{\\|x^{k+1}-x^k\\|}^2.\\nonumber\\end{aligned}\\ ] ] using the third line of , the inequality is reduced to @xmath200 -   f(x^{k+1 } ) \\geq \\frac{1}{2}[\\rho^f + \\sum_{i=1}^m\\rho^{u_i}\\lambda^{k+1}_i]{\\|x^{k+1}-x^k\\|}^2.\\ ] ] now , since @xmath51 @xmath185 is @xmath187 - convex , we have @xmath201 where @xmath202 .",
    "this inequality implies that @xmath203 using the second line of for , we obtain @xmath204 applying with @xmath205 instead of @xmath152 to yields @xmath206{\\|x^{k+1}-x^k\\|}^2 + \\frac{1}{2}\\sum_{i=1}^m\\rho^{v_i}\\lambda^{k+1}_i{\\|x^k - x^{k-1}\\|}^2,\\ ] ] which proves .",
    "@xmath82    [ re : feasible_point ] from the proof of lemma [ le : descent_dir ] ( see ) we can see that algorithm [ alg : a1 ] always generates a feasible sequence @xmath207 to . if @xmath208 then it is strictly feasible .",
    "thus algorithm [ alg : a1 ] can be considered as an _",
    "inner approximation method_.    [ re : strong_convex ] if either @xmath29 is strongly convex or at least one function @xmath50 ( reps .",
    ", @xmath51 ) @xmath68 with respect to @xmath209 is strongly convex then the sequence of the objective values @xmath210 is decreasing .",
    "the convergence of algorithm [ alg : a1 ] is stated by the following result .",
    "[ th : convegence_theorem ] suppose that @xmath29 is bounded from below on @xmath15 , and the sequence @xmath211 generated by algorithm [ alg : a1 ] is bounded on @xmath212 . then :    * if @xmath213 then @xmath214 , and every accumulation point @xmath75 of @xmath211 is a kkt point of .",
    "* if there exists an index @xmath215 such that @xmath216 ( resp . ,",
    "@xmath217 ) then @xmath218 and every accumulation point of @xmath75 of @xmath211 such that @xmath219 is a kkt point of .",
    "* if the set of the kkt points of is finite then the whole sequence @xmath220 converges to a kkt point of .    from lemma",
    "[ le : descent_dir ] , it turns out that the sequence @xmath210 is nonincreasing and is bounded from below by assumption .",
    "then it converges to @xmath221 .",
    "summing up inequality from @xmath222 to @xmath223 and then passing to the limit as @xmath224 we obtain @xmath225 \\leq f(x^0 ) \\!-\\ !",
    "f^ { * } < + \\infty .",
    "\\end{aligned}\\ ] ] if @xmath30 then the inequality implies that @xmath214 . since @xmath226 is bounded by assumption , it has at least one limit point .",
    "suppose that @xmath75 is a limit point of @xmath211 , which means that there exists a subsequence @xmath227 of @xmath211 such that @xmath228 , where @xmath229 .",
    "since @xmath230 , @xmath231 and @xmath232 ( @xmath81 ) are upper semicontinuous , passing to the limit of the subsequence as @xmath233 in we conclude that @xmath75 is a kkt point of . the statement ( i )",
    "is proven .    for the statement ( ii ) , it is sufficient to prove the first case ( i.e. , there exists @xmath234 such that @xmath235 ) , the second case is done similarly .",
    "suppose that there exists at least one index @xmath215 such that @xmath216 .",
    "using again , it is easy to show that @xmath236 . as before ,",
    "if @xmath75 is a limit point of a subsequence @xmath227 such that @xmath237 then we have @xmath238 .",
    "passing to the limit through the subsequence as @xmath239 in we conclude again that @xmath75 is a kkt point of .    the last statement ( iii ) can be proved similarly using the same technique as in @xcite[chapt .",
    "@xmath82    suppose that @xmath56 is a stationary point of associated with a multiplier @xmath240 .",
    "if we denote by @xmath241 the strictly active set of at @xmath56 , then the assumption ( ii ) in theorem [ th : convegence_theorem ] requires that @xmath242 and at least one function @xmath50 ( or @xmath51 ) @xmath243 is strongly convex .",
    "[ re : regularization](_regularization _ ) . from lemma [ le : descent_dir ]",
    ", we see that if @xmath29 , @xmath50 and @xmath51 are only convex for all @xmath243 ( but not strongly convex ) then algorithm [ alg : a1 ] might not make @xmath29 strictly decreasing , i.e , @xmath244 for @xmath245 . in order to overcome this issue",
    ", a regularization term can be added to the objective function of [ eq : convex_subprob ] . instead of solving problem [ eq : convex_subprob ] , algorithm [ alg : a1 ]",
    "is modified at step 2 by solving the following regularized problem : @xmath246 where @xmath144 is arbitrary , and @xmath33 is a regularization parameter .",
    "this technique is closely related to the _ proximal point methods _ @xcite .    however , using the regularization term with a large @xmath247 may lead to short steps .",
    "consequently , algorithm [ alg : a1 ] converges slowly to a kkt point . in practice",
    ", we only add the regularization term if the solution of [ eq : convex_subprob ] does not make @xmath29 strictly decreasing at the current iteration .",
    "note that if @xmath33 then algorithm [ alg : a1 ] always makes @xmath29 strictly decreasing , i.e. , @xmath248 for @xmath249 .",
    "[ re : dc_objective](_handling the dc objective function _ ) .",
    "if the objective function @xmath29 of is also a dc function and @xmath250 is a dc decomposition of @xmath29 , then subproblem [ eq : convex_subprob ] at step 2 of algorithm [ alg : a1 ] is replaced by the following convex subproblem : @xmath251 with matrix @xmath144 and vector @xmath252 .",
    "the conclusions of theorem [ th : convegence_theorem ] are still valid for this modification . a smooth variant of this algorithm was considered in @xcite applied to dc programs arising in support vector machines , without convergence theory , however .",
    "according to dca approaches , to handle dc constraints , a penalty function is used to bring these constraints into the objective function @xcite .",
    "the obtained problem becomes an unconstrained or convex constrained dc program , and the unconstrained dca can be applied to solve this problem .",
    "we start this section by introducing one possible dc decomposition to handle the dc constraints using @xmath17-penalty functions , which is often used in practice @xcite .",
    "we will show through an example that by using an @xmath17-penalty function to handle the dc constraints , dca may make only slow progress to a stationary point of .",
    "let us define the @xmath17-penalty function of as follows : @xmath253_{+}\\|}_1,\\ ] ] where @xmath254 is a penalty parameter and @xmath255_{+ } = \\max\\ { g(x ) , 0 \\}$ ] .",
    "note that if @xmath45 has a dc decomposition @xmath44 then we have @xmath255_{+ } = \\max\\{u(x ) , v(x)\\ } - v(x)$ ] .",
    "since @xmath163 and @xmath73 are convex , @xmath256 is also convex .",
    "thus @xmath257 is a dc decomposition of @xmath258_{+}$ ] .",
    "since @xmath259 = f(x ) + \\mu\\sum_{i=1}^m[\\max\\{u_i(x ) , v_i(x)\\ } - \\mu v_i(x)]$ ] , if we define @xmath260 and @xmath261 then @xmath262 is a dc function , and @xmath263 is a dc decomposition of @xmath262 .    the @xmath17-penalized problem associated with can be rewritten as a convex constrained dc program : @xmath264 dca @xcite starts from an initial point @xmath162 and generates a sequence @xmath265 by solving the following convex subproblem : @xmath266 where @xmath267 and @xmath268 is fixed to a suitable large value .",
    "it is proved in @xcite that for this dc decomposition , there exists an exact penalty parameter @xmath269 such that for all @xmath270 , any solution of problem solves .",
    "now , we show that by using this standard technique , dca may lead to slow convergence to a stationary point .",
    "indeed , we consider an example by minimizing a convex function @xmath29 subject to a dc quadratic constraint @xmath271 , where matrix @xmath272 is symmetric positive semidefinite , @xmath273 is symmetric positive definite , @xmath274 , and @xmath275 .",
    "if we define @xmath276 and @xmath277 then @xmath73 is strongly convex with parameter @xmath278 , where @xmath279 is the smallest eigenvalue of @xmath273 .",
    "applying dca to problem we have @xmath280 that is strongly convex with parameter @xmath281 . if @xmath268 is large then @xmath282 is also large . in this case",
    ", dca makes only slow progress to a stationary point of .    instead of using the penalty function directly ,",
    "in the scp framework , we automatically obtain a different relaxed algorithm .",
    "we first relax the dc constraints by @xmath283 we use a relaxation technique to handle possibly inconsistent linearizations that may lead to infeasibility of the convex subproblem [ eq : convex_subprob ] in algorithm [ alg : a1 ] . note that @xmath284 is convex in @xmath285 as well as @xmath286 .",
    "each scp - dc subproblem is then given by : @xmath287 a relaxed variant of algorithm [ alg : a1 ] called _ relaxed scp algorithm with dc constraints _ ( rscp - dc ) is described as follows :    ' '' ''       -0.3cm[alg : a2 ]     -0.4 cm    ' '' ''    * initialization : * choose a penalty parameter @xmath288 .",
    "take an initial point @xmath153 in @xmath6 .",
    "set @xmath154 . +",
    "* iteration @xmath155 : * for a given @xmath147 , execute the three steps below :    * _ step 1 _ : compute a subgradient matrix @xmath144 . *",
    "_ step 2 _ : solve the convex subproblem [ eq : convex_subprob_mu ] with @xmath289 to get a solution @xmath290 and the corresponding multiplier @xmath156 . * _ step 3 _ : if @xmath157 and @xmath291 with a given tolerance @xmath158 , then stop .",
    "otherwise , update the parameter @xmath292 , increase @xmath155 by @xmath159 and go back to step 1 .",
    "-0.3 cm    ' '' ''    note that the subproblem [ eq : convex_subprob_mu ] is always feasible and the convergence theory of the previous section is applicable .",
    "however , the parameter @xmath268 influences the behaviour of algorithm [ alg : a2 ] . if the parameter @xmath268 is chosen too large , the minimization enforces @xmath293 to decrease , which reduces the infeasibility gap of the subproblems [ eq : convex_subprob_mu ] too fast .",
    "otherwise , the infeasibility gap @xmath293 may be increased .",
    "balancing between the optimality and the infeasibility plays an important role in algorithm [ alg : a2 ] .",
    "the parameter @xmath292 can be fixed to a `` suitable '' value or updated at each iteration of the algorithm . a refined variant , which is however not the topic of this paper , separately updates penalty parameters @xmath294 for each @xmath295 and make sure that they are sufficiently large , but not much larger than the corresponding constraint multipliers .",
    "the following inequality shows that algorithm [ alg : a2 ] makes a decreasing progress of the objective function @xmath296 .",
    "[ co : descent_lemma_for_penalty ] suppose that @xmath29 , @xmath50 and @xmath51 @xmath185 are @xmath22 , @xmath297 and @xmath298 - convex , respectively .",
    "then the sequence @xmath299 generated by algorithm [ alg : a2 ] satisfies @xmath300\\label{eq : descent_dir_for_penalty}\\\\[-1.5ex ] & & + \\frac{1}{2}\\sum_{i=1}^m\\rho^{v_i}\\lambda^{k+1}_i{\\|x^k - x^{k-1}\\|}^2,\\nonumber\\end{aligned}\\ ] ] where @xmath296 .",
    "the conclusions of theorem [ th : convegence_theorem ] still hold for this case , where the objective function is @xmath301 ( with a fixed value @xmath254 ) instead of @xmath29 .",
    "to verify the performance of algorithms [ alg : a1 ] and [ alg : a2 ] , we implement two numerical examples .",
    "the first example solves nonconvex quadratically constrained quadratic programs ( ncvqcqp ) .",
    "the second one is a mathematical program with complementarity constraints .      consider the following indefinite quadratically constrained quadratic programming problem : @xmath302 where @xmath303 , @xmath304 , @xmath305 , @xmath273 is a symmetric positive semidefinite matrix in @xmath306 , @xmath307 is an @xmath308 real matrix , and @xmath272 is an @xmath309 symmetric indefinite matrix . if @xmath272 is symmetric positive semidefinite then problem is a convex quadratically constrained quadratic programming problem ( qcqp ) @xcite .",
    "we first test algorithms 1 and 2 with some random data in @xmath310 $ ] and compare the performance with the built - in matlab solver fmincon for @xmath311 problems .",
    "the data is created as follows :    * generate a random matrix @xmath312 and compute @xmath313 , where @xmath314 is the identity matrix in @xmath306 . * vectors @xmath315 , @xmath316 , @xmath317 and matrix @xmath307 are random in @xmath318 $ ] , and @xmath319 . *",
    "generate a random matrix @xmath320 in @xmath310 $ ] and then compute @xmath321 . *",
    "the lower bound vector @xmath322 and the upper bound vector @xmath323 are given by @xmath324 and @xmath325 , respectively .",
    "since every symmetric matrix @xmath272 can be decomposed as @xmath326 , where @xmath327 and @xmath328 are symmetric positive semidefinite ( using spectral decompositions ) . the constraint @xmath329 is expressed as a dc constraint : @xmath330 where @xmath331 and @xmath332 with @xmath333 and @xmath334 , @xmath335 , @xmath336 , @xmath337 is the @xmath338 eigenvalue of matrix @xmath272 , and @xmath339 is a matrix whose columns are formed by the eigenvectors of @xmath272 .",
    "we implement algorithms [ alg : a1 ] and [ alg : a2 ] in matlab 7.8.0 ( r2009a ) running on a pc desktop with intel(r ) core(tm)2 quad cpu q6600 2.4ghz , 3 gb ram .",
    "we use the same dc decomposition of the dc constraint in both algorithms . to solve the convex quadratic subproblems",
    ", we use the cvx package ( with sedumi as a solver ) .",
    "the tolerance is given by @xmath340 and the penalty parameter @xmath292 is fixed to a certain value in algorithm [ alg : a2 ] ( see tables [ tb : example_01 ] and [ tb : example_01.2 ] ) .",
    "the computational results are reported in table [ tb : example_01 ] .",
    ".computational results of algorithms [ alg : a1 ] and [ alg : a2 ] for .",
    "[ cols=\"^,^,>,^,^,>,^,>,^,^,^ , > \" , ]     the solutions reported by algorithm [ alg : a2 ] for @xmath341 , @xmath342 and @xmath343 are @xmath344 respectively .",
    "algorithm [ alg : a1 ] failed in this case because the set of interior points @xmath14 of the feasible set @xmath15 is empty .",
    "the main aim of this paper is to investigate the relation between sequential convex programming ( scp ) @xcite and dc programming @xcite .",
    "we have provided a variant of the scp algorithm for finding local minimizers of a nonconvex programming problem with dc constraints .",
    "we have proved a global convergence theorem for this particular algorithm .",
    "then we have addressed some extensions and proposed a relaxation technique to handle possibly inconsistent linearizations .",
    "although finding a dc decomposition of a certain dc function is in general still a hard problem , in some applications ( as we have shown in the examples ) it is available or easy to compute .",
    "we have not concentrated on the local convergence . however , under mild assumptions , it had been proved in @xcite that the scp method converges linearly to a kkt point of the original problem .",
    "applications to nonconvex quadratic programming problems as well as mathematical programming problems with complementarity constraints have been presented through two numerical examples .",
    "* acknowledgments . *",
    "this research was supported by research council kul : coe ef/05/006 optimization in engineering(optec ) , goa ambiorics , iof - scores4chem , several phd / postdoc & fellow grants ; the flemish government via fwo : phd / postdoc grants , projects g.0452.04 , g.0499.04 , g.0211.05 , g.0226.06 , g.0321.06 , g.0302.07 , g.0320.08 ( convex mpc ) , g.0558.08 ( robust mhe ) , g.0557.08 , g.0588.09 , research communities ( iccos , anmmm , mldm ) and via iwt : phd grants , mcknow - e , eureka - flite+eu : ernsi ; fp7-hd - mpc ( collaborative project strep - grantnr .",
    "223854 ) , contract research : aminal , and helmholtz gemeinschaft : vicerp ; austria : accm , and the belgian federal science policy office : iuap p6/04 ( dysco , dynamical systems , control and optimization , 2007 - 2011 ) .",
    "bock , h. and schlder , j. : recent progress in the development of algorithm and software for large - scale parameter estimation problems in chemical reaction systems .",
    "_ in _ : automatic control in petrol , petrochemical and desalination industries .",
    "kotobh , p. ( ed . )",
    "( 1986 ) .",
    "diehl , m. , bock , h. , schlder , j. , findeisen , r. , nagy , z. and allgwer , f. : real - time optimization and nonlinear model predictive control of processes governed by differential - algebraic equations .",
    "contr . , * 12 * , 577585 ( 2002 ) .",
    "diehl , m. , ferreau , h. j. and haverbeke , n. : efficient numerical methods for nonlinear mpc and moving horizon estimation . _ in _ :",
    "nonlinear model predictive control .",
    "magni , l. , raimondo , m. and allgwer , f. ( eds . ) .",
    "springer , * 384 * , 391417 ( 2009 ) .",
    "garcs , r. , gomez , w. b. and jarre , f. : two theoretical results for sequential semidefinite programming .",
    "optimization online ( http://www.optimization-online.org/db_html/2007/11/1823.html ) , 116 ( 2008 ) .",
    "hiriart - urruty , j.b . : generalized differentiability , duality and optimization for problems dealing with difference of convex functions , chapter : convexity and duality in optimization , pages 3770 .",
    "springer - verlag ( 1986 ) .",
    "quoc , t.d . and diehl , m. : local convergence of sequential convex programming for nonconvex optimization .",
    "_ in _ : diehl , m. , glineur , f. , jarlebring , e. and michiels , m. ( eds . ) : recent advances in optimization and its applications in engineering .",
    "springer - verlag ( 2010 ) .",
    "smola , a. j. , vishwanathan , s.v.n . and hofmann , t. : kernel methods for missing variables . in proc .",
    "of the @xmath346 international workshop on articial intelligence and statistics , aistats05 , ghahramani , z. and cowell , r. ( eds . ) ( 2005 ) ."
  ],
  "abstract_text": [
    "<S> this paper investigates the relation between sequential convex programming ( scp ) as , e.g. , defined in @xcite and dc ( difference of two convex functions ) programming . </S>",
    "<S> we first present an scp algorithm for solving nonlinear optimization problems with dc constraints and prove its convergence . </S>",
    "<S> then we combine the proposed algorithm with a relaxation technique to handle inconsistent linearizations . </S>",
    "<S> numerical tests are performed to investigate the behaviour of the class of algorithms .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}