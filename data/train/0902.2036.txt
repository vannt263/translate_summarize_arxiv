{
  "article_text": [
    "in many real world problems , instead of the complete signal , we have some observations of the signal of interest from which we want to reconstruct the original signal . in the simplest case , which is fortunately applicable to many practical situations ,",
    "the observation process can be approximated as a linear operator : @xmath0 where @xmath1 is the original signal of interest , @xmath2 is the observed features , i.e. samples , and @xmath3 is the ( linear ) observation operator .",
    "we are interested in the case where the number of observations is much fewer than the length of the original signal .",
    "that is , @xmath4 has much fewer rows than columns , i.e. @xmath5 .",
    "furthermore , in practice the observation process is not exact and typically we can only obtain a distorted version of the observed features .",
    "this distortion is usually modeled by an additive error term .",
    "so the observation process can be modeled as @xmath6 in which @xmath7 represents the ( additive ) observation error , e.g. noise .",
    "the objective is to find the original signal , @xmath1 , from the set of available observations , @xmath8 , while @xmath3 is also known . that is , to solve the linear inverse problem . in order to do",
    "so one might minimize the discrepancy @xmath9 . however , except for the very special case that the operator @xmath3 has a trivial null space ( see @xcite for details ) , the minimizer is not unique . in order to address this problem , i.e. to regularize the inverse problem",
    ", one might suppose a priori assumptions and impose different constraints on the solution , which is usually taken into account by adding a penalization term to the discrepancy .",
    "in this letter we are especially interested in the case where we have a sparsity constraint on the solution . for the sake of a more precise explanation , suppose there exists an orthonormal basis @xmath10 in which the signal of interest can be expanded with the expansion coefficients @xmath11 , a large number of which are zero or negligibly small . in order to make use of this constraint for solving the inverse problem ,",
    "define the @xmath12-norm @xmath13 .",
    "one might then find the minimizer of the following functional , as the solution of with the aforementioned sparsity constraint : @xmath14 where @xmath15 and @xmath16 is the regularization parameter that can be chosen based on application .",
    "this is a well - known problem and different approaches to solving it have been proposed . here",
    "we will not go through the details of the problem , which have been widely studied by other authors .",
    "the reader is referred to @xcite for a comprehensive discussion of the problem . for the sake of consistency",
    ", we follow the same mathematical notations as those used in @xcite .",
    "furthermore , it is worth mentioning that beside the @xmath17-norm introduced above , some authors have as well used the total variation ( tv ) norm as the constraint .",
    "see , for example , @xcite .",
    "there are several methods of recovery available in the literature , among of which iterative thresholding algorithms are an important class .",
    "iterative thresholding algorithms are , more or less , based on a thresholded version of the landweber iterations ( see , for example , @xcite ) .",
    "i.e. the sequence of iterates has the general form @xmath18 where @xmath19 denotes the conjugate of @xmath3 , and @xmath20 is a thresholding operator . in @xcite",
    "the authors prove the convergence of the above iterative algorithm to the ( unique ) minimizer of , when @xmath20 is the soft thresholding operator ( see the definition of soft thresholding in section [ sec : description of algorithm ] ) .",
    "noticeable effort has been put into accelerating the original algorithm . in @xcite",
    "the authors propose a method for accelerating thresholded landweber iterations , which is based on alternating subspace corrections .",
    "other methods for this purpose are introduced in @xcite , and @xcite .",
    "although use of soft thresholding is more common , some authors have , as well , used _",
    "hard _ thresholding to address the above inverse problem .",
    "see @xcite , @xcite or @xcite as examples .",
    "in order to explain the underlying idea of the proposed method , let us begin with the following problem",
    ". in @xcite papoulis introduces _ an iteration method _ for reconstruction of a band - limited signal from a known segment .",
    "suppose @xmath21 is a signal of which we only know a small segment , @xmath22 , where @xmath23 .",
    "also suppose @xmath24 is the fourier transform of @xmath21 and @xmath25 for @xmath26 ( bandlimitedness ) .",
    "the objective is to reconstruct @xmath21 from @xmath27 .    in order to solve this problem ,",
    "we begin with @xmath28 , the fourier transform of @xmath27 , and form @xmath29 , i.e. truncate @xmath28 for @xmath26 . in other words",
    ", we change @xmath8 so that it satisfies the constraint on the original signal ( bandlimitedness in this case ) .",
    "@xmath30 , the inverse transform of @xmath31 , is then used to form @xmath32 , which recovers the known segment of @xmath1 .",
    "@xmath33 is supposed to be a better estimate of the desired signal , @xmath21 , than @xmath27 .",
    "this estimate can be further improved by repeating the above procedure in an iterative manner .",
    "that is , in the @xmath34th iteration , we form the function : @xmath35 compute its inverse transform , @xmath36 , and recover the known segment of the original signal @xmath37 it can be proved that @xmath38 tends to @xmath24 as @xmath39 @xcite .    in brief , in each iteration , we change the latest estimate of the desired signal , i.e. the output of the previous iteration , so that it satisfies the constraint ( bandlimitedness in this case ) . since this process might affect the entire signal , including the known segment , the known segment is then recovered before further progress .",
    "this problem is obviously different from our original problem stated in , because , firstly , it concentrates on the special case of recovering a _ continuous _ signal from _ a known segment _ and , secondly , the constraint on the signal is _",
    "bandlimitedness _ while the constraint of is _",
    "sparsity_. nevertheless , we will implement the above idea to solve our own problem as explained below .",
    "based on the above algorithm , our _ iterative _ algorithm involves two main operations in each iteration , namely , an operation to maintain the constraint followed by an operation to recover the original observations . since we are interested in problems with sparsity constraint , a thresholding operation can maintain this constraint for us , i.e. @xmath40 where @xmath41 is the latest estimate of the original signal , obtained in the previous iteration , and @xmath20 is the _ soft _ thresholding operator , defined as @xmath42 where @xmath43    analogous to , the original observations are then recovered by @xmath44    the sequence of iterates can , thus , be expressed in the following form : @xmath45 with @xmath46 .",
    "although is not exactly a sequence of landweber iterations , it can still be viewed as a modified version of the thresholded landweber iterations .",
    "note , especially , the analogy between and .    in this letter",
    "we only introduce the algorithm and experimentally evaluate its performance , compared to similar state - of - the - art algorithms . a detailed discussion of the convergence of the iterative algorithm and its relation to the thresholded landweber iterations is beyond the scope of the current letter and will be postponed to future publications .",
    "the motivation behind the proposed algorithm was briefly discussed , though .",
    "in all the experiments described below , the thresholding operator is applied to stationary wavelet transform ( swt ) @xcite coefficients , obtained using db1 ( haar ) mother function for 1 level of decomposition .",
    "all thresholds are obtained using the well - known birge - massart strategy @xcite .",
    "the iterative algorithm continues until a convergence criterion , e.g. @xmath47 , is met . for the sake of comparison , the results are compared with those obtained by @xmath48 norm minimization @xcite and total - variation ( tv ) norm minimization @xcite , which are two well - known state - of - the - art methods of sparse signal recovery . due to space constraints , the results of the experiments are included very concisely .",
    "more comprehensive results can be found at http://mkayvan.googlepages.com/sparsesignalrecovery .",
    "first , we consider the ideal case of sampling with no distortion , i.e. we assume @xmath49 in .",
    "the heavisine test signal ( figure [ fig : heavisine phantom ] ) , from the well - known donoho - johnstone @xcite collection of synthetic test signals , is reconstructed from different numbers , @xmath50 , of randomly selected samples .",
    "table [ tbl : mse nonoise 1d heav ] shows the the mean squared error ( mse ) between the reconstructed and the original signal for reconstruction by as well as by @xmath51 and tv norm minimization . as it is obvious from results , reconstruction by",
    "outperforms the two other methods in almost all cases .",
    ".mse between the reconstructed and the original signal for reconstruction by as well as by @xmath51 and tv norm minimization , from @xmath50 observed samples .",
    "the original heavisine test signal constitutes @xmath52 samples . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "motivated by the papoulis - gerchberg algorithm , a method for recovery of sparse signals from very limited numbers of observations was proposed .",
    "iterative thresholding algorithms have been widely used to address this problem .",
    "our algorithm also takes advantage of thresholding to maintain the sparsity constraint in each iteration .",
    "the signal is then reconstructed by iteratively going through a constraint - maintaining operation followed by recovery of the known features .",
    "the performance of the method was experimentally evaluated and compared to other state - of - the - art methods .",
    "i. daubechies , m. defrise , c. d. mol , `` an iterative thresholding algorithm for linear inverse problems with a sparsity constraint , '' communications on pure and applied mathematics , vol.57 , pp.1413 - 1457,2007 .",
    "e. j. candes , m. b. wakin , `` an introduction to compressive sampling [ a sensing / sampling paradigm that goes against the common knowledge in data acquisition ] , '' ieee signal processing magazine , vol.25 , pp.21 - 30 , 2008 .          j. m. bioucas - dias , m. a. t. figueiredo , `` a new twist : two - step iterative shrinkage / thresholding algorithms for image restoration , '' ieee trans . on image processing , vol.16 , no.12 , pp.2992 - 3004 , 2007 .",
    "l. birge and p. massart , `` from model selection to adaptive estimation , '' research papers in probability and statistics : festschrift for lucien le cam , ( d. pollard , e. torgersen and g. yang , eds . ) , springer , new york , 1996 ."
  ],
  "abstract_text": [
    "<S> motivated by the well - known papoulis - gerchberg algorithm , an iterative thresholding algorithm for recovery of sparse signals from few observations is proposed . </S>",
    "<S> the sequence of iterates turns out to be similar to that of the thresholded landweber iterations , although not the same . </S>",
    "<S> the performance of the proposed algorithm is experimentally evaluated and compared to other state - of - the - art methods . </S>"
  ]
}