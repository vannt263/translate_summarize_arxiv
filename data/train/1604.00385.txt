{
  "article_text": [
    "the process of extracting or reconstructing neuronal shapes from an em dataset is laborious @xcite . as a result , the largest connectomes produced involve only hundreds to thousands of neurons @xcite .",
    "nanometer resolution imaging is necessary to resolve important features of neurons , but as a consequence , a neuron that spans just 50 microns might intersect thousands of image planes .",
    "also , neurons often exhibit complicated branching patterns .",
    "figure [ fig : neuronex]a shows a small part of an em dataset and an intersecting neuron .",
    "* partial neurons extracted from em data .",
    "* a ) neurons can have intricate branching and span thousands of images .",
    "b ) single false merge or false split errors can greatly impact the neuron shape and its corresponding connectivity.,scaledwidth=80.0% ]    image segmentation attempts to reconstruct these shapes automatically , by first identifying voxels belonging to neuronal boundaries and then partitioning the data using these boundaries , such that each partition yields a different neuron . despite continual advances in machine learning ,",
    "the state - of - the - art segmentation approaches @xcite still require manual `` proofreading '' @xcite .",
    "proofreading involves merging falsely split segments and splitting falsely merged ones .",
    "algorithms are generally biased in favor of false split over false merge errors .",
    "this is because manual correction of a falsely split segment is trivial , but manually splitting a falsely merged segment is much more labor intensive , requiring definition of the split boundary .",
    "recent efforts aim to segment and evaluate large - scale image segmentation @xcite .",
    "the general approach involves segmenting small subvolumes and concatenating these subvolumes to form a segmentation for the entire image volume .",
    "such approaches still do not take the global context of the segmentation problem into account .",
    "thus , even though state - of - the - art strategies may perform well when compared with small , manually reconstructed ground truth volumes , the quality of their results still suffers on large datasets .",
    "for example , if the segmentation strategy employs a classifier which fails to generalize to all parts of the dataset , false merges may be concentrated in one region of the global dataset ( figure [ fig : badseg ] ) , propagating errors throughout the volume .",
    "this scenario is especially likely in regions where the original data contains image artifacts .    even in regions with good classifier performance , sparse errors may not corrupt the local segmentation quality , but the effect of these sparse errors within the larger volume can be catastrophic .",
    "since neurons typically span vast lengths within a large volume , even relatively sparse errors within a local context can have significant nonlocal consequences .",
    "these can severely affect the accuracy of the extracted connectome , as shown in figure [ fig : neuronex]b .     * em examples that are hard to segment . *",
    "a ) this example contains parts of the neuron not included in the training set .",
    "poor classifier generalizability contributes to a bad segmentation result .",
    "b ) image artifacts such as membrane holes can result in false merging.,scaledwidth=80.0% ]    in addition to these challenges , performing image segmentation on teravoxel-(or higher ) scale datasets involves many practical considerations :    1 .   processing a large dataset requires significant compute resources .",
    "unavoidable crashes ( e.g. , those due to network outages ) or bugs that kill a long running cluster job require costly re - runs .",
    "2 .   segmentation algorithms are continually advancing .",
    "it is important to be able to quickly swap and evaluate algorithm components and to partially refine an existing segmentation .",
    "large software frameworks tend to be difficult to understand and deploy by non - experts in diverse compute environments .    in this paper",
    ", we introduce an open - source segmentation framework that runs efficiently on large image datasets .",
    "our framework is implemented in apache spark , a fast distributed engine for processing data in parallel @xcite .",
    "spark is widely adopted on multiple platforms making it easy to deploy in many different compute environments .",
    "we also layer this framework over a volume data - service for accessing and versioning volume data , dvid @xcite , which allows us to abstract the storage / data layer from the core algorithms .",
    "other key features of our framework include :    1 .   a flexible plugin system to enable easy swapping of segmentation components 2 .",
    "check - pointing mechanisms that save partial computations to enable fast rollback and recovery 3 .   in - memory representations of segmentation to reduce disk i",
    "a novel segmentation stitching strategy to prevent propagation of local segmentation errors    we evaluated the pipeline on several em datasets .",
    "we demonstrate the scalability and the efficiency of rollback / recovery on a 400 gb volume .",
    "furthermore , we show the effectiveness of our stitching approach by comparing automatic image segmentation to large ground - truth datasets .    the paper is organized as follows . in section [ sec : background ]",
    ", a standard framework for segmentation is described .",
    "we then introduce our large - scale framework in section [ sec : framework ] and highlight the key features of the code design in section [ sec : dvidsparkservices ] . finally , we present results and conclusions . for reference ,",
    "the appendix outlines the segmentation plugins available and provides a sample configuration file .",
    "a general strategy for segmenting an em volume is shown in figure [ fig : basicseg ] .",
    "a classifier is trained to distinguish neuronal membrane boundaries from the rest of the image . applying the classifier to",
    "the image volume produces a boundary probability map .",
    "typically , some supervoxel generation algorithm such as watershed @xcite is applied to the data , yielding an oversegmentation of regions within likely boundaries .",
    "depending on the voxel resolution ( nearly isotropic or anisotropic ) , the previous steps could be applied to either 2d slices or 3d subvolumes . because over - segmentation errors are easier to correct than under - segmentation errors , the boundary prediction is often conservative , producing a segmentation with many small fragments .",
    "clustering or agglomerating this over - segmentation produces the final result @xcite . in most cases , a neuron is still split into multiple segments .",
    "* segmentation workflow for em dataset . * the workflow typically involves voxel prediction , generation of an over - segmentation ( using algorithms like watershed ) , and agglomerating these segments.,scaledwidth=100.0% ]     * framework for segmenting large datasets . *",
    "the dataset is split into several overlapping regions .",
    "each region is segmented and the overlapping bodies are stitched together.,scaledwidth=100.0% ]    for large datasets , the computation must be partitioned across multiple machines .",
    "computing the boundary prediction typically requires only local context and is easily parallelized .",
    "while watershed and agglomeration would ideally be computed as global operations across the entire volume , it is reasonable to spatially partition the dataset and apply these algorithms on overlapping blocks .",
    "the shared regions between blocks are then used to merge the blockwise segmentation results together at the end of the pipeline .",
    "this pipeline is shown in figure [ fig : largeseg ] and is roughly the strategy used in previous work @xcite . in principle , agglomeration could be done with more global scope by successively partitioning the global graph that describes the inter - connections between connected components produced by watershed .",
    "we introduce our framework at the algorithm and system level in this section , modeled off of the design in figure [ fig : largeseg ] . in section [ sec : dvidsparkservices ] , we will discuss lower - level details .",
    "after first generating segmentation on small , overlapping subvolumes , their segments are stitched based on voxel overlap along a subvolume face .",
    "we will first assume that the subvolume segmentation is strictly over - segmented ( no false merges ) .",
    "even with this simplifying assumption , there will not be 100% overlap between segments from the same neuron . in one scenario , slight algorithm instability and differing contexts",
    "can cause small boundary shifts in the overlap region between subvolumes of a few pixels .",
    "for example , figure [ fig : stitch]a shows two neurons running in parallel whose exact boundary varies between subvolumes . even with slight overlap @xmath0 should only merge with @xmath1 and not @xmath2 . in another scenario",
    ", a neuron can branch near the subvolume border .",
    "figure [ fig : stitch]b shows that segment @xmath3 and @xmath0 should be merged together and joined with segment @xmath2 .",
    "stitching based on any overlap will cause the neuronal segments to be merged correctly , unlike in the first scenario .     *",
    "scenarios for stitching neurons based on overlap between subvolumes . *",
    "four scenarios are given showing two subvolumes where the overlap is shown in gray .",
    "a ) slightly different context and algorithm instability can cause a small shift in the segmentation leading to a non - exact match .",
    "b ) neuronal branching can link two segments in another subvolume together .",
    "c ) false merging in subvolume @xmath4 causes false merging in subvolume @xmath5 due to segment overlap .",
    "d ) conservative matching can avoid propagating false mergers.,scaledwidth=100.0% ]    a simple stitching rule properly handles both scenarios .",
    "a segment @xmath6 in subvolume @xmath5 matches a segment @xmath7 in subvolume @xmath4 ( meaning they are in the match set @xmath8 ) , if and only if , @xmath6 overlaps with @xmath7 the most or vice versa :    @xmath9    applying this rule to all @xmath6 and @xmath7 means that each segment in @xmath5 ( and @xmath4 ) will be linked to at least one other segment in @xmath4 ( and @xmath5 ) .",
    "similarly , we could define the matching condition as needing a minimum overlap threshold :    @xmath10    this would not guarantee one match for each segment .    for an over - segmented volume ,",
    "these definitions handle most scenarios . there are some remaining corner cases that could be handled with a few additional considerations .",
    "for instance , segments that branch outside of the overlap region can be temporarily split into connected components and each component can separately seek a match .",
    "segments with a large overlap in absolute voxel count could be matched even if the above conditions fail to hold .",
    "complications arise in practice because of false merging .",
    "when two neuronal segments in a substack are falsely joined , these errors can propagate to the substack boundary .",
    "given the complexity of using higher - level biological context to guide segmentation and stitching , it not easy to tell whether branching that occurs at a substack face is the result of error or biology .",
    "figure [ fig : stitch]c shows an example where two neurons are falsely merged due to an image artifact , causing the simple stitching rule to result in more errors . a single substack with bad image quality or poor segmentation for any other reason could potentially propagate errors throughout the entire volume . the effects are more dramatic for larger volumes since a single neuron spans multiple subvolumes , and there are more opportunities for false merging .",
    "future work will exploit shape priors to eliminate error propagation and hopefully identify the sources of the false merges . for now , we introduce a straightforward strategy that admits many matches but conservatively avoids ones that are the most dangerous .",
    "namely , we avoid multiplying errors by eliminating matches that bridge multiple segments along a single subvolume face .",
    "for a given subvolume face , we specify that a match between two segments implies that a segment in @xmath5 and a segment in @xmath4 each only have one match :    @xmath11    we implement a couple of heuristics to satisfy the above constraint if @xmath6 or @xmath7 has multiple matches according to equation [ eq : match ] . in one approach",
    ", we can satisfy the condition by changing @xmath12 to @xmath13 in equation [ eq : match ] .",
    "for example , if a neuron branches , we potentially choose just one branch , where there is largest mutual overlap .",
    "this is shown in figure [ fig : stitch]d .",
    "we also consider an even more conservative approach , by changing the @xmath12 to @xmath13 in equation [ eq : match2 ] with @xmath14 .",
    "image segmentation over a large dataset can be very time consuming , requiring significant cluster resources .",
    "this computational load increases the likelihood that the segmentation job will fail , or , even when successful , will be invalidated by newer , better segmentation results .",
    "first , a long - running job is more vulnerable to network outages or other events that might disrupt the computation on a shared compute resource .",
    "second , any software bugs might be uncovered only after significant portion of the computation has already completed successfully . in both cases ,",
    "a potentially costly rerun of the pipeline may be necessary , but unnecessary recomputation of satisfactory results should be avoided .",
    "figure [ fig : robust ] illustrates our strategy to make the segmentation pipeline more robust to failures .",
    "specifically , we focus on the subvolume segmentation component since each task is disjoint and the other parts of the pipeline are comparatively less time consuming . our solution is to divide the set of subvolumes spanning the large dataset across multiple iterations . in each iteration",
    ", a disjoint subset of the subvolumes is processed .",
    "the segmentation labels for each subvolume in this procedure are compressed using lossless compression ( such as lz4 ) , and serialized to disk .",
    "if there are any unexpected errors in the middle of the job , the pipeline will automatically rollback to the previously computed result ( _ i.e. , _ the most recently completed iteration ) .",
    "we note in the results that the high compressability of these label volumes results in a minimal memory and i / o footprint for these checkpoints .",
    "* our robust segmentation framework .",
    "* disjoint subsets of subvolumes are processed and checkpointed to allow recovery from any software crash .",
    "note that the dataset and final segmentation is stored using dvid.,scaledwidth=100.0% ]    as mentioned earlier , there is another risk to time consuming segmentation runs .",
    "namely , it is often desirable to run multiple algorithms and to make successive refinements to the segmentation approach .",
    "however , in our application domain , the segmentation is continually refined and examined and it is undesirable to wait several months to begin this process . to address these issues",
    ", we have the option to retain previously examined ( proofread ) results across future segmentations .",
    "specifically , we provide the option to preserve a set of segment ids .",
    "these segments are masked out of the image , so that the watershed algorithm floods around these regions .",
    "our segmentation approach requires efficient access to subsets of a large dataset , which poses many infrastructural challenges . as em datasets extend to the 100tbs and beyond , storing this data in a single file on a single machine , such as in hdf5 format ,",
    "is limiting in bandwidth .",
    "furthermore , distributing data using a customized system of sharding file blobs might not generalize across different compute environments and also fails to reuse ongoing efforts outside of connectomics in large - scale database technology .",
    "these issues are complicated by the desire to also keep track of segmentation changes due to proofreading or revisions in the segmentation algorithms .    to handle these issues , we adopt the distributed , versioned , image - oriented dataservice ( dvid ) @xcite to access our large image volume data .",
    "dvid allows one to access large volumetric data through a restful api and track changes in label data using a _",
    "git_-like model of commits and branches . by satisfying the service interface",
    ", we can isolate the details of data storage from our segmentation services .",
    "in particular , we fetch subvolumes through dvid and write segmentation label data back through dvid .",
    "we implement the segmentation framework _ dvidsparkservices _ using apache spark , which allows us to manipulate these large label volumes in shared memory across several cluster nodes . while the subvolume segmentation part of the pipeline requires minimal inter - process communication and might not benefit much from shared memory dataset representation , we believed spark to be an ideal framework for several reasons :    * spark supports several primitives for working on large distributed datasets which support more structured semantics than more traditional approaches , such as ad - hoc batch scripts , would provide .",
    "* spark is supported on many different cluster compute environments .",
    "* an in - memory representation for large label data will empower future algorithms to use high - level , global context to improve segmentation quality .",
    "the next subsection details the framework and its use of spark primitives",
    ". then we highlight a plugin system to enable outside contributors to flexibly swap algorithms for different parts of the pipeline .",
    "dvidsparkservices contains several workflows for working with the dvid dataservice .",
    "each workflow is designed as a custom spark application , written in python using pyspark .",
    "access to and from dvid is controlled through the sparkdvid module .",
    "our segmentation pipeline framework is one such workflow .",
    "the key stages of the pipeline and technical details are listed below , with a description of the spark primitive used in each stage .    1 .",
    "logically split the dataset space by defining overlapping bounding boxes ( one per subvolume ) which , collectively , provide coverage over a region of interest ( roi ) .",
    "( primitive : parallelize ) 2 .",
    "map each subvolume s bounding box into a grayscale subvolume .",
    "this requires reading subvolumes in parallel using dvid .",
    "( primitive : map ) 3 .",
    "divide the subvolumes into groups , each of which will be processed in a separate iteration . 1 .",
    "map each grayscale subvolume into a volume of probabilities using some voxel - level classifier .",
    "( primitive : map ) 2 .",
    "map each probability volume into an over - segmentation of supervoxels .",
    "( primitive : map ) 3 .",
    "map each over - segmentation into a final subvolume segmentation via agglomeration .",
    "( primitive : map ) 4 .",
    "optionally serialize the subvolume segmentation to disk for checkpointing .",
    "( primitive : saveasobject ) 4 .",
    "extract overlapping regions from each pair of neighboring subvolumes .",
    "( primitive : flatmap ) 5 .",
    "group subvolume overlap regions together .",
    "this requires data to be shuffled throughout the network .",
    "( primitive : groupbykey ) 6 .",
    "map each overlap region into a set of matches .",
    "return these matches to the driver , transitively resolve the matches across the subvolumes , and broadcast to each subvolume .",
    "( primitives : map , collect , broadcast ) 7 .",
    "apply the matches to each subvolume .",
    "( primitive : map ) 8 .",
    "write segmentation to dvid through sparkdvid .",
    "( primitive : foreach )     * main components of our spark framework . *",
    "the segmentation framework is a spark application that transforms the raw image dataset into a segmentation .",
    "sparkdvid handles communication with dvid .",
    "the subvolume segmentation can be defined by a custom plugin .",
    "the default plugin allows customization of boundary prediction , supervoxel creation , and agglomeration .",
    "subvolume stitching is the only major operation that causes data to be shuffled across the network.,scaledwidth=100.0% ]    the main points are emphasized in figure [ fig : sparkarch ] .",
    "most operations require little communication from the driver .",
    "the primary exception is extracting the overlapping regions of a subvolume and matching them , which requires data to be moved throughout the spark cluster . given the relatively small size of the overlap region compared to the subvolume ( close to 10% of the volume for each subvolume face in our experiments ) and the high compressibility of the labels , there is not much data that needs to be moved around .",
    "step # 3 executes the core segmentation algorithms .",
    "our framework is intended to flexibly adopt new algorithms and tailor solutions to specific application domains . to do this ,",
    "we provide several plugin interfaces to our segmentation framework , which can be specified when calling the workflow using a configuration file ( see appendix [ sec : config ] for an example configuration file ) . the plugin is defined in python and can call any code necessary to satisfy the given interface .",
    "figure [ fig : sparkarch ] highlights the main plugin options available currently in the segmentation framework . at the top - level , the entire segmentation module which takes grayscale volume data and returns labels for a given subvolume can be overwritten .",
    "the default segmentation plugin itself allows fine - grain plugin - based control over each stage of the pipeline",
    ". the voxel prediction , supervoxel creation , and agglomeration can all be customized and implemented with an alternative python function .",
    "we briefly describe available plugins in appendix [ sec : plugin ] .",
    "the package dvidsparkservices implements our segmentation workflow and is available on github ( https://github.com/janelia-flyem/dvidsparkservices ) .",
    "we evaluate this code both for its robustness at scale and the effectiveness of its stitching strategies . in all examples",
    ", we read and write data through dvid running over an embedded leveldb database @xcite on a single machine .",
    "therefore , read and write throughput will be a be limited by the i / o capacity of the database server .",
    "we are using the ilastik @xcite plugin for boundary prediction and the neuroproof @xcite plugin for agglomeration .",
    "the subvolume size used is 512x512x512 , plus an additional 20 pixel border on all faces to create overlap with adjacent subvolumes .",
    "we applied image segmentation on a portion of the fly optic lobe around 232,000 cubic microns in size imaged at approximately 8x8x8 nm resolution using fib - sem @xcite .",
    "this encompasses 453 gb of data or 3,375 subvolumes based on our partitioning .",
    "the segmentation preserves pre - existing proofread labels .",
    "that is , only as - yet unproofread voxels are replaced with automated segmentation results .",
    "figure [ fig : fib19seg ] shows an example segmented slice .",
    "* one image plane from the segmentation of a large optic lobe dataset . *",
    "the false coloring depicts the segmented neurons and the 3d neuron is a previously proofread body that was untouched during this re - segmentation.,scaledwidth=100.0% ]    .",
    "[ tab : runtime ] * rough breakdown of time to run segmentation on a large portion of the optic lobe dataset*. the subvolume segmentation involves seven checkpoint iterations .",
    "the main component of runtime is the subvolume voxel prediction .",
    "we expect significant improvements to subvolume writes if we use a distributed back - end database behind dvid .",
    "[ cols=\"<,>\",options=\"header \" , ]      * segmentation quality of aggressive stitching throughout a large dataset .",
    "* the vi metric is computed on small regions to provide a heatmap indicating how segmentation quality varies as a function of location .",
    "connected components is run in each subvolume to compare against the ground truth .",
    "( it is possible that bad false merging outside of a given subvolume is not fixable by connected components within the region . )",
    "a ) the larger medulla sample has poor segmentation in the lower regions which results in a lot of false merging .",
    "b ) the smaller mb sample has more uniform segmentation quality.,scaledwidth=100.0% ]    table [ tab : vi ] shows the quality of segmentation using the conservative and straightforward matching strategy over the two datasets .",
    "the similarity is scored using variation of information ( vi ) @xcite .",
    "this metric allows us to decompose the similarity into a false merge ( f.merge ) and false split ( f.split ) score .",
    "a higher number indicates more errors . in both datasets",
    ", we see that both stitching strategies results in better total vi than no stitching .",
    "conservative stitching results in more false splits than aggressive stitching .",
    "however , for the medulla dataset , the conservative stitching produces significantly less false merge errors and therefore a better overall segmentation . in general , false merges are harder to fix manually .",
    "a parameter controls the aggressiveness of the stitching procedure .",
    "the setting should be chosen based on the expected quality of the subvolume segmentation results .",
    "in general , a large dataset probably increases the odds that there is some anomaly or problem that could cause false merging .",
    "( the medulla dataset is considerably larger than the mb dataset . )",
    "figure [ fig : heatmap ] shows the false merge vi in small subregions of the large volume as a heat map .",
    "notice that there are few areas toward the bottom of the medulla dataset that contains more false merge errors . despite the segmentation being good in many parts of the dataset",
    ", the overall similarity is compromised .",
    "we introduce a large - scale , open - source em segmentation infrastructure implemented in apache spark .",
    "we show that our segmentation can robustly segment large datasets by avoiding the propagation of bad false mergers and efficiently maintaining checkpoints .",
    "our implementation allows custom plugins for different parts of the segmentation pipeline .",
    "we have tested our system on both our internal compute cluster and on google compute engine .",
    "the use of spark will allow the solution to easily port to different compute environments .",
    "future work entails exploiting the more global context afforded by our system to guide new segmentation and stitching algorithms .",
    "the ability to store a large segmentation in memory across several machines will enable novel strategies . we also plan to leverage ongoing research to add different storage backends to dvid . by using a distributed database backend ,",
    "read and write bottlenecks will be greatly alleviated .",
    "* acknowledgements : * we would like to thank the flyem project team at janelia research campus .",
    "zhiyuan lu , harald hess , and shan xu prepared fly samples and created the image datasets .",
    "pat rivlin , shin - ya takemura , and the flyem proofreading team provided biological guidance and the groundtruthing effort .",
    "toufiq parag provided image segmentation insights and helped in tuning segmentation performance .",
    "bill katz implemented dvid api that used in our segmentation system .",
    "1 b. andres , _",
    ", `` 3d segmentation of sbfsem images of neuropil by a graphical model over supervoxel boundaries , '' _ med .",
    "image anal _ , 2012 , pp .",
    "796805 .",
    "s. beucher , f. meyer , `` the morphological approach to segmentation : the watershed transformation , '' _ mathematical morphology in image processing _ 1993 , pp .",
    "433481 .",
    "j. funke , b. andres , f. hamprecht , a. cardona , m. cook , `` efficient automatic 3d - reconstruction of branching neurons from em data . '' _ proc .",
    "ieee conference on computer vision and pattern recognition _",
    ", 2012 , pp . 10041011 .",
    "g. huang , v. jain , `` deep and wide multiscale recursive networks for robust image labeling , '' _ corr _ , 2013 .",
    "w. katz , `` distributed , versioned , image - oriented dataservice ( dvid ) , '' http://github.com/janelia-flyem/dvid    v. kaynig , _ et al .",
    "_ , `` large - scale automatic reconstruction of neuronal processes from electron microscopy images , '' _ medical image analysis _ , 2015 , 22(1 ) , pp .",
    "77 - 88 .",
    "j. kim , m. greene , a. zlateski , k. lee , m. richardson , `` space  time wiring specificity supports direction selectivity in the retina , '' _ nature _ , may 2014 , pp .",
    "331 - 336 .",
    "g. knott , h. marchman , d. wall , b. lich , `` serial section scanning electron microscopy of adult brain tissue using focused ion beam milling , '' _ j. neurosci _",
    ", 2008 , pp .",
    "2959 - 2964 .",
    "m. meila , `` comparing clusterings . ''",
    "_ proceedings of the sixteenth annual conference on computational learning theory _ , 2003 , springer .",
    "j. nunez - iglesias , r. kennedy , t. parag , j. shi , d. chklovskii , `` machine learning of hierarchical clustering to segment 2d and 3d images , '' _ plos one _ , august 2013 , 8(8 ) : e71715 .",
    "doi : 10.1371/journal.pone.0071715 .",
    "d. olbris , p. winston , s. plaza , m. bolstad , p. rivlin , l. scheffer , d. chklovskii , `` raveler : a proofreading tool for em reconstruction , '' _ unpublished _ , 2016 .",
    "t. parag , a. chakraborty , s. plaza , ` a context - aware delayed agglomeration framework for em segmentation`analysis paper , '' _ arxiv _ , june 2014 .",
    "s. plaza , l. scheffer , d. chklovskii ,  toward large - scale connectome reconstructions , \" _ current opinion in neurobiology _ , april 2014 , pp .",
    ".    s. plaza , l. scheffer , m. saunders , `` minimizing manual image segmentation turn - around time for neuronal reconstruction by embracing uncertainty , '' _ plos one _ , september 2012 , 7(9 ) : e44448 .",
    "doi : 10.1371/journal.pone.0044448    w. roncal , _ et al .",
    "_ , `` an automated images - to - graphs framework for high resolution connectomics , '' _ frontiers in neuroinformatics _ , 2015 , 9:20 .",
    "doi : 10.3389/fninf.2015.00020 .    c. sommer , c. straehle , u. koethe , f. hamprecht , `` ilastik : interactive learning and segmentation toolkit , '' _ proc .",
    "ieee international symposium on biomedical imaging _",
    ", 2011 , pp . 230233 .",
    "s. takemura , a. bharioke , z. lu , a. nern , s. vitaladevuni , _ et al _ , `` a visual motion detection circuit suggested by drosophila connectomics , '' _ nature _ , 2013 , pp",
    ". 175 - 181 .",
    "s. takemura , _ et al .",
    "_ , `` synaptic circuits and their variations within different columns in the visual system of drosophila , '' _ pnas _ , 2015 , 112 , 44 , pp . 13711 - 13716",
    ".    t. zhao , `` neutu , '' http://github.com/janelia-flyem/neutu    m. zaharia , m. chowdhury , m. franklin , s. shenker , i. stoica , `` spark : cluster computing with working sets , '' _ hotcloud _ , 2010 , pp .",
    "10 - 10 .",
    " leveldb : a fast and lightweight key / value database , `` '' https://github.com/google/leveldb .",
    "the segmentation workflow in the dvidsparkservices package is defined by the createsegmentation class .",
    "this class implements the pre - segmentation logic for the following steps :    1 .   dividing the region of interest into logical subvolumes 2 .   grouping subvolumes into iterations 3 .",
    "mapping subvolumes to grayscale data    and the post - segmentation logic for the following steps :    1 .   serializing the subvolume segmentation results 2 .   extracting the segmentation pixels for overlapping regions between substacks and finding matches between adjacent subvolume segments 3 .   uploading the final stitched segmentation results to dvid    the work of actually segmenting each subvolume block and stitching the blocks together",
    "is performed in a separate class , segmentor .",
    "the segmentor class implements two methods : segment ( ) and stitch ( ) .      the segment ( )",
    "function is implemented in three steps , each of which can be customized by implementing a python function , which will be called at the appropriate time :    * predict - voxels * create - supervoxels * agglomerate - supervoxels    to customize one of these steps , a developer must simply implement a python function with the appropriate signature , and provide the name of that function in the configuration json file as described below .",
    "the logic for obtaining and uncompressing the input data to each function is handled in segmentor , so the custom python functions at each stage merely deal with plain numpy arrays .",
    "for example , an extremely simple method for producing voxel - wise membrane `` predictions '' might be the following :    .... # mymodule.py def predict_via_threshold(grayscale_vol , mask_vol , threshold ) :      # this function assumes dark pixels are definitely membranes .      # all dark pixels are returned as 1.0 , and everything else 0.0 .",
    "return ( grayscale_vol < threshold).astype(numpy.float32 ) ....    the segmentor class can be instructed to call this function during the `` predict - voxels '' step via the config file .",
    "parameters such as `` threshold '' should also be specified in the config file , as shown in the example below .    .... {    \" options \" : {      \" segmentor \" : {        \" class \" : \" dvidsparkservices.reconutils.segmentor \" ,        \" configuration \" : {          \" predict - voxels \" : {            \" function \" : \" mymodule.predict_via_threshold \" ,            \" parameters \" : {              \" threshold \" : 30            }          }        }      } ,      ... additional configuration settings omitted ...    } } ....    as demonstrated above , each custom function must accept a specific set of required arguments , and any number of optional keyword arguments .",
    "the following describes each customizable function s purpose , along with its required arguments .    * * predict - voxels * given a grayscale volume and a boolean mask of background pixels for which voxel predictions are not needed , return a volume of predictions ( range : 0.0 - 1.0 , dtype : float32 ) indicating the probability of the presence of a membrane at each pixel .",
    "additional channels representing other probability classes may be optionally appended to the first channel .",
    "required arguments : grayscale_vol , mask_vol * * create - supervoxels * given the prediction volume from the `` predict - voxels '' step and a background mask volume , produce an oversegmentation of the volume into supervoxels .",
    "the supervoxels must not bleed into any background regions , as indicated by the background mask",
    ". required arguments : prediction_vol , mask_vol * * agglomerate - supervoxels * given the prediction volume from the `` predict - voxels '' step and the oversegmentation from the `` create - supervoxels '' step , produce a segmentation volume . required arguments : predictions , supervoxels      as described above , each stage of the segmentor.segment ( ) method can be customized , but there is no need to implement your own functions for every stage from scratch .",
    "the dvidsparkservices package already includes built - in functions which can be used for each stage .",
    "each of these is defined within the dvidsparkservices.reconutils.plugins namespace .",
    "* predict - voxels step * * ilastik_predict_with_array ( ) + performs voxel prediction using a trained ilastik pixel classification project file ( .ilp ) . * * two_stage_voxel_predictions ( ) + run a two - stage voxel prediction using two trained ilastik pixel classification project files .",
    "the output of the first stage will be saved to a temporary location on disk and used as input to the second stage . * * naive_membrane_predictions ( ) + implements an extremely naive heuristic for membrane probabilities by simply inverting the grayscale data .",
    "this function is mostly intended for testing purposes , but for extremely clean data , this might be sufficient .",
    "* create - supervoxels step * * seeded_watershed ( ) + computes a seeded watershed over the membrane prediction volume .",
    "the seeds are generated by simply thresholding the prediction volume .",
    "* agglomerate - supervoxels step * * neuroproof_agglomerate ( ) + agglomerates the oversegmentation image using a trained neuroproof classifier file .",
    ".... { \" dvid - info \" : {      \" dvid - server \" : \" 127.0.0.1:8000 \" ,      \" uuid \" : \" abcde12345 \" ,      \" segmentation - name \" : \" my - segmentation - result \" ,      \" roi \" : \" my - predefined - region - of - interest \" ,      \" grayscale \" : \" grayscale \"    } ,    \" options \" : {      \" segmentor \" : {        \" class \" : \" dvidsparkservices.reconutils.segmentor \" ,        \" configuration \" : {          \" predict - voxels \" : {            \" function \" :              \" dvidsparkservices.reconutils.plugins.ilastik_predict_with_array \" ,            \" parameters \" : {              \" ilp_path \" : \" /path / to / my / trained - membrane - predictor.ilp \" ,              \" lazyflow_threads \" : 1 ,              \" lazyflow_total_ram_mb \" : 1024            }          } ,          \" create - supervoxels \" : {            \" function \" : \" dvidsparkservices.reconutils.plugins.seeded_watershed \" ,            \" parameters \" : {              \" boundary_channel \" : 0 ,              \" seed_threshold \" : 0.01 ,              \" minsegmentsize \" : 300 ,              \" seed_size \" : 5            }          } ,          \" agglomerate - supervoxels \" : {            \" function \" :              \" dvidsparkservices.reconutils.plugins.neuroproof_agglomerate \" ,            \" parameters \" : {              \" classifier \" : { \" path \" : \" /path / to / my / np - agglom.xml \" } ,              \" threshold \" : 0.2 ,              \" mitochannel \" : 2            }          }        }      } ,      \" stitch - algorithm \" : \" medium \" ,      \" chunk - size \" : 512    } } ...."
  ],
  "abstract_text": [
    "<S> the emerging field of connectomics aims to unlock the mysteries of the brain by understanding the connectivity between neurons . to map this connectivity , </S>",
    "<S> we acquire thousands of electron microscopy ( em ) images with nanometer - scale resolution . after aligning these images , the resulting dataset has the potential to reveal the shapes of neurons and the synaptic connections between them . </S>",
    "<S> however , imaging the brain of even a tiny organism like the fruit fly yields terabytes of data . </S>",
    "<S> it can take years of manual effort to examine such image volumes and trace their neuronal connections . </S>",
    "<S> one solution is to apply image segmentation algorithms to help automate the tracing tasks . in this paper </S>",
    "<S> , we propose a novel strategy to apply such segmentation on very large datasets that exceed the capacity of a single machine . </S>",
    "<S> our solution is robust to potential segmentation errors which could otherwise severely compromise the quality of the overall segmentation , for example those due to poor classifier generalizability or anomalies in the image dataset . </S>",
    "<S> we implement our algorithms in a spark application which minimizes disk i / o , and apply them to a few large em datasets , revealing both their effectiveness and scalability . </S>",
    "<S> we hope this work will encourage external contributions to em segmentation by providing 1 ) a flexible plugin architecture that deploys easily on different cluster environments and 2 ) an in - memory representation of segmentation that could be conducive to new advances . </S>"
  ]
}