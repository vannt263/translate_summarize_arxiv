{
  "article_text": [
    "estimation of posterior distributions plays a central role when developing probabilistic graphical models . with conjugate priors , we are likely able to derive efficient sampling algorithms for estimation @xcite .",
    "when nonconjugate priors are used , the estimation problem is much more difficult , as observed in the topic modeling literature by .",
    "a popular approach is to cast estimation as an optimization problem .",
    "nonetheless , the resulting problems are often non - convex .",
    "non - convexity poses various obstacles for designing efficient algorithms , and does not allow us to directly exploit the nice theory of convex optimization .    in this work ,",
    "we introduce the concept of _ probable convexity _ that aims at two targets : ( 1 ) to reveal how hard an optimization problem in practice is ; ( 2 ) to support us smoothly employ efficient methods of convex optimization to deal with non - convex problems . in a perspective",
    ", probable convexity of a family @xmath0 of real functions essentially says that most members of @xmath0 are convex . with such families , in practice we probably rarely meet non - convex functions from @xmath0 .",
    "we remark that in many situations of data analytics ( e.g. , posterior estimation in graphical models ) we often have to deal with not only one but many members of a family at once .",
    "hence some appearances of non - convex members may not affect significantly the overall result .",
    "hence a direct employment of convex optimization is possible and beneficial . in other words , we could do minimization efficiently for functions of @xmath0 in practice .",
    "we next use the concept to investigate estimation of posterior distributions in the _ correlated topic model _ ( ctm ) @xcite and related nonconjugate models .",
    "in particular , we study the problem of a posteriori estimating theta ( topic mixture ) for a given document : @xmath1 .",
    "this is an map problem and is intractable for many models in the worst case @xcite .",
    "we show that under certain conditions , the objective function of this map problem is in fact _ probably concave _",
    ", i.e. , concave with high probability .",
    "this suggests that posterior estimation of theta may be tractable in practice .",
    "similar results are obtained for related nonconjugate topic models .",
    "the cornerstone of our analyses of nonconjugate models is the logistic - normal function which originates from the logistic - normal distribution @xcite .",
    "we show in this work that the logistic - normal function is probably concave under certain conditions .",
    "this result may be of interest elsewhere and beneficial in practical applications , because the logistic - normal distribution is used as an effective prior in many contexts including topic modeling @xcite and grammar induction @xcite .    as a consequence of our analysis , a novel algorithm for learning ctm is proposed .",
    "this algorithm is surprisingly simple in which posterior estimation of theta is done by the online frank - wolfe ( ofw ) algorithm @xcite . from empirical experiments we find that the new algorithm is significantly faster than existing ones , while maintaining or making better the quality of the learned models .",
    "this further suggests that even though map inference for ctm is intractable in the worst case , most instances in practice may be resolved efficiently .",
    "finally , we find that stochastic gradient decent ( sgd ) might be a practical choice to resolve efficiently non - convex problems .",
    "sgds such as ofw @xcite are originally introduced in the convex optimization literature .",
    "they are often very efficient and have many advantages over deterministic algorithms , especially in large - scale settings . however , to our best knowledge , no prior study has been made to investigate the role of sgds for resolving non - convex problems .",
    "we argue that due to their stochastic nature , sgd algorithms might be able to jump out of local optima to reach closer to global ones .",
    "hence sgds seem to be more advantageous than traditional ( deterministic ) methods for non - convex problems .",
    "we complement this observation by the successful use of ofw to solve posterior estimation of theta in ctm .",
    "organization : we present the concept of probable convexity in section  [ ch5-sec : concepts ] .",
    "section  [ ln - concavity ] presents our analysis of the logistic - normal function .",
    "the study of ctm and related nonconjugate models is presented in section  [ ch5-sec : map - in - ctm ] . the new algorithm for learning ctm and experimental results",
    "are discussed in section  [ ch5-sec : learning - ctm ] .",
    "we also investigate in this section how well sgds could resolve non - convex problems by analyzing ofw .",
    "the final section is for further discussion and conclusion .",
    "notation : throughout the paper , we use the following conventions and notations .",
    "bold faces denote vectors or matrices .",
    "@xmath2 denotes the @xmath3 element of vector @xmath4 , and @xmath5 denotes the element at row @xmath6 and column @xmath7 of matrix @xmath8 .",
    "notation @xmath9 means that matrix @xmath8 is _ negative semidefinite_. for a given vector @xmath10 , we denote @xmath11 and @xmath12 .",
    "@xmath13 denotes the diagonal matrix whose diagonal entries are @xmath14 , respectively .",
    "more notations are :    [ cols= \" > , < \" , ]     table [ fig :- ofw - inf - compare ] shows some statistics from our experiments .",
    "it can be observed that slsqp often needs intensive time to solve a problem , while ofw consumes substantially less time .",
    "we observe that ofw often works 150 - 2000 times faster than slsqp .",
    "slow performance of slsqp mainly comes from the need to solve many intermediate quadratic programming problems , each of which often requires considerable time in our observations . on contrary , each iteration of ofw is very modest , which mostly requires computation of partial derivatives .    in terms of quality",
    ", we observe that ofw was able to find significantly better approximate solutions than slsqp .",
    "when inspecting individual problems , we found that slsqp failed to find feasible solutions for many problems , e.g. , a large number of returned solutions were significantly out of domain ( @xmath15 ) .",
    "in contrast , ofw always manages to find feasible solutions . among 11197 problems ,",
    "ofw performed significantly worse than slsqp for only 2 .",
    "those observations demonstrate that ofw has many advantages over ( deterministic ) slsqp .",
    "further , it is able to find good approximation solutions for non - concave problems with a modest requirement of computation .",
    "we have introduced the concept of probable convexity to analyze real functions or families of functions .",
    "it is the way to see how probable a real function is convex . in particular",
    ", it can reveals how many members of a family of functions are convex .",
    "when a family contains most convex members , we could deal with the family efficiently in practice .",
    "hence probable convexity provides a feasible way to deal with non - convexity of real problems such as posterior estimation in probabilistic graphical models .",
    "when analysing probable convexity of the problem of estimating topic mixtures in ctm @xcite , we found that this problem is concave under certain conditions .",
    "the same results were obtained for many nonconjugate models .",
    "these results suggest that posterior inference of topic mixtures in those models might be done efficiently in practice , which seems to contradict with the belief of intractability in the literature . benefiting from those theoretical results , we proposed a novel algorithm for learning ctm which can work 60 - 170 times faster than the variational method by @xcite , while keeping or making better the quality of the learned models .",
    "we believe that by using the same methodology as ours , learning for many existing nonconjugate models can be significantly accelerated .",
    "an implementation of our algorithm is freely available at http://is.hust.edu.vn/~khoattq/codes/fctm/    there is a unusual employment of the online frank - wolfe algorithm ( ofw ) @xcite to solve nonconvex problems ( inference of topic mixtures in ctm ) .",
    "ofw is a specific instance of stochastic gradient descent algorithms ( sgds ) for solving convex problems . by a careful employment",
    ", ofw behaves well in solving the inference problem which is nonconcave in the worse case .",
    "it helps us to design an efficient and effective algorithm for learning ctm .",
    "such a successful use of ofw suggests that sgds might be a practical choice to deal with nonconvex problems . in our experiments ,",
    "ofw found significantly better solutions whereas performed 150 - 2000 times faster than slsqp ( the standard algorithm for nonconvex optimization ) .",
    "this further supports our highlight about sgds .",
    "we hope that this highlight would open various rooms for future studies on connection of sgds with nonconvex optimization ."
  ],
  "abstract_text": [
    "<S> non - convex optimization problems often arise from probabilistic modeling , such as estimation of posterior distributions . </S>",
    "<S> non - convexity makes the problems intractable , and poses various obstacles for us to design efficient algorithms . in this work , </S>",
    "<S> we attack non - convexity by first introducing the concept of _ probable convexity _ for analyzing convexity of real functions in practice . </S>",
    "<S> we then use the new concept to analyze an inference problem in the _ correlated topic model _ ( ctm ) and related nonconjugate models . </S>",
    "<S> contrary to the existing belief of intractability , we show that this inference problem is concave under certain conditions . </S>",
    "<S> one consequence of our analyses is a novel algorithm for learning ctm which is significantly more scalable and qualitative than existing methods . finally , we highlight that stochastic gradient algorithms might be a practical choice to resolve efficiently non - convex problems . </S>",
    "<S> this finding might find beneficial in many contexts which are beyond probabilistic modeling .    </S>",
    "<S> non - convex optimization , posterior estimation , posterior inference , non - conjugate models , ctm , stochastic gradient decent . </S>"
  ]
}