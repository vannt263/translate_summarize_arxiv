{
  "article_text": [
    "maximum likelihood estimation of shape - constrained densities has received a great deal of interest recently .",
    "the allure is the prospect of obtaining fully automatic nonparametric estimators , with no tuning parameters to choose .",
    "the general idea dates back to @xcite , who derived the maximum likelihood estimator of a decreasing density on @xmath1 .",
    "a characteristic feature of these shape - constrained maximum likelihood estimators is that they are not smooth .",
    "for instance , the grenander estimator has discontinuities at some of the data points .",
    "the maximum likelihood estimator of a multi - dimensional log - concave density is the exponential of what @xcite call a _ tent function _ ; it may have several ridges .",
    "moreover , in this ( and other ) examples , the estimator drops discontinuously to zero outside the convex hull of the data .    in some applications",
    ", the lack of smoothness may not be a drawback in itself .",
    "however , in other circumstances , a smooth estimate might be preferred , because :    a.   it has a more attractive visual appearance , without ridges or discontinuities that might be difficult to justify to a practitioner ; b.   it has the potential to offer substantially improved estimation performance , particularly for small sample sizes , where the convex hull of the data is likely to be rather small ; c.   for certain applications , e.g. classification , the maximum likelihood estimator being zero outside the convex hull of the data may present problems ; see section  [ sec : classification ] for further discussion .    for these reasons , we investigate a smoothed version of the @xmath2-dimensional log - concave maximum likelihood estimator .",
    "the smoothing is achieved by a convolution with a gaussian density , which preserves the log - concavity shape constraint . to decide how much to smooth",
    ", we exploit an interesting property of the log - concave maximum likelihood estimator , which provides a canonical choice of covariance matrix for the gaussian density , thereby retaining the fully automatic nature of the estimate . the basic idea , which was introduced by @xcite for the case @xmath3 and touched upon in @xcite , is described in greater detail in section  [ sec : mdp ] .    the challenge of computing the estimator , which involves a @xmath2-dimensional convolution integral , is taken up in section  [ sec : computation ] ; see figure  [ fig : lcdsmlcd ] for an illustration of the estimates obtained",
    ". the theoretical properties of the smoothed log - concave estimator are studied in section  [ sec : theory ] .",
    "our framework handles both cases where the log - concavity assumption holds and where it is violated . in section  [ sec : projections ] , we present new results on the infinite - dimensional projection from a probability distribution on @xmath0 to its closest log - concave approximation ; these give further insight into the misspecified setting .",
    "a simulation study follows in section  [ sec : fsp ] , confirming the excellent finite - sample performance .",
    "@xmath4{lcd2.ps } & \\includegraphics[scale=0.32]{smlcd2.ps } \\\\",
    "\\mathrm{(a ) } & \\mathrm{(b ) }     \\end{array}$ ]    in section  [ sec : test ] , we introduce a new hypothesis test of log - concavity of multivariate distributions based on our choice of covariance matrix for the gaussian density .",
    "this test is consistent , easy to implement , and has much improved finite - sample performance compared to existing methods . section  [ sec : applications ] is devoted to applications of the smoothed log - concave maximum likelihood estimator to classification and other functional estimation problems .",
    "we provide theory , under both correct and incorrect model specification , for the performance of the resulting procedures in these cases .",
    "the classification methodology is applied to the wisconsin breast cancer data set , where the aim is to aid the diagnosis of future potential breast cancer instances .",
    "all proofs are deferred to the appendix .",
    "theoretical properties of the unsmoothed log - concave maximum likelihood estimator have been studied in @xcite , @xcite , @xcite and @xcite for the case @xmath3 , and @xcite , @xcite and @xcite for the multivariate case .",
    "further properties of log - concave distributions are discussed in @xcite , and @xcite provides an overview of the field .",
    "other methods for enforcing various shape constraints have been studied in @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "let @xmath5 denote the set of all probability distributions @xmath6 on @xmath0 such that @xmath7 for all hyperplanes @xmath8 . in this section",
    ", we assume that @xmath9 are independent random vectors in @xmath0 with distribution @xmath10 . in that case , for sufficiently large @xmath11 the convex hull of the data , denoted @xmath12 , is @xmath2-dimensional with probability 1 .",
    "it is then known that there exists a unique log - concave density @xmath13 that maximises the likelihood function @xmath14 over all log - concave densities @xmath15 .",
    "the estimator @xmath13 is supported on @xmath16 , and @xmath17 is piecewise affine on this set .",
    "more precisely , there exists an index set @xmath18 consisting of @xmath19-tuples @xmath20 of distinct indices in @xmath21 ,",
    "such that @xmath16 can be triangulated into simplices @xmath22 in such a way that @xmath23 for some vectors @xmath24 in @xmath0 and real numbers @xmath25 .",
    "such a function was called a _ tent function _ in @xcite because when @xmath26 one can think of associating a ` tent pole ' with each observation , extending vertically out of the plane . for certain tent pole heights ,",
    "the graph of @xmath17 is then the roof of a taut tent stretched over the tent poles .    despite the attractive asymptotic properties of @xmath13 derived in the papers cited in the introduction ,",
    "the simulation results in @xcite and @xcite indicate that the finite - sample performance is only strong relative to competitors ( e.g. kernel - based methods ) for moderate or large sample sizes ( say @xmath27 ) .",
    "it appears that for smaller values of @xmath11 , the convex hull of the data is typically not large enough for good performance .",
    "the idea for fully automatic smoothing of the log - concave maximum likelihood estimator comes from the following observation : remark  2.3 of @xcite ( see also corollary  2.3 of @xcite ) shows that while the log - concave maximum likelihood estimator is a good estimator of the first moment of @xmath28 , it underestimates the covariance matrix .",
    "more precisely , we have that @xmath29 say . on the other hand , however , @xmath30 here , @xmath31 and @xmath32 mean the matrix @xmath33 is non - negative definite and positive definite respectively .",
    "this allows us to define our modified estimator , which we call the _ smoothed log - concave maximum likelihood estimator _ and denote @xmath34 .",
    "it is given by @xmath35 where @xmath36 is the @xmath2-variate normal density with zero mean and covariance matrix @xmath37 .",
    "note that the level of smoothing is automatically determined through the matrix @xmath38 .",
    "the basic properties of @xmath34 are summarised in the proposition below .",
    "[ prop : basic ] let @xmath10 , and let @xmath34 denote the smoothed log - concave maximum likelihood estimator @xmath34 based on independent observations @xmath39 having distribution @xmath28 . then    ( a ) : :    @xmath34 is log - concave ; ( b ) : :    the support of @xmath34 is    @xmath0 ; ( c ) : :    @xmath34 is a real analytic function on    @xmath0 ( in particular , it is infinitely    differentiable ) ; ( d ) : :    the mean and covariance matrix corresponding to    @xmath34 agree with the sample mean and sample    covariance matrix :    @xmath40 and    @xmath41 .",
    "the aim of this section is to describe algorithms for computing the smoothed log - concave maximum likelihood estimator @xmath34 . as a preliminary step , we need to compute the covariance matrix @xmath38 of the multivariate normal distribution used in the convolution  ( [ eq : fntilde ] ) .",
    "recall that @xmath37 , where @xmath42 is the sample covariance matrix , and @xmath43 we make an affine transformation of each of the regions of integration onto the unit simplex . recall that @xmath22 , set @xmath44 $ ] , and let @xmath45 be the unit simplex in @xmath0 .",
    "following @xcite , we further define the auxiliary functions @xmath46 by @xmath47 where @xmath48 .",
    "then , writing @xmath49 , we have @xmath50 we have applied the basic results of @xcite in the last step .",
    "an exact expression for @xmath51 is given in appendix  b.1 of @xcite when its arguments are non - zero and distinct .",
    "the taylor approximation of @xcite can be used when some of the arguments are small or have similar ( or equal ) values .",
    "we have @xmath52 by making an affine transformation of each @xmath53 onto the unit simplex as in section  [ sec : covmatrix ] , we reduce the problem to integrating the exponential of a quadratic polynomial over the unit simplex . in general , this has no explicit solution , so it has to be evaluated numerically .",
    "@xcite gives a brief introduction to the problem of evaluating integrals over the unit simplex , while @xcite proposed a combinatorial method .",
    "we apply their method , first noting that by integrating out one variable , the dimensionality of the integral can be reduced by one . to see this , consider any @xmath54 positive definite , symmetric matrix @xmath55 $ ] , any vector @xmath56 and any constant @xmath57 .",
    "writing @xmath58 for the standard normal distribution function , @xmath59 and @xmath60 we have @xmath61 here , @xmath62 , @xmath63 and @xmath64 are defined by @xmath65_{1 \\leq l , l ' \\leq d-1}u_{-d}+\\sum_{l=1}^{d-1 } b_lu_l + c,\\ ] ] where @xmath66 .",
    "it follows that we can use the combinatorial method to integrate over the @xmath67-dimensional unit simplex .",
    "some special cases include :    a.   @xmath68 . in this case ,",
    "( [ eq : integ1d ] ) is a simple function of @xmath58 , and the smoothed log - concave maximum likelihood estimator can be computed straightforwardly .",
    "this method is implemented in the ` r ` package ` logcondens ` @xcite .",
    "b.   @xmath69 . in this case , ( [ eq : integ1d ] ) is an integral over @xmath70 $ ] , and other standard numerical integration methods such as the gaussian quadrature rule , can be applied .",
    "the combinatorial method and its variations are implemented in the latest version of the ` r ` package ` logconcdead ` @xcite .",
    "we found this method to be numerically stable even with several thousand observations , when @xmath71 may be rather small ( note that in such cases , @xmath62 in  ( [ eq : integ1d ] ) will typically not be close to zero ) .",
    "however , we briefly present below two other ways of computing @xmath72 ; while slower in most cases , they do not require the inversion of @xmath38 , so can be used even when @xmath71 is very small .",
    "a.   * monte carlo method*. 1 .",
    "conditional on @xmath39 , generate independent random vectors @xmath73 from the @xmath74 distribution .",
    "approximate @xmath72 by @xmath75",
    ". + the validity of this approximation follows from the strong law of large numbers , applied conditional on @xmath39 . b.   * fourier transform*. we can take advantage of the convolution property of the fourier transform @xmath76 as follows .",
    "first note that @xmath77 which can be evaluated by extending the auxiliary functions @xmath78 to the complex plane . since @xmath79 , we can invert @xmath80 on a fine grid using the fast fourier transform .      since @xmath34 is the convolution of @xmath13 and a multivariate normal density , conditional on @xmath39 , it is straightforward to draw an observation @xmath81 from @xmath82 as follows :    a.   draw @xmath83 from @xmath84 using the algorithm described in appendix  b.3 of @xcite or the algorithm of @xcite",
    ". b.   draw @xmath85 , independent of @xmath83 . c.   return @xmath86 .",
    "it is convenient to define , for @xmath87 , the classes of probability distributions on @xmath0 given by @xmath88 the condition @xmath89 is necessary and sufficient for the existence of a unique upper semi - continuous log - concave density @xmath90 that maximises @xmath91 over all log - concave densities @xmath15 ( * ? ? ? * theorem  2.2 ) .",
    "in fact , if @xmath28 has a density @xmath92 , and provided that @xmath93 ( which is certainly the case if @xmath92 is bounded ) , @xmath90 minimises the kullback ",
    "leibler divergence @xmath94 over all log - concave densities @xmath15 . in this sense",
    ", @xmath90 is the closest log - concave density to @xmath28 .",
    "the density @xmath90 plays an important role in the following theorem , which describes the asymptotic behaviour of the smoothed log - concave maximum likelihood estimator @xmath34 .",
    "[ thm : asymp ] suppose that @xmath95 , and write @xmath96 and @xmath97 .",
    "let @xmath98 , where @xmath99 with @xmath100 .",
    "taking @xmath101 and @xmath102 such that @xmath103 , we have for all @xmath104 that @xmath105 and , if @xmath106 is continuous , @xmath107 .",
    "the condition that @xmath95 imposed in theorem  [ thm : asymp ] ensures the finiteness of @xmath108 .",
    "we see that in general , @xmath34 converges to a slightly smoothed version of the closest log - concave density to @xmath28 .",
    "however , if @xmath28 has a log - concave density @xmath92 , then @xmath109 , so @xmath34 is strongly consistent in these exponentially weighted total variation and supremum norms .",
    "in fact , suppose that @xmath110 is a sublinear function , i.e. @xmath111 and @xmath112 for all @xmath113 and @xmath114 , satisfying @xmath115 as @xmath116 .",
    "it can be shown that under the conditions of theorem  [ thm : asymp ] , @xmath117 @xcite .    despite being smooth and having full support",
    ", it turns out that @xmath34 is rather close to @xmath13 .",
    "this is quantified in the finite - sample bound below .",
    "[ prop : bounds ]    if @xmath118 , and @xmath119 , then @xmath120 moreover , @xmath121 where @xmath122 , and @xmath123 .      in this subsection",
    ", we give new insights into the maps from a probability distribution @xmath6 to its log - concave approximation @xmath90 , and its smoothed version @xmath106 .",
    "results such as these enhance our understanding of the behaviour of maximum likelihood estimators in non - convex , misspecified models , where existing results are very limited .",
    "theorem  [ thm : independent ] below shows that log - concave approximations and their smoothed analogues preserve independence of components . as well as",
    "being of use in our simulation studies , this is the key result which underpins a new approach to fitting independent component analysis models using nonparametric maximum likelihood @xcite .",
    "[ thm : independent ] suppose that @xmath124 is a product measure on @xmath0 , so that @xmath125 , say , where @xmath126 and @xmath127 are probability measures on @xmath128 and @xmath129 respectively , with @xmath130 .",
    "let @xmath90 denote the log - concave approximation to @xmath6 , and let @xmath131 denote the log - concave approximation to @xmath132 , for @xmath133 . then , writing @xmath134 , where @xmath135 and @xmath136 , we have @xmath137 now suppose further that @xmath138 .",
    "let @xmath106 denote the smoothed log - concave approximation to @xmath6 , and let @xmath139 denote the smoothed log - concave approximation to @xmath132 , for @xmath133 .",
    "then , for all @xmath134 , @xmath140    our next theorem characterises the log - concavity constraint through the trace of the non - negative definite matrix @xmath108 defined in theorem  [ thm : asymp ] .",
    "[ thm : covariance ] suppose that @xmath124",
    ". then @xmath141 if and only if @xmath6 has a log - concave density .    the `",
    "if ' part of this statement is well - known , but the ` only if ' part is new .",
    "the two parts together motivate our testing procedure for log - concavity , which is developed in section  [ sec : test ] .    in most cases , it is very difficult to find explicitly the log - concave approximation @xmath90 to a given distribution @xmath124 .",
    "our final result of this section is straightforward to prove , but is of interest because it shows that some log - concave densities can have a large ` domain of attraction ' .    [",
    "prop : convex ] let @xmath90 be an upper semi - continuous , log - concave density on @xmath0",
    ". then the class of distributions @xmath124 with log - concave approximation @xmath90 is convex .",
    "for instance , if @xmath142 is a symmetrised pareto density with @xmath143 and @xmath144 , then it can be shown that its log - concave projection is @xmath145 .",
    "thus the class of distributions with whose log - concave projection is the standard laplace density is infinite - dimensional .",
    "our simulation study considered the normal location mixture density @xmath146 for @xmath147 1 , 2 and 3 , where @xmath148 .",
    "this mixture density is log - concave if and only if @xmath149 . for each density , for @xmath26 and @xmath150 , and for sample sizes @xmath151 and @xmath152 , we computed the integrated squared error ( ise ) of the smoothed log - concave maximum likelihood estimator for each of 50 replications .",
    "we also computed the ise of the log - concave maximum likelihood estimator and that of a kernel density estimator with a gaussian kernel and the optimal ise bandwidth for each individual data set , which would be unknown in practice .",
    "the boxplots of the ises for the different methods are given in figure  [ fig : box3d ] for @xmath150 .",
    "the analogous plots for the case @xmath26 can be found in @xcite .     with the gaussian location mixture true density for the smoothed log - concave maximum likelihood estimator smlcd , log - concave maximum likelihood estimator lcd and kernel density estimator with the ` oracle ' optimal ise bandwidth : ( a ) @xmath151 , @xmath153 ; ( b ) @xmath151 , @xmath154 ; ( c ) @xmath151 , @xmath155 ; ( d ) @xmath152 , @xmath153 ; ( e ) @xmath152 , @xmath154 ; ( f ) @xmath152 , @xmath155 . ]",
    "we see that when the true density is log - concave , the smoothed log - concave estimator offers substantial ise improvements over its unsmoothed analogue for both sample sizes , particularly at the smaller sample size @xmath151 .",
    "it also outperforms by a considerable margin the kernel density estimator with the optimal ise bandwidth .",
    "when the log - concavity assumption is violated , the smoothed log - concave estimator is still competitive with the optimal - ise kernel estimator at the smaller sample size @xmath151 , and also improves on its unsmoothed analogue .",
    "however , at the larger sample size @xmath152 , the bias caused by the fact that @xmath156 dominates the contribution from the variance of the estimator , and the kernel estimator is an improvement .",
    "these results confirm that the smoothed log - concave estimator has excellent performance when the true density is log - concave , and remains competitive in situations where the log - concavity assumption is violated , provided that the modelling bias caused by this misspecification is not too large relative to the sampling variability of the estimator .",
    "several tests of log - concavity have been proposed in the literature . @xcite and @xcite",
    "discuss various tests for univariate data , while @xcite presented two tests of log - concavity for multivariate data .",
    "@xcite proposed another multivariate test based on kernel density estimates which had improved finite - sample performance on his simulated examples .",
    "however , none of these multivariate tests has theoretical support .",
    "suppose @xmath157 , and we seek a size @xmath158 test of @xmath159 has a log - concave density against @xmath160 does not have a log - concave density .",
    "motivated by theorem  [ thm : covariance ] , we propose the following procedure :    a.   compute the log - concave maximum likelihood density estimate @xmath13 .",
    "b.   compute the test statistic @xmath161 , where @xmath37 , as in  ( [ eq : sigmas ] ) .",
    "c.   generate a reference distribution as follows : for @xmath162 , draw conditionally independent samples @xmath163 from @xmath13 . for each bootstrap sample , first compute the log - concave maximum likelihood estimator @xmath164 .",
    "then compute @xmath165 , where @xmath166 and @xmath167 .",
    "d.   reject @xmath168 if @xmath169 .",
    "we call this procedure a _ trace _ test .",
    "it is justified by the following result :    [ thm : test ] suppose that @xmath89 .",
    "the trace test is consistent : that is , if @xmath28 is not log - concave , then for each @xmath170 , the power of the test converges to one as @xmath171 .",
    "we remark that if @xmath95 , one can also draw bootstrap samples from @xmath34 instead of @xmath13 in step ( c ) . to illustrate the performance of the test , we ran two small simulation studies . in the first study",
    ", we simulated from the bivariate mixture of normal distributions density @xmath172 , with @xmath173 ( which we recall is log - concave if and only if @xmath149 ) .",
    "for each simulation setup , we performed 200 hypothesis tests with @xmath174 .",
    "the proportion of times that the null hypothesis was rejected in a size @xmath175 test is reported in table  [ tab : sim1 ] . for comparison",
    ", we also report the results from the critical bandwidth test proposed by @xcite .",
    "the permutation test studied by @xcite did not perform as well as the critical bandwidth test , so we omitted its results here .",
    ".proportion of times out of 200 repetitions that the null hypothesis was rejected with @xmath176 .",
    "[ cols=\"^,^,^,^,^\",options=\"header \" , ]     the first study confirms that the trace test controls the type i error satisfactorily ( and appears to be less conservative than the critical bandwidth test when @xmath177 ) .",
    "the results of the second study , though , are quite striking , and suggest that our new test for log - concavity has considerably improved finite - sample power compared to the critical bandwidth test .",
    "@xcite noted that the critical bandwidth test can have reduced power due to the boundary bias of the kernel estimators and is quite sensitive to the outliers ( in fact , one also needs to pick a compact region containing the majority of the data , and this choice is somewhat arbitrary ) .",
    "our test avoids these issues and performs well even in the presence of outliers or when the true density has bounded support .",
    "changing notation slightly from the previous section , we now assume that @xmath178 , @xmath179 , @xmath180 , @xmath181 are independent and identically distributed pairs taking values in @xmath182 .",
    "let @xmath183 for @xmath184 , and suppose that conditional on @xmath185 , the random vector @xmath186 has distribution @xmath187 .",
    "a _ classifier _ is a measurable function @xmath188 , with the interpretation that the classifier assigns the point @xmath189 to class @xmath190 .",
    "the _ misclassification error rate _ , or _",
    "risk _ , of @xmath191 is @xmath192 in the case where each distribution @xmath187 has a density @xmath193 , the classifier that minimises the risk is the _ bayes classifier _ @xmath194 , given by @xmath195 ( for all classifiers defined by an @xmath196 as above , we will for the sake of definiteness split ties by taking the smallest element of the @xmath196 . )",
    "we will also be interested in the _ log - concave bayes classifier _ and _ smoothed log - concave bayes classifier _ , defined respectively by @xmath197 here , @xmath198 and @xmath199 are the log - concave approximation to @xmath187 and its smoothed analogue , defined in theorem  [ thm : asymp ] .",
    "in particular , both classifier coincide with the bayes classifier when @xmath200 have log - concave densities .",
    "empirical analogues of these theoretical classifiers are given by @xmath201 here , @xmath202 is the number of observations from the @xmath203th class , and @xmath204 and @xmath205 are respectively the log - concave maximum likelihood estimator of @xmath193 and its smoothed analogue , based on @xmath206 .",
    "the theorem below describes the asymptotic behaviour of these classifiers .",
    "it reveals that the risk of @xmath207 and @xmath208 converges not ( in general ) to the bayes risk , but instead to the risk of @xmath209 and @xmath210 respectively .",
    "this is a similar situation to that encountered when a parametric classifier such as linear or quadratic discriminant analysis is used , but the relevant parametric modelling assumptions fail to hold .",
    "it suggests that the classifiers @xmath207 and @xmath208 should only be used when the hypothesis of log - concavity can be expected to hold , at least approximately .",
    "[ thm : classifiers ]    ( a ) : :    assume @xmath211 for    @xmath212 .",
    "let    @xmath213 .    then    @xmath214    for almost all @xmath215 , and    @xmath216 ( b ) : :    now assume @xmath217 for    @xmath212 .",
    "let    @xmath218 .",
    "then    @xmath219    for almost all @xmath220 , and    @xmath221    in fact , the smoothed log - concave classifier is somewhat easier to apply in practical classification problems than its unsmoothed analogue .",
    "this is because if @xmath222 is outside the convex hull of the training data for each of the @xmath223 classes ( an event of positive probability ) , then the log - concave maximum likelihood estimates of the densities at @xmath224 are all zero .",
    "thus all such points would be assigned by @xmath207 to class 1 . on the other hand",
    ", @xmath208 avoids this problem altogether . for these reasons , we considered only @xmath208 in our simulation study @xcite and below .",
    "we remark that the direct use of @xmath208 ( or any other classifier based on nonparametric density estimation ) is not recommended when @xmath225 , due to the curse of dimensionality . in such circumstances",
    "there are two options : dimension reduction ( cf .  section  [ sec : breastcancer ] below ) , or further modelling assumptions such as independent component analysis models @xcite . in either case , the methodology we develop remains applicable , but now as part of a more involved procedure .      in the wisconsin breast cancer data set @xcite ,",
    "30 measurements were taken from a digitised image of a fine needle aspirate of different breast masses .",
    "there are 357 benign and 212 malignant instances , and we aim to construct a classifier based on this training data set to aid future diagnoses .",
    "only the first two principal components of the training data were considered , and these capture 63% of the total variability ; cf .",
    "figure  [ fig : wbcd](a ) .",
    "this was done to make our procedure computationally feasible , to reduce the effect of the curse of dimensionality , and to facilitate plots such as figure  [ fig : wbcd ] below .",
    "@xmath226{wbcd_a.ps } &    \\includegraphics[scale=0.30]{wbcd_b.ps } \\\\",
    "\\mathrm{(a ) } & \\mathrm{(b ) }              \\\\    \\includegraphics[scale=0.76]{wbcd_c.ps } &    \\includegraphics[scale=0.76]{wbcd_d.ps } \\\\",
    "\\mathrm{(c ) } & \\mathrm{(d ) }   \\end{array}$ ]    in figure  [ fig : wbcd](b ) , we show the smoothed log - concave density estimates of both the benign and malignant classes . figure  [ fig : wbcd](c ) plots the decision boundaries of the smoothed log - concave classifier , where we treat benign cases and malignant cases equally .",
    "however in practice , misdiagnosing a malignant tumour as benign is much more serious than misidentifying a benign one as malignant .",
    "one may therefore seek to incorporate different losses into the classifier .",
    "for @xmath227 , let @xmath228 denote the cost of failure to recognise the class @xmath203 ( this notion can easily be generalised to multicategory situations were @xmath229 is the loss incurred in assigning the pair @xmath178 to class @xmath230 when @xmath185 ) .",
    "redefining the risk as @xmath231 the same asymptotic properties continue to hold , mutatis mutandis , for the classifier @xmath232 we observe that this modification requires no recalculation of the smoothed log - concave density estimates and there is no loss of generality in taking @xmath233 .",
    "a gui with slider is implemented in the ` r ` package ` logconcdead ` , which provides a way of demonstrating how the decision boundaries change as @xmath234 varies .",
    "for the purpose of illustration , figure  [ fig : wbcd](d ) plots the decision boundaries of @xmath235 when the cost @xmath234 of misidentifying a malignant tumour is 100 . compared with figure  [ fig : wbcd](c ) , observations are of course considerably more likely to be classified as malignant under this setting .",
    "classification problems are an important example of a situation where one is interested in a functional of one or more density estimates , rather than the density estimate itself .",
    "for simplicity of exposition , we return in this section to the situation where we have a single independent sample @xmath39 distributed according to a distribution @xmath28 .    in general",
    ", we can consider estimating a functional @xmath236 using the plug - in smoothed log - concave estimate @xmath237 , where @xmath238 is the distribution with density @xmath34 .",
    "note that even if this functional can not be computed directly , it is usually straightforward to construct a monte carlo approximation to @xmath239 by applying the algorithm for sampling from @xmath34 outlined in section  [ sec : sampling ] . to describe the theoretical properties of these functional estimates , for @xmath240 ,",
    "let @xmath241 denote the set of signed measures @xmath6 on @xmath0 with @xmath242 .",
    "equip @xmath241 with the norm @xmath243 we can then consider @xmath244 as a measurable function on @xmath245 taking values in some other normed space @xmath246 .    [ prop : functionals ]",
    "let @xmath95 , and let @xmath247 denote the probability distribution whose density is the smoothed version of the log - concave approximation to @xmath28 .",
    "suppose that @xmath248 is continuous , and let @xmath249 .",
    "then @xmath250 as @xmath171",
    ".    once again , we remark that if @xmath28 has a log - concave density , then @xmath251 .",
    "the fact that the topology on @xmath241 is rather strong means that the continuity requirement on @xmath244 is relatively weak .",
    "this is illustrated in the following corollary , which considers the special case of _ linear _ functionals in proposition  [ prop : functionals ] .",
    "[ cor : linearfunctionals ] let @xmath95 , and let @xmath101 and @xmath102 be such that @xmath103 , where @xmath106 is the smoothed log - concave approximation to @xmath28 .",
    "let @xmath252 for some measurable function @xmath253 satisfying @xmath254 for some @xmath104 . then @xmath255 .",
    "we thank the associate editor and two anonymous referees for their helpful comments .",
    "the second author is grateful for the support of a leverhulme research fellowship and an epsrc early career fellowship .",
    "of proposition  [ prop : basic ]        * ( c ) * the fact that @xmath34 is infinitely differentiable follows from proposition  8.10 of @xcite .",
    "in fact , using standard multi - index notation with @xmath258 and @xmath259 , we have @xmath260 . writing @xmath261 and @xmath262 , it follows that for any @xmath222 and @xmath263 , @xmath264 as @xmath265 , by the dominated convergence theorem and lemma  1 of @xcite .    *",
    "( d ) * conditional on @xmath39 , let @xmath83 and @xmath266 be independent , with @xmath83 having density @xmath13 and @xmath266 having density @xmath36 , so that @xmath267 has conditional density @xmath34",
    ". then @xmath268 and @xmath269 @xmath270    of theorem  [ thm : asymp ] let @xmath271 and @xmath272 denote the prohorov and total variation metrics on the space of probability measures on @xmath0 . recall that @xmath271 metrises weak convergence , and that @xmath273",
    "let @xmath274 denote the probability measure corresponding to the density @xmath34 , let @xmath275 denote the probability measure corresponding to the convolution of @xmath90 with the measure @xmath276 , and let @xmath277 denote the probability measure corresponding to @xmath106 . then @xmath278 the first term of  ( [ eq : twoterms ] ) converges almost surely to zero , by theorem  2.15 of @xcite .",
    "the second term also converges almost surely to zero , using the fact that @xmath279 as @xmath171 .",
    "proposition  2 of @xcite strengthens the mode of convergence and yields the result .",
    "@xmath270    of proposition  [ prop : bounds ] if @xmath118 , and @xmath119 , then @xmath280 for all @xmath281 .",
    "it follows that @xmath282 now @xmath283 but @xmath284 it therefore follows from this and  ( [ eq : internalupper ] ) that @xmath285 as required .",
    "@xmath270    of theorem  [ thm : independent ] * ( a ) * let @xmath15 be an arbitrary log - concave density on @xmath0 , and let @xmath186 be a random vector with density @xmath15 . letting @xmath286 , where @xmath287 and @xmath288 take values in @xmath128 and @xmath129 respectively , we write @xmath289 for the marginal density of @xmath287 and @xmath290 for the conditional density of @xmath288 given @xmath291 . by theorem  6 of @xcite , @xmath289 is log - concave and by proposition  1 of @xcite , @xmath290 is log - concave for each @xmath292 .",
    "there is also no loss of generality in assuming @xmath15 is upper semi - continuous . since @xmath124 , we may assume without loss of generality that @xmath293 .",
    "we may therefore apply fubini s theorem and seek to maximise over all upper semi - continuous log - concave densities the quantity @xmath294 the first term on the right - hand side of  ( [ eq : fubini ] ) is maximised uniquely over all upper semi - continuous log - concave densities by setting @xmath295 .",
    "moreover , for any fixed @xmath292 , the quantity @xmath296 is maximised uniquely over upper semi - continuous log - concave densities by setting @xmath297 . since",
    "this choice does not depend on @xmath292 , it maximises the second term on the right - hand side of  ( [ eq : fubini ] ) . because both terms can be maximised simultaneously",
    ", it follows that @xmath298 , as desired .    *",
    "( b ) * write @xmath299 and @xmath300 for the covariance matrices corresponding to the probability distribution @xmath6 and the density @xmath90 respectively .",
    "the independence structure of @xmath28 and @xmath90 gives that @xmath301 $ ] and @xmath302 $ ] .",
    "here , @xmath303 and @xmath304 are @xmath305 submatrices , while @xmath306 and @xmath307 are @xmath308 submatrices .",
    "therefore , @xmath99 is of the form @xmath309 $ ] . writing @xmath113 as @xmath310 and @xmath311 respectively , where @xmath312 and @xmath313 , it follows again by fubini s theorem that @xmath314 @xmath270    of theorem  [ thm : covariance ] let @xmath124 , and let @xmath90 denote its log - concave approximation . without loss of generality",
    ", we may assume @xmath315 , so it suffices to show that if @xmath316 is the zero matrix , then @xmath6 has a log - concave density .",
    "let @xmath317 denote the distribution corresponding to @xmath90 , let @xmath318 and let @xmath319 .",
    "for an arbitrary @xmath320 , let @xmath321 and @xmath322 denote the distribution functions of @xmath323 and @xmath324 respectively , and let @xmath325 fix @xmath326 . by applying remark  2.3 of @xcite to the convex function @xmath327 and fubini s theorem , we have that @xmath328",
    "since all moments of log - concave densities are finite , we have @xmath329 .",
    "so , since @xmath330 , we must have @xmath138 .",
    "we can therefore integrate by parts as follows : @xmath331 combining  ( [ eq : cdfmarginal ] ) ,  ( [ eq : cdfmarginal2 ] ) and the fact that @xmath332 is continuous , we deduce that @xmath333 .",
    "thus @xmath334 , by the fundamental theorem of calculus and the fact that @xmath321 and @xmath322 are both right - continuous .",
    "it follows that @xmath335 since @xmath320 was arbitrary , we deduce that @xmath336 , so @xmath6 has a log - concave density . @xmath270    of proposition",
    "[ prop : convex ] suppose that the upper semi - continuous log - concave density @xmath90 is the log - concave approximation to @xmath337 .",
    "then for each @xmath338 , we see that @xmath90 also maximises @xmath339 over all upper semi - continuous log - concave densities @xmath15 on @xmath0 . @xmath270    of theorem  [ thm : test ]",
    "let @xmath340 denote the second mallows metric on @xmath341 , so @xmath342 , where the infimum is taken over all pairs @xmath178 of random vectors @xmath318 and @xmath343 on a common probability space .",
    "recall that the infimum in this definition is attained , and that if @xmath344 , then @xmath345 if and only if both @xmath346 and @xmath347 .",
    "let @xmath317 denote the distribution corresponding to the log - concave approximation to @xmath28 , and for @xmath348 to be chosen later , let @xmath349 denote the subset of @xmath341 consisting of those distributions @xmath350 with @xmath351 that have a log - concave density .",
    "fix @xmath352 and let @xmath353 .",
    "let @xmath354 and @xmath355 denote the empirical distribution of an independent sample of size @xmath11 from @xmath317 and an independent sample from @xmath350 respectively .",
    "we will require a bound for @xmath356 that holds uniformly over @xmath349 , and obtain this using the following coupling argument .",
    "we may suppose that @xmath357 are independent and identically distributed pairs with @xmath358 and @xmath359 and that @xmath354 and @xmath355 are obtained as the empirical distribution of @xmath39 and @xmath360 respectively .",
    "we may further suppose that @xmath361 ; in other words , @xmath362 and @xmath363 are coupled in such a way that they attain the infimum in the definition of the second mallows distance .",
    "using standard results on the mallows distance ( e.g. equation  ( 8.2 ) and lemma  8.7 of @xcite ) , we deduce that for @xmath364 , @xmath365 now let @xmath366 denote the distribution corresponding to the log - concave maximum likelihood estimator constructed from @xmath39 , and let @xmath367 denote the empirical distribution of a sample of size @xmath11 which , conditional on @xmath39 , is drawn independently from @xmath366 . by reducing @xmath348 if necessary",
    ", we may assume @xmath368 .",
    "it follows that @xmath369 for sufficiently large @xmath11 .",
    "the final convergence of the second term here follows from the weak law of large numbers , while for the third term it follows from proposition  2(c ) of @xcite and the dominated convergence theorem .",
    "let @xmath370 and @xmath371 denote respectively the empirical distribution and the distribution corresponding to the log - concave maximum likelihood estimator of the @xmath372th bootstrap sample @xmath163 drawn from @xmath366 .",
    "we deduce from  ( [ eq : mallows ] ) , theorem 2.15 of @xcite and another application of proposition  2(c ) of @xcite that there exists @xmath240 such that @xmath373 now let @xmath374 where @xmath167 . from  ( [ eq : mallows ] ) ,  ( [ eq : hatq ] ) , the dominated convergence theorem and the continuous mapping theorem , we have that @xmath375 as @xmath171 . on the other hand , in the notation of theorem  [ thm : asymp ] , @xmath376 where the final claim follows from theorem  [ thm : covariance ] and the fact that @xmath28 does not have a log - concave density .",
    "note that this claim holds even if @xmath377 , in which case @xmath378 .",
    "write @xmath379 , and note that @xmath380 are exchangeable ( so in particular , identically distributed ) .",
    "thus , for any @xmath158 , @xmath381 as @xmath171 .",
    "we deduce that for any given size of test @xmath158 , the power at any alternative converges to 1 .",
    "@xmath270    of theorem  [ thm : classifiers ] * ( a ) * note that @xmath382 we have that @xmath383 as @xmath171 for every @xmath203 , and in fact , by theorem  10.8 of @xcite , it is almost surely the case that @xmath204 converges to @xmath198 uniformly on compact sets in the interior of the support of @xmath198 . by the strong law of large numbers and",
    "the fact that the boundary of the support of @xmath198 has zero @xmath2-dimensional lebesgue measure , it therefore follows that @xmath384 for almost all @xmath215 .",
    "in fact , with probability one , @xmath385 converges to @xmath386 uniformly on compact sets in the interior of the support of @xmath198 .",
    "it follows immediately from this and the dominated convergence theorem that @xmath216      of proposition  [ prop : functionals ] the conclusion of theorem  [ thm : asymp ] can be stated in the notation of section  [ sec : functionals ] as @xmath387 the result therefore follows immediately by the continuous mapping theorem . @xmath270    of corollary  [ cor : linearfunctionals ] it suffices to show that under condition  ( [ eq : funccond ] ) , the functional @xmath252 is continuous .",
    "fix @xmath104 such that @xmath388 , and choose a sequence @xmath389 such that @xmath390 .",
    "then @xmath391 as @xmath171 .",
    "thus @xmath244 is continuous , as required .",
    "@xmath270                carroll , r. j. , delaigle , a. and hall , p. ( 2011 ) testing and estimating shape - constrained nonparametric density and regression in the presence of measurement error .",
    "_ j. amer .",
    "_ , * 106 * , 191202 .",
    "gopal , v. and casella , g. ( 2010 ) discussion of _ maximum likelihood estimation of a multi - dimensional log - concave density _ by m. cule , r. samworth and m. stewart _",
    "b _ , * 72 * , 580582 .            pal , j.  k. , woodroofe , m. and meyer , m. ( 2007 ) estimating a polya frequency function . in _",
    "complex datasets and inverse problems : tomography , networks and beyond_. vol .",
    "54 of _ lecture notes - monograph series _ , 239249 .",
    "ohio : institute of mathematical statistics .",
    "street , w.  n. wolberg , w.  h. and mangasarian , o.  l. ( 1993 ) nuclear feature extraction for breast tumor diagnosis . in _ proc .",
    "electronic imaging : science and technology_. vol . * 1905 * , 861870 ."
  ],
  "abstract_text": [
    "<S> we study the smoothed log - concave maximum likelihood estimator of a probability distribution on @xmath0 . </S>",
    "<S> this is a fully automatic nonparametric density estimator , obtained as a canonical smoothing of the log - concave maximum likelihood estimator . </S>",
    "<S> we demonstrate its attractive features both through an analysis of its theoretical properties and a simulation study . </S>",
    "<S> moreover , we use our methodology to develop a new test of log - concavity , and show how the estimator can be used as an intermediate stage of more involved procedures , such as constructing a classifier or estimating a functional of the density . here again , the use of these procedures can be justified both on theoretical grounds and through its finite sample performance , and we illustrate its use in a breast cancer diagnosis ( classification ) problem .    </S>",
    "<S> key words : classification ; functional estimation ; log - concave maximum likelihood estimation ; testing log - concavity ; smoothing </S>"
  ]
}