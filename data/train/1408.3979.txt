{
  "article_text": [
    "boundary regression models arise naturally in image analysis , analysis of auctions and records , or in extreme value analysis with covariates .",
    "we consider such a model where the boundary regression function @xmath1 is defined as the right endpoint of the conditional distribution of the response @xmath2 , given the covariate @xmath3 .",
    "for such a boundary regression model with multivariate random covariates and twice differentiable regression functions , hall and van keilegom ( 2009 ) establish a minimax rate for estimation of @xmath4 ( for fixed @xmath5 ) under quadratic loss and determined pointwise asymptotic distributions of an estimator which is defined as a solution of a linear optimization problem ( cf .",
    "remark [ rem : hallkeilegom ] ) .",
    "mller and wefelmeyer ( 2010 ) consider a mean regression model with ( unknown ) symmetric support of the error distribution and hlder continuous regression function .",
    "they discuss pointwise mse rates for estimators of the regression function that are defined as the average of local maxima and local minima .",
    "meister and rei ( 2013 ) consider a regression model with known bounded support of the errors .",
    "they show asymptotic equivalence in the strong lecam sense to a continuous - time poisson point process model when the error density has a jump at the endpoint of its support . for a regression model with error distribution that is one - sided and regularly varying at 0 with index @xmath6 ,",
    "jirak , meister and rei ( 2014 ) suggest an estimator for the boundary regression function which adapts simultaneously to the unknown smoothness of the regression function and to the unknown extreme value index @xmath7 .",
    "rei and selk ( 2014 ) construct efficient and unbiased estimators of linear functionals of the regression function in the case of exponentially distributed errors as well as in the limiting poisson point process experiment by meister and rei ( 2013 ) .",
    "closely related to regression estimation in models with one - sided errors is the estimation of a boundary function @xmath1 based on a sample from @xmath8 with support @xmath9\\times [ 0,\\infty]\\mid y\\leq g(x)\\}$ ] . for such models hrdle al al .",
    "( 1995 ) and hall et al .",
    "( 1998 ) proved minimax rates both for @xmath4 and for the @xmath10-distance between @xmath1 and its estimator .",
    "moreover , they showed that an approach using local polynomial approximations of @xmath1 yields this optimal rate .",
    "explicit estimators in terms of higher order moments were proposed and analyzed by girard and jacob ( 2008 ) and girard et al .",
    "( 2013 ) . in this",
    "setting , there is also an extensive literature on the estimation of monotonically increasing boundary functions @xmath1 , which naturally arise in production frontier models ; see gijbels et al .",
    "( 1999 ) and the literature cited therein .",
    "the aim of the paper is inference on the error distribution in regression models with one - sided errors . to this end",
    "we need uniform rates of convergence for the regression estimator . to obtain those rates for general smoothness order @xmath11 of the regression function and extreme value index @xmath12 of the error distribution",
    "is an end of its own . to the authors knowledge no such uniform rates are available so far in the literature .",
    "the results can also be applied to mean regression models with bounded symmetric error distribution . for regression functions",
    "@xmath1 in a hlder class of order @xmath13 we obtain the rate @xmath14 , where @xmath7 denotes the extreme value index of the error distribution .",
    "thus for @xmath15 the rate is faster than the typical rate one has in mean regression models with regular errors . for pointwise and @xmath16-rates of convergence",
    "it has been known in the literature that faster rates are possible for nonparametric regression estimation in models with irregular error distribution , see e.g.gijbels and peng ( 2000 ) , hall and van keilegom ( 2009 ) , or mller and wefelmeyer ( 2010 ) .",
    "the uniform rate of convergence for the regression estimator enables us to derive asymptotic expansions for residual - based empirical distribution functions and weak convergence of the residual - based empirical distribution function .",
    "we state conditions under which the influence of the regression estimation is negligible such that the same results are obtained as in the iid - case ( i.e.  in the case of observable errors ) .",
    "we apply those results to derive goodness - of - fit tests for parametric classes of error distributions .",
    "asymptotic properties of residual empirical distribution functions in mean regression models with regular errors were investigated by akritas and van keilegom ( 2001 ) , among others . here in contrast to our results the regression estimation vastly influences the asymptotic behavior of the empirical distribution function .",
    "as a consequence asymptotic distributions of goodness - of - fit test statistics are complicated and typically bootstrap is applied , see neumeyer , dette and nagel ( 2006 ) .",
    "the remainder of the article is organized as follows . in section 2 the regression model under consideration",
    "is presented and model assumptions are formulated .",
    "the regression estimator is defined and uniform rates of convergence are given .",
    "a smooth modification of the estimator is considered and uniform rates of convergence for this estimator as well as its derivative are shown . in section 3 residual based empirical distribution functions based on both regression estimators",
    "are investigated .",
    "conditions are stated under which the influence of regression estimation is asymptotically @xmath0-negligible .",
    "further an expansion of the residual empirical distribution function is shown that is valid under more general conditions .",
    "goodness - of - fit tests are discussed in general and in some detailed examples .",
    "a small simulation study shows the small sample performance of the tests .",
    "all proofs are given in the appendix .",
    "we consider a regression model with fixed equidistant design and one - sided errors , @xmath17 under the following assumptions .    1",
    ".   [ f1 ] the errors @xmath18 are independent and identically distributed and supported on @xmath19 $ ] .",
    "the error distribution function fulfills @xmath20 for some @xmath6 , with @xmath21 for @xmath22 .    1",
    ".   [ g1 ] the regression function @xmath1 belongs to some hlder class of order @xmath11 , i.e.  @xmath1 is @xmath23-times differentiable on @xmath24 $ ] and the @xmath23-th derivative satisfies @xmath25\\atop t\\neq x}\\frac{|g^{(\\lfloor \\beta\\rfloor)}(t)-g^{(\\lfloor \\beta\\rfloor)}(x)|}{|t - x|^{\\beta-\\lfloor \\beta\\rfloor}}<\\infty.\\ ] ]    for the estimator of the regression function we need the following assumption .    1 .   [ h1 ]",
    "let @xmath26 be a sequence of positive bandwidths that satisfies @xmath27 and @xmath28 .",
    "we consider an estimator that locally approximates the regression function by a polynomial while lying above the data points . more specifically , for @xmath29 $ ] let @xmath30 where @xmath31 is a polynomial of order @xmath32 and minimizes the local integral @xmath33 under the constraints @xmath34 for all @xmath35 such that @xmath36 .",
    "we obtain the following uniform rates of convergence .",
    "[ theo2b ] under model ( [ model ] ) and assumptions [ f1 ] , [ g1 ] , [ h1 ] we have @xmath37}|\\hat g(x)-g(x)|=o(h_n^\\beta)+o_p\\big(\\big(\\frac{|\\log h_n|}{nh_n}\\big)^{1/\\alpha}\\big).\\ ] ]    note that the deterministic rate @xmath38 stems from approximating the regression function by a polynomial , whereas the random rate originates from the observational error .",
    "balancing the two convergence rates by setting @xmath39 gives @xmath40}|\\hat g(x)-g(x)|=o_p\\big(\\big(\\frac{\\log n}{n}\\big)^{\\frac{\\beta}{\\alpha\\beta+1}}\\big).\\ ] ] this result is of particular interest in the case of irregular error distributions in the sense that large mass in concentrated in a neighborhood of the right endpoint , i.e.  @xmath15 .",
    "then the rate improves upon the typical optimal rate @xmath41 for estimating mean regression functions in models with regular errors .",
    "jirak , meister and rei ( 2014 ) consider a similar boundary regression estimator while replacing the integral in ( [ hat - g ] ) by its riemann approximation @xmath42 .",
    "for this modified estimator we obtain the same uniform rate of convergence as in theorem [ theo2b ] by replacing proposition [ prop : estbound ] in the proof of theorem [ theo2b ] by theorem 3.1 in jirak , meister and rei ( 2014 ) .",
    "@xmath43    for hlder continuous regression functions with exponent @xmath44 $ ] the estimator reduces to a local maximum , i.e.@xmath45 . in this case",
    "we obtain the uniform rate of convergence as given in theorem [ theo2b ] in the whole design interval . @xmath43    [ mw][mw - lin]mller and wefelmeyer ( 2010 ) consider a mean regression model @xmath46 , @xmath47 with symmetric error distribution supported on @xmath48 $ ] ( with @xmath49 unknown ) .",
    "the error distribution function fulfills @xmath50 for @xmath51 .",
    "the local empirical midrange of the responses , i.e.@xmath52 is shown to have pointwise rate of convergence @xmath53 to @xmath54 if @xmath54 is hlder continuous with exponent @xmath44 $ ] .",
    "theorem [ theo2b ] allows us to extend mller and wefelmeyer s ( 2010 ) results to the general hlder case ( @xmath55 ) for uniform rates of convergence ( in a model with fixed design @xmath56 ) . to this end",
    "we use the mean regression estimator @xmath57 with @xmath58 as before and @xmath59 defined analogously , but based on @xmath60 , @xmath61 .",
    "@xmath43    [ rem : hallkeilegom ] in the case @xmath62 $ ] hall and van keilegom ( 2009 ) consider the following local linear boundary regression estimator , @xmath63 note that due to @xmath64 this estimator coincides with @xmath58 for @xmath62 $ ] .",
    "however , in the case @xmath65 replacing the linear function in ( [ g - lin ] ) by a polynomial of order @xmath32 renders the estimator @xmath66 useless .",
    "one obtains @xmath67 for @xmath68 while @xmath69 , @xmath70 .",
    "this was already observed by jirak , meister and rei ( 2014 ) .",
    "@xmath43    note that the estimator @xmath58 is not smooth .",
    "one might prefer to consider a smooth estimator by convoluting @xmath58 with a kernel .",
    "such a modified estimator will also be advantageous when deriving an expansion for the residual based empirical distribution function in the next section .",
    "therefore we define @xmath71 and formulate some additional assumptions .    1 .   [ k1 ]",
    "@xmath72 is a kernel with support @xmath73 $ ] and order @xmath74 , i.e.  @xmath75 @xmath76 .",
    "further , @xmath77 and @xmath72 is differentiable with lipschitz - continuous derivative @xmath78 .    1 .",
    "[ b1 ] the sequence @xmath79 of positive bandwidths satisfies @xmath80 .",
    "[ b2 ] there exists some @xmath81 such that either @xmath82 or @xmath83    the estimator @xmath84 is differentiable and we obtain the following uniform rates of convergence for @xmath84 and its derivative @xmath85 .",
    "[ theo - smooth1 ] let model ( [ model ] ) and the assumptions [ f1 ] , [ g1 ] with @xmath86 , [ h1 ] , [ k1 ] , and [ b1 ] be fulfilled .",
    "then for @xmath87 $ ] it holds that    * @xmath88 * @xmath89 , + where the last equality holds under the additional assumption [ b2 ] * @xmath90 under the additional assumption [ b2 ] .",
    "in this section we consider estimators for the error distribution in model ( [ model ] ) .",
    "for the asymptotic analysis we need an additional assumption on the error distribution .    1 .   [ f2 ] the error distribution @xmath91 is hlder continuous of order @xmath92 .",
    "we build residuals @xmath93 , and define the following modified empirical distribution function , @xmath94 where @xmath95 .",
    "we first treat a simple case where the influence of the regression estimation on the residual empirical process is negligible . to this end",
    "let @xmath96 denote the standard empirical distribution function of the unobservable errors @xmath18 .",
    "then we have the following asymptotic result .    [ theo3a ]",
    "let model ( [ model ] ) with assumptions [ f1 ] , [ g1 ] , and [ f2 ] be fulfilled .",
    "let further @xmath97 and choose @xmath98 .",
    "then we have @xmath99 thus the process",
    "@xmath100 converges weakly to a centered gaussian process with covariance function @xmath101 .",
    "the proof applies the uniform convergence rate from theorem [ theo2b ] and is given in the appendix . note that the condition @xmath97 is fulfilled for all cases of irregularity @xmath102 subject to a sufficiently smooth regression function .",
    "it is needed in the proof of theorem [ theo3a ] in order to assure that for the rate obtained in ( [ opt - rate ] ) , say @xmath103 , one has @xmath104    from theorem [ theo3a ] it follows that under the condition @xmath105 the estimation of the regression function has no impact on the estimation of the irregular error distribution .",
    "this is remarkably different from corresponding results on error distribution estimation in mean regression models with regular error distributions . here",
    "the empirical distribution function of residuals , say @xmath106 , is not asymptotically @xmath0-equivalent to the empirical distribution function of true errors , and @xmath107 converges to a gaussian process with complicated covariance structure , compare to theorem 1 in akritas and van keilegom ( 2001 ) .",
    "in the simple case of a mean regression model with equidistant design and an error distribution @xmath91 with bounded density @xmath108 one has @xmath109 uniformly with respect to @xmath110 when the regression function is estimated by a local polynomial estimator , under appropriate bandwidth conditions ( see proposition 3 in neumeyer and van keilegom ( 2009 ) ) .",
    "@xmath43    in order to obtain asymptotic results for error distribution estimators beyond the simple case @xmath97 a finer analysis is needed .",
    "we will use the smooth regression estimator @xmath111 defined in ( [ tilde - g ] ) in what follows .",
    "let @xmath112 denote the empirical distribution function based on residuals @xmath113 , i.e.@xmath114 where @xmath87 $ ] and @xmath115 .",
    "then the following asymptotic expansion is valid .",
    "[ theo - smooth2 ] let model [ model ] and assumptions [ f1 ] , [ g1 ] , [ h1 ] , [ k1 ] , [ b1 ] , and [ b2 ] be fulfilled",
    ". then @xmath116 uniformly with respect to @xmath117 , if [ f2 ] holds and @xmath118 in assumption [ b2 ] is chosen such that @xmath119 .    for each @xmath120",
    "the expansion for @xmath121 holds uniformly with respect to @xmath122 $ ] without the latter additional conditions on @xmath91 and @xmath118 if @xmath91 has a bounded density on @xmath123 $ ] .    in order for the expansion to be valid on the whole real line we need for @xmath62 $ ] that @xmath124 ( see assumption [ b2 ] ) , which implies @xmath125 . for @xmath126",
    "one obtains the only constraint @xmath127 .",
    "next we examine under which conditions the additional term in depending on the estimation error is asymptotically negligible .",
    "we focus on those arguments @xmath128 which are bounded away from 0 , because in this setting weaker conditions on @xmath7 and @xmath13 are needed .",
    "moreover , for the analysis of the tail behavior of the error distribution at 0 , tail empirical processes are better suited .",
    "note that the estimator @xmath58 tends to underestimate the true function because it is defined via a polynomial which is minimal under the constraint that it lies above all observations @xmath129 , which in turn all lie below the true boundary function . as",
    "this systematic underestimation does not vanish from ( local or global ) averaging , we first have to introduce a bias correction .",
    "let @xmath130 denote the expectation if the true regression function is identical 0 .",
    "for the remaining part of this section , we assume that @xmath131 is known or that it can be estimated sufficiently accurately .",
    "for example , if the empirical process of residuals shall be used to test a simple null hypothesis , then one may calculate or simulate this expectation under the given null distribution .",
    "we define a bias corrected version of the smoothed estimator by @xmath132 for @xmath133 .",
    "the following lemma ensures that the above results for @xmath84 carry over to this variant if the following condition on the lower tail of @xmath91 holds :    1 .",
    "[ f3 ] there exists @xmath134 such that @xmath135 as @xmath136 .",
    "[ lem : expectg0 ] if model [ model ] holds with @xmath1 identical 0 and the conditions [ f1 ] , [ f3 ] , [ g1 ] , and [ h1 ] are fulfilled , then for all @xmath137 $ ] @xmath138    we need some additional conditions on the rates at which the bandwidths @xmath139 and @xmath140 tend to 0 :    1 .",
    "[ h2 ] @xmath141    1 .",
    "[ b3 ] @xmath142    in particular , these assumptions ensure that the bias terms of order @xmath143 are of smaller order than @xmath144 and @xmath145 and hence asymptotically negligible , and that quadratic terms in the estimation error are uniformly negligible , that is , @xmath146 .",
    "[ theo : remainderterm ] suppose model [ model ] holds and assumptions [ f1 ] , [ f3 ] , [ g1 ] , [ h1 ] , [ h2 ] , [ k1 ] , [ b1 ] , [ b2 ] , and [ b3 ] and @xmath91 has a bounded density on @xmath123 $ ] for some @xmath120 .",
    "then @xmath147}\\bigg|\\frac 1{m_n}\\sjn \\left(f\\left(y+({{\\tilde g_n^*}}-g)(\\textstyle{\\frac jn})\\right)-f(y)\\right)i\\{{\\textstyle{\\frac jn}}\\in i_n\\}\\bigg|=o_p(n^{-1/2}).\\ ] ]    the conditions on @xmath139 and @xmath140 used in theorem [ theo : remainderterm ] can be fulfilled if and only if @xmath148 .",
    "in particular , this theorem is applicable if @xmath149 and the error distribution is irregular , i.e. , @xmath150 .",
    "then the empirical process of the residuals ( restricted to @xmath123 $ ] ) is asymptotically equivalent to the empirical process of the errors .",
    "let @xmath151 denote a parametric class of error distributions such that for each @xmath152 , @xmath153 with @xmath154 for @xmath22 .",
    "our aim is to test the null hypothesis @xmath155 we assume that @xmath156 for all @xmath152 , such that under @xmath157 theorem [ theo3a ] is valid .",
    "let @xmath158 denote an estimator for @xmath159 based on residuals @xmath93 , @xmath61 .",
    "the goodness - of - fit test is based on the empirical process @xmath160 under any fixed alternative @xmath58 still consistently estimates @xmath1 such that @xmath161 estimates the error distribution @xmath91 . if @xmath158 even under the alternative converges to some @xmath162 , then",
    "a consistent hypothesis test can be obtained by rejecting @xmath157 for large values of , e.g. , a kolmogorov - smirnov test statistic @xmath163 .",
    "note that under @xmath157 from theorem [ theo3a ] it follows that @xmath164 where @xmath159 denotes the true parameter .",
    "we consider some examples .",
    "[ ex - uniform ] consider the mean regression model @xmath165 , @xmath47 with symmetric error distribution @xmath91 as in remark [ mw - lin ] .",
    "assume @xmath166 and let @xmath167 be defined as in the aforementioned remark .",
    "our aim is to test the null hypothesis @xmath168 , where @xmath169 denote the distribution function of the uniform distribution on @xmath170 $ ] ( with @xmath171 for all @xmath172 ) .",
    "let residuals be defined as @xmath173 , @xmath61 , and let @xmath174 then @xmath175 can be bounded by @xmath176}|\\hat m(x)-m(x)|$ ] and thus is of rate @xmath177 . from the definition @xmath178}(y)+i_{(\\vartheta,\\infty)}(y)$ ] one straightforwardly obtains @xmath179 uniformly with respect to @xmath110 .",
    "thus the process @xmath180 converges weakly to a brownian bridge @xmath181 composed with @xmath91 .",
    "the kolmogorov - smirnov test statistic @xmath163 converges in distribution to @xmath182}|b(t)|$ ] .",
    "thus although our testing problem requires the estimation of a nonparametric function and we have a composite null hypothesis , the same asymptotic distribution appears as in the kolmogorov - smirnov test for the simple hypothesis @xmath183 based on an iid sample with distribution @xmath91 . @xmath43",
    "regard the regression model @xmath184 , @xmath185 with regression function @xmath1 hlder of order @xmath186 and error distribution function @xmath91 .",
    "we want to test the null hypothesis @xmath168 , where @xmath187 denotes the distribution function of the mirrored exponential distribution .",
    "then @xmath188 for @xmath22 , i.e.  @xmath171 for all @xmath189 .",
    "+ define @xmath190 and note that & & _ n- + & = & _ n(1m_n(_j+1)i\\{h_n < jn1-h_n}+1m_n(g(jn)-g(jn))i\\{h_n < jn1-h_n } ) with the central limit theorem and theorem [ theo2b ] one can deduce _",
    "n-&=&^21m_n(_j+1)i\\{h_n < jn1-h_n}+o_p_(1n ) and thus for @xmath191 and some @xmath192 between @xmath193 and @xmath159 , f__n(y)&=&e^_ny= e^y+e^yy(_n-)+12e^_n , yy^2(_n-)^2 + & = & f_(y)+e^yy^21m_n(_j+1)i\\{h_n < jn1-h_n}+o_p_(1n ) uniformly in @xmath128 .",
    "now analogously to the proof of theorem 19.23 in van der vaart ( 2000 ) we can deduce weak convergence of @xmath194 to a gaussian process with covariance function @xmath195 , where the covariance function follows by simple calculations and the fact that @xmath196=ye^{\\vartheta y}$ ] . the asymptotic distribution of the cramr - von - mises test statistic @xmath197 is distribution free and we can use tabled quantiles for the test , see e.g.  stephens ( 1976 ) .",
    "@xmath43      to study the small sample performance of our goodness - of - fit test we investigate its behaviour on simulated data according to example [ ex - uniform ] .",
    "the observations are generated with @xmath198 and errors @xmath199 that are distributed according to the density function @xmath200 for different values of @xmath201 .",
    "note that for @xmath202 the null hypothesis @xmath203 $ ] holds whereas for @xmath204 the test should reject the null hypothesis . in figure [ fig - uniform ] the results for different sample sizes and level @xmath205 are displayed .",
    "they are based on @xmath206 monte carlo repetitions in each case and calculated with the cramr - von - mises test statistic .",
    "the bandwidth is chosen as @xmath207 ( this is the optimal bandwidth for @xmath208 and @xmath209 apart from the log - term ) and the regression function is estimated by the local linear approach . it can be seen that the asymptotic level is approximated well and and the power increases with increasing @xmath210 as well as with increasing @xmath211 .",
    "+ to understand the influence of the bandwidth we simulate the same model with @xmath212 for different @xmath213 from @xmath214 to @xmath215 .",
    "the results are similar to those displayed in figure [ fig - uniform ] for @xmath213 between @xmath216 and @xmath217 but for ca .",
    "@xmath218 the power and the size get worse especially for the small sample sizes @xmath219 and @xmath220 .    ]",
    "the following proposition can be verified by an obvious modification of the proof of theorem 3.1 by jirak , meister and rei ( 2014 ) .    [ prop : estbound ] assume that model ( 2.1 ) holds and that the regression function @xmath1 fulfills condition [ g1 ] for some @xmath221 $ ] and some @xmath222 $ ] .",
    "then there exist constants @xmath223 and a natural number @xmath224 ( depending only on the respective subscripts ) such that @xmath225    [ lem1 ] under assumptions [ f1 ] and [ h1 ] for any fixed set @xmath226 of disjoint non - degenerate subintervals of @xmath73 $ ] we have @xmath227}\\max_{1\\le j\\le m } \\min_{i\\in\\{1,\\dots , n\\}\\atop ( i / n - x)/h_n\\in i_j } ( { -\\eps_i})=o_p\\big(\\big(\\frac{|\\log h_n|}{nh_n}\\big)^{1/\\alpha}\\big).\\ ] ]    * proof . *",
    "let @xmath228 .",
    "obviously it suffices to prove that for all non - degenerate subintervals @xmath229 $ ] there exists a constant @xmath230 such that @xmath231 } \\min_{i\\in\\{1,\\ldots , n\\ } \\atop ( i / n - x)/h_n\\in i } |\\eps_i|>l b_n\\big\\ } = 0.\\ ] ] denote by @xmath232 the diameter of @xmath233 and let @xmath234 and @xmath235 .",
    "then for all @xmath236 @xmath237 } \\min_{i\\in\\{1,\\ldots , n\\ } \\atop ( i / n - x)/h_n\\in i } |\\eps_i|>x\\big\\ }    & \\le & p\\big\\ { \\max_{j\\in\\{1,\\ldots , n - d_n\\ } } \\min_{i\\in\\{j,\\ldots , j+d_n\\ } } |\\eps_i|>x\\big\\ } \\\\    & \\le & p\\big\\ { \\max_{l\\in\\{0,\\ldots , l_n\\ } \\atop l \\text { even } } m_{n , l}>x\\big\\ } + p\\big\\ { \\max_{l\\in\\{0,\\ldots , l_n\\ } \\atop l \\text { odd } } m_{n , l}>x\\big\\}\\end{aligned}\\ ] ] with @xmath238 since the random variables @xmath239 for @xmath240 even are iid , we have @xmath241 and an analogous equation holds for the maxima over the odd numbered block maxima @xmath239 .",
    "let @xmath242 be the cdf of @xmath243 .",
    "if @xmath244 exceeds @xmath5 , then there is a smallest index @xmath245 for which @xmath246 . hence @xmath247 to sum up , we have shown that @xmath248 } \\min_{i\\in\\{1,\\ldots , n\\ } \\atop ( i / n - x)/h_n\\in i } |\\eps_i|>lb_n\\big\\ }   \\le 2\\bigg(1-\\big(1-(1+d_ng(lb_n))(1-g(lb_n))^{d_n}\\big)^{{\\lfloor l_n/2 \\rfloor}+1}\\bigg)\\ ] ] it remains to show that the right hand side tends to 0 for sufficiently large @xmath230 which is true if and only if @xmath249 this is an immediate consequence of @xmath250 and @xmath251 if @xmath252 .",
    "@xmath253      the assertion directly follows from proposition [ prop : estbound ] and lemma [ lem1 ] .",
    "@xmath253      \\(i ) one obtains straightforwardly _ xi_n|g(x)-g(x)| & & _ xi_n|_h_n^1-h_n(g(z)-g(z))1b_nk()dz| + & & + _",
    "xi_n|_h_n^1-h_n(g(z)-g(x))1b_nk()dz| + & & _",
    "z|(z)-g(z)|o(1)+_xi_n|_-1 ^ 1(g(x - ub_n)-g(x))k(u)du| + & & o(h_n^)+o_p(()^1 ) + & & + b_n^_xi_n|1!_-1 ^ 1u^(g^()(x - ub_n)-g^()(x))k(u)du| + with application of theorem [ theo2b ] , a taylor expansion of @xmath1 of order @xmath254 and assumption [ k1 ] . now an application of assumption [ g1 ] on the last term yields the desired result .",
    "\\(ii ) since @xmath1 is bounded on @xmath255 $ ] and @xmath256}|\\hat g(x)-g(x)|=o_p(1)$ ] , we can suppose that @xmath58 is bounded on @xmath255 $ ] too .",
    "from the equicontinuity of @xmath78 it follows that @xmath257 is equicontinuous in @xmath5 for @xmath258 $ ] and we can exchange integration and differentiation and obtain @xmath259 integration by parts yields _ h_n^1-h_ng(z)1b_n^2k()dz & = & _ h_n^1-h_ng(z)1b_nk()dz since @xmath77 and therefore _ xi_n|(x)-g(x)|&&_xi_n|_h_n^1-h_n(g(z)-g(z))1b_n^2k()dz| + & & + _",
    "xi_n|_h_n^1-h_n(g(z)-g(x))1b_nk()dz| + & & _",
    "z|(z)-g(z)|o(1b_n)+_xi_n|_-1 ^ 1(g(x - ub_n)-g(x))k(u)du| the assertion follows by theorem [ theo2b ] , a taylor expansion of @xmath260 of order @xmath261 and the assumptions [ k1 ] and [ g1 ] .",
    "\\(iii ) we distinguish between the cases @xmath262 and @xmath263 for some suitable sequence @xmath264 with @xmath265 discussed later .",
    "for the first case we obtain @xmath266 for the second case we use a decomposition like in the proof of ( ii ) & & _ x , yi_n,0<|x- y|a_n + & & _ x , yi_n,0<|x- y|a_n + & & + _ x , yi_n0<|x- y|a_n + _ x , yi_n0<|x - y|a_n . by lipschitz continuity of @xmath78 and theorem [ theo2b ] the first term on the right hand side is of rate @xmath267 for the second term one obtains the rate @xmath268 by differentiability of @xmath260 in the case @xmath126 . in the case @xmath269 by assumption",
    "[ g1 ] for the same term one obtains the rate @xmath270 which is @xmath271 by assumption [ b2 ] .",
    "+ the last term on the right hand side can be reformulated as @xmath272 and is thus of the same order as the second term by assumption [ g1 ] .",
    "to finish the proof one needs a sequence @xmath273 such that the remaining rates ( [ iii-1 ] ) and ( [ iii-2 ] ) are of negligible order @xmath274 . using the notation",
    "@xmath275 the existence of a sequence @xmath264 such that @xmath276 is equivalent to @xmath277 which in turn is equivalent to the bandwidth condition in assumption [ b2 ] .",
    "@xmath253      by choice of the bandwidth sequence @xmath278 equation ( [ opt - rate ] ) holds .",
    "let @xmath279 , then , by assumption , @xmath280 from this and ( [ opt - rate ] ) one can deduce the existence of some sequence @xmath281 with @xmath282 such that @xmath283}|\\hat g(x)-g(x)|\\leq a_n\\big ) { \\xrightarrow[n\\to\\infty]{}}1.\\ ] ] since _ n(y)&=&1m_ni\\{_jy+(-g)(jn)}i\\{h_n",
    "< jn1-h_n } this implies that with probability converging to one @xmath284 where we define @xmath285 .",
    "we take a closer look at the bounds . to this end",
    "let @xmath286 , then the sequential empirical process @xmath287}$ ] converges weakly to a kiefer process , see e.g.  theorem 2.12.1 in van der vaart and wellner ( 1996 ) .",
    "now uniformly with respect to @xmath117 ( since @xmath288 ) & & n(|f_n(ya_n)-f_n(y ) ) + & = & ( e_n(ya_n,1-h_n)-e_n(y,1-h_n)-e_n(ya_n , h_n)+e_n(y , h_n ) ) + & & + n(f(ya_n)-f(y))+n(|f_n(y)-f_n(y ) ) + & = & o_p(1)+n(|f_n(y)-f_n(y ) ) since @xmath289 , the process @xmath290 is asymptotically equicontinuous , assumption [ f2 ] and ( [ a_n - alpha ] ) it remains to show that @xmath291 uniformly in @xmath117 .",
    "it holds that |f_n(y)-f_n(y)&= & ( i\\{_jy}-f(y))i\\{h_n < jn1-h_n}-(i\\{_jy}-f(y ) ) + & = & ( - ) ( e_n(y,1-h_n)-e_n(y , h_n ) ) + & & - ( e_n(y , h_n)+e_n(y,1)-e_n(y,1-h_n ) ) + & = & o ( ) ( e_n(y,1)+o_p(1))+o_p ( ) because @xmath292 , @xmath293 and the process @xmath290 is asymptotically equicontinuous . since the limit of the standard empirical process @xmath294 is tight we obtain @xmath295 uniformly with respect to @xmath117 and , hence , the assertion of the theorem .",
    "@xmath253      define for any interval @xmath296 and constant @xmath297 the following class of differentiable functions , @xmath298 then with theorem [ theo - smooth1 ] ( i ) - ( iii ) we obtain @xmath299 for @xmath300 . from this",
    "we can deduce the existence of some sequence of random functions @xmath301\\to\\er$ ] with @xmath302 for all @xmath133 and @xmath303)\\right)\\to 1 $ ] for @xmath300 .    to prove the asserted expansion we are going to apply theorem 2.11.9 in van der vaart and wellner ( 1996 ) to the process @xmath304),\\quad\\varphi=(y",
    ", d)\\in\\cf,\\ ] ] where @xmath305)\\}$ ] , equipped with the semimetric @xmath306 defined by @xmath307}\\sup_{\\gamma\\in c_1^{1+\\delta}([0,1])}\\left|f(y+\\gamma(x))-f(y'+\\gamma(x))\\right|,\\sup_{x\\in [ 0,1]}|d(x)-d'(x)|\\},\\ ] ] and @xmath308 we will show at the end of the proof that @xmath309 is a totally bounded semimetric space . in the following we check the assumptions in the theorem 2.11.9 in van der vaart and wellner ( 1996 ) .",
    "the proof is analogous to the proof of lemma 3 in neumeyer and van keilegom ( 2009 ) ( see the online supporting information to that article ) .",
    "the first condition holds since @xmath310 and hence @xmath311 s.t .  for all",
    "@xmath312 en(+1n)i\\{+1n>}=0 .",
    "further note that n(z_nj(y , d)-z_nj(y,d))^2&=&((i\\{_jy+d(jn)}-i\\{_jy+d(jn)})i\\{jni_n } + & & -(i\\{_jy}-i\\{_jy } ) + & & + ( i\\{_jy+d(jn)}-i\\{_jy+d(jn)})i\\{jni_n})^2 . now with @xmath313 and @xmath314 we have & & _ ,  ,  ( , )<e + & & 4m_n _ ,  ,  ( , )<nm_nei\\{jni_n } + & & + 4n _ ,  ,  ( , )<e + & & + 4m_n _ ,  ,  ( , )<nm_nei\\{jni_n } + & = & 4m_n _ ,  ,  ( , )<nm_n|f(y+d(jn))-f(y+d(jn))|i\\{jni_n } + & & + 4 _ ,  ,  ( , )<|f(y)-f(y)| + & & + 4m_n _ ,  ,  ( , )<nm_n|f(y+d(jn))-f(y+d(jn))|i\\{jni_n } + & & 4nm_n_, ,  ( , )<_x_dc_1 ^ 1+([0,1])|f(y+d(x))-f(y+d(x))| + & & + 4_, ,  ( , )<|f(y)-f(y)| + & & + 4nm_n_, ,  ( , )<_x|f(y+d(x))-f(y+d(x))| + & = & o()+o(^1)= o(1 ) by the definition of @xmath306 since @xmath289 and by assumption [ f2 ] .",
    "this is the second condition in the aforementioned theorem .",
    "the validity of the third assumption can be proved in the same way as in the proof of lemma 3 in neumeyer and van keilegom ( 2009 ) ( see their online supporting information ) .",
    "we just have to replace one step since we do not assume a bounded error density . by assumption [ f2 ]",
    "it holds that @xmath315}\\left|f(y+d_m^u(x))-f(y+d_m^l(x))\\right|\\leq \\sup_{x\\in [ 0,1]}c_f\\left|d_m^u(x)-d_m^l(x)\\right|^{\\alpha\\wedge 1}\\ ] ] where @xmath316 is the hlder constant of @xmath91",
    ". therefore with brackets that fulfill + @xmath317}\\left|d_m^u(x)-d_m^l(x)\\right|\\leq\\eta^{2(\\alpha^{-1}\\vee 1)}$ ] we get @xmath318}\\left|f(y+d_m^u(x))-f(y+d_m^l(x))\\right|=o(\\eta^2).\\ ] ] with this the bracketing number @xmath319}(\\eta,\\cf , l_n^2)$ ] is of the order @xmath320 and therefore , if @xmath119 , _ 0^",
    "d&=&o(_0^d)+o(_0^d ) + 0  0 , which is the third condition in theorem 2.11.9 in van der vaart and wellner ( 1996 ) .",
    "now weak convergence of the process @xmath321 follows from its marginal convergence , which can be shown easily using cramr - wold s device and lindeberg s condition .",
    "therefore @xmath321 is also asymptotically equicontinuous , that is @xmath322 for all @xmath323 . with @xmath324 and @xmath325",
    "we get & & _",
    "y|((i\\{_jy+d_n(jn)}-f(y+d_n(jn)))i\\{jni_n}-1n(i\\{_jy}-f(y ) ) ) + & & -((i\\{_jy}-f(y))i\\{jni_n}-1n(i\\{_jy}-f(y)))|=o_p(1 ) which together with |((i\\{_jy}-f(y))i\\{jni_n}-1n(i\\{_jy}-f(y)))|=o_p(1 ) and the fact that @xmath302 @xmath326 implies our assertion .    it remains to show that @xmath309 is a totally bounded semimetric space .",
    "this can be proved in the same way as in the proof of lemma 3 in neumeyer and van keilegom ( 2009 ) with one small change : we again replace the bounds @xmath327 by @xmath328 .",
    "this may change the bracketing number @xmath319}(\\eta,\\cf,\\rho)$ ] , but it is still finite for all @xmath329 , which implies that @xmath330 is totally bounded .",
    "note that if we restrict to @xmath122 $ ] for some @xmath120 and assume a bounded error density on @xmath123 $ ] , instead of the inequality ( [ + + ] ) one applies the mean value theorem .",
    "consequently the bracketing number in ( [ brack ] ) changes to @xmath331 and for finiteness of the bracketing integral no additional assumption on @xmath332 is needed .",
    "@xmath253    in the remaining proofs ,",
    "we use the index @xmath211 for the estimators to emphasis the dependence on the sample size and to distinguish between estimators and polynomials corresponding to a given sample on the one hand and corresponding objects in a limiting setting on the other hand .",
    "proposition [ prop : estbound ] and the proof of lemma [ lem1 ] show that there exist constants @xmath333 depending only on @xmath13 and @xmath334 such that @xmath335 and @xmath336 for all @xmath337 .",
    "let @xmath338 for a suitable constant @xmath339 and fix some @xmath340 such that @xmath341 for all @xmath342 $ ] . then @xmath343 now @xmath344 for all @xmath345 if @xmath49 is chosen sufficiently large .",
    "hence the assertion follows from @xmath346 for all @xmath345 .",
    "@xmath253      as the density @xmath108 is bounded and lipshitz continuous , one has @xmath347 uniformly for @xmath348 .",
    "hence the remainder term can be approximated by a sum of estimation errors as follows : @xmath349 where for the last conclusions we have used theorem [ theo2b ] , lemma [ lem : expectg0 ] and the assumptions [ h2 ] and [ b3 ] .",
    "thus the assertion follows if we show that @xmath350    to this end , note that @xmath351 and @xmath352 are independent for @xmath353 . for simplicity , we assume that @xmath354 is a natural number .",
    "if we split the whole sum into blocks with @xmath355 consecutive summands , then all blocks with odd numbers are independent and all blocks with even numbers are independent .",
    "it suffices to show that @xmath356 where @xmath357 , @xmath358 .",
    "we only consider the second sum , because the first convergence obviously follows by the same arguments .",
    "it suffices to verify the following two conditions .",
    "@xmath359 uniformly for all @xmath360 . for",
    "then @xmath361 which implies the assertion",
    ".    to prove , note that according to lemma [ eq : wlln_cond1 ] , proposition [ prop : estbound ] , and the proofs of lemma [ lem1 ] and of theorem [ theo - smooth1](i ) , there exist constants @xmath362 ( depending only on @xmath13 , @xmath334 and the kernel @xmath72 ) such that @xmath363 where @xmath364 are independent random variables such that @xmath365 with @xmath366 because @xmath367 by [ h2 ] and [ b3 ] , it suffices to show that @xmath368    fix some @xmath369 such that @xmath341 for all @xmath342 $ ] . in what follows , @xmath213 denotes a generic constant ( depending only on @xmath370 and @xmath72 ) which may vary from line to line .",
    "applying the inequality @xmath371 , which holds for all @xmath372 and @xmath373 , twice , we obtain for @xmath374 @xmath375^{c_2(h_n+b_n)/h_n}\\\\      & \\le & 1-\\exp\\big(-c n(h_n+b_n)t^{\\alpha/2}\\exp\\big(-c_3c_fnh_nt^{\\alpha/2}/2\\big)\\big)\\\\      & \\le & c n(h_n+b_n)t^{\\alpha/2}\\exp\\big(-c_3c_fnh_nt^{\\alpha/2}/2\\big ) .",
    "\\end{aligned}\\ ] ] therefore , for sufficiently large @xmath376 , @xmath377 where in the last but one step we apply the conditions [ b3 ] and [ h2 ] .",
    "now , assertion ( and hence ) follows from @xmath378^{c_2(h_n+b_n)/h_n}\\ , dt\\\\      & \\le & \\int_{t_0 ^ 2}^\\infty 1-\\exp\\big(-cn(h_n+b_n ) ( f(-t^{1/2}))^{c_3nh_n}\\big)\\ , dt\\\\      & \\le & c n(h_n+b_n ) \\big(nh_n(f(-t_0))^{c_3nh_n}+\\int_{nh_n}^\\infty t^{-\\tau c_3 nh_n } \\ , dt\\big)\\\\      & = & o(n^{-\\xi } )    \\end{aligned}\\ ] ] for all @xmath345 where we have used [ h2 ] and [ f3 ] .    to establish , first note that for a kernel @xmath72 of order @xmath379 with @xmath380 @xmath381 uniformly for all @xmath382 $ ]",
    "in view of [ h2 ] and [ b3 ] , it thus suffices to show that @xmath383 uniformly for lebesgue almost all @xmath137 $ ] .",
    "recall that @xmath384 where @xmath385 is a polynomial of degree @xmath386 on@xmath73 $ ] that solves the linear optimization problem @xmath387 under the constraints @xmath388.\\ ] ] define polynomials @xmath389.\\ ] ] then @xmath390 is the taylor expansion of order @xmath386 of @xmath391 at @xmath5 and the estimation error can be written as @xmath392 note that @xmath393 is a polynomial of degree @xmath386 that solves the linear optimization problem @xmath394 subject to @xmath395,\\ ] ] with @xmath396 we now use point process techniques to analyze the asymptotic behavior of this linear program .",
    "denote by @xmath397 } \\delta_{((i / n - x)/h_n , ( nh_n)^{1/\\alpha}\\bar\\eps_i)}\\ ] ] a point process of standardized error random variables .",
    "then the constraints can be reformulated as @xmath398 where @xmath399\\times\\r\\mid u > f(t)\\}$ ] denotes the open epigraph of a function @xmath108 .",
    "it is well known ( see , e.g. , resnick ( 2007 ) , theorem 6.3 ) that @xmath400 converges weakly to a poisson process @xmath401 on @xmath73\\times\\r$ ] with intensity measure @xmath402}\\otimes \\nu_\\alpha$ ] where @xmath403 has lebesgue density @xmath404 , because by [ h2 ] @xmath405 uniformly for all @xmath406 $ ] @xmath407\\times(-1,\\infty)))=2nh_n p\\big\\{\\bar\\eps_1>(nh_n)^{-1/\\alpha}\\big\\ } \\to 2c_f.\\ ] ] by skorohod s representation theorem , we may assume that the convergence holds a.s .",
    "next we analyze the corresponding linear program in the limiting model to minimize @xmath408 over polynomials of degree @xmath386 subject to @xmath409 . in what follows we use a representation of the poisson process as @xmath410 where @xmath411 are independent random variables which are uniformly distributed on @xmath73 $ ] .",
    "first we prove by contradiction that the optimal solution is almost surely unique .",
    "suppose that there exist more than one solution .",
    "from the theory of linear programs it is known that then there exists a solution @xmath31 such that @xmath412 has at most @xmath386 elements . because @xmath31 is bounded and @xmath401 has a.s .",
    "finitely many points in any bounded set , @xmath413 a.s . since @xmath31 is an optimal solution , for all polynomials @xmath414 of degree",
    "@xmath386 satisfying @xmath415 , @xmath416 , and @xmath417 must satisfy @xmath418 , because both @xmath419 and @xmath420 satisfy the constraints @xmath421 . in particular , for all sufficiently small @xmath134 and all polynomials @xmath422 of degree @xmath423 , @xmath424 is of that type . write @xmath425 in the form @xmath426 .",
    "then necessarily @xmath427 for all @xmath428 .",
    "this implies that @xmath429 lies on a manifold @xmath430 of dimension @xmath431 which only depends on @xmath432 and @xmath386 .",
    "however , by proposition [ prop : estbound ] , @xmath433 where @xmath434\\}.\\ ] ] the above conclusion contradicts @xmath435 as @xmath436 , since @xmath437 for all @xmath438 ( i.e. , the fact that among finitely many values @xmath411 a.s .",
    "there does not exist a subset which lies on a given manifold of lower dimension ) .    therefore the solution @xmath31 must be a.s .",
    "unique which in turn implies that it is a basic feasible solution , i.e. , the @xmath439 . on the other hand , because the intensity measure of @xmath401 is absolutely continuous , @xmath440 a.s .  and thus @xmath441 .",
    "because of @xmath442 a.s .",
    ", one has @xmath443\\times[-k_dz_{\\max},\\infty))= n([-1,1]\\times[-k_dz_{\\max},\\infty))=:m$ ] for sufficiently large @xmath211 .",
    "moreover , one can find a numeration of the points @xmath444 , @xmath445 , of @xmath400 and @xmath446 , @xmath445 , of @xmath401 in @xmath73\\times[-k_dz_{\\max},\\infty)$ ] such that @xmath447 .",
    "next we prove that the solution to the linear program to minimize @xmath448 subject to @xmath398 is eventually unique with @xmath449 a.s . since any optimal solution can be written as a convex combination of a basic feasible solution , w.l.o.g .",
    "we may assume that @xmath450 has at least @xmath379 elements . the polynomial @xmath393 is uniquely determined by this set @xmath451 .",
    "suppose that along a subsequence @xmath452 the set @xmath453 is constant , but not equal to @xmath454 .",
    "then @xmath455 converges uniformly to the polynomial @xmath456 of degree @xmath386 that is uniquely determined by the conditions @xmath457 for all @xmath458 .",
    "in particular , @xmath456 is different from the optimal polynomial @xmath31 for the limit poisson process , but it satisfies the constraints @xmath409 . thus @xmath459 . on the other hand , for all @xmath329",
    "the polynomial @xmath460 eventually satisfies the constraints @xmath461 and thus @xmath462 , which leads to a contradiction .",
    "hence , @xmath463 for all sufficiently large @xmath211 and the optimal solution @xmath393 for @xmath400 is unique and it converges uniformly to the optimal solution @xmath31 for the poisson process @xmath401 .",
    "moreover , using the relation @xmath464 ( which is a system of linear equation in the coefficients of @xmath393 ) , @xmath465 can be calculated as @xmath466 for some vector @xmath467 which converges to a limit vector @xmath468 ( corresponding to the analogous relation for @xmath31 ) .",
    "exactly the same arguments apply if we replace @xmath469 with @xmath470 , which corresponds to the case that @xmath1 is identical 0 .",
    "since the points @xmath471 of the pertaining point process equal @xmath472 and thus @xmath473 , the difference of the resulting values for optimal polynomial at 0 is bounded by a multiple of @xmath474 . in view of , we may conclude that the estimation errors differ only by a multiple of @xmath475 , which finally yields and thus the assertion .",
    "akritas , m. and van keilegom , i. ( 2001 ) .",
    "nonparametric estimation of the residual distribution",
    ". _ scand .",
    "j.  statist .",
    "_ * 28 * , 549567 ."
  ],
  "abstract_text": [
    "<S> we consider a nonparametric regression model with one - sided errors and regression function in a general hlder class . </S>",
    "<S> our aim is inference on the error distribution . to this end </S>",
    "<S> we estimate the regression function via minimization of the local integral of a polynomial approximation . we show uniform rates of convergence for the simple regression estimator as well as for a smooth version . </S>",
    "<S> those rates continue to hold in mean regression models with symmetric and bounded error distribution . </S>",
    "<S> here one obtains faster rates of convergence compared to optimal rates in mean regression when the error distribution is irregular in the sense that sufficient mass is concentrated near the endpoints . </S>",
    "<S> the results are applied to prove asymptotic @xmath0-equivalence of a residual - based empirical distribution function to the empirical distribution function of unobserved errors in the case of irregular error distributions . </S>",
    "<S> this result is remarkably different from corresponding results in mean regression with regular errors . </S>",
    "<S> it can readily be applied to develop goodness - of - fit tests for the error distribution . </S>",
    "<S> we present some examples and investigate the small sample performance in a simulation study .    </S>",
    "<S> [ section ] [ theo]lemma [ theo]corollary [ theo]remark [ theo]proposition [ theo]definition [ theo]example    ams 2010 classification : primary 62g08 ; secondary 62g10 , 62g30 , 62g32 keywords and phrases : goodness - of - fit testing , irregular error distribution , one - sided errors , residual empirical distribution function , uniform rates of convergence </S>"
  ]
}