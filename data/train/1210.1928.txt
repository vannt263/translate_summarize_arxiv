{
  "article_text": [
    "in applications such as space - exploration , mining or agriculture automation , modeling the underlying resource is a fundamental problem . for such applications , an efficient , flexible and high - fidelity representation of the geology is critical .",
    "the key challenges in realizing this are that of dealing with the problems of uncertainty and incompleteness .",
    "uncertainty and incompleteness are virtually ubiquitous in any sensor based application as sensor capabilities are limited .",
    "the problem is magnified in a field automation scenario due to sheer scale of the application .",
    "incompleteness is a major problem in any large scale resource modeling endeavor as sensors have limited range and applicability .",
    "a more significant contributor to this issue is that of cost - sampling and collecting such data is expensive .",
    "geological data is typically collected through various sensors / processes of widely differing characteristics and consequently lead to different kinds of information .",
    "often the resource is characterized by numerous quantities ( for example , soil composition in terms of numerous elements ) .",
    "these quantities often are correlated .",
    "given these issues , large scale geological resource modeling needs a representation that can handle spatially correlated , incomplete and uncertain data . not only must the correlation between homogeneous quantities be modeled but also that between heterogeneous quantities .",
    "this paper uses a gaussian process ( gp ) representation of resource data similar to that described in @xcite .",
    "gps are ideally suited to handling spatially correlated data .",
    "this paper further uses an extension of the basic gaussian process model , the multi - task gaussian process ( mtgp ) , to simultaneously model multiple quantities of interest .",
    "the proposed model not only captures spatial correlations between individual quantities with themselves ( at different locations ) but also that between totally different quantities that together quantify the resource . that the quantities modeled in this paper exhibit strong correlation is known from geological sciences .",
    "this paper presents an empirical evaluation to understand ( 1 ) if simultaneous modeling of multiple quantities of interest ( i.e. modeling and using the correlations between them and hence performing data fusion ) is better than modeling these quantities independently and ( 2 ) if the nonstationary kernels are more effective than stationary kernels at modeling geological data .",
    "experiments are performed on large scale real sensor data .",
    "gaussian processes ( gps ) @xcite are powerful non - parametric bayesian learning techniques that can handle correlated , uncertain and incomplete data .",
    "they have been used in a range of fields , the gaussian process web - site lists several examples .",
    "gps produce a scalable multi - resolution model of the entity under consideration .",
    "they yield a continuous domain representation of the data and hence can be sampled at any desired resolution .",
    "gps incorporate and handle uncertainty in a statistically sound manner and represent spatially correlated data appropriately .",
    "they model and use the spatial correlation of the given data to estimate the values for other unknown points of interest .",
    "gps basically perform a standard interpolation technique known as _ kriging",
    "_ @xcite .",
    "the work @xcite , modeled large scale terrain modeling using gps .",
    "it proposed the use of non - stationary kernels ( neural network ) to model large scale discontinuous spatial data .",
    "a performance comparison between gps based on stationary ( squared exponential ) and non - stationary ( neural network ) kernels as well as several other standard interpolation methods applicable to alternative representations of terrain data , was reported .",
    "the non - stationary neural network kernel was found to be superior to the stationary squared exponential kernel and at least as good as most standard interpolation techniques for a range of terrain ( in terms of sparsity / complexity / discontinuities ) .",
    "the work presented in this paper builds on this gp representation .",
    "however , it addresses the problem of simultaneous modeling multiple heterogeneous quantities of interest , in the context of geological resource modeling .",
    "this requires the modeling and usage of the correlations between these quantities towards improving predictions of each of them - an instance of data fusion using gaussian processes .",
    "data fusion in the context of gaussian processes is necessitated by the presence of multiple , multi - sensor , multi - attribute , incomplete and/or uncertain data sets of the entity being modeled .",
    "two preliminary attempts towards addressing this problem include @xcite and @xcite .",
    "the former bears a `` hierarchical learning '' flavor to it in that it demonstrates how a gp can be used to model an expensive process by ( a ) modeling a gp on an approximate or cheap process and ( b ) using the many input - output data from the approximate process and the few samples available of the expensive process together in order to learn a gp for the latter . the work @xcite attempts to generalize arbitrary transformations on gp priors through linear transformations .",
    "it hints at how this framework could be used to introduce heteroscedasticity ( random variables with non - constant variance ) and how information from different sources could be fused .",
    "however , specifics on how the fusion can actually be performed are beyond the scope of the work .",
    "girolami in @xcite integrated heterogeneous feature types within a gaussian process classification setting , in a protein fold recognition application domain .",
    "each feature representation is represented by a separate gp .",
    "the fusion uses the idea that individual feature representations are considered independent and hence a composite covariance function would be defined in terms of a linear sum of gaussian process priors . a recent work by reece et al .",
    "@xcite integrated `` hard '' data obtained from sensors with `` soft '' information obtained from human sources within a gaussian process classification framework . this problem / approach is different from the work presented here .",
    "it uses heterogeneous information domains ( i.e. kinds of information ) as mutually independent sources of information that are transformed into the kernel representation ( a kernel for each kind of information ) and combined using a product rule ( a linear sum in girolami s work ) .",
    "the focus thus , is on encoding or representing different kinds of information in a common mathematical framework using kernels .",
    "this paper is concerned with a `` higher level '' data fusion problem of heterogeneous - source information integration _",
    "after _ it has been represented using kernel methods .",
    "the experiments of this paper demonstrate the case when information from each source is itself from a homogeneous domain - e.g. the heterogeneous input data are all real numbers .",
    "the approach presented in this paper improves the estimate of several different quantities being simultaneously modeled by explicitly modeling the correlation between multiple heterogeneous information sources .",
    "if this is not the case ( e.g. input data is made up of qualitative and quantitative data dimensions ) , each of heterogeneous information types can be represented by separate kernels and these can be combined using a sum or product as has been done in @xcite .",
    "simpler data fusion approaches , based on gps , heteroscedastic gps and their variants ( see @xcite ) , may be applied .",
    "however , the application of the approach presented in this paper , based on multi - output or multi - task gps , will require a non - trivial derivation of auto and cross covariances for kernels applied on heterogeneous information types .",
    "examples of related works that use multiple sources of the same kind of information within a single gp representation framework include @xcite and @xcite .",
    "whereas the former uses single output gps to incorporate in - situ surface spectra information and remotely sensed spectra information into a kilometer scale map of the environment , the latter uses a gp implicit surface representation of an object that has to be grasped and manipulated .",
    "the representation incorporates visual , haptic and laser data into a single representation of the object .",
    "data from each of these sensor modalities conditions the gp prior based on the implied surface at that point ( on / outside / inside the object ) .",
    "two recent approaches demonstrating data fusion with gaussian processes in the context of large scale terrain modeling were based on heteroscedastic gps @xcite and dependent gps @xcite .",
    "these address the problem of fusing multiple , multi - sensor data sets of a single quantity of interest .",
    "this paper describes the framework for extending this concept to multiple heterogeneous quantities of interest . the work @xcite treated the data - fusion problem as one of combining different noisy samples of a common entity ( terrain ) being modeled . in the machine learning community",
    ", this idea is referred to as heteroscedastic gps @xcite . the works @xcite and @xcite treated the data fusion problem as one of improving gp regression through modeling the spatial correlations ( auto and cross covariances ) between several dependent gps representing the respective data sets",
    "this idea has been inspired by recent machine learning contributions in multi - task or multi - output or dependent gp modeling including @xcite and @xcite , the latter being based on @xcite . in kriging terminology",
    ", this idea is akin to co - kriging @xcite . the work @xcite performed a model complexity analysis of multiple approaches to data fusion using gps , applied in the context of large scale terrain modeling .",
    "the work presented in this paper , focuses on the most generic of these approaches in the context of geological resource modeling .",
    "the significantly stronger evaluation , the discussion of `` big - picture '' issues relating to the application of the approach in practical problems , the fusion of heterogeneous data , the use of more kernels and the tying together of different prior works that have studied this approach @xcite are enhancements presented in this work .",
    "the work @xcite provided preliminary findings to geological resource modeling using various combinations of stationary kernel including the squared exponential ( sqexp ) , matern 3/2 and a sparse covariance function @xcite . for a geological resource modeling data set taken from a mine , it found the matern 3/2 - matern 3/2 - sqexp kernel combination provided best performance in terms of the prediction error . this paper reports a detailed multi - metric benchmarking experiment , using cross validation methods , performed on a multi - task gp , an equivalent set of gps and a set of independently optimized gps , to provide for an exact and an independent comparison between them .",
    "the objective is to quantify the benefit ( if any ) of simultaneous modeling of the multiple quantities by modeling and using the correlations between them as against modeling each of these quantities separately .",
    "this paper also compares data fusion using multiple stationary and nonstationary kernels in the context of modeling geological data .",
    "an extensive review of kernel methods applied in modeling vector valued functions was presented in a recent survey paper @xcite .",
    "the paper discusses different approaches to develop kernels for multi - task applications and draws parallels between regularization perspective of this problem and a bayesian one .",
    "the latter perspective is discussed through gaussian processes .",
    "the work presented in this paper focuses on one of the approaches reviewed in @xcite ; specifically , it addresses modeling and information fusion of multi - task geological data using gaussian processes developed using the process convolution approach .",
    "the paper presents a detailed empirical study of the approach applied to a large scale real world problem in order to evaluate its efficacy for information fusion , to understand the modeling capabilities of different kernels ( chosen apriori ) with such data and to understand broader approach - related questions from an application perspective .",
    "the paper also ties together past works of the authors within the process convolution theme .",
    "gaussian processes @xcite ( gps ) are stochastic processes wherein any finite subset of random variables is jointly gaussian distributed",
    ". they may be thought of as a gaussian probability distribution in function space .",
    "they are characterized by a mean function @xmath0 and the covariance function @xmath1 that together specify a distribution over functions . in the context of geological resource modeling ,",
    "each @xmath2 ( 3d coordinates ) and @xmath3 , the concentration of the quantity being modeled .",
    "although not necessary , the mean function @xmath0 may be assumed to be zero by scaling / shifting the data appropriately such that it has an empirical mean of zero .",
    "the covariance function or kernel models the relationship between the random variables corresponding to the given data .",
    "it can take numerous forms @xcite . the stationary squared exponential ( or gaussian ) kernel ( sqexp ) is given by @xmath4 where @xmath5 is the covariance function or kernel ; @xmath6^{-2}$ ]",
    "is a @xmath7 x @xmath7 diagonal length - scale matrix ( @xmath7 = dimensionality of input = 3 in this case ) , a measure of how quickly the modeled function changes in the east , north and depth directions ; @xmath8 is the signal variance .",
    "the set of parameters @xmath9 are referred to as the kernel hyperparameters .",
    "the non - stationary neural network ( nn ) kernel @xcite takes the form @xmath10 where @xmath11 and @xmath12 are augmented input vectors ( each point is augmented with a 1 ) , @xmath13 is a @xmath14 x @xmath14 diagonal length - scale matrix given by @xmath15^{-2}$ ] , @xmath16 being a bias factor and @xmath7 being the dimensionality of the input data .",
    "the variables @xmath17 constitute the kernel hyperparameters .",
    "the nn kernel represents the covariance function of a neural network with a single hidden layer between the input and output , infinitely many hidden nodes and using a sigmoidal transfer function @xcite for the hidden nodes .",
    "hornik , in @xcite , showed that such neural networks are universal approximators and neal , in @xcite , observed that the functions produced by such a network would tend to a gaussian process .",
    "prior work in @xcite found the nn kernel to be more effective than the sqexp kernel at modeling discontinuous data .",
    "the matern 3/2 kernel is another stationary kernel differing from the sqexp kernel in that the latter is infinitely differentiable and consequently tends to have a strong smoothing nature , which is argued as being detrimental to modeling physical processes @xcite .",
    "it takes the form specified in equation [ eqn : matern3 ] .",
    "@xmath18 where @xmath19 is the dimension of the input data ( @xmath7 = dimensionality of input = 3 in this case ) , @xmath20 $ ] is a @xmath21 x @xmath7 length - scale matrix , a measure of how quickly the modeled function changes in the east , north and depth directions ; @xmath8 is the signal variance .",
    "the set of parameters @xmath9 is referred to as the kernel hyperparameters .",
    "regression using gps uses the fact that any finite set of training ( evaluation ) data and test data of a gp are jointly gaussian distributed . assuming noise free data , this idea is shown in expression [ eqn : gpmodel ] ( hereafter referred to as equation [ eqn : gpmodel ] ) .",
    "this leads to the standard gp regression equations yielding an estimate ( the mean value , given by equation [ eqn : gpmean ] ) and its uncertainty ( equation [ eqn : gpcov ] ) .",
    "@xmath22    \\;\\sim\\ ;    n \\left ( 0 \\,,\\ ,      \\left [        \\begin{array}{c c }          k(x , x )    & k(x , x _ * ) \\\\",
    "k(x_*,x ) & k(x_*,x _ * )        \\end{array }      \\right ]    \\right )        \\label{eqn : gpmodel}\\ ] ]    @xmath23    for @xmath24 training points @xmath25 and @xmath26 test points @xmath27 , @xmath28 denotes the @xmath29 matrix of covariances evaluated at all pairs of training and test points .",
    "the terms @xmath30 , @xmath31 and @xmath32 are defined likewise . in the event",
    "that the data being modeled is noisy , a noise hyperparameter ( @xmath33 ) is also learnt with the other gp hyperparameters and the covariance matrix of the training data @xmath30 is replaced by @xmath34 $ ] in equations [ eqn : gpmodel ] , [ eqn : gpmean ] and [ eqn : gpcov ] .",
    "gp hyperparameters may be learnt using various techniques such as cross validation based approaches @xcite and maximum - a - posteriori approaches using markov chain monte carlo techniques @xcite and maximizing the marginal likelihood of the observed training data @xcite .",
    "this paper adopts the latter most approach based on the intuition that it may be more suited for large data sets .",
    "the marginal likelihood to be maximized is described in equation [ eqn : gplml ] . @xmath35",
    "the problem being addressed in this paper can be described as follows .",
    "the objective is to model multiple heterogeneous quantities ( e.g. concentrations of various elements ) of the entity in consideration ( e.g. land mass ) .",
    "the data fusion aspect of this problem is the improved estimation of each one of these quantities by integration or use of all other quantities of interest .",
    "if each quantity is modeled using a separate gp , the objective is to improve one gps prediction estimates given all other gp models .",
    "multi - task gaussian processes ( mtgps or multi - output gps or dependent gps ) extend gaussian processes to handle multiple correlated outputs simultaneously .",
    "the main advantage of this technique is that the model exploits not only the spatial correlation of data corresponding to one output but also those of the other outputs .",
    "this improves gp regression / prediction of an output given the others , thus performing data fusion .",
    "figure [ fig : mogp ] shows a simulated example of this concept .",
    "let the number of outputs / tasks that need to be simultaneously modeled be denoted by @xmath36 .",
    "equations [ eqn : gpmodel ] , [ eqn : gpmean ] and [ eqn : gpcov ] represent respectively the mtgp data fusion model , the regression estimates and their uncertainties , subject to the following modifications to the basic notation .",
    "the set @xmath37'\\ ] ] represents the output values of the selected training data from the individual @xmath36 tasks that need to be simultaneously modeled .",
    "the term @xmath38\\ ] ] denotes the input location values ( east , north , depth ) of the selected training data from the individual data sets .",
    "any kernel @xcite may be used and even different kernel could be used for different data sets using the technique demonstrated in @xcite ( for stationary kernel ) or the convolution process technique demonstrated in @xcite and in this paper ( for both stationary and nonstationary kernel ) .",
    "the covariance matrix of the training data is given by @xmath39\\ ] ] where @xmath40 here , @xmath41 represents the auto - covariance of the @xmath42 data set with itself and @xmath43 represents the cross covariance between the @xmath42 and @xmath44 data sets .",
    "these terms model the covariance between the noisy observed data points ( @xmath45 values ) .",
    "thus , they also take the noise components of the individual data sets / gps into consideration .",
    "the corresponding noise free terms are respectively given by @xmath46 and @xmath47 .",
    "these are derived by using the process convolution approach to formulating gaussian processes ; details of this follow in the next subsection .",
    "the covariance matrix between the test points and training points is given by    @xmath48 \\;,\\ ] ]    where @xmath49 is the gp that is being evaluated given all other gps .",
    "the matrix @xmath28 is defined likewise .",
    "finally , the covariance of the test points is given by @xmath50 assuming the @xmath42 gp needs to be evaluated for the particular test point .",
    "the mean and variance of the concentration estimate can thus be obtained by applying equations [ eqn : gpmean ] and [ eqn : gpcov ] , after incorporating multiple outputs / tasks , multiple gp / noise hyperparameters and deriving appropriate auto and cross covariances functions that model the spatial correlation between the individual data sets .",
    "data fusion is thus achieved in the mtgp approach by correlating individual heterogeneous outputs / tasks and using this correlation information to improve the prediction estimates of each of them .",
    "the main challenge in the use of multi - task gps is the derivation of closed form cross ( and auto ) covariance functions .",
    "the process convolution approach to modeling gps , proposed in @xcite , can address this problem .",
    "the cited paper ( 1 ) modeled a gp as the convolution of a `` smoothing kernel '' and a gaussian white noise process , ( 2 ) expressed a relationship between the `` smoothing kernel '' and the corresponding covariance function through the fourier transform , ( 3 ) noted that for stationary isotropic kernels , there existed a one - to - one relationship between the covariance function and its smoothing kernel and that for non - isotropic and/or non - stationary kernels , there was no unique solution to the smoothing kernel and ( 4 ) hinted at how this approach may be used to develop gp models with complex properties ( e.g. nonstationarity ) . as a consequence of this approach , modeling",
    "the gp amounted to modeling the hyperparameters of the smoothing kernel .",
    "for the second point above , the paper suggested that the smoothing kernel for a covariance function could be obtained as the inverse fourier transform of the square root of the spectrum ( fourier transform ) of the covariance function .",
    "the process convolution approach to mtgps has been used with the stationary sqexp kernel in @xcite and the nonstationary nn kernel in @xcite .",
    "once the smoothing kernel is identified for a covariance function , the cross - covariance between two covariance functions can be derived as a kernel correlation between the respective smoothing kernels @xcite .",
    "the following mathematical formalism is based on @xcite and @xcite .",
    "@xmath51    @xmath52    mathematically , if @xmath53 represents the observed data in equation [ eqn : pc1 ] , it is expressed as a combination of a noise - free gp @xmath54 and gaussian white noise process @xmath55 .",
    "the gp @xmath54 is further modeled as a convolution of a smoothing kernel @xmath56 and a gaussian white noise process @xmath57 , as shown in equation [ eqn : pc2 ] .",
    "a stationary and/or isotropic smoothing kernel would take the form @xmath58 as it would be a function of the distance between the input points .",
    "if two covariance functions ( corresponding to two gps @xmath54 and @xmath59 ) have smoothing kernels @xmath60 and @xmath61 respectively , then the cross covariance between them can be derived as shown in equation [ eqn : ccov ] .",
    "the auto covariance can be deduced from the cross covariance expression and take the form shown in equation [ eqn : acov ] .",
    "the smoothing kernel @xmath62 and @xmath63 need to be finite energy kernels i.e. @xmath64 .",
    "this can be intrinsically true of some kernel ( e.g. squared exponential kernel ) or can be true subject to the bounded application of the kernel ( e.g. neural network kernel ) .",
    "the work @xcite suggested that if a covariance function could be written as a convolution of its `` basis functions '' ( the form specified in equation [ eqn : acov ] ) , then a cross - covariance between two covariance functions could be derived as a kernel correlation of their respective basis functions ( the form specified in equation [ eqn : ccov ] ) .",
    "the paper proved that the resulting cross - covariance would be positive definite . in order to find the basis function for a particular covariance function , the paper derived an expression in terms of its fourier transform .",
    "this relationship is identical to that suggested by @xcite and valid for stationary kernels only .",
    "the paper also derives closed form cross - covariance functions for different combinations of stationary kernels including the squared exponential , matern 3/2 and a sparse covariance function developed by the authors in @xcite .",
    "this paper argues that both of these methods using the `` smoothing kernel '' @xcite and the `` basis functions '' @xcite are actually equivalent with the former providing a sound basis to explain the latter as well as a powerful framework to develop other complex gp models such as space - time models and nonstationary gps .",
    "the key insight obtained here is in the methodology of identifying the smoothing kernel for the process convolution approach .",
    "if the covariance function is a stationary kernel , there is an exact one - to - one relationship between the covariance function and the smoothing kernel as pointed out in @xcite and whose expression is derived in @xcite .",
    "if the covariance function is nonstationary , several possible smoothing kernels may lead to the same covariance function , as pointed out in @xcite .",
    "however , attempting to express the kernel in a separable form ( e.g. as the correlation of two identically formed basis functions ) and thereby identifying the smoothing kernel would be _ one _ possible approach , if the form of the kernel form allowed for such separation . needless to say , this idea would be applicable only in a restricted class of covariance functions and finding a universal approach to identifying the smoothing kernel for other nonstationary kernel remains an open question . given the smoothing kernel of the covariance functions in consideration , the cross - covariance terms can be derived as a kernel correlation as demonstrated in @xcite .",
    "assume two gps @xmath65 and @xmath66 , with with length scale matrices @xmath67 and @xmath68 .",
    "based on @xcite , the cross and auto covariances for the stationary sqexp kernel are given by equations [ eqn : ccov_sqexp ] and [ eqn : acov_sqexp ] respectively .",
    "the corresponding expressions for the nonstationary nn kernel are derived in @xcite and given in equations [ eqn : ccov_nn ] and [ eqn : acov_nn ] respectively . for the matern 3/2 kernel ,",
    "the expressions for the cross covariance and auto covariance are derived in @xcite and given in equations [ eqn : ccov_matern3 ] and [ eqn : acov_matern3 ] respectively .",
    "also based on @xcite , the cross covariance function between an sqexp and a matern 3/2 kernel is given by equation [ eqn : ccov_sqexp_matern3 ] .    in equations [ eqn : ccov_nn ] and [ eqn : acov_nn ] , the term , @xmath69 , is the nn kernel for two data @xmath70 , @xmath71 and length scale matrix @xmath72 .",
    "it is given by equation [ eqn : nn ] , excluding the signal variance term ( @xmath8 ) .",
    "likewise , in equation [ eqn : acov_matern3 ] , @xmath73 refers to the matern 3/2 kernel for two data @xmath70 , @xmath71 and length scale matrix @xmath72 , given by equation [ eqn : matern3 ] ( excluding the @xmath8 term ) .",
    "the @xmath74 terms in equations [ eqn : ccov_sqexp ] , [ eqn : acov_sqexp ] , [ eqn : ccov_nn ] and [ eqn : acov_nn ] are inspired by @xcite .",
    "this term models the task similarity between individual tasks .",
    "incorporating it in the auto and cross covariances provides additional flexibility to the multi - task gp modeling process .",
    "it is a symmetric matrix of size @xmath36 x @xmath36 and is learnt along with the other gp hyperparameters .",
    "thus , the hyperparameters of the system that need to be learnt include @xmath75 task similarity values , @xmath76 or @xmath77 length scale values respectively for the individual sqexp / matern3 or nn kernels and @xmath36 noise values corresponding to the noise in the observed data sets .",
    "learning these hyperparameters by adapting the gp learning procedure described before ( equation [ eqn : gplml ] ) for multiple outputs / tasks @xcite .",
    "experiments were conducted on a large scale geological resource data set made up of real sensor data .",
    "the data consists of 63,667 measurements from a 3478.4 m x 1764.6 m x 345.9 m region in australia that has undergone drilling and chemical assays to determine its composition .",
    "the holes are generally 25 - 100 m apart and tens to hundreds of meters deep . within each hole ,",
    "data is collected at an interval of 2 m .",
    "the measurements include the ( east , north , depth ) position data along with the concentrations of three elements , element-1 , element-2 and element-3 , hereafter denoted as e1 , e2 and e3 respectively .",
    "these three quantities are known to be correlated and hence the objective is to use each of their gp models to improve the others prediction estimates by capturing the correlation between these quantities .",
    "the data set is shown in figure [ fig : data ] .",
    "the methodology of testing is described in section [ sec : exp : testprocedure ] .",
    "multiple metrics have been used to evaluate the methods , these are described in section [ sec : exp : metrics ] .",
    "results obtained are then presented and discussed in section [ sec : exp : results ] .",
    "outputs of the data fusion process provided by the best performing model as suggested by the evaluation are also presented .",
    "the objective of the experiment was to compare the multi - task gp approach with a conventional gp approach and quantify if the data fusion in the mtgp actually improves estimation .",
    "a second objective of the experiments was to compare the nonstationary nn kernel with the stationary sqexp kernel , the matern 3/2 kernel and a combination of them that proved effective in prior testing @xcite . towards these aims ,",
    "a ten fold cross validation experiment was performed on the data set , with each of the kernels .",
    "this was motivated by the work @xcite , which suggests a ten fold stratified ( similar number of samples in each fold ) cross validation as the best way of testing the estimation accuracy of machine learning methods on real world data sets .",
    "the mtgp and simple gp approaches each require an optimization step for model learning .",
    "the optimization step in each method can result in different local minima in each trial ( and with each kernel ) .",
    "thus , to do a one - on - one comparison between the two approaches and quantify their relative performances , an exact comparison is required .",
    "the benchmarking experiment presented in this paper provides an _ exact _ comparison between the mtgp and gp approaches . to do this ,",
    "* the best available mtgp parameters were found for each kernel . from this",
    ", appropriate subsets of the parameters were chosen for the gp approach . *",
    "the approaches were compared on identical test points and identical training / evaluation points selected for each of the test points . *",
    "it is also necessary that the covariance function for the simple gp approach _ must _ be identical to the auto - covariance function of the dgp approach .",
    "for this reason , the auto - covariance function ( for both kernels ) is used as the covariance function for the gp approach to data fusion .",
    "in addition to this , three independent gps ( denoted as gpi here after ) were optimized for e1 , e2 and e3 and their estimates for the same set of test points were also compared .",
    "thus the effect of information integration in the context of the geological resource modeling can be seen in terms of both an exact comparison ( mtgp vs gp ) and an independent comparison ( mtgp vs gpi ) .",
    "for the cross validation , a `` block '' sampling technique ( see figure [ fig : bs ] ) was used , a 3d version of the `` patch '' sampling method used in @xcite .",
    "the idea was that rather than selecting test points uniformly , blocks of data test the robustness of the approach better as the support points to the query point are situated farther away than in uniform point selection .",
    "the data set is gridded into blocks of different sizes .",
    "collections of blocks represent individual folds . in each cross validation test , one fold was designated as a test fold and points from it were used exclusively for testing .",
    "all other folds together constituted the evaluation data , a small subset of which were labeled as the training data .",
    "note that this technique of testing will naturally lead to larger errors . for the test fold , the e1 , e2 and e3 concentrations ( and error metrics defined in the following section ) are estimated first using the mtgp approach , then with the gp approach using parameters from the optimized mtgp parameters and finally , with an independently optimized gp for each of the three quantities .",
    "the result of a 10 fold cross validation test is a 63,667 point evaluation in tougher test conditions than what would be attainable with uniform sampling ( e.g. every tenth point ) of test points .",
    "block sizes were chosen empirically , in proportion ( arbitrarily rounded up or down ) to the dimensions of the whole data set and with a view of performing a stratified cross validation test . the block sizes chosen and the resulting implications on the cross validation testing are shown in table [ tab : cvp_bs ]",
    ". the smaller block size of 22 m x 11 m x 2 m results in each fold having a similar number of points ( i.e. numbers of points in folds with min / max test points are similar ) and thus results in the most stratified cross validation test .",
    "with increasing block size , prediction error increases ( support data is farther away ) , stratification is reduced and hence , variance in prediction error also increases .",
    "uniform sampling of test points may be considered as a limiting case of block sampling with the smallest block size possible .        .10 fold cross validation with block sampling ; 63667 points in data set spread over 3478.4 m x",
    "1764.6 m x 345.9 m ; block sizes tested vs relative implications on results [ cols=\"^,^,^,^,^,^,^,^ \" , ]    + rather than the individual training times , the relative amount of training ( under similar conditions , with different kernel ) required to produce a reasonable set of parameters is of more interest .",
    "experience suggests that the nn kernel based mtgp / gp models converged faster and better as compared to other kernels .",
    "mtgp models based on the nn kernel outperform other kernels tested .",
    "* see figures [ fig : e1_mtgp_se ] , [ fig : e1_mtgp_var ] , [ fig : e1_mtgp_nlp ] for e1 , figures [ fig : e2_mtgp_se ] , [ fig : e2_mtgp_var ] , [ fig : e2_mtgp_nlp ] for e2 and figures [ fig : e3_mtgp_se ] , [ fig : e3_mtgp_var ] , [ fig : e3_mtgp_nlp ] for e3 .",
    "* the nn kernel is the best performing kernel of the four tested , across all block sizes tested .",
    "the mtgp based on the nn kernel produces lower se ( better estimate ) and reduced nlp ( better model ) estimates than other kernels tested . * for small block sizes , both the nn and mm kernel are competitive ; in case of e3 , the mm even marginally outperforms the nn kernel for the two smallest block sizes tested .",
    "note however that considering all test sizes and all three elements , the observation is that the mm kernel produces lower var for a higher se , meaning that it is more confident of its se values which are worse / higher than those of the nn kernel .",
    "this makes its nlp higher and the model poorer than an mtgp based on the nn kernel .",
    "note also that as the test block size increases , the advantage in performance of the mtgp based on the nn kernel over that based on the mm kernel becomes more distinctive .",
    "not only are the se values smaller for the nn kernel , the nlp values remain in the same range whereas those of the mm kernel rise significantly .",
    "this proves that the mtgp - nn is better performing and more robust than the mtgp - mm .",
    "the latter property suggests that the mtgp - nn will be able to cope better with incomplete data sets . *",
    "both the ms and sqexp kernels are not competitive with respect to the nn or mm kernels considering both the se and nlp metrics .",
    "these kernels are discussed individually in the following paragraphs .",
    "mtgp models perform significantly better than three separate gps ( using the mtgp parameters ) or three independently optimized gps as information fusion improves estimation . *",
    "see figures [ fig : e1_mtgp_gp_gpi_nn_se ] and [ fig : e1_mtgp_gp_gpi_nn_nlp ] for e1 , figures [ fig : e2_mtgp_gp_gpi_nn_se ] and [ fig : e2_mtgp_gp_gpi_nn_nlp ] for e2 and figures [ fig : e3_mtgp_gp_gpi_nn_se ] and [ fig : e3_mtgp_gp_gpi_nn_nlp ] for e3 .",
    "* for the nn kernel , the mtgp metrics are always lower than the corresponding derived gp ( gp ) or independent gp ( gpi ) metrics - lower se ( better estimate ) with lower nlp ( better model ) .",
    "this clearly demonstrates the benefits of information fusion across heterogeneous information sources so as to improve individual predictions using the mtgp model . * from tables [ tab : cvp_e1 ] , [ tab : cvp_e2 ] and [ tab : cvp_e3 ] , the average reduction in error ( i.e. improvement in performance ) of mtgp models over gp / gpi models for the smallest , intermediate and largest test block sizes are - * * e1 * * * 22 x 11 x 2 - 95.6% over gp , 96.2% over gpi * * * 84 x 45 x 9 - 96.1% over gp , 96.0% over gpi * * * 696 x 353 x 70 - 44.6% over gp , 38.1% over gpi * * e2 * * * 22 x 11 x 2 - 89.6% over gp , 92.3% over gpi * * * 84 x 45 x 9 - 91.6% over gp , 92.4% over gpi * * * 696 x 353 x 70 - 42.1% over gp , 37.5% over gpi * * e3 * * * 22 x 11 x 2 - 82.4% over gp , 83.5% over gpi * * * 84 x 45 x 9 - 85.9% over gp , 85.3% over gpi * * * 696 x 353 x 70 - 30.9% over gp , 22.5% over gpi + these numbers demonstrate significant improvements in performance , even in very large test block sizes , when using the mtgp - nn model for correlated data . 5",
    ".   the ms kernel was uncompetitive * see figures [ fig : e1_mtgp_gp_gpi_ms_se ] and [ fig : e1_mtgp_gp_gpi_ms_nlp ] for e1 , figures [ fig : e2_mtgp_gp_gpi_ms_se ] and [ fig : e2_mtgp_gp_gpi_ms_nlp ] for e2 and figures [ fig : e3_mtgp_gp_gpi_ms_se ] and [ fig : e3_mtgp_gp_gpi_ms_nlp ] for e3 . *",
    "the ms kernel is not competitive with respect to the nn and mm kernels as discussed earlier .",
    "however , the mtgp using this kernel combination proves to be better than a derived gp and an independently optimized gp with respect to the se metric . from the nlp perspective , the mtgp - ms model is more competitive than the other gp models for small block sizes . for larger block sizes , using an independently optimized gp proves to be a more trust worthy modeling option as the increase in error",
    "is met with a corresponding increase in uncertainty ( hence low nlp ) for the independent gp models .",
    "the exception to this behavior is seen in the results for e3 , the mtgp model is poor in this case . this is attributed to do with inferior parameters relevant to the element e3 obtained from the optimization process . *",
    "the ms kernel performs better than the sqexp with respect to the nlp metric and hence can be trusted more ( prediction error compensated by prediction uncertainty ) , but in two of the three elements ( e1 and e3 ) , its se was inferior to that of the sqexp .",
    "the sqexp kernel was uncompetitive and unreliable * see tables [ tab : cvp_e1 ] , [ tab : cvp_e2 ] and [ tab : cvp_e3 ] ; see figures [ fig : e1_mtgp_se ] , [ fig : e1_mtgp_var ] [ fig : e1_mtgp_nlp ] , [ fig : e1_mtgp_gpi_mm_sqexp_se ] , [ fig : e1_mtgp_gpi_mm_sqexp_nlp ] , [ fig : e1_mtgp_gp_gpi_ms_se ] and [ fig : e1_mtgp_gp_gpi_ms_nlp ] for e1 , figures [ fig : e2_mtgp_se ] , [ fig : e2_mtgp_var ] , [ fig : e2_mtgp_nlp ] , [ fig : e2_mtgp_gpi_mm_sqexp_se ] , [ fig : e2_mtgp_gpi_mm_sqexp_nlp ] , [ fig : e2_mtgp_gp_gpi_ms_se ] and [ fig : e2_mtgp_gp_gpi_ms_nlp ] for e2 and figures [ fig : e3_mtgp_se ] , [ fig : e3_mtgp_var ] , [ fig : e3_mtgp_nlp ] , [ fig : e3_mtgp_gpi_mm_sqexp_se ] , [ fig : e3_mtgp_gpi_mm_sqexp_nlp ] , [ fig : e3_mtgp_gp_gpi_ms_se ] and [ fig : e3_mtgp_gp_gpi_ms_nlp ] for e3 . * the mtgp - sqexp model performs poorly in comparison with the equivalent models using the nn / mm kernels , with respect to both se and nlp . * for elements e1 and",
    "e3 , the mtgp - sqexp has a better se than the corresponding model based on the ms kernel ; it has an se better than the corresponding derived / independent gp models but an inferior ( overconfident or low uncertainty ) var and a fluctuating nlp trend . for element e2 , the mtgp - sqexp is worse off than both the equivalent model based on the ms kernel as well as its corresponding gp models .",
    "* considering the results for e2 , the nlp is directly proportional to the se and inversely to the prediction variance . at the smallest block size",
    ", the mtgp - sqexp produces relatively high se ( with respect to e.g. mtgp - nn ) but very low prediction variance .",
    "this basically suggests that the model is confident of its poor estimates - a bad outcome .",
    "this results in a high nlp and poor model . as the block size increases , the prediction variance increases more relative to the prediction error resulting in the decreasing nlp trend .",
    "for elements e1 and e3 , the largest block size results in a stronger increase in prediction error than the variance in the prediction resulting in an increase in nlp .",
    "overall , the mtgp - sqexp model is poor . *",
    "the sqexp kernel is a limiting case of the mm kernel ; both are stationary kernels . considering the behavior of the gpi model using the sqexp kernel and its competitive results with respect to those of the gpi - mm kernel , it is possible that the poor performance of the mtgp - sqexp ( as compared to the mtgp - mm ) is due to poor optimization output ( a bad local minima )",
    ". further investigation on this result is ongoing but the findings are not expected to change the conclusions of this paper .",
    "7 .   in general , the stationary kernels tested seemed to have an inadequate increase in prediction uncertainty with increasing test block size and worsening predictions .",
    "this leads a higher nlp metric and a poor model that is overly confident of its worsening predictions .",
    "this behavior can be attributed to the correlation profile of the stationary kernels tested - they all share the `` correlation decreases with increasing distance of support data from point of interest '' trend .",
    "this results in stationary kernels not being able to cope with large test block sizes as the support data is farther away ( i.e. less correlated and not of much use ) .",
    "in contrast , the nonstationary nn kernel has a sigmoidal profile that can handle this issue across a range of test block sizes .",
    "the se metric taken alone can be misleading .",
    "the experiments have reinforced the need for a multi - metric analysis .",
    "the se metric only provides information on the prediction error but it does not describe the prediction uncertainty which is very important in understanding if a model is reliable or otherwise .",
    "the var and nlp metrics provided key insights on the difference in performance between different models and kernels .",
    "a model that is very confident of its poor predictions is unreliable ( as was the case for the sqexp kernel ) .",
    "worsening predictions ( due to increasing test block size ) is itself not a bad outcome , provided it is met with an equivalent increase in prediction uncertainty .",
    "on the basis of this study , an attempt is made in answering two fundamental questions - ( 1 ) how can i know if i have a good mtgp model ? and ( 2 ) which gp model or kernel should i use ? by no means is this intended to be a ready - made prescription , universal formula or short - cut to be used as a substitute for context specific and statistically apt decisions in developing gaussian process models .",
    "rather , this is a reflection of the authors experiences based on the scope of this and past work in other domains such as terrain modeling .",
    "note that there are numerous very sophisticated gp techniques ( kernels , approximation etc . ) which are beyond the scope of this work and which may change some of these inferences .      to effectively develop and validate mtgp models , the experiments are suggestive of the following -    1 .",
    "the use of multiple kernel from the same family would provide a good method for validating the general behavior / trends of the model in question .",
    "for instance when developing an mtgp model based on the sqexp kernel , developing a matern 3/2 kernel based mtgp model could provide a means to validate the behavior of the mtgp - sqexp model .",
    "the model hyperparameter optimization performed in this paper is based on maximizing the marginal likelihood . typically , error metrics such as the se being sufficiently low is suggestive of the model being good .",
    "a cross validation test could also be performed to ensure that this is indeed the case .",
    "however , it is also important to check if the model in question is under / over confident ( high / low uncertainty ) for a given level of error",
    ". this can be done , not as a standalone test , but in comparison with alternative models or test cases .",
    "when developing a mtgp model , it is a good idea to compare with an equivalent derived gp model and an independently optimized gp model .",
    "the availability of more information and the effective use of this information through the mtgp model should ideally result in significantly lower error metrics ( e.g. se ) with a significant improvement in confidence ( i.e. decrease in prediction variance , var ) and a net reduction in the negative log loss ( nlp ) metric .",
    "4 .   it may be useful to design a variety of different test cases ( e.g. different test block sizes ) and check if the performance metrics behave as expected .",
    "such a test would also be indicative of the robustness of the model .",
    "it may be useful to optimize independent gp models for each task and use these hyperparameters as the initial parameters for the mtgp model .",
    "this obviously depends on the data set at hand and the constraints of the modeling problem .",
    "the following are purely indicative , based on our experiences in multiple problem domains @xcite and may change considering alternative kernels , other novel ways of treating the modeling problem or approximation methods .    1 .",
    "_ time , complexity , computational resources are a premium .",
    "i need a method that just works _ : independently optimized gp models using the neural network kernel or the matern 3/2 kernel would be a competitive solution .",
    "note that the outcome will only be as good as the data being modeled and other information sources can not be leveraged .",
    "i need the best possible model over a range of test sizes and i do not know much about my data _ : multi - task gp models using the neural network kernel would be a competitive solution .",
    "i need the best possible model over a range of test sizes and i know how my data changes _ : multi - task gp models with a kernel representative of the variation of the data e.g. a uniform variation ( no sudden changes in trend ) can be effectively modeled using the matern 3/2 or squared exponential kernels . 4 .",
    "_ i need a model that can cope with sparse data and/or incomplete data sets _ : neural network kernel based gp or mtgp models depending on the computational complexity constraints and model accuracy requirements . 5 .",
    "_ i have `` good '' multi - attribute data .",
    "i need to model this well and fast _ : independent gp models for each of the attributes , using either a neural network kernel or some other kernel more suited to the data , would provide a competitive solution .",
    "the use of independent gp models will result in the ability to parallelize the modeling process and significantly reduce the possibility of poor models ( poor local minima ) as a consequence of a reduction in number of model parameters .",
    "note that `` good '' here is application dependent but would certainly require being well sampled , not noisy and reasonably complete ( no large gaps where other information modalities can be leveraged ) .",
    "this paper studied the problem of geological resource modeling using multi - task gaussian processes ( mtgps ) .",
    "the concentrations of three elements were modeled and predicted over a region of interest using the mtgp as well as individual gaussian processes ( gps ) for each of these quantities .",
    "the paper demonstrates that mtgps perform significantly better than individual gps at the modeling problem as they effectively integrate heterogeneous sources of information ( concentrations of individual elements ) to improve the individual predictions of each of them .",
    "the benefits of information integration using the mtgp as against independent gps for the task of geological resource modeling have been quantified by a multi - metric and multi - test - size cross validation study that performed both an exact and an independent comparison between mtgps and gps .",
    "multi - task gaussian process models based on the neural network kernel was shown to be a competitive and robust option across a range of test block sizes .",
    "this work has been funded by the rio tinto centre for mine automation .",
    "30    s. vasudevan , f. ramos , e. nettleton and h. durrant - whyte , `` gaussian process modeling of large scale terrain '' , journal of field robotics , volume 26(10 ) , pages 812840 , 2009 .",
    "rasmussen and c.k.i .",
    "williams , `` gaussian processes for machine learning '' , mit press , 2006 .",
    "g. matheron , `` principles of geostatistics '' , economic geology , volume 58 , pages 12461266 , 1963 .    m.a",
    ". el - beltagy and w.a .",
    "wright , `` gaussian processes for model fusion '' , in proc . of the international conference on artificial neural networks ( icann ) , 2001 .",
    "r. murray - smith and b.a .",
    "pearlmutter , `` transformations of gaussian process priors '' , chapter in deterministic and statistical methods in machine learning , lecture notes in ai ( lnai ) 3635 , pages 110123 , springer - verlag , 2005 .",
    "m. girolami , `` bayesian data fusion with gaussian process priors : an application to protein fold recognition '' , in workshop on probabilistic modeling and machine learning in structural and systems biology ( pmsb ) , 2006 .",
    "s. reece , s. roberts , d. nicholson and c. lloyd , `` determining intent using hard / soft data and gaussian process classifiers '' , in proc . of the 14th international conference on information fusion ( fusion ) , 2011 .",
    "s. vasudevan , `` data fusion using gaussian processes '' , elsevier journal of robotics and autonomous systems , volume 60 , issue 12 , december 2012 , pages 15281544 .",
    "thompson and d. wettergreen , `` intelligent maps for autonomous kilometer - scale science survey '' , in proc . of the international symposium on artificial intelligence , robotics and automation in space ( i - sairas ) , 2008 .",
    "s. dragiev , m. toussaint and m. gienger , `` gaussian process implicit surfaces for shape estimation and grasping '' , in proc . of the ieee international conference on robotics and automation ( icra ) , pages 28452850 , 2011 .",
    "s. vasudevan , f. ramos , e. nettleton and h. durrant - whyte , `` heteroscedastic gaussian processes for data fusion in large scale terrain modeling '' , in proc . of the international conference for robotics and automation ( icra ) , 2010 .",
    "s. vasudevan , f. ramos , e. nettleton and h. durrant - whyte , `` large - scale terrain modeling from multiple sensors with dependent gaussian processes '' , in the proc . of the ieee / rsj international conference on intelligent robots and systems ( iros ) , taipei , october 2010 .",
    "s. vasudevan , f. ramos , e. nettleton and h. durrant - whyte , `` non - stationary dependent gaussian processes for data fusion in large scale terrain modeling '' , in the proc . of the ieee international conference on robotics and automation ( icra ) , shanghai , china , 2011 .",
    "goldberg , c.k.i .",
    "williams , and c.m .",
    "bishop , `` regression with input - dependent noise : a gaussian process treatment '' , in m.i .",
    "jordan , m.j .",
    "kearns , s.a .",
    "solla and l. erlbaum ( editors ) , advances in neural information processing systems ( nips ) 10 , mit press , cambridge , ma , 1998 .",
    "m. yuan and g. wabha , `` doubly penalized likelihood estimator in heteroscedastic regression '' , technical report , department of statistics , university of wisconsin , madison , usa , 2004 .",
    "smola , and s. canu , `` heteroscedastic gaussian process regression '' , in proc . of the international conference on machine learning ( icml ) , 2005 .",
    "k. kersting , c. plagemann , p. pfaff and w. burgard , `` most likely heteroscedastic gaussian process regression '' , in proc . of the international conference on machine learning ( icml ) , 2007 .",
    "e. bonilla , k.m .",
    "chai and c. williams , `` multi - task gaussian process prediction '' , in j.c .",
    "platt , d. koller , y. singer , and s. roweis ( editors ) , advances in neural information processing systems ( nips ) 20 , pages 153160 , mit press , cambridge , ma , 2007 .    p. boyle and m. frean , `` dependent gaussian processes '' , in l. saul , y. weiss and l. bottou ( editors ) , advances in neural information processing systems ( nips ) 17 , pages 217224 , mit press , cambridge , ma , 2004 .",
    "d. higdon , `` space and space - time modeling using process convolutions '' , chapter in quantitative methods for current environmental issues , pages 3754 , springer , 2002 .",
    "h. wackernagel , `` multivariate geostatistics : an introduction with applications '' , springer , 2003 .",
    "a. melkumyan and f. ramos , `` multi - kernel gaussian processes '' , in proc . of the international joint conference on artificial intelligence ( ijcai ) , 2011 .",
    "a. melkumyan and f. ramos , `` a sparse covariance function for exact gaussian process inference in large data sets '' , in proc . of the international joint conferences on artificial intelligence ( ijcai ) ,",
    "volume 21 , pages 19361942 , 2009 .",
    "m. alvarez , l. rosasco and n.d .",
    "lawrence , `` kernels for vector - valued functions : a review '' , in foundations and trends in machine learning , pages 195266 , 2012 .",
    "neal , `` bayesian learning for neural networks '' , lecture notes in statistics 118 .",
    "springer , new york , 1996 .",
    "williams , `` computation with infinite neural networks '' , neural computation , volume 10(5 ) , pages 12031216 , 1998 .",
    "williams , `` prediction with gaussian processes : from linear regression to linear prediction and beyond '' , in m. jordan ( editor ) , learning in graphical models , pages 599622 , springer , 1998 .",
    "k. hornik , `` some new results on neural network approximation '' , neural networks , volume 6(8 ) , pages 10691072 , 1993 .",
    "m. alvarez and n.d .",
    "lawrence , `` sparse convolved gaussian processes for multi - output regression '' , in d. koller , d. schuurmans , y. bengio and l. bottou ( editors ) , advances in neural information processing systems ( nips ) 21 , pages 5764 , 2009 .",
    "r. kohavi , `` a study of cross - validation and bootstrap for accuracy estimation and model selection '' , in proc . of the international joint conferences on artificial intelligence ( ijcai ) ,",
    "volume 14 , pages 11371145 , 1995 ."
  ],
  "abstract_text": [
    "<S> this paper evaluates heterogeneous information fusion using multi - task gaussian processes in the context of geological resource modeling . </S>",
    "<S> specifically , it empirically demonstrates that information integration across heterogeneous information sources leads to superior estimates of all the quantities being modeled , compared to modeling them individually . </S>",
    "<S> multi - task gaussian processes provide a powerful approach for simultaneous modeling of multiple quantities of interest while taking correlations between these quantities into consideration . </S>",
    "<S> experiments are performed on large scale real sensor data . </S>",
    "<S> +    - gaussian process , information fusion , geological resource modeling </S>"
  ]
}