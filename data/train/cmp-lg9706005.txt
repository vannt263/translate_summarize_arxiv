{
  "article_text": [
    "there are currently two main methods for automatic part - of - speech tagging .",
    "the prevailing one uses essentially statistical language models automatically derived from usually hand - annotated corpora .",
    "these corpus - based models can be represented e.g.  as collocational matrices ( garside et al .",
    "1987 ; church 1988 ) , hidden markov models ( cf .  cutting et al .",
    "1992 ) , local rules ( e.g.  hindle 1989 ) and neural networks ( e.g.  schmid 1994 ) .",
    "taggers using these statistical language models are generally reported to assign the correct and unique tag to 95 - 97% of words in running text , using tag sets ranging from some dozens to about 130 tags .",
    "the less popular approach is based on hand - coded linguistic rules .",
    "pioneering work was done in the 1960 s ( e.g.  greene and rubin 1971 ) .",
    "recently , new interest in the linguistic approach has been shown e.g.  in the work of ( karlsson 1990 ; voutilainen et al .",
    "1992 ; oflazer and kuruz 1994 ; chanod and tapanainen 1995 ; karlsson et al .",
    "1995 ; voutilainen 1995 ) .",
    "the first serious linguistic competitor to data - driven statistical taggers is the english constraint grammar parser , engcg ( cf .",
    "voutilainen et al .",
    "1992 ; karlsson et al .",
    "the tagger consists of the following sequentially applied modules :    1 .",
    "tokenisation 2 .",
    "morphological analysis 1 .",
    "lexical component 2 .",
    "rule - based guesser for unknown words 3 .",
    "resolution of morphological ambiguities    the tagger uses a two - level morphological analyser with a large lexicon and a morphological description that introduces about 180 different ambiguity - forming morphological analyses , as a result of which each word gets 1.7 - 2.2 different analyses on an average .",
    "morphological analyses are assigned to unknown words with an accurate rule - based ` guesser ' .",
    "the morphological disambiguator uses constraint rules that discard illegitimate morphological analyses on the basis of local or global context conditions .",
    "the rules can be grouped as ordered subgrammars : e.g.  heuristic subgrammar 2 can be applied for resolving ambiguities left pending by the more ` careful ' subgrammar 1 .",
    "older versions of engcg ( using about 1,150 constraints ) are reported ( voutilainen et al .",
    "1992 ; voutilainen and heikkil 1994 ; tapanainen and voutilainen 1994 ; voutilainen 1995 ) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04 - 1.09 alternative analyses on an average , i.e.  some of the ambiguities remain unresolved .",
    "these results have been seriously questioned .",
    "one doubt concerns the notion `` correct analysis '' . for example church ( 1992 ) argues that linguists who manually perform the tagging task using the double - blind method disagree about the correct analysis in at least 3% of all words even after they have negotiated about the initial disagreements .",
    "if this were the case , reporting accuracies above this 97% ` upper bound ' would make no sense .",
    "however , voutilainen and jrvinen ( 1995 ) empirically show that an interjudge agreement virtually of 100% is possible , at least with the engcg tag set if not with the original brown corpus tag set .",
    "this consistent applicability of the engcg tag set is explained by characterising it as grammatically rather than semantically motivated .",
    "another main reservation about the engcg figures is the suspicion that , perhaps partly due to the somewhat underspecific nature of the engcg tag set , it must be so easy to disambiguate that also a statistical tagger using the engcg tags would reach at least as good results .",
    "this argument will be examined in this paper . it will be empirically shown ( i ) that the engcg tag set is about as difficult for a probabilistic tagger as more generally used tag sets and ( ii ) that the engcg disambiguator has a clearly smaller error rate than the probabilistic tagger when a similar ( small ) amount of ambiguity is permitted in the output .",
    "a state - of - the - art statistical tagger is trained on a corpus of over 350,000 words hand - annotated with engcg tags , then both taggers ( a new version known as engcg-2 with 3,600 constraints as five subgrammars , and a statistical tagger ) are applied to the same held - out benchmark corpus of 55,000 words , and their performances are compared .",
    "the results disconfirm the suspected ` easiness ' of the engcg tag set : the statistical tagger s performance figures are no better than is the case with better known tag sets .",
    "two caveats are in order .",
    "what we are not addressing in this paper is the work load required for making a rule - based or a data - driven tagger .",
    "the rules in engcg certainly took a considerable effort to write , and though at the present state of knowledge rules could be written and tested with less effort , it may well be the case that a tagger with an accuracy of 95 - 97% can be produced with less effort by using data - driven techniques .",
    "another caveat is that engcg alone does not resolve all ambiguities , so it can not be compared to a typical statistical tagger if full disambiguation is required .",
    "however , voutilainen ( 1995 ) has shown that engcg combined with a syntactic parser produces morphologically unambiguous output with an accuracy of 99.3% , a figure clearly better than that of the statistical tagger in the experiments below ( however , the test data was not the same ) .    before examining the statistical tagger ,",
    "two practical points are addressed : the annotation of the corpora used , and the modification of the engcg tag set for use in a statistical tagger .",
    "the stochastic tagger was trained on a sample of 357,000 words from the brown university corpus of present - day english @xcite that was annotated using the engcg tags .",
    "the corpus was first analysed with the engcg lexical analyser , and then it was fully disambiguated and , when necessary , corrected by a human expert .",
    "this annotation took place a few years ago .",
    "since then , it has been used in the development of new engcg constraints ( the present version , engcg-2 , contains about 3,600 constraints ) : new constraints were applied to the training corpus , and whenever a reading marked as correct was discarded , either the analysis in the corpus , or the constraint itself , was corrected . in this way , the tagging quality of the corpus was continuously improved .",
    "our comparisons use a held - out benchmark corpus of about 55,000 words of journalistic , scientific and manual texts , i.e. , no training effects are expected for either system .",
    "the benchmark corpus was annotated by first applying the preprocessor and morphological analyser , but not the morphological disambiguator , to the text .",
    "this morphologically ambiguous text was then independently and fully disambiguated by two experts whose task was also to detect any errors potentially produced by the previously applied components .",
    "they worked independently , consulting written documentation of the tag set when necessary .",
    "then these manually disambiguated versions were automatically compared with each other . at this stage , about 99.3% of all analyses were identical .",
    "when the differences were collectively examined , virtually all were agreed to be due to clerical mistakes . only in the analysis of 21 words , different ( meaning - level ) interpretations persisted , and",
    "even here both judges agreed the ambiguity to be genuine .",
    "one of these two corpus versions was modified to represent the consensus , and this ` consensus corpus ' was used as a benchmark in the evaluations .    as explained in voutilainen and jrvinen ( 1995 ) ,",
    "this high agreement rate is due to two main factors .",
    "firstly , distinctions based on some kind of vague semantics are avoided , which is not always case with better known tag sets .",
    "secondly , the adopted analysis of most of the constructions where humans tend to be uncertain is documented as a collection of tag application principles in the form of a grammarian s manual ( for further details , cf .",
    "voutilainen and jrvinen 1995 ) .",
    "the corpus - annotation procedure allows us to perform a text - book statistical hypothesis test .",
    "let the null hypothesis be that any two human evaluators will necessarily disagree in at least 3% of the cases . under this assumption ,",
    "the probability of an observed disagreement of less than 2.88% is less than 5% .",
    "this can be seen as follows : for the relative frequency of disagreement , @xmath0 , we have that @xmath0 is approximately @xmath1 , where @xmath2 is the actual disagreement probability and @xmath3 is the number of trials , i.e. , the corpus size .",
    "this means that @xmath4 where @xmath5 is the standard normal distribution function .",
    "this in turn means that @xmath6 here @xmath3 is 55,000 and @xmath7 . under the null hypothesis ,",
    "@xmath2 is at least 3% and thus : @xmath8 we can thus discard the null hypothesis at significance level 5% if the observed disagreement is less than 2.88% .",
    "it was in fact 0.7% before error correction , and virtually zero ( @xmath9 ) after negotiation .",
    "this means that we can actually discard the hypotheses that the human evaluators in average disagree in at least 0.8% of the cases before error correction , and in at least 0.1% of the cases after negotiations , at significance level 5% .",
    "the engcg morphological analyser s output formally differs from most tagged corpora ; consider the following 5-ways ambiguous analysis of `` walk '' :    .... walk     walk < sv >",
    "< svo > v subjunctive vfin     walk < sv >",
    "< svo > v imp vfin     walk < sv >",
    "< svo > v inf     walk < sv > < svo > v pres -sg3",
    "vfin     walk n nom sg ....    statistical taggers usually employ single tags to indicate analyses ( e.g.  `` nn '' for `` n nom sg '' ) . therefore a simple conversion program was made for producing the following kind of output , where each reading is represented as a single tag :    .... walk v - subjunctive v - imp v - inf       v - pres - base n - nom - sg ....    the conversion program reduces the multipart engcg tags into a set of 80 word tags and 17 punctuation tags ( see appendix ) that retain the central linguistic characteristics of the original engcg tag set .",
    "a reduced version of the benchmark corpus was prepared with this conversion program for the statistical tagger s use .",
    "also engcg s output was converted into this format to enable direct comparison with the statistical tagger .",
    "the statistical tagger used in the experiments is a classical trigram - based hmm decoder of the kind described in e.g.   @xcite , @xcite and numerous other articles .",
    "following conventional notation , e.g. @xcite and @xcite , the tagger recursively calculates the @xmath10 , @xmath11 , @xmath12 and @xmath13 variables for each word string position @xmath14 and each possible state @xmath15 : @xmath16 here @xmath17 where @xmath18 is the event of the @xmath19th word being emitted from state @xmath20 and @xmath21 is the event of the @xmath19th word being the particular word @xmath22 that was actually observed in the word string",
    ".    note that for @xmath23 ; @xmath24 @xmath25 \\cdot a_{jk_{t+1}}\\\\ \\beta_t(i ) & = & \\sum_{j=1}^n \\beta_{t+1}(j ) \\cdot p_{ij } \\cdot a_{jk_{t+1}}\\\\ \\delta_{t+1}(j ) & = & \\left[\\max_i \\delta_t(i ) \\cdot p_{ij}\\right ] \\cdot a_{jk_{t+1}}\\end{aligned}\\ ] ] where @xmath26 are the transition probabilities , encoding the tag n - gram probabilities , and @xmath27 are the lexical probabilities . here",
    "@xmath28 is the random variable of assigning a tag to the @xmath19th word and @xmath29 is the last tag of the tag sequence encoded as state @xmath30 .",
    "note that @xmath31 need not imply @xmath32 .",
    "more precisely , the tagger employs the converse lexical probabilities @xmath33 this results in slight variants @xmath34 , @xmath35 , @xmath36 and @xmath37 of the original quantities : @xmath38 and thus @xmath39 @xmath40 and @xmath41 @xmath42    the rationale behind this is to facilitate estimating the model parameters from sparse data . in more detail , it is easy to estimate @xmath43 for a previously unseen word by backing off to statistics derived from words that end with the same sequence of letters ( or based on other surface cues ) , whereas directly estimating @xmath44 is more difficult .",
    "this is particularly useful for languages with a rich inflectional and derivational morphology , but also for english : for example , the suffix `` -tion '' is a strong indicator that the word in question is a noun ; the suffix `` -able '' that it is an adjective .",
    "more technically , the lexicon is organised as a reverse - suffix tree , and smoothing the probability estimates is accomplished by blending the distribution at the current node of the tree with that of higher - level nodes , corresponding to ( shorter ) suffixes of the current word ( suffix ) .",
    "the scheme also incorporates probability distributions for the set of capitalized words , the set of all - caps words and the set of infrequent words , all of which are used to improve the estimates for unknown words .",
    "employing a small amount of back - off smoothing also for the known words is useful to reduce lexical tag omissions .",
    "empirically , looking two branching points up the tree for known words , and all the way up to the root for unknown words , proved optimal .",
    "the method for blending the distributions applies equally well to smoothing the transition probabilities @xmath45 , i.e. , the tag n - gram probabilities , and both the scheme and its application to these two tasks are described in detail in @xcite , where it was also shown to compare favourably to ( deleted ) interpolation , see @xcite , even when the back - off weights of the latter were optimal .",
    "the @xmath13 variables enable finding the most probable state sequence under the hmm , from which the most likely assignment of tags to words can be directly established .",
    "this is the normal modus operandi of an hmm decoder . using the @xmath12 variables",
    ", we can calculate the probability of being in state @xmath20 at string position @xmath19 , and thus having emitted @xmath22 from this state , conditional on the entire word string . by summing over all states that would assign the same tag to this word ,",
    "the individual probability of each tag being assigned to any particular input word , conditional on the entire word string , can be calculated : @xmath46 this allows retaining multiple tags for each word by simply discarding only low - probability tags ; those whose probabilities are below some threshold value .",
    "of course , the most probable tag is never discarded , even if its probability happens to be less than the threshold value . by varying the threshold",
    ", we can perform a recall - precision , or error - rate - ambiguity , tradeoff .",
    "a similar strategy is adopted in @xcite .",
    "the statistical tagger was trained on 357,000 words from the brown corpus @xcite , reannotated using the engcg annotation scheme ( see above ) . in a first set of experiments , a 35,000 word subset of this corpus",
    "was set aside and used to evaluate the tagger s performance when trained on successively larger portions of the remaining 322,000 words .",
    "the learning curve , showing the error rate after full disambiguation as a function of the amount of training data used , see figure  [ figx ] , has levelled off at 322,000 words , indicating that little is to be gained from further training .",
    "we also note that the absolute value of the error rate is 3.51%  a typical state - of - the - art figure . here , previously unseen words contribute 1.08% to the total error rate , while the contribution from lexical tag omissions is 0.08% .",
    "95% confidence intervals for the error rates would range from @xmath47 0.30% for 30,000 words to @xmath47 0.20% at 322,000 words .",
    "the tagger was then trained on the entire set of 357,000 words and confronted with the separate 55,000-word benchmark corpus , and run both in full and partial disambiguation mode .",
    "table  [ table ] shows the error rate as a function of remaining ambiguity ( tags / word ) both for the statistical tagger , and for the engcg-2 tagger .",
    "the error rate for full disambiguation using the @xmath13 variables is 4.72% and using the @xmath12 variables is 4.68% , both @xmath48 with confidence degree 95% .",
    "note that the optimal tag sequence obtained using the @xmath12 variables need not equal the optimal tag sequence obtained using the @xmath13 variables .",
    "in fact , the former sequence may be assigned zero probability by the hmm , namely if one of its state transitions has zero probability ."
  ],
  "abstract_text": [
    "<S> concerning different approaches to automatic pos tagging : engcg-2 , a constraint - based morphological tagger , is compared in a double - blind test with a state - of - the - art statistical tagger on a common disambiguation task using a common tag set . </S>",
    "<S> the experiments show that for the same amount of remaining ambiguity , the error rate of the statistical tagger is one order of magnitude greater than that of the rule - based one . </S>",
    "<S> the two related issues of priming effects compromising the results and disagreement between human annotators are also addressed . </S>"
  ]
}