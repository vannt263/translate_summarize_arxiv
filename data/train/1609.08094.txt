{
  "article_text": [
    "when formulating the criteria for what defines _",
    "portable _ source code , one tends to draw a distinction between two types of portability , namely that associated with functionality and that associated with performance . while the former of these covers",
    "what is perhaps usually implied when referring to code portability , i.e. , the ability of a single code to run anywhere ( rebuild the code on various architectures , the code will run and produce correct results ) , the latter , the existence of which have often been declared a fallacy  , relates to the ability of a single code to run productively anywhere , i.e. , achieving , say , @xmath1 of the hand - tuned performance on any given architecture . as such",
    ", making a code portable will often result in a compromise between superior performance on a single platform and decent performance on all potential platforms .",
    "one possible way of circumventing this problem is by cluttering the source code with multiple conditional preprocessor directives and retain architecture - specific versions of key kernels with vendor - specific intrinsics .",
    "however , pursuing this strategy is bound to severely hamper two of the other main best practices of code development , namely those which are concerned with securing code _",
    "maintainability _ and testability  @xcite . + for execution on standard multi - core central processing unit ( cpu ) architectures , the _ openmp standard _",
    "@xcite  first released in the second half of the 1990 s as a means to parallelize loops across compute threads ",
    "has nowadays manifested itself as the de facto programming model for intranode worksharing and , in particular , single - instruction multiple - data ( simd ) parallel instructions . the advantages of employing openmp compiler directives are manifold ; the standard is open and supported by most vendors , the compiler directives ( or pragmas ) are ignored entirely by those few compilers that do not offer support , and the syntax is relatively simple to comprehend , even to the uninitiated , making the learning barrier less steep than , e.g. , that for intra- and internode message passing using the mpi protocol .",
    "recently , with the advent of openmp versions 4.0 and 4.5 , the standard now also allows for targeting other types of architectures such as many - core x86 processors , e.g. , intel^^ xeon phi^^ products , and graphics processing units ( gpus ) , e.g. , nvidia^^ or amd^^ gpus .",
    "these feature enhancements from homo- to heterogenenous platforms allow for offloading of compute intensive code regions to coprocessors via so - called _ device _ constructs ( in this terminology , the cpu , to which the coprocessor is connected , is denoted the _ host _ ) .",
    "however , there exist intrinsic difficulties in transferring the worksharing capabilities of openmp on the host to a corresponding worksharing on any attached device , in particular gpus , as the architectural details of these are fundamentally different from those of the host .",
    "specifically , the traditional openmp model of spawning a single team of threads , which then divide loop iterations among them , will not be suitable per se for execution on gpus , as these are composed of multiple processing elements ( pes ) that run in parallel ( with no synchronization options between the threads in different pes ) and where each individual pe has the ability to efficiently perform vector - like operations .",
    "there are ways to bypass this issue , though , for instance by creating multiple teams of threads , across which the workload may be distributed .",
    "however , this is almost guaranteed to result in _ accelerated _ code that will not execute satisfactorily on the host ( thereby losing performance portability ) , which will reintroduce the need for * ( i ) * preprocessing of different code blocks in addition to * ( ii ) * code duplication with the required maintenance that inevitably arises as a result of this  .",
    "+ instead , an alternative standard  the _ openacc standard _  @xcite , recently founded by a consortium consisting of nvidia^^ and several key vendors , including cray^^ and pgi^^has been established in an attempt to facilitate such portable and maintainable parallelism across various architectures , that is , provide the ability to expose to the compiler in a simple way what parts of the code can be run in parallel , and then leave it to the compiler to build the accelerated code _ on a given platform _ , be that the host ( as a real alternative to openmp ) or the device ( as an inherently different approach to openmp ) . since openacc is designed with heterogenous programming in mind",
    ", it is not biased towards multi - core cpu architectures in the same way as openmp is , while still being able to advantage from past endeavours made by compiler developers that tuned these towards vectorization . in particular , as opposed to adhering to a strictly prescriptive programming model of simple thread teams , the openacc standard extends this by adding an additional layer of flexibility ; for instance , when targeting gpus , on which it is reasonable to think of a pe as a streaming multiprocessor ( sm ) , the standard offers the possibility of parallelizing over threadblocks in a grid , i.e. , individual sms , warps ( a group of 32 threads comprising the minimum execution unit ) , as well as individual cuda threads within a warp . in the openacc terminology ,",
    "these three layers are known as gangs , workers , and vectors .",
    "+ the openacc programming model is a so - called host - directed sequential model capable of leveraging the parallel potential of one or even multiple accelerator devices , be that gpus or multi - core coprocessors .",
    "the constructs , which the standard is designed to target , are the same as those the openmp standard are aimed at , namely nested loop structures , which themselves are suitable for shared memory multiprocessing .",
    "the translator between the input program and the individual simd units on the target architecture is the compiler , and the vectorization inhibitors are thus the same as for openmp , e.g. , loop carried dependencies , prohibitive complexity ( e.g. , function calls ) , as well as indirect addressing and striding . as the compiler is directed by pragmas used to expose the parallel regions in the code , the code developer may approach the problem of offloading a certain kernel in a descriptive and , importantly , incremental manner , which makes for a less cumbersome workflow than if the entire code has to be implemented from scratch using some low - level approach unrelated to the language in which the actual code base is written ( e.g. , cuda or opencl , in a gpu context ) . furthermore , as the accelerated code will be based on the original source , this warrants , in turn , a functionally and performance portable , intuitively transparent , and easily extendable and maintainable implementation .",
    "+ in the present work , it will be demonstrated how to make use of openacc compiler directives in the acceleration of electronic structure theories , with an illustrative application to two popular many - body methods , namely the second - order mller - plesset ( mp2 ) model  @xcite in its resolution - of - the - identity ( ri ) approximated form and the preeminent perturbative ( t ) triples correction to the coupled cluster  @xcite ( cc ) singles and doubles ( ccsd ) model  @xcite , compositely known as the ccsd(t ) model  @xcite . to focus the discussion",
    ", the present work will be solely concerned with offloading to gpu accelerators , although we stress once again how the openacc standard , and hence the present approach , is completely general and , as such , not limited to nor biased against accelerators of a specific kind .",
    "on that note , we will start by briefly alluding to the existing literature , which  despite the use of general - purpose gpus in natural sciences being a rather recent topic  is rich with work devoted to gpu - accelerated quantum chemistry , concerned with diverse topics ranging from the generation of electron repulsion integrals ( eris )  @xcite , over self - consistent hartree - fock ( hf ) , complete active space ( cas ) , and density functional theory ( dft ) methods  @xcite , to solvent models  @xcite , force fields  @xcite , and semi - empirical  @xcite as well as many - body methods  @xcite . adding to this quantum , several program packages are today released with full or partial gpu - enabled features , e.g. , terachem  @xcite , nwchem  @xcite , among others .",
    "+ that said , the adaption of gpus and other types of coprocessors in the acceleration of quantum chemical and physical methods and applications as a whole has often been criticized for being too difficult a task to undertake implementation - wise and hence not worth the effort , in part also because of the uncertainty of ( the shape of / general consensus on ) future architectures that have always permeated the community ( hesitation towards vectorization , massive parallelism , high - performance computing , etc . )  @xcite .",
    "on top of that , it is important to note how the clear majority of all quantum chemical implementations in existence today have been developed and are being maintained by domain scientists , many of which hold temporary positions working ( simultaneously ) on various theoretical and/or application - oriented projects  @xcite .",
    "thus , while said scientists might decide to implement a given accelerated method from scratch ( again , presumably making use of some low - level approach ) , they might not be responsible for extending it with new features and capabilities in the future nor for the routine maintenance required with the emergence of novel architectures , not to mention the finer architectural diversities between different generations of , for instance , gpu accelerators .",
    "add to that the typical requirements of a single code base and , in particular , platform - independence , which most codes are subject to in the sense that any addition of accelerated code must not interfere with the standard compilation process ( cf .",
    "the discussion in the opening paragraph of the present section ) , and one will potentially arrive at more reasons against investing the necessary efforts than actual reasons in favor of doing so .",
    "+ however , a recent paper from the martnez group at stanford university has outlined an alternative strategy for avoiding the above issues , albeit one targeted exclusively at gpu hardware  @xcite .",
    "the paper , which succeeds earlier work on so - called meta - programming from within the same group  @xcite , proposes the application of an automatic code generator capable of employing mathematical knowledge of the underlying algorithms ( for the efficient generation of eris , in this case ) to create a wide spectrum of possible code variants that explore , in their own words , the full space of all possible program implementations .",
    "this type of empirical , so - called profile - guided optimization subsequently tests the ( multiple ) candidate implementations on the target architecture at hand before deciding upon a final version , and the combination of a graph - based code generator , a tester , and an optimizer hence aims at producing the optimal implementation for previous and current generations of gpus as well as , at least in principle , future generations .",
    "+ these numerous code variants , which may be automatically produced using the engine in ref .",
    ", will each make up different compromises between a number of parameters and factors ; for instance , the efficiency of a given kernel will be tightly bound to its utilization of the complex memory hierarchy on the gpus , the latency of which increases drastically upon traversing up through it , and the algorithmic complexity of the individual kernels trivially increases with respect to the maximum angular momentum of the one - electron basis set . furthermore , the choice of resulting implementation will also depend on the choice of floating point precision , since lowering this from double to single precision ",
    "granted that the requested accuracy allows for it  may give preference to other variants .",
    "importantly , however , the use of single precision arithmetics will result in an overall speed - up of the calculation , for execution on the host cpu and particularly so for execution on gpus .",
    "now , in comparison with the generation of eris , the computation of correlation energies using ( perturbative ) many - body methods poses rather different requirements on the final implementation , as we will return to in [ implementations_section ] .",
    "that being said , it is the intent of the present author to illustrate , through openacc implementations of the ri - mp2 and ccsd(t ) methods , how simple and transparent such methods may be accelerated .",
    "it should be noted , however , that the adaption of an existing cpu eri code to gpus ( or other types of accelerators , _ vide supra _ ) by means of openacc directives will no doubt be more challenging , but it is still probably easier for the developer than using , e.g. , cuda and writing the code from scratch . in particular , the programmer may start by incrementally porting one kernel after another by exposing the inherent parallelism ( loop structures ) of the algorithm using directives , thereby verifying the correctness of the implementation along the way prior to subjecting the initial implementation to a subsequent optimization . in addition , the final implementation will require a minimum of maintenance , as the optimizer  with respect to novel architectures and compute capabilities  remains the compiler itself , and since the directives are treated as mere comments to a non - accelerating compiler , this applies for the efforts that will have to be invested into platform - independence of the source as well .",
    "+ the present work is organized as follows .",
    "following a brief introduction to the ri - mp2 and ccsd(t ) methods in [ theory_section ] , the actual openacc implementations of these  using a combination of single and double precision arithmetics  are discussed in [ implementations_section ] . in [ com_details_section ] , we provide some computational details on the calculations to follow in [ results_section ] , in which the performance of the implementations is assessed through calculations on test systems consisting of alanine amino acids in @xmath2-helix arrangements using one - electron basis sets of increasing size ( ranging from double- to pentuple-@xmath0 quality ) . finally , a short summary alongside some conclusive remarks are presented in [ conclusion_section ] .",
    "in electronic structure theory , cc and many - body perturbation theory  @xcite ( mbpt ) both offer a systematic approach towards the full configuration interaction  @xcite ( fci ) wave function  the exact solution to the time - independent , non - relativistic electronic schrdinger equation within a given one - electron basis set . in both hierarchies of methods , the mean - field hf solution acts as the reference to which correlated corrections are made by including excited configurations in the wave function . moving beyond the hf approximation , not only fermi correlation",
    "is considered , as also the correlated motion of electrons of opposite spin start being described .",
    "this implies that upon traversing up through either of the cc or mbpt hierarchies , increasingly more of such dynamical correlation is included in the wave function , with a complete description met at the target fci limit .",
    "+ in the mbpt series , the lowest - order perturbative correction to the hf energy for the isolated effect of connected double excitations is that of the non - iterative mp2 model  @xcite . as an improvement",
    ", the cc hierarchy offers the iterative ccsd model  @xcite , which accounts not only for connected double excitations , but does so to infinite order in the space of all single and double excitations out of the hf reference .",
    "however , in order to achieve relative energies of chemical accuracy ( @xmath3 kcal / mol ) , the ccsd energy has to be augmented by correction terms built from higher - level excitations .",
    "to a lowest approximation , corrections for effects due to connected triple excitations have to be considered . among such higher - level approaches ,",
    "the ccsd(t ) model  @xcite is by far the most prominent .",
    "unfortunately , the additional accuracy of the ccsd and , in particular , the ccsd(t ) models over the mp2 model comes at a price , with the evaluation of the rate - determining step scaling non - iteratively as @xmath4 in the mp2 model ( where @xmath5 is a composite measure of the total system size ) , iteratively as @xmath6 in the ccsd model , and non - iteratively as @xmath7 for the ( t ) correction to the converged ccsd energy .",
    "thus , whereas all three methods have nowadays become standard tools to computational quantum chemists , the current application range of the mp2 model considerably exceeds that of the ccsd(t ) model , which are the two models that we will limit our focus to herein , due to the practical differences in associated computational cost .      in spite of the fact that the mp2 model is formally far less expensive than the ccsd(t ) model , the time - to - solution for the average mp2 calculation may still be substantial when targeting increasingly larger systems , e.g. , in comparison with cheaper , albeit less rigorous and less systematic methods such as those of semi - empirical nature or those that calculate the energy from a functional of the one - electron density ( dft methods ) .",
    "for this reason , most efficient implementations of the mp2 model invoke an ri approximation for reducing the computational cost ( the prefactor ) as well as lowering the memory constraints  @xcite . despite being an approximation to the mp2 model ,",
    "the focused development of optimized auxiliary basis sets has managed to significantly reduce the inherent ri error , and for most reasonable choices of the intrinsic thresholds of a modern implementation , the error affiliated with the actual approximation will be negligible . however , while the computational cost is notably reduced in an ri - mp2 calculation with respect to its mp2 counterpart , its formal @xmath4 scaling with the size of the system will often still deem it demanding if no fundamental algorithmic changes are made  @xcite . + in the closed - shell ( canonical ) mp2 model , the correlation energy is given by the following expression @xmath8 in terms of two - electron eris , @xmath9 ( mulliken notation ) , over spatial ( spin - free ) hf virtual orbitals @xmath10 and occupied orbitals @xmath11 , as well as the difference in energy between these @xmath12 besides the final evaluation of the mp2 energy in [ mp2_energy ] , the dominant part of an mp2 calculation is the construction of the four - index eris . in the ri - mp2 method , these are approximated by products of two- , @xmath13 , and three - index , @xmath14 , integrals by means of the following symmetric decomposition @xmath15 in [ eri_sym_decomp ] , greek indices denote atomic orbitals within an auxiliary fitting basis set used for spanning the ri , and the fitting coefficients , @xmath16 , are defined as @xmath17_{\\alpha\\gamma } \\ .",
    "\\label{fitting_coef}\\end{aligned}\\ ] ] in terms of computational cost , the evaluation of two- and three - index integrals and/or the calculation of the fitting coefficients will dominate the overall calculation for small systems ( @xmath18- and @xmath19-scaling processes , respectively ) .",
    "however , upon an increase in system size , the final @xmath4-scaling assembly of the two - electron integrals in [ eri_sym_decomp ] will start to dominate , and this process is thus an ideal candidate for accelerating the ri - mp2 model , cf .",
    "[ implementations_section ] .",
    "as touched upon in the opening paragraph of the present section , the ( t ) correction scales non - iterative as @xmath7 , as opposed to the @xmath6-scaling iterative evaluation of the energy and cluster amplitudes in the preceding ccsd calculation .",
    "thus , for all but the smallest systems , the evaluation of the ( t ) correction , at least in its conventional canonical formulation , will dominate . in a basis of local hf orbitals , however , this balance might shift , making the underlying evaluation of the ccsd energy ( and amplitudes ) the computational bottleneck  @xcite . + for closed - shell systems , the ccsd(t ) energy is defined by a triples correction , @xmath20 , to the ccsd energy , in turn defined by two energy contributions rationalized from mbpt : @xmath21}$ ] , a fourth - order term involving ccsd doubles amplitudes , @xmath22 , and @xmath23}$ ] , a fifth - order term involving ccsd singles amplitudes , @xmath24 .",
    "in a basis of canonical hf spatial orbitals , these may be expressed as  @xcite    [ piecuch_4th_and_5th_order ] @xmath25 } & = \\sum_{abc}\\sum_{ijk}\\tilde{t}^{abc}_{ijk}t^{abc}_{ijk}\\epsilon^{abc}_{ijk } \\label{piecuch_4th_order } \\\\",
    "e^{[5 ] } & = \\sum_{abc}\\sum_{ijk}\\tilde{z}^{abc}_{ijk}t^{abc}_{ijk}\\epsilon^{abc}_{ijk } \\ .",
    "\\label{piecuch_5th_order } \\end{aligned}\\ ] ]    in [ piecuch_4th_and_5th_order ] , @xmath26 is defined on par with @xmath27 in [ e_orb_diff ] , and the triples amplitudes , @xmath28 , are defined as @xmath29 where @xmath30 is a symmetrization operator @xmath31 the @xmath32 coefficients in [ piecuch_5th_order ] are given as @xmath33 and an arbitrary six - index quantity @xmath34 in [ piecuch_4th_and_5th_order ] is defined as @xmath35 the rate - determining step is now identified as being the construction of the triples amplitudes in [ analytical_triples_ampl ] , whereas the final evaluation of @xmath20 only scales as @xmath6 . as opposed to the evaluation of the ri - mp2 energy in [ rimp2_theory_subsection ] , for which only parts of the algorithm will be accelerated ( [ mp2_energy ] and [ eri_sym_decomp ] combined ) , all of the ( t ) kernels will be offloaded to the accelerator(s ) , as will be detailed in the following [ implementations_section ] .",
    "as is obvious from [ theory_section ] , the evaluation of both the ri - mp2 energy and the ( t ) correction relies on a vast number of matrix - matrix multiplications ( mmms ) , which in all modern implementations of the methods are handled by making calls to the ` dgemm ` routine of an optimized level-3 basic linear algebra subprograms  @xcite ( blas-3 ) library .",
    "the fact that both methods may be recasted such that they utilize blas libraries optimized for modern multi - core cpu architectures ( e.g. , intel^^ mkl , atlas  @xcite , or gotoblas  @xcite , to name a few ) implies that they are amenable to accelerators as well ( gpus , in the present context ) , for which optimized blas libraries too exist , e.g. , magma  @xcite and cublas  @xcite .",
    "thus , in contrast to the generation of eris , cf .",
    "the discussion in [ intro_section ] , which is comprised of extensive parallel regions , each with numerous tightly nested loop structures , this forces somewhat different requirements on an accelerated implementation for coprocessors ; in particular , aspects such as efficient data movement patterns and data reuse become highly important , since the tensors involved in many - body methods tend to grow large ( in excess of what might be afforded by the available device memory ) and the current interconnects between a host and its attached accelerator(s ) ( e.g. , a pcie bus between a cpu host and a gpu ) generally impede unnecessary ( or redundant ) movement of data . the few remaining loop constructs ( _ vide infra _ ) are then translated by descriptive directives into kernels that can run in parallel on the accelerator . by letting the host processor orchestrate the execution of all parallel kernels , those cores on the host that do not control an accelerator are free to participate in the workload .",
    "this is an important feature ; by allowing for the final code to execute in a hybrid manner on both the host cpu as well as the attached accelerator device(s ) , the present approach is not biased towards neither the former nor the latter of the two platforms ( making for a fair comparison of the relative cpu vs. gpu throughput  @xcite ) . if the implementation was instead formulated in terms of some low - level language such as cuda , such a bias would be present . in addition , the openacc standard further provides features for circumventing the default host - directed behavior , in which a sequence of operations gets scheduled for sequential execution on the device , by instead allowing for these to overlap in different pipelines , i.e. , executing operations ( computations and data transfers ) asynchronously , given that all potential race conditions have been appropriately mapped out .",
    "+ throughout the years , elaborate attempts have been made at pushing the limits for the ri - mp2  @xcite and ccsd(t )  @xcite methods by devising massively parallel implementations of the standard canonical formulations in [ theory_section ] .",
    "in addition , the methods have previously been ported to gpus within the groups of aln aspuru - guzik  @xcite ( ri - mp2 ) , nakajima  @xcite ( ri - mp2 ) , mark gordon  @xcite ( ccsd(t ) ) , and karol kowalski  @xcite ( ccsd(t ) ) , as , e.g. , reviewed in recent contributions to a book devoted to the subject of electronic structure calculations on graphics units  @xcite . in",
    "most , if not all of these implementations , the main driver routine , regardless of the method , consists of an outer nested loops , most often over occupied orbital indices with restricted summations to incorporate permutational symmetry of the two - electron eris  @xcite . for both methods ,",
    "each loop cycle then involves ( numerous ) large mmms , possibly intertwined by tensor ( index ) permutations and the division by orbital energy differences .",
    "since each loop cycle is data - independent of all others , these may be trivially distributed over multiple cpu nodes using mpi .",
    "+ in a likeminded fashion , openmp worksharing directives may be used to parallelize the individual loop cycles over the available ( @xmath36 , or more generally , ` omp\\_num\\_threads ` ) cores on the host cpu node . in this case ,",
    "@xmath36 single - threaded mmms are executed concurrently , which will generally be more efficient than running a single multi - threaded mmm ( over @xmath36 threads ) sequentially @xmath36 times , albeit at the expense of having to store multiple private copies of some of the involved intermediates . as a prerequisite for using openmp this way",
    ", one needs to flatten ( or collapse ) the two ( in the case of the ri - mp2 method , cf . [ mp2_energy ] ) or three ( in the case of the ( t ) correction , cf . [ piecuch_4th_and_5th_order ] ) outer loops into one large , composite outer loop with loop cycles of identical weight .",
    "this way , the evaluation of the ri - mp2 energy or ( t ) correction is programmed to execute in a hybrid heterogeneous manner on both the cpu ( host ) cores and one or even multiple accelerator devices ; combined , the accelerators may be regarded as augmenting the host cpu by additional cores ( one for each attached accelerator ) , albeit some that operate at a significantly higher clock rate .",
    "as far as the use of the gpus is concerned , all of the required data blocks ( tiles of the fitting coefficient tensor in the ri - mp2 code , and tiles of the involved ccsd doubles amplitudes and eris in the ccsd(t ) code ) can be transferred asynchronously , thereby overlapping all data movement between the host and the device(s ) with computations on the latter , and for the actual tensors , we allocate these directly into page - locked ( or pinned ) memory in order to lower the cost of the data transfers .",
    "+ recently , in an invited chapter to a new book devoted to parallel programming with openacc , the present author outlined in some detail how to incrementally accelerate an openmp - parallelized cpu - only version of the ri - mp2 @xmath4-scaling kernel ( [ mp2_energy ] and [ eri_sym_decomp ] combined ) by means of openacc compiler directives  @xcite .",
    "herein , the ( t ) correction in [ ccsdpt_theory_subsection ] is accelerated in a similar fashion , and the source code for both implementations ( ri - mp2 and the ( t ) correction ) accompany the present work under an mit license  @xcite .",
    "furthermore , we will here assess the potential of lowering the numerical precision in certain parts of the ri - mp2 and ( t ) kernels . in standard implementations of the two methods , all of the involved steps ( evaluation of eris , amplitudes , fitting coefficients , etc .",
    ", as well as the final energy assemblies ) are performed using double precision arithmetics exclusively . however , motivated by two recent papers  @xcite , which claim that single precision arithmetics can be applied in the construction of the four - index eris ( [ eri_sym_decomp ] ) used for ri - mp2 as well as in the construction of the triples amplitudes ( [ analytical_triples_ampl ] ) for ccsd(t ) , we will do exactly this by substituting selective calls to ` dgemm ` with corresponding calls to ` sgemm ` in the codes .",
    "not only is the performance bound to improve from the use of single precision arithmetics , but also the storage requirements on the gpu main memory will be lowered .",
    "for the purpose of evaluating the implementations of the two methods in double or mixed floating point precision , we will conduct performance tests on chains ( @xmath2-helix arrangements ) of alanine amino acids ( [ ala]-@xmath37 , where @xmath37 is the number of residues ) in cc - pv@xmath38z ( @xmath39 d , t , q , and 5 ) one - electron basis sets  @xcite and corresponding cc - pv@xmath38z - ri auxiliary basis sets  @xcite ( in the case of ri - mp2 ) . due to the difference in application range , the ri - mp2 and ccsd(t ) implementations will be tested for different problems sizes ( a composite measure of the number of electrons and number of basis functions ) ; specifically , the ri - mp2 implementations are tested for the [ ala]-6 system in the cc - pv@xmath38z ( @xmath39 t , q , and 5 ) basis sets as well as the systems [ ala]-7 to [ ala]-10 in a cc - pvqz basis , whereas the ccsd(t ) implementations are tested for the [ ala]-1 system in the cc - pv@xmath38z ( @xmath39 d , t , and q ) basis sets as well as the systems [ ala]-2 to [ ala]-6 in a cc - pvdz basis . + in terms of computational hardware , the accelerators used are nvidia^^ kepler k40 gpus ( 2880 processor cores @xmath40 745 mhz ( gpu boost @xmath40 875 mhz enabled for all calculations ) and 12 gb main memory , from here on simply abbreviated as ` k40s ' ) and the host nodes are intel^^ ivy bridge e5 - 2690 v2 , dual socket 10-core cpus ( 20 cores @xmath40 3.00 ghz and 128 gb main memory ) , i.e. , ` omp\\_num\\_threads ` @xmath41 if not otherwise noted .",
    "the host math library is intel^^ mkl ( version 11.2 ) and the corresponding device math library is cublas ( cuda 7.5 ) . all calculations are serial ( non - mpi ) , and the openmp-/openacc - compatible fortran compiler used is that of the pgi compiler suite ( version 16.4 ) .",
    "+ in all implementations , the tensors containing orbital energies , fitting coefficients , ccsd singles and doubles amplitudes , as well as eris are initialized with unique , yet arbitrary single and/or double precision numbers ( depending on the context ) , as the calculation of these is not the objective of the present study , cf .",
    "the git hash of the code in ref . used for the actual production runs is ` 42f76337 ` .",
    "in [ figure_1_fig ] and [ figure_2_fig ] , results for the total speed - up over the threaded cpu - only implementations ( using a total of 20 openmp threads ) are reported for fixed system size  varying basis sets ( [ figure_1_fig ] ) and varying system sizes  fixed basis set ( [ figure_2_fig ] ) as a function of the number of k40s involved in the calculations . focusing first on the double precision ri - mp2 results , we note from [ figure_1_fig ] how the [ ala]-6/cc - pvtz problem ( by far the smallest of all the systems in the present ri - mp2 test ) is too small for the use of the k40s to be optimal , while for all other combinations of system sizes and basis sets in [ figure_1_fig ] and [ figure_2_fig ] , the use indeed appears so ( _ vide infra _ ) . from the double precision ccsd(t ) results , on the other hand , a performance improvement is observed in both cases , with no obvious sign of saturation visible neither with respect to the size of the employed basis set in [ figure_1_fig ] nor with respect to system size in [ figure_2_fig ] . moving from double to mixed precision arithmetics , the multi - gpu scaling of the ri - mp2 implementation",
    "is observed to be unaffected for the [ ala]-6/cc - pvtz problem , but significantly improved for all other tests ; in particular , the performance is even improved in the transition from a quadruple- to a pentuple-@xmath0 basis set in [ figure_1_fig ] . for the mixed precision ccsd(t )",
    "implementation , however , the picture is much the same as for the equivalent implementation in double precision , although the improvement in scaling with increasing system size is now not as prominent . in explaining why this is so , we recall how only parts of the overall calculation of the ( t ) correction is performed using single precision arithmetics ( the construction of the triples amplitudes ) , as opposed to the ri - mp2 implementation , in which only the final ( computationally insignificant ) energy assembly is performed in double precision numbers .",
    "thus , the speed - up from calling ` sgemm ` over ` dgemm ` will be smaller .",
    "also , the dimensions of the involved mmms for the tested ccsd(t ) problem sizes are greatly reduced with respect to those of the ri - mp2 problem sizes  a practical necessity , due to the larger memory and cost requirements of the former of the two methods  which further impedes ( or disfavors ) the scaling of the single precision implementation in the case of ccsd(t ) .",
    "the use of single precision does , however , allow for even larger problem sizes to be addressed , as we will see later on , since the overall cpu and gpu memory requirements are reduced .    -6 and ccsd(t)/[ala]-1 ) and varying basis sets ( cc - pv@xmath38z where @xmath39 d , t , q , and 5 ) . for details ,",
    "please see the text . ]",
    "-@xmath37 where @xmath42 ) and a fixed basis set ( ri - mp2/cc - pvqz and ccsd(t)/cc - pvdz ) . for details",
    ", please see the text . ]",
    "having assessed the accumulated speed - up over the ordinary cpu - only implementations , we next report results for the actual scaling with the number of k40s . in [ figure_3_fig ] and [ figure_4_fig ] ,",
    "these scalings are presented , with the relative deviation from ideal behavior written out in the limit of six gpus . as opposed to if the calculations were executed exclusively on the gpus , i.e. , in the non - hybrid limit where @xmath43 , for which the ideal scaling is the trivial proportionality with the number of gpus , @xmath44 ( performance doubling , tripling , etc .",
    ", on two , three , etc . , gpus ) , this is not the case for hybrid cpu / gpu execution , i.e. , whenever @xmath45 , as each cpu core is now treated as an accelerator on its own .",
    "thus , the ideal speed - up for the latter , heterogeneous case is defined as @xmath46 in the definition of @xmath47 in [ r_speed_up ] , the constant factor @xmath48 , where @xmath49 is the time ratio between a cpu - only calculation ( @xmath50 ` omp\\_num\\_threads ` ; @xmath51 ) and a gpu - only calculation using a single gpu ( @xmath52 ) , accounts for the relative difference in processing power between a single cpu core ( assuming ideal openmp parallelization on the host ) and a single gpu . from the results in [ figure_3_fig ] and [ figure_4_fig ] , we note how the scaling becomes ( near-)ideal for all but the smallest problem sizes , a statement valid for both models in both double and mixed precision , regardless of the fact that these dimensions are significantly reduced in the ccsd(t ) tests , cf .",
    "the discussion above .",
    "/mp-@xmath37 where @xmath53 ) using the implementations in either double ( dp ) or mixed precision ( mp ) numbers . ]    in addition , we may monitor how large a percentage of the actual computations is being handled by the gpus in the hybrid ri - mp2 and ccsd(t ) implementations by noting how these involve a total of @xmath54 and @xmath55 tasks of equal weight    @xmath56    where @xmath57 denotes the number of occupied orbitals . through a dynamic openmp schedule , these individual tasks are distributed among the cpu cores and k40s in either of the implementations , so the scaling results in [ figure_3_fig ] and [ figure_4_fig ] may be complemented by corresponding results for the relative gpu workload . from the results , presented in [ figure_5_fig ] as the accumulated workload in percent , we note how for the present problem sizes and cpu / gpu hardware , the actual utilization of the host node is minor ( less than @xmath58 ) when , say , three or more gpus are attached to said node , regardless of the model .",
    "still , the hybrid schemes are always faster than non - hybrid analogues , i.e. , when @xmath43 .",
    "finally , we compare the total time - to - solution for the cpu - only and hybrid cpu / gpu implementations of the ri - mp2 and ccsd(t ) models in [ figure_6_fig ]  . from these results , using six k40s , as in the present study , is seen to reduce the total time - to - solution over the cpu - only implementations  in either double or mixed precision  by at least an order of magnitude for all but the smallest possible problem sizes .",
    "this is indeed a noteworthy acceleration of both models . in particular , we note how for the ri - mp2 model , the accelerated calculation on the [ ala]-10 system in a cc - pvqz basis ( @xmath59 , @xmath60 , and @xmath61 where @xmath62 and @xmath63 are the number of virtual orbitals and auxiliary basis functions , respectively ) took less time than the corresponding cpu - only calculation on the significantly smaller [ ala]-6 system within the same basis ( @xmath64 , @xmath65 , and @xmath66 ) , while for the calculation of the ( t ) correction , an accelerated calculation on the [ ala]-6/cc - pvdz system ( @xmath64 and @xmath67 , cf . the note in ref . )",
    "terminates in less time than a corresponding cpu - only calculation on the [ ala]-4 system within the same basis ( @xmath68 and @xmath69 ) . on the basis of these results",
    ", it may be argued that the use of a combination of openmp and openacc compiler directives  as long as the complete fitting coefficients , integrals , ccsd amplitudes , etc .",
    ", fit into main memory on the host  makes it somewhat unnecessary to explicitly parallelize the code using mpi with the complications that inevitably arise from this .",
    "in the present work , openacc compiler directives have been used to compactly and efficiently accelerate the @xmath4- and @xmath7-scaling rate - determining steps of the evaluation of the ri - mp2 energy and ( t ) triples correction , respectively . in the accelerated implementations ,",
    "the kernels have all been offloaded to gpus , and an asynchronous pipelining model has been introduced for the involved computations and data traffic . due to their minimal memory footprints and efficient dependence on optimized math libraries , the implementations using either double or a combination of both single and double precision arithmetics are practically capable of scaling to as large systems as allowed for by the capacity of the host main memory .",
    ", it was argued that one can not completely rely on a general - purpose compiler in the search for better program transformations since such an imaginary object as an ideal compiler can not exist , with reference to the somewhat demoralizing full employment theorem for compiler writers which states that for any optimizing compiler there will always exist a superior  @xcite . while this is perfectly true , instead of viewing it as a motivation for attempting to _ beat the compiler _ by creating additional compilation layers , such as code generators , testers , etc .",
    ", one might take an alternative stand by obeying to a sort of ad hoc , yet generalizable philosophy of _ embracing the compiler _ ; namely , given that an optimal coprocessor has not yet been mutually decided upon ( for obvious reasons , as different vendors seek to promote their own variant ) , and given that consensus is yet to be reached on whether or not there is indeed a need for accelerators in the field of electronic structure theory , one might instead try to make the most out of what hardware is currently available ( as well as near - future prospective hardware ) by investing only the least possible amount of effort into porting one s code to this .",
    "the present work is intended to argue the case that the compiler directives of the openacc standard serve exactly this purpose by providing the means for targeting various coprocessors ( e.g. , gpus or many - core x86 processors ) in addition to the multi - core host node itself in an _ efficient _ , _ transparent _ , and _ portable _ high - level manner .",
    "+ while the performance degradation of an openacc implementation ( with respect to a hand - tuned low - level implementation ) is bound to be larger for more complex electronic structure algorithms , such as the generation of eris or mean - field hf and dft methods , than in the present case of mbpt methods , the application to the evaluation of the ri - mp2 energy and ( t ) triples correction is intended to illustrate a number of key advantages of the use of compiler directives over a reformulation of an optimized implementation for cpus in terms of , e.g. , cuda or opencl for execution on gpus .",
    "first and foremost , it is the opinion of the present author that accelerated code needs to be relatively easy and fast to implement , as new bottlenecks are bound to appear as soon as one part of a complex algorithm has been ported to accelerators ( cf .",
    "law  @xcite ) .",
    "second , the use of compiler directives guarantees  on par with the use of openmp worksharing directives for simd instructions on standard cpu architectures  that the final code remains functionally portable , i.e. , the addition of accelerated code does not interfere with the standard compilation of the code on commodity hardware using standard non - accelerating compilers .",
    "third , since the ri - mp2 and ccsd(t ) methods alongside other , more advanced non - iterative cc many - body methods alike  @xcite are intrinsically reliant on large matrix - vector and matrix - matrix operations , the main need for accelerators in this context is for offloading exactly these .",
    "thus , besides a number of small generic kernels , e.g. , tensor index permutations or energy summations , compiler directives are primarily used for optimizing the data transfers between the host and the device(s ) , for instance by overlapping these with device computations .",
    "hopefully , the generality of the discussion in the present work will encourage and possibly aid others to accelerate similar codes of their own . as a means to facilitate exactly this , the present implementations come distributed alongside this work for others to reuse in part or even in full  @xcite .",
    "+ finally , one particular potential area of application for the present implementations deserves a dedicated mentioning .",
    "while the discussion of the methods herein has been exclusively concerned with their standard canonical formulations for full molecular systems , we note how both methods have also been formulated within a number of so - called local correlation schemes , of which one branch relies on either a physical  @xcite or orbital - based  @xcite fragmentation of the molecular system . in these schemes ,",
    "standard ( pseudo-)canonical calculations are performed for each of the fragments before the energy for the full system is assembled at the end of the total calculation .",
    "thus , by accelerating each of the individual fragment ( and possible pair fragment ) calculations , the total calculation will be accelerated as well without the need for investing additional efforts , and the resulting reduction in time - to - solution hence has the potential to help increase the range of application even further for these various schemes .",
    "the author gratefully acknowledges support from the nvidia^^ corporation , in particular for granting access to an internal test cluster , the resources of which were used in the preparation of the present work .",
    "the author furthermore wishes to thank mark berger , jeff larkin , roberto gomperts , michael wolfe , and brent leback of the nvidia^^ corporation for general support and discussions , as well as prof .",
    "t. daniel crawford of virginia tech , dr . filippo lipparini of johannes gutenberg - universitt mainz , and dr .",
    "radovan bast of uit the arctic university of norway for providing fruitful comments to the manuscript .",
    "see , for instance , the talk by jeff larkin on performance portability from the sc15 conference : http://images.nvidia.com/events/sc15/pdfs/sc15-performance-portability-openacc-larkin.pdf ( accessed ) wilson ,  g. ; aruliah ,  d.  a. ; brown ,  c.  t. ; hong ,  n. p.  c. ; davis ,  m. ; guy ,  r.  t. ; haddock ,  s. h.  d. ; huff ,  k.  d. ; mitchell ,  i.  m. ; plumbley ,  m.  d. ; waugh ,  b. ; white ,  e.  p. ; wilson ,  p. _ plos biol . _ * 2014 * , _ 12 _ , e1001745 ( accessed ) see , for instance , the talk by james beyer and jeff larkin on openmp vs. openacc from the gtc2016 conference : + http://on-demand.gputechconf.com/gtc/2016/presentation/s6410-jeff-larkin-beyer-comparing-open-acc-openmp.pdf ( accessed ) ( accessed ) mller ,  c. ; plesset ,  m.  s. _ phys .",
    "rev . _ * 1934 * , _ 46 _ , 618ek ,  j. _ j. chem . phys . _ * 1966 * , _ 45 _ , 4256ek ,  j. _ adv .",
    "* 1969 * , _ 14 _ , 35 paldus ,  j. ; ek ,  j. ; shavitt ,  i. _ phys .",
    "a _ * 1972 * , _ 5 _ , 50 purvis ,  g.  d. ; bartlett ,  r.  j. _ j. chem . phys . _ * 1982 * , _ 76 _ , 1910 raghavachari ,  k. ; trucks ,  g.  w. ; pople ,  j.  a. ; head - gordon ,  m. _ chem . phys .",
    "lett . _ * 1989 * , _ 157 _ , 479 stanton ,  j.  f. _ chem .",
    "lett . _ * 1997 * , _ 281 _ , 130 ufimtsev ,  i.  s. ; martnez ,  t.  j. _ j. chem . theory comp .",
    "_ * 2008 * , _ 4 _ , 222 luehr ,  n. ; ufimtsev ,  i.  s. ; martnez ,  t.  j. _ j. chem .",
    "theory comp . _",
    "* 2011 * , _ 7 _ , 949 asadchev ,  a. ; allada ,  v. ; felder ,  j. ; bode ,  b.  m. ; gordon ,  m.  s. ; windus ,  t.  l. _ j. chem .",
    "theory comp . _ * 2010 * , _ 6 _ , 696 wilkinson ,  k.  a. ; sherwood ,  p. ; guest ,  m.  f. ; naidoo ,  k.  j. _ j. comp .",
    "chem . _ * 2011 * , _ 32 _ , 2313 miao ,  y. ; merz  jr .",
    ",  k.  m. _ j. chem .",
    "theory comp . _ * 2013 * , _ 9 _ , 965 miao ,  y. ; merz  jr .",
    ",  k.  m. _ j. chem .",
    "theory comp . _ * 2015 * , _ 11 _ , 1449 toivanen ,  e. ; losilla ,  s.  a. ; sundholm ,  d. _ phys .",
    "_ * 2015 * , _ 17 _ , 31480 yasuda ,  k. _ j. chem . theory comp . _ * 2008 * ,",
    "_ 4 _ , 1230 ufimtsev ,  i.  s. ; martnez ,  t.  j. _ j. chem . theory comp . _ * 2009 * , _ 5 _ , 1004 ufimtsev ,  i.  s. ; martnez ,  t.  j. _ j. chem . theory comp . _ * 2009 * , _ 5 _ , 2619 asadchev ,  a. ; gordon ,  m.  s. _ j. chem .",
    "theory comp . _ * 2012 * , _ 8 _ , 4166 hohenstein ,  e.  g. ; luehr ,  n. ; ufimtsev ,  i.  s. ; martnez ,  t.  j. _ j. chem . phys . _ * 2015 * , _ 142 _ , 224103 hohenstein ,  e.  g. ; bouduban ,  m. e.  f. ; song ,  c. ; luehr ,  n. ; ufimtsev ,  i.  s. ; martnez ,  t.  j. _ j. chem .",
    "* 2015 * , _ 143 _ , 014111 snyder  jr . ,  j.  w. ; curchod ,  b. f.  e. ; martnez ,  t.  j. _ j. phys . chem .",
    "lett . _ * 2016 * , _ 7 _ , 2444 yoshikawa ,  t. ; nakai ,  h. _ j. comp . chem . _ * 2015 * , _ 36 _ , 164 andrade ,  x. ; aspuru - guzik ,  a. _ j. chem .",
    "theory comp . _ * 2013 * , _ 9 _ , 4360 cawkwell ,  m.  j. ; wood ,  m.  a. ; niklasson ,  a. m.  n. ; mniszewski ,  s.  m. _ j. chem .",
    "theory comp . _ * 2014 * , _ 10 _ , 5391 losilla ,  s.  a. ; watson ,  m.  a. ; aspuru - guzik ,  a. ; sundholm ,  d. _ j. chem .",
    "theory comp . _",
    "* 2015 * , _ 11 _ , 2053 kussmann ,  j. ; ochsenfeld ,  c. _ j. chem .",
    "theory comp . _ * 2015 * , _ 11 _ , 918 liu ,  f. ; luehr ,  n. ; kulik ,  h.  j. ; martnez ,  t.  j. _ j. chem .",
    "theory comp . _",
    "* 2015 * , _ 11 _ , 3131 nitsche ,  m.  a. ; ferreria ,  m. ; mocskos ,  e.  e. ; lebrero ,  m. c.  g. _ j. chem . theory comp . _ * 2014 * , _ 10 _ , 959 gtz ,  a.  w. ; williamson ,  m.  j. ; xu ,  d. ; poole ,  d. ; le  grand ,  s. ; walker ,  r.  c. _ j. chem .",
    "theory comp . _ * 2012 * , _ 8 _ , 1542 salomon - ferrer ,  r. ; gtz ,  a.  w. ; poole ,  d. ; le  grand ,  s. ; walker ,  r.  c. _ j. chem .",
    "theory comp .",
    "_ * 2013 * , _ 9 _ , 3878 wu ,  x. ; koslowski ,  a. ; thiel ,  w. _ j. chem .",
    "theory comp .",
    "_ * 2012 * ,",
    "_ 8 _ , 2272 vogt ,  l. ; olivares - amaya ,  r. ; kermes ,  s. ; shao ,  y. ; amador - bedolla ,  c. ; aspuru - guzik ,  a. _ j. phys . chem .",
    "* 2008 * , _ 112 _ , 2049 olivares - amaya ,  r. ; watson ,  m.  a. ; edgar ,  r.  g. ; vogt ,  l. ; shao ,  y. ; aspuru - guzik ,  a. _ j. chem .",
    "theory comp . _ * 2010 * , _ 6 _ , 135 watson ,  m.  a. ; olivares - amaya ,  r. ; edgar ,  r.  g. ; aspuru - guzik ,  a. _ comput .",
    "eng . _ * 2010 * , _ 12 _ , 40 katouda ,  m. ; naruse ,  a. ; hirano ,  y. ; nakajima ,  t. _ j. comp .",
    "chem . _ * 2016 * , doran ,  a.  e. ; hirata ,  s. _ j. chem .",
    "theory comp . _ * 2016 * , deprince  iii ,  a.  e. ; hammond ,  j.  r. _ j. chem .",
    "theory comp . _",
    "* 2011 * , _ 7 _ , 1287 deprince  iii ,  a.  e. ; kennedy ,  m.  r. ; sumpter ,  b.  g. ; sherrill ,  c.  d. _ mol .",
    "phys . _ * 2014 * , _ 112 _ , 844 asadchev ,  a. ; gordon ,  m.  s. _ j. chem . theory comp .",
    "_ * 2013 * , _ 9 _ , 3385 leang ,  s.  s. ; rendell ,  a.  p. ; gordon ,  m.  s. _ j. chem .",
    "theory comp .",
    "_ * 2014 * , _ 10 _ , 908 ma ,  w. ; krishnamoorthy ,  s. ; villa ,  o. ; kowalski ,  k. ; agrawal ,  g. _ cluster comput . _ * 2013 * , _ 16 _ , 131 ma ,  w. ; krishnamoorthy ,  s. ; villa ,  o. ; kowalski ,  k. _ j. chem .",
    "theory comp . _ * 2011 * , _ 7 _ , 1316 bhaskaran - nair ,  k. ; ma ,  w. ; krishnamoorthy ,  s. ; villa ,  o. ; van dam ,  h. j.  j. ; apr ,  e. ; kowalski ,  k. _ j. chem .",
    "theory comp . _ * 2013 * , _ 9 _ , 1949 fales ,  b.  s. ; levine ,  b.  g. _ j. chem .",
    "theory comp . _ * 2015 * , _ 11 _ , 4708 ufimtsev ,  i.  s. ; martnez ,  t.  j. _ comput .",
    "eng . _ * 2008 * , _ 10 _ , 26 valiev ,  m. ; bylaska ,  e.  j. ; govind ,  n. ; kowalski ,  k. ; straatsma ,  t.  p. ; van dam ,  h. j.  j. ; wang ,  d. ; nieplocha ,  j. ; apr ,  e. ; windus ,  t.  l. ; de  jong ,  w.  a. _ comput . phys .",
    "commun . _ * 2010 * , _ 181 _ , 1477 pratx ,  g. ; xing ,  l. _ med .",
    "phys . _ * 2011 * , _ 38 _ , 2685 jacob ,  c.  r. _ j. phys . chem .",
    "lett . _ * 2016 * , _ 7 _ , 351 song ,  c. ; wang ,  l .- p .",
    "; martnez ,  t.  j. _ j. chem .",
    "theory comp . _ * 2016 * , _ 12 _ , 92 titov ,  a. ; ufimtsev ,  i.  s. ; luehr ,  n. ; martnez ,  t.  j. _ j. chem .",
    "theory comp . _ * 2013 * , _ 9 _ , 213 shavitt ,  i. ; bartlett ,  r.  j. _ many - body methods in chemistry and physics : many - body perturbation theory and coupled - cluster theory _ ; cambridge university press : cambridge , uk , 2009 helgaker ,  t. ; jrgensen ,  p. ; olsen ,  j. _ molecular electronic - structure theory _",
    ", 1st ed . ;",
    "wiley & sons , ltd . : west sussex , uk , 2000 whitten ,  j.  l. _ j. chem .",
    "phys . _ * 1973 * , _ 58 _ , 4496 feyereisen ,  m.  w. ; fitzgerald ,  g. ; komornicki ,  a. _ chem . phys .",
    "lett . _ * 1993 * , _ 208 _ , 359 vahtras ,  o. ; almlf ,  j. ; feyereisen ,  m.  w. _ chem .",
    "lett . _ * 1993 * , _ 213 _ , 514 weigend ,  f. ; hser ,  m. _ theor .",
    "acc . _ * 1997 * , _ 97 _ , 331 almlf ,  j. _ chem .",
    "lett . _ * 1991 * , _ 181 _ , 319 ayala ,  p.  y. ; scuseria ,  g.  e. _ j. chem .",
    "* 1999 * , _ 110 _ , 3660 werner ,  h .- j . ;",
    "manby ,  f.  r. ; knowles ,  p.  j. _ j. chem . phys . _ * 2003 * , _ 118 _ , 8149 doser ,  b. ; lambrecht ,  d.  s. ; kussmann ,  j. ; ochsenfeld ,  c. _ j. chem .",
    "phys . _ * 2009 * , _ 130 _ , 064107 doser ,  b. ; zienau ,  j. ; clin ,  l. ; lambrecht ,  d.  s. ; ochsenfeld ,  c. _ z. phys . chem . _ * 2010 * , _ 224 _ , 397 baudin ,  p. ; ettenhuber ,  p. ; reine ,  s. ; kristensen ,  k. ; kjrgaard ,  t. _ j. chem .",
    "* 2016 * , _ 144 _ , 054102 schtz ,  m. ; werner ,  h .- j . _",
    "_ * 2000 * , _ 318 _ , 370 schtz ,  m. _ j. chem .",
    "_ * 2000 * , _ 113 _ , 9986 werner ,  h .- j . ;",
    "schtz ,  m. _ j. chem . phys . _ * 2011 * , _ 135 _ , 144116 friedrich ,  j. ; dolg ,  m. _ j. chem . theory comput . _ * 2009 * , _ 5 _ , 287 kobayashi ,  m. ; nakai ,  h. _ j. chem . phys . _ * 2009 * , _ 131 _ , 114108 li ,  w. ; piecuch ,  p. ; gour ,  j.  r. ; li ,  s. _ j. chem . phys . _ * 2009 * , _ 131 _ , 114109 rolik ,  z. ; kllay ,  m. _ j. chem .",
    "phys . _ * 2011 * , _ 135 _ , 104111 rolik ,  z. ; szegedy ,  l. ; ladjnszki ,  i. ; ladczki ,  b. ; kllay ,  m. _ j. chem . phys . _ * 2013 * , _ 139 _ , 094105 schtz ,  m. ; yang ,  j. ; chan ,  g. k .- l . ; manby ,  f.  r. ; werner ,  h .- j .",
    "_ j. chem .",
    "phys . _ * 2013 * , _ 138 _ , 054109 riplinger ,  c. ; sandhoefer ,  b. ; hansen ,  a. ; neese ,  f. _ j. chem .",
    "phys . _ * 2013 * , _ 139 _ , 134101 eriksen ,  j.  j. ; baudin ,  p. ; ettenhuber ,  p. ; kristensen ,  k. ; kjrgaard ,  t. ; jrgensen ,  p. _ j. chem .",
    "theory comput .",
    "_ * 2015 * , _ 11 _ , 2984 piecuch ,  p. ; kucharski ,  s.  a. ; kowalski ,  k. ; musia ,  m. _ comp",
    "* 2002 * , _ 149 _ , 71 lawson ,  c.  l. ; hanson ,  r.  j. ; kincaid ,  d. ; krogh ,  f.  t. _ acm trans . math .",
    "* 1979 * , _ 5 _ , 308 dongarra ,  j.  j. ; ducroz ,  j. ; hammarling ,  s. ; hanson ,  r. _ acm trans . math",
    "_ * 1988 * , _ 14 _ , 1 dongarra ,  j.  j. ; duff ,  i. ; ducroz ,  j. ; hammarling ,  s. _ acm trans . math . soft . _ * 1990 * , _ 16 _ , 1 whaley ,  r.  c. ; petitet ,  a. ; dongarra ,  j.  j. _ parallel comput . _ * 2001 * , _ 27 _ , 3 goto ,  k. ; van de  geijn ,  r. _ acm trans .",
    "* 2008 * , _ 35 _ , 1 tomov ,  s. ; dongarra ,  j.  j. ; baboulin ,  m. _ parallel comput . _ * 2010 * , _ 36 _ , 232 nath ,  r. ; tomov ,  s. ; dongarra ,  j.  j. _ int . j. high perform .",
    "appl . _ * 2010 * , _ 24 _ , 511 ( accessed ) lee ,  v.  w. ; kim ,  c. ; chhugani ,  j. ; deisher ,  m. ; kim ,  d. ; nguyen ,  a.  d. ; satish ,  n. ; smelyanskiy ,  m. ; chennupaty ,  s. ; hammarlund ,  p. ; singhal ,  r. ; dubey ,  p. debunking the 100x gpu vs. cpu myth : an evaluation of throughput computing on cpu and gpu .",
    "2010 bernholdt ,  d.  e. ; harrison ,  r.  j. _ chem . phys .",
    "lett . _ * 1996 * , _ 250 _ , 477 katouda ,  m. ; nagase ,  s. _ int . j. quantum chem . _ * 2009 * , _ 109 _ , 2121 katouda ,  m. ; nakajima ,  t. _ j. chem .",
    "theory comput . _ * 2013 * , _ 9 _ , 5373 maschio ,  l. _ j. chem .",
    "theory comput . _ * 2011 * , _ 7 _ , 2818 werner ,  h .- j . ;",
    "knowles ,  p.  j. ; knizia ,  g. ; manby ,  f. ; schutz ,  m. _ wires comput . mol .",
    "sci . _ * 2012 * , _ 2 _ , 242 lotrich ,  v. ; flocke ,  n. ; ponton ,  m. ; yau ,  a.  d. ; perera ,  a. ; deumens ,  e. ; bartlett ,  r.  j. _ j. chem .",
    "phys . _ * 2008 * , _ 128 _ , 15 deumens ,  e. ; lotrich ,  v. ; perera ,  a. ; ponton ,  m.  j. ; sanders ,  b.  a. ; bartlett ,  r.  j. _ wires comput . mol .",
    "sci . _ * 2012 * , _ 1 _ , 895 prochnow ,  e. ; harding ,  m.  e. ; gauss ,  j. _ j. chem .",
    "theory comput . _ * 2010 * , _ 6 _ , 2339 janowski ,  t. ; pulay ,  p. _ j. chem .",
    "theory comput .",
    "_ * 2008 * , _ 4 _ , 1585 baker ,  j. ; janowski ,  t. ; wolinski ,  k. ; pulay ,  p. _ wires comput .",
    "sci . _ * 2012 * , _ 2 _ , 63 bentz ,  j.  l. ; olson ,  r.  m. ; gordon ,  m.  s. ; schmidt ,  m.  w. ; kendall ,  r.  a. _ comp .",
    "comm . _ * 2007 * , _ 176 _ , 589 olson ,  r.  m. ; bentz ,  j.  l. ; kendall ,  r.  a. ; schmidt ,  m.  w. ; gordon ,  m.  s. _ j. chem .",
    "theory comput . _",
    "* 2007 * , _ 3 _ , 1312 kobayashi ,  r. ; rendell ,  a.  p. _ chem .",
    "lett . _ * 1997 * , _ 265 _ , 1 valiev ,  m. ; bylaska ,  e.  j. ; govind ,  n. ; kowalski ,  k. ; straatsma ,  t.  p. ; van dam ,  h. j.  j. ; wang ,  d. ; nieplocha ,  j. ; apr ,  e. ; windus ,  t.  l. ; de  jong ,  w.  a. _ comput .",
    "commun . _ * 2010 * , _ 181 _ , 1477 apr ,  e. ; harrison ,  r.  j. ; shelton ,  w.  a. ; tipparaju ,  v. ; vzquez - mayagoitia ,  a. _ journal of physics : conference series _ * 2009 * , _ 180 _ , 012027 olivares - amaya ,  r. ; jinich ,  a. ; watson ,  m.  a. ; aspuru - guzik ,  a. in _ electronic structure calculations on graphics processing units : from quantum chemistry to condensed matter physics _ ; walker ,  r.  c. , gtz ,  a.  w. , eds . ; john wiley & sons , ltd .",
    ": chichester , uk , 2016 ma ,  w. ; bhaskaran - nair ,  k. ; villa ,  o. ; apr ,  e. ; tumeo ,  a. ; krishnamoorthy ,  s. ; kowalski ,  k. in _ electronic structure calculations on graphics processing units : from quantum chemistry to condensed matter physics _ ; walker ,  r.  c. , gtz ,  a.  w. , eds . ; john wiley & sons , ltd .",
    ": chichester , uk , 2016 rendell ,  a.  p. ; lee ,  t.  j. ; komornicki ,  a. _ chem .",
    "lett . _ * 1991 * , _ 178 _ , 462 rendell ,  a.  p. ; lee ,  t.  j. ; komornicki ,  a. ; wilson ,  s. _ theor . chim .",
    "acc . _ * 1992 * , _ 84 _ , 271 eriksen ,  j.  j. in _ parallel programming with openacc _ ; farber ,  r. , ed . ; morgan kaufmann : burlington , usa , 2016 openacc - accelerated coupled cluster codes : + https://gitlab.com/januseriksen/openacc_cc_codes ( accessed ) vysotskiy ,  v.  p. ; cederbaum ,  l.  s. _ j. chem .",
    "theory comp . _ * 2011 * , _ 7 _ , 320 knizia ,  g. ; li ,  w. ; simon ,  s. ; werner ,  h .- j .",
    "_ j. chem .",
    "theory comp . _ * 2011 * , _ 7 _ , 2387 dunning  jr .",
    ",  t.  h. _ j. chem .",
    "phys . _ * 1989 * , _ 90 _ , 1007 weigend ,  f. ; khn ,  a. ; httig ,  c. _ j. chem .",
    "phys . _ * 2002 * , _ 116 _ , 3175 httig ,  c. _ phys . chem .",
    "phys . _ * 2005 * , _ 7 _ , 59 due to a limited amount of memory on the host node ( 128 gb ) , the accelerated ccsd(t ) calculation on the [ ala]-6/cc - pvdz problem only spawned a total of 14 openmp threads instead of the default 20 threads .",
    "out of these fourteen threads , six were devoted to the six k40s , while the remaining eight threads did computations on the host node .",
    "finally , due to timelimit restrictions , the mixed precision cpu - only ccsd(t)/[ala]-6 calculation ( using 14 openmp threads ) was not performed , as were none of the corresponding [ ala]-6 calculations in double precision since the memory requirements for these exceeded the size of the host node memory ( i.e. , the calculations required more than 128 gb ) .",
    "appel ,  a.  w. ; ginsburg ,  m. _ modern compiler implementation in c _ ; cambridge university press : cambridge , uk , 2004 amdahl ,  g. _ afips conf .",
    "proc . _ * 1967 * , _ 30 _ , 483 kucharski ,  s.  a. ; bartlett ,  r.  j. _ j. chem .",
    "phys . _ * 1998 * , _ 108 _ , 5243 kucharski ,  s.  a. ; bartlett ,  r.  j. _ j. chem . phys . _ * 1998 * , _ 108 _ , 9221 crawford ,  t.  d. ; stanton ,  j.  f. _ int . j. quant .",
    "chem . _ * 1998 * , _ 70 _ , 601 kowalski ,  k. ; piecuch ,  p. _ j. chem .",
    "_ * 2000 * , _ 113 _ , 18 piecuch ,  p. ; wloch ,  m. _ j. chem . phys . _",
    "* 2005 * , _ 123 _ , 224105 piecuch ,  p. ; wloch ,  m. ; gour ,  j.  r. ; kinal ,  a. _ chem . phys .",
    "_ * 2006 * , _ 418 _ , 467 kllay ,  m. ; gauss ,  j. _ j. chem .",
    "phys . _ * 2005 * , _ 123 _ , 214105 kllay ,  m. ; gauss ,  j. _ j. chem . phys . _ * 2008 * , _ 129 _ , 144101 eriksen ,  j.  j. ; kristensen ,  k. ; kjrgaard ,  t. ; jrgensen ,  p. ; gauss ,  j. _ j. chem .",
    "phys . _ * 2014 * ,",
    "_ 140 _ , 064108 eriksen ,  j.  j. ; jrgensen ,  p. ; olsen ,  j. ; gauss ,  j. _ j. chem . phys . _ * 2014 * , _ 140 _ , 174114 eriksen ,  j.  j. ; jrgensen ,  p. ; gauss ,  j. _ j. chem . phys . _ * 2015 * , _ 142 _ , 014102 eriksen ,  j.  j. ; matthews ,  d.  a. ; jrgensen ,  p. ; gauss ,  j. _ j. chem . phys . _ * 2015 * , _ 143 _ , 041101 kristensen ,  k. ; eriksen ,  j.  j. ; matthews ,  d.  a. ; olsen ,  j. ; jrgensen ,  p. _ j. chem . phys . _ * 2016 * , _ 144 _ , 064103 eriksen ,  j.  j. ; matthews ,  d.  a. ; jrgensen ,  p. ; gauss ,  j. _ j. chem . phys . _ * 2016 * , _ 144 _ , 194102 eriksen ,  j.  j. ; matthews ,  d.  a. ; jrgensen ,  p. ; gauss ,  j. _ j. chem . phys . _ * 2016 * , _ 144 _ , 194103 kobayashi ,  m. ; imamura ,  y. ; nakai ,  h. _ j. chem . phys . _ * 2007 * , _ 127 _ , 074103 mochizuki ,  y. ; yamashita ,  k. ; murase ,  t. ; nakano ,  t. ; fukuzawa ,  k. ; takematsu ,  k. ; watanabe ,  h. ; tanaka ,  s. _ chem . phys . lett . _ * 2008 * , _ 457 _ , 396 ishikawa ,  t. ; kuwata ,  k. _ chem . phys . lett . _ * 2009 * , _ 474 _ , 195 guo ,  y. ; li ,  w. ; li ,  s. _ j. phys . chem .",
    "_ 118 _ , 8996 guo ,  y. ; li ,  w. ; yuan ,  d. ; li ,  s. _ sci . china chem . _ * 2014 * , _ 57 _ , 1393 friedrich ,  j. _ j. chem .",
    "phys . _ * 2007 * , _ 126 _ , 154110 friedrich ,  j. ; dolg ,  m. _ j. chem .",
    "theory comput . _ * 2009 * , _ 5 _ , 287 zikowski ,  m. ; jansk ,  b. ; kjrgaard ,  t. ; jrgensen ,  p. _ j. chem .",
    "phys . _ * 2010 * , _ 133 _ , 014107 kristensen ,  k. ; zikowski ,  m. ; jansk ,  b. ; kjrgaard ,  t. ; jrgensen ,  p. _ j. chem",
    ". theory comput . _ * 2011 * , _ 7 _ , 1677 ettenhuber ,  p. ; baudin ,  p. ; kjrgaard ,  t. ; jrgensen ,  p. ; kristensen ,  k. _ j. chem .",
    "phys . _ * 2016 * , _ 144 _ , 164116"
  ],
  "abstract_text": [
    "<S> it is demonstrated how the non - proprietary openacc standard of compiler directives may be used to compactly and efficiently accelerate the rate - determining steps of two of the most routinely applied many - body methods of electronic structure theory , namely the second - order mller - plesset ( mp2 ) model in its resolution - of - the - identity ( ri ) approximated form and the ( t ) triples correction to the coupled cluster singles and doubles model ( ccsd(t ) ) . by means of compute directives as well as the use of optimized device math libraries , the operations involved in the energy kernels have been ported to graphics processing unit ( gpu ) accelerators , and the associated data transfers correspondingly optimized to such a degree that the final implementations ( using either double and/or single precision arithmetics ) are capable of scaling to as large systems as allowed for by the capacity of the host central processing unit ( cpu ) main memory . </S>",
    "<S> the performance of the hybrid cpu / gpu implementations is assessed through calculations on test systems of alanine amino acid chains using one - electron basis sets of increasing size ( ranging from double- to pentuple-@xmath0 quality ) . for all but the smallest problem sizes of the present study , </S>",
    "<S> the optimized accelerated codes ( using a single multi - core cpu host node in conjunction with six gpus ) are found to be capable of reducing the total time - to - solution by at least an order of magnitude over optimized , openmp - threaded cpu - only reference implementations . </S>"
  ]
}