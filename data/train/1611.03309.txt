{
  "article_text": [
    "let @xmath0 be the random variable of interest , and let @xmath1 be a vector of @xmath2 explanatory variables . whenever a single set of regression coefficients is not adequate for all realizations of the pair @xmath3 finite mixture of linear regression models can be used to estimate clusterwise linear regression parameters .",
    "let @xmath0 be distributed as a clusterwise linear regression model , that is    @xmath4    where @xmath5 is the total number of clusters and @xmath6 @xmath7 and @xmath8 are respectively the mixing proportion , the vector of @xmath2 regression coefficients , and the variance term for the @xmath9-th cluster .",
    "the model in equation is also known under the name of finite mixture of linear regression models , or switching regression model ( quandt , 1972 ; quandt and ramsey , 1978 ; kiefer , 1978 ) .    in addition",
    "let us denote the set of parameters to be estimated @xmath10 where + @xmath11 @xmath12 unlike finite mixtures of other densities , the parameters of finite mixtures of linear regression models are identified if some mild regularity conditions are met ( hennig , 2000 ) .",
    "the clusterwise linear regression model of equation can naturally serve as a classification model .",
    "based on the model , one computes the posterior membership probabilities as follows @xmath13 and then classify each observation according , for instance , to fuzzy or crisp classification rules .",
    "let @xmath14 be a sample of independent observations .",
    "in order to estimate @xmath15 one has to maximize the following sample likelihood function    @xmath16,\\ ] ]    which can be done using iterative procedures like the em algorithm ( dempster , laird , and rubin , 1977 ) .",
    "unfortunately , maximum likelihood ( ml ) estimation of univariate mixtures of normals - or conditional normals - suffers from the well - known issue of unboundedness of the likelihood function : whenever a sample point coincides with the group s centroid and the relative variance approaches zero , the likelihood function increases without bound ( kiefer and wolfowitz , 1956 ; day , 1969 ) . hence a global maximum can not be found .",
    "kiefer ( 1978 ) showed that there is a sequence of consistent , asymptotically efficient and normally distributed estimators for switching regressions with different group - specific variances ( heteroscedastic switching regressions ) .",
    "these roots correspond , with probability approaching one , to local maxima in the interior of the parameter space . yet , although there is a local maximum which is also a consistent root , there is no tool for choosing it among the local maxima . day ( 1969 )",
    "showed that potentially each sample point - or any pair of sample points being sufficiently close together - can generate a singularity in the likelihood function of a mixture with heteroscedastic components .",
    "this gives rise to a number of spurious maximizers ( mclachlan and peel , 2000 ) , which , in a multivariate context , arise from data points being almost coplanar ( ritter , 2014 ) .",
    "the issue of degeneracy can be dealt with by imposing constraints on the component variances .",
    "this approach is based on the seminal work of hathaway ( 1985 ) , who showed that imposing a lower bound , say @xmath17 , to the ratios of the scale parameters prevents the likelihood function from degeneracy .",
    "although the resulting ml estimator is consistent and the method is equivariant under linear affine transformations of the data - that is , if the data are linearly transformed , the estimated posterior probabilities do not change and the clustering remains unaltered - the proposed constraints are very difficult to apply within iterative procedures like the em algorithm .",
    "furthermore , the issue of how to choose @xmath18 which controls the strength of the constraints , remains open .",
    "ingrassia ( 2004 ) showed that it is sufficient , for hathaway s constraints to hold , to impose bounds on the component variances .",
    "this provides a constrained solution that 1 ) can be implemented at each iteration of the em algorithm , and 2 ) still preserves the monotonicity of the resulting em ( ingrassia and rocci , 2007 ) .",
    "we propose a constrained solution to the problem of degeneracy , which extends the one proposed in a recent paper by rocci , gattone , and di mari ( rgd , 2016 ) for multivariate mixtures of gaussians , in finite mixtures of linear regression models .",
    "this constrained estimation method is characterized by 1 ) fully data - dependent constraints , 2 ) affine equivariance of the clustering algorithm under change of scale in the data , and 3 ) ease of implementation within standard routines ( ingrassia , 2004 ; ingrassia and rocci , 2007 ) .",
    "while inheriting such properties from rgd ( 2016 ) , where the focus was mainly on classification , the extension we consider in this work intrinsically pays attention to how well regression parameters are estimated with the new constrained method , together with looking at how accurately the method classifies observations within clusters .",
    "the affine equivariance property , in our context , translates to having a clustering model which yields the same clustering of the data if the variable of interest @xmath19 is re - scaled .",
    "this covers , for instance , cases in which a different scale of the variable of interest is more appropriate for interpreting the regression coefficient than the one the variable was originally measured at . whereas the simulation study will aim at showing the soundness of the method",
    ", the three empirical applications will enlighten the _",
    "shrinkage_-like nature of the approach in terms of estimated model parameters and clustering .",
    "the remainder of the paper is organized as follows . in section",
    "[ constrained ] we briefly review hathaway s constraints and the sufficient condition in ingrassia ( 2004 ) . section [ constreq ] is devoted to a description of the proposed methodology and of the estimation algorithm , which is evaluated through a simulation study , presented in section [ simulation ] , and three real - data examples , in section [ data ] .",
    "section [ conclusion ] concludes with some final discussion and some ideas for future research .",
    "hathaway ( 1985 ) proposed relative constraints on the variances of the kind @xmath20.\\ ] ] hathaway s formulation of the maximum likelihood problem presents a strongly consistent global solution , no singularities , and a smaller number of spurious maxima .",
    "consistency and robustness of estimators of this sort was already pointed out by huber ( 1967 , 1981 ) .",
    "ingrassia ( 2004 ) formulated a sufficient condition such that hathaway s constraints hold , which is implementable directly within the em algorithm , where the covariance matrices are iteratively updated . in a univariate setup",
    ", he shows that constraints in   are satisfied if @xmath21 where @xmath22 and @xmath23 are positive numbers such that @xmath24 complementing the work of ingrassia ( 2004 ) , ingrassia and rocci ( 2007 ) reformulated ingrassia ( 2004 ) s constraints in such a way that they can be implemented directly at each iteration of the em algorithm .",
    "the conditions under which the proposed constrained algorithm preserves the monotonicity of the unconstrained em were also stated : the proposed algorithm yields a non - decreasing sequence of the likelihood values , provided that the initial guess @xmath25 satisfies the constraints .",
    "starting form the set of constraints of equation  , let @xmath26 be a _",
    "target _ variance .",
    "the set of constraints for a clusterwise linear regression context are formulated as follows    @xmath27 or equivalently @xmath28 it is easy to show that   implies   - since   is more stringent than   - while the converse is not necessarily true , that is @xmath29    the constraints are affine equivariant",
    "( rgd , 2016 ) , and have the effect of shrinking the component variances to @xmath30 the _ target _ variance term , and the level of shrinkage is given by the value of @xmath17 : such a formulation makes it possible to reduce the number of tuning constants from two - @xmath31 as in ingrassia ( 2004 ) s proposal - to one - @xmath32 note that for @xmath33 , @xmath34 whereas for @xmath35 @xmath36 equals the unconstrained ml estimate .",
    "intuitively , the constraints provide with a way to obtain a model in between a too restrictive model , the homoscedastic , and an ill - conditioned model , the heteroscedastic . in other terms ,",
    "high scale balance is generally an asset - as it means that there is some unknown transformation of the sample space that transfers the component not too far from the common variance setting - but it has to be traded with fit ( ritter , 2014 ) .      selecting @xmath17 jointly with the mixture parameters by maximizing the likelihood on the entire sample would trivially yield an overfitted scale balance approaching zero ( rgd , 2016 ) . following rgd ( 2016 ) , we propose a cross - validation strategy in order to let the data decide the optimal scale balance , which can be described according to the following steps .    1 .",
    "select a plausible value for @xmath37.$ ] 2 .   obtain a temporary estimate @xmath38 for the model parameters using the entire sample , which is used as starting value for the cross - validation procedure .",
    "3 .   partition the full data set into a training set , of size @xmath39 and a test set , of size @xmath40 4 .",
    "estimate the parameters on the training set using the starting values obtained in step 2 .",
    "compute the contribution to the log - likelihood of the test set .",
    "repeat @xmath41 times steps 3 - 4 and sum the contributions of the test sets to the log - likelihood which yields to the so - called cross - validated log - likelihood .",
    "select @xmath17 which maximizes the cross - validated log - likelihood .",
    "smyth ( 1996 ; 2000 ) advocates the use of the test set log - likelihood for selecting the number of mixture components .",
    "the rationale is that it can be shown to be an unbiased estimator ( within a constant ) of the kullback - leibler divergence between the _ truth _ and the model under consideration .",
    "as large test sets are hardly available , the cross - validate log - likelihood can be used to estimate the test set log - likelihood . in our case - like in smyth s case ( 1996 ) - given the model parameters , the cross - validated log - likelihood is a function of @xmath17 only , and maximizing it with respect to @xmath18 given the other model parameters , would handle the issue of overfitting as training and test sets are independent ( arlot and celisse , 2000 ) .",
    "the updates for the quantities of interest are obtained as follows . for a chosen value of the constant @xmath18 the expectation step ( e - step ) at the @xmath42-th iteration",
    "produces a guess for the quantity @xmath43 where @xmath44 and @xmath45 using the computed quantities from step e , the maximization step ( m - step ) involves the following closed - form updates : @xmath46 @xmath47 @xmath48    at each m - step , the final update for the variance is given by the following expression @xmath49 where @xmath50 indicates the @xmath9-th component constrained variance at @xmath51-th iteration .    constraining the components to common scale yields instead the following update for the variance @xmath52 which is the natural candidate for _ target _ for our shrinkage - like procedure .    in this constrained version of the em ,",
    "provided the initial guess satisfies the constraints , the algorithm is monotonic and the complete log - likelihood is maximized .",
    "a simulation study was conducted in order to evaluate the quality of the parameter estimates of our constrained algorithm .",
    "the equivariant data - driven constrained algorithm ( conc ) was compared with the unconstrained algorithm with common ( homoscedastic ) component - scales ( homn ) , and the unconstrained algorithm with different ( heretoscedastic ) component - scales ( hetn ) .",
    "the target measures used for the comparisons were average mean squared errors ( mse ) of the regression coefficients ( averaged across regressors and groups ) , and mse of the component variances ( averaged across groups ) .",
    "these measures will allow us to state the precision of the estimates .",
    "we measured how well the algorithms were able to classify sample units within clusters through the adjusted _ rand _ index ( adj - rand , hubert and arabie , 1985 ) .",
    "the data were generated from a clusterwise linear regression with 3 regressors and intercept , with 2 and 3 components and sample sizes of 100 and 200 .",
    "the class proportions considered were , respectively , @xmath53 and @xmath54 and @xmath55 and @xmath56 regressors have been drawn from 3 independent standard normals , whereas regression coefficients have been drawn from u@xmath57 and intercepts are @xmath58 and @xmath59 for the 2-component and 3-component models respectively .",
    "the component variances have been drawn from inv - gamma@xmath60 .    for each of the 8 combinations @xmath61 we generated 250 samples",
    ": for each sample and each algorithm - homn , hetn , conc ( our proposal ) - we select the best solution ( highest likelihood ) out of 10 random starts .",
    "the simulated - data analysis was conducted in matlab .",
    "table [ tab:100obs ] displays the results for @xmath62 2 groups and even class proportions ( the @xmath53 case ) is the most favorable condition for the unconstrained algorithms in terms of regression parameters mse .",
    "as we consider a larger number of classes ( @xmath63 ) , conc yields an mse for the betas which is 34% smaller than the ones reported for hetn and homn .",
    "the percentage difference nearly doubles when class proportions are uneven ( the @xmath64 case ) .",
    "similar results are reported for the variances mse , with exception of the @xmath53 case .",
    "conc delivers the most accurate classification in all 4 conditions .    increasing the sample size to 200 ( table [ tab:200obs ] ) improves the performance of all methods , especially hetn and conc .",
    "similarly to what has been reported for @xmath65 for the 3-group cases conc has an mse for the betas at least 66% smaller than the unconstrained methods - the difference is even larger for the variances mse .",
    "the adj - rand reported for conc is the closest to 1 , with exception of the @xmath53 case , where hetn does slightly better than conc ( 0.9784 , compared to 0.9777 for conc ) .",
    ".250 samples , @xmath65 10 random starts , 3 regressors and intercept .",
    "values averaged across samples .",
    "[ tab:100obs ] [ cols=\"<,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     .",
    "@xmath66 , and test set size = @xmath67    conc yields a clustering which is the closest to the true classification compared to homn and hetn ( figure [ fig : iris ] ) .",
    "the adj - rand obtained by conc , as we observe from table [ tab : iris ] , is 0.82 , whereas homn and hetn obtain much lower values - 0.55 and 0.44 respectively . on the other hand ,",
    "the computational time it takes for conc to run , multiple starting value strategy included , is more than 10 times longer than hetn and homn .",
    "in the present paper an equivariant data - driven constrained approach to maximum likelihood estimation of clusterwise linear regression model is formulated .",
    "this extends the approach proposed in rgd ( 2016 ) for multivariate mixtures of gaussians to the clusterwise linear regression context . through the simulation study and the three empirical applications , we are able to show that the method does not only solve the issue of degeneracy , but it is also able to improve upon the unconstrained approaches it was compared with .    whereas rgd ( 2016 ) showed that the method has merit in both fuzzy and crisp classification , the additional step ahead we take is twofold : 1 ) we look at how well model parameters are estimated , and 2 ) we are able to give a strong model interpretation . that is",
    ", the value of the selected constant @xmath17 indicates the most suitable - to - the - data model as a way through homoscedasticity and heteroscedasticity .",
    "this does not only relate to the estimated scales , but to the entire set of model parameters and classification , as also the clusterwise linear regressions and clustering estimated with our method correspond to an estimated model in between the homoscedastic model and the heteroscedastic model .",
    "our method shares common ground with the plain constrained maximum likelihood approach in that parameter updates are the same as in a constrained em ( ingrassia , 2004 ; ingrassia and rocci , 2007 ) .",
    "nevertheless , the final solution we obtain maximizes the ( log ) likelihood through a maximization of the cross - validated log - likelihood , and the constraints are tuned on the data .",
    "this eliminates all unreasonable boundary solutions standard constrained algorithm might converge to due to the arbitrary way constraints could be put .",
    "the equivariance property in a clusterwise linear regression framework is related to linear transformations of the dependent variable only .",
    "yet , it is as crucial as in the multivariate mixture of gaussians case , as it does not uniquely imply that the final clustering remains unaltered as one acts affine transformation on the variable of interest .",
    "more broadly , no matter how the data come in , affine equivariance means that there is no data transformation ensuring better results , since the method is unaffected by changes of scale in the response variable .",
    "as it was noticed by ritter ( 2014 ) , common scale is highly valuable , but it can be a too restrictive assumption for the clusters scales . in this respect ,",
    "our approach does not suffer the inappropriateness of the homoscedastic model , as the constant @xmath17 controls how close to ( or how far from ) it the final model will be . especially in the empirical application , we observed that the method is able to detect departures from homoscedasticity in terms of selected @xmath32    the simulation study and the empirical applications have highlighted two open issues .",
    "first , the proposed method is computationally intensive compared to the unconstrained approaches it was compared with . building up a computationally more efficient procedure to select the constant @xmath17 from the data can be an interesting topic for future research .",
    "second , the bic computed using the unconstrained models is not always reliable , as we observed from the empirical applications . how to carry out reliable model selection using our equivariant data - driven constrained approach requires further research .",
    "dempster , a.p . ,",
    "laird , n.m . , and rubin , d.b .",
    "maximum likelihood from incomplete data via the em algorithm .",
    "_ journal of the royal statistical society : series b ( statistical methodology ) , 39 , 1 - 38 .",
    "_            huber , p. j. ( 1967 ) .",
    "the behavior of maximum likelihood estimates under nonstandard conditions . in _ proceedings of the fifth berkeley symposium on mathematical statistics and probability ( vol . 1 , no",
    ". 1 , pp . 221 - 233 ) .",
    "_                                    smyth , p. ( 1996 ) .",
    "_ clustering using monte - carlo cross validation .",
    "_ in _ proceedings of the second international conference on knowledge discovery and data mining , menlo park , ca , aaai press , pp ."
  ],
  "abstract_text": [
    "<S> constrained approaches to maximum likelihood estimation in the context of finite mixtures of normals have been presented in the literature . a fully data - dependent constrained method for maximum likelihood estimation of clusterwise linear regression </S>",
    "<S> is proposed , which extends previous work in equivariant data - driven estimation of finite mixtures of gaussians for classification . </S>",
    "<S> the method imposes plausible bounds on the component variances , based on a target value estimated from the data , which we take to be the homoscedastic variance . </S>",
    "<S> nevertheless , the present work does not only focus on classification recovery , but also on how well model parameters are estimated . in particular , the paper sheds light on the shrinkage - like interpretation of the procedure , where the target is the homoscedastic model : this is not only related to how close to the target the estimated scales are , but extends to the estimated clusterwise linear regressions and classification . </S>",
    "<S> we show , based on simulation and real - data based results , that our approach yields a final model being the most appropriate - to - the - data compromise between the heteroscedastic model and the homoscedastic model . </S>",
    "<S> + * key words * : clusterwise linear regression , mixtures of linear regression models , adaptive constraints , equivariant estimators , plausible bounds , shrinkage estimators , constrained em algorithm . </S>"
  ]
}