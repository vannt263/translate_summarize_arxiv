{
  "article_text": [
    "a standard relu neural network with @xmath0 layers of @xmath1 nodes whose top layer has a linear activation is a piecewise linear function of its input",
    ". the number of pieces on which this function is defined can be quite large : with a single input and output , it can be @xmath2 ; see  @xcite . however , we prove in  @xcite that with standard initializations and under reasonable assumptions , after a small number of gradient descent training steps , such a network will have @xmath3 pieces , rather than @xmath2 . hence , single input and output networks start training with far less complexity than a naive parameter count would suggest .",
    "the purpose of this paper is threefold : to empirically demonstrate this simplicity in a wider variety of situations than is proved theoretically in  @xcite and to show it persists throughout training , to highlight training hyperparameters which can affect the quality of the trained network , and to indicate why the observed simplicity makes neural networks so useful .",
    "it is our hope that understanding these phenomena can guide research into improving the optimization of neural networks . in the following sections , we pursue each of these three goals . as a general rule ,",
    "all of our networks are standard feedforward relu neural networks with a linear final activation layer .",
    "we always initialize with glorot uniform method  @xcite and use the adadelta optimizer built in to keras  @xcite .",
    "in  @xcite , we prove that standard initializations on relu neural networks with one dimensional input and output will produce many neurons which are identically zero as functions of the input data .",
    "we show here that this phenomenon holds more generally .",
    "we constructed a network with three inputs , 20 hidden layers of 64 nodes , all with relu activation , and one linear output .",
    "we trained this network for 5 gradient steps on 1000 points of gaussian noise .",
    "we then computed the number of neurons which were identically zero as functions of the input data in the third , tenth , and twentieth layers .",
    "averaged over 10 experiments , these layers had 5% , 20% , and 30% identially zero neurons , respectively .",
    "thus the degeneracy which we proved in a simple case in fact holds more generally .",
    "it is an open question whether ( and how ) initialization and optimization should attempt to make use of these neurons more quickly .      as discussed above , we expect a relu neural network with one input and output and @xmath0 layers of @xmath1 neurons to generically produce a piecewise linear function with @xmath3 pieces , as opposed to the technically possible @xmath2 .",
    "when studying networks in higher dimensions ( with more inputs and outputs ) , it is necessary to use a different measure of size : the number of expected pieces grows exponentially in the dimension , so comparison becomes difficult .",
    "a better definition of size , which applies to any black box model @xmath4 , is as follows : fit @xmath4 to gaussian noise , and report the sum of the squares of the outputs of @xmath4 on the training inputs . see  @xcite and  @xcite for more information on gaussian complexity .",
    "we used this measure of size to demonstrate that our proved simplicity of networks with a single input and output holds more generally .",
    "we created networks with 3 inputs and 2 , 3 , and 4 hidden layers of 64 relu neurons , initialized with the glorot uniform method .",
    "we created many training data sets of gaussian noise with sizes between 20 and 2500 . for each network and each training data size , we trained the network for 50,000 epochs , each epoch consisting of a single gradient step with all the data .",
    "after each set of 500 epochs , we recorded the mean squared error on the training data and the sum of the squares of the predictions on the training data .    , @xmath5 , and @xmath6 , respectively.,title=\"fig : \" ] , @xmath5 , and @xmath6 , respectively.,title=\"fig : \" ]    the results of this experiment are shown in figure  [ fig : time_n_size ] , where each pixel shows the mean squared error or sum of squares for a single training data size at a particular epoch .",
    "each pixel is an average of 128 trials . on the right , the largest sum of squares achieved ( the deepest red ) is @xmath7 , @xmath5 , and @xmath6 respectively .",
    "the data itself has a sum of squares which has a @xmath8 distribution with degrees of freedom given by the number of data points .",
    "hence we compare @xmath7 , @xmath5 , and @xmath6 to @xmath9 , which is what a pefect fit of the data would achieve .",
    "even though these networks have more than enough parameters to fit the data perfectly , they clearly do not .",
    "when training , data is typically broken into batches . our definition of the batch size is the number of samples taken for each gradient step ( which aligns with the keras software  @xcite ) .",
    "although batch size does not typically receive much attention , the behavior of the network under training is highly dependent on the batching parameters .",
    ", left , when the training is done with batching .",
    "the top region appears different because of a slightly different color scale . ]",
    "figure  [ fig : time_n_size_batched ] shows the result of the same experiment as in figure  [ fig : time_n_size ] , left , except that a batch size of 256 is used if the number of points is at least 1024 . if there are remainder points at the end of an epoch , a gradient step is taken with this smaller batch .",
    "clearly , the size of the remainder batch can have a strong impact on the quality of the trained network , and batch selection is an important decision .",
    "note that the effect of the batching can be positive : the network clearly trains in fewer epochs ( due to more gradient steps per epoch ) , but the exact nature of the training is highly dependent on the batching parameter . in order to smooth the plot to produce a more predictable training",
    ", we could discard remainder data at the end of an epoch .",
    "we could also randomly sample every batch from the entire data set ; this gives us the most flexibility .",
    "we have established that optimization leaves networks much simpler than they could be .",
    "pushing optimization to the limit can highlight some degeneracies which can occur .",
    "in particular , initializing biases to zero causes networks to learn from the inside out , and local minima can trap networks before training is satisfactory .",
    "we created a neural network with a single input and output and 4 hidden layers of 500 relu neurons each .",
    "we sampled 1000 points on the high - frequency sawtooth wave shown in figure  [ fig : stuck ] , and we trained the network for 8000 , 16000 , and 20000 epochs , each with 2 batches of 500 points .",
    "we did two trials of this procedure .",
    "figure  [ fig : stuck ] shows the output of the networks as functions of the input for each of the epochs .",
    "note the degeneracy of the situation .",
    "we have shown that trained networks are much simpler than they could be , and that optimization can produce degenerate , undesirable effects . in this section",
    "we show that the difficulty optimization has can be valuable : the training time necessary to learn a function of a given frequency increases with the frequency .",
    "in addition , training is linear on combinations of functions : when fitting the sum of low and high frequency components , the network will fit the low frequency component first , leaving the high frequency component ( in practice , noise ) unfit .",
    "this gives one explanation for why training enormous networks for short periods of time can produce high quality models .",
    "we produced a training set of 64 points by taking a random linear spline with 8 knots and adding it to gaussian noise .",
    "we then fixed a network structure with one input and output and 4 hidden layers of 32 relu neurons , with glorot uniform initialization .",
    "we trained this network on three data sets : the spline plus the noise @xmath10 , the noiseless linear spline @xmath11 , and the noise @xmath1 itself . in the first three rows of figure  [ fig :",
    "linear_combo ] , we plot the output functions of 5 trials of this experiment after 20 , 100 , 200 , 1000 , 2000 , and 4000 epochs .        in the fourth row , we plot the difference between each trial trained on @xmath10 and the average of the trials trained on @xmath11 . essentially , the difference between the networks trained on the noisy and noiseless data ; this should approximate the noise . note that rows 3 and 4 fit the noise in a meaningful way at approximately the same rate .",
    "figure  [ fig : mse_pure_noise_vs_diff ] compares the mean squared error of rows 3 and 4 at 10 equally spaced points in training time .",
    "these experiments show that the training time necessary to learn the noise is essentially unaffected by the addition of a low frequency component .",
    "the difference between the noisy and noiseless plots ( row 4 of figure  [ fig : linear_combo ] ) qualitatively fits the noise more uniformly than row 3 , which learns from the origin outward , in a manner similar to figure  [ fig : stuck ] .",
    "this suggests that if we are actually interested in learning the noise , we might consider adding a low fequency function to it , training on the result , and subtracting the known function .",
    "it is an interesting open question wether this `` artificial boosting '' can improve the quality of a fit in general .",
    "peter l. bartlett and shahar mendelson . _",
    "rademacher and gaussian complexities : risk bounds and structural results_. journal of machine learning research * 3 * ( 3 ) 2002 , 463482 .",
    "kevin k. chen .",
    "_ the upper bound on knots in neural networks _",
    ", preprint .",
    "kevin k. chen , anthony gamst , and alden walker .",
    "_ knots in random neural networks _ , preprint .",
    "kevin k. chen , anthony gamst , and alden walker . _",
    "the empirical size and risk of trained neural networks _ ,",
    "franois chollet .",
    "_ keras_. github , 2015 .",
    "url : ` https://github.com/fchollet/keras ` .",
    "xavier glorot and yoshua bengio _ understanding the difficulty of training deep feedforward neural networks _",
    ", international conference on artificial intelligence and statistics , 2010 .",
    "guido montfar , razvan pascanu , kyunghyun cho , and yoshua bengio , _ on the number of linear regions of deep neural networks _ , advances in neural information processing systems 2014 , 29242932 .",
    "pascanu razvan , guido montfar , and yoshua bengio , _ on the number of response regions of deep feedforward networks with piecewise linear activations _ , preprint : arxiv:1312.6098v5 , 2014 .",
    "maithra raghu , ben poole , jon kleinberg , surya ganguli , and jascha sohl - dickstein , _ on the expressive power of deep neural networks _ , preprint : arxiv:1606.05336v2 , 2016 ."
  ],
  "abstract_text": [
    "<S> relu neural networks define piecewise linear functions of their inputs . however , initializing and training a neural network is very different from fitting a linear spline . in this paper , we expand empirically upon previous theoretical work to demonstrate features of trained neural networks . </S>",
    "<S> standard network initialization and training produce networks vastly simpler than a naive parameter count would suggest and can impart odd features to the trained network </S>",
    "<S> . however , we also show the forced simplicity is beneficial and , indeed , critical for the wide success of these networks . </S>"
  ]
}