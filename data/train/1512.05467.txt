{
  "article_text": [
    "most machine learning algorithms use a representation space based on a _ -based _ format .",
    "this format is a simple way to describe an instance as a measurement vector on a set of predefined . in the case of supervised learning ,",
    "a class label is also available .",
    "one limitation of the -based format is that supplied features sometimes do not adequately describe , in terms of classification , the semantics of the dataset .",
    "this happens , for example , when general - purpose are used to describe a collection that contains certain relations between individuals .    in order to obtain good results in classification tasks , many algorithms and preprocessing techniques ( e.g. , svm  @xcite , pca  @xcite _ etc .",
    "_ ) deal with non - adequate variables by internally changing the description space .",
    "the main drawback of these approaches is that they function as a black box , where the new representation space is either hidden ( for svm ) or completely synthetic and incomprehensible to human readers ( pca ) .",
    "the purpose of our work is to construct a new feature set that is more descriptive for both supervised and unsupervised classification tasks . in the same way that * frequent itemsets * @xcite help users to understand the patterns in transactions , our goal with the new is to help understand relations between individuals of datasets .",
    "therefore , the new features should be easily comprehensible by a human reader .",
    "literature proposes algorithms that construct features based on the original user - supplied ( called primitives ) .",
    "however , to our knowledge , all of these algorithms construct the feature set in a supervised way , based on the class information , supplied _ a priori _ with the data .    in order to construct new features ,",
    "we propose two algorithms that create new feature sets in the absence of classified examples , in an unsupervised manner .",
    "the first algorithm is an adaptation of an established supervised algorithm , making it unsupervised . for the second algorithm ,",
    "we have developed a completely new heuristic that selects , at each iteration , pairs of highly correlated and replaces them with conjunctions of literals that do not co - occur .",
    "therefore , the overall redundancy of the feature set is reduced .",
    "later iterations create more complex boolean formulas , which can contain negations ( meaning absence of features ) .",
    "we use statistical considerations ( hypothesis testing ) to automatically determine the value of parameters depending on the dataset , and a _ pareto front _",
    "@xcite - inspired method for the evaluation .",
    "the main advantage of the proposed methods over pca or the kernel of the svm is that the newly - created features are comprehensible to human readers ( features like @xmath1 and @xmath2 are easily interpretable ) .    in sections  [ sec : ufringe ]  and",
    "[ sec : our - proposition ] , we present our proposed algorithms and in section  [ sec : oi ] we describe the evaluation metrics and the complexity measures . in section  [ sec : xps ] , we perform a set of initial experiments and outline some of the inconveniences of the algorithms . in section  [ sec : improvements ] , by use of statistical hypothesis testing , we address these weak points , notably the choice of the threshold parameter . in section  [ sec : second - xp ] , a second set of experiments validates the proposed improvements .",
    "finally , section  [ sec : conclusion - future - work ] draws the conclusion and outlines future works .",
    "c@c@c & & + & & +    in the context of classification ( supervised or unsupervised ) , a useful feature needs to portray new information .",
    "a feature @xmath3 , that is highly correlated with another feature @xmath4 , does not bring any new information , since the value of @xmath3 can be deduced from that of @xmath4 .",
    "subsequently , one could filter out `` irrelevant '' before applying the classification algorithm . but by simply removing certain features , one runs the risk of losing important information of the * hidden structure of the feature set * , and this is the reason why we perform * feature construction*.    we deal primarily with datasets described with * boolean * .",
    "any dataset described by using the -value format can be converted to a binary format using discretization and binarization .",
    "in real - life datasets , most binary have specific meanings .",
    "let us consider the example of a set of images that are tagged using boolean features .",
    "each feature marks the presence ( * true * ) or the absence ( * false * ) of a certain object in the image .",
    "these objects could include : @xmath5 , @xmath6 , @xmath7 , @xmath8 , @xmath9 or @xmath10 . in this case ,",
    "part of the semantic structure of the feature set can be guessed quite easily .",
    "relations like `` is - a '' and `` part - of '' are fairly intuitive : @xmath6 is a sort of @xmath5 , @xmath11 is part of @xmath12 etc . but other relations might be induced by the semantics of the dataset ( images in our example ) .",
    "@xmath7 will co - occur with @xmath8 , for they usually take place in the city .",
    "[ fig : tags - examples ] depicts a simple image dataset described using the feature set @xmath13 , @xmath14 , @xmath15 , @xmath16 .",
    "the feature set is quite redundant and some of the features are non - informative ( e.g. , feature @xmath9 is present for all individuals ) .",
    "considering co - occurrences between features , we could create the more eloquent features @xmath17 ( describing the top row ) and @xmath18 ( describing the bottom row ) .",
    "the idea is to create a data - dependent feature set , so that the new features are as independent as possible , limiting co - occurrences between the new features . at the same time",
    "they should be comprehensible to the human reader .",
    "he * svm algorithm *  @xcite constructs a kernel function that changes the description space into a new separable one . supervised and",
    "non - supervised algorithms can be boosted by pre - processing with * principal component analysis * ( pca )  @xcite .",
    "pca is a mathematical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of uncorrelated variables , called _",
    "principal components_. * manifold learning *  @xcite can be seen as a classification approach where the representation space is changed internally in order to boost the performances .",
    "feature extraction mainly seeks to reduce the description space and redundancy .",
    "newly - created features and very difficult to interpret .",
    "* feature construction * feature construction has mainly been used with decision tree learning .",
    "new served as hypotheses and were used as discriminators in decision trees .",
    "supervised feature construction can also be applied in other domains , like decision rule learning  @xcite .",
    "@xmath19  set of primitive user - given features @xmath20  the data expressed using @xmath19 which will be used to construct features + * inner parameters : * @xmath21  set of operators for constructing features , @xmath22  machine learning algorithm to be employed @xmath23  set of new ( constructed and/or primitives ) features .",
    "@xmath24 @xmath25 @xmath26 @xmath27 * convert*(@xmath28 ) @xmath29 run * m*(@xmath30 ) @xmath31 new feat .",
    "constructed with * * op**@xmath32 prune useless features in @xmath23    algorithm  [ algo : general - algorithm ] , presented in @xcite , represents the general schema followed by most constructive induction algorithms .",
    "the general idea is to start from @xmath20 , the dataset described with the set of primitive . using a set of constructors and the results of a machine learning algorithm ,",
    "the algorithm constructs new features that are added to the feature set . in the end",
    ", useless features are pruned .",
    "these steps are iterated until some stopping criterion is met ( e.g. , a maximum number of iterations created features ) .",
    "most constructive induction systems construct as conjunctions or disjunctions of literals .",
    "literals are the features or their negations .",
    "e.g. , for the feature set @xmath33 the literal set is @xmath34 .",
    "operator sets @xmath35 and @xmath36 are both complete sets for the boolean space .",
    "any boolean function can be created using only operators from one set .",
    "fringe  @xcite creates new using a decision tree that it builds at each iteration .",
    "new are conjunctions of the last two nodes in each positive path ( a positive path connects the root with a leaf having the class label * true * ) .",
    "the newly - created features are added to the feature set and then used in the next iteration to construct the decision tree .",
    "this first algorithm of feature construction was initially designed to solve replication problems in decision trees .",
    "other algorithms have further improved this approach .",
    "citre  @xcite adds other search strategies like _ root _",
    "( selects first two nodes in a positive path ) or _ root - fringe _ ( selects the first and last node in the path ) .",
    "it also introduces domain - knowledge by applying filters to prune the constructed features .",
    "cat  @xcite is another example of a hypothesis - driven constructive algorithm similar to fringe .",
    "it also constructs conjunctive features based on the output of decision trees .",
    "it uses a dynamic - path based approach ( the conditions used to generate new features are chosen dynamically ) and it includes a pruning technique .",
    "there are alternative representations , other than conjunctive and disjunctive .",
    "the m - of - n and x - of - n representations use -value pairs .",
    "an -value pair @xmath37 is * true * for an instance if and only if the @xmath38 has the value @xmath39 for that instance .",
    "the difference between m - of - n and x - of - n is that , while the second one counts the number of true -value pairs , the first one uses a threshold parameter to assign a value of truth for the entire representation .",
    "the algorithm @xmath40  @xcite uses m - of - n representations for the newly - created features .",
    "it has a specialization and a generalization construction operator and it does not need to construct a new decision tree at each step , but instead integrates the feature construction in the decision tree construction .",
    "the @xmath41  @xcite algorithm functions similarly , except that it uses the x - of - n representation .",
    "it also takes into account the complexity of the features generated .",
    "comparative studies like @xcite show that conjunctive and disjunctive representations have very similar performances in terms of prediction accuracy and theoretical complexity .",
    "m - of - n , while more complex , has a stronger representation power than the two before .",
    "the x - of - n representation has the strongest representation power , but the same studies show that it suffers from data fragmenting more than the other three .",
    "the problem with all of these algorithms is that they all work in a supervised environment and they can not function without a class label . in the following sections , we will propose two approaches towards unsupervised feature construction .",
    "we propose * ufringe * , an unsupervised version of fringe , one of the first feature construction algorithms .",
    "fringe  @xcite is a framework algorithm ( see section  [ sec : state - of - the - art ] ) , following the same general schema shown in algorithm  [ algo : general - algorithm ] .",
    "it creates new features using a logical decision tree , created using a traditional algorithm like id3  @xcite or c4.5  @xcite .",
    "taking a closer look at fringe , one would observe that its only component that is supervised is the decision tree construction .",
    "the actual construction of features is independent of the existence of a class attribute .",
    "hence , using an unsupervised decision tree construction algorithm renders fringe unsupervised .    * clustering trees *  @xcite were introduced as generalized logical decision trees .",
    "they are constructed using a top - down strategy . at each step , the cluster under a node is split into two , seeking to maximize the intra - cluster variance .",
    "the authors argue that supervised indicators , used in traditional decision trees algorithms , are special cases of intra - cluster variance , as they measure intra - cluster * class * diversity . following this interpretation ,",
    "clustering trees can be considered generalizations of decision trees and are suitable candidates for replacing id3 in * ufringe*.    adapting fringe to use clustering trees is straightforward : it is enough to replace * m * in algorithm  [ algo : general - algorithm ] with the clustering trees algorithm . at each step ,",
    "ufringe constructs a clustering tree using the dataset and the current feature set . just like in fringe , new",
    "are created using the conditions under the last two nodes in each path connecting the root to a leaf .",
    "fringe constructs new features starting only from positive leaves ( leaves labelled true ) .",
    "but unlike decision trees , in classification trees the leaves are not labelled using class . therefore ,",
    "ufringe constructs new features based on all paths from root to a leaf .",
    "newly - constructed are added to the feature set and used in the next classification tree construction .",
    "the algorithm stops when either no more features can be constructed from the clustering tree or when a maximum allowed number of features have already been constructed .",
    "we address the limitations of ufringe by proposing a second , innovative approach .",
    "we propose an iterative algorithm that reduces the overall correlation of features of a dataset by iteratively replacing pairs of highly correlated with conjunctions of literals .",
    "we use a greedy search strategy to identify the features that are highly correlated , then use a construction operator to create new features . from two correlated features @xmath42 and @xmath43",
    "we create three new features : @xmath44 , @xmath45 and @xmath46 . in the end ,",
    "both @xmath42 and @xmath43 are removed from the feature set .",
    "the algorithm stops it has performed a maximum number of iterations .",
    "the different key parts of the algorithm ( e.g. , the search strategy , construction operators or feature pruning ) will be presented in the next sections .",
    "[ fig : algo - evolution ] illustrates visually , using venn diagrams , how the algorithm replaces the old features with new ones .",
    "features are represented as rectangles , where the rectangle for each feature contains the individuals having that feature set to * true*. naturally , the individuals in the intersection of two rectangles have both features set to * true*. @xmath47 and @xmath48 have a big intersection , showing that they co - occur frequently . on the contrary , @xmath48 and",
    "@xmath49 have a small intersection , suggesting that their co - occurrence is less than that of the hazard ( negatively correlated ) .",
    "@xmath50 is included in the intersection of @xmath47 and @xmath48 , while @xmath51 has no common elements with any other .",
    "@xmath51 is incompatible with all of the others .    in the first iteration , @xmath47 and @xmath48",
    "are combined and 3 features are created : @xmath52 , @xmath53 and @xmath54 .",
    "these new features will replace the original ones . at the second iteration ,",
    "@xmath52 is combined with @xmath50 . as @xmath50",
    "is contained in @xmath52 , the @xmath55 will have a support equal to zero and will be removed .",
    "note that @xmath48 and @xmath49 are never combined , as they are considered uncorrelated .",
    "the final feature set will be @xmath56",
    "we define the set @xmath57 of @xmath58 user - supplied features and @xmath59 the dataset described using @xmath19 .",
    "we start from the hypothesis that even if the primitive set @xmath19 can not adequately describe the dataset @xmath20 , there is a data - specific feature set @xmath60 that can be created in order to represent the data better .",
    "new features are iteratively created using conjunctions of primitive features or their negations ( as seen in fig .",
    "[ fig : algo - evolution ] ) .",
    "our algorithm does not use the output of a learning algorithm in order to create the new features .",
    "instead we use a and a feature set evaluation function that can determine if a newly - obtained feature set is more appropriate than the former one .",
    "@xmath19  set of primitive user - given features @xmath20  the data expressed using @xmath19 which will be used to construct features * inner parameters : * @xmath61  correlation threshold for searching , @xmath62  max no of iterations . @xmath23  set of newly - constructed features .",
    "@xmath63 @xmath25    @xmath26 @xmath64 * search_correlated_pairs*(@xmath65 ) @xmath66 @xmath67 * highest_scoring_pair*(@xmath68 ) @xmath69 * construct_new_feat*(@xmath70 ) * remove_candidate*(@xmath71 )    * prune_obsolete_features*(@xmath72 ) @xmath73 * convert*(@xmath74 )    @xmath75    the schema of our proposal is presented in algorithm  [ algo : our - proposition ] .",
    "the feature construction is performed starting from the dataset @xmath20 and the primitives @xmath19 .",
    "the algorithm follows the general inductive schema presented in algorithm  [ algo : general - algorithm ] . at each iteration ,",
    "* ufc * searches for frequently co - occurring pairs in the feature set created at the previous iteration ( @xmath76 ) .",
    "it determines the candidate set @xmath68 and then creates new features as conjunctions of the highest scoring pairs .",
    "the new features are added to the current set ( @xmath77 ) , after which the set is filtered in order to remove obsolete features . at the end of each iteration , the dataset @xmath20 is translated to reflect the feature set @xmath77 .",
    "a new iteration is performed as long as new features were generated in the current iteration and a maximum number of iterations have not yet been reached ( @xmath62 is a parameter for the algorithm ) .",
    "the * search_correlated_pairs * function searches for frequently co - occurring pairs of features in a feature set @xmath23 .",
    "we start with an empty set @xmath78 and we investigate all possible pairs of features @xmath79 .",
    "we use a function ( @xmath80 ) to measure the co - occurrence of a pair of features @xmath81 and compare it to a threshold @xmath61 .",
    "if the value of the function is above the threshold , then their co - occurrence is considered as significant and the pair is added to @xmath68 .",
    "therefore , @xmath68 will be @xmath82    the @xmath80 function is the empirical * pearson correlation coefficient * , which is a measure of the strength of the linear dependency between two variables . @xmath83 $ ] and it is defined as the covariance of the two variables divided by the product of their standard deviations .",
    "the sign of the @xmath80 function gives the direction of the correlation ( inverse correlation for @xmath84 and positive correlation for @xmath85 ) , while the absolute value or the square gives the strength of the correlation .",
    "a value of 0 implies that there is no linear correlation between the variables . when applied to boolean variables , having the contingency table as shown in table  [ tab : contingency - table ] , the @xmath80 function has the following formulation : @xmath86    .contingency table for two boolean [ cols=\"<,<,<\",options=\"header \" , ]     fig .",
    "[ sub - fig : hu - risk ] presents the distribution of solutions created by the `` risk - based '' heuristic with multiple @xmath87 , plotted on the same graphics as the pareto front in the ( @xmath88 , @xmath89 ) space .",
    "solutions for different values of risk @xmath87 are grouped closely together .",
    "not all of them are on the pareto front , but they are never too far from the `` closest - point '' solution , providing a good equilibrium between quality and complexity .    on ` street ` , performances of the `` risk - based '' heuristic start to degrade compared to * ufc**. table  [ tab : result - risk - based ] shows differences in the resulted complexity and only @xmath90 of the constructed features are common for the two approaches .",
    "[ sub - fig : street - risk - based ] shows that solutions found by the `` risk - based '' approach are moving away from the `` closest - point '' .",
    "the cause is the large size of the ` street ` dataset . as the sample size increases",
    ", the null hypothesis tends to be rejected at lower levels of p - value .",
    "the auto - determined @xmath61 threshold is set too low and the constructed feature sets are too complex .",
    "pruning solves this problem as shown in fig .",
    "[ sub - fig : street - risk - based - f ] and section  [ subsec : pruning ] .",
    "the pruning technique is independent of the `` risk - based '' heuristic and can be applied in conjunction with the classical * ufc * algorithm .",
    "an execution of this type will be denoted * * ufc**@xmath91 .",
    "we execute * * ufc**@xmath91 with the same parameters and on the same datasets as described in section  [ subsec : pareto - evaluation ] .",
    "we compare * ufc * with and without pruning by plotting on the same graphic the two pareto fronts resulted from each set of executions .",
    "[ sub - fig : hu - f_and_nf ] shows the pruned and non - pruned pareto fronts on ` hungarian ` .",
    "the graphic should be interpreted in a manner similar to a roc curve , since the algorithm seeks to minimize @xmath88 and @xmath89 at the same time .",
    "when one pareto front runs closer to the origin of the graphic @xmath92 than a second , it means that the first dominates the second one and , thus , its corresponding approach yields better results . for all datasets , the pruned pareto front dominates the non - pruned one .",
    "the difference is marginal , but proves that filtering improves results .",
    "the most important conclusion is that filtering limits complexity . as the initial experiments ( fig .",
    "[ sub - fig : pareto - front ] ) showed , most of the non - pruned solutions correspond to very high complexities .",
    "visually , the pareto front is tangent to the vertical axis ( the complexity ) and showing complexities around @xmath93 ( out of 1 ) . on the other hand , the pareto front corresponding to the pruned approach stops , for all datasets , for complexities lower than @xmath94 .",
    "this proves that filtering successfully discards solutions that are too complex to be interpretable .",
    "last , but not least , filtering corrects the problem of automatically choosing @xmath61 for the `` risk - based '' heuristic on big datasets .",
    "we ran * * ufc**@xmath95 with risk @xmath96 , @xmath971 , @xmath98 , @xmath99 , @xmath100 , @xmath101 , @xmath102 , @xmath103 , @xmath104 , @xmath105 .",
    "[ sub - fig : street - risk - based - f ] presents the distributions of solutions found with the `` risk - based pruned '' heuristic on ` street ` . unlike results without pruning ( fig .",
    "[ sub - fig : street - risk - based ] ) , solutions generated with pruning are distributed closely to those generated by `` closest - point '' and to the pareto front .",
    "[ [ subsec : stability ] ]",
    "in this article , we propose two approaches towards feature construction . unlike the other feature construction algorithms proposed so far in the literature , our proposals work in an unsupervised learning paradigm . *",
    "ufringe * is an unsupervised adaptation of the fringe algorithm , while * ufc * is a new approach that replaces linearly correlated features with conjunctions of literals .",
    "we prove that our approaches succeed in reducing the overall correlation in the feature set , while constructing comprehensible and interpretable features .",
    "we have performed extensive experiments to highlight the impact of parameters on the total correlation measure and feature set complexity . based on the first set of experiments , we have proposed a heuristic that finds a suitable balance between quality and complexity and avoids time consuming multiple executions , followed by a pareto front construction .",
    "we use statistical hypothesis testing and confidence levels for parameter approximation and reasoning on the pareto front of the solutions for evaluation .",
    "we also propose a pruning technique , based on hypothesis testing , that limits the complexity of the generated features and speeds up the construction process .    for future development , we consider taking into account non - linear correlation between variables by modifying the metric of the search and the co - occurence measure .",
    "another research direction will be adapting our algorithms for data of the web 2.0 ( e.g. , automatic treatment of labels on the web ) . several challenges arise , like very large label sets ( it is common to have over 10 000 features ) , non - standard label names ( see standardization preprocessing task that we have performed for the labelme dataset in section  [ sec : xps ] ) and missing data ( a value of * false * can mean absence or missing data ) .",
    "we also consider converting generated features to the disjunctive normal form for easier reading and suppressing features that have a low support in the dataset .",
    "this would reduce the size of the feature set by removing rare features , but would introduce new difficulties such as detecting nuggets .",
    "lallich s , rakotomalala r ( 2000 ) fast feature selection using partial correlation for multi - valued attributes . in :",
    "zighed da , komorowski j , zytkow jm ( eds ) proceedings of the 4th european conference on principles of data mining and knowledge discovery , lnai springer - verlag , pp 221231                murphy pm , pazzani mj ( 1991 ) id2-of-3 : constructive induction of m - of - n concepts for discriminators in decision trees . in : proceedings of the eighth international workshop on machine learning , pp 183187                  yang ds , rendell l , blix g ( 1991 ) a scheme for feature construction and a comparison of empirical methods . in : proceedings of the twelfth international joint conference on artificial intelligence , pp 699704"
  ],
  "abstract_text": [
    "<S> -based format is the main data representation format used by machine learning algorithms . when the do not properly describe the initial data , performance starts to degrade . </S>",
    "<S> some algorithms address this problem by internally changing the representation space , but the newly - constructed features are rarely . </S>",
    "<S> we seek to construct , in an unsupervised way , new that are more appropriate for describing a given dataset and , at the same time , comprehensible for a human user . </S>",
    "<S> we propose two algorithms that construct the new as conjunctions of the initial primitive or their negations . </S>",
    "<S> the generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset . </S>",
    "<S> for example , a feature like @xmath0 would be true for non - urban images and is more informative than simple features expressing the presence or the absence of an object . </S>",
    "<S> the notion of pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set . </S>",
    "<S> statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data - dependent feature set . </S>",
    "<S> we experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets . </S>"
  ]
}