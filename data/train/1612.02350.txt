{
  "article_text": [
    "music summarization is a task deemed important by the  community that has been studied for several years .",
    "there are two types of music summarization , defined by the purpose they are intended to address : human- and machine - oriented . since their focus is different , naturally , the corresponding requirements also differ .",
    "human - oriented summarization needs to take into account that humans will directly consume the summary of the original creative artifact @xcite .",
    "thus , perceptually relevant requirements are at play , such as clarity and coherence , so that people can enjoy listening to the summary in the same way they enjoy listening to the whole song .",
    "generic machine - oriented summarization algorithms , however , are agnostic to the fact that they are dealing with creative artifacts , that is , their purpose is to output a shorter version of the input song whose content is optimal , e.g. , in terms of relevance and diversity .",
    "such summarizers , although not adequate for human consumption , can be used for further automatic ( and optimized ) processing .",
    "generic summarization algorithms have been originally developed for and successfully applied in text and speech @xcite .",
    "these algorithms are media - agnostic and have no explicit criteria for taking clarity or coherence into account .",
    "therefore , when applying them to music , undesirable effects ( from a human consumption point of view ) may be observed in the resulting summaries , e.g. , harsh discontinuities or irregularities in beat synchronization .",
    "however , they have already been proven to be state - of - the - art for machine - oriented music processing , namely , for genre classification tasks @xcite .    in this work , we approach machine - oriented summarization from an information - theoretic perspective . unlike previous efforts , where we assessed the contributions of summaries to specific tasks ( i.e. , genre classification ) ,",
    "we intend to assess the contribution of these algorithms to  in a task - agnostic way by comparing probabilistic descriptions of music .",
    "specifically , we use gaussian distributions , which provide a high - level , probabilistic explanation of the full and summarized songs .",
    "we then compute the  divergence from the full gaussians to the summary gaussians . since each gaussian is a high - level explanation of the data , the  divergence is a measure of",
    "how well a gaussian represents the data that is best represented by another gaussian . in this instance , since each summary gaussian represents a summary of a song , this also reflects how well a summary clip represents the full song .",
    "thus , we can generically assess the information content of summarized music and show that state - of - the - art summarizers lose less information than the baselines ( i.e. , continuous - segment beginning , middle , end of the songs , and average similarity @xcite ) .",
    "moreover , we propose a simple method , focusing on minimizing the  divergence between the original and summarized songs , that outperforms previous state - of - the - art methods on machine - oriented summarization .",
    "both of these observations strongly suggest that the information content of these summaries , as assessed by the  divergence , is a good predictor of summarization performance in tasks that rely on a bag - of - frames model .",
    "benefits of using summarized data include faster processing , less disk space use , more efficient use of bandwidth , and alleviation of copyright issues .",
    "we summarize each of 1250 songs of a multi - genre music dataset , using a selection of algorithms :  @xcite ,  @xcite , and support sets @xcite , as examples of explicitly addressing diversity , latent topic decomposition , and classic centrality , respectively .",
    "we also summarize using average similarity  @xcite and fixed - segment extraction as baselines , and our proposed gaussian sampling summarizer .",
    "then , we estimate a gaussian for each song version , i.e. , the full song and all of the summarized versions of it ( including baselines ) .",
    "the contribution of summarization is assessed by comparing the  divergences from the full gaussian ( the one estimated using the original , full song ) to every other gaussian .",
    "we show every previous state - of - the - art methods outperform the baselines and that our method outperforms every state - of - the - art method .",
    "these results strengthen and generalize previous conclusions derived from music genre classification @xcite .",
    "the rest of the article is organized as follows : section [ sec : previous - work ] reviews previous work on machine - oriented music summarization . in section [ sec : gaussian - sampler ] , we describe our proposed gaussian sampling summarization method .",
    "the  divergence is described in section [ sec : kullback - leibler ] .",
    "section [ sec : experiments ] presents details of the experiments .",
    "section [ sec : results ] reports results on the performance of summaries in describing the full songs . section [ sec : discussion ] discusses the results , and section [ sec : conclusions ] concludes the paper and considers future work .",
    "the focus of music - specific summarization algorithms is on extracting an enjoyable summary that people can listen clearly and coherently .",
    "since our approach considers summaries exclusively for automatic consumption , these algorithms as well as many of their issues and requirements are outside the scope of this paper . a full discussion concerning this issue and that of using other audio proxies ( such as ) can be found in @xcite . as opposed to using these methods , we employ the following generic media - agnostic summarization algorithms :  @xcite ,  @xcite , and support sets @xcite .",
    "we also consider a music - specific summarization algorithm , average similarity , as baseline .",
    "refer to @xcite for an overview of these algorithms .",
    "music summarization for machine consumption has already been evaluated in the past @xcite , in the contexts of binary and multiclass music genre classification . in those instances , generic summarization methods , which were originally developed for text and speech summarization ,",
    "were proven to be very good at machine - oriented music summarization .",
    "the application of these algorithms to music is not direct , since an initial fixed - size segmentation of the songs and a discretization step must be performed in order to map the continuous stream of real - valued audio features to the discrete concepts of sentences and words @xcite , respectively .",
    "this must be done for all of these generic summarizers , since all of them build summaries by ranking phrases and picking the top ranked ones until reaching the required summary length . in turn , the resulting audio summaries are characterized by a concatenation of continuous audio phrases .",
    "for instance , if we consider 0.5s words and 10-word phrases and a 30s summary , the generic summarizers will output a summary consisting of a concatenation of six 5s phrases .",
    "the difference between these algorithms lies solely in the way they rank phrases . despite being well suited for machine - oriented tasks",
    ", this whole process leads to summaries that are not enjoyable to listen , since there usually exist harsh discontinuities between phrases , even though each phrase is a continuous segment .",
    "we propose a novel method for machine - oriented summarization that aims at building summaries whose gaussian distribution is as close as possible to the gaussian distribution of the original song .",
    "note that this corresponds to building a summary that minimizes the  divergence of its gaussian distribution to the gaussian distribution of the original song , that is , building a summary that minimizes information loss .",
    "the summarization procedure consists of estimating a multivariate gaussian for the full song and , iteratively , drawing synthetic samples form that gaussian and picking the closest frame to the sample , using the scale - invariant mahalanobis distance @xcite .",
    "we are essentially sampling from the original pool of frames .",
    "however , the frames in the audio are rarely exactly equal to the generated samples .",
    "therefore , the distribution of the selected frames will not be exactly equal to the distribution of the samples , namely , their mean will be shifted . in order to minimize this error",
    ", we introduce a heuristic that updates a difference vector ( initialized as 0 ) , keeping track of the resulting shift and influencing frame selection in every iteration .",
    "the following pseudo - code illustrates this procedure :    @xmath0 @xmath1 @xmath2",
    "in this work , we generalize conclusions of previous work by evaluating those algorithms in an information - theoretic and task - agnostic way , i.e. , by computing information loss according to the  divergence . the  divergence , also called relative entropy , is a non - symmetric measure of the difference between two probability distributions @xmath3 and @xmath4 . specifically , the  divergence of @xmath4 from @xmath3 , @xmath5 , is a measure of information gain achieved by using @xmath3 instead of @xmath4 . in other words , it is a measure of how much information is lost when @xmath4 is used as an approximation of @xmath3 . in this work , @xmath4 always models some summarized version of the original data , which itself is modeled by @xmath3 , so the relative entropy is ideal for measuring how much information is lost by the corresponding summarization process .",
    "the  divergence between two gaussians @xmath6 and @xmath7 is defined as : @xmath8 where @xmath9 is the dimensionality of the gaussians , @xmath10 and @xmath11 are the mean and covariance of gaussian @xmath12 , respectively , @xmath13 is the trace operator , and @xmath14 is the determinant operator .",
    "we evaluate summarization by modeling the full and summarized versions of songs with full - covariance gaussians and by computing the  divergence from each full gaussian to each of the corresponding summarized gaussians . since the summarizers perform well at selecting relevant and diverse information ( as has been shown in the context of music classification @xcite )",
    ", then we should be able to assess that performance from an information - theoretic perspective , by measuring the relative entropy from the full to the summarized gaussians . as baselines for this task , we compute gaussians for naive summarization heuristics , namely , the beginning , middle , and end sections of the songs , as well as for average similarity",
    ". these are common practice in  tasks , specifically , in , where 30s segments are considered . when evaluating the performance of the gaussian sampler , every other summarizer and heuristic",
    "is considered as a baseline .",
    "note that there are two different feature extraction steps .",
    "the first is done by the summarizers , every time a song is summarized .",
    "the summarizers output the audio signal corresponding to the selected parts , to be used in the second step , i.e. , when estimating the gaussians , where another set of features is extracted from the full and summarized songs .      in line with previous work @xcite , we compute the first 20  after a 0.5s framing of the input signal ( no overlap ) as summarization features .",
    "when estimating the gaussians , we use a 29-dimensional vector per frame , concatenating several features used in @xcite .",
    "these state - of - the - art features describe the timbral texture of a music piece and they consist of the first 20  as well as 9 spectral features : centroid , spread , skewness , kurtosis , flux , rolloff , brightness , entropy , and flatness .",
    "these are computed for every 50ms frame without overlap .",
    "the dataset used in our experiments is a multi - genre music dataset @xcite . in order to remove irrelevant information",
    ", we trim silent segments from the beginning and end of each song as a preprocessing step .",
    "then , we compute each summarized version of the full dataset .",
    "this translates into summarizing the dataset , each song at a time , for each algorithm / heuristic and parameter combination we consider . for all baselines ,",
    "we consider output summary durations from 5s to 120s .",
    "furthermore , we summarize the dataset with , , and support sets , for vocabulary sizes ranging from 5% to 30% of the duration of the songs ( in frames ) and summary durations from 5s to 30s .",
    "we also consider 10-word sentences and experiment with binary and dampened tf - idf weighting .",
    "our method is only parameterized in terms of frame size , so we experimented with sizes of 0.01s , 0.05s , 0.10s , and 0.50s .",
    "we also summarized the dataset without the mean shift correction heuristic to assess its impact .",
    "after computing every song version , we estimate full - covariance gaussians for all of them .",
    "then , we evaluate how much information is lost by each song version , when compared to the full song , by computing its relative entropy , i.e. , the  divergence from the full gaussian to the corresponding summarized gaussian .",
    "all algorithms were implemented in c++ .",
    "opensmile @xcite was used for feature extraction , armadillo @xcite and eigen @xcite for matrix operations , and marsyas @xcite for synthesizing audio .",
    "we present results comparing the descriptive performance of summarized versions of the dataset , by computing the average of @xmath5 for each summarization setup , where @xmath3 is the full songs gaussian and for each of the corresponding summarized song gaussians @xmath4 .",
    "table [ tab : kl ] shows some of these values for the ( summarization ) parameter combinations that performed best for the 30s summaries and baselines .",
    "these parameters are : vocabulary sizes ( as a fraction of the song s length , in words ) of 0.30 , 0.05 , and 0.05 for the , , and support sets , respectively ; binary sentence weighting for  and , and dampened tf - idf for support sets .",
    "since these are not necessarily the results that maximize performance for each summarizer and summary duration , we emphasize those that do with a @xmath15 . for our method",
    ", we show the results obtained using 0.10s and 0.50s framing .    [",
    "cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : kl ]    average similarity was the best performing baseline , clearly surpassing any of the naive segment selection heuristics in every summary duration shown here , with the exception of 5s summaries , where it is worse than the beginning and middle sections .",
    "the middle sections were always the best continuous section heuristic for these summary sizes .",
    "our method clearly outperforms every other approach .",
    "we also point to the fact that the mean - shift correction heuristic consistently improves summarization performance .",
    "support sets were the second best algorithm for summary sizes of 5s and 10s ,  was the second best algorithm for 15s summaries , and  was the second best algorithm for summary sizes of 20s , 25s , and 30s .",
    "every summarizer outperformed all baselines for summary sizes ranging from 10s to 30s .",
    "however , the middle sections outperform  for 5s summaries . even though this specific  parameter combination is outperformed by average similarity for summaries of 5s and 10s",
    ", we can easily find others that are not .",
    "in fact , the best  results for summaries of 5s and 10s are @xmath16 and @xmath17 , respectively .",
    "furthermore , none of the baselines ( up to 120s summary durations ) achieves similar performance to that of the , for instance , 20s gaussian sampler summaries , even though we are considering 6 times the duration .",
    "concerning baseline results , it is intuitive to think of the middle segments as being more representative of the whole song than the beginning and end segments , since the beginning and end sections of songs usually differ from the rest of it .",
    "the average similarity can also be thought of as a middle segment that is more informatively selected , which explains why it is also better than the beginning and end sections .",
    "however , this only holds for lower summary durations . for higher summary durations ( i.e. , 1 minute and more )",
    ", the beginning and end sections already contain much of the information that is repeated across the middle of the song , thus , becoming more representative of the whole song .",
    "meanwhile , the middle sections and average similarity just accumulate more redundant information that is present in the middle of the song .",
    "this is why , at this point , an increase in summary duration is more beneficial to the beginning and end sections .",
    "it is expected for a method that builds summaries by minimizing a certain measure to get better results according to that same measure .",
    "therefore , we also ran the 5-way genre classification  @xcite to confirm the benefits of this new summarizer and found that it outperforms every other summarizer , especially when considering 0.05s framing . we also ran the wilcoxon signed - rank test on the confusion matrices that resulted from the classification results of the gaussian summaries against the full songs confusion matrix . the p - values ( and accuracies ) were 0.3696 ( 88.96% ) , 0.3063 ( 88.88% ) , 0.2415 ( 88.72% ) , 0.0093 ( 88.56% ) , 0.3650 ( 88.80% ) , and 0.0539 ( 88.16% ) for the 30s , 25s , 20s , 15s , 10s , and 5s summaries , respectively .",
    "this means that , concerning 5-way genre classification , using these summaries is not significantly different from using full songs , statistically speaking , except for the 15s summaries .",
    "it is noteworthy that , with 5s summaries , the  was able to correctly predict the genre with 88.16% accuracy , when the full songs scenario had 89.44% accuracy ( note that this value is not the one reported in @xcite because we trimmed the silences ) .",
    "the fact that a summarizer aiming at minimizing the relative entropy between the original and summarized data achieves state - of - the - art performance , for machine - oriented summarization as evaluated by genre classification , strongly suggests that this information - theoretic way of measuring summary content is relevant for this type of tasks , i.e. , tasks that rely on a bag - of - features model .",
    "this is because bag - of - features only allows for statistical descriptions of data that are not timeline - constrained , i.e. , each audio frame is independent from every other .",
    "therefore , measuring the relative entropy is the natural way of measuring information loss .",
    "the average  divergence reveals that the summarizers lose less information than the standard baselines ( continuous segment heuristics ) and the human - oriented baseline average similarity .",
    "these results are in line with previous conclusions drawn on 5-way genre classification @xcite . in order to see how correlated these conclusions are , we ran a spearman test on all 360 different summarization setups tested in this research , comparing the relative entropy results with the classification accuracy results .",
    "the result is a @xmath18-value of -0.772 , indicating that lower values of relative entropy are correlated with higher values of classification accuracy , i.e , the relative entropy is a good predictor of classification performance .",
    "the relative entropy can measure the amount of information loss incurred by a summarizer . in turn , this means that a summarization setup that provides a good statistical description of the original data will likely be very useful for any bag - of - features - based discriminative task as well , since the performance of these tasks is based on frame - level feature statistics .",
    "this is demonstrated in two complementary ways : there is a correlation between classification performance and information loss measured as accuracy and relative entropy , respectively ; and our proposed summarization method , which minimizes information loss , achieves state - of - the - art performance in machine - oriented summarization , as measured by information loss and classification performance .",
    "the proposed gaussian summarizer has several advantages for machine - oriented summarization against the other generic media - agnostic summarizers .",
    "as opposed to previous state - of - the - art summarizers , that have several parameters to tune , this one only has to consider framing ( also shared by the others ) .",
    "this means that we no longer have the issue of finding a good vocabulary size for discretizing the frames , since no discretization is necessary . the same also happens for sentence size and , more importantly , sentence weighting , which is something that some generic summarizers are very sensitive to @xcite .",
    "moreover , the benefits of machine - oriented summarization range from faster processing to more efficient use of bandwidth but they also include alleviating copyright issues .",
    "the reason for this is that since the whole song is not present and the summary clip is not a continuous segment , it will not serve as enjoyment for potential listeners .",
    "informal listening revealed that summaries with small frame sizes can even be perceived as noise .",
    "this means that  datasets can be more easily distributed among the research community .",
    "the gaussian sampling summarizer is much better in making the clip not enjoyable .",
    "this is because it picks one frame at a time resulting in much more discontinuities , as opposed to summarizers that pick whole phrases of frames .",
    "this research showed the descriptive power of generic media - agnostic summarization methods in music , by assessing the information loss incurred by summaries from an information - theoretic perspective , by measuring their relative entropy from the full songs .",
    "moreover , we proposed a new method that minimizes information loss and achieves state - of - the - art performance . furthermore , we showed that the relative entropy is a good predictor of summary performance for bag - of - features - based tasks .",
    "music topic modeling and summarization can be jointly studied in order to better understand the semantics of each other . tweaking of these algorithms for human - oriented summarization is also worth exploring .",
    "m.  cooper and j.  foote , `` summarizing popular music via structural similarity analysis , '' in _ proc . of the ieee workshop on applications of signal processing to audio and acoustics _ , 2003 , pp .",
    "127130 .        j.  carbonell and j.  goldstein , `` the use of mmr , diversity - based reranking for reordering documents and producing summaries , '' in _ proc . of the 21st annual intl .",
    "acm sigir conf . on research and development in information retrieval _ , 1998 , pp .",
    "335336 .",
    "t.  k. landauer and s.  t. dutnais , `` a solution to plato s problem : the latent semantic analysis theory of acquisition , induction , and representation of knowledge , '' _ psychological review _ , vol .",
    "104 , no .  2 ,",
    "pp . 211240 , 1997 .",
    "x.  zhu , a.  b. goldberg , j.  v. gael , and d.  andrzejewski , `` improving diversity in ranking using absorbing random walks , '' in _ proc . of the 5th north american chapter of the association for computational linguistics - human language technologies conf .",
    "_ , 2007 , pp .",
    "97104 .          y.  gong and x.  liu , `` generic text summarization using relevance measure and latent semantic analysis , '' in _ proc . of the 24th annual intl .",
    "acm sigir conf . on research and development in information retrieval _ , 2001 , pp . 1925 .",
    "f.  eyben , f.  weninger , f.  gross , and b.  schuller , `` recent developments in opensmile , the munich open - source multimedia feature extractor , '' in _ proc . of the 21st acm intl .",
    "conf . on multimedia _ , 2013 , pp ."
  ],
  "abstract_text": [
    "<S> applying generic media - agnostic summarization to music allows for higher efficiency in automatic processing , storage , and communication of datasets while also alleviating copyright issues . </S>",
    "<S> this process has already been proven useful in the context of music genre classification . in this paper , we generalize conclusions from previous work by evaluating the impact of generic summarization in music from a probabilistic perspective and agnostic relative to certain tasks </S>",
    "<S> . we estimate gaussian distributions for original and summarized songs and compute their relative entropy to measure how much information is lost in the summarization process . based on this observation </S>",
    "<S> , we further propose a simple yet expressive summarization method that objectively outperforms previous methods and is better suited to avoid copyright issues . </S>",
    "<S> we present results suggesting that relative entropy is a good predictor of summarization performance in the context of tasks relying on a bag - of - features model . </S>"
  ]
}