{
  "article_text": [
    "in kernel machines such as support vector machines ( svm )  @xcite , objects are represented as a kernel matrix , where @xmath0 objects are represented as an @xmath1 positive semidefinite matrix .",
    "essentially the @xmath2 entry of the kernel matrix describes the similarity between @xmath3-th and @xmath4-th objects .",
    "due to positive semidefiniteness , the objects can be embedded as @xmath0 points in an euclidean feature space such that the inner product between two points equals to the corresponding entry of kernel matrix .",
    "this property enables us to apply diverse learning methods ( for example , svm or kernel pca ) without explicitly constructing a feature space  @xcite .",
    "biological data such as amino acid sequences , gene expression arrays and phylogenetic profiles are derived from expensive experiments  @xcite . typically initial experimental measurements are so noisy that they can not be given to learning machines directly .",
    "since high quality data are created by extensive work of human experts , it is often the case that good data are available only for a subset of samples .",
    "when a kernel matrix is derived from such incomplete data , we have to leave the entries for unavailable samples as _ missing_. we call such a matrix an `` incomplete matrix '' . our aim is to estimate the missing entries , but it is obviously impossible without additional information .",
    "so we make use of a _ parametric model _ of admissible matrices , and estimate missing entries by fitting the model to existing entries .    in this scheme , it is important to define a parametric model appropriately .",
    "for example , @xcite used the set of all positive definite matrices as a model .",
    "although this model worked well when only a few entries are missing , this model is too general for our cases where whole columns and rows are missing .",
    "thus we need another information source for constructing a parametric model .",
    "fortunately , in biological data , it is common that one object is described by two or more representations .",
    "for example , genes are represented by gene networks and gene expression arrays at the same time  @xcite . also a bacterium is represented by several marker sequences  @xcite . in this paper , we assume that a complete matrix is available from another information source , and a parametric model is created by giving perturbations to the matrix .",
    "we call the complete matrix a `` base matrix '' .",
    "when creating a parametic model of admissible matrices from a base matrix , one typical way is to define the parametric model as all _ spectral variants _ of the base matrix , which have the same eigenvectors but different eigenvalues  @xcite .",
    "when several base matrices are available , the weighted sum of these matrices would be a good parametric model as well  @xcite .    in order to fit a parametric model , the distance between two matrices has to be determined .",
    "a common way is to define the euclidean distance between matrices ( for example , the frobeneous norm ) and make use of the euclidean geometry .",
    "recently @xcite tackled with the incomplete matrix approximation problem by means of kernel cca .",
    "also @xcite proposed a similarity measure called `` alignment '' , which is basically the cosine between two matrices .",
    "in contrast that their methods are based on the euclidean geometry , this paper will follow an alternative way : we will define the kullback - leibler ( kl ) divergence between two kernel matrices and make use of the riemannian information geometry  @xcite .",
    "the kl divergence is derived by relating a kernel matrix to a covariance matrix of gaussian distribution .",
    "the primal advantage is that the kl divergence allows us to use the @xmath5 algorithm  @xcite to approximate an incomplete kernel matrix .",
    "the @xmath6 and @xmath7 steps are formulated as convex programming problems , and moreover they can be solved analytically when spectral variants are used as a parametric model .",
    "we performed bacteria clustering experiments using two marker sequences : 16s and gyrb  @xcite .",
    "we derived the incomplete and base kernel matrices from gyrb and 16s , respectively . as a result , even when 50% of columns / rows are missing",
    ", the clustering performance of the completed matrix was better than that of the base matrix , which illustrates the effectiveness of our approach in real world problems .",
    "this paper is organized as follows : sec .  [",
    "sec : ig ] introduces the information geometry to the space of positive definite matrices . based on geometric concepts , the _ em _ algorithm for matrix approximation is presented in sec .",
    "[ sec : em ] , where detailed computations are deferred in sec .",
    "[ sec : compproj ] . in sec .",
    "[ sec : em ] , the matrix approximation problem is formulated as statistical inference and the equivalence between the _ em _ and em algorithms  @xcite is shown .",
    "then the bacteria clustering experiment is described in sec .",
    "[ sec : experiment ] . after seeking for possible extensions in sec .",
    "[ sec : ext ] , we conclude the paper in sec .  [",
    "sec : con ] .",
    "we first explain how to introduce the information geometry in the space of positive definite matrices",
    ". only necessary parts of the theory will be presented here , so refer to @xcite for details .",
    "let us define the set of all @xmath8 positive definite matrices as @xmath9 .",
    "the first step is to relate a @xmath8 positive definite matrix @xmath10 to the gaussian distribution with mean 0 and covariance matrix @xmath11 : @xmath12 it is well known that the gaussian distribution belongs to the exponential family .",
    "the canonical form of an exponential family distribution is written as @xmath13 where @xmath14 is the vector of sufficient statistics , @xmath15 is the natural parameter and @xmath16 is the normalization factor .",
    "when ( [ eq : gaussian ] ) is rewritten in the canonical form , we have the sufficient statistics as @xmath17 and the natural parameter as @xmath18_{11 } , \\ldots , [ p^{-1}]_{dd } ,   [ p^{-1}]_{12 } , \\ldots , [ p^{-1}]_{d-1,d } \\right)^\\top,\\ ] ] where @xmath19_{ij}$ ] denotes the @xmath2 entry of matrix @xmath20 .",
    "the natural parameter @xmath15 provides a coordinate system to specify a positive definite matrix @xmath11 , which is called the @xmath15-coordinate system ( or the @xmath6-coordinate system ) . on the other hand",
    ", there is an alternative representation for the exponential family .",
    "let us define the mean of @xmath21 as @xmath22 : for example , when @xmath23 , @xmath24 this new set of parameters @xmath22 provides another coorninate system , called @xmath25-coordinate system ( or the @xmath7-coordinate system ) : @xmath26 let us consider the following curve @xmath27 connecting two points @xmath28 and @xmath29 linearly in @xmath15 coordinates : @xmath30 when written is the matrix form , this reads @xmath31 this curve is regarded as a straight line from the exponential viewpoint and is called an exponential geodesic or @xmath6-geodesic . in particular , each coordinate curve @xmath32 , @xmath33 is an @xmath6-geodesic . when the @xmath6-geodesic between any two points in a manifold @xmath34 is included in @xmath35 , the manifold @xmath35 is said to be @xmath6-flat . on the other hand ,",
    "the mixture geodesic or @xmath7-geodesic is defined as @xmath36 in the matrix form , this reads @xmath37 when the @xmath7-geodesic between any two points in @xmath35 is included in @xmath35 , the manifold @xmath35 is said to be @xmath7-flat . in information geometry , the distance between probability distributions is defined as the kullback - leibler divergence  @xcite : @xmath38 by relating a positive definite matrix to the covariance matrix of gaussian ( [ eq : gaussian ] ) , we have the kullback - leibler ( kl ) divergence for two matrices @xmath39 : @xmath40 with respect to a manifold @xmath34 and a point @xmath10 , the projection from @xmath11 to @xmath35 is defined as the point in @xmath35 closest to @xmath11 .",
    "since the kl divergence is asymmetric , there are two kinds of projection :    * @xmath6-projection : @xmath41 . *",
    "@xmath7-projection : @xmath42 .",
    "it is proved that the @xmath7-projection to an @xmath6-flat submanifold is unique , and @xmath6-projection to an @xmath7-flat manifold is unique  @xcite .",
    "this uniqueness property means that the corresponding optimization problem is convex and so the global optimal solution is easily obtained by any reasonable method .",
    "in this section , we describe the _ em _ algorithm to approximate an incomplete kernel matrix .",
    "let @xmath43 be the set of samples in interest .",
    "in supervised learning cases , this set includes both training and test sets , thus we are considering the transductive setting  @xcite .",
    "let us assume that the data is available for the first @xmath0 samples , and unavailable for the remaining @xmath44 samples . denote by @xmath45 an @xmath1 kernel matrix , which is derived from the data for the first @xmath0 samples .",
    "then , an incomplete kernel matrix is described as @xmath46 where @xmath47 is an @xmath48 matrix and @xmath49 is an @xmath50 symmetric matrix .",
    "since @xmath51 has missing entries , it can not be presented as a point in @xmath9 . instead , all the possible kernel matrices form a manifold @xmath52 where @xmath53 means that @xmath51 is positive definite .",
    "we call it the _ data manifold _ as in the conventional em algorithm  @xcite .",
    "it is easy to verify that @xmath54 is an @xmath7-flat manifold ; hence , the @xmath6-projection to @xmath54 is unique .",
    "next let us define the parametric model to approximate @xmath51 . here",
    "the model is derived as the spectral variants of @xmath55 , which is an @xmath56 base kernel matrix derived from another information source .",
    "let us decompose @xmath55 as @xmath57 where @xmath58 and @xmath59 is the @xmath3-th eigenvalue and eigenvector , respectively .",
    "define @xmath60 then all the spectral variants are represented as @xmath61 we call it the model manifold  @xcite . for notational simplicity ,",
    "we choose a different parametrization of @xmath62 : @xmath63 where @xmath64 .",
    "it is easily seen that the manifold @xmath62 is @xmath6-flat and @xmath7-flat at the same time .",
    "such a manifold is called dually - flat .",
    "our approximation problem is formulated as finding the nearest points in two manifolds : find @xmath65 and @xmath66 to minimize @xmath67 . in geometric terms , this problem is to find the nearest points between @xmath6-flat and @xmath7-flat manifolds .",
    "it is well known that such a problem is solved by an alternating procedure called the _",
    "_ algorithm  @xcite .",
    "algorithm gradually minimizes the kl divergence by repeating @xmath6-step and @xmath7-step alternately ( fig .",
    "[ fig : em ] ) .     algorithm .",
    "the data manifold @xmath68 corresponds to the set of all completed matrices , whereas the model manifold @xmath62 corresponds to the set of all spectral variants of a base matrix .",
    "the nearest points are found by gradually minimizing the kl divergence by repeating @xmath6 and @xmath7 projections.,scaledwidth=50.0% ]    in the @xmath6-step , the following optimization problem is solved with fixing @xmath20 : find @xmath65 that minimizes @xmath67 .",
    "this is rewritten as follows : find @xmath47 and @xmath49 that minimize @xmath69 subject to the constraint that @xmath53 .",
    "notice that this constraint is not needed , because @xmath70 where @xmath71 is the @xmath3-th eigenvalue of @xmath51 .",
    "here @xmath72 is undefined when one of eigenvalues is negative , and @xmath72 decreases to @xmath73 as an eigenvalue get closer to 0 .",
    "so , at the optimal solution , @xmath51 is necessarily positive definite , because the kl divergence is infinite otherwise . as indicated by information geometry , this is a convex problem , which can readily be solved by any reasonable optimizer .",
    "moreover the solution is obtained in a closed form : let us partition @xmath74 as @xmath75 the solution of ( [ eq : eopt ] ) is described as @xmath76 the derivation of ( [ eq : dvhlast ] ) and ( [ eq : dhhlast ] ) will be described in sec .",
    "[ sec : eproj ] .    in the @xmath7-step ,",
    "the following optimization problem is solved with fixing @xmath51 : find @xmath66 that minimizes @xmath67 .",
    "this is rewritten as follows : find @xmath77 that minimizes @xmath78 subject to the constraint that @xmath79 .",
    "notice that this constraint can be ignored as well .",
    "when @xmath80 are defined as ( [ eq : mi ] ) , the closed form solution of ( [ eq : mopt ] ) is obtained as @xmath81 the derivation of ( [ eq : msol ] ) will be described in sec .",
    "[ sec : mproj ] .",
    "this section presents the derivation of @xmath6 and @xmath7-projections in detail .",
    "first we will show the derivation of @xmath6-projection ( [ eq : dvhlast ] ) and ( [ eq : dhhlast ] ) .",
    "the log determinant of a partitioned matrix is rewritten as @xmath82 when we partition @xmath74 as ( [ eq : mdivide ] ) , it turns out that @xmath83 the saddle point equation with respect to @xmath49 is obtained as @xmath84 because @xmath85 for any symmetric matrix @xmath86 . solving ( [ eq : dhh ] ) with respect to @xmath49",
    ", we have @xmath87 substituting ( [ eq : dhhmid ] ) into ( [ eq : le ] ) , we have @xmath88 now the saddle point equation with respect to @xmath47 is obtained as @xmath89 solving this equation , we have the solution ( [ eq : dvhlast ] ) for @xmath47 . by substituting ( [ eq : dvhlast ] ) into ( [ eq : dhhmid ] ) , we have the solution ( [ eq : dhhlast ] ) for @xmath49 .",
    "next , we will show the derivation of @xmath7-projection ( [ eq : msol ] ) . the @xmath7 projection is obtained as the solution @xmath90 to minimize @xmath91 since @xmath92 ,",
    "the saddle point equations are described as @xmath93 remembering that @xmath94 , we have @xmath95 since the left hand side of ( [ eq : mstep ] ) is @xmath96 the solution of ( [ eq : mstep ] ) is analytically obtained as @xmath97 we have shown that the @xmath7-projection is obtained analytically when the model manifold corresponds to spectral variants of a matrix .",
    "however , it is not always the case .",
    "for example , consider we have @xmath98 base matrices @xmath99 and the model manifold is constructed as harmonic mixture of them : @xmath100 this is an @xmath6-flat manifold so the optimization problem is convex , but the analytical solvability depends on geometric properties of base matrices @xmath101@xcite .",
    "we will briefly discuss this issue in the appendix .",
    "in statistical inference with missing data , the em algorithm  @xcite is commonly used . by posing the matrix approximation problem as statistical inference",
    ", the em algorithm can be applied , and  as shown later  it eventually leads to the same procedure . in a sense , it is misleading to relate matrix approximation to statistical concepts , such as random variables , observations and so on .",
    "nevertheless it would be meaningful to rewrite our method in terms of statistical concepts for establishing connections to other literature .",
    "let @xmath102 and @xmath103 be the @xmath0 and @xmath7 dimensional visible and hidden variables . from observed data and @xmath104 .",
    "] , the covariance matrix of @xmath102 is known as @xmath105 = k_i,\\ ] ] where @xmath106 denotes the expectation with respect to observed data .",
    "however , we do not know the covariances @xmath107 $ ] and @xmath108 $ ] . our purpose is to obtain the maximum likelihood estimate of parameter @xmath90 of the following gaussian model : @xmath109^\\top m^{-1 } \\left [ \\begin{array}{c } { { \\boldsymbol{v}}}\\\\ { { \\boldsymbol{h}}}\\end{array } \\right ] \\right),\\ ] ] where @xmath20 is described as ( [ eq : mdef ] ) . in the course of maximum likelihood estimation",
    ", we have to estimate the observed covariances @xmath47 and @xmath49 in an appropriate way .",
    "the em algorithm consists of the following two steps .    *",
    "e - step : fix @xmath90 and update @xmath47 and @xmath49 by conditional expectation . * m - step : fix @xmath51 and update @xmath90 by maximum likelihood estimation .",
    "it is shown that the likelihood of observed data increases monotonically by repeating these two steps  @xcite .",
    "the m - step maximizes the likelihood , which is easily seen to be equivalent to minimizing the kl divergence  @xcite .",
    "so the @xmath20-step is equivalent to the @xmath7-step ( [ eq : mopt ] ) .",
    "however , the equivalence between e - step and @xmath6-step is not obvious , because the former is based on conditional expectation and the latter minimizes the kl divergence . in the e - step ,",
    "the covariance matrices are computed from the conditional distribution described as @xmath110 where @xmath111 matrices are derived as ( [ eq : mdivide ] ) . taking expectation with this distribution , we have @xmath112 & = & -{{\\boldsymbol{v}}}{{\\boldsymbol{v}}}^\\top s_{vh } s_{hh}^{-1 } , \\\\ e_{{\\boldsymbol{b}}}[{{\\boldsymbol{h}}}{{\\boldsymbol{h}}}^\\top\\mid{{\\boldsymbol{v } } } ] & = & s_{hh}^{-1 } + s_{hh}^{-1}s_{vh}^\\top{{\\boldsymbol{v}}}{{\\boldsymbol{v}}}^\\top s_{vh } s_{hh}^{-1}.\\end{aligned}\\ ] ] then the covariance matrices are estimated as @xmath113 & = & - k_i s_{vh } s_{hh}^{-1 } , \\\\",
    "d_{hh } = e_o e_{{\\boldsymbol{b}}}[{{\\boldsymbol{h}}}{{\\boldsymbol{h}}}^\\top\\mid{{\\boldsymbol{v } } } ] & = & s_{hh}^{-1 } + s_{hh}^{-1}s_{vh}^\\top k_i s_{vh } s_{hh}^{-1}.\\end{aligned}\\ ] ] since these solutions are equivalent to ( [ eq : dvhlast ] ) and ( [ eq : dhhlast ] ) , respectively , the e - step is shown to be equivalent to the @xmath6-step in this case .",
    "refer to @xcite for general discussion of the equivalence between em and @xmath5 algorithms .",
    "in this section , we perform unsupervised classification experiments for bacteria based on two marker sequences : 16s and gyrb .",
    "basically we would like to identify the genus of a bacterium by means of extracted entities from the cell .",
    "it is known that several specific proteins and rnas can be used for genus identification  @xcite . among them , we especially focus on 16s rrna and gyrase subunit b ( gyrb ) protein .",
    "16s rrna is an essential constituent in all living organisms , and the existence of many conserved regions in the rrna genes allows the alignment of their sequences derived from distantly related organisms , while their variable regions are useful for the distinction of closely related organisms .",
    "gyrb is a type ii dna topoisomerase which is an enzyme that controls and modifies the topological states of dna supercoils .",
    "this protein is known to be well preserved over evolutional history among bacterial organisms thus is supposed to be a better identifier than the traditional 16s rrna @xcite .",
    "notice that 16s is represented as a nucleotide sequence with 4 symbols , and gyrb is an amino acid sequence with 20 symbols . since gyrb has been found to be useful more recently than 16s  @xcite , gyrb sequences are available only for a limited number of bacteria .",
    "thus , it is considered that gyrb is more `` expensive '' than 16s .",
    "our dataset has 52 bacteria of three genera ( _ corynebacterium _ : 10 , _ mycobacterium _ : 31 , _ rhodococcus _ : 11 ) , each of which has both 16s and gyrb sequences . for simplicity ,",
    "let us call these genera as class 1 - 3 , respectively . for 16s and gyrb , we computed the second order count kernel , which is the dot product of bimer counts  @xcite .",
    "each kernel matrix is normalized such that the norm of each sample in the feature space becomes one .",
    "the kernel matrices of gyrb and 16s can be seen in fig .",
    "[ fig : kermat ] ( b ) and ( c ) , respectively . for reference",
    ", we show an ideal matrix as fig .",
    "[ fig : kermat](a ) , which indicates the true classes . in our senario , for a considerble number of bacteria , gyrb sequences are not available as in fig .",
    "[ fig : kermat](d )",
    ". we will complete the missing entries by the _",
    "algorithm with the spectral variants of 16s matrix .",
    "when the _ em _ algorithm converges , we end up with two matrices : the _ completed matrix _ on data manifold @xmath68 ( fig .",
    "[ fig : kermat](e ) ) and the _ estimated matrix _ on model manifold @xmath62 ( fig .",
    "[ fig : kermat](f ) ) .",
    "these two matrices are in general not the same , because the two manifolds may not have intersection .    in order to evaluate the quality of completed and estimated matrices ,",
    "k - means clustering is performed in the feature space of each kernel . in evaluating the partition",
    ", we use the adjusted rand index ( ari )  @xcite .",
    "let @xmath114 be the obtained clusters and @xmath115 be the ground truth clusters .",
    "let @xmath116 be the number of samples which belongs to both @xmath117 and @xmath118 .",
    "also let @xmath119 and @xmath120 be the number of samples in @xmath117 and @xmath118 , respectively .",
    "ari is defined as @xmath121 / { \\begin{pmatrix}}n \\\\ 2 { \\end{pmatrix } } } { \\frac{1}{2}\\left [ \\sum_i { \\begin{pmatrix}}n_{i . }",
    "\\\\ 2 { \\end{pmatrix}}+ \\sum_j { \\begin{pmatrix}}n_{.j } \\\\ 2 { \\end{pmatrix}}\\right ] - \\left [ \\sum_i { \\begin{pmatrix}}n_{i . }",
    "\\\\ 2 { \\end{pmatrix}}\\sum_j { \\begin{pmatrix}}n_{.j } \\\\ 2 { \\end{pmatrix}}\\right ] / { \\begin{pmatrix}}n \\\\ 2 { \\end{pmatrix}}}.\\ ] ] the attractive point of ari is that it can measure the difference of two partitions even when the number of clusters is different . when the two partitions are exactly the same , ari is 1 , and the expected value of ari over random partitions is 0 ( see @xcite for details ) .",
    "the clustering experiment is performed by randomly removing samples from gyrb data .",
    "the ratio of missing samples is changed from 0% to 90% .",
    "the aris of completed and estimated matrices averaged over 20 trials are shown in fig .",
    "[ fig : ari_completed ] and [ fig : ari_estimated ] , respectively . comparing the two matrices ,",
    "the estimated matrix performed significantly worse than the complete matrix .",
    "it is because the completed matrix maintains existing entries unchanged , and so the class information in gyrb matrix is well preserved .",
    "we especially focus on the comparison between the completed matrix and 16s matrix , because there is no point in performing the _ em _ algorithm when 16s matrix works better than the completed matrix . according to the plot , the ari of completed matrix was larger than 16s matrix up to 50% missing ratio .",
    "it implies that the matrix completion is meaningful even in quite hard situations  50% sample loss implies 75% loss in entries .",
    "this result encourages us ( and hopefully readers ) to apply the _ em _ algorithm to other data such as gene networks  @xcite .    [ cols=\"^,^ \" , ]",
    "as we related the _ em _ algorithm to maximum likelihood inference in sec .",
    "[ sec : em ] , it is straightforward to generalize it to the maximum _ a posteriori _ ( map ) inference or more generally the bayes inference  @xcite .",
    "for example , we are going to modify the _",
    "algorithm to obtain the map estimate .",
    "the map estimation amounts to minimizing the kl divergence penalized by a prior , @xmath122 where @xmath123 is a prior distribution for @xmath20 . since the additional term @xmath124 depends only on the model @xmath20 , only the @xmath7-step",
    "is changed so as to minimize the above objective function with respect to @xmath20 .",
    "let us give a simple example of map estimation in the spectral variants case .",
    "in bayesian inference , it is common to take a _",
    "conjugate prior _ , so that the posterior distribution remains as a member of the exponential family . since the model parameter @xmath90 is related to a covariance matrix , we choose the gamma distribution , which works as a conjugate prior for the variance of gaussian distribution  @xcite .",
    "the prior distribution is defined independently for each @xmath125 as @xmath126 where @xmath127 and @xmath128 denote hyperparameters , by which the mean and the variance are specified by @xmath129 and @xmath130 .",
    "the @xmath7 step for map estimation is to minimize @xmath131 which leads to the equation @xmath132 in the spectral variants case , the left hand side is reduced to @xmath133 , thus we obtain the map solution in a closed form as @xmath134",
    "in this paper , we introduced the information geometry in the space of kernel matrices , and applied the _ em _ algorithm in matrix approximation . the main difference to other euclidean methods is that we use the kl divergence .",
    "in general , we can not determine which distance is better , because it is highly data dependent .",
    "however our method has a great utility , because it can be implemented only with algebraic computation and we do not need any specialized optimizer such as semidefinite programmming unlike @xcite .",
    "one of our contribution is that we related matrix approximation to statistical inference in sec .",
    "[ sec : em ] .",
    "thus , in future works , it would be interesting to involve advanced methods in statistical inference , such as generalized em  @xcite and variational bayes  @xcite .",
    "also we are looking forward to apply our method to diverse kinds of real data which are not limited to bioinformatics .",
    "in this appendix , we discuss the solvability of the @xmath7-step . the left hand side of ( [ eq : mstep ] ) is the @xmath7-coordinate of the submanifold @xmath135 , while @xmath125 denote the @xmath6-coordinate of @xmath135 .",
    "the @xmath6-coordinate and @xmath7-coordinate are connected by the legendre transform  @xcite . in the mother manifold @xmath9 , the legendre transform is easily obtained as the inverse of the matrix . in the submanifold @xmath135 of @xmath136 , however , it is difficult to obtain the legendre transform in general .",
    "the difficulty is caused by the difference of geodesics defined in @xmath135 and @xmath136 .",
    "when the geodesic defined by a coordinate system of a submanifold @xmath34 coincides the geodesic defined by the corresponding global coordinate system of @xmath9 , the submanifold is called _ autoparallel_. in our case , @xmath135 is autoparallel for the @xmath6-coordinate , but it is not always autoparallel for the @xmath7-coordinate . when the submanifold is autoparallel for the both coordinate systems , the submanifold is called doubly autoparallel .    let us consider when a submanifold becomes doubly autoparallel . to begin with , let us define the product @xmath137 between two @xmath8 symmetric matrices @xmath138 , @xmath139 the algebra equipped with the usual matrix sum and the product ( [ eq : prod ] ) is called the jordan algebra of the vector space of _ sym_@xmath140 .",
    "the following theorem provides the necessary and sufficient condition for doubly autoparallel submanifold .",
    "assume the identity matrix @xmath141 is an element of the submanifold @xmath135 , then @xmath135 is doubly autoparallel if and only if the tangent space of @xmath135 is a jordan subalgebra of _ sym_@xmath140 .",
    "when a submanifold @xmath142 is determined as ( [ eq : generalm ] ) , @xmath62 is doubly autoparallel if the following holds for all @xmath143 : @xmath144 @xcite has shown that , if and only if @xmath62 is doubly autoparallel , the @xmath7-projection can be solved analytically , that is , the optimal solution is obtained by one newton step .",
    "for example , in the spectral variants case , @xmath145 and @xmath146 thus the @xmath7-projection is obtained analytically in this case .",
    "the authors gratefully acknowledge that the bacterial _ gyrb _ amino acid sequences are offered by courtesy of identification and classification of bacteria ( icb ) database team of marine biotechnology institute , kamaishi , japan .",
    "the authors would like to thank t.  kin , y.  nishimori , t.  tsuchiya and j .- p .",
    "vert for fruitful discussions .",
    "h.  attias .",
    "inferring parameters and structure of latent variable models by variational bayes . in _ uncertainty in artificial intelligence : proceedings of the fifteenth conference ( uai-1999 ) _ , pages 2130 , san francisco , ca , 1999 .",
    "morgan kaufmann publishers .",
    "n.  cristianini , j.  shawe - taylor , j.  kandola , and a.  elisseeff . on kernel - target alignment . in t.g .",
    "dietterich , s.  becker , and z.  ghahramani , editors , _ advances in neural information processing systems 14_. mit press , 2002 .",
    "s.  ikeda , s.  amari , and h.  nakahara .",
    "convergence of the wake - sleep algorithm . in m.s .",
    "kearns , s.a .",
    "solla , and d.a .",
    "cohn , editors , _ advances in neural information processing systems 11 _ , pages 239245 . mit press , 1999 .",
    "h.  kasai , a.  bairoch , k.  watanabe , k.  isono , s.  harayama , e.  gasteiger , and s.  yamamoto .",
    "construction of the gyrb database for the identification and classification of bacteria . in _",
    "genome informatics 1998 _ , pages 1321 .",
    "universal academic press , 1998 .",
    "a.  ohara .",
    "information geometric analysis of an interior point method for semidefinite programming . in o.e .",
    "barndorff - nielsen and e.b .",
    "vedel jensen , editors , _ geometry in present day science _ , pages 4974 .",
    "world scientific , 1999 .",
    "s.  yamamoto , h.  kasai , d.l .",
    "arnold , r.w .",
    "jackson , a.  vivian , and s.  harayama .",
    "phylogeny of the genus pseudomonas : intrageneric structure reconstructed from the nucleotide sequences of gyrb and rpod genes . , 146:0 23852394 , 2000 ."
  ],
  "abstract_text": [
    "<S> in biological data , it is often the case that observed data are available only for a subset of samples . when a kernel matrix is derived from such data , we have to leave the entries for unavailable samples as missing . in this paper </S>",
    "<S> , we make use of a parametric model of kernel matrices , and estimate missing entries by fitting the model to existing entries . </S>",
    "<S> the parametric model is created as a set of spectral variants of a complete kernel matrix derived from another information source . for model </S>",
    "<S> fitting , we adopt the _ em _ algorithm based on the information geometry of positive definite matrices . </S>",
    "<S> we will report promising results on bacteria clustering experiments using two marker sequences : 16s and gyrb . </S>"
  ]
}