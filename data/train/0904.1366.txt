{
  "article_text": [
    "recent years have seen a dramatic increase in the number of applications domains that naturally generate uncertain data and that demand support for executing complex decision support queries over them .",
    "these include information retrieval  @xcite , data integration and cleaning  @xcite , text analytics  @xcite , social network analysis  @xcite , sensor data management  @xcite , financial applications , biological and scientific data management , etc .",
    "uncertainty arises in these environments for a variety of reasons .",
    "sensor data typically contains noise and measurement errors , and is often incomplete because of sensor faults or communication link failures . in social networks and scientific domains ,",
    "the observed interaction or experimental data is often very noisy , and ubiquitous use of predictive models adds a further layer of uncertainty .",
    "use of automated tools in data integration and information extraction can introduce significant uncertainty in the output .    by their very nature , many of these applications require support for _ ranking _ or _ top - k query processing _ over large volumes of data .",
    "for instance , consider a _ house search _ application where a user is searching for a house using a real estate sales dataset that lists the houses for sale .",
    "such a dataset , which may be constructed by crawling and combining data from multiple sources , is inherently uncertain and noisy .",
    "in fact , the houses that the user prefers the most , are also the most likely to be sold by now .",
    "we may denote such uncertainty by associating with each advertisement a _ probability _ that it is still valid .",
    "incorporating such uncertainties into the returned answers is , however , a challenge considering the complex interplay between the relevance of a house by itself , and the probability that the advertisement is still valid .",
    "many other application domains also exhibit resource constraints of some form , and we must somehow rank the entities or tuples under consideration to select the most relevant objects to focus our attention on . for example , in financial applications , we may want to choose the best stocks in which to invest , given their expected performance in the future ( which is uncertain at best ) . in learning or classification tasks ,",
    "we often need to choose the best `` k '' features to use  @xcite . in sensor networks or scientific databases",
    ", we may not know the `` true '' values of the physical properties being measured because of measurement noises or failures  @xcite , but we may still need to choose a set of sensors or entities in response to a user query .    ranking in presence of uncertainty is non - trivial even if the relevance scores can be computed easily ( the main challenge in the deterministic case ) , mainly because of the complex trade - offs introduced by the score distributions and the tuple uncertainties . this has led to many ranking functions being proposed for combining the scores and the probabilities in recent years , all of which appear quite natural at the first glance ( we review several of them in detail later ) .",
    "we begin with a systematic exploration of these issues by recognizing that ranking in probabilistic databases is inherently a multi - criteria optimization problem , and by deriving a set of _ features _ , the key properties of a probabilistic dataset that influence the ranked result .",
    "we empirically illustrate the diverse and conflicting behavior of several natural ranking functions , and argue that a single specific ranking function may not be appropriate to rank different uncertain databases that we may encounter in practice .",
    "furthermore , different users may weigh the features differently , resulting in different rankings over the same dataset .",
    "we then define a general and powerful ranking function , called _ @xmath3 _ , that allows us to explore the space of possible ranking functions .",
    "we discuss its relationship to previously proposed ranking functions , and also identify two specific parameterized ranking functions , called @xmath1  and @xmath2 , as being interesting .",
    "the @xmath1  ranking function is essentially a linear , weighted ranking function that resembles the scoring functions typically used in information retrieval , web search , data integration , keyword query answering etc .",
    "we observe that @xmath1  may not be suitable for ranking large datasets due to its high running time , and instead propose @xmath2 , which uses a single parameter , and can effectively approximate previously proposed ranking functions for probabilistic databases very well .",
    "we then develop novel algorithms based on _ generating functions _ to efficiently rank the tuples in a probabilistic dataset using any @xmath3  ranking function .",
    "our algorithm can handle a probabilistic dataset with arbitrary correlations ; however , it is particularly efficient when the probabilistic database contains only _ mutual exclusivity _ and/or _",
    "co - existence _ correlations ( called _ probabilistic and / xor trees _",
    "our main contributions can be summarized as follows :    @xmath40.10 in    we develop a framework for _ learning _ ranking functions over probabilistic databases by identifying a set of key _ features _ , by proposing several parameterized ranking functions over those features , and by choosing the parameters based on user preferences or feedback .",
    "we present novel algorithms based on _ generating functions _ that enable us to efficiently rank very large datasets .",
    "our key algorithm is an @xmath5 algorithm for evaluating a @xmath2 function over datasets with low correlations ( specifically , constant height probabilistic and / xor trees ) .",
    "the algorithm runs in @xmath6 time if the dataset is pre - sorted by score .",
    "we present a polynomial time algorithm for ranking a correlated dataset when the correlations are captured using a bounded - treewidth graphical model .",
    "the algorithm we present is actually for computing the probability that a given tuple is ranked at a given position across all the possible worlds , and is of independent interest .",
    "we develop a novel , dft - based algorithm for approximating an arbitrary weighted ranking function using a linear combination of @xmath2  functions .",
    "we show that a @xmath1  ranked result can be seen as a _",
    "answer under a suitably defined distance function ",
    "a consensus answer is defined to be the answer that is closest in expectation to the answers over the possible worlds .",
    "we present a comprehensive experimental study over several real and synthetic datasets , comparing the behavior of the ranking functions and the effectiveness of our proposed algorithms .",
    "we begin with a brief discussion of the related work ( section [ sec : related work ] ) . in section [ sec : problem formulation ] , we review our probabilistic database model and the prior work on ranking in probabilistic databases , and propose two parameterized ranking functions . in section [ sec : algorithms ] , we present our generating functions - based algorithms for ranking . we then present an approach to approximate different ranking functions using our parameterized ranking functions , and to learn a ranking function from user preferences ( section [ sec : approximating and learning ] ) . in section",
    "[ sec : contop - k ] , we explore the connection between @xmath1  and consensus top - k query results . in section [ sec : prfeprop ] , we observe an interesting property of the @xmath2  function that helps us gain better insight into its behavior . we then present a comprehensive experiment study in section [ sec : experiments ] . finally , in section [ sec : correlations ] , we develop an algorithm for handling correlated datasets where the correlations are captured using bounded - treewidth graphical models .",
    "there has been much work on managing probabilistic , uncertain , incomplete , and/or fuzzy data in database systems ( see , e.g. , @xcite ) .",
    "the work in this area has spanned a range of issues from theoretical development of data models and data languages to practical implementation issues such as indexing techniques ; several research efforts are underway to build systems to manage uncertain data ( e.g. , mystiq  @xcite , trio  @xcite , orion  @xcite , maybms  @xcite , prdb  @xcite ) . the approaches can be differentiated based on whether they support _ tuple - level uncertainty _ where `` existence '' probabilities are attached to the tuples of the database , or _",
    "attribute - level uncertainty _ where ( possibly continuous ) probability distributions are attached to the attributes , or both .",
    "the proposed approaches differ further based on whether they consider correlations or not .",
    "most work in probabilistic databases has either assumed independence  @xcite or has restricted the correlations that can be modeled  @xcite .",
    "more recently , several approaches have been presented that allow representation of arbitrary correlations and querying over correlated databases  @xcite .",
    "the area of ranking and top - k query processing has also seen much work in databases ( see , e.g. , ilyas et al.s survey  @xcite ) .",
    "more recently , several researchers have considered top - k query processing in probabilistic databases .",
    "soliman et al .",
    "@xcite defined the problem of ranking over probabilistic databases , and proposed two ranking functions to combine tuple scores and probabilities .",
    "yi et al .",
    "@xcite present improved algorithms for the same ranking functions .",
    "zhang and chomicki  @xcite present a desiderata for ranking functions , and propose the notion of _ global top - k _ answers .",
    "ming hua et al .",
    "@xcite propose _ probabilistic threshold ranking _ , which is quite similar to global top - k .",
    "cormode et al .",
    "@xcite also present a semantics of ranking functions and a new ranking function called _ expected rank_. liu et al .",
    "@xcite propose the notion of _ k_-selection queries ; unlike most of the above definitions , the result here is sensitive to the actual tuple scores .",
    "we will review these ranking functions in detail in next section .",
    "ge et al .",
    "@xcite propose the notion of _ typical answers _ , where they propose returning a collection of typical answers instead of just one answer .",
    "this can be seen as complementary to our approach here ; one could show the typical answers to the user to understand the user preferences during an exploratory phase , and then learn a single ranking function to rank using the techniques developed in this article .",
    "there has also been work on top - k query processing in probabilistic databases where the ranking is by the result tuple _ probabilities _",
    "( i.e. , probability and score are identical )  @xcite .",
    "the main challenge in that work is efficient computation of the probabilities , whereas we assume that the probability and score are either given or can be computed easily .",
    "the aforementioned work has focused mainly on tuple uncertainty and discrete attribute uncertainty .",
    "soliman and ilyas  @xcite were the first to consider the problem of handling continuous distributions .",
    "recently , in a followup work  @xcite , we extended the algorithm for @xmath3  to arbitrary continuous distributions .",
    "we were able to obtain exact polynomial time algorithms for some continuous probability distribution classes , and efficient approximation schemes with provable guarantees for arbitrary probability distributions .",
    "one important ingredient of those algorithms is an extension of the generating function used in this article .",
    "recently , there has also been much work on nearest neighbor - style queries over uncertain datasets  @xcite .",
    "in fact , a nearest neighbor query ( or a @xmath0-nearest neighbor query ) can be seen as a ranking query where the score of a point is the distance of that point to the given query point .",
    "thus , our new ranking semantics and algorithms can be directly used for nearest neighbor queries over uncertain points with discrete probability distributions .",
    "there is a tremendous body of work on ranking documents in information retrieval , and learning how to rank documents given user preferences ( see liu  @xcite for a comprehensive survey ) . that work has considered aspects such as different ranking models , loss functions , different scoring techniques etc .",
    "the techniques developed there tend to be specific to document retrieval ( focusing on keywords , terms , and relevance ) , and usually do not deal with _ existence _ uncertainty ( although they often do model document relevance as a random variable ) . furthermore , our work here primarily focuses on highly efficient algorithms for ranking using a spectrum of different ranking functions .",
    "exploring and understanding the connections between the two research areas is a fruitful direction for further research .",
    "finally , we note that one @xmath3  function is only able to model preferences of one user .",
    "there is an increasing interest in finding a ranking that satisfies multiple users having diverse preferences and intents .",
    "several new theoretical models have been proposed recently  @xcite . however",
    ", all the inputs are assumed to be certain in those models .",
    "incorporating uncertainty into those models or introducing the notion of diversity into our model is an interesting research direction .",
    "[ sec : problem formulation ] we begin with defining our model of a probabilistic database , called _ probabilistic and / xor tree _  @xcite , that captures several common types of correlations .",
    "we then review the prior work on top - k query processing in probabilistic databases , and argue that a single specific ranking function may not capture the intricacies of ranking with uncertainty .",
    "we then present our parameterized ranking functions , @xmath1  and @xmath2 .",
    "we use the prevalent _ possible worlds semantics _ for probabilistic databases  @xcite .",
    "we denote a probabilistic relation with tuple uncertainty by @xmath7 , where @xmath8 denotes the set of tuples ( in section [ sec : attribute_uncertainty ] , we present extensions to handle attribute uncertainty ) .",
    "the set of all possible worlds is denoted by @xmath9 .",
    "each tuple @xmath10 is associated with an existence probability @xmath11 and a score @xmath12 , computed based on a scoring function @xmath13 .",
    "usually @xmath14 is computed based on the tuple attribute values and measures the relative user preference for different tuples . in a deterministic database ,",
    "tuples with higher scores should be ranked higher .",
    "we use @xmath15 to denote the rank of the tuple @xmath16 in a possible world @xmath17 according to @xmath18 .",
    "if @xmath16 does not appear in the possible world @xmath17 , we let @xmath19 .",
    "we say @xmath20 _ ranks higher _ than @xmath21 in the possible world @xmath17 if @xmath22 . for each tuple @xmath16",
    ", we define a random variable @xmath23 that denotes the rank of @xmath16 in @xmath7 .",
    "the _ positional probability _ of a tuple @xmath16 being ranked at position @xmath0 , denoted @xmath24 , is the total probability of the possible worlds where @xmath16 is ranked at position @xmath0 .",
    "rank distribution _ of a tuple @xmath16 , denoted @xmath25 , is simply the probability distribution of the random variable @xmath23 .",
    "our algorithms can handle arbitrarily correlated relations where correlations modeled using markov networks ( section [ sec : correlations ] ) .",
    "however , in most of this article , we focus on the _ probabilistic and / xor tree model _ , introduced in our prior work  @xcite , that can capture only a more restricted set of correlations , but admits highly efficient query processing algorithms .",
    "more specifically , an and / xor tree captures two types of correlations : ( 1 ) _ mutual exclusivity _",
    "( denoted @xmath26 ( _ xor _ ) ) and ( 2 ) _ mutual co - existence _ ( @xmath27 ( _ and _ ) ) .",
    "two events satisfy the mutual co - existence correlation if , in any possible world , either both events occur or neither occurs .",
    "similarly two events are mutually exclusive if there is no possible world where both happen .",
    "now , let us formally define a probabilistic and / xor tree . in tree @xmath28",
    ", we denote the set of children of node @xmath29 by @xmath30 and the least common ancestor of two leaves @xmath31 and @xmath32 by @xmath33 .",
    "we omit the subscript if the context is clear . for simplicity , we separate the attributes of the relation into two groups : ( 1 ) a possible worlds _ key _ , denoted @xmath34 , which is unique in any possible world ( i.e. , two tuples that agree on @xmath34 are mutually exclusive ) , and ( 2 ) the value attributes , denoted @xmath35 . if the relation does not have any key attributes , @xmath36 .    [ and / xor ] a probabilistic and / xor tree @xmath28 represents the mutual exclusion and co - existence correlations in a probabilistic relation @xmath37 , where @xmath34 is the possible worlds key , and @xmath35 denotes the value attributes . in @xmath28 ,",
    "each leaf denotes a tuple , and each inner node has a mark , @xmath26 or @xmath27 . for each @xmath26 node @xmath38 and each of its children",
    "@xmath39 , there is a nonnegative value @xmath40 associated with the edge @xmath41 .",
    "moreover , we require :    @xmath40.25 in    ( probability constraint ) @xmath42 .",
    "( key constraint ) for any two different leaves @xmath43 holding the same key , @xmath44 is a @xmath26 node .",
    "let @xmath45 be the subtree rooted at @xmath29 and @xmath46 .",
    "the subtree @xmath45 inductively defines a random subset @xmath47 of its leaves by the following independent process :    @xmath40.25 in    if @xmath29 is a leaf , @xmath48 .",
    "if @xmath45 roots at a @xmath26 node , then + @xmath49    if @xmath45 roots at a @xmath27 node , then @xmath50    _ x - tuples _",
    "( which can be used to specify mutual exclusivity correlations between tuples ) correspond to the special case where we have a tree of _ height _ 2 , with a @xmath27 node as the root and only @xmath26 nodes in the second level .",
    "figure [ eg_possibleworld ] shows an example of an and / xor tree that models the data from a traffic monitoring application  @xcite , where the tuples represent automatically captured traffic data .",
    "the inherent uncertainty in the monitoring infrastructure is captured using an and / xor tree , that encodes the tuple existence probabilities as well as the correlations between the tuples .",
    "for example , the leftmost @xmath26 node indicates @xmath20 is present with probability @xmath51 and the second @xmath26 node dictates that exactly one of @xmath21 and @xmath52 should appear .",
    "the topmost @xmath27 node tells us the random sets derived from these @xmath26 nodes coexist .",
    "we note that and / xor trees are able to represent any finite set of possible worlds .",
    "this can be done by listing all possible worlds , creating one @xmath27 node for each world , and using a @xmath26 node as the root to capture that these worlds are mutual exclusive .",
    "figure  [ eg_possibleworld ] shows an example of this .",
    "probabilistic and / xor trees significantly generalize _ x - tuples _  @xcite , block - independent disjoint tuples model , and @xmath53-or - sets  @xcite , and as discussed above , can represent a finite set of arbitrary possible worlds . the correlations captured by such a tree can be represented by probabilistic c - tables  @xcite and provenance semirings  @xcite .",
    "however , that does not directly imply an efficient algorithm for ranking .",
    "we remark that markov or bayesian network models are able to capture more general correlations in a compact way  @xcite , however , the structure of the model is more complex and probability computations on them ( inference ) is typically exponential in the treewidth of the model .",
    "the treewidth of an and / xor tree ( viewing it as a markov network ) is not bounded , and hence the techniques developed for those models can not be used to obtain polynomial time algorithms for and / xor trees . and / xor trees also exhibit superficial similarities to ws - trees  @xcite , which can also capture mutual exclusivity and coexistence between tuples . _ we note that no prior work on ranking in probabilistic databases has considered more complex correlations than x - tuples .",
    "_      [ sec : definitions ] the interplay between probabilities and scores complicates the semantics of ranking in probabilistic databases .",
    "this was observed by soliman et al .",
    "@xcite , who first considered this problem and presented two definitions of top - k  queries in probabilistic databases .",
    "several other definitions of ranking have been proposed since then .",
    "we briefly review the ranking functions we consider in this work .",
    "0.15 in    _ * uncertain top - k ( @xmath54-@xmath55 ) * _  @xcite : here the query returns the @xmath0-tuple set that appears as the top - k answer in most possible worlds ( weighted by the probabilities of the worlds ) .    _",
    "* uncertain rank - k ( @xmath54-@xmath56 ) * _  @xcite : at each rank @xmath57 , we return the tuple with the maximum probability of being at the @xmath57th rank in all possible worlds . in other words , @xmath54-@xmath56  returns : + @xmath58 , where @xmath59 .",
    "note that , under these semantics , the same tuple may be ranked at multiple positions . in our experiments , we use a slightly modified version that enforces distinct tuples in the answer ( by not choosing a tuple at a position if it is already chosen at a higher position ) .    _ * probabilistic threshold top - k ( @xmath60 ) * _  @xcite : the original definition of a probabilistic threshold query asks for all tuples with probability of being in top-@xmath61 answer larger than a pre - specified threshold , i.e. , all tuples @xmath16 such that @xmath62 . for consistency with other ranking functions ,",
    "we slightly modify the definition and instead ask for the @xmath63 tuples with the largest @xmath64 values .",
    "_ * expected ranks ( @xmath65-@xmath56 ) * _  @xcite : the tuples are ranked in the increasing order by the _ expected _ value of their ranks across the possible worlds , i.e. , by : +    @xmath66 ,     + where @xmath67 is defined to be @xmath68 if @xmath69 .    _",
    "* expected score ( @xmath65-@xmath70 ) * _ : another natural ranking function , also considered by  @xcite , is simply to rank the tuples by their expected score , @xmath71 .",
    "* @xmath0-selection query  @xcite : * a _",
    "@xmath0-selection _ query returns the set of @xmath0 tuples , such that the expected score of the best available tuple across the possible worlds is maximized .    _ * consensus top - k ( @xmath72-@xmath73 ) * _ : this is a semantics for top-@xmath0 queries developed under the framework of _ consensus answers _ in probabilistic databases  @xcite .",
    "we defer its definition till section  [ sec : contopk ] where we discuss in detail its relationship with the @xmath3  function proposed in this article .    to compare different ranking functions or criteria , we need a distance measure to evaluate the closeness of two top - k answers .",
    "we use the prevalent _ kendall tau _ distance defined for comparing top - k  answers for this purpose  @xcite .",
    "it is also called _",
    "kemeny distance _ in the literature and is considered to have many advantages over other distance metrics  @xcite .",
    "let @xmath74 and @xmath75 denote two full ranked lists , and let @xmath76 and @xmath77 denote the top - k ranked tuples in @xmath74 and @xmath75 respectively .",
    "then _ kendall tau distance _ between @xmath76 and @xmath77 is defined to be : +    @xmath78    , + where @xmath79 is the set of all unordered pairs of @xmath80 ; @xmath81 if it can be inferred from @xmath76 and @xmath77 that @xmath57 and @xmath82 appear in opposite order in the two full ranked lists @xmath74 and @xmath75 , otherwise @xmath83 .",
    "intuitively the kendall distance measures the number of inversions or flips between the two rankings .",
    "for ease of comparison , we divide the kendall distance by @xmath84 to obtain _ normalized kendall _ distance , which always lies in @xmath85 $ ] .    a higher value of the kendall distance indicates a larger disagreement between the two top - k  lists .",
    "it is easy to see that if the kendall distance between two top - k answers is @xmath86 , then the two answers must share at least @xmath87 fraction of tuples ( so if the distance is 0.09 , then the top - k  answers share at least 70% , and typically 90% or more tuples ) .",
    "the distance is 0 if two top - k  answers are identical and 1 if they are disjoint .",
    ".normalized kendall distance between top - k answers according to various ranking functions for two datasets [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      we briefly describe how we can do ranking over tuples with discrete attribute uncertainty where the uncertain attributes are part of the tuple scoring function ( if the uncertain attributes do not affect the tuple score , then they can be ignored for the ranking purposes ) .",
    "more generally , this approach can handle the case when there is a discrete probability distribution over the score of the tuple .",
    "assume @xmath88 for all @xmath57",
    ". the score @xmath89 of tuple @xmath90 takes value @xmath91 with probability @xmath92 and @xmath90 does not appear in the database with probability @xmath93 .",
    "it is easy to see the @xmath3  value of @xmath90 is @xmath94 the algorithm works by treating the alternatives of the tuples ( with a separate alternative for each different possible score for the tuple ) as different tuples .",
    "in other words , we create a new tuple @xmath95 for each @xmath91 value .",
    "@xmath95 has existence probability @xmath92 .",
    "then , we add an _ xor _ constraint over the alternatives @xmath96 of each tuple @xmath90 .",
    "we can then use the algorithm for the probabilistic and / xor tree model to find the values of the @xmath3  function for each @xmath95 separately . note that @xmath97 is exactly the probability that @xmath98 in the and / xor tree .",
    "thus , by the above equation , we have that @xmath99 and @xmath100 therefore , in a final step , we calculate the @xmath101 score for each original tuple @xmath90 by adding the @xmath101 scores of its alternatives @xmath96 . if the original tuples were independent , the complexity of this algorithm is @xmath102 for computing the @xmath3  function , and @xmath5 for computing the @xmath2  function where @xmath103 is the size of the input , i.e. , the total number of different possible scores .",
    "we summarize the complexities of the algorithms for different models in table  [ table : runningtime ] .",
    "now , we explain some entries in the table which has not been discussed .",
    "the first is the @xmath3  computation over an and / xor tree with height @xmath104 .",
    "we have two choices here .",
    "one is just to use the algorithm for arbitrary and / xor trees , i.e. , to use the algorithm in appendix  [ app_runningtime2 ] to expand @xmath105 for each @xmath57 , which runs in @xmath102 time .",
    "the overall running time is @xmath106 .",
    "the other one is to use the divide - and - conquer algorithm in appendix  [ app_runningtime ] to expand the polynomial for each @xmath27 node in @xmath105 .",
    "we can easily see that expanding nodes for each level of the tree requires at most @xmath107 time .",
    "therefore , the running time for expanding @xmath105 is at most @xmath108 and the overall running time is @xmath109 which is much better than @xmath106 if @xmath110 . for @xmath1(@xmath61 )",
    "computation over and / xor trees , we do not know how to achieve a better bound as in the tuple - independent datasets .",
    "we leave it as an interesting open problem .",
    "for @xmath2  computation on and / xor trees , we use @xmath111-@xmath112-@xmath113 .",
    "now , the procedure @xmath114 runs in @xmath115 time where @xmath116 is the depth of tuple @xmath90 in the and / xor tree , i.e. , the length of path from the root to @xmath90 .",
    "therefore , the total running time is @xmath117 .",
    "if the height of the and / xor tree is bounded by @xmath104 , the running time is simply @xmath118 .",
    "in this section , we discuss how to choose the @xmath3  functions and their parameters . depending on the application domain and the scenarios , there are two approaches to this",
    ":    if we know the ranking function we would like to use ( say @xmath60 ) , then we can either simulate or approximate it using appropriate @xmath3  functions .",
    "if we are instead provided user preferences data , we can learn the parameters from them .",
    "clearly , we would prefer to use a @xmath2  function , if possible , since it admits highly efficient ranking algorithms .",
    "for this purpose , we begin with presenting an algorithm to find an approximation to an arbitrary @xmath1  function using a linear combination of @xmath2  functions .",
    "we then discuss how to learn a @xmath1  function from user preferences , and finally present an algorithm for learning a single @xmath2  function .",
    "[ sec : approximating ]    a linear combination of complex exponential functions is known to be very expressive , and can approximate many other functions very well  @xcite . specifically , given a @xmath1  function ,",
    "if we can write @xmath119 as : @xmath120 , then we have that : @xmath121 this reduces the computation of @xmath122 to @xmath123 individual @xmath2function computations , each of which only takes linear time .",
    "this gives us an @xmath124 time algorithm for approximately ranking using @xmath1function for independent tuples ( as opposed to @xmath102 for exact ranking ) .",
    "several techniques have been proposed for finding such approximations using complex exponentials  @xcite .",
    "those techniques are however computationally inefficient , involving computation of the inverses of large matrices and the roots of polynomials of high orders . in this section",
    ", we present a clean and efficient algorithm , based on discrete fourier transforms ( dft ) , for approximating a function @xmath119 , that approaches zero for large values of @xmath57 ( in other words , @xmath125 ) . as we noted earlier",
    ", this captures the typical behavior of the @xmath119 function .",
    "an example of such a function is the step function ( @xmath126 ) which corresponds to the ranking function @xmath60 . at a high level ,",
    "our algorithm starts with a dft approximation of @xmath119 and then adapts it by adding several damping , scaling and shifting factors .",
    "discrete fourier transformation ( dft ) is a well known technique for representing a function as a linear combination of complex exponentials ( also called _ frequency domain representation _ ) .",
    "more specifically , a discrete function @xmath119 defined on a finite domain @xmath127 $ ] can be decomposed into exactly @xmath128 exponentials as : @xmath129 where @xmath130 is the imaginary unit and @xmath131 denotes the dft transform of @xmath132 , @xmath133 .",
    "if we want to approximate @xmath134 by fewer , say @xmath123 , exponentials , we can instead use the @xmath123 dft coefficients with maximum absolute value .",
    "assume that @xmath135 are those coefficients .",
    "then our approximation @xmath136 of @xmath134 by @xmath123 exponentials is given by : @xmath137    , l = 20,width=288 ]    [ fig : approx_1 ]    however , dft utilizes only complex exponentials of unit norm , i.e. , @xmath138 ( where @xmath139 is a real ) , which makes this approximation periodic ( with a period of @xmath128 ) .",
    "this is not suitable for approximating an @xmath134 function used in prf , which is typically a monotonically non - increasing function .",
    "if we make @xmath128 sufficiently large , say larger than the total number of tuples , then we usually need a large number of exponentials ( @xmath123 ) to get a reasonable approximation .",
    "moreover , computing dft for very large @xmath128 is computationally non - trivial .",
    "furthermore , the number of tuples @xmath103 may not be known in advance .",
    "we next present a set of nontrivial tricks to adapt the base dft approximation to overcome these shortcomings .",
    "we assume @xmath119 takes non - zero values within interval @xmath127 $ ] and the absolute values of both @xmath119 and @xmath140 are bounded by @xmath141 . to illustrate our method",
    ", we use the step function : @xmath142 with @xmath143 as our running example to show our method and the specific shortcomings it addresses .",
    "figure [ fig : approx_1 ] illustrates the effect of each of these adaptations .        [ fig : approx_2 ]    1 .   *",
    "( dft ) * we perform pure dft on the domain @xmath144 $ ] , where @xmath145 is a small integer constant ( typically @xmath146 ) .",
    "as we can see in figure [ fig : approx_1 ] ( where @xmath147 and @xmath148 ) , this results in a periodic approximation with a period of 2000 .",
    "although the approximation is reasonable for @xmath149 , the periodicity is unacceptable if the number of tuples is larger than 2000 ( since the positions between 2000 and 3000 ( similarly , between 4000 and 5000 ) would be given high weights ) .",
    "( damping factor ( df ) ) * to address this issue , we introduce a damping factor @xmath150 such that @xmath151 where @xmath152 is a small positive real ( for example , @xmath153 ) .",
    "our new approximation becomes : @xmath154 by incorporating this damping factor , the periodicity is mitigated , since we have : @xmath155 .",
    "especially , @xmath156 for @xmath157 .",
    "( initial scaling ( is ) ) * however the use of damping factor introduces another problem : it gives a biased approximation when @xmath57 is small ( see figure [ fig : approx_1 ] ) . taking the step function as an example ,",
    "@xmath158 is approximately @xmath159 for @xmath160 instead of @xmath161 . to rectify this",
    ", we initially perform dft on a different sequence @xmath162 ( rather than @xmath119 ) on domain @xmath163 $ ] .",
    "therefore , @xmath164 is a reasonable approximation of @xmath165 .",
    "then , if we apply the damping factor , it will give us an unbiased approximation of @xmath134 , which we denote by @xmath166 .",
    "( extending and shifting ( es ) ) * this step is in particular tailored for optimizing the approximation performance for ranking functions .",
    "dft does not perform well at discontinuous points , specifically at @xmath167 ( the left boundary ) , which can significantly affect the ranking approximation . to handle this",
    ", we extrapolate @xmath134 to make it continuous around @xmath168 .",
    "let the resulting function be @xmath169 which is defined on @xmath170 $ ] for small @xmath171 .",
    "again , taking the step function for example , we let @xmath172 then , we shift @xmath173 rightwards by @xmath174 to make its domain lie entirely in positive axis , do initial scaling and perform dft on the resulting sequence .",
    "we denote the approximation of the resulting sequence by @xmath175(by performing ( [ eqn_approx_df ] ) ) . for the approximation of original @xmath119 values",
    ", we only need to do corresponding leftward shifting , namely @xmath176 .",
    "figure [ fig : approx_1 ] shows that dft+df+is+es gives a much better approximation than others around @xmath167 .",
    "figures [ fig : approx_1 ] and [ fig : approx_2](i ) illustrate the efficacy of our approximation technique for the step function . as we can see , we are able to approximate that function very well with just 20 or 30 coefficients .",
    "figure [ fig : approx_2](ii ) and ( iii ) show the approximations for a piecewise linear function and an arbitrarily generated continuous function respectively , both of which are much easier to approximate than the step function .",
    "[ sec : learn prfe ] next we address the question of how to learn the weights of a @xmath1  function or the @xmath177 for a single @xmath2  function from user preferences . to learn a linear combination of @xmath2  functions , we first learn a @xmath1  function and then approximate it as above .",
    "prior work on learning ranking functions ( e.g. ,  @xcite ) assumes that the user preferences are provided in the form of a set of pairs of tuples , and for each pair , we are told which tuple is ranked higher .",
    "our problem differs slightly from this prior work in that , the features that we use to rank the tuples ( i.e. , @xmath178 ) can not be computed for each tuple individually , but must be computed for the entire dataset ( since the values of the features for a tuple depend on the other tuples in the dataset ) .",
    "hence , we assume that we are instead given a small sample of the tuples , and the user ranking for all those tuples .",
    "we compute the features assuming this sample constitutes the entire relation , and learn a ranking function accordingly , with the goal to find the parameters ( the weights @xmath179 for @xmath1  or the parameter @xmath177 for @xmath2 ) that minimize the number of disagreements with the provided ranking over the samples .    given this , the problem of learning @xmath1  is identical to the problem addressed in the prior work , and we utilize the algorithm based on _ support vector machines ( svm ) _",
    "@xcite in our experiments .",
    "on the other hand , we are not aware of any work that has addressed learning a ranking function like @xmath2 .",
    "we use a simple binary search - like heuristic to find the optimal real value of @xmath177 that minimizes the kendall distance between the user - specified ranking and the ranking according to @xmath2(@xmath177 ) .",
    "in other words , we try to find @xmath180}({\\mathsf{dis}}(\\sigma , \\sigma(\\alpha)))$ ] where @xmath181 is the kendall distance between two rankings , @xmath182 is the ranking for the given sample and @xmath183 is the one obtained by using @xmath2(@xmath177 ) function .",
    "suppose we want to find the optimal @xmath145 within the interval @xmath184 $ ] now .",
    "we first compute @xmath185 for @xmath186 and find @xmath57 for which the distance is the smallest .",
    "then we reduce our search range to @xmath187 $ ] and repeat the above recursively .",
    "although this algorithm can only converge to a local minimum , in our experimental study , we observed that all of the prior ranking functions exhibit a uni - valley behavior ( section [ sec : experiments ] ) , and in such cases , this algorithm finds the global optimal .",
    "[ sec : contop - k ] in this section , we show there is a close connection between @xmath1  and the notion of consensus top-@xmath0 answer ( @xmath72-@xmath73 ) proposed in @xcite .",
    "we first review the definition of a consensus top-@xmath0 ranking .",
    "let @xmath181 denote a distance function between two top-@xmath0 rankings .",
    "then the _ most consensus answer _",
    "@xmath188 is defined to be the top-@xmath0 ranking such that the expected distance between @xmath188 and the answer @xmath189 of the ( random ) world @xmath17 is minimized , i.e. , @xmath190\\}.\\ ] ]    @xmath181 may be any distance function defined on pairs of top-@xmath0 answers . in @xcite",
    ", we discussed how to compute or approximate @xmath72-@xmath73  under a number of distance functions , such as spearman s rho , kendall s tau and intersection metric  @xcite .",
    "consider the example in figure  [ fig_example ] .",
    "assume @xmath191 and the distance function is the symmetric difference metric @xmath192 . the most consensus top-@xmath193 answer is @xmath194 and the expected distance is @xmath195=.112\\times 2 + .168\\times 2 + .048\\times 4+.072\\times 4 + .168\\times 2+.252\\times 0 + .072\\times 4+.108\\times 2 $ ] .",
    "we first show that a @xmath72-@xmath73  answer under symmetric difference is equivalent to @xmath60(@xmath0 ) , a special case of @xmath1 .",
    "then , we generalize the result and show that any @xmath1  function is in fact equivalent to some @xmath72-@xmath73  answer under some suitably defined distance function that generalizes symmetric difference .",
    "this new connection further justifies the semantics of @xmath1  from an optimization point of view in that the top-@xmath0 answer obtained by @xmath1  minimizes the expected value of some distance function , and it may shed some light on designing the weight function for @xmath1  in particular applications .",
    "recall @xmath60(@xmath0 ) query returns @xmath0 tuples with the largest @xmath196 values .",
    "we show that the answer returned is the @xmath72-@xmath73  under symmetric difference metric @xmath197 where @xmath198 .    for ease of notation",
    ", we let @xmath199 includes the probability that @xmath16 s rank is larger than @xmath57 and that @xmath16 does nt exist .",
    "we use the symbol @xmath188 to denote a top - k ranked list .",
    "we use @xmath200 to denote the @xmath201 item in the list @xmath188 for positive integer @xmath57 , and @xmath202 to denote the position of @xmath203 in @xmath188 .",
    "[ thm_mindis_prfk ] if @xmath204 is the set of @xmath63 tuples with the largest @xmath205 , then @xmath188 is the @xmath72-@xmath73  answer under metric @xmath197 , i.e. , the answer minimizes @xmath206 $ ] .",
    "suppose @xmath188 is fixed .",
    "we write @xmath206 $ ] as follows : @xmath207&= { \\operatorname{e}}\\,\\bigl[\\sum_{t\\in t } \\delta(t\\in \\tau \\wedge t\\notin \\tau_{pw})+\\delta(t\\in \\tau_{pw } \\wedge t\\notin \\tau)\\bigr ]   \\\\ & = \\sum_{t\\in t\\setminus \\tau } { \\operatorname{e}}[\\delta(t\\in \\tau_{pw } ) ] + \\sum_{t\\in \\tau }   { \\operatorname{e}}[\\delta(t\\notin \\tau_{pw } ) ] \\\\ & = \\sum_{t\\in t\\setminus\\tau } { \\pr}(r(t)\\leq { k } ) + \\sum_{t\\in \\tau } { \\pr}(r(t ) > { k } ) \\\\ & = { k}+ \\sum_{t\\in t } { \\pr}(r(t)\\leq{k } ) - 2\\sum_{t\\in \\tau } { \\pr}(r(t)\\leq { k})\\end{aligned}\\ ] ] the first two terms are invariant with respect to @xmath188 . therefore , it is clear that the set of @xmath63 tuples with the largest @xmath205 minimizes the expectation .",
    "@xmath208      we present a generalization of theorem  [ thm_mindis_prfk ] that shows the equivalence between any @xmath1  function and @xmath72-@xmath73  under _ weighted symmetric difference _ distance functions which generalize the symmetric difference .",
    "suppose @xmath134 is a positive function defined on @xmath209 and @xmath210 .",
    "the weighted symmetric difference with weight @xmath134 of two top-@xmath0 answers @xmath211 and @xmath212 is defined to be @xmath213    intuitively , if the @xmath201 item of @xmath212 can not be found in @xmath211 , we pay a penalty of @xmath119 and the distance is just the total penalty .",
    "if @xmath134 is a decreasing function , the distance function captures the intuition that top ranked items should carry more weight .",
    "if @xmath134 is a constant function , it reduces to the ordinary symmetric difference distance .",
    "note that @xmath214 is not necessarily symmetric .",
    "now , we present the theorem which is a generalization of theorem  [ thm_mindis_prfk ] .",
    "[ thm_mindis_prfk2 ] suppose @xmath134 is a positive function defined on @xmath209 and @xmath210 . if @xmath204 is the set of @xmath63 tuples with the largest @xmath215 values , then @xmath188 is the @xmath72-@xmath73  answer under the weighted symmetric difference @xmath214 , i.e. , the answer minimizes @xmath216 $ ] .",
    "the proof mimics the one for theorem  [ thm_mindis_prfk ] .",
    "suppose @xmath188 is fixed .",
    "we can write @xmath216 $ ] as follows : @xmath217&= { \\operatorname{e}}\\,\\bigl[\\sum_{t\\in t } \\omega(\\tau_{pw}(t ) ) \\delta(t\\in \\tau_{pw } \\wedge",
    "t\\notin \\tau)\\bigr ]   \\\\ & = \\sum_{t\\in t\\setminus \\tau } { \\operatorname{e}}[\\omega(\\tau_{pw}(t))\\delta(t\\in \\tau_{pw } ) ] \\\\ & = \\sum_{t\\in t\\setminus\\tau } \\sum_{i=1}^{k } \\omega(i ) { \\pr}(r(t)=i ) = \\sum_{t\\in t\\setminus\\tau } { \\upsilon}_{\\omega}(t)\\end{aligned}\\ ] ] therefore , it is clear that the set of @xmath63 tuples with the largest @xmath215 values minimizes the above quantity .",
    "@xmath208    although the weighted symmetric difference appears to be a very rich class of distance functions , its relationship with other well studied distance functions , such at spearman s rho and kendall s tau , is still not well understood .",
    "we leave it as an interesting open problem .",
    "we have seen that @xmath2(@xmath177 ) admits very efficient evaluation algorithms .",
    "we also suggest that the parameter @xmath177 should be learned from samples or user feedback .",
    "in fact , we do so since since we hold the promise that by changing the parameter @xmath177 , @xmath2  can span a spectrum of rankings , and the true ranking should be part of this spectrum or close to some point in it .",
    "we provide empirical support for this claim shortly in the next section ( section  [ sec : experiments ] ) . in this section",
    ", we make some interesting theoretical observations about @xmath2 , which help us further understand the behavior of @xmath2  itself .",
    "first , we observe that for @xmath218 , the @xmath2  ranking is equivalent to the ranking of tuples by their existence probabilities ( @xmath2  value in that case is simply the total probability ) . on the other hand ,",
    "when @xmath177 approaches @xmath168 , @xmath2  tends to rank the tuples by their probabilities to be the top-@xmath161 answer , i.e. , @xmath219 .",
    "thus , it is a natural question to ask how the ranking changes when we vary @xmath177 from @xmath168 to @xmath161 .",
    "now , we prove the following theorem which gives an important characterization of the behavior of @xmath2on tuple independent databases .",
    "let @xmath220 denote the ranking obtained by @xmath2(@xmath177 ) . for simplicity",
    ", we ignore the possibility of ties and assume this ranking is unique .",
    "as two special cases , let @xmath221 and @xmath211 denote the rankings obtained by sorting the tuples in a decreasing @xmath219 and @xmath222 order , respectively .    1 .",
    "if @xmath223 ( @xmath224 is ranked higher than @xmath225 in @xmath221 ) and @xmath226 , then @xmath227 any @xmath228 .",
    "if @xmath223 and @xmath229 , then there is exactly one point @xmath230 such that @xmath227 for @xmath231 and @xmath232 for @xmath233 .",
    "let @xmath234 be the @xmath2(@xmath177 ) value of tuple @xmath224 .",
    "then : @xmath235 assume that @xmath236 .",
    "dividing @xmath237 by @xmath234 , we get @xmath238 notice that @xmath239 is always non - negative and an increasing function of @xmath177 .",
    "therefore , @xmath240 is increasing in @xmath177 . if @xmath241 , the same argument show @xmath240 is decreasing in @xmath177 . in either case",
    ", the ratio is monotone in @xmath177 . if @xmath242 and @xmath243 , then @xmath244 for all @xmath245 .",
    "therefore , the first half of the theorem holds . if @xmath242 and @xmath246 , then there is exactly one point @xmath247 such that @xmath248 , @xmath244 for all @xmath249 , and @xmath250 for all @xmath251 .",
    "this proves the second half .",
    "@xmath208    some nontrivial questions can be immediately answered by the theorem .",
    "for example , one may ask the question `` is it possible that we get some ranking @xmath211 , increase @xmath177 a bit and get another ranking @xmath212 , and increase @xmath177 further and get @xmath211 back ? '' , and we can quickly see that the answer is no ; if two tuples change positions , they never change back .",
    "another observation we can make is that : if @xmath252 dominates @xmath253 ( i.e. , @xmath252 has a higher score and probability ) , then @xmath252 always ranks above @xmath253 for any @xmath177 ( this is because @xmath252 ranks above @xmath253 in both @xmath221 and @xmath211 ) .",
    "interestingly , the way the ranking changes as @xmath177 is increased from 0 to 1 is reminiscent of the execution of the _ bubble sort algorithm_. we assume the true order of the tuples is @xmath211 and the initial order is @xmath221 .",
    "we increase @xmath177 from @xmath168 to @xmath161 gradually .",
    "each change in the ranking is just a swap of a pair of adjacent tuples that are not in the right relative order initially .",
    "the number of swaps is exactly the number of reversed pairs .",
    "this is just like bubble sort !",
    "the only difference is that the order of those swaps may not be the same .",
    "[ ex : change ] suppose we have four independent tuples : +    @xmath254     + using ( [ eqn_prfe ] ) , it is easy to see that @xmath255 and @xmath256 . in figure",
    "[ eg_change ] , each curve corresponds to one tuple . in interval",
    "@xmath257 $ ] , any two curves intersect at most once .",
    "changes in the ranking happen right at the intersection points and one adjacent pair of tuples swap their positions .",
    "for instance , the @xmath258 sign in the figure is the intersection point of @xmath259 and @xmath260 .",
    "the rank list is @xmath261",
    "right before the point and @xmath262 right after the point .",
    "[ eg_change ]    in fact , if we think of @xmath61 as a parameter of @xmath60  and we vary @xmath61 from @xmath161 to @xmath103 , the process that the rank list changes is quite similar to the one for @xmath2 : on one extreme where @xmath263 , the rank list is @xmath221 , i.e. , the tuples are sorted by @xmath219 and on the other extreme where @xmath264 , the rank list is @xmath211 , i.e. , the tuples are sorted by @xmath265 .",
    "however , @xmath60  is only able to explore at most @xmath103 different rankings ( one for each @xmath61 ) `` between '' @xmath221 and @xmath211 , while @xmath2  may explore @xmath266 of them .",
    "[ sec : experiments ]    we conducted an extensive empirical study over several real and synthetic datasets to illustrate : ( a ) the diverse and conflicting behavior of different ranking functions proposed in the prior literature , ( b ) the effectiveness of our parameterized ranking functions , especially @xmath2 , at approximating other ranking functions , and ( c ) the scalability of our new generating functions - based algorithms for exact and approximate ranking .",
    "we discussed the results supporting ( a ) in section  [ sec : prior work ] . in this section ,",
    "we focus on ( b ) and ( c ) .",
    "we mainly use the international ice patrol ( iip ) iceberg sighting dataset for our experiments .",
    "this dataset was also used in prior work on ranking in probabilistic databases  @xcite .",
    "the database contains a set of _ iceberg sighting records _",
    ", each of which contains the location ( _ latitude , longitude _ ) of the iceberg , and the _ number of days _ the iceberg has drifted , among other attributes . detecting the icebergs that have been drifting for long periods is crucial , and hence we use the number of days drifted as the ranking score .",
    "the sighting record is also associated with a _ confidence - level _ attribute according to the source of sighting : r / v ( radar and visual ) , vis ( visual only ) , rad ( radar only ) , sat - low ( low earth orbit satellite ) , sat - med ( medium earth orbit satellite ) , sat - high ( high earth orbit satellite ) , and est ( estimated ) .",
    "we converted these six confidence levels into probabilities 0.8 , 0.7 , 0.6 , 0.5 , 0.4 , 0.3 , and 0.4 respectively .",
    "we added a very small gaussian noise to each probability so that ties could be broken .",
    "there are nearly a million records available from 1960 to 2007 ; we created 10 different datasets for our experimental study containing @xmath267 ( iip-100,000 ) to @xmath268 ( iip-1,000,000 ) records , by uniformly sampling from the original dataset .",
    "along with the real datasets , we also use several synthetic datasets with varying degrees of correlations , where the correlations are captured using probabilistic and / xor trees . the tuple scores ( for ranking )",
    "were chosen uniformly at random from @xmath269 $ ] .",
    "the corresponding and / xor trees were also generated randomly starting with the root , by controlling the _ height ( l ) _ , the _ maximum degree of the non - root nodes ( d ) _ , and the _ proportion of @xmath26 and @xmath27 nodes ( x / a ) _ in the tree .",
    "specifically , we use five such datasets :    1 .",
    "syn - ind ( independent tuples ) : the tuple existence probabilities were chosen uniformly at random from @xmath85 $ ] .",
    "syn - xor ( l=2,x / a=@xmath270,d=5 ) : note that the syn - xor dataset , with height set to 2 and no @xmath27 nodes , exhibits only mutual exclusivity correlations ( mimicking the x - tuples model  @xcite ) 3 .",
    "syn - low ( l=3,x / a=10,d=2 ) 4 .",
    "syn - med ( l=5,x / a=3,d=5 ) 5 .",
    "syn - high ( l=5,x / a=1,d=10 ) .",
    "we use the normalized kendall distance ( section [ sec : distance - topk ] ) for comparing two top - k rankings .",
    "all the algorithms were implemented in c++ , and the experiments were run on a 2.4ghz linux pc with 2 gb memory .",
    "we begin with a set of experiments illustrating the effectiveness of our parameterized ranking functions at approximating other ranking functions . due to space constraints , we focus on @xmath2  here because it is significantly faster to rank according to a @xmath2  function ( or a linear combination of several @xmath2  functions ) than it is to rank according a @xmath1  function",
    ".    figures [ fig : approximating_1 ] ( i ) and ( ii ) show the kendall distance between the top-100 answers computed using a specific ranking function and @xmath2  for varying values of @xmath177 , for the iip-100,000 and syn - ind-1000 datasets . for better visualization , we plot @xmath57 on the x - axis , where @xmath271 .",
    "the reason behind this is that the behavior of the @xmath2  function changes rather drastically , and spans a spectrum of rankings , when @xmath177 approaches @xmath161 .",
    "first , as we can see , the @xmath2  ranking is close to ranking by _ score _",
    "alone for small values of @xmath177 , whereas it is close to the ranking by _ probability _ when @xmath177 is close to 1 ( in fact , for @xmath218 , the @xmath2  ranking is equivalent to the ranking of tuples by their existence probabilities ) , @xmath2  ranks the tuples by their probabilities to be the top-@xmath161 answer . ] .",
    "second , we see that , for all other functions ( @xmath65-@xmath70 , @xmath60 , @xmath54-@xmath56 , @xmath65-@xmath56 ) , there exists a value of @xmath177 for which the distance of that function to @xmath2  is very small , indicating that @xmath2  can indeed approximate those functions quite well .",
    "moreover we observe that this `` uni - valley '' behavior of the curves justifies the binary search algorithm we advocate for learning the value of @xmath177 in section  [ subsec_learnprfe ] .",
    "our experiments with other synthetic and real datasets indicated a very similar behavior by the ranking functions .",
    "next we evaluate the effectiveness of our approximation technique presented in section  [ sec : approximating and learning ] . in figure",
    "[ fig : approximating_2 ] ( i ) , we show the kendall distance between the top - k answers obtained using @xmath60  ( for @xmath272 ) and using a linear combination of @xmath2  functions found by our algorithms .",
    "as expected , the approximation using the vanilla dft technique is very bad , with the kendall distance close to 0.8 indicating little similarity between the top - k  answers .",
    "however , the approximation obtained using our proposed algorithm ( indicated by dft+df+is+es curve ) achieves a kendall distance of less than 0.1 with just @xmath273 exponentials .    in figure",
    "[ fig : approximating_2 ] ( ii ) , we compare the approximation quality ( found by our algorithm dft+df+is+es ) for three ranking functions for two datasets : iip-100,000 with @xmath274 , and iip-1,000,000 dataset with @xmath275 .",
    "the ranking functions we compared were : ( 1 ) @xmath60  ( @xmath276 ) , ( 2 ) an arbitrary smooth function , @xmath277 , and ( 3 ) a linear function ( figure [ fig : approximating_2](ii ) ) .",
    "we see that @xmath278 suffices to bring the kendall distance to @xmath279 in all cases .",
    "we also observe that smooth functions ( for which the absolute value of the first derivative of the underlying continuous function is bounded by a small value ) are usually easier to approximate .",
    "we only need @xmath273 exponentials to achieve a kendall distance less than @xmath280 for @xmath277 .",
    "the linear function is even easier to approximate .",
    "next we consider the issue of learning ranking functions from user preferences . lacking real user preference data",
    ", we instead assume that the user ranking function , denoted _ user - func _ , is identical to one of : @xmath65-@xmath70 , @xmath60 , @xmath54-@xmath56 , @xmath65-@xmath56 , or @xmath2(@xmath281 ) .",
    "we generate a set of user preferences by ranking a random sample of the dataset using _ user - func _ ( thus generating five sets of user preferences ) .",
    "these are then fed to the learning algorithm , and finally we compare the kendall distance between the learned ranking and the true ranking for the entire dataset .        in figure",
    "[ fig : learning - correlations_1](i ) , we plot the results for learning a single @xmath2  function ( i.e. , for learning the value of @xmath177 ) using the binary search - like algorithm presented in section [ sec : learn prfe ] .",
    "the experiment reveals that when the underlying ranking is done by @xmath2 , the value of @xmath177 can be learned perfectly .",
    "when one of @xmath60  or @xmath54-@xmath56  is the underlying ranking function , the correct value @xmath145 can be learned with a fairly small sample size , and increasing the number of samples does not help in finding a better @xmath177 . on the other hand , @xmath65-@xmath56",
    "can not be learned well by @xmath2  unless the sample size approaches the total size of whole dataset .",
    "this phenomenon can be partly explained using figure [ fig : approximating_1](i ) and ( ii ) in which the curves for @xmath60  and @xmath54-@xmath55  have a fairly smooth valley , while the one for @xmath65-@xmath56  is very sharp and the region of @xmath177 values where the distance is low is extremely small ( @xmath282 $ ] ) .",
    "hence , the minimum point for @xmath65-@xmath56  is harder to reach .",
    "another reason is that @xmath65-@xmath56  is quite sensitive to the size of the dataset , which makes it hard to learn it using a smaller - sized sample dataset .",
    "we also observe that while extremely large samples are able to learn @xmath65-@xmath70  well , the behavior of @xmath65-@xmath70  is quite unstable when the sample size is smaller .",
    "note that if we already know the form of the ranking function , we do nt need to learn it in this fashion ; we can instead directly find an approximation for it using our dft - based algorithm .    in figure",
    "[ fig : learning - correlations_1 ] ( ii ) , we show the results of an experiment where we tried to learn a @xmath1  function ( using the svm - lite package  @xcite ) .",
    "we keep our sample size @xmath283 since svm - lite becomes drastically slow with larger sample sizes .",
    "first we observe that @xmath60  and @xmath2  can be learned very well from a small size sample ( distance @xmath284 in most cases ) and increasing the sample size does not benefit significantly .",
    "@xmath54-@xmath56  can also be learned , but the approximation is nt nearly as good .",
    "this is because @xmath54-@xmath56  can not be written as a single @xmath1  function .",
    "we observed similar behavior in our experiments with other datasets . due to space constraints ,",
    "we omit a further discussion on learning a @xmath1  function ; the issues in learning such weighted functions have been investigated in prior literature , and if the true ranking function can be written as a @xmath1  function , then the above algorithm is expected to learn it well given a reasonable number of samples .",
    "next we evaluate the behavior of ranking functions over probabilistic datasets modeled using probabilistic and / xor trees .",
    "we use the four synthetic correlated datasets , syn - xor , syn - low , syn - med , and syn - high , for these experiments . for each dataset and each ranking function considered ,",
    "we compute the rankings by considering the correlations , and by ignoring the correlations , and then compute the kendall distance between these two ( e.g. , for @xmath2 , we compute the rankings using * prob - andor - prf - rank * and * ind - prf - rank * algorithms ) .",
    "figure [ fig : learning - correlations_2](i ) shows the results for the @xmath2  ranking function for varying @xmath177 , whereas in figure [ fig : learning - correlations_2](ii ) , we plot the results for @xmath2(@xmath285 ) , @xmath286 , and @xmath54-@xmath56 .",
    "as we can see , on highly correlated datasets , ignoring the correlations can result in significantly inaccurate top - k  answers .",
    "this is not as pronounced for the syn - xor dataset .",
    "this is because , in any group of tuples that are mutually exclusive , there are typically only a few tuples that may have sufficiently high probabilities to be part of the top - k answer ; the rest of the tuples may be ignored for ranking purposes . because of this , assuming tuples to be independent of each other does not result in significant errors . as @xmath177 approaches @xmath161 ,",
    "@xmath2  tends to sort the tuples by probabilities , so all four curves in figure [ fig : learning - correlations_2](i ) become close to @xmath168 .",
    "we note that ranking by @xmath65-@xmath70  is invariant to the correlations , which is a significant drawback of that function .",
    "figure [ fig : execution - times](i ) shows the execution times for four ranking functions : @xmath2 , @xmath60 , @xmath54-@xmath56  and @xmath65-@xmath56 , for the iip - datasets , for different dataset sizes and @xmath63 .",
    "we note that the running time for @xmath1  is similar to that of @xmath60 .",
    "as expected , ranking by @xmath2  or @xmath65-@xmath56  is very efficient ( 1000000 tuples can be ranked within 1 or 2 seconds ) . indeed , after sorting the dataset in an non - decreasing score order , @xmath2  needs only a single scan of the dataset , and @xmath65-@xmath56needs to scan the dataset twice .",
    "execution times for @xmath287 and @xmath54-@xmath56-@xmath63 increase linearly with @xmath61 and @xmath63 respectively and the algorithms become very slow for high @xmath61 and @xmath63 . the running times of both @xmath2  and @xmath65-@xmath56",
    "are not significantly affected by @xmath63 .",
    "figure [ fig : execution - times](ii ) compares the execution time for @xmath60  and its approximation using a linear combination of @xmath2  functions ( see figure [ fig : approximating_2](i ) ) , for two different values of @xmath0 .",
    "@xmath288 indicates that 50 exponentials were used in the approximation ( note that the approximate ranking , based on @xmath2 , is insensitive to the value of @xmath63 ) .",
    "as we can see , for large datasets and for higher values of @xmath63 , exact computation takes several orders of magnitude more time to compute than the approximation . for example , the exact algorithm takes nearly 1 hour for @xmath289 and @xmath290 while the approximate answer obtained using @xmath291 @xmath2  functions takes only @xmath292 seconds and achieves a kendall distance @xmath293 .    for correlated datasets ,",
    "the effect is even more pronounced . in figure",
    "[ fig : execution - times](iii ) , we plot the results of a similar experiment , but using two correlated datasets : syn - xor and syn - high .",
    "note that the number of tuples in these datasets is smaller by a factor of 10 .",
    "as we can see , our generating functions - based algorithms for computing @xmath2  are highly efficient , even for datasets with high degrees of correlation .",
    "as above , approximation of the @xmath60  ranking function using a linear combination of @xmath2  functions is significantly cheaper to compute than using the exact algorithm .    combined with the previous results illustrating that a linear combination of @xmath2  functions can approximate other ranking functions very well , this validates the unified ranking approach that we propose in this paper .",
    "among many models for capturing the correlations in a probabilistic database , graphical models ( markov or bayesian networks ) perhaps represent the most systematic approach  @xcite .",
    "the appeal of graphical models stems both from the pictorial representation of the dependencies , and a rich literature on doing inference over them . in this section",
    ", we present an algorithm for computing the @xmath3  function values for all tuples of a correlated dataset when the correlations are represented using a graphical model .",
    "the resulting algorithm is a non - trivial dynamic program over the _ junction tree _ of the graphical model .",
    "our main result is that we can compute the prf function in polynomial time if the junction tree of the graphical model has bounded treewidth .",
    "it is worth noting that this result can not subsume our algorithm for and / xor trees ( section [ subsec_andortree ] ) since the treewidth of the moralized graph of a probabilistic and / xor tree may not be bounded . in some sense , this is close to _ instance - optimal _ since the complexity of the underlying inference problem is itself exponential in the treewidth of the graphical model ( this however does not preclude the possibility that the ranking itself could be done more efficiently without computing the prf function explicitly ",
    "however , such an algorithm is unlikely to exist ) .",
    "we start with briefly reviewing some notations and definitions related to graphical models and junction trees .",
    "let @xmath294 be the set of tuples in @xmath7 , sorted in an non - increasing order of their score values .",
    "for each tuple @xmath16 in @xmath8 , we associate an indicator random variable @xmath295 , which is @xmath161 if @xmath16 is present , and @xmath168 otherwise .",
    "let @xmath296 and @xmath297 .",
    "for a set of variables @xmath298 , we use @xmath299 to denote the joint probability distribution over those variables .",
    "so @xmath300 denotes the joint probability distribution that we are trying to reason about .",
    "this joint distribution captures all the correlations in the dataset .",
    "however , directly trying to represent it would take @xmath301 space , and hence is clearly infeasible .",
    "probabilistic graphical models allow us to represent this joint distribution compactly by exploiting the conditional independences present among the variables . given three disjoint sets of random variables @xmath302",
    ", we say that @xmath35 is conditionally independent of @xmath141 _ given _",
    "@xmath303 if and only if : @xmath304    we assume that we are provided with a _ junction tree _ over the variables @xmath305 that captures the correlations among them . a junction tree can be constructed from a graphical model using standard algorithms  @xcite .",
    "recently junction trees have also been used as a internal representation for probabilistic databases , and have been shown to be quite effective at handling lightly correlated probabilistic databases  @xcite .",
    "we describe the key properties of junction trees next .",
    "let @xmath28 be a tree with each node @xmath29 associated with a subset @xmath306 .",
    "we say @xmath28 is a _ junction tree _ if any intersection @xmath307 for any @xmath308 is contained in @xmath309 for every node @xmath310 on the unique path between @xmath38 and @xmath29 in @xmath28 ( this is called the _ running intersection property _ ) .",
    "the treewidth @xmath311 of a junction tree is defined to be @xmath312 .",
    "denote @xmath313 for each edge @xmath314 .",
    "we call @xmath315 a _ separator _ since removal of @xmath315 disconnects the graphical model . the set of conditional independences embodied by a junction tree can be found using the markov property : + * ( markov property ) * given variable sets @xmath302 , if @xmath303 separates @xmath35 and @xmath141 ( i.e. , removal of variables in @xmath303 disconnects the variables in @xmath35 from variables in @xmath141 in the junction tree ) , then @xmath35 is conditionally independent of @xmath141 given @xmath303 .",
    "let @xmath316 .",
    "figure [ fig : graphical - model ] ( i ) and ( ii ) show the ( undirected ) graphical model and the corresponding junction tree @xmath28 .",
    "@xmath28 has four nodes : @xmath317 , @xmath318 , @xmath319 and @xmath320 .",
    "the treewidth of @xmath28 is @xmath161 .",
    "we have , @xmath321 , @xmath322 and @xmath323 . using the markov property",
    ", we observe that @xmath324 is independent of @xmath325 given @xmath326 .    with each clique",
    "@xmath327 in the junction tree , we associate a _ potential _",
    "@xmath328 , which is a function over all variables @xmath329 and captures the correlations among those variables .",
    "similarly , with each separator @xmath315 , we associate a _ potential _",
    "without loss of generality , we assume that the potentials are _ calibrated _ ,",
    "i.e. , the potential corresponding to a clique ( or a separator ) is exactly the joint probability distribution over the variables in that clique ( separator ) . given a junction tree with arbitrary potentials ,",
    "calibrated potentials can be computed using a standard _ message passing algorithm _",
    "the complexity of this algorithm is @xmath331 .",
    "then the joint probability distribution of @xmath305 , whose correlations can be captured using a calibrated junction tree @xmath28 , can be written as : @xmath332          we begin with describing the first step of our algorithm , and defining a reduced and simpler to state problem .",
    "recall that our goal is to rank the tuples according to @xmath333 .",
    "for this purpose , we first compute the positional probabilities , @xmath334   @xmath335 , using the algorithms presented in the next two subsections . given those ,",
    "the values of @xmath336 can be computed in @xmath102 time for all tuples , and the ranking itself can be done in @xmath337 time ( by sorting ) .",
    "the positional probabilities ( @xmath338 ) may also be of interest by themselves .",
    "for each tuple @xmath90 , we compute @xmath339 at once . recall that @xmath334 is the probability that @xmath90 exists ( i.e. , @xmath340 ) and exactly @xmath341 tuples with scores higher than @xmath90 are present ( i.e. , @xmath342 ) .",
    "in other words : @xmath343    hence , we begin with first conditioning the junction tree by setting @xmath340 , and re - calibrating .",
    "this is done by identifying all cliques and separators which contain @xmath344 , and by updating the corresponding probability distributions by removing the values corresponding to @xmath345 .",
    "more precisely , we replace a probability distribution @xmath346 , by a potential @xmath347 computed as : @xmath348    @xmath349 is not a probability distribution since the entries in it may not sum up to 1 .",
    "further , the potentials may not be consistent with each other .",
    "hence , we need to recalibrate this junction tree using message passing  @xcite .",
    "as mentioned earlier , this takes @xmath331 time .",
    "figure [ fig : junction - tree - remove-5 ] shows the resulting ( uncalibrated ) junction tree after conditioning on @xmath350 .",
    "results in a smaller junction tree , with uncalibrated potentials , that captures the distribution over @xmath351 given @xmath350 .",
    ", scaledwidth=60.0% ]    if @xmath344 is a separator in the junction tree , then we get more than one junction tree after conditioning on @xmath340 .",
    "figure [ fig : junction - tree - remove-4 ] shows the two junction trees we would get after conditioning on @xmath352 .",
    "the variables in these junction trees are independent of each other ( this follows from the markov property ) , and the junction trees can be processed separately from each other .    since the resulting junction tree or junction trees capture the probability distribution conditioned on the event @xmath340 , our problem now reduces to finding the probability distribution of @xmath353 in those junction trees . for cleaner description of the algorithm",
    ", we associate an indicator variable @xmath354 with each variable @xmath355 in the junction tree .",
    "@xmath354 is set to 1 if @xmath356 , and is 0 otherwise .",
    "this allows us to state the key problem to be solved as follows :     results in two junction trees.,scaledwidth=16.0% ]    if the result of the conditioning was a single junction tree ( over @xmath357 variables ) , we multiply the resulting probabilities by @xmath358 to get the rank distribution of @xmath90 .",
    "however , if we get @xmath359 junction trees , then we need one additional step .",
    "let @xmath360 be the random variables denoting the partial sums for each of junction trees .",
    "we need to combine the probability distributions over these partial sums , @xmath361 , into a single probability distribution over @xmath362 .",
    "this can be done by repeatedly applying the following general formula : @xmath363    a naive implementation of the above takes time @xmath102 .",
    "although this can be improved using the ideas presented in appendix [ app_expandpoly ] , the complexity of computing @xmath364 is much higher and dominates the overall complexity .",
    "next we present algorithms for solving the redefined problem .",
    "we first describe an algorithm for markov chains , a special , yet important , case of the graphical models .",
    "markov chains appear naturally in many settings , and have been studied in probabilistic database literature as well  @xcite .",
    "any finite - length markov chain is a markov network whose underlying graph is simply a path : each variable is directly dependent on only its predecessor and successor .",
    "the junction tree for a markov chain is also a path in which each node corresponds to an edge of the markov chain .",
    "the treewidth of such a junction tree is one . without loss of generality",
    ", we assume that the markov chain is @xmath365 ( figure [ fig : general - junction - tree](i ) ) . the corresponding junction tree @xmath28 is a path with cliques @xmath366 as shown in the figure .",
    "we compute the distribution @xmath367 recursively .",
    "let @xmath368 denote the partial sum over the first @xmath82 variables @xmath369 .    at the clique @xmath370 , @xmath371 ,",
    "we recursively compute the joint probability distribution : @xmath372 . the initial distribution @xmath373 , @xmath374 , is computed directly :",
    "@xmath375 given @xmath376 , we compute @xmath377 as follows . observe that @xmath378 and @xmath379 are conditionally independent given the value of @xmath380 ( by markov property ) .",
    "thus we have : @xmath381 using @xmath382 , we can compute : @xmath383    at the end , we have the joint distribution : @xmath384 .",
    "we can compute a distribution over @xmath385 as : @xmath386    the complexity of the above algorithm to compute @xmath387 is @xmath388  although we only perform @xmath389 steps , @xmath377 contains @xmath390 terms , each of which takes @xmath391 time to compute . since we have to repeat this for every tuple , the overall complexity of ranking the dataset can be seen to be @xmath106 .        [",
    "fig : general - junction - tree ]      we follow the same general idea for general junction trees .",
    "let @xmath28 denote the junction tree over the variables @xmath392 .",
    "we begin by rooting @xmath28 at an arbitrary clique , and recurse down the tree . for a separator @xmath298 ,",
    "let @xmath393 denote the subtree rooted at @xmath298 .",
    "denote by @xmath394 the partial sum over the variables in the subtree @xmath393 that are _ not present _ in @xmath298 , i.e. , : @xmath395    consider a clique node @xmath303 , and let @xmath298 denote the separator between @xmath303 and its parent node ( @xmath396 for the root clique node ) .",
    "we will recursively compute the joint probability distribution @xmath397 for each such separator @xmath298 .",
    "since the root clique node has no parent , at the end we are left with precisely the probability distribution that we need , i.e. , @xmath398 ) .",
    "let the separators to the children of @xmath303 be @xmath399 ( see figure [ fig : general - junction - tree](ii ) ) .",
    "we recursively compute @xmath400 .",
    "let @xmath401 .",
    "we observe that @xmath402 is precisely the set of variables that contribute to the partial sum @xmath394 , but do not contribute to any of the partial sums @xmath403 , i.e. : @xmath404    we begin with computing @xmath405 .",
    "observe that the variable set @xmath406 is independent of @xmath407 given the values of the variables in @xmath408 ( by markov property ) .",
    "note that it was critical that the variables in @xmath408 not contribute to the partial sum @xmath407 , otherwise this independence would not hold .",
    "given that , we have : @xmath409    using @xmath410 is independent of @xmath411 given @xmath412 , we get : @xmath413 now we can compute the probability distribution over @xmath414 as follows : @xmath415 by repeating this process for @xmath416 to @xmath417 , we get the probability distribution : @xmath405 .    next , we need to add in the contributions of the variables in @xmath402 to the partial sum @xmath418 .",
    "let @xmath402 contain @xmath419 variables , @xmath420 , and let @xmath421 denote the corresponding indicator variables .",
    "it is easy to see that : + @xmath422 where @xmath423 .",
    "although it looks complex , we only need to touch every entry of the probability distribution @xmath424 once to compute @xmath425 .",
    "all that remains is marginalizing that distribution to sum out the variables in @xmath426 , giving us @xmath397 .",
    "this is similar to the final step above .",
    "let @xmath401 denote the variables that contribute to the partial sum @xmath394 .",
    "we can apply the same procedure as above to compute @xmath427 , which we marginalize to obtain @xmath397 .",
    "the complexity of the above algorithm for a specific clique @xmath303 is dominated by the cost of computing the different probability distributions of the form @xmath428 , where @xmath429 is a partial sum .",
    "we have to compute @xmath6 such probability distributions , and each of those computations takes @xmath430 time .",
    "since there are at most @xmath103 cliques , and since we have to repeat this process for every tuple , the overall complexity of ranking the dataset can be seen to be : @xmath431 , where @xmath311 denotes the treewidth of the junction tree , i.e. , the size of the maximum clique minus 1 .",
    "in this article we presented a unified framework for ranking over probabilistic databases , and presented several novel and highly efficient algorithms for answering top - k  queries .",
    "considering the complex interplay between probabilities and scores , instead of proposing a specific ranking function , we propose using two parameterized ranking functions , called @xmath1  and @xmath2 , which allow the user to control the tuples that appear in the top - k  answers .",
    "we developed novel algorithms for evaluating these ranking functions over large , possibly correlated , probabilistic datasets .",
    "we also developed an approach for approximating a ranking function using a linear combination of @xmath2  functions thus enabling highly efficient , albeit approximate computation , and also for learning a ranking function from user preferences .",
    "our work opens up many avenues for further research .",
    "there may be other non - trivial subclasses of @xmath3  functions , aside from @xmath2 , that can be computed efficiently .",
    "understanding the behavior of various ranking functions and their relationships across probabilistic databases with diverse uncertainties and correlation structures also remains an important open problem in this area .",
    "finally , the issues of ranking have been studied for many years in disciplines ranging from economics to information retrieval ; better understanding the connections between that work and ranking in probabilistic databases remains a fruitful direction for further research .",
    "* theorem [ thm_generating ] * _ the coefficient of the term @xmath432 in @xmath433 is the total probability of the possible worlds for which , for all @xmath82 , there are exactly @xmath434 leaves associated with variable @xmath435 . _    suppose @xmath28 is rooted at @xmath139 , @xmath436 are @xmath139 s children , and @xmath437 is the subtree rooted at @xmath438 .",
    "we denote by @xmath298 ( or @xmath439 ) the random set of leaves generated according to model @xmath28 ( or @xmath437 ) .",
    "we let @xmath440 ( or @xmath441 ) be the generating function corresponding to @xmath28 ( or @xmath442 ) . for ease of notation",
    ", we use @xmath443 to denote index vector @xmath444 , @xmath445 to denote the set of all such @xmath443s and @xmath446 to denote @xmath447 .",
    "therefore , we can write @xmath448 we use the notation @xmath449 for some @xmath450 to denote the event that @xmath298 contains @xmath451 leaves associated with variable @xmath452 for all @xmath82 .",
    "given the notations , we need to show @xmath453",
    ".    we shall prove by induction on the height of the and / xor tree .",
    "we consider two cases . if @xmath139 is a @xmath27 node , we know from definition  [ and / xor ] that @xmath454 .",
    "first , it is not hard to see that given @xmath455 for @xmath456 , the event @xmath449 happens if and only if @xmath457 .",
    "therefore , @xmath458 assume @xmath441 can be written as @xmath459 . from the construction of the generating function ,",
    "we know that @xmath460 by induction hypothesis , we have @xmath461 for any @xmath419 and @xmath462",
    ". therefore , we can conclude from ( [ eq : prob - subset ] ) and ( [ eq : ind - gene ] ) that @xmath463 .",
    "now let us consider the other case where @xmath139 is a @xmath26 node . from definition  [ and / xor ] , it is not hard to see that @xmath464 moreover , we have @xmath465 where the last equality follows from ( [ eq : prob - subset2 ] ) and induction hypothesis .",
    "this completes the proof .",
    "this section is devoted to several algorithms for expanding polynomials into standard forms .",
    "given a set of polynomials in the form of @xmath466 for @xmath467 , we want to compute the multiplication @xmath468 written in the standard form @xmath469 , i.e. , we need to compute the coefficients @xmath470 .",
    "let @xmath471 be the degree of the polynomial @xmath472 .",
    "let @xmath473 be the degree of @xmath474 .",
    "first we note that the naive method ( multiply @xmath472s one by one ) gives us an @xmath102 time algorithm by simple counting argument .",
    "let @xmath475 .",
    "it is easy to see @xmath476 .",
    "so the time to multiply @xmath477 and @xmath478 is @xmath479 .",
    "then , we can see the total time complexity is : @xmath480    now , we show how to use divide - and - conquer and fft ( fast fourier transformation ) to achieve an @xmath481 time algorithm .",
    "it is well known that the multiplication of two polynomials of degree @xmath6 can be done in @xmath482 time using fft .",
    "the divide - and - conquer algorithm is as follows : if there exists any @xmath472 such that @xmath483 , we evaluate @xmath484 recursively and then multiply it with @xmath472 using fft . if not , we partition all @xmath472s into two sets @xmath408 and @xmath412 such that @xmath485",
    ". then we evaluate @xmath408 and @xmath412 separately and multiply them together using fft .",
    "it is easy to see the time complexity of the algorithm running on input size @xmath103 satisfies @xmath486 where @xmath487 and @xmath488 . by solving the above recursive formula , we know @xmath489 .",
    "we consider a more general problem of expanding a nested expression of uni - variable polynomial ( with variable @xmath490 ) into its standard form @xmath491 . here",
    "a nested expression refers to a formula that only involves constants , the variable @xmath490 , addition @xmath258 , multiplication @xmath492 , and parenthesis @xmath493 and @xmath494 , for example , @xmath495 .",
    "formally , we define recursively an _ expression _ to be either      we assume the degree of the polynomial and the length of the expression are of sizes @xmath6 .",
    "the naive method runs in time @xmath106 ( each inner node needs @xmath266 time as shown in the last subsection ) .",
    "if we use the previous divide - and - conquer method for expanding each inner node , you can easily get @xmath496 .",
    "now we sketch two improved algorithms with running time @xmath266 .",
    "the first is conceptual simpler while the second is much easier to implement .    1",
    ".   choose @xmath497 different numbers @xmath498 .",
    "2 .   evaluate the polynomial at these points , i.e. , compute @xmath499 .",
    "it is easy to see that each evaluation takes linear time ( bottom - up over the tree ) .",
    "so this step takes @xmath102 time in total .",
    "3 .   use any @xmath102 polynomial interpolation algorithm to find the coefficient .",
    "in fact , the interpolation reduces to finding a solution for the following linear system : @xmath500 the commonly used gaussian elimination for inverting a matrix requires @xmath501 operations .",
    "the matrix we used is a special type of matrix and is commonly referred to as a vandermonde matrix .",
    "there exists numerical algorithms that can invert a vandermonde matrix in @xmath266 time , for example @xcite .",
    "a small drawback of the above algorithm is that the algorithms used to invert a vandermonde matrix is nontrivial to implement .",
    "the next algorithm does not need to invert a matrix , is much simpler to implement and has the same running time of @xmath266 .",
    "we need some notation first .",
    "suppose the polynomial is @xmath502 ( @xmath503s are unknown yet ) .",
    "let @xmath504 be the @xmath505-dimensional zero vector except that the @xmath201 entry is @xmath161 , i.e. , @xmath506 . let @xmath507 be the @xmath497-dimensional vector which is the dft ( discrete fourier transformation ) of @xmath504 .",
    "let @xmath508 be the @xmath509 root of unit .",
    "let @xmath510 and @xmath511 .    by definition , @xmath512 where @xmath513 is the @xmath514 entry of @xmath515 .",
    "let @xmath516 be the coefficient vector of @xmath517 .",
    "it is trivial to see @xmath518 ( the inner product ) .",
    "therefore , we have that @xmath519 the last equality holds by the definition of @xmath520 . if we use @xmath521 to denote the vector @xmath522 and @xmath523 to denote the matrix @xmath524 , the above equation can be simply written as @xmath525 now , we are ready describe our algorithm :    1 .",
    "compute @xmath526 for all @xmath0 .",
    "this consists of evaluating @xmath520 over complex @xmath490 @xmath103 times , which takes @xmath102 time . 2 .",
    "use ( [ eq : poly ] ) to compute the coefficients .",
    "this again takes @xmath266 time .",
    "in fact , the above algorithm can be seen as a specialization of the first algorithm .",
    "instead of picking arbitrary @xmath497 real points @xmath527 to evaluate the polynomial , we pick @xmath497 complex points @xmath528 . the vandermonde matrix formed by these points ,",
    "i.e. , @xmath529 has a very nice property that @xmath530 where @xmath531 is the conjugate of @xmath532 ( this can be verified easily ) .",
    "therefore , we can obtain @xmath533 for free .",
    "actually , it is easy to see that @xmath531 is exactly @xmath523 ."
  ],
  "abstract_text": [
    "<S> ranking is a fundamental operation in data analysis and decision support , and plays an even more crucial role if the dataset being explored exhibits uncertainty . </S>",
    "<S> this has led to much work in understanding how to rank the tuples in a probabilistic dataset in recent years . in this article , we present a unified approach to ranking and top-@xmath0 query processing in probabilistic databases by viewing it as a multi - criteria optimization problem , and by deriving a set of _ features _ that capture the key properties of a probabilistic dataset that dictate the ranked result . </S>",
    "<S> we contend that a single , specific ranking function may not suffice for probabilistic databases , and we instead propose two _ parameterized ranking functions _ , called @xmath1  and @xmath2 , that generalize or can approximate many of the previously proposed ranking functions . </S>",
    "<S> we present novel _ generating functions_-based algorithms for efficiently ranking large datasets according to these ranking functions , even if the datasets exhibit complex correlations modeled using _ </S>",
    "<S> probabilistic and / xor trees _ or _ </S>",
    "<S> markov networks_. we further propose that the parameters of the ranking function be _ learned _ from user preferences , and we develop an approach to learn those parameters . </S>",
    "<S> finally , we present a comprehensive experimental study that illustrates the effectiveness of our parameterized ranking functions , especially @xmath2 , at approximating other ranking functions and the scalability of our proposed algorithms for exact or approximate ranking . </S>"
  ]
}