{
  "article_text": [
    "in many applications of economics , finance , and other scientific fields , researchers often face a large panel data set in which there are multiple observations for each individual ; here individuals can be families , firms , countries , etc .",
    "modern applications usually involve data - rich environments in which both the number of observations for each individual and the number of individuals are large . one useful method for summarizing information in a large dataset is the factor model : @xmath0 where @xmath1 is an individual effect",
    ", @xmath2 is an @xmath3 vector of factor loadings and @xmath4 is an @xmath3 vector of common factors ; @xmath5 denotes the idiosyncratic component of the model .",
    "note that @xmath6 is the only observable random variable in this model .",
    "if we write @xmath7 , @xmath8 , @xmath9 and @xmath10 , then model ( 1.1 ) can be equivalently written as @xmath11    because @xmath6 is the only observable in the model , both factors and loadings are treated as parameters to estimate . as was shown by chamberlain and rothschild ( 1983 ) , in many applications of factor analysis , it is desirable to allow dependence among the error terms @xmath12 not only serially but also cross - sectionally .",
    "this gives rise to the _ approximate factor model _",
    ", in which the @xmath13 covariance matrix @xmath14 is not diagonal .",
    "in addition , the diagonal entries may vary in a large range . as a result , efficiently estimating the factor model under both large @xmath15 and large @xmath16 is difficult because to take into account both cross - sectional heteroskedasticity and dependence of @xmath12 , it is essential to estimate the large covariance @xmath17 .",
    "the latter has been known as a challenging problem when @xmath15 is larger than @xmath16 .    in this paper",
    ", we assume the model to be _ conditionally sparse _",
    ", in the sense that @xmath17 is a sparse matrix with bounded eigenvalues .",
    "this assumption effectively reduces the number of parameters to be estimated in the model , and allows a consistent estimation of @xmath17 .",
    "the latter is needed to efficiently estimate the factor loadings .",
    "in addition , it enables the model to identify the common components @xmath18 asymptotically as @xmath19 .",
    "we propose two alternative methods , both are likelihood - based .",
    "the first one is a two - step procedure . in step one , we apply the _ principal orthogonal complement thresholding _ ( poet ) estimator of fan et al .",
    "( 2012 ) to estimate @xmath17 using the adaptive thresholding as in cai and liu ( 2011 ) ; in step two , we estimate the factor loadings by maximizing a gaussian - quasi likelihood function , which depends on the covariance estimator in the first step .",
    "these two steps can be carried out iteratively .",
    "we also propose an alternative method for jointly estimating the factor loadings and the error covariance matrix by maximizing a weighted @xmath20 penalized likelihood function .",
    "the likelihood penalizes the estimation of the off - diagonal entries of the error covariance and automatically produces a sparse covariance estimator .",
    "we present asymptotic analysis for both methods .",
    "in particular , we derive the uniform rate of convergence and limiting distribution of the estimators for the two - step procedure .",
    "the analysis of the joint - estimation is more difficult as it involves penalizing a large covariance with diverging eigenvalues .",
    "we establish the consistency for this method .",
    "moreover , we achieve the  sparsistency \" for the estimated error covariance matrix in factor analysis ( see section 3 for detailed explanations ) .",
    "the estimated covariance is consistent for both approaches under the normalized frobenius norm even when @xmath15 is much larger than @xmath16 .",
    "this is important in the applications of approximate factor models .",
    "there has been a large literature on estimating the approximate factor model .",
    "stock and watson ( 1998 , 2002 ) and bai ( 2003 ) considered the principal components analysis ( pca ) , and they developed large - sample inferential theory .",
    "however , the pca essentially treats @xmath5 to have the same variance across @xmath21 , hence is inefficient when cross - sectional heteroskedasticity is present .",
    "choi ( 2012 ) proposed a generalized pca that requires @xmath22 to invert the error sample covariance matrix .",
    "more recently , bai and li ( 2012 ) estimated the factor loadings by maximizing the gaussian - quasi likelihood , which addresses the heteroskedasticity under large @xmath15 , but they consider the strict factor model in which @xmath23 are uncorrelated",
    ". additional literature on factor analysis includes , e.g. , bai and ng ( 2002 ) , wang ( 2009 ) , dias , pinherio and rua ( 2008 ) , breitung and tenhofen ( 2011 ) , han ( 2012 ) , etc ; most of these studies are based on the pca method .",
    "in contrast , our methods are maximum - likelihood - based .",
    "maximum likelihood methods have been one of the fundamental tools for statistical estimation and inference .",
    "our approach is closely related to the large covariance estimation literature , which has been rapidly growing in recent years .",
    "there are in general two ways to estimate a sparse covariance in the literature : thresholding and penalized maximum likelihood . for our two - step procedure ,",
    "we apply the poet estimator recently proposed by fan et al .",
    "( 2012 ) , corresponding to the thresholding approach of bickel and levina ( 2008a ) , rothman et al .",
    "( 2009 ) and cai and liu ( 2011 ) . for the joint estimation procedure",
    ", we use the penalized likelihood , corresponding to that of lam and fan ( 2009 ) , bien and tibshirani ( 2011 ) , etc . in either way , we need to show that the impact of estimating the large covariances is asymptotically negligible for an efficient estimation , which is not easy in our context since the likelihood function is highly nonlinear , and @xmath24 contains a few eigenvalues that grow very fast .",
    "it was recently shown by fan et al .",
    "( 2012 ) that estimating a covariance matrix with fast diverging eigenvalues is a challenging problem .",
    "other works on large covariance estimation include cai and zhou ( 2012 ) , fan et al .",
    "( 2008 ) , jung and marron ( 2009 ) , witten , tibshirani and hastie ( 2009 ) , deng and tsui ( 2010 ) , yuan ( 2010 ) , ledoit and wolf ( 2012 ) , el karoui ( 2008 ) , pati et al .",
    "( 2012 ) , rohde and tsybakov ( 2011 ) , zhou et al .",
    "( 2011 ) , ravikumar et al .",
    "( 2011 ) etc .",
    "this paper focuses on high - dimensional _",
    "static factor models _ although the factors and errors can be serially correlated .",
    "the model considered is different from the generalized _ dynamic factor models _ as in forni , hallin , lippi and reichlin ( 2000 ) , forni and lippi ( 2001 ) , hallin and lika ( 2007 ) , and other references therein .",
    "both static and dynamic factor models are receiving increasing attention in applications of many fields .",
    "the paper is organized as follows .",
    "section 2 introduces the conditional sparsity assumption and the likelihood function .",
    "section 3 proposes the two - step estimation procedure .",
    "in particular , we present asymptotic inferential theory of the estimators",
    ". both uniform rate of convergence and limiting distributions are derived .",
    "section 4 gives the joint estimation as an alternative procedure , where we demonstrate the estimation consistency .",
    "section 5 illustrates some numerical examples which compare the proposed methods with the existing ones in the literature .",
    "finally , section 6 concludes with further discussions .",
    "all proofs are given in the appendix .    *",
    "notation *    let @xmath25 and @xmath26 denote the maximum and minimum eigenvalues of a matrix @xmath27 respectively .",
    "also let @xmath28 , @xmath29 and @xmath30 denote the @xmath20 , spectral and frobenius norms of @xmath27 , respectively .",
    "they are defined as @xmath31 , @xmath32 , @xmath33 .",
    "note that @xmath34 is also the euclidean norm when @xmath27 is a vector . for two sequences @xmath35 and @xmath36 , we write @xmath37 , and equivalently @xmath38 , if @xmath39 as @xmath40",
    "the approximate factor model ( 1.1 ) implies the following covariance decomposition : @xmath41 assuming @xmath4 to be uncorrelated with @xmath42 , where @xmath43 and @xmath17 denote the @xmath13 covariance matrices of @xmath44 and @xmath42 ; @xmath45 denotes the @xmath46 covariance of @xmath4 , all assumed to be time - invariant .",
    "the approximate factor model typically requires the idiosyncratic covariance @xmath17 have bounded eigenvalues and @xmath47 have eigenvalues diverging at rate @xmath48 .",
    "one of the key concepts of approximate factor models is that it allows @xmath49 to be non - diagonal .",
    "stock and watson ( 1998 ) and bai ( 2003 ) derived the rates of convergence as well as the inferential theory of the method of principal component analysis ( pca ) for estimating the factors and loadings .",
    "let @xmath50 be the @xmath51 data matrix .",
    "then pca estimates the @xmath52 factor matrix @xmath53 by maximizing @xmath54 subject to normalization restrictions for @xmath53 .",
    "the pca method essentially restricts to have cross - sectional homoskedasticity and independence .",
    "thus it is known to be inefficient when the idiosyncratic errors are either cross sectionally heteroskedastic or correlated .",
    "this paper aims at the efficient estimation of the approximate factor model , and assumes the number of factors @xmath55 to be known . in practice",
    ", @xmath55 can be estimated from the data , and there has been a large literature addressing its consistent estimation , e.g. , bai and ng ( 2002 ) , kapetanios ( 2010 ) , onatski ( 2010 ) , alessi et al .",
    "( 2010 ) , hallin and lika ( 2007 ) , lam and yao ( 2012 ) , among others .",
    "an efficient estimation of the factor loadings and factors should take into account both cross - sectional dependence and heteroskedasticity , which will then involve estimating @xmath56 , or more precisely , the precision matrix @xmath57 . in a data - rich environment , @xmath15 can be either comparable with or much larger than @xmath16 . then estimating @xmath49 is a challenging problem even when the idiosyncratics @xmath12 are observable , because the sample covariance is nonsingular when @xmath58 , whose spectrum is inconsistent ( johnstone and ma 2009 ) .    under the regular approximate factor model considered by chamberlain and rothschild ( 1983 ) and stock and watson ( 2002 ) , it is difficult to estimate @xmath17 without further structural assumptions .",
    "a natural assumption to go one - step further is that of sparsity , which assumes that many off - diagonal elements of @xmath17 be either zero or vanishing as the dimensionality increases . in an approximate factor model",
    ", it is more appropriate to assume @xmath17 be a sparse matrix instead of @xmath43 . due to the presence of common factors , we call such a special structure of the factor model to be _ conditionally sparse_.",
    "therefore , the model studied in the current paper is the approximate factor model with conditional sparsity ( sparsity structure on @xmath49 ) , which is sightly more restrictive than that of chamberlain and rothschild ( 1983 ) .",
    "the conditional sparsity is required to regularize a large idiosyncratic covariance , which allows us to take both cross sectional correlation and heteroskedasticity into account , and is needed for an efficient estimation .",
    "however , such an assumption is still quite general and covers most of the applications of factor models in economics , finance , genomics , and many important applied areas .      compared to pca , a more efficient estimation for model ( 2.1 ) of high dimension is based on a gaussian quasi - likelihood approach .",
    "let @xmath59 because of the existence of @xmath60 , the model @xmath61 is observationally equivalent to @xmath62 , where @xmath63 and @xmath64 therefore without loss of generality , we assume @xmath65 . the guassian quasi - likelihood for @xmath66 is given by @xmath67 where @xmath68 is the sample covariance matrix , with @xmath69 . plugging in ( [ eq2.2 ] ) , using the notation @xmath70",
    ", we obtain the quasi - likelihood function for the factors and loadings : @xmath71 where @xmath72 is an @xmath73 matrix of factor loadings .",
    "it has been well known that the factors and loadings are not separably identified without further restrictions .",
    "note that the factors and loadings enter the likelihood through @xmath74 .",
    "hence for any invertible @xmath46 matrix @xmath75 , if we define @xmath76 , @xmath77 and @xmath78 , then @xmath79 , and they produce observationally equivalent models . in this paper , we focus on a usual restriction for mle of factor analysis ( see e.g. , lawley and maxwell 1971 ) as follows : @xmath80and the diagonal entries of @xmath81 are distinct and are arranged in a decreasing order .",
    "restriction ( [ eq2.4 ] ) guarantees a unique solution to the maximization of the log - likelihood function up to a column sign change for @xmath82 .",
    "therefore we assume the estimator @xmath83 and @xmath84 have the same column signs , as part of the identification conditions .",
    "the negative log - likelihood function ( [ eq2.3 ] ) simplifies to @xmath85 in the presence of cross sectional dependence , @xmath17 is not necessarily diagonal .",
    "therefore there can be up to @xmath86 free parameters in the likelihood function ( [ eq2.5 ] ) .",
    "there are in general two main regularization approaches to estimating a large sparse covariance : ( adaptive ) thresholding ( bickel and levina 2008a , rothman et al .",
    "2009 , cai and liu 2011 , etc . ) and penalized maximum likelihood ( lam and fan 2009 , bien and tibshirani 2011 ) .",
    "correspondingly in this paper , we propose two methods for regularizing the likelihood function to efficiently estimate the factor loadings as well as the unknown factors .",
    "one estimates @xmath17 and @xmath84 in two steps and the other estimates them jointly .",
    "the two - step estimation estimates @xmath87 separately . in the first step",
    ", we estimate @xmath17 by the principal orthogonal complement thresholding ( poet ) , proposed by fan et al .",
    "( 2012 ) , and in the second step we estimate @xmath84 only , using the quasi - maximum likelihood , replacing @xmath88 by the covariance estimator obtained in step one .      the poet is based on a spectrum expansion of the sample covariance matrix and adaptive thresholding .",
    "let @xmath89 be the eigenvalues - vectors of the sample covariance @xmath90 of @xmath44 , in a decreasing order such that @xmath91 then @xmath90 has the following spectrum decomposition : @xmath92 where @xmath93 is the _ orthogonal complement component_. define a general thresholding function @xmath94 as in rothman et al .",
    "( 2009 ) and cai and liu ( 2011 ) with an entry - dependent threshold @xmath95 such that : + ( i ) @xmath96 if @xmath97 + ( ii ) @xmath98 + ( iii ) there are constants @xmath99 and @xmath100 such that @xmath101 if @xmath102 . + examples of @xmath103 include the hard - thresholding : @xmath104 ; scad ( fan and li 2001 ) , mpc ( zhang 2010 ) etc",
    ". then we obtain the step - one consistent estimator for @xmath17 : @xmath105 we can choose the threshold as @xmath106 for some universal constant @xmath107 , which corresponds to applying the threshold @xmath108 to the correlation matrix of @xmath109 [ defined to be diag@xmath110 diag@xmath111 .",
    "the poet estimator also has an equivalent expression using pca .",
    "let @xmath112 denote the pca estimators of @xmath12 ( bai 2003 ) . then @xmath113 .",
    "it was shown by fan et al .",
    "( 2012 ) that under some regularity conditions @xmath114 , which guarantees the positive definiteness asymptotically , given that @xmath115 is bounded away from zero .",
    "replacing @xmath116 in ( [ eq2.5 ] ) by @xmath117 , we obtain the objective function for @xmath82 . under the identification condition ( [ eq2.4 ] ) , in this step , we estimate the loadings as : @xmath118 where @xmath119 is a parameter space for the loading matrix , to be defined later .",
    "suppose that @xmath120 , the negative log - likelihood is then the same ( up to a constant ) as ( [ eq3.1 ] ) except that @xmath117 should be replaced by @xmath17 .",
    "consequently , ( [ eq3.1 ] ) can be treated as a gaussian quasi - likelihood of @xmath82 , which will give an efficient estimation of @xmath84 since it takes into account the cross sectional heteroskedasticity and dependence in @xmath17 through its consistent estimator .    after obtaining @xmath121",
    ", we estimate @xmath4 via the generalized least squares ( gls ) as suggested by bai and li ( 2012 ) : @xmath122    the proposed two - step procedure can be carried out iteratively . after obtaining @xmath123",
    ", we update @xmath124 then @xmath117 in the objective function ( [ eq3.1 ] ) is updated , which gives updated @xmath121 and @xmath125 respectively .",
    "this procedure can be continued until convergence .",
    "the objective function ( [ eq3.1 ] ) requires @xmath126 be positive definite for any given finite sample .",
    "a sufficient condition is the finite - sample positive definiteness of @xmath117 , which also depends on the choice of the adaptive threshold value @xmath95 .",
    "we specify @xmath127 where @xmath128 is an entry - dependent value that captures the variability of individual variables such as @xmath129 ; @xmath107 is a pre - determined universal constant . more concretely",
    ", the finite sample positive definiteness depends on the choice of @xmath130 if we write @xmath131 in step one to indicate its dependence on the threshold , then @xmath132 should be chosen in the interval @xmath133 $ ] , where @xmath134 and @xmath135 is a large constant that thresholds all the off - diagonal elements of @xmath117 to zero . then by construction",
    ", @xmath136 is finite - sample positive definite for any @xmath137 ( see figure [ f1 ] ) .",
    ", width=377 ]    data are simulated from the setting of section 5 with @xmath138 .",
    "both hard and scad with adaptive thresholds ( cai and liu 2011 ) are plotted .",
    "[ f1 ]      we now present the asymptotic analysis of the proposed two - step estimator .",
    "we first list a set of regularity conditions and then present the consistency .",
    "a more refined set of assumptions are needed to achieve the optimal rate of convergence as well as the limiting distributions .",
    "[ ass3.1 ] let @xmath139 denote the @xmath140th entry of @xmath17 .",
    "there is @xmath141 such that @xmath142 in particular , when @xmath143 , we define @xmath144 , which corresponds to the  exactly sparse \" case .",
    "the first assumption sets a condition on the sparsity of @xmath17 , under which fan et al .",
    "( 2012 ) showed that the poet estimator @xmath117 is consistent under the operator norm .",
    "the sparsity is in terms of the maximum row sum , considered by bickel and levina ( 2008a ) .",
    "the following assumption provides the regularity conditions on the data generating process .",
    "we introduce the strong mixing condition .",
    "let @xmath145 and @xmath146 denote the @xmath147-algebras generated by @xmath148 and @xmath149 respectively .",
    "in addition , define the mixing coefficient @xmath150    [ ass3.2 ] ( i ) @xmath151 is strictly stationary .",
    "in addition , @xmath152 for all @xmath153 and @xmath154 + ( ii ) there exist constants @xmath155 such that @xmath156 and @xmath157 .",
    "+ ( iii ) exponential tail : there exist @xmath158 and @xmath159 , such that for any @xmath160 , @xmath161 and @xmath162 , @xmath163 ( iv ) strong mixing : there exists @xmath164 such that @xmath165 , and @xmath107 satisfying : for all @xmath166 , @xmath167    the following assumptions are standard in the approximate factor models , see e.g. , stock and watson ( 1998 , 2002 ) and bai ( 2003 ) . in particular , assumption [ ass3.3 ] implies that the first @xmath55 eigenvalues of @xmath24 are growing rapidly at @xmath48 .",
    "intuitively , it requires the factors be pervasive in the sense that they impact a non - vanishing proportion of time series @xmath168 .",
    "[ ass3.3 ] there is a @xmath169 such that for all large @xmath15 , @xmath170 therefore all the eigenvalues of @xmath171 are bounded away from both zero and infinity as @xmath172    [ ass3.4 ] there exists @xmath173 such that for all @xmath174 and @xmath175 , + ( i ) @xmath176 ^ 4<m$ ] , + ( ii ) @xmath177 .",
    "the following assumption defines the threshold @xmath95 on the @xmath140th entry of @xmath178 for the step - one poet estimator .",
    "[ ass3.5 ] the threshold @xmath179 where @xmath180 is entry - dependent , either stochastic or deterministic , such that @xmath181 , there are positive @xmath182 and @xmath183 so that @xmath184for all large @xmath15 and @xmath16 . here",
    "@xmath107 is a deterministic constant .",
    "condition ( [ eq3.3 ] ) requires the rate @xmath185 uniformly in @xmath140 .",
    "this condition is satisfied by the universal threshold @xmath186 for all @xmath140 , the correlation threshold @xmath187 as discussed before , and the adaptive threshold in cai and liu ( 2011 ) .    for identification",
    ", we require the objective function be minimized subject to the diagonality of @xmath188 .",
    "in addition , since assumption [ ass3.3 ] is essential in asymptotically identifying the covariance decomposition @xmath189 , we need to take it into account when minimizing the objective function .",
    "therefore we assume @xmath190 in assumption [ ass3.3 ] is sufficiently large , which leads to the following parameter space : @xmath191 write @xmath192 and @xmath193 .",
    "we have the following theorem .",
    "[ th3.1 ] suppose @xmath194 @xmath195 under assumptions [ ass3.1]-[ass3.5 ] , @xmath196    by a more careful large - sample analysis , we can improve the above result and derive the rate of convergence . throughout the paper",
    ", we will frequently use the notation : @xmath197    [ th3.2 ] under the assumptions of theorem [ th3.1 ] , @xmath198 where @xmath199 and @xmath200 are defined in assumption [ ass3.1 ] .    in the above theorem @xmath199",
    "does not need to be bounded .",
    "but in order to achieve the @xmath201-consistency for each @xmath202 , the uniform rate of convergence above would require it be bounded ( which is a strong assumption on the sparsity of @xmath17 ) . later in section 3.4.3",
    "we will enhance this convergence rate so that the boundedness of @xmath199 is not necessary and @xmath201-consistency can still be achieved .",
    "this will require additional regularity conditions .      in order to obtain the limiting distribution for each individual @xmath203",
    ", we also need to achieve the _ sparsistency _ for estimating @xmath204 by sparsistency , we mean the property that all small entries of @xmath17 are estimated as exactly zeros with a probability arbitrarily close to one .",
    "besides being important for deriving the limiting distribution of @xmath203 , the sparsistency itself is of independent interest for large covariance estimation , and has been studied by many authors , for instance , lam and fan ( 2009 ) and rothman et al .",
    "( 2009 ) . to our best knowledge , this is the first place where the sparsistency for an estimated idiosyncratic @xmath17 is achieved in a high dimensional approximate factor model .",
    "let @xmath205 and @xmath206 denote two disjoint sets and respectively include the indices of small and large elements of @xmath17 in absolute value , and @xmath207 because the diagonal elements represent the individual variances of the idiosyncratic components , we assume @xmath208 for all @xmath209 the sparsity assumes that most of the indices @xmath140 belong to @xmath205 when @xmath210 .",
    "a special case arises when @xmath49 is strictly sparse , in the sense that its elements in small magnitudes ( @xmath205 ) are exactly zero . for the banded matrix as an example , @xmath211 for some fixed @xmath212 then @xmath213 and @xmath214 .",
    "the following assumption quantifies the  small \" and  big \" entries of @xmath17 . by ",
    "small \" entries we mean those of smaller order than @xmath215 the partition @xmath216 may not be unique .",
    "our analysis suffices as long as such a partition exists .",
    "[ ass3.6 ] there is a partition @xmath216 such that @xmath208 for all @xmath217 and @xmath205 is nonempty . in addition , @xmath218    the conditional sparsity assumption requires most off - diagonal entries of @xmath17 be inside @xmath205 , hence it is reasonable to have @xmath219 in the condition .",
    "it is likely that @xmath206 only contains the diagonal elements .",
    "it then essentially corresponds to the strict factor model where @xmath17 is almost a diagonal matrix and error terms are only weakly cross - sectionally correlated .",
    "that is also a special case of assumption [ ass3.6 ] .",
    "[ th3.3 ] under assumption [ ass3.6 ] and those of theorem [ th3.2 ] , for any @xmath220 and @xmath173 , there is an integer @xmath221 such that as long as @xmath16 and @xmath222 , @xmath223    it was shown by fan et al .",
    "( 2012 ) that @xmath224 .",
    "theorem [ l3.1 ] below demonstrates a strengthened convergence rate for the averaged estimation error .    [ ass3.7 ] there is @xmath225 such that @xmath226    in addition to assumptions [ ass3.1 ] and [ ass3.6 ] , we require the following condition on the sparsity of @xmath17 , which further characterizes @xmath205 and @xmath206 :    [ ass3.8add ] the index sets @xmath205 and @xmath206 satisfy : @xmath227 and @xmath228 .",
    "assumption [ ass3.8add ] requires that the number of off - diagonal large entries of @xmath17 be of order @xmath48 , and that the absolute sum of the small entries is bounded .",
    "this assumption is satisfied , for example , if @xmath229 follows an heteroskedastic ma(@xmath230 ) process with a fixed @xmath230 , where @xmath227 and @xmath231 .",
    "it is also satisfied by banded matrices ( bickel and levina 2008b , cai and yuan 2012 ) and block - diagonal matrices with fixed block size .",
    "define an @xmath232 matrix @xmath233 .",
    "then @xmath234 implies @xmath235    the following assumption corresponds to those of pca in bai ( 2003 ) , and also extends to the non - diagonal @xmath17 .",
    "[ ass3.9 ] ( i ) @xmath236 + ( ii ) for each element @xmath237 of @xmath238 ( @xmath239 ) , + @xmath240 , + @xmath241 + ( iii ) for each element @xmath242 of @xmath243 , + @xmath244 , + @xmath245 .    under assumption [ ass3.9 ]",
    ", we can achieve the following improved rate of convergence for the averaged estimation error @xmath246 :    [ l3.1 ] under the assumptions of theorem [ th3.3 ] and assumption [ ass3.9 ] , @xmath247\\lambda_0\\|_f = o_p(m_n^2\\omega_t^{2 - 2q}).\\ ] ]    1 .   a simple application of + @xmath224 by fan et al . ( 2012 ) yields + @xmath248\\lambda_0\\|_f = o_p(m_n\\omega_t^{1-q}).$ ] in contrast",
    ", the rate we present in theorem [ l3.1 ] requires more refined asymptotic analysis .",
    "it shows that after weighted by the factor loadings , the averaged convergence rate is faster .",
    "the condition on the large - entry - set @xmath206 in assumption [ ass3.8add ] can be relaxed a bit to @xmath249 for an arbitrarily small @xmath220 , which will allow less sparse covariances .",
    "for example , suppose @xmath229 follows a cross sectional ar(@xmath250 ) process such that @xmath251 for @xmath252 and @xmath253 being independent across both @xmath21 and @xmath254 .",
    "we can then find a partition @xmath255 such that @xmath228 and @xmath249 for any @xmath256 theorems [ l3.1 ] and [ th3.5 ] below still hold .",
    "but conditions in assumption [ ass3.9 ] need to be adjusted accordingly .",
    "for example , in condition ( iii ) the normalizing constant @xmath257 in the first equation should be changed to @xmath258 , and @xmath259 in the second equation should be changed to @xmath260 . the current assumption [ ass3.9 ] , on the other hand , keeps our presentation simple .      as a result of theorem [ l3.1 ] ,",
    "the impact of estimating @xmath17 at step one is asymptotically negligible .",
    "this enables us to achieve the @xmath201-consistency and the limiting distribution of @xmath203 for each @xmath261 .",
    "we impose further assumptions .",
    "[ ass3.10 ] ( i ) @xmath262 . + for each @xmath263,@xmath264 + ( iii ) @xmath265 .",
    "[ th3.5 ] suppose @xmath266 , and @xmath267 .",
    "in addition , @xmath268 . then under the assumptions of theorem [ l3.1 ] and assumption [ ass3.10 ] , for each @xmath263 , @xmath269    we make some technical remarks regarding theorem [ th3.5 ] .    1 .",
    "the condition @xmath268 ( roughly speaking , this is @xmath270 when @xmath15 is very large and @xmath143 ) strengthens the sparsity condition of assumption [ ass3.1 ] .",
    "the required upper bound for @xmath199 is tight . roughly speaking",
    ", the estimation error of @xmath117 plays a role in the asymptotic expansion of @xmath271 only through an averaged term as in theorem [ l3.1 ] .",
    "condition @xmath268 is required for that term to be asymptotically negligible .",
    "the asymptotic normality also holds jointly for finitely many estimators . for any finite and fixed @xmath272",
    ", we have , @xmath273).\\ ] ] where @xmath274 .",
    "3 .   if assumption [ ass3.10](i ) is replaced by a uniform convergence , by assuming @xmath275 we can then improve the uniform rate of convergence in theorem [ th3.2 ] and obtain @xmath276      for the limiting distribution of @xmath125 , we make the following additional assumption :    [ ass3.11 ] there is a positive definite matrix @xmath277 such that for each @xmath278 @xmath279    for the next assumption , we define @xmath280",
    ". then @xmath281 has mean zero and covariance matrix @xmath282 .",
    "[ ass3.12 ] for any fixed @xmath174 , + ( i ) @xmath283 , + @xmath284 + @xmath285 + @xmath286 + ( ii ) for each @xmath287 , + @xmath288 + @xmath289    [ th3.6 ] under the assumptions of theorem [ th3.5 ] , we have for each fixed @xmath174 , @xmath290where @xmath158 are defined in assumption [ ass3.2 ] .",
    "if in addition assumptions [ ass3.11 ] , [ ass3.12 ] are satisfied and @xmath291 . then when @xmath292 , @xmath293    1 .",
    "it follows from theorem [ th3.6 ] that for each fixed @xmath254 , @xmath125 is a root- @xmath15 consistent estimator of @xmath4 .",
    "root- @xmath15 consistency for the estimated common factors also holds for the principal components estimator as in bai ( 2003 ) .",
    "in addition , the above limiting distribution holds only when @xmath294 2 .",
    "if we strengthen the assumption to @xmath295 , then the uniform rate of convergence can be achieved : @xmath296 to compare this rate with that of the pca estimator , we consider for simplicity , the strictly sparse case @xmath143 .",
    "then when @xmath297 and @xmath199 is either bounded or growing slowly ( @xmath298 ) , the above rate is faster than that of the pca estimator .",
    "( the above rate is @xmath299 when @xmath300 , whereas the uniform convergence rate for pca estimator is @xmath301 )",
    "one can also jointly estimate @xmath302 to take into account the cross - sectional dependence and heteroskedasticity simultaneously . as in the sparse covariance estimation literature ( e.g. , lam and fan 2009 , bien and tibshirani 2011 )",
    ", we penalize the off - diagonal elements of the error covariance estimator , and minimize the following weighted-@xmath20 penalized objective function , motivated by a penalized gaussian likelihood function : @xmath303 where @xmath304 is the parameter space for @xmath88 , to be defined later .",
    "we introduce the weighted @xmath20-penalty @xmath305 with @xmath306 to penalize the inclusion of many off - diagonal elements of @xmath307 in small magnitudes , which therefore produces a sparse estimator @xmath308 . here",
    "@xmath309 is a tuning parameter that converges to zero at a not - too - fast rate ; @xmath310 is an entry - dependent weight parameter , which can be either deterministic or stochastic .",
    "popular choices of @xmath310 in the literature include :    lasso : :    the choice @xmath311 for all @xmath210 gives    the well - known lasso penalty    @xmath312 studied by    tibshirani ( 1996 ) .",
    "the lasso penalty puts an equal weight to each    element of the idiosyncratic covariance matrix .",
    "adaptive - lasso : :    let @xmath313 be a preliminary    consistent estimator of @xmath314 .",
    "let    @xmath315 for some    @xmath316 , then    @xmath317    corresponds to the adaptive - lasso penalty proposed by zou ( 2006 ) .",
    "note    that the adaptive - lasso puts an entry - adaptive weight on each    off - diagonal element of @xmath88 , whose reciprocal is    proportional to the preliminary estimate .",
    "if the true element    @xmath318 , the weight    @xmath319 should be quite    large , and results in a heavy penalty on that entry .",
    "the preliminary    estimator @xmath313 can be taken , for    example , as the pca estimator    @xmath320 .",
    "it was shown by bai ( 2003 ) that under mild conditions ,    @xmath321 .",
    "scad : : :    fan and li ( 2001 ) proposed to use , for some @xmath322 ( e.g ,    @xmath323 ) @xmath324    the notation @xmath325 stands for the positive part of    @xmath326 ; @xmath325 is @xmath326 if    @xmath327 , zero otherwise . here    @xmath313 is still a preliminary    consistent estimator , which can be taken as the pca estimator .",
    "we assume the parameter space for @xmath17 to be , for some known sufficiently large @xmath173 , @xmath328 then @xmath329 implies that all the eigenvalues of @xmath17 are bounded away from both zero and infinity .",
    "there are many examples where both the covariance and its inverse have bounded row sums . for example , for each @xmath254 , when @xmath330 follows a cross sectional autoregressive process ar@xmath331 for some fixed @xmath230 , then the maximum row sum of @xmath49 is bounded .",
    "the inverse of @xmath49 is a banded matrix , whose maximum row sum is also bounded .",
    "as before we assume @xmath332 and @xmath333 be diagonal for identification .",
    "in addition , assumptions [ ass3.2 ] and [ ass3.3 ] for the two - step estimation are still needed .",
    "those conditions such as strong mixing , weakly dependence and bounded eigenvalues of @xmath171 regulate the data generating process , and asymptotically identify the covariance decomposition ( [ eq2.2 ] ) .",
    "the conditions for the partition @xmath334 of @xmath17 are replaced by the following , which are weaker than those of two - step estimation in assumption [ ass3.8add ] .",
    "define the number of off - diagonal large entries : @xmath335    [ ass4.1]there exists a partition @xmath216 where @xmath206 and @xmath205 are disjoint , which satisfies : + ( i ) @xmath336 for all @xmath217 , + ( ii ) @xmath337 + ( iii ) @xmath338    the following assumption is imposed on the penalty parameters .",
    "define the weights ratios @xmath339    [ ass4.2]the tuning parameter @xmath309 and the weights @xmath340 satisfy : + ( i ) @xmath341,\\ ] ] @xmath342 ( ii ) @xmath343 + @xmath344 + @xmath345    the above assumption is not as complicated as it looks , and is satisfied by many examples .",
    "for instance , the lasso penalty sets @xmath311 for all @xmath346 .",
    "hence @xmath347 then condition ( i ) of assumption [ ass4.2 ] follows from assumption [ ass4.1](ii ) , which is also satisfied if @xmath348",
    ". condition ( ii ) is also straightforward to verify .",
    "this immediately implies the following lemma .",
    "[ l4.1 ] choose @xmath311 for all @xmath349 .",
    "suppose in addition @xmath348 and @xmath350 .",
    "then assumption [ ass4.2 ] is satisfied if the tuning parameter @xmath351 is such that @xmath352    one of the attractive features of this lemma is that the condition on @xmath309 does not depend on the unknown @xmath353 we will present the adaptive lasso and scad as another two examples of the weighted-@xmath20 penalty in section 4.3 below , both satisfy the above assumption .",
    "our main theorem is stated as follows .",
    "[ th4.1 ] suppose @xmath350 . under assumptions",
    "[ ass3.2 ] , [ ass3.3 ] , [ ass3.7 ] , [ ass4.1 ] , and [ ass4.2 ] , the penalized ml estimator satisfies : as @xmath16 and @xmath354 @xmath355 for each @xmath174 , @xmath356    1 .",
    "the consistency for @xmath357 can be made uniformly in @xmath174 if the condition is strengthened to @xmath358 .",
    "2 .   to establish the consistency in the high dimensional literature",
    ", one usually constructs a neighborhood of the true parameters @xmath359 ( e.g. , rothman et al .",
    "2008 , lam and fan 2009 ) , and show that with probability approaching one , @xmath360 .",
    "this strategy however , does not work here due to the technical difficulty in dealing with the term @xmath361 in the likelihood function , because its largest @xmath55 eigenvalues are unbounded and grow at rate @xmath48 uniformly in the parameter space .",
    "one of the contributions of theorem [ th4.1 ] is to achieve consistency using a new strategy to deal with the penalized likelihood function , which involves diverging eigenvalues .    in this paper",
    "we only present the consistency for the joint estimation , which is already technically difficult as one needs to deal with an equilibrium of the first order conditions for both @xmath362 simultaneously .",
    "deriving the limiting distributions for the joint estimators is difficult , and we leave this as a future topic .",
    "we present two popular choices for the weights as examples : one is adaptive lasso , proposed by zou ( 2006 ) , and the other is scad by fan and li ( 2001 ) .",
    "both weights depend on a preliminary consistent estimate of each element of @xmath49 . in the high dimensional approximate factor model ,",
    "a simple consistent estimate for each element can be obtained by the principal component analysis ( stock and watson 1998 and bai 2003 ) .    to simplify the presentation",
    ", we will assume that @xmath348 , which controls the number of off - diagonal large entries of @xmath17 .",
    "moreover , we retain assumption [ ass3.6 ] : @xmath363 and recall that @xmath364 .",
    "let the initial estimate @xmath365 , where @xmath178 is the pca estimator of @xmath139 as in bai ( 2003 ) .",
    "the adaptive lasso chooses the weights to be , for some constant @xmath366 $ ] , @xmath367 where @xmath368 is a pre - determined nonnegative sequence .",
    "the additive @xmath369 was not included in the original definition of adaptive lasso in zou ( 2006 ) , but has often been seen in recent literature , e.g. , xue and zou ( 2012 ) .",
    "we include it here in the weights to prevent @xmath310 getting too large if @xmath370 is very close to zero .",
    "the adaptive lasso has been used extensively in the high dimensional literature , see for example , huang , ma and zhang ( 2006 ) , van de geer , bhlmann and zhou ( 2011 ) , caner and fan ( 2011 ) , etc .",
    "another important example is scad , defined as : for some @xmath322 , @xmath371    we have the following theorem .    [ th4.2 ] suppose either the adaptive lasso or scad is used for the weighted-@xmath20 penalized objective function .",
    "also , suppose @xmath372 , @xmath348 , @xmath373 and assumptions [ ass3.2 ] , [ ass3.3 ] , [ ass3.7 ] , [ ass4.1 ] hold .",
    "in addition , assume the tuning parameters are such that : + ( i ) for adaptive lasso , @xmath374 @xmath375 ( ii ) for scad : @xmath376    then assumption [ ass4.2 ] is satisfied , and @xmath377 @xmath356    as in the case of lemma [ l4.1 ] , an attractive feature of this theorem is that , if both the upper bound of @xmath378 and the lower bound of + @xmath379 are known , [ e.g. , in the strictly sparse model , + @xmath380 , and assume @xmath379 is bounded away from zero as in ma(1 ) ] then conditions ( [ e4.2 ] ) - ( [ eq4.7 ] ) do not depend on any other unknown feature of @xmath49 .",
    "we propose a novel algorithm to numerically minimize the objective function @xmath381 ( [ eq4.1addd ] ) for joint estimation , which combines the em algorithm with the majorize - minimize method recently proposed by bien and tibshirani ( 2011 ) .",
    "the algorithm uses the pca as initial values , and updates the estimator iteratively . at each iteration",
    ", an em - algorithm is carried out to estimate @xmath82 and the empirical residual covariance @xmath382 then a majorize - minimize method ( bien and tibshirani 2011 ) is used to obtain a positive definite estimate of the covariance @xmath88 based on @xmath383 and soft - thresholding .",
    "the algorithm is summarized as follows ( see bai and li ( 2012 ) and bien and tibshirani ( 2011 ) for detailed descriptions of the algorithm ) .    1",
    ".   initialize @xmath83 and @xmath384 as the pca estimators .",
    "initialize @xmath385 as a diagonal matrix of the sample covariance based on the pca residuals .",
    "2 .   at step k+1 , @xmath386 , where + @xmath387 , @xmath388 let @xmath389 3 .   still at step @xmath390 , for some small value @xmath391 , let @xmath392 . let @xmath393 where @xmath394 and @xmath395 is a matrix whose off - diagonal @xmath396 is @xmath397 and diagonal elements are zero .",
    "repeat 2 - 3 until converge .",
    "we present a numerical experiment to illustrate the performance of the proposed method .",
    "the data was generated as following : @xmath253 are both serially and cross - sectionally independent as @xmath398 .",
    "let @xmath399 @xmath400 where @xmath401 are i.i.d .",
    "let the two factors @xmath403 be i.i.d .",
    "@xmath398 , and @xmath404 be uniform on @xmath405 $ ] . then @xmath49 is a banded matrix .",
    "we apply the adaptive lasso penalty for our joint estimation , with various choices of the tuning parameters @xmath406 and @xmath309 .",
    "the result is compared with the pca estimator and the regular maximum likelihood restricted to diagonal @xmath385 ( dml , bai and li 2012 )",
    ". more specifically , dml estimates @xmath87 by : @xmath407 therefore dml forces the covariance estimator to be diagonal even though the true @xmath17 is not .",
    "hence it does not take the idiosyncratic cross - sectional dependence into account .    for each estimator , the smallest canonical correlation ( the higher the better ) between the estimator and",
    "the parameter has been used as a measurement to assess the accuracy of each estimator .",
    "tables [ table1 ] and [ table2 ] list the results of the estimated factor loadings and common factors from joint - estimation .",
    ".canonical correlations between @xmath408 and @xmath84 [ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]     [ table3 ]    _ the scad@xmath409 threshold has been used for the covariance estimation , where @xmath410 with the adaptive threshold constant @xmath128 proposed by cai and liu ( 2011 ) . _",
    "we study the estimation of a high dimensional approximate factor model in the presence of cross sectional dependence and heteroskedasticity .",
    "the classical pca method does not efficiently estimate the factor loadings or common factors because it essentially treats the idiosyncratic error to be homoskedastic and cross sectionally uncorrelated .",
    "for the efficient estimation it is essential to estimate a large error covariance matrix .",
    "we assume the model to be conditionally sparse in the sense that after the common factors are taken out , the idiosyncratic components have a sparse covariance matrix .",
    "this enables us to combine the merits of both sparsity and high dimensional factor analysis .",
    "two maximum - likelihood - based approaches are proposed to estimate the common factors and factor loadings , both involve regularizing a large covariance sparse matrix .",
    "extensive asymptotic analysis has been carried out . in particular , we develop the inferential theory for the two - step estimation .    it remains to derive the limiting distribution as well as the optimal rates of convergence for the estimators by the joint - estimation method .",
    "this will extend the consistency results obtained in the current paper . in the presence of a covariance @xmath24 that has fast - diverging eigenvalues ,",
    "the task is difficult because it requires the consistency of the penalized covariance estimator under the operator norm .",
    "we intend to address this issue in future research .",
    "we need to establish the results for two sets of estimators : the two - step estimator and the joint estimator , whose proofs for consistency share some similarities .",
    "therefore in this section we establish some preliminary results for generic estimators that can be used for both cases .",
    "we denote by @xmath411 as a generic estimator for @xmath302 , which can be either @xmath412 or @xmath413 .",
    "define @xmath414 @xmath415 define the set @xmath416    we first present a lemma that will be needed throughout the proof .",
    "[ la.1 ] ( i ) @xmath417 .",
    "+ ( ii ) @xmath418 .",
    "+ ( iii ) @xmath419 .",
    "see lemmas a.3 and b.1 in fan , liao and mincheva ( 2011 ) .",
    "[ la.2 ] under assumption 3.2 , for any @xmath169 , @xmath420 therefore we can write @xmath421    first of all , note that @xmath422 , and @xmath423 hence we have @xmath424 where @xmath425 is uniform in @xmath426 .",
    "equation ( [ eqa.2 ] ) will be used later in the proof .",
    "we now consider the term @xmath427 . with the identification condition",
    "@xmath428 @xmath429 and @xmath430 , @xmath431 by the matrix inversion formula @xmath432 , @xmath433 where @xmath434 @xmath435 , @xmath436 and @xmath437 term @xmath438 uniformly in the parameter space , and hence can be ignored .",
    "let us look at terms @xmath439 and @xmath440 subsequently .",
    "note that @xmath441 and @xmath442 are both bounded from above uniformly in @xmath426 , we have , @xmath443\\leq\\sup_{(\\lambda,\\sigma_u)\\in\\xi_{\\delta}}\\frac{\\lambda_{\\max}(\\sigma_u)}{\\lambda_{\\min}(\\lambda'\\lambda)}=o(n^{-1}),\\ ] ] @xmath444\\leq\\sup_{(\\lambda,\\sigma_u)\\in\\xi_{\\delta}}\\lambda_{\\max}[(\\lambda'\\sigma_u^{-1}\\lambda)^{-1}]=o(n^{-1}).\\ ] ] in addition , @xmath445 , @xmath446 uniformly in @xmath426 , and @xmath447 . applying the matrix inversion formula yields @xmath448 where @xmath425 is uniform over @xmath449 . in the second equality above we applied ( [ eqa.4 ] ) and ( [ eqa.5 ] ) and the following inequality : @xmath450 \\lambda_{\\max}[(i_r+\\lambda'\\sigma_u^{-1}\\lambda)^{-1}]\\cr & & \\leq o(n^{-3})\\|\\lambda_0\\|_f^2\\|\\lambda\\|_f^2\\lambda_{\\max}(\\sigma_u^{-1})=o(n^{-1}).\\end{aligned}\\ ] ] by lemma [ la.1](iii ) , and @xmath451 uniformly in @xmath426 , @xmath452 similarly , @xmath453 again by the matrix inversion formula , @xmath454 the second term on the right hand side is of smaller order ( uniformly ) than the first term ,",
    "because it has an additional term @xmath455 , whose maximum eigenvalue is @xmath456 uniformly by ( [ eqa.5 ] ) .",
    "the first term is bounded by ( uniformly in @xmath426 ): @xmath457 hence @xmath458 results ( [ eqa.2 ] ) and ( [ eqa.3 ] ) then yield @xmath459    throughout the proofs , we note that the consistency depends crucially on the consistency of the following quantities : @xmath460 we state the following lemma for the generic estimators .",
    "[ assa.1 ] ( i ) @xmath461 + ( ii ) first order condition : @xmath462    we will prove lemma [ assa.1 ] for both @xmath412 and @xmath413 later when we deal with these two estimators individually .",
    "[ la.3 ] suppose lemma [ assa.1 ] holds , then + ( i ) @xmath463 + ( ii ) @xmath464",
    "\\(i ) using the matrix inverse formula , the same argument of bai and li ( 2012 ) s ( a.2 ) implies @xmath465 .",
    "thus part ( i ) follows from the first order condition in lemma [ assa.1 ] .",
    "\\(ii ) let @xmath466 . part ( i ) can be equivalently written as @xmath467 where @xmath468 @xmath469 note that for @xmath470 , @xmath471 , @xmath472 for each element , @xmath473 , @xmath474 , hence @xmath475 moreover , for the empirical covariance @xmath476 by lemma [ la.1 ] , which implies @xmath477 .",
    "also , @xmath478 .",
    "therefore @xmath479 it then implies ( ii ) .",
    "[ la.4 ] suppose lemma [ assa.1 ] holds , then @xmath480 .    by our assumption",
    ", both @xmath481 and @xmath482 are diagonal .",
    "moreover , the eigenvalues of @xmath483 and @xmath484 are bounded away from zero",
    ". therefore by lemma [ assa.1](i ) and lemma [ la.3](ii ) , there are two diagonal matrices @xmath485 and @xmath486 whose eigenvalues are all bounded away from zero , such that @xmath487 applying lemma a.1 of bai and li ( 2012 ) , we have @xmath480 and @xmath488 . we also assumed @xmath83 and @xmath84 have the same column signs , as a part of identification condition .",
    "in this section , @xmath489 and @xmath490 throughout appendix b , we will let @xmath491 . for notational simplicity , we let @xmath197    we first cite a result from fan et al . ( 2012 ) :    [ thb.1]suppose @xmath492 and @xmath493 , then under assumptions 3.1- 3.5 , @xmath494    the sufficient conditions of this theorem are satisfied by our assumptions .",
    "see fan et al .",
    "( 2012 ) .",
    "we then prove lemma [ assa.1 ] , which then enables us to apply lemmas [ la.3 ] and [ la.4 ] . under assumptions",
    "3.1- 3.3 , there is @xmath169 such that @xmath495 and @xmath496 with probability approaching one for @xmath426 in appendix a.    [ lb.1 ] for @xmath489 , lemma [ assa.1 ] is satisfied .",
    "the first order condition with respect to @xmath121 in ( ii ) is easy to verify , which is the same as that in bai and li ( 2012 ) .",
    "we only show part ( i ) .    by definition , @xmath497 .",
    "also the representation defined in lemma [ la.2 ] yields @xmath498 thus @xmath499 note that @xmath500 is always nonnegative and @xmath501 .",
    "therefore by lemma [ la.2 ] , @xmath502 .",
    "moreover , the matrix in the trace operation of @xmath500 is semi - positive definite , hence @xmath503 it remains to show that @xmath504 , which follows immediately from theorem [ thb.1 ] and that @xmath505 .",
    "the equality ( [ eqb.1 ] ) implies @xmath506 the second term is bounded by @xmath507 .",
    "lemma [ la.4 ] then implies the second term is @xmath508 , which then implies that the first term is @xmath508 . because @xmath509 has eigenvalues bounded away from zero asymptotically , we have @xmath510 .",
    "lemma [ la.3 ] ( i ) can be equivalently written as : for any @xmath263 , @xmath511 where @xmath512 denotes the @xmath261th column of @xmath117 , and @xmath513 is an @xmath514 vector @xmath515 the consistency of @xmath516 follows from lemma [ la.4 ] and the following lemma [ lb.2 ] .",
    "[ lb.2]@xmath517    by lemma [ la.1 ] , uniformly in @xmath263 , @xmath518 @xmath519 @xmath520 finally , @xmath521 .",
    "the result then follows from a triangular inequality and that @xmath505 .        by ( [ lb.2 ] )",
    ", the uniform rate of convergence follows from lemma [ lb.2 ] and the following lemma [ lb.3 ] .",
    "[ lb.3 ] @xmath522 .",
    "the first order condition in lemma [ la.3 ] ( i ) is equivalent to : @xmath523 where @xmath524 we have , @xmath525 , @xmath526 , and @xmath527 therefore @xmath528 since @xmath480 , @xmath529 can be ignored .",
    "it follows from ( [ eqb.2 ] ) that @xmath530 let @xmath531 denote the @xmath140the entry of @xmath532 .",
    "it then follows that @xmath533 for all @xmath534 it is also not hard to verify that @xmath535 for any @xmath536 since @xmath537 .    on the other hand , due to the identification condition ,",
    "both @xmath538 and @xmath539 are diagonal .",
    "let ndg@xmath540 denote the off - diagonal elements of @xmath541 .",
    "then ndg@xmath542ndg@xmath543 is equivalent to @xmath544 @xmath545 note that if @xmath546 then @xmath547 for two matrices @xmath485 and @xmath486 since @xmath548 is diagonal . also , @xmath549 .",
    "the above identification condition implies @xmath550 note that @xmath551 let @xmath552 denote the @xmath21th diagonal entry of @xmath548 .",
    "let @xmath553 .",
    "then for @xmath210 , ( [ eqb.3 ] ) and ( [ eqb.4 ] ) imply that @xmath554 @xmath555by assumption , with probability one , there is @xmath169 such that @xmath556 , and @xmath557 for @xmath558 moreover , since all the eigenvalues of @xmath385 are bounded away from zero and infinity , wpa1 , @xmath559 for some @xmath560 then the above two equations imply that for any @xmath210 , @xmath561 ( since @xmath562 ) . then @xmath563 moreover , by lemma [ lb.2 ] , @xmath564 .",
    "we now show that @xmath522 .",
    "suppose this does not hold , then ( [ eqb.6addd ] ) implies @xmath565 . by the definition @xmath566 @xmath567 .",
    "therefore @xmath565 yields @xmath568",
    ". the first order condition ( [ eqb.2foc ] ) also yields @xmath569 which implies @xmath570 .",
    "therefore @xmath571 which contradicts with the consistency @xmath510 .",
    "this concludes the proof .",
    "therefore , ( [ eqb.2foc ] ) gives @xmath572 .",
    "the rate of convergence for @xmath573 then follows immediately since it is bounded by @xmath516 .      by the definition of the covariance estimator in the first step , @xmath574 , where @xmath575 is a chosen thresholding function .",
    "it was shown by fan et al .",
    "( 2012 , theorem 2.1 ) that @xmath178 is the pca estimator of @xmath576 , that is , @xmath577 .",
    "[ lb.4 ] for any @xmath220 , and any constant @xmath173 , for all large enough @xmath578 , @xmath579    we have , @xmath580 .",
    "thus for all large enough @xmath581 , @xmath582 where in the second and last inequalities we used the assumption that @xmath583 and the fact that @xmath584 .",
    "* proof of theorem 3.3 *    by fan et al .",
    "( 2012 ) , @xmath585 , which implies for any @xmath220 , there is @xmath107 such that @xmath586 .",
    "for some universal @xmath173 , we set the threshold @xmath587 at entry @xmath140 , where @xmath128 is a data - dependent value that satisfies , for any @xmath220 , there is @xmath588 such that @xmath589 then as long as the constant @xmath541 in the definition of the threshold is larger than @xmath590 , @xmath591 note also that if @xmath592 , then @xmath593 , by the definition of @xmath575 .",
    "this implies , @xmath594 @xmath595 since @xmath596 by assumption , for all large @xmath597 @xmath598 on the other hand , for arbitrarily small @xmath220 , @xmath599 for some @xmath600 , which implies @xmath601 by the definition of @xmath575 , @xmath602 for all @xmath603 .",
    "therefore @xmath604 , hence for arbitrarily large @xmath173 , @xmath605 @xmath606 where the last inequality follows from lemma [ lb.4 ] .",
    "a simple derivation implies that @xmath607 .",
    "this rate is not tight enough for the @xmath201-consistency and limiting distribution @xmath203 .",
    "a more refined rate of @xmath608 depends on the convergence properties of the pca estimator .",
    "we begin by citing some results proved by fan et al .",
    "recall that @xmath178 denotes the @xmath140th entry of the orthogonal complement covariance in the sample covariance s spectrum decomposition , and @xmath609    let @xmath610 be the pca estimates of @xmath12 .",
    "let @xmath611 and @xmath612 denote the pca estimators of the factor loadings and factors .",
    "[ lb.5 ] ( i ) for any @xmath613 , with probability one @xmath614 , + ( ii ) @xmath615 + ( iii ) there is a nonsingular matrix @xmath75 such that @xmath616 and @xmath617 + ( iv ) @xmath618    see theorem 2.1 and lemma c.11 of fan et al .",
    "( 2012 ) .",
    "[ lb.5add ] @xmath619 .    by bai ( 2003 )",
    ", there are two @xmath46 matrices @xmath75 and @xmath620 , @xmath621 , @xmath622 such that @xmath623 $ ] .",
    "the desired result then follows from the following lemma [ lb.51add ] .",
    "[ lb.51add ] ( i ) @xmath624 + ( ii ) @xmath625 + ( iii ) @xmath626 .",
    "\\(i ) we have , @xmath627 we bound @xmath628 separately . here",
    "@xmath629 is upper bounded by @xmath630 , where by cauchy - schwarz , @xmath631 note that @xmath632 , which is @xmath633 by assumption 3.9 .",
    "hence @xmath634 .",
    "@xmath635 since @xmath636 by the strong mixing condition ( lemma c.5 of fan liao and mincheva 2012 ) , we have @xmath637 .",
    "this implies @xmath638 .",
    "now we bound @xmath639 . using cauchy schwarz inequality , we have @xmath640 where @xmath641 where the second inequality follows from @xmath642 .",
    "using cauchy - schwarz inequality , we also obtain @xmath643 ( ii ) let @xmath237 be the @xmath644th element of @xmath238",
    ". then the @xmath644th element of the object of interest is bounded by @xmath645 , where , by cauchy schwarz inequality , @xmath646 the last equality follows from assumption 3.9 . also ,",
    "thus @xmath648 ( iii ) the object of interest is bounded by @xmath649 , where @xmath650 and we used the fact that @xmath651 from lemma [ lb.5 ] , and that @xmath652 , whose expectation is @xmath653 . note that @xmath654 uniformly in @xmath217 . ]",
    "@xmath655    [ lb.81add ] for @xmath206 in the partition @xmath656 , + ( i ) @xmath657 + ( ii ) @xmath658 + ( iii ) @xmath659 .",
    "\\(i ) the term of interest is bounded by @xmath660 , where @xmath661 here @xmath629 is upper bounded by @xmath630 , where + @xmath662 , and + @xmath663 note that @xmath664 and @xmath665 can be bounded in the same way as ( [ eqb.7add ] ) and ( [ eqb.8add ] ) .",
    "the only difference is that @xmath666 is replaced by a double sum @xmath667 . by the assumption , @xmath668 .",
    "the result of the proof is exactly the same , so is omitted .",
    "we conclude that @xmath638 .    on the other hand , @xmath640 where + @xmath669 , and + @xmath670 .",
    "using cauchy - schwarz inequality and the strong mixing condition , @xmath671 and @xmath672 can be also bounded in an exactly the same way of ( [ eqb.9add ] ) and ( [ eqb.10add ] ) .",
    "we conclude that @xmath673 .",
    "\\(ii ) let @xmath674 be the @xmath644th element of @xmath243 .",
    "then the @xmath644th element of the object of interest is bounded by @xmath645 , where + @xmath675 , and + @xmath676 .",
    "bounding @xmath677 is slightly different from ( [ eqb.11add ] ) and ( [ eqb.12add ] ) , and we give the detail here . by cauchy schwarz inequality ,",
    "@xmath678 which is @xmath679 by assumption 3.9 . on the other hand , + @xmath680 .",
    "note that @xmath681 , where @xmath199 is as defined in assumption 3.1 .",
    "thus @xmath682 .",
    "\\(iii ) the object of interest is bounded by @xmath649 , where + @xmath683 + @xmath684 .    since @xmath685",
    ", we conclude that @xmath686 , and @xmath687 .    from lemma [ lb.81add ]",
    ", immediately we have the following result .",
    "[ lb.9add ] @xmath688 .",
    "note that results ( i)(ii)(iii ) in lemma [ lb.81add ] sum up to @xmath689 .",
    "hence lemma [ lb.9add ] follows from the equality @xmath623 $ ] .",
    "the following lemma strengthens the results of bai ( 2003 ) when @xmath17 is sparse .",
    "[ lb.6 ] for the pca estimator , + ( i ) @xmath690 + ( ii ) @xmath691 .",
    "\\(i ) @xmath692 . by assumption 3.9 ,",
    "@xmath693 on the other hand , @xmath694 is equal to @xmath695 the first term on the right hand side is @xmath696 .",
    "we now work on the second term . by bai ( 2003 )",
    ", there is a nonsingular matrix @xmath75 such that @xmath697 by lemma [ lb.5add ] @xmath698 in addition , for each element @xmath237 of @xmath238 , @xmath699 which is @xmath700 also , @xmath701",
    "@xmath702 ^ 2\\right)^{1/2}=o_p(\\frac{\\omega_t}{\\sqrt{t}}+\\frac{\\omega_t}{\\sqrt{n}}).\\ ] ]    \\(ii ) since @xmath614 , the term of interest equals @xmath703 @xmath704 by assumption 3.9 , the third term is @xmath679 . by the assumption that @xmath227 and cauchy schwarz inequality , the second term is @xmath696 .",
    "we now work out the first term .",
    "again we use the equality @xmath705 .",
    "lemma [ lb.9add ] gives @xmath706 on the other hand , @xmath707 is bounded by , @xmath708 since @xmath709 .",
    "also , @xmath710 is bounded by @xmath711 ^ 2\\right)^{1/2}\\ ] ] which is @xmath712 .",
    "* proof of theorem 3.4 @xmath713 *    by the triangular inequality , the left - hand - side is bounded by @xmath714 the first term is @xmath715 .",
    "we now bound the second term , which is @xmath716 where @xmath717",
    "the first term on the right hand side is @xmath696 by lemma [ lb.6 ] .",
    "the third term is dominated by , @xmath718 by theorem 3.3 , for any @xmath220 and any @xmath173 , @xmath719 this implies the third term is @xmath720 the second term equals @xmath721 by lemma [ lb.6 ] ( ii ) , @xmath722 on the other hand , recall that @xmath101 when @xmath102 ( section 3.1 ) , @xmath723 write @xmath724 , then for any @xmath107 , and @xmath220 , lemma [ lb.4 ] implies @xmath725 , which yields @xmath726 .",
    "therefore @xmath727 .",
    "this implies @xmath728 .",
    "we now improve the rate in lemma [ lb.3 ] .",
    "[ lb.8 ] ( i ) @xmath729{({{\\widehat\\sigma}_u^{(1)}})^{-1}}{{\\widehat \\lambda}^{(1)}}h = o_p(m_nt^{-1/2}(\\log n)^{1/2}\\omega_t^{1-q}).$ ] + ( ii ) @xmath730    \\(i ) by theorem 3.2 , @xmath731 therefore the rhs of part ( i ) equals @xmath732{\\sigma_{u0}^{-1}}\\lambda_0 h+o_p(m_n\\sqrt{\\frac{\\log n}{t}}\\omega_t^{1-q}).\\ ] ] now it follows from assumption 3.10 that @xmath733 which then yields the desired result .",
    "\\(ii ) recall that @xmath734 and that    @xmath735 . by theorem [ thb.1 ]",
    ", the rhs of ( ii ) equals @xmath736 by assumption 3.10 ( note that @xmath471 ) , @xmath737    [ lb.9 ] @xmath738 .    by ( [ eqb.2 ] ) and lemma [ lb.8 ] , ignoring the smaller order @xmath529 , we have @xmath739 this implies that @xmath740    moreover , since @xmath741 , ( [ eqb.4 ] ) and theorem 3.4 imply @xmath742 .",
    "therefore for @xmath210 , @xmath743 the desired result follows immediately .",
    "[ lb.10 ] ( i ) @xmath744 .",
    "+ ( ii ) @xmath745    \\(i ) we have , @xmath735 . hence @xmath746 hence part ( i ) equals @xmath747 where @xmath748 is uniform in @xmath749 by assumption 3.10 , for each @xmath263 , @xmath750    \\(ii ) we have @xmath751 . hence ( ii ) equals @xmath752 by assumption 3.10 , the first term equals @xmath753 , which yields the desired result .",
    "[ lb.11 ] for each fixed @xmath263 , @xmath754    note that those two terms in lemma [ lb.10 ] ( i ) ( ii ) are dominated by @xmath755 .",
    "therefore , the desired expansion follows from the first order condition ( [ eqb.2foc ] ) and lemma [ lb.9 ] .      by lemma",
    "[ lb.11 ] , and ( [ eqb.6add ] ) @xmath756 by the assumption that @xmath268 , we have @xmath757 .",
    "the limiting distribution follows since @xmath758      for any @xmath174 , @xmath759 .",
    "hence @xmath760      since both @xmath4 and @xmath42 have exponential tails , using bonferroni s method we have , @xmath761 and @xmath762 .",
    "thus by lemma [ lb.9 ] , @xmath763 . the term with @xmath764 in ( [ eqb.9 ] ) is of smaller order hence is negligible .",
    "also @xmath765 , where we used @xmath735 .",
    "hence @xmath766 @xmath767 finally , because @xmath768 , whose eigenvalues are bounded .",
    "hence @xmath769 .",
    "also , @xmath770 is of smaller order than + @xmath771 .",
    "this implies @xmath772 the above proof also shows that the rate can be made uniform if @xmath773      recall that @xmath774 and @xmath280 .",
    "[ lb.12 ] for any fixed @xmath174 , @xmath775 .",
    "we expand @xmath776 using the first order condition @xmath777\\ ] ] and investigate each term separately .",
    "first of all , since @xmath738 , and by assumption that @xmath778 , we have @xmath779 second , by the assumption that @xmath780 , we have @xmath781 third , @xmath782 moreover , + @xmath783 .",
    "therefore , by the assumption that @xmath784 , we have , @xmath785 finally , @xmath786    [ lb.13 ] for any fixed @xmath174 , @xmath787    we note that , @xmath788 . on the other hand , @xmath789 the result of",
    "the proof is very similar to that of lemmas [ lb.6 ] and theorem l3.1 , based on the expansion ( [ lb.6 ] ) and theorem 3.3 , hence is omitted .",
    "* proof of asymptotic normality *    we now fix @xmath254 , then lemma [ lb.9 ] gives @xmath790 hence @xmath791 is negligible as @xmath792 moreover , @xmath793 is of smaller order of @xmath794 , hence is negligible .",
    "next , @xmath795 @xmath796 where we used @xmath797 . by lemmas",
    "[ lb.12 ] and [ lb.13 ] , @xmath798 .",
    "this implies , for each fixed @xmath254 , @xmath799 the asymptotic normality then follows from the fact that @xmath800",
    "let @xmath820 , @xmath821 . for any @xmath88 , let @xmath822 . define a function @xmath823 , @xmath824 then @xmath825 @xmath826 and @xmath827 by the integral remainder taylor expansion , @xmath828 .",
    "we now calculate @xmath829 and @xmath830 . using the matrix differentiation formula",
    ", we have , @xmath831 which implies , @xmath832 note that both @xmath833 and @xmath834 are bounded from above for @xmath835 . by lemma [ la.1](ii ) , @xmath836 therefore , @xmath837 .",
    "in addition , @xmath838 where @xmath839 dentoes the vectorization operator and @xmath840 denotes the kronecker product . since",
    "both @xmath841 and @xmath842 are inside @xmath843 , @xmath844 is bounded from above , which then implies @xmath845=\\inf_{0\\leq t\\leq",
    "1}\\lambda_{\\max}^{-1}(t{({{\\widehat\\sigma}_u^{(2)}})^{-1}}+(1-t)\\sigma_{u0}^{-1})$ ] is bounded below by a positive constant @xmath846 .",
    "hence @xmath847 from ( [ ea.9 ] ) and @xmath848 , we have @xmath849 since @xmath850 , and @xmath851 .",
    "it follows that @xmath852-{\\mu_t }   \\sum_{(i , j)\\in s_l } w_{ij}|\\sigma_{u0 , ij}|\\cr & \\geq & ( \\mu_t\\min_{(i , j)\\in s_l}w_{ij}-o_p(\\sqrt{\\frac{\\log n}{t}}))\\sum_{(i , j)\\in s_l}|{\\widehat\\sigma}_{u , ij}-\\sigma_{u0 , ij}|+c\\|\\delta\\|_f^2\\cr & & -2{\\mu_t}\\sum_{(i , j)\\in s_l } w_{ij}|\\sigma_{u0 , ij}|-o_p(\\sqrt{\\frac{\\log n}{t}})\\sum_{\\sigma_{u0,ij}\\in s_u}|\\sigma_{u0 , ij}-{\\widehat\\sigma}_{u , ij}|\\cr & & -\\mu_t\\max_{i\\neq j , ( i , j)\\in s_u}w_{ij}\\sum_{i\\neq j , ( i , j)\\in s_u } |\\sigma_{u0 , ij}-{\\widehat\\sigma}_{u , ij}|\\cr & \\geq&\\frac{1}{2}\\mu_t\\min_{(i , j)\\in s_l}w_{ij}\\sum_{(i , j)\\in s_l}|{\\widehat\\sigma}_{u , ij}-\\sigma_{u0 , ij}|+c\\|\\delta\\|_f^2 - 2\\mu_t\\max_{(i , j)\\in s_l}w_{ij}k_t\\cr & & -o_p(\\sqrt{\\frac{\\log n}{t}})\\sqrt{n+d}\\|\\delta\\|_f-\\mu_t\\max_{i\\neq j , ( i , j)\\in s_u}w_{ij}\\|\\delta\\|_f\\sqrt{d},\\end{aligned}\\ ] ] which implies the desired result .",
    "let @xmath865 , @xmath866 , and @xmath867 .",
    "since the @xmath20 norms of @xmath868 and @xmath57 are bounded away from infinity , we have , @xmath869 and @xmath870",
    ". then @xmath871 the first term on the right hand side is @xmath508 by lemma [ lc.4 ] , and the second is bounded by @xmath872 ( using cauchy - schwarz inequality ) , which is also @xmath508 by lemma [ lc.3 ] and assumption 4.2 .",
    "we first show part ( i ) of lemma [ assa.1 ] .",
    "since @xmath874 , and @xmath875 , there is a nonnegative sequence @xmath876 such that @xmath877 lemma [ lc.2 ] then implies @xmath878 . on the other hand ,",
    "@xmath879.\\]]the matrix in the bracket is semi - positive definite .",
    "hence @xmath880 finally , the desired result follows from lemma [ lc.5 ] .",
    "@xmath881 follows from lemma [ lc.3 ] and assumption 4.2 . on the other hand , equation ( [ eqc.2 ] )",
    "also implies @xmath882 by lemma [ la.4 ] , @xmath883 .",
    "hence @xmath884 , which implies the consistency @xmath885 because the eigenvalues of @xmath886 are bounded away from zero .",
    "q.e.d .    to prove the consistency of @xmath357",
    ", we note that the expansion ( [ eqb.9 ] ) still holds for @xmath357 .",
    "since @xmath480 by lemma [ la.4 ] , and @xmath764 is of smaller order than @xmath42 for each fixed @xmath254 .",
    "hence @xmath887 moreover , since @xmath888 and @xmath889 are both @xmath890 and @xmath891 by the restriction of the parameter space @xmath843 , we have @xmath892 , which is @xmath508 as proved above .",
    "therefore , since @xmath893 , @xmath894          by lemma [ lb.5 ] @xmath899 given this result and the assumption that @xmath900 , we have result ( i ) . for any @xmath901 ,",
    "the following inequality holds : @xmath902 which then implies results ( ii ) and ( iii ) , due to the assumptions that @xmath903 , and @xmath904      it follows from the previous lemma that @xmath905 and @xmath906 . by the assumption that @xmath348 , @xmath907 hence @xmath908",
    "this together with the lower bound assumption on @xmath369 yields assumption 4.2 ( i ) .      by lemma [ lc.7](i ) and",
    "the assumptions that @xmath348 and @xmath914 we have @xmath915 due to the upper bound on @xmath916 finally , by lemma [ lc.7](iii ) and the assumption that @xmath917 , we have @xmath918                                                    van de geer , s. , bhlmann , p. and zhou , s. ( 2011 ) . the adaptive and the thresholded lasso for potentially misspecified models ( and a lower bound for the lasso ) .",
    "_ electronic journal of statistics_. * 5 * , 688 - 749 .",
    "ravikumar , p. , wainwright , m. , raskutti , g. and yu , b. ( 2011 ) , high - dimensional covariance estimation by minimizing @xmath20-penalized log - determinant divergence , _ electronic journal of statistics . _ * 5 * 935 - 980 .",
    "witten , d.m .",
    ", tibshirani , r. and hastie , t. ( 2009 ) . a penalized matrix decomposition , with applications to sparse principal components and canonical correlation analysis . _ biostatistics _ , * 10 * , 515 - 534 ."
  ],
  "abstract_text": [
    "<S> we study the estimation of a high dimensional approximate factor model in the presence of both cross sectional dependence and heteroskedasticity . </S>",
    "<S> the classical method of principal components analysis ( pca ) does not efficiently estimate the factor loadings or common factors because it essentially treats the idiosyncratic error to be homoskedastic and cross sectionally uncorrelated . </S>",
    "<S> for efficient estimation it is essential to estimate a large error covariance matrix . </S>",
    "<S> we assume the model to be conditionally sparse , and propose two approaches to estimating the common factors and factor loadings ; both are based on maximizing a gaussian quasi - likelihood and involve regularizing a large covariance sparse matrix . in the first approach </S>",
    "<S> the factor loadings and the error covariance are estimated separately while in the second approach they are estimated jointly . </S>",
    "<S> extensive asymptotic analysis has been carried out . </S>",
    "<S> in particular , we develop the inferential theory for the two - step estimation . </S>",
    "<S> because the proposed approaches take into account the large error covariance matrix , they produce more efficient estimators than the classical pca methods or methods based on a strict factor model .    </S>",
    "<S> * keywords : * high dimensionality , unknown factors , principal components , sparse matrix , conditional sparse , thresholding , cross - sectional correlation , penalized maximum likelihood , adaptive lasso , heteroskedasticity </S>"
  ]
}