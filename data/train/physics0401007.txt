{
  "article_text": [
    "most data in contemporary science are the product of increasingly complex computations and procedures applied on the fast increasing flows of raw information coming from more and more sophisticated measurement devices ( the `` measurements '' ) , or from growingly detailed numeric simulations - e.g. pattern recognition , calibration , selection , data mining , noise reduction , filtering , estimation of parameters etc .",
    "high energy physics and many other sciences are increasingly cpu and data intensive .",
    "in fact , many new problems can only be addressed at the high data volume frontier . in this context , not only data analysis transformations , but also the detailed log of how those transformations were applied , become a vital intellectual resource of the scientific community .",
    "the collaborative processes of these ever - larger groups require new approaches and tools enabling the efficient sharing of knowledge and data across a geographically distributed and diverse environment .    here",
    "is where the concept of virtual data is bound to play a central role in the scientific analysis process .",
    "we will explore this concept using as a case study the coming generation of high energy physics ( hep ) experiments at the large hadron collider ( lhc ) , under construction at the european laboratory for particle physics cern close to geneva , switzerland .",
    "this choice is motivated by the unprecedented amount of data ( from petabytes to exabytes ) and the scale of the collaborations that will analyze it ( four worldwide collaborations , the biggest two with more than two thousand scientists each ) . at the same time , the problems to be solved are general and will promote scientific discoveries in different disciplines , enhance business processes and improve security .",
    "the challenge facing hep is a major driving force for new developments in computing , e.g. the grid  @xcite .",
    "the computing landscape today is marked by the rise of grid  @xcite and web services  @xcite and service oriented architectures ( soa )  @xcite .",
    "an event - driven soa is well suited for data analysis and very adaptable to evolution and change over time .",
    "in our project we will explore and adopt service oriented solutions as they mature and provide the performance needed to meet mission - critical requirements .",
    "this paper is organized as follows : in the next section we introduce the concept of virtual data , than we discuss the issues arising when dealing with data equivalence , describe how data analysis is done in hep , digress with a metaphor , elucidate the ideas driving the caves project , continue with a detailed treatment of the caves architecture , sketch the first implementation , discuss the relationship with other grid projects , and conclude with an outlook .",
    "the scientific analysis process demands the precise tracking of how data products are to be derived , in order to be able to create and/or recreate them on demand . in this",
    "context virtual data are data products with a well defined method of production or reproduction .",
    "the concept of `` virtuality '' with respect to existence means that we can define data products that may be produced in the future , as well as record the `` history '' of products that exist now or have existed at some point in the past .",
    "the virtual data paradigm logs data provenance by tracking how new data is derived from transformations on other data  @xcite .",
    "_ data provenance _ is the exact history of any existing ( or virtual ) data product",
    ". often the data products are large datasets , and the management of dataset transformations is critical to the scientific analysis process .",
    "we need a `` virtual data management '' tool that can `` re - materialize '' data products that were deleted , generate data products that were defined but never created , regenerate data when data dependencies or algorithms change , and/or create replicas at remote locations when recreation is more efficient than data transfer .    from the scientist s point of view , data trackability and result auditability are crucial , as the reproducibility of results is fundamental to the nature of science . to support this need we require and envision something like a `` virtual logbook '' that provides the following capabilities :    * easy sharing of tools and data to facilitate collaboration - all data comes complete with a `` recipe '' on how to produce or reproduce it",
    "; * individuals can discover in a fast and well defined way other scientists work and build from it ; * different teams can work in a modular , semi - autonomous fashion ; they can reuse previous data / code / results or entire analysis chains ; * on a higher level , systems can be designed for workflow management and performance optimization , including the tedious processes of staging in data from a remote site or recreating it locally on demand ( transparency with respect to location and existence of the data ) .",
    "if we delete or accidentally loose a piece of data , having a log of how it came into existence will come in handy . immediately",
    "the question arises : is the `` new '' chunk of data after reproduction identical to the `` old '' one ?",
    "there are two extreme answers to this question :    * the two pieces of data are identical bitwise - we are done ; * not only are the two pieces of data not identical bitwise , but they contain different information from the viewpoint of the application using them .",
    "the second point needs discussion : clearly two chunks of data can be `` identical enough '' for some types of applications and different for other types .",
    "each application has to define some `` distance '' measure between chunks of data and specify some `` minimal '' distance between chunks below which the pieces are considered identical . in this language bitwise sameness",
    "would correspond to zero distance .",
    "let us illustrate this with two examples .",
    "in an ideal world , if we generate events with the monte carlo method , starting from the same seeds and using portable random number generators , we should get the same sequence of events everywhere . or if we do monte carlo integration , we should get exactly the same result . in practice , due to floating point",
    "rounding errors , even on systems with processors with the same word length simulations tend to go down different branches , diverging pretty soon .",
    "so the results are not guaranteed to be identical bitwise .",
    "usually this is not a problem : two monte carlo integrations within the statistical uncertainty are certainly acceptable . and",
    "even two different sequences of events , when their attributes are statistically equivalent ( e.g. histograms of all variables , correlations etc . ) , are good enough for many practical purposes .",
    "there are exceptions though : if our code crashes at event 10583 , we would like to be able to reproduce it bitwise .",
    "one way to proceed in such a situation is to store the initial random seeds for _ each _ event along with the how - to ( i.e. the algorithm and code for producing events ) .",
    "then any single divergence will affect at most one event .",
    "the second example is analysis of real data . if we are interested in statistical distributions ( histograms , scatter plots , pie charts etc . ) , a `` weak '' equivalence in the statistical sense can be enough .",
    "if we are selecting e.g. rare events in a search for new particles , we would like to isolate the same sample each time we run a particular selection on the same input ( `` strong '' equivalence ) .",
    "one way to proceed here is to keep the list of selected events along with the how - to of the selection for future verifications .",
    "as long as the input sample is available , we will have reproducibility of the selection even if portability is not guaranteed .    to sum it up -",
    "each application has to define criteria establishing the equivalence of data for its domain .",
    "good choice of metadata about a chunk of data ( e.g. a dataset ) can be very useful later when trying to decide if your reproduction is good enough .",
    "for instance , if we kept the moments like mean value and standard deviation with their statistical uncertainties from a distribution with millions of events , it will help in determining if our replica is statistically equivalent later .",
    "last but not least , an important aspect in recording the data provenance is the level of detail .",
    "the result of the execution of the same algorithm with the same input in today s complex software world may depend on environment variables , linked libraries containing different versions of supporting applications , different compilers or levels of optimization etc .",
    "when these factors are important , they have to be included in the data provenance log for future use .",
    "the high energy physics field is sociologically very interesting .",
    "the experimental collaborations have grown from being counted on the fingers of one or two hands in the sixties to around five hundred in the nineties and two thousand today . even in theoretical physics collaborations",
    "are growing with time .",
    "that explains why the field was always in the forefront of developing and/or adopting early new collaborative tools , the best known example being of course the invention of the world wide web at cern . at present , the lhc experiments are heavily involved in grid efforts , continuing the tradition , see e.g.  @xcite .",
    "after a high energy physics detector is triggered , the information from the different systems is read and ultimately recorded ( possibly after cleaning , filtering and initial reconstruction ) to mass storage .",
    "the high intensity of the lhc beams usually results in more than one interaction taking place simultaneously , so a trigger records the combined response to all particles traversing the detector in the time window when the system is open .",
    "the first stages in the data processing are well defined and usually tightly controlled by the teams responsible for reconstruction , calibration , alignment , `` official '' simulation etc .",
    "the application of virtual data concepts in this area is discussed e.g. in  @xcite .",
    "here we are interested in the later stages of data processing and analysis , when various teams and individual scientists look at the data from many different angles - refining algorithms , updating calibrations or trying out new approaches , selecting and analyzing a particular data set , estimating parameters etc . , and ultimately producing and publishing physics results . even in today s large collaborations this is a decentralized , `` chaotic '' activity , and is expected to grow substantially in complexity and scale for the lhc experiments .",
    "decentralization does not mean lack of organization - on the contrary , this will be one of the keys for building successful structures , both from the social and technical points of view .",
    "clearly flexible enough systems , able to accommodate a large user base , and use cases not all of which can be foreseen in advance , are needed .",
    "many users should be able to work and share their results in parallel , without stepping on each other s toes . here",
    "we explore the benefits that a virtual data system can bring in this vast and dynamic field .    moving from production to analysis",
    ", the complexity grows fast with the number of users while the average wall and cpu time to complete a typical task goes down , as illustrated in figure  [ useranal ] .",
    "an intermediate phase are large analysis tasks which require batch mode .",
    "the ultimate challenge comes from interactive analyses , where users change their minds often and need fast response times to stay productive .",
    "the latency of the underlying systems at this stage is critical .",
    "there should be no single point of failure and the system should re - route requests automatically to the next available service .",
    "the redundancy should be accompanied by efficient synchronization , so that new results are published fast and made accessible for all interested parties regardless of their geographical location .",
    "an important feature of analysis systems is the ability to build scripts and/or executables `` on the fly '' , including user supplied code and parameters . on the contrary",
    ", production systems often rely on pre - build applications , distributed in a centralized way from `` officially controlled '' repositories .",
    "the user should be in position to modify the inputs on her / his desk(lap)top and request a derived data product , possibly linking with preinstalled libraries on the execution sites .",
    "a grid - type system can store large volumes of data at geographically remote locations and provide the necessary computing power for larger tasks .",
    "the results are returned to the user or stored and published from the remote site(s ) .",
    "an example of this vision is presented in  @xcite . at each stage in the analysis interesting events can be visualized and plots for all quantities of interest can be produced .",
    "what will a virtual data system bring to this picture ?",
    "i have in my office a large collection of paper folders for different analyses performed working on various tasks , stored in varied ways .",
    "they match with codes stored and archived on different systems .",
    "so when a colleague comes in and asks me how i obtained that plot three months ago , i have to sift for some time ( depending on my organization ) through my folders , make a photocopy , then find the corresponding code , make sure it is the `` historically '' right version , and that i wrote down which version of the pattern recognition was used at the time etc . or if i go to my colleague , she will go through similar steps , but her organization will be different and we will possibly exchange information in a different format . or if one of us is on leave things will slow down .",
    "so we are recording data provenance , but manually , very often incomplete , and not easily accessible . and",
    "clearly this scales poorly for larger and geographically distributed collaborating groups .    in a `` virtual logbook ''",
    "all steps of an analysis , even the blind alleys , can be recorded and retrieved automatically .",
    "let us assume that a new member joins the group . even without bugging one s colleagues too often",
    ", it will be quite easy to discover exactly what has been done so far for a particular analysis branch , to validate how it was done , and to refine the analysis .",
    "a scientist wanting to dig deeper can add a new derived data branch by , for example , applying a more sophisticated selection , and continuing to investigate down the new road .",
    "of course , the results of the group can be shared easily with other teams and individuals in the collaboration , working on similar topics , providing or re - using better algorithms etc .",
    "the starting of new subjects will profit from the availability of the accumulated experience . at publication time",
    "it will be much easier to perform an accurate audit of the results , and to work with internal referees who may require details of the analysis or additional checks .",
    "at the beginning of a new project , a suitable metaphor can be helpful .",
    "as we would like to make our `` virtual data logbooks '' persistent , distributed and secure , the following analogy came quite naturally :    * a cave is a secure place to store stuff . *",
    "usually you need a key to enter .",
    "* stuff can be retrieved when needed ( and if the temperature is kept constant , usually in good shape ) . *",
    "small caves can be private , larger ones are usually owned by cooperatives . * when a cave is full , a new one is build . * to get something , one starts at the local caves and , if needed , widens the search ...    we can go on , but , as we will see in a moment , these are striking similarities with the goals of our project , so caves seemed a peculiarly apt name .",
    "the use of metaphors is inspired from the adoption of extreme programming techniques  @xcite in our project .",
    "for their relationship to the programming style in hep see  @xcite .",
    "the collaborative analysis versioning environment system ( caves ) project concentrates on the interactions between users performing data and/or computing intensive analyses on large data sets , as encountered in many contemporary scientific disciplines . in modern science increasingly larger groups of researchers collaborate on a given topic over extended periods of time .",
    "the logging and sharing of knowledge about how analyses are performed or how results are obtained is important throughout the lifetime of a project .",
    "here is where virtual data concepts play a major role .",
    "the ability to seamlessly log , exchange and reproduce results and the methods , algorithms and computer programs used in obtaining them enhances in a qualitative way the level of collaboration in a group or between groups in larger organizations .",
    "it makes it easier for newcomers to start being productive almost from day one of their involvement or for referees to audit a result and gain easy access to all the relevant details .",
    "also when scientists move on to new endeavors they can leave their expertise in a form easily utilizable by their colleagues .",
    "the same is true for archiving the knowledge accumulated in a project for reuse in future undertakings .",
    "the caves project takes a pragmatic approach in assessing the needs of a community of scientists by building series of prototypes with increasing sophistication .",
    "our goal is to stay close to the end users and listen carefully to their needs at all stages of an analysis task . in this way we can develop an architecture able to satisfy the varied requirements of a diverse group of researchers .",
    "our main line of development draws on the needs of , but is not limited to , high energy physics experiments , especially the cms collaboration  @xcite , planning to begin data taking in 2007 at the large hadron collider",
    ". the cms experiment will produce large amounts of simulated and real data , reaching tens and hundreds of petabytes .",
    "the analysis of datasets of this size , with its distributed nature , by a large community of users is a very challenging task and one of the strongest driving forces for grid computing .",
    "the caves project explores and develops these emerging technologies to facilitate the analysis of real and simulated data .",
    "we start by analyzing the simulated data from the data challenges of the cms experiment , which will grow in scale and complexity approaching the situation when real data will start to flow .    in extending the functionality of existing data analysis packages with virtual data capabilities",
    ", we build functioning analysis suites , providing an easy and habitual entry point for researchers to explore virtual data concepts in real life applications , and hence give valuable feedback about their needs , helping to guide the most useful directions for refining the system design . by just adding capabilities in a plug - in style we facilitate the acceptance and ease of use , and",
    "thus hope to attract a critical mass of users from different fields in a short time .",
    "there is no need to learn yet another programming language , and our goal is simplicity of design , keeping the number of commands and their parameters to the bare minimum needed for rich and useful functionality .",
    "the architecture is modular based on web , grid and other services which can be plugged in as desired .",
    "in addition to working in ways considered standard today the scientists are able to log or checkpoint their work throughout the lifetime of an analysis task .",
    "we envisage the ability to create private `` checkpoints '' which can be stored on a local machine and/or on a secure remote server .",
    "when an user wants to share some work , he can store the relevant know - how on the group servers accessible to the members of a group working on a given task .",
    "this could be a geographically distributed virtual organization . in the case of collaboration between groups or with internal or external referees portions of this know - how",
    "can be made accessible to authorized users , or a shared system of servers can be created as needed .",
    "the provenance of results can be recorded at different levels of detail as decided by the users and augmented by annotations .",
    "along with the knowledge of how analyses are performed , selected results and their annotations can be stored in the same system .",
    "they can be browsed by the members of a group , thus enhancing the analysis experience both for experts and newcomers . when desirable , information from different phases of an analysis can easily be shared with other groups or peers .",
    "we stressed already the value of complete logs . in the heat of an active analysis session ,",
    "when there is no time or need to be pedantical , users may see merit in storing sometimes also partial logs , a classical example being a program with hidden dependencies , e.g. the calling of a program or reading of a file within a program , not exposed externally . in this case , the data product is not reproducible , but at least the log will point what is missing . or the users may even store a non - functional sequence of actions in the debugging phase for additional work later , even without producing a virtual data product",
    ". our system should be able to support partial logging , provided that the users are aware of the limitations and risks of this approach .",
    "an important point is how groups will structure their analyses .",
    "each virtual data product needs an unique identifier , which may be provided by the users or appended automatically by the system with e.g. project i d , user i d and date to render it unique . for smaller tasks , all identifiers and logs",
    "can be located in a single place , like a big barrel in a cave .",
    "then the group will benefit from adopting a policy for meaningful selection of identifiers , making subsequent browsing and finding of information easy . for larger projects",
    "the virtual data space can be structured in chunks corresponding to subtasks , like many barrels in a large cave . then at the beginning of a session",
    "the user will select the barrel to be opened for that session .",
    "when needed , information from related ( linked ) barrels can be retrieved .",
    "in principal there are no restrictions on how deep the hierarchy can be , only the practical needs will determine it .",
    "we base our first functional system on popular and well established data analysis frameworks and programming tools , making them virtual data enabled .",
    "in the course of our project we will leverage best - of - breed existing technologies ( e.g. databases , code management systems , web services ) , as well as the developments in forward - looking grid enabled projects , e.g. the virtual data system chimera  @xcite , the clarens server  @xcite for secure remote dataset access , the condor  @xcite , pegasus  @xcite and sphinx  @xcite schedulers for executing tasks in a grid environment .",
    "further down the road we envisage building distributed systems capable of analyzing the datasets used in the cms collaboration at all stages of data analysis , starting from monte carlo generation and simulation of events through reconstruction and selection all the way to producing results for publication .",
    "we plan to use the grid test bed of the grid physics network ( griphyn ) project  @xcite for grid enabling the collaborative services .",
    "the griphyn project is developing grid technologies for scientific and engineering projects that will collect and analyze distributed , petabyte - scale datasets .",
    "griphyn research will enable the development of petascale virtual data grids ( pvdgs ) through its virtual data toolkit ( vdt  @xcite ) .",
    "the caves system can be used as a building block for a collaborative analysis environment , providing `` virtual data logbook '' capabilities and the ability to explore the metadata associated with different data products .",
    "our first functioning system extends the very popular object - oriented data analysis framework root  @xcite , widely used in high energy physics and other fields , making it virtual data enabled .",
    "the root framework provides a rich set of data analysis tools and excellent graphical capabilities , able to produce publication - ready pictures .",
    "it is easy to execute user code , written in c++ , and to extend the framework in a plug - in style .",
    "new systems can be developed by subclassing the existing root classes . and the cint  @xcite interpreter runs the user code `` on - the - fly '' , facilitating fast development and prototyping .",
    "all this is very helpful in the early phases of a new project .",
    "in addition , root has a large and lively user base , so we plan to release early and often and to have a development driven largely by user feedback .",
    "last but not least , root is easy to install and very portable .",
    "versions for many flavors of linux and unix and for windows are available .",
    "we leverage a well established source code management system - the concurrent versions system cvs  @xcite .",
    "it is well suited to provide version control for a rapid development by a large team and to store , by the mechanism of tagging releases , many versions so that they can be extracted in exactly the same form even if modified , added or deleted since that time .",
    "the cvs tags assume the role of unique identifiers for virtual data products .",
    "cvs can keep track of the contributions of different users .",
    "the locking mechanism makes it possible for two or more people to modify a file at the same time , important for a team of people working on large projects .",
    "the system has useful self - documenting capabilities . besides the traditional command line interface",
    "several products provide web frontends which can be used when implementing web services .",
    "all these features of cvs make it a good match for our system .",
    "nowadays cvs is already installed by default on most unix and linux systems and a windows port is available . in this way , our system can be used both from unix and windows clients , making it easy for users at all levels to reproduce results .",
    "a key aspect of the project is the distributed nature of the input data , the analysis process and the user base .",
    "this has to be addressed from the earliest stages .",
    "our system should be fully functional both in local and remote modes , provided that the necessary repositories are operational and the datasets available .",
    "this allows the users to work on their laptops ( maybe handhelds tomorrow ) even without a network connection , or just to store intermediate steps in the course of an active analysis session for their private consumption , only publishing a sufficiently polished result .",
    "this design has the additional benefit of utilizing efficiently the local cpu and storage resources of the users , reducing the load on the distributed services ( e.g. grid ) system .",
    "the users will have the ability to replicate , move , archive and delete data provenance logs .",
    "gaining experience in running the system will help to strike the right balance between local and remote usage .",
    "more details about the distribution of services is given in the next section .",
    "the architecture of caves builds upon the concept of sandbox programming . by sandbox programming",
    "we mean users work on per session basis , creating a new sandbox for a given session . all the changes or modifications and",
    "work the user does in a session between checkpoints is logged into a temporary logfile , which can be checked in the cvs repository with a unique tag .",
    "the system checks if the user executed external programs ( in the root case these are typically c++ programs ) and logs them automatically with the same tag .",
    "here an interesting point arises : a possible scenario is that a user runs the same program many times , just varying the inputs . in this case cvs",
    "will do the right thing : store the program only _ once _ , avoiding duplication of code , and tagging it many times with different tags , reflecting the fact that the program was executed several times to produce distinct data products . or the user can choose during the same session to browse through the tags of other users to see what work was done , and select the log / session of interest by extracting the peers log with the tag used to log the corresponding session activities . here",
    "two modes of operation are possible : the user may want to reproduce a result by extracting and executing the commands and programs associated with a selected tag , or just extract the history of a given data product in order to inspect it , possibly modify the code or the inputs and produce new results .",
    "we also have the concept that users can log annotations or results in the repository along with the data provenance , storing useful metadata about a data product for future use ( see the discussion in the data equivalence section ) .",
    "it is possible to record the metadata in relational databases too , e.g. in popular open source products like mysql  @xcite , so that another user first queries the database to retrieve the annotations or condensed logs of what other users have done already .",
    "this approach will ensure scalability for large groups of researchers accumulating large repositories , and will reduce the load on the cvs servers , improving the latency of the overall system . in this case",
    "the searching of a database is expected to be faster than the direct search for a given tag among a large number of stored tags .",
    "this will be investigated by building functional systems and monitoring their performance .",
    "the additional burden of synchronizing the information between the databases and the repositories is worthwhile only if we can improve the overall performance and scalability of the system .",
    "a further enhancement can come from retrieving first the metadata , and only if the user is interested , the complete log about a particular data product .",
    "the architecture is shown in graphical form in figure  [ cavesarch ] .",
    "let us discuss now some possible scenarios , which can take place in our architecture .",
    "* case1 : simple * + user 1 : does some analysis and produces a result with tag _ * projectx - stepy - user1*_. + user 2 : browses all current tags in the repository and fetches the session stored with tag _ * projectx - stepy - user1*_. + * case2 : complex * + user 1 : does some analysis and produces a result with tag _ * projectx - stepy - user1*_. + user 2 : browses all current tags in the repository and fetches the session stored with tag _ * projectx - stepy - user1*_. + user 2 : does some modifications in the code files , which were obtained from the session of user1 , runs again and stores the changes along with the logfile with a new tag . + user 1 : browses the repository and discovers that the previous session was used and contains modified or new code files , so decides to extract that session using the new tag and possibly reuse it to produce the next step and so on .",
    "+ this scenario can be extended to include an arbitrary number of steps and users in a working group or groups in a collaboration .",
    "based on our work so far , the following set of commands emerges as useful :    1 .   _",
    "session commands : _ * * open @xmath0 * : authentication and authorization , connection , selection of cvs services , local or remote mode , the barrel to be opened etc . *",
    "* close @xmath0 * : save opened sessions , clean - up etc .",
    "2 .   _ during analysis : _ * * help @xmath1 * : get help for a command or list of commands * * browse @xmath2 * : browse all tags in ( a part of ) a repository , subsets of tags beginning or containing a string etc ; possibly browse the metadata about a specific virtual data product e.g. by clicking on a selected tag from a list displayed in a graphical user interface * * startlog * : define the starting checkpoint for a log ( part of a session between user - defined points ) , which will be closed by a * log * command * * log @xmath2 * : log ( part of ) a session between user - defined checkpoints together with all programs executed in the session ; this may be a complete or optionally a partial log with user - defined level of detail * * annotate @xmath2 * : store user - supplied notes ( metadata ) about the work being done , preferably in a concise and meaningful manner ; optionally , selected results can be stored along with the annotations e.g. a summary plot , subject of course to space considerations ; this can be a separate command or part of the * log * command e.g. the user may select to be prompted to provide annotations when logging a tag * * inspect @xmath3 * : get a condense ( annotations plus user commands , in a sense something like header files ) , or the complete log for a tag including the programs executed , but do not reproduce the data product ; useful for reusing analysis work * * extract @xmath2 * : in addition to * inspect * , reproduce the virtual data product 3 .   _",
    "administrative tasks : _ * * copy @xmath2 @xmath4 @xmath5 * : clone a log to a new repository * * move @xmath2 @xmath4 @xmath5 * : as you expect * * delete @xmath2 @xmath4 * : remove a log ; cvs has the nice feature of storing such files in the attic , so it is similar to moving a file in the trash can without emptying it * * archive @xmath2 @xmath5 * : store in an archive ( e.g. a mass storage system ) * * retrieve @xmath2 @xmath4 * : retrieve from an archive ( for whole repositories normal cvs techniques can be used ) .",
    "our first release is based on the minimal scope of commands providing interesting functionality .",
    "future developments will be guided by the value users put on different options . as we want a fast release cycle this limits the number of features introduced and tested in any new version .",
    "the command set is extensible and new commands may be introduced as needed .",
    "in this section we sketch the process of building the first caves release as prototype of a real analysis system , and examine some of the issues that arise .",
    "the first prototype has been demonstrated at the supercomputing conference in phoenix , arizona , in november 2003 , and the first release was made public on december 12 , 2003 . more details can be found in  @xcite , and an in - depth technical description about the virtual data enabled root client and the remote services is in preparation  @xcite .",
    "we limit the scope to the most basic commands described in the previous section :    * * open @xmath0 * : sets the cvs service ( default or user choice ) * * help * : short help about the commands below * * browse @xmath2 * : browse tags in the repository * * log @xmath2 * : log part of a session between user - defined checkpoints * * extract @xmath2 * : reproduce a virtual data product .",
    "these commands are implemented by subclassing basic root classes .",
    "commands not recognized by our system are delegated to root for execution or catching exceptions .",
    "the root framework provides the ability to access both local and remote files and datasets .",
    "one way to realize the second option is to store datasets on apache  @xcite servers which are root - enabled with a plug - in provided by the root team . in this way",
    "we implement a remote data service from web servers .",
    "the cvs system also is able to access both local and remote repositories .",
    "one way to realize the second option is to use the cvs pserver .",
    "contrary to some opinions it can be configured in quite a secure and efficient way as follows : the remote users need just cvs accounts with password authentication , they never get unix accounts on the server .",
    "a dedicated cvs user ( or several for different groups ) acts on their behalf on the server .",
    "the mapping is done by a special cvs administrative file .",
    "similar design was adopted by the globus  @xcite toolkit with the grid mapfiles to control user access to remote sites , the only difference being the use of certificates in place of passwords , thus enhancing the security and providing a temporarily limited single sign - on to a grid .",
    "the virtual organization tools developed by globus can be used also for cvs services .",
    "in addition , we implement access control lists per cvs user for reading of the repository or writing to specific directories only for authorized users .",
    "this makes the server secure : even compromising the password a normal user can not modify administrative files and thus can not run shell commands on the server .",
    "only the administrator needs an unix server account .",
    "adding and managing cvs users is undoubtedly much simpler , safer and more scalable compared to dealing with unix accounts , not to talk about the dreaded group variety .",
    "a single server can handle multiple repositories , making it easy to `` fine structure '' projects .    to test the functionality after each modification we have a test suite",
    ": we use events generated with pythia  @xcite ( or the cms pythia implementation in cmkin ) , and analyze , histogram and visualize them with code  @xcite ( for results obtained with this code see e.g.  @xcite ) built on top of the object - oriented data analysis framework root  @xcite . to conclude we give a couple of snapshots of the first caves system in action :    ....    start a virtual data enabled root client : rltest    rltest    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *                         welcome to caves                                * *        collaborative analysis versioning environment system             * *                                                                        * *                   dimitri bourilkov & mandar kulkarni                   * *                        university of florida                            * *                          gainesville , usa                               * *                                                                        * *                   you are welcome to visit our website                  * *                        cern.ch/bourilkov/caves.html                     * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *     please set the cvs pserver or hit enter for default     caves :      pserver for this session :   : pserver:test@ufgrid02.phys.ufl.edu:/home/caves     * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *                         to get started :                                 * *                   just type   help   at the command prompt                * *                                                                        * *             commands beginning with ' . '",
    "are delegated to root           * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * caves : help    list of commands and how to use them :       = = = help : to get this help         help       = = = browse : to list all existing tags or a subset beginning with string    = = = >        de facto the content of the virtual data catalog is displayed        browse      browse < prefix - string - of - tag >      = = = log : to store all command line activities of the user labeling them with a tag    = = =          the actions after the last log command or    = = =          from the beginning of the session are stored    = = =             = = =          < tag > must be cvs compliant i.e. start with uppercase or lowercase letter    = = =          and contain uppercase and lowercase letters , digits , ' - ' and ' _ '     = = =          hint : this a powerful tool to structure your project     = = =             = = = >       in effect this logs how a chunk of virtual data was produced    = = = >       and can be ( re)produced later ; the macro files executed by the user    = = = >       are stored in their entirety along with the command line activities    = = = >       creating a complete log        log < tag >      = = = extract : to produce a chunk of virtual data identified by a tag    = = = >       the necessary macro files are downloaded automatically to the client    = = = >       and can be reused and modified for new analyses        extract < tag >    caves : browse higgs - ww - plotpxpypz-500      ( revision : 1.5 )    higgs - ww - plotpxpypz-100      ( revision : 1.4 )        caves : extract higgs - ww - plotpxpypz-500 * * * * * * * * * * * * * * * * * * * * storing data for usage .... * * * * * * * * * * * * * * * * * * * * * root command is : .x   macro is : dbpit1web.c   macro is : dbpit1web.c   u data / dbpit1web.c you have [ 0 ] altered files in this repository .",
    "are you sure you want to release ( and delete ) directory ` data ' : y argument is 500   argument is input \" http://ufgrid02.phys.ufl.edu/~bourilkov/higgs.root \"   argument is output \" higgs - ww - plotpxpypz-500 \"   command is : .x dbpit1web.c(500,\"http://ufgrid02.phys.ufl.edu/~bourilkov / higgs.root \" , \" higgs - ww - plotpxpypz-500 \" ) tfile * *      higgs-ww-plotpxpypz-500.root       tfile *      higgs-ww-plotpxpypz-500.root        key : tcanvas   canv2;1 root pythia plotter   d.bourilkov   university of florida .x dbpit1web.c(500,\"http://ufgrid02.phys.ufl.edu/~bourilkov / higgs.root \" , \" higgs - ww - plotpxpypz-500 \" )    * * * * * * * * * * * * * * * * * * * * * * * * * * end * * * * * * * * * * * * * * * * * * * * * * * * * * * * you have [ 0 ] altered files in this repository .",
    "are you sure you want to release ( and delete ) directory ` v01 ' : y caves : .q .q ....    the running of this example produces the following plot from five hundred simulated input events , as illustrated in figure  [ higgsplot ] .",
    "it is worth mentioning that the plot materializes on the client machine out of `` thin air '' .",
    "the user can first download the caves code from our remote repository and needs just root and cvs to build the client in no time .",
    "then the remote logs are browsed , one is selected for extraction , the corresponding commands and programs are downloaded , built on the fly and executed on the client machine , the input data are accessed from our remote web data server , `` et voil '' , the plot pops up on the client s machine .",
    "an example of our event display built also on top of root is shown in figure  [ eventdisplay ] .",
    "we envisage extending the service oriented architecture by encompassing grid and web services as they mature and provide the performance needed to meet mission - critical requirements .",
    "our system will benefit from developments like the globus authentication system , enhancing the security and providing a temporarily limited single sign - on to grid services , the griphyn virtual data toolkit for job executions on grids , possibly augmented by grid schedulers like sphinx or pegasus .",
    "other promising developments are remote data services e.g. clarens , which can possibly be used also with our set of cvs services to provide a globus security infrastructure , distributed databases etc .",
    "another closely watched development is the griphyn virtual data system chimera , which evolves a virtual data language .",
    "our `` virtual data logbooks '' in the first implementation are formatted as standard ascii files , in future versions we might use also a more structured format e.g. xml , which is well suited for web services .",
    "chimera also converts the virtual data transformations and derivations to xml and further to directed acyclic graphs .",
    "it is a challenging research question if all analysis activities can be expressed easily in the present chimera language . with the developments of both projects it may be possible to generate automatically virtual data language derivations from our logs of interactive root sessions for execution on a grid or storage in a chimera virtual data catalog .",
    "another promising path is integration with the root / proof  @xcite system for parallel analysis of large data sets .",
    "caves can be used as a building block for a collaborative analysis environment in a web and grid services oriented architecture , important at a time when web and grid services are gaining in prominence .",
    "we are monitoring closely the evolving architecture and use cases of projects like caigee  @xcite , hepcal  @xcite , arda  @xcite and collaborative workflows  @xcite , and are starting useful collaborations . besides the root - based client a web browser executing commands on a remote root or clarens server is a possible development for `` ultralight '' clients .",
    "in this white paper we have developed the main ideas driving the caves project for exploring virtual data concepts for data analysis .",
    "the decomposition of typical analysis tasks shows that the virtual data approach bears great promise for qualitatively enhancing the collaborative work of research groups and the accumulation and sharing of knowledge in todays complex and large scale scientific environments .",
    "the confidence in results and their discovery and reuse grows with the ability to automatically log and reproduce them on demand .",
    "we have built a first functional system providing automatic data provenance in a typical analysis session .",
    "the system has been demonstrated successfully at supercomputing 2003 and a first public release is available for interested users , which are encouraged to visit our web pages , currently located at the following url : + http://cern.ch/bourilkov/caves.html    in the process of shaping and launching the project the author enjoyed discussions with paul avery , rene brun , federico carminati , harwey newman , lothar bauerdick , david stickland , stephan wynhoff , torre wenaus , rob gardner , fons rademakers , richard cavanaugh , john yelton , darin acosta , mike wilde , jens vckler , conrad steenberg , julian bunn , predrag buncic and martin ballantijn .",
    "the coffee pauses with jorge rodriguez were always stimulating . the first prototype and first release",
    "are coded by mandar kulkarni and myself , the members of the caves team at present .",
    "this work is supported in part by the united states national science foundation under grants nsf itr-0086044 ( griphyn ) and nsf phy-0122557 ( ivdgl ) .",
    "`` the grid : blueprint for a new computing infrastructure '' , edited by ian foster and carl kesselman , july 1998 , isbn 0 - 97028 - 467 - 5 .",
    "i.  foster , c.  kesselman , s.  tuecke , `` the anatomy of the grid : enabling scalable virtual organizations '' , international j. supercomputer applications , 15(3 ) , 2001 . i.  foster , c.  kesselman , j.  nick , s.  tuecke , `` the physiology of the grid : an open grid services architecture for distributed systems integration '' , open grid service infrastructure wg , global grid forum , june 22 , 2002 .",
    "world wide web consortium ( w3c ) , http://www.w3.org/2002/ws/ .",
    "see e.g. beth gold - bernstein , `` making soa a reality ?",
    "an appeal to software vendors and developers '' , november 2003 , http://www.ebizq.net/topics/soa/features/3142.html?related .",
    "i.  foster _ et al .",
    "_ , `` chimera : a virtual data system for representing , querying , and automating data derivation '' , presented at the 14th international conference on scientific and statistical database management ( ssdbm 2002 ) , edinburgh , 2002 ; griphyn technical report 2002 - 7 , 2002 .",
    "i.  foster _ et al .",
    "_ , `` the virtual data grid : a new paradigm and architecture for data intensive collaboration '' , proceedings of cidr 2003 - conference on innovative data research ; griphyn technical report 2002 - 18 , 2002 .",
    "p.  avery , i.  foster , `` the griphyn project : towards petascale virtual - data grids '' , griphyn technical report 2001 - 14 , 2001 ; http://www.griphyn.org/index.php .",
    "the particle physics data grid , http://www.ppdg.net/ .",
    "the european union datagrid project , http://eu-datagrid.web.cern.ch/eu-datagrid/ .",
    "p.  avery , i.  foster , r.  gardner , h.  newman , a.  szalay , `` an international virtual - data grid laboratory for data intensive science '' , griphyn technical report 2001 - 2 , 2001 ; http://www.ivdgl.org/ .",
    "the lhc computing grid project , http://lcg.web.cern.ch/lcg/ .",
    "the enabling grids for e - science in europe project , + http://egee-intranet.web.cern.ch/egee-intranet/gateway.html .",
    "a.  arbree _ et al .",
    "_ , `` virtual data in cms production '' , computing in high - energy and nuclear physics ( chep 03 ) , la jolla , california , 24 - 28 mar 2003 ; published in econf c0303241:tuat011 , 2003 ; e - print archive : cs.dc/0306009 .",
    "a.  arbree , p.  avery , d.  bourilkov _ et al .",
    "_ , `` virtual data in cms analysis '' , computing in high - energy and nuclear physics ( chep 03 ) , la jolla , california , 24 - 28 mar 2003 ; fermilab - conf-03 - 275 , griphyn - report-2003 - 16 , cms - cr-2003 - 015 ; published in econf c0303241:tuat010 , 2003 ; [ arxiv : physics/0306008 ] . k.  beck , `` extreme programming explained - embrace change '' , addison - wesley , 2000 , isbn 0201616416 .",
    "r.  brun , f.  carminati _ et al .",
    "_ , `` software development in hep '' , computing in high - energy and nuclear physics ( chep 03 ) , la jolla , california , 24 - 28 mar 2003 ; http://www.slac.stanford.edu/econf/c0303241/proc/pres/10006.ppt .",
    "the cms collaboration , `` the compact muon solenoid - technical proposal '' , cern / lhcc 94 - 38 , cern , geneva , 1994 . v.  innocente , l.  silvestris , d.  stickland , `` cms software architecture software framework , services and persistency in high level trigger , reconstruction and analysis '' cms note-2000/047 , cern , 2000 . http://cms-project-ccs.web.cern.ch/cms-project-ccs/ . c.  steenberg",
    "_ et al . _ , `` the clarens web services architecture '' , computing in high - energy and nuclear physics ( chep 03 ) , la jolla , california , 24 - 28 mar 2003 ; published in econf c0303241:mont008 , 2003 ; e - print archive : cs.dc/0306002 ; http://clarens.sourceforge.net/",
    ". the condor project , http://www.cs.wisc.edu/condor .",
    "e.  deelman , j.  blythe , y.  gil , c.  kesselman , `` pegasus : planning for execution in grids '' , griphyn technical report 2002 - 20 , 2003 . j.  in _ et al .",
    "_ , `` policy based scheduling for simple quality of service in grid computing '' , griphyn technical report 2003 - 32 , 2003 .",
    "the griphyn virtual data toolkit , http://www.lsc-group.phys.uwm.edu/vdt/ .",
    "rene brun and fons rademakers , `` root - an object oriented data analysis framework '' , proceedings aihenp96 workshop , lausanne , sep .",
    "1996 , nucl .",
    "inst . & meth .",
    "a 389 ( 1997 ) 81 - 86 ; see also http://root.cern.ch/ .",
    "masaharu goto , `` c++ interpreter - cint '' cq publishing , isbn4 - 789 - 3085 - 3 ( japanese ) .",
    "the concurrent versions system cvs , http://www.cvshome.org/ .",
    "the mysql database , http://www.mysql.com/ .",
    "d.  bourilkov and m.  kulkarni , `` the caves project - collaborative analysis versioning environment system '' , presented at ppdg collaboration meeting , berkeley lab , december 15 - 16 , 2003 ; http://www.ppdg.net/archives/talks/2003/bin00010.bin .",
    "d.  bourilkov and m.  kulkarni , in preparation , to be presented at the root 2004 users workshop .",
    "apache web server , apache software foundation , http://www.apache.org/ .",
    "the globus alliance , http://www.globus.org/ .",
    "t.  sjstrand _ et al .",
    "_ , `` high - energy - physics event generation with pythia 6.1 '' comp .",
    "* 135 * ( 2001 ) 238 .",
    "pythia and cmkin viewers and plotters developed by the author , + http://cern.ch/bourilkov/viewers.html .",
    "d.  bourilkov , `` sensitivity to contact interactions and extra dimensions in di - lepton and di - photon channels at future colliders '' , arxiv : hep - ph/0305125 .",
    "d.  bourilkov , `` study of parton density function uncertainties with lhapdf and pythia at lhc '' , arxiv : hep - ph/0305126 . m.  ballintijn , r.  brun , f.  rademakers and g.  roland , `` the proof distributed parallel analysis framework based on root '' , econf * c0303241 * , tuct004 ( 2003 ) , econf * c0303241 * , tult003 ( 2003 ) , [ arxiv : physics/0306110 ] .",
    "the caigee project and grid enabled analysis ( gae ) , + http://pcbunn.cithep.caltech.edu/gae/caigee/default.htm , + http://pcbunn.cithep.caltech.edu/gae/gae.htm .",
    "hep common grid applications layer use cases - hepcal ii document ( lcg - sc2 - 2003 - 032 ) , http://lcg.web.cern.ch/lcg/sc2/gag/hepcal-ii.doc .",
    "rtag11 : an architectural roadmap towards distributed analysis ( arda ) , + http://www.uscms.org/s&c/lcg/arda/ .",
    "+ show_docs.php?series = ivdgl&category = talks&id=691"
  ],
  "abstract_text": [
    "<S> the collaborative analysis versioning environment system ( caves ) project concentrates on the interactions between users performing data and/or computing intensive analyses on large data sets , as encountered in many contemporary scientific disciplines . in modern science increasingly larger groups of researchers collaborate on a given topic over extended periods of time . </S>",
    "<S> the logging and sharing of knowledge about how analyses are performed or how results are obtained is important throughout the lifetime of a project . </S>",
    "<S> here is where virtual data concepts play a major role . </S>",
    "<S> the ability to seamlessly log , exchange and reproduce results and the methods , algorithms and computer programs used in obtaining them enhances in a qualitative way the level of collaboration in a group or between groups in larger organizations . </S>",
    "<S> it makes it easier for newcomers to start being productive almost from day one of their involvement or for referees to audit a result and gain easy access to all the relevant details . </S>",
    "<S> also when scientists move on to new endeavors they can leave their expertise in a form easily utilizable by their colleagues . </S>",
    "<S> the same is true for archiving the knowledge accumulated in a project for reuse in future undertakings .    </S>",
    "<S> the caves project takes a pragmatic approach in assessing the needs of a community of scientists by building series of prototypes with increasing sophistication . in extending the functionality of existing data analysis packages with virtual data capabilities these prototypes provide an easy and habitual entry point for researchers to explore virtual data concepts in real life applications and to provide valuable feedback for refining the system design . </S>",
    "<S> the architecture is modular based on web , grid and other services which can be plugged in as desired . as a proof of principle </S>",
    "<S> we build a first system by extending the very popular data analysis framework root , widely used in high energy physics and other fields , making it virtual data enabled .    </S>",
    "<S> physics/0401007 + january 2004 </S>"
  ]
}