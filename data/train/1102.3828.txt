{
  "article_text": [
    "approximate nearest neighbors ( ann ) search methods  @xcite are required to handle large databases , especially for computer vision  @xcite and music retrieval  @xcite applications .",
    "one of the most popular techniques is euclidean locality - sensitive hashing  @xcite .",
    "however , most of these approaches are memory consuming , since several hash tables or trees are required . the methods of  @xcite , which embeds the vector into a binary space , better satisfies the memory constraint .",
    "they are , however , significantly outperformed in terms of the trade - off between memory usage and accuracy by recent methods that cast high dimensional indexing to a source coding problem  @xcite , in particular the product quantization - based method of  @xcite exhibits impressive results for large scale image search  @xcite .",
    "state - of - the - art approaches usually perform a re - ranking stage to produce a ranked list of nearest neighbors .",
    "this is always done in partitioning based method such as  lsh  @xcite or flann  @xcite , as the neighbor hypotheses are not ranked on output of the index .",
    "but as shown in  @xcite , this post verification is also important for methods based on binary  @xcite or quantized codes  @xcite , as the ranking provided on output of the large scale search is significantly improved when verifying the few first hypotheses . in all these approaches , re - ranking",
    "is performed by computing the exact euclidean distance between each query and the returned hypotheses . for large datasets , this raises a memory issue : the vectors can not be stored in central memory and must be read from disk on - the - fly .",
    "moreover , the vectors are to a large extent accessed randomly , which in practice limits the number of hypotheses that can be verified .    in this paper",
    ", we propose a new post - verification scheme in the context of compression - based indexing methods .",
    "we focus on the method presented in  @xcite , which offers state - of - the - art performance , outperforming the flann which was previously shown to outperform lsh  @xcite .",
    "it also provides an explicit approximation of the indexed vectors .",
    "our algorithm exploits the approximation resulting from the first ranking , and refines it using codes stored in ram .",
    "there is an analogy between this approach and the scalable compression techniques proposed in the last decade  @xcite , where the term `` scalable '' means that a reconstruction of the compressed signal is refined in an incremental manner by successive description layers .    in order to evaluate our approach",
    ", we introduce a dataset of one billion vectors extracted from millions of images using the standard sift descriptor  @xcite .",
    "testing on a large scale is important , as most ann methods are usually evaluated on sets of unrealistic size , thereby ignoring memory issues that arise in real applications , where billions of vectors have to be handled  @xcite .",
    "the groundtruth nearest - neighbors have been computed for 10000 queries using exact distance computations . to our knowledge , this set is the largest ever released to evaluate ann algorithms against an exact linear scan on real data : the largest other experiment we are aware of is the one reported in  @xcite , where a private set of 179 million vectors was considered .",
    "@xcite reports an experiment on 1 billion vectors , but on synthetic data with a known model exploited by the algorithm .",
    "experiments performed on this dataset show that the proposed approach offers an alternative to the standard post - verification scheme .",
    "the precision of the search is significantly improved by the re - ranking step , leading to state - of - the - art performance on this scale , without accessing the disk .",
    "in this section , we briefly review the indexing method of  @xcite , which finds the approximate @xmath0 nearest neighbors using a source coding approach . for the sake of presentation , we describe only the asymmetric distance computation ( adc ) method proposed in  @xcite .",
    "we also assume that we search for the nearest neighbor ( i.e. , @xmath1 ) ,    let @xmath2 be a query vector and @xmath3 a set of vectors in which we want to find the nearest neighbor nn@xmath4 of @xmath5 .",
    "the adc approach consists in encoding each vector @xmath6 by a quantized version @xmath7 . for a quantizer @xmath8 with @xmath9 centroids ,",
    "the vector is encoded by @xmath10 bits , assuming @xmath9 is a power of 2 .",
    "an approximate distance @xmath11 between a query @xmath5 and a database vector is computed as @xmath12    the approximate nearest neighbor  nn@xmath13 of  @xmath5 is obtained by minimizing this distance estimator : @xmath14 which is an approximation of the exact distance calculation @xmath15 note that , in contrast with the binary embedding method of  @xcite , the query @xmath5 is not converted to a code : there is no approximation error on the query side .    to get a good vector approximation , @xmath9 should be large ( @xmath16 for a 64 bit code ) . for such large values of @xmath9 ,",
    "learning a @xmath9-means codebook is not tractable , neither is the assignment of the vectors to their nearest centroids . to address this issue ,",
    "@xcite uses a product quantizer , for which there is no need to explicitly enumerate the centroids .",
    "a vector @xmath17 is first split into @xmath18 subvectors @xmath19 .",
    "a product quantizer is then defined as a function @xmath20 which maps the input vector @xmath21 to a tuple of indices by separately quantizing the subvectors .",
    "each individual quantizer @xmath22 has @xmath23 reproduction values , learned by @xmath9-means . to limit the assignment complexity , @xmath24 ,",
    "@xmath23 is set to a small value ( e.g. @xmath23=256 ) .",
    "however , the set of @xmath9 centroids induced by the product quantizer @xmath8 is large , as @xmath25 .",
    "the squared distances in equation  [ equ : searchann ] are computed using the decomposition @xmath26 where @xmath27 is the @xmath28^th^ subvector of @xmath21 .",
    "the squared distances in the sum are read from look - up tables .",
    "these tables are constructed on - the - fly for a given query , prior to the search in the set of quantization codes , from each subvector @xmath29 and the @xmath30 centroids associated with the corresponding quantizer @xmath31 .",
    "the complexity of the table generation is @xmath32 . when @xmath33 , this complexity is negligible compared to the summation cost of @xmath34 in equation  [ equ : searchann ] .",
    "this approximate nearest neighbor method implicitly sees multi - dimensional indexing as a vector approximation problem : a database vector @xmath21 can be decomposed as @xmath35 where @xmath36 is the centroid associated with @xmath21 and @xmath37 the error vector resulting from the quantization , called the _ residual _ vector .",
    "it is proved  @xcite that the square error between the distance and its estimation is bounded , on average , by the quantization error .",
    "this ensures , asymptotically , perfect search results when increasing the number of bits allocated for the quantization indexes .",
    "the adc indexing method is fully parametrized by the number of subvectors @xmath18 and the total number of bits @xmath38 . in the following",
    ", we set  @xmath39 ( i.e. , @xmath40 ) , as suggested in  @xcite , which means that we use exactly @xmath18 bytes per indexed vector .",
    "the objective of the method proposed in this paper is to avoid the costly post - verification scheme adopted in most state - of - the - art approximate search techniques  @xcite .",
    "the idea is to take advantage of the information on the database point provided by the indexing .",
    "this is possible when using the adc method  @xcite : this search algorithm provides an explicit approximation @xmath36 of database vector @xmath21 .",
    "we first assume that the first retrieval stage returns a set of @xmath41 hypotheses .",
    "these vectors are the one for which a post - verification is required . for each database",
    "vectors  @xmath6 , the error vector is equal to @xmath42    the proposed method consists in reducing the energy of this residual vector to limit the impact of the approximation error on the estimated distances .",
    "this is done by encoding the residual vector @xmath37 using another product quantizer @xmath43 defined by its reproduction values  @xmath44 : @xmath45 where the product quantizer  @xmath46 is learned on an independent set of residual vectors .",
    "similar to @xmath8 , the set of reproduction values  @xmath44 is never exhaustively listed , as all operations are performed using the product space decomposition .",
    "the coded residual vector can be seen as the `` least significant bits '' , except that the term `` bits '' usually refers to scalar quantization .",
    "an improved estimation  @xmath47 of  @xmath21 is the sum of the approximation vector and the decoded residual vector : @xmath48    , the distance @xmath49 is computed to build the short - list of potential nearest neighbors . for",
    "selected @xmath21 vectors , the distance is re - estimated by @xmath50 , which is obtained by computing the distance between @xmath21 and its improved approximation @xmath51 . ]",
    "[ fig : incremental ]    as shown in figure  [ fig : incremental ] , this estimator will be used at search time to update the distance estimation between the query @xmath5 and the database vectors @xmath21 that are selected as potential neighbors : @xmath52    the refinement product quantizer @xmath43 is parametrized by its number of subquantizers and the total number of bits .",
    "similar to @xmath53 , we use 8 bits per subquantizer .",
    "therefore the only parameter for the refinement quantizer is the number  @xmath54 of bytes for each code .",
    "the total memory usage per indexed vector is @xmath55 bytes .",
    "this subsection details how the refinement codes are used to re - rank the hypotheses provided by the adc .",
    "the resulting approach will be denoted by adc+r in the experimental section . as for most indexing algorithms , we distinguish between the offline stage and the query stage , during which the system needs to be very efficient .",
    "the offline stage consists in learning the indexing parameters and building the indexing structure associated with a vector dataset .",
    "it is performed as follows .    1 .",
    "the quantizers @xmath8 and @xmath46 are learned on a training set .",
    "the vector dataset @xmath3 to be indexed is encoded using  @xmath53 , producing codes @xmath56 for @xmath57 .",
    "the residual vectors are encoded , producing the codes @xmath58 associated with all the indexed vectors .    searching a query vector  @xmath5 proceeds as follows :    1 .",
    "the adc distance estimation is used to generate a list @xmath59 of @xmath41 hypotheses .",
    "the selected vectors minimize the estimator of equation  [ equ : dc ] , which is computed directly in the compressed domain  @xcite .",
    "2 .   for each vector @xmath60",
    ", the approximate vector  @xmath61 is explicitly reconstructed using the first approximation  @xmath56 and the coded residual vector @xmath62 , see equation  [ equ : hatyi ] .",
    "the squared distance estimator  @xmath63 is subsequently computed .",
    "3 .   the vectors of @xmath59 associated with the @xmath0 smallest refined distances are computed .    on output",
    ", we obtain a re - ranked list of @xmath0 approximate nearest neighbors .",
    "the choice of the number  @xmath41 of vectors in the short - list depends on parameters @xmath18 , @xmath54 , @xmath41 and on the distribution of the vectors . in order for the post - verification scheme to have a negligible complexity",
    ", we typically set the ratio @xmath64 to 2 .      up to now",
    ", we have only considered the adc method of  @xcite , which requires an exhaustive scan of the dataset codes .",
    "note however that the re - ranking method proposed in this paper can be applied to any method for which an approximate reconstruction of the indexed vectors is possible , e.g. ,  @xcite .",
    "in particular , in the experimental section we evaluate our approach with the ivfadc variant of  @xcite , that avoids the aforementioned exhaustive scan by using an inverted file structure .",
    "this requires an additional coarse quantizer .",
    "adapting our re - ranking method to the ivfadc method is straightforward , as this method also provides an explicit approximation of the indexed vectors .",
    "in addition to the numbers  @xmath18 and  @xmath54 of bytes used to encode the vector , this variant requires two additional parameters : the number  @xmath65 of reproduction values of the coarse quantizer and the number  @xmath66 of inverted lists that are visited for a given query .",
    "the main advantage of this variant is to compute the distance estimators only for a fraction ( in the order of @xmath67 ) of the database , at the risk of missing some nearest neighbors if @xmath67 is not large enough .",
    "note finally that the memory usage is increased by @xmath68 bits ( typically 4 bytes ) , due to the inverted file structure . in the following",
    ", the ivfadc method used jointly with our re - ranking method will be denoted by ivfadc+r .",
    "to evaluate ann search methods , we propose a new evaluation dataset available online : ` http://corpus-texmex.irisa.fr ` .",
    "this benchmark , called bigann , consists of 128-dimensional sift descriptors ( widely adopted image descriptors  @xcite ) extracted from approximately 1 million images .",
    "it comprises three distinct subsets :    * base vectors : one billion vectors to search in * query vectors : 10000 vectors that are submitted to the system * learning vectors : a set of 100 million vectors to compute the parameters involved in the indexing method .",
    "the groundtruth has been pre - computed : for each query , we provide the @xmath0 nearest neighbors that are obtained when computing exact euclidean distance , as well as their square distance to the query vector .",
    "the groundtruth for smaller sets ( @xmath69=1 m , 2 m , 5 m , 10 m , ... , 200 m vectors ) is also provided .",
    "as our own approach does not require many training vectors , we only used the first million vectors from the learning set .",
    "all measurements ( accuracy and timings ) were averaged over the 1000 first queries .",
    "the search quality is measured by the recall@@xmath70 measure , i.e. , the proportion of queries whose nearest neighbor is ranked in the first @xmath70 positions .",
    "the curve obtained by varying @xmath70 corresponds to the distribution function of the ranks , and the point @xmath70=1 corresponds nearest neighbors ( @xmath71 ) and not only the nearest neighbor .",
    "we do not include these measures in the paper , as qualitative conclusions for @xmath0=1 remain valid for @xmath71 .",
    "] to the `` precision '' measure used in  @xcite to evaluate ann methods . also , the recall@@xmath70 is the fraction of queries for which the nearest neighbor would be retrieved correctly if a short - list of @xmath72 vectors was verified using exact euclidean distances .",
    "the efficiency is measured by actual timings .      unless explicitly specified , our approach is evaluated by querying in the whole bigann set ( i.e. , one billion vectors ) .",
    "the performance of the adc and ivfadc algorithms are given for reference , and compared with the re - ranked versions adc+r and ivfadc+r .",
    "in all experiments , we have set @xmath0=10000 and @xmath41=20000 .",
    "in addition , for the ivfadc+r we have fixed @xmath65=8192 and @xmath66=64 , which means that the query is compared to approximately 1/128^th^ of the indexed vectors .",
    "* the re - ranking gain : * we first consider the improvement brought by the re - ranking stage compared with the reference methods . figure  [ fig : pqr_vs_pq ] shows the importance of this re - ranking stage on the recall@r measure : the performance of pq+r ( resp .",
    "ivfpq+r ) is significantly better than that of adc ( resp .",
    "ivfadc ) .",
    "these observations are confirmed by table  [ tab : m_m_user_times ] , which additionally shows that the impact of the re - ranking stage on the query time is limited . as already reported in  @xcite ,",
    "the ivfadc version is better than the adc method , at the cost of 4 additional bytes per indexed vector .",
    "as expected , this observation remains valid when comparing ivfadc+r with adc+r .",
    "[ fig : pqr_vs_pq ] ]    .performance and efficiency measured on 1 billion vectors , @xmath18=8 .",
    "the query time is measured in seconds per query .",
    "the timings validate the limited impact of the re - ranking stage on efficiency .",
    "[ tab : m_m_user_times ] [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     * impact of the dataset size : * figure  [ fig : rec_vs_nbase ] shows the impact of the database size on the recall@10 measure .",
    "the re - ranking stage becomes more important as the database grows in size , due to an increasing number of outliers .",
    "interestingly , the quality of the search degrades more gracefully when using a precise post - verification scheme ( with @xmath54=16 bytes ) .",
    "= 8.[fig : rec_vs_nbase ] ]",
    "in this paper , following recent works on multi - dimensional indexing based on source coding , we propose a method to re - rank the vectors with a limited amount of memory , thereby avoiding costly disk accesses . refining the neighbor hypotheses strongly improves recall at a rather low cost on response time .",
    "our experimental validation is performed on a new public dataset of one billion vectors .",
    "this work was partly realized as part of the quaero programme , funded by oseo , french state agency for innovation ."
  ],
  "abstract_text": [
    "<S> recent indexing techniques inspired by source coding have been shown successful to index billions of high - dimensional vectors in memory . in this paper </S>",
    "<S> , we propose an approach that re - ranks the neighbor hypotheses obtained by these compressed - domain indexing methods . </S>",
    "<S> in contrast to the usual post - verification scheme , which performs exact distance calculation on the short - list of hypotheses , the estimated distances are refined based on short quantization codes , to avoid reading the full vectors from disk .    we have released a new public dataset of one billion 128-dimensional vectors and proposed an experimental setup to evaluate high dimensional indexing algorithms on a realistic scale . </S>",
    "<S> experiments show that our method accurately and efficiently re - ranks the neighbor hypotheses using little memory compared to the full vectors representation .    </S>",
    "<S> nearest neighbor search , quantization , source coding , high dimensional indexing , large databases </S>"
  ]
}