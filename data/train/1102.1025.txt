{
  "article_text": [
    "the generalized ( also , interchangeably , nonadditive , deformed , or nonextensive ) statistics of tsallis has recently been the focus of much attention in statistical physics , complex systems , and allied disciplines [ 1 ] .",
    "nonadditive statistics suitably generalizes the extensive , orthodox boltzmann - gibbs - shannon ( b - g - s ) one . the scope of tsallis statistics has lately been extended to studies of lossy data compression in communication theory [ 2 ] and machine learning [ 3,4 ] .",
    "a critical allied concept is that of relative entropy , also known as kullback - leibler divergence ( k - ld ) , which constitutes a fundamental distance - measure in information theory [ 5 ] .",
    "the generalized k - ld [ 6 ] encountered in deformed statistics has been described by naudts [ 7 ] as a special form of f - divergences [ 8 ] .",
    "a related notion is that of bregman divergences [ 9 ] .",
    "these are information - geometric tools of great significance in a variety of disciplines ranging from lossy data compression and machine learning [ 10 ] to statistical physics [ 11 ] .",
    "the generalized k - ld in a tsallis scenario ( see eq . ( 6 ) of this letter ) is not a bregman divergence , which constitutes a serious shortcoming .",
    "this is unlike the case of the k - ld in the b - g - s framework , which is indeed a bregman divergence [ 10 ] .",
    "this forecloses the ability of the generalized k - ld to extend to the case of generalized statistics the bijection - property between exponential families of distributions and the k - ld , and other fundamental properties of bregman divergences , true in the b - g - s framework . _",
    "the consequence of the bijection property is that every regular exponential family corresponds to a unique and distinct bregman divergence ( one - to - one mapping ) , and , there exists a regular exponential family corresponding to every choice of bregman divergence ( onto mapping)_. the bijection property has immense utility in machine learning , feature extraction , and allied disciplines [ 10 , 12 , 13 ] .",
    "a recent study [ 14 ] has established that the dual generalized k - ld is a scaled bregman divergence in a discrete setting .",
    "further , ref .",
    "[ 14 ] has tacitly put forth the necessity of employing within the framework of generalized statistics the dual generalized k - ld ( see eq . ( 7 ) of this letter ) , a scaled bregman divergence , as the measure of uncertainty in analysis based on the minimum discrimination information ( minimum cross entropy ) principle of kullback [ 15 ] and kullback and khairat [ 16 ] . _",
    "scaled bregman divergences , formally introduced by stummer [ 17 ] and stummer and vajda [ 18 ] , unify separable bregman divergences [ 9 ] and f - divergences [ 8]_.    at this juncture , introduction of some definitions is in order . + * definition 1 * ( bregman divergences)[9 ] : let @xmath0 be a real valued strictly convex function defined on the convex set @xmath1 , the domain of @xmath0 such that @xmath0 is differentiable on @xmath2 , the relative interior of @xmath3 .",
    "the bregman divergence @xmath4 is defined as : @xmath5 , where : @xmath6 is the gradient of @xmath0 evaluated at @xmath7 .",
    "denotes the inner product .",
    "calligraphic fonts denote sets . ] + * definition 2 * ( notations)[18 ] : @xmath8 denotes the space of all finite measures on a measurable space @xmath9 and @xmath10 the subspace of all probability measures . unless otherwise explicitly stated _",
    "p_,_r_,_m _ are mutually measure - theoretically equivalent measures on @xmath9 dominated by a @xmath11-finite measure @xmath12 on @xmath9",
    ". then the densities defined by the radon - nikodym derivatives @xmath13 have a common support which will be identified with @xmath14 . unless stated otherwise , it is assumed that @xmath15 and that @xmath16 is a continuous and convex function .",
    "* definition 3 * ( scaled bregman divergences ) [ 18 ] _ the bregman divergence _ of probability measures _ p _ , _ r _ _ scaled _ by an arbitrary measure m on @xmath9 measure - theoretically equivalent with _ p _ , _ r _ is defined by @xmath17 } dm \\\\    = \\int_\\mathcal{x } { \\left [ { m\\phi \\left ( { \\frac{p}{m } } \\right ) - m\\phi \\left ( { \\frac{r}{m } } \\right ) - \\left ( { p - r } \\right)\\nabla \\phi \\left ( { \\frac{r}{m } } \\right ) } \\right ] } d\\lambda . \\\\   \\end{array}\\ ] ] the convex @xmath18 may be interpreted as the generating function of the divergence .    * definition 4*[19 , 20 ] : let @xmath9 be a measurable space while symbols @xmath19,@xmath20 denote probability measures on @xmath9 .",
    "let @xmath21 denote @xmath22-measurable functions on the finite set @xmath14 .",
    "a @xmath22-measurable function @xmath23 is said to be a probability density function ( pdf ) if @xmath24 . in this",
    "setting , the measure @xmath19 is induced by @xmath25 , i.e. , @xmath26 definition 4 provides a principled theoretical basis to seamlessly alternate between probability measures and pdf s as per the convenience of the analysis .",
    "the generalized k - ld is defined in the continuous form as [ 7 ] @xmath27d\\lambda } , \\ ] ] where @xmath25 is an arbitrary pdf , @xmath28 is the reference pdf , and @xmath29 is some nonadditivity parameter satisfying : @xmath30 . here",
    ", ( 1 ) employs the definition of the _ deduced logarithm _ [ 7 ] @xmath31 specializing the above theory to the case of tsallis scenario by setting @xmath32 yields the usual doubly convex generalized k - ld [ 6 ] @xmath33d\\lambda.\\ ] ] note that the normalization condition is : @xmath34 . this result",
    "is readily extended to the continuous case .",
    "the additive duality is a fundamental property in generalized statistics [ 1 ] .",
    "one implication of the additive duality is that it permits a deformed logarithm defined by a given nonadditivity parameter ( say , @xmath35 ) to be inferred from its _ dual deformed _ logarithm [ 1,7 ] parameterized by : @xmath36 .",
    "section 4 of this letter highlights an important feature of tsallis measures of uncertainty subjected to the additive duality when performing variational minimization .",
    "re - parameterizing ( 6 ) by specifying : @xmath37 yields the dual generalized k - ld '' denotes a re - parameterization of the nonadditivity parameter , and is not a limit . ]",
    "@xmath38d\\lambda\\\\   = \\int_{\\mathcal{x}}p\\ln_{q^*}\\left ( { \\frac{{p } } { { r } } } \\right)d\\lambda=\\int_{\\mathcal{x}}\\ln_{q^*}\\left ( { \\frac{{dp } } { { dr } } } \\right)dp = d_{k - l}^{q^ * }   \\left ( { \\left .",
    "p \\right\\|r } \\right).\\\\ \\end{array}\\ ] ]    * proposition 1 * : @xmath39 is jointly convex in the pair @xmath40 . given probability mass functions @xmath41 and @xmath42 , then @xmath43 @xmath44 $ ] .",
    "this result seamlessly extends to the continuous setting .",
    "an important issue to address concerns the manner in which expectation values are computed .",
    "nonextensive statistics has employed a number of forms in which expectations may be defined .",
    "prominent among these are the linear constraints originally employed by tsallis [ 1 ] ( also known as _ normal averages _ ) of the form : @xmath45 , the curado - tsallis ( c - t ) constraints [ 21 ] of the form : @xmath46 , and the normalized tsallis - mendes - plastino ( tmp ) constraints [ 22 ] ( also known as _",
    "@xmath35-averages _ ) of the form : @xmath47 .",
    "denotes an expectation . ]",
    "a fourth constraining procedure is the optimal lagrange multiplier ( olm ) approach [ 23 ] . of these four methods",
    "to describe expectations , the most commonly employed by tsallis - practitioners is the tmp - one .",
    "the originally employed normal averages constraints were abandoned because of difficulty in evaluating the partition function , except for very simple cases .",
    "the c - t constraints were replaced by the tmp constraints because : @xmath48 .",
    "recent works by abe [ 24 ] suggest that in generalized statistics expectations defined in terms of normal averages , in contrast to those defined by @xmath35-averages , are consistent with the generalized h - theorem and the generalized _ stosszahlansatz _ ( molecular chaos hypothesis ) . understandably , a re - formulation of the variational perturbation approximations in nonextensive statistical physics followed [ 25 ] , via an application of @xmath35-deformed calculus [ 26 ] .",
    "the minimum k - ld principle is of fundamental interest in information theory and allied disciplines .",
    "the nonadditive pythagorean theorem and triangular equality have been studied previously by dukkipati , _ et . al . _",
    "[ 27,28 ] .",
    "these studies were however performed on the basis of minimizing the generalized k - ld using questionable constraints defined by c - t expectations and @xmath35-averages . _ the pythagorean theorem is a fundamental relation in information geometry whose form and properties are critically dependant upon the measure of uncertainty employed , and , the manner in which expectations ( constraints ) are defined_.    this letter fundamentally differs from the studies in refs . [ 27 ] and [ 28 ] in a two - fold manner : @xmath49 the measure of uncertainty is the dual generalized k - ld ( a scaled bregman divergence ) , and @xmath50 the constraints employed are defined by normal average constraints , whose use in generalized statistics has been revived by the methodology of ferri , martinez , and plastino [ 29 ] .    at this stage , it is important to interpret the findings in ref .",
    "[ 24 ] within the context of the equivalence relationships between normal averages , c - t , @xmath35-averages , and olm forms of expectations derived in ref .",
    "first , while ref . [ 24 ] has suggested the inadequacy of @xmath35-averages on physics - based arguments , the equivalence relationships in [ 29 ] are purely mathematical in nature .",
    "next , [ 29 ] provides a mathematical framework to minimize lagrangians using the tsallis entropy employing normal averages expectations .",
    "a notable consequence of minimizing the generalized k - ld or the dual generalized k - ld using normal averages constraints is that the expression for the posterior probability is _ self - referential_[1 ] .",
    "specifically , the expression contains a function of the posterior probability , which is unknown and to be determined .",
    "fundamental differences in deriving the generalized pythagorean theorem in this letter _ vis -  - vis _ the analysis presented in refs.[27 ] and [ 28 ] lead to results which are qualitatively distinct from both an information - geometric as well as a statistical - physics perspectives .",
    "thus , this letter establishes the pythagorean decomposition of the dual generalized k - ld ( a scaled bregman divergence ) within the framework of deformed statistics for physically tenable normal averages expectations .",
    "such an analysis forms the basis to generalize the analysis in [ 12 ] for information theoretic co - clustering for mutual information based models . by definition , co - clustering",
    "involves clustering of data that inhabits a @xmath51 matrix . co - clustering has utility in a number of critical applications such as text clustering [ 30 ] , bio - informatics [ 31 ] , amongst others .",
    "note that for mutual information based models , defining the scaled bregman information as the normal averages expectation of the dual generalized k - ld [ 14 ] , the pythagorean theorem derived for the dual generalized k - ld in this letter provides the foundation to extend the optimality of minimum bregman information principle [ 12 ] , [ 32 ] which has immense utility in machine learning and allied disciplines , and , the bregman projection theorem to the case of deformed statistics . finally , the pythagorean theorem and the minimum dual generalized k - ld principle developed in this letter serve as a basis to generalize the concept of i - projections [ 33 ] to the case of deformed statistics .",
    "this introductory section concludes by establishing the qualitatively distinct nature of this letter :    * @xmath49__this letter generalizes and extends the analysis in ref .",
    "[ 14]__. in ref .",
    "[ 14 ] , it was shown that the dual generalized k - ld is a scaled bregman divergence .",
    "this was demonstrated in a discrete setting .",
    "the generalization is accomplished in section 3 by demonstrating that this property also holds true in a continuous setting .",
    "this is accomplished by expressing the radon - nikodym derivatives ( 1 ) as lebesgue integrals ( 3 ) .",
    "note that in a continuous measure - theoretic framework , the relationship ( ( 1 ) and ( 3 ) ) between probability densities and probability measures is transparent . the extension of the generalization of the results derived in ref .",
    "[ 14 ] is presented in sections 4 and 5 of this letter .",
    "+ section 4 takes advantage of the seamless relationship between probability densities and probability measures in a continuous setting to perform minimization of the dual generalized k - ld by employing ( 1 ) and ( 3 ) .",
    "first , the lagrangian for the minimum dual generalized k - ld defined by probability densities for normal averages expectations ( 17 ) , which is characterized by lebesgue integrals , is subjected to a straightforward transformation by invoking ( 1 ) and ( 3 ) .",
    "this step is followed by a simple minimization of the transformed lagrangian with respect to the probability measure , which yields the minimum dual generalized k - ld criterion ( 25 ) defined in terms of probability densities .",
    "+ this minimum dual generalized k - ld criterion is then employed as the basis to derive the legendre transform relations ( 26 ) .",
    "the legendre transform conditions , in conjunction with the shore expectation matching condition [ 34 ] , are central in deriving the pythagorean theorem for the dual generalized k - ld with normal averages constraints ( eq .",
    "( 40 ) in section 5 of this letter ) . at this stage",
    ", it is necessary to explain the tenability of employing the shore expectation matching condition in generalized statistics , given the finding in [ 24 ] that the shore - johnson axioms [ 35 ] ( notably axiom iii - system independence ) are not applicable in generalized statistics which models complex systems whose elements have strong correlations with each other . in the shore expectation matching condition ( see section 5 of this letter ) , the correlations and interactions between elements are self - consistently incorporated into the probability density with which the expectation is evaluated .",
    "specifically , the probability density is unambiguously determined during the process of minimizing the dual generalized k - ld , using normal averages constraints .",
    "thus , the shore expectation matching condition is not adversely affected by the inapplicability of the shore - johnson axioms when utilized in deformed statistics .",
    "+ * @xmath50 as stated above , the basis for establishing the dual generalized k - ld as a scaled bregman divergence , and , the subsequent derivation of the pythagorean theorem for normal averages expectations is motivated by extending the theory of i - projections [ 33 ] to the case of generalized statistics , and the derivation of iterative numerical schemes ( such as iterative scaling , alternating divergence minimization , and the em algorithm ) based on a candidate deformed statistics theory of i - projections [ 36 ] . for this ,",
    "the candidate deformed statistics i - divergence between two probability densities @xmath25 and @xmath35 is to be _ strictly convex_. + this is true for the case of the usual k - ld , the generalized k - ld , and as stated in proposition 1 of this section , also holds true for the dual generalized k - ld . in ref .",
    "[ 7 ] , a form of a generalized k - ld which is bregman divergences has been derived , and employed with normal averages constraints in ( for example , baci , arda , and server [ 37 ] ) .",
    "however , it is convex only in terms of one variable and is unsuitable to the primary leitmotif of this study , i.e. generalizing i - projections and the above stated iterative numerical schemes [ 33 , 36 ] to the case of deformed statistics .",
    "this form of the generalized k - ld which is a bregman divergence does appear to have applications in other disciplines , as demonstrated by ref .",
    "[ 37 ] , amongst other works .",
    "the essential concepts around which this communication revolves are reviewed in the following sub - sections .      by definition , the tsallis entropy ,",
    "is defined in terms of discrete variables as [ 1 ] @xmath52 the constant @xmath53 is referred to as the nonadditive parameter . here",
    ", ( 9 ) implies that extensive b - g - s statistics is recovered as @xmath54 . taking the limit @xmath55 in ( 9 ) and invoking lhspital s rule , @xmath56 , i.e. , the shannon entropy .",
    "nonextensive statistics is intimately related to _ q - deformed _ algebra and calculus ( see [ 26 ] and the references within ) .",
    "the _ q - deformed _ logarithm and exponential are defined as [ 26 ] @xmath57^{\\frac{1}{{1 - q } } } ; 1 + \\left ( { 1 - q } \\right)x \\ge 0 \\\\   0;otherwise , \\\\   \\end{array } \\right .",
    "\\end{array}\\ ] ] in this respect , an important relation from _ q - deformed _ algebra is the @xmath35-deformed difference [ 26 ] @xmath58 the tsallis entropy may be written as [ 1 ] @xmath59    this letter makes prominent use of the _ additive duality _ in nonextensive statistics . setting @xmath60 , from ( 11 )",
    "the _ dual deformed _ logarithm and exponential are defined as @xmath61    the dual tsallis entropy , and , the dual generalized k - ld may thus be written as @xmath62 = \\int_{\\mathcal{x}}{p } \\ln _ { q^ * } ( \\frac{{p}}{{r}})d\\lambda , \\end{array}\\ ] ] respectively .",
    "note that the dual tsallis entropy acquires a form identical to the b - g - s entropies , with @xmath63 replacing @xmath64 [ 2 ] .",
    "* theorem 1 * : let @xmath65 , and , @xmath66 being the scaling . for the convex generating function of the scaled bregman divergence : @xmath67",
    ", the scaled bregman divergence acquires the form of the dual generalized k - ld : @xmath68 .    * proof * : + from ( 1 ) and ( 2 ) @xmath69}dm   \\\\ = \\int_{\\mathcal{x}}{\\left [ { p \\ln _ { q^ *   } \\frac{{p } } { { m } } - p \\ln _ { q^ *   } \\frac{{r } } { { m } } - \\left ( { p   - r } \\right)\\left ( { \\frac{{r } } { { m } } } \\right)^{1 - q^ *   } } \\right]}d\\lambda   \\\\",
    "\\mathop   = \\limits^{\\left ( a \\right ) } \\int_x { \\left\\ { { pm^{q^ *    - 1 } \\left [ { \\ln _ { q^ *   } p - \\ln _ { q^ *   } r } \\right ] - \\left ( { p - r } \\right)\\left ( { \\frac{r}{m } } \\right)^{1 - q^ *   } } \\right\\}d\\lambda } , \\end{array}\\ ] ] where @xmath70 implies invoking the @xmath35-deformed difference ( 11 ) with @xmath71 replacing @xmath35 . setting @xmath72 in the integrand of ( 15 ) and re - invoking ( 11 ) yields ( 7 ) @xmath73 this is a @xmath71-deformed f - divergence and is consistent with the theory derived in refs . [ 17 ] and [ 18 ] , when extended to deformed statistics in a continuous setting .",
    "consider the lagrangian @xmath74 where @xmath75 are some @xmath22-measurable observables . in the second relation in",
    "( 17 ) , @xmath70 implies invoking ( 3 ) and definition 4 .. ] here , the normal average expectations are defined as @xmath76 the variational minimization with respect to the probability measure @xmath19 acquires the form    @xmath77    thus @xmath78 thus , the posterior probability minimizing the dual generalized k - ld is @xmath79 _ here , ( 21 ) highlights the operational advantage in employing dual tsallis measures of uncertainty , since they readily yield the @xmath71-deformed exponential form as a consequence of variational minimization when using normal average constraints .",
    "_ multiplying ( 19 ) by @xmath80 , integrating with respect to the measure @xmath81 , and invoking ( 18 ) and the normalization condition : @xmath82 yields @xmath83 from ( 21 ) and ( 22 ) , the canonical partition function is @xmath84 _ note that @xmath85 and @xmath86 are to be evaluated @xmath87_. this feature is exhibited by the variational minimization of generalized k - ld s and generalized mutual informations employing normal average constraints [ 2,3 ] . from ( 23 ) @xmath88 from ( 21 ) , ( 22 ) , and ( 24 ) , it is evident that the form of the posterior probability minimizing the dual generalized k - ld is _ self - referential _ [ 1 ] .",
    "further , for : @xmath89 < 0 $ ] , the canonical posterior probability in ( 21 ) : @xmath90 .",
    "this is known as the _ tsallis cut - off condition _ [ 1 ] . substituting ( 24 ) into ( 22 ) yields the minimum dual generalized k - ld @xmath91 =    \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left ( { x,\\tilde\\beta _ m^{q^ *   } \\left ( x \\right ) } \\right ) } } } \\right ) - \\sum\\limits_{m = 1}^m { \\beta _ m \\left\\langle { u_m } \\right\\rangle } .\\ ] ] from ( 25 ) , the following legendre transform relations are obtained @xmath92}}{{\\partial \\left\\langle { u_m } \\right\\rangle } } =   - \\beta _ m ,   \\\\",
    "\\frac{\\partial } { { \\partial \\beta _ m } } \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left ( { x,\\tilde \\beta _ m^{q^ *   } \\left ( x \\right ) } \\right ) } } } \\right ) =    \\left\\langle { u_m } \\right\\rangle . \\\\",
    "\\end{array}\\ ] ]",
    "* theorem 2 * : let @xmath93 be the prior probability distribution , and @xmath80 be the posterior probability distribution that minimizes the dual generalized k - ld subject to a set of constraints @xmath94 let @xmath95 be any other ( _ unknown _ ) distribution satisfying the constraints @xmath96 then + @xmath49 @xmath97 $ ] is minimum only if ( shore expectation matching condition ) @xmath98 @xmath50 from ( 29 ) @xmath99 = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] + d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . } \\right ]",
    "\\\\    + \\left ( { 1 - q^ *   } \\right)d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . } \\right]d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] .",
    "\\\\   \\end{array}\\ ] ]    * proof * : taking the difference between the dual generalized k - ld s yields @xmath99 - d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] \\\\    = \\int_{\\mathcal{x } } { l\\left ( x \\right)\\left [ { \\ln _ { q^ *   }    \\left ( { \\frac{{l\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right ) -    \\ln _ { q^ *   } \\left ( { \\frac{{l\\left ( x \\right)}}{{p\\left ( x \\right ) } } } \\right ) } \\right ] } d\\lambda \\left ( x \\right ) , \\\\   \\end{array}\\ ] ] while multiplying and dividing the integrand of ( 31 ) by @xmath100 $ ] leads to @xmath99 - d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] \\\\   = \\int_{\\mathcal{x } } { l\\left ( x \\right)\\left\\ { { \\frac{{\\left [ { \\ln _ { q^ *   } \\left ( { \\frac{{l\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right ) - \\ln _ { q^ *   } \\left ( { \\frac{{l\\left ( x \\right)}}{{p\\left ( x \\right ) } } } \\right ) } \\right]}}{{1 + \\left ( { 1 - q^ *   } \\right)\\ln _ { q^ *   } \\left ( { \\frac{{l\\left ( x \\right)}}{{p\\left ( x \\right ) } } } \\right ) } } } \\right . }",
    "\\left . { \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\ln _ { q^ *   } \\left ( { \\frac{{l\\left ( x \\right)}}{{p\\left ( x \\right ) } } } \\right ) } \\right ] } \\right\\}d\\lambda \\left ( x \\right ) . \\\\   \\end{array}\\ ] ]    invoking now the definition of the @xmath71-deformed difference from ( 11 ) ( by replacing @xmath35 with @xmath71 ) results in : @xmath101 .",
    "thus , after re - arranging the terms ( 32 ) results in @xmath102 = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ] \\\\   -\\int_{\\mathcal{x } } { l\\left ( x \\right)\\left\\ { { \\ln _ { q^ *   } \\left ( { \\frac{{p\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right)\\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\ln _ { q^ *   } \\left ( { \\frac{{l\\left ( x \\right)}}{{p\\left ( x \\right ) } } } \\right ) } \\right ] } \\right\\}d\\lambda \\left ( x \\right ) }",
    "\\\\ = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ]    - \\int_{\\mathcal{x } } { \\left\\ { { l\\left ( x \\right)\\ln _ { q^ *   } \\left ( { \\frac{{p\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right ) } \\right . }   \\\\   \\left .",
    "{ + \\left ( { 1 - q^ *   } \\right)\\ln _ { q^ *   } \\left ( { \\frac{{p\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right)d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] } \\right\\}d\\lambda \\left ( x \\right ) . \\\\   \\end{array}\\ ] ] at this point we expand ( 33 ) and invoke ( 19 ) , ( 24 ) , and ( 28 ) to arrive at @xmath102",
    "\\\\    = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ] - \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left (   \\bullet   \\right ) } } } \\right)\\int_x { l\\left ( x \\right)d\\lambda \\left ( x \\right ) }   \\\\    + \\int_{\\mathcal{x } } { l\\left ( x \\right)\\sum\\limits_{m = 1}^m { \\beta _ m u_m(x ) } d\\lambda \\left ( x \\right ) }   \\\\    - \\left ( { 1 - q^ *   } \\right)\\ln _ { q^ *   } \\left ( { \\frac{{p\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right)d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] \\\\    = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ] - \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left (   \\bullet   \\right ) } } } \\right ) + \\sum\\limits_{m = 1}^m { \\beta _ m \\left\\langle { w_m } \\right\\rangle }   \\\\   - \\left ( { 1 - q^ *   } \\right)d_{k - l}^{q^ *   } \\left [ { l\\left\\| p \\right . } \\right]\\ln _ { q^ *   } \\left ( { \\frac{{p\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right ) , \\end{array}\\ ] ] where : @xmath103 .",
    "note that : @xmath104 . multiplying and dividing the fourth term on the rhs of ( 34 ) by @xmath80 and integrating over the measure @xmath81 yields @xmath102 \\\\",
    "= d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ] - \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left (   \\bullet   \\right ) } } } \\right ) + \\sum\\limits_{m = 1}^m { \\beta _ m \\left\\langle { w_m } \\right\\rangle }   \\\\   - \\left ( { 1 - q^ *   } \\right)d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right]\\frac{{\\int_{\\mathcal{x } } { p\\left ( x \\right)\\ln _ { q^ *   } \\left ( { \\frac{{p\\left ( x \\right)}}{{r\\left ( x \\right ) } } } \\right)d\\lambda \\left ( x \\right ) } } } { { \\int_{\\mathcal{x } } { p\\left ( x \\right)d\\lambda \\left ( x \\right ) }   } } .",
    "\\end{array}\\ ] ] now , setting @xmath82 , ( 35 ) acquires the form @xmath102",
    "\\\\   = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ] - \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left (   \\bullet   \\right ) } } } \\right ) + \\sum\\limits_{m = 1}^m { \\beta _ m \\left\\langle { w_m } \\right\\rangle }   \\\\   - \\left ( { 1 - q^ *   }   \\right)d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . }",
    "\\right]d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . }",
    "\\right ] , \\end{array}\\ ] ] and , with the aid of ( 25 ) , ( 36 ) yields @xmath102 = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . }",
    "\\right ] \\\\   - \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left (   \\bullet   \\right ) } } } \\right ) + \\sum\\limits_{m = 1}^m { \\beta _ m \\left\\langle { w_m } \\right\\rangle }   \\\\    - \\left ( { 1 - q^ *   } \\right)\\left ( { \\ln _ { q^ *   } \\left ( { \\frac{1}{{\\tilde z\\left (   \\bullet   \\right ) } } } \\right ) - \\sum\\limits_{m = 1}^m { \\beta _ m \\left\\langle { u_m } \\right\\rangle } } \\right)d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ] .",
    "\\\\   \\end{array}\\ ] ] the minimum dual generalized k - ld condition is @xmath105}}{{\\partial \\beta _ m } } = 0.\\ ] ] this implies that the posterior pdf @xmath25 whose canonical form is given by ( 21 ) not only minimizes : @xmath106 $ ] , but also minimizes : @xmath107 $ ] as well .",
    "subjecting ( 37 ) to ( 38 ) and invoking the second legendre transform relation in ( 26 ) yields the shore expectation matching condition [ 34 ] for the dual generalized k - ld @xmath98 substituting now ( 39 ) into ( 37 ) and invoking ( 25 ) allows one to write @xmath102 = d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . } \\right ] - d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . }",
    "\\right ] \\\\    - \\left ( { 1 - q^ *   } \\right)d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . }",
    "\\right]d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . } \\right ]",
    ". \\\\   \\end{array}\\ ] ] the pythagorean theorem for the dual generalized k - ld with normal average constraints has two distinct regimes , depending upon the range of the dual nonadditive parameter @xmath99",
    "\\ge d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . }",
    "\\right ] + d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . }",
    "\\right];q^ *    > 1 , \\\\",
    "d_{k - l}^{q^ *   } \\left [ { l\\left\\| { r } \\right . }",
    "\\right ] \\le d_{k - l}^{q^ *   } \\left [ { l\\left\\| { p } \\right . }",
    "\\right ] + d_{k - l}^{q^ *   } \\left [ { p\\left\\| { r } \\right . }",
    "\\right];0 < q^ *    < 1 . \\\\",
    "\\end{array}\\ ] ] while theorem 2 is called the pythagorean theorem , ( 30 ) is referred to as the nonadditive triangular equality for the dual generalized k - ld .",
    "it is interesting to note that the expectation - matching condition ( 29 ) has a form identical to the case of the b - g - s model ( @xmath108 ) , and differs from that of the pythagorean theorem for the  usual \" form of the generalized k - ld for the case of constraints defined by c - t expectations and @xmath35-averages , respectively [ 27 , 28 ] . also to be noted",
    "is the fact that the minimum dual generalized k - ld condition ( 38 ) is guaranteed .",
    "this differs from the case of the @xmath35-averages constraints derived in previous works [ 27 , 28 ] .",
    "this feature is of importance when generalizing the minimum bregman information principle to the case of deformed statistics .",
    "this letter has proven that the dual generalized kullback - leibler divergence ( k - ld ) is a scaled bregman divergence , within a measure - theoretic framework .",
    "also , the pythagorean theorem for the dual generalized k - ld has been established from normal average constraints which are consistent with both the generalized h - theorem and the generalized _ stosszahlansatz _ ( molecular chaos hypothesis ) [ 24 ] .",
    "qualitative distinctions of the present treatment _ vis -  - vis _ previous studies have been briefly discussed .",
    "ongoing work serves a two - fold objective : @xmath49 the pythagorean theorem for the dual generalized k - ld derived herein has been employed to provide a deformed statistics information geometric description of plefka s expansion in mean - field theory [ 38 ] . while details of this analysis are beyond the scope of this letter , only a cursory overview of this analysis is presented herein .    extending the procedure followed in [ 38 ] to obtain the mean - field equations [ 39 ] , to the case of generalized statistics , a _",
    "deformed statistics mean - field criterion _ is obtained in terms of minimizing a dual generalized k - ld .",
    "this is accomplished by extrapolation of the information geometric arguments in [ 40 ] to the case of deformed statistics .",
    "application of the pythagorean theorem ( 40 ) results in a _",
    "modified deformed statistics mean - field criterion _ , which when subjected to a perturbation expansion employing results of the @xmath35-deformed variational perturbation theory developed in [ 25 ] , yields candidate deformed statistics mean - field equations ; @xmath50 the results obtained in this letter serve as the foundation to extend the sufficient dimensionality reduction model [ 41 ] to the case of deformed statistics",
    ". results of these studies will be published elsewhere .",
    "a. dukkipati , m. narasimha murty , and s. bhatnagar , `` properties of kullback- leibler cross - entropy minimization in nonextensive framework '' , _ proceedings of ieee international symposium on information theory(isit ) _ , 2374 , ieee press , 2005 .",
    "j. e. shore and r. w. johnson , ieee trans . inf .",
    "th . , * it-26 * ( 1980 ) 26 ; * it-27 * ( 1981 ) 472 ; * it-29 * ( 1983 ) 942 .",
    "i. csiszr , p. c. shields , information theory and statistics : a tutorial . foundations and trends in communications and information",
    "theory , * 1 * 4 ( 2004 ) 417 ."
  ],
  "abstract_text": [
    "<S> the generalized kullback - leibler divergence ( k - ld ) in tsallis statistics [ constrained by the additive duality of generalized statistics ( dual generalized k - ld ) ] is here reconciled with the theory of bregman divergences for expectations defined by normal averages , within a measure - theoretic framework . </S>",
    "<S> specifically , it is demonstrated that the dual generalized k - ld is a scaled bregman divergence . </S>",
    "<S> the pythagorean theorem is derived from the minimum discrimination information - principle using the dual generalized k - ld as the measure of uncertainty , with constraints defined by normal averages . </S>",
    "<S> the minimization of the dual generalized k - ld , with normal averages constraints , is shown to exhibit distinctly unique features .    </S>",
    "<S> generalized tsallis statistics , additive duality , dual generalized kullback - leibler divergence , scaled bregman divergences , pythagorean theorem .    </S>",
    "<S> pacs : 05.20.-y ;  89.70.-a </S>"
  ]
}