{
  "article_text": [
    "generalized entropies , such as the renyi and tsallis entropies , have been studied in different aspects of statistical physics  @xcite and non - linear dynamics  @xcite . in information theory , these entropies are viewed as a generalizations of the shannon entropy that are potentially useful in particular problems .",
    "many problems require the comparison of the divergence ( or , its opposite , the similarity ) between two or more signals , a problem that can be quantified through the use of divergence measures based on generalized ( joint ) entropies , e.g. in analysis of dna sequences  @xcite or image processing  @xcite .",
    "a traditional and increasingly important application of information theory is the analysis of ( signals based on ) natural language  @xcite .",
    "this analysis often happens at the level of words , i.e. , in which each word ( type ) is considered a different symbol of analysis .",
    "one important statistical feature in the statistical analysis of word frequencies is the existence of linguistic laws  @xcite , i.e. , statistical regularities observed in a variety of databases .",
    "the most famous case is zipf s law , which specifies how the frequencies of words are distributed  @xcite .    in this paper",
    "we explore the implications of linguistic laws to the computation of information - theoretic measures in written text .",
    "while information - theoretic approaches typically measure the similarity of an ensemble of words ( the vocabulary ) , we show how generalized entropies can be used to assess the influence of individual words to these ( global ) measures , providing a bridge to the studies on evolution of language following trajectories of individual words  @xcite .",
    "in particular , we show how the contribution of individual words , appearing in different scales of frequency , vary in the different generalized entropies .",
    "we explore the implications of our findings to two problems : ( i ) the best generalized entropy for highlighting the contribution of physics keywords ; and ( ii ) determining how large a given database has to be in order obtain sufficient coverage / sampling of the generalized entropies .",
    "we are interested in extracting information about written documents based on the number of times @xmath0 each word @xmath1 appears in each database . for each database , we denote by @xmath2 the frequency of the word @xmath3 ( i.e. , @xmath4 ) , which we consider to be an estimator of the probability @xmath5 of occurrence of this word in the generative process underlying the production of the texts .",
    "we say that the word @xmath3 has rank @xmath6 if it is the @xmath7 most frequent word .",
    "different databases show similar distributions of word frequencies , a statistical regularity also known as zipf s law . while zipf originally proposed the simple relationship @xmath8 , more recent analysis in large text databases suggest that the data is better described by a double power - law ( dp ) distribution  @xcite @xmath9 where @xmath10 and @xmath11 are free parameters , @xmath12 is the normalization constant ( which can be approximated as @xmath13 , and @xmath14 is the @xmath10-th generalized harmonic number  @xcite . the more common single - power - law distribution is recovered for @xmath15 and our results below apply in this limit as well . in plots and numerical calculations we use the distribution  ( [ eq.modeldp ] ) with @xmath16 and @xmath17 , values obtained in ref .",
    "@xcite for english books published in different centuries . in fig .",
    "[ fig.1 ] we show that the modified zipf s law indeed provides good account of different databases .",
    "+    [ fig.1 ]      in line with the long - tradition of information theory , we use entropies to quantify the amount of information contained in written texts . here",
    "we consider the generalized entropy of order @xmath18  @xcite @xmath19 where @xmath20 , the sum runs over all words for which @xmath21 , and @xmath18 is a free parameter yielding a spectrum of entropies . for @xmath22",
    "we recover the gibbs - shannon entropy , i.e. @xmath23 . in physics ,",
    "( [ eq.halpha ] ) is known as tsallis entropy  @xcite and has been proposed as a ( non - extensive ) generalization of the traditional statistical mechanics .",
    "we are particularly interested in using @xmath24 to quantify the distance ( or dissimilarity ) between different databases . here",
    "we focus on the generalized jensen - shannon divergence  @xcite @xmath25 where @xmath26 and @xmath27 are the word frequencies of the two databases and @xmath28 is obtained summing over all symbols for which either @xmath29 or @xmath30 .",
    "we focus on @xmath31 because @xmath32 can be shown to be a metric for @xmath33 , i.e. , it is positive @xmath34 ( with @xmath35 if and only if @xmath36 ) , symmetric @xmath37 , and @xmath38 satisfies the triangular inequality  @xcite .",
    "we expect our main results to apply also to other quantities obtained from @xmath39 and @xmath40 , such as the generalized mutual information and kullback - leibler divergence  @xcite .",
    "the usual ( @xmath22 , jensen - shannon ) divergence is a traditional method in different statistical analysis of natural language  @xcite . for generalized entropies , increasing ( decreasing )",
    "@xmath18 one increases ( decreases ) the weight of the most frequent words allowing for different insights into the relationship between the databases  @xcite .",
    "the goal of this paper is to investigate the consequences of known properties of word statistics to the computation of generalized entropic measures . for instance , the number of different words is virtually unbounded and therefore we should carefully consider finite - size effects and the role played by the number of observed symbols in our analysis  @xcite . more specifically ,",
    "we explore the consequences of zipf s law  as reviewed in sec .",
    "[ ssec.zipf ]  to the computation of the information - theoretic measures based on @xmath24  reviewed in secs .",
    "[ ssec.halpha ] and  [ ssec.dalpha ] . in ref .",
    "@xcite we have shown that zipf s law implies that finite - size estimators of @xmath24 and @xmath31 scale very slowly with database size . here",
    "we focus on the contribution of individual words to @xmath24 and @xmath31 , showing how different frequency ranges dominate the estimation for different values of @xmath18 .",
    "the entropy  ( [ eq.halpha ] ) is uniquely defined by the frequency of the words @xmath41 . from the double power - law ( dp )",
    "frequency distribution , eq .",
    "( [ eq.modeldp ] ) , we obtain @xmath42 with @xmath43 and @xmath44 where @xmath45 is the riemann zeta function and the right hand side is obtained approximating the sum by the integral and is valid for @xmath46 ( where @xmath47 ) .",
    "the divergence of @xmath24 for @xmath48 appears because the sum / integral diverges for @xmath49 ( i.e. , for a growing number of different words ) . a comparison between @xmath24 in real data and @xmath50 is shown in fig .",
    "[ fig.2](a ) .",
    "the difference between the theory and the data for @xmath51 is due to the finite number of symbols in the database .",
    "this is a finite - size effect that depends sensitively on the size of the database used to estimate @xmath41 .",
    "we now focus on the contribution of individual words for @xmath24 . to do that",
    ", we take advantage of the fact that @xmath52 can be written as a sum over different words and consider the ratio @xmath53 as a proxy for the contribution of the first @xmath6 terms to the computation of @xmath24 . for the case of the double power - law distribution @xmath54",
    ", we obtain that @xmath55 for @xmath56 we can approximate the sum @xmath57 by an integral and obtain @xmath58 in fig .  [ fig.2](b ) we show the dependence of @xmath59 and @xmath60 on @xmath6 for different values of @xmath18 . a deviation due to finite - size effects",
    "is again observed when @xmath61 ( finite database size ) .",
    "the analysis of @xmath59 reveals a convergence that varies dramatically with @xmath18 ( see also refs .",
    "@xcite ) , suggesting that for different @xmath18 s different ranges in @xmath62 contribute to @xmath24 .",
    "one quantity of interest is the rank @xmath63 so that @xmath64 accounts for a fraction @xmath65 of the effect , e.g. , for @xmath66 we have that @xmath67 meaning that the first @xmath68 terms are responsible for @xmath69 of the total @xmath70 . for small @xmath65 or large @xmath18 , such that @xmath71 , @xmath68 is obtained from the first line of eq .",
    "( [ eq.rdp ] ) as the solution of @xmath72 for large @xmath65 or small @xmath18 , such that @xmath73 , @xmath68 can be obtained explicitly from eq .",
    "( [ eq.rhalpha ] ) as @xmath74 the estimations  ( [ eq.rstarhss1 ] ) and  ( [ eq.rstarh ] ) , which are based on the double power - law distribution  ( [ eq.modeldp ] ) , and the results obtained in the data are shown in fig .  [ fig.2](c ) .",
    "we see that for @xmath22 one typically needs around 200,000 different word types in order to obtain @xmath69 of the asymptotic value of @xmath59 .",
    "this number quickly decays with @xmath18 so that for @xmath75 , the 100 most frequent words lead to the same relative contribution and therefore all other words are irrelevant in practice .              the divergence @xmath31 defined in eq .",
    "( [ eq.jsd ] ) quantifies how dissimilar two databases are ( @xmath26 and @xmath27 ) and the distribution of frequencies in these databases alone does not specify @xmath31 . still , we expect the general shape of zipf s law in eq .",
    "( [ eq.modeldp ] ) to affect the statistical properties of @xmath31 . here",
    "we explore this connection by following steps similar to those performed in the previous section for @xmath24 . to do this",
    ", it is convenient to introduce the relative coordinates @xmath76 , where @xmath77 and @xmath78 , such that : @xmath79 this equation emphasizes that @xmath31 is computed as a sum over a contribution @xmath80 of different words ranked by @xmath6 .",
    "we order the words according to the rank @xmath6 of the word in @xmath41 , i.e. , if a word has rank @xmath81 it means that there are exactly @xmath82 other words for which the average frequency @xmath83 .    the relative contribution @xmath84 of the top @xmath6 words to @xmath31 is given by @xmath85 which is analogous to eq .",
    "( [ eq.r ] ) but in this case @xmath80 is not necessarily monotonically decaying with @xmath6 .",
    "we finally define @xmath86 as the rank at which a fraction @xmath65 of the total @xmath31 is achieved , i.e. @xmath87 .    figure  [ fig.3 ] shows our analysis of the divergence ( @xmath31 , @xmath84 , and @xmath86 ) for two pairs of databases ( books2000books1900 and books2000physics , see caption of fig .  [ fig.1 ] for details on the data ) .",
    "the left panel shows that the divergence @xmath31 for books2000physics is systematically larger than for books2000books1900 suggesting that stylistic and topical differences between books and scientific papers are more significant than historical changes in the language throughout the 20-th century .",
    "the most striking feature of fig .  [ fig.3 ] is the similarity between the results obtained with different data ( e.g. , the variation across the databases is much smaller than the variation across @xmath18 or @xmath6 ) .",
    "furthermore , the general behavior observed for @xmath31 resembles the results shown in fig .",
    "[ fig.2 ] for @xmath24 , which were analytically computed from the word - frequency distribution  ( [ eq.modeldp ] ) .",
    "the @xmath31-observation , however , depends not only on the word frequencies  @xmath2 but also on the variation @xmath88 across databases .",
    "next we consider two very simplistic models for @xmath88 in order to understand these observations .",
    "[ [ constant - relative - fluctuation . ] ] constant relative fluctuation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a simple assumption is that the relative fluctuations across databases are the same for each word independent of its frequency , in which case @xmath89 is proportional to the average frequencies @xmath41 and thus @xmath90 in this case we obtain from  ( [ eq.ddelta ] ) that @xmath91 where the approximation is valid for @xmath92 .",
    "now we notice that @xmath41 is the word frequency distribution of the combined database and that therefore it should also be well approximated by the generalized zipf s law  ( [ eq.modeldp ] ) .",
    "even if this model is too simplistic to account for the observed @xmath31 ( see dotted line in the left panel of fig .",
    "[ fig.3 ] ) , it shows how the statistical properties of @xmath31 and of @xmath24 can be connected to each other .",
    "[ [ log - corrected - fluctuations . ] ] log - corrected fluctuations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to get some insights on the reason for the failure of the previous model , we look at the empirical relative fluctuation @xmath93 for the two pairs of databases described above . the results in fig .",
    "[ fig.4 ] show two features : an expected large fluctuation around different words and a surprising decay of relative fluctuation with @xmath2 .",
    "the roughly linear decay in the semi - logarithmic plot suggests that an improvement of eq .",
    "( [ eq.a ] ) is obtained including a logarithmic correction as @xmath94 .",
    "since @xmath88 is bounded from above by @xmath2 ( i.e. @xmath95 ) we introduce a lower cutoff frequency in our log - corrected model @xmath96 where we empirically find that @xmath97 and @xmath98 capture the main qualitative behaviour shown in fig .",
    "[ fig.4 ] .",
    "the log - corrected model , obtained combining eq .",
    "( [ eq.alog ] ) with the generalized zipf s law  ( [ eq.modeldp ] ) , provides a much better account of the results in the three panels of fig .",
    "[ fig.3 ] .",
    "this shows that the weak dependence of the relative fluctuations on the frequency is crucial in order to understand the results in fig .",
    "[ fig.3 ] .",
    "our results shows that the zipf s law is responsible for the general statistical properties of both @xmath24 and @xmath31 .",
    "one consequence of this result is that the contribution of ( a set of ) particular words is also pre - determined by zipf s law and depends largely on the range of frequencies of the words .",
    "consider the problem of comparing the divergence between the corpus of scientific papers in physics to a general corpus of books written in english .",
    "one of the effects one may want to capture when computing @xmath31 is the over - representation of physics - related words in the database of physics articles , i.e. , the fact that @xmath99 for words @xmath3 related to physics .",
    "we denote this set of words as physics keywords .",
    "this is not the only effect contributing to the divergence @xmath31 between the texts , e.g. , stylistic effects affecting the most frequent words ( so - called stopwords ) may also be relevant . here",
    "we wish to quantify the effect of physics keywords to @xmath31 in comparison to a set of stopwords .",
    "the key insight that connects this problem to our results is that physics keywords are typically distributed in a specific range of frequencies .",
    "for instance , we compiled a list of @xmath100 physics keywords from all words appearing in the pacs system ( removing a list of common stop words ) .",
    "as illustrated in the fig .",
    "[ fig.5](left panel ) the words range from _ electron _  with rank @xmath101 and frequency of one every thousand words @xmath102  to _ gravitation _  with rank @xmath103 and frequency of one every hundred thousand words @xmath104 .",
    "most physics keywords lie in between these two frequencies . by increasing @xmath18 from @xmath105 one moves from a configuration in which @xmath31 and @xmath24 are dominated by the least frequent words to a configuration in which @xmath31 and @xmath24 are determined mostly by the most frequent stopwords ( e.g. , for @xmath106 ) .",
    "indeed , the results in fig .  [ fig.5](right panel ) confirm that the contribution of the physics keywords has a maximum around @xmath107 . at the maximum ,",
    "these @xmath100 keywords contribute with more than @xmath108 of the total value of @xmath31 .",
    "this value is comparable to the contribution of the @xmath109 most frequent words ( stopwords ) at the same value of @xmath18 .",
    "the contribution of the stopwords quickly increases with @xmath18 and completely dominates @xmath31 for @xmath110        [ fig.5 ]      when computing @xmath24 and @xmath31 one usually aims at characterizing the properties of the source ( stochastic process ) underlying the data .",
    "stationarity and ergodicity of this process imply that computed values should converge for increasing database size .",
    "in practice , we are not interested in results which depend mainly on the size of the database , and that change dramatically with the amount of available data . below",
    "we show how our results allow for an estimation of the database size required to provide a reliable estimation of @xmath31 .",
    "the most important effect of changing the database size is to increase the number of different words found in the databases .",
    "this simple observation , the cornerstone of our analysis , has two ramifications .",
    "first , it implies that a necessary condition for a robust estimation of @xmath31 is that @xmath111 , i.e. the number of observed different words @xmath112 needs to be larger than the number of ranks @xmath6 needed to estimate a fraction @xmath113 of @xmath31 .",
    "second , a connection to the size of the database  @xmath114 ( measured in number of word tokens ) is possible through heaps law , which states that the number of different words grows sublinear with the total number of words ,  @xmath115  @xcite . in fig .",
    "[ fig.6 ] we present the result of this analysis , in which @xmath116 was obtained from the double - power - law distribution with log - corrected fluctuations ( as in fig .",
    "[ fig.3 ] ) and the heaps law relationship derived in ref .",
    "@xcite .",
    "the main message of this paper is that the characteristic shape of word - frequency distributions ( @xmath117 following zipf s law ) plays a dominant role in the properties of information - theoretic measures computed in texts at the level of words . while there is a one - to - one relationship between @xmath117 and entropies @xmath24  given in eq .",
    "( [ eq.hdp ] )  here we showed that a close connection exists also between @xmath117 and measures intended to compare databases such as @xmath31 , a result that presumably extends also to other measures such as the mutual information and kullback - leibler divergence .",
    "the influence of @xmath117 occurs not only in the convergence of finite - size estimators , as reported previously in refs .",
    "@xcite , it affects the value of @xmath31 and the weight of the contributions of words in different frequency ranges .",
    "this connection relies not only on the universality of @xmath117 but also on our empirical finding that , for different pairs of databases , the relative fluctuations decay with the logarithm of the frequency , see eq .",
    "( [ eq.alog ] ) and fig .",
    "[ fig.4 ] .",
    "the finding that zipf s law directly controls the expected weights of contribution of different words provides a further motivation for our choice of using generalized entropies @xmath24 .",
    "the variation of the free parameter @xmath18 effectively tunes the range of frequency of the words that contribute to @xmath24 and @xmath31 : for large @xmath18 ( e.g. , @xmath75 ) only the most frequent words contribute , while for @xmath118 the results are dominated by the least frequent words . from an example based on @xmath100 keywords in physics , we obtain that these words contribute with @xmath119 of @xmath120 , @xmath108 of @xmath121 , but only @xmath122 of @xmath123 .",
    "words in different frequency ranges have different semantic and syntactic properties so that the variation of @xmath18 can characterize also different types of divergencies between the databases .    as @xmath18 is reduced and approaches ( from above ) the critical value @xmath124 , where @xmath11 is the exponent of zipf s law defined in eq .",
    "( [ eq.modeldp ] ) , the convergence of @xmath24 and @xmath31 becomes extremely slow and increasingly large text sizes are needed for a robust estimation ( see fig .",
    "[ fig.6 ] ) .",
    "for instance , for the usual jensen - shannon divergence @xmath120 we estimate that databases of size @xmath125 tokens ( @xmath126 book or @xmath127 word types ) is needed while for @xmath128 the size grows dramatically to the unrealistic number of @xmath129 tokens ( @xmath130 books or @xmath131 word types ) . for @xmath132",
    "there is no convergence and therefore these quantities are not properly defined .",
    "this is one of the most dramatic consequences of zipf s law and reflects the effectively unbounded number of different symbols ( vocabulary ) in which @xmath24 is computed .",
    "l.d . was funded by capes ( brazil ) . e.g.a . and m. g. are grateful to f. font - clos for helpful discussions on the subject of this manuscript .",
    "10 url # 1#1urlprefix[2][]#2 gell - mann m and tsallis c 2004 _ nonextensive entropy : interdisciplinary applications _",
    "( oxford : oxford university press )"
  ],
  "abstract_text": [
    "<S> we show how generalized gibbs - shannon entropies can provide new insights on the statistical properties of texts . </S>",
    "<S> the universal distribution of word frequencies ( zipf s law ) implies that the generalized entropies , computed at the word level , are dominated by words in a specific range of frequencies . here </S>",
    "<S> we show that this is the case not only for the generalized entropies but also for the generalized ( jensen - shannon ) divergences , used to compute the similarity between different texts . </S>",
    "<S> this finding allows us to identify the contribution of specific words ( and word frequencies ) for the different generalized entropies and also to estimate the size of the databases needed to obtain a reliable estimation of the divergences . </S>",
    "<S> we test our results in large databases of books ( from the google n - gram database ) and scientific papers ( indexed by web of science ) . </S>"
  ]
}