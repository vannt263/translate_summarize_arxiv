{
  "article_text": [
    "@xmath0-body techniques are used in cosmological simulations to follow the nonlinear evolution of a system of particles , and to give theoretical predictions about the matter distribution that can be compared with observations .",
    "the traditional @xmath0-body methods are the particle - mesh ( pm ) , particle - particle / particle - mesh ( p@xmath12 m ) , and tree methods ( hockney & eastwood 1981 ; klypin & shandarin 1983 ; efstathiou et al .",
    "1985 ; bouchet & hernquist 1988 , and references therein ) .",
    "although numerous fundamental results have been obtained using these codes , the codes often can not provide desirable spatial or mass resolution _ with currently available computers _ because of either memory or cpu limitations .",
    "thus , for example , the pm code can handle a large number of particles ( the latest pm simulations follow evolution of approximately @xmath13 particles ) but is limited in spatial resolution ( to increase resolution by a factor of 2 requires 8 times as much memory ; the largest pm simulations have reached dynamic range of @xmath14 ) .",
    "tree and p@xmath12 m codes are cpu limited , the tree and p@xmath12 m codes can provide higher dynamic range than the pm code ] because calculation of forces in these codes is considerably slower than in the pm code and , in the case of the p@xmath12 m code , is also strongly dependent on the degree of particle clustering . in an ideal cosmological simulation",
    "one needs a resolution @xmath15 kpc to resolve a galaxy and a simulation cube of @xmath16 mpc to sample appropriately the longest perturbation waves or to get sufficient statistics",
    ". the number of particles should be sufficiently large ( usually a few million or larger ) to allow halo properties to be reliably determined .",
    "the required dynamical range is thus @xmath17 , which is higher than the above codes can provide for the required number of particles and with currently available computers .",
    "these limitations have motivated the development of new methods with better resolution and/or performance .",
    "villumsen ( 1989 ) developed a code in which the pm grid was complemented by finer cubic subgrids to increase the force resolution in regions of interest .",
    "the local potential was calculated as a sum of the potentials on the subgrids and on the pm grid .",
    "a similar approach was adopted by jessop , duncan , & chau ( 1994 ) in their particle - multiple - mesh code .",
    "however , instead of summing the potentials from subgrids , the potential on each level was obtained independently by solving the boundary problem .",
    "boundary values of the potential were interpolated from the coarser parent grid .",
    "couchman ( 1991 ) used cubic refinement grids to improve the performance of the p@xmath12 m algorithm . here , the resolution of the p@xmath12 m code was retained while the computational speed was considerably increased . in the lagrangian approach ( gnedin 1995 ; pen 1995 ) the computational mesh is not static but moves with the matter so that the resolution increases ( smaller mesh cells ) in the high density regions and decreases elsewhere .",
    "although potentially powerful , this approach has its caveats and drawbacks ( gnedin & bertschinger 1996 ) .",
    "the mesh distortions , for example , may introduce severe force anisotropies .",
    "a different approach was adopted by xu ( 1995 ) , who developed the tpm code , a hybrid of the pm and tree algorithms .",
    "the gravitational forces in the tpm are calculated via a pm scheme on the grid and via multipole expansions ( tree algorithm ) in the regions where higher force resolution is desired .",
    "the forces on the particles in low - density regions are calculated by the pm scheme , while forces on the particles in high - density regions are _ the sum _ of external large - scale pm force and internal short - scale force from the neighboring particles .",
    "although this code may not be faster than a pure tree code , it is effectively parallel because particles in different regions can be evolved independently .",
    "an adaptive multigrid code for cosmological simulations was recently presented by suisalu & saar ( 1995 ) . in this code ,",
    "finer rectangular subgrids are adaptively introduced in regions where the density exceeds a specified threshold . for each subgrid",
    ", the potential is calculated using boundary conditions interpolated from the coarser grid .",
    "the solution on the finer grid is used to improve the solution on the coarser grid .",
    "another variant of an adaptive particle - multiple - mesh code for cosmological simulations was recently presented by gelato , chernoff , & wasserman ( 1996 ) .",
    "this code can handle isolated boundary conditions , which makes it applicable to noncosmological problems .",
    "all of the above _ multigrid _ methods use _ rectangular _ subgrids to increase force resolution . for simulations where there are only a few small regions of interest ( e.g. , a few galaxies or clusters of galaxies )",
    "the rectangular refinements may be a good choice because these regions can be easily covered by rectangular subgrids .",
    "it is , however , well known that the geometry of structures in realistic cosmological models is usually a complicated network of sheets , filaments , and clumps which are difficult to cover efficiently with rectangular grids .    in this paper",
    "we present a new adaptive refinement tree ( art ) high - resolution @xmath0-body code .",
    "this code was developed to improve the spatial resolution of a particle - mesh code by about two orders of magnitude without loss of mass resolution or computational speed . in our scheme , the computational volume is covered by a cubic grid that defines the minimum resolution . on this grid ,",
    "the poisson equation is solved with a traditional fast fourier transform ( fft ) technique using periodic boundary conditions .",
    "the finer meshes are built as collections of cubic , non - overlapping cells of various sizes organized in _ octal threaded trees _ in regions where the density exceeds a predefined threshold .",
    "any mesh can be subject to further refinements ; the local refinement process stops when the density criterion is satisfied .",
    "once constructed , the mesh , rather than being destroyed at each time step , is promptly adjusted to the evolving particle distribution . to solve the poisson equation on these refinement meshes",
    ", we use a relaxation method with boundary conditions and an initial solution guess interpolated from the previous coarser mesh .",
    "below we present the method (  2 ) , describe the code (  3 ) , and discuss the tests (  4 ) .",
    "we then compare the code with other algorithms (  4 ) and finally apply it to a real cosmological problem (  5 ) .",
    "adaptive mesh refinement ( amr ) techniques for solving partial differential equations ( pdes ) have numerous applications in different fields of physics , astrophysics , and engineering in which large dynamic range is important .",
    "there are two major approaches in the application of these techniques . in the first approach ( e.g. , berger & oliger 1984 ; berger 1986 ; berger & colella 1989 ) ,",
    "the computational volume is divided into cubic elements ( cells ) , while in the second ( e.g. , lhner & baum 1991 ) the cells can have an arbitrary shape .",
    "collections of cells are used as computational meshes on which the pdes are discretized .",
    "we will call meshes composed of cubic cells _ regular _ ,",
    "calling meshes _ irregular _ otherwise .",
    "the integration of pdes is simpler on regular meshes , but dealing with complicated boundaries may be a difficult problem . with irregular meshes one can handle complicated boundaries much more easily .",
    "the price , however , is more elaborate algorithms , data structures , and associated cpu and memory overhead .",
    "a particular choice of the mesh structure is a tradeoff between these considerations . in astrophysics",
    "there are no complicated boundaries , and a cubic computational volume is usually used to model a system . in this case , there is no need for irregular meshes and it is preferable to use meshes made of cubic cells .",
    "the regular meshes themselves can be organized in different ways .",
    "the usual practice is to use regular meshes of cubic or rectangular shape ( e.g. , berger & colella 1989 ) organized in arrays ( grids ) , which allows one to simplify data structures and to use standard pde solvers .",
    "these arrays can be organized in a tree ( berger 1986 ) to form a multigrid hierarchy .",
    "the main disadvantage of the grids is that one can not cover regions of complicated shape in an efficient way .",
    "moreover , the arrays are an inflexible data structure , and the whole refinement hierarchy should be periodically rebuilt , not adjusted , when dealing with unsteady solutions .",
    "in our approach , we use regular meshes but they are handled in a completely different way .",
    "cells are treated as individual units which are organized in _ refinement trees _ ( see  3.2 ) .",
    "each tree has a root  a cell belonging to a base cubic grid that covers the entire computational volume .",
    "if the root is refined ( split )  it has eight children ( smaller nonoverlapping cubic cells residing in its volume ) , which can be refined in their turn , and so on .",
    "cells of a given _ refinement level _ are organized in _",
    "linked lists _ and form a refinement mesh .",
    "the tree data structures make mesh storage and access in memory logical and simple , while linked lists allow for efficient mesh structure traversals . in the current version of the code",
    "we make use of octal threaded trees ( khokhlov 1997 ) and doubly linked lists ( e.g. , knuth 1968 ; aho , hopcroft , & ulman 1983 ; corner , leiserson , & rivest 1994 ) . the fact that cells are treated as independent units rather than element of a grid allows us to build a very flexible mesh hierarchy which can be easily modified .",
    "the details of the mesh generation and modification in our code are described in  3 .",
    "the multigrid techniques of solving partial differential equations ( brandt 1977 ) are very successful in reducing the computational and storage requirements for solving many types of pdes ( wesseling 1992 ; press et al .",
    "there are two kinds of multigrid algorithms . the first , sometimes called _ the multigrid method _ ,",
    "is used to speed up convergence of relaxation methods . in this method ,",
    "the source term is defined only on the base finest grid  all the other , coarser grids are used as a workspace . in the second algorithm , called _ full multigrid _ , the source term is defined on all grids , and the method obtains successive solutions on finer and finer grids . the latter method is useful when dealing with grids created in adaptive refinement process . the full multigrid scheme can be used differently in its turn , depending on how the solutions on different levels influence each other . in the _ one - way interface _",
    "scheme , the solution from a coarser grid is used to get a first - guess solution on the finer grid and often to get boundary values as well .",
    "however , the solution on the coarser grid is not influenced by the solution on the finer grid ( e.g. , jessop , duncan , & chau 1994 ) .",
    "in the _ two - way _ interface scheme , the coarser grid solution is used to correct the solution on the finer grids and vice - versa .",
    "the choice of a particular scheme is usually determined empirically and is problem dependent .",
    "the two - way interface scheme is more difficult to implement in the case of periodic boundary conditions ( suisalu & saar 1997 ) .    in our approach , each refinement mesh is composed of cells of the same refinement level , but these meshes are completely different from grids .",
    "the techniques are thus multilevel rather than multigrid .",
    "we use an analog of the full multigrid algorithm with a one - way interface between the meshes .",
    "we use a regular cubic grid covering the whole computational volume as the zeroth or coarsest level . at this level",
    ", the poisson equation is solved using a standard fft method with periodic boundary conditions .",
    "this solution is then interpolated onto the finer first - level mesh to get the boundary values and first - guess solution .",
    "once the boundary problem is defined , we use a _ relaxation _ method ( e.g. , press et al .",
    "1992 ) to solve the poisson equation on the mesh .",
    "since we start from an initial guess which is already close to the final solution , the iterative relaxation procedure converges quickly .",
    "after we get the solution on the first refinement level , the same procedure ( obtaining boundary values and initial guess by interpolation from the previous coarser level ) is repeated for the next level , and so forth . at the end of this process we have the solution ( potential ) for all cells .",
    "the description of the code is given in the next section .",
    "the structure of the code can be outlined as follows .",
    "first of all , we set up the initial positions and velocities of the particles using the zeldovich approximation , as described by klypin & shandarin ( 1983 ) .",
    "once the initial conditions are set , we construct the regular cubic grid covering the whole computational volume and then proceed to check whether additional refinement levels are required according to the current density threshold . at this point",
    "the code enters the main computational loop , which includes :    * density assignment on all existing meshes ; * a gravitational solver ; * routine updating of particle positions and velocities ; * modifications to the mesh hierarchy .",
    "the mesh modifications ( refinement and derefinement ) are based on the density distribution .",
    "the modifications are made at the end of the computational cycle . at this point",
    "the density distribution is available , since it was calculated for the gravitational solver .    below we will describe each of these major functional blocks in detail .",
    "we will also discuss timing , energy conservation , and the memory requirements of the code .",
    "the adaptive mesh refinement block of the code generates new meshes and modifies existing ones .",
    "the refinement hierarchy in our implementation is based on the regular cubic grid that covers the entire computational volume . with",
    "the refinement block turned off , the density assignment and gravity solver on this grid are similar to those in the pm code of kates , kotok , & klypin ( 1991 ) .",
    "the data structures that we use to organize the mesh cells are very similar to those implemented in the hydrodynamical eulerian tree refinement code ( khokhlov 1997 ) .",
    "all mesh cells are organized in _ refinement trees_. a cell can be a _",
    "parent _ of eight _ children _  smaller cubic cells of equal volume residing in it",
    ". each child may be in its turn split and have children .",
    "each tree has a _ root _ ( a zeroth - level cell ) that may be the only cell in this tree if it is unsplit .",
    "the tree ends with unsplit cells , which we call _",
    "leaves_. this structure is called an _ octal rooted tree _ , and is the construct used in tree codes",
    ". there is , however , an important difference between our code and tree codes .",
    "we use _ fully threaded trees _ , in which cells are connected with each other _ on all levels_. in addition , cells that belong to different trees are connected to each other across tree boundaries .",
    "in fact , we can consider all cells as belonging to a single threaded tree with a root being the entire computational domain and the base grid being one of the tree levels .",
    "the tree structure is supported through a set of pointers .",
    "each cell has a pointer to its parent and a pointer to its first child .",
    "in addition , cells have pointers to the six adjacent cells ( these make the tree fully threaded ) so that information about a cell s neighbors is easily accessible ( see fig.1 ) .",
    "overall , the following information is provided for each cell @xmath18 belonging to a tree :    * @xmath19 , the level of the cell in the tree ; * @xmath20 , the pointer to the parent cell ; * @xmath21 , the pointer to the cell s first child , or @xmath22 if the cell is a leaf ; * @xmath23 , pointers to neighboring cells ( @xmath24 ) ; * @xmath25 , position in space ( @xmath26 ) ; * @xmath27 , storage for associated physical variables ( in our case @xmath28 , as we store both the density and the potential ) .",
    "the above set of pointers is sufficient to support the tree structure and to change it dynamically with minimum cost .",
    "in addition , the cells on each level of the mesh hierarchy are organized in doubly linked listsm codes ) is that in the former we keep not only a pointer to the next element but also a pointer to the previous element in the list .",
    "this allows us to insert and delete list entries without rebuilding the whole list . ]",
    "( e.g. , knuth 1968 ) so that a sweep through a given level ( the operation used extensively in the multigrid relaxations described below ) can be done with minimum cpu time .",
    "this organization adds two pointers for each eight siblings , or @xmath29 storage elements per cell .",
    "the cells belonging to the base regular grid ( level zero ) , while part of the same data structure as the other cells , are created only at the very beginning of a simulation and are never destroyed .",
    "it is therefore unnecessary to keep information about a cell s position or pointers to its neighbors because they can be easily computed .",
    "the number of pointers can be considerably reduced ( by as much as a factor of 2 ) because some of them can be shared by siblings ( sets of eight cells with the same parent ) .",
    "an elementary refinement process creates eight new cubic cells of equal volume ( _ children _ ) inside a _ parent _ cell .",
    "when the parent is refined , we check if all six neighbors are of the same level as the parent .",
    "if there are coarser neighbors ( of smaller level than the parent ) , we split those neighbors .",
    "if a neighbor in its turn has coarser neighbors , we split the neighbor s neighbors , and so forth .",
    "we thus build a refinement structure which obeys a rule allowing _ no neighbor cells with level difference greater than _ 1 .",
    "examples of allowed and prohibited configurations are shown in figure 2 .",
    "although this is the only rule in the whole refinement process , it determines the global structure of the resulting refinement hierarchy , assuring that there are no sharp resolution gradients on a level s boundaries . on the next refinement",
    "pass , each of the newborn children is checked against the density criterion and can be subdivided into 8 children in its turn if further splitting is needed .",
    "the process stops when either the density criterion is satisfied everywhere or the maximum allowed refinement level is reached .",
    "the refinement process proceeds level by level starting from the base grid . on any level of the mesh hierarchy",
    "the process can be split into two major parts .",
    "first , we mark up all the cells which need to be split , creating a refinement map .",
    "however , a map constructed in this way tends to be `` noisy '' .",
    "we smooth it by marking additional cells so that any cell which was originally marked is surrounded by a buffer of at least two other marked cells .",
    "we construct this buffer using an algorithm which includes several passes through a level , each one marking additional cells . during the first pass the neighbors of cells marked in the refinement map",
    "are marked for splitting also .",
    "after that , two passes are made in which we mark for splitting only those cells which have at least two neighbors already marked for refinement ( note that when we speak of marked cells , we mean cells marked only during passes before the one we are discussing , not during the pass under consideration ) .",
    "these three passes create a one - cell cubic buffer around each of the cells marked in the original refinement map .",
    "each additional set of three passes similar to those described above will build one more cubic layer around every originally marked cell . therefore , to build a two - cell buffer we make six passes . when the map is completed , it is used to make the actual splitting .    the refinement procedure described above",
    "can be used either to construct the mesh hierarchy from scratch or to modify the existing meshes . however , in the course of a simulation the structure is neither constructed nor destroyed . instead , in every computational cycle we _ modify _ existing meshes to account for the changes in particle distribution . therefore , we need to make not only refinements but also derefinements ( in the places where it is no longer necessary to keep resolution at the current level ) , which is accomplished in the same manner as refinement by constructing a derefinement map  that is , a map of cells marked for joining . if the joining violates the above - mentioned neighbor rule , nothing is done and the cell remains split . therefore , the code modifies the existing structure dynamically , keeping the refinements in accord with the ever - changing density field .",
    "modifying the hierarchy requires much less cpu time than rebuilding it because only a small number of cells needs to be modified at any given time step .",
    "figure 3 shows an example of the refinement mesh hierarchy built in one of the @xmath7cdm cosmological simulations described in  4.4 .",
    "a selected region of figure 3 is shown expanded in figure 4 .",
    "particle coordinates are not sufficient to specify the particle - mesh connection because cells of different levels can share the same volumes ; we need to know , however , which particles belong to a given cell . we keep track of the particles by arranging them in doubly linked lists so that every cell `` knows '' its head linked - list particle ( the head is nil if the cell is empty ) and thus all the other particles in this linked list .",
    "if a particle moves from cell to cell , it is deleted from the linked list of the cell it leaves and is added to the new cell s linked list .",
    "only leaves are allowed to own particles .",
    "once a cell is split , all its particles are divided among its children .",
    "however , we solve the poisson equation on every refinement level , so that the value of the density must be computed for every cell regardless of whether or not it is a leaf . on each level",
    ", starting from the finest level and up to the zeroth level , the density is assigned using the standard cloud - in - cell ( cic ) technique ( hockney & eastwood 1981 ) . because particles belong only to the finest cells enclosing them , when we change between levels the particles",
    "are passed from children to their parents .",
    "the particles are transferred in this way only as far as the density assignment is concerned ; the linked list is not changed .",
    "the particles , therefore , contribute to the density on any level in which they are physically located .",
    "the fact that the zeroth level of the mesh hierarchy is a cubic regular grid of fixed resolution allows us to use the fft method to solve the poisson equation on this grid ( hockney & eastwood 1981 ) .",
    "the fft technique naturally supports periodic boundary conditions which is important for cosmological simulations .",
    "moreover , the fft is well benchmarked and is about twice as fast as the relaxation method described below .",
    "the poisson equation on the refinement meshes is defined as a dirichlet boundary problem for which boundary values are obtained by interpolating the potential from the parent grid . in our algorithm",
    ", the boundaries of the refinement meshes can have an arbitrary shape , which narrows the range of pde solvers one can use . to solve the poisson equation on these meshes ,",
    "we have chosen the _ relaxation _ method ( hockney & eastwood 1981 ; press et al .",
    "1992 ) , which is relatively fast and efficient in dealing with complicated boundaries . in this method",
    "the poisson equation @xmath30 is rewritten in the form of a diffusion equation , @xmath31 the point of the method is that an initial solution guess @xmath32 _ relaxes _ to an equilibrium solution ( i.e. , solution of the poisson equation ) as @xmath33 .",
    "the finite - difference form of equation ( 2 ) is : @xmath34 where the summation is performed over a cell s neighbors .",
    "here , @xmath35 is the actual spatial resolution of the solution ( potential ) , while @xmath36 is a fictitious time step ( not related to the actual time integration of the @xmath0-body system ) .",
    "this finite difference method is stable when @xmath37 ( press et al .",
    "if we choose the maximum allowed time step @xmath38 , the above equation can be rewritten in the form of the following iteration formula : @xmath39 the relaxation iteration thus averages the potential of a cell s six neighbors and subtracts the contribution from the source term .",
    "cells in the boundary layer will have some neighbors belonging to the coarser level . in this case",
    ", we need to interpolate to get the potential at the location of the expected neighbor .",
    "it is desirable that the interpolation maintain continuity and isotropy of the force ( see discussion in jessop et al .",
    "we have found that linear interpolation perpendicular to the boundary which incorporates both coarser and finer cell potentials is satisfactory ; we get the interpolated value of the potential on the boundary of level @xmath40 as : @xmath41 here @xmath42 is a weight , and @xmath43 and @xmath44 are the potentials of a boundary cell of level @xmath40 and of its @xmath45-level neighbor .",
    "we found the optimal value of @xmath42 to be @xmath46 by minimizing the force discontinuity for particles moving through mesh boundaries .",
    "the iterative procedure described above is repeated until the desired level of convergence is achieved .",
    "we can speed up the convergence of the relaxation procedure considerably by using an initial guess for the solution that is already close to the final solution .",
    "such an initial guess can be obtained by interpolating the potential from the previous coarser mesh , for which the poisson equation was already solved . by doing so ,",
    "we need only @xmath47 iterations to find the potential to an accuracy of a one or two percent .",
    "nevertheless , a higher accuracy is needed because the potential is then differentiated to get the accelerations ; the errors in accelerations are thus larger than the errors in the potential .",
    "therefore , we would need to make more iterations to reach the same @xmath48 accuracy level in the acceleration .",
    "the number of required iterations , however , can be considerably reduced by using the so - called _ successive overrelaxation _ ( sor ) technique ( hockney & eastwood 1981 ; press et al .",
    "1992 ) . in this technique ,",
    "the solution in a given cell is computed as a weighted average , @xmath49 where @xmath50 is the solution obtained via the iteration equation ( 4 ) , @xmath51 is the solution from previous iteration step , and @xmath52 is the _",
    "overrelaxation parameter_. the parameter @xmath52 can be adjusted to minimize the number of iterations required to achieve a certain accuracy level .",
    "the ultimate goal of any @xmath0-body algorithm is to get an accurate approximation to the pairwise interparticle forces .",
    "therefore , we use the force accuracy ( see  4 ) to determine the required number of iterations .",
    "of course , there is no point in using more iterations than the number needed to make the iteration error smaller than truncation error .",
    "the latter can be estimated by making the number of iterations very large , so that the iteration error is negligible .",
    "we then find the minimum number of iterations for which the force accuracy is still at the level of truncation errors .",
    "the number of iterations is further minimized by adjusting the overrelaxation parameter .",
    "we have found empirically that only 10 relaxation iterations are needed if @xmath53 .",
    "to integrate the trajectories of the dark matter particles we use the newtonian equations of motion in an expanding cosmological framework ( e.g. , peebles 1980 ) .",
    "these equations can be expressed in terms of comoving coordinates @xmath54 related to the proper coordinates as @xmath55 , where @xmath56 is the expansion factor : @xmath57 where @xmath58 is the momentum of a particle and @xmath59 is given by the poisson equation relating the potential @xmath32 to deviations of density from the background : @xmath60 the above equations are integrated numerically using dimensionless variables @xmath61 where @xmath62 is the length of a zeroth - level mesh cell and @xmath63 is the hubble constant .",
    "we also use the expansion factor @xmath64 instead of the time @xmath65 , so that equations ( 7)(8 ) can be rewritten as : @xmath66 here @xmath67 is the present - day ( @xmath68 ) contribution of matter to the total density of the universe and @xmath69 is the corresponding contribution of the vacuum energy ( measured by the cosmological constant ) .",
    "the function @xmath70 is specific to a given cosmological model .",
    "the general form of this function , valid for open , flat , and closed cosmologies , is ( e.g. , carrol et al .",
    "1992 ) : @xmath71    we adopt a standard second - order leapfrog integration scheme of advancing particles to the next time step . for a step @xmath72 ,",
    "corresponding to time step @xmath73 , the momenta and positions of particles are updated as follows : @xmath74 here the indices @xmath72 , @xmath75 , and @xmath76 refer to quantities evaluated at @xmath77 , @xmath78 , and @xmath79 respectively . although multiple time stepping is probably very efficient in terms of cpu time , in the current version of the code we use a constant time step for all particles",
    ". we plan to implement individual time steps for different levels in the future .",
    "particle coordinates and velocities are updated using their accelerations obtained via numerical differentiation of the potential and interpolation to the particle location using the cic method ( hockney & eastwood 1981 ) .",
    "there are , however , some complications because particles can move through the level boundaries .",
    "the resolution gradients , for example , may induce unwanted force fluctuations and anisotropies ( jessop et al . 1994 ; anninos , norman , & clarke 1994 ) .",
    "in addition , momentum conservation , achieved by exact cancellation of numerical terms in the cic method , is no longer guaranteed .",
    "this means that additional care must be taken to minimize these effects .",
    "usually , this is done by introducing extra _",
    "buffer regions _ along the mesh interfaces so that force interpolation on the boundaries is avoided . in our code , we do not introduce additional buffer cells on the mesh boundaries because the meshes are already expanded by smoothing ( see  3.2 ) .",
    "therefore , we simply prohibit force interpolation that uses both coarse and fine boundary cells , interpolating instead on the coarse level . in this way",
    ", particles are driven by the coarse force until they move sufficiently far into the finer mesh .",
    "the same is true for particles moving from the finer to coarser mesh .",
    "the memory requirements of the code are determined by the number of dark matter particles @xmath80 , the number of cells in the zeroth - level base grid @xmath81 , and the number of cells on refinement levels @xmath82 . in the current implementation of the code the total number of memory storage elements",
    "@xmath0 used by the code is : @xmath83 the particle information consists of coordinates , momenta , and two pointers used to organize the doubly linked list .",
    "the overhead for @xmath82 is determined by the pointers used to support the tree refinement hierarchy ( see  3.2 ) .",
    "it can be significantly reduced ( this scheme was implemented in khokhlov 1997 ) if most of the cell information ( namely @xmath84 , @xmath85 , @xmath86 , and @xmath87 ; see  3.2 ) is shared between siblings ( the eight cells which have the same parent ) .",
    "the cells have individual pointers to their first child and to two physical variables ( density and potential ) . in this case , for eight refinement cells we need only @xmath88level@xmath89parent@xmath90nb@xmath91pos@xmath92child @xmath93var@xmath94 , storage elements ( @xmath95 per cell instead of 15 ) .",
    "we plan to implement these improvements in the future versions of the code .",
    "@xmath96 can be compared to the corresponding number of storage elements in a pm code : @xmath97 the apparent overhead of the art code compared to the pm code is the price for a fully adaptive and flexible mesh structure .",
    "it should be noted , however , that to increase resolution by a factor of 2 in a pm code the number of cells @xmath81 must be increased by a factor of 8 , which severely limits the maximum possible dynamic range ( @xmath14 , with largest currently available computers ) . in the art code ,",
    "the resolution is improved by increasing @xmath82 which changes very slowly when resolution is increased .",
    "for example , to increase resolution in the highest density regions by a factor of 2 in the simulations described in section 5 ( see also table 1 ) , the total number of cells was increased only by @xmath98 .",
    "note also that the dynamic range of @xmath99 was achieved with only @xmath100 cells while a pm code would require @xmath101 cells to reach the same resolution . for comparison ,",
    "the memory requirements of the publicly available version of tree code ( kindly provided by j.barnes ) is @xmath102 where @xmath103 is the number of tree cells",
    ". the memory requirements of ap@xmath12 m code ( couchman 1991 ) are @xmath104 .      in this section",
    "we present timings of the current version of the code and compare the performance with other high - resolution @xmath0-body codes .",
    "the present version of the code was parallelized to run in shared memory mode on the hp - convex spp-1200 exemplar  a multipurpose scalable parallel computer .",
    "currently , the code is not fully parallelized .",
    "the blocks of code that require considerable parallelization efforts , namely the density assignment and cell splitting / joining during the mesh modifications , are run serially .",
    "the parallelization of the most cpu - expensive parts of the code , the poisson solver and the force interpolation , was straightforward .",
    "we are now working on the complete parallelization ( including distributed memory architectures ) and optimization of the code and will present details elsewhere .    in figure",
    "5a we show the performance of different blocks of the code with respect to the expansion parameter in a @xmath7cdm simulation ( @xmath8 , @xmath9 , @xmath10 ) of an @xmath105 mpc box with @xmath106 particles and base grid of @xmath107 cells ( more details are given in ",
    "the simulation was run on 8 cpus of the ncsa sp-1200 exemplar in shared memory mode .",
    "the cpu overhead for running code in parallel is about @xmath108 and the same simulation run on an ibm rs/6000 workstation performs @xmath109 times faster ( in terms of cpu but not in terms of wall - clock time ! ) .",
    "the overhead is mostly due to the unusually large penalty for cache - missing events that results if memory is accessed randomly . in table 1",
    "we present timing for different code blocks for the final time step ( @xmath68 ) of two similar @xmath7cdm simulations with @xmath107 particles ( see section 5.2 ) and with different resolutions .",
    "the base grid in both simulations was @xmath110 and numbers of maximum allowed refinement levels were 4 and 6 ( the number of mesh cells in table 1 includes zeroth - level cells ) .",
    "as before , the simulations were run on 8 cpus of the spp-1200 exemplar .",
    "we compare the timings from table 1 with the performance of the ap@xmath12 m code ( couchman 1991 ) .",
    "the final step in a two - level ap@xmath12 m simulation of an open ( @xmath111 , @xmath112 , @xmath113 , @xmath114 ) cosmology ( box @xmath115 mpc , @xmath110 grid , @xmath110 particles , and smoothing kernel @xmath116 cell giving a dynamic range of about @xmath117 ) took 1316 cpu seconds on one processor of ibm sp-2 computer ( s.borgani 1996 , private communication ) .",
    "this number is roughly consistent with the timings presented in original paper of couchman ( 1991 ) if we account for the difference in mflops between the machines used .",
    "the first simulation in table 1 is comparable in spatial resolution to the above ap@xmath12 m simulation but we must account for the different number of particles . only density assignment and particle motion directly scale with number of particles .",
    "we , therefore , multiply the cpu time spent by these routines by 8 which gives a total time for the final step @xmath118 cpu seconds .",
    "note , however , that about half of this cpu time is penalty for the cache missings which would be negligible for a serial run on the sp-2 .",
    "the art code , therefore , is about 3 times faster than ap@xmath12 m code of comparable resolution .",
    "note also that although in the second simulation from table 1 the resolution was increased by a factor of 4 , the cpu time did not change significantly because only a relatively small number of additional cells was required to resolve the highest density regions .      in an expanding universe ,",
    "energy conservation is expressed by the irvine - layzer - dmitriev - zeldovich equation , @xmath119=-t\\frac{da}{dt},\\ ] ] or @xmath120 where @xmath121 the error in energy conservation at a time @xmath122 is then measured by comparing the change in total energy with the change in @xmath123 : @xmath124 in figure 5b we show the energy conservation error versus the expansion parameter @xmath64 for two @xmath7cdm simulations with @xmath125 and @xmath107 particles . in both simulations",
    "the time step , @xmath126 , was chosen so that none of the particles would move more than a fraction ( @xmath127 ) of the mesh cell they were residing in over one step .",
    "we note that energy is conserved at the level of @xmath128 in the @xmath125 particle simulation and at the level of @xmath129 in the @xmath107 particle simulation .",
    "the maximum errors of @xmath130 and @xmath131 for the two simulations occurred when the first two refinement levels were opened at @xmath132 .",
    "this may be a result of the rather fast change in resolution in the regions of ongoing nonlinear collapse .",
    "one of the important parameters of the code is the density threshold , which is used to decide whether a given patch of the computational volume should be refined or derefined .",
    "the usual practice in the adaptive refinement algorithms is to consider this threshold a free parameter ( see , e.g. , suisalu & saar 1995 ) .",
    "while we also take it as a free parameter , we will discuss here some practical limits .",
    "first of all , we do not want to set up the density threshold too low because it usually leads to shot noise in the refinement procedure and can also lead to unwanted two - body effects ( we should not force a resolution considerably less than mean separation between particles ) .",
    "our tests ( particularly the spherical infall test presented in  4 ) have shown that two - body effects are negligible and that the refinement / derefinement procedure is stable if we use a density threshold corresponding to @xmath133 particles in a mesh cell .",
    "we choose the minimum value of @xmath134 for the simulations described in  5 .",
    "we can also consider this parameter from a different point of view .",
    "the threshold can be thought of as a parameter defining the overdensity at which the refinement takes place . in the case of simulations presented in  5",
    "the threshold value corresponds to an overdensity of @xmath135 on the first refinement level .",
    "this overdensity is reasonable for the purpose of resolving a halo ",
    "the first refinement happens well beyond the halo virial radius ( corresponding to the overdensity @xmath136 ) .",
    "we have also found that results are not particularly sensitive to the threshold in the range of @xmath137 particles per cell .",
    "in this section we present tests of the developed code . in particular , we show results describing the accuracy of the force calculation in art scheme and related issues of resolution . we discuss the results of the zeldovich pancake test and of the spherical infall test . finally , we compare results for a set of realistic cosmological runs obtained with the art and pm codes .",
    "it is important to know the shape and accuracy of both short- and long - range forces in order to compare the resolution of different codes .",
    "here we present a test showing the accuracy of force calculations in pm and art schemes and compare them to the plummer softened force often used in both p@xmath12 m and tree codes .",
    "we used a @xmath107 base grid with a massive particle in the center and a second particle placed randomly nearby .",
    "the refinement meshes were constructed up to the specified level , so that both particles were located on this level .",
    "the usual potential and force calculations ( described above ) were then performed to get the pairwise force between these two particles which was compared with the `` exact '' newton force .",
    "figure 6 shows the results of the force calculation with the fft method ( i.e. , art without refinements ) and with the relaxation method at different levels of refinement .",
    "we plot relative acceleration errors calculated as follows : @xmath138 where @xmath139 is the acceleration calculated on the mesh and @xmath140 is the theoretical acceleration .",
    "the upper panel in figure 6 shows the relative force error versus interparticle separation , given in the units of the base grid , for a pure pm calculation and for the second and fourth refinement levels .",
    "note that despite the general similarity of the shape of fft and relaxation forces , the scatter of the latter at small separations is larger than that of the former .",
    "the scatter at small separations does not , however , mean that we have the same errors in particle orbits .",
    "we have performed a test placing two particles at the distance of 2 grid cells ( where the scatter is largest ) and giving them velocities in such a way that in case of a zero force error they would stay on a circular orbit .",
    "the deviation of the diameter of the orbit from its true value is a measure of the code accuracy .",
    "we performed the test for the pm force ( two particles on the zeroth level at the distance of two base grid cells from each other ) and relaxation force ( two particles on the second refinement level at the distance of 2 second - level cells or 0.5 zeroth - level cells ) .",
    "the resulting particle trajectories are shown in figure 7 .",
    "after more than two orbital periods the particles stayed on the nearly circular orbit with _",
    "error in diameter of about 8% .",
    "note that we did not observe either significant drift of the orbit center or any catastrophic break of the trajectory .",
    "we conclude that even for separation equal to the resolution the code produces reasonably accurate trajectories of particles .",
    "the lower panel in figure 6 shows the relative error for art force calculated on the fourth level versus distance in units of the fourth - level mesh cell , together with the relative error corresponding to the plummer softened force ( _ solid line _ ) with the softening parameter @xmath141 [ @xmath142 equal to the size of the fourth - level cell .",
    "comparison shows that while the art force error fluctuates around zero for distances greater than two mesh cells , the plummer softened force is considerably weaker than @xmath143 law for up to six grid cells .",
    "the relation between resolution of the art code ( 2 mesh cells of the maximum refinement level ) , @xmath144 , and plummer softening length is thus @xmath145 gelb ( 1992 ) studied the shape of the plummer softened force in his p@xmath12 m code .",
    "the result is consistent with ours ( gelb 1992 , fig .",
    "2.4 )  the force starts to fall down at a distance about five times larger than the softening length .",
    "one - dimensional plane wave collapse in an expanding universe is one of the traditional tests of @xmath0-body codes ( klypin & shandarin 1983 ; efstathiou et al .",
    "1985 ) . in this test , the analytic solution ( zeldovich 1970 ) is used to check how accurately the code integrates particle trajectories .",
    "if we know the initial conditions , the solution predicts particle positions for any other moment of time : @xmath146 here @xmath147 are the initial ( unperturbed ) positions , @xmath148 is the wavevector , and @xmath149 is the amplitude . in an @xmath150 universe , @xmath151 , where @xmath152 is a scale factor at the crossing time .",
    "the corresponding velocities can be obtained by differentiating the above formula for positions .",
    "we used a base grid of @xmath125 cells with @xmath107 particles .",
    "the particle positions and velocities were initially perturbed using the zeldovich approximation .",
    "the particle trajectories were integrated by the art code with three levels of refinement ( the density threshold for opening a new level was twenty particles in a cell on the base grid and three particles in a cell for any refinement level ) .",
    "in figure 8 we show one - dimensional phase diagrams at the crossing time that compare results of the art code with results of the pm code ( grid of @xmath125 cells ) .",
    "the figure shows that the art code follows the analytical solution more accurately than the pm code .",
    "this result is shown quantitatively in figure 9 , where we plot rms deviations of the particle positions and velocities from the exact solution as a function of time ( efstathiou et al .",
    "1985 ) : @xmath153^{1/2},$ ] @xmath154^{1/2}.$ ] starting from the moment when the first refinement level was created ( @xmath155 ) the rms deviations were systematically lower in the art code than in the pm code .",
    "an analytic solution describing spherical infall of material onto an overdensity in an expanding cosmological framework was developed by fillmore & goldreich ( 1984 ) and bertschinger ( 1985 ) . as noted by splinter ( 1996 ) ,",
    "the problem possesses a symmetry different from the intrinsic planar symmetry of the mesh codes , which makes it a useful and strong test .",
    "the analytic solution describes the evolution of a spherical uniform overdensity @xmath156 in a region which has _ proper _ radius @xmath157 at some initial time @xmath158 in a flat einstein - de sitter universe .",
    "as the density contrast grows , the matter initially inside @xmath157 is increasingly decelerated .",
    "eventually it stops expanding and turns around to collapse .",
    "if the initial hubble flow is unperturbed ( all peculiar velocities are initially equal to zero ) , the initial turnaround occurs at the time @xmath159 : @xmath160 at this moment the initial overdensity reaches its maximum radius : @xmath161 later on , shells of successively larger and larger radii turn around . at a given time @xmath162 ,",
    "a shell of a radius @xmath163 starts to collapse ( bertschinger 1985 ) .",
    "the solution for the density profile of the overdensity is self - similar in terms of the dimensionless variable @xmath164 and for @xmath165 can be expressed as ( fillmore & goldreich 1984 ; bertschinger 1985 ) : @xmath166 while at larger @xmath167 the main feature of the solution is the presence of sharp caustics .",
    "we modeled the spherical infall problem described above by distributing @xmath125 particles uniformly on the @xmath125-cell grid ( in the centers of the grid cells ) and placing additional particles in a sphere of radius 2 grid cell lengths in the center of the computational volume .",
    "the number of additional particles determines the initial overdensity , which we have chosen to be @xmath168 .",
    "we integrated particle trajectories from @xmath169 up to @xmath170 , taking 10,000 steps to ensure that particles move only a fraction of a mesh cell in a single time step on any of the five refinement levels , which is the condition required for the integration to be stable .",
    "the refinement levels were introduced in the regions where density was equivalent to more than six particles in a cell and the number of levels was limited to five , thus making the effective resolution @xmath171 in the highest density regions . in figure 10 the calculated density profile is compared with the analytic solution ( from tables 4 and 5 of bertschinger 1985 ) .",
    "we see a good agreement between the calculated density profile and the analytic solution at all radii down to the resolution limit .      to test the performance of the code in realistic cosmological simulations",
    ", we made a set of runs using art and pm codes with the same initial conditions and different spatial resolutions and compared the resulting particle distributions .",
    "in the first four runs we simulated an @xmath172 mpc box with @xmath106 particles , assuming a flat @xmath7cdm cosmological model ( @xmath173 , @xmath9 , @xmath10 ) . in the art runs we allowed for two levels of refinement starting from @xmath107 and @xmath110 grids  we will call these runs art @xmath174l and art @xmath175l .",
    "the number of time steps was @xmath176 in the art @xmath174l and @xmath177 in the art @xmath175l run .",
    "the refinement levels were introduced wherever the density exceeded a threshold value equivalent to more than 5 particles per cell .",
    "the pm code was run with @xmath107-cell ( pm @xmath107 ) and @xmath178-cell ( pm @xmath178 ) grids . in figure 11",
    "we show the projected particle distribution at @xmath68 from these four runs , with a subsets of particles belonging to one of the halos shown in a smaller window .",
    "the global distribution of particles and halos is well reproduced by the art code .",
    "also , halos in the art @xmath174l simulation are much more compact than in the pm @xmath107 simulation . this result",
    "is shown quantitatively in figure 12 , where we compare density distribution functions ( the fraction of the total mass in the regions of a given overdensity ) in these simulations .",
    "the density distributions for all runs were computed after rebinning the density field to the @xmath178 grid .",
    "the resolution of a simulation puts limits on the maximum density in the halo cores because gravitational collapse virtually stops at scales of @xmath179 grid cell ( e.g. , klypin 1996 ) .",
    "therefore , the density in the halo cores ( the high - density tail of the distribution ) is a good indicator of the spatial resolution .",
    "we note that the density distribution functions for both the art @xmath174l and the pm @xmath178 runs show approximately the same behavior , reaching overdensities of @xmath180 , while the pm @xmath107 run fails to produce halos with overdensities greater than @xmath181 .",
    "we therefore conclude that the art code produces density fields similar to those of a pm code of comparable resolution .",
    "the first application of the code was the study of the structure of dark matter halos .",
    "therefore , as a final test we compared the halo density profiles in the art and pm simulations .",
    "the size of the simulation box , @xmath105 mpc , was chosen to be the same as in the larger simulations described in the next section .",
    "the rest of the parameters were the same as in the above simulations .",
    "we simulated the evolution of the @xmath125 particles using the pm code with @xmath178-cell mesh and the art code with a @xmath107-cell base grid and three levels of refinement .",
    "as before , we refined regions where the local density exceeded a threshold value of about 5 particles per cell .",
    "a halo - finding algorithm ( described in the next section ) was applied to the resulting particle distribution . in figure 13",
    "we present the density profiles for six halos of different masses .",
    "the mass resolution in these simulations ( @xmath182 ) determined the mass range of halos .",
    "the most massive halo in figure 13 consists of about 5000 particles , while the least massive contains only about 100 particles .",
    "the density profiles of pm and art halos agree reasonably well down to the resolution limit ( @xmath183 kpc ) .",
    "we used the code to study the structure of dark matter halos in two of the currently popular cosmological models : standard cold dark matter ( scdm ) and cold dark matter with cosmological constant ( @xmath7cdm ) models .",
    "the dark matter halos play a crucial role in the formation and dynamics of galaxies and galaxy clusters .",
    "therefore , theoretical predictions about the structural and dynamic properties of the halos can be compared with observations and used as a powerful test of a given theoretical model .",
    "the numerical study of the halo structure requires very high spatial dynamic range ( at least @xmath184 ) because the simulation box has to be large enough to account correctly for large perturbation waves and the force resolution has to be high enough to make predictions in the observational range ( @xmath185 kpc ) .",
    "the art code was designed to handle such high dynamic ranges .",
    "the properties of dark matter halos were intensively investigated recently for a variety of cosmological models .",
    "early numerical studies ( frenk et al . 1985 , 1988 ; quinn , salmon , & zurek 1986 ; efstathiou et al .",
    "1988 ) indicated that the density profiles of dark matter halos in hierarchical clustering models in a flat , @xmath4 , universe were approximately isothermal [ @xmath186 , in agreement with analytic results ( fillmore & goldreich 1984 ; bertschinger 1985 ) .",
    "the dependence of the halo density profiles on the initial perturbation spectrum and on specific parameters of the cosmological model were also studied both analytically ( hoffman & shaham 1985 ; hoffman 1988 ) and numerically ( e.g. , crone , evrard , & richstone 1994 ) .",
    "these early numerical studies , however , lacked the necessary mass and spatial resolution to make reliable predictions on the structure of the halo cores . to overcome the resolution limits , substantial efforts were made to simulate the formation of halos from isolated density perturbations ( e.g. , dubinski & carlberg 1991 ; katz 1991 ) or to resimulate with a higher resolution halos identified in large low - resolution runs ( navarro , frenk , & white 1996a , hereafter nfw ; tormen , bouchet , & white 1997 ) .",
    "these simulations have the advantage of simulating halos in a wide mass range with homogeneous spatial and mass resolutions .",
    "however , it is hard to infer the statistically reliable results from these simulations because only a few halos are simulated . in a direct simulation",
    "one can get a statistically significant sample of halos suited for more detailed analysis .",
    "studies of the structure and dynamics of halos extracted from high - resolution direct simulations were done by warren et al .",
    "( 1992 ) and cole & lacey ( 1996 ) .",
    "warren et al .",
    "( 1992 ) used a tree code to simulate the evolution of @xmath110 particles in an @xmath4 universe with scale - free initial conditions .",
    "the force resolution was determined by imposing a plummer softening of @xmath187 kpc .",
    "special emphasis was given to the investigation of halo shapes .",
    "similar initial conditions were used in the study by cole & lacey ( 1996 ) , who used a p@xmath12 m code to evolve @xmath110 particles .",
    "the resolution in the latter simulations was @xmath188 , where @xmath189 is the size of the computational volume and @xmath141 is the plummer softening parameter .",
    "the results indicated that the density profiles of _ all _ simulated halos are well fit by the analytical model of nfw .",
    "this model has @xmath190 at small radii and steepens smoothly to @xmath191 at a _ scale radius _",
    "@xmath192 : @xmath193 the density profile described by this expression is singular , because the density rises arbitrarily high when @xmath194 , forming a cusp .",
    "the cuspy structure of the central parts of a halo thus represents a generic prediction of the model .",
    "although the nfw profile is consistent with current x - ray and gravitational lensing observations of galaxy clusters ( nfw ) , the @xmath195 behavior is in contradiction with observations of dynamics of dwarf spiral galaxies that imply flat central density profiles ( flores & primack 1994 ; moore 1994 ; burkert 1996 ) .",
    "these observations can serve as one of the critical tests of any model that includes a dark matter component because it is generally believed that the dynamics of the dwarf spiral galaxies is dominated by dark matter on scales @xmath196 kpc .",
    "the fact that the nfw profile holds for a variety of cosmological models ( nfw ; cole & lacey 1996 ) indicates its possible universality for cdm - like models .",
    "the goal of the present study was to investigate the structure of dark matter halos formed in a @xmath7cdm model .",
    "this model is currently one of the most successful scenarios of structure formation in the universe .",
    "it is , therefore , important to check whether the central cusp is present in halos formed in this model .      to study the structure of dark matter halos , we simulated the evolution of @xmath107 particles in standard cdm ( @xmath4 , @xmath5 , @xmath6 ) and @xmath7cdm ( @xmath197 , @xmath173 , @xmath9 , @xmath10 ) models .",
    "we made three runs : one high - resolution ( resolution @xmath3 kpc ) run for each models , and a lower resolution run ( resolution @xmath198 kpc ) for the @xmath7cdm model to study the effects of resolution . in terms of the plummer softening length ( see  4.1 ) , our resolution corresponds to @xmath199 for the high resolution runs , and @xmath200 for the low - resolution run .",
    "the simulations were started at @xmath201 and the particles trajectories were integrated by taking 3872 time steps in the low - resolution run , and 7743 time steps in the high - resolution runs .",
    "the size of the simulation box , @xmath105 mpc , determined the mass resolution ( particle mass ) as @xmath202 for scdm and @xmath203 for @xmath7cdm .      to identify halos in our simulations",
    ", we use an algorithm similar to that described in klypin , primack , & holtzman ( 1996 ) .",
    "the algorithm identifies halos as local maxima of mass inside a given radius .",
    "the efficiency of the algorithm was improved by incorporating the idea of warren et al .",
    "( 1992 ) of finding approximate locations of density peaks using particle accelerations .",
    "this idea is based on the principle that particles with the largest accelerations should reside near the halo centers , which is true for halos with roughly isothermal density profiles . in practice , this way of finding density maxima has proved to be quite efficient . the halo identification algorithm can be described as follows .",
    "the particles are sorted according to the magnitude of their scalar accelerations .",
    "the particle with the largest acceleration determines the approximate location of the first halo center .",
    "particles located inside a sphere of radius @xmath204 centered at the halo center are assigned to the same halo and are excluded from the list of particles used to identify halos .",
    "the radius @xmath204 is an adjustable parameter ; we use a radius approximately twice as large as the force resolution of a simulation .",
    "the procedure repeats for the particle with the largest acceleration in the list of remaining particles .",
    "the peaks are identified until there are no particles in the list .",
    "when all the density peaks are identified , we proceed to find more accurate positions of the halo centers .",
    "this is done iteratively by finding the center of mass of all particles inside @xmath204 and displacing the center of the sphere to the center of mass .",
    "the procedure is iterated until convergence .",
    "when the halo centers are found , we increase @xmath204 until the overdensity inside the corresponding sphere reaches a certain limit .",
    "the limit is based on the top - hat model of gravitational collapse , which predicts the typical overdensity for virialized objects",
    "@xmath205 in cdm and @xmath206 for our @xmath7cdm model ( e.g. lahav et al . 1991 ; kitayama & suto 1996 ) .",
    "however , we denote halo radius and the mass inside this radius defined as @xmath207 and @xmath208 regardless of the actual value of the limit .",
    "smaller halos located within a radius @xmath208 of a bigger halo are deleted from the list .    as an output",
    "we get a list of halo positions , velocities , and parameters ( such as @xmath208 and @xmath207 ) .",
    "we applied the halo - finding algorithm described above to identify halos in the simulations at zero redshift . only halos with more than 100 particles within @xmath208",
    "were taken from the full list .",
    "we also present results for relatively isolated halos , excluding all halos which have close ( @xmath209 ) neighbors of mass more than half of the halo mass .",
    "the density profiles were constructed by estimating the density in concentric spherical shells of logarithmically increasing thickness , with the smallest radius corresponding to the maximum resolution .",
    "the resulting profiles were fitted with the analytical formula of nfw ( eq . [ 19 ] ) , taking the the scale radius @xmath192 as the fit parameter . in figures 14 and 15",
    "we show density profiles of nine halos of different mass identified in the high - resolution cdm and @xmath7cdm simulations , along with the analytic fits to the halos .",
    "the nfw profile appears to be a good approximation for halos of all masses ( within the mass range of our simulations ) in both cdm and @xmath7cdm models .",
    "nfw argued that the _ concentration parameter _ , @xmath210 ( see eq.[19 ] ) , of a dark matter halo depends on the halo mass .",
    "they found that low - mass halos are more centrally concentrated than high - mass ones , which possibly reflects a trend in the formation redshifts of halos .",
    "figure 16 shows the concentration @xmath211 as a function of halo mass ( @xmath207 ) for halos identified in our simulations .",
    "the solid curve represents a theoretical prediction ( see nfw for discussion ) assuming the definition for the formation time of a halo as the first time when half of its final mass @xmath207 lies in progenitors with individual masses exceeding a fraction @xmath212 of @xmath207 .",
    "this particular value of @xmath70 seemed to provide the best approximation to the numerical results of nfw for the cdm model .",
    "the results of both the cdm and the @xmath7cdm simulations agree reasonably well with this curve .",
    "the larger spread of parameter @xmath211 for low - mass halos arises mainly from statistical noise .",
    "the lowest mass halos ( @xmath213 m@xmath214 ) contain a few hundred particles within their @xmath208 and thus have more noisy density profiles ( typical @xmath215 error in @xmath216 @xmath217 ) than more massive halos ( @xmath218 m@xmath214 ) which have tens of thousands of particles ( error in @xmath216 @xmath219 ) .",
    "it is important to model reliably the structure of the very central part ( @xmath220 kpc ) of a halo because that is the part for which we can compare model predictions with observational results .",
    "unfortunately , that part is also where force resolution may strongly affect the shape of the density profiles . to study possible effects of force resolution , we have compared density profiles of _ the same _ halos taken from @xmath7cdm simulations of different resolution described in  5.2 . in figure 17",
    "we compare density profiles of four halos from these simulations .",
    "as before , the density profile is drawn only to the resolution limit of the simulation .",
    "we conclude that , up to the resolution limit , the lower resolution density profile follows the higher resolution density profile .",
    "we studied the structure of dark matter halos in cdm and @xmath7cdm models with a resolution of @xmath3 kpc in a box of @xmath221 mpc .",
    "we found that for @xmath222 , the density profiles of _ all _ halos in both cdm and @xmath7cdm simulations are well fitted by the analytical formula ( eq . [ 19 ] ) of nfw .",
    "the mass dependence of the halo concentration parameter @xmath211 in our simulations is consistent with the results of nfw .",
    "the fact that our results for the cdm model agree with the results of nfw serves both as a final test of the presented code and as an independent check of their method with results from _ direct _ cosmological simulations .",
    "we present a new high - resolution @xmath0-body code that incorporates the idea of an adaptive refinement tree ( khokhlov 1997 ) to build a hierarchy of refinement meshes in regions where higher resolution is desired .",
    "unlike other @xmath0-body codes that make use of refinement meshes , our code is able to construct meshes of arbitrary shape covering both elongated structures ( such as filaments and walls ) and roughly spherical dark matter halos equally well .",
    "the meshes are _ modified _ to adjust to the evolving particle distribution instead of being _ rebuilt _ at every time step .",
    "we use a cubic grid as the zeroth level of the mesh hierarchy .",
    "the size of this grid determines the minimum possible resolution of a simulation ( i.e. , resolution in regions where there are no refinements ) .",
    "the code blocks working on the zeroth - level grid are similar to those of a pm code . to solve the poisson equation on refinement meshes , we have developed a new solver that uses a multilevel relaxation method with successive overrelaxation ( hockney & eastwood 1981 ; press et al .",
    "the solver is fully parallel and an fft solver is only twice as fast as our solver for the same number of mesh cells . in real simulations with the same resolution",
    ", the relaxation solver outperforms the fft because the resolution is achieved with a much smaller number of cells ( see  3.7 ) .",
    "the tests presented (  4 ) show that our code adequately computes gravitational forces down to scales of @xmath223 mesh cells .",
    "the memory overhead in the current version of the code is rather large compared to that of other high - resolution codes .",
    "the number of required mesh cells , however , changes very slowly with increasing resolution . at present",
    ", the code is capable of handling a dynamic range of @xmath224 and higher . in our latest runs ,",
    "for which we have used modified version of the code incorporating multiple time steps , we reached a dynamic range of @xmath225 for a system of @xmath110 particles .",
    "tests of the code performance show that it is about three times faster than an ap@xmath12 m code ( and , therefore , tree code ; see couchman 1991 ) of comparable resolution .",
    "still , the condition requiring that particles move only a certain fraction of a mesh cell at every time step makes the code cpu rather than memory limited .",
    "the memory requirement of the code can be significantly reduced if pointers that are used to support the tree refinement structure are shared by siblings ( descendants of the same parent cell ) .",
    "the memory overhead can be reduced even further by incorporating more elaborate data - storage algorithms ( khokhlov 1997 ) .",
    "the data structures can also be changed to allow for parallelization on distributed memory architectures .",
    "the version of the code presented , like any other high - resolution code , is constrained by the condition that particles move only a fraction of a mesh cell in a single time step on any of the refinement levels . to ensure that this condition is satisfied on the maximum refinement level requires small time steps redundant for particles moving on coarser meshes .",
    "we are now working on integration scheme with multiple time steps .",
    "we have used the art code to study the structure of dark matter halos in two cosmological models ",
    "standard cdm ( @xmath4 , @xmath5 , @xmath6 ) and a variant of @xmath7cdm ( @xmath197 , @xmath173 , @xmath10 ) .",
    "we have found that halos formed in @xmath7cdm model have density profiles similar to halos formed in cdm model .",
    "the density profiles are well described by the analytical formula ( eq.[19 ] ) presented by navarro et al .",
    "( 1996a ) and have cuspy [ @xmath11 structure in the central ( @xmath220 kpc ) parts of a halo with no indications of a core down to the resolution limit of our simulations .",
    "the similar model with different parameters ( @xmath226 and @xmath5 ) , whose are not ideal because of rather small hubble constant , was independently studied in recent papers by navarro ( 1996 ) and navarro , frenk , & white ( 1996b ) with conclusions different from ours .",
    "these authors conclude that halos formed in @xmath7cdm model are less centrally concentrated and are thus more in accord with dynamics of dwarf galaxies .",
    "our analysis has shown that the major source of this incosistency lies in the definitions of halo radius and concentration parameter @xmath211 : in the above studies the authors neglect the fact that virialization radius in the @xmath7cdm model corresponds to overdensity @xmath227 rather than to @xmath136 ; they also normalize their densities to the critical density rather than average density of the universe ( as assumed in the top hat collapse model ) .",
    "we were able to reproduce their results when we followed their definitions .",
    "we therefore conclude that halos formed in the @xmath7cdm model have structure similar to that of the cdm halos and thus can not explain the dynamics of the central parts of dwarf spiral galaxies inferred from the galaxies rotation curves .",
    "we would like to thank almadena chtchelkanova ( berkeley research associates , inc . ) for useful advice on various data structures and algorithms .",
    "we would like also to thank gustavo yepes , michael norman , and michael gross for fruitful discussions on various aspects of numerical simulations .",
    "we are grateful to chris loken for help in improving the presentation of the paper .",
    "this project was supported in part by the grant ast9319970 from the national science foundation and by the naval research laboratory through the office of naval research .",
    "the simulations were done at the national center for supercomputing applications ( ncsa ) .",
    "aho , a.v .",
    ", hopcroft , j.e . , & ullman , j.d .",
    "1983 , _ data structures and algorithms _",
    "( reading : addison - wesley ) anninos , p. , norman , m.l . , & clarke , d.a .",
    "1994 , 436 , 11 berger , m.j .",
    "1986 , siam j.sci.stat.comput . 7 ,",
    "904 berger , m.j . , & colella , p. 1989 , j.comp.phys .",
    "82 , 64 berger , m.j . , & oliger , j. 1984 , j.comp.phys .",
    "53 , 484 bertschinger , e. 1985 , 58 , 39 bouchet , f.r . , & hernquist , l. 1988 , 68 , 521 brandt , a. 1977 , math .",
    "comput . , 31 , 333 burkert , a. , 1996 , , 447 , l25 carrol , s.m . , press , w.h . , & turner , e.l .",
    "1992 , ara&a , 30 , 499 cole , s. , & lacey , c. 1996 , , 281 , 716 corner , t.h . ,",
    "leiserson , c.e .",
    ", & rivest , r.l .",
    "1994 , _ introduction to algorithms _",
    "( new york : mcgraw - hill ) couchman , h.m.p .",
    "1991 , , 368 , l23 crone , m.m . ,",
    "evrard , a.e . , & richstone , d.o . , 1994 ,  434 , 402 dubinski , j. , & carlberg , r.g . 1991 , , 378 , 496 efstathiou , g. , davis , m. , frenk , c.s . , & white , s.d.m . 1985 , , 57 , 241 efstathiou , g. , frenk , c.s . ,",
    "white , s.d.m .",
    ", & davis , m. 1988 , , 235 , 715 fillmore , j.a . , & goldreich , p. 1984",
    ", , 281 , 1 flores , r.a . , & primack , j.r .",
    "1994 , , 427 , l1 frenk , c.s . ,",
    "white , s.d.m . ,",
    "efstathiou , g. , & davis , m. , 1985 , nature , 317 , 595 frenk , c.s . , white , s.d.m . , davis , m. , & efstathiou , g. 1988 , , 327 , 507 gelato , s. , chernoff , d.f . , & wasserman , i. 1996 , , 480 , 115 gelb , j.m .",
    ", 1992 , phd thesis , mit gnedin , n.y .",
    "1995 , , 97 , 231 gnedin , n.y . , & bertschinger , e. 1996 , , 470 , 115 hockney , r.w .",
    ", & eastwood , j.w .",
    "1981 , _ computer simulations using particles _",
    "( new york : mcgraw - hill ) hoffman , y. 1988 , , 328 , 489 hoffman , y. , & shaham , j. 1985 , , 297 , 16 jessop , c. , duncan , m. , & chau , w.y .",
    "1994 , j.comput.phys . , 115 , 339 kates , r.e .",
    ", kotok , e.v . , &",
    "klypin , a.a .",
    "1991 , , 243 , 295 katz , n. , 1991 , , 368 , 325 kitayama , t. , & suto , y. 1996 , , 469 , 480 khokhlov , a.m. , 1997 , j.comput.phys . , submitted ( preprint astro - ph/9701194 )",
    "klypin , a.a .",
    "1996 , in international school of physics `` enrico fermi '' : dark matter in the universe , ed .",
    "s.bonometto , j.r.primack , & a.provenzale , preprint astro - ph/9605183 klypin , a.a . ,",
    "primack , j.r .",
    ", & holtzman , j. 1996 , , 466 , 13 klypin , a.a .",
    ", & shandarin , s.f .",
    "1983 , , 204 , 891 knuth , d. 1968 , _ the art of computer programming _ , vol.1 ( reading : addison - wesley ) lahav , o. , lilje , p.b . , primack , j.r . , & rees , m.j .",
    "1991 mnras , 251 , 128l lhner , r. , & baum , j.d .",
    "1991 , aiaa j. , 91 - 0620 moore , b. 1994 , nature , 370 , 629 navarro , j.f .",
    "1996 , preprint astro - ph/9610188 navarro , j.f . ,",
    "frenk , c.s . , & white , s.d.m . , 1996a , , 462 , 563 ( nfw ) navarro , j.f . ,",
    "frenk , c.s .",
    ", & white , s.d.m .",
    ", 1996b , apj , in press ( preprint astro - ph/9611107 ) peebles , p.j.e .",
    "1980 , _ the large scale structure of the universe _ ( princeton : princeton univ . press ) pen , u .- l .",
    "1995 , , 100 , 269 press , w.h . , teukolsky , s.a .",
    ", vetterling , w.t . , &",
    "flannery , b.p .",
    "1992 , _ numerical recipes in fortran _ , 2nd ed .",
    "quinn , p.j . ,",
    "salmon , j.k .",
    ", & zurek , w.h . , 1986 , nature , 322 , 329 splinter , r.j .",
    "1996 , , 281 , 281 suisalu , i. , & saar , e. 1995 , , 274 , 287 suisalu , i. , & saar , e. 1997 , , submitted ( preprint astro - ph/9511120 ) tormen , g. , bouchet , f.r . ,",
    "white , s.d.m .",
    "1996 , , 286 , 865 villumsen , j.v .",
    "1989 , , 71 , 407 warren , m.s . ,",
    "quinn , p.j . ,",
    "salmon , j.k .",
    ", & zurek , w.h . 1992 , , 399 , 405 wesseling , p. 1992",
    ", _ an introduction to multigrid methods _",
    "( new york : wiley ) xu , g. 1995 , , 98 , 355 zeldovich , ya.b .",
    ", 1970 , , 5 , 84"
  ],
  "abstract_text": [
    "<S> we present a new high - resolution @xmath0-body algorithm for cosmological simulations . </S>",
    "<S> the algorithm employs a traditional particle - mesh technique on a cubic grid and successive multilevel relaxations on the finer meshes , introduced recursively in a _ fully adaptive _ manner in the regions where the density exceeds a predefined threshold . </S>",
    "<S> the mesh is generated to effectively match an _ </S>",
    "<S> arbitrary _ geometry of the underlying density field  a property particularly important for cosmological simulations . in a simulation </S>",
    "<S> the mesh structure is not created at every time step but is properly adjusted to the evolving particle distribution . </S>",
    "<S> the algorithm is fast and effectively parallel : the gravitational relaxation solver is approximately half as fast as the fast fourier transform solver on the same number of mesh cells . </S>",
    "<S> the required cpu time scales with the number of cells , @xmath1 , as @xmath2 . </S>",
    "<S> the code allows us to improve considerably the spatial resolution of the particle - mesh code without loss in mass resolution . </S>",
    "<S> we present a detailed description of the methodology , implementation , and tests of the code .    </S>",
    "<S> we further use the code to study the structure of dark matter halos in high - resolution ( @xmath3 kpc ) simulations of standard cdm ( @xmath4 , @xmath5 , @xmath6 ) and @xmath7cdm ( @xmath8 , @xmath9 , @xmath10 ) models . </S>",
    "<S> we find that halo density profiles in both cdm and @xmath7cdm models are well fitted by the analytical model presented recently by navarro et al . , which predicts a singular [ @xmath11 behavior of the halo density profiles at small radii . </S>",
    "<S> we therefore conclude that halos formed in the @xmath7cdm model have structure similar to cdm halos and thus can not explain the dynamics of the central parts of dwarf spiral galaxies , as inferred from the galaxies rotation curves . </S>"
  ]
}