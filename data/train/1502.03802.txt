{
  "article_text": [
    "the current state - of - art coding method like hevc  @xcite usually relies on a block based hybrid scheme . a block in a video frame is first predicted through the best inter- or intra - predcition , and the prediction error ( residual )",
    "is then passed into a transform coder , where a fixed orthogonal transform such as dct is used .",
    "the resulting transform coefficients are then quantized and the non - zero dct coefficients , as well as the prediction candidate locations ( e.g. the motion vectors in the case of inter - prediction ) and/or modes are finally coded using an entropy coder . in  @xcite",
    ", we proposed a new video coding scheme that directly represents each block in a video frame using a self - adaptive dictionary consisting of all possible temporal prediction candidate blocks over a search range in the previous frame . here in this work ,",
    "we propose an extension of that by including a second stage that codes the residual after approximating the current block using the chosen atoms in the first stage .",
    "the rationale for this second stage is that we have identified the self - adaptive dictionary loses its representation efficiency after first few chosen atoms .",
    "this is because the atoms in the self - dictionary may be highly correlated and concentrated in a sub - space .",
    "we code the residual after the second stage using a non - redundant orthonormal dictionary that spans the null - space of the sub - space spanned by chosen atoms in the first stage . in our current implementation",
    ", we derive this second dictionary by orthonormalizing all dct transform basis blocks with respect to the chosen atoms in the first stage .",
    "we would like to emphasize that the proposed two - stage framework differs significantly from the state - of - art video coding standards . specifically , although our self - adaptive dictionary consists of conventional motion search candidate blocks and we use a linear combination of a few motion search candidates for the first stage representation ( which we term as atoms as in sparse recovery literature ) , we allow any combination of possible candidates and the weights are solved through an optimization problem ; we do not restrict the number of chosen atoms to a pre - set number , but minimizing this number through minimizing the l0-norm of the weights . by comparison",
    ", state - of - art coding standards like hevc only permit a fixed linear combination of several adjacent motion candidates through the use of fractional - pel motion compensation , where the weights are implicitly determined by the interpolation filter for deriving fractional - pel motion vectors .",
    "besides , our second stage orthonormalizes the fixed dct basis with respect to the block - dependent first stage chosen atoms .",
    "effectively , the orthonormalization in the second stage adaptively alters the dct basis to span over the null space of the first stage reconstruction and hence is more efficient in representing the residuals .",
    "to the contrary , the transform coding stage in all current video coding standards is fixed .",
    "several research groups have attempted using redundant dictionaries for block - based image and video coding , including  @xcite . in",
    "all reported dictionary - based video coders , the dictionary atoms are used to represent the motion - compensated residual block for intra / inter - frame video coding .",
    "therefore , they are very different from what is proposed here .",
    "the work in  @xcite codes each frame in the intra - mode , with a dictionary that is updated in real time based on the previously coded frames .",
    "although such online adaptation can yield a dictionary that matches with the video content very well , it is computationally very demanding",
    ". furthermore , the dictionary is not block adaptive .",
    "the proposed framework uses a self - adaptive dictionary that only depends on the block location , without requiring realtime design / redesign of the dictionary .",
    "two major challenges have to be addressed for making the two - stage framework efficient .",
    "the first is the fact that the redundant dictionary is highly correlated . directly quantizing the coefficients with the highly correlated atoms",
    "is inefficient . in this work , as our original work in  @xcite , we recursively orthonormalizes the chosen atoms during the iteration of omp .",
    "however we have developed an improved omp algorithm to solve the sparse recovery problem , which can yields a sparser representation than the original omp method  @xcite .",
    "the coefficients associated with the orthonormalized chosen atoms can be quantized independently without losing efficiency .",
    "the second challenge is how to determine the switch point from the first stage to the second stage . in this work",
    ", we switch to the second stage when the first stage self - adaptive dictionary begins to slowly correct the residual , which indicates that the remaining atoms in the self - adaptive dictionary have limited representation power for the remaining residual .    in the remainder of this paper , we describe the proposed two - stage framework in sec .",
    "[ sec : spdcp ] ; we present the entropy coder design in sec .",
    "[ sec : ent ] ; in sec .",
    "[ sec : res ] we show the rd performance and comparisons .",
    "we conclude the paper in sec .",
    "[ sec : con ] .",
    "the proposed two - stage framework is illustrated in fig .",
    "[ fig : framework ] . we describe the different components in more details below .",
    "the first stage is similar to the one reported in  @xcite . for each mean - removed block",
    "represented as a vector @xmath0 , denote the dictionary by @xmath1 , which consists of the mean - removed vector representation of all possible motion search candidate blocks corresponding to integer motion vectors in a preset motion search range , and the coefficients by @xmath2 .",
    "we find the sparse set of coefficients by solving the following l0-norm minimization problem .",
    "@xmath3 we set @xmath4 to be slightly larger than the expected distortion introduced by the quantization stepsize @xmath5 , i.e. @xmath6 .",
    "because of the non - convexity of l0-norm , greedy algorithm is often used to solve this problem .",
    "one prominent algorithm is the orthogonal matching pursuit ( omp ) . in the original omp algorithm",
    "@xcite , at every iteration , the correlation between the residual and the remaining atoms are evaluated , and the best correlated atom is chosen ; a least square step is then used to update all the coefficients , where the coefficients are related to the original chosen atoms .",
    "the least squares update step effectively makes the residual to lie in the null space of the chosen atoms . in  @xcite , we implemented the original omp algorithm with embedded orthonormalization of chosen atoms so that we could solve the sparse reconstruction and atom orthonromalization simultaneously .",
    "this is accomplished by orthonormalizing the latest chosen atom with respect to all previously chosen atoms using a gram - schmidt procedure , and finding the coefficient associated with the orthonormalized latest chosen atom by a simple inner product . in this work ,",
    "we use an improved version of omp , denoted by eomp  @xcite . in each iteration , instead of evaluating the correlation between the residual and the remaining atoms , we first update the remaining atoms by orthonormalizing them to all previously chosen orthonormalized atoms . this orthonormalization can be done efficiently through a recursive one - step orthonormalization procedure .",
    "let @xmath7 denote the @xmath8-th remaining atoms , @xmath9 the last chosen atom , the one - step orthonormalization is given by the following : @xmath10 the correlation between the residual and orthonormalized remaining atoms are then evaluated and the atom with the largest correlation ( i.e. the magnitude of the inner product ) is chosen . note that this chosen atom is already orthonormalized and the coefficient corresponding to this atom has already been calculated . by recursively updating the remaining atoms , we have shown in  @xcite that this improved eomp has better sparsity recovery capability than the original omp algorithm .    the early termination constraint in eq .",
    "[ eq : l0 ] is expressed in the terms of residual norm reduction ratio threshold @xmath11 , a threshold on the ratio between the residual norm difference of the current and last iterations and the residual norm of the last iteration .",
    "this constraint is specially designed to terminate the omp algorithm before reaching the fidelity constraint if newly chosen atom does not decrease the residual norm significantly , which usually signifies the low correlation of the residual and the remaining atoms and the loss of the representation power of the remaining atoms in the self - adaptive dictionary .",
    "obviously , the choice of @xmath11 will affect the overall coding efficiency and must be chosen appropriately .",
    "once all the orthonormalized atoms denoted by a matrix @xmath12 and their corresponding coefficients @xmath13 are found , we apply uniform quantization with deadzone to each coefficient with the same stepsize @xmath5 , with the deadzone @xmath14 , following the hevc standard recommendation for inter - coded block . denoting the quantized coefficients by @xmath15 , the first stage reconstructed block",
    "can be represented by @xmath16 , and the residual is @xmath17 .",
    "we start our discussion with a plot demonstrating the motivation for designing the second stage . in fig .",
    "[ fig:2stageprof ] , the continuous black curve shows the residual norm versus number of chosen atoms ( @xmath18 ) from a particular block in sequence _ touchdownpass _ , when only self - adaptive first stage is used . the beginning part of the curve shows a very fast decay of the residual norm , which indicates the effectiveness of the self - adaptive dictionary .",
    "however , after few iterations , the decaying rate significantly slows down for each newly chosen atom .",
    "this suggests that the remaining atoms after a few iteration may lie largely in the same subsapce as the chosen atoms and can not represent efficiently the residual , which is orthogonal to this subspace .",
    "in other words , the self - adaptive dictionary may not span over the full space @xmath19 and it is very inefficient to reduce the residual after a few iterations .",
    "in addition to the continuous black curve , we also show two red curves originating from two specific @xmath18s .",
    "they represent the residual norm reductions using dct basis had the first stage stopped at that @xmath18 . in the left red curve , it is apparent that stopping first stage too early yields suboptimal result , i.e. at that stage , self - adaptive dictionary is still more efficient .",
    "interestingly , the right red curve shows an opposite trend , that using dct basis becomes more efficient in reducing the residual norm compared to continuing with the self - adaptive dictionary .",
    "we would like to note that the horizontal axis of fig .",
    "[ fig:2stageprof ] is in terms of @xmath18 rather then the bit rate used to represent @xmath18 coefficients . considering the huge size difference between the self - adaptive dictionary ( in our experiments , the number of atoms is 2304 ) and dct dictionary ( 256 for @xmath20 block size ) , greater rate savings will be obtained by switching to dct at the starting point of the right red curve .    , comparing using the self - adaptive atoms alone , vs. using two stage . ]    now we give the description of the second stage coder . given the residual @xmath21 , the dct basis vectors are first orthonormalized with respect to the first stage chosen atoms .",
    "the coefficients of the residual with respect to these altered dct basis vectors are then derived simply by performing inner product of the residual vector with each altered dct basis vector .",
    "these coefficients @xmath22 are quantized with quantization stepsize @xmath23 using the same deadzone quantizer .",
    "the residual @xmath21 is therefore reconstructed as @xmath24 , where @xmath25 contains the altered dct basis vectors and @xmath26 are the quantized second stage coefficients .",
    "finally the reconstructed block @xmath27 is given by @xmath28 .",
    "the information we need to code in the first stage consists of mainly three parts : 1 ) the location of the chosen atoms ; 2 ) the order of the chosen atoms , since decoder needs such information to perform the orthonormalization using the same order as the encoder ; and 3 ) the levels of each quantized coefficients . the second stage codes the residual after the first stage using an approach similar to the residual coding method of hevc  @xcite using dct , but with a fixed transform block size equal to the block size .",
    "for the first stage , we have developed a new content adaptive arithmetic entropy coder inspired from the cabac design in hevc  @xcite .",
    "we would like to emphasize that one major difference between the proposed first stage coder and the cabac coder in hevc is that our coder needs to code the order of chosen atoms to enable the decoder to perform the same orthonormalization . as with cabac in hevc",
    ", we define two probability update modes , the normal mode when the probability for coding each bin is updated based on the past data ; and the bypass mode when such probability is fixed .",
    "we indicate which atoms are chosen by a 2d binary significance map .",
    "for example , if the search range is @xmath29 , the map size is @xmath30 , with the top - left element corresponding to the candidate block that is shifted from the current block position by @xmath31 .",
    "an element is assigned `` 1 '' if the corresponding atom is chosen , and `` 0 '' otherwise .",
    "we code the significance map using a three - layer multi - resolution approach . for the search range of @xmath32 , the bottom , middle , and top layer have sizes of @xmath30 , @xmath33 , and @xmath34 , respectively , as shown in fig .",
    "[ fig:3layers ] .",
    "each element in an upper layer corresponds to a @xmath35 region in the next layer .",
    "the encoding process starts from the top layer .",
    "every time when a `` 1 '' is coded , the subsequent @xmath35 block should be encoded until the bottom layer is reached .",
    "every pixel in the top @xmath34 layer is coded directly , with a position dependent probability . in the subsequent layers , for every @xmath35 block that corresponds to a `` 1 '' in the upper layer ,",
    "the encoder first specifies how many `` 1 ' 's are in this block .",
    "the coder then specifies the position of the first `` 1 '' the @xmath35 block in a forward horizontal scanning order ( i.e. row by row , from left to right ) using 4 bits . for the remaining `` 1 '",
    "'s in this block , the proposed coder forms a run - length representation , which is binarized using truncated unary code and entropy coded using a bitstream using the normal mode .",
    "this coding scheme is developed based on the observation that the first stage chosen atom locations are usually sparse and separated .",
    "a chosen atom in a particular location often indicates that its nearby atoms are unlikely to be chosen .",
    "therefore , in each @xmath35 block , there are usually only one atom chosen .",
    "we code the orders of the chosen atoms after encoding their locations . because we can deduce the total number of chosen atoms , i.e. @xmath18 , from the decoded significance map , we adaptively determine the total bits needed to represent the order values .",
    "for example , if 3 atoms are chosen , we will form a sequence of 3 values , each represented by 2 bits .",
    "after binarization , a bit - plane coding method is employed to code each bit plane of the @xmath18 order values .",
    "the proposed entropy coder encodes the run - lengths of `` 1 ' 's in each bit - plane and different bit - planes are encoded separately .",
    "furthermore , given the non - repeatability of the order values , the number of `` 1 ' 's is deterministic in each bit - plane .",
    "we make use of this property to further reduce the bits spent for coding .",
    "coefficient levels are encoded in a reversed chosen order , which has been specified in the previous step .",
    "we split the levels into the absolute values and signs , and code them separately . for the signs ,",
    "the bypass mode is always used . for absolute values ,",
    "predictive coding is used .",
    "the last chosen atom s level is predictively encoded first . for remaining levels but the first chosen atom , the coder always uses the previously encoded level as prediction .",
    "the first chosen atom s level however , is directly binarized by truncated unary and exponential golomb code .",
    "this is because we have observed a different distribution for the first chosen atoms .",
    "finally all binarized absolute values are coded using the normal mode and written to the bitstream .",
    "we show the coding performance of the proposed coder and other four comparison coders including the one - stage only baseline coder in  @xcite for two test sequences , _ football _ , and _",
    "city_. the _ football _ sequence has a frame size @xmath36 and the _ city _ sequence has a frame size @xmath37 .    in the proposed two - stage framework",
    ", there are a total of three parameters controlling the final rate .",
    "two are the quantization stepsizes for first stage and second stage , @xmath5 and @xmath23 , respectively ; and the third is the residual norm reduction ratio threshold @xmath11 for switching from the first stage . at present time , we used an exhaustive search to determine the optimal @xmath5 , @xmath23 , and @xmath11 for each target rate in each sequence .",
    "we have found that for a given @xmath23 , optimal @xmath5 is usually same or slightly smaller than @xmath23 , and @xmath11 increases with @xmath5 .    for both the one - stage and two - stage coders , we use the eomp proposed in  @xcite to derive the first stage atom coefficients . for hevc",
    ", we restrict the prediction and transform unit up to @xmath20 , but allow the smaller prediction and transform sizes . for h.264 , we compare both a version with cabac and all other advanced coding tools and subpartitions enabled ( referred as h264cabac ) , as well as the baseline cavlc with only @xmath20 block size , denoted by h264cavlc .",
    "figure  [ fig : rd ] shows the rate - distortion curves of encoding one p - frame ( based on a previous frame coded as an intra - frame using h.264 ) for proposed and comparison methods . for both sequences ,",
    "the two - stage coder significantly improved upon the one - stage coder .",
    "it also outperforms both h.264 options significantly .",
    "more importantly , it provides substantial gain over hevc in the low rate range , although it fails to compete with hevc in the high rate region .",
    "we believe one reason our two - stage coder becomes less efficient than hevc at higher rates is because our second stage coding using a fixed transform size , whereas hevc allows variable transform sizes as well as transform skip .",
    "figure  [ fig : rd10frm ] shows the rate - distortion curves for encoding ten p - frames , using ippp structure .",
    "the rate and psnr are averaged from all ten p - frames .",
    "our proposed coder can still outperform both h.264 options , and is competitive with hevc in the low to intermediate rate range ( in fact the proposed coder is up to 0.5db better for _ football _ at low rate range ) .",
    "the significant gain that was achieved for coding the first p - frame at low rates vanished unfortunately .",
    "we suspect that this is because our coder has not implemented in - loop filters such as deblocking filter and sample adaptive offset filter .",
    "those filters are known to mitigate the error propagation effect . because our first stage dictionary consists of blocks from the previously decoded frame ,",
    "its representation power reduces when the previous frame has large quantization error .",
    "effective in - loop filtering is likely to suppress this efficiency loss .",
    "besides , our proposed coder only uses inter - prediction candidates in the self - adaptive first - stage dictionary .",
    "we speculate the absence of intra - modes may also contribute to the loss of efficiency in coding subsequent frames .",
    "in this work we propose a two - stage video encoder framework , which uses a self - adaptive redundant dictionary in the first stage and uses dct bases ( after orthonormalization with the first stage chosen atoms ) in the second stage .",
    "we use a residual norm reduction ratio threshold to switch from the first to the second stage .",
    "we further propose a complete context adaptive binary entropy coding method to efficiently code the location , order and the coefficient values of the chosen atoms of the first stage .",
    "the location and values of non - zero coefficients in the second stage are coded following the same transform coding method of hevc for coding the prediction residual , but using a fixed transform size of @xmath20 .",
    "the two - stage coder achieved around 1db psnr improvement over our previous one - stage approach in  @xcite .",
    "its performance is generally competitive with hevc , with slightly better performance at low rates , and worse performance at very high rates .",
    "note that our present coder uses a fixed block size of @xmath20 , has only inter - prediction candidates in the self - adaptive dictionary , and has not implemented in - loop filtering .",
    "the fact that it is competitive with hevc is quite promising , considering that hevc allows variable block size both for prediction and for coding of prediction residual , adpative switching between inter- and intra - prediction , and incorporates sophisticated in - loop filtering .",
    "there are many aspects of the present coder that can be improved .",
    "adaptive switching from the first stage to the second stage on a block - basis is likely to yield significant gain .",
    "incorporation of in - loop filtering and intra - prediction may also bring significant improvements ."
  ],
  "abstract_text": [
    "<S> in this work , we propose a two - stage video coding framework , as an extension of our previous one - stage framework in  @xcite . </S>",
    "<S> the two - stage frameworks consists two different dictionaries . </S>",
    "<S> specifically , the first stage directly finds the sparse representation of a block with a self - adaptive dictionary consisting of all possible inter - prediction candidates by solving an l0-norm minimization problem using an improved orthogonal matching pursuit with embedded orthonormalization ( eomp ) algorithm , and the second stage codes the residual using dct dictionary adaptively orthonormalized to the subspace spanned by the first stage atoms . the transition of the first stage and the second stage is determined based on both stages quantization stepsizes and a threshold . </S>",
    "<S> we further propose a complete context adaptive entropy coder to efficiently code the locations and the coefficients of chosen first stage atoms . </S>",
    "<S> simulation results show that the proposed coder significantly improves the rd performance over our previous one - stage coder . </S>",
    "<S> more importantly , the two - stage coder , using a fixed block size and inter - prediction only , outperforms the h.264 coder ( x264 ) and is competitive with the hevc reference coder ( hm ) over a large rate range . </S>"
  ]
}