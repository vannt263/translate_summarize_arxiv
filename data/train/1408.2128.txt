{
  "article_text": [
    "for @xmath1-variate data assumed to arise from a continuous random variable , statistical inference is commonly focused on elliptical distributions @xcite ; in this class , the gaussian distribution is the most widely considered because of its computational and theoretical convenience .",
    "however , for many applied problems , the tails of the gaussian distribution are lighter than required .",
    "the @xmath0 distribution , thanks to the degrees of freedom , provides a common way to broaden the gaussian tails ( @xcite and @xcite ) .",
    "a further elliptical alternative is represented by the contaminated gaussian distribution , a two - component gaussian mixture in which one of the components , with a large prior probability , represents the `` good '' observations , and the other , with a small prior probability , the same mean , and an inflated covariance matrix , represents the `` bad '' observations @xcite .",
    "it constitutes a common and simple theoretical model for the occurrence of outliers , spurious points , or noise ( collectively referred to as  bad \" points herein ) .",
    "the contaminated gaussian distribution is the core of this paper .",
    "firstly , for maximum likelihood ( ml ) estimation of its parameters , we propose an expectation - conditional maximization ( ecm ) algorithm @xcite which stems from the characterization of the contaminated gaussian distribution as a gaussian scale mixture model .",
    "in contrast with the ecm algorithm illustrated in @xcite , in each cm step of the new algorithm we have the advantage that the mean and the covariance matrix ( for the good observations ) are updated independently from the other two parameters ( proportion of good observations and inflation parameter ) . secondly , we introduce the contaminated gaussian factor analysis model , a robust extension of the classical ( gaussian ) factor analysis model obtained by adopting the contaminated gaussian distribution for the errors and the latent factors .",
    "our proposal advantageously embeds the ( gaussian ) factor analysis model in a larger model with two additional parameters that : ( 1 ) afford protection against elliptical non - gaussianity ( of errors and/or latent factors ) and ( 2 ) allow for automatic detection of bad points .",
    "thirdly , @xcite have recently proposed mixtures of contaminated gaussian distributions both as a robust generalization of mixtures of gaussian distributions , and as an improvement of mixtures of @xmath0 distributions in terms of automatic detection of bad points in a clustering perspective .",
    "however , the mixture of contaminated gaussian distributions , with unrestricted component - covariance matrices of the good observations , say @xmath3 , is a highly parametrized model with @xmath4 parameters for each @xmath3 , @xmath5 .",
    "to introduce parsimony , @xcite also define thirteen variants of the general model obtained , as in @xcite , via eigen - decomposition of @xmath6 .",
    "but if @xmath1 is large relative to the sample size @xmath2 , it may not be possible to use this decomposition to infer an appropriate model for @xmath6",
    ". even if it is possible , the results may not be reliable due to potential problems with near - singular estimates of @xmath3 when @xmath1 is large relative to @xmath2 . to address this problem , following the literature on the adoption of factor analyzers within mixture models ( see , among many others , ( * ? ? ?",
    "* chapter  8) , @xcite , @xcite , @xcite , @xcite , and @xcite ) , we propose mixtures of contaminated gaussian factor analyzers , where a contaminated gaussian factor analysis model is used for each mixture component .",
    "the result is a means of fitting mixtures of contaminated gaussian distributions in situations where @xmath1 would be sufficiently large relative to the sample size @xmath2 to cause potential problems with singular or near - singular estimates of @xmath6 .",
    "the number of free parameters is controlled through the dimension of the latent factor space .",
    "the paper is organized as follows .",
    "section  [ sec : the contaminated gaussian distribution ] contextualizes the contaminated gaussian distribution as a gaussian scale mixture model .",
    "the contaminated gaussian factor analysis model is introduced in section  [ sec : the contaminated gaussian factor analysis model ] while mixtures of contaminated gaussian factor analyzers are presented in section  [ sec : mixtures of contaminated gaussian factor analyzers ] .",
    "in particular , in each section , hence for each model : ( 1 ) identifiability is discussed , ( 2 ) an em - based algorithm is outlined for ml estimation of the parameters , ( 3 ) computational details are given , and ( 4 ) a real data analysis is discussed to appreciate the advantages of the model .",
    "computation is done in the software environment for statistical computing and graphics @xcite .",
    "the paper concludes with a discussion in section  [ sec : discussion and future work ] .",
    "for robustness sake , one of the most common ways to generalize the gaussian distribution is represented by the gaussian scale mixture model @xmath7 where @xmath8 is a probability distribution function and @xmath9 denotes the density of a @xmath1-variate gaussian random vector @xmath10 with mean @xmath11 and covariance matrix @xmath12 .",
    "model   is unimodal , symmetrical , and guarantees heavier tails than those of the gaussian distribution ( see , e.g. , * ? ? ?",
    "it also includes some well - known models ; for example , if @xmath13 , then we obtain the @xmath0 distribution with location parameter @xmath11 , positive definite inner product matrix @xmath14 , and @xmath15 degrees of freedom .    for our aim , it is important to note that if we focus on the dichotomous random variable @xmath16 with probability mass function @xmath17 then , from model  , we obtain the contaminated gaussian distribution @xmath18 where @xmath19 and @xmath20 ( cf . * ? ? ?",
    "* ) . in summary , @xmath21 @xmath22 and @xmath23 as we can see in",
    ", a contaminated gaussian distribution is a two - component gaussian mixture in which one of the components , typically with a large prior probability @xmath24 , represents the `` good '' observations , and the other , with a small prior probability , the same mean , and an inflated covariance matrix @xmath25 , represents the `` bad '' observations @xcite . as a special case of , if @xmath24 and/or @xmath26 tend to one , then @xmath27 .",
    "an advantage of with respect to the often used @xmath0 distribution is that , once the parameters in @xmath28 are estimated , say @xmath29 , we can establish whether a generic observation @xmath30 is either good or bad via the _ a  posteriori _ probability .",
    "that is , compute @xmath31 and @xmath30 will be considered good if @xmath32 , while it will be considered bad otherwise .",
    "given an observed random sample @xmath33 from @xmath34 , we consider now the application of the expectation - conditional maximization either ( ecme ) algorithm of @xcite for maximum likelihood ( ml ) estimation of @xmath28 ; here , based on , , and , @xmath35 is partitioned as @xmath36 , where @xmath37 and @xmath38 .",
    "the ecme algorithm is an extension of the ecm algorithm of @xcite . with this extension , some or all of the cm - steps of the ecm algorithm are replaced by steps that conditionally directly maximize the observed - data log - likelihood function , and not the expectation of the complete - data log - likelihood .",
    "anyway , both these algorithms are variants of the classical expectation - maximization ( em ) algorithm @xcite , which is a natural approach for ml estimation when data are incomplete . in our case",
    ", incompleteness arises from the characterization of the contaminated gaussian distribution given by , , and .",
    "the complete data are taken to be @xmath39 , and the complete - data likelihood can be written @xmath40 accordingly , the complete - data log - likelihood can be written @xmath41 where @xmath42 and @xmath43      the e - step on the @xmath44th iteration requires the calculation of @xmath45\\nonumber\\\\ & = &   e_{\\boldsymbol{\\vartheta}^{\\left(k \\right)}}\\left[l_{1c}\\left(\\boldsymbol{\\vartheta}_1\\right)|\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_n\\right ]   + e_{\\boldsymbol{\\vartheta}^{\\left(k \\right)}}\\left[l_{2c}\\left(\\boldsymbol{\\vartheta}_2\\right)|\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_n\\right ]   \\nonumber\\\\ & = & q_1\\left(\\boldsymbol{\\vartheta}_1;\\boldsymbol{\\vartheta}^{\\left(k\\right)}\\right)+q_2\\left(\\boldsymbol{\\vartheta}_2;\\boldsymbol{\\vartheta}^{\\left(k\\right)}\\right ) , \\label{eq : contaminated : expected complete - data loglikelihood}\\end{aligned}\\ ] ] where the expectation , as it can be noted by the subscript , is taken using the current fit @xmath46 for @xmath35 . here",
    ", we replace @xmath47 by @xmath48=w_i^{\\left(k \\right ) } , \\label{eq : e(w|x)}\\ ] ] where @xmath49      the first cm - step on the @xmath44th iteration requires the calculation of @xmath50 by maximizing @xmath51 with @xmath52 fixed at @xmath53 .",
    "this yields @xmath54 and @xmath55      the updating of @xmath52 , with @xmath56 fixed at @xmath50 , can not be directly derived based on the @xmath57-function because it is meaningless to estimate @xmath26 when @xmath58 is given . to solve this issue , the updating of @xmath24 and @xmath26 is directly performed based on the observed - data log - likelihood ; this choice leads to the ecme algorithm .",
    "in particular , the second cm - step , on the @xmath44th iteration , requires the calculation of @xmath59 by maximizing the observed - data log - likelihood @xmath60 , \\label{eq : conditional likelihood for alpha and eta}\\ ] ] under the constraints @xmath61 and @xmath62 ; the latter constraint is justified by the fact that , for practical purposes , one could require that the proportion of good data is at least equal to a pre - determined value @xmath63 , the most natural choice being @xmath64 ; this choice is justified because , in robust statistics , it is usually assumed that at least half of the points are good ( cf . * ? ? ?",
    "the second cm - step is operationally performed using the ` optim ( ) ` function in the * stats * package for .",
    "the method of @xcite is considered for the numerical search of the maximum .",
    "the choice of the starting values for the ecm algorithm constitutes an important issue .",
    "the standard initialization consists of selecting a value for @xmath65 .",
    "in particular , a random initialization is usually repeated @xmath0 times , from different random positions , and the solution maximizing the observed - data log - likelihood @xmath66 among these @xmath0 runs is selected ( see @xcite , @xcite , and @xcite for more complicated strategies ) .    instead of selecting @xmath65 randomly , we suggest the following technique .",
    "the gaussian distribution can be seen as nested in the corresponding contaminated gaussian distribution . in particular ,",
    "the former can be obtained from the latter when @xmath67 and @xmath68 .",
    "then , the closed - form ml estimates of @xmath11 and @xmath14 for the gaussian distribution , along with the constraints @xmath69 ( with @xmath70 ) and @xmath71 ( with @xmath72 ) , can be used as @xmath65 ; in the analysis of section  [ subsec : cn : real data analysis ] , we fix @xmath73 and @xmath74 . from an operational point of view , thanks to the monotonicity property of the ecm algorithm ( see , e.g. , * ? ? ?",
    "* ) , this also guarantees that the observed - data log - likelihood of the contaminated gaussian distribution will be always greater than or equal to the observed - data log - likelihood of the corresponding gaussian distribution .",
    "this is a fundamental consideration for the use of likelihood - based model selection criteria , and likelihood ratio tests , for choosing / assessing between a gaussian distribution and a contaminated gaussian distribution .",
    "the aitken acceleration @xcite is used to estimate the asymptotic maximum of the log - likelihood at each iteration of the ecm algorithm .",
    "based on this estimate , we can decide whether or not the algorithm has reached convergence ; i.e. , whether or not the log - likelihood is sufficiently close to its estimated asymptotic value . the aitken acceleration at iteration @xmath75 is given by @xmath76 where @xmath77 is the observed - data log - likelihood value from iteration @xmath78 .",
    "then , the asymptotic estimate of the log - likelihood at iteration @xmath79 is given by @xmath80 cf .",
    "the ecm algorithm can be considered to have converged when @xmath81 . in the analysis of section  [ subsec : cn : real data analysis ] , we fix @xmath82 .      the bivariate data considered here relate 298 daily returns of two financial indexes ( dax 30 and ftse mib ) spanning the period from july 24th , 2012 , to september 30th , 2013 ( the share prices used to compute the daily returns are downloadable from http://finance.yahoo.com/ ) .",
    "a scatter plot of the data is provided in  [ fig : financial data ] .",
    "mardia s test of multivariate symmetry , as implemented by the ` mardia ( ) ` function of the * psych * package @xcite , produces a @xmath1-value of @xmath83 , leading us to not reject the null at the commonly considered significance levels .    in the class of symmetric bivariate distributions we focus on the classical gaussian distribution , that we recall to be nested in the contaminated gaussian distribution . on data at hand ,",
    "these distributions can be statistically compared via the likelihood - ratio ( lr ) statistic @xmath84,\\ ] ] where the hat denotes the ml estimate of the underlying parameter , while @xmath85 and @xmath86 are the log - likelihood functions for the gaussian and the contaminated gaussian distributions , respectively . under the null of bivariate gaussianity ( versus the alternative of bivariate contaminated gaussianity ) ,",
    "@xmath87 is asymptotically distributed as a @xmath88 with two degrees of freedom , corresponding to the difference in the number of free parameters of the models under the two hypotheses .",
    "the resulting @xmath1-value is @xmath89 , which leads to the rejection of the null , in favor of the alternative , at any reasonable significance level .",
    "[ fig : cn : financial data ] shows the graphical results from the ml estimation of the contaminated gaussian distribution ; bad points are represented by black bullets and contour lines from the estimated distribution are superimposed .",
    "the ml estimates for @xmath24 and @xmath26 are @xmath90 and @xmath91 , respectively ; in some sense , such result indicates that we are far to infer that data at hand arise from a single gaussian distribution .",
    "the ( gaussian ) factor analysis model @xcite is a well - known , and widely used , data reduction tool aiming to find latent factors that explain the variability in the data .",
    "the model ( see * ? ? ?",
    "* chapter  3 ) assumes that the @xmath1-variate random vector @xmath10 is modeled using a @xmath92-variate vector of factors @xmath93 where @xmath94 .",
    "the model is @xmath95 where @xmath96 is a @xmath97 matrix of factor loadings and @xmath98 is the error term , with @xmath99 .",
    "it follows from that @xmath100 .",
    "the factor analysis model is , however , sensitive to bad points as it adopts the gaussian distribution for errors and latent factors . to improve its robustness , for data having longer than gaussian tails or bad points , @xcite introduce the @xmath0-factor analysis model which considers the multivariate @xmath0 for the distributions of the errors and the latent factors",
    "( see also * ? ? ?",
    "* section  5.14.4 ) .",
    "although the @xmath0-factor analysis model robustifies the classical factor analysis model , once applied to data at hand , it does not allow for automatic detection of bad points . to solve this problem , recalling , we introduce the contaminated gaussian factor analysis model .    based on , the contaminated gaussian factor analysis model generalizes the corresponding gaussian factor analysis model by assuming @xmath101 where @xmath102 using the characterization of the contaminated gaussian distribution discussed in section  [ sec : the contaminated gaussian distribution ] , the joint density of @xmath10 and @xmath103 , given @xmath104 , can be written @xmath105 with @xmath106 .",
    "thus , @xmath107 so that @xmath108 the factors and error terms are no longer independently distributed as in the normal - based model for factor analysis ; however , they are uncorrelated . to see this",
    ", we have from that conditional on @xmath109 , @xmath103 and @xmath110 are uncorrelated , and hence , unconditionally uncorrelated .      literally speaking , the number of parameters of the contaminated gaussian factor analysis model is @xmath111 : we have @xmath1 values in @xmath11 , @xmath112 values in @xmath96 , @xmath1 values in @xmath113 , one @xmath24 , and one @xmath26 .",
    "however , for identifiability sake when @xmath114 , we have to impose @xmath115 constraints for @xmath96 to be uniquely defined ( cf .",
    "@xcite and @xcite ) ; in fact , there is an infinity of choices for @xmath96 because the model is still satisfied if we replace @xmath103 by @xmath116 and @xmath96 by @xmath117 , where @xmath118 is an orthogonal matrix of order @xmath92 .",
    "the number @xmath119 of free parameters for the model is then @xmath120+p+2 .",
    "\\label{eq : free parameters in the cn factor model}\\ ] ]      to find ml estimates for the parameters @xmath121 of the contaminated gaussian factor analysis model , we consider the application of the alternating expectation - conditional maximization ( aecm ) algorithm of @xcite .",
    "the aecm algorithm is an extension of the ecm algorithm , where the specification of the complete data is allowed to be different on each cm - step . to apply the aecm algorithm , we partition @xmath35 as @xmath36 , where @xmath122 and @xmath123 . for this application of the aecm algorithm ,",
    "the @xmath124th iteration consists of two cycles , and there is one e - step and one cm - step for each cycle .",
    "the two cm - steps correspond to the partition of @xmath35 into @xmath56 and @xmath52 .",
    "the first cycle of the @xmath124th iteration of the aecm algorithm is practically equivalent to the @xmath124th iteration of the ecme algorithm for the contaminated gaussian distribution ( see section  [ subsec : cn : ml estimation via the ecme algorithm ] ) .",
    "the only difference is that we do not update @xmath14 , that is @xmath125 , but we only update @xmath11 according to and @xmath24 and @xmath26 according to what described in section  [ subsubsec : cm - step 2 : contaminated gaussian distribution ] . at the end of the first cycle , we set @xmath126 .      in the second cycle of the @xmath124th iteration of the aecm algorithm , we update @xmath52 by specifying the missing data to be the factors @xmath127 and the weights @xmath128 .",
    "from we have that @xmath129 thus , the complete data are @xmath130 , and the complete - data likelihood can be factored as @xmath131 the complete - data log - likelihood is @xmath132 \\nonumber\\\\ & & + \\sum_{i=1}^nw_i\\left(\\bx_i-\\bmu^{\\left(k+1\\right)}\\right)'\\bpsi^{-1}\\blambda\\bu_i - \\frac{1}{2}\\text{tr}\\left(\\blambda'\\bpsi^{-1}\\blambda \\sum_{i=1}^n w_i\\bu_i\\bu_i'\\right ) ,    \\label{eq : factor : complete - data log - likelihood}\\end{aligned}\\ ] ] where @xmath133 denotes the trace operator .",
    "the e - step on the second cycle of the @xmath124th iteration requires the calculation of @xmath134.\\ ] ] in addition , we update @xmath47 to @xmath135 where @xmath136 , due to the last two rows of we also need to calculate @xmath137 and @xmath138 . from",
    "we obtain that @xmath139 where @xmath140 .",
    "hence , from and we have that @xmath141 and @xmath142 where @xmath143 .",
    "starting from , the expected complete - data log - likelihood of second cycle is @xmath144 where @xmath145 and @xmath146 includes the terms that do not depend on @xmath52 .    the cm - step on this second cycle of the @xmath124th iteration is implemented by the maximization of @xmath147 over @xmath52 with @xmath56 set equal to @xmath50 .",
    "after some algebra , this yields the updated estimates @xmath148 and @xmath149      in the second cycle of the @xmath44th iteration of the aecm algorithm , we need to compute @xmath150 which , in turn , requires the inversion of the @xmath151 matrix @xmath152",
    ". this inversion can be slow for large values of @xmath1 . to ease it ,",
    "we use the woodbury identity @xcite @xmath153^{-1}\\blambda^{\\left(k\\right)'}\\left(\\bpsi^{\\left(k\\right)}\\right)^{-1 } , \\label{eq : woodbury identity}\\ ] ] which requires the simpler inversions of the diagonal @xmath151 matrix @xmath154 and the @xmath155 matrix @xmath156 .",
    "this leads to a significant speed - up when @xmath157 .",
    "based on the idea of section  [ subsubsec : cn : initialization ] , the aecm algorithm is initialized with the estimates of @xmath11 , @xmath96 and @xmath113 provided by a gaussian factor analysis model , along with the constraints @xmath69 ( with @xmath70 ) and @xmath71 ( with @xmath72 ) ; in the analysis of section  [ subsec : cgfam : real data analysis ] , the ( preliminary ) gaussian factor analysis model is estimated by the ` pgmmem ( ) ` function of the * pgmm * package for @xcite .",
    "the ` pgmmem ( ) ` function implements an aecm algorithm to obtain ml estimates .",
    "finally , the aitken acceleration is considered as convergence criterion ( see section  [ subsubsec : cn : convergence criterion ] for details ) .",
    "we illustrate the contaminated gaussian factor analysis model on the ` state.x77 ` data set available on the * datasets * package for .",
    "this data set is a compilation of data about the @xmath158 us states put together from the 1977 _ statistical abstract of the united states _ ( available for free online at http://www.census.gov/compendia/statab/ ) , with the actual measurements mostly made a few years before .",
    "the @xmath159 variables , included into the data set , are :    l x ` population ` & population estimate as of july 1 , 1975 ; + ` income ` & per capita income in dollars ( 1974 ) ; + ` illiteracy ` & illiteracy ( 1970 , percent of population ) ; + ` life exp ` & life expectancy in years ( 196971 ) ; + ` murder ` & murder and non - negligent manslaughter rate per 100,000 population ( 1976 ) ; + ` hs grad ` & percent high - school graduates ( 1970 ) ; + ` frost ` & mean number of days with minimum temperature below freezing ( 19311960 ) in capital or large city ; + ` area ` & land area in square miles .",
    "+    the scatterplot matrix of the data is displayed in  [ fig : state.obs ] .        to reduce the dimensionality of the data , so to have a lower number @xmath92 of latent factors that explain their variability",
    ", we compare the gaussian , and the contaminated gaussian , factor analysis models . with regards to the former , the ` pgmmem ( ) ` function of the * pgmm * package is run to obtain ml estimates .",
    "two values for @xmath92 are considered , @xmath160 and @xmath161 , and the bayesian information criterion ( bic ; * ? ? ?",
    "* ) @xmath162 where @xmath119 is the overall number of free parameters , is used to select the best one .",
    "for both the models , the best bic values ( @xmath163 for the gaussian factor analysis model , and @xmath164 for the contaminated gaussian factor analysis model ) correspond to @xmath160 .",
    "as the models are nested , the bic also suggests that the selected contaminated gaussian factor analysis model is better .",
    "nevertheless , we prefer to have a @xmath1-value quantifying this choice . following the lines of section  [ subsec :",
    "cn : real data analysis ] , we perform an lr test to compare the best bic models .",
    "the asymptotic @xmath88 distribution has , also in this case , two degrees of freedom and the @xmath1-value is 0.001 ; this result leads us to reject , at the usual significance levels , the null hypothesis that a gaussian factor analysis model works well on these data , in favor of the alternative contaminated gaussian factor analysis model .",
    "the advantage of this choice is that not only can we reduce dimensionality in the presence of bad points , but we can identify them .",
    "when we view the results of our analysis of the ` state.x77 ` data in this way , we see from  [ fig : state.bad ] that there is an anomalous point ( black bullet ) .",
    "note also that the ml estimates of @xmath24 and @xmath26 are @xmath165 and @xmath166 , respectively .    .",
    ", scaledwidth=80.0% ]",
    "to robustify the classical mixture of gaussian distributions to the occurrence of bad points , and also to allow for their automatic detection , @xcite propose the mixture of contaminated gaussian distributions @xmath167 where , for the @xmath168th mixture component , @xmath169 is its mixing proportion , with @xmath170 and @xmath171 , while @xmath172 , @xmath173 , @xmath174 and @xmath175 are defined as in .    in , there are @xmath4 parameters for each @xmath173 , @xmath5 .",
    "this means that as the number of components @xmath176 grows , the total number of parameters can quickly become very large relative to the sample size @xmath2 , leading to overfitting . to model high - dimensional data , and to add parsimony , we consider the contaminated gaussian factor analysis model of section  [ sec : the contaminated gaussian factor analysis model ] in each mixture component ; this leads to the mixture of contaminated gaussian factor analyzers given by but with component covariance matrices given by @xmath177      intuitively , the identifiability of the family of mixtures of contaminated gaussian factor analyzers requires the identifiability of the family of mixtures of contaminated gaussian distributions , as well as the identifiability of the family of factor analysis models . since the identifiability of the class of contaminated gaussian distributions has been established ( see *",
    "* ) , this leaves the question of the identifiability of the family of factor analysis models ; in other words , it requires the identifiability of the class of covariance structures defined in . unfortunately , the topic of identification may itself deserve a separate research project . in this paper , we will not attempt to establish general rules for the identification of the proposed mixture models .",
    "however , based on the considerations leading to , we can say that the overall number @xmath119 of free parameters for the model is @xmath178 + gp + 2g.\\ ] ]      to find ml estimates for the parameters @xmath179 of the mixture of contaminated gaussian factor analyzers model , we consider the aecm algorithm .",
    "we partition @xmath35 as @xmath180 , where @xmath181 , @xmath182 , and @xmath183 .",
    "the @xmath124th iteration of the algorithm consists of three cycles , and there is one e - step and one cm - step for each cycle . concerning the specification of the incomplete data , it is useful to introduce the component - indicator vector @xmath184 , where @xmath185 is one or zero according to whether @xmath30 belongs or does not belong to the @xmath168th component , with @xmath186 and @xmath5 .",
    "for the first cycle of the @xmath124th iteration of the aecm algorithm , we specify the missing data to be the component - indicator vectors @xmath187 and the weights @xmath188 , with @xmath189 .",
    "thus , the complete data are @xmath190 and the complete - data likelihood can be factored as @xmath191^{z_{ig}}.\\ ] ] accordingly , the complete - data log - likelihood can be written as @xmath192 where @xmath193 and @xmath194 with @xmath195 .",
    "the e - step for the first cycle of the @xmath124th iteration requires the calculation of @xmath196.\\ ] ] this e - step can be effected by first taking the expectation of @xmath197 conditional on @xmath198 and @xmath187 , and then finally over the @xmath199 given @xmath30 .",
    "it can be seen from and that in order to do this , we need to calculate @xmath200 and @xmath201 , where @xmath202 and @xmath203 using these results we have that @xmath204 where @xmath205 and @xmath206 with @xmath207 and @xmath146 including the terms that do not depend on @xmath208 .",
    "the cm - step for the first cycle of the @xmath124th iteration requires the maximization of @xmath209 .",
    "the solutions for @xmath210 and @xmath211 exist in closed form and are @xmath212 and @xmath213 at the end of this cycle , we write @xmath214 .      for the second cycle of the @xmath124th iteration of the aecm algorithm , for the updating of @xmath215 , we specify the missing data to be only @xmath187 .",
    "the complete - data likelihood is @xmath216^{z_{ig}},\\ ] ] where @xmath217 .",
    "accordingly , the complete - data log - likelihood is @xmath218 , \\label{eq : mixture : aecm : second cycle : complete - data log - likelihood}\\ ] ] where @xmath146 includes the terms that do not depend on @xmath52 .",
    "the e - step on the second cycle of the @xmath124th iteration requires the calculation of @xmath219.\\ ] ] based on , in order to do this , we need to calculate @xmath220 , where @xmath221 using this result we have that @xmath222 where @xmath223 , \\label{eq : mixture : aecm : second cycle : q2g}\\ ] ] with @xmath224 including the terms that do not depend on @xmath174 and @xmath175 . maximizing with respect to @xmath52 , under the constraints on these parameters ,",
    "is equivalent to independently maximizing each of the @xmath176 expressions over @xmath225 and @xmath226 , under the constraints @xmath227 and @xmath228 , @xmath5 .",
    "this maximization is equivalent to the numerical maximization problem discussed in section  [ subsubsec : cm - step 2 : contaminated gaussian distribution ] for the contaminated gaussian distribution , with the only difference being that each observation @xmath30 contributes to the log - likelihood with a known weight @xmath229 . at the end of this cycle",
    ", we write @xmath230 .      for the third cycle of the @xmath124th iteration of the aecm algorithm , for the updating of @xmath231",
    ", we specify the missing data to be @xmath187 , @xmath188 , and @xmath232 .",
    "thus , the complete data are @xmath233 and , according to the definition of the contaminated gaussian factor analysis model given in section  [ sec : the contaminated gaussian factor analysis model ] , the complete - data likelihood can be factored as @xmath234^{z_{ig}}.\\ ] ] accordingly , the complete - data log - likelihood is @xmath235 where @xmath195    the e - step on the third cycle of the @xmath124th iteration requires the calculation of @xmath236.\\ ] ] in addition to update @xmath185 to @xmath237 and @xmath238 to @xmath239 where @xmath217 , due to the last two rows of we also calculate @xmath240 and @xmath241 where @xmath242 .",
    "hence , starting from , the expected complete - data log - likelihood of the third cycle is @xmath243 with @xmath244 where @xmath245 and @xmath224 includes the terms that do not depend on @xmath246 and @xmath247 , @xmath5 . after some algebra , maximization of @xmath248 over @xmath246 and @xmath247 yields the updated estimates @xmath249 and @xmath250      analogously to the estimation of the contaminated gaussian distribution , the ` optim ( ) ` function",
    "is used to maximize each @xmath251 , @xmath5 , and the woodbury identity , given in , is considered to compute @xmath252 , @xmath5 . moreover , based on section  [ subsec : fcn : computational details ] , the aecm algorithm is initialized with the estimates of @xmath169 , @xmath253 , @xmath246 , and @xmath247 provided by a mixture ( with @xmath176 components ) of gaussian factor analyzers , along with the constraint @xmath254 ( with @xmath70 ) and @xmath255 ( with @xmath72 ) , @xmath5 . in the analysis of section  [ subsec : mcn : real data : blue crab data ] , the ( preliminary ) mixture of gaussian factor analyzers is estimated by the ` pgmmem ( ) ` function of the * pgmm * package for .",
    "the aitken acceleration is considered as convergence criterion ( see section  [ subsubsec : cn : convergence criterion ] for details ) .",
    "the ` f.voles ` data set , detailed in @xcite and available in the * flury * package for @xcite , consists of measurements on female voles from two species , _",
    "m.  californicus _ and _ m.  ochrogaster_. the data refer to @xmath256 observations for which we have a binary variable @xmath257 , denoting the species ( 45 _ m.  ochrogaster _ and 41 _ m.  californicus _ ) , as well as the following @xmath258 continuous variables ( the names of the variables are the same as in the original analysis of this data set by * ? ? ? * ) :    l x ` age ` & age measured in days ; + ` l2.condylo ` & condylo incisive length ; + ` l9.inc.foramen ` & incisive foramen length ; + ` l7.alveolar ` & alveolar length of upper molar tooth row ; + ` b3.zyg ` & zygomatic width ; + ` b4.interorbital ` & interoribital width ; + ` h1.skull ` & skull height .",
    "+    all of the variables related to the skull are measured in units of 0.1 mm .",
    "the scatterplot matrix of these data , with the clustering induced by @xmath259 , is shown in  [ fig : scatter f.voles ] .    for our purposes",
    ", we assume that data are unlabelled with respect to and that our interest is in evaluating , with respect to mixtures of gaussian factor analyzers , clustering and robustness to anomalous points of our proposed model . following the scheme adopted by @xcite on the well - known crabs data ,",
    "36 `` perturbed '' data sets are created by substituting the original value 122 of ` age ` for the 81th point ( highlighted by a black bullet in  [ fig : scatter f.voles ] ) with atypical values ranging from 450 to 800 , with increments of 10 .",
    "the two competing techniques are run for @xmath260 and @xmath261 ; the best model is selected by bic .    as concerns the mixture of gaussian factor analyzers model , apart from the perturbations 530 and 640 , where the selected values are @xmath262 and @xmath161 , in the former case , and @xmath263 and @xmath264 in the latter case , the best values are @xmath263 and @xmath161 .",
    "hence , the value @xmath265 is never selected and the obtained clustering is far from that induced by . when the number of mixture components is fixed to @xmath265 , in a sort of model - based classification analysis",
    ", the best bic model has always a single factor ( @xmath160 ) and the classification on the unperturbated 85 observations is shown in  [ tab : f.voles classification mixture of factor analyzers ] .    .",
    "classification results for the mixture of gaussian factor analyzers model on the perturbed variants of the ` f.voles ` data set .",
    "classification is only evaluated for the unperturbed observations . [ cols=\"<,^,>,>\",options=\"header \" , ]     in contrast with the clustering results obtained for the mixture of gaussian factor analyzers model , the true number of groups is always selected ; furthermore , the selected model is robust to these perturbations , with the number of misallocated observations being null regardless of the particular value perturbed .",
    "interestingly , we can also note how the model is able to detect the introduced bad point ( see the last column of  [ tab : f.voles clustering mixture of contaminated factor analyzers ] ) . finally , by recalling that the original value of ` age ` for the 81th point was 122 , it is also interesting to note that the estimated value of @xmath175 ( in the group containing the outlier ) increases almost linearly as the value of this point further departs from its true value ( cf .",
    "[ fig : eta hat ] ) .",
    "in this paper , the factor analysis model has been extended to the contaminated gaussian distribution .",
    "methodological contributions have been contextualized in the high - dimensional setting and have involved the definition of both the contaminated gaussian factor analysis model ( cf .",
    "section  [ sec : the contaminated gaussian factor analysis model ] ) and the mixture of contaminated gaussian factor analyzers model ( cf . section  [ sec : mixtures of contaminated gaussian factor analyzers ] ) . in one sense",
    ", these models can be respectively viewed as a generalization of the ( gaussian ) factor analysis model , and of the mixture of ( gaussian ) factor analyzers model , that accommodates outlying observations , spurious observations , or noise , which we have collectively referred to as bad points .",
    "although approaches for high - dimensional data such as the @xmath0-factor analysis model and mixtures of @xmath0-factor analyzers can be used for data comprising bad points , they `` assimilate ''",
    "bad points rather than separating them out .",
    "computational contributions have concerned the detailed illustration of aecm algorithms for fitting the above models , as well as the definition of a new version , with respect to @xcite , of the ecm algorithm for maximum likelihood parameter estimation of the contaminated gaussian distribution ; the new version adopts the characterization of this distribution as a member of the gaussian scale mixtures family .    when applied to real data ,",
    "our models have shown their supremacy with respect to their gaussian counterparts .",
    "on the bivariate `` symmetric '' financial data of section  [ subsec : cn : real data analysis ] , we have pointed out that , with respect to the contaminated gaussian distribution , there were about the fifty percent of bad points creating departure from normality , while on the ` state.x77 ` data of section  [ subsec : cgfam : real data analysis ] we have shown how the contaminated gaussian factor analysis model can reduce dimensionality and simultaneously detect bad points .",
    "finally , in the application to the ` f.voles ` data , our mixture of contaminated gaussian factor analyzers model performed better in terms of clustering / classification and gave consistent results regardless of the extent of the perturbation ( cf .  section  [ subsec : mcn : real data : blue crab data ] ) .",
    "future work will focus on the development of an package to facilitate dissemination of our approaches .",
    "in addition , high - dimensional contamination of non - elliptical densities will be explored and once realized , will lead to an even more flexible modelling paradigm .",
    "airoldi , j. and hoffmann , r. ( 1984 ) .",
    "age variation in voles ( _ microtus californicus , m. ochrogaster _ ) and its significance for systematic studies .",
    "occasional papers of the museum of natural history 111 , university of kansas , lawrence , ks .",
    "bhning , d. , dietz , e. , schaub , r. , schlattmann , p. , and lindsay , b. ( 1994 ) .",
    "the distribution of the likelihood ratio for mixtures of densities from the one - parameter exponential family .",
    ", * 46*(2 ) , 373388 ."
  ],
  "abstract_text": [
    "<S> the contaminated gaussian distribution represents a simple robust elliptical generalization of the gaussian distribution ; differently from the often - considered @xmath0-distribution , it also allows for automatic detection of outliers , spurious points , or noise ( collectively referred to as bad points herein ) . starting from this distribution , we propose the contaminated gaussian factor analysis model as a method for robust data reduction and detection of bad points in high - dimensions . </S>",
    "<S> a mixture of contaminated gaussian factor analyzers model follows therefrom , and extends the recently proposed mixtures of contaminated gaussian distributions to high - dimensional data , i.e. , where @xmath1 ( number of dimensions ) is large relative to @xmath2 ( sample size ) . </S>",
    "<S> the number of free parameters is controlled through the dimension of the latent factor space . for each discussed model </S>",
    "<S> , we outline a variant of the classical expectation - maximization algorithm for parameter estimation . </S>",
    "<S> various implementation issues are discussed , and we use real data for illustration .    * keywords : * contaminated gaussian distribution ; em algorithm ; factor analysis models ; mixture models ; model - based clustering . </S>"
  ]
}