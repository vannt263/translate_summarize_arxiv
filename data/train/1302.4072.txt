{
  "article_text": [
    "the approximation of optimal control problems for evolutionary partial differential equations of parabolic and hyperbolic type is a very challenging topic with a strong impact on industrial applications .",
    "although there is a large number of papers dealing with several aspects of control problems from controllability to optimal control , the literature dealing with the numerical approximation of such huge problems is rather limited .",
    "it is worth to note that when dealing with optimal control problems for parabolic equations we can exploit the regularity of the solutions , regularity which is lacking for many hyperbolic equations .",
    "we also recall that the main tools is still given by the pontryagin maximum principle .",
    "this is mainly due to the fact that the discretization of partial differential equations already involves a large number of variables so that the resulting finite dimensional optimization problem easily reaches the limits of what one can really compute .",
    "the forward - backward system which describes pontryagin s optimality condition is certainly below that limit .",
    "however just solving that system one is using necessary conditions for optimality so , in principle , there is no guarantee that these are optimal controls . by this approach for general nonlinear control problems we can obtain just open - loop control .",
    "one notable exception is the linear quadratic regulator problem for which we have a closed - loop solution given by the riccati equation .",
    "this explains why the most popular example for the control of evolutive partial differential equations is the control of the heat equation subject to a quadratic cost functional .",
    "+ in recent years , new tools have been developed to deal with optimal control problems in infinite dimension .",
    "in particular , new techniques emerged to reduce the number of dimensions in the description of the dynamical system or , more in general , of the solution of the problem that one is trying to optimize .",
    "these methods are generally called _ reduced - order methods _ and include for example the pod ( proper orthogonal decomposition ) method and reduced basis approximation ( see @xcite ) . the general idea for all this method is that , when the solution are sufficiently regular , one can represent them via galerkin expansion so that the number of variables involved in this discretization will be strongly reduced . in some particular case , as for the heat equation , even 5 basis functions will suffice to have a rather accurate pod representation of the solution . having this in mind , it is reasonable to start thinking to a different approach based on dynamic programming ( dp ) and hamilton - jacobi - bellman equations ( hjb ) . in this new approach",
    "we will first develop a reduced basis representation of the solution along a reference trajectory and then use this basis to set - up a control problem in the new space of coordinates .",
    "the corresponding hamilton - jacobi equation will just need 3 - 5 variables to represent the state of the system .",
    "moreover , by this method one can obtain optimal control in feedback form looking at the gradient of the value function .",
    "+ however , the solution of hjb equation it is not an easy task from the numerical point of view : the analytical solution of the hjb equation are non regular ( typically , just lipschitz continuous ) .",
    "optimal control problems for odes were solved by dynamic programming , both analytically and numerically ( see @xcite for a general presentation of this theory ) . from the numerical point of view , this approach has been developed for many classical control problems obtaining convergence results and a - priori error estimates ( @xcite , @xcite and the book @xcite ) .",
    "although this approach suffers from the curse - of - dimensionality some algorithms in high - dimension are now available ( @xcite and @xcite ) and the coupling with pod reppresentation techniques will allow to attack by this technique optimal control problems in infinite dimension .",
    "+ to set this paper into perspective we must say that a first tentative in this direction has been made by kunisch and co - authors in a series of papers @xcite for diffusion dominated equations . in particular , in the paper by kunisch , volkwein and xie",
    "@xcite one can see a feedback control approach based on coupling between pod basis approximation and hjb equations for the viscous burgers equation .",
    "our contribution here is twofold .",
    "the first novelty is that we deal with advection - diffusion equations .",
    "the solutions to these equations exhibit low regularity properties with respect to non degenerate diffusion equations so that a rather large number of pod basis functions will be required to obtain a good approximation if we want to compute the pod basis just once .",
    "naturally , this increases the number of variable in the hjb approach and constitutes a is a real bottle - neck . in order to apply the dynamic programming approach to this problem we have developed an adaptive technique which allows to recompute the pod basis on different sub - intervals in order to have always accurate results without an increase of the number of basis functions .",
    "the second contribution of this paper is the way the sub - intervals are determined .",
    "in fact , we do not use a simple uniform subdivision but rather decide to recompute the pod basis when an error indicator ( detailed in section 4 ) is beyond a given threshold . as we will show in the sequel",
    ", this procedure seems to be rather efficient and accurate to deal with these large scale problems .",
    "we briefly describe some important features of the pod approximation , more details as well as precise results can be found in the notes by volkwein @xcite .",
    "let us consider a matrix @xmath0 with rank @xmath1 we will call @xmath2 the @xmath3th column of the matrix @xmath4 we are looking for an orthonormal basis @xmath5 with @xmath6 such that the minimum of the following functional is reached : @xmath7 the solution of this minimization problem is given in the following theorem    [ the_pod ] let @xmath8\\in{\\mathbb{r}}^{m\\times n}$ ] be a given matrix with rank @xmath1 further , let @xmath9 be the singular value decomposition ( svd ) of @xmath10 , where @xmath11\\in{\\mathbb{r}}^{m\\times m}$ ] , @xmath12\\in{\\mathbb{r}}^{n\\times n}$ ] are orthogonal matrices and the matrix @xmath13 is diagonal , @xmath14 .",
    "then , for any @xmath15 the solution to ( [ min_pod ] ) is given by the left singular vectors @xmath16 , i.e , by the first @xmath17 columns of @xmath18 .    we will call the vectors @xmath16 _ pod basis _ of rank @xmath19 this idea is really usefull , in fact we get a solution solving an equation whose dimension is decreased with respect to the initial one .",
    "whenever it s possible to compute a pod basis of rank @xmath20 we get a problem with much smaller dimension of the starting one due to the fact @xmath17 is properly chosen very small . + let us consider the following odes system @xmath21\\\\\\\\ y(0)=y_0 \\end{array}\\right.\\ ] ] where @xmath22 and @xmath23\\times{\\mathbb{r}}^m\\rightarrow{\\mathbb{r}}^m$ ] is continuous and locally lipschitz to ensure uniqueness . + the system ( [ pode ] )",
    "can be also interpreted as a semidiscrete problem , where the matrix @xmath24 represents the discretization in space of an elliptic operator , say laplacian for instance . to compute the pod basis functions ,",
    "first of all we have to construct a time grid @xmath25 and we suppose to know the solution of ( [ pode ] ) at given time @xmath26 , @xmath27 .",
    "we call _ snapshots _ the solution at those fixed times . for the moment we will not deal with the problem of selecting the snapshots sequence which is a difficult problem in itself ,",
    "we refer the interested readers to @xcite ) .",
    "as soon as we get the snapshots sequence , by theorem [ the_pod ] , we will be able to compute our pod basis , namely , @xmath28 .",
    "+ let us suppose we can write the solution in reduced form as @xmath29\\ ] ] substituting this formula into ( [ pode ] ) we obtain the reduced dynamics @xmath30\\\\\\\\ \\sum\\limits_{j=1}^\\ell y_j^\\ell(0)\\psi_j = y_0 .",
    "\\end{array}\\right.\\ ] ] we note that our new problem ( [ p22 ] ) is a problem for the @xmath31 coefficient functions @xmath32 thus , the problem is low dimensional and with compact notation we get : @xmath33 where @xmath34 @xmath35\\rightarrow{\\mathbb{r}}^\\ell\\ ] ] @xmath36\\times{\\mathbb{r}}^\\ell\\rightarrow{\\mathbb{r}}^\\ell,$ ] @xmath37\\;\\ ; y=(y_1,\\ldots y_\\ell)\\in{\\mathbb{r}}^\\ell,\\ ] ] finally obtaining the representation of @xmath38 in @xmath39 @xmath40 in order to apply the pod method to our optimal control problem , the number @xmath17 of pod basis functions is crucial . in particular we would like to keep @xmath17 as low as possible still capturing the behaviour of the original dynamics .",
    "the problem is to define an indicator of the accuracy of our pod approximation .",
    "a good choice for this indicator is the following ratio @xmath41 where the @xmath42 are the the singular value obtained by the svd .    as much @xmath43 is close to one as much",
    "our approximation will be improved .",
    "this is strictly related to the truncation error due to the projection of @xmath2 onto the space generated by the orthonormal basis @xmath44 in fact : @xmath45",
    "we will present this approach for the finite horizon control problem . consider the controlled system @xmath46\\\\ y(t)=x\\in{\\mathbb{r}}^n , \\end{array } \\right.\\ ] ]",
    "we will denote by @xmath47\\rightarrow{\\mathbb{r}}^n$ ] its the solution , by @xmath48 the control @xmath49\\rightarrow{\\mathbb{r}}^m$ ] , @xmath50 , @xmath51 $ ] and by @xmath52\\rightarrow u \\}\\ ] ] the set of admissible controls where @xmath53 is a compact set . whenever we want to emphasize the depence of the solution from the control @xmath48 we will write @xmath54 .",
    "assume that there exists a unique solution trajectory for provided the controls are measurable ( a precise statement can be found in @xcite ) .",
    "for the finite horizon optimal control problem the cost functional will be given by @xmath55 where @xmath56 is the running cost and @xmath57 is the discount factor .",
    "+ the goal is to find a state - feedback control law @xmath58 in terms of the state equation @xmath59 where @xmath60 is the feedback map .",
    "to derive optimality conditions we use the well - known _ dynamic programming principle _ due to bellman ( see @xcite ) .",
    "we first define the value function : @xmath61    for all @xmath62and @xmath63 then : @xmath64    due to ( [ dpp ] ) we can derive the _",
    "hamilton - jacobi - bellman _ equations ( hjb ) : @xmath65 this is nonlinear partial differential equation of the first order which is hard to solve analitically although a general theory of weak solutions is available @xcite .",
    "rather we can solve it numerically by means of a finite differences or semi - lagrangian schemes ( see the book @xcite for a comprehensive analysis of approximation schemes for hamilton - jacobi equations ) . for a semi - lagrangian discretization one",
    "starts by a discrete version of ( hjb ) by discretizing the underlined control problem and then project the semi - discrete scheme on a grid obtaining the fully discrete scheme @xmath66(x_i+\\delta t\\ , f(x_i , t_n , u))]\\\\\\\\ v_i^0=g(x_i ) .",
    "\\end{array}\\right.\\ ] ] with @xmath67 and @xmath68 $ ] is an interpolation operator which is necessary to compute the value of @xmath69 at the point @xmath70 ( in general , this point will not be a node of the grid ) .",
    "the interested reader will find in @xcite a detailed presentation of the scheme and a priori error estimates for its numerical approximation .",
    "note that , we also need to compute the minimum in order to get the value @xmath71 . since @xmath69 is not a smooth function ,",
    "we compute the minimum by means of a minimization method which does not use derivatives ( this can be done by the brent algorithm as in @xcite ) .",
    "as we already told the hjb allows to compute the optimal feedback via the value function , but there are two major difficulties : the solution of an hjb equation are in general non - smooth and the approximation in high dimension is not feasible .",
    "the request to solve an hjb in high dimension comes up naturally whenever we want to control evolutive pdes .",
    "just to give an idea , if we build a grid in @xmath72\\times[0,1]$ ] with a discrete step @xmath73 we have @xmath74 nodes : to solve an hjb in that dimension is simply impossible .",
    "fortunatelly , the pod method allows us to obtain reduced models even for complex dynamics .",
    "let us focus on the following abstract problem : @xmath75 where @xmath76 is a linear and continuous operator .",
    "we assume that a space of admissible controls @xmath77 is given in such a way that for each @xmath78 and @xmath79 there exists a unique solution @xmath80 of ( [ pabs ] ) . @xmath81 and @xmath82 are two hilbert spaces , with @xmath83 we denote the scalar product in @xmath84 @xmath85 is symmetric coercive and bilinear .",
    "then , we introduce the cost functional of the finite horizon problem @xmath86 where @xmath87\\rightarrow { \\mathbb{r}}.$ ] the optimal control problem is @xmath88 with @xmath89 where @xmath90 is the standard sobolev space : @xmath91    the model reduction approach for an optimal control problem ( [ kp ] ) is based on the galerkin approximation of dynamic with some informations on the controlled dynamic ( snapshots ) . to compute a pod solution for ( [ kp ] )",
    "we make the following ansatz @xmath92 where @xmath93 is the pod basis computed as in the previous section .",
    "+ we introduce mass and stiffness matrix : @xmath94 @xmath95 and the control map @xmath96 is defined by : @xmath97 the coefficients of the initial condition @xmath98 are determined by @xmath99 and the solution of the reduced dynamic problem is denoted by @xmath100 then , the galerkin approximation is given by @xmath101 with @xmath78 and @xmath102 solves the following equation : @xmath103 the cost functional is defined : @xmath104 with @xmath105 and @xmath106 linked to ( [ k314 ] ) and the nonlinear map @xmath107 is given by @xmath108 the value function @xmath109 , defined for the initial state @xmath110 @xmath111 and @xmath112 solves ( [ kpl ] ) with the control @xmath48 and initial condition @xmath113    we give an idea how we have computed the intervals for reduced hjb .",
    "hjbs are defined in @xmath114 but we have restricted our numerically domain @xmath115 which is a bounded subset of @xmath116 this is justified since @xmath117 for each @xmath118 and @xmath119 we can chose @xmath120\\times[a_2,b_2]\\times\\ldots[a_\\ell , b_\\ell]$ ] with @xmath121 how should we compute these intervals @xmath122 $ ] ? + ideally the intervals should be chosen so that the dynamics contains all the components of the controlled trajectory .",
    "moreover , they should be encapsulated because we expect that their importance should decrease monotonically with their index and that our interval lengths decrease quickly . +",
    "let us suppose to discretize the space control @xmath123 where @xmath124 is symmetric , to be more precise if @xmath125 + hence , if @xmath126 as a consequence , the coefficients @xmath127.$ ] we consider the trajectories solution @xmath128 such that the control is constant @xmath129 for each @xmath26 , @xmath130 then , we have @xmath131 we write @xmath132 to stress the dependence on the constant control @xmath133 each trajectory @xmath132 has some coefficients @xmath134 for @xmath135 the coefficients @xmath136 will belong to intervals of the type @xmath137 $ ] where we chose for @xmath138 @xmath139 such that : @xmath140 @xmath141 then , we have a method to compute the intervals and we turn our attention to the numerical solution of an optimal control problem for evolutive equation , as we will see in the following section .",
    "we now present an adaptive method to compute pod basis .",
    "since our final goal is to obtain the optimal feedback law by means of hjb equations , we will have a big constraint on the number of variables in the state space for numerical solution of an hjb .",
    "+ we will see that , for a parabolic equation , one can try to solve the problem with only three / four pod basis functions ; they are enough to describe the solution in a rather accurate way .",
    "in fact the singular values decay pretty soon and it s easier to work with a really low - rank dimensional problem .",
    "+ on the contrary , hyperbolic equations do not have this nice property for their singular values and they will require a rather large set of pod basis functions to get accurate results . note that we can not follow the approach suggested in @xcite because we can not add more basis functions when it turns to be necessary due to the constraint already mentioned . then",
    ", it is quite natural to split the problem into subproblems having different pod basis functions .",
    "the crucial point is to decide the splitting in order to have the same number of basis functions in each subdomain with a guaranteed accuracy in the approximation .",
    "+ let us first give an illustrative example for the parabolic case , considering a 1d advection - diffusion equation : @xmath142 with @xmath143 , s\\in[0,t ] , \\varepsilon , c\\in{\\mathbb{r}}.\\\\$ ] we use a finite difference approximation for this equation based on an explicit euler method in time combined with the standard centered approximation of the second order term and with an up - wind correction for the advection term .",
    "the snapshots will be taken from the sequence generated by the finite difference method .",
    "the final time is @xmath144 , moreover @xmath145 , @xmath146 .",
    "the initial condition is @xmath147 when @xmath148 , 0 otherwise .",
    "+ for @xmath149 and @xmath150 with only 3 pod basis functions , the approximation fails ( see figure [ test10 ] ) . note that in this case the advection is dominating the diffusion",
    ", a low number of pod basis functions will not suffice to get an accurate approximation ( figure 1.b ) .",
    "however , the adaptive method which only uses 3 pod basis functions will give accurate results ( figure 1.d ) .",
    "+    ) : ( a ) solved with finite difference ; ( b ) pod - galerkin approximation with 3 pod basi ; ( c ) solved via pod - galerkin approximation with 5 pod basis ; ( d ) adapting 3 pod basis functions.,title=\"fig : \" ] ) : ( a ) solved with finite difference ; ( b ) pod - galerkin approximation with 3 pod basi ; ( c ) solved via pod - galerkin approximation with 5 pod basis ; ( d ) adapting 3 pod basis functions.,title=\"fig : \" ]    ( a)(b )    ) : ( a ) solved with finite difference ; ( b ) pod - galerkin approximation with 3 pod basi ; ( c ) solved via pod - galerkin approximation with 5 pod basis ; ( d ) adapting 3 pod basis functions.,title=\"fig : \" ] ) : ( a ) solved with finite difference ; ( b ) pod - galerkin approximation with 3 pod basi ; ( c ) solved via pod - galerkin approximation with 5 pod basis ; ( d ) adapting 3 pod basis functions.,title=\"fig : \" ]    ( c)(d )    the idea which is behind the adaptive method is the following : we do not consider all the snapshots together in the whole interval @xmath151 $ ] but we group them . instead of taking into account the whole interval @xmath151,$ ] we prefer to split it in sub - intervals @xmath152=\\cup_{k=0}^{k}[t_k , t_{k+1}]\\ ] ] where @xmath153 is a - priori unknown , @xmath154 and @xmath155 for some @xmath156 in this way , choosing properly the length of the @xmath157th interval @xmath158,$ ] we consider only the snapshots falling in that sub - interval , typically there will be at least three snapshots in every sub - interval .",
    "then we have enough informations in every sub - interval and we can apply the standard routines ( explained in section 2 ) to get a `` local '' pod basis . +",
    "now let us explain how to divide our time interval @xmath151 $ ] .",
    "we will choose a parameter to check the accuracy of the pod approximation and define a threshold .",
    "above that threshold we loose in accuracy and we need to compute a new pod basis . a good parameter to check the accuracy is @xmath43 ( see ( [ ind : ratio ] ) ) , as it was suggested by several authors . the method to define the splitting of @xmath151 $ ] and the size of every sub - interval works as follows .",
    "we start computing the svd of the matrix @xmath10 that gives us informations about our dynamics in the whole time interval .",
    "we check the accuracy at every @xmath159 , @xmath160 , and if at @xmath161 the indicator is above the tolerance we set @xmath162 and we divide the interval in two parts , @xmath163 and @xmath164 $ ] .",
    "now we just consider the snapshots related the solution up to the time @xmath165 .",
    "then we iterate this idea until the indicator is below the threshold .",
    "when the first interval is found , we restart the procedure in the interval @xmath166 $ ] and we stop when we reach the final time @xmath167 .",
    "note that the extrema of every interval coincide by construction with one of our discrete times @xmath168 so that the global solution is easily obtained linking all the sub - problems which always have a snapshot as initial condition . a low value for the threshold will also guarantee that we will not have big jumps passing from one sub - interval to the next .",
    "this idea can be applied also when we have a controlled dynamic ( see ( [ eq : test_cont ] ) ) .",
    "first of all we have to decide how to collect the snapshots , since the control @xmath169 is completely unknown .",
    "one can make a guess and use the dynamics and the functional corresponding to that guess , by these informations we can compute the pod basis .",
    "once the pod basis is obtained we will get the optimal feedback law after having solved a reduced hjb equation as we already explained .",
    "let us summarize the method in the following step - by - step presentation .",
    "+     + _ start : _ inizialization + * step 1 : * collect the snapshots in [ 0,t ] + * step 2 : * divide @xmath151 $ ] according to @xmath43 + _ for _ i=0 to n-1 + _ do _ + apply svd to get the pod basis in each sub - interval @xmath170 $ ] + discretize the space of controls + project the dynamics onto the ( reduced ) pod space + select the intervals for the pod reduced variables + solve the corresponding hjb in the reduced space + for the interval @xmath170 $ ] + go back to the original coordinate space + _ end _",
    "in this section we present some numerical tests for the controlled heat equation and for the advection - diffusion equation with a quadratic cost functional .",
    "consider the following advection - diffusion equation : @xmath171 with @xmath143 $ ] , @xmath172 $ ] , @xmath173 and @xmath174note that changing the parameters @xmath175 and @xmath176 we can obtain the heat equation ( @xmath177 ) and the advection equation ( @xmath178 ) .",
    "the functional to be minimized is @xmath179 i.e. we want to stay close to a reference trajectory @xmath180 while minimizing the norm of @xmath48 .",
    "note that we dropped the discount factor setting @xmath181 .",
    "typically in our test problems @xmath180 is obtained by applying a particular control @xmath182 to the dynamics .",
    "the numerical simulations reported in this papers have been made on a server supermicro 8045c-3rb with 2 cpu intel xeon quad - core 2.4 ghz and 32 gb ram under slurm ( https://computing.llnl.gov/linux/slurm/ ) .",
    "[ [ test-1-heat - equation - with - smooth - initial - data ] ] test 1 : heat equation with smooth initial data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we compute the snapshots with a centered / forward euler scheme with space step @xmath183 , and time step @xmath184 , @xmath185 and @xmath144 .",
    "the initial condition is @xmath147 and @xmath186 in figure [ test ] we compare four different approximations concerning the heat equation : ( a ) is the solution for @xmath187 , ( b ) is its approximation via pod ( non adaptive ) , ( c ) is the direct lqr solution computed by matlab without pod and , finally , the approximate optimal solution obtained coupling pod and hjb .",
    "the approximate value function is computed for @xmath188 @xmath189 whereas the optimal trajectory as been obtained with @xmath190 test 1 , and even test 2 , have been solved in about half an hour of cpu time .",
    "+ note that in this example the approximate solution is rather accurate because the regularity of the solution is high due to the diffusion term .",
    "since in the limit the solution tends to the average value the choice of the snapshots will not affect too much the solution , i.e. even with a rough choice of the snapshots will give us a good approximation . the difference between figure 2c and figure 2d",
    "is due to the fact that the control space is continuous for 2c and discrete for 2d .",
    "( a)(b )        ( c)(d )    [ [ test-2-heat - equation - with - no - smooth - intial - data ] ] test 2 : heat equation with no - smooth intial data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this section we change the initial condition with a function which is only lipschitz continuos : @xmath191 according to test 1 , we consider the same parameters .",
    "( see figure [ testass ] ) .    ; ( b ) exact solution for @xmath192 pod ( 3 basis functions ) ; ( c ) approximate optimal solution for lqr - matlab ; ( d ) approximate solution pod ( 3 basis functions)+ hjb.,title=\"fig : \" ] ; ( b ) exact solution for @xmath192 pod ( 3 basis functions ) ; ( c ) approximate optimal solution for lqr - matlab ; ( d ) approximate solution pod ( 3 basis functions)+ hjb.,title=\"fig : \" ]    ( a)(b )    ; ( b ) exact solution for @xmath192 pod ( 3 basis functions ) ; ( c ) approximate optimal solution for lqr - matlab ; ( d ) approximate solution pod ( 3 basis functions)+ hjb.,title=\"fig : \" ] ; ( b ) exact solution for @xmath192 pod ( 3 basis functions ) ; ( c ) approximate optimal solution for lqr - matlab ; ( d ) approximate solution pod ( 3 basis functions)+ hjb.,title=\"fig : \" ]    ( c)(d )    riccati s equation has been solved by a matlab lqr routine .",
    "thus , we have used the solution given by this routine as the correct solution in order to compare the errors in @xmath193 and @xmath194 norm between the reduced riccati s equation and our approach based on the reduced hjb equation .",
    "since we do not have any information , the snapshots are computed for @xmath195 this is only a guess , but in the parabolic case fits well due to the diffusion term .",
    "+    .test 2 : @xmath193 and @xmath194 errors at time @xmath167 for the optimal approximate solution . [ cols=\"^,^,^\",options=\"header \" , ]     as in test 1 , the choice of the snapshots does not effect strongly the approximation due to the asymptotic behavior of the solution .",
    "the presence of a lipschitz continuous initial condition has almost no influence on the global error ( see table 1 ) .",
    "[ [ test-3-advection - diffusion - equation ] ] test 3 : advection - diffusion equation + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the advection - diffusion equation needs a different method .",
    "we can not use the same @xmath180 we had in the parabolic case , mainly because in riccati s equation the control is free and is not bounded , on the contrary when we solve an hjb we have to discretize the space of controls .",
    "we modified the problem in order to deal with bang - bang controls .",
    "we get @xmath180 in ( [ lqr_oss ] ) just plugging in the control @xmath196 .",
    "we have considered the control space corresponding only to three values in @xmath197 $ ] , then @xmath198 we first have tried to get a controlled solution , without any adaptive method and , as expected , we obtained a bad approximation ( see figure [ dt_noad ] ) .     on the left , approximate solution on the right with pod ( 4 basis functions),title=\"fig : \" ]   on the left , approximate solution on the right with pod ( 4 basis functions),title=\"fig : \" ]",
    "from figure [ dt_noad ] it s clear that pod with four basis functions is not able to catch the behavior of the dynamics , so we have applied our adaptive method .",
    "+     ( left ) , approximate optimal solution ( right).,title=\"fig : \" ] ( left ) , approximate optimal solution ( right).,title=\"fig : \" ]    we have consider : @xmath199 , @xmath145 , @xmath146 , @xmath200 according to our algorithm , the time interval @xmath201 $ ] was divided into @xmath202\\cup[0.744 , 1.496]\\cup[1.496,3].$ ] as we can see our last interval is bigger than the others , this is due to the diffusion term ( see figure [ dt_ad ] ) .",
    "the @xmath203error is 0.0761 , and the computation of the optimal solution via hjb has required about six hours of cpu time . in figure 4",
    "we compare the exact solution with the numerical solution based on a pod representation .",
    "note that , in this case , the choice of only 4 basis functions for the whole interval @xmath151 $ ] gives a very poor result due to the presence of the advection term .",
    "looking at figure 5 one can see the improvement of our adaptive technique which takes always 4 basis functions in each sub - interval .",
    "+ in order to check the quality of our approximation we have computed the numerical residual , defined as : @xmath204 the residual for the solution of the control problem computed without our adaptive technique is 1.1 , whereas the residual for the adaptive method is @xmath205 . as expected from the pictures , there is a big difference between these two value .",
    "[ [ test-4-advection - diffusion - equation ] ] test 4 : advection - diffusion equation + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this test we take a different @xmath180 , namely the solution of ( [ eq : test_cont ] ) corresponding to the control @xmath206 we want to emphasize we can obtain nice results when the space of controls has few element .",
    "the parameters were the same used in test 3 .",
    "the @xmath203error is 0.09 , and the time was the same we had in test 3 . in figure [ dt_ad4 ]",
    "we can see our approximation .",
    "( left ) , approximate optimal solution ( right).,title=\"fig : \" ] ( left ) , approximate optimal solution ( right).,title=\"fig : \" ]    in figure 6 one can see that the adaptive technique can also deal with discontinuous controls .",
    "+    in this test , the residual for the solution of the control problem without our adaptive technique is 2 , whereas the residual for the adaptive method is @xmath207 .",
    "again , the residual shows the higher accuracy of the adaptive routine .",
    "as we have discussed , a reasonable coupling between pod and hjb equation can produce feedback controls for infinite dimensional problem . for advection dominated equations that simple idea has to be implemented in a clever way to be successful .",
    "it particular , the application of an adaptive technique is crucial to obtain accurate approximations with a low number of pod basis functions .",
    "this is still an essential requirement when dealing with the dynamic programming approach , which suffers from the curse - of - dimensionality although recent developments in the methods used for hjb equations will allow to increase this bound in the next future ( for example by applying patchy techniques ) .",
    "another important point is the discretization of the control space . in our examples ,",
    "the number of optimal control is rather limited and this will be enough for problems which have a bang - bang structure for optimal controls . in general , we will need also an approximation of the control space via reduced basis methods .",
    "this point as well as a more detailed analysis of the procedure outlined in this paper will be addressed in our future work .    1 m. bardi , i. capuzzo dolcetta .",
    "_ optimal control and viscosity solutions of hamilton - jacobi - bellman equations_. birkhauser , basel , 1997 .",
    "s. cacace , e. cristiani , m. falcone , a. picarelli . _ a patchy dynamic programming scheme for a class of hamilton - jacobi - bellman equations _ , preprint , 2011 to appear on siam j. sci",
    "e. carlini , m. falcone , r. ferretti .",
    "_ an efficient algorithm for hamilton - jacobi equations in high dimension_. computing and visualization in science , vol.7 , no.1 ( 2004 ) pp .",
    "m. falcone .",
    "_ numerical solution of dynamic programming equations _",
    ", appendix of the book m. bardi , i. capuzzo dolcetta , optimal control and viscosity solutions of hamilton - jacobi - bellman equations , birkhser , boston , 1997 , 471 - 504 . m. falcone , r. ferretti . semi - lagrangian approximation schemes for linear and hamilton - jacobi equations , siam , to appear m. falcone , t. giorgi . _ an approximation scheme for evolutive hamilton - jacobi equations _",
    "mceneaney , g. yin and q. zhang ( eds . ) , `` stochastic analysis , control , optimization and applications : a volume in honor of w.h .",
    "fleming '' , birkhuser , 1999 , 289 - 303 .",
    "k. kunisch , s. volkwein .",
    "_ control of burgers equation by a reduced order approach using proper orthogonal decomposition_. journal of optimization theory and applications , 102 ( 1999 ) , 345- 371 .",
    "k. kunisch , s. volkwein .",
    "_ galerkin proper orthogonal decomposition methods for parabolic problems _ numer .",
    "( 2001 ) , 117 - 148 .",
    "k. kunisch , s. volkwein .",
    "_ optimal snapshot location for computing pod basis functions _ esaim : m2an 44 ( 2010 ) , 509 - 529 .",
    "k. kunisch , s. volkwein , l. xie .",
    "_ hjb - pod based feedback design for the optimal control of evolution problems_. siam j. on applied dynamical systems , 4 ( 2004 ) , 701 - 722 .",
    "k. kunisch , l. xie . _ pod - based feedback control of burgers equation by solving the evolutionary hjb equation _ , computers and mathematics with applications .",
    "* 49 * ( 2005 ) , 1113 - 1126 .",
    "a. t. patera , g. rozza .",
    "_ reduced basis approximation and a posteriori error estimation for paramtrized partial differential equations . _ mit pappalardo graduate monographs in mechanical engineering , 2006",
    ". m. l. rapun j.m .",
    "_ reduced order models based on local pod plus galerkin projection .",
    "_ j. comput .",
    "phys . , 229 ( 2010 ) , pp 3046 - 3063 . s. volkwein , _ model reduction using proper orthogonal decomposition _ , 2011 www.math.uni-konstanz.de/numerik/personen/volkwein/index.php f. trltzsch .",
    "_ optimal control of partial differential equations : theory , methods and applications _ , ams 2010"
  ],
  "abstract_text": [
    "<S> we present an algorithm for the approximation of a finite horizon optimal control problem for advection - diffusion equations . </S>",
    "<S> the method is based on the coupling between an adaptive pod representation of the solution and a dynamic programming approximation scheme for the corresponding evolutive hamilton - jacobi equation . </S>",
    "<S> we discuss several features regarding the adaptivity of the method , the role of error estimate indicators to choose a time subdivision of the problem and the computation of the basis functions . </S>",
    "<S> some test problems are presented to illustrate the method . </S>"
  ]
}