{
  "article_text": [
    "divergence measures are widely used in information theory , machine learning , statistics , and other theoretical and applied branches of mathematics ( see , e.g. , @xcite , @xcite , @xcite , @xcite ) .",
    "the class of @xmath0-divergences , introduced independently in @xcite and @xcite , forms an important class of divergence measures .",
    "their properties , including relations to statistical tests and estimators , were studied , e.g. , in @xcite and @xcite .    in @xcite",
    ", gilardoni studied the problem of minimizing an arbitrary _ symmetric _",
    "@xmath0-divergence for a given total variation distance , providing a closed - form solution of this optimization problem . in a follow - up paper by the same author @xcite , pinsker s and vajda s type inequalities were studied for symmetric @xmath0-divergences , and the issue of obtaining lower bounds on @xmath0-divergences for a fixed total variation distance was further studied .",
    "one of the main results in @xcite was a derivation of a simple closed - form lower bound on the relative entropy in terms of the total variation distance , which suggests an improvement over pinsker s and vajda s inequalities , and a derivation of a simple and reasonably tight closed - form upper bound on the infimum of the relative entropy in terms of the total variation distance .",
    "an exact characterization of the minimum of the relative entropy subject to a fixed total variation distance has been derived in @xcite and @xcite .",
    "more generally , sharp inequalities for @xmath0-divergences were recently studied in @xcite as a problem of maximizing or minimizing an arbitrary @xmath0-divergence between two probability measures subject to a finite number of inequality constraints on other @xmath0-divergences .",
    "the main result stated in @xcite is that such infinite - dimensional optimization problems are equivalent to optimization problems over finite - dimensional spaces where the latter are numerically solvable .",
    "following previous work , _ tight _ bounds on symmetric @xmath0-divergences and related distances are derived in this paper .",
    "an application of these bounds for lossless source coding is provided , refining and improving a certain bound by csiszr from 1967 @xcite .",
    "the paper is organized as follows : preliminary material is introduced in section  [ section : preliminaries ] , tight bounds for several symmetric divergence measures , which are either symmetric @xmath0-divergences or related symmetric distances , are derived in section  [ section : derivation of tight bounds on symmetric distance measures ] ; these bounds are expressed in terms of the total variation distance , and their tightness is demonstrated .",
    "one of these bounds is used in section  [ section : bound for lossless source coding ] for the derivation of an improved and refined bound for lossless source coding .",
    "we introduce , in the following , some preliminaries and notation that are essential to this paper .",
    "let @xmath1 and @xmath2 be two probability distributions with a common @xmath3-algebra @xmath4 .",
    "the _ total variation distance _ between @xmath1 and @xmath2 is defined by @xmath5 [ definition : total variation distance ]    if @xmath1 and @xmath2 are defined on a countable set , is simplified to @xmath6 so it is equal to one - half the @xmath7-distance between @xmath1 and @xmath2",
    ".    let @xmath8 be a convex function with @xmath9 , and let @xmath1 and @xmath2 be two probability distributions .",
    "the _ @xmath0-divergence _ from @xmath1 to @xmath2 is defined by @xmath10 with the convention that @xmath11 [ definition : f - divergence ]    an @xmath0-divergence is _ symmetric _ if @xmath12 for every @xmath1 and @xmath2 .",
    "symmetric @xmath0-divergences include ( among others ) the squared hellinger distance where @xmath13 and the total variation distance in where @xmath14    an @xmath0-divergence is symmetric if and only if the function @xmath0 satisfies the equality ( see @xcite ) @xmath15 for some constant @xmath16 . if @xmath0 is differentiable at @xmath17 then a differentiation of both sides of equality at @xmath17 gives that @xmath18 .",
    "note that the relative entropy ( a.k.a .",
    "the kullback - leibler divergence ) @xmath19 is an @xmath0-divergence with @xmath20 ; its dual , @xmath21 , is an f - divergence with @xmath22 ; clearly , it is an asymmetric @xmath0-divergence since @xmath23 .",
    "the following result , which was derived by gilardoni ( see @xcite ) , refers to the infimum of a symmetric @xmath0-divergence for a fixed value of the total variation distance :    let @xmath8 be a convex function with @xmath9 , and assume that @xmath0 is twice differentiable .",
    "let @xmath24 \\label{eq : definition of infimum of f - divergence under a total variation distance}\\ ] ] be the infimum of the @xmath0-divergence for a given total variation distance .",
    "if @xmath25 is a symmetric @xmath0-divergence , and @xmath0 is differentiable at @xmath17 , then @xmath26 .",
    "\\label{eq : infimum of a symmetric f - divergence for a given total variation distance}\\ ] ] [ theorem : lower bound on symmetric f - divergence in terms of the total variation distance ]    consider an arbitrary symmetric @xmath0-divergence .",
    "note that it follows from and that the infimum in , is attained by the pair of 2-element probability distributions where @xmath27 ( or by switching @xmath1 and @xmath2 since @xmath25 is assumed to be a symmetric divergence ) .    throughout this paper ,",
    "the logarithms are on base  @xmath28 unless the base of the logarithm is stated explicitly .",
    "the following section introduces tight bounds for several symmetric divergence measures for a fixed value of the total variation distance .",
    "the statements are introduced in section  [ subsection : tight bounds on the bhattacharyya distance][subsection : bounds on jeffreys divergence ] , their proof are provided in , followed by discussions on the statements in section  [ subsection : discussions on the tight bounds ] .",
    "let @xmath1 and @xmath2 be two probability distributions that are defined on the same set .",
    "bhattacharyya coefficient _",
    "@xcite between @xmath1 and @xmath2 is given by @xmath29 the _ bhttacharyya distance _ is defined as minus the logarithm of the bhattacharyya coefficient , so that it is zero if and only if @xmath30 , and it is non - negative in general ( since @xmath31 , and @xmath32 if and only if @xmath30 ) .",
    "[ definition : probability metrics ]    let @xmath1 and @xmath2 be two probability distributions . then",
    ", for a fixed value @xmath33 $ ] of the total variation distance ( i.e. , if @xmath34 ) , the respective bhattacharyya coefficient satisfies the inequality @xmath35 both upper and lower bounds are tight : the upper bound is attained by the pair of 2-element probability distributions @xmath36 and the lower bound is attained by the pair of 3-element probability distributions @xmath37 [ proposition : tight bounds on the bhattacharyya distance for a given total variation distance ]      the _ chernoff information _ between two probability distributions @xmath1 and @xmath2 , defined on the same set , is given by @xmath38 } \\ ; \\log \\left ( \\sum_{x } p(x)^{\\lambda } \\ , q(x)^{1-\\lambda } \\right ) .",
    "\\label{eq : chernoff information}\\end{aligned}\\ ] ] [ definition : chernoff information and relative entropy ]    note that @xmath39 } \\left\\ { -\\log \\left ( \\sum_{x } p(x)^{\\lambda } \\ , q(x)^{1-\\lambda } \\right ) \\right\\ } \\nonumber \\\\ & = \\max_{\\lambda \\in ( 0,1 ) } \\ , \\bigl\\{(1-\\lambda ) \\ , d_{\\lambda}(p , q ) \\bigr\\ }",
    "\\label{eq : connection between the chernoff information and renyi divergence}\\end{aligned}\\ ] ] where @xmath40 designates the rnyi divergence of order @xmath41 @xcite .",
    "the endpoints of the interval @xmath42 $ ] are excluded in the second line of since the chernoff information is non - negative , and the logarithmic function in the first line of is equal to zero at both endpoints .",
    "let @xmath43 \\label{eq : minimum of the chernoff information for a fixed total variation distance}\\end{aligned}\\ ] ] be the minimum of the chernoff information for a fixed value @xmath33 $ ] of the total variation distance .",
    "this minimum indeed exists , and it is equal to @xmath44                   + \\infty                                  & \\mbox{if $ \\varepsilon = 1$. } \\end{array } \\right .",
    "\\label{eq : characterization of the minimum of the chernoff information for a fixed total variation distance}\\ ] ] for @xmath45 , it is achieved by the pair of 2-element probability distributions @xmath46 , and @xmath47 .",
    "[ proposition : minimum of the chernoff information for a fixed total variation distance ]    for any pair of probability distributions @xmath1 and @xmath2 , @xmath48 and this lower bound is tight for a given value of the total variation distance .",
    "[ corollary : tight lower bound on the chernoff information for a given total variation distance ]    from corollary  [ corollary : tight lower bound on the chernoff information for a given total variation distance ] , a lower bound on the total variation distance implies a lower bound on the chernoff information ; consequently , it provides an upper bound on the best achievable bayesian probability of error for binary hypothesis testing ( see , e.g. , ( * ? ? ? * theorem  11.9.1 ) ) .",
    "this approach has been recently used in @xcite to obtain a lower bound on the chernoff information for studying a communication problem that is related to channel - code detection via the likelihood ratio test ( the authors in @xcite refer to our previously un - published manuscript @xcite , where this corollary first appeared ) .      [",
    "subsection : bounds on the capacitory discrimination ] the capacitory discrimination ( a.k.a . the jensen - shannon divergence ) is defined as follows :    let @xmath1 and @xmath2 be two probability distributions . the capacitory discrimination between @xmath1 and @xmath2",
    "is given by @xmath49 \\label{eq : capacitory discrimination } \\end{split}\\ ] ] where @xmath50 .",
    "[ definition : capacitory discrimination ]    this divergence measure was studied in @xcite , @xcite , @xcite , @xcite , @xcite and @xcite . due to the parallelogram identity for relative entropy ( see , e.g. , ( * ? ? ?",
    "* problem  3.20 ) ) , it follows that @xmath51 where the minimization is taken w.r.t .",
    "all probability distributions @xmath52 .    for a given value",
    "@xmath33 $ ] of the total variation distance , the minimum of the capacitory discrimination is equal to @xmath53 and it is achieved by the 2-element probability distributions @xmath46 , and @xmath47 . in , @xmath54 , \\label{eq : d function}\\ ] ] with the convention that @xmath55 , denotes the divergence ( relative entropy ) between the two bernoulli distributions with parameters @xmath56 and @xmath57 .",
    "[ proposition : minimum of the capacitory discrimination for a fixed total variation distance ]    the lower bound on the capacitory discrimination was obtained independently by brit and harremos ( see ( * ? ? ?",
    "* eq .  ( 18 ) ) for @xmath58 ) whose derivation was based on a different approach .",
    "the following result provides a measure of the concavity of the entropy function :    for arbitrary probability distributions @xmath1 and @xmath2 , the following inequality holds : @xmath59 and this lower bound is tight for a given value of the total variation distance . [ corollary : tight lower bound on the capacitory discrimination in terms of the total variation distance ]      let @xmath1 and @xmath2 be two probability distributions .",
    "jeffreys divergence @xcite is a symmetrized version of the relative entropy , which is defined as @xmath60    this forms a symmetric @xmath0-divergence where @xmath61 with @xmath62 which is a convex function on @xmath63 , and @xmath9 .",
    "@xmath64    and the two respective suprema are equal to @xmath65 .",
    "the minimum of jeffreys divergence in , for a fixed value @xmath66 of the total variation distance , is achieved by the pair of 2-element probability distributions @xmath46 and @xmath47 .",
    "[ proposition : tight lower bound on jeffreys divergence in terms of the total variation distance ]        from , and the cauchy - schwartz inequality , we have @xmath67 & = \\frac{1}{2 } \\ , \\sum_{x } \\left| \\sqrt{p(x ) } - \\sqrt{q(x ) } \\ , \\right| \\left ( \\sqrt{p(x ) } + \\sqrt{q(x ) } \\ , \\right ) \\\\[0.1 cm ] & \\leq \\frac{1}{2 } \\",
    ", \\left ( \\sum_{x } \\left ( \\sqrt{p(x ) } - \\sqrt{q(x ) } \\ , \\right)^2 \\right)^{\\frac{1}{2 } } \\left ( \\sum_{x } \\left ( \\sqrt{p(x ) } + \\sqrt{q(x ) } \\ , \\right)^2 \\right)^{\\frac{1}{2 } } \\\\[0.1 cm ] & = \\frac{1}{2 } \\bigl(2 - 2z(p , q)\\bigr)^{\\frac{1}{2 } } \\bigl(2 + 2z(p , q)\\bigr)^{\\frac{1}{2 } } \\\\[0.1 cm ] & = \\left(1 - z^2(p , q)\\right)^{\\frac{1}{2}}\\end{aligned}\\ ] ] which implies that @xmath68 .",
    "this gives the upper bound on the bhattacharyya coefficient in . for proving the lower",
    "bound , note that @xmath69 & = 1 - \\frac{1}{2 } \\sum_{x } |p(x)-q(x)| \\left(\\frac{|\\sqrt{p(x ) } - \\sqrt{q(x)}| } { \\sqrt{p(x ) } + \\sqrt{q(x ) } } \\right ) \\\\[0.1 cm ] & \\geq 1 - \\frac{1}{2 } \\sum_{x } |p(x)-q(x)| =   1-d_{\\text{tv}}(p , q).\\end{aligned}\\ ] ] the tightness of the bounds on the bhattacharyya coefficient in terms of the total variation distance is proved in the following . for a fixed value of the total variation distance",
    "@xmath33 $ ] , let @xmath1 and @xmath2 be the pair of 2-element probability distributions @xmath46 and @xmath47 .",
    "this gives @xmath70 so the upper bound is tight .",
    "furthermore , for the pair of 3-element probability distributions @xmath71 and @xmath72 , we have @xmath73 so also the lower bound is tight .",
    "the lower bound on the bhattacharyya coefficient in dates back to kraft ( * ? ? ?",
    "* lemma  1 ) , though its proof was simplified here .",
    "both the bhattacharyya distance and coefficient are functions of the hellinger distance , so a tight upper bound on the bhattacharyya coefficient in terms of the total variation distance can be also obtained from a tight upper bound on the hellinger distance ( see @xcite ) .",
    "@xmath74    where inequality  ( a ) follows by selecting the possibly sub - optimal value of @xmath75 in , equality  ( b ) holds by definition ( see ) , and inequality  ( c ) follows from the upper bound on the bhattacharyya distance in . by the definition in",
    ", it follows that @xmath76 in order to show that provides a tight lower bound for a fixed value of the total variation distance @xmath77 , note that for the pair of 2-element probability distributions @xmath1 and @xmath2 in proposition  [ proposition : minimum of the chernoff information for a fixed total variation distance ] , the chernoff information in is given by @xmath78 } \\log \\left ( \\frac{1-\\varepsilon}{2 } \\left(\\frac{1+\\varepsilon}{1-\\varepsilon}\\right)^{\\lambda } + \\frac{1+\\varepsilon}{2 } \\left(\\frac{1-\\varepsilon}{1+\\varepsilon}\\right)^{\\lambda } \\right ) .",
    "\\label{eq : chernoff information for a pair of 2-element probability distributions}\\end{aligned}\\ ] ] a minimization of the function in gives that @xmath75 , and @xmath79 which implies that the lower bound in is tight .      in @xcite ,",
    "the capacitory discrimination is expressed as an @xmath0-divergence where @xmath80 is a convex function with @xmath9 .",
    "the combination of and implies that @xmath81 & = ( 1+\\varepsilon ) \\ ,",
    "\\log(1+\\varepsilon ) + ( 1-\\varepsilon ) \\ ,",
    "\\log(1-\\varepsilon ) \\nonumber \\\\[0.1 cm ] & = 2 \\left [ \\log 2 - h\\left(\\frac{1-\\varepsilon}{2 } \\right ) \\right ] = 2 \\ , d\\left(\\frac{1-\\varepsilon}{2 } \\ , \\big|\\big| \\ , \\frac{1}{2 } \\right ) .",
    "\\label{eq : infimum of the capacitory discrimination}\\end{aligned}\\ ] ] the last equality holds since @xmath82 for @xmath83 $ ] where @xmath84 denotes the binary entropy function .",
    "note that the infimum in is a minimum since for the pair of 2-element probability distributions @xmath46 and @xmath47 , we have @xmath85 so , @xmath86 .",
    "jeffreys divergence is a symmetric @xmath0-divergence where the convex function @xmath0 in satisfies the equality @xmath87 for every @xmath88 with @xmath9 . from theorem  [ theorem : lower bound on symmetric f",
    "- divergence in terms of the total variation distance ] , it follows that @xmath89 this infimum is achieved by the pair of 2-element probability distributions @xmath90 and @xmath91 , so it is a minimum .",
    "this proves .",
    ".   follows from and the fact that , given the value of the relative entropy @xmath92 , its dual @xmath93 can be made arbitrarily small .",
    "the two respective suprema are equal to infinity because given the value of the total variation distance or the relative entropy , the dual of the relative entropy can be made arbitrarily large .",
    "let @xmath94 the exact parametric equation of the curve @xmath95 was introduced in different forms in ( * ? ? ?",
    "* eq .  ( 3 ) ) , @xcite , and ( * ? ? ?",
    "* eq .  ( 59 ) ) .",
    "for @xmath45 , this infimum is attained by a pair of 2-element probability distributions ( see @xcite ) . due to the factor of one - half in the total variation distance of , it follows that @xmath96 } \\left\\ { \\left(\\frac{\\varepsilon+1-\\beta}{2}\\right ) \\ , \\log \\left(\\frac{\\beta-1-\\varepsilon}{\\beta-1+\\varepsilon}\\right ) + \\left(\\frac{\\beta+1-\\varepsilon}{2}\\right ) \\ , \\log\\left(\\frac{\\beta+1-\\varepsilon}{\\beta+1+\\varepsilon}\\right ) \\right\\ } \\label{eq : reid and willimason 's bound for the relative entropy}\\end{aligned}\\ ] ] where",
    ", it can be verified that the numerical minimization w.r.t .",
    "@xmath97 in can be restricted to the interval @xmath98 $ ] .    since @xmath99 ( see ( * ? ? ?",
    "* section  11.9 ) ) , it follows from and that @xmath100 where the right and left - hand sides of correspond to the minima of the relative entropy and chernoff information , respectively , for a fixed value of the total variation distance @xmath77 .",
    "figure  [ figure : plot of c versus l ] plots these minima as a function of the total variation distance . for small values of @xmath66 ,",
    "@xmath101 and @xmath102 , respectively , are approximately equal to @xmath103 and @xmath104 ( note that pinsker s inequality is tight for @xmath105 ) , so @xmath106    the lower bound on the capacitory discrimination in , expressed in terms of the total variation distance , forms a closed - form expression of the bound by topse in ( * ? ? ?",
    "* theorem  5 ) .",
    "the bound in @xcite is @xmath107 the equivalence of and follows from the power series expansion of the binary entropy function @xmath108\\ ] ] which yields that @xmath109 \\\\ & = 2 d\\left(\\frac{1-d_{\\text{tv}}(p , q)}{2 } \\",
    ", \\big|\\big| \\ , \\frac{1}{2}\\right)\\end{aligned}\\ ] ] where @xmath110 is defined in .",
    "note , however , that the proof here is more simple than the proof of ( * ? ? ?",
    "* theorem  5 ) ( which relies on properties of the triangular discrimination in @xcite and previous theorems of this paper ) , and it also leads directly to a closed - form expression of this bound . consequently , one concludes that the lower bound in ( * ? ? ?",
    "* theorem  5 ) is a special case of theorem  [ theorem : lower bound on symmetric f - divergence in terms of the total variation distance ] ( see @xcite and ( * ? ? ?",
    "* corollary  5.4 ) ) , which provides a lower bound on a symmetric @xmath0-divergence in terms of the total variation distance .",
    "we illustrate in the following a use of proposition  [ proposition : tight lower bound on jeffreys divergence in terms of the total variation distance ] for the derivation of an improved and refined bound for lossless source coding .",
    "this tightens , and also refines under a certain condition , a bound by csiszr @xcite .",
    "consider a memoryless and stationary source with alphabet @xmath111 that emits symbols according to a probability distribution @xmath1 , and assume that a uniquely decodable ( ud ) code with an alphabet of size @xmath112 is used .",
    "it is well known that such a ud code achieves the entropy of the source if and only if the length @xmath113 of the codeword that is assigned to each symbol @xmath114 satisfies the equality @xmath115 this corresponds to a dyadic source where , for every @xmath114 , we have @xmath116 with a natural number @xmath117 ; in this case , @xmath118 for every symbol @xmath114 .",
    "let @xmath119 $ ] designate the average length of the codewords , and @xmath120 be the entropy of the source ( to the base @xmath112 ) .",
    "furthermore , let @xmath121 according to the kraft - mcmillian inequality ( see ( * ? ? ?",
    "* theorem  5.5.1 ) ) , the inequality @xmath122 holds in general for ud codes , and the equality @xmath123 holds if the code achieves the entropy of the source ( i.e. , @xmath124 ) .",
    "define a probability distribution @xmath125 by @xmath126 and let @xmath127 designate the average redundancy of the code .",
    "note that for a ud code that achieves the entropy of the source , its probability distribution @xmath1 is equal to @xmath125 ( since @xmath123 , and @xmath128 for every @xmath114 ) .    in @xcite ,",
    "a generalization for ud source codes has been studied by a derivation of an upper bound on the @xmath7 norm between the two probability distributions @xmath1 and @xmath125 as a function of the average redundancy @xmath129 of the code . to this end",
    ", straightforward calculation shows that the relative entropy from @xmath1 to @xmath125 is given by @xmath130 the interest in @xcite is in getting an upper bound that only depends on the average redundancy @xmath129 of the code , but is independent of the distribution of the lengths of the codewords .",
    "hence , since the kraft - mcmillian inequality states that @xmath122 for general ud codes , it is concluded in @xcite that @xmath131 consequently , it follows from pinsker s inequality that @xmath132 since also , from the triangle inequality , the sum on the left - hand side of can not exceed  2 .",
    "this inequality is indeed consistent with the fact that the probability distributions @xmath1 and @xmath125 coincide when @xmath133 ( i.e. , for a ud code that achieves the entropy of the source ) .    at this point",
    "we deviate from the analysis in @xcite .",
    "one possible improvement of the bound in follows by replacing pinsker s inequality with the result in @xcite , i.e. , by taking into account the exact parametrization of the infimum of the relative entropy for a given total variation distance .",
    "this gives the following tightened bound : @xmath134 where @xmath135 is the inverse function of @xmath136 in ( it is calculated numerically ) .    in the following , the utility of proposition  [ proposition : tight lower bound on jeffreys divergence in terms of the total variation distance ] is shown by refining the latter bound in",
    "let @xmath137    calculation of the dual divergence gives @xmath138 \\nonumber \\\\ & = -\\log(c_{d , l } ) -",
    "\\frac{\\log d}{c_{d , l } } \\sum_{u \\in \\mathcal{u } } \\delta(u ) \\ ,",
    "d^{-l(u ) } \\nonumber \\\\ & = -\\log \\bigl(c_{d , l}\\bigr ) -",
    "\\frac{\\log d}{c_{d , l } } \\ , \\sum_{u \\in \\mathcal{u } } p(u ) \\ , \\delta(u ) \\ ,",
    "d^{-\\delta(u ) } \\nonumber \\\\ & = -\\log \\bigl(c_{d , l}\\bigr ) -",
    "\\left(\\frac{\\log d}{c_{d , l } } \\right ) { { \\rm i\\!e}}\\bigl[\\delta(u ) \\ , d^{-\\delta(u)}\\bigr ] \\label{eq : relative entropy from q to p}\\end{aligned}\\ ] ] and the combination of , and yields that @xmath139 \\right ] .",
    "\\label{eq : jeffreys ' divergence between p and q}\\ ] ] in the continuation of this analysis , we restrict our attention to ud codes that satisfy the condition @xmath140 in general , it excludes huffman codes ; nevertheless , it is satisfied by some other important ud codes such as the shannon code , shannon - fano - elias code , and arithmetic coding ( see , e.g. , ( * ? ? ?",
    "* chapter  5 ) ) . since is equivalent to the condition that @xmath141 is non - negative on @xmath111 , it follows from that @xmath142 so , the upper bound on jeffreys divergence in is twice smaller than the upper bound on the relative entropy in .",
    "it is partially because the term @xmath143 is canceled out along the derivation of the bound in , in contrast to the derivation of the bound in where this term was upper bounded by zero ( hence , it has been removed from the bound ) in order to avoid its dependence on the length of the codeword for each individual symbol .",
    "following proposition  [ proposition : tight lower bound on jeffreys divergence in terms of the total variation distance ] , for @xmath144 , let @xmath145 be the unique solution in the interval @xmath146 of the equation @xmath147 the combination of and implies that @xmath148 the bounds in , and are depicted in figure  [ figure : comparison of bounds for lossless source coding ] for ud codes where the size of their alphabet is @xmath149 .    in the following , the bounds in and are compared analytically for the case where the average redundancy is small ( i.e. , @xmath150 ) . under this approximation ,",
    "the bound in ( i.e. , the original bound from @xcite ) coincides with its tightened version in . on the other hand ,",
    "since for @xmath151 , the left - hand side of is approximately @xmath104 , it follows from that , for @xmath152 , we have",
    "@xmath153 it follows that , if @xmath150 , inequality gets approximately the form @xmath154 hence , even for a small average redundancy , the bound in improves by a factor of @xmath155 .",
    "this conclusion is consistent with the plot in figure  [ figure : comparison of bounds for lossless source coding ] .",
    "the author thanks the anonymous reviewers for their helpful comments .                                                        a. d. yardi .",
    "a. kumar , and s. vijayakumaran , `` channel - code detection by a third - party receiver via the likelihood ratio test , '' _ proceedings of the 2014 ieee international symposium on information theory _ , pp .",
    "10511055 , honolulu , hawaii , usa , july 2014 ."
  ],
  "abstract_text": [
    "<S> tight bounds for several symmetric divergence measures are derived in terms of the total variation distance . </S>",
    "<S> it is shown that each of these bounds is attained by a pair of 2 or 3-element probability distributions . </S>",
    "<S> an application of these bounds for lossless source coding is provided , refining and improving a certain bound by csiszr . </S>",
    "<S> another application of these bounds has been recently introduced by yardi . </S>",
    "<S> _ et al . _ for channel - code detection .    </S>",
    "<S> _ index terms _  bhattacharyya distance , chernoff information , @xmath0-divergence , jeffreys divergence , lossless source coding , total variation distance . </S>"
  ]
}