{
  "article_text": [
    "the maximum likelihood estimator @xmath0 of the mean @xmath1 of a gaussian random vector @xmath2 in @xmath3 with covariance @xmath4 under a probability @xmath5 is well - known to be equal to @xmath2 itself , and can be computed by maximizing the likelihood ratio @xmath6 with respect to @xmath7 , where @xmath8 denotes the euclidean norm on @xmath3 .",
    "it is efficient in the sense that it attains the cramer - rao bound @xmath9   = \\inf_z   \\e_\\mu [ \\vert z - \\mu \\vert_d^2 ] , \\qquad   \\mu \\in \\real^d,\\ ] ] over all unbiased estimators @xmath10 satisfying @xmath11 = \\mu$ ] , for all @xmath12 . + in @xcite , james and",
    "stein have constructed superefficient estimators for the mean of @xmath13 , of the form @xmath14 whose risk is lower than the cramer rao bound @xmath15 in dimension @xmath16 .",
    "+ drift estimation for gaussian processes is of interest in several fields of application .",
    "for example in the decomposition @xmath17,\\ ] ] the process @xmath18}$ ] is interpreted as an observed output signal , the drift @xmath19}$ ] is viewed as an input signal to be estimated and perturbed by a centered gaussian noise @xmath20}$ ] , cf .",
    "e.g. @xcite , ch .  vii .",
    "such results find applications in e.g. telecommunication ( additive gaussian channels ) and finance ( identification of market trends ) .",
    "+ berger and wolpert @xcite , @xcite , have constructed estimators of james - stein type for the drift of a gaussian process @xmath18}$ ] by applying the james - stein procedure to the independent gaussian random variables appearing in the karhunen - love expansion of the process . in this context",
    ", @xmath21 is seen as a minimax estimator of its own drift @xmath22 .",
    "+ stein  @xcite has shown that the james - stein estimators on @xmath3 could be extended to a wider family of estimators , using integration by parts for gaussian measures .",
    "let us briefly recall stein s argument , which relies on integration by parts with respect to the gaussian density and on the properties of superharmonic functionals for the laplacian on @xmath3 .",
    "given an estimator of @xmath12 of the form @xmath23 , where @xmath24 is sufficiently smooth , and applying the integration by parts formula @xmath25 =   \\sigma^2   \\e_\\mu [ \\partial_i g_i ( x)],\\ ] ] @xmath26 , one obtains @xmath27 = \\sigma^2 d   +   4 \\sigma^4   \\sum_{i=1}^d   \\e_\\mu \\left [ \\frac {   \\partial^2_i \\sqrt{f } ( x)}{\\sqrt{f } ( x ) } \\right ] , \\ ] ] i.e. @xmath28 is a superefficient estimator if @xmath29 which is possible if @xmath16 . in this case , @xmath28 improves in the mean square sense over the efficient estimator @xmath30 which attains the cramer - rao bound @xmath15 on unbiased estimators of @xmath31 .",
    "+ in this paper we present an extension of stein s argument to an infinite - dimensional setting using the malliavin integration by parts formula , with application to the construction of stein type estimators for the drift of a gaussian process @xmath18}$ ] .",
    "our approach applies to gaussian processes such as volterra processes and fractional brownian motions .",
    "it also extends the results of berger and wolpert @xcite in the same way that the construction of stein  @xcite extends that of james and stein  @xcite , and this allows us to recover the estimators of james - stein type introduced by berger and wolpert @xcite as particular cases .",
    "here we replace the stein equation with the integration by parts formula of the malliavin calculus on gaussian space .",
    "our estimators are given by processes of the form @xmath32 , \\ ] ] where @xmath33 is a positive superharmonic random variable on gaussian space and @xmath34 is the malliavin derivative indexed by @xmath35 $ ] .",
    "in contrast to the minimax estimator @xmath30 , such estimators are not only biased but also anticipating with respect to the brownian filtration @xmath36}$ ] .",
    "this however poses no problem when one has access to complete paths from time @xmath37 to @xmath38 .",
    "+ for large values of @xmath39 it can be shown that the percentage gain of this estimator is at least equal to the universal constant @xmath40 which approximately represents @xmath41 , see below .",
    "+ we proceed as follows . in section",
    "[ 2 ] we use stochastic calculus in the independent increment case to derive a cramer - rao bound over all unbiased drift estimators .",
    "this bound is attained by the process @xmath42}$ ] , which will be considered as an efficient drift estimator . in section",
    "[ 2.0 ] we compute the bayes estimators obtained under prior gaussian distributions .",
    "we show that these bayes estimators are admissible , and use them to prove that the drift estimator @xmath30 is minimax .",
    "the tools and results presented in sections  [ 2 ] and [ 2.0 ] are not surprising , but we did not find any source covering them in the literature . in section",
    "[ 2.1 ] we recall the elements of analysis and integration by parts on gaussian space which will be needed in section  [ 3 ] to construct superefficient drift estimators for gaussian processes using superharmonic random functionals on gaussian space .",
    "the superefficiency of these estimators will show , as in the classical case , that the minimax estimator @xmath30 is not admissible . in section  [ app ]",
    "we give examples of nonnegative superharmonic functionals using cylindrical functionals and potential theory on gaussian space .",
    "examples are considered in section  [ 4 ] in case @xmath43 is deterministic .",
    "we show that the james - stein estimators of berger and wolpert @xcite can be recovered as particular cases in our approach , and we provide numerical simulations for the gain of such estimators .",
    "it turns out that in those examples , the gain obtained in comparison with the minimax estimator @xmath18}$ ] is a function of @xmath44 , thus making @xmath39 and @xmath38 play inverse roles , unlike in the usual setting of brownian rescaling .",
    "+ this paper is an extended version of @xcite and provides proofs of the results presented in @xcite .",
    "let @xmath45 . consider a real - valued centered gaussian process @xmath18}$ ] with covariance function @xmath46 , \\qquad s , t\\in [ 0,t],\\ ] ] on a probability space @xmath47 , where @xmath48 is the @xmath39-algebra generated by @xmath2 . recall that @xmath18}$ ]",
    "can be represented in different ways as an isonormal gaussian process on a real separable hilbert space @xmath49 , i.e. as an isometry @xmath50 such that @xmath51 is a family of centered gaussian random variables satisfying @xmath52   = \\langle h , g \\rangle_h ,   \\qquad   h , g \\in h , \\ ] ] where @xmath53 and @xmath54 denote the scalar product and norm on @xmath49 .",
    "+ one can distinguish two main types of such isonormal representations of @xmath55 , see e.g. @xcite and @xcite respectively for details .    *",
    "( a ) * paley - wiener expansions . in this case",
    ", @xmath49 is the completion of the linear space generated by the functions @xmath56 , @xmath57 $ ] , with respect to the norm @xmath58 , \\ ] ] and @xmath59 is constructed on @xmath49 from @xmath60 , @xmath35 $ ] , i.e. we have @xmath61,\\ ] ] for any orthonormal basis @xmath62 of @xmath49 .",
    "assume in addition @xmath63 has the form @xmath64 , \\ ] ] where @xmath65 is a deterministic kernel and @xmath66 is differentiable in @xmath35 $ ] , and let @xmath67 denote the adjoint of @xmath68 with respect to @xmath69 , dt ) } .\\ ] ] the scalar product in @xmath49 then satisfies @xmath70 where @xmath71 , and we have the decomposition @xmath72 }   ,   \\dot{\\gamma } h_k   \\rangle_{l^2([0,t ] , dt ) }   x ( h_k )   = \\sum_{k=0}^\\infty   \\gamma h_k ( t )   x ( h_k ) , \\ ] ] @xmath35 $ ] . in this case",
    "we also have the representation @xmath73,\\ ] ] where @xmath74}$ ] is a standard brownian motion , cf .",
    "@xcite .    *",
    "( b ) * karhunen - love expansions .",
    "this framework is used in @xcite . in this case",
    ", @xmath31 is a finite borel measure on @xmath75 $ ] and @xmath49 is defined from @xmath76 where @xmath77,d\\mu ) } , \\ ] ] and @xmath78 , \\ ] ] with @xmath79 given @xmath62 an orthonormal basis of @xmath80,d\\mu)$ ] , we have the expansion @xmath81 .\\ ] ]    in the sequel we will use mainly the framework @xmath82 with @xmath83 , dt ) } $ ] , which is better adapted to our approach , although some results valid in the general framework of gaussian processes will be valid for @xmath84 as well .",
    "+ the girsanov theorem for gaussian processes , cf .",
    "e.g. @xcite , states that @xmath85 defined as @xmath86 where @xmath87 is deterministic , has same law as @xmath2 under the probability @xmath88 defined by @xmath89 in other terms , in case @xmath82 we have @xmath90 , \\end{aligned}\\ ] ] where @xmath62 is orthonormal basis of @xmath49 , and in case @xmath84 , @xmath91,d\\mu ) }   ) \\\\   &",
    "= &   \\sum_{k=0}^\\infty   h_k ( t )   (   x(h_k ) - \\langle h_k , \\gamma^{-1 } u \\rangle_h   ) \\\\   & = &   \\sum_{k=0}^\\infty   h_k ( t ) x^u ( h_k ) , \\qquad   t\\in [ 0,t ] , \\end{aligned}\\ ] ] where @xmath62 is orthonormal basis of @xmath80,d\\mu)$ ] .",
    "here we work in the framework of @xmath82 , in the particular case where @xmath18}$ ] has independent increments , i.e. @xmath92 where @xmath93 , dt ) $ ] is an a.e .",
    "non - vanishing function , @xmath94,\\ ] ] with @xmath95 } ( r ) \\sigma_r$ ] and @xmath96.\\ ] ] in other terms , @xmath97}$ ] is a continuous gaussian martingale with quadratic variation @xmath98 , which can be represented as the time change @xmath99,\\ ] ] of the standard brownian motion @xmath100 , or as the stochastic integral process @xmath101 ,",
    "@xmath35 $ ] , and we have @xmath102 , @xmath103 , where @xmath104 \\to \\real   \\ : \\   v ( t ) = \\int_0^t \\dot{v}(s ) ds ,   \\",
    "t\\in [ 0,t ] ,   \\",
    "\\dot{v } \\in l^2 ( [ 0,t ] , \\sigma^2_t dt )   \\right\\}\\ ] ] is the cameron - martin space with inner product @xmath105    let @xmath36}$ ] denote the filtration generated by @xmath18}$ ] , and for @xmath43 an @xmath106-adapted process , let @xmath107 denote the translation of the wiener measure on @xmath108 by @xmath43 , i.e. @xmath107 is the measure on @xmath108 under which @xmath109,\\ ] ] is a continuous gaussian martingale with quadratic variation @xmath110 consider @xmath43 an @xmath106-adapted processes of the form @xmath111 , \\ ] ] with @xmath112   < \\infty .\\ ] ] by the girsanov theorem , @xmath107 is absolutely continuous with respect to @xmath113 , with @xmath114 where @xmath115 denotes the girsanov - cameron - martin density , the canonical process @xmath18}$ ] becomes a continuous gaussian semimartingale under @xmath107 , with quadratic variation @xmath98 and drift @xmath116 .",
    "the expectation under @xmath88 will be denoted by @xmath117 .",
    "a drift estimator @xmath118 is called unbiased if @xmath119= \\e_u [ u_t ] , \\quad t \\in [ 0,t],\\ ] ] for all square - integrable @xmath106-adapted process @xmath120}$ ] .",
    "it is called adapted if the process @xmath121}$ ] is @xmath106-adapted .    here , the canonical process @xmath18}$ ] will be considered as an unbiased estimator of own its drift @xmath120}$ ] under @xmath107 , with risk defined as @xmath122 , d\\mu ) } ^2   \\right ]   =   \\int_0^t \\e^\\sigma_u   \\left [   | x^u_t |^2 \\right ]   \\mu ( dt )   =   \\int_0^t \\int_0^t \\sigma^2_s ds   \\mu ( dt ) , \\ ] ] where @xmath31 is a finite borel measure on @xmath75 $ ] .",
    "clearly this estimator is consistent as @xmath39 or @xmath38 tend to @xmath37 : precisely , given @xmath123 independent samples @xmath124 } , \\ldots , ( x_t^n ) _ { t\\in [ 0,t]},\\ ] ] of @xmath18}$ ] , the process @xmath125,\\ ] ] is an unbiased estimator of @xmath120}$ ] whose risk @xmath126 , d\\mu ) } ^2   \\right ]   =   \\frac{1}{n }   \\int_0^t \\int_0^t \\sigma^2_s ds   \\mu ( dt ) \\ ] ] converges to zero as @xmath123 goes to infinity .",
    "+ the justification of the use of @xmath127}$ ] as an efficient estimator comes from the following proposition which allows us to compute a cramer - rao bound attained by @xmath30 . here the parameter space is restricted to the space of adapted processes in @xmath128 , \\p \\otimes \\mu ) $ ] , which corresponds in a sense to a parametric estimation .",
    "[ prop : cramerraobound ] cramer - rao inequality . for any unbiased and",
    "adapted estimator @xmath118 of @xmath43 we have @xmath129   \\geq   \\crb ( \\sigma,\\mu , \\hat{u } ) , \\ ] ] where @xmath130 , \\p_u^\\sigma \\otimes \\mu ) $ ] is adapted and the cramer - rao type bound @xmath131 is independent of @xmath43 and attained by the efficient estimator @xmath132 .    since @xmath118 is unbiased , for all @xmath133 we have @xmath134   & = &   \\e^\\sigma_{u+\\varepsilon \\zeta}[u_t   +   \\varepsilon   \\zeta_t ] \\\\   & = &   \\e^\\sigma_{u+\\varepsilon \\zeta}[u_t ]   +   \\varepsilon   \\e^\\sigma_{u+\\varepsilon \\zeta}[\\zeta_t ] \\\\   & = &   \\e^\\sigma_{u+\\varepsilon \\zeta}[u_t ] + \\varepsilon \\zeta_t ,   \\quad t\\in [ 0,t ] , \\quad \\varepsilon \\in \\real,\\end{aligned}\\ ] ] hence @xmath135_{|\\varepsilon=0 } \\\\ & = & \\frac{d}{d\\varepsilon } \\e^\\sigma   [   ( \\xi_t - u_t )   \\lambda ( u+\\varepsilon \\zeta ) ] _ { | \\varepsilon=0 } \\\\   & = &   \\e^\\sigma   \\left [   (   \\xi_t - u_t   )   \\frac{d}{d\\varepsilon }   \\lambda ( u+\\varepsilon \\zeta ) _ { | \\varepsilon=0 }   \\right ] \\\\ & = &   \\e^\\sigma_u \\left [   ( \\xi_t - u_t )   \\frac{d}{d\\varepsilon }   \\log \\lambda ( u+\\varepsilon \\zeta ) _ { | \\varepsilon=0 } \\right ] \\\\   & = &   \\e^\\sigma_u \\left [   (   \\xi_t - u_t   )   \\left (   \\int_0^t \\frac{\\dot{\\zeta}_s}{\\sigma^2_s }   dx_s   -   \\int_0^t \\frac{\\dot{\\zeta}_s\\dot{u}_s}{\\sigma^2_s }   ds   \\right )   \\right ] \\\\   & = &   \\e^\\sigma_u \\left [   (   \\xi_t - u_t   )   \\int_0^t   \\frac{\\dot{\\zeta}_s}{\\sigma^2_s }   dx^u_s   \\right ] \\\\   & = &   \\e^\\sigma_u \\left [   (   \\xi_t - u_t   )   \\int_0^t   \\frac{\\dot{\\zeta}_s}{\\sigma^2_s }   dx^u_s   \\right ] , \\end{aligned}\\ ] ] where the exchange between expectation and derivative is justified by classical uniform integrability arguments .",
    "thus , by the cauchy - schwarz inequality and the it isometry we have @xmath136 \\e^\\sigma_u [ | \\xi_t - u_t|^2 ]   =   \\int_0^t   \\frac{\\dot{\\zeta}_s^2}{\\sigma^2_s }    ds \\e^\\sigma_u [ | \\xi_t - u_t|^2 ] ,   \\qquad   t\\in [ 0,t].\\ ] ] it then suffices to take @xmath137,\\ ] ] to get @xmath138   =   \\e^\\sigma_u [ |\\xi_t - u_t|^2 ] \\geq   \\int_0^t \\sigma^2_s ds , \\qquad t\\in [ 0,t ] , \\ ] ] which leads to after integration with respect to @xmath139 . as noted above , @xmath140}$ ] is clearly unbiased under @xmath107 and it attains the lower bound @xmath141 .    recall that the classical linear parametric estimation problem for the drift of a diffusion consists in estimating the coefficient @xmath142 appearing in @xmath143 with a maximum likelihood estimator @xmath144 given by @xmath145 cf .",
    "@xcite , @xcite for brownian motion and @xcite for an extension to fractional brownian motions .",
    "+ here we consider the nonparametric functional estimation of the drift of a one - dimensional drifted brownian motion @xmath146 with decomposition @xmath147 where @xmath148 } \\in l^2 ( \\omega \\times [ 0,t])$ ] is an adapted process and @xmath149 is a standard brownian motion with quadratic variation @xmath150 under a probability @xmath107 . + in case @xmath43 is constrained to have the form @xmath151 , @xmath35 $ ] , @xmath152 , our efficient estimator @xmath30 satisfies @xmath153 , where @xmath144 is given by , @xmath45 , with the asymptotics @xmath154 in probability as @xmath38 tends to infinity .",
    "the asymptotics is not in large time since @xmath38 can be a fixed parameter , but the efficient estimator @xmath155 converges to @xmath43 as @xmath39 tends to @xmath37 , or equivalently as @xmath38 tends to @xmath37 by rescaling .",
    "+ to close this section we note that , at least informally , @xmath140}$ ] can be viewed as a maximum likelihood estimator of its own adapted drift @xmath120}$ ] under @xmath156 .",
    "indeed the functional differentiation of the cameron - martin density @xmath157 implies @xmath158 which leads to @xmath159 .",
    "in this section we consider bayes estimators which will be useful in proving the minimaxity of the estimator @xmath155 in the framework of @xmath82 for gaussian processes with non - necessarily independent increments .",
    "we will make use of the next lemma which is classical in the framework of gaussian filtering and is proved in the appendix .",
    "[ ljk ] let @xmath10 be a gaussian process with covariance operator @xmath160 and drift @xmath161 , and assume that @xmath2 is a gaussian process with drift @xmath10 and covariance operator @xmath162 given @xmath10 .",
    "then , conditionally to @xmath2 , @xmath10 has drift @xmath163    note that unlike in proposition  [ prop : cramerraobound ] , no adaptedness or unbiasedness restriction is made on @xmath118 in the infimum taken in below .",
    "[ best ] bayes estimator .",
    "let @xmath164 denote the gaussian distribution on @xmath108 with covariance operator @xmath160 and drift @xmath161 .",
    "the bayes risk @xmath165   d\\p^\\tau_v ( z)\\ ] ] of any estimator @xmath121}$ ] on @xmath108 under the prior distribution @xmath166 is uniquely minimized by @xmath167 , \\ ] ] which has risk @xmath168   d\\p^\\tau_v ( z ) .\\ ] ]    let @xmath10 denote a gaussian process with drift @xmath161 and covariance @xmath160 .",
    "recall ( cf .",
    "lemma  [ ljk ] ) that if @xmath2 has drift @xmath10 and covariance @xmath162 then , conditionally to @xmath2 , @xmath169}$ ] has drift @xmath170 and covariance @xmath171 .",
    "hence the bayes risk of an estimator @xmath118 under the prior distribution @xmath172 is given by @xmath173   d\\p^\\tau_v ( z )   =   \\e \\left [   \\e \\left [ \\int_0^t | \\xi_t - z_t|^2 \\mu ( dt ) \\big| x   \\right ]   \\right ] } \\\\   & = &   \\e \\left [   \\int_0^t | \\xi_t - \\e[z_t \\mid x ] |^2 \\mu ( dt )   \\right ]   +   \\e \\left [   \\int_0^t   \\var ( z_t | x )   \\mu ( dt )   \\right ] \\\\   & = &   \\e \\left [   \\int_0^t   \\left| \\xi_t -   \\langle   \\stackrel{}{\\chi}_t ,   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma   v \\rangle   -   x (   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma_\\tau   \\stackrel{}{\\chi}_t   )   \\right|^2   \\mu ( dt )   \\right ] \\\\   & &   +   \\int_0^t   \\langle   \\stackrel{}{\\chi}_t   ,   \\gamma   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma_\\tau   \\stackrel{}{\\chi}_t   \\rangle   \\mu ( dt ) , \\end{aligned}\\ ] ] which is minimized by @xmath174   =   \\langle   \\stackrel{}{\\chi}_t ,   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma   v \\rangle   -   x (   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma_\\tau   \\stackrel{}{\\chi}_t   ) ,   \\qquad t\\in [ 0,t ] .\\ ] ]    clearly @xmath175 is unique in the sense that it is the only estimator to minimize the bayes risk .",
    "this shows in particular that every @xmath175 is admissible in the sense that if an estimator @xmath118 satisfies @xmath176 , d\\mu ) } ^2   \\right ]   \\leq   \\e_z \\left [   \\vert \\xi^{\\tau , v } - z \\vert_{l^2 ( [ 0,t ] , d\\mu ) } ^2   \\right ] , \\qquad   z\\in \\omega , \\ ] ] then @xmath177 , d\\mu ) } ^2   \\right ]   d\\p^\\tau_v   & \\leq &   \\int_\\omega   \\e_z \\left [   \\vert \\xi^{\\tau , v } - z \\vert_{l^2 ( [ 0,t ] , d\\mu ) } ^2   \\right ]   d\\p^\\tau_v \\\\   & = &   \\int_0^t   \\langle   \\stackrel{}{\\chi}_t   ,   \\gamma   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma_\\tau   \\stackrel{}{\\chi}_t   \\rangle   \\mu ( dt ) , \\end{aligned}\\ ] ] hence @xmath178 , d\\mu ) } ^2   \\right ]   d\\p^\\tau_v   =   \\int_0^t   \\langle   \\stackrel{}{\\chi}_t   ,   \\gamma   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma_\\tau   \\stackrel{}{\\chi}_t   \\rangle   \\mu ( dt ) , \\ ] ] and @xmath179 by proposition  [ best ] .",
    "+ the bayes estimator @xmath180 is biased in general , and for deterministic @xmath87 its mean square error under @xmath88 is equal to @xmath181 } \\\\",
    "\\nonumber   & = &   \\e_u \\left [ \\int_0^t \\big|   \\xi^{\\tau , v}_t - \\e_u [ \\xi^{\\tau , v}_t ]   \\big|^2   \\mu ( dt )   \\right ]   +   \\e_u \\left [   \\int_0^t |   \\e_u [ \\xi^{\\tau , v}_t ]   - u_t   |^2   \\mu ( dt )   \\right ] \\\\",
    "\\nonumber   & = &   \\e_u \\left [ \\int_0^t \\big|   x^u (   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma_\\tau   \\stackrel{}{\\chi}_t   )   \\big|^2   \\mu ( dt )   \\right ]   +   \\int_0^t \\big|   \\langle   \\stackrel{}{\\chi}_t ,   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma   ( v - u )   \\rangle   \\big|^2   \\mu ( dt ) \\\\ \\nonumber   & = &   \\int_0^t   \\langle   ( \\gamma_\\tau+\\gamma ) ^{-1 }   \\gamma_\\tau   \\stackrel{}{\\chi}_t   ,   \\gamma ( \\gamma_\\tau+\\gamma ) ^{-1 }   \\gamma_\\tau   \\stackrel{}{\\chi}_t   \\rangle   \\mu ( dt ) \\\\",
    "\\nonumber & &   +   \\int_0^t \\big|   \\langle   \\stackrel{}{\\chi}_t ,   ( \\gamma_\\tau+\\gamma ) ^{-1 } \\gamma   ( v - u )   \\rangle   \\big|^2   \\mu ( dt ) , \\end{aligned}\\ ] ] which shows that @xmath182   =   + \\infty , \\ ] ] hence @xmath180 is not minimax .",
    "+ in the independent increment case of section  [ 2 ] we have , if @xmath183 , @xmath184 $ ] : @xmath185 , \\ ] ] with risk @xmath186   d\\p^\\tau_v ( z ) .\\ ] ] assuming now that @xmath187 , @xmath35 $ ] , the bayes risk @xmath188 of @xmath180 , @xmath189 , converges as @xmath190 to the bound @xmath191 hence it follows in the next proposition that , as in the finite dimensional gaussian case , the estimator @xmath140}$ ] is minimax .",
    "note again that unlike in proposition  [ prop : cramerraobound ] , no adaptedness condition is imposed on @xmath118 in the infima and .",
    "[ minimax ] the estimator @xmath132 is minimax . for all @xmath192 we have",
    "@xmath193   = \\inf_\\xi   \\sup_{v \\in \\omega }   \\e_v \\left [ \\int_0^t | \\xi_t - v_t|^2 \\mu ( dt ) \\right ] .\\ ] ]    clearly , taking @xmath194 yields @xmath195   \\geq \\inf_\\xi   \\sup_{u\\in \\omega }   \\e_u \\left [ \\int_0^t | \\xi_t - u_t|^2 \\mu ( dt ) \\right ] .\\ ] ] on the other hand , from proposition  [ best ] , for all processes",
    "@xmath118 we have @xmath196   & \\geq &   \\int_\\omega",
    "\\e_z \\left [ \\int_0^t | \\xi_t - z_t|^2 \\mu ( dt )   \\right ]   d\\p^\\tau_0 ( z ) \\\\   & \\geq &   \\int_0^t   \\langle   \\stackrel{}{\\chi}_t ,   ( i + \\gamma /\\tau^2 ) ^{-1 }   \\gamma   \\stackrel{}{\\chi}_t   \\rangle   \\mu ( dt ) , \\end{aligned}\\ ] ] for all @xmath197 , hence @xmath198",
    "\\geq   \\int_0^t   \\langle   \\stackrel{}{\\chi}_t ,   \\gamma   \\stackrel{}{\\chi}_t   \\rangle   \\mu ( dt )   =   \\crb ( \\gamma , \\mu , \\hat{u } ) .\\ ] ]",
    "before proceeding to the construction of stein type estimators , we need to introduce some elements of analysis on gaussian space , see e.g. @xcite . this construction is valid in both frameworks @xmath82 and @xmath84 . given @xmath199 , let @xmath200 we fix @xmath201 a total subset of @xmath49 and let @xmath202 denote the space of cylindrical functionals of the form @xmath203 where @xmath204 is in the space of infinitely differentiable rapidly decreasing functions on @xmath205 , @xmath206 .",
    "the @xmath207-valued malliavin derivative is defined as @xmath208 for @xmath209 of the form .",
    "it is known that @xmath210 is closable , cf .",
    "proposition  1.2.1 of @xcite , and its closed domain will be denoted by @xmath211 .",
    "let @xmath212 be defined on @xmath213 as @xmath214 , \\quad f\\in \\dom ( \\nabla ) .\\ ] ]    let @xmath215 denote the closable adjoint of @xmath210 , i.e. the divergence operator under @xmath88 , which satisfies the integration by parts formula @xmath216   = \\e_u [ \\langle v , \\nabla f \\rangle_{h } ] ,   \\qquad   f \\in \\dom ( \\nabla )   ,   \\quad   v \\in \\dom ( \\delta ) , \\ ] ] with the relation @xmath217 cf .",
    "@xcite , for @xmath218 and @xmath219 such that @xmath220 .",
    "note that is an infinite - dimensional version of the integration by parts , which can be proved e.g. using the countable gaussian random variables constructed from @xmath2 .",
    "[ prt ] we have @xmath221   = \\e_u [   d_t f   ] ,   \\qquad   t\\in [ 0,t ]   ,   \\quad   f \\in \\dom ( \\nabla ) .\\ ] ]    \\(a ) in the case of paley - wiener expansions we have @xmath222   & = &   \\e_u [ f x^u   ( \\stackrel{}{\\chi}_t ) ] \\\\   & = &   \\e_u [ f \\delta ( \\stackrel{}{\\chi}_t ) ] \\\\   & = &   \\e_u [ \\langle \\stackrel{}{\\chi}_t , \\nabla f\\rangle_h ] \\\\   & = &   \\e_u [ \\langle 1_{[0,t ] } , \\dot{\\gamma } \\nabla f\\rangle_{l^2([0,t ] , dt ) } ] \\\\   & = &   \\e_u [ ( \\gamma \\nabla f ) ( t ) ] ,   \\qquad   f \\in \\dom ( \\nabla )   ,   \\quad   t\\in [ 0,t ] .\\end{aligned}\\ ] ]    \\(b ) in the case of karhunen - love expansions we have @xmath222   & = &   \\sum_{k=0}^\\infty   h_k(t )   \\e_u [ f x^u ( h_k ) ] \\\\   & = &   \\sum_{k=0}^\\infty   h_k(t )   \\e_u [ f \\delta ( h_k ) ] \\\\   & = &   \\sum_{k=0}^\\infty   h_k(t )   \\e_u [ \\langle h_k , \\nabla f\\rangle_h ] \\\\   & = &   \\sum_{k=0}^\\infty   h_k(t )   \\e_u [ \\langle h_k , \\gamma \\nabla f\\rangle_{l^2([0,t],\\mu ) } ] \\\\   & = &   \\e_u [   ( \\gamma \\nabla f ) ( t )   ] ,   \\qquad   f \\in \\dom ( \\nabla )   ,   \\quad   t\\in [ 0,t ] .\\end{aligned}\\ ] ]    we define the laplacian @xmath223 by @xmath224 , d\\mu ) ^{\\otimes 2 } } d d f   = \\int_0^t d_t d_t f \\mu ( dt)\\ ] ] on the space @xmath225 made of all @xmath218 such that @xmath226 , @xmath35 $ ] , and @xmath227}\\in l^2([0,t],\\mu)$ ] , @xmath228-a.s .    if @xmath229 has the form we have @xmath230,\\mu ) }   \\partial_i \\partial_j f_n   \\left (   x^u ( h_1 )   ,   \\ldots   ,   x^u ( h_n )   \\right ) .\\ ] ] unlike the gross laplacian @xmath231 defined by @xmath232 the operator @xmath223 is closable , as shown in the following proposition .",
    "[ prop : closability ] closability of @xmath223 . for any sequence @xmath233 of random variables converging to @xmath37 in @xmath234 and such that @xmath235 converges in @xmath234",
    ", we have @xmath236    let @xmath237 a sequence in @xmath238 converging to @xmath37 in @xmath239 , and such that @xmath240 converges to @xmath33 in @xmath239 .",
    "for all @xmath241 we have , in the notation of @xmath82 : @xmath242 \\right\\vert } \\\\ & = & { \\displaystyle } { \\left\\vert \\int_0^t \\e_u [ \\langle\\nabla d_t g_n ,   \\stackrel{}{\\chi}_t g \\rangle_h ] \\ , \\mu(dt )   \\right\\vert } \\\\ & = & { \\displaystyle } { \\left\\vert \\int_0^t \\e_u [ d_t g_n \\ , \\delta (   \\stackrel{}{\\chi}_t   g ) ] \\ , \\mu(dt )   \\right\\vert } \\\\ & = & { \\displaystyle } { \\left\\vert \\int_0^t \\e_u [ \\langle \\nabla g_n ,   \\stackrel{}{\\chi}_t   \\delta (   \\stackrel{}{\\chi}_t   g ) \\rangle_h ] \\ , \\mu(dt )   \\right\\vert } \\\\ & = & { \\displaystyle } { \\left\\vert \\int_0^t \\e_u [ g_n \\delta (   \\stackrel{}{\\chi}_t   \\delta (   \\stackrel{}{\\chi}_t   g ) ) ] \\ , \\mu(dt )   \\right\\vert } \\\\ & \\leq & { \\displaystyle } { \\|g_n\\|_{l^2 ( \\omega , \\p_u ) } \\int_0^t \\|\\delta (   \\stackrel{}{\\chi}_t   \\delta (   \\stackrel{}{\\chi}_t   g))\\|_{l^2(\\omega , \\p_u ) } \\ , \\mu(dt ) , } \\end{aligned}\\ ] ] hence @xmath243 , which implies @xmath244 .",
    "we will say that a random variable @xmath33 in @xmath225 is @xmath223-superharmonic on @xmath108 if @xmath245    in the independent increment case where @xmath63 is given by @xmath246 , \\ ] ] we have @xmath247 for every @xmath106-adapted process @xmath248 .",
    "our aim is to construct a superefficient estimator of @xmath43 of the form @xmath249 , whose mean square error is strictly smaller than the minimax risk @xmath250 of proposition  [ minimax ] when @xmath251\\times \\omega , \\p_u \\otimes \\mu ) $ ] is a suitably chosen stochastic process .",
    "this estimator will be biased and anticipating with respect to the brownian filtration . in the next lemma",
    "we follow stein s argument which uses integration by parts but we replace by the duality relation between the gradient and divergence operators on gaussian space .",
    "the results of this section are valid in both frameworks @xmath82 and @xmath84 .",
    "[ lemma1 ] unbiased risk estimate . for any @xmath252 , \\p_u \\otimes \\mu ) $ ] such that @xmath253 , @xmath35 $ ] , and @xmath254}\\in l^1 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) $ ] , we have @xmath255 , \\mu ) } ^2   \\right ]   =   \\crb ( \\gamma , \\mu , \\hat{u } )   +   \\vert \\xi \\vert_{l^2 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) } ^2   + 2   \\e_u \\left [   \\int_0^t   d_t \\xi_t   \\mu ( dt )   \\right ] .\\ ] ]    we have @xmath256 , d\\mu ) } ^2 \\right ]   =   { \\e_u \\left [ \\int_0^t \\big{| } x^u_t + \\xi_t \\big{|}^2   \\mu ( dt )   \\right ] } } \\\\ & = & { \\e_u \\left [ \\int_0^t | x^u_t |^2   \\mu ( dt )   \\right ] +   \\vert \\xi \\vert_{l^2 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) } ^2   +   2 \\e_u \\left [ \\int_0^t x^u_t \\xi_t   \\mu ( dt )   \\right ] } \\\\   & = &   \\crb ( \\gamma , \\mu , \\hat{u } )   +   \\vert \\xi \\vert_{l^2 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) } ^2   + 2 \\e_u \\left [ \\int_0^t x^u_t \\xi_t   \\mu ( dt )   \\right ] , \\end{aligned}\\ ] ] and apply lemma  [ prt ] to obtain .",
    "the next proposition specializes the above lemma to processes @xmath118 of the form @xmath257,\\ ] ] where @xmath33 is an a.s .",
    "strictly positive and sufficiently smooth random variable .",
    "[ prop : erreurcylindricalfunctions.1 ] logarithmic gradient . stein - type estimator .",
    "for any @xmath228-a.s .",
    "positive random variable @xmath258 such that @xmath259 , @xmath35 $ ] , and @xmath260}\\in l^1 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) $ ] , we have @xmath261 , d\\mu ) } ^2   \\right ]   =   \\crb ( \\gamma , \\mu , \\hat{u } )   -   \\e_u \\left [   \\vert   d \\log f \\vert_{l^2 ( [ 0,t ] , \\mu ) } ^2   \\right ]   +   2   \\e_u \\left [   \\frac { \\delta f } { f }   \\right ] .\\ ] ]    from we have @xmath262,\\mu ) } ^2   \\right ] } \\\\   & = &   \\crb ( \\gamma , \\mu , \\hat{u } )   +   \\vert d \\log f \\vert_{l^2 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) } ^2   +   2 \\e_u \\left [ \\int_0^t d_t d_t \\log f   \\mu ( dt )   \\right ] \\\\ & = &   \\crb ( \\gamma , \\mu , \\hat{u } )   +   \\e_u \\left [   \\int_0^t \\left (   \\bigg| \\frac{d_t f } { f } \\bigg|^2   + 2 d_t d_t \\log f   \\right )   \\mu ( dt )   \\right ] , \\end{aligned}\\ ] ] and we use the relation @xmath263.\\ ] ]    from the above proposition it suffices that @xmath33 be @xmath223-superharmonic for @xmath264 to be superefficient . in this case",
    "we have @xmath265 , d\\mu ) } ^2   \\right ]   \\leq   \\crb ( \\gamma , \\mu , \\hat{u } )   -   \\e_u \\left [   \\vert   d \\log f \\vert_{l^2([0,t ] , d\\mu ) } ^2   \\right ] , \\ ] ] with equality in when @xmath33 is @xmath223-harmonic .",
    "+ in the next proposition we show that the @xmath223-superharmonicity of @xmath33 is not necessary for @xmath264 to be superefficient , namely the @xmath223-superharmonicity of @xmath33 can be replaced by the @xmath223-superharmonicity of @xmath266 , which is a weaker assumption , see @xcite in the finite dimensional case .",
    "in particular , @xmath267 is a superefficient estimator of @xmath43 if @xmath268 on a set of strictly positive @xmath228-measure .",
    "[ prop : erreurcylindricalfunctions ] stein - type estimator . for any @xmath228-a.s .",
    "positive random variable @xmath258 such that @xmath259 , @xmath35 $ ] , and @xmath260}\\in l^1 ( \\omega \\times [ 0,t ] , \\p_u \\otimes \\mu ) $ ] , we have @xmath269 , d\\mu ) } ^2   \\right ]   =   \\crb ( \\gamma , \\mu , \\hat{u } )   +   4   \\e_u   \\left [ \\frac{\\delta \\sqrt{f}}{\\sqrt{f } } \\right ] .\\ ] ]    for any @xmath270 such that @xmath271 , @xmath228-a.s . , and @xmath272",
    ", we have @xmath273 , \\ ] ] which implies @xmath274 and allows us to conclude from lemma  [ lemma1 ] .",
    "relation extends to any @xmath218 such that @xmath275 , and @xmath276 , @xmath277 , @xmath228-a.s .",
    "+ in case @xmath18}$ ] is a brownian motion with constant variance @xmath278 , @xmath35 $ ] , we have @xmath279)}^2   \\right ]   \\leq   \\frac{\\sigma^2t^2}{2 }   +   4   \\e_u \\left [   \\frac{\\delta \\sqrt{f}}{\\sqrt{f } }   \\right ] .\\ ] ] given @xmath280 } , \\ldots , ( x_t^{\\sigma , n})_{t\\in [ 0,t]}$ ] are @xmath123 independent samples of @xmath18}$ ] , the process @xmath281 defined in satisfies @xmath282)}^2   \\right ]   =   \\frac{1}{n }   \\crb ( \\sigma,\\mu , \\hat{u } )   +   \\frac{4}{n^2 }   \\e_u \\left [   \\frac{\\delta \\sqrt{f}}{\\sqrt{f } }   \\right ] .\\ ] ]    as in @xcite , the superefficient estimators constructed in this way are minimax in the sense that from proposition  [ minimax ] and proposition  [ prop : erreurcylindricalfunctions.1 ] , for all @xmath87 we have @xmath283 , \\mu ) } ^2   \\right ]   <   \\crb ( \\gamma , \\mu , \\hat{u } )    = \\inf_\\xi   \\sup_{v \\in \\omega }   \\e_v \\left [ \\int_0^t | \\xi_t - v_t|^2 dt \\right ] , \\ ] ] provided @xmath268 on a set of strictly positive @xmath228-measure , thus showing that the minimax estimator @xmath140}$ ] is inadmissible . +    both estimators @xmath284 and @xmath285 $ ] have bias @xmath286   = \\e_u [ d_t \\log f ] , \\qquad   t\\in [ 0,t],\\ ] ] which can be bounded as follows from : @xmath287 , \\mu ) } ^2   & = &   \\int_0^t   |   \\e_u [ d_t \\log",
    "f ]   |^2 dt \\\\   & \\leq &   \\e_u \\left [ \\int_0^t   | d_t \\log f",
    "|^2 dt   \\right ] \\\\   & = &   2   \\e_u \\left [   \\frac { \\delta f } { f }   \\right ]   -   4   \\e_u \\left [   \\frac{\\delta \\sqrt{f}}{\\sqrt{f } }   \\right ] .\\end{aligned}\\ ] ]    in the independent increment case of section  [ 2 ] , the formulas obtained in this section also hold for @xmath43 an adapted process in @xmath128 ) $ ] .",
    "however , in this case the computation of the gradient @xmath288 requires in principle the knowledge of @xmath85 , except when @xmath43 is deterministic , in which case the knowledge of @xmath2 is sufficient .",
    "thus , assuming @xmath43 to be deterministic will be necessary for the applications of section  [ 4 ] .",
    "in this section we give examples of nonnegative superharmonic functionals with respect to the laplacian @xmath223 .",
    "we start by reviewing the construction of such functionals using potential theory on the gaussian space @xmath289 , and next we turn to cylindrical functionals which will be used in the numerical applications of section  [ 4 ] .",
    "we assume that @xmath290 is orthogonal in @xmath291 , d\\mu ) $ ] , and we let @xmath292,\\mu ) } , \\qquad   k \\geq 1.\\ ] ] the sequence @xmath293 can be realized as the solution of the eigenvalue problem @xmath294 in case @xmath82 , provided @xmath295 , and @xmath296 in case @xmath84 for general @xmath31 .",
    "we refer to @xcite and @xcite for the notion of harmonicity on the wiener space with respect to the gross laplacian . from our orthonormality assumption on @xmath293 , the laplacian @xmath223",
    "is written as @xmath297 on cylindrical functionals .",
    "let @xmath298 denote the standard @xmath108-valued wiener process with generator @xmath299 on @xmath300 , represented as @xmath301 where @xmath302 , @xmath206 , are independent standard brownian motions on @xmath300 , given as @xmath303 we have the covariance relation @xmath304 =   ( s \\wedge t )   \\langle   v_1 ,   v_2   \\rangle_{h^ { { } } } ,   \\qquad   s , t \\in \\real_+ ,   \\quad   v_1 , v_2 \\in h .\\ ] ] in other terms we have @xmath305   =   ( s\\wedge t )   \\sum_{n=1}^\\infty   \\frac {   \\int_0^a \\dot{h}_n(s ) \\sigma^2_s ds   \\int_0^b \\dot{h}_n(s ) \\sigma^2_s ds   } { \\vert h_n \\vert_{h^{{}}}^2 } } \\\\   & = &   ( s\\wedge t ) \\left",
    "<   \\sum_{n=1}^\\infty   \\frac{\\dot{h}_n}{\\vert h_n \\vert_{h^{{}}}^2 }   \\int_0^a \\dot{h}_n(s ) \\sigma^2_s ds   ,   \\sum_{n=1}^\\infty   \\frac{\\dot{h}_n}{\\vert h_n \\vert_{h^{{}}}^2 }   \\int_0^b \\dot{h}_n(s ) \\sigma^2_s ds   \\right>_{l^2([0,t],\\sigma^2_t dt ) } \\\\   & = &   ( s\\wedge t )   \\langle { \\bf 1}_{[0,a ] }   ,   { \\bf 1}_{[0,b ] }   \\rangle_{l^2([0,t],\\sigma^2_t dt ) } \\\\   & = &   ( s \\wedge t )   \\int_0^{a \\wedge b }   \\sigma^2_r dr , \\qquad 0 \\leq a , b \\leq t ,   \\quad s , t \\in \\real_+,\\end{aligned}\\ ] ] which shows that @xmath306}$ ] is a continuous gaussian martingale with quadratic variation @xmath307 for fixed @xmath308 .",
    "+ denote by @xmath309 the @xmath310-valued wiener process represented as @xmath311 with @xmath312 , @xmath206 , and covariance @xmath313 =   { \\bf 1}_{\\ { n = m \\ } } ( s \\wedge t ) ,   \\qquad   s , t \\in \\real_+,\\ ] ] i.e. @xmath314   = ( s \\wedge t )   \\langle q v_1 , v_2\\rangle_{h^ { { } } } ,   \\qquad   s , t \\in \\real_+ ,   \\quad   v_1,v_2\\in h^{{}},\\ ] ] where @xmath315 is the operator with eigenvalues @xmath316 in the hilbert basis @xmath317 .",
    "it s formula for hilbert - valued wiener processes , cf .",
    "theorem  4.17 of @xcite , shows that @xmath318 hence @xmath309 has generator @xmath319 . + dynkin s formula , cf .",
    "@xcite , theorem  5.1 , shows that for all stopping time @xmath320 such that @xmath321 < \\infty$ ] we have , @xmath322-a.s . :",
    "@xmath323   -   f ( \\omega )   =   \\frac{1}{2 }   \\tilde{\\e }   \\left [   \\int_0^\\tau   \\delta^{{}}f ( b_s )   ds   \\mid b_0 = \\omega   \\right ] , \\ ] ] hence @xmath324 implies @xmath325 .\\ ] ] for @xmath326 , let @xmath327 denotes the first exit time of @xmath328}$ ] from the open ball @xmath329 of radius @xmath326 , centered at @xmath330 .",
    "we have the following converse .",
    "let @xmath331 be such that @xmath332 is continuous on @xmath108 , and assume that there exists @xmath333 such that @xmath334 , \\qquad   \\p^\\sigma_u ( d\\omega ) -a.s .",
    ",   \\quad   0 < r < r_0 .\\ ] ] then @xmath33 is @xmath335-superharmonic on @xmath108 in the sense of relation .    from remark  3 , page 134 of @xcite , we have @xmath336   -   f ( \\omega ) } { \\tilde{\\e } [ \\tau_{1/n } \\mid b_0 = \\omega ] } , \\ ] ] which shows that @xmath324 when is satisfied .",
    "this yields in particular the following class of @xmath335-superharmonic functionals .",
    "let the potential of @xmath337 be defined by @xmath338   dt , \\qquad   \\p^\\sigma_u ( d\\omega ) -a.s . , \\ ] ] assume that @xmath339 and that @xmath340 is continuous on @xmath108 .",
    "then @xmath341 is a @xmath335-superharmonic on @xmath108 .    for all @xmath326 we have @xmath342   +   \\tilde{\\e }   [   g ( b_{\\tau_r } )   \\mid   b_0 = \\omega   ] \\\\",
    "& \\geq &   \\tilde{\\e }   [   g ( b_{\\tau_r } )   \\mid   b_0 = \\omega   ] , \\end{aligned}\\ ] ] which shows that @xmath341 is @xmath335-superharmonic .",
    "note that if @xmath33 is bounded with bounded support in @xmath108 then @xmath341 is bounded on @xmath108 , see e.g. remark  3.5 of @xcite .",
    "+      positive superharmonic functionals can also be obtained by convolution , i.e. if @xmath33 is @xmath335-superharmonic and @xmath341 is positive and sufficiently integrable , then @xmath343 is positive and @xmath335-superharmonic .",
    "superharmonic functionals on gaussian space can also be constructed as cylindrical functionals , by composition with finite - dimensional functions .",
    "here we use the expansions of case @xmath82 . from the expression of @xmath223 on cylindrical functionals",
    "@xmath344 we check that @xmath345 is superharmonic on @xmath108 if and only if @xmath204 is superharmonic on @xmath205 . given @xmath346 and @xmath347 ,",
    "let @xmath348 be defined as @xmath349 then @xmath350 is superharmonic on @xmath205 , @xmath351 , if and only if @xmath352 $ ] .",
    "let @xmath353 we have @xmath354 and @xmath355 since @xmath290 is orthogonal in @xmath80 , dt ) $ ] , hence @xmath356 is negative if @xmath357 , which is minimal for @xmath358 .",
    "we also have @xmath359 which is negative for @xmath360 $ ] and vanishes for @xmath358 . in this case",
    "the estimator is given by @xmath361 and from proposition  [ prop : erreurcylindricalfunctions.1 ] , inequality actually also holds as an equality : @xmath362 , dt ) } ^2   \\right ]   =   \\crb ( \\sigma,\\mu , \\hat{u } )   -   \\e_u \\left [   \\int_0^t   | d_t \\log f_{n,2-n , b } |^2   dt \\right ] , \\ ] ] with @xmath363 , dt ) } ^2   =   \\frac { ( n-2)^2 } { \\left|   b_1 +   \\lambda_1^{-1 }   x^u ( h_1 )   \\right|^2   +   \\cdots   +   \\left|   b_n   +   \\lambda_n^{-1 }   x^u ( h_n )   \\right|^2 } .\\ ] ] note that when @xmath43 is deterministic , any superharmonic functional of the form @xmath364 can be replaced with @xmath365 which retains the same harmonicity property , and can be directly computed from an observation of @xmath2 .",
    "+ the stein type estimator of @xmath43 is given by @xmath366,\\ ] ] with @xmath367 i.e. @xmath368_t } { \\vert \\pi_n x \\vert_{l^2([0,t ] , dt ) } ^2 } , \\ ] ] where @xmath369 denotes the orthogonal projection @xmath370 we have @xmath371\\times \\omega , \\p_u \\otimes dt ) } ^2   & = &   - 4   \\e_u \\left [   \\frac{\\delta \\sqrt{f_{n,2-n , b}}}{\\sqrt{f_{n,2-n , b } } } \\right ] \\\\   & = &   ( n-2)^2   \\e_u \\left [   \\frac { 1 } { \\left|   \\lambda_1^{-1 } x ( h_1 )   \\right|^2   +   \\cdots   +   \\left|   \\lambda_n^{-1 }   x ( h_n )   \\right|^2 }   \\right ] \\\\   & = &   ( n-2)^2   \\e_u \\left [   \\vert \\pi_n x \\vert_{l^2([0,t ] , dt ) } ^{-2 }   \\right ] , \\end{aligned}\\ ] ] and @xmath372 , dt ) } ^2   \\right ]   =   \\crb ( \\sigma,\\mu , \\hat{u } )   -   ( n-2)^2   \\e_u \\left [   \\vert \\pi_n x \\vert_{l^2([0,t ] , dt ) } ^{-2 }   \\right ] .\\ ] ] note that the estimator @xmath373_t } { \\vert \\pi_n x \\vert_{l^2([0,t ] , dt ) } ^2 } ,   \\qquad   t\\in [ 0,t],\\ ] ] is of james - stein type , but it is not a shrinkage operator .",
    "another difference with james - stein estimators is that here the denominator consists in a sum of squared gaussians with different variances .",
    "+ given @xmath374 } , \\ldots , ( x_t^n ) _",
    "{ t\\in [ 0,t]}$ ] , @xmath123 independent samples of @xmath18}$ ] , the process @xmath375 is a brownian motion with drift @xmath43 and quadratic variation @xmath376 under @xmath88 , and can be used for both efficient and stein type estimation .",
    "in this section we present numerical simulations which allow us to measure the efficiency of our estimators .",
    "we use the framework of case @xmath82 and the superharmonic functionals constructed as cylindrical functionals in the previous section , and we assume that @xmath199 is deterministic .",
    "+ we work in the independent increment framework of section  [ 2 ] and we additionally assume that @xmath278 is constant , @xmath35 $ ] , i.e. @xmath18}$ ] is a brownian motion with variance @xmath377 , @xmath378 , @xmath35 $ ] , and @xmath379 letting @xmath380 ,   \\quad    n \\geq 1,\\ ] ] i.e. @xmath381 ,   \\quad n \\geq 1,\\ ] ] provides an orthonormal basis @xmath317 of @xmath49 such that @xmath382 is orthogonal in @xmath80 , dt ) $ ] , with @xmath383 solution of .",
    "the estimator of @xmath43 will be given by @xmath384 for simulation purposes we will use @xmath385 , and construct the ( nondrifted ) brownian motion @xmath386}$ ] via the paley - wiener expansion @xmath387 where @xmath388 are independent standard gaussian random variables with unit variance under @xmath88 and @xmath389 in this case we have @xmath390 recall that the improvement obtained in comparison with the efficient estimator @xmath30 is not obtained pathwise , but in expectation .",
    "the gain of the superefficient estimator @xmath391 compared to the efficient estimator @xmath30 is given by @xmath392\\ ] ] as a function of @xmath393 . from and we have @xmath394 , \\ ] ] hence @xmath395 converges to @xmath396 , \\ ] ] as @xmath39 tends to infinity .",
    "the quantity can be evaluated as a gaussian integral to yield . unlike in the classical stein method , we stress that here @xmath397 becomes a free parameter and there is some interest in determining the values of @xmath397 which yield the best performance .",
    "[ prop : limitesestimateur ] for all @xmath398 , and @xmath87 we have @xmath399 as @xmath397 goes to infinity .",
    "let @xmath400 we have @xmath401 , \\ ] ] and by the strong law of large numbers , @xmath402 converges to @xmath403 as @xmath397 goes to infinity , since @xmath404}{n^3 }   =   \\frac{\\pi^2}{4 }   \\lim_{n\\to \\infty }   \\frac{1}{n^3 }   \\sum_{i=1}^n ( 2i-1)^2   =   \\frac{\\pi^2}{3 } .\\ ] ] now for all @xmath405 we have @xmath406   & = & \\e \\left [ \\lambda ( u )   \\left (   \\frac{(n-2)^3}{s_n }   \\right)^2 \\right ] \\\\   & \\leq &   n^2 \\pi^2 \\e \\left [ \\lambda ( u)^2 \\right]^{1/2 } \\e \\left [   \\left (   \\sum_{l=1}^{[n/2 ] }   \\left (   1 - \\frac{l}{n }   +   \\frac{1}{2n }   \\right)^2   \\eta_l^2   \\right)^{-4 } \\right]^{1/2 } \\\\   & \\leq &   n^2   \\frac{4}{\\pi^4 } \\e \\left [ \\lambda ( u)^2 \\right]^{1/2 } \\e \\left [   \\left (   \\sum_{l=1}^{[n/2 ] }   \\eta_l^2   \\right)^{-4 } \\right]^{1/2 } \\\\   & \\leq &   \\frac{4n^2}{\\pi^4 } \\e \\left [ \\lambda ( u)^2 \\right]^{1/2 }   \\left (   \\prod_{k=1}^4   \\left (   [ n/2 ] - 2 k   \\right )   \\right)^{-1/4 } , \\end{aligned}\\ ] ] hence @xmath407 is uniformly integrable in @xmath408 , where @xmath409 $ ] denotes the integer part of @xmath410 .",
    "this concludes the proof .    in the sequel",
    "we choose @xmath411 , @xmath35 $ ] , @xmath412 .",
    "figure  [ g1 ] gives a sample path representation of the process @xmath385 .    in this case , from we have @xmath413 , \\ ] ] from which it follows that @xmath414 converges to @xmath415 , \\ ] ] when @xmath416 tends to infinity , and is equivalent to @xmath417 as @xmath416 tends to @xmath37 .",
    "figure  [ g3 ] represents the gain in percentage of the superefficient estimator @xmath418 compared to the efficient estimator @xmath30 using monte - carlo simulations , i.e. we represent @xmath419 as a function of @xmath351 .",
    "an optimal value @xmath420 of @xmath397 exists in general and is equal to @xmath421 when @xmath422 .",
    "figure  [ g4 ] shows the variation of the gain as a function of @xmath397 and @xmath38 for @xmath423 .",
    "-0.5 cm    figure  [ g5 ] represents the variation of the gain as a function of @xmath397 and @xmath39 .",
    "the next proposition is classical in the framework of gaussian filtering and is needed in section  [ 2 ] for bayes estimation .",
    "its proof is stated for completeness since we did not find it in the literature .",
    "let @xmath10 be a gaussian process with covariance operator @xmath160 and drift @xmath161 , and assume that @xmath2 is a gaussian process with drift @xmath10 and quadratic covariance operator @xmath162 given @xmath10 .",
    "then , conditionally to @xmath2 , @xmath10 has drift @xmath424    for convenience of notation , let @xmath425 for all @xmath426 we have : @xmath427   & = &   \\e\\left [   \\e\\left [   \\exp   \\left (   i   x ( f )   \\right )   \\big |   z   \\right ]   \\right ] \\\\   & = &   \\e\\left [   \\exp   \\left (   i   z ( f )   -   \\frac{1}{2 }   \\langle   f   ,   \\gamma   f \\rangle",
    "\\right ) \\right ] \\\\   & = &   \\exp   \\left (   i   v ( f )   -   \\frac{1}{2 }   \\langle   f   ,   (   \\gamma_\\tau   +   \\gamma   )   f \\rangle",
    "\\right ) , \\end{aligned}\\ ] ] and @xmath428   } \\\\   & = &   \\e\\left [   \\exp   \\left (   i   z ( f )   \\right )   \\e\\left [   \\exp   \\left (   i   x ( g )   \\right )   \\big |   z   \\right ]   \\right ] \\\\   & = &   \\e\\left [   \\exp   \\left (   i   z (   f +   g   )   -   \\frac{1}{2 }   \\langle   g , \\gamma g   \\rangle",
    "\\right ) \\right ] \\\\   & = &   \\exp   \\left (   -   \\frac{1}{2 }   \\langle   f+g ,   \\gamma_\\tau ( f+g )   \\rangle   -   \\frac{1}{2 }   \\langle   g   ,   \\gamma   g   \\rangle   +   i   v ( f+g ) \\right ) \\\\   & = &   \\exp   \\left (   i   v (   ( \\gamma + \\gamma_\\tau ) ^{-1 } \\gamma_\\tau   f   )   +   i   v (   g   +   ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma   f   )   -   \\frac{1}{2 }   \\langle   \\gamma_\\tau f , ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma f   \\rangle \\right . \\\\ & & \\left",
    ".   -   \\frac{1}{2 }   \\langle   g   +   ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma_\\tau   f   ,   ( \\gamma + \\gamma_\\tau )   (   g   +   ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma_\\tau   f   )   \\rangle   \\right ) \\\\   & = &   \\e\\left [   \\exp   \\left (   i   x ( g )   \\right ) \\right . \\\\ & & \\left .   \\exp   \\left (   i   x (   ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma_\\tau f )   +   i   v((\\gamma + \\gamma_\\tau)^{-1}\\gamma f )   -   \\frac{1}{2 }   \\langle   \\gamma_\\tau f   ,   ( \\gamma_\\tau + \\gamma ) ^{-1 }   \\gamma   f   \\rangle",
    "\\right )   \\right ] , \\end{aligned}\\ ] ] which shows that @xmath429 } \\\\   & = &   \\exp   \\left (   i   v (   ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma   f )   +   i",
    "x (   ( \\gamma + \\gamma_\\tau)^{-1}\\gamma_\\tau   f )   -   \\frac{1}{2 }   \\langle   \\gamma_\\tau f   ,   ( \\gamma + \\gamma_\\tau ) ^{-1 }   \\gamma   f \\rangle",
    "\\right ) .\\end{aligned}\\ ] ]      [ ljk2 ] let @xmath169}$ ] be a brownian motion with quadratic variation @xmath430 , @xmath431 , dt ) $ ] , and drift @xmath432}$ ] , @xmath161 , and let @xmath18}$ ] have drift @xmath169}$ ] and quadratic variation @xmath433}$ ] , given @xmath10 .",
    "then , conditionally to @xmath2 , the process @xmath169}$ ] has drift @xmath434 .\\ ] ]                                          r.  wolpert and j.  berger . incorporating prior information in minimax estimation of the mean of a gaussian process . in _ statistical decision theory and related topics , iii , vol . 2 ( west lafayette , ind . , 1981 ) _ , pages 451464 . academic press , new york , 1982 ."
  ],
  "abstract_text": [
    "<S> in this paper we consider the nonparametric functional estimation of the drift of gaussian processes using paley - wiener and karhunen - love expansions . we construct efficient estimators for the drift of such processes , and prove their minimaxity using bayes estimators . </S>",
    "<S> we also construct superefficient estimators of stein type for such drifts using the malliavin integration by parts formula and stochastic analysis on gaussian space , in which superharmonic functionals of the process paths play a particular role . </S>",
    "<S> our results are illustrated by numerical simulations and extend the construction of james - stein type estimators for gaussian processes by berger and wolpert @xcite .    </S>",
    "<S> * key words : * nonparametric drift estimation , stein estimation , gaussian space , malliavin calculus , harmonic analysis . + _ mathematics subject classification : _ 62g05 , 60h07 , 31b05 .    0.7 cm </S>"
  ]
}