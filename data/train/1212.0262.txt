{
  "article_text": [
    "the dynamics of systems with long - range interactions have been intensively studied over the last two decades due to their unusual and intriguing phenomenology , such as the existence of quasi - stationary non - gaussian states with diverging life - times with the number of particles , negative microcanonical heat capacity , inequivalence of ensembles and non - ergodicity  @xcite .",
    "self - gravitating systems  @xcite , non - neutral plasmas  @xcite and models as the ring model  @xcite hamiltonian mean field ( hmf )  @xcite , one - dimensional gravity ( infinite uniform density sheets )  @xcite , two - dimensional gravity ( infinite uniform rods )  @xcite , free electron laser  @xcite and plasma single wave models  @xcite are among many examples of systems with long - range forces .",
    "a pair potential interaction has a long - range if it scales for large distances as @xmath1 with @xmath2 , @xmath3 the interparticle distance and @xmath4 the spatial dimension .",
    "this slow decaying interparticle potential is responsible for the coupling of distant components of the system in such a way that all particles contribute to the dynamics of a given particle .",
    "for out of equilibrium situations , many of these studies rely on molecular dynamics simulations , i.  e.  solving numerically the hamiltonian equations of motion for the @xmath0-particle system .",
    "it is also a well known fact that , under suitable conditions , the statistical description of the dynamics of long range interacting systems is equivalent to the vlasov equation in the @xmath5 limit  @xcite .    here",
    "we present a cuda implementation  @xcite of molecular dynamics ( md ) algorithms on graphics processing units ( gpus ) to solve the hamiltonian equations of motion for such systems , using one and two gpus .",
    "the algorithms used can be efficiently extended for any number of gpus .",
    "molecular dynamics simulations have been extensively used to study many properties of these systems from first principles ( see for instance reference  @xcite and references therein ) .",
    "some numerical parallel algorithms were implemented in the literature with applications ranging from condensed matter to astrophysics  @xcite . although md simulations are widely used in the study of long - range systems , only recently a gpu code of a symplectic integrator was implemented and used by the author and collaborators alongside a gpu vlasov solver to study non - equilibrium phase transitions in the hmf model  @xcite . in such studies simulations",
    "are performed without introducing any simplifying hypothesis such as the particle - mesh technique  @xcite or similar methods commonly used in condensed matter physics  @xcite , with a full n - body method with a computational effort scaling as @xmath6 .",
    "for some models the explicit form of the pair interaction potential allows further simplifications with important reduction in computational time .",
    "the structure of the paper is the following : in section  [ mlri ] we describe the model systems that are studied in this paper .",
    "section  [ algcuda ] presents the main algorithms and how they are implemented in cuda , and in section  [ resdis ] we present the timings and speedups for the three models taken as examples , and discuss how the relative error in the energy behave for each of them .",
    "we close the paper with some concluding remarks in section  [ conc ] .",
    "here we present some simplified models that are discussed in the present work .",
    "the ring model is composed of @xmath0 particles with unit mass on a ring of radius @xmath7 and interacting though a regularized gravitational potential with hamiltonian  @xcite : @xmath8 with @xmath9 a softening parameter introduced to avoid the divergence at zero distance and @xmath10 denotes the position angle of a particle on the circle . by considering the limit for large values of the parameter @xmath9",
    "we obtain the hamiltonian mean field ( hmf ) model with hamiltonian  @xcite : @xmath11 . \\label{hmfham}\\ ] ]    the two - dimensional self - gravitating particles is composed of identical particles with unit mass . by solving the poisson equation in two dimensions",
    "we obtain the hamiltonian  @xcite : @xmath12 , \\label{2dham}\\ ] ] where @xmath13 , @xmath14 , @xmath15 and @xmath16 are the @xmath17 and @xmath18 components of the momentum and the coordinates for the @xmath19-th particle , respectively , and @xmath9 is again a ( small ) parameter used to avoid the divergence of the potential at zero distance .",
    "the standard integration method for the numerical solution of hamilton equations for the model systems described in the previous section is a fourth - order symplectic integrator . here",
    "we adopt the yoshida symplectic integrator that approximates the evolution operator as  @xcite : @xmath20 where @xmath21 is the integration step and @xmath22 , @xmath23 , @xmath24 and @xmath25 are numerical constants obtained in ref .",
    "@xcite , @xmath26 , @xmath27 and @xmath28 with @xmath29 and @xmath30 the kinetic and potential energies respectively and @xmath31 stands for the poisson bracket of @xmath32 with any function it operates on .",
    "the approximation in eq .",
    "( [ yoshint ] ) is time reversible .",
    "a whole integration step is given by the following steps where @xmath33 stand for the force array at positions @xmath34 :    1 .",
    "@xmath35 , 2 .",
    "@xmath36 , 3 .",
    "@xmath37 , 4 .",
    "@xmath38 , 5 .",
    "@xmath39 , 6 .",
    "@xmath40 , 7 .",
    "@xmath41 , 8 .",
    "@xmath42 , 9 .",
    "@xmath43 , 10 .",
    "@xmath44 , 11 .",
    "@xmath45 , 12 .",
    "compute any properties of interest , then go to step ( 2 ) with @xmath46 .",
    "each time step requires three force calculations , being the most demanding step .",
    "we now present the specific cuda implementation for each model in section  [ mlri ] .      from the hamiltonian in eq .",
    "( [ hmfham ] ) the potential energy and the force on particle @xmath19 can be written respectively as @xmath47 , \\label{hmfpot}\\ ] ] and @xmath48 where the components of the `` magnetization '' are given by @xmath49 we note that due to the form of the interaction potential the symplectic integration time for the hmf scales with @xmath0 in contrast to the usual @xmath6 scaling for the other two models below . the most demanding part of an integration step is the computation of the sine and cosine of the position angles for each particle .",
    "for the force in eq .",
    "( [ hmfforce ] ) each @xmath50 and @xmath51 is used twice : to compute @xmath52 and @xmath53 and to obtain the force on each particle from expression  ( [ hmfforce ] ) . to avoid redundant computations their values",
    "are first computed and stored in an array in the gpu global memory taking care to ensure coalesced memory access , which is an important issue in any cuda implementation .",
    "a cuda kernel is composed of blocks each with a given number of threads .",
    "each thread in a block computes the value of the cosine and sine of a given particle and the corresponding values are also stored in shared memory . a reduction procedure",
    "is then used to compute the values of @xmath52 and @xmath53 and the force is obtained using the precomputed values of @xmath51 and @xmath50 .",
    "the potential energy is trivially obtained from eq .",
    "( [ hmfpot ] ) and the kinetic energy is efficiently computed using a straightforward reduction procedure .    for two gpus half of particle data ( position , momenta and force )",
    "is stored in each gpu which then computes the magnetization components for its corresponding number of particles , and their sum give the total magnetization components .",
    "the forces on each particle are the trivially computed for the particles on each gpu , and the subsequent evolution is also performed independently by each gpu for its set of particles .",
    "the computational time for symplectic integration of the ring model scales as @xmath6 .",
    "here we follow a strategy similar to the one described in ref .",
    "@xcite based on a decomposition of the force calculation in tiles as depicted in figure  [ fig1 ] .",
    "the force on particle @xmath19 is obtained from hamiltonian  ( [ ringham ] ) as : @xmath54 this last expression can be rewritten as @xmath55 each @xmath56 and @xmath57 for @xmath58 is computed and stored in global memory in order to avoid computing twice their values .",
    "each tile has @xmath59 ( the number of threads in a block ) particles in the horizontal direction ( index @xmath19 in fig .",
    "[ fig1 ] ) and @xmath59 in the vertical direction ( index @xmath60 ) .",
    "the total number of tiles in each direction is thus @xmath61 .",
    "the algorithm can be expressed as :    1 .",
    "store @xmath62 and the values of @xmath63 and @xmath64 , @xmath65 in shared memory , with @xmath66 the block number , and synchronize threads in the block .",
    "2 .   @xmath67 ; 3 .",
    "store @xmath63 and @xmath64 , @xmath68 in shared memory and synchronize threads in the block ; 4 .   for @xmath68 compute @xmath69 and sum its values to @xmath70",
    "@xmath71 and goto step ( 3 ) while @xmath72 .",
    "newton s third law is not used here as its implementation in a cuda kernel would introduce unnecessary complications with no significant speed gain .",
    "the computation of @xmath73 is performed using the cuda function rsqrt(x ) with significant gain in computing time and no significant loss in accuracy .",
    "for the two - dimensional self - gravitating system the approach is essentially the same as for the ring model described above , except for the number of components for each particle ( two ) and the interpaticle force . in this case ,",
    "a significant gain in speed without significantly compromising accuracy consists in using double precision for the storage of all data in global memory but storing each component of the position in single precision shared memory before computing the force on each particle , which allows to load more blocks concomitantly on the gpu , and therefore augmenting occupancy .",
    "the force between two given particles is then computed in single precision but added in a double precision variable to determine the total force on a given particle .",
    "all remaining computations are performed in double precision . for determining the timings and for comparison purposes the same is done for the cpu code .    for two gpus",
    ", each one computes separately the forces for half of the particles , which contrary to the hmf model requires the positions for all particles .",
    "therefore each gpu stores in global memory half on the momenta , half of the forces but all the particle position coordinates . as for the hmf model the time evolution",
    "is performed independently on each gpu for its respective set of particles , and only half of the positions on each gpu is up to date after a time step . to compute the force we first copy half of the updated values from one gpu to the other using an asynchronous memory copy",
    "then each gpu computes half of the forces with @xmath74 tile in the horizontal and @xmath75 tiles in the vertical directions .",
    "the whole computation is synchronized after all forces are evaluated using stream synchronization , as shown in fig .",
    "[ fig2 ] . a single partial integration step composed of a force calculation and free drift with constant momentum and a force increment with constant position ( steps 1 , 2 and 3 in section  [ algcuda ] for instance )",
    "can be summarized for both the ring and 2d self - gravitating model as ( each gpu has a stream defined for it ) :    1 .",
    "asynchronous copy of the position components of particles @xmath76 from gpu 0 to gpu 1",
    "asynchronous copy of the position components of particles @xmath77 from gpu 1 to gpu 0 .",
    "stream synchronization for each gpu .",
    "4 .   launch a kernel to compute the components of @xmath78 , @xmath79 .",
    "5 .   launch a kernel to compute the components of @xmath78 , @xmath80 .",
    "stream synchronization for each gpu",
    "launch a kernel to update halt of the momenta and half of the position coordinates on gpu 0 .",
    "launch a kernel to update halt of the momenta and half of the position coordinates on gpu 1 .",
    "stream synchronization for each gpu .    the asynchronous copy between gpus in steps ( 1 ) and ( 2 ) above is efficiently handled by the cuda routine cudamemcpypeerasync .",
    "the computer used for the simulations is an i7 - 2600/ 3.40 ghz and 16 gb of ram .",
    "the gpu is a geforce gtx 690 dual with a kepler architecture , 2048 mbytes of memory and 1536 cuda cores for each gpu .",
    "results are presented using one and both units tables  [ tab1 ] , [ tab2 ] and  [ tab3 ] show the timings for a complete time step for the hmf , ring and 2d gravity models , respectively .",
    "all cpu implementations were optimized on a single cpu core but parallel implementations on many cores on the cpu are also possible with a maximum speedup given by the number of cores .",
    "the speedups obtained range from 23 to 73 for the hmf model , 104 to 141 for the ring model and 53 to 100 for the 2d self - gravitating system .",
    "the greater the number of particles the greater the speedup due to a higher occupancy of the gpu cores . for the same reason using more",
    "than one gpu becomes more advantageous for a large number of particles .",
    "it is interesting to note that the smaller speedups were obtained for the hmf model .",
    "a similar result is obtained in ref .",
    "@xcite for the solution of the vlasov equation and is a consequence of the scaling property with @xmath0 of the computational time and the fact the cpu code being already highly optimized .    to assess the accuracy of the present approach , we consider all three systems and compare the relative error in the energy @xmath81 for the different implementations . for the sake of comparisons , the error for the cpu case",
    "is obtained using double precision in all steps .",
    "for all three system we consider an initial state with all particles initially at rest .",
    "for the hmf and ring models the initial state is spatially homogeneous .",
    "figures  [ fig3 ] and  [ fig5 ] show the plot of the kinetic and potential energies and the corresponding error @xmath82 for a time window large enough to encompass the initial violent relaxation of the system which is more pronounced for the hmf model .",
    "for the latter , the error for the cpu and gpu implementation are indistinguishable in the plot . for the ring model",
    "both cases are always very close .",
    "thence computing the force between the two particles using single precision arithmetic do not compromise the overall accuracy .",
    "for the two - dimensional self - gravitating system , we consider all particles initially at rest and homogeneously distributed on a circular shell of inner and outer radius @xmath83 and @xmath84 .",
    "the system then undergoes a violent relaxation towards a quasi - stationary state with some damping oscillations as show in fig .",
    "figure  [ fig8 ] shows the plot of the error for a cpu implementation ( in fortran ) using double precision for all steps , and the error for the gpu implementation also in full double precision , and gpu implementation using single precision for the computation of forces between pairs of particles , as described above yielding the same order of magnitude for the error .",
    "the error grows as the particles get closer during the violent relaxation with a threshold at the same values for the two gpu implementations .",
    ".run times in seconds for a single complete time step for the hmf model with @xmath0 particles for cpu , and the gpu used as single and dual units . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "we presented implementations in one and two gpus of a forth order symplectic integrator for three different systems with long - range interactions .",
    "those systems have been extensively studied in the literature and the present implementations allowed a more extensive investigation of properties such as the nature of non - equilibrium phase transitions and non - ergodic behavior in the hmf model  @xcite .",
    "shared memory is used to allow faster memory access crucial in gpu implementations .",
    "since it is a very limited resource using single precision floating point storage optimizes its use .",
    "some steps of the computation , such as the force between pairs of particles , or the inverse of the interparticle distance , can be performed in single floating point precision without compromising the overall error in the simulation .",
    "the speedups obtained range typically from 30 to 140 depending on the system and the number of particles , simulations that would be unfeasible in any reasonable time using a typical cpu .",
    "the generalization of the present approach to multi - gpu ( more than two ) and other @xmath0 particle systems is straightforward .",
    "the author would like to thank cnpq and capes ( brazil ) for partial financial support .",
    "99 a.  campa , t.  dauxois and s.  ruffo , phys .  rep .",
    "* 480 * ( 2009 ) 57 .",
    ", t.  dauxois , s.  ruffo , e.  arimondo and m.  wilkens ( eds . ) , springer ( berlin , 2002 ) , a.  campa , a.  giansanti , g.  morigi and f.  s.  labini ( eds . ) , aip conf.proceedings vol .  970 ( 2008 ) .",
    ", t.  dauxois , s.  ruffo and l.  f.  cugliandolo eds , oxford univ .  press ( oxford , 2010 ) .",
    "t.  m.  rocha filho , a.  figueiredo and m.  a.  amato , phys .",
    "* 95 * ( 2005 ) 190601 .",
    "a.  figueiredo , t.  m.  rocha filho and m.  a.  amato , europhys .",
    "* 83 * ( 2008 ) 30011 .",
    "f.  p.  c.  benetti , t.  n.  teles , r.  pakter and y.  levin , cond - mat:1202.1810 .",
    "t.  padmanabhan , phys .  rep .",
    "* 188 * ( 1990 ) 285 .",
    "y.  levin , r.  pakter and t.  n.  teles , _ phys .",
    "_ * 100 * ( 2008 ) 040604 .",
    "y.  levin , r.  pakter and t.  n.  teles , _ phys .",
    "e _ * 78 * ( 2008 ) 021130 .",
    "y.  sota , o.  iguchi , m.  morikawa , t.  tatekawa and k.  i.  maeda , phys .",
    "e * 64 * ( 2001 ) 056133 .",
    "t.  tatekawa , f.  bouchet , t.  dauxois and s.  ruffo , phys .",
    "e * 71 * ( 2005 ) 056111 .",
    "m.  antoni and s.  ruffo , phys .",
    "e * 52 * ( 1995 ) 2361 .",
    "t.  n.  teles , y.  levin and r.  pakter , mon .  not .",
    "r.  atron .",
    "* 417 * ( 2011 ) l21 .",
    "m.  joyce and t.  worrakitpoonpon , phys .",
    "e * 84 * ( 2011 ) 011139 .",
    "k.  r.  yawn and b.  n.  miller , phys .",
    "e * 68 * ( 2003 ) 056120 .",
    "t.  n.  teles , y.  levin , r.  pakter and f.  b.  rizzato , j.  stat .",
    "( 2010 ) p05007 .",
    "r.  bonifacio , f.  casagrande , g.  cerchioni , l.  de salvo souza , p.  pierini and n.  piovella , riv .",
    "nuovo cimento * 13 * ( 1990 ) 1 .",
    "j.  l.  tennyson , j.  d.  meiss and p.  j.  morrison , physica d * 71 * ( 1994 ) 1 .",
    "w.  braun and k.  hepp , commun .",
    "* 56 * ( 1977 ) 101 .",
    "nvidia , cuda programming guide , ver .  4.0 , 2011 .",
    "a.  moore and a.  c.  quillen , new astronomy * 16 * ( 2011 ) 445 . c.  r.  trott , l.  winterfeld and p.  s.  crozier , arxiv:1009.4330 ( 2011 ) .",
    "j.  bdorf and s.  p.  zwart , eur .",
    "j.  spiec .",
    "* 210 * ( 2012 ) 201 .",
    "d.  c.  rapaport , comp .",
    "comm .  * 182 * ( 2011 ) 926 .",
    "i.  v.  morozov , a.  m.  kazennov , r.  g.  bystryi , g.  e.  norman , v.  v.  pisarev and v.  v.  stegailov , comp .",
    "* 182 * ( 2011 ) 1974 .",
    "a.  sunarso , t.  tsuji and s.  chono , j.  comp .",
    "* 229 * ( 2010 ) 5486 .",
    "t.  m.  rocha filho , m.  a.  amato , b.  a.  mello and a.  figueiredo , phys .",
    "e * 84 * ( 2011 ) 041121 .",
    "t. m. rocha filho , m. a. amato , and a. figueiredo , phys .",
    "e * 85 * ( 2012 ) 062103 .",
    "t.  m.  rocha filho , comp .  phys .",
    "* 184 * ( 2013 ) 34 .",
    "m.  fellhauer , particle - mesh technique and superbox , in _ the cambridge n - body lectures _ , s.  j.  aarseth , c.  a.  tout and r.  a.  mardling ( eds . ) , springer ( berlin , 2008 ) . j.  a.  baker and j.  d.  hisrt , molecular informatics * 30 * ( 2011 ) 498 . h.  yoshida , phys .",
    "a * 150 * ( 1990 ) 262 .",
    "l.  nyland , m.  harris and j.  prins , fast n - body simulation with cuda , in _",
    "gpu gems 3 _ , h.  nguyen ed , addison wesley ( new york , 2008 ) .",
    "a. figueiredo , t. m. rocha filho , and m. a. amato , z. t. oliveira jr , r. matsus , arxiv : 1208.4868 ."
  ],
  "abstract_text": [
    "<S> we present implementations of a fourth - order symplectic integrator on graphic processing units for three @xmath0-body models with long - range interactions of general interest : the hamiltonian mean field , ring and two - dimensional self - gravitating models . </S>",
    "<S> we discuss the algorithms , speedups and errors using one and two gpu units . </S>",
    "<S> speedups can be as high as 140 compared to a serial code , and the overall relative error in the total energy is of the same order of magnitude as for the cpu code . the number of particles used in the tests range from 10,000 to 50,000,000 depending on the model .    molecular dynamics ; symplectic integrator ; long - range interaction ; </S>"
  ]
}