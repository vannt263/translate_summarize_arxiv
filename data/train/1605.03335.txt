{
  "article_text": [
    "prediction and variable selection are two important goals in many contemporary large - scale problems .",
    "many regularization methods in the context of penalized empirical risk minimization have been proposed to select important covariates .",
    "see , for example , fan & lv ( 2010 ) for a review of some recent developments in high - dimensional variable selection . penalized empirical risk minimization has two components : empirical risk for a chosen loss function for prediction , and a penalty function on the magnitude of parameters for reducing model complexity .",
    "the loss function is often chosen to be convex .",
    "the inclusion of the regularization term helps prevent overfitting when the number of covariates @xmath1 is comparable to or exceeds the number of observations @xmath2 .",
    "generally speaking , two classes of penalty functions have been proposed in the literature : convex ones and concave ones .",
    "when a convex penalty such as the lasso penalty @xcite is used , the resulting estimator is a well - defined global optimizer . for the properties of @xmath0-regularization methods ,",
    "see , for example , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "in particular , @xcite proved that using the @xmath0-penalty leads to estimators satisfying the oracle inequalities under the prediction loss and @xmath3-loss , with @xmath4 , in high - dimensional nonparametric regression models .",
    "an oracle inequality means that with an overwhelming probability , the loss of the regularized estimator is within a logarithmic factor , a power of @xmath5 , of that of the oracle estimator , with the power depending on the chosen estimation loss . despite these nice properties",
    ", the @xmath0-penalty tends to yield a larger model than the true one for optimizing predictions , and many of the selected variables may be insignificant , showing that the resulting method may not be ideal for variable selection .",
    "the relatively large model size also reduces the interpretability of the selected model .",
    "concave penalties , on the other hand , have been shown to lead to nice variable selection properties .",
    "the oracle property was introduced in @xcite to characterize the performance of concave regularization methods , in relation to the oracle procedure knowing the true sparse model in advance . in fixed dimensions ,",
    "concave regularization has been shown to have the oracle property , recovering the true model with asymptotic probability one .",
    "this work has been extended to higher dimensions in different contexts , and the key message is the same .",
    "see , for example , @xcite , @xcite , and @xcite .",
    "in particular , the weak oracle property , a surrogate of the oracle property , was introduced in @xcite . when @xmath6 , it is generally difficult to study the properties of the global optimizer for concave regularization methods .",
    "thus , most studies have focused on some local optimizer that has appealing properties in high - dimensional settings .",
    "the sampling properties of the global optimizers for these methods are less well - understood in high dimensions .    in this article",
    ", we characterize theoretically the global optimizer of the regularization method with the combined @xmath0 and concave penalty , in the setting of the high - dimensional linear model .",
    "we prove that the resulting estimator combines the prediction power of the @xmath0-penalty and the variable selection power of the concave penalty . on the practical side",
    ", the @xmath0-penalty contributes the minimum amount of regularization necessary to remove noise variables for achieving oracle prediction risk , while the concave penalty incorporates additional regularization to control model sparsity . on the theoretical side ,",
    "the use of an @xmath0-penalty helps us to study the various properties of the global optimizer . specifically , we prove that the global optimizer enjoys the oracle inequalities under the prediction loss and @xmath3-loss , with @xmath4 , as well as an asymptotically vanishing bound on false sign rate .",
    "we also establish its oracle risk inequalities under various losses , as well as the sampling properties of computable solutions .",
    "in addition , we show that the refitted least - squares estimator can enjoy the oracle property , in the context of @xcite .",
    "these results are also closely related to those in @xcite .",
    "our work complements theirs in three important respects .",
    "first , the bound on the number of false positives in @xcite is generally of the same order as the true model size , while our bound on the stronger measure of the rate of false signs can be asymptotically vanishing . second , our estimation and prediction bounds depend only on the universal regularization parameter for the @xmath0-component and are free of the regularization parameter @xmath7 for the concave component , whereas the bounds in @xcite generally depend on @xmath7 alone .",
    "third , our oracle risk inequalities are new and stronger than those for losses , since the risks involve the expectations of losses and thus provide a more complete view of the stability of the method . it is unclear whether the concave method alone may enjoy similar risk bounds .",
    "our proposal shares a similar spirit to that in @xcite , who proposed a combination of @xmath8- and @xmath0-penalties for variable selection and studied its properties in linear regression with fixed dimensionality . their new penalty yields more stable variable selection results than the @xmath8-penalty , and outperforms both @xmath8- and @xmath0-penalties in terms of variable selection , while maintaining good prediction accuracy .",
    "our theoretical results and numerical study reveal that this advantage still exists in high dimensions and for more general concave penalties .",
    "our work differs from theirs in two main respects : we provide more complete and unified theory in ultra - high dimensional settings , and we consider a large class of concave penalties with only mild conditions on their shape .",
    "the idea of combining strengths of different penalties has also been exploited in , for example , @xcite .",
    "consider the linear regression model @xmath9 where @xmath10 is an @xmath2-dimensional vector of responses , @xmath11 is an @xmath12 design matrix , @xmath13 is an unknown @xmath1-dimensional vector of regression coefficients , and @xmath14 is an @xmath2-dimensional vector of noises .",
    "we are interested in variable selection when the true regression coefficient vector @xmath15 has many zero components .",
    "the main goal is to effectively identify the true underlying sparse model , that is , the support @xmath16 , with asymptotic probability one , and to efficiently estimate the nonzero regression coefficients @xmath17 s .",
    "a popular approach to estimating sparse @xmath18 is penalized least squares , which regularizes the conventional least - squares estimation by penalizing the magnitude of parameters @xmath19 .",
    "a zero component of the resulting estimate indicates that the corresponding covariate @xmath20 is screened from the model .",
    "penalized least - squares estimation minimizes the objective function @xmath21 over @xmath22 , where we use the compact notation @xmath23 with @xmath24 , and @xmath25 , @xmath26 , is a penalty function indexed by the regularization parameter @xmath27 .",
    "the lasso ( tibshirani , 1996 ) corresponds to the @xmath0-penalty @xmath28 . as shown in bickel et al .",
    "( 2009 ) , the lasso enjoys the oracle inequalities for prediction and estimation , but it tends to yield large models .",
    "concave penalties have received much attention due to their oracle properties . yet , as discussed in ",
    "[ sec1 ] , the sampling properties of the global optimizer for concave regularization methods are relatively less well - understood in high dimensions . to overcome these difficulties , we suggest combining the @xmath0-penalty @xmath29 with a concave penalty @xmath25 , and study the resulting regularization problem @xmath30 where @xmath31 for some positive constant @xmath32 . throughout the paper ,",
    "we fix such a choice of the universal regularization parameter for the @xmath0-penalty , and the minimizer of ( [ e006 ] ) is implicitly referred to as the global minimizer .",
    "the @xmath0-component @xmath33 helps study the global minimizer of ( [ e006 ] ) , and reflects the minimum amount of regularization for removing the noise in prediction .",
    "the concave component @xmath34 serves to adapt the model sparsity for variable selection .",
    "to understand why the combination of @xmath0- and concave penalties can yield better variable selection than can the @xmath0-penalty alone , we consider the hard - thresholding penalty @xmath35 , @xmath36 .",
    "assume that each covariate @xmath20 is rescaled to have @xmath37-norm @xmath38 .",
    "let @xmath39 be the global minimizer of ( [ e006 ] ) with @xmath40 .",
    "the global optimality of @xmath41 entails that each @xmath42 is the global minimizer of the corresponding univariate penalized least - squares problem along the @xmath43th coordinate .",
    "all these univariate problems share a common form , with generally different scalar @xmath44 s , @xmath45 since all covariates have @xmath37-norm @xmath38 .",
    "simple calculus shows that the solution in ( [ e010 ] ) is @xmath46 so the resulting estimator has the same feature as the hard - thresholded estimator : each component is either zero or of magnitude larger than @xmath7 .",
    "this provides an appealing distinction between insignificant covariates , whose coefficients are zero and should be estimated as such , and significant covariates , whose coefficients are significantly nonzero and should be estimated as nonzero , improving the variable selection performance of soft - thresholding by @xmath0-penalty .",
    "the hard - thresholding feature is shared by many other penalty functions , as now shown .",
    "[ prop1 ] assume that @xmath47 , @xmath36 , is increasing and concave with @xmath48 on @xmath49 $ ] , @xmath50 for some @xmath51 , and @xmath52 decreasing on @xmath53 $ ] .",
    "then any local minimizer of ( [ e006 ] ) that is a global minimizer in each coordinate has the hard - thresholding feature that each component is either zero or of magnitude larger than @xmath54 .",
    "although we used the derivatives @xmath55 and @xmath56 in the above proposition , the results continue to hold if we replace @xmath57 with the subdifferential of @xmath58 , and @xmath59 with the local concavity of @xmath47 at point @xmath60 , when the penalty function is nondifferentiable at @xmath60 ( lv & fan , 2009 ) .",
    "the hard - thresholding penalty @xmath61 satisfies conditions of proposition [ prop1 ] , with @xmath62 .",
    "this class of penalty functions also includes , for example , the @xmath8-penalty and the smooth integration of counting and absolute deviation penalty ( lv & fan , 2009 ) , with suitably chosen @xmath51 and tuning parameters .",
    "we consider a wide range of error distributions for the linear model ( [ e001 ] ) . throughout this paper",
    ", we make the following assumption on the distribution of model error @xmath63 : @xmath64 where @xmath65 is some arbitrarily large , positive constant depending only on @xmath32 , the constant defining @xmath66 .",
    "this condition was imposed in @xcite , who showed for independent @xmath67 that gaussian errors and bounded errors satisfy ( [ e024 ] ) without any extra assumption , and that light - tailed error distributions satisfy ( [ e024 ] ) with additional mild assumptions on the design matrix @xmath68 .    without loss of generality , we assume that only the first @xmath69 components of @xmath18 are nonzero , where the true model size @xmath69 can diverge with the sample size @xmath2 .",
    "write the true regression coefficient vector as @xmath70 with @xmath71 the subvector of all nonzero coefficients and @xmath72 , and let @xmath73 .",
    "we impose the following conditions on the design matrix and penalty function , respectively .",
    "[ cond1 ] for some positive constant @xmath74 , @xmath75 and @xmath76 where @xmath77 with @xmath78 and @xmath79 the subvector of @xmath80 consisting of the components with the @xmath69 largest absolute values .",
    "[ cond2 ] the penalty @xmath47 satisfies the conditions of proposition [ prop1 ] with @xmath81 , and @xmath82 .",
    "the first part of condition [ cond1 ] is a mild sparse eigenvalue condition , and the second part combines the restricted eigenvalue assumptions in @xcite , which were introduced for studying the oracle inequalities for the lasso estimator and dantzig selector @xcite . to see the intuition for ( [ 002 ] ) , recall that the ordinary least - squares estimation requires that the gram matrix @xmath83 be positive definite ,",
    "that is , @xmath84 in the high - dimensional setting @xmath85 , condition ( [ 001 ] ) is always violated .",
    "condition [ cond1 ] replaces the norm @xmath86 in the denominator of ( [ 001 ] ) with the @xmath37-norm of only a subvector of @xmath87 .",
    "condition [ cond1 ] also has an additional bound involving @xmath88 .",
    "this is needed only when dealing with the @xmath3-loss with @xmath89 $ ] . for other losses",
    ", the bound can be relaxed to @xmath90 for simplicity , we use the same notation @xmath91 in these bounds .    in view of the basic constraint ( [ e012 ] ) , the restricted eigenvalue assumptions in ( [ 002 ] )",
    "can be weakened to other conditions such as the compatibility factor or the cone invertibility factor @xcite .",
    "we adopt the assumptions in @xcite to simplify our presentation .",
    "condition [ cond2 ] ensures that the concave penalty @xmath47 satisfies the hard - thresholding property , requires that its tail should be relatively slowly growing , and puts a constraint on the minimum signal strength .",
    "in this section , we study the sampling properties of the global minimizer @xmath41 of ( [ e006 ] ) with @xmath1 implicitly understood as @xmath92 in all bounds . to evaluate the variable selection performance",
    ", we consider the number of falsely discovered signs @xmath93 which is a stronger measure than the total number of false positives and false negatives .",
    "[ thm1 ] assume that conditions [ cond1][cond2 ] and deviation probability bound ( [ e024 ] ) hold , and that @xmath25 is continuously differentiable .",
    "then the global minimizer @xmath41 of ( [ e006 ] ) has the hard - thresholding property stated in proposition [ prop1 ] , and with probability @xmath94 , satisfies simultaneously that @xmath95 , \\\\ \\label{e033 } { \\mathrm{fs}}({\\widehat{\\beta } } ) & = o\\{\\kappa^{-4 } ( \\lambda_0/\\lambda)^2 s\\}.\\end{aligned}\\ ] ] if in addition @xmath96 , then with probability @xmath94 , it also holds that @xmath97 and @xmath98 , where @xmath99 is the @xmath100 submatrix of @xmath68 corresponding to @xmath69 nonzero @xmath101 s .    from theorem [ thm1 ]",
    ", we see that if @xmath7 is chosen such that @xmath102 , then the number of falsely discovered signs @xmath103 is of order @xmath104 and thus the false sign rate @xmath105 is asymptotically vanishing .",
    "in contrast , @xcite showed that under the restricted eigenvalue assumptions , the lasso estimator , with the @xmath0-component @xmath33 alone , generally gives a sparse model with size of order @xmath106 , where @xmath107 is the largest eigenvalue of the gram matrix @xmath108 .",
    "this entails that the false sign rate for the lasso estimator can be of order @xmath109 , which does not vanish asymptotically .",
    "similarly , @xcite proved that the number of false positives of the concave regularized estimator is generally of order @xmath110 , which means that the false sign rate can be asymptotically nonvanishing .",
    "the convergence rates in oracle inequalities ( [ e034])([e035 ] ) , involving both sample size @xmath2 and dimensionality @xmath1 , are the same as those for the @xmath0-component alone in @xcite , and are consistent with those for the concave component alone in @xcite .",
    "a distinctive feature is that our estimation and prediction bounds in ( [ e034])([e035 ] ) depend only on the universal regularization parameter @xmath31 for the @xmath0-component , and are independent of the regularization parameter @xmath7 for the concave component .",
    "in contrast , the bounds in @xcite generally depend on @xmath7 alone .",
    "the logarithmic factor @xmath5 reflects the general price one needs to pay to search for important variables in high dimensions .",
    "in addition , when the signal strength is stronger and the regularization parameter @xmath7 is chosen suitably , with the aid of the concave component , we have a stronger variable selection result of sign consistency than using @xmath0-penalty alone , in addition to the oracle inequality .",
    "thanks to the inclusion of the @xmath0-component , another nice feature is that our theory analyzes the sampling properties on the whole parameter space @xmath111 , the full space of all possible models , in contrast to the restriction to the union of lower - dimensional coordinate subspaces such as in @xcite .",
    "the bound on the @xmath112-estimation loss in theorem [ thm1 ] involves @xmath113 , which is bounded from above by @xmath114 .",
    "the former bound is in general tighter than the latter one . to see this ,",
    "let us consider the special case when all column vectors of the @xmath115 subdesign matrix @xmath116 have equal pairwise correlation @xmath117 .",
    "then the gram matrix takes the form @xmath118 . by the sherman  morrison ",
    "woodbury formula , we have @xmath119 , which gives @xmath120 \\leq 2 ( 1-\\rho)^{-1}.\\ ] ] it is interesting to observe that the above matrix @xmath121-norm has a dimension - free upper bound .",
    "thus in this case , the bound on @xmath112-estimation loss becomes @xmath122 $ ] .    due to the presence of the @xmath0-penalty in ( [ e006 ] ) , the resulting global minimizer @xmath41 characterized in theorem [ thm1 ]",
    "may not have the oracle property in the context of @xcite .",
    "this issue can be resolved using the refitted least - squares estimator on the support @xmath123 .",
    "[ cor1 ] assume that all conditions of theorem [ thm1 ] hold , and let @xmath124 be the refitted least - squares estimator given by covariates in @xmath123 , with @xmath41 the estimator in theorem [ thm1 ]",
    ". then with probability @xmath94 , @xmath124 equals the oracle estimator , and has the oracle property if the oracle estimator is asymptotic normal .",
    "corollary [ cor1 ] follows immediately from the second part of theorem [ thm1 ] .",
    "additional regularity conditions ensuring the asymptotic normality of the oracle estimator can be found in , for example , theorem 4 in @xcite .",
    "[ thm2 ] assume that conditions of theorem [ thm1 ] hold , with @xmath67 independent and identically distributed as @xmath125 .",
    "then the regularized estimator @xmath41 in theorem [ thm1 ] satisfies that for any @xmath126 , @xmath127 , \\quad q \\in [ 1 , 2 ] , \\\\ \\label{e055 } e \\{\\emph{\\mbox{fs}}({\\widehat{\\beta}})\\ } & = o\\{\\kappa^{-4 } ( \\lambda_0/\\lambda)^2 s + \\lambda^{-2 } m_{2 , \\tau } + ( \\gamma \\lambda_0/\\lambda^2 + s ) p^{-c_0}\\},\\end{aligned}\\ ] ] where @xmath128 denotes tail moment and @xmath129 .",
    "if in addition @xmath96 , then we also have @xmath130 and @xmath131 .",
    "observe that @xmath66 enters all bounds for the oracle risk inequalities , whereas @xmath7 enters only the risk bound for the variable selection loss .",
    "this again reflects the different roles played by the @xmath0-penalty and concave penalty in prediction and variable selection .",
    "the estimation and prediction risk bounds in ( [ e052])([e054 ] ) as well as the variable selection risk bound in ( [ e055 ] ) can have leading orders given in their first terms . to understand this , note that each of these first terms is independent of @xmath132 and @xmath133 , and the remainders in each upper bound can be sufficiently small , since @xmath132 and @xmath65 can be chosen arbitrarily large .",
    "in fact , for bounded error @xmath134 with range @xmath135 $ ] , taking @xmath136 makes the tail moments @xmath137 vanish . for gaussian error @xmath138 , by the gaussian tail probability bound",
    ", we can show that @xmath139 $ ] for positive integer @xmath140 .",
    "in general , the tail moments can have sufficiently small order by taking a sufficiently large @xmath132 diverging with @xmath2 .",
    "all terms involving @xmath133 can also be of sufficiently small order by taking a sufficiently large positive constant @xmath32 in @xmath66 ; see ( [ e024 ] ) .",
    "our new oracle risk inequalities complement the common results on the oracle inequalities for losses .",
    "the inclusion of the @xmath0-component @xmath29 stabilizes prediction and variable selection , and leads to oracle risk bounds .",
    "it is , however , unclear whether the concave method alone can enjoy similar risk bounds .      in  [ sec3.3 ]",
    "we have shown that the global minimizer for combined @xmath0 and concave regularization can enjoy the appealing asymptotic properties .",
    "such a global minimizer , however , may not be guaranteed to be found by a computational algorithm due to the general nonconvexity of the objective function in ( [ e006 ] ) .",
    "thus a natural question is whether these nice properties can be shared by the computable solution by any algorithm , where a computable solution is typically a local minimizer .",
    "zhang & zhang ( 2012 ) showed that under regularity conditions , any two sparse local solutions can be close to each other .",
    "this result along with the sparsity of the global minimizer in theorem [ thm1 ] entails that any sparse computable solution , in the sense of being a local minimizer , can be close to the global minimizer , and thus can enjoy properties similar to the global minimizer .",
    "the following theorem establishes these results for sparse computable solutions .",
    "[ thm3 ] let @xmath41 be a computable local minimizer of ( [ e006 ] ) that is a global minimizer in each coordinate produced by any algorithm satisfying @xmath141 and @xmath142 , @xmath143 , and @xmath144 for some positive constants @xmath145 and sufficiently large positive constant @xmath146 .",
    "then under conditions of theorem [ thm1 ] , @xmath41 has the same asymptotic properties as for the global minimizer in theorem [ thm1 ] .    for practical implementation of method in ( [ e006 ] )",
    ", we employ the path - following coordinate optimization algorithm ( fan & lv , 2011 ; mazumder et al . , 2011 ) and choose the initial estimate as the lasso estimator @xmath147 with the regularization parameter tuned to minimize the cross - validated prediction error .",
    "an analysis of the convergence properties of such an algorithm was presented by @xcite .",
    "the use of the lasso estimator as the initial value has also been exploited in , for example , zhang & zhang ( 2012 ) . with the coordinate optimization algorithm",
    ", one can obtain a path of sparse computable solutions that are global minimizers in each coordinate .",
    "theorem [ thm3 ] suggests that a sufficiently sparse computable solution with small correlation between the residual vector and all covariates can enjoy desirable properties .",
    "we simulated 100 data sets from the linear regression model ( [ e001 ] ) with @xmath148 and @xmath149 0@xmath15025 .",
    "for each simulated data set , the rows of @xmath68 were sampled as independent and identically distributed copies from @xmath151 with @xmath1520@xmath1505@xmath153 .",
    "we considered @xmath154 and @xmath155 , and set @xmath156 as @xmath1570@xmath1505@xmath158 0@xmath1507@xmath1591@xmath1502@xmath1590@xmath1509@xmath158 0@xmath1503@xmath158 0@xmath15055@xmath160 . for each data",
    "set , we employed the lasso , combined @xmath0 and the smoothly clipped absolute deviation @xcite , combined @xmath0 and hard - thresholding , and combined @xmath0 and the smooth integration of counting and absolute deviation penalties to produce a sparse estimate .",
    "the minimax concave penalty in @xcite performed very similarly to the smoothly clipped absolute deviation penalty , so we omit its results to save space .",
    "the tuning parameters were selected using bic .",
    "[ tab1 ]    @xmath0+scad , combined @xmath0 and smoothly clipped absolute deviation ; @xmath0+hard , combined @xmath0 and hard - thresholding ; @xmath0+sica , combined @xmath0 and smooth integration of counting and absolute deviation ; pe , prediction error ; fp , number of false positives ; fn , number of false negatives .",
    "we considered six performance measures for the estimate @xmath41 : the prediction error , @xmath37-loss , @xmath0-loss , @xmath112-loss , the number of false positives , and the number of false negatives .",
    "the prediction error is defined as @xmath161 , with @xmath162 an independent observation , which was calculated based on an independent test sample of size 10,000 .",
    "the @xmath3-loss for estimation is @xmath163 .",
    "a false positive means a selected covariate outside the true sparse model @xmath164 , and a false negative means a missed covariate in @xmath164 .",
    "table [ tab1 ] lists the results under different performance measures .",
    "the combined @xmath0 and smoothly clipped absolute deviation , combined @xmath0 and hard - thresholding , and combined @xmath0 and smooth integration of counting and absolute deviation all performed similarly to the oracle procedure , outperforming the lasso .",
    "when the sample size increases , the performance of all methods tends to improve .",
    "although theoretically the oracle inequalities for the @xmath0-penalty and combined @xmath0 and concave penalty can have the same convergence rates , the constants in these oracle inequalities matter in finite samples .",
    "this explains the differences in prediction errors and other performance measures in table [ tab1 ] for various methods .",
    "we also compared our method with the concave penalty alone .",
    "simulation studies suggest that they have similar performance , except that our method is more stable .",
    "to illustrate this , we compared the smoothly clipped absolute deviation with combined @xmath0 and the smoothly clipped absolute deviation .",
    "boxplots of different performance measures by the two methods showed that the latter reduces the outliers and variability , and thus stabilizes the estimate .",
    "this result reveals that the same advantage as advocated in @xcite remains true in high dimensions , with more general concave penalties .",
    "we applied our method to the lung cancer data originally studied in gordon et al .",
    "( 2002 ) and analyzed in fan & fan ( 2008 ) .",
    "this consists of 181 tissue samples , with 31 from the malignant pleural mesothelioma of the lung , and 150 from the adenocarcinoma of the lung .",
    "each sample tissue is described by 12533 genes .    to better evaluate the suggested method , we randomly split the 181 samples into a training set and a test set such that the training set consists of 16 samples from the malignant pleural mesothelioma class and 75 samples from the adenocarcinoma class .",
    "correspondingly , the test set has 15 samples from the malignant pleural mesothelioma class and 75 samples from the adenocarcinoma class . for each split , we employed the same methods as in  [ sec4 ] to fit the logistic regression model to the training data , and then calculated the classification error using the test data .",
    "the tuning parameters were selected using the cross - validation .",
    "we repeated the random splitting 50 times , and the means and standard errors of classification errors were 2@xmath150960 ( 0@xmath150254 ) for the lasso , 3@xmath150080 ( 0@xmath150262 ) for combined @xmath0 and the smoothly clipped absolute deviation , 2@xmath150960 ( 0@xmath150246 ) for combined @xmath0 and hard - thresholding , and 2@xmath150980 ( 0@xmath150228 ) for combined @xmath0 and the smooth integration of counting and absolute deviation .",
    "we also calculated the median number of variables chosen by each method : 19 for the first one , 11 for the second one , 11 for the third one , and 12 for the fourth one ; the mean model sizes are almost the same as the medians . for each method , we computed the percentage of times each gene was selected , and list the most frequently chosen @xmath165 genes in the supplementary material , with @xmath165 equal to the median model size by the method .",
    "the sets of genes selected by the combined @xmath0 and concave penalties are subsets of those selected by the lasso .",
    "our theoretical analysis shows that the regularized estimate , as the global optimum , given by combined @xmath0 and concave regularization enjoys the same asymptotic properties as the lasso estimator , but with improved sparsity and false sign rate , in ultra - high dimensional linear regression model .",
    "these results may be extended to more general model settings and other convex penalties , such as the @xmath37-penalty . to quantify the stability of variable selection , one can use , for example , the bootstrap method @xcite to estimate the selection probabilities , significance , and estimation uncertainty of selected variables by the regularization method in practice .",
    "the authors sincerely thank the editor , an associate editor , and two referees for comments that significantly improved the paper .",
    "this work was supported by the u.s . national science foundation and the university of southern california .",
    "supplementary material available at  online includes the proofs of proposition [ prop1 ] and theorem [ thm3 ] , and further details for  [ sec5 ] .",
    "let @xmath166 denote the estimation error with @xmath41 the global minimizer of ( [ e006 ] ) . by condition [ cond2 ] ,",
    "we see from proposition [ prop1 ] that each @xmath42 is either 0 or of magnitude larger than @xmath54 .",
    "it follows from the global optimality of @xmath41 that @xmath167 with some simple algebra , ( [ e003 ] ) becomes @xmath168 for notational simplicity , we let @xmath169 and @xmath170 denote the subvectors of a @xmath1-vector @xmath171 consisting of its first @xmath69 components and remaining @xmath172 components , respectively . since @xmath72 ,",
    "we have @xmath173 . thus we can rewrite ( [ e004 ] ) as @xmath174 the reverse triangle inequality @xmath175 along with ( [ e005 ] ) yields @xmath176 which is key to establishing bounds on prediction and variable selection losses .    to analyze the behavior of @xmath87",
    ", we need to use the concentration property of @xmath177 around its mean zero , as given in the deviation probability bound ( [ e024 ] ) .",
    "condition on the event @xmath178 . on this event",
    ", we have @xmath179 this inequality together with ( [ e014 ] ) gives @xmath180 in order to proceed , we need to construct an upper bound for @xmath181 .",
    "we claim that such an upper bound is @xmath182 . to prove this , we consider two cases .    _ case 1 _ : @xmath183",
    ". then by condition [ cond2 ] , we have @xmath184 ( @xmath185 ) and @xmath81 . for each @xmath185 , if @xmath186 , we must have @xmath187 and thus by the mean - value theorem , @xmath188 , where @xmath60 is between @xmath189 and @xmath190 , and @xmath191 is the @xmath43th component of @xmath87 .",
    "clearly @xmath192 , which along with the concavity of @xmath25 leads to @xmath193 .",
    "this shows that @xmath194 for each @xmath195 with @xmath186 .",
    "we now consider @xmath185 with @xmath196 .",
    "since @xmath183 , there exists some @xmath197 such that @xmath198 and @xmath199 s are distinct for different @xmath43 s .",
    "similarly as above , we have for some @xmath200 between @xmath54 and @xmath189 and some @xmath201 between @xmath54 and @xmath202 , @xmath203 since @xmath196 and @xmath204 .",
    "combining these two sets of inequalities yields the desired upper bound @xmath205 .",
    "_ case 2 _ : @xmath206 for some @xmath207 .",
    "then we have @xmath208 and @xmath209 , since there are at least @xmath210 such @xmath43 s with @xmath195 and @xmath196 .",
    "thus it follows from the first part of condition [ cond1 ] and @xmath211 in condition [ cond2 ] that @xmath212 since @xmath213 and there are @xmath214 nonzero @xmath42 s , applying the same arguments as in case 1 gives our desired upper bound @xmath215 .",
    "combining cases 1 and 2 above along with ( [ e011 ] ) and @xmath216 yields @xmath217 which entails a basic constraint @xmath218 with the aid of ( [ e012 ] ) , we will first establish a useful bound on @xmath219 . in view of ( [ e012 ] ) , the restricted eigenvalue assumption in the second part of condition [ cond1 ] and ( [ e015 ] ) , as well as the cauchy  schwartz inequality , lead to @xmath220 solving this inequality gives @xmath221 since the @xmath210th largest absolute component of @xmath222",
    "is bounded from above by @xmath223 , we have @xmath224 , where @xmath225 is a subvector of @xmath80 consisting of components excluding those with the @xmath69 largest magnitude .",
    "this inequality , ( [ e012 ] ) , and the cauchy ",
    "schwartz inequality entail that @xmath226 and thus @xmath227 . by ( [ e027 ] )",
    ", we have @xmath228 .",
    "combining these two inequalities with ( [ e016 ] ) gives @xmath229 this bound enables us to conduct more delicate analysis on @xmath87 .",
    "we proceed to prove the first part of theorem [ thm1 ] .",
    "the inequality ( [ e034 ] ) on the prediction loss can be obtained by inserting ( [ e016 ] ) into ( [ e015 ] ) : @xmath230 combining ( [ e016 ] ) with ( [ e025 ] ) yields the following bound on the @xmath37-estimation loss , @xmath231 for each @xmath232 , an application of hlder s inequality gives @xmath233 now we bound the number of falsely discovered signs @xmath103 . if @xmath234 , then by proposition [ prop1 ] and condition 2 , @xmath235 .",
    "thus , it follows that @xmath236 .",
    "this together with ( [ e113 ] ) entails that @xmath237 we finally note that all the above bounds for @xmath41 are conditional on the event @xmath238 , and thus hold simultaneously with probability @xmath94 , which concludes the proof for the first part of theorem [ thm1 ] .",
    "it remains to prove the second part of theorem [ thm1 ] .",
    "since @xmath239 , we have by condition [ cond2 ] that @xmath240 .",
    "this inequality together with ( [ e016 ] ) entails that for each @xmath241 , @xmath242 by a simple contradiction argument . in view of ( [ e025 ] ) and the hard - thresholding feature of @xmath243 with @xmath244",
    ", a similar contradiction argument shows that @xmath245 . combining this result with ( [ e037 ] ) leads to @xmath97 . with this strong result on sign consistency of @xmath41",
    ", we can derive tight bounds on the @xmath112-loss . by theorem 1 of lv & fan ( 2009 )",
    ", @xmath246 solves the following equation for @xmath247 @xmath248 where @xmath99 is an @xmath100 submatrix of @xmath68 corresponding to @xmath69 nonzero @xmath101 s and @xmath249 , with the derivative taken componentwise and @xmath250 the hadamard , componentwise , product .",
    "it follows from the concavity and monotonicity of @xmath25 and condition [ cond2 ] that for any @xmath192 , we have @xmath251 . in view of ( [ e037 ] ) and the hard - thresholding feature of @xmath41 , each component of @xmath246 has magnitude larger than @xmath54 .",
    "since @xmath252 on the event @xmath238 , combining these results leads to @xmath253 clearly @xmath254 .",
    "thus it follows from ( [ e036 ] ) , ( [ e038 ] ) , and the first part of condition [ cond1 ] that @xmath255 which concludes the proof for the second part of theorem [ thm1 ] .",
    "let @xmath41 be the global minimizer of ( [ e006 ] ) given in theorem [ thm1 ] , with @xmath166 denoting the estimation error . to calculate the risk of the regularized estimator @xmath41 for different losses , we need to analyze its tail behavior on the event @xmath256 .",
    "we work directly with inequality ( [ e003 ] ) .",
    "it follows easily from ( [ e003 ] ) that @xmath257 we need to bound the term @xmath258 from above , where @xmath14 .",
    "consider the cases of bounded or unbounded error .    _ case 1 _ : bounded error with range @xmath135 $ ] .",
    "then in view of the deviation probability bound ( [ e024 ] ) , we have @xmath259    _ case 2 _ : unbounded error .",
    "then it follows from ( [ e024 ] ) that for each @xmath260 and any @xmath261 , @xmath262 thus we have @xmath263 clearly , the bound ( [ e041 ] ) is a special case of the general bound ( [ e042 ] ) , with @xmath136 .",
    "we first consider the risks under the @xmath0-loss and prediction loss .",
    "note that @xmath264 . by ( [ e040 ] ) , ( [ e042 ] ) , and ( [ e024 ] ) , we have @xmath265 \\\\ \\label{e044 } & \\leq ( 2 \\lambda_0)^{-1 } e\\{{\\varepsilon}_0 ^ 2 1_{\\{|{\\varepsilon}_0| > \\tau\\}}\\ } + o(\\gamma p^{-c_0}),\\end{aligned}\\ ] ] where @xmath129 .",
    "this inequality along with ( [ e112 ] ) on the event @xmath238 yields for any @xmath261 , @xmath266 note that @xmath267 .",
    "thus in view of ( [ e040 ] ) , a similar argument as for ( [ e044 ] ) applies to show that @xmath268 .",
    "combining this inequality with ( [ e109 ] ) on the event @xmath238 gives @xmath269    we now consider the risk under the variable selection loss . to this end",
    ", we need to bound @xmath270 on the event @xmath271 .",
    "since @xmath41 always has the hard - thresholding property ensured by proposition [ prop1 ] , it follows from the monotonicity of @xmath25 and condition [ cond2 ] that @xmath272 .",
    "this inequality along with ( [ e040 ] ) shows @xmath273 clearly , @xmath274 .",
    "thus by ( [ e045 ] ) , applying a similar argument as for ( [ e044 ] ) gives @xmath275 it follows from this bound and inequality ( [ e007 ] ) on the event @xmath238 that @xmath276    we finally consider the risks under the @xmath3-loss with @xmath89 $ ] . by ( [ e040 ] ) and the norm inequality @xmath277 , we have @xmath278 with this inequality and ( [ e024 ] ) , a similar argument as for ( [ e042 ] ) applies to show that for any @xmath261 , @xmath279 \\\\ \\label{e046 } & \\leq ( 3/4 ) \\lambda_0^{-2 } e({\\varepsilon}_0 ^ 4 1_{\\{|{\\varepsilon}_0| > \\tau\\ } } ) + o(\\gamma^2 p^{-c_0}).\\end{aligned}\\ ] ] combining ( [ e046 ] ) with ( [ e113 ] ) on the event @xmath238 yields @xmath280 . for the @xmath3-loss with @xmath281 , an application of hlder s inequality and young s inequality with ( [ e044 ] ) and ( [ e046 ] ) gives @xmath282,\\end{aligned}\\ ] ] where @xmath283 .",
    "it follows from this inequality and ( [ e112 ] ) on the event @xmath238 that @xmath284,\\end{aligned}\\ ] ] which completes the proof for the first part of theorem [ thm2 ] .",
    "the second part of theorem [ thm2 ] can be proved by noting @xmath97 under the additional condition and using similar arguments as above .",
    "7 natexlab#1#1    bickel , p. j. , ritov , y. & tsybakov , a. b. ( 2009 ) . .",
    "_ * 37 * , 170532 .",
    "cands , e. & tao , t. ( 2007 ) . .",
    "_ * 35 * , 2313404 .",
    "chen , s. s. , donoho , d. l. & saunders , m. a. ( 1999 ) . .",
    "_ siam j. sci .",
    "* 20 * , 3361 .",
    "efron , b. ( 1979 ) . .",
    "_ * 7 * , 126 .",
    "efron , b. , hastie , t. j. , johnstone , i. m. & tibshirani , r. j. ( 2004 ) . .",
    "_ * 32 * , 40799 .",
    "fan , j. & fan , y. ( 2008 ) . .",
    "statist . _",
    "* 36 * , 260537 .",
    "fan , j. & li , r. ( 2001 ) . .",
    "_ j. amer .",
    "assoc . _ * 96 * ,",
    "134860 .    fan , j. & lv , j. ( 2010 ) . .",
    "_ statist .",
    "sinica _ * 20 * ,",
    "10148 .    fan , j. & lv , j. ( 2011 ) . .",
    "_ ieee trans .",
    "inform . theory _ * 57 * , 546784 .",
    "gordon , g. j. , jensen , r. v. , hsiao , l. l. , gullans , s. r. , blumenstock , j. e. , ramaswamy , s. , richards , w. g. , sugarbaker , d. j. & bueno , r. ( 2002 ) . .",
    "_ cancer res . _ * 62 * , 49637 .",
    "lin , w & lv , j. ( 2013 ) . .",
    "_ j. amer .",
    "_ * 108 * , 24764 .",
    "liu , y. & wu , y. ( 2007 ) . .",
    "_ j. comput . graph .",
    "* 16 * , 78298 .",
    "lv , j. & fan , y. ( 2009 ) . .",
    "_ * 37 * , 3498528 .",
    "mazumder , r. , friedman , j. h. & hastie , t. j. ( 2011 ) . .",
    "_ j. amer .",
    "assoc . _ * 106 * , 112538 .",
    "rosset , s. & zhu , j. ( 2007 ) . .",
    "_ * 35 * , 101230 .",
    "tibshirani , r. j. ( 1996 ) . .",
    "b _ * 58 * , 26788",
    ".    zhang , c .- h .",
    "( 2010 ) . .",
    "_ * 38 * , 894942 .",
    "zhang , c .- h .",
    "& zhang , t. ( 2012 ) . .",
    "_ statist .",
    "* 27 * , 57693 .",
    "zou , h. ( 2006 ) . .",
    "_ j. amer .",
    "assoc . _ * 101 * , 141829 .",
    "zou , h. & zhang , h. h. ( 2009 ) . .",
    "statist . _",
    "* 37 * , 173351 .    *",
    "supplementary material for `` asymptotic properties for + combined @xmath0 and concave regularization '' *    yingying fan and jinchi lv    this supplementary material contains the proofs of proposition [ prop1 ] and theorem [ thm3 ] , and further details for  [ sec5 ] .",
    "let @xmath41 be any local minimizer of ( [ e006 ] ) that is the global minimizer along each coordinate .",
    "when constrained on one coordinate , the minimization problem ( [ e006 ] ) becomes a univariate penalized least - squares problem of form as in ( [ e010 ] ) , with @xmath285 replaced by @xmath286 , in which the value of scalar @xmath44 depends on the coordinate .",
    "when @xmath25 is chosen as the hard - thresholding penalty @xmath61 , the univariate solution @xmath287 is the soft - hard - thresholded estimator @xmath288 given in ( [ e013 ] ) .",
    "consider two cases .",
    "_ case 1 _ : @xmath289 .",
    "then @xmath290 , meaning that 0 is the global minimizer of @xmath291 .",
    "denote by @xmath292 the function @xmath293 with @xmath286 in place of @xmath285 . by assumption and @xmath294 for @xmath295",
    ", it follows that @xmath296 .",
    "these along with @xmath297 entail @xmath298 .",
    "_ case 2 _ : @xmath299 .",
    "then by the monotonicity of @xmath25 , @xmath287 has the same sign as @xmath44 .",
    "it follows from @xmath50 and @xmath299 that @xmath300 since @xmath52 is decreasing on @xmath53 $ ] , the function @xmath292 is convex , or first concave and then convex as @xmath301 varies from @xmath302 to @xmath303 , in view of @xmath304 .",
    "this shape constraint of @xmath292 between @xmath302 to @xmath303 along with @xmath305 entails that its minimum on this interval is attained at @xmath302 or @xmath303 .",
    "since @xmath305 , the point @xmath303 can not be the global minimizer of @xmath292 .",
    "thus @xmath298 or @xmath306 , which completes the proof .",
    "we first make a simple observation .",
    "let @xmath307 be the global minimizer of ( [ e006 ] ) with the @xmath112-constraint @xmath308 .",
    "then conditional on the event @xmath178 , @xmath18 is a feasible solution to this new minimization problem .",
    "thus the proof of theorem [ thm1 ] applies to show that @xmath307 has the same asymptotic properties as in theorem [ thm1 ] .",
    "let @xmath41 be a computable local minimizer of ( [ e006 ] ) that is global minimizer in each coordinate produced by any algorithm satisfying @xmath141 and @xmath142 .",
    "since @xmath143 , it follows from theorem [ thm1 ] that @xmath309 , which entails that @xmath310 .",
    "denote by @xmath311 .",
    "then in view of @xmath141 , we have @xmath312 for some sufficiently large positive constant @xmath146 . in light of @xmath313 and @xmath142 ,",
    "similarly as in the proof of theorem 5 in zhang & zhang ( 2012 ) we can show that @xmath314 where @xmath315 denotes a submatrix of @xmath68 consisting of columns in @xmath316 and @xmath317 denotes a subvector of @xmath318 consisting of components in @xmath316 .",
    "since by assumption @xmath319 , the smallest singular value of @xmath320 is bounded from below by @xmath74 , which along with inequality ( [ n001 ] ) yields the same asymptotic bounds on @xmath321 and @xmath322 as in theorem [ thm1 ] .",
    "combining these results with theorem [ thm1 ] leads to desired asymptotic bounds on the @xmath3-estimation and prediction losses for the computable solution @xmath41 . since @xmath41 is global minimizer in each coordinate , it follows from proposition [ prop1 ] that each component of @xmath41 is either zero or of magnitude larger than @xmath54 .",
    "thus we can obtain similar bound on @xmath323 using the bound on @xmath324 and the same argument as in theorem [ thm1 ] , which concludes the proof .",
    "[ tab2 ]    @xmath0+scad , combined @xmath0 and smoothly clipped absolute deviation ; @xmath0+hard , combined @xmath0 and hard - thresholding ; @xmath0+sica , combined @xmath0 and smooth integration of counting and absolute deviation ."
  ],
  "abstract_text": [
    "<S> two important goals of high - dimensional modeling are prediction and variable selection . in this article , we consider regularization with combined @xmath0 and concave penalties , and study the sampling properties of the global optimum of the suggested method in ultra - high dimensional settings . </S>",
    "<S> the @xmath0-penalty provides the minimum regularization needed for removing noise variables in order to achieve oracle prediction risk , while concave penalty imposes additional regularization to control model sparsity . in the linear model setting , we prove that the global optimum of our method enjoys the same oracle inequalities as the lasso estimator and admits an explicit bound on the false sign rate , which can be asymptotically vanishing . </S>",
    "<S> moreover , we establish oracle risk inequalities for the method and the sampling properties of computable solutions . </S>",
    "<S> numerical studies suggest that our method yields more stable estimates than using a concave penalty alone .    </S>",
    "<S> concave penalty ; global optimum ; lasso penalty ; prediction and variable selection . </S>"
  ]
}