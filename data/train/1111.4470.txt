{
  "article_text": [
    "the classical problem of estimating a continuous - valued function from noisy observations , known as _ regression _ , is of central importance in statical theory with a broad range of applications , see e.g. @xcite .",
    "when no structural assumptions concerning the target function are made , the regression problem is termed _",
    "nonparametric_. informally , the main objective in the study of nonparametric regression is to understand the relationship between the regularity conditions that a function class might satisfy ( e.g. , lipschitz or hlder continuity , or sparsity in some representation ) and the minimax risk convergence rates @xcite . a further consideration is the computational efficiency of constructing the regression function .",
    "the general ( univariate ) nonparametric regression problem may be stated as follows .",
    "let @xmath1 be a metric space , namely @xmath2 is a set of points and @xmath3 a distance function , and let @xmath4 be a collection of functions ( `` hypotheses '' ) @xmath5 $ ] .",
    "( although in general , @xmath6 is not explicitly restricted to have bounded range , typical assumptions on the diameter of @xmath2 and the noise distribution amount to an effective truncation . ) the space @xmath7 $ ] is endowed with some fixed , unknown probability distribution @xmath8 , and the learner observes @xmath9 i.i.d",
    ".  draws @xmath10 .",
    "the learner then seeks to fit the observed data with some hypothesis @xmath11 so as to minimize the _",
    ", usually defined as the expected loss @xmath12 for @xmath13 and some @xmath14 .",
    "two limiting assumptions have traditionally been made when approaching this problem : ( i ) the space @xmath2 is euclidean and ( ii ) @xmath15 , where @xmath16 is the target function and @xmath17 is an i.i.d .",
    "noise process , often taken to be gaussian . although our understanding of nonparametric regression under these assumptions is quite elaborate , little is known about nonparametric regression in the absence of either assumption .",
    "the present work takes a step towards bridging this gap .",
    "specifically , we consider nonparametric regression in an arbitrary metric space , while making no assumptions on the distribution of the data or the noise .",
    "our results rely on the structure of the metric space only to the extent of assuming that the metric space has a low `` intrinsic '' dimensionality . specifically , we employ the doubling dimension of @xmath2 , denoted @xmath18 , which was introduced by @xcite based on earlier work of @xcite , and has been since utilized in several algorithmic contexts , including networking , combinatorial optimization , and similarity search , see e.g. @xcite .",
    "( a formal definition and prevailing examples appear in section [ sec : tech ] . ) following the work in @xcite on classification problems , our risk bounds and algorithmic runtime bounds are stated in terms of the doubling dimension of the ambient space and the lipschitz constant of the regression hypothesis , although neither of these quantities need be known in advance .    [ [ our - results . ] ] our results .",
    "+ + + + + + + + + + + +    we consider two kinds of risk : @xmath19 ( mean absolute ) and @xmath20 ( mean square ) . more precisely ,",
    "for @xmath21 we associate to each hypothesis @xmath11 the empirical @xmath22-risk [ eq : emprisk ] r_n(h ) = r_n(h ,",
    "q ) = n_i=1^n ^q and the ( expected ) @xmath22-risk [ eq : exprisk ] r(h ) = r(h , q ) = @xmath23^q = _ ^q ( dx , dy ) .",
    "it is well - known that @xmath24 $ ] ( where @xmath25 is the median ) minimizes @xmath26 over all integrable @xmath27^{\\mathcal{x}}$ ] and @xmath28 $ ] minimizes @xmath29 .",
    "however , these expressions are of little use as neither is computable without knowledge of @xmath8 . to circumvent this difficulty , we minimize the empirical @xmath22-risk and",
    "assert that the latter is a good approximation of the expected risk , provided @xmath4 meets certain regularity conditions . to this end",
    ", we define the following random variable , termed _ uniform deviation _ : [ eq : deltah ] _",
    "n ( ) = _ n(,q ) = _ h .    it is immediate that [ eq : hemp ] r(h ) r_n(h ) + _ n ( ) holds for all @xmath11 ( i.e. , the expected risk of any hypothesis does not exceed its empirical risk by much ) , and it can further be shown @xcite that [ eq : hopt ] r(h ) r(h^ * ) + 2_n ( ) , where @xmath30 is a minimizer of the empirical risk and @xmath31 is a minimizer of the expected risk ( i.e. , the expected risk of @xmath32 does not exceed the risk of the best admissible hypothesis by much ) .",
    "our contribution is twofold : statistical and computational .",
    "the algorithm in theorem [ thm : risk - minimization ] computes an @xmath33-additive approximation to the empirical risk minimizer in time @xmath34 . by theorem [ thm : lipext ] , this hypothesis can be evaluated on new points in time @xmath35 .",
    "the expected risk of this hypothesis decays as the empirical risk plus @xmath36 , as proved in theorem [ thm : delta[h ] ] .",
    "although our bounds explicitly depend on the doubling dimension , the latter may be efficiently estimated from the data @xcite .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    there are many excellent references for classical euclidean nonparametric regression assuming i.i.d .",
    "noise , see for example @xcite . for metric regression , a simple risk bound follows from classic vc theory via the pseudo - dimension , see e.g.  @xcite .",
    "however , the pseudo - dimension of many non - trivial function classes , including lipschitz functions , grows linearly with the sample size , ultimately yielding a vacuous bound .",
    "an approach to nonparametric regression based on empirical risk minimization , though only for the euclidean case , may already be found in @xcite ; see the comprehensive historical overview therein .",
    "indeed , theorem 5.2 in @xcite gives a kernel regressor for lipschitz functions that achieves the minimax rate .",
    "note , however that ( a ) the setting is restricted to euclidean spaces ; and ( b ) the cost of evaluating the hypothesis at a new point grows linearly with the sample size ( while our complexity is roughly logarithmic ) . as noted above",
    ", another feature of our approach is its ability to give efficiently computable finite - sample bounds , as opposed to the asymptotic convergence rates obtained in @xcite and elsewhere .",
    "more recently , risk bounds in terms of doubling dimension and lipschitz constant were given in @xcite , assuming an additive noise model , and hence these results are incomparable to ours ; for instance , these risk bounds worsen with an increasingly smooth regression function . following up",
    ", a regression technique based on random partition trees was proposed in @xcite , based on mappings between euclidean spaces and assuming an additive noise model .",
    "another recent advance in nonparametric regression was rodeo @xcite , which escapes the curse of dimensionality by adapting to the sparsity of the regression function .",
    "our work was inspired by the paper of von luxburg and bousquet @xcite , who were apparently the first to make the connection between lipschitz classifiers in metric spaces and large - margin hyperplanes in banach spaces , thereby providing a novel generalization bound for nearest - neighbor classifiers .",
    "they developed a powerful statistical framework whose core idea may be summarized as follows : to predict the behavior at new points , find the smoothest function consistent with the training sample .",
    "their work raises natural algorithmic questions like how to estimate the risk for a given input , how to perform model selection ( structural risk minimization ) to avoid overfitting , and how to perform the learning and prediction quickly .",
    "follow - up work @xcite leveraged the doubling dimension simultaneously for statistical and computational efficiency , to design an efficient classifier for doubling spaces .",
    "its key feature is an efficient algorithm to find the optimal balance between the empirical risk and the penalty term for a given input .",
    "minh and hoffman @xcite",
    "take the idea in @xcite in a more algebraic direction , establishing a representer theorem for lipschitz functions on compact metric spaces .",
    "[ [ paper - outline . ] ] paper outline .",
    "+ + + + + + + + + + + + + +    we start by defining the basic concepts in section [ sec : tech ] .",
    "the information - theoretic bulk of the paper is in section [ sec : fat - bounds ] , where risk bounds for lipschitz functions are given via fat - shattering dimension estimates .",
    "our efficient model selection procedure is described in section [ sec : bv ] and the local prediction algorithm is described in section [ sec : lipext ] .",
    "finally , we establish the strong , universal consistency of our regression estimate in section [ sec : consist ] .",
    "we use standard notation and definitions throughout .    [ [ metric - spaces - lipschitz - constants . ] ] metric spaces , lipschitz constants .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a _ metric _ @xmath37 on a set @xmath2 is a positive symmetric function satisfying the triangle inequality @xmath38 ; together the two comprise the metric space @xmath39 .",
    "the diameter of a set @xmath40 is defined by @xmath41 . in this paper",
    ", we always take @xmath42 .",
    "the _ lipschitz constant _ of a function @xmath43 , denoted @xmath44 , is defined to be the smallest @xmath45 that makes @xmath46 hold for all @xmath47 .",
    "[ [ doubling - dimension . ] ] doubling dimension .",
    "+ + + + + + + + + + + + + + + + + + +    for a metric @xmath1 , let @xmath48 be the smallest value such that every ball in @xmath2 can be covered by @xmath49 balls of half the radius .",
    "the _ doubling dimension _ of @xmath2 is @xmath50 .",
    "a metric ( or family of metrics ) is called _ doubling _ if its doubling dimension is uniformly bounded .",
    "note that while a low euclidean dimension implies a low doubling dimension ( euclidean metrics of dimension @xmath37 have doubling dimension @xmath51 ) , low doubling dimension is strictly more general than low euclidean dimension .",
    "doubling metrics occur naturally in many data analysis applications , including for instance the geodesic distance of a low - dimensional manifold residing in a possibly high - dimensional space ( assuming mild conditions , e.g.  on curvature ) .",
    "some concrete examples for doubling metrics include : ( i ) @xmath52 for fixed @xmath37 equipped with an arbitrary norm , e.g.  @xmath53 or a mix between @xmath54 and @xmath55 ; ( ii ) the planar earthmover metric between point sets of size @xmath56 ( * ? ? ?",
    "* section 6 ) ; ( iii ) the @xmath9-cycle graph and its continuous version , the quotient @xmath57 , and similarly bounded - dimensional tori .",
    "in addition , various networks that arise in practice , such as peer - to - peer communication networks and online social networks , can be modeled reasonably well by a doubling metric space .",
    "the following packing property can be demonstrated via repeated applications of the doubling property ( see , for example @xcite ) :    [ lem : doublpack ] let @xmath2 be a metric space and suppose that @xmath58 has a minimum interpoint distance of at least @xmath59 .",
    "then |s| & & ( ) ^ ( ) .",
    "note that there is no loss of generality in assuming @xmath42 , since we can always scale the distances and lipschitz constants to ensure this .",
    "[ [ graph - spanner . ] ] graph spanner .",
    "+ + + + + + + + + + + + + +    a graph @xmath60 is a @xmath61-stretch spanner for graph @xmath62 if @xmath60 is a subgraph of @xmath62 that contains all nodes of @xmath62 ( but not all edges ) , and @xmath63 for all @xmath64 , where @xmath65 ( @xmath66 ) denotes the shortest path distance between @xmath67 and @xmath68 in @xmath62 ( @xmath60 ) .",
    "if spanner @xmath60 achieves this bound even when its distance function is restricted to paths in @xmath60 of @xmath69 edges or fewer , then @xmath60 is an @xmath61-stretch @xmath69-hop spanner for @xmath62 .",
    "the definitions above apply also when the edges of @xmath62 have positive lengths .    a spanner for a metric space @xmath2",
    "is defined by thinking of the metric space as a graph @xmath62 on the vertex set @xmath2 , which is the complete graph with edge - lengths corresponding to distances in @xmath2 .",
    "doubling metrics are known to admit good spanners @xcite .",
    "we will use a specific variant described in appendix [ sec : spanner ] .",
    "[ [ fat - shattering - dimension . ] ] fat - shattering dimension .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    we recall the definition of the fat - shattering dimension @xcite .",
    "a finite set @xmath70 is said to be @xmath71-shattered by a family of functions @xmath72 if there exists some function @xmath73 such that for each label assignment @xmath74 there is an @xmath11 satisfying @xmath75 for all @xmath76 .",
    "the @xmath71-fat - shattering dimension of @xmath4 , denoted by @xmath77 , is the cardinality of the largest set @xmath71-shattered by @xmath4 ( or @xmath78 if the latter is unbounded ) .    in order for the random variable @xmath79 defined in ( [ eq : deltah ] ) to be measurable",
    ", @xmath4 must satisfy some mild measure - theoretic conditions ( pathologies are known to exist @xcite ) . to avoid the measure - theoretic technicalities associated with taking suprema over uncountable function classes , all hypothesis classes @xmath4 in this paper are assumed to be admissible , in the sense of @xcite .",
    "in this section , we derive tail bounds on the uniform deviation @xmath79 defined in ( [ eq : deltah ] ) in terms of the the smoothness properties of @xmath4 and the doubling dimension of the underlying metric space @xmath1 .",
    "we rely on the powerful framework of fat - shattering dimension developed by @xcite , which requires us to incorporate the value of a hypothesis and the loss it incurs on a sample point into a single function .",
    "this is done by associating to any family of hypotheses @xmath4 mapping @xmath80 $ ] , the induced family @xmath81 of functions mapping @xmath7\\mapsto[0,1]$ ] as follows : for each @xmath11 the corresponding @xmath82 is given by [ eq : fhq ] f^q_h(x , y ) = ^q , q. in a slight abuse of notation , we define the uniform deviation of a class @xmath83 of @xmath84$]-valued functions over @xmath7 $ ] : [ eq : delf ] _",
    "n ( ) = _ f ; it is obvious that @xmath85 .",
    "let us write [ eq : hldef ] _",
    "l = to denote the collection of @xmath84$]-valued @xmath86-lipschitz functions on @xmath2 .",
    "we proceed to bound the @xmath71-fat - shattering dimension of @xmath87 .",
    "[ thm : fatf ] let @xmath88 be defined on a metric space @xmath1 , where @xmath42 . then _ ( ^q__l ) & & ^()+1 holds for @xmath21 and all @xmath89 .",
    "the notation @xmath90 is a convenient shorthand for combining the results for @xmath91 and @xmath92 and is not intended to imply an interpolation for intermediate values .",
    "fix a @xmath93 and recall what it means for @xmath87 to @xmath71-shatter a set @xmath94 ( where @xmath95 and @xmath96^{|s|}$ ] ) : there exists some function @xmath97 such that for each label assignment @xmath98 there is an @xmath99 satisfying @xmath100 for all @xmath101 .",
    "put @xmath102 and define the map @xmath103 by @xmath104 thus , we may view @xmath105 as being partitioned into @xmath106 buckets : [ eq : s ] s = _",
    "k=0^k ( k ) .",
    "consider two points , @xmath107 and @xmath108 , belonging to some fixed bucket @xmath109 . by construction ,",
    "the following hold :    @xmath110    since @xmath87 @xmath71-shatters @xmath105 ( and recalling ( [ eq : fhq ] ) ) , there is an @xmath111 satisfying @xmath112 and @xmath113 for some @xmath114 . conditions ( i ) and ( ii ) imply that [ eq : i , ii ] ^(q+1)/2 , where lemma [ lem : rrg ] is invoked for @xmath92 .    the fact that @xmath6 is @xmath86-lipschitz implies that @xmath115 and by lemma [ lem : doublpack ] we have [ eq : pik ] & & ^()+1 for each @xmath116 .",
    "together ( [ eq : s ] ) and ( [ eq : pik ] ) yield our desired bound on @xmath117 , and hence on the fat shattering dimension of @xmath87 .",
    "the following generalization bound , implicit in @xcite , establishes the learnability of continuous - valued functions in terms of their fat - shattering dimension .",
    "[ thm : alon97 ] let @xmath83 be any admissible function class mapping @xmath7 $ ] to @xmath84 $ ] and define @xmath118 as in ( [ eq : delf ] ) .",
    "then for all @xmath119 and all @xmath120 , p(_n ( ) > ) & & 24n ^d(24en/ ) ( -^2n/36 ) where @xmath121 .",
    "[ cor : deltah ] fix an @xmath122 and @xmath21 .",
    "let @xmath88 be defined on a metric space @xmath1 and recall the definition of @xmath123 in ( [ eq : deltah ] ) .",
    "then for all @xmath120 , [ eq : deps ] p(_n(_l , q ) > ) & & 24n ^d(24en/ ) ( -^2n/36 ) where d = ^()+1 .",
    "we can conclude from corollary [ cor : deltah ] that there exists @xmath124 such that with probability at least @xmath125 , [ eq : delta - bound ] _",
    "n(_l , q ) ( n , l , ) , and by essentially inverting equation , we have [ eq : eps(n , l , d ) ] ( n , l , ) o .",
    "( for simplicity , the dependence of @xmath126 on @xmath18 is suppressed . )",
    "this implies via ( [ eq : hemp ] ) that @xmath127 uniformly for all @xmath111 with high probability .",
    "since the actual regression functions we compute in section [ sec : bv ] are additive approximations to smooth functions , but not necessarily smooth themselves , we will need some machinery for handling these . to this end , let us write [ eq : hetadef ] [ ] _ = to denote all @xmath33-perturbations of some function class @xmath128 for @xmath129 .",
    "our next objective is an analogue of theorem [ thm : fatf ] for additively perturbed lipschitz functions .",
    "[ lem : fateps ] let @xmath130 be a collection of real - valued functions defined over some set @xmath131 and let @xmath132_\\eta$ ] be the @xmath33-perturbation of @xmath130 , as defined in ( [ eq : hetadef ] )",
    ". then @xmath133_\\eta ) \\leq { \\mathrm{fat}}_{\\gamma-\\eta}({\\mathcal{g}}).\\ ] ] holds for all @xmath134 .",
    "suppose that @xmath132_\\eta$ ] shatters the set @xmath135 at level @xmath71 .",
    "then there is some @xmath136 so that for all @xmath98 there is an @xmath137_\\eta$ ] such that [ eq : gameps ] b(x)(f_b(x)-r(x))for all @xmath138 .",
    "now by definition , for each @xmath139 there is some @xmath140 so that @xmath141 .",
    "define @xmath142 to be such an @xmath33-approximation for each @xmath143 .",
    "we claim that the collection @xmath144 shatters @xmath105 at level @xmath145 . indeed , replacing @xmath146 with @xmath147 in ( [ eq : gameps ] ) perturbs the left - hand side by an additive term of at most @xmath33 .",
    "[ cor : fatfeta ] let @xmath88 be defined on a metric space @xmath1 and @xmath148_\\eta$ ] be the @xmath33-perturbation of @xmath88 for @xmath149 , as defined in ( [ eq : hetadef ] ) .",
    "further , let @xmath150_\\eta}}$ ] be the induced family of functions @xmath151\\to[0,1]$ ] as defined in ( [ eq : fhq ] ) .",
    "then , for @xmath21 and @xmath152 , we have _ ( ^q_[_l ] _ ) & & ^()+1    lemma [ lem : liphf ] shows that perturbing an @xmath111 by an additive @xmath33 perturbs the corresponding @xmath99 by an additive @xmath153 .",
    "lemma [ lem : fateps ] relates the fat - shattering dimension of an @xmath33-perturbed function class to that of its unperturbed version .",
    "finally , theorem [ thm : fatf ] gives an estimate on the fat - shattering dimension for the case where the unperturbed function class consists of the @xmath86-lipschitz functions on the doubling metric space @xmath1 .",
    "we are now able to extend corollary [ cor : deltah ] to perturbations of lipschitz functions .",
    "[ thm : delta[h ] ] fix an @xmath154 and @xmath21 .",
    "let @xmath88 be defined on a metric space @xmath1 and @xmath148_\\eta$ ] be the @xmath33-perturbation of @xmath88 for @xmath155 .",
    "then for all @xmath120 , [ eq : delprob ] p(_n([_l]_,q ) > ) & & 24n ^d(24en/ ) ( -^2n/36 ) where @xmath156 is the uniform deviation defined in ( [ eq : deltah ] ) and d = d(l , ) = ^()+1 .    inverting the relation in ( [ eq : delprob ] ) we get an estimate analogous to ( [ eq : delta - bound ] ) : with probability at least @xmath125 , @xmath157_\\eta , q)\\le \\epsilon(n , l,\\delta)+24q\\eta,\\ ] ] where @xmath126 is as in ( [ eq : eps(n , l , d ) ] ) , implying a risk bound of @xmath158      so far , we have established the following .",
    "let @xmath1 be a doubling metric space and @xmath88 a collection of @xmath86-lipschitz @xmath84$]-valued functions on @xmath2 .",
    "then corollary [ cor : deltah ] guarantees that for all @xmath159 and @xmath160 , we have [ eq : deltal ] p(_n(_l ) > ) , where @xmath161 is the uniform deviation defined in ( [ eq : deltah ] ) . theorem [ thm : delta[h ] ] extends this to perturbed lipschitz functions .",
    "since our computational approach in section [ sec : bv ] requires optimizing over lipschitz constants , we will need a bound such as ( [ eq : deltal ] ) that holds for many function classes of varying smoothness simultaneously .",
    "this is easily accomplished by stratifying the confidence parameter @xmath162 , as in @xcite .",
    "we will need the following theorem :    [ thm : delta - strat ] let @xmath163 be a sequence of function classes taking @xmath2 to @xmath84 $ ] and let @xmath164 $ ] , @xmath165 , be a sequence summing to @xmath166 .",
    "suppose that @xmath167 $ ] is a function such that for each @xmath168 , with probability at least @xmath125 , we have @xmath169 then , whenever some @xmath170_\\eta$ ] achieves empirical risk @xmath171 on a sample of size @xmath9 , we have that with probability at least @xmath125 , [ eq : rneps ] r(h ) r_n(h ) + (",
    "n , k , p_k ) k .    an immediate consequence of the union bound .",
    "the structural risk minimization principle implied by theorem [ thm : delta - strat ] amounts to the following model selection criterion : choose an @xmath172 for which the right - hand side of ( [ eq : rneps ] ) is minimized .    in applying theorem [ thm : delta - strat ] to lipschitz classifiers in section",
    "[ sec : bv ] below , we impose a discretization on the lipschitz constant @xmath86 to be multiples of @xmath173 . formally , we consider the stratification @xmath174 , @xmath175 where @xmath176 with corresponding @xmath177 for @xmath165 .",
    "this means that whenever we need a hypothesis that is an @xmath86-lipschitz regression function , we may take @xmath178 and use @xmath179 as the generalization error bound .",
    "note that all possible values of @xmath86 are within a factor of @xmath180 of the discretized sequence @xmath181 .",
    "in this section , we address the problem of efficient model selection when given @xmath9 observed samples .",
    "the algorithm described below computes a hypothesis that approximately attains the minimum risk over all hypotheses .",
    "recall the risk bound achieved as a consequence of theorems [ thm : delta[h ] ] and [ thm : delta - strat ] . whenever some @xmath182_\\eta$ ] achieves empirical risk @xmath171 on a sample of size @xmath9 , we have the following bound on @xmath183 , the true risk of @xmath6 : [ eq : riskbound ] r(h ) r_n(h ) + ( n , k , p_k ) + 24q , with probability at least @xmath125 ( where the diameter of the point set has been taken as 1 , and @xmath184 is the minimum value of @xmath185 for which the right - hand side of equation is at most @xmath162 ) . in the rest of this section , we devise an algorithm that computes a hypothesis that approximately minimizes our bound from on the true risk , denoted henceforth @xmath186 notice that on the right - hand side , the first two terms depend on @xmath86 , but only the first term depends on the choice of @xmath6 , and only the third term depends on @xmath33 .    [ thm : risk - minimization ] let @xmath187 for @xmath188 be an i.i.d .",
    "sample drawn from @xmath189 , let @xmath190 , and let @xmath16 be a hypothesis that minimizes @xmath191 over all @xmath192_\\eta$ ] .",
    "there is an algorithm that , given the @xmath9 samples and @xmath33 as input , computes in time @xmath34 a hypothesis @xmath193_\\eta$ ] with [ eq : risk - minimization ] r_(h ) 2r_(h^ * ) .",
    "we show in theorem [ thm : lipext ] how to quickly evaluate the hypothesis @xmath194 on new points .    in proving the theorem",
    ", we will find it convenient to compare the output @xmath194 to a hypothesis @xmath195 that is smooth ( i.e.  lipschitz but unperturbed ) . indeed , let @xmath16 be as in the theorem , and @xmath196 be a hypothesis that minimizes @xmath197 .",
    "then @xmath198 , and we get @xmath199 . accordingly , the analysis below will actually prove that @xmath200 , and then would follow easily , essentially increasing the additive error by @xmath201 .",
    "moreover , once equation is proved , we can use the above to conclude that @xmath202 , which compares the risk bound of our algorithm s output @xmath194 to what we could possibly get using smooth hypotheses .    in the rest of this section",
    "we consider the @xmath9 observed samples as fixed values , given as input to the algorithm , so we will write @xmath203 instead of @xmath204 .",
    "suppose that the lipschitz constant of an optimal _ unperturbed _ hypothesis @xmath205 were known to be @xmath206",
    ". then @xmath207 is fixed , and the problem of computing both @xmath205 and its empirical risk @xmath208 can be described as the following optimization program with variables @xmath209 for @xmath210 $ ] to represent the assignments @xmath211 .",
    "note it is a linear program ( lp ) when @xmath91 and a quadratic program when @xmath92 .",
    "[ eq : program ]    it follows that @xmath205 could be computed by first deriving @xmath212 , and then solving the above program .",
    "however , it seems that computing these exactly is an expensive computation .",
    "this motivates our search for an approximate solution to risk minimization .",
    "we first derive a target lipschitz constant @xmath213 that `` approximates '' @xmath212 , in the sense that it minimizes the objective @xmath214 .",
    "notice that @xmath215 may be computed by solving lp using the given value @xmath213 for @xmath86 .",
    "we wish to find such @xmath213 via a binary search procedure , which requires a method to determine whether a candidate @xmath86 satisfies @xmath216 , but since our objective need not be a monotone function of @xmath86 , we can not rely on the value of the objective at the candidate @xmath86 . instead ,",
    "recall that the empirical risk term @xmath215 is monotonically non - increasing , and the penalty term @xmath207 is monotonically non - decreasing , and therefore we can take @xmath213 to be the minimum value @xmath86 for which @xmath217 ( notice that both terms are right - continuous in @xmath86 ) .",
    "our binary search procedure can thus determine whether a candidate @xmath86 satisfies @xmath216 by checking instead whether @xmath217 .",
    "were the binary search on @xmath86 to be carried out indefinitely ( that is , with infinite precision ) , it would yield @xmath213 and a smooth hypothesis @xmath194 satisfying @xmath218 , where the factor @xmath180 originates from the gap between maximum and summation .",
    "the formal proof actually gives a slightly stronger bound : @xmath219 ( in our actual lp solver below , @xmath194 will not be necessarily smooth , but rather a perturbation of a smooth hypothesis . ) however , to obtain a tractable runtime , we fix an additive precision of @xmath33 to the lipschitz constant , and restrict the target lipschitz constant to be a multiple of @xmath33 . notice that @xmath220 for sufficiently large @xmath9 ( since this bound can even be achieved by a hypothesis with lipschitz constant @xmath221 ) , so by equation it must be that @xmath222 .",
    "it follows that the binary search will consider only @xmath223 candidate values for @xmath213 .    to bound the effect of discretizing the target @xmath213 to multiples of @xmath33",
    ", we shall show the existence of a hypothesis @xmath32 that has lipschitz constant @xmath224 and satisfies @xmath225 . to see this ,",
    "assume by translation that the minimum and maximum values assigned by @xmath205 are , respectively @xmath221 and @xmath226 .",
    "thus , its lipschitz constant is @xmath227 ( recall we normalized @xmath42 ) . assuming first the case @xmath228",
    ", we can set @xmath229 , and it is easy to verify that its lipschitz constant at most @xmath230 , and @xmath225 .",
    "the case @xmath231 is even easier , as now there is trivially a function @xmath32 with lipschitz constant @xmath221 and @xmath225 .",
    "it follows that when the binary search is analyzed using this @xmath32 instead of @xmath195 , we actually get @xmath232    it now remains to show that given @xmath213 , program may be solved quickly ( within certain accuracy ) , which we do in sections [ sec : lp ] and [ sec : cp ] .",
    "we show how to solve the linear program , given target lipschitz constant @xmath213 .",
    "[ [ fast - lp - solver - framework . ] ] fast lp - solver framework .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    to solve the linear program , we utilize the framework presented by young @xcite for lps of following form : given non - negative matrices @xmath233 , vectors @xmath234 and precision @xmath235 , find a non - negative vector @xmath236 such that @xmath237 and @xmath238 .",
    "young shows that if there exists a feasible solution to the input instance , then a solution to a relaxation of the input program ( specifically , @xmath239 and @xmath238 ) can be found in time @xmath240 , where @xmath241 is the number of constraints in the program and @xmath37 is the maximum number of constraints in which a single variable may appear .    in utilizing this framework for our problem",
    ", we encounter a difficulty that both the input matrices and output vector must be non - negative , while our lp has difference constraints . to bypass this limitation ,",
    "for each lp variable @xmath209 we introduce a new variable @xmath242 and two new constraints :    @xmath243    by the guarantees of the lp solver , we have that in the returned solution @xmath244 and @xmath245 .",
    "this technique allows us to introduce negated variables @xmath246 into the linear program , at the loss of additive precision .",
    "[ [ reduced - constraints . ] ] reduced constraints .",
    "+ + + + + + + + + + + + + + + + + + + +    a central difficulty in obtaining a near - linear runtime for the above linear program is that the number of constraints in lp is @xmath247 .",
    "we show how to reduce the number of constraints to near - linear in @xmath9 , namely , @xmath248 .",
    "we will further guarantee that each of the @xmath9 variables @xmath209 appears in only @xmath249 constraints .",
    "both these properties will prove useful for solving the program quickly .",
    "recall that the purpose of the @xmath247 constraints is solely to ensure that the target lipschitz constant is not violated between any pair of points .",
    "we will show below that this property can be approximately maintained with many fewer constraints : the spanner described in appendix [ sec : spanner ] has stretch @xmath250 , degree @xmath251 and hop - diameter @xmath252 for some constant @xmath253 , that can be computed quickly .",
    "build this spanner for the observed sample points @xmath254\\}$ ] with stretch @xmath255 ( i.e. , set @xmath256 ) and retain a constraint in lp if and only if its two variables correspond to two nodes that are connected in the spanner .",
    "it follows from the bounded degree of the spanner that each variable appears in @xmath249 constraints , which implies that there are @xmath248 total constraints .",
    "[ [ modifying - remaining - constraints . ] ] modifying remaining constraints .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    each spanner - edge constraint @xmath257 is replaced by a set of two constraints    @xmath258    by the guarantees of the lp solver we have that in the returned solution , each spanner edge constraint will satisfy    @xmath259   \\\\      & =    & \\beta + ( 1+\\beta ) l ' \\cdot \\rho(x_i , x_j )       \\end{array}\\ ] ]    now consider the lipschitz condition for two points not connected by a spanner edge : let @xmath260 be a @xmath261-stretch @xmath262-hop spanner path connecting points @xmath263 and @xmath264 .",
    "then the spanner stretch guarantees that    @xmath265   \\\\      & \\le & \\beta c ' \\log n + ( 1+\\beta)l ' \\cdot ( 1+\\eta ) \\rho(x , x ' )      \\end{array}\\ ] ]    choosing @xmath266 , and noting that @xmath267 , we have that for all point pairs    @xmath268    we claim that the above inequality ensures that the computed hypothesis @xmath194 ( represented by variables @xmath209 above ) is a @xmath269-perturbation of some hypothesis with lipschitz constant @xmath213 . to prove this , first note that if @xmath270 , then the statement follows trivially .",
    "assume then that ( by the discretization of @xmath213 ) , @xmath271 .",
    "now note that a hypothesis with lipschitz constant @xmath272 is a @xmath273-perturbation of some hypothesis with lipschitz constant @xmath213 .",
    "( this follows easily by scaling down this hypothesis by a factor of @xmath274 , and recalling that all values are in the range @xmath84 $ ] . )",
    "hence , it suffices to show that the computed hypothesis @xmath194 is a @xmath273-perturbation of some hypothesis @xmath275 with lipschitz constant @xmath272 .",
    "we can construct @xmath275 as follows : extract from the sample points @xmath276}$ ] a @xmath277-net @xmath278 , is at least @xmath277 ; and ( ii ) every point in @xmath105 is within distance @xmath277 from at least one point in @xmath278 .",
    "it can be easily constructed by a greedy process . ] then for every net - point @xmath279 set @xmath280 , and extend this function @xmath281 from @xmath278 to all of @xmath105 without increasing lipschitz constant by using the mcshane - whitney extension theorem @xcite for real - valued functions .",
    "observe that for every two net - points @xmath282 , @xmath283 it follows that @xmath281 ( defined on all of @xmath105 ) has lipschitz constant @xmath284 . now , consider any point @xmath138 and its closest net - point @xmath285 ; then @xmath286 .",
    "using the fact @xmath287 , we have that @xmath288 + ( 1 + 3\\eta)l ' \\cdot \\rho(x , y ) \\le \\frac{\\eta^2}{24q } + 2 + 5\\eta^2 \\le 3 \\eta$ ] .",
    "we conclude that @xmath194 is @xmath273-perturbation of @xmath275 , and a @xmath269-perturbation of some hypothesis with lipschitz constant @xmath213 .",
    "[ [ objective - function . ] ] objective function .",
    "+ + + + + + + + + + + + + + + + + + +    we now turn to the objective function @xmath289 .",
    "we use the same technique as above for handling difference constraints : for each term @xmath290 in the objective function we introduce the variable @xmath291 and the constraint @xmath292 note that the solver imposes the constraint that @xmath293 , so we have that @xmath294 .",
    "now consider the term @xmath295 , and note that the minimum feasible value of this term in the solution of the linear program is exactly equal to @xmath296 : if @xmath297 then the minimum feasible value of @xmath291 is @xmath221 , which yields @xmath298 .",
    "otherwise we have that @xmath299 , so the minimum feasible value of @xmath291 is @xmath300 , which yields @xmath301 .",
    "the objective function is then replaced by the constraint @xmath302 which by the above discussion is equal to @xmath303 , and hence is a direct bound on the empirical error of the hypothesis .",
    "we choose bound @xmath304 via binary search : recalling that @xmath305 ( since even a hypothesis with lipschitz constant @xmath221 can achieve this bound ) , we may set @xmath306 . by discretizing @xmath304 in multiples of @xmath33 ( similar to what was done for @xmath213 ) , we have that the binary search will consider only @xmath307 guesses for @xmath304 .",
    "note that for guess @xmath308 , the solver guarantees only that the returned sum is less than @xmath309 .",
    "if follows that the discretization of @xmath304 and its solver relaxation of @xmath304 introduce , together , at most an additive error of @xmath310 in the lp objective , i.e. , in @xmath215 and in @xmath311 .",
    "[ [ correctness - and - runtime - analysis . ] ] correctness and runtime analysis .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the fast lp solver ensures that @xmath194 computed by the above - described algorithm is a @xmath269-perturbation of a hypothesis with lipschitz constant @xmath213 . as for @xmath312 , which we wanted to minimize , an additive error of @xmath310",
    "is incurred by comparing @xmath194 to @xmath205 instead of to @xmath16 , another additive error of @xmath310 arises from discretizing @xmath313 into @xmath213 ( i.e. , comparing to @xmath32 instead of @xmath195 ) , and another additive error @xmath314 introduced through the discretization of @xmath304 and its solver relaxation .",
    "overall , the algorithm above computes a hypothesis @xmath315_{6\\eta}$ ] with @xmath316 .",
    "the parameters in theorem [ thm : risk - minimization ] are achieved by scaling down @xmath33 to @xmath317 and the simple manipulation @xmath318 .",
    "finally , we turn to analyze the algorithmic runtime .",
    "the spanner may be constructed in time @xmath319 .",
    "young s lp solver @xcite is invoked @xmath320 times , where the @xmath321 term is due to the binary search for @xmath213 , and the @xmath322 term is due to the binary search for @xmath304 . to determine the runtime per invocation , recall that each variable of the program appears in @xmath323 constraints , implying that there exist @xmath324 total constraints .",
    "since we set @xmath325 , we have that each call to the solver takes time @xmath326 , for a total runtime of @xmath327 .",
    "this completes the proof of theorem [ thm : risk - minimization ] for @xmath91 .",
    "above , we considered the case when the loss function is linear . here",
    "we modify the objective function construction to cover the case when the loss function is quadratic , that is @xmath328 .",
    "we then use the lp solver to solve our quadratic program .",
    "( note that the spanner - edge construction above remains as before , and only the objective function construction is modified . )",
    "let us first redefine @xmath291 by the constraints @xmath329 it follows from the guarantees of the lp solver that in the returned solution , @xmath330 and @xmath293",
    ".    now note that a quadratic inequality @xmath331 can be approximated for @xmath332 $ ] by a set of linear inequalities of the form @xmath333 for @xmath334 ; these are just a collection of tangent lines to the quadratic function .",
    "note that the slope of the quadratic function in the stipulated range is at most @xmath180 , so this approximation introduces an additive error of at most @xmath310 .",
    "since @xmath335 takes values in the range @xmath84 $ ] , we will consider an equation set of the form @xmath336 which satisfies that the minimum feasible value of @xmath337 is in the range @xmath338 $ ] .",
    "it remains to model these difference constraints in the lp framework : when @xmath339 , the equation set @xmath340 exactly models the above constraints .",
    "when @xmath341 , the lower bound of this set may not be tight , and instead the equation set @xmath342 models the above constraints , though possibly increasing the value of @xmath337 by @xmath343 .",
    "( note that when @xmath299 , the lower bound of the second equation set may not be tight , so the first equation set is necessary .",
    "also , note that whenever the right hand side of an equation is negative , the equation is vacuous and may be omitted . )",
    "the objective function is then replaced by the inequality @xmath344 where @xmath304 is chosen by binary search as above .    turning to the runtime analysis",
    ", the replacement of a constraint by @xmath345 new constraints does not change the asymptotic runtime . for the analysis of the approximation error , first note that a solution to this program is a feasible solution to the original quadratic program .",
    "further , given a solution to the original quadratic program , a feasible solution to the above program can be found by perturbing the quadratic program solution by at most @xmath273 ( since additive terms of @xmath310 and @xmath33 are lost in the above construction ) .",
    "the proof of theorem [ thm : risk - minimization ] for @xmath92 follows by an appropriate scaling of @xmath33 .",
    "in this section , we show how to evaluate our hypothesis on a new point .",
    "more precisely , given a hypothesis function @xmath346 $ ] , we wish to evaluate a minimum lipschitz extension of @xmath347 on a new point @xmath348 . that is , denoting @xmath349",
    ", we wish to return a value @xmath350 that minimizes @xmath351 . necessarily , this value is not greater than the lipschitz constant of the classifier , meaning that the extension of @xmath347 to the new point does not increase the lipschitz constant of @xmath347 and so theorem [ thm : delta - strat ] holds for the single new point .",
    "( by this local regression analysis , it is not necessary for newly evaluated points to have low lipschitz constant with respect to each other , since theorem [ thm : delta - strat ] holds for each point individually . )",
    "first note that the lipschitz extension label @xmath352 of @xmath348 will be determined by two points of @xmath105 .",
    "that is , there are two points @xmath353 , one with label greater than @xmath352 and one with a label less than @xmath352 , such that the lipschitz constant of @xmath354 relative to each of these points ( that is , @xmath355 ) is maximum over the lipschitz constant of @xmath354 relative to any point in @xmath105 .",
    "hence , @xmath352 can not be increased or decreased without increasing the lipschitz constant with respect to one of these points .",
    "note then that an exact lipschitz extension may be derived in @xmath247 time in brute - force fashion , by enumerating all point pairs in @xmath105 , calculating the optimal lipschitz extension for @xmath236 with respect to each pair alone , and then choosing the candidate value for @xmath352 with the highest lipschitz constant .",
    "however , we demonstrate that an approximate solution to the lipschitz extension problem can be derived more efficiently .",
    "[ thm : lipext ] an @xmath33-additive approximation to the lipschitz extension problem can be computed in time @xmath35 .",
    "the algorithm is as follows : round up all labels @xmath209 to the nearest term @xmath356 ( for any integer @xmath357 ) , and call the new label function @xmath358 .",
    "we seek the value of @xmath359 , the optimal lipschitz extension value for @xmath236 for the new function @xmath358 .",
    "trivially , @xmath360 .",
    "now , if we were given for each @xmath361 the point with label @xmath356 that is the nearest neighbor of @xmath236 ( among all points with this label ) , then we could run the brute - force algorithm described above on these @xmath362 points in time @xmath363 and derive @xmath359 . however , exact metric nearest neighbor search is potentially expensive , and so we can not find these points efficiently .",
    "we instead find for each @xmath361 a point @xmath364 with label @xmath365 that is a @xmath366-approximate nearest neighbor of @xmath236 among points with this label .",
    "( this can be done by presorting the points of @xmath105 into @xmath362 buckets based on their @xmath358 label , and once @xmath236 is received , running on each bucket a @xmath366-approximate nearest neighbor search algorithm due to @xcite that takes @xmath35 time . )",
    "we then run the brute force algorithm on these @xmath362 points in time @xmath363 .",
    "the nearest neighbor search achieves approximation factor @xmath367 , implying a similar multiplicative approximation to @xmath86 , and thus also to @xmath368 , which means at most @xmath369 additive error in the value @xmath352 .",
    "we conclude that the algorithm s output solves the lipschitz extension problem within additive approximation @xmath33 .",
    "in previous sections , we defined an efficient regression algorithm and analyzed its finite - sample performance . here we show that it enjoys the additional property of being strongly consistent .",
    "note that our regression hypothesis is constructed via approximate nearest neighbors ; see @xcite for consistency results of nearest - neighbor regression functions in euclidean spaces .",
    "we say that a regression estimator is _ strongly consistent _ if its expected risk converges almost surely to the optimal expected risk .",
    "further , it is called _ universal _ if this rate of convergence does not depend on the sampling distribution @xmath8 . in this section ,",
    "we establish the strong , universal consistency of our regression estimate .",
    "[ thm : exact - consist ] let @xmath1 be a compact metric space and suppose @xmath7 $ ] is endowed with a probability measure @xmath8 . for @xmath21 ,",
    "suppose that there is a continuous @xmath370 $ ] that achieves @xmath371 where @xmath372 is defined in ( [ eq : exprisk ] ) and the infimum is taken over all continuous @xmath5 $ ] .",
    "then there exists a sequence @xmath373 increasing to @xmath78 such that @xmath374 almost surely , where @xmath375 is a minimizer of @xmath376 over @xmath377 .",
    "assume for now that @xmath16 is lipschitz with constant @xmath378 .",
    "define @xmath379 and pick any @xmath154 .    given a sample of @xmath9 pairs",
    "@xmath187 drawn i.i.d .  under @xmath189 ,",
    "let @xmath375 be a minimizer of r_n(h , q ) = n_i=1^n ^q over @xmath380 .",
    "( the infimum is achieved since @xmath380 is compact by the arzel - ascoli theorem . ) similarly , define @xmath381 to be a minimizer of r(h , q ) = _ ^q ( dx , dy ) over @xmath380 .",
    "consider the event @xmath382 . by ( [ eq : hopt ] ) and theorem [ thm : delta[h ]",
    "] , we have , for @xmath383 , p(a_n ) & & 24n ^d_n(24en/ ) ( -^2n/36 ) where d_n = ^()+1 our choice of @xmath384 ensures that @xmath385 .",
    "now the series @xmath386 converges by the @xmath9th root test , and so the borel - cantelli lemma implies that almost surely , we have r(h_n ) - r(h_n^ * ) & & 2for all but finitely many @xmath9 . since @xmath387 , there is an @xmath388 for which @xmath389 .",
    "the inclusion @xmath390 for all @xmath391 implies that @xmath392 for all @xmath391 .",
    "we conclude that @xmath393 almost surely , as claimed .",
    "the case where @xmath16 is continuous but not lipschitz is handled by an approximation argument .",
    "it is easy to show that on a compact set , every continuous function is a uniform limit of lipschitz functions .",
    "that is , there exists a sequence of @xmath394 such that @xmath395 pick an @xmath154 , let @xmath396 be such that there is an @xmath397 with @xmath398 .",
    "the above argument shows that almost surely @xmath399 holds for all but finitely many @xmath9 .",
    "additionally , & = & + & & _ | .",
    "|h^*_n(x)-y|^q - |h^*(x)-y|^q .",
    "|(dx , dy ) + & & _ q ( dx ) + & & q , where we invoked lemma [ lem : liphf ] in the second inequality .",
    "this shows that @xmath400 almost surely for almost all @xmath9 and completes the proof .",
    "we thank larry wasserman for helpful comments on the manuscript , and robert schapire for useful discussion and feedback .",
    "the following lemma is used in the proof of theorem [ thm : fatf ] .",
    "[ lem : rrg ] for all @xmath401 we have [ eq : rrg ] - & & 2 ^ 3/2 .",
    "fix some @xmath93 .",
    "an elementary calculation shows that for any given @xmath71 , the l.h.s . of ( [ eq : rrg ] )",
    "is minimized when @xmath402 .",
    "thus it remains to show that @xmath403 which again is a standard calculation .",
    "the following lemma is used in the proof of corollary [ cor : fatfeta ] and theorem [ thm : exact - consist ] .",
    "[ lem : liphf ] for @xmath21 and @xmath404 $ ] , we have @xmath405    consider the case @xmath91 .",
    "then = which proves the claim for this case . for @xmath92 ,",
    "recall that the lipschitz constant of a differentiable real function is bounded by the maximal absolute value of its derivative .",
    "the function @xmath406\\to[0,1]$ ] defined by @xmath407 for a fixed @xmath408 $ ] has @xmath409 , which proves the claim for @xmath92 .",
    "in this section , we prove the following theorem . see section [ sec : tech ] for the definition of a spanner .    [ thm : spanner ]",
    "every finite metric space @xmath2 on @xmath9 points admits a @xmath61-stretch spanner with degree @xmath251 ( for @xmath410 ) and hop - diameter @xmath411 , that can be constructed in time @xmath412 .",
    "gottlieb and roditty @xcite presented for general metrics a @xmath61-stretch spanner with degree @xmath251 and construction time @xmath412 , but this spanner has potentially large hop - diameter .",
    "our goal is to modify this spanner to have low hop - diameter , without significantly increasing the spanner degree .",
    "now , as described in @xcite , the points of @xmath2 are arranged in a tree of degree @xmath251 , and a spanner path is composed of three consecutive parts : ( a ) a path ascending the edges of the tree ; ( b ) a single edge ; and ( c ) a path descending the edges of the tree .",
    "we will show to decrease the number of hops in parts ( a ) and ( c ) . below we will prove the following lemma .",
    "[ lem : tree ] let @xmath413 be a tree containing directed child - parent edges ( @xmath414 ) , and let @xmath415 be the degree of @xmath413 . then @xmath413 may be augmented with directed descendant - ancestor edges to create a dag @xmath62 with the following properties : ( i ) @xmath62 has degree @xmath416 ; and ( ii ) the hop - distance in @xmath62 from any node to each of its ancestors is @xmath411 .    note that theorem [ thm : spanner ] is an immediate consequence of lemma [ lem : tree ] applied to the spanner of @xcite .",
    "it remains only to prove lemma [ lem : tree ] .",
    "we will first need a simply preliminary lemma :    [ lem : path ] consider an ordered path on nodes @xmath417 .",
    "let these nodes be assigned positive weights @xmath418 , and let the weight of the path be @xmath419 .",
    "there exists a dag @xmath62 on these nodes with the following properties :    1 .",
    "edges in @xmath62 always point to the antecedent node in the ordering .",
    "the hop - distance from any node @xmath203 to the root node @xmath420 is not more than @xmath421 .",
    "3 .   the hop - distance from any node @xmath203 to an antecedent",
    "@xmath422 is not more than @xmath423 .",
    "4 .   @xmath62 has degree 3 .",
    "the construction is essentially the same as in the biased skip - lists of bagchi et al .",
    "let @xmath420 and @xmath424 be the left and right _ end nodes _ of the path , and let the other nodes be the _ middle nodes_. partition the middle nodes into two child subpaths @xmath425 ( the left child path ) and @xmath426 ( the right child path ) , where @xmath203 is chosen so that the weight of the middle nodes of each child path is not more than half the weight of the middle nodes of the parent path .",
    "( if the parent path has three middle nodes or fewer , then there will be a single child path . )",
    "the child paths are then recursively partitioned , until the recursion reaches paths with no middle nodes .",
    "the edges are assigned as follows .",
    "a right end node of a path has two edges leaving it .",
    "one points to the left end node of the path ( unless the path has only one node ) .",
    "the other edge points to the right end node of the right ( or single ) child path .",
    "a left end node of a path has one edge leaving it : if this path is a right child path , the edge points to the left sibling path s right end node . if this path is a left or single child path , then the edge points to the parent s left end node .",
    "the lemma follows via standard analysis .",
    "given tree @xmath413 , decompose @xmath413 into _ heavy paths _ :",
    "a heavy path is one that begins at the root and continues with the heaviest child , the child with the most descendants . in a heavy path decomposition",
    ", all off - path subtrees are recursively decomposed . for each heavy path , let the weight of each node in the path be the number of descendant nodes in its off - path subtrees . for each heavy path",
    ", we build the weighted construction of lemma [ lem : path ] .",
    "now , a path from node @xmath427 to @xmath428 traverses a set of at most @xmath429 heavy paths , say paths @xmath430 .",
    "the number of hops from @xmath67 to @xmath68 is bounded by @xmath431 ) , and the degree of @xmath62 is at most @xmath416 ."
  ],
  "abstract_text": [
    "<S> we present a framework for performing efficient regression in general metric spaces . roughly speaking </S>",
    "<S> , our regressor predicts the value at a new point by computing a lipschitz extension  the smoothest function consistent with the observed data  while performing an optimized structural risk minimization to avoid overfitting . </S>",
    "<S> the offline ( learning ) and online ( inference ) stages can be solved by convex programming , but this naive approach has runtime complexity @xmath0 , which is prohibitive for large datasets . </S>",
    "<S> we design instead an algorithm that is fast when the the doubling dimension , which measures the `` intrinsic '' dimensionality of the metric space , is low .    </S>",
    "<S> we use the doubling dimension multiple times ; first , on the statistical front , to bound fat - shattering dimension of the class of lipschitz functions ( and obtain risk bounds ) ; and second , on the computational front , to quickly compute a hypothesis function and a prediction based on lipschitz extension . </S>",
    "<S> our resulting regressor is both asymptotically strongly consistent and comes with finite - sample risk bounds , while making minimal structural and noise assumptions . </S>"
  ]
}