{
  "article_text": [
    "approximating the distribution of a sum of weakly dependent discrete random variables by a poisson distribution is an important and well - studied problem in probability ; see @xcite and the references therein for an extensive account .",
    "strong connections between these results and information - theoretic techniques were established @xcite@xcite . in particular , for the special case of approximating a binomial distribution by a poisson , some of the sharpest results to date are established using a combination of the techniques @xcite@xcite and pinsker s inequality @xcite@xcite@xcite . earlier work on information - theoretic bounds for poisson approximation is reported in @xcite@xcite@xcite .",
    "the thinning operation , which we define next , was introduced by rnyi in @xcite , who used it to provide an alternative characterization of poisson measures .",
    "[ def : thin ] given @xmath0 $ ] and a discrete random variable @xmath1 with distribution @xmath2 on @xmath3 , the _",
    "@xmath4-thinning of @xmath2 _ is the distribution @xmath5 of the sum , @xmath6 where the random variables @xmath7 are independent and identically distributed ( i.i.d . ) each with a bernoulli distribution with parameter @xmath4 , denoted bern(@xmath4 ) , and also independent of @xmath1 .",
    "[ as usual , we take the empty sum @xmath8 to be equal to zero . ] an explicit representation of @xmath5 can be given as,@xmath9 when it causes no ambiguity , the thinned distribution @xmath10 is written simply @xmath11 .",
    "for any random variable @xmath1 with distribution @xmath2 on @xmath12 , we write @xmath13 for the @xmath14-fold convolution of @xmath2 with itself , i.e. , the distribution of the sum of @xmath14 i.i.d .",
    "copies of @xmath1 . for example , if @xmath15 bern@xmath16 , then @xmath17 , the binomial distribution with parameters @xmath14 and @xmath18 .",
    "it is easy to see that its @xmath19-thinning , @xmath20 , is simply @xmath21 ; see example  [ ex : berns ] below . therefore , the classical binomial - to - poisson convergence result",
    " sometimes referred to as the `` law of small numbers ''  can be phrased as saying that , if @xmath15 bern(@xmath18 ) , then , @xmath22 where po(@xmath23 ) denotes the poisson distribution with parameter @xmath24 .",
    "one of the main points of this work is to show that this result holds for very wide class of distributions @xmath2 , and to provide conditions under which several stronger and more general versions of ( [ eq : lsn ] ) can be obtained .",
    "we refer to results of the form ( [ eq : lsn ] ) as _ laws of thin numbers_.    section  [ sec : exclass ] contains numerous examples that illustrate how particular families of random variables behave on thinning , and it also introduces some of the particular classes of random variables that will be considered in the rest of the paper . in sections  [ sec : ltn ] and  [ sec : ltn2 ] several versions of the law of thin numbers are formulated ; first for i.i.d.random variables in section  [ sec : ltn ] , and then for general classes of ( not necessarily independent or identically distributed ) random variables in section  [ sec : ltn2 ] .",
    "for example , in the simplest case where @xmath25 are i.i.d.with distribution @xmath2 on @xmath12 and with mean @xmath23 , so that the distribution of their sum , @xmath26 , is @xmath27 , theorem  [ thm : ltnstrong ] shows that , @xmath28 as long as @xmath29 , where , as usual , @xmath30 denotes the _ information divergence _ , or _ relative entropy _ , from @xmath2 to @xmath31 , denotes the natural logarithm to base @xmath32 , and we adopt the usual convention that @xmath33 . ]",
    "@xmath34 note that , unlike most classical poisson convergence results , the law of thin numbers in ( [ eq : ltn1 ] ) proves a poisson limit theorem for the sum of a single sequence of random variables , rather than for a triangular array .",
    "it may be illuminating to compare the result ( [ eq : ltn1 ] ) with the information - theoretic version of the central limit theorem ( clt ) ; see , e.g. , @xcite@xcite .",
    "suppose @xmath25 are i.i.d.continuous random variables with density @xmath35 on @xmath36 , and with zero mean and unit variance .",
    "then the density of their sum @xmath26 , is the @xmath14-fold convolution @xmath37 of @xmath35 with itself .",
    "write @xmath38 for the standard scaling operation in the clt regime : if a continuous random variable @xmath1 has density @xmath35 , then @xmath39 is the density of the scaled random variable @xmath40 , and , in particular , the density of the standardized sum @xmath41 is @xmath42 .",
    "the information - theoretic clt states that , if @xmath43 , we have , @xmath44 where @xmath45 is the standard normal density .",
    "note the close analogy between the statements of the law of thin numbers in  ( [ eq : ltn1 ] ) and the clt in  ( [ eq : clt ] ) .    before describing the rest of our results , we mention that there is a significant thread in the literature on thinning limit theorems and associated results for point processes .",
    "convergence theorems of the `` law of thin numbers '' type , as in ( [ eq : lsn ] ) and ( [ eq : ltn1 ] ) , were first examined in the context of queueing theory by palm @xcite and khinchin @xcite , while more general results were established by grigelionis @xcite .",
    "see the discussion in the text , @xcite , for details and historical remarks ; also see the comments following theorem  [ thm : ltnweak2 ] in section  [ sec : ltn2 ] . more specifically ,",
    "this line of work considered asymptotic results , primarily in the sense of weak convergence , for the distribution of a superposition of the sample paths of independent ( or appropriately weakly dependent ) point processes .",
    "here we take a different direction and , instead of considering the full infinite - dimensional distribution of a point process , we focus on finer results ",
    "e.g. , convergence in information divergence and non - asymptotic bounds  for the one - dimensional distribution of the thinned sum of integer - valued random variables .    with these goals in mind , before examining the finite-@xmath46 behavior of @xmath47 , in section  [ sec : mc ] we study a simpler but related problem , on the convergence of a continuous - time `` thinning '' markov chain on @xmath48 .",
    "in the present context , this chain plays a role parallel to that of the ornstein - uhlenbeck process in the context of gaussian convergence and the entropy power inequality @xcite@xcite@xcite .",
    "we show that the thinning markov chain has the poisson law as its unique invariant measure , and we establish its convergence both in total variation and in terms of information divergence . moreover , in theorem  [ thm : chisquare ] we characterize precisely the rate at which it converges to the poisson law in terms of the @xmath49 distance , which also leads to an upper bound on its convergence in information divergence .",
    "a new characterization of the poisson distribution in terms of thinning is also obtained .",
    "the main technical tool used here is based on an examination of the @xmath50 properties of the poisson - charlier polynomials in the thinning context .    in section  [ sec : ub ]",
    "we give both asymptotic and finite-@xmath14 bounds on the rate of convergence for the law of thin numbers .",
    "specifically , we employ the _ scaled fisher information _ functional introduced in @xcite to give precise , explicit bounds on the divergence , @xmath51 .",
    "an example of the type of result we prove is the following : suppose @xmath1 is an ultra bounded ( see definition  [ def : classes ] in section  [ sec : exclass ] ) random variable , with distribution @xmath2 , mean @xmath23 , and finite variance @xmath52",
    ". then , @xmath53 for a nonzero constant @xmath54 we explicitly identify ; cf .",
    "corollary  [ cor : ub ] .    similarly , in section  [ sec : tv ]",
    "we give both finite-@xmath14 and asymptotic bounds on the law of small numbers in terms of the total variation distance , @xmath55 , between @xmath47 and the @xmath56 distribution .",
    "in particular , theorem  [ 2tv ] states that if @xmath57 has mean @xmath23 and finite variance @xmath58 , then , for all @xmath14 , @xmath59    a closer examination of the monotonicity properties of the scaled fisher information in relation to the thinning operation is described in section  [ sec : monotone ] .",
    "finally , section  [ sec : comp ] shows how the idea of thinning can be extended to compound poisson distributions .",
    "the appendix contains the proofs of some of the more technical results .",
    "finally we mention that , after the announcement of the present results in @xcite , yu @xcite also obtained some interesting , related results .",
    "in particular , he showed that the conditions of the strong and thermodynamic versions of the law of thin numbers ( see theorems  [ thm : ltnstrong ] and  [ thm : ltnthermo ] ) can be weakened , and he also provided conditions under which the convergence in these limit theorems is monotonic in @xmath14 .",
    "this section contains several examples of the thinning operation , statements of its more basic properties , and the definitions of some important classes of distributions that will be play a central role in the rest of this work .",
    "the proofs of all the lemmas and propositions of this section are given in the appendix .",
    "note , first , two important properties of thinning that are immediate from its definition :    1 .",
    "the thinning of a sum of independent random variables is the convolution of the corresponding thinnings .    2 .  for all @xmath60 $ ] and any distribution @xmath2 on @xmath48 ,",
    "we have , @xmath61    thinning preserves the poisson law , in that @xmath62 .",
    "this follows from  ( [ eq : intro2 ] ) , since , @xmath63 where po@xmath64 , @xmath65 , denotes the poisson mass function .",
    "as it turns out , the factorial moments of a thinned distribution are easier to work with than ordinary moments .",
    "recall that the _ @xmath66th factorial moment _ of @xmath1 is @xmath67 $ ] , where @xmath68 denotes the falling factorial , @xmath69    the factorial moments of an @xmath4-thinning are easy to calculate :    [ lem : scalmom ] for any random variable @xmath70 with distribution @xmath2 on @xmath48 and for @xmath71 , writing @xmath72 for a random variable with distribution @xmath73 : @xmath74   = \\alpha^{k}e [   y^{\\underline { k } } ]   .\\;\\;\\;\\mbox { for all $ k$.}\\ ] ] that is , thinning scales factorial moments in the same way as ordinary multiplication scales ordinary moments .",
    "we will use the following result , which is a multinomial version of vandermonde s identity and is easily proved by induction .",
    "the details are omitted .",
    "[ lem : fallfact ] the falling factorial satisfies the multinomial expansion , i.e. , for any positive integer @xmath75 , all integers @xmath76 , and any @xmath77 , @xmath78{cccc}k_{1 } & k_{2 } & \\cdots & k_{y}\\end{array } } { \\displaystyle\\prod\\limits_{i=1}^{y } } x_i^{\\underline{k_i}}.\\ ] ]    the following is a basic regularity property of the thinning operation .",
    "[ prop : inj ] for any @xmath71 , the map @xmath79 is injective .",
    "[ ex : berns ] thinning preserves the class of bernoulli sums .",
    "that is , the thinned version of the distribution of a finite sum of independent bernoulli random variables ( with possibly different parameters ) is also such a sum .",
    "this follows from property  1 stated in the beginning of this section , combined with the observation that the @xmath4-thinning of the bern(@xmath18 ) distribution is the bern@xmath80 distribution .",
    "in particular , thinning preserves the binomial family : @xmath81 .",
    "thinning by @xmath4 transforms a geometric distribution with mean @xmath23 into a geometric distribution with mean @xmath82 . recalling that the geometric distribution with mean @xmath23 has point probabilities,@xmath83 using ( [ eq : intro2 ] ) ,",
    "@xmath84 the sum of @xmath14 i.i.d .",
    "geometrics has a negative binomial distribution .",
    "thus , in view of this example and property  1 stated in the beginning of this section , the thinning of a negative binomial distribution is also negative binomial .",
    "partly motivated by these examples , we describe certain classes of random variables ( some of which are new ) .",
    "these appear as natural technical assumptions in the subsequent development of our results .",
    "the reader may prefer to skip the remainder of this section and only refer back to the definitions when necessary .",
    "[ def : classes ]    1 .   a _ bernoulli sum _",
    "is a distribution that can be obtained from the sum of finitely many independent bernoulli random variables with possibly different parameters .",
    "the class of bernoulli sums with mean @xmath23 is denoted by @xmath85 and the the union @xmath86 is denoted by @xmath87 2 .",
    "a distribution @xmath2 satisfying the inequality @xmath88 is said to be _ ultra log - concave _",
    "( ulc ) ; cf .",
    "the set of ultra log - concave distributions with mean @xmath23 shall be denoted @xmath89 , and we also write @xmath90 for the union @xmath91 note that ( [ eq : ulc ] ) is satisfied for a single value of @xmath24 if and only if it is satisfied for all @xmath24 .",
    "the distribution of a random variable @xmath1 that satisfies @xmath92 \\leq\\lambda e [   x^{\\underline{k } } ]   $ ] for all @xmath93 will be said to be _ ultra bounded _ ( ub ) with ratio @xmath23 .",
    "the set of ultra bounded distributions with this ratio is denoted @xmath94 4 .",
    "the distribution of a random variable @xmath1 satisfying @xmath67 \\leq\\lambda^{k}$ ] for all @xmath93 will be said to be _ poisson bounded _",
    "( pb ) with ratio @xmath23 .",
    "the set of poisson bounded distributions with this ratio is denoted @xmath95 5 .",
    "a random variable will be said to be ulc , ub or pb , if its distribution is ulc , ub or pb , respectively .    first we mention some simple relationships between these classes .",
    "walkup @xcite showed that if @xmath96 and @xmath97 then @xmath98 .",
    "hence @xmath99 . in @xcite",
    "it was shown that , if @xmath100 then @xmath101 . clearly , @xmath102 .",
    "further , @xmath2 is poisson bounded if and only if the @xmath4-thinning @xmath103 is poisson bounded , for some @xmath104 .",
    "the same holds for ultra boundedness .",
    "[ prop : ub ] in the notation of definition  [ def : classes ] , @xmath105 .",
    "that is , if the distribution of @xmath1 is in @xmath89 then @xmath92   \\leq\\lambda e [   x^{\\underline{k } } ]   .$ ]    the next result states that the pb and ub properties are preserved on summing and thinning .",
    "[ prop : preserve ]    1 .",
    "if @xmath106 and @xmath107 are independent , then @xmath108 and @xmath109 2 .",
    "if @xmath110 and @xmath111 are independent , then @xmath112 and @xmath113    formally , the above discussion can be summarized as , @xmath114 finally , we note that each of these classes of distributions is `` thinning - convex , '' i.e. , if @xmath2 and @xmath31 are element of a set then @xmath115 is also an element of the same set . in particular , thinning maps each of these sets into itself , since @xmath116 where @xmath117 , the point mass at zero , has @xmath118",
    "in this section we state and prove three versions of the law of thin numbers , under appropriate conditions ; recall the relevant discussion in the introduction .",
    "theorem  [ thm : ltnweak ] proves convergence in total variation , theorem  [ thm : ltnthermo ] in entropy , and  theorem  [ thm : ltnstrong ] in information divergence .",
    "recall that the total variation distance @xmath119 between two probability distributions @xmath120 on @xmath12 is , @xmath121    [ weak version][thm : ltnweak ] for any distribution @xmath2 on @xmath12 with mean @xmath23 , @xmath122    in view of scheff s lemma , pointwise convergence of discrete distributions is equivalent to convergence in total variation , so it suffices to show that , @xmath123 converges to @xmath124 for all @xmath125    note that @xmath126 and that ( [ eq : intro2 ] ) implies the following elementary bounds for all @xmath4 , using jensen s inequality:@xmath127 since for i.i.d .",
    "variables @xmath128 , the probability @xmath129 , taking @xmath130 we obtain , @xmath131 now , for any fixed value of @xmath132 and @xmath14 tending to infinity,@xmath133 and@xmath134 and by monotone convergence , @xmath135 therefore,@xmath136 since all @xmath137 are probability mass functions and so is po@xmath138 , the above @xmath139 is necessarily a limit .    as usual ,",
    "the entropy of a probability distribution @xmath2 on @xmath48 is defined by , @xmath140    [ thermodynamic version][thm : ltnthermo ] for any poisson bounded distribution @xmath2 on @xmath12 with mean @xmath23 , @xmath141    the distribution @xmath142 converges pointwise to the poisson distribution so , by dominated convergence , it is sufficient to prove that @xmath143 is dominated by a summable function .",
    "this easily follows from the simple bound in the following lemma .",
    "[ lem : pbb ] suppose @xmath2 is poisson bounded with ratio @xmath144 . then , @xmath145 for all @xmath65 .",
    "note that , for all @xmath146 , @xmath147 so that , in particular , @xmath148 , and , @xmath149    according to ( * ? ? ?",
    "* proof of theorem 2.5 ) , @xmath150 if @xmath2 is ultra log - concave , so for such distributions the theorem states that the entropy converges to its maximum .",
    "for ultra log - concave distributions the thermodynamic version also implies convergence in information divergence .",
    "this also holds for poisson bounded distributions , which is easily proved using dominated convergence . as shown in the next theorem , convergence in information divergence",
    "can be established under quite general conditions .",
    "[ strong version ] [ thm : ltnstrong ] for any distribution @xmath2 on @xmath12 with mean @xmath23 and @xmath151 , @xmath152    the proof of theorem  [ thm : ltnstrong ] is given in the appendix ; it is based on a straightforward but somewhat technical application of the following general bound .",
    "[ prop : llogl ] let @xmath1 be a random variable with distribution @xmath2 on @xmath12 and with finite mean @xmath153 , for some @xmath71 .",
    "if @xmath154 , then , d(t_(p)po ( ) ) + e < .",
    "[ eq : pre - var - state ]    first note that , since @xmath2 has finite mean , its entropy is bounded by the entropy of a geometric with the same mean , which is finite , so @xmath155 is finite .",
    "therefore , the divergence @xmath156 can be expanded as , d(ppo ( ) ) & = & e + & = & e[(x!)]+-h(p)- ( ) .",
    "+ & & e[^+(2x)]+e[xx ] -h(p ) - ( ) , [ eq : dexpand ] where the last inequality follows from the stirling bound , @xmath157 and @xmath158 denotes the function @xmath159 . since @xmath160 , ( [ eq : dexpand ] ) implies that @xmath161 $ ] is finite .",
    "[ recall the convention that @xmath162 . ]",
    "also note that the representation of @xmath10 in ( [ eq : intro2 ] ) can be written as , @xmath163 using this and the joint convexity of information divergence in its two arguments ( see , e.g. , ( * ? ? ?",
    "* theorem  2.7.2 ) ) , the divergence of interest can be bounded as , @xmath164 where the first term ( corresponding to @xmath165 ) equals @xmath23 . since the poisson measures form an exponential family , they satisfy a pythagorean identity @xcite which , together with the bound , d((x , p)(xp ) ) , [ eq : elementary ] see , e.g. , @xcite or @xcite , gives , for each @xmath166 , d((x , ) po ( ) ) & = & d((x , ) po(x ) ) + d((x ) po ( ) ) + & & + _ j=0^ po(x ,",
    "j ) ( ) + & = & + ( x ( ) -x+ ) . since the final bound clearly remains valid for @xmath165 , substituting it into ( [ eq : pre - var1 ] ) gives ( [ eq : pre - var - state ] ) .",
    "in this section we state and prove more general versions of the law of thin numbers , for sequences of random variables that are not necessarily independent or identically distributed .",
    "although some of the results in this section are strict generalizations of theorems  [ thm : ltnweak ] and  [ thm : ltnstrong ] , their proofs are different .",
    "we begin by showing that , using a general proof technique introduced in @xcite , the weak law of thin numbers can be established under weaker conditions than those in theorem  [ thm : ltnweak ] .",
    "the main idea is to use the data - processing inequality on the total variation distance between an appropriate pair of distributions .",
    "[ weak version , non - i.i.d . ]",
    "[ thm : ltnweak2 ] let @xmath167 be an arbitrary sequence of distributions on @xmath48 , and write @xmath168 for the convolution of the first @xmath14 of them . then , @xmath169 as long as the following three conditions are satisfied as @xmath170 :    * @xmath171\\to 0 $ ] ; * @xmath172\\to\\lambda$ ] ; * @xmath173 \\to 0 $ ] .    note that theorem  [ thm : ltnweak2 ] can be viewed as a one - dimensional version of grigelionis theorem  1 in @xcite ; recall the relevant comments in the introduction .",
    "recently , schuhmacher @xcite established nonasymptotic , quantitative versions of this result , in terms of the barbour - brown distance , which metrizes weak convergence in the space of probability measures of point processes .",
    "as the information divergence is a finer functional than the barbour - brown distance , schuhmacher s results are not directly comparable with the finite-@xmath14 bounds we obtain in propositions  [ prop : llogl ] ,  [ prop : var ] and corollary  [ cor : ub ] .    before giving the proof of the theorem , we state a simple lemma on a well - known bound for @xmath174 .",
    "its short proof is included for completeness .",
    "[ lem : simpbd ] for any @xmath175 , @xmath176 , and define two independent random variables @xmath177 po@xmath178 and @xmath179 po@xmath180 , so that , @xmath181 po@xmath138 . then , by the coupling inequality @xcite , @xmath182.\\ ] ] the second inequality in the lemma is trivial .    _",
    "proof of theorem  [ thm : ltnweak2 ] : _ first we introduce some convenient notation .",
    "let @xmath183 be independent random variables with @xmath184 for all @xmath185 ; for each @xmath186 , let @xmath187 be independent random variables with @xmath188 for all @xmath185 ; and similarly let @xmath189 be independent po(@xmath190 random variables , where @xmath191 , for @xmath192 . also we define the sums , @xmath193 and @xmath194 , and",
    "note that , @xmath195 , and @xmath196 po(@xmath197 ) , where @xmath198 , for all @xmath186 .",
    "note that @xmath199 as @xmath170 , since , @xmath200 and , by assumption , @xmath201 and @xmath202 , as @xmath170 .    with these definitions in place ,",
    "we approximate , t_1/n(p^(n))- ( ) t_1/n(p^(n))-(^(n ) ) + ( ^(n))- ( ) , [ eq : tria ] where , by lemma  [ lem : simpbd ] , the second term is bounded by @xmath203 which vanishes as @xmath204 therefore , it suffices to show that the first term in ( [ eq : tria ] ) goes to zero . for that term , t_1/n(p^(n))-po(^(n ) ) & = & p_s_n - p_t_n + & & p_\\{y_i^(n)}- p_\\{z_i^(n ) } + & & _ i=1^n t_1/np_i- ( _ i^(n ) ) + & & _ i=1^n , where the first inequality above follows from the fact that , being an @xmath35-divergence , the total variation distance satisfies the data - processing inequality @xcite ; the second inequality comes from the well - known bound on the total variation distance between two product measures as the sum of the distances between their respective marginals ; and the third bound is simply the triangle inequality .    finally , noting that , for any random variable @xmath57 , @xmath205 , and also recalling the simple estimate , @xmath206 yields , t_1/n(p^(n))-po(^(n ) ) c_n + _",
    "i=1^n(_i^(n))^2 c_n+^(n)_1 in _",
    "i^(n ) c_n+^(n)a_n , and , by assumption , this converges to zero as @xmath170 , completing the proof .",
    "recall that , in the i.i.d .",
    "case , the weak law of thin numbers only required the first moment of @xmath2 to be finite , while the strong version also required that the divergence from @xmath2 to the poisson distribution be finite . for a sum of independent , non - identically distributed random variables with finite _ second _ moments , proposition  [ prop : llogl ] can be used as in the proof of theorem  [ thm : ltnstrong ] to prove the following result .",
    "note that the precise conditions required are somewhat analogous to those in theorem  [ thm : ltnweak2 ] .",
    "[ strong version , non - i.i.d . ] [ thm : ltnstrong2 ] let @xmath167 be an arbitrary sequence of distributions on @xmath48 , where each @xmath207 has finite mean @xmath208 and finite variance . writing @xmath209 for the convolution @xmath210 , we have , @xmath211 as long as the following two conditions are satisfied :    * @xmath212 , as @xmath170 ; * @xmath213 .",
    "the proof of theorem  [ thm : ltnstrong2 ] is given in the appendix , and it is based on proposition  [ prop : llogl ] .",
    "it turns out that under the additional condition of finite second moments , the proof of proposition  [ prop : llogl ] can be refined to produce a stronger upper bound on the divergence .",
    "[ prop : var ] if @xmath2 is a distribution on @xmath12 with mean @xmath153 and variance @xmath214 , for some @xmath71 , then , @xmath215    recall that in the proof of proposition  [ prop : llogl ] it was shown that , d ( t_(p ) po ( ) ) _",
    "x=0^p(x ) d ( ( x , ) po ( ) ) , [ eq : pre - var1b ] where , @xmath216 and where in the last step above we used the simple bound @xmath217 for @xmath218 . substituting ( [ eq : tosub2 ] ) into ( [ eq : pre - var1b ] ) yields , @xmath219 as claimed .    using the bound ( [ varulig ] ) instead of proposition  [ prop : llogl ] , the following more general version of the law of thin numbers can be established :    [ strong version , non - i.i.d . ]",
    "[ thm : noniid ]",
    "let @xmath220 be a sequence of ( not necessarily independent or identically distributed ) random variables on @xmath12 , and write @xmath209 for the distribution of the partial sum @xmath221 , @xmath186 .",
    "assume that the @xmath220 have finite means and variances , and that :    1 .",
    "they are `` uniformly ultra bounded , '' in that , @xmath222 for all @xmath185 , with a common @xmath223 ; 2 .",
    "their means satisfy @xmath224 as @xmath170 ; 3 .",
    "their covariances satisfy , @xmath225    if in fact @xmath226 for all @xmath185 , then , @xmath227 more generally , @xmath228    obviously it suffices to prove the general statement .",
    "proposition  [ prop : var ] applied to @xmath209 gives , @xmath229 the first and third terms tend to zero by assumptions  ( b ) and  ( c ) , respectively . and",
    "using assumption  ( a ) , the second term is bounded above by , @xmath230 which also tends to zero by assumption  ( b ) .",
    "before examining the rate of convergence in the law of thin numbers , we consider a related and somewhat simpler problem for a markov chain .",
    "several of the results in this section may be of independent interest .",
    "the markov chain we will discuss was first studied in ( * ? ? ?",
    "* proof of theorem  2.5 ) , and , within this context , it is a natural discrete analog of the ornstein - uhlenbeck process associated with the gaussian distribution .",
    "let @xmath2 be a distribution on @xmath48 .",
    "for any @xmath231 $ ] and @xmath24 , we write @xmath232 for the distribution , @xmath233 for simplicity , @xmath232 is often written simply as @xmath234 .",
    "we note that @xmath235 , and that , obviously , @xmath236 maps probability distributions to probability distributions . therefore , if for a _ fixed _",
    "@xmath23 we define @xmath237 for all @xmath238 , the collection @xmath239 of linear operators on the space of probability measures on @xmath48 defines a markov transition semigroup . specifically , for @xmath240 ,",
    "the transition probabilities , @xmath241 define a continuous - time markov chain @xmath242 on @xmath48 .",
    "it is intuitively clear that , as @xmath243 ( or , equivalently , @xmath244 ) , the distribution @xmath245 should converge to the @xmath56 distribution .",
    "indeed , the following two results state that @xmath246 is ergodic , with unique invariant measure @xmath56 .",
    "theorem  [ thm : chisquare ] gives the rate at which it converges to @xmath56 .",
    "[ prop : tvconv ] for any distribution @xmath2 on @xmath48 , @xmath247 converges in total variation to @xmath248 , as @xmath243 .    from the definition of @xmath249 , @xmath250      + \\frac{1}{2 } \\sum_{x=1}^{\\infty }      ( t_\\alpha ( p)(x ) + { { { \\rm{po}}}}(\\alpha \\lambda , x ) )       \\label{eq : tv2 } \\\\ & = &       2-t_{\\alpha}(p)(0)-{{{\\rm{po}}}}(\\alpha\\lambda,0 ) ,       \\label{eq : tv3}\\end{aligned}\\ ] ] where ( [ eq : tv1 ] ) follows from the fact that convolution with any distribution is a contraction with respect to the @xmath251 norm , ( [ eq : tv2 ] ) follows from the triangle inequality , and ( [ eq : tv3 ] ) converges to zero because of the bound ( [ eq : tap0 ] ) .    using this",
    ", we can give a characterization of the poisson distribution .",
    "[ cor : tvdconv ] let @xmath2 denote a discrete distribution with mean @xmath23 .",
    "if @xmath252 for some @xmath71 , then @xmath253 that is , @xmath56 is the unique invariant measure of the markov chain @xmath246 , and , moreover , @xmath254 if and only if @xmath255 for some @xmath256    assume that @xmath257 .",
    "then for any @xmath14 , @xmath258 , so for any @xmath259 , by proposition  [ prop : tvconv ] , @xmath260 for @xmath14 sufficiently large . the strengthened convergence of @xmath261 to zero if @xmath255 can be proved using standard arguments along the lines of the corresponding discrete - time results in @xcite@xcite@xcite .",
    "next we shall study the rate of convergence of @xmath262 to the poisson distribution .",
    "it is easy to check that the markov chain @xmath246 is in fact _",
    "reversible _ with respect to its invariant measure @xmath56 .",
    "therefore , the natural setting for the study of its convergence is the @xmath50 space of functions @xmath263 such that , @xmath264<\\infty$ ] for @xmath265 .",
    "this space is also endowed with the usual inner product , @xmath266 , \\;\\;\\;\\;\\mbox{for}\\;z\\sim\\mbox{po}(\\lambda ) , \\;f , g\\in l^2,\\ ] ] and the linear operators @xmath267 act on functions @xmath268 by mapping each @xmath35 into , @xmath269 \\;\\;\\;\\;\\mbox{for}\\;z_{\\alpha,\\lambda , x}\\sim u_\\alpha^\\lambda(\\delta_x).\\ ] ] in other words , @xmath270 , \\;\\;\\;\\;x\\in{\\mathbb n}_0.\\ ] ] the reversibility of @xmath246 with respect to @xmath56 implies that @xmath271 is a self - adjoint linear operator on @xmath50 , therefore , its eigenvectors are orthogonal functions . in this context , we introduce the poisson - charlier family of orthogonal polynomials @xmath272    for given @xmath23 , _ the poisson - charlier polynomial of order @xmath66 _ is given by , @xmath273    some well - known properties of the poisson - charlier polynomials are listed in the following lemma without proof .",
    "note that their exact form depends on the chosen normalization ; other authors present similar results , but with different normalizations .    for any @xmath274 and @xmath275:@xmath276    observe that , since the poisson - charlier polynomials form an orthonormal set , any function @xmath268 can be expanded as , f(x ) = _ k=0^f , p_k^p_k^(x ) . [ eq : pcexpand ]",
    "it will be convenient to be able to translate between factorial moments and the `` poisson - charlier moments , '' @xmath277   $ ] .",
    "for example , if @xmath278 , then taking @xmath279 in ( [ poisnorm ] ) shows that @xmath280=0 $ ] for all @xmath281 .",
    "more generally , the following proposition shows that the role of the poisson - charlier moments with respect to the markov chain @xmath246 is analogous to the role played by the factorial moments with respect to the pure thinning operation ; cf .",
    "lemma  [ lem : scalmom ] .",
    "its proof , given in the appendix , is similar to that of lemma  [ lem : scalmom ] .",
    "[ charlierexpo]let @xmath57 be a random variable with mean @xmath23 and write @xmath282 for a random variable with distribution @xmath232 .",
    "then , @xmath283 =   \\alpha^{k}e\\left [   p_{k}^{\\lambda } \\left (   x\\right )   \\right ]   .\\ ] ]    if we replace @xmath4 by @xmath284 and assume that the thinning markov chain @xmath246 has initial distribution @xmath285 with mean @xmath23 , then , proposition  [ charlierexpo ] states that , @xmath286 =   e^{-kt}e [   p_{k}^{\\lambda } (   z_0 ) ] , \\ ] ] that is , the poisson - charlier moments of @xmath287 tend to @xmath288 like @xmath289.$ ] similarly , expanding any @xmath268 in terms of poisson - charlier polynomials , @xmath290 , and using proposition  [ charlierexpo ] , @xmath291    = e\\left [   \\sum_{k=0}^{\\infty } \\langle f , p_k^\\lambda\\rangle p^\\lambda_{k}(z_t )   \\right ]    = \\sum_{k=0}^{\\infty } \\exp\\left (   -kt\\right )    \\langle f , p_k^\\lambda\\rangle e\\left [   p^\\lambda_{k } \\left(x\\right )   \\right].\\ ] ] thus , the rate of convergence of @xmath246 will be dominated by the term corresponding to @xmath292,$ ] where @xmath293 is the first @xmath281 such that @xmath294   \\neq0.$ ]    the following proposition ( proved in the appendix ) will be used in the proof of theorem  [ thm : chisquare ] below , which shows that this is indeed the right rate in terms of the @xmath49 distance .",
    "note that there is no restriction on the mean of @xmath57 in the proposition .",
    "[ prop : radon ] if @xmath57 is poisson bounded , then the the likelihood ratio @xmath295 can be expanded as : @xmath296 p_{k}^{\\lambda}(x ) , \\;\\;\\;\\;x\\geq 0.\\ ] ]    assuming @xmath106 , combining propositions  [ charlierexpo ] and  [ prop : radon ] , we obtain that , @xmath297   p_{k}^{\\lambda}(x ) \\nonumber \\\\ &   = & 1+\\sum_{k=\\kappa}^{\\infty}\\alpha^{k}e\\left [   p_{k}^{\\lambda}\\left ( x\\right )   \\right ]   p_{k}^{\\lambda}(x ) \\nonumber \\\\ &   = & 1+\\alpha^{\\kappa}\\sum_{k=\\kappa}^{\\infty}\\alpha^{k-\\kappa}e\\left [ p_{k}^{\\lambda}\\left (   x\\right )   \\right ]   p_{k}^{\\lambda}(x ) ,   \\label{eq : edgeworth}\\end{aligned}\\ ] ] where , as before , @xmath293 denotes the first integer @xmath281 such that @xmath294   \\neq0.$ ] this sum can be viewed as a discrete analog of the well - known edgeworth expansion for the distribution of a continuous random variable .",
    "a technical disadvantage of both this and the standard edgeworth expansion is that , although the sum converges in @xmath298 , truncating it to a finite number of terms in general produces an expression which may take negative values . by a more detailed analysis",
    "we shall see in the following two sections how to get around this problem .",
    "for now , we determine the rate of convergence of @xmath245 to @xmath56 in terms of the @xmath49 distance between @xmath245 and @xmath56 ; recall the definition of the @xmath49 distance between two probability distributions @xmath2 and @xmath31 on @xmath48 : @xmath299    [ thm : chisquare ] if @xmath57 is poisson bounded , then @xmath300 is finite for all @xmath231 $ ] and , @xmath301   ^{2 } , \\;\\;\\;\\;\\mbox{as}\\;\\alpha\\downarrow 0,\\ ] ] where @xmath293 denotes the smallest @xmath302 such that @xmath303   \\neq0.$ ]    the proof is based on a hilbert space argument using the fact that the poisson - charlier polynomials are orthogonal .",
    "suppose @xmath304 .",
    "using proposition  [ prop : radon ] , @xmath305 p_k^\\lambda(x)\\right)^2\\\\ & = \\sum_{k=\\kappa}^{\\infty}\\alpha^{2k}e[p_k^\\lambda(x)]^2,\\end{aligned}\\ ] ] where the last step follows from the orthogonality relation ( 22 ) .",
    "for @xmath306 we have , @xmath307 which is finite . from the previous expansion",
    "we see that @xmath308 is increasing in @xmath4 , which implies the finiteness claim . moreover , that expansion has @xmath309 ^ 2 $ ] as its dominant term , implying the stated limit .",
    "theorem  [ thm : chisquare ] readily leads to upper bounds on the rate of convergence in terms of information divergence via the standard bound , @xmath310 which follows from direct applications of jensen s inequality .",
    "furthermore , replacing this bound by the well - known approximation @xcite , @xmath311 gives the estimate , @xmath312   ^{2}}{2}=\\frac{e\\left [   p_{\\kappa}^{\\lambda}\\left ( u_{\\alpha}x\\right )   \\right ]   ^{2}}{2}.\\ ] ] we shall later prove that , in certain cases , this approximation can indeed be rigorously justified .",
    "let @xmath57 be a random variable on @xmath48 with mean @xmath23 .",
    "in theorem  [ thm : ltnstrong ] we showed that , if @xmath313 is finite , then , d(t_1/n(p^*n)po())0 , n. [ eq : ltni ] if @xmath2 also has finite variance @xmath58 , then proposition  [ prop : var ] implies that , for all @xmath314 , @xmath315 suggesting a convergence rate of order @xmath316 . in this section ,",
    "we prove more precise upper bounds on the rate of convergence in the strong law of thin numbers ( [ eq : ltni ] ) .",
    "for example , if @xmath1 is an ultra bounded random variable with @xmath52 , then we show that in fact , @xmath317 where @xmath318=(\\sigma^2-\\lambda)/(\\lambda\\sqrt{2})\\neq 0 $ ] .",
    "this follows from the more general result of corollary  [ cor : ub ] ; its proof is based on a detailed analysis of the _ scaled fisher information _ introduced in in @xcite .",
    "we begin by briefly reviewing some properties of the scaled fisher information :    the _ scaled fisher information _ of a random variable @xmath57 with mean @xmath23 , is defined by , @xmath319\\ ] ] where @xmath320 denotes the scaled score function , @xmath321    in ( * ? ? ?",
    "* proposition  2 ) it was shown , using a logarithmic sobolev inequality of bobkov and ledoux @xcite , that for any @xmath57 , d ( ppo ( ) ) k ( x ) , [ eq : logs ] under mild conditions on the support of @xmath2 .",
    "also , ( * ? ? ?",
    "* proposition  3 ) states that @xmath322 satisfies a subadditivity property : for independent random variables @xmath323 , k ( _ i=1^nx_i ) _ i=1^n k(x_i ) [ eq : ksub ] where @xmath324 in particular , recalling that the thinning of a convolution is the convolution of the corresponding thinnings , if @xmath323 are i.i.d .",
    "random variables with mean @xmath23 then the bounds in ( [ eq : logs ] ) and ( [ eq : ksub ] ) imply , d ( t_1/n ( p^n ) po ( ) ) k(t_1/n(p ) ) .",
    "[ eq : oldk ] therefore , our next goal is to determine the rate at which @xmath325 tends to @xmath288 for @xmath4 tending to @xmath326 we begin with the following proposition ; its proof is given in appendix .",
    "[ prop : altsum ] if @xmath57 is poisson bounded , then @xmath2 admits the representation , @xmath327   } { \\ell!}.\\ ] ] moreover , the truncated sum from @xmath279 to @xmath328 is an upper bound for @xmath329 if @xmath328 is even , and a lower bound if @xmath328 is odd .",
    "an important consequence of this proposition is that @xmath330 tends to zero like @xmath331 , as @xmath332 moreover , it leads to the following asymptotic result for the scaled fisher information , also proved in the appendix .",
    "[ thm : kthin ] suppose @xmath57 has mean @xmath23 and it is ultra bounded with ratio @xmath23 .",
    "let @xmath293 denote the smallest integer @xmath77 such that @xmath333   \\neq 0 $ ] .",
    "then , @xmath334 where @xmath335   .$ ]    combining theorem  [ thm : kthin ] with ( [ eq : oldk ] ) immediately yields :    [ cor : ub ] suppose @xmath57 has mean @xmath23 and it is ultra bounded with ratio @xmath23 .",
    "let @xmath293 denote the smallest integer @xmath77 such that @xmath333   \\neq 0 $ ] .",
    "then , @xmath336 where @xmath335   .$ ]",
    "in this section we establish a finer result for the behavior of the scaled fisher information upon thinning , and use that to deduce a stronger finite-@xmath14 upper bound for the strong law of thin numbers . specifically ,",
    "if @xmath57 is ulc with mean @xmath23 , and @xmath337 denotes a random variable with distribution @xmath11 , we will show that @xmath338 .",
    "this implies that , for all ulc random variables @xmath1 , we have the following finite-@xmath14 version of the strong law of thin numbers , @xmath339 note that , unlike the more general result in ( [ eq : linear ] ) which gives a bound of order @xmath316 , the above bound is of order @xmath340 , as long as @xmath1 is ulc .",
    "the key observation for these results is in the following lemma .",
    "[ lem : kder ] suppose @xmath1 is a ulc random variable with distribution @xmath2 and mean @xmath23 . for any @xmath341 , write @xmath337 for a random variable with distribution @xmath11 .",
    "then the derivative of @xmath342 with respect to @xmath4 satisfies , @xmath343 where , for a random variable @xmath70 with mass function @xmath31 and mean @xmath144 , we define , @xmath344    this result follows on using the expression for the derivative of @xmath11 arising as the case @xmath345 in proposition 3.6 of @xcite , that is , @xmath346.\\ ] ] using this , for each @xmath146 we deduce that , @xmath347 the result follows ( with the term - by - term differentiation of the infinite sum justified ) if the sum of these terms in @xmath146 is absolutely convergent .",
    "the first terms are positive , and their sum is absolutely convergent to @xmath348 by assumption .",
    "the second terms form a collapsing sum , which is absolutely convergent assuming that , @xmath349 note that , for any ulc distribution @xmath31 , by definition we have for all @xmath146 , @xmath350 , so that the above sum is bounded above by , @xmath351 which is finite by proposition  [ prop : ub ] .",
    "we now deduce the following theorem , which parallels theorem 8 respectively of @xcite , where a corresponding result is proved for the information divergence .",
    "let @xmath57 be a ulc random variable with mean @xmath23 .",
    "write @xmath337 for a random variable with distribution @xmath11 .",
    "then : @xmath352    the first part follows from the observation that @xmath353 is increasing in @xmath4 , since , by lemma  [ lem : kder ] , its derivative is @xmath354 .",
    "taking @xmath355 in the more technical lemma  [ lem : poincarish ] below , we deduce that @xmath356 for any random variable @xmath70 , and this proves  @xmath357 .",
    "then  @xmath358 immediately follows from  @xmath357 combined with the earlier bound  ( [ eq : oldk ] ) , upon recalling that thinning preserves the ulc property @xcite .",
    "consider the finite difference operator @xmath359 defined by , @xmath360 , for functions @xmath361 .",
    "we require a result suggested by relevant results in @xcite@xcite .",
    "its proof is given in the appendix .",
    "[ lem : poincarish ] let @xmath70 be ulc random variable with distribution @xmath2 on @xmath48 .",
    "then for any function @xmath362 , defining @xmath363 , @xmath364",
    "in this section , we show that a modified version of the argument used in the proof of proposition  [ prop : var ] gives an upper bound to the rate of convergence in the weak law of small numbers . if @xmath57 has mean @xmath23 and variance @xmath58 , then combining the bound ( [ varulig ] ) of proposition  [ prop : var ] with pinsker s inequality we obtain , t_1/n ( p^n ) -po ( ) ( + ) ^1/2 , [ eq : weakb ] which gives an upper bound of order @xmath365 from the asymptotic upper bound on information divergence , corollary  [ cor : ub ] , we know that one should be able to obtain upper bounds of order @xmath366 here we derive an upper bound on total variation using the same technique used in the proof of proposition  [ prop : var ] .",
    "[ 2tv ] let @xmath2 be a distribution on @xmath12 with finite mean @xmath23 and variance @xmath367",
    ". then , @xmath368 for all @xmath369 .",
    "the proof uses the following simple bound , which follows easily from a result of yannaros , ( * ? ? ?",
    "* theorem  2.3 ) ; the details are omitted .    [ lemma3]for any @xmath24 , @xmath370 and @xmath371 $ ] , we have , @xmath372    the first inequality in the proof of proposition  [ prop : var ] remains valid due to the convexity of the total variation norm ( since it is an @xmath35-divergence ) .",
    "the next equality becomes an inequality , and it is justified by the triangle , and we have : @xmath373 \\right\\vert \\nonumber\\\\ &   \\leq\\sum_{y\\geq0}p^{*n}(y)\\frac{1}{2}\\sum_x\\left\\vert    \\pr\\{\\mathrm{{bin}}(y,1/n)=x\\}-{{{\\rm{po}}}}(\\lambda , x ) \\right\\vert \\nonumber\\\\ &   = \\sum_{y\\geq0}p^{*n}(y)\\vert\\mathrm{{bin}}(y,1/n)-{{{\\rm{po}}}}(\\lambda ) \\vert.\\nonumber\\end{aligned}\\ ] ] and using lemma  [ lemma3 ] leads to , @xmath374 and the result follows by an application of hlder s inequality .",
    "there is a natural generalization of the thinning operation , via a process which closely parallels the generalization of the poisson distribution to the compound poisson . starting with a random variable @xmath375 with values in @xmath12 , the @xmath4-thinned version of @xmath70",
    "is obtained by writing @xmath376 ( @xmath70 times ) , and then keeping each of these @xmath377s with probability @xmath4 , independently of all the others ; cf .",
    "( [ eq : intro ] ) above .",
    "more generally , we choose and fix a `` compounding '' distribution @xmath31 on @xmath378 . given @xmath375 on @xmath48 and @xmath231 $ ] ,",
    "then the _ compound @xmath4-thinned version of @xmath70 with respect to q _ , or , for short , the _ @xmath379-thinned version of @xmath70 _ , is the random variable which results from first thinning @xmath70 as above and then replacing of the @xmath377s that are kept by an independent random sample from @xmath31 , @xmath380 where all the random variables involved are independent . for fixed @xmath4 and @xmath31 , we write @xmath381 for the distribution of the @xmath379-thinned version of @xmath382 then @xmath381 can be expressed as a mixture of compound binomials  in the same way as @xmath5 is a mixture of binomials .",
    "the _ compound binomial distribution _ with parameters @xmath383 denoted @xmath384 , is the distribution of the sum of @xmath14 i.i.d .",
    "random variables , each of which is the product of a bern@xmath385 random variable and an independent @xmath386 random variable . in other words , it is the @xmath379-thinned version of the point mass at @xmath14 , i.e. , the distribution of ( [ eq : thinned - sum ] ) with @xmath387 w.p.1",
    ". then we can express the probabilities of the @xmath379-thinned version of @xmath2 as , @xmath388 .",
    "the following two observations are immediate from the definitions .    1 .",
    "compound thinning maps a bernoulli sum into a compound bernoulli sum : if @xmath2 is the distribution of the bernoulli sum @xmath389 where the @xmath390 are independent bern@xmath391 , then @xmath381 is the distribution of the `` compound bernoulli sum , '' @xmath392 where the @xmath393 are independent bern@xmath394 , and the @xmath395 are i.i.d .  with distribution @xmath31 , independent of the @xmath390 .",
    "compound thinning maps the poisson to the compound poisson distribution , that is , @xmath396 , the compound poisson distribution with rate @xmath82 and compounding distribution @xmath31 . recall that @xmath397 is defined as the distribution of , @xmath398 where the @xmath395 are as before , and @xmath399 is a @xmath56 random variable that is independent of the @xmath395 .",
    "perhaps the most natural way in which the compound poisson distribution arises is as the limit of compound binomials .",
    "that is , @xmath400 as @xmath401 , or , equivalently , @xmath402 where @xmath2 denotes the bern@xmath138 distribution .",
    "as with the strong law of thin numbers , this results remains true for general distributions @xmath2 , and the convergence can be established in the sense of information divergence :    let @xmath2 be a distribution on @xmath12 with mean @xmath24 and finite variance @xmath367 .",
    "then , for any probability measure @xmath31 on @xmath403 , @xmath404 as long as @xmath405 .",
    "the proof is very similar to that of theorem  [ thm : ltnstrong ] and thus omitted .",
    "in fact , the same argument as that proof works for non - integer - valued compounding . that is , if @xmath31 is an _",
    "probability measure on @xmath406 , then compound thinning a @xmath12-valued random variable @xmath375 as in ( [ eq : thinned - sum ] ) gives a probability measure @xmath381 on @xmath406 .",
    "it is somewhat remarkable that the statement _ and _ proof of most of our results concerning the information divergence remain essentially unchanged in this case . for example , we easily obtain the following analog of proposition  [ prop : var ] .    if @xmath2 is a distribution on @xmath12 with mean @xmath153 and variance @xmath214 , for some @xmath71 , then , for any probability measure @xmath31 on @xmath406 , @xmath407    the details of the argument of the proof of the proposition are straightforward extensions of the corresponding proof of proposition  [ prop : var ] .",
    "the authors wish to thank emre telatar and christophe vignat for hosting a small workshop in january 2006 , during which some of these ideas developed .",
    "jan swart also provided us with useful comments .",
    "_ proof of lemma  [ lem : scalmom ] : _ simply apply lemma  [ lem : fallfact ] to definition  [ def : thin ] with @xmath375 , to obtain , e[y_^ ] & = & e + & = & e\\ { e } + & = & e\\ { e } + & = & e + & = & ^ke[y^ ] , using the fact that the sequence of factorial moments of the bern(@xmath4 ) distribution are @xmath408 .      _ proof of proposition  [ prop : inj ] : _ assume that @xmath409 for a given @xmath410 then , recalling the property stated in  ( [ eq : semigroup ] ) , it follows that , @xmath411 for all @xmath412 $ ] .",
    "in particular , @xmath413 for all @xmath414 $ ] , i.e. , @xmath415 for all @xmath412,$ ] which is only possible if @xmath416 for all @xmath65 .",
    "_ proof of proposition  [ prop : ub ] : _ note that the expectation , @xmath417 by the chebyshev rearrangement lemma , since it is the covariance between an increasing and a decreasing function . rearranging this inequality",
    "gives , @xmath418   = \\sum_{x=0}^{\\infty}p(x+1 ) (   x+1 )   ^{\\underline{k+1}}\\leq\\lambda\\sum_{x=0}^{\\infty}p(x )    x^{\\underline{k}}=\\lambda e [   x^{\\underline{k}}],\\ ] ] as required .",
    "_ proof of proposition  [ prop : preserve ] : _ to prove part ( a ) , using lemma  [ lem : fallfact ] , we have , @xmath419    &   = e\\big [   \\sum _ { \\ell=0}^{k}\\binom{k}{\\ell}x^{\\underline{\\ell}}y^{\\underline{k-\\ell}}\\big ]   \\\\ &   = \\sum_{\\ell=0}^{k}\\binom{k}{\\ell}e [   x^{\\underline{\\ell } } ]   e [ y^{\\underline{k-\\ell } } ]   \\\\ &   \\leq\\sum_{\\ell=0}^{k}\\binom{k}{\\ell}\\lambda^{\\ell}\\mu^{k-\\ell}\\\\ &   =(   \\lambda+\\mu )   ^{k}.\\end{aligned}\\ ] ] it is straightforward to check , using lemma  [ lem : scalmom ] , that @xmath420 .    to prove part ( b ) , using lemma  [ lem : fallfact ] , pascal s identity and relabelling , yields , @xmath421    &   = e\\left [ \\sum_{\\ell=0}^{k+1 } \\binom{k+1}{\\ell}x^{\\underline{\\ell}}y^{\\underline{k+1-\\ell}}\\right ] \\\\ &   = e\\left [   \\sum_{\\ell=0}^{k+1}\\left (   \\binom{k}{\\ell-1}+\\binom{k}{\\ell}\\right ) x^{\\underline{\\ell}}y^{\\underline{k+1-\\ell}}\\right ] \\\\ &   = \\sum_{\\ell=0}^{k+1}\\binom{k}{\\ell-1}e\\left [   x^{\\underline{\\ell } } y^{\\underline { k+1-\\ell}}\\right ]   + \\sum_{\\ell=0}^{k+1}\\binom{k}{\\ell } e\\left [   x^{\\underline{\\ell}}y^{\\underline{k+1-\\ell}}\\right ] \\\\ &   = \\sum_{\\ell=0}^{k}\\binom{k}{\\ell } e\\left [   x^{\\underline{\\ell+1}}\\right ]   e\\left [ y^{\\underline{k-\\ell}}\\right ]   + \\sum_{\\ell=0}^{k } \\binom{k}{\\ell}e\\left [   x^{\\underline { \\ell}}\\right ]   e\\left [   y^{\\underline{k+1-\\ell}}\\right ] \\\\ &   \\leq\\sum_{\\ell=0}^{k}\\binom{k}{\\ell}\\lambda   e\\left [   x^{\\underline{\\ell}}\\right ] e\\left [   y^{\\underline{k-\\ell}}\\right ]   + \\sum_{\\ell=0}^{k}\\binom{k}{\\ell } e\\left [ x^{\\underline{\\ell}}\\right ]   \\mu e\\left [   y^{\\underline{k-\\ell}}\\right ] \\\\ &   =(   \\lambda+\\mu )   e [   (   x+y )   ^{\\underline{k } } ]   .\\end{aligned}\\ ] ] the second property is again easily checked using lemma  [ lem : scalmom ] .",
    "_ proof of theorem  [ thm : ltnstrong ] : _ in order to apply proposition  [ prop : llogl ] with @xmath27 in place of @xmath2 and @xmath130 , we need to check that @xmath422 is finite .",
    "let @xmath423 denote the sum of @xmath14 i.i.d.random variables @xmath424 , so that @xmath27 is the distribution of @xmath423 .",
    "similarly , @xmath425 is the sum of @xmath14 independent @xmath56 variables .",
    "therefore , using the data - processing inequality @xcite as in @xcite implies that @xmath426 , which is finite by assumption .",
    "proposition  [ prop : llogl ] gives , @xmath427      -\\lambda\\log\\lambda.\\ ] ] by the law of large numbers , @xmath428 a.s .",
    ", so @xmath429 a.s . , as @xmath170 . therefore , to complete the proof it suffices to show that @xmath430 converges to @xmath431 also in @xmath251 , or , equivalently , that the sequence @xmath432 is uniformly integrable .",
    "we will actually show that the nonnegative random variables @xmath433 are bounded above by a different uniformly integrable sequence .",
    "indeed , by the log - sum inequality , t_n & = & _ i=1^n ( ) + & & _ i=1^nx_ix_i . [ eq : lln ] arguing as in the beginning of the proof of proposition  [ prop : llogl ] shows that the mean @xmath434 $ ] is finite , so the law of large numbers implies that the averages in ( [ eq : lln ] ) converge to @xmath144 a.s .  and in @xmath251 .",
    "hence , they form a uniformly integrable sequence ; this implies that the @xmath433 are also uniformly integrable , completing the proof .",
    "_ proof of theorem  [ thm : ltnstrong2 ] : _ the proof is similar to that of theorem  [ thm : ltnstrong ] , so some details are omitted . for each @xmath186 , let @xmath435 and write @xmath436 where the random variables @xmath437 are independent , with each @xmath184 .",
    "first , to see that @xmath438 is finite , applying the data - processing inequality @xcite as in @xcite gives , @xmath439 , and it is easy to check that each of these terms is finite because all @xmath440 have finite second moments . as before , proposition  [ prop : llogl ] gives , d(t_1/n(p^(n))po(^(n ) ) ) + e[(s_n",
    "/ n)(s_n / n ) ] -^(n)^(n ) .",
    "[ eq : fromp ] letting @xmath441 for each @xmath185 , the independent random variables @xmath442 have zero mean and , @xmath443 which is finite by assumption  ( b ) . then , by the general version of the law of large numbers on @xcite , @xmath444 , a.s . , and hence , by assumption  ( a ) , @xmath428 a.s .",
    ", so that also , @xmath429 a.s . , as @xmath170",
    ". moreover , since @xmath445 for every integer @xmath166 , we have , e\\ { ( ) ^4/3 } & & e\\ { ( ) ^2 } + & = & _ i=1^ne(x_i^2 ) + _",
    "1ijn e(x_i)e(x_j ) + & & _",
    "i=1^n e(x_i^2 ) + ( ^(n))^2 , which is uniformly bounded over @xmath14 by our assumptions .",
    "therefore , the sequence @xmath446 is bounded in @xmath447 with @xmath448 , which implies that it is uniformly integrable , therefore it converges to @xmath431 also in @xmath251 , so that , @xmath449 as @xmath170 .    finally , recalling once more that the poisson measures form an exponential family , they satisfy a pythagorean identity @xcite , so that d(t_1/n(p^(n))po ( ) ) & = & d(t_1/n(p^(n))po(^(n ) ) ) + d(po(^(n))po ( ) ) , where the first term was just shown to go to zero as @xmath170 , and the second term is actually equal to , @xmath450 which also vanishes as @xmath170 by assumption  ( a ) .      _",
    "proof of proposition  [ charlierexpo ] : _ let @xmath337 and @xmath451 denote independent random variables with distributions @xmath11 and @xmath452 , respectively . then from the definitions , and using lemmas  [ lem : fallfact ] and  [ lem : scalmom ] , e[p_k^(x _ , ) ]",
    "& = & _ = 0^k ( -)^k- e\\ { ( x_+z)^ } + & = & _ = 0^k ( -)^k- \\ { _ m=0^ e(x_^ ) e(z^ ) } + & = & _ = 0^k ( -)^k- \\ { _ m=0^ ^m e(x^ ) ( ( 1-))^ } , where we have used the fact that the factorial moments of a @xmath453 random variable @xmath287 satisfy , @xmath454=t^n$ ] . simplifying and interchanging the two sums , e[p_k^(x _ , ) ] & = & _ m=0^k ^m e(x^ ) _ = m^k ( -)^k- ( ( 1-))^ + & = & _ m=0^k ^m e(x^ ) ( -)^k - m + & = & ^ke[p_k^(x ) ] , as claimed .",
    "turning to the proof of proposition  [ prop : altsum ] , assume @xmath57 is poisson bounded with ratio @xmath23 .",
    "then the series in the statement converges , since@xmath461   } { \\ell!}\\right|   \\leq\\,\\frac{1}{x ! } \\sum_{\\ell=0}^{\\infty}\\frac{\\lambda^{x+\\ell}}{\\ell!}={{{\\rm{po}}}}(\\lambda , x ) < \\infty.\\ ] ] for @xmath328 even we have , @xmath462 therefore , @xmath463 multiplying by @xmath464 and summing over @xmath132 , @xmath465   } { \\ell!}\\;.\\end{aligned}\\ ] ] a similar argument holds for @xmath328 odd .",
    "_ proof of theorem  [ thm : kthin ] : _ let @xmath337 have distribution @xmath11 . using lemma  [ lem : scalmom ] , proposition  [ prop : altsum ] , and the fact that @xmath1 is ultra bounded , the score function of @xmath337 can be bounded as , _",
    "x_(z ) & = & -1 + & & -1 + & = & -1 + & = & ^-1 - 1 + & & ^-1 - 1 + & = & .",
    "since the lower bound @xmath466 is obvious , it follows that , _",
    "x_(z)^21 , [ eq : scoreu ]    we express @xmath467 in three terms : k(t_p)= _ z=0 ^ -2 t_p(z)_x_(z)^2 + t_p(-1)_x_(-1)^2 + _ z=^t_p(z)_x_(z)^2 .",
    "[ eq : three - terms ] for the third term note that , applying markov s inequality to the function @xmath468 which increases on the integers , we obtain , @xmath469 } { \\kappa!}=\\frac{\\alpha^{\\kappa}e[x^{\\underline{\\kappa}}]}{\\kappa ! } \\leq\\frac{(\\alpha\\lambda)^{\\kappa}}{\\kappa!}.\\ ] ] therefore , using this and ( [ eq : scoreu ] ) , for small enough @xmath104 the third term in ( [ eq : three - terms ] ) is bounded above by , @xmath470 which , divided by @xmath471 , tends to zero as @xmath472 .    for the other two terms we use the full expansion of proposition  [ prop : altsum ] , together with lemma  [ lem : scalmom ] , to obtain a more accurate expression for the score function , _",
    "x_(z ) & = & + & = & + & = & .",
    "since , by assumption , @xmath473    -\\lambda e\\left [   x^{\\underline{z+\\ell } } \\right ]   = 0 $ ] for @xmath474 , the first terms in the series in the numerator above vanish .",
    "therefore , @xmath475    -\\lambda e\\left [   x^{\\underline{\\ell+z-1}}\\right ]   \\right)/(\\ell+\\kappa - z-1 ) ! } { \\lambda\\sum_{\\ell=0}^{\\infty}\\left (   -1\\right )   ^{\\ell}\\alpha^{\\ell}e [   x^{\\underline{z+\\ell}}]/\\ell!}.\\ ] ] for @xmath476 , the numerator and denominator above are both bounded functions of @xmath4 , and the denominator is bounded away from zero ( because of the term corresponding to @xmath279 ) . therefore , for each @xmath477 , the score function @xmath478 is of order @xmath479 .",
    "for the first term in ( [ eq : three - terms ] ) we thus have , _",
    "z=0 ^ -2 t_p(z)_x_(z)^2 = _",
    "z=0 ^ -2 o(^z ) o(^2 - 2z-2 ) = o(^+1 ) , which , again , when divided by @xmath471 , tends to zero as @xmath480    thus only the second term in ( [ eq : three - terms ] ) contributes . for this term ,",
    "we similarly obtain , _",
    "0_x_(-1 ) & = & _ 0 + & = & + & = & , [ eq : exchange1 ] and , _ 0 & = & _ 0 + & = & _ 0 + & = & . [ eq : exchange2 ] finally , combining the above limits with ( [ eq : three - terms ] ) yields , @xmath481   -\\lambda^{\\kappa}}{\\lambda^{\\kappa}}\\right ) ^{2}\\\\ &   = \\kappa\\left (   \\frac{e\\left [   x^{\\underline{\\kappa}}\\right ]   -\\lambda ^{\\kappa}}{\\lambda^{\\kappa/2}\\left (   \\kappa!\\right )   ^{1/2}}\\right )   ^{2}\\\\ &   = \\kappa e\\left [   p_{\\kappa}\\left (   x\\right )   \\right ]   ^{2},\\end{aligned}\\ ] ] as claimed .      _",
    "proof of lemma  [ lem : poincarish ] : _ the key is to observe that for @xmath70 ulc , since @xmath482 is decreasing in @xmath75 , and @xmath75 is increasing in @xmath75 , there exists an integer @xmath483 such that @xmath484 for @xmath485 and @xmath486 for @xmath487 .",
    "hence : @xmath488 further , by cauchy - schwarz , for @xmath485 , @xmath489 while for @xmath490 , @xmath491 this means that ( with the reversal of order of summation justified by fubini , since all the terms have the same sign ) , @xmath492 and the result holds .",
    "note that the inequality in ( [ eq : step1 ] ) follows by ( [ eq : cs ] ) and ( [ eq : cs2 ] ) , and the inequality in ( [ eq : step2 ] ) by the discussion above .",
    "j.  fritz .",
    "an information - theoretical proof of limit theorems for reversible markov processes . in _",
    "sixth prague conf . on inform .",
    "theory , statist .",
    "decision functions , random processes _ , prague , 1973 .",
    "science , academia publ .",
    "prague , sept ."
  ],
  "abstract_text": [
    "<S> rnyi s _ thinning _ operation on a discrete random variable is a natural discrete analog of the scaling operation for continuous random variables . </S>",
    "<S> the properties of thinning are investigated in an information - theoretic context , especially in connection with information - theoretic inequalities related to poisson approximation results . the classical binomial - to - poisson convergence ( sometimes referred to as the law of small numbers ) </S>",
    "<S> is seen to be a special case of a thinning limit theorem for convolutions of discrete distributions . </S>",
    "<S> a rate of convergence is provided for this limit , and nonasymptotic bounds are also established . </S>",
    "<S> this development parallels , in part , the development of gaussian inequalities leading to the information - theoretic version of the central limit theorem . in particular , a `` thinning markov chain '' is introduced , and it is shown to play a role analogous to that of the ornstein - uhlenbeck process in connection to the entropy power inequality .    </S>",
    "<S> thinning , entropy , information divergence , poisson distribution , law of small numbers , law of thin numbers , binomial distribution , compound poisson distribution , poisson - charlier polynomials </S>"
  ]
}