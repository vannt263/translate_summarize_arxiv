{
  "article_text": [
    "content on the internet is becoming increasingly multilingual .",
    "a prime example is wikipedia . in 2001 , the majority of pages were written in english , while in 2015 , the percentage of english articles has dropped to 14% . at the same time , online news has begun to dominate reporting of current events .",
    "however , machine translation remains relatively rudimentary .",
    "it allows people to understand simple phrases on web pages , but remains inadequate for more advanced understanding of text . in this paper",
    "we consider the intersection of these developments : how to track events which are reported about in multiple languages .",
    "the term event is vague and ambiguous , but for the practical purposes , we define it as `` any significant happening that is being reported about in the media .",
    "'' examples of events would include shooting down of the malaysia airlines plane over ukraine on july 18th , 2014 and hsbc s admittance of aiding their clients in tax evasion on february 9th , 2015 ( figure  [ fig : event2 ] ) .",
    "events such as these are covered by many articles and the question is how to find all the articles in different languages that are describing a single event .",
    "generally , events are more specific than general themes as the time component plays an important role  for example , the two wars in iraq would be considered as separate events .",
    "events are represented by collections of articles about an event , in this case the malaysian airliner which was shot down over ukraine .",
    "the results shown in the figure can be obtained using the query http://eventregistry.org/event/997350#?lang=eng&tab=articles .",
    "the content presented is part of the event registry system , developed by the authors.,scaledwidth=100.0% ]    as input , we consider a stream of articles in different languages and a list of events . our goal is to assign articles to their corresponding events .",
    "a priori , we do not know the coverage of the articles , that is , not all the events may be covered and we do not know that all the articles necessarily fit into one of the events .",
    "the task is divided into two parts : detecting events within each language and then linking events across languages . in this paper",
    "we address the second step .",
    "we consider a high volume of articles in different languages . by using a language detector ,",
    "the stream is split into separate monolingual streams . within each monolingual stream ,",
    "an online clustering approach is employed , where tracked clusters correspond to our definition of events - this is based on the event registry system  @xcite .",
    "our main goal in this paper is to connect such clusters ( representations of events ) across languages , that is , to detect that a set of articles in language @xmath0 reports on the same event as a set of articles in language @xmath1 .",
    "our approach to link clusters across languages combines two ingredients : a cross - lingual document similarity measure , which can be interpreted as a language independent topic model , and semantic annotation of documents , which enables an alternative way to comparing documents . since this work represents a complicated pipeline ,",
    "we concentrate on these two specific elements .",
    "overall , the approach should be considered from a systems perspective ( considering the system as a whole ) rather than considering these problems in isolation .",
    "the first ingredient of our approach to link clusters across languages represents a continuation of previous work  @xcite where we explored representations of documents which were valid over multiple languages .",
    "the representations could be interpreted as multilingual topics , which were then used as proxies to compute cross - lingual similarities between documents . to learn the representations , we use wikipedia as a training corpus .",
    "significantly , we do not only consider the major or _ hub _ languages such as english , german , french , etc . which have significant overlap in article coverage , but also smaller languages ( in terms of number of wikipedia articles ) such as slovenian and hindi , which may have a negligible overlap in article coverage .",
    "we can then define a similarity between any two articles regardless of language , which allows us to cluster the articles according to topic .",
    "the underlying assumption is that articles describing the same event are similar and will therefore be put into the same cluster .",
    "based on the similarity function , we propose a novel algorithm for linking events / clusters across languages .",
    "the approach is based on learning a classification model from labelled data based on several sets of features .",
    "in addition to these features , cross - lingual similarity is also used to quickly identify a small list of potential linking candidates for each cluster .",
    "this greatly increases the scalability of the system .",
    "the paper is organized as follows : we first provide an overview of the system as a whole in section  [ sec : pipeline ] , which includes a subsection that summarizes the main system requirements .",
    "we then present related work in section  [ sec : related ] .",
    "the related work covers work on cross - lingual document similarity as well as work on cross - lingual cluster linking . in section  [ sec : crosslingual ] , we introduce the problem of cross - lingual document similarity computation and describe several approaches to the problem , most notably a new approach based on hub languages . in section  [ sec : linking ] , we introduce the central problem of cross - lingual linking of clusters of news articles and our approach that combines the cross - lingual similarity functions with knowledge extraction based techniques .",
    "finally , we present and interpret the experimental results in section  [ sec : evaluation ] and discuss conclusions and point out several promising future directions .",
    "we base our techniques of cross - lingual event linking on an online system for detection of world events , called event registry  @xcite .",
    "event registry is a repository of events , where events are automatically identified by analyzing news articles that are collected from numerous news outlets all over the world .",
    "the important components in the pipeline of the event registry are shown in figure  [ fig : erpipeline ] .",
    "we will now briefly describe the main components .     the event registry pipeline .",
    "after new articles are collected , they are first analyzed individually ( article - level processing ) . in the next step ,",
    "groups of articles about the same event are identified and relevant information about the event is extracted ( event construction phase ) .",
    "although the pipeline contains several components , we focus only on the two highlighted in the image . ]",
    "the collection of the news articles is performed using the newsfeed service  @xcite .",
    "the service monitors rss feeds of around 100,000 mainstream news outlets available globally . whenever a new article is detected in the rss feed",
    ", the service downloads all available information about the article and sends the article through the pipeline .",
    "newsfeed downloads daily on average around 200,000 news articles in various languages , where english , spanish and german are the most common .",
    "collected articles are first semantically annotated by identifying mentions of relevant concepts  either entities or important keywords .",
    "the disambiguation and entity linking of the concepts is done using wikipedia as the main knowledge base .",
    "the algorithm for semantic annotation uses machine learning to detect significant terms within unstructured text and link them to the appropriate wikipedia articles .",
    "the approach models link probability and combines prior word sense distributions with context based sense distributions .",
    "the details are reported in  @xcite and  @xcite . as a part of the semantic annotation we also analyze the dateline of the article to identify the location of the described event as well as to extract dates mentioned in the article using a set of regular expressions .",
    "since articles are frequently revised we also detect if the collected article is just a revision of a previous one so that we can use this information in next phases of the pipeline .",
    "the last important processing step on the document level is to efficiently identify which articles in other available languages are most similar to this article .",
    "the methodology for this task is one of the main contributions of this paper and is explained in details in section  [ sec : crosslingual ] .    as the next step ,",
    "an online clustering algorithm is applied to the articles in order to identify groups of articles that are discussing the same event . for each new article",
    ", the clustering algorithm determines if the article should be assigned to some existing cluster or into a new cluster .",
    "the underlying assumption is that articles that are describing the same event are similar enough and will therefore be put into the same cluster . for clustering , each new article is first tokenized , stop words are removed and the remaining words are stemmed .",
    "the remaining tokens are represented in a vector - space model and normalized using tf - idf ( see section  [ sec : tfidf ] for the definition ) .",
    "cosine similarity is used to find the most similar existing cluster , by comparing the document s vector to the centroid vector of each cluster .",
    "a user - defined threshold is used to determine if the article is not similar enough to any existing clusters ( 0.4 was used in our experiments ) .",
    "if the highest similarity is above the threshold , the article is assigned to the corresponding cluster , otherwise a new cluster is created , initially containing only the single article . whenever an article is assigned to a cluster ,",
    "the cluster s centroid vector is also updated . since articles about an event",
    "are commonly written only for a short period of time , we remove clusters once the oldest article in the cluster becomes more than 4 days old .",
    "this housekeeping mechanism prevents the clustering from becoming slow and also ensures that articles are not assigned to obsolete clusters .",
    "details of the clustering approach are described in  @xcite .",
    "once the number of articles in a cluster reaches a threshold ( which is a language dependent parameter ) , we assume that the articles in the cluster are describing an event . at that point , a new event with a unique i d is created in event registry , and the cluster with the articles is assigned to it . by analyzing the articles , we extract the main information about the event , such as the event location , date , most relevant entities and keywords , etc .",
    "since articles in a cluster are in a single language , we also want to identify any other existing clusters that report about the same event in other languages and join these clusters into the same event .",
    "this task is performed using a classification approach which is the second major contribution of this paper .",
    "it is described in detail in section  [ sec : linking ] .    when a cluster is identified and information about the event is extracted , all available data is stored in a custom - built database system .",
    "the data is then accessible through the api or a web interface at http://eventregistry.org/ , which provide numerous search and visualization options .",
    "our goal is to build a system that monitors global media and analyzes how events are being reported on .",
    "our approach consists of two steps : tracking events separately in each language ( based on language detection and an online clustering approach ) and then connecting them .",
    "the pipeline must be able to process millions of articles per day and perform billions of similarity computations each day .",
    "both steps rely heavily on similarity computation , which implies that this must be highly scalable .    therefore , we focus on implementations that run on a single shared memory machine , as opposed to clusters of machines .",
    "this simplifies implementation and system maintenance .",
    "to summarize , the following properties are desirable :    * * training * - the training ( building cross - lingual models ) should scale to many languages and should be robust to the quality of training resources .",
    "the system should be able to take advantage of comparable corpora ( as opposed to parallel translation - based corpora ) , with missing data .",
    "* * operation efficiency * - the similarity computation should be fast - the system must be able to handle billions of similarity computations per day",
    ". computing the similarity between a new document and a set of known documents should be efficient ( the main application is linking documents between two monolingual streams ) . * * operation cost * - the system should run on a strong shared machine server and not rely on paid services . * * implementation * - the system is straightforward to implement , with few parameters to tune .",
    "we believe that a cross - lingual similarity component that meets such requirements is very desirable in a commercial setting , where several different costs have to be taken into consideration .",
    "in this section , we describe previous work described in the literature .",
    "since there are two distinctive tasks that we tackle in this paper ( computing cross - lingual document similarity and cross - lingual cluster linking ) , we have separated the related work into two corresponding parts .",
    "there are four main families of approaches to cross - lingual similarity .",
    "* translation and dictionary based*. the most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity , see  @xcite for several variations of translation based approaches .",
    "one can use free tools such as moses  @xcite or translation services , such as google translate ( https://translate.google.com/ ) .",
    "there are two issues with such approaches : they solve a harder problem than needs to be solved and they are less robust to training resource quality - large sets of translated sentences are typically needed .",
    "training moses for languages with scarce linguistic resources is thus problematic .",
    "the issue with using online services such as google translate is that the apis are limited and not free .",
    "the operation efficiency and cost requirements make translation - based approaches less suited for our system .",
    "closely related are works cross - lingual vector space model ( cl - vsm )  @xcite and the approach presented in  @xcite which both compare documents by using dictionaries , which in both cases are eurovoc dictionaries  @xcite .",
    "the generality of such approaches is limited by the quality of available linguistic resources , which may be scarce or non - existent for certain language pairs .",
    "* probabilistic topic models*. there exist many variants to modelling documents in a language independent way by using probabilistic graphical models .",
    "the models include : joint probabilistic latent semantic analysis ( jplsa )  @xcite , coupled probabilistic lsa ( cplsa )  @xcite , probabilistic cross - lingual lsa ( pcllsa )  @xcite and polylingual topic models ( pltm )  @xcite which is a bayesian version of pcllsa . the methods ( except for cplsa ) describe the multilingual document collections as samples from generative probabilistic models , with variations on the assumptions on the model structure .",
    "the topics represent latent variables that are used to generate observed variables ( words ) , a process specific to each language .",
    "the parameter estimation is posed as an inference problem which is typically intractable and one usually solves it using approximate techniques .",
    "most variants of solutions are based on gibbs sampling or variational inference , which are nontrivial to implement and may require an experienced practitioner to be applied .",
    "furthermore , representing a new document as a mixture of topics is another potentially hard inference problem which must be solved .    *",
    "matrix factorization*. several matrix factorization based approaches exist in the literature .",
    "the models include : non - negative matrix factorization based  @xcite , cross - lingual latent semantic indexing cl - lsi  @xcite , canonical correlation analysis ( cca )  @xcite , oriented principal component analysis ( opca )  @xcite . the quadratic time and space dependency of the opca method makes it impractical for large scale purposes .",
    "in addition , opca forces the vocabulary sizes for all languages to be the same , which is less intuitive . for our setting , the method in  @xcite",
    "has a prohibitively high computational cost when building models ( it uses dense matrices whose dimensions are a product of the training set size and the vocabulary size ) .",
    "our proposed approach combines cca and cl - lsi .",
    "another closely related method is cross - lingual explicit semantic analysis ( cl - esa )  @xcite , which uses wikipedia ( as do we in the current work ) to compare documents .",
    "it can be interpreted as using the sample covariance matrix between features of two languages to define the dot product which is used to compute similarities .",
    "the authors of cl - esa compare it to cl - lsi and find that cl - lsi can outperform cl - esa in an information retrieval , but is costlier to optimize over a large corpus ( cl - esa requires no training ) .",
    "we find that the scalability argument does not apply in our case : based on advances in numerical linear algebra we can solve large cl - lsi problems ( millions of documents as opposed to the 10,000 document limit reported in  @xcite ) . in addition , cl - esa is less suited for computing similarities between two large monolingual streams .",
    "for example , each day we have to compute similarities between 500,000 english and 500,000 german news articles . comparing each german news article with 500,000 english news articles",
    "is either prohibitively slow ( involves projecting all english articles on wikipedia ) or consumes too much memory ( involves storing the projected english articles , which for a wikipedia of size 1,000,000 is a 500,000 by 1000,0000 non - sparse matrix ) .",
    "* monolingual*. finally , related work includes monolingual approaches that treat document written in different languages in a monolingual fashion .",
    "the intuition is that named entities ( for example , `` obama '' ) and cognate words ( for example , `` tsunami '' ) are written in the same or similar fashion in many languages .",
    "for example , the cross - language character n - gram model ( cl - cng )  @xcite represents documents as bags of character @xmath2-grams .",
    "another approach is to use language dependent keyword lists based on cognate words  @xcite .",
    "these approaches may be suitable for comparing documents written in languages that share a writing system , which does not apply to the case of global news tracking .    based on our requirements in section  [ sec :",
    "sysreq ] , we chose to focus on methods based on vector space models and linear embeddings .",
    "we propose a method that is more efficient than popular alternatives ( a clustering - based approach and latent semantic indexing ) , but is still simple to optimize and use .",
    "although there are a number of services that aggregate news by identifying clusters of similar articles , there are almost no services that provide linking of clusters over different languages .",
    "google news as well as yahoo ! news are able to identify clusters of articles about the same event , but offer no linking of clusters across languages .",
    "the only service that we found , which provides cross - lingual cluster linking , is the european media monitor ( emm )  @xcite .",
    "emm clusters articles in 60 languages and then tries to determine which clusters of articles in different languages describe the same event .",
    "to achieve cluster linking , emm uses three different language independent vector representations for each cluster .",
    "the first vector contains the weighted list of references to countries mentioned in the articles , while the second vector contains the weighted list of mentioned people and organizations .",
    "the last vector contains the weighted list of eurovoc subject domain descriptors .",
    "these descriptors are topics , such as _ air transport _ , _ ec agreement _ , _ competition _ and _ pollution control _ into which articles are automatically categorized  @xcite .",
    "similarity between clusters is then computed using a linear combination of the cosine similarities computed on the three vectors .",
    "if the similarity is above the threshold , the clusters are linked .",
    "compared to emm , our approach uses document similarities to obtain a small set of potentially equivalent clusters .",
    "additionally , we do not decide if two clusters are equivalent based on a hand - set threshold on a similarity value ",
    "instead we use a classification model that uses a larger set of features related to the tested pair of clusters .",
    "a system , which is significantly different but worth mentioning , is the gdelt project  @xcite . in gdelt",
    ", events are also extracted from articles , but in their case , an event is specified in a form of a triple containing two actors and a relation .",
    "the project contains an extensive vocabulary of possible relations , mostly related to political events . in order to identify events",
    ", gdelt collects articles in more than 65 languages and uses machine translation to translate them to english .",
    "all information extraction is then done on the translated article .",
    "document similarity is an important component in techniques from text mining and natural language processing .",
    "many techniques use the similarity as a black box , e.g. , a kernel in support vector machines .",
    "comparison of documents ( or other types of text snippets ) in a monolingual setting is a well - studied problem in the field of information retrieval  @xcite .",
    "we first formally introduce the problem followed by a description of our approach .",
    "we will first describe how documents are represented as vectors and how to compare documents in a mono - lingual setting .",
    "we then define a way to measure cross - lingual similarity which is natural for the models we consider .",
    "* document representation . *",
    "the standard vector space model  @xcite represents documents as vectors , where each term corresponds to a word or a phrase in a fixed vocabulary .",
    "formally , document @xmath3 is represented by a vector @xmath4 , where @xmath2 corresponds to the size of the vocabulary , and vector elements",
    "@xmath5 correspond to the number of times term @xmath6 occurred in the document , also called _ term frequency _ or @xmath7 .",
    "we also used a term re - weighting scheme that adjusts for the fact that some words occur more frequently in general .",
    "a term weight should correspond to the importance of the term for the given corpus .",
    "the common weighting scheme is called _ term frequency inverse document frequency _ ( @xmath8 ) weighting .",
    "an _ inverse document frequency _ ( @xmath9 ) weight for the dictionary term @xmath6 is defined as @xmath10 , where @xmath11 is the number of documents in the corpus which contain term @xmath6 . when building cross - lingual models , the idf scores were computed with respect to the wikipedia corpus . in the other part of our system",
    ", we computed tfidf vectors on streams of news articles in multiple languages . there",
    "the idf scores for each language changed dynamically - for each new document we computed the idf of all news articles within a 10 day window .",
    "therefore we can define a document s @xmath8 as @xmath12 the @xmath8 weighted vector space model document representation corresponds to a map @xmath13 defined by : @xmath14    * mono - lingual similarity . *",
    "a common way of computing similarity between documents is _ cosine similarity _ , @xmath15 where @xmath16 and @xmath17 are standard inner product and euclidean norm .",
    "when dealing with two or more languages , one could ignore the language information and build a vector space using the union of tokens over the languages .",
    "a cosine similarity function in such a space can be useful to some extent , for example `` internet '' or `` obama '' may appear both in spanish and english texts and the presence of such terms in both an english and a spanish document would contribute to their similarity .",
    "in general however , large parts of vocabularies may not intersect .",
    "this means that given a language pair , many words in both languages can not contribute to the similarity score .",
    "such cases can make the similarity function very insensitive to the data .",
    "* cross - lingual similarity .",
    "* processing a multilingual dataset results in several vector spaces with varying dimensionality , one for each language .",
    "the dimensionality of the vector space corresponding to the @xmath18-th language is denoted by @xmath19 and the vector space model mapping is denoted by @xmath20 .",
    "the similarity between documents in language @xmath18 and language @xmath21 is defined as a bilinear operator represented as a matrix @xmath22 : @xmath23 where @xmath24 and @xmath25 are documents written in the @xmath18-th and @xmath21-th language respectively .",
    "if the maximal singular value of @xmath26 is bounded by @xmath27 , then the similarity scores will lie on the interval @xmath28 $ ] .",
    "we will provide an overview of the models in section [ sec : models ] and then introduce additional notation in [ sec : notation ] . starting with section [ sec : kmeans ] and ending with section [ sec : hublang ]",
    "we will describe some approaches to compute @xmath26 given training data .      in this section",
    ", we will describe several approaches to the problem of computing the multilingual similarities introduced in section  [ sec : tfidf ] .",
    "we present four approaches : a simple approach based on @xmath6-means clustering in section  [ sec : kmeans ] , a standard approach based on singular value decomposition in section  [ sec : lsi ] , a related approach called canonical correlation analysis ( cca ) in section  [ sec : cca ] and finally a new method , which is an extension of cca to more than two languages in section  [ sec : hublang ] .",
    "cca can be used to find correlated patterns for a pair of languages , whereas the extended method optimizes a sum of squared correlations ( sscor ) between several language pairs , which was introduced in  @xcite .",
    "the sscor problem is difficult to solve in our setting ( hundreds of thousands of features , hundreds of thousands of examples ) . to tackle this ,",
    "we propose a method which consists of two ingredients .",
    "the first one is based on an observation that certain datasets ( such as wikipedia ) are biased towards one language ( english for wikipedia ) , which can be exploited to reformulate a difficult optimization problem as an eigenvector problem .",
    "the second ingredient is dimensionality reduction using cl - lsi , which makes the eigenvector problem computationally and numerically tractable .",
    "we concentrate on approaches that are based on linear maps rather than alternatives , such as machine translation and probabilistic models , as discussed in the section on related work",
    ". we will start by introducing some notation .",
    "the cross - lingual similarity models presented in this paper are based on comparable corpora .",
    "a _ comparable corpus _ is a collection of documents in multiple languages , with alignment between documents that are of the same topic , or even a rough translation of each other .",
    "wikipedia is an example of a comparable corpus , where a specific entry can be described in multiple languages ( e.g. ,  berlin \" is currently described in 222 languages ) .",
    "news articles represent another example , where the same event can be described by newspapers in several languages .    more formally , a _",
    "multilingual document _",
    "@xmath29 is a tuple of @xmath30 documents on the same topic ( comparable ) , where @xmath31 is the document written in language @xmath18 .",
    "note that an individual document @xmath31 can be an empty document ( missing resource ) and each @xmath3 must contain at least * two nonempty documents*. this means that in our analysis we discard strictly monolingual documents for which no cross - lingual information is available .",
    "a comparable corpus @xmath32 is a collection of @xmath33 multilingual documents . by using the vector space model",
    ", we can represent @xmath34 as a set of @xmath30 matrices @xmath35 , where @xmath36 is the matrix corresponding to the language @xmath18 and @xmath19 is the vocabulary size of language @xmath18 .",
    "furthermore , let @xmath37 denote the @xmath38-th column of matrix @xmath39 and the matrices respect the document alignment - the vector @xmath40 corresponds to the tfidf vector of the @xmath18-th component of multilingual document @xmath41 .",
    "we use @xmath42 to denote the total row dimension of @xmath43 , i.e. , @xmath44 .",
    "see figure  [ fig : stacked_matrices ] for an illustration of the introduced notation .",
    "multilingual corpora and their matrix representations using the vector space model.,width=340 ]    we will now describe four models to cross - lingual similarity computation in the next sub - sections .",
    "the @xmath6-means algorithm is perhaps the most well - known and widely - used clustering algorithm . here , we present its application to compute cross - lingual similarities .",
    "the idea is based on concatenating the corpus matrices , running standard @xmath6-means clustering to obtain the matrix of centroids ,  reversing \" the concatenation step to obtain a set of aligned bases , which are finally used to compute cross - lingual similarities .",
    "see figure  [ fig : kmeans ] for overview of the procedure .",
    "the left side of figure  [ fig : kmeans ] illustrates the decomposition and the right side summarizes the coordinate change .",
    "@xmath6-means algorithm and coordinate change.,width=377 ]    in order to apply the algorithm , we first merge all the term - document matrices into a single matrix @xmath43 by stacking the individual term - document matrices ( as seen in figure  [ fig : stacked_matrices ] ) : @xmath45 such that the columns respect the alignment of the documents ( here matlab notation for concatenating matrices is used ) .",
    "therefore , each document is represented by a long vector indexed by the terms in all languages .",
    "we then run the @xmath6-means algorithm  @xcite and obtain a centroid matrix @xmath46 , where the @xmath6 columns represent centroid vectors .",
    "the centroid matrix can be split vertically into @xmath30 blocks : @xmath47^t,\\ ] ] according to the number of dimensions of each language , i.e. , @xmath48 .",
    "to reiterate , the matrices @xmath49 are computed using a multilingual corpus matrix @xmath43 ( based on wikipedia for example ) .    to compute cross - lingual document similarities on new documents , note that each matrix @xmath49 represents a vector space basis and can be used to map points in @xmath50 into a @xmath6-dimensional space , where the new coordinates of a vector @xmath51 are expressed as : @xmath52    the resulting matrix for similarity computation between language @xmath18 and language @xmath21 is defined up to a scaling factor as : @xmath53    the matrix is a result of mapping documents in a language independent space using pseudo - inverses of the centroid matrices @xmath54 and then comparing them using the standard inner product , which results in the matrix @xmath55 . for the sake of presentation , we assumed that the centroid vectors are linearly independent .",
    "( an independent subspace could be obtained using an additional gram - schmidt step  @xcite on the matrix @xmath56 , if this was not the case . )      the second approach we consider is cross - lingual latent semantic indexing ( cl - lsi )  @xcite which is a variant of lsi  @xcite for more than one language .",
    "the approach is very similar to @xmath6-means , where we first concatenate the corpus matrices , compute a decomposition , which in case of cl - lsi is a truncated singular value decomposition ( svd ) , decouple the column space matrix and use the blocks to compute linear maps to a common vector space , where standard cosine similarity is used to compare documents .",
    "the method is based on computing a truncated singular value decomposition of the concatenated corpus matrix @xmath57 .",
    "see figure  [ fig : lsi ] for the decomposition . representing documents in  topic",
    " coordinates is done in the same way as in the @xmath6-means case ( see figure  [ fig : kmeans ] ) , we will describe how to compute the coordinate change functions .",
    "lsi multilingual corpus matrix decomposition.,width=377 ]    the cross - lingual similarity functions are based on a rank-@xmath6 truncated svd : @xmath58 where @xmath59 are basis vectors of interest and @xmath60 is a truncated diagonal matrix of singular eigenvalues .",
    "an aligned basis is obtained by first splitting @xmath61 vertically according to the number of dimensions of each language : @xmath62^t$ ] .",
    "then , the same as with @xmath6-means clustering , we compute the pseudoinverses @xmath63 .",
    "the matrices @xmath64 are used to change the basis from the standard basis in @xmath50 to the basis spanned by the columns of @xmath65 .",
    "[ [ implementation - note ] ] implementation note + + + + + + + + + + + + + + + + + + +    since the matrix @xmath43 can be large we could use an iterative method like the lanczos algorithm with reorthogonalization  @xcite to find the left singular vectors ( columns of @xmath61 ) corresponding to the largest singular values .",
    "it turns out that the lanczos method converges slowly as the gap between the leading singular values is small .",
    "moreover , the lanczos method is hard to parallelize .",
    "instead , we use a randomized version of the svd  @xcite that can be viewed as a block lanczos method .",
    "that enables us to use parallelization and speeds up the computation considerably .    to compute the matrices @xmath64 we used the qr algorithm  @xcite to factorize @xmath65 as @xmath66 , where @xmath67 and @xmath68 is a triangular matrix",
    "@xmath64 is then obtained by solving @xmath69 .",
    "we now present a statistical technique to analyze data from two sources , an extension of which will be presented in the next section .",
    "canonical correlation analysis ( cca )  @xcite is a dimensionality reduction technique similar to principal component analysis ( pca )  @xcite , with the additional assumption that the data consists of feature vectors that arose from two sources ( two views ) that share some information .",
    "examples include : bilingual document collection  @xcite and collection of images and captions  @xcite . instead of looking for linear combinations of features that maximize the variance ( pca )",
    "we look for a linear combination of feature vectors from the first view and a linear combination for the second view , that are maximally correlated .    interpreting the columns of @xmath39 as observation vectors sampled from an underlying distribution @xmath70 ,",
    "the idea is to find two weight vectors @xmath71 and @xmath72 so that the random variables @xmath73 and @xmath74 are maximally correlated ( @xmath75 and @xmath76 are used to map the random vectors to random variables , by computing weighted sums of vector components ) .",
    "let @xmath77 denote the sample - based correlation coefficient between two vectors of observations @xmath78 and @xmath79 . by using the sample matrix notation @xmath39 and",
    "@xmath80 ( assuming no data is missing to simplify the exposition ) , this problem can be formulated as the following optimization problem : @xmath81 where @xmath82 and @xmath83 are empirical estimates of variances of @xmath84 and @xmath85 respectively and @xmath86 is an estimate for the covariance matrix . assuming that the observation vectors are centered ( only for the purposes of presentation ) , the matrices are computed in the following way : @xmath87 , and similarly for @xmath82 and @xmath83 . the optimization problem can be reduced to an eigenvalue problem and includes inverting the variance matrices @xmath82 and @xmath83 .",
    "if the matrices are not invertible , one can use a regularization technique by replacing @xmath82 with @xmath88 , where @xmath89 $ ] is the regularization coefficient and @xmath90 is the identity matrix .",
    "( the same can be applied to @xmath83 . )",
    "a single canonical variable is usually inadequate in representing the original random vector and typically one looks for @xmath6 projection pairs @xmath91 , so that @xmath92 and @xmath93 are highly correlated and @xmath92 is uncorrelated with @xmath94 for @xmath95 and analogously for @xmath96 vectors .",
    "note that the method in its original form is only applicable to two languages where an aligned set of observations is available .",
    "the next section will describe a scalable extension of cca to more than two languages .      building cross - lingual similarity models based on comparable corpora is challenging for two main reasons .",
    "the first problem is related to missing alignment data : when a number of languages is large , the dataset of documents that cover all languages is small ( or may even be empty ) .",
    "even if only two languages are considered , the set of aligned documents can be small ( an extreme example is given by the piedmontese and hindi wikipedias where no inter - language links are available ) , in which case none of the methods presented so far are applicable .",
    "the second challenge is scale - the data is high - dimensional ( many languages with hundreds of thousands of features per language ) and the number of multilingual documents may be large ( over one million in case of wikipedia ) .",
    "the optimization problem posed by cca is not trivial to solve : the covariance matrices themselves are prohibitively large to fit in memory ( even storing a 100,000 by 100,000 element matrix requires 80 gb of memory ) and iterative matrix - multiplication based approaches to solving generalized eigenvalue problems are required ( the covariance matrices can be expressed as products of sparse matrices , which means we have fast matrix - vector multiplication ) .",
    "we now describe an extension of cca to more than two languages , which can be trained on large comparable corpora and can handle missing data .",
    "the extension we consider is based on a generalization of cca to more than two views , introduced in  @xcite , namely the sum of squared correlations sscor , which we will state formally later in this section .",
    "our approach exploits a certain characteristic of the data , namely the _ hub language _ characteristic ( see below ) in two ways : to reduce the dimensionality of the data and to simplify the optimization problem .",
    "[ [ hub - language - characteristic . ] ] hub language characteristic .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the case of wikipedia , we observed that even though the training resources are scarce for certain language pairs , there often exists indirect training data . by considering a third language , which has training data with both languages in the pair",
    ", we can use the composition of learned maps as a proxy .",
    "we refer to this third language as a hub language .",
    "a _ hub language _ is a language with a high proportion of non - empty documents in @xmath97 .",
    "as we have mentioned , we only focus on multilingual documents that include at least two languages .",
    "the prototypical example in the case of wikipedia is english .",
    "our notion of the hub language could be interpreted in the following way .",
    "if a non - english wikipedia page contains one or more links to variants of the page in other languages , english is very likely to be one of them .",
    "that makes english a hub language .",
    "we use the following notation to define subsets of the multilingual comparable corpus : let @xmath98 denote the index set of all multilingual documents with non - missing data for the @xmath18-th and @xmath21-th language : @xmath99 and let @xmath100 denote the index set of all multilingual documents with non missing data for the @xmath18-th language .",
    "we now describe a two step approach to building a cross - lingual similarity matrix .",
    "the first part is related to lsi and reduces the dimensionality of the data .",
    "the second step refines the linear mappings and optimizes the linear dependence between data .",
    "[ [ step-1-hub - language - based - dimensionality - reduction . ] ] step 1 : hub language based dimensionality reduction .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the first step in our method is to project @xmath101 to lower - dimensional spaces without destroying the cross - lingual structure . treating the nonzero columns of @xmath39 as observation vectors sampled from an underlying distribution @xmath102 , we can analyze the empirical cross - covariance matrices : @xmath103 where @xmath104 . by finding low - rank approximations of @xmath86 we can identify the subspaces of @xmath105 and @xmath106 that are relevant for extracting linear patterns between @xmath84 and @xmath85 .",
    "let @xmath107 represent the hub language corpus matrix .",
    "the lsi approach to finding the subspaces is to perform the singular value decomposition on the full @xmath108 covariance matrix composed of blocks @xmath86 . if @xmath109 is small for many language pairs ( as it is in the case of wikipedia ) , then many empirical estimates @xmath86 are unreliable , which can result in overfitting . for this reason , we perform the truncated singular value decomposition on the matrix @xmath110 \\approx u s v^t$ ] , where @xmath111 .",
    "we split the matrix @xmath112 vertically in blocks with @xmath113 rows : @xmath114^t$ ] .",
    "note that columns of @xmath61 are orthogonal but columns in each @xmath105 are not ( columns of v are orthogonal ) .",
    "let @xmath115 .",
    "we proceed by reducing the dimensionality of each @xmath39 by setting : @xmath116 , where @xmath117 .",
    "to summarize , the first step reduces the dimensionality of the data and is based on cl - lsi , but optimizes only the hub language related cross - covariance blocks .",
    "[ [ step-2-simplifying - and - solving - sscor . ] ] step 2 : simplifying and solving sscor .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the second step involves solving a generalized version of canonical correlation analysis on the matrices @xmath118 in order to find the mappings @xmath64 .",
    "the approach is based on the sum of squares of correlations formulation by kettenring  @xcite , where we consider only correlations between pairs @xmath119 due to the hub language problem characteristic .",
    "we will present the original unconstrained optimization problem , then a constrained formulation based on the hub language problem characteristic .",
    "then we will simplify the constraints and reformulate the problem as an eigenvalue problem by using lagrange multipliers .",
    "the original sum of squared correlations is formulated as an unconstrained problem : @xmath120 we solve a similar problem by restricting @xmath121 and omitting the optimization over non - hub language pairs .",
    "let @xmath122 denote the empirical covariance of @xmath123 and @xmath124 denote the empirical cross - covariance computed based on @xmath123 and @xmath125 .",
    "we solve the following constrained ( unit variance constraints ) optimization problem : @xmath126 the constraints @xmath127 can be simplified by using the cholesky decomposition @xmath128 and substitution : @xmath129 . by inverting the @xmath130 matrices and defining @xmath131 ,",
    "the problem can be reformulated : @xmath132 a necessary condition for optimality is that the derivatives of the lagrangian vanish .",
    "the lagrangian of ( [ squaredcorhub ] ) is expressed as : @xmath133 stationarity conditions give us : @xmath134 @xmath135 multiplying the equations ( [ dldxi ] ) with @xmath136 and applying the constraints , we can eliminate @xmath137 which gives us : @xmath138 plugging this into ( [ dldx1 ] ) , we obtain an eigenvalue problem : @xmath139 the eigenvectors of @xmath140 solve the problem for the first language . the solutions for @xmath141 are obtained from ( [ eqy1yi ] ) : @xmath142 .",
    "note that the solution ( [ squaredcorhuboriginal ] ) can be recovered by : @xmath143 .",
    "the linear transformation of the @xmath144 variables are thus expressed as : @xmath145 @xmath146 @xmath147 where @xmath42 is a diagonal matrix that normalizes @xmath148 , with @xmath149 .",
    "[ [ remark . ] ] remark . + + + + + + +    the technique is related to generalization of canonical correlation analysis ( gcca ) by carroll  @xcite , where an unknown group configuration variable is defined and the objective is to maximize the sum of squared correlations between the group variable and the others .",
    "the problem can be reformulated as an eigenvalue problem .",
    "the difference lies in the fact that we set the unknown group configuration variable as the hub language , which simplifies the solution .",
    "the complexity of our method is @xmath150 , where @xmath6 is the reduced dimension from the lsi preprocessing step , whereas solving the gcca method scales as @xmath151 , where @xmath33 is the number of samples ( see  @xcite ) .",
    "another issue with gcca is that it can not be directly applied to the case of missing documents .    to summarize , we first reduced the dimensionality of our data to @xmath6-dimensional features and then found a new representation ( via linear transformation ) that maximizes directions of linear dependence between the languages",
    ". the final projections that enable mappings to a common space are defined as : @xmath152",
    "the main application on which we test the cross - lingual similarity is cross - lingual event linking . in online media streams",
    " particularly news articles  there is often duplication of reporting , different viewpoints or opinions , all centering around a single event .",
    "the same events are covered by many articles and the question we address is how to find all the articles in different languages that are describing a single event . in this paper",
    "we consider the problem of matching events from different languages .",
    "we do not address the problem of detection of events and instead base our evaluation on an online system for detection of world events , event registry .",
    "the events are represented by clusters of articles and so ultimately our problem reduces to finding suitable matchings between clusters with articles in different languages .",
    "the problem of cross - lingual event linking is to match monolingual clusters of news articles that describing the same event across languages .",
    "for example , we want to match a cluster of spanish news articles and a cluster of english news articles that both describe the same earthquake .    each article @xmath153 is written in a language @xmath38 , where @xmath154 .",
    "for each language @xmath38 , we obtain a set of monolingual clusters @xmath155 .",
    "more precisely , the articles corresponding to each cluster @xmath156 are written in the language @xmath38 . given a pair of languages @xmath157 , @xmath158 and @xmath159 , we would like to identify all cluster pairs @xmath160 such that @xmath161 and @xmath162 describe the same event .",
    "clusters composed of english and spanish news articles .",
    "arrows link english articles with their spanish @xmath6-nearest neighbor matches according to the cross - lingual similarity.,scaledwidth=70.0% ]    matching of clusters is a _ generalized matching _",
    "we can not assume that there is only one cluster per language per event , nor can we assume complete coverage ",
    "i.e. , that there exists at least one cluster per event in every language .",
    "this is partly due to news coverage which might be more granular in some languages , partly due to noise and errors in the event detection process .",
    "this implies that we can not make assumptions on the matching ( e.g. , one - to - one or complete matching ) and excludes the use of standard weighted bipartite matching type of algorithms for this problem .",
    "an example is shown in figure  [ fig : clusters ] , where a cluster may contain articles which are closely matched with many clusters in a different language .",
    "we are also seeking for an algorithm which does not do exhaustive comparison of all clusters , since that can become prohibitively expensive when working in a real - time setting .",
    "more specifically , we wish to avoid testing cluster @xmath161 with all the clusters from all the other languages . performing exhaustive comparison would result in @xmath163 tests , where @xmath164 is the number of all clusters ( over all languages ) , which is not feasible when the number of clusters is on the order of tens of thousands .",
    "we address this by testing only clusters that are connected with at least one @xmath6-nearest neighbor ( marked as _ candidate clusters _ in figure  [ fig : clusters ] ) .      in order to identify clusters that are equivalent to cluster @xmath161",
    ", we have developed a two - stage algorithm . for a cluster @xmath161",
    ", we first efficiently identify a small set of candidate clusters and then find those clusters among the candidates , which are equivalent to @xmath161 .",
    "an example is shown in figure  [ fig : clusters ] .",
    "the details of the first step are described in algorithm  [ cluster_merge_algo1 ] .",
    "the algorithm begins by individually inspecting each news article @xmath165 in the cluster @xmath161 . using a chosen method for computing cross - lingual document similarity ( see section  [ sec : models ] )",
    ", it identifies the 10 most similar news articles to @xmath165 in each language @xmath166 . for each similar article @xmath167",
    ", we identify its corresponding cluster @xmath162 and add it to the set of candidates .",
    "the set of candidate clusters obtained in this way is several orders of magnitude smaller than the number of all clusters , and at most linear with respect to the number of news articles in cluster @xmath161 . in practice , clusters contain highly related articles and as such similar articles from other languages mostly fall in only a few candidate clusters .",
    "although computed document similarities are approximate , our assumption is that articles in different languages describing the same event will generally have a higher similarity than articles about different events .",
    "while this assumption does not always hold , redundancy in the data mitigates these false positives . since we compute the 10 most similar articles for each article in @xmath161 , we are likely to identify all the relevant candidates for cluster @xmath161 .",
    "@xmath168    the second stage of the algorithm determines which ( if any ) of the candidate clusters are equivalent to @xmath161 .",
    "we treat this task as a supervised learning problem . for each candidate cluster @xmath169",
    ", we compute a vector of learning features that should be indicative of whether @xmath161 and @xmath162 are equivalent or not and apply a binary classification model that predicts if the clusters are equivalent or not .",
    "the classification algorithm that we used to train a model was a linear support vector machine ( svm ) method  @xcite .",
    "we use three groups of features to describe cluster pair @xmath170 .",
    "the first group is based on * cross - lingual article links * , which are derived using cross - lingual similarity : each news article @xmath165 is linked with its @xmath171-nearest neighbors articles from all other languages ( 10 per each language ) .",
    "the group contains the following features :    * ` linkcount ` is the number of times any of the news articles from @xmath162 is among @xmath171-nearest neighbors for articles from @xmath161 . in other words",
    ", it is the number of times an article from @xmath161 has a very similar article ( i.e. , is among 10 most similar ) in @xmath162 . * ` avgsimscore ` is the average similarity score of the links , as identified for ` linkcount ` , between the two clusters .",
    "the second group are * concept - related features*. articles that are imported into event registry are annotated by disambiguating mentioned _ entities _ and _ keywords _ to the corresponding wikipedia pages  @xcite .",
    "whenever barack obama is , for example , mentioned in the article , the article is annotated with a link to his wikipedia page . in the same way ,",
    "all mentions of entities ( people , locations , organizations ) and ordinary keywords ( e.g. , bank , tax , ebola , plane , company ) are annotated .",
    "although the spanish article about obama will be annotated with his spanish version of the wikipedia page , in many cases we can link the wikipedia pages to their english versions .",
    "this can be done since wikipedia itself provides information regarding which pages in different languages represent the same concept / entity . using this approach ,",
    "the word `` avin '' in a spanish article will be annotated with the same concept as the word `` plane '' in an english article .",
    "although the articles are in different languages , the annotations can therefore provide a language - independent vocabulary that can be used to compare articles / clusters . by analyzing all the articles in clusters @xmath161 and @xmath162",
    ", we can identify the most relevant entities and keywords for each cluster .",
    "additionally , we can also assign weights to the concepts based on how frequently they occur in the articles in the cluster . from the list of relevant concepts and corresponding weights",
    ", we consider the following features :    * ` entitycossim ` is the cosine similarity between vectors of entities from clusters @xmath161 and @xmath162 . *",
    "` keywordcossim ` is the cosine similarity between vectors of keywords from clusters @xmath161 and @xmath162 . * ` entityjaccardsim ` is jaccard similarity coefficient  @xcite between sets of entities from clusters @xmath161 and @xmath162 . * ` keywordjaccardsim ` is jaccard similarity coefficient between sets of keywords from clusters @xmath161 and @xmath162 .",
    "the last group of features contains three * miscellaneous features * that seem discriminative but are unrelated to the previous two groups :    * ` hassamelocation ` feature is a boolean variable that is true when the location of the event in both clusters is the same .",
    "the location of events is estimated by considering the locations mentioned in the articles that form a cluster and is provided by event registry . *",
    "` timediff ` is the absolute difference in hours between the two events .",
    "the publication time and date of the events is computed as the average publication time and date of all the articles and is provided by event registry . * ` shareddates ` is determined as the jaccard similarity coefficient between sets of date mentiones extracted from articles .",
    "we use extracted mentions of dates provided by event registry , which uses an extensive set of regular expressions to detect and normalize mentions of dates in different forms .",
    "we will describe the main dataset for building cross - lingual models which is based on wikipedia and then present two sets of experiments .",
    "the first set of experiments establishes that the hub based approach can deal with language pairs where little or no training data is available .",
    "the second set of experiments compares the main approaches that we presented on the task of mate retrieval and the task of event linking .",
    "finally , we examine how different choices of features impact the event linking performance .      to investigate the empirical performance of the low - rank approximations we will test the algorithms on a large - scale , real - world multilingual dataset that we extracted from wikipedia by using inter - language links for alignment .",
    "this results in a large number of weakly comparable documents in more than @xmath172 languages .",
    "wikipedia is a large source of multilingual data that is especially important for the languages for which no translation tools , multilingual dictionaries as eurovoc  @xcite , or strongly aligned multilingual corpora as europarl  @xcite are available .",
    "documents in different languages are related with so called inter - language links that can be found on the left of the wikipedia page .",
    "the wikipedia is constantly growing .",
    "there are currently 12 wikipedias with more than 1 million articles , @xmath173 with more than 100k articles , @xmath174 with more than 10k articles , and @xmath175 with more than @xmath176 articles .",
    "each wikipedia page is embedded in the page tag .",
    "first , we check if the title of the page starts with a wikipedia namespace ( which includes categories and discussion pages ) and do not process the page if it does .",
    "then , we check if this is a redirection page and we store the redirect link because inter - language links can point to redirection links also . if none of the above applies , we extract the text and parse the wikipedia markup .",
    "currently , all the markup is removed .",
    "we get inter - language link matrix using previously stored redirection links and inter - language links .",
    "if an inter - language link points to the redirection we replace it with the redirection target link .",
    "it turns out that we obtain the matrix @xmath177 that is not symmetric , consequently the underlying graph is not symmetric .",
    "that means that existence of the inter - language link in one way ( i.e. , english to german ) does not guarantee that there is an inter - language link in the reverse direction ( german to english ) . to correct this",
    "we transform this matrix to be symmetric by computing @xmath178 and obtaining an undirected graph . in the rare case that after symmetrization we have multiple links pointing from the document , we pick the first one that we encountered .",
    "this matrix enables us to build an alignment across all wikipedia languages ( for dumps available in 2013 ) .      in this subsection",
    ", we will investigate the empirical performance of hub cca approach .",
    "we will demonstrate that this approach can be successfully applied even in the case of fully missing alignment information . to this purpose , we select a subset of wikipedia languages containing three major languages , english ( 4,212k articles)_en _ ( hub language ) , spanish ( 9,686k articles)_es _ , russian ( 9,662k articles)_ru _ , and five minority ( in terms of wikipedia sizes ) languages , slovenian ( 136k articles)_sl _ , piedmontese ( 59k articles)_pms _ , waray - waray ( 112k articles)_war _ ( all with about 2 million native speakers ) , creole ( 54k articles)_ht _ ( 8 million native speakers ) , and hindi ( 97k articles)_hi _ ( 180 million native speakers ) . for preprocessing , we remove the documents that contain less than 20 different words ( referred to as stubs ) and remove words occurring in less than 50 documents as well as the top 100 most frequent words ( in each language separately ) .",
    "we represent the documents as normalized tfidf  @xcite weighted vectors .",
    "the idf scores are computed for each language based on its aligned documents with the english wikipedia .",
    "the english language idf scores are based on all english documents for which aligned spanish documents exist .",
    "the evaluation is based on splitting the data into training and test sets .",
    "we select the test set documents as all multilingual documents with at least one nonempty alignment from the list : ( _ hi _ , _ ht _ ) , ( _ hi _ , _ pms _ ) , ( _ war _ , _ ht _ ) , ( _ war _ , _ pms _ ) .",
    "this guarantees that we cover all the languages .",
    "moreover this test set is suitable for testing the retrieval through the hub as the chosen pairs have empty alignments .",
    "the remaining documents are used for training . in table",
    "[ table : train_test ] , we display the corresponding sizes of training and test documents for each language pair .    on the training set , we perform the two step procedure to obtain the common document representation as a set of mappings @xmath64 .",
    "a test set for each language pair , @xmath179 , consists of comparable document pairs ( linked wikipedia pages ) , where @xmath180 is the test set size .",
    "we evaluate the representation by measuring mate retrieval quality on the test sets : for each @xmath38 , we rank the projected documents @xmath181 according to their similarity with @xmath182 and compute the rank of the mate document @xmath183 . the final retrieval score ( between -100 and 100 ) is computed as : @xmath184 . a score that is less than 0 means that the method performs worse than random retrieval and a score of 100 indicates perfect mate retrieval .",
    "the mate retrieval results are included in table [ table : retrieval ] .",
    "we observe that the method performs well on all pairs of languages , where at least 50,000 training documents are available(_en _ , _ es _ , _ ru _ , _ sl _ ) .",
    "we note that taking @xmath185 or @xmath186 multilingual topics usually results in similar performance , with some notable exceptions : in the case of ( _ ht _ , _ war _ ) the additional topics result in an increase in performance , as opposed to ( _ ht _ , _ pms _ ) where performance drops , which suggests overfitting .",
    "the languages where the method performs poorly are _",
    "ht _ and _ war _ , which can be explained by the quality of data ( see table [ table : rank ] and explanation that follows ) . in case of _ pms",
    "_ , we demonstrate that solid performance can be achieved for language pairs ( _ pms _ , _ sl _ ) and ( _ pms _ , _ hi _ ) , where only 2,000 training documents are shared between _ pms _ and _ sl _ and no training documents are available between _ pms _ and _ hi_. also observe that in the case of ( _ pms _ , _ ht _ ) the method still obtains a score of 62 , even though training set intersection is zero and _ ht _ data is corrupted , which we will show in the next paragraph .",
    "c|c|c|c|c|c|c|c|c| & en & es & ru & sl & hi & war & ht & pms + en & 671  -  4.64 & 463  -  4.29 & 369  -  3.19 & 50.3  -  2 & 14.4  -  2.76 & 8.58  -  2.41 & 17  -  2.32 & 16.6  -  2.67 + es & & 463  -  4.29 & 187  -  2.94 & 28.2  -  1.96 & 8.72  -  2.48 & 6.88  -  2.4 & 13.2  -  2 & 13.8  -  2.58 + ru & & 369  -  3.19 & 29.6  -  1.92 & 9.16  -  2.68 & 2.92  -  1.1 & 3.23  -  2.2 & 10.2  -  1.29 + sl & & 50.3  -  2 & 3.83  -  1.65 & 1.23  -  0.986 & 0.949  -  1.23 & 1.85  -  0.988 + hi & & 14.4  -  2.76 & 0.579  -  0.76 & 0.0  -  2.08 & 0.0  -  0.796 + war & & 8.58  -  2.41 & 0.043  -  0.534 & 0.0  -  1.97 + ht & & 17  -  2.32 & 0.0  -  0.355 + pms & & 16.6  -  2.67 +     en & & 98  -  98 & 95  -  97 & 97  -  98 & 82  -  84 & 76  -  74 & 53  -  55 & 96  -  97 + es & 97  -  98 & & 94  -  96 & 97  -  98 & 85  -  84 & 76  -  77 & 56  -  57 & 96  -  96 + ru & 96  -  97 & 94  -  95 & & 97  -  97 & 81  -  82 & 73  -  74 & 55  -  56 & 96  -  96 + sl & 96  -  97 & 95  -  95 & 95  -  95 & & 91  -  91 & 68  -  68 & 59  -  69 & 93  -  93 + hi & 81  -  82 & 82  -  81 & 80  -  80 & 91  -  91 & & 68  -  67 & 50  -  55 & 87  -  86 + war & 68  -  63 & 71  -  68 & 72  -  71 & 68  -  68 & 66  -  62 & & 28  -  48 & 24  -  21 + ht & 52  -  58 & 63  -  66 & 66  -  62 & 61  -  71 & 44  -  55 & 16  -  50 & & 62  -  49 + pms & 95  -  96 & 96  -  96 & 94  -  94 & 93  -  93 & 85  -  85 & 23  -  26 & 66  -  54 & +    we further inspect the properties of the training sets by roughly estimating the fraction @xmath187 for each english training matrix and its corresponding mate matrix , where @xmath188 and @xmath189 denote the number of rows and columns respectively .",
    "the denominator represents the theoretically highest possible rank the matrix @xmath0 could have .",
    "ideally , these two fractions should be approximately the same - both aligned spaces should have reasonably similar dimensionality .",
    "we display these numbers as pairs in table [ table : rank ] .",
    "ht & en  pms + 0.81  0.89 & 0.8  0.89 & 0.98  0.96 & 1  1 & 0.74  0.56 & 1  0.22 & 0.89  0.38 +    it is clear that in the case of the creole language only at most @xmath190 documents are unique and suitable for the training .",
    "though we removed the stub documents , many of the remaining documents are nearly the same , as the quality of some smaller wikipedias is low .",
    "this was confirmed for the creole , waray - waray , and piedmontese languages by manual inspection .",
    "the low quality documents correspond to templates about the year , person , town , etc . and contain very few unique words .",
    "there is also a problem with the quality of the test data .",
    "for example ,",
    "if we look at the test pair ( _ war _ , _ ht _ ) only 386/534 waray - waray test documents are unique but on the other side almost all creole test documents ( 523/534 ) are unique .",
    "this indicates a poor alignment which leads to poor performance .      in order to determine",
    "how accurately we can predict cluster equivalence , we performed two experiments in a multilingual setting using english , german and spanish languages for which we had labelled data to evaluate the linking performance . in the first experiment , we tested how well the individual approaches for cross - lingual article linking perform when used for linking the clusters about the same event .",
    "in the second experiment we tested how accurate the prediction model is when trained on different subsets of learning features . to evaluate the prediction accuracy for a given dataset we used 10-fold cross validation .",
    "we created a manually labelled dataset in order to evaluate cross - lingual event linking using two human annotators .",
    "the annotators were provided with an interface listing the articles , their content from and top concepts for a pair of clusters and their task was to determine if the clusters were equivalent or not ( i.e. , discuss same event ) . to obtain a pair of clusters @xmath170 to annotate , we first randomly chose a cluster @xmath161 , used algorithm  [ cluster_merge_algo1 ] to compute a set of potentially equivalent clusters @xmath56 and randomly chose a cluster @xmath169 .",
    "the dataset provided by the annotators contains 808 examples , of which 402 are equivalent clusters pairs and 406 are not .",
    "clusters in each learning example are either in english , spanish or german . although event registry imports articles in other languages as well , we restricted our experiments to these three languages .",
    "we chose only these three languages since they have very large number of articles and clusters per day which makes the cluster linking problem hard due to large number of possible links .    in section",
    "[ sec : models ] , we described three main algorithms for identifying similar articles in different languages .",
    "these algorithms were @xmath6-means , lsi and hub cca .",
    "as a training set , we used common wikipedia alignment for all three languages . to test which of these algorithms performed best , we made the following test .",
    "for each of the three algorithms , we analyzed all articles in event registry and for each article computed the most similar articles in other languages . to test",
    "how informative the identified similar articles are for cluster linking we then trained three classifiers as described in section  [ algo : features ] ",
    "one for each algorithm .",
    "each classifier was allowed to use as learning features * only * the cross - lingual article linking features for which values are determined based on the selected algorithm ( @xmath6-means , lsi and hub cca ) .",
    "the results of the trained models are shown in table  [ table : linkingevalalgos ] .",
    "we also show how the number of topics ( the dimensions of the latent space ) influences the quality , except in the case of the @xmath6-means algorithm , where only the performance on 500 topic vectors is reported , due to higher computational cost .",
    "we observe that , for the task of cluster linking , lsi and hub cca perform comparably and both outperform @xmath6-means .",
    "we also compared the proposed approaches on the task of wikipedia mate retrieval ( the same task as in section  [ experiments : hubcca ] ) .",
    "we computed the average ( over language pairs ) mean reciprocal rank ( amrr )  @xcite performance of the different approaches on the wikipedia data by holding out @xmath191 aligned test documents and using @xmath192 aligned documents as the training set .",
    "figure  [ pic : amrr ] shows amrr score as the function of the number of feature vectors .",
    "it is clear that hub cca outperforms lsi approach and @xmath6-means lags far behind when testing on wikipedia data .",
    "the hub cca approach with @xmath193 topic vectors manages to perform comparably to the @xmath194-based approach with @xmath176 topic vectors , which shows that the @xmath195 method can improve both model memory footprint as well as similarity computation time .",
    "furthermore , we inspected how the number of topics influences the accuracy of cluster linking . as we can see from table  [ table : linkingevalalgos ] choosing a number of features larger than @xmath193 barely affects linking performance , which is in contrast with the fact that additional topics helped to improve ammr , see figure  [ pic : amrr ] .",
    "such differences may have arisen due to different domains of training and testing ( wikipedia pages versus news articles ) .",
    "we also analyzed how cluster size influences the accuracy of cluster linking .",
    "we would expect that if the tested pair of clusters has a larger number of articles then the classifier should be able to more accurately predict whether the clusters should be linked or not .",
    "the reasoning is that the large clusters would provide more document linking information ( more articles mean more links to other similar articles ) as well as more accurately aggregated semantic information . in the case of smaller clusters ,",
    "the errors of the similarity models have greater impact which should decrease the performance of the classifier , too .",
    "to validate this hypothesis we have split the learning examples into two datasets  one containing cluster pairs where the combined number of articles from both clusters is below 20 and one dataset where the combined number is 20 or more .",
    "the results of the experiment can be seen in table  [ table : linkingevalalgoslargesmall ] .",
    "as it can be seen , the results confirm our expectations : for smaller clusters it is indeed harder to correctly predict if the cluster pair should be merged or not .",
    "the hub cca attains higher precision and classification accuracy on the task of linking small cluster pairs than the other methods , while lsi is slightly better on linking large cluster pairs .",
    "the gain in precision of lsi over hub cca on linking large clusters is much smaller than the gain in precision of hub cca over lsi on linking small clusters .",
    "for that reason we decided to use hub cca as the similarity computation component in our system .",
    "table[row sep = crcr]100 0.574094231082746 + 200 0.645978450670063 + 300 0.682226506762222 + 400 0.710722094692085 + 500 0.731517169423203 + 600 0.746223870375363 + 700 0.758312886611436 + 800 0.766279310559898 + 900 0.768159613081689 + 1000 0.76200517888255 + ; ;    table[row sep = crcr]100 0.388773162433835 + 200 0.49061571487968 + 300 0.550801867794634 + 400 0.592565604079171 + 500 0.625657099289601 + 600 0.65240747960386 + 700 0.675602522126668 + 800 0.694149727717174 + 900 0.711511053167843 + 1000 0.726455461653153 + ; ;    table[row sep = crcr]100 0.203799120889507 + 200 0.275537657758881 + 300 0.334738107118325 + 400 0.378380649062479 + 500 0.409668545430712 + 600 0.427765004442194 + 700 0.454449272479224 + 800 0.477291178317808 + 900 0.490692600634559",
    "+ 1000 0.518392823243787 + ; ;     @xmath196 % + hub cca & 78.2/79.6/80.3 & 76.3/78.0/80.5 & 81.6/82.1/79.9 & 78.9/80.0/80.2 + lsi & 78.9/78.7/80.6 & 76.8/77.0/78.7 & 83.3/80.6/83.6 & 79.9/78.8/81.1 + @xmath6-means & 73.9/-/- & 69.5/-/- & 84.6/-/- & 76.3/-/- +     @xmath196 % + hub cca & 81.2 - 77.8 & 80.5 - 74.5 & 91.3 - 57.5 & 85.6 - 64.9 + lsi & 82.8 - 76.4 & 81.3 - 70.9 & 93.1 - 57.5 & 86.8 - 63.5 + @xmath6-means & 75.5 - 71.2 & 72.8 - 70.8 & 95.3 - 36.2 & 82.5 - 47.9 +    in the second experiment , we evaluate how relevant individual groups of features are to correctly determine cluster equivalence . for this purpose , we computed accuracy using individual groups of features , as well as using different combination of groups . since hub cca had the best performance of the three algorithms , we used it to compute the values of the cross - lingual article linking features .",
    "the results of the evaluation are shown in table  [ table : linkingeval ] .",
    "we can see that using a single group of features , the highest prediction accuracy can be achieved using concept - related features .",
    "the classification accuracy in this case is 88.5% . by additionally including also the cross - lingual article linking features , the classification accuracy rises slightly to 89.4% . using all three groups of features , the achieved accuracy is 89.2% .    to test",
    "if the accuracy of the predictions is language dependent we have also performed the evaluations separately on individual language pairs .",
    "for this experiment we have split the annotated learning examples into three datasets , where each dataset contained only examples for one language pair . when training the classifier all three groups of features were available .",
    "the results are shown in table  [ table : langpaireval ] .",
    "we can see that the performance of cluster linking on the english - german dataset is the highest in terms of accuracy , precision , recall and @xmath196 .",
    "the performance on the english - spanish dataset is comparable to the performance on the english - german dataset , where the former achieves higher recall ( and slightly higher @xmath196 score ) , while the latter achieves higher precision .",
    "a possible explanation of these results is that the higher quantity and quality of english - german language resources leads to a more accurate cross - lingual article similarity measure as well as to a more extensive semantic annotation of the articles .",
    "based on the performed experiments , we can make the following conclusions . the cross - lingual similarity algorithms provide valuable information that can be used to identify clusters that describe the same event in different languages .",
    "for the task of cluster linking , the cross - lingual article linking features are however significantly less informative compared to the concept - related features that are extracted from the semantic annotations .",
    "nevertheless , the cross - lingual article similarity features are very important for two reasons .",
    "the first is that they allow us to identify for a given cluster a limited set of candidate clusters that are potentially equivalent .",
    "this is a very important feature since it reduces the search space by several orders of magnitude .",
    "the second reason these features are important is that concept annotations are not available for all articles as the annotation of news articles is computationally intensive and can only be done for a subset of collected articles .",
    "the prediction accuracies for individual language pairs are comparable although it seems that the achievable accuracy correlates with the amount of available language resources .",
    "@xmath196 % + hub cca & @xmath197 & @xmath198 & @xmath199 & @xmath200 + concepts & @xmath201 & @xmath202 & @xmath203 & @xmath204 + misc & @xmath205 & @xmath206 & @xmath207 & @xmath208 + hub cca + concepts & @xmath209 & @xmath210 & @xmath211 & @xmath212 + hub cca + misc & @xmath213 & @xmath214 & @xmath215 & @xmath216 + concepts + misc & @xmath217 & @xmath218 & @xmath219 & @xmath220 + all & @xmath221 & @xmath222 & @xmath223 & @xmath224 +     @xmath196 % + en , de & @xmath225 & @xmath226 & @xmath227 & @xmath228 + en , es & @xmath229 & @xmath230 & @xmath231 & @xmath232 + es , de & @xmath233 & @xmath234 & @xmath235 & @xmath236 +      one of the main advantages of our approach is that it is highly scalable .",
    "it is fast , very robust to quality of training data , easily extendable , simple to implement and has relatively small hardware requirements .",
    "the similarity pipeline is the most computationally intensive part and currently runs on a machine with two intel xeon e5 - 2667 v2 , 3.30ghz processors with 256 gb of ram .",
    "this is sufficient to do similarity computation over a large number of languages if needed .",
    "it currently uses wikipedia as a freely available knowledge base and experiments show that the similarity pipeline dramatically reduces the search space when linking clusters .    currently , we compute similarities over @xmath237 languages with tags : _ eng _ , _ spa _ , _ deu _ , _ zho _ , _ ita _ , _ fra _ , _ rus _ , _ swe _ , _ nld _ , _ tur _ , _ jpn _ , _ por _ , _ ara _ , _ fin _ , _ ron _ , _ kor _ , _ hrv _ , _ tam _ , _ hun _ , _ slv _ , _ pol _ , _ srp _ , _ cat _ , _ ukr _ but we support any language from the top @xmath238 wikipedia languages .",
    "our data stream is newsfeed ( http://newsfeed.ijs.si/ ) which provides 430k unique articles per day .",
    "our system currently computes 2 million similarities per second , that means that we compute @xmath239 similarities per day .",
    "we store one day buffer for each language which requires 1.5 gb of memory with documents stored as 500-dimensional vectors .",
    "we note that the time complexity of the similarity computations scales linearly with dimension of the feature space and does not depend on number of languages . for each article",
    ", we compute the top @xmath171 most similar ones in every other language .    for all linear algebra matrix and vector operations ,",
    "we use high performance numerical linear algebra libraries as blas , openblas and intel mkl , which currently allows us to process more than one million articles per day . in our current implementation",
    ", we use the variation of the hub approach .",
    "our projector matrices are of size @xmath240 so every projector takes about @xmath241 gb of ram .",
    "moreover , we need proxy matrices of size @xmath242 for every language pair . that is 0.5 gb for @xmath237 languages and @xmath243 gb for @xmath238 languages . all together",
    "we need around 135 gb of ram for the system with 100 languages .",
    "usage of proxy matrices enables the projection of all input documents in the common space and handling language pairs with missing or low alignment .",
    "that enables us to do block - wise similarity computations further improving system efficiency .",
    "our code can therefore be easily parallelized using matrix multiplication rather than performing more matrix - vector multiplications .",
    "this speeds up our code by a factor of around @xmath244 in this way , we obtain some caching gains and ability to use vectorization . our system is also easily extendable .",
    "adding a new language requires the computation of a projector matrix and proxy matrices with all other already available languages .",
    "in this paper we have presented a cross - lingual system for linking events in different languages .",
    "building on an existing system , event registry , we present and evaluate several approaches to compute a cross - lingual similarity function . we also present an approach to link events and evaluate effectiveness of various features .",
    "the final pipeline is scalable both in terms of number of articles and number of languages , while accurately linking events .    on the task of mate retrieval",
    ", we observe that refining the lsi - based projections with hub cca leads to improved retrieval precision , but the methods perform comparably on the task of event linking .",
    "further inspection showed that the cca - based approach reached a higher precision on smaller clusters .",
    "the interpretation is that the linking features are highly aggregated for large clusters , which compensates the lower per - document precision of lsi .",
    "another possible reason is that the advantage that we show on wikipedia is lost on the news domain .",
    "this hypothesis could be validated by testing the approach on documents from a different domain .",
    "the experiments show that the hub cca - based features present a good baseline , which can greatly benefit from additional semantic - based features .",
    "even though in our experiments the addition of cca - based features to semantic features did not lead to great performance improvements , there are two important benefits in the approach .",
    "first , the linking process can be sped up by using a smaller set of candidate clusters .",
    "second , the approach is robust to languages where semantic extraction is not available , due to scarce linguistic resources .",
    "currently the system is loosely - coupled  the language component is built independently from the rest of the system , in particular the linking component .",
    "it is possible that better embeddings can be obtained by methods that jointly optimize a classification task and the embedding .",
    "another point of interest is to evaluate the system on languages with scarce linguistic resources , where semantic annotation might not be available . for this purpose",
    ", the labelled dataset of linked clusters should be extended first .",
    "the mate retrieval evaluation showed that even for language pairs with no training set overlap , the hub cca recovers some signal .    in order to further improve the performance of the classifier for cluster linking",
    ", additional features should also be extracted from articles and clusters and checked if they can increase the accuracy of the classification . since the amount of linguistic resources vary significantly from language to language it would also make sense to build a separate classifier for each language pair . intuitively , this should improve performance since weights of individual learning features could be adapted to the tested pair of languages .",
    "janez brank , gregor leban , and marko grobelnik . a high - performance multithreaded approach for clustering a stream of documents . in _ proceedings of the 17th international multiconference information society 2014 , volume e , ljubljana , slovenia _ , pages 58 , 2014 .",
    "dumais , t.a .",
    "letsche , m.l .",
    "littman , and t.k landauer .",
    "automatic cross - language retrieval using latent semantic indexing . in _",
    "aaai spring symposium on cross - language text and speech retrieval .",
    "american association for artificial intelligence , vol . 16 .",
    ", page  21 , 1997 .",
    "philipp koehn , hieu hoang , alexandra birch , chris callison - burch , marcello federico , nicola bertoldi , brooke cowan , wade shen , christine moran , richard zens , chris dyer , ondej bojar , alexandra constantin , and evan herbst .",
    "moses : open source toolkit for statistical machine translation . in _ proceedings of the 45th annual meeting of the acl on interactive poster and demonstration sessions _ , acl 07 , pages 177180 , prague , czech republic , 2007 .",
    "association for computational linguistics .",
    "gregor leban , blaz fortuna , janez brank , and marko grobelnik .",
    "cross - lingual detection of world events from news articles . in _ proceedings of the 13th international semantic web conference _ , pages 2124 , riva del garda - trentino , italy , 2014 .",
    "gregor leban , blaz fortuna , janez brank , and marko grobelnik .",
    "event registry : learning about world events from news . in _ proceedings of the companion publication of the 23rd international conference on world wide web companion _ ,",
    "www companion 14 , pages 107110 , seoul , republic of korea , 2014 . international world wide web conferences steering committee .",
    "kalev leetaru and philip  a schrodt .",
    "gdelt : global data on events , location , and tone , 19792012 . in _",
    "international studies association ( isa ) annual convention _ , volume  2 , page  4 , san francisco , california , usa , 2013 .",
    "david milne and ian  h. witten . learning to link with wikipedia . in _ proceedings of the 17th acm conference on information and knowledge management _ , cikm 08 , pages 509518 , napa valley , california , usa , 2008 .",
    "david mimno , hanna  m. wallach , jason naradowsky , david  a. smith , and andrew mccallum .",
    "polylingual topic models . in _ proceedings of the 2009 conference on empirical methods in natural language processing : volume 2 - volume 2 _ , emnlp 09 , pages 880889 , stroudsburg , pa , usa , 2009 .",
    "association for computational linguistics .",
    "andrej muhi , jan rupnik , and primo kraba .",
    "cross - lingual document similarity . in _",
    "information technology interfaces ( iti ) , proceedings of the iti 2012 34th international conference on _ , pages 387392 , cavtat / dubrovnik , croatia , 2012 .",
    "ieee .",
    "john  c platt , kristina toutanova , and wen - tau yih .",
    "translingual document representations from discriminative projections . in _ proceedings of the 2010 conference on empirical methods in natural language processing _ ,",
    "pages 251261 , massachusetts , usa , 2010 .",
    "association for computational linguistics .",
    "martin potthast , benno stein , and maik anderka . a wikipedia - based multilingual retrieval model . in _ advances in information retrieval ,",
    "30th european conference on information retrieval research ( ecir ) _ , pages 522530 ,",
    "glasgow , uk , 2008 .",
    "bruno pouliquen , ralf steinberger , and olivier deguernel .",
    "story tracking : linking similar news over time and across languages . in _ proceedings of the workshop on multi - source multilingual information extraction and summarization _ , pages 4956 , manchester , united kingdom , 2008 .",
    "association for computational linguistics .",
    "jose mara  lvarez rodrguez , emilio  rubiera azcona , and luis  polo paredes . promoting government controlled vocabularies for the semantic web : the eurovoc thesaurus and the cpv product classification system .",
    ", page 111 , 2008 .",
    "jan rupnik , andrej muhic , and primoz skraba .",
    "multilingual document retrieval through hub languages . in _ proceedings of the 15th multiconference on information society 2012 ( is-2012 ) _ , pages 201204 , ljubljana , slovenia , 2012 .",
    "duo zhang , qiaozhu mei , and chengxiang zhai .",
    "cross - lingual latent topic extraction . in _ proceedings of the 48th annual meeting of the association for computational linguistics _ , pages 11281137 , uppsala , sweden , 2010 .",
    "association for computational linguistics .",
    "lei zhang and achim rettinger .",
    "semantic annotation , analysis and comparison : a multilingual and cross - lingual text analytics toolkit . in _ proceedings of the demonstrations at the 14th conference of the european chapter of the association for computational linguistics ( eacl 2014 ) _ , pages 1316 ,",
    "gothenburg , sweden , april 2014 .",
    "association for computational linguistics ."
  ],
  "abstract_text": [
    "<S> in today s world , we follow news which is distributed globally . </S>",
    "<S> significant events are reported by different sources and in different languages . in this work , </S>",
    "<S> we address the problem of tracking of events in a large multilingual stream . within a recently developed system event registry  @xcite </S>",
    "<S> we examine two aspects of this problem : how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event . taking a multilingual stream and clusters of articles from each language </S>",
    "<S> , we compare different cross - lingual document similarity measures based on wikipedia . </S>",
    "<S> this allows us to compute the similarity of any two articles regardless of language . </S>",
    "<S> building on previous work , we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data . using this capability </S>",
    "<S> , we then propose an approach to link clusters of articles across languages which represent the same event . </S>",
    "<S> we provide an extensive evaluation of the system as a whole , as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm . </S>"
  ]
}