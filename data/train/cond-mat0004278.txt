{
  "article_text": [
    "one of the important but unsolved problems in neuroscience is to determine how information is coded in neuronal activities .",
    "most of the traditional neural network models consisting of binary units are constructed on the assumption that information is coded only in the mean firing rate of the neurons .",
    "although these models have provided us with theoretically interesting information , they ignore many dynamical aspects of neuronal activities .",
    "in fact , oscillatory activity appears to be ubiquitous in many neuronal systems . for example",
    ", some recent biological experiments have revealed that spatially synchronous oscillations of neuronal ensembles are dependent on the global properties of the external stimulus .",
    "it has been suggested that such synchronization is computationally significant in information processing @xcite .",
    "the hippocampus is also one of the areas in which neuronal synchronization is observed and is believed to play an important role in memory processing .    with these new findings ,",
    "many models concerning the dynamical aspects of neurons and memory processing have been proposed @xcite . among these , models consisting of networks of oscillator components are particularly attractive , owing to their mathematical tractability . like the hopfield model",
    ", the simplicity of these models allows us to obtain useful analytic results .",
    "in fact , it has been reported that under suitable conditions , the oscillator network models model associative memory , and we can theoretically evaluate their maximum storage capacities and basins of attraction @xcite .",
    "however , such phase oscillator models are based on the unrealistic situation that all the components in the network are always in the firing state .",
    "in fact , real neurons can be in either the firing or non - firing state .",
    "in addition , it is known that the level of activity in our brain is very low .",
    "( i.e. , at any given time only a small percentage of neurons are in the firing state . )",
    "this situation is termed _",
    "sparse coding_. from the results of theoretical studies of associative memory with binary units , it has been found that the storage capacity diverges as @xmath0 as the firing rate @xmath1 becomes small @xcite .",
    "it thus seems that to faithfully to capture the essential dynamics of real oscillatory neuronal systems , it is necessary to extend the oscillator model to treat the non - firing state as well as the firing state . in this paper , we present a simple extended oscillator neural network model of this nature . using this extended model , we study its associative memory ability to recall sparsely coded phase patterns in which some neurons are in the non - firing state and",
    "others encode information in the timings of spikes .    in the next section ,",
    "we first review the theoretical basis of the phase oscillator model and propose an extend version of the oscillator model to treat non - firing states . in the analysis of this model",
    ", we estimate the maximum storage capacity , derive the basin of attraction , and evaluate the quality of recalled memories .",
    "then , we find that when we define the threshold as a dynamical variable in a certain manner , the size of the basin of attraction can be increased .",
    "embedding patterns with different activity levels and the influence of synaptic dilution are also studied .",
    "let us first start with a survey of the theoretical basis of the phase oscillator model .",
    "we assume that neurons fire periodically and interact weakly with each other . in general ,",
    "although such a neural system can be described in terms of many internal dynamical variables , it is well known that it can be reduced to a system of simple coupled phase oscillators @xcite . in this form",
    ", we can characterize the state of the i - th neuron by a single variable , @xmath2 , which is referred to as the _ phase_. this variable represents the timing of the neuronal spikes at time @xmath3 .",
    "a typical reduced equation takes the form @xmath4 where @xmath5 and @xmath6 characterize the interaction between the i - th and j - th neurons .",
    "assuming that all natural frequencies @xmath7 are equal to some fixed value @xmath8 , we can eliminate the @xmath9 term in eq.([reduce ] ) by redefining @xmath10 through @xmath11 .",
    "when we represent the state of the i - th neuron by the complex form @xmath12 , eq.([reduce ] ) can be written in the alternative form @xmath13 where the complex variable @xmath14 represents the effect of the interaction between the i - th and j - th neurons and @xmath15 is the complex conjugate of @xmath16 .",
    "considering the fact that all neurons relax toward the equilibrium state satisfying the relation @xmath17 , we can simplify the above to the following discrete time system : @xmath18 this is known as a phase oscillator network model .",
    "it can be thought of as a synchronous updated version of the phase oscillator neural network .",
    "the weakness of the model described by eq.([original ] ) is that it can only be used to treat the firing state .",
    "we wish to extend this model so that it has the ability of retrieving sparsely coded phase patterns . in eq.([original ] ) , @xmath16 can be regarded as the local field produced by all other neurons .",
    "this field determines the state of the i - th neuron at the next time step . in a real neuron , when the membrane voltage is less than some threshold , the neuron generates no neuronal spikes .",
    "therefore , it is reasonable to extend the above model so that the generation of spikes depends on the strength of the local field .",
    "based on this consideration , for oscillator neural networks we propose the following generalized model : @xmath19 in this paper , we assume that @xmath20 , where @xmath21 is a step function defined by @xmath22 and @xmath23 is a threshold parameter controlling the activity of the network .",
    "figure[fw](a ) displays the function @xmath24 , and ( b ) illustrates the dynamical change at updating . since the amplitude @xmath25 depends on the threshold @xmath23 , it has a strong influence on the activity of the system .",
    "now , we define a set of complex patterns denoted by @xmath26 @xmath27 ; @xmath28 , where @xmath29 is the total number of patterns and @xmath30 is the total number of neurons .",
    "the variables @xmath31 and @xmath32 represent the phase and the amplitude of the i - th neuron in the @xmath33-th pattern , respectively . for theoretical simplicity , we choose @xmath32 independently with the probability distribution : @xmath34 for the firing state , @xmath31 is chosen at random from a uniform distribution between @xmath35 and @xmath36 .",
    "note that all the patterns have the same mean firing rate , @xmath1 .",
    "as the learning rule , we adopt a generalized hebb rule taking the form @xmath37 this relation is based on information obtained from experiments on biological systems that when the i - th and j - th neurons are firing simultaneously , the connection between them is enhanced , and otherwise no modification occurs .      to analyze the recalling process theoretically",
    ", we must introduce several macroscopic order parameters .",
    "the load parameter @xmath38 is defined by @xmath39 .",
    "the overlap @xmath40 between @xmath41 and @xmath42 at time @xmath3 is defined by @xmath43 in practice , owing to rotational symmetry , the similarity between the state of the system @xmath41 and the @xmath33-th pattern @xmath44 can be measured by @xmath45 , the amplitude of @xmath46 .",
    "now , let us consider the situation in which the system is retrieving the pattern @xmath47 , that is , @xmath48 the local field @xmath16 can be separated as @xmath49 where @xmath50 is defined by @xmath51 the first term in eq.([h_i ] ) is the signal driving the system to recall the pattern , and the second term can be regarded as the noise arising from the other learned patterns .",
    "it is the essence of our analysis that we treat the second noise term as a complex gaussian noise characterized by @xmath52 under the above assumptions , we study the properties of this network by applying the methods of statistical neurodynamics @xcite . to begin with ,",
    "we calculate the overlap at time @xmath53 . from eqs.([m_i ] ) , ( [ w_i ] ) and ( [ h_i ] ) , @xmath54 is given by @xmath55 now , we assume that the phase of @xmath54 is almost constant , that is , @xmath56 . the validity of this assumption is supported by the results of preliminary numerical simulations .",
    "owing to the rotational symmetry of the complex gaussian noise , we can replace @xmath57 with @xmath58 $ ] .",
    "after some calculations , in the limit @xmath59 , we obtain @xmath60 where @xmath61 represents the average over the complex gaussian noise @xmath62 defined by eq.([sigma ] ) . to calculate eq.([m ] )",
    "numerically , we need the value of @xmath63 , which is the variance of @xmath62 . thus , in the next step , we consider the relation between @xmath64 and @xmath50 , from which we can obtain the relation between @xmath65 and @xmath63 . from eq.([z_i ] )",
    ", the noise at time @xmath53 can be written as @xmath66 where @xmath67 and @xmath68 are defined by @xmath69 note that these functions satisfy the relation @xmath70 .",
    "we can carry out the summation in eq.([z ] ) under the assumption that @xmath67 is independent of @xmath71 .",
    "doing so , we obtain @xmath72 note that eqs.([z2 ] ) , ( [ k ] ) and ( [ g ] ) are needed to calculate @xmath73 in both the equilibrium and the non - equilibrium cases we will describe in the next two sections .      in this section , we consider the equilibrium state of our model , in which @xmath50 is constant .",
    "applying @xmath74 to eq.([z2 ] ) , we obtain @xmath75 using this equation , we immediately obtain @xmath76 where @xmath77 consequently , we find that the equilibrium state satisfies the equations @xmath78    solving the above equilibrium equations , we can find two types of solutions .",
    "if the load parameter @xmath38 is smaller than a certain value @xmath79 , there exists a solution for which the overlap @xmath80 between the system and the pattern is not zero .",
    "as @xmath81 implies that the system has a correlation with the retrieved pattern , this solution corresponds to the retrieval state .",
    "then , if @xmath38 is larger than @xmath79 , there is only one solution , and for this solution @xmath82 .",
    "this solution therefore corresponds to the non - retrieval state .",
    "the critical load parameter @xmath79 is called `` the maximum storage capacity '' .",
    "we now examine our theoretical results by comparing them with results from numerical simulations . in fig.[capacity2d](a ) , we show the dependence of @xmath79 on several parameters , such as the threshold @xmath83 and the activity level @xmath1 . in ( a ) , we can see that as the activity level @xmath1 decreases , the storage capacity @xmath79 increases for each @xmath23 .",
    "in particular , in the limit @xmath84 , we numerically find that the storage capacity diverges in proportion to @xmath0 , as in the case of the hopfield model .",
    "the maximum storage capacity as a function of @xmath1 and @xmath23 is illustrated in ( b ) .",
    "it is shown there that for any @xmath23 , the storage capacity diverges as @xmath1 decreases to zero .      in this section ,",
    "we derive dynamical equations to study the retrieving process . to obtain a recursion equation for @xmath73 , we start with eq.([z2 ] ) . squaring eq.([z2 ] ) , we obtain @xmath85 where @xmath86 and @xmath87 are given by eqs.([k ] ) and ( [ g ] ) , and @xmath88    to proceed with the calculation , we must estimate the time correlation of the noise . for the first - order approximation",
    ", we ignore the temporal correlation of @xmath50 .",
    "we then obtain @xmath89 this corresponds to the amari - maginu theory in the case of traditional neural networks .    for the second - order approximation",
    ", we take into account the fact that @xmath50 is correlated only with @xmath90 , while all correlations with @xmath91 for @xmath92 are ignored . in this case",
    ", @xmath65 can be obtained from @xmath93 where @xmath94 is given by eq.([q(t ) ] ) , @xmath87 is given by eq.([g ] ) , and @xmath95 here , @xmath96 is needed to evaluate eq.([xt ] ) .",
    "note that @xmath97 is the correlation coefficient between @xmath62 and @xmath98 .",
    "for the second order approximation , consequently , the retrieving process of the network is described by the equations @xmath99    for initial conditions , we choose @xmath100 , @xmath101 and @xmath102 .",
    "we choose many values of the initial overlap @xmath103 and carry out numerical calculations for each . in this way",
    ", we determine the lower bound , under which the network fails to retrieve the pattern .    for higher - order approximations",
    ", we could derive similar generalized equations describing the retrieval dynamical process .",
    "however , as shown in fig.[timestep ] , the theoretical prediction at second order gives reasonable agreement with the numerical simulation in contrast to that at first order .",
    "we thus conclude that it is sufficient to use the second - order approximation for the theoretical analysis in this paper .",
    "figure[basin1 ] displays a phase diagram for @xmath104 and @xmath105 at various mean activity levels @xmath1 and thresholds @xmath23 .",
    "the solid lines and the data points here correspond to the theoretical and the numerical results , respectively .",
    "it is seen that even in the region satisfying @xmath106 , the system can not retrieve the pattern if the initial overlap @xmath103 is smaller than @xmath104 .",
    "the boundary corresponding to @xmath104 is represented by the lower curves in fig.[basin1 ] .",
    "thus , for @xmath107 , @xmath108 reaches the value of the upper curves @xmath105 , while for @xmath109 , @xmath108 decreases to zero .",
    "we should note that the basins of attraction remain wide even near @xmath79 .",
    "this can be thought of as an advantage of associative memory .",
    "to predict the correct behavior of the retrieval dynamics , it is necessary for the time correlation of the noise terms to be taken into account , and thus the second - order approximation discussed above is necessary . in the case",
    "we consider , this order is also sufficient , but interestingly , it has been reported that the fourth - order approximation is necessary when @xmath110 and @xmath111 @xcite .",
    "the question arises why the second - order approximation is not sufficient in this case , while it is sufficient in the case we consider .",
    "the key point here is that the order of the last term in eq.([2ndsigma ] ) is proportional to the square of the activity level @xmath112 . for @xmath113 , this factor",
    "@xmath112 weakens the influence of the time correlation on the recalling process .",
    "consequently , for a sparse coded pattern with @xmath113 , even the second - order approximation results in reasonable agreement with the numerical results .      in this section ,",
    "we consider the case in which the network retrieves a cyclic sequence of @xmath29 patterns associatively , say @xmath114 . in order to allow for such a process",
    ", we employ synaptic connections of the form @xmath115 in a manner similar to that in the derivation of eqs.([1stsigma ] ) and ( [ 2ndsigma ] ) , we obtain the following equations : @xmath116 note that , since the target pattern changes from time to time , for the definition of the overlap we adopt latexmath:[$m(t)\\equiv m^\\mu(t)=    @xmath33 is the number of the target pattern at time @xmath3 . here",
    "@xmath87 and @xmath94 are the same as those defined by eqs.([2ndg ] ) and ( [ 2ndq ] ) , respectively .",
    "we should note that in the limit @xmath118 , the last term in eqs.([1stsigma ] ) and ( [ 2ndsigma ] ) vanishes in eq.([secsigma ] ) , because the effect of the time correlation can be ignored in the above derivation .",
    "therefore , in a sequence generator , it is expected that our theoretical prediction is almost exact .",
    "figure[basinseq](a ) displays a phase diagram obtained from our theoretical analysis .",
    "all the lines of theoretical results agree with the numerical ones quite well , as expected .",
    "this agreement suggests that higher - order approximations will result in even better agreement and leads us to believe that our theoretical derivation is valid . in ( b ) , it is also shown that , like auto - associative memory , the storage capacity diverges in the limit @xmath84 .",
    "this is regarded as expressing the meaning that in the limit of sparse coding ( i.e. , as @xmath84 ) , we can embed an infinite length of sequential patterns .      from fig.[basin1 ]",
    ", it appears that the basin of attraction for our model is smaller than those in the binary model and the phase oscillator . in this section",
    ", we attempt to increase the size of the basin of attraction by defining the threshold as a dynamical variable that is proportional to the standard deviation of the noise .",
    "therefore , assuming the same condition as in sec.[model1 ] , we add the equation @xmath119 where @xmath120 is the threshold at time @xmath3 .",
    "this choice for the form of @xmath120 is made to insure the relations @xmath121 and [ firing rate ] @xmath122 , which cause the hamming distance between the state of the neuron and the retrieved pattern to be small @xcite . comparing fig.[basinself ] with fig.[basin1 ] , it is seen that the basin of attraction can be enlarged by introducing the dynamically adjusted threshold into our model .",
    "therefore , when we introduce the dynamically adjusted threshold , our model has an advantage similar to that of the binary model and the phase oscillator .",
    "to this point , we have assumed that the activity levels are equal for all patterns . however , it is likely that the actual activity level depends on the pattern which the network is retrieving presently , in other words , on the content of the required information",
    ". unfortunately , using a traditional neural network , we encounter difficulties in storing patterns with different activity levels simultaneously . in this section ,",
    "we demonstrate that , unlike traditional models , the proposed oscillator model can easily store multiple patterns with different activity levels using a simple hebbian learning rule .",
    "let us consider a set of complex patterns defined by @xmath123=    \\cases {      a_1 & for $ 1\\leq\\mu\\leq p_1 $ \\cr      a_2 & for $ p_1 + 1\\leq\\mu\\leq p$ , \\cr      } \\ ] ] where generally @xmath124 .",
    "thus , the total number of the patterns with activity level @xmath125 is @xmath126 , while the total number of patterns with the activity level @xmath127 is @xmath128 . using the above patterns , grouped into two different activity levels",
    ", we examine whether the network can retrieve patterns having different firing rates . to retrieve such patterns , we use the following modified form of the learning rule eq.([cij ] ) :",
    "@xmath129 note that this is slightly different from the covariance rule adopted in the context of the learning of sparsely coded patterns .",
    "owing to the rotational symmetry of the phase distribution in the patterns , @xmath130 can be defined in terms of the patterns @xmath131 themselves , rather than the difference between the patterns and the average activity .    with the method described in sec.[onn ]",
    ", we can derive both equilibrium and dynamical equations .",
    "the results have the same forms as those in sec.[onn ] .",
    "however , here the value @xmath125 should be used in place of the activity parameter @xmath1 in the previous equations , because the retrieval pattern @xmath47 has an activity level @xmath125 .",
    "let us define the load parameter as @xmath132 with respect to the retrieval pattern .",
    "( the usual load parameter is defined as @xmath133 . ) the theoretical analysis yields @xmath134 , where @xmath135 is the maximum storage capacity for @xmath125 , and @xmath136 is the storage capacity for @xmath127 .",
    "figure[doublecapa ] displays @xmath135 as a function of @xmath137 .",
    "we can see from fig.[doublecapa ] that @xmath138 , where the constant value @xmath139 is given by the equations in ( [ mconst ] ) , ( [ sconst ] ) , ( [ gconst ] ) and ( [ qconst ] ) for @xmath140 . for the maximum storage capacity @xmath141 associated with the activity level @xmath127 , we have @xmath142 .",
    "let us consider the case @xmath143 .",
    "note that in this case , @xmath144 and the total storage capacity @xmath38 has the relation as @xmath145 .",
    "we assume that @xmath146 , @xmath147 and @xmath148 . under these conditions ,",
    "the basin of attraction obtained numerically from the theory is displayed in the left panel of fig.[doublebasin ] .",
    "we should note that in the region ( b ) , the patterns with activity @xmath125 can be recalled , while the patterns with activity @xmath127 can not .",
    "since we consider the case @xmath143 , the vertical lines correspond to half of the usual maximum storage capacities . in the right figures , we display typical behavior of the overlap @xmath108 for the initial condition @xmath149 .",
    "the load parameters @xmath150 and @xmath151 used here correspond to the regions ( a ) , ( b ) and ( c ) shown in the left figure , respectively . in the right panel of fig.[doublebasin ] appear comparisons of the time evolution of the overlap for two different values of @xmath1 in three different regions of @xmath38 .",
    "the evolutions corresponding to these values of @xmath1 represent the retrieval processes of @xmath47 and @xmath152 associated with the activity levels @xmath125 and @xmath127 , respectively .",
    "note that in the region ( b ) , the patterns with @xmath153 act only as noise when the network is recalling the pattern with @xmath154 . as a whole",
    ", the above findings suggest that the network has a good ability to retrieve patterns with different firing rates .",
    "we should remark that the patterns with activity @xmath125 can be stored more stably rather than those with activity @xmath127 when @xmath155 .",
    "in this section , we study the influence of random synaptic dilution on the model s associative memory capability . for the case of the phase oscillator model , that is , when @xmath110 and @xmath111 , this effect has already been reported @xcite . following the method used in that case to treat random synaptic dilution in our model ,",
    "we assume that the synaptic efficacies take the form    @xmath156    where @xmath130 is the standard hebbian matrix , as defined by eq.([cij ] ) , and , the @xmath157 are independent random variables , taking the values @xmath158 and @xmath35 with probabilities @xmath159 and @xmath160 , respectively .",
    "note that the dilution parameter @xmath159 represents the ratio of connected synapses . in the limit @xmath118 , eq.([cutcij ] ) can be regarded as @xmath161 where the synaptic noise @xmath162 is a complex gaussian noise with mean @xmath35 and variance @xmath163 @xcite . the relationship between the dilution parameter @xmath159 and the variance @xmath164 can be calculated as @xmath165 under the same assumption as that used in obtaining eq.([order ] ) , we can separate the local field into the signal and two noise parts as follows : @xmath166 where @xmath167 and @xmath168 are defined as @xmath169 the new noise term @xmath168 is caused by synaptic dilution , while the term @xmath167 is like the noise defined by eq.([z_i ] ) , representing the crosstalk noise arising from the other embedded patterns . in analogy to our treatment in sec.[onn ] , we assume that @xmath167 and @xmath168 are independent complex gaussian noises with mean @xmath35 and variance @xmath170 and @xmath171 , respectively . therefore , @xmath50 can be regarded as a complex gaussian noise with mean @xmath35 and variance @xmath172 .",
    "thus , we can derive dynamical equations in the same way as in sec.[onn ] .",
    "the form of the macroscopic order parameter @xmath173 is the same as that given by eq.([m ] ) : @xmath174    first , applying the theory of statistical neurodynamics to @xmath168 , we calculate @xmath171 .",
    "however , to apply the theory of statistical neurodynamics , it is necessary to take into account the second term in @xmath175 where @xmath176 . as reported in @xcite , for the asymmetrical case ,",
    "@xmath177 , the second term here can be ignored .",
    "then , as seen from fig.([sym - asym ] ) , there is little difference between symmetric and asymmetric cases in a sparse coding system .",
    "thus we conjecture that this term can be ignored altogether .",
    "doing so , in the @xmath118 limit , we obtain @xmath178 where @xmath94 is defined by eq.([2ndq ] ) .    for the crosstalk noise @xmath167 , in analogy to eq.([z2 ] )",
    ", we obtain @xmath179 where @xmath86 and @xmath87 are defined by eqs.([k ] ) and ( [ g ] ) , respectively .      in the equilibrium state , putting @xmath180 into eq.([zc ] ) , we obtain @xmath181 therefore , @xmath182 , the variance of @xmath62 , takes the form @xmath183 consequently , the properties of the network in the equilibrium state can be calculated from eqs.([mconst ] ) , ( [ sigmaconst ] ) , ( [ gconst ] ) and ( [ qconst ] ) . in fig.[dilution1 ] , we summarize the theoretical results concerning the dependence on the ratio of connected synapses @xmath159 . in figs.[dilution1](a ) and ( b ) , it is found that the maximum storage capacity is an increasing function of the connectivity @xmath159 .",
    "particularly , we can see from ( b ) that , as the activity level @xmath1 becomes small , the storage capacity decreases almost linearly with the connectivity @xmath159 .",
    "a similar linear dependence is observed in the case of the diluted hopfield model . on the other hand , in ( c ) , it is found that even if the ratio of connectivity is small , the maximum storage capacity tends to diverge in the limit @xmath184 .",
    "however , it seems that the rate of this divergence decreases as @xmath159 decreases .      in order to estimate the robustness with respect to synaptic damage",
    ", we should also consider the influence of synaptic dilution on the retrieval process , particularly , on the basin of attraction .",
    "for this purpose , we can apply the same method as used in the derivation of eq.([2ndsigma ] ) .",
    "after some calculations , the resulting recursion equations at second order are given by @xmath185 where @xmath164 is related to the connectivity @xmath159 via eq.([eta ] ) .",
    "as mentioned in sec.[onn ] , eq.([ro2 ] ) is used to calculate eq.([xt2 ] ) . for initial conditions ,",
    "we adopt the values used in sec.[onn ] , except in the present case we use @xmath186 . in the case of @xmath154 and @xmath187 , fig.[dilution2 ]",
    "illustrates the theoretical results concerning the basins of attraction for various values of the connectivity @xmath159 .",
    "we can see that near saturation @xmath188 , the basin of attraction remains large even for low connectivity .",
    "therefore , we find that synaptic dilution has little influence on the width of the basin of attraction , even though the storage capacity decreases with the connectivity .",
    "in this paper , we have presented a simple extended model of oscillator neural networks to allow for the description of the non - firing state .",
    "we have studied the model s associative memory capability for sparsely coded phase patterns , in which some neurons are in the non - firing state and the other neurons encode information in the phase variable representing the timing of neuronal spikes . in particular , applying the theory of statistical neurodynamics , we have evaluated the maximum storage capacity and derived the basin of attraction .",
    "we have found the following properties of our model in its basic form :    * the storage capacity diverges as the activity level decreases to zero .",
    "it was numerically found that the storage capacity diverges proportionally with @xmath189 in the limit @xmath184 .",
    "* even just below the maximum storage capacity , the basin of attraction remains large .",
    "we then investigated the model with regard to the size of the basin of attraction .",
    "we found that with the model in its basic form , the basin of attraction is smaller than those of the binary model and the phase oscillator . for associative memory ,",
    "it is desirable that the basin of attraction be large .",
    "for this reason , we considered employing a dynamically adjusted threshold , and we found the following :    * the basin of attraction can be enlarged by using the dynamically adjusted threshold .    in view of biology",
    ", the neurons may die of age or be injured by accident .",
    "thus , the robustness with respect to synaptic damage is important for real neuronal systems .",
    "for this reason , we also investigated our model with regard to robustness , and we found the following :    * it was found that the system is robust with respect to synaptic damage : even in the case of a high cutting rate , the basin of attraction remains large , and the maximum of the storage capacity diverges in the @xmath184 limit . for low activity patterns ,",
    "the maximum storage capacity decreases almost linearly with the ratio of connected synapse .",
    "the above properties are common with the hopfield model .",
    "in addition , we have found that our model possesses a novel feature not seen in the hopfield model . in realistic situations ,",
    "the activity level of the firing pattern may generally depend on the content of the information . using traditional neural network models based only on firing rate coding , however , we encounter difficulties when the network simultaneously stores such patterns with different activity levels .",
    "contrastingly , with our model , we found the following :    * unlike the hopfield model , provided that the phase distribution in the embedded patterns is uniform , it was shown that patterns with different activity levels can be memorized simultaneously .    in conclusion , from the above findings it is seen that the oscillator neural network exhibits good performance in the sparse coding situation .",
    "therefore , we believe that these results support the plausibility of temporal coding and we hope that they encourage attempts for more detailed explorations of the nature of temporal coding . as a future work ,",
    "we wish to consider one of the applications of the oscillator model . in this paper",
    "we considered coherent oscillations of only a uniform distribution .",
    "it is more interesting , however , to consider systems that are not so strongly constrained .",
    "for example , neurons could be organized into ensembles characterized by different phase values .",
    "in such a situation , each ensemble would carry out a different function independently , and the neural system as a whole could carry out several tasks simultaneously . using this kind of synchrony , we may obtain a simple solution to the binding problem .",
    "we express our gratitude to prof .",
    "t. munakata and dr .",
    "k. kitano for helpful discussions .",
    "this work was supported by the japanese grant - in - aid for science research fund from the ministry of education , science and culture .",
    "99 c.m.gray , p.knig , a.k.engel and w.singer , _ nature _ ( london ) * 338 * , _ 334 _ ( 1989 ) .",
    "w.singer , a.k.engel , a.k.kreiter , m.h.j.munk , s.neuenschwander and p.r.roelfsema , _ trens cogn.sci_ * 1 * , _ 252 _ ( 1997 )",
    ". c.von der malsburg and w.schneider , _ bio.cybernet . _ * 54 * , _ 29 _ ( 1986 ) .",
    "h.sompolinsky , d.golomb and d.kleinfeld , _ phys.rev.a _ * 43 * , _ 6990 _ ( 1991 ) .",
    "d.terman and d.l.wang , _ physica _",
    "( amsterdam ) * 81d * , _ 148 _ ( 1995 ) .",
    "g.b.erementrout and n.kopell , _ siam j.math.anal._ * 15 * , _ 215 _ ( 1984 ) .",
    "y.kuramoto , chemical oscillators , waves , and turbulence .",
    "( springer , new york , 1984 ) .",
    "l.f.abbot , _",
    "j.phys.a _ * 23 * , _ 3835 _ ( 1990 ) .",
    "j.cook , _ j.phys.a _ * 22 * , _ 2057 _ ( 1989 ) . a.j.noest , _ europhys.lett . _",
    "* 6 * , _ 469 _ ( 1988 ) .",
    "t.aoyagi and k.kitano , _ neural computation _ * 10 * , _ 1527 _ ( 1998 ) . k.kitano and t.aoyagi , _ phys.rev.e _ * 57 * , _ 5914 _ ( 1998 ) .",
    "t.aoyagi , _ phys.rev.lett .",
    "_ * 74 * , _ 4075 _ ( 1995 ) .",
    "t.aoyagi and k.kitano , _ phys.rev.e _ * 55 * , _ 7424 _ ( 1997 ) .",
    "d.j.willshaw , o.p.buneman and h.c.longuet-higgins , _ nature _ ( london ) * 222 * , _ 960 _ ( 1969 ) .",
    "m.v.tsodyks and m.v.feigelman , _ europhys.lett . _ * 6 * , _ 101 _ ( 1988 ) . c.j.p.vicente and d.j.amit ,",
    "_ j.phys.a _ * 22 * , _ 559 _ ( 1989 ) .",
    "j.buhmann , r.divko and k.schulten , _ phys.rev.a _ * 39 * , _ 2689 _ ( 1989 ) .",
    "m.okada , _ neural netw . _ * 9 * , _ 1429 _ ( 1996 ) .",
    "e.gardner , _ j.phys.a _ * 21 * , _ 257 _ ( 1988 ) . s.amari and k.maginu , _ neural netw . _ * 1 * , _ 63 _ ( 1998 ) .",
    "m.shiino and t.fukai , _ j.phys.a _ * 25 * , _ l375 _ ( 1992 ) .",
    "m.okada , _ neural netw . _",
    "* 8 * , _ 833 _ ( 1995 ) . a.c.c.coolen and d.sherrington ,",
    "_ phys.rev.lett . _ * 71 * , _ 3886 _ ( 1993 ) .",
    "d.r.c.dominguez and d.boll , _ phys.rev.lett .",
    "_ * 80*,_2961 _ ( 1998 ) .",
    "h.sompolinsky , _ phys.rev.a _ * 34*,_2571 _ ( 1986 )"
  ],
  "abstract_text": [
    "<S> we study a simple extended model of oscillator neural networks capable of storing sparsely coded phase patterns , in which information is encoded both in the mean firing rate and in the timing of spikes . applying the methods of statistical neurodynamics to our model , we theoretically investigate the model s associative memory capability by evaluating its maximum storage capacities and deriving its basins of attraction . </S>",
    "<S> it is shown that , as in the hopfield model , the storage capacity diverges as the activity level decreases . </S>",
    "<S> we consider various practically and theoretically important cases . </S>",
    "<S> for example , it is revealed that a dynamically adjusted threshold mechanism enhances the retrieval ability of the associative memory . </S>",
    "<S> it is also found that , under suitable conditions , the network can recall patterns even in the case that patterns with different activity levels are stored at the same time . in addition , we examine the robustness with respect to damage of the synaptic connections . </S>",
    "<S> the validity of these theoretical results is confirmed by reasonable agreement with numerical simulations . </S>"
  ]
}