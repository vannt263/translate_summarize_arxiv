{
  "article_text": [
    "preprocessing and the analysis of preprocessed data are ubiquitous components of statistical inference , but their treatment has often been informal .",
    "we aim to develop a theory that provides a set of formal statistical principles for such problems under the banner of multiphase inference .",
    "the term `` multiphase '' refers to settings in which inferences are obtained through the application of multiple procedures in sequence , with each procedure taking the output of the previous phase as its input .",
    "this encompasses settings such as multiple imputation ( mi , @xcite ) and extends to other situations . in a multiphase setting , information can be passed between phases in an arbitrary form ; it need not consist of ( independent ) draws from a posterior predictive distribution , as is typical with multiple imputation .",
    "moreover , the analysis procedure for subsequent phases is not constrained to a particular recipe , such as rubin s mi combining rules ( @xcite ) .",
    "the practice of multiphase inference is currently widespread in applied statistics .",
    "it is widely used as an analysis technique within many publications  any paper that uses a `` pipeline '' to obtain its final inputs or clusters estimates from a previous analysis provides an example .",
    "furthermore , projects in astronomy , biology , ecology , and social sciences ( to name a small sampling ) increasingly focus on building databases for future analyses as a primary objective .",
    "these projects must decide what levels of preprocessing to apply to their data and what additional information to provide to their users . providing all of the original data",
    "clearly allows the most flexibility in subsequent analyses . in practice",
    ", the journey from raw data to a complete model is typically too intricate and problematic for the majority of users , who instead choose to use preprocessed output .",
    "unfortunately , decisions made at this stage can be quite treacherous .",
    "preprocessing is typically irreversible , necessitating assumptions about both the observation mechanisms and future analyses .",
    "these assumptions constrain all subsequent analyses .",
    "consequently , improper processing can cause a disproportionate amount of damage to a whole body of statistical results .",
    "however , preprocessing can be a powerful tool .",
    "it alleviates complexity for downstream researchers , allowing them to deal with smaller inputs and ( hopefully ) less intricate models",
    ". this can provide large mental and computational savings .",
    "two examples of such trade - offs come from nasa and high - throughput biology .",
    "when nasa satellites collect readings , the raw data are usually massive .",
    "these raw data are referred to as the `` level 0 '' data ( @xcite ) .",
    "the level 0 data are rarely used directly for scientific analyses .",
    "instead , they are processed to levels 1 , 2 , and 3 , each of which involves a greater degree of reduction and adjustment .",
    "level 2 is typically the point at which the processing becomes irreversible .",
    "@xcite provide an excellent illustration of this process for the atmospheric infrared sounder ( airs ) experiment .",
    "this processing can be quite controversial within the astronomical community .",
    "several upcoming projects , such as the advanced technology solar telescope ( atst ) will not be able to retain the level 0 or level 1 data ( @xcite ) .",
    "this inability to obtain raw data and increased dependence on preprocessing has transformed low - level technical issues of calibration and reduction into a pressing concern .",
    "high - throughput biology faces similar challenges .",
    "whereas reproducibility is much needed ( e.g. , @xcite ) , sharing raw datasets is difficult because of their sizes .",
    "the situation within each analysis is similar . confronted with an overwhelming onslaught of raw data , extensive",
    "preprocessing has become crucial and ubiquitous .",
    "complex models for genomic , proteomic , and transcriptomic data are usually built upon these heavily - processed inputs .",
    "this has made the intricate details of observation models and the corresponding preprocessing steps the groundwork for entire fields .    to many statisticians",
    ", this setting presents something of a conundrum .",
    "after all , the ideal inference and prediction will generally use a complete correctly - specified model encompassing the underlying process of interest and all observation processes . then , why are we interested in multiphase ?",
    "we focus on settings where there is a natural separation of knowledge between analysts , which translates into a separation of effort .",
    "the first analyst(s ) involved in preprocessing often have better knowledge of the observation model than those performing subsequent analyses .",
    "for example , the first analyst may have detailed knowledge of the structure of experimental errors , the equipment used , or the particulars of various protocols .",
    "this knowledge may not be easy to encapsulate for later analysts  the relevant information may be too large or complex , or the methods required to exploit this information in subsequent analyses may be prohibitively intricate . hence , the practical objective in such settings is to enable the best possible inference given the constraints imposed and provide an account of the trade - offs and dangers involved . to borrow the phrasing of @xcite and @xcite",
    ", we aim for achievable practical efficiency rather than theoretical efficiency that is practically unattainable .    multiphase inference currently represents a serious gap between statistical theory and practice .",
    "we typically delineate between the informal work of preprocessing and feature engineering and formal , theoretically - motivated work of estimation , testing , and so forth .",
    "however , the former fundamentally constrains what the latter can accomplish . as a result",
    ", we believe that it represents a great challenge and opportunity to build new statistical foundations to inform statistical practice .",
    "we present two examples that show both the impetus for and perils of undertaking multiphase analyses in place of inference with a complete , joint model .",
    "the first concerns microarrays , which allow the analysis of thousands of genes in parallel .",
    "we focus on expression microarrays , which measure the level of gene expression in populations of cells based upon the concentration of rna from different genes .",
    "these are typically used to study changes in gene expression between different experimental conditions .    in such studies ,",
    "the estimand of interest is typically the log - fold change in gene expression between conditions .",
    "however , the raw data consist only of intensity measurements for each probe on the array , which are grouped by gene along with some form of controls .",
    "these intensities are subject to several forms of observation noise , including additive background variation and additional forms of interprobe and interchip variation ( typically modeled as multiplicative noise ) . to deal with these forms of observation noise , a wide range of background correction and normalization strategies",
    "have been developed ( for a sampling , see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "later analyses then focus on the scientific question of interest without , for the most part , addressing the underlying details of the observation mechanisms .",
    "background correction is a particularly crucial step in this process , as it is typically the point at which the analysis moves from the original intensity scale to the log - transformed scale . as a result",
    ", it can have a large effect on subsequent inferences about log - fold changes , especially for genes with low expression levels in one condition ( @xcite , @xcite ) .",
    "one common method ( mas5 ) , provided by one microarray manufacturer , uses a combination of background subtraction and truncation at a fixed lower threshold for this task ( @xcite ) .",
    "other more sophisticated techniques use explicit probability models for this de - convolution .",
    "a model with normally - distributed background variation and exponentially distributed expression levels has proven to be the most popular in this field ( @xcite , @xcite ) .    unfortunately , even the most sophisticated available techniques pass only point estimates onto downstream analyses .",
    "this necessitates ad - hoc screening and corrections in subsequent analyses , especially when searching for significant changes in expression ( e.g. , @xcite ) . retaining more information from the preprocessing phases of these analyses",
    "would allow for better , simpler inference techniques with greater power and fewer hacks .",
    "the motivation behind the current approach is quite understandable : scientific investigators want to focus on their processes of interest without becoming entangled in the low - level details of observation mechanisms .",
    "nevertheless , this separation can clearly compromise the validity of their results .",
    "the role of preprocessing in microarray studies extends well beyond background correction .",
    "normalization of expression levels across arrays , screening for data corruption , and other transformations preceding formal analysis are standard .",
    "each technique can dramatically affect downstream analyses .",
    "for instance , quantile normalization equates quantiles of expression distributions between arrays , removing a considerable amount of information .",
    "this mutes systematic errors ( @xcite ) , but it can seriously compromise analyses in certain contexts ( e.g. , mirna studies ) .",
    "another example of multiphase inference can be found in the estimation of correlations based upon indirect measurements .",
    "this appears in many fields , but astrophysics provides one recent and striking case .",
    "the relationships between the dust s density , spectral properties , and temperature are of interest in studies of star - forming dust clouds .",
    "these characteristics shed light on the mechanisms underlying star formation and other astronomical processes .",
    "several studies ( e.g. , @xcite , @xcite , @xcite , @xcite ) have investigated these relationships , finding negative correlations between the dust s temperature and spectral index . this finding is counter to previous astrophysical theory , but it has generated many alternative explanations .",
    "such investigations may , however , be chasing a phantasm .",
    "these correlations have been estimated by simply correlating point estimates of the relevant quantities ( temperature @xmath0 and spectral index @xmath1 ) based on a single set of underlying observations . as a result",
    ", they may conflate properties of this estimation procedure with the underlying physical mechanisms of interest .",
    "this has been noted in the field by @xcite , but the scientific debate on this topic continues .",
    "@xcite provide a particularly strong argument , using a cohesive hierarchical bayesian approach , that improper multiphase analyses have been a pervasive issue in this setting .",
    "improper preprocessing led to incorrect , negative estimates of the correlation between temperature and spectral index , according to @xcite .",
    "these incorrect estimates even appeared statistically significant with narrow confidence intervals based on standard methods . on a broader level",
    ", this case again demonstrates some of the dangers of multiphase analyses when they are not carried out properly .",
    "those analyzing this data followed an intuitive strategy : estimate what we want to work with ( @xmath0 and @xmath1 ) , then use it to estimate the relationship of interest .",
    "unfortunately , such intuition is not a recipe for valid statistical inference .",
    "multiphase inference has wide - ranging connections to both the theoretical and applied literatures .",
    "it is intimately related to previous work on multiple imputation and missing data ( @xcite ( @xcite ) , @xcite , @xcite , @xcite ) . in general",
    ", the problem of multiphase inference can be formulated as one of missing data . however , in the multiphase setting , missingness arises from the preprocessing choices made , not a probabilistic response mechanism .",
    "thus , we can leverage the mathematical and computational methods of this literature , but many of its conceptual tools need to be modified .",
    "multiple imputation addresses many of the same issues as multiphase inference and is indeed a special case of the latter .",
    "concepts such as congeniality between imputation and analysis models and self - efficiency ( @xcite ) have natural analogues and roles to play in the analysis of multiphase inference problems .",
    "multiphase inference is also tightly connected to work on the comparison of experiments and approximate sufficiency , going back to @xcite ( @xcite ) and continuing through @xcite and @xcite , among others .",
    "this literature has addressed the relationship between decision properties and the probabilistic structure of experiments , the relationship between different notions of statistical information , and notions of approximate sufficiency  all of these are quite relevant for the study of multiphase inference .",
    "we view the multiphase setting as an extension of this work to address a broader range of real - world problems , as we will discuss in section  [ sec : riskmonotone ] .",
    "the literature on bayesian combinations of experts also informs our thinking on multiphase procedures .",
    "@xcite provides an excellent review of the field , while @xcite provides the core formalisms of interest for the multiphase setting .",
    "overall , this literature has focused on obtaining coherent ( or otherwise favorable ) decision rules when combining information from multiple bayesian agents , in the form of multiple posterior distributions .",
    "we view this as a best - case scenario , focusing our theoretical development towards the mechanics of passing information between phases .",
    "we also focus on the sequential nature of multiphase settings and the challenges this brings for both preprocessors and downstream analysts , in contrast to the more `` parallel '' or simultaneous focus of the literature mentioned above .",
    "there are also fascinating links between multiphase inference and the signal processing literature .",
    "there has been extensive research on the design of quantizers and other compression systems ; see for example @xcite .",
    "such work is often focused on practical questions , but it has also yielded some remarkable theory .",
    "in particular , the work of @xcite on the relationship between surrogate loss functions in quantizer design and @xmath2-divergences suggests possible ways to develop and analyze a wide class of multiphase procedures , as we shall discuss in section  [ sec : future ] .",
    "to formalize the notion of multiphase inference , we begin with a formal model for two - phase settings .",
    "the first phase consists of the data generation , collection , and preprocessing , while the second phase consists of inference using the output from the first phase .",
    "we will call the first - phase agent the `` preprocessor '' and the second - phase agent the `` downstream analyst '' . the preprocessor observes the raw data @xmath3 .",
    "this is a noisy realization of @xmath4 , variables of interest that are not directly obtainable from a given experiment , e.g. , gene expression from sequencing data , or stellar intensity from telescopic observations .",
    "we assume that the joint density of @xmath4 and @xmath3 with respect to product measure @xmath5 can be factored as @xmath6 here , @xmath7 encapsulates the underlying process of interest and @xmath8 encapsulates the observation process .",
    "we assume that @xmath9 is of fixed dimension in all asymptotic settings . in practice ,",
    "the preprocessor should be able to postulate a reasonable `` observation model '' @xmath10 , but will not always know the true `` scientific model '' @xmath11 .",
    "this is analogous to the mi setting , where the imputer does not know the form of the final analysis .     from the original data generating process and outputs @xmath0 , with @xmath4 as missing data .",
    "the downstream analyst observes the preprocessor s output @xmath0 and has both @xmath4 and @xmath3 missing . ]    using this model , the preprocessor provides the downstream analyst with some output @xmath12 , where @xmath13 is a ( possibly stochastic ) additional input .",
    "when @xmath14 is stochastic ( e.g. , an mcmc output ) , the conditional distribution @xmath15 is its theoretical description instead of its functional form .",
    "however , for simplicity , we will present our results when @xmath0 is a deterministic function of @xmath3 only , but many results generalize easily .",
    "given such @xmath0 , downstream analysts can carry out their inference procedures .",
    "figure  [ fig : models ] depicts our general model setup .",
    "this model incorporates several restrictions .",
    "first , it is markovian with respect to @xmath3 , @xmath4 , and @xmath9 ; @xmath3 is conditionally independent of @xmath9 given @xmath4 ( and @xmath16 ) .",
    "second , the parameters governing the observation process ( @xmath16 ) and those governing the scientific process ( @xmath9 ) are distinct . in bayesian settings",
    ", we further assume that @xmath16 and @xmath9 are independent _ a priori_. the parameters @xmath16 are nuisance from the perspective of all involved ; the downstream analyst wants to draw inferences about @xmath4 and @xmath9 , and the preprocessor wants to pass forward information that will be useful for said inferences .",
    "if downstream inferences are bayesian with respect to @xmath16 , then @xmath17 ( which holds under  ( [ e : model ] ) ) is sufficient for all inference under the given model and prior .",
    "hence , this conditional density is frequently of interest in our theoretical development , as is the corresponding marginalized model @xmath18 . we will compare results obtained with a fixed prior to those obtained in a more general setting to better understand the effects of nuisance parameters in multiphase inference .",
    "these restrictions are somewhat similar to those underlying rubin s ( @xcite ) definition of `` missing at random '' ; however , we do not have missing data mechanism ( mdm ) in this setting _ per se_. the distinction between missing and observed data ( @xmath4 and @xmath3 ) is fixed by the structure of our model . in place of mdm , we have two imposed patterns of missingness : one for the data - generating process , and one for the inference process .",
    "the first is @xmath10 , which creates a noisy version of the desired scientific variables . here",
    ", @xmath4 can be considered the missing data and @xmath3 the observed . for the inference process",
    ", the downstream analyst observes @xmath0 in place of @xmath3 but desires inference for @xmath9 based upon @xmath19 .",
    "hence , @xmath3 and @xmath4 are both missing for the downstream analyst .",
    "neither pattern is entirely intrinsic to the problem  both are fixed by choice .",
    "the selection of scientific variables @xmath4 for a given marginal likelihood @xmath20 is a modeling decision .",
    "the selection of preprocessing @xmath21 is a design decision .",
    "this contrasts with the typical missing data setting , where mdm is forced upon the analyst by nature . with multiphase problems ,",
    "we seek to design and evaluate engineered missingness .",
    "thus the investigation of multiphase inference requires tools and ideas from design , inference , and computation in addition to the established theory of missing data .      with this model in place , we turn to formally defining multiphase procedures .",
    "this is more subtle than it initially appears . in the mi setting , we focus on complete - data procedures for the downstream analyst s estimation and do not restrict the dependence structure between missing data and observations . in contrast , we restrict the dependence structure as in ( [ e : model ] ) , but place far fewer constraints on the analysts procedures . here , we focus our definitions and discussion on the two - phase case of a single preprocessor and downstream analyst .",
    "this provides the formal structure to describe the interface between any two phases in a chain of multiphase analyses .    in our multiphase setting ,",
    "downstream analysts need not have any complete - data procedure in the sense of one for inferring @xmath9 from @xmath4 and @xmath3 ; indeed , they need not formally have one based only upon @xmath4 for inferring @xmath9 . we require only that they have a set of procedures for their desired inference using the quantities provided from earlier phases as inputs ( @xmath0 ) , not necessarily using direct observations of @xmath4 or @xmath3 .",
    "such situations are common in practice , as methods are often built around properties of preprocessed data such as smoothness or sparsity that need not hold for the actual values of @xmath4 .    for the preprocessor ,",
    "the input is @xmath3 and the output is @xmath0 . here",
    "@xmath0 could consist of a vector of means with corresponding standard errors , or , for discrete @xmath3 , @xmath0 could consist of carefully selected cross - tabulations . in general , @xmath0 clearly needs to be related to @xmath4 to capture inferential information , but its actual form is influenced by practical constraints ( e.g. , aggregation to lower than desired resolutions due to data storage capacity ) .",
    "for the downstream analyst , the input is @xmath0 and the output is an inference for @xmath22",
    ". this analyst can obviously adapt .",
    "for example , suppose @xmath23 for each entry @xmath24 of @xmath4 .",
    "if the preprocessor provides @xmath25 , the analyst may simply use an unweighted mean to estimate @xmath9 .",
    "if the preprocessor instead gives the analyst @xmath26 , where @xmath27 contains standard errors , the latter could instead use a weighted mean to estimate @xmath9 .",
    "this adaptation extends to an arbitrary number of possible inputs @xmath28 , each of which corresponds to a set of constraints facing the preprocessor .    to formalize this notion of adaptation",
    ", we first define an index set @xmath29 with one entry for each such set of constraints .",
    "this maps between forms of input provided by the preprocessor and estimators selected by the downstream analyst . in this way",
    ", @xmath29 captures the downstream analyst s knowledge of previous processing and the underlying probability model .",
    "thus , this index set plays an central role in the definition of multiphase inference problems , far beyond that of a mere mathematical formality ; it regulates the amount of mutual knowledge shared between the preprocessor and the downstream analyst .",
    "now , we turn to the estimators themselves .",
    "we start with point estimation as a foundation for a broader class of problems .",
    "testing begins with estimating rejection regions , interval estimation with estimating coverage , classification with estimating class membership , and prediction with estimating future observations and , frequently , intermediate parameters .",
    "the framework we present therefore provides tools that can be adapted for more than estimation theory .",
    "we define multiphase estimation procedures as follows :    a _ multiphase estimation procedure _ @xmath30 is a set of estimators @xmath31 indexed by the set @xmath29 , where @xmath28 corresponds to the output of the @xmath32th first - phase method ; that is , @xmath30 is a family of estimators with different inputs .",
    "when clear , we will drop the subscripts @xmath32 and index the estimators in @xmath30 by their inputs .",
    "this definition provides enough flexibility to capture many practical issues with multiphase inference , and it can be iterated to define procedures for analyses involving a longer sequence of preprocessors and analysts .",
    "it also encompasses the definition of a missing data procedure used by @xcite .",
    "such procedures can not , of course , be arbitrarily constructed if they are to deliver results with general validity .",
    "hence , having defined these procedures , we will cull many of them from consideration in section  [ sec : riskmonotone ] .",
    "the obvious choice of our estimand , suggested by our notation thus far , is the parameter for the scientific model , @xmath9 .",
    "this is very amenable to mathematical analysis and relevant to many investigations .",
    "hence , it forms the basis for our results in section  [ sec : theory ] .",
    "however , for multiphase analyses , other classes of estimands may prove more useful in practice .",
    "in particular , functions of @xmath4 , future scientific variables @xmath33 , or future observations @xmath34 may be of interest .",
    "prediction of such quantities is a natural focus in the multiphase setting because such statements are meaningful to both the preprocessor and downstream analyst .",
    "such estimands naturally encompass a broad range of statistical problems including prediction , classification , and clustering .",
    "however , there is often a lack of mutual knowledge about @xmath35 , so the preprocessor can not expect to `` target '' estimation of @xmath9 in general , as we shall discuss in section  [ sec : remarks ] .",
    "it is not automatic for multiphase estimation procedures to produce better results as the first phase provides more information . to obtain a sensible context for theoretical development",
    ", we must regulate the way that the downstream analyst adapts to different inputs .",
    "for instance , they should obtain better results ( in some sense ) when provided with higher - resolution information .",
    "this carries over from the mi setting ( @xcite , @xcite , @xcite , @xcite ) , where notions such as self - efficiency are useful for regulating the downstream analyst s procedures .",
    "we define a similar property for multiphase estimation procedures , but without restricting ourselves to the missing data setting .",
    "specifically , let @xmath36 indicate @xmath37 is a deterministic function of @xmath38 . in practice , @xmath37 could be a subvector , aggregation , or other summary  of  @xmath38 .    a multiphase estimation procedure @xmath30 is _ risk monotone _ with respect to a loss function @xmath39",
    "if , for all pairs of outputs @xmath40 , @xmath36 implies @xmath41 .",
    "an asymptotic analogue of risk monotonicity is defined as would be expected , scaling the relevant risks at an appropriate rate to obtain nontrivial limits .",
    "this is a natural starting point for regulating multiphase estimation procedures ; stronger notions may be required for certain theoretical results .",
    "note that this definition does not require that `` higher - quality '' inputs necessarily lead to lower risk estimators .",
    "risk monotonicity requires only that estimators based upon a larger set of inputs perform no worse than those with strictly less information ( in a deterministic sense ) .",
    "however , risk monotonicity is actually quite tight in another sense .",
    "it requires that additional information can not be misused by the downstream analyst , imposing a strong constraint on mutual knowledge . for an example , consider the case of unweighted and weighted means . to obtain better results when presented with standard errors , the downstream analyst must know that they are being given ( the correct ) standard errors and to weight by inverse variances .",
    "this definition is related to the comparison of experiments , as explored by @xcite ( @xcite ) , but diverges on a fundamental level .",
    "our ordering of experiments , based on deterministic functions , is more stringent than that of @xcite , but they are related . indeed , our @xmath42 relation implies that of @xcite . in the latter work , an experiment @xmath43 is defined as more informative than experiment @xmath1 , denoted @xmath44 , if all losses attainable from @xmath1 are also attainable from @xmath43 .",
    "this relation is also implied when @xmath43 is sufficient for @xmath1 .",
    "our stringency stems from our broader objectives in the multiphase setting . from a decision - theoretic perspective , the partial ordering of experiments investigated by blackwell and others deal with which risks are attainable given pairs of experiments , allowing for arbitrary decision procedures .",
    "in contrast , our criterion restricts procedures based on whether such risks are actually attained , with respect to a particular loss function .",
    "this is because , in the multiphase setting , it is not generally realistic to expect downstream analysts to be capable of obtaining optimal estimators for all forms of preprocessing .    the conceptually - simplest way to generate such",
    "a procedure is to begin with a complete probability model for @xmath45 . under traditional asymptotic regimes ,",
    "all procedures consisting of bayes estimators based upon such a model will ( with full knowledge of the transformations involved in each @xmath28 and a fixed prior ) be risk monotone . the same is true asymptotically under the same regimes ( for squared - error loss ) for procedures consisting of mles under a fixed model .",
    "under some other asymptotic regimes , however , these principles of estimation do not guarantee risk - monotonicity ; we explore this further in section  [ sec : missinfo ] .",
    "but such techniques are not the only way to generate risk monotone procedures from probability models .",
    "this is analogous to self - efficiency , which can be achieved by procedures that are neither bayesian nor mle ( @xcite , @xcite ) .     and @xmath38 form the basis set of statistics .",
    "each of these has three descendants ( @xmath46 from @xmath37 and @xmath47 from  @xmath38 ) .",
    "these descendants are deterministic functions of their parent , but they are not deterministic functions of any other basis statistics . given correctly - specified models for @xmath37 and  @xmath38 , a risk monotone procedure can be constructed for all statistics ( @xmath48 ) shown here as described in the text . ]",
    "a risk monotone procedure can be generated from any set of probability models for distinct inputs that `` span '' the space of possible inputs .",
    "suppose that an analyst has a set of probability models , all correctly specified , for @xmath49 , where @xmath50 ranges over a subset @xmath51 of the relevant index set @xmath29 .",
    "we also assume that this analyst has a prior distribution @xmath52 for each such basis models .",
    "these priors need not agree between models ; the analyst can build a risk - monotone procedure from an inconsistent set of prior beliefs .",
    "suppose that the inputs @xmath53 are not deterministic functions of each other and all other inputs can be generated as nontrivial deterministic transformations of one of these inputs .",
    "formally , we require @xmath54 for all distinct @xmath55 and , for each @xmath56 there exists a unique @xmath57 such that @xmath58 ( each output is uniquely descended from a single @xmath59 ) , as illustrated in figure  [ fig : risk - monotone ] .",
    "this set can form a basis , in a sense , for the given procedure .    using the given probability models with a single loss function and set of priors ( potentially different for each model ) ,",
    "the analyst can derive a bayes rule under each model .",
    "for each @xmath57 , we require @xmath60 to be an appropriate bayes rule on said model .",
    "as @xmath61 for some function @xmath62 , we then have the implied @xmath63 , yielding the bayes rule for estimating @xmath9 based on @xmath28 , which is no less risky than @xmath60 . the requirement that each output @xmath28 derives from a unique @xmath59 means that each basis component @xmath59 has a unique line of descendants .",
    "within each line , each descendant is comparable to only a single @xmath59 in the sense of deterministic dependence . between these lines ,",
    "such comparisons are not possible .",
    "this ensures the overall risk - monotonicity .",
    "biology provides an illustration of such bases . a wide array of methodological approaches",
    "have been used to analyze high - throughput gene expression data .",
    "one approach , builds upon order and rank statistics ( @xcite , @xcite , @xcite ) .",
    "another common approach uses differences in gene expression between conditions or experiments , often aggregating over pathways , replicates , and so forth .",
    "each class of methods is based upon a different form of preprocessing : ranks transformations for the former , normalization and aggregation for the latter .",
    "taking procedures based on rank statistics and aggregate differences in expression as a basis , we can consider constructing a risk - monotone procedure as above .",
    "thus , the given formulation can bring together apparently disparate methods as a first step in analyzing their multiphase properties .",
    "such constructions are , unfortunately , not sufficient to generate all possible risk monotone procedures .",
    "obtaining more general conditions and constructions for risk monotone procedures is a topic for further work .      by casting the examples in section  [ sec : examples ] into the formal structure just established",
    ", we can clarify the practical role of each mathematical component and see how to map theoretical results into applied guidance .",
    "we also provide an example that illustrates the boundaries of the framework s utility , and another that demonstrates its formal limits .",
    "these provide perspective on the trade - offs made in formalizing the multiphase inference problem .",
    "the case of microarray preprocessing presented previously fits quite nicely into the model of section  [ sec : model ] .",
    "there , @xmath3 corresponds to the observed probe - level intensities , @xmath4  corresponds to the true expression level for each gene under each condition , and @xmath9 corresponds to the parameters governing the organism s patterns of gene expression . in the microarray",
    "setting , @xmath8 would characterize the relationship between expression levels and observed intensities , governed by  @xmath16 .",
    "these nuisance parameters could include chip - level offsets , properties of any additive background , and the magnitudes of other sources of variation .",
    "the assumptions of a markovian dependence structure and distinct parameters for each part of the model appear quite reasonable in this case , as ( 1 ) the observation @xmath3 can only ( physically ) depend upon the sample preparation , experimental protocol , and rna concentrations in the sample and ( 2 ) the distributions @xmath7 and @xmath8 capture physically distinct portions of the experiment .",
    "background correction , normalization , and the reduction of observations to log - fold changes are common examples of preprocessing @xmath21 . as discussed previously",
    ", estimands based upon @xmath4 may be of greater scientific interest than those based upon @xmath9 .",
    "for instance , we may want to know whether gene expression changed between two treatments in a particular experiment ( a statement about  @xmath4 ) than whether a parameter regulating the overall patterns of gene expression takes on a particular value .    for the astrophysical example",
    ", the fit is similarly tidy .",
    "the raw astronomical observations correspond to @xmath3 , the true temperature , density , and spectral properties of each part of the dust cloud become @xmath4 , and the parameters governing the relationship between these quantities ( e.g. , their correlation ) form @xmath9 .",
    "the @xmath8 distribution governs the physical observation process , controlled by  @xmath16 .",
    "this process typically includes the instruments response to astronomical signals , atmospheric distortions , and other earthbound phenomena .",
    "as before , the conditional independence of @xmath9 and @xmath3 given @xmath4 and @xmath16 is sensible based upon the problem structure , as is the separation of @xmath9 and @xmath16 . here",
    "@xmath4 corresponds to signals emitted billions or trillions of miles from earth , whereas the observation process occurs within ground- or space - based telescopes .",
    "hence , any non - markovian effects are quite implausible .",
    "preprocessing @xmath21 corresponds to the ( point ) estimates of temperature , density , and spectral properties from simple models of @xmath3 given @xmath4 and  @xmath16 .",
    "the multiphase framework encompasses a broad range of settings , but it does not shed additional light on all of them . if @xmath0 is a many - to - one transformation of @xmath3 , then our framework implies that the preprocessor and downstream analyst face structurally different inference ( and missing data ) problems .",
    "this is the essence of multiphase inference , in our view .",
    "settings where @xmath64 is degenerate or @xmath0 is a one - to - one function of @xmath3 are boundary cases where our multiphase interpretation and framework add little .    for a concrete example of these cases , consider a time - to - failure experiment , with the times of failure @xmath65 , @xmath66 . now",
    ", suppose that the experimenters actually ran the experiment in @xmath67 equally - sized batches .",
    "they observe each batch only until its first failure ; that is , they observe and report @xmath68 for each batch @xmath50 .",
    "subsequent analysts have access only to @xmath69 .",
    "this seems to be a case of preprocessing , but it actually resides at the very edge of our framework .",
    "we could take the complete observations to be @xmath4 and the batch minima to be @xmath3 .",
    "this would satisfy our markov constraint , with a singular , and hence deterministic , observation process @xmath70 simply selecting a particular order statistic within each batch .",
    "however , @xmath21 is one - to - one ; the preprocessor observes only the order statistics , as does the downstream analyst .",
    "there is no separation of inference between phases ; the same quantities are observed and missing to both the preprocessor and the downstream analyst .",
    "squeezing this case into the multiphase framework is technically valid but unproductive .",
    "the framework we present is not , however , completely generic .",
    "consider a chemical experiment involving a set of reactions .",
    "the underlying parameters @xmath9 describe the chemical properties driving the reactions , @xmath4 are the actual states of the reaction , and @xmath3 are the ( indirectly ) measured outputs of the reactions .",
    "the measurement process for these experiments , as described by @xmath64 , could easily violate the structure of our model in this case .",
    "for instance , the same chemical parameters could affect both the measurement and reaction processes , violating the assumed separation of @xmath9 and @xmath16 .",
    "even careful preprocessing in such a setting can create a fundamental incoherence .",
    "suppose the downstream analysis will be bayesian , so the preprocessor provides the conditional density of @xmath3 as a function of @xmath4 , latexmath:[${p_y}(y    @xmath16 share components , and the preprocessor uses their prior on @xmath16 to create @xmath70 , the conditional density need not be sufficient for @xmath9 under the downstream analyst s model .",
    "because the downstream analyst s prior on @xmath9 need not be compatible with the preprocessor s prior on  @xmath16 , inferences based on the preprocessor s @xmath70 can be seriously flawed in this setting .",
    "hence , we exclude such cases from our investigation for the time being .",
    "thinking bayesianly , our model ( [ e : model ] ) obviously does not exclude the possibility that the downstream analyst has more knowledge about @xmath9 than the preprocessor in the form of a prior on @xmath22 .",
    "however , _",
    "prior _ information means that it is based on studies that do not overlap with the current one .",
    "probabilistically speaking , this means that our model permits the downstream analyst to formally incorporate another data set @xmath72 , as long as @xmath72 is conditionally independent of the scientific variables @xmath4 and observations @xmath3 given @xmath73 or  @xmath9 .",
    "for example , the downstream analyst could observe completely separate experiments pertaining to the same underlying process governed by @xmath9 or the outcomes of separate calibration pertaining to @xmath16 , but not additional replicates governed by the same realization of @xmath4 . in a biological setting , this means that the downstream analyst could have access to results from samples not available to the preprocessor ( e.g. , biological replicates ) , possibly using the same equipment ; however , they could not have access to additional analyses of the same biological sample ( e.g. , technical replicates ) , as a single biological sample would typically correspond to a single realization of @xmath4 .",
    "these examples remind us that our multiphase setting does not encompass all of statistical inference .",
    "this is quite a relief to us .",
    "our work aims to open new directions for statistical research , but it can not possibly address every problem under the sun !      multiphase theory hinges on procedural constraints .",
    "consider , for example , finding the optimal multiphase estimation procedure in terms of the final estimator s bayes risk .",
    "without stringent procedural constraints , the result is trivial : compute the appropriate bayes estimator using the distribution of @xmath0 given @xmath9 .",
    "similarly , the optimal preprocessing @xmath0 will , without tight constraints , simply compute an optimal estimator using @xmath3 and pass it forward . note that both of these cases respect risk - monotonicity to the letter ; it is not sufficiently tight to enable interesting , relevant theory .",
    "more constraints , based upon careful consideration of applied problems , are clearly required .",
    "this is not altogether bad news .",
    "we need only look to the history of multiple imputation to see how rich theory can arise from stringent , pragmatic constraints .",
    "multiple imputation forms a narrow subset of multiphase procedures : @xmath4 corresponds to the complete data ( @xmath74 , in mi notation ) , @xmath3 corresponds to the observed data @xmath75 and missing data indicator @xmath76 , and @xmath0 usually consists of posterior predictive draws of the missing data together with the observed data .",
    "the markovian property depicted in figure  [ fig : models ] holds when the parameter ( @xmath77 ) for the missing data mechanism @xmath78 ) is distinct from the parameter of interest ( @xmath22 ) in @xmath79 , which is a common assumption in practice . the second - phase procedure",
    "is then restricted to repeatedly applying a complete - data procedure and combining the results .",
    "these constraints were originally imposed for practical reasons  in particular , to make the resulting procedure feasible with existing software .",
    "however , they have opened the door to deep theoretical investigations .    in that spirit",
    ", we consider two types of practically - motivated constraints for multiphase inference : restrictions on the downstream analyst s procedure and restrictions on the preprocessor s methods . these constraints are intended to work in concert with coherence conditions ( e.g. , risk monotonicity ) , not in isolation , to enable meaningful theory .",
    "constraints on the downstream analyst are intended to reflect practical limitations of their analytic capacity .",
    "examples include restricting the downstream analyst to narrow classes of estimators ( e.g. , linear functions of preprocessed inputs ) , to specific principles of estimation ( e.g. , mles ) , or to special cases of a method we can reasonably assume the downstream analyst could handle , such as a complete - data estimator @xmath80 , available from software with appropriate inputs .",
    "estimators derived from nested families of models are often suitable for this purpose .",
    "for example , whereas @xmath80 may involve only an ordinary regression , the computation of @xmath81 may require a weighted least - squares regression.=1 another constraint on the downstream analyst pertains to nuisance parameters .",
    "such constraints are of great practical and theoretical interest , as we believe that the preprocessor will typically have better knowledge and statistical resources available to address nuisance parameters than the downstream analyst .",
    "an extreme but realistic case of this is to assume that the downstream analyst can not address nuisance parameters at all . as we shall discuss in section  [ sec : theory ] , this would force the preprocessor to either marginalize over the nuisance parameters , find a pivot with respect to them , or trust the downstream analyst to use a method robust to the problematic parameters .",
    "turning to the preprocessor , we consider restricting either the form of the preprocessor s output or the mechanics of their methods . in the simplest case of the former",
    ", we could require that @xmath0 consist of the posterior mean ( @xmath82 ) and posterior covariance ( @xmath83 ) of the unknown @xmath4 under the preprocessor s model .",
    "a richer , but still realistic , class of output would be finite - dimensional real or integer vectors .",
    "restricting output to such a class would prevent the preprocessor from passing arbitrary functions onto the downstream analyst .",
    "this leads naturally to the investigation of ( finite - dimensional ) approximations to the preprocessor s conditional density , aggregation , and other such techniques .    on the mechanical side",
    ", we can restrict either the particulars of the preprocessor s methods or their broader properties .",
    "examples of the former include particular computational approximations to the likelihood function or restrictions to particular principles of inference ( e.g. , summaries of the likelihood or posterior distribution of @xmath4 given @xmath84 ) .",
    "such can focus our inquiries to specific , feasible methods of interest or reflect the core statistical principles we believe the preprocessor should take into account . in a different vein , we can require that preprocessor s procedures be distributable across multiple researchers , each with their own experiments and scientific variables of interest .",
    "such settings are of interest for both the accumulation of scientific results for later use and for the development of distributed statistical computation .",
    "this leads to preprocessing based upon factored `` working '' models for @xmath4 , as we explore further in section  [ sec : sufficiency ] .",
    "nuisance parameters play an important role in these constraints , narrowing the class of feasible methods ( e.g. , marginalization over such parameters may be exceedingly difficult ) and largely determining the extent to which preprocessing can be distributed .",
    "we explore these issues in more detail throughout section  [ sec : theory ] .",
    "we now present a few steps towards a theory of multiphase inference . in this , we endeavor to address three basic questions : ( 1 ) how can we determine what to retain , ( 2 ) what limits the performance of multiphase procedures , and ( 3 ) what are some minimal requirements for being an ideal preprocessor ?",
    "we find insight into the first question from the language of classical sufficiency .",
    "we leverage and specialize results from the missing - data literature to address the second . for the third question , we turn to the tools of decision theory .",
    "suppose we have a group of researchers , each with their own experiments .",
    "they want to preprocess their data to reduce storage requirements , ease subsequent analyses , and ( potentially ) provide robustness to measurement errors .",
    "this group is keenly aware of the perils of preprocessing and want to ensure that the output they provide will be maximally useful for later analyses .",
    "their question is , `` which statistics should we retain ? ''    if each of these researchers was conducting the final analysis themselves , using only their own data , they would be in a single - phase setting .",
    "the optimal strategy then is to keep a minimal sufficient statistic for each researcher s model .",
    "similarly , if the final analysis were planned and agreed upon among all researchers , we would again have a single - phase setting , and it is optimal to retain the sufficient statistics for the agreed - upon model .",
    "we use the term _ optimal _ here because it achieves maximal data reduction without losing information about the parameters of interest . such lossless compression  in the general sense of avoiding statistical redundancy  is often impractical , but it provides a useful theoretical gold standard . in the multiphase setting , especially with multiple researchers in the first phase , achieving optimal preprocessing is far more complicated even in theory .",
    "if @xmath21 is the output of the _ entire _ preprocessing phase , then in order to retain all information we must require @xmath21 to be a sufficient statistics for @xmath85 under model ( [ e : model ] ) ; that is , @xmath86 where @xmath39 denotes a likelihood function ; or at least in the ( marginal ) bayesian sense , @xmath87 where @xmath88 is the posterior of @xmath22 given data @xmath89 with the likelihood given by ( [ e : model ] ) .",
    "note that ( [ e : con1 ] ) implies ( [ e : con2 ] ) , and ( [ e : con2 ] ) is useful when the downstream analyst wants only a bayesian inference of @xmath22 .",
    "in either case the construction of the sufficient statistic generally depends on the joint model for @xmath3 as implied by ( [ e : model ] ) , requiring more knowledge than individual researchers typically possess .",
    "often , however , it is reasonable to assume the following conditional independence .",
    "let @xmath90 be the specification of @xmath91 for researcher @xmath92 , where @xmath93 forms a _ partition _ of @xmath3 .",
    "we then assume that @xmath94 note in the above definition implicitly we also assume the baseline measure @xmath95 is a product measure @xmath96 , such as lebesgue measure . the assumption ( [ e : obsm ] ) holds , for example , in microarray applications , when different labs provide conditionally - independent observations of probe - level intensities .",
    "the preceding discussion suggests that this assumption is necessary for ensuring ( [ e : con1 ] ) or even ( [ e : con2 ] ) , but obviously it is far from sufficient because it says nothing about the model on @xmath4 .",
    "it is reasonable  or at least more logical than not  to assume each researcher has the best knowledge to specify his / her own observation model @xmath97 ( @xmath98 .",
    "but , for the scientific model @xmath99 used by the downstream analyst , the best we can hope is that each researcher has _ a working model _ @xmath100",
    "that is in some way related to @xmath99 .",
    "the notation @xmath101 reflects our hope to construct a common working parameter @xmath102 that can ultimately be _ linked _ to the scientific parameter @xmath22 .",
    "given this working model , the @xmath24th researcher can obtain the corresponding ( minimal ) sufficient statistic @xmath103 for @xmath104 with respect to @xmath105 when one has a prior @xmath106 for @xmath107 , one could alternately decide to retain the ( bayesian ) sufficient statistic @xmath108 with respect to the model @xmath109    our central interest here is to determine when the collection @xmath110 will satisfy ( [ e : con1 ] ) and when @xmath111 will satisfy ( [ e : con2 ] ) .",
    "this turns out to be an exceedingly difficult problem if we seek a necessary and sufficient condition for _ when _ this occurs .",
    "however , it is not difficult to identify sufficient conditions that can provide useful practical guidelines .",
    "we proceed by first considering cases where @xmath112 forms a partition of @xmath4 .",
    "compared to the assumption on partitioning @xmath3 , this assumption is less likely to hold in practice because different researchers can share common parts of @xmath4 s or even the entire scientific variable @xmath4 . however , as we shall demonstrate shortly , we can extend our results formally to all models for @xmath4 , as long as we are willing to put tight restrictions on the allowed class of working models .",
    "specifically , the following condition describes a class of working models that are ideal because they permit separate preprocessing yet retain joint information .",
    "note again that an implicit assumption here is that the baseline measure @xmath113 is a product measure @xmath114 .",
    "[ ( distributed separability condition ( dsc ) ) ] a set of working models @xmath115 is said to satisfy the _ distributed separability condition _ with respect to @xmath116 if there exists a probability measure @xmath117 such that @xmath118\\,{\\mathrm{d}}p_{\\eta}({\\eta}| { \\theta } ) .",
    "\\label{eq : dsc}\\ ] ]    [ thm : dsc ] under the assumptions ( [ e : obsm ] ) and ( [ eq : dsc ] ) , we have    the collection of individual sufficient statistics from ( [ e : prob ] ) , that is , @xmath119 , is jointly sufficient for @xmath85 in the sense that ( [ e : con1 ] ) holds .    under the additional assumption that @xmath120 forms a partition of @xmath77 and @xmath121 , both @xmath21 corresponding to ( [ e : prob ] ) and @xmath122 corresponding to ( [ e : prom ] ) are bayesianly sufficient for @xmath22 in the sense that ( [ e : con2 ] ) holds .",
    "by the sufficiency of @xmath123 for @xmath124 , we can write @xmath125 this implies that , @xmath126 } & = & \\int _ { x } \\biggl [ \\prod_{i=1}^{r } { p_y}(y_i | x_i , { \\xi}_i ) \\biggr ] \\\\ & & { } \\times\\biggl [ \\int_{\\eta } \\biggl [ \\prod _ { i=1}^{r } { \\tilde{p}_x}\\bigl(x_i | g_i({\\eta})\\bigr ) \\biggr]\\,{\\mathrm{d}}p_{\\eta}({\\eta}| { \\theta } ) \\biggr]\\,{\\mathrm{d}}\\mu _ x(x ) , \\\\ { [ \\mbox{by factorization of }   \\mu_x ] } & = & \\int _ { \\eta } \\prod_{i=1}^{r } \\biggl [ \\int_{x_i } { p_y}(y_i | x_i , { \\xi}_i ) { \\tilde{p}_x}\\bigl(x_i | g_i ( { \\eta})\\bigr)\\,{\\mathrm{d}}\\mu_{x_i}(x_i ) \\biggr]\\,{\\mathrm{d}}p_{\\eta}({\\eta}| { \\theta } ) , \\\\ { \\bigl[\\mbox{by } ( \\ref{eq : a1})\\bigr ] } & = & \\biggl [ \\prod _ { i=1}^{r } h_i(y_i ) \\biggr ] \\biggl [ \\int_{\\eta } \\prod_{i=1}^{r } f_i\\bigl(t_i;g_i({\\eta } ) , { \\xi}_i\\bigr)\\,{\\mathrm{d}}p_{\\eta}({\\eta}| { \\theta } ) \\biggr ] .\\end{aligned}\\ ] ] this establishes ( 1 ) by the factorization theorem . assertion ( 2 ) is easily established via an analogous argument , by integrating all the expressions above with respect to @xmath127 .    we emphasize that dsc does not require individual researchers to model their parts of @xmath4 in the same way as the downstream analyst would , which would make it an essentially tautological condition .",
    "rather , it requires that individual researchers understand their own problems and how they can fit into the broader analysis hierarchically .",
    "this means that the working model for each @xmath128 @xmath129 can be more saturated than the downstream analyst s model for the same part of @xmath4 .",
    "consider a simple case with @xmath130 , where the preprocessor correctly assumes the multivariate normality for @xmath4 but is unaware that its covariance actually has a block structure or is unwilling to impose such a restriction to allow for more flexible downstream analyses .",
    "clearly any sufficient statistic under the unstructured multivariate model is also sufficient for any ( nested ) structured ones .",
    "the price paid here is failing to achieve the greatest possible sufficient reduction of the data , but this sacrifice may be necessary to ensure the broader validity of downstream analyses .",
    "for example , even if downstream analysts adopt a block - structured covariance , they may still want to perform a model checking , which would not be possible if all they are given is a _",
    "minimal _ sufficient statistic for the model to be checked .",
    "knowledge suitable for specifying a saturated model is more attainable than complete knowledge of @xmath116 , although ensuring common knowledge of its ( potential ) hierarchical structure still requires some coordination among the researchers .",
    "each of them could independently determine for which classes of scientific models their working model satisfies the dsc .",
    "however , without knowledge of the partition of @xmath4 across researchers and the overarching model(s ) of interest , their evaluations need not provide any useful consensus .",
    "this suggests the necessity of some general communications and a practical guideline for distributed preprocessing , even when we have chosen a wise division of labors that permits dsc to hold .",
    "formally , dsc is similar in flavor to de finetti s theorem , but it does not require the components of the factorized working model to be exchangeable .",
    "dsc , however , is by no means necessary ( even under ( [ e : con1 ] ) ) , as an example in section  [ sec : counterexamples ] will demonstrate .",
    "its limits stem from `` unparameterized '' dependence  dependence between @xmath128 s that is not controlled by @xmath9 .",
    "when such dependence is present , statistics can exist that are sufficient for both @xmath131 and @xmath9 without the working model satisfying dsc .",
    "however , a simple necessary condition for distributed sufficiency is available .",
    "unsurprisingly , it links the joint sufficiency of @xmath132 under @xmath133 to the joint sufficiency of @xmath134 under the scientific model @xmath99 , where @xmath135 is any sufficient statistic for the working model @xmath136 .",
    "[ thm : necessary ] if , for all observation models satisfying ( [ e : obsm ] ) , the collections of individual sufficient statistics from ( [ e : prob ] ) @xmath119 are jointly sufficient for @xmath137 in the sense that ( [ e : con1 ] ) holds , then any collection of individual sufficient statistics under @xmath138 , that is , @xmath139 , must be sufficient for @xmath22 under @xmath140 .",
    "the proof of this condition emerges easily by considering the trivial observation model @xmath141 , where @xmath142 is the indicator function of set @xmath143 .",
    "theorem  [ thm : necessary ] holds even if we require the observation model to be nontrivial , as the case of @xmath144\\}}$ ] for arbitrary @xmath145-neighborhoods of @xmath128 demonstrates .",
    "the result says that if we want distributed preprocessing to provide a lossless compression regardless of the actual form of the observation model , then even under the conditional independence assumption ( [ e : obsm ] ) , we must require the individual working models to _ collectively _ preserve sufficiency under the scientific model .",
    "note that preserving sufficiency for a model is a much weaker requirement than preserving the model itself .",
    "indeed , two models can have very different model spaces yet share the same _ form _ of sufficient statistics , as seen with i.i.d . @xmath146 and @xmath147 models , both yielding the sample average as a complete sufficient statistic .",
    "although we find this sufficiency - preserving condition quite informative about the limits of lossless distributed preprocessing , it is not a sufficient condition . as a counterexample , consider @xmath148 independent for @xmath149 , @xmath150 , where @xmath151 .",
    "for the true model , we assume @xmath152 as follows : @xmath153 , @xmath154 , and all variables are mutually independent .",
    "for the working model , we take @xmath155 as follows : @xmath156 independently , and @xmath157 with probability 1 for all @xmath158 . obviously @xmath159 is a sufficient statistic for both @xmath155 and @xmath99 because of their normality .",
    "because @xmath27 is _ minimally _ sufficient for @xmath102 , this implies that any sufficient statistic for @xmath155 must be sufficient for @xmath99 , therefore the sufficiency preserving condition holds .    however , the collection of the complete sufficient statistics @xmath160 for @xmath102 under @xmath161 is not sufficient for @xmath22 under @xmath162 because the latter is no longer an exponential family .",
    "the trouble is caused by the failure of the working models to capture additional flexibility in the scientific model that is not controlled by its parameter @xmath22 .",
    "therefore , obtaining a condition that is both necessary and sufficient for lossless compression via distributed preprocessing is a challenging task . such a condition appears substantially more intricate than those presented in theorems  [ thm : dsc ] and  [ thm : necessary ] and may therefore be less useful as an applied guideline .",
    "below we discuss a few further subtleties .",
    "although theorem  [ thm : dsc ] covers both likelihood and bayesian cases , it is important to note a subtle distinction between their general implications . in the likelihood setting ( [ e : con1 ] ) , we achieve lossless compression for all downstream analyses targeting @xmath73 .",
    "this allows the downstream analyst to obtain inferences that are robust to the preprocessor s beliefs about @xmath16 , and they are free to revise their inferences if new information about @xmath16 becomes available .",
    "but , the downstream analyst must address the nuisance parameter @xmath77 from the preprocessing step , a task a downstream analyst may not be able or willing to handle .    in contrast , the downstream analyst need not worry about @xmath77 in the bayesian setting ( [ e : con2 ] ) .",
    "however , this is achieved at the cost of robustness .",
    "all downstream analyses are potentially affected by the preprocessors beliefs about @xmath77 .",
    "furthermore , because @xmath122 is required only to be sufficient for @xmath22 , it may not carry any information for a downstream analyst to check the preprocessor s assumptions about @xmath16 .",
    "fortunately , as it is generally logical to expect the preprocessor to have better knowledge addressing @xmath77 than the downstream analyst , such robustness may not be a serious concern from a practical perspective .",
    "theoretically , the trade - off between robustness and convenience is not clear - cut ; they can coincide for other types of preprocessing , as seen in section  [ sec : missinfo ] below .      as discussed earlier , ( conditional ) dependencies among the observation variables",
    "@xmath163 across different @xmath24 s will generally rule out the possibility of achieving lossless compression by collecting individual sufficient statistics .",
    "this points to the importance of appropriate separation of labors when designing distributed preprocessing .",
    "in contrast , dependencies among @xmath128 s are permitted , at the expense of redundancy in sufficient statistics .",
    "we first consider deterministic dependencies , and for simplicity , take @xmath164 and constrain attention to the case of sufficiency for @xmath9 .",
    "suppose we have @xmath165 and @xmath166 forming a partition of @xmath4 , with a working model @xmath167 that satisfied the dsc for some @xmath168 .",
    "imagine we need to add a common variable @xmath72 to both @xmath165 and @xmath166 that is conditionally independent of @xmath169 given @xmath22 and has density @xmath170 , with the remaining model unchanged .",
    "however , the two researchers are unaware of the sharing of  @xmath72 , so they set up @xmath171 and @xmath172 , with @xmath173 does not correspond to the scientific variable @xmath174 of interest .",
    "however , we notice that if we can force @xmath175 in @xmath176 , then we can recover @xmath4 .",
    "this forcing is not a mere mathematical trick .",
    "rather , it reflects an extreme yet practical strategy when researchers are unsure whether they share some components of their @xmath177 with others . the strategy is simply to retain statistics sufficient for the entire part that they may _ suspect _ to be common , which in this case means that both researchers will retain statistics sufficient for the @xmath178s @xmath179 in their entirety .",
    "mathematically , this corresponds to letting @xmath180 , where @xmath181 .",
    "it is then easy to verify that dsc holds , if we take @xmath182 , where @xmath183 .",
    "this is because when @xmath184 , both sides of ( [ eq : dsc ] ) are zero .",
    "when @xmath175 , we have ( adopting integration over @xmath185 functions ) @xmath186\\,{\\mathrm{d}}p_{\\eta } ' \\bigl ( { \\eta } ' | { \\theta}\\bigr ) \\\\ & & \\quad= \\int_\\eta \\int_{\\zeta _ 1 } \\biggl [ \\prod_{i=1}^2 \\tilde p_{x_i}(x_i |\\eta_i ) \\delta_{\\ { z=\\zeta_i\\ } } \\biggr]\\,{\\mathrm{d}}p_{\\eta } ( { \\eta}| { \\theta } ) \\delta_{\\{\\zeta_1=\\zeta_2\\}}\\,{\\mathrm{d}}p_{z}(\\zeta_1|\\theta ) \\\\ & & \\quad = \\biggl [ \\int_\\eta\\prod_{i=1}^2 \\tilde p_{x_i}(x_i |\\eta_i)\\,{\\mathrm{d}}p_{\\eta } ( { \\eta}| { \\theta } ) \\biggr ] \\int_{\\zeta_1 } \\delta_{\\{\\zeta_1=z\\}}\\,{\\mathrm{d}}p_{z}(\\zeta_1|\\theta ) \\\\ & & \\quad = p_{x_{}}(x_1 , x_2|\\theta)p_{z}(z| \\theta)=p_{x}(x|\\theta).\\end{aligned}\\ ] ] this technique of expanding @xmath102 to include shared parts of the @xmath4 allows the dsc and theorem  [ thm : dsc ] to be applied to all models @xmath140 , not only those with with distinct @xmath128 s .",
    "however , this construction also restricts working models to those with deterministic relationships between parts of @xmath102 and each @xmath128 .",
    "the derivation above demonstrates both the broader applications of dsc as a theoretical condition and its restrictive nature as a practical guideline . retaining sufficient statistics for both @xmath187 and @xmath188 can create redundancy .",
    "if each preprocessor observes @xmath72 without noise , then only one of them actually needs to retain and report their observation of @xmath72 .",
    "however , if each observes @xmath72 with independent noise , then both of their observations are required to obtain a sufficient statistic for  @xmath22 .",
    "the noise - free case also provides a straightforward counterexample to the necessity of dsc . assuming both preprocessors observe @xmath72 directly , as long as one of the copies of @xmath72 is retained via the use of the saturated @xmath185 density",
    ", the other copy can be modeled in any way  and hence can be made to violate dsc  without affecting their joint sufficiency for  @xmath22 .",
    "regardless of the dependencies among the @xmath128 s , there is always a safe option open to the preprocessors for data reduction : retain @xmath123 sufficient for @xmath189 under @xmath190 .",
    "this will preserve sufficiency for @xmath9 under any scientific model @xmath116 :    [ thm : safe ] if @xmath191 is correctly specified and satisfies ( [ e : obsm ] ) , then any collection of individual sufficient statistics @xmath192 with each @xmath123 sufficient for @xmath189 is jointly sufficient for @xmath73 in the sense of ( [ e : con1 ] ) for all @xmath116 .    by the factorization theorem",
    ", we have @xmath193 for any @xmath24 .",
    "hence , by ( [ e : obsm ] ) , @xmath194 \\int_x   [ \\prod_{i=1}^r p_t(t_i | x_i , { \\xi}_i )   ] { p_x}(x    @xmath195 is sufficient for @xmath22 , by the factorization theorem for sufficiency .",
    "theorem  [ thm : safe ] provides a universal , safe strategy for sufficient preprocessing and a lower bound on the compression attainable from distributed sufficient preprocessing . as all minimal sufficient statistics for @xmath22 are functions of any sufficient statistic for @xmath196 , retaining minimal sufficient statistics for each @xmath189 results in less compression than any approach properly using knowledge of @xmath140",
    ". however , the compression achieved relative to retaining @xmath3 itself may still be significant .",
    "minimal sufficient statistics for @xmath22 provide an upper bound on the attainable degree of compression by the same argument . achieving this compression generally requires that each preprocessor knows the true scientific model @xmath140 . between these bounds ,",
    "the dsc ( [ eq : dsc ] ) shows a trade - off between the generality of preprocessing ( with respect to different scientific models ) and the compression achieved : the smaller the set of scientific models for which a given working model satisfies ( [ eq : dsc ] ) , the greater the potential compression from its sufficient statistics .      more generally , stochastic dependence among @xmath128 s reduces compression and increases redundancy in distributed preprocessing .",
    "these costs are particularly acute when elements of @xmath9 control dependence among @xmath128 s , as seen in the following example where @xmath197 here @xmath198 is a column vector with @xmath199 @xmath200 s as its components , and @xmath201 is the usual kronecker product .",
    "if @xmath202 is known , then each researcher can reduce their observations @xmath163 to a scalar statistic @xmath203 and preserve sufficiency for @xmath204 .",
    "if @xmath202 is unknown , then each researcher must retain all of @xmath205 ( but not @xmath206 for @xmath207 ) in addition to these sums to ensure sufficiency for @xmath208 , because the minimal sufficient statistic for @xmath209 requires the computation of @xmath210 .",
    "thus , the cost of dependence here is @xmath89 additional pieces of information per preprocessor .",
    "dependence among the @xmath128 s forces the preprocessors to retain enough information to properly combine their individual contributions in the final analysis , downweighting redundant information .",
    "this is true even if they are interested only in efficient estimation of @xmath204 , leading to less reduction of their raw data and less compression from preprocessing than the independent case .      from this investigation",
    ", we see that it is generally not enough for each researcher involved in preprocessing to reduce data based on even a correctly - specified model for their problem at hand .",
    "we instead need to look to other models that include each experimenter s data hierarchically , explicitly considering higher - level structure and relationships . however , significant reductions of the data are still possible despite these limitations .",
    "each @xmath123 need not be sufficient for each  @xmath128 , nor must @xmath0 be sufficient for @xmath4 overall .",
    "this often implies that much less data need to be retained and shared than retaining sufficient statistics for each @xmath128 would demand .",
    "for instance , if a working model with @xmath211 satisfies the dsc for a given model @xmath116 and @xmath212 , then only means and covariance matrices of @xmath206 within each experiment @xmath24 need to be retained .",
    "the discussions above demonstrate the importance of involving downstream analysts in the design of preprocessing techniques .",
    "their knowledge of @xmath116 is extremely useful in determining what compression is appropriate , even if said knowledge is imperfect . constraining the scientific model to a broad class may be enough to guarantee effective preprocessing . for example , suppose we fix a working model and consider all scientific models that can be expressed as  ( [ eq : dsc ] ) by varying the choices of @xmath117 .",
    "this yields a very broad class of hierarchical scientific models for downstream analysts to evaluate , while permitting effective distributed preprocessing based on the given working model.=1    practically , we see two paths to distributed preprocessing : coordination and caution .",
    "coordination refers to the downstream analyst evaluating and guiding the design of preprocessing as needed .",
    "such guidance can guarantee that preprocessed outputs will be as compact and useful as possible .",
    "however , it is not always feasible",
    ". it may be possible to specify preprocessing in detail in some industrial and purely computational settings .",
    "accomplishing the same in academic research or for any research conducted over time is an impractical goal . without such overall coordination",
    ", caution is needed .",
    "it is not generally possible to maintain sufficiency for @xmath9 without knowledge of the possible models @xmath116 unless the retained summaries are sufficient for @xmath4 itself .",
    "preprocessors should therefore proceed cautiously , carefully considering which scientific models they effectively exclude through their preprocessing choices .",
    "this is analogous to the oft - repeated guidance to include as many covariates and interactions as possible in imputation models ( @xcite , @xcite ) .",
    "having considered the lossless preprocessing , we now turn to more realistic but less clear - cut situations .",
    "we consider a less careful preprocessor and a sophisticated downstream analyst .",
    "the preprocessor selects an output @xmath0 , which may discard much information in @xmath3 but nevertheless preserves the identifiability of @xmath22 , and the downstream analyst knows enough to make the best of whatever output they are given .",
    "that is , the index set @xmath29 completely and accurately captures all relevant preprocessing methods @xmath213 .",
    "this does not completely capture all the practical constraints discussed in section  [ sec : concepts ] .",
    "however , it is important to establish an upper bound on the performance of multiphase procedures before incorporating such issues .",
    "this upper bound is on the fisher information , and hence a lower bound on the asymptotic variances of estimators @xmath214 of @xmath9 .",
    "as we will see , nuisance parameters ( @xmath16 ) play a crucial role in these investigations .",
    "when using a lossy compression , an obvious question is how much information is lost compared to a lossless compression .",
    "this question has a standard asymptotic answer when the downstream analyst adopts an mle or bayes estimator , so long as nuisance parameters behave appropriately ( as will be discussed shortly ) .",
    "if the downstream analyst adopts some other procedures , such as an estimating equation , then there is no guarantee that the procedure based on @xmath3 is more efficient than the one based on @xmath0 .",
    "that is , one can actually obtain a more efficient estimator with less data when one is not using _ probabilistically principled _ methods , as discussed in detail in @xcite .",
    "therefore , as a first step in our theoretical investigations , we will focus on mles ; the results also apply to bayesian estimators under the usual regularity conditions to guarantee the asymptotic equivalence between mles and bayesian estimators .",
    "specifically , let @xmath215 and @xmath216 be the mles of @xmath217 based respectively on @xmath3 and @xmath0 under model ( [ e : model ] ) .",
    "we place standard regularity conditions for the joint likelihood of @xmath73 , assuming bounded third derivatives of the log - likelihood , common supports of the observation distributions with respect to @xmath73 , full rank for all information matrices at the true parameter value @xmath218 , and the existence of an open subset of the parameter space that contains @xmath218 .",
    "these conditions imply the first and second bartlett identities .    however , the most crucial assumption here is a sufficient accumulation of information , indexed by an _ information size _",
    "@xmath219 , to constrain the behavior of remainder terms in quadratic approximations of the relevant score functions .",
    "independent identically distributed observations and fixed - dimensional parameters would satisfy this requirement , in which case @xmath219 is simply the data size of @xmath3 , but weaker conditions can suffice ( for an overview , see @xcite ) . in general , this assumption requires that the dimension of both @xmath220 and @xmath77 are bounded as we accumulate more data , preventing the type of phenomenon revealed in @xcite . for multiphase inferences , cases where these dimensions are unbounded are common ( at least in theory ) and represent interesting settings where preprocessing can actually improve asymptotic efficiency , as we discuss shortly .    to eliminate the nuisance parameter @xmath77 , we work with the observed fisher information matrices based on the profile likelihoods for @xmath9 , denoted by @xmath221 and @xmath222 respectively .",
    "let @xmath223 be the limit of @xmath224 , the so - called _ fraction of missing information _ ( see @xcite ) , as @xmath225 .",
    "the proof of the following result follows the standard asymptotic arguments for mles , with the small twist of applying them to profile likelihoods instead of full likelihoods .",
    "( we can also invoke the more general arguments based on decomposing estimating equations , as given in @xcite . )",
    "[ thm : missinfo ] under the conditions given above , we have asymptotically as @xmath225 , @xmath226^{-1 } \\rightarrow f\\ ] ] and @xmath227^{-1 } \\rightarrow i - f.\\ ] ]    this establishes the central role of the fraction of missing information @xmath223 in determining the asymptotic efficiency of multiphase procedures under the usual asymptotic regime . as mentioned above , this is an ideal - case bound on the performance of multiphase procedures , and it is based on the usual squared - error loss ; both the asymptotic regime and amount of knowledge held by the downstream analyst are optimistic .",
    "we explore these issues below , focusing on ( 1 ) mutual knowledge and alternative definitions of efficiency , ( 2 ) the role of reparameterization , ( 3 ) asymptotic regimes and multiphase efficiency , and ( 4 ) the issue of robustness in multiphase inference .      in practice , downstream analysts are unlikely to have complete knowledge of @xmath8 .",
    "therefore , even if they were given the entire @xmath3 , they would not be able to produce the optimal estimator @xmath228 , making the @xmath223 value given by theorem  [ thm : missinfo ] an unrealistic yardstick .",
    "nevertheless , theorem  [ thm : missinfo ] suggests a direction for a more realistic standard .",
    "the classical theory of estimation focuses on losses of the form @xmath229 , where @xmath230 denotes the truth .",
    "risk based on this type of loss , given by @xmath231 $ ] , is a raw measure of performance , using the truth as a baseline .",
    "an alternative is regret , the difference between the risk of a given estimator and an ideal estimator @xmath232 ; that is , @xmath233 .",
    "regret is popular in the learning theory community and forms the basis for oracle inequalities .",
    "it provides a more adaptive baseline for comparison than raw risk , but we can push further . consider evaluating loss with respect to an estimator rather than the truth . for mean - squared error ,",
    "this yields @xmath234 .\\ ] ] can this provide a better baseline , and what are its properties ?    for mles , @xmath235 behaves the same ( asymptotically ) as additive regret because theorem  [ thm : missinfo ] implies that , as @xmath225 under the classical asymptotic regime , @xmath236 \\\\[-8pt ] \\nonumber & = & r\\bigl({\\hat{{\\theta}}}(t ) , { \\theta}_0\\bigr)-r\\bigl({\\hat{{\\theta}}}(y ) , { \\theta}_0\\bigr ) .\\end{aligned}\\ ] ] for inefficient estimators , ( [ eq : same ] ) does not hold in general because @xmath237 is no longer guaranteed to be asymptotically uncorrelated with @xmath238 . in such cases , this is precisely the reason @xmath81 can be more efficient than @xmath238 or , more generally , there exists a constant @xmath239 such that @xmath240 is ( asymptotically ) more efficient than @xmath238 . in the terminology of @xcite , the estimation procedure",
    "@xmath241 is not _ self - efficient _ if ( [ eq : same ] ) does not hold , viewing @xmath3 as the complete data @xmath242 and @xmath0 as the observed data  @xmath243 . indeed ,",
    "if @xmath244 , @xmath245 may actually be _ larger _ for a _ better _",
    "@xmath81 because of the inappropriate baseline @xmath238 ; it is a measure of difference , not dominance , in such cases .",
    "hence , some care is needed in interpreting this measure .",
    "therefore , we can view ( [ eq : risk ] ) as a generalization of the usual notion of regret , or the relative regret if we divide it by @xmath246 .",
    "this generalization is appealing for the study of preprocessing : we are evaluating the estimator based on preprocessed data directly against what could be done with the complete raw data , sample by sample , and we no longer need to impose the restriction that the downstream analysts must carry out the most efficient estimation under a model that captures the actual preprocessing .",
    "this direction is closely related to the idea of strong efficiency from @xcite and @xcite , which generalizes the idea of asymptotic decorrelation beyond the simple ( but instructive ) setting covered here .",
    "such ideas from the theory of missing data provide a strong underpinning for the study of multiphase inference and preprocessing .",
    "theorem  [ thm : missinfo ] also emphasizes the range of effects that preprocessing can have , even in ideal cases .",
    "consider the role that @xmath223 plays under different transformations of @xmath9 .",
    "although the eigenvalues of @xmath223 are invariant under one - to - one transformations of the parameters , submatrices of @xmath223 can change substantially .",
    "formally , if @xmath208 is transformed to @xmath247 , then the fraction of missing information for @xmath248 can be very different from that for @xmath204 .",
    "these changes mean that changes in parameterization can reallocate the fractions of missing information among resulting subparameters in unexpected  and sometimes very unpleasant  ways .",
    "this is true even for linear transformations ; a given preprocessing technique can preserve efficiency for @xmath204 and @xmath202 individually while performing poorly for @xmath249 .",
    "such issues have arisen in , for instance , the work of @xcite when attempting to characterize the behavior of multiple imputation estimators under uncongeniality .      on a fundamental level ,",
    "theorem  [ thm : missinfo ] is a negative result for preprocessing , at least for mles .",
    "reducing the data from @xmath3 to @xmath0 can only hinder the downstream analyst .",
    "formally , this means that @xmath250 ( asymptotically ) in the sense that @xmath251 is positive semi - definite . as a result , @xmath238 will dominate @xmath81 in asymptotic variance for any preprocessing @xmath0 .",
    "thus , the only justification for preprocessing appears to be pragmatic ; if the downstream analyst could not make use of @xmath8 for efficient inference or such knowledge could not be effectively transmitted , preprocessing provides a feasible way to obtain the inferences of interest .",
    "however , this conclusion depends crucially on the assumed behavior of the nuisance parameter @xmath16 .",
    "the usual asymptotic regime is not realistic for many multiphase settings , particularly with regards to @xmath16 . in many problems of interest",
    ", @xmath252 does not tend to zero as @xmath219 increases , preventing sufficient accumulation of information on the nuisance parameter @xmath77 .",
    "a typical regime of this type would accumulate observations @xmath163 from individual experiments @xmath24 , each of which brings its own nuisance parameter @xmath253 .",
    "such a process could describe the accumulation of data from microarrays , for instance , with each experiment corresponding to a chip with its own observation parameters , or the growth of astronomical datasets with time - varying calibration .",
    "in such a regime , preprocessing can have much more dramatic effects on asymptotic efficiency .    in the presence of nuisance parameters , inference based on @xmath0",
    "can be more robust and even more efficient than inference based on @xmath3 .",
    "it is well - known that the mle can be inefficient and even inconsistent in regimes where @xmath254 ( going back to at least @xcite ) .",
    "bayesian methods provide no panacea either .",
    "marginalization over the nuisance parameter @xmath16 is appealing , but resulting inferences are typically sensitive to the prior on @xmath16 , even asymptotically . in many cases ( such as the canonical neyman  scott problem ) , only a minimal set of priors",
    "provide even consistent bayes estimators .",
    "careful preprocessing can , however , enable principled inference in such regimes .",
    "such phenomena stand in stark contrast to the theory of multiple imputation .",
    "in that theory , complete data inferences are typically assumed to be valid .",
    "thus , under traditional missing data mechanisms , the observed data ( corresponding to  @xmath0 ) can not provide better inferences than @xmath3 .",
    "this is not necessarily true in multiphase settings .",
    "if the downstream analyst is constrained to particular principles of inference ( e.g. , mle or bayes ) , then estimators based on @xmath0 can provide lower asymptotic variance than those based on  @xmath3 .",
    "this occurs , in part , because the mechanisms generating @xmath3 and @xmath0 from @xmath4 are less restricted in the multiphase setting compared to the traditional missing - data framework .",
    "principled inferences based on @xmath4 would , in the multiphase setting , generally dominate those based on either @xmath3 or @xmath0 .",
    "however , such a relationship need not hold between @xmath3 and @xmath0 without restrictions on the behavior of @xmath16 .",
    "we emphasize that this does not contradict the general call in @xcite to follow the probabilistically - principled methods ( such as mle and bayes recipes ) to prevent violations of self - efficiency , precisely because the well - established principles of single - phase inference may need to be `` re - principled '' before they can be equally effective in the far more complicated multiphase setting .      in the simplest case ,",
    "if a @xmath0 can be found such that it is a pivot with respect to @xmath16 and remains dependent upon @xmath9 , then sensitivity to the behavior of @xmath77 can be eliminated by preprocessing . in such cases ,",
    "an mle or bayes rule based on @xmath0 can dominate that based on @xmath3 even asymptotically .",
    "one such example would be providing @xmath255-statistics from each of a set of experiments to the downstream analyst .",
    "this clearly limits the range of feasible downstream inferences . with these @xmath255-statistics , detection of signals via multiple",
    "testing ( e.g. , @xcite ) would be straightforward , but efficient combination of information across experiments could be difficult .",
    "this is a ubiquitous trade - off of preprocessing : reductions that remove nuisance parameters and improve robustness necessarily reduce the amount of information available from the data .",
    "these trade - offs must be considered carefully when designing preprocessing techniques ",
    "universal utility is unattainable without the original data .",
    "a more subtle case involves the selection of @xmath0 as a `` partial pivot '' . in some settings ,",
    "there exists a decomposition of @xmath16 as @xmath256 such that @xmath257 for some fixed @xmath89 and all @xmath219 , and the distribution of @xmath0 is free of @xmath258 for all values of @xmath259 .",
    "many normalization techniques used in the microarray application of section  [ sec : examples ] can be interpreted in this light .",
    "these methods attempt to reduce the unbounded set of experiment - specific nuisance parameters affecting @xmath0 to a bounded , manageable size .",
    "for example , suppose each processor @xmath24 observes @xmath260 , @xmath261 .",
    "the downstream analyst wants to estimate @xmath262 , considering @xmath263 and @xmath264 as nuisance parameters . in our previous notation",
    ", we have @xmath265 and @xmath266 .",
    "suppose each preprocessor reduces her data to @xmath267 , where @xmath268 is the ols estimator of @xmath269 based on @xmath270 .",
    "the distribution of each @xmath123 depends on @xmath264 but is free of @xmath269 .",
    "hence , @xmath271 is a partial pivot as defined above , with @xmath272 and @xmath273 .",
    "such pivoting techniques can allow @xmath81 to possess favorable properties even when @xmath238 is inconsistent or grossly inefficient .",
    "as mentioned before , this kind of careful preprocessing can dominate bayesian procedures in the presence of nuisance parameters when @xmath274 can grow with @xmath275 . in these regimes , informative priors on @xmath16 can affect inferences even asymptotically .",
    "however , reducing @xmath3 to @xmath0 so only the @xmath276-part of @xmath16 is relevant for @xmath0 s distribution allows information to accumulate on @xmath259 , making inferences far more robust to the preprocessor s beliefs about @xmath16 .",
    "these techniques share a common conceptual framework : invariance .",
    "invariance has a rich history in the bayesian literature , primarily as a motivation for the construction of noninformative or reference priors ( e.g. , @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "it is fundamental to the pivotal methods discussed above and arises in the theory of partial likelihood ( @xcite ) .",
    "we see invariance as a core principle of preprocessing , although its application is somewhat different from most bayesian settings .",
    "we are interested in finding functions of the data whose distributions are invariant to subsets of the parameter , not priors invariant to reparameterization .",
    "for instance , the rank statistics that form the basis for cox s proportional hazards regression in the absence of censoring ( @xcite ) can be obtained by requiring a statistic invariant to monotone transformations of time .",
    "indeed , cox s regression based on rank statistics can be viewed as an excellent example of eliminating an infinite dimensional nuisance parameter , i.e. , the baseline hazard , via preprocessing , which retains only the rank statistics .",
    "the relationship between invariance in preprocessing , modeling , and prior formulation is a rich direction for further investigation .",
    "an interesting practical question arises from this discussion of robustness : how realistic is it to assume efficient inference with preprocessed data ?",
    "this may seem unrealistic as preprocessing is frequently used to simplify problems so common methods can be applied .",
    "however , preprocessing can make many assumptions more appropriate .",
    "for example , aggregation can make normality assumptions more realistic , normalization can eliminate nuisance parameters , and discretization greatly reduces reliance on parametric distributional assumptions altogether",
    ". it may therefore be more appropriate to assume that efficient estimators are generally used with preprocessed data than with raw data .",
    "the results and examples explored here show that preprocessing is a complex topic in even large - sample settings .",
    "it appears formally futile ( but practically useful ) in standard asymptotic regimes . under other realistic asymptotic regimes , preprocessing emerges as a powerful tool for addressing nuisance parameters and improving the robustness of inferences .",
    "having established some of the formal motivation and trade - offs for preprocessing , we discuss further extensions of these ideas into more difficult settings in section  [ sec : future ] .      in some cases , effective preprocessing techniques are quite apparent .",
    "if @xmath10 forms an exponential family with parameter @xmath4 or @xmath196 , then we have a straightforward procedure : retain a minimal sufficient statistic . to be precise , we mean that one of the following factorizations holds for a sufficient statistic @xmath21 of bounded dimension : @xmath277 retaining this sufficient statistic will lead to a lossless compression , assuming that the first - phase model is correct",
    ". unfortunately , such nice cases are rare .",
    "even the bayesian approach offers little reprieve . integrating @xmath10 with respect to a prior @xmath278",
    "typically removes the observation model from the exponential family  consider , for instance , a normal model with unknown variance becoming a @xmath279 distribution .    if @xmath280 is approximately quadratic as a function of @xmath4 ,",
    "then retaining its mode and curvature would seem to provide much of the information available from the data to downstream analysts",
    ". however , such intuition can be treacherous .",
    "if a downstream analyst is combining inferences from a set of experiments , each of which yielded an approximately quadratic likelihood , the individual approximations may not be enough to provide efficient inferences .",
    "approximations that hold near the mode of each experiment s likelihood need not hold away from these modes  including at the mode of the joint likelihood from all experiments . thus , remainder terms can accumulate in the combination of such approximations , degrading the final inference on @xmath9 .",
    "furthermore , the requirement that @xmath280 be approximately quadratic in @xmath4 is quite stringent . to justify such approximations",
    ", we must either appeal to asymptotic results from likelihood theory or confine our attention to a narrow class of observation models @xmath281 .",
    "unfortunately , asymptotic theory is often an inappropriate justification in multiphase settings , because @xmath4 grows in dimension with @xmath3 in many asymptotic regimes of interest , so there is no general reason to expect information to accumulate on @xmath4 .",
    "these issues are of particular concern as such quadratic approximations are a standard implicit justification for passing point estimates with standard errors onto downstream analysts .",
    "moving away from these cases , solutions become less apparent .",
    "no processing ( short of passing the entire likelihood function ) will preserve all information from the sample when sufficient statistics of bounded dimension do not exist .",
    "however , multiphase approaches can still possess favorable properties in such settings .",
    "we begin by considering a stubborn downstream analyst ",
    "she has her method and will not consider anything else .",
    "for example , this analyst could be dead set on using linear discriminant analysis or anova .",
    "the preprocessor has only one way to affect her results : carefully designing a particular @xmath0 given to the downstream analyst .",
    "such a setting is extreme .",
    "we are saying that the downstream analyst will charge ahead with a given estimator regardless of her input with neither reflection nor judgment .",
    "we investigate this setting because it maximizes the preprocessor s burden in terms of her contribution to the final estimate s quality .",
    "formally , we consider a fixed second - stage estimator @xmath81 ; that is , the form of its input @xmath0 and the function producing @xmath214 are fixed , but the mechanism actually used to generate @xmath0 is not",
    ". @xmath0 could be , for example , a vector of fixed dimension .",
    "as we discuss below , admissible designs for the first - phase with a fixed second - phase method are given by a ( generalized ) bayes rule .",
    "this uses the known portion of the model @xmath10 to construct inputs for the second stage and assumes that any prior the preprocessor has on @xmath16 is equivalent to what a downstream analyst would have used in the preprocessor s position .",
    "formally , this describes all rules that are admissible among the class of procedures using a given second - stage method , following from previous complete class results in statistical decision theory ( e.g. , @xcite , @xcite ) .",
    "assume that the second - stage procedure @xmath81 is fixed as discussed above and we are operating under the model ( [ e : model ] ) .",
    "further assume that the preprocessor s prior on @xmath16 is the only such prior used in all bayes rule constructions .",
    "for @xmath282 , consider a smooth , strictly convex loss function  @xmath39 . then",
    ", under appropriate regularity conditions ( e.g. , @xcite , @xcite ) , if @xmath81 is a smooth function of @xmath0 , then all admissible procedures for generating @xmath0 are bayes or generalized bayes rules with respect to the risk @xmath283 .",
    "the same holds when @xmath0 is restricted to a finite set .",
    "this guideline follows directly from conventional complete class results in decision theory .",
    "we omit technical details here , focusing instead on the guideline s implications . however , a sketch of its proof proceeds along the following lines .",
    "there are two ways to approach this argument : intermediate loss and geometry .",
    "the intermediate loss approach uses an intermediate loss function @xmath284 .",
    "this @xmath285 is the loss facing the preprocessor given a fixed downstream procedure @xmath81 . if @xmath285 is well - behaved , in the sense of satisfying standard conditions ( strict convexity , or a finite parameter space , and so on ) , then the proof is complete from previous results for real @xmath0 .",
    "similarly , if @xmath0 is restricted to a finite discrete set , then we face a classical multiple decision problem and can apply previous results to @xmath286 .",
    "these straightforward arguments cover a wide range of realistic cases , as @xcite has shown .",
    "otherwise , we must turn to a more intricate geometric argument .",
    "broadly , this construction uses a convex hull of risks generated by attainable rules .",
    "this guideline has direct bearing upon the development of inputs for machine learning algorithms , typically known as _",
    "feature engineering_. given an algorithm that uses a fixed set of inputs , it implies that using a correctly - specified observation model to design these inputs is necessary to obtain admissible inferences .",
    "thus , it is conceptually similar to `` rao - blackwellization '' over part of a probability model .",
    "however , several major caveats apply to this result .",
    "first , on a practical level , deriving such bayes rules is quite difficult for most settings of interest .",
    "second , and more worryingly , this result s scope is actually quite limited . as we discussed in section",
    "[ sec : missinfo ] , even bayesian estimators can be inconsistent in realistic multiphase regimes .",
    "however , these estimators are still admissible , as they can not be dominated in risk for particular values of the nuisance parameters @xmath77 .",
    "admissibility therefore is a minimal requirement ; without it , the procedure can be improved uniformly , but with it , it can still behave badly in many ways . finally , there is the problem of robustness .",
    "an optimal input for one downstream estimator @xmath287 may be a terrible input for another estimator @xmath288 , even if @xmath289 and @xmath290 take the same form of inputs .",
    "such considerations are central to many real - world applications of preprocessing , as researchers aim to construct databases for a broad array of later analyses .",
    "however , this result does show that engineering inputs for downstream analyses using bayesian observation models can improve overall inferences .",
    "how to best go about this in practice is a rich area for further work .      as befits first steps , we are left with a few loose ends and puzzles .",
    "starting with the dsc condition ( [ eq : dsc ] ) of section  [ sec : sufficiency ] , we provide a simple counterexample to its necessity .",
    "suppose we have @xmath291 .",
    "let @xmath292 independent of each other .",
    "now , let @xmath293 , @xmath294 , @xmath295 , where @xmath296 , @xmath297 , @xmath298 is a vector of signs @xmath299 , or @xmath300 for @xmath165 , @xmath301 denotes the element - wise absolute value , and @xmath302 denotes the hadamard product .",
    "we fix @xmath303 .    as our working model , we posit that @xmath304 independently .",
    "then , we clearly have @xmath305 as a sufficient statistic for both @xmath131 and @xmath9 .",
    "however , the dsc does not hold for this working model .",
    "we can not write the actual joint distribution of @xmath4 as a marginalization of @xmath306 with respect to some distribution over @xmath131 in such a way that @xmath307 is sufficient for @xmath131 . to enforce @xmath308 under the working model , any such model must use @xmath131 to share this information .    for this example , we can obtain a stronger result : no factored working model @xmath306 exists such that ( 1 ) @xmath309 is sufficient for @xmath310 under @xmath311 and ( 2 ) the dsc holds . for contradiction ,",
    "assume such a working model exists . under this working model , @xmath163 is conditionally independent of @xmath101 given @xmath309 , so we can write @xmath312 . as the dsc holds for this working model , we have @xmath313 \\int_{\\eta } \\biggl [ \\prod _ { i=1}^2 h_i\\bigl(y_i^\\top y_i ; g_i(\\eta)\\bigr ) \\biggr ] p_{\\eta}({\\mathrm{d}}{\\eta}| { \\theta } ) .\\ ] ] hence , we must have @xmath314 conditionally independent of @xmath315 given @xmath316 .",
    "however , this conditional independence does not hold under the true model .",
    "hence , the given working model can not both satisfy the dsc and have @xmath309 sufficient for each @xmath101 .",
    "the issue here is unparameterized dependence , as mentioned in section  [ sec : sufficiency ] .",
    "the @xmath4 s have a dependence structure that is not captured by @xmath9 .",
    "thus , requiring that a working model preserves sufficiency for @xmath9 does not ensure that it has enough flexibility to capture the true distribution of @xmath3 . a weaker condition than the dsc ( [ eq : dsc ] ) that is necessary and sufficient to ensure that all sufficient statistics for @xmath131 are sufficient for @xmath9 may be possible .    from sections  [ sec : missinfo ] and",
    "[ sec : completeclass ] , we are left with puzzles rather than counterexamples . as mentioned previously ,",
    "many optimality results are trivial without sufficient constraints .",
    "for instance , minimizing risk or maximizing fisher information naively yield uninteresting ( and impractical ) multiphase strategies : have the preprocessor compute optimal estimators , then pass them downstream .",
    "overly tight constraints bring their own issues . restricting downstream procedures to excessively narrow classes ( e.g. , point estimates with standard errors ) limits the applied utility of resulting theory and yields little insight on the overall landscape of multiphase inference .",
    "striking the correct balance with these constraints is a core challenge for the theory of multiphase inference and will require a combination of computational , engineering , and statistical insights .",
    "as we discussed in sections  [ sec : concepts ] and  [ sec : theory ] , we have a deep well of questions that motivate further research on multiphase inference . these range from",
    "the extremely applied ( e.g. , enhancing preprocessing in astrophysical systems ) to the deeply theoretical ( e.g. , bounding the performance of multiphase procedures in the presence of nuisance parameters and computational constraints ) .",
    "we outline a few directions for this research below .",
    "but , before we look forward , we take a moment to look back and place multiphase inference within the context of broader historical debates .",
    "such `` navel gazing '' helps us to understand the connections and implications of the theory of multiphase inference .      on a historical note ,",
    "the study of multiphase inference touches the long - running debate over the role of decision theory in statistics .",
    "one side of this debate , championed by wald and lehmann ( among others ) , has argued that decision theory lies at the core of statistical inference .",
    "risk - minimizing estimators and , more generally , optimal decision rules play a central role in their narrative .",
    "even subjectivists such as savage and de finetti have embraced the decision theoretic formulation to a large extent .",
    "other eminent statisticians have objected to such a focus on decisions .",
    "as noted by @xcite , fisher in particular vehemently rejected the decision theoretic formulation of statistical inference .",
    "one interpretation of fisher s objections is that he considered decision theory useful for eventual economic decision - making , but not for the growth of scientific knowledge .",
    "we believe that the study of multiphase inference brings a unifying perspective to this debate .",
    "fisher s distinction between intermediate processing and final decisions is fundamental to the problem of multiphase inference .",
    "however , we also view decision theory as a vital theoretical tool for the study of multiphase inference .",
    "passing only risk - minimizing point estimators to later analysts is clearly not a recipe for valid inference .",
    "the key is to consider the use of previously generated results explicitly in the final decision problem . in the study of multiphase inference",
    ", we do so by focusing on the separation of knowledge and objectives between agents . such separation between preprocessing and downstream inference maps nicely to fisher s distinction between building scientific knowledge and reaching actionable decisions .",
    "thus , we interpret fisher s line of objections to decision - theoretic statistics as , in part , a rejection of adopting a myopic single - phase perspective in multiphase settings .",
    "we certainly do not believe that our work will bring closure to such an intense historical debate .",
    "however , we do see multiphase inference as an important bridge between these competing schools of thought .",
    "we see a wide range of open questions in multiphase inference .",
    "can more systematic ways to leverage the potential of preprocessing be developed ?",
    "is it possible to create a mathematical `` warning system , '' alerting practitioners when their inferences from preprocessed data are subject to severe degradation and showing where additional forms of preprocessing are required ? and ,",
    "can multiphase inference inform developments in distributed statistical computation and massive - data inference ( as outlined below in section  [ sec : computation ] ) ?",
    "all of these problems call for a shared collection of statistical principles , theory , and methods .",
    "below , we outline a few directions for the development of these tools for multiphase inference .",
    "the mechanics of passing information between phases constitute a major direction for further research .",
    "one approach leverages the fact that the likelihood function itself is always a minimal sufficient statistic .",
    "thus , a set of ( computationally ) efficient approximations to the likelihood function @xmath317 for @xmath318 could provide the foundation for a wide range of multiphase methods .",
    "many probabilistic inference techniques for the downstream model ( e.g. , mcmc samplers ) would be quite straightforward to use given such an approximation .",
    "the study of such multiphase approximations also offers great dividends for distributed statistical computation , as discussed below .",
    "we believe these approximations are promising direction for general - purpose preprocessing . however , there are stumbling blocks",
    ".    first , nuisance parameters remain an issue .",
    "we want to harness and understand the robustness benefits offered by preprocessing , but likelihood techniques themselves offer little guidance in this direction .",
    "even the work of @xcite on partial likelihood focuses on the details of estimation once the likelihood has been partitioned .",
    "we would like to identify the set of formal principles underlying techniques such as partial pivoting ( to mute the effect of infinite - dimensional nuisance parameters ) , building a more rigorous understanding of the role of preprocessing in providing robust inferences .",
    "as discussed in section  [ sec : missinfo ] , invariance relationships may be a useful focus for such investigations , guiding both bayesian and algorithmic developments .",
    "second , we must consider the burden placed on downstream analysts by our choice of approximation .",
    "probabilistic , model - based techniques can integrate such information with little additional development .",
    "however , it would be difficult for a downstream analyst accustomed to , say , standard regression methods to make use of a complex emulator for the likelihood function",
    ". the burden may be substantial for even sophisticated analysts .",
    "for instance , it could require a significant amount of effort and computational sophistication to obtain estimates of @xmath4 from such an approximation , and estimates of @xmath4 are often of interest to downstream analysts in addition to estimates of @xmath9 .      with these trade - offs in mind and through the formal analysis of widely - applicable multiphase techniques",
    ", we can begin to establish bounds on the error properties of such techniques in a broad range of problems under realistic constraints ( in both technical and human terms ) .",
    "more general constraints , for instance , can take the form of upper bounds on the regret attainable with a fixed amount of information passed from preprocessor to downstream analyst for fixed classes of scientific models .",
    "extensions to nonparametric downstream methods would have both practical and theoretical implications . in cases where the observation model is well - specified but",
    "the scientific model is less clearly defined , multiphase techniques can provide a useful alternative to computationally - expensive semi - parametric techniques .",
    "fusing principled preprocessing with flexible downstream inference may provide an interesting way to incorporate model - based subject - matter knowledge while effectively managing the bias - variance trade - off .",
    "the directions discussed above share a conceptual , if not technical , history with the development of congeniality ( @xcite ) .",
    "both the study of congeniality in mi and our study of multiphase inference seek to bound and measure the amount of degradation in inferences that can occur when agents attempt ( imperfectly ) to combine information . despite these similarities ,",
    "the treatment of nuisance parameters are rather different .",
    "nuisance parameters lie at the very heart of multiphase inference , defining many of its core issues and techniques . for mi ,",
    "the typical approaches have been to integrate them out in a bayesian analysis ( e.g. , @xcite ) or assume that the final analyst will handle them ( e.g. , @xcite ) .",
    "recent work by @xcite has shed new light on the role of nuisance parameters in mi , but the results are largely negative , demonstrating that nuisance parameters are often a stumbling block for practical mi inference .",
    "understanding the role of preprocessing in addressing nuisance parameters , providing robust analyses , and effectively distributing statistical inference represent further challenges beyond those pursued with mi .",
    "therefore , much remains to be done in the study of multiphase inference , both theoretical and methodological .",
    "we also see multiphase inference as a source for computational techniques , drawing inspiration from the history of mi .",
    "mi was initially developed as a strategy for handling missing data in public data releases . however , because mi separates the task of dealing with incomplete data from the task of making inferences , its use spread .",
    "it has frequently been used as a practical tool for dealing with missing - data problems where the joint inference of missing data and model parameters would impose excessive modeling or computational burdens .",
    "that is , increasingly the mi inference is carried out from imputation through analysis by a single analyst or research group .",
    "this is feasible as a computational strategy only because the error properties and conditions necessary for the validity of mi are relatively well - understood ( e.g. , @xcite , @xcite).=1    multiphase methods can similarly guide the development of efficient , statistically - valid computational strategies .",
    "once we have a theory showing the trade - offs and pitfalls of multiphase methods , we will be equipped to develop them into general computational techniques . in particular",
    ", our experience suggests that models with a high degree of conditional independence ( e.g. , exchangeable distributions for @xmath4 ) can often provide useful inputs for multiphase inferences , even when the true overall model has a greater degree of stochastic structure .",
    "the conditional independence structure of such models allows for highly parallel computation with first - phase procedures , providing huge computational gains on modern distributed systems compared to methods based on the joint model.=1    for example , in @xcite , a factored model was used to preprocess a massive collection of irregularly - sampled astronomical time series .",
    "the model was sophisticated enough to account for complex observation noise , yet its independence structure allowed for efficient parallelization of the necessary computation .",
    "its output was then combined and used for population - level analyses .",
    "just as markov chain monte - carlo ( mcmc ) has produced a windfall of tools for approximate high - dimensional integration ( see @xcite for many examples ) , we believe that this type of principled preprocessing , with further theoretical underpinnings , has the potential to become a core tool for the statistical analysis of massive datasets.=1",
    "we would like to acknowledge support from the arthur p. dempster award and partial financial support from the nsf .",
    "we would also like to thank arthur p. dempster and stephen blyth for their generous feedback .",
    "this work developed from the inaugural winning submission for said award .",
    "we also thank david van dyk , brandon kelly , nathan stein , alex damour , and edo airoldi for valuable discussions and feedback , and steven finch for proofreading .",
    "finally , we would like to thank our reviewers for their thorough and thoughtful comments , which have significantly enhanced this paper.=1"
  ],
  "abstract_text": [
    "<S> preprocessing forms an oft - neglected foundation for a wide range of statistical and scientific analyses . </S>",
    "<S> however , it is rife with subtleties and pitfalls . </S>",
    "<S> decisions made in preprocessing constrain all later analyses and are typically irreversible . </S>",
    "<S> hence , data analysis becomes a collaborative endeavor by all parties involved in data collection , preprocessing and curation , and downstream inference . </S>",
    "<S> even if each party has done its best given the information and resources available to them , the final result may still fall short of the best possible in the traditional single - phase inference framework . </S>",
    "<S> this is particularly relevant as we enter the era of `` big data '' . </S>",
    "<S> the technologies driving this data explosion are subject to complex new forms of measurement error . </S>",
    "<S> simultaneously , we are accumulating increasingly massive databases of scientific analyses . as a result </S>",
    "<S> , preprocessing has become more vital ( and potentially more dangerous ) than ever before .    </S>",
    "<S> we propose a theoretical framework for the analysis of preprocessing under the banner of multiphase inference . </S>",
    "<S> we provide some initial theoretical foundations for this area , including distributed preprocessing , building upon previous work in multiple imputation . </S>",
    "<S> we motivate this foundation with two problems from biology and astrophysics , illustrating multiphase pitfalls and potential solutions . </S>",
    "<S> these examples also emphasize the motivations behind multiphase analyses  both practical and theoretical . </S>",
    "<S> we demonstrate that multiphase inferences can , in some cases , even surpass standard single - phase estimators in efficiency and robustness . </S>",
    "<S> our work suggests several rich paths for further research into the statistical principles underlying preprocessing . to tackle our increasingly complex and massive data </S>",
    "<S> , we must ensure that our inferences are built upon solid inputs and sound principles . </S>",
    "<S> principled investigation of preprocessing is thus a vital direction for statistical research . </S>"
  ]
}