{
  "article_text": [
    "the divergence of the correlation length @xmath1 of a classical spin system as it approaches its critical temperature means that larger and larger volumes of spins must be updated coherently in order to sample statistically independent configurations . in monte carlo simulations",
    "this gives rise to a correlation time @xmath2 measured in monte carlo steps per site which diverges with @xmath1 as @xmath3 where @xmath4 is a dynamic exponent whose value depends on the update method . for a system of finite dimension @xmath5 close to the critical temperature , this means that the amount of cpu time required to generate an independent lattice configuration increases as @xmath6 , where @xmath7 is the dimensionality of the lattice . if we take the example of the ising model and make the assumption that the movement of domain walls under a dynamics employing a local update move is diffusive , then the number of monte carlo steps required to generate an independent spin configuration should scale as @xmath8 , and hence @xmath9 .",
    "the scaling arguments of bausch  ( 1981 ) indicate that this is in fact a lower bound on the value of @xmath4 for all dimensions up to four , and numerical investigations have in general measured values slightly higher than this .",
    "the most accurate determination of @xmath4 of which we are aware is that of nightingale and blte  ( 1996 ) , who measured a value of @xmath10 for the dynamic exponent of the two - dimensional ising model with a local update algorithm .",
    "thus the cpu time required to perform a simulation of this model with a given degree of accuracy increases slightly faster than @xmath11 with system size for temperatures close to the phase transition , which severely limits the system sizes which can be studied , even with the fastest computers .",
    "one solution to this problem is to try and find monte carlo algorithms which can update volumes on the order of @xmath12 in one step , rather than by the slow diffusion of domain walls .",
    "since @xmath1 can become arbitrarily large , this implies an algorithm with a true non - local update move ",
    "one capable of updating an arbitrary number of spins in one time - step . in the first part of this chapter we will study one of the most widely used and successful such algorithms , the wolff algorithm , which has a measured critical exponent close to zero for the ising model , a great improvement over the metropolis case",
    "an elegant non - local monte carlo algorithm applicable to the ising model has been proposed by wolff  ( 1989 ) , based on previous work by swendsen and wang  ( 1987 ) . at each monte carlo step",
    ", this algorithm flips one contiguous cluster of similarly - oriented spins .",
    "clusters are built up by starting from a randomly - chosen seed spin and adding further spins if they are ( a )  adjacent to a spin which already belongs to the cluster and ( b )  pointing in the same direction as the spins in the cluster .",
    "spins which satisfy these criteria are added to the cluster with some probability @xmath13 .",
    "eventually the cluster will stop growing when all possible candidates for addition have been considered and at this point the cluster is flipped over with some acceptance probability @xmath14 .",
    "it is straightforward to calculate what value @xmath14 should take for a given choice of @xmath13 .",
    "consider two states of the system , @xmath15 and @xmath16 , illustrated in figure  .",
    "they differ from one another by the flipping of a single cluster of similarly - oriented spins .",
    "the crucial thing to notice is the way the spins are oriented around the edge of the cluster ( which is indicated by the line in the figure ) .",
    "notice that in each of the two states , some of the spins just outside the cluster are pointing the same way as the spins in the cluster .",
    "the bonds between these spins and the ones in the cluster have to be broken when the cluster is flipped to reach the other state .",
    "inevitably , those bonds which are not broken in going from @xmath15 to @xmath16 must be broken if we make the reverse move from @xmath16 to @xmath15 .",
    "[ frames ]    consider now a cluster monte carlo move which takes us from @xmath15 to @xmath16 .",
    "there are in fact many such moves  we could choose any of the spins in the cluster as our seed spin , and then we could add the rest of the spins to it in a variety of orders . for the moment , however , let us just consider one particular move , starting with a particular seed spin and then adding others to it in a particular order .",
    "consider also the reverse move , which takes us back to @xmath15 from @xmath16 , starting with the same seed spin , and adding others to it in exactly the same order as the forward move .",
    "the probability of choosing the seed is the same in the two directions , as is the probability of adding each spin to the cluster .",
    "the only thing which changes between the two is the probability of `` breaking '' bonds around the edge of the cluster because the bonds which have to be broken are different in the two cases .",
    "suppose that , for the forward move , there are @xmath17 bonds which have to be broken in order to flip the cluster .",
    "these broken bonds represent correctly - oriented spins which were not added to the cluster by the algorithm . the probability of not adding such a spin is @xmath18 .",
    "thus the probability of not adding all of them , which is proportional to the selection probability @xmath19 for the forward move , is @xmath20 .",
    "if there are @xmath21 bonds which need to be broken in the reverse move then the probability of doing it will be @xmath22 , which is proportional to @xmath23 .",
    "the condition of detailed balance then tells us that @xmath24 where @xmath25 and @xmath26 are the acceptance ratios for the moves in the two directions . the change in energy @xmath27 between the two states also depends on the bonds which are broken . for each of the @xmath17 bonds which are broken in going from @xmath15 to @xmath16 , the energy changes by @xmath28 , where @xmath29 is the interaction strength between neighbouring spins . for each of the @xmath21 bonds which are made , the energy changes by @xmath30 .",
    "thus @xmath31 substituting into equation   and rearranging we derive the following condition on the acceptance ratios : @xmath32^{n - m}. \\label{wolffaccept}\\ ] ] but now we notice a delightful fact : if we choose @xmath33 then the right - hand side of equation   is just 1 , independent of any properties of the states @xmath15 and @xmath16 , or the temperature , or anything else at all . with this choice , we can make the acceptance ratios for both forward and backward moves unity , which is the best possible value they could take .",
    "every move we propose is accepted and the algorithm still satisfies detailed balance .",
    "the choice   then defines the wolff cluster algorithm for the ising model , whose precise statement would go something like this :    1 .",
    "choose a seed spin at random from the lattice .",
    "2 .   look in turn at each of the neighbours of that spin . if they are pointing in the same direction as the seed spin , add them to the cluster with probability @xmath34 .",
    "3 .   for each spin that was added in the last step , examine each of _ its _ neighbours to find the ones which are pointing in the same direction and add each of them to the cluster with the same probability @xmath13 .",
    "( notice that as the cluster becomes larger we may find that some of the neighbours are already members of the cluster , in which case obviously you do nt have to consider adding them again .",
    "also , some of the spins may have been considered for addition before , as neighbours of other spins in the cluster , but rejected . in this case , they get another chance to be added to the cluster on this step . )",
    "this step is repeated as many times as necessary until there are no spins left in the cluster whose neighbours have not been considered for inclusion in the cluster .",
    "4 .   flip all the spins in the cluster .",
    "the flipping of these clusters is a much less laborious task than in the case of the metropolis algorithm  as we shall see it takes a time proportional to the size of the cluster to grow it and then turn it over  so we have every hope that the algorithm will indeed have a lower dynamic exponent and less critical slowing down . in section",
    "we show that this is indeed the case .",
    "first we look briefly at how the wolff algorithm is implemented .",
    "the standard way to implement the wolff algorithm is a very straightforward version of the list of steps given above .",
    "first , we choose at random one of the spins on the lattice to be our seed .",
    "we look at each of its neighbours , to see if it points in the same direction as the seed spin .",
    "if it does then with probability @xmath13 we add that spin to the cluster and we store its coordinates on a stack .",
    "when we have exhausted the neighbours of the spin we are looking at , we pull a spin off the stack , and we start checking _ its _ neighbours one by one .",
    "any new spins added to the cluster are again also added to the stack , and we go on pulling spins off the stack and looking at their neighbours , until the stack is empty .",
    "this algorithm guarantees that we consider the neighbours of every spin added to the cluster , as we should .",
    "furthermore , the amount of time spent on each spin in the cluster is , on average , the same .",
    "each one gets added to the stack , pulled off again at some later stage and has all of its neighbours considered as candidates for addition to the cluster . the time taken to flip over all the spins in the cluster also goes like the number of spins , so the entire time taken by one monte carlo move in the wolff algorithm is proportional to the number of spins in the cluster .      in this section",
    "we look at how the wolff algorithm actually performs in practice .",
    "as we will see , it performs extremely well when we are near the critical temperature , but is actually a little slower than the metropolis algorithm at very high or low temperatures .",
    "[ wolffsnaps ]    [ csize ]    figure   shows a series of states generated by the algorithm at each of three temperatures , one above @xmath35 , one close to @xmath35 , and one below it . up and down spins are represented by the black and white dots .",
    "consider first the middle set of states , the ones near @xmath35 .",
    "if you examine the four frames , it is not difficult to make out which cluster flipped at each step . clearly the algorithm is doing its job , flipping large areas of spins when we are in the critical region . in the @xmath36 case",
    ", it is much harder to make out the changes between one frame and the next .",
    "the reason for this is that in this temperature region @xmath13 is quite small ( see equation  ) and this in turn makes the clusters small , so it is hard to see when they flip over .",
    "of course , this is exactly what the algorithm is supposed to do , since the correlation length here is small , and we do nt expect to get large regions of spins flipping over together .",
    "in figure  , we have plotted the mean size of the clusters flipped by the wolff algorithm over a range of temperatures , and , as we can see , it does indeed become small at large temperatures .",
    "when the temperature gets sufficiently high ( around @xmath37 in two dimensions ) , the mean size of the clusters becomes hardly greater than a single spin .",
    "in other words , the single seed spin for each cluster is being flipped over with probability one at each step , but none of its neighbours are .",
    "however , this is just exactly what the single - spin flip metropolis algorithm does in this temperature regime .",
    "when @xmath38 is large , the metropolis acceptance ratio is 1 , or very close to it , for any transition between two states @xmath15 and @xmath16 .",
    "thus in the limit of high temperatures , the wolff algorithm and the metropolis algorithm become the same thing .",
    "but notice that the wolff algorithm will actually be the slower of the two in this case , because for each seed spin it has to go through the business of testing each of the neighbours for possible inclusion in the cluster , whereas the metropolis algorithm only has to decide whether to flip a single spin or not , a comparatively simple computational task .",
    "thus , even if the wolff algorithm is a good thing near to the phase transition ( which it is ) , there comes a point as the temperature increases where the metropolis algorithm becomes better .",
    "now let us turn to the simulation at low @xmath38 .",
    "looking at the bottom row of figure  , the action of the algorithm is dramatically obvious in this case  almost every spin on the lattice is being flipped at each step .",
    "the reason for this is clear .",
    "when we are well below the critical temperature the ising model develops a backbone of similarly - oriented spins which spans the entire lattice .",
    "when we choose our seed spin for the wolff algorithm , it is likely that we will land on one of the spins comprising this backbone .",
    "furthermore , the probability @xmath13 , equation  , is large when @xmath38 is small , so the neighbours of the seed spin are not only likely to be aligned with it , but are also likely to be added to the growing cluster .",
    "the result is that the cluster grows ( usually ) to fill almost the entire backbone of spontaneously magnetized spins , and then they are all flipped over in one step .    on the face of it",
    ", this seems like a very inefficient way to generate states for the ising model .",
    "after all , we know that what should really be happening in the ising model at low temperature is that most of the spins should be lined up with one another , except for a few excitation spins , which are pointing the other way ( see the figure  ) .",
    "every so often , one of these excitation spins flips back over to join the majority pointing the other way , or perhaps one of the backbone spins gets flipped by a particularly enthusiastic thermal excitation and becomes a new excitation spin .",
    "this of course is exactly what the metropolis algorithm does in this regime .",
    "the wolff algorithm on the other hand is removing the single - spin excitations by the seemingly extravagant measure of _ flipping all the other spins in the entire lattice _ to point the same way as the single lonely excitation .",
    "in fact , however , it s actually not such a stupid thing to do .",
    "first , let us point out that , since the wolff algorithm never flips spins which are pointing in the opposite direction to the seed spin , all the excitation spins on the entire lattice end up pointing the same way as the backbone when the algorithm flips over a percolating cluster .",
    "therefore , the wolff algorithm gets rid of _ all _ the excitation spins on the lattice in a single step .",
    "second , since we know that the wolff algorithm generates states with the correct boltzmann probabilities , it must presumably create some new excitation spins at the same time as it gets rid of the old ones .",
    "and indeed it does do this , as is clear from the frames in the figure .",
    "each spin in the backbone gets a number of different chances to be added to the cluster ; for most of the spins this number is just the lattice coordination number @xmath4 , which is four in the case of the square lattice .",
    "thus the chance that a spin will not be added to the cluster at all is @xmath39 .",
    "this is a small number , but still non - zero .",
    "( it is @xmath40 at the temperature @xmath41 used in the figure . )",
    "thus there will be a small number of spins in the backbone which get left out of the percolating cluster and are not flipped along with the others .",
    "these spins become the new excitations on the lattice .",
    "the net result of all this is that after only one wolff monte carlo step , all the old excitations have vanished and a new set have appeared .",
    "this behaviour is clear in figure  . at low temperatures ,",
    "the wolff algorithm generates a complete new configuration of the model at every monte carlo step , though the payoff is that it has to flip very nearly every spin on every step too . in the metropolis algorithm at low temperatures",
    "it turns out that you have to do about one monte carlo step per site ( each one possibly flipping one spin ) to generate a new independent configuration of the lattice . to see this ,",
    "we need only consider again the excitation spins .",
    "the average acceptance ratio for flipping these over will be close to 1 in the metropolis algorithm , because the energy of the system is usually lowered by flipping them .",
    "( only on the rare occasions when several of them happen to be close together might this not be true . )",
    "thus , it will on average just take @xmath42 steps , where @xmath42 is the number of sites on the lattice , before we find any particular such spin and flip it  in other words , one monte carlo step per site .",
    "but we know that the algorithm correctly generates states with their boltzmann probability , so , in the same @xmath42 steps that it takes to find all the excitation spins and flip them over to join the backbone , the algorithm must also choose a roughly equal number of new spins to excite out of the backbone , just as the wolff algorithm also does .",
    "thus , it takes one sweep of the lattice to replace all the excitation spins with a new set , generating an independent state of the lattice .",
    "( this is in fact the best possible performance that any monte carlo algorithm can have , since one has to allow at least one sweep of the lattice in order to generate a new configuration , so that each spin gets the chance to change its value . )    again then , we find that the wolff algorithm and the metropolis algorithm are roughly comparable , this time at low temperatures . both need to consider about @xmath42 spins for flipping in order to generate an independent state of the lattice . again",
    ", however , the additional complexity of the wolff algorithm is its downfall , and the extremely simple metropolis algorithm has the edge in speed .",
    "so , if the metropolis algorithm beats the wolff algorithm ( albeit only by a slim margin ) at both high and low temperatures , that leaves only the intermediate regime , close to @xmath35 in which the wolff algorithm might be worthwhile .",
    "this of course is the regime in which we designed the algorithm to work well , so we have every hope that it will beat the metropolis algorithm there , and indeed it does , very handily .",
    "before we measure the correlation time of the wolff algorithm , we need to consider exactly how it should be defined .",
    "if we are going to compare the correlation times of the wolff algorithm and the metropolis algorithm near to the phase transition as a way of deciding which is the better algorithm in this region , it clearly would not be fair to measure it for both algorithms in terms of number of monte carlo steps .",
    "a single monte carlo step in the wolff algorithm is a very complicated procedure , flipping maybe hundreds of spins and potentially taking quite a lot of cpu time , whereas the metropolis monte carlo step is a very simple , quick thing  we can do a million of them in a second on a good computer  though each one only flips at most one spin .",
    "in section   we showed that the time taken to complete one step of the wolff algorithm is proportional to the number of spins @xmath43 in the cluster .",
    "such a cluster covers a fraction @xmath44 of the entire lattice , and so on average each monte carlo step will take an amount of cpu time comparable to @xmath45 sweeps of the lattice using the single - spin - flip metropolis algorithm .",
    "thus the correct way to define the correlation time is to write @xmath46 , where @xmath47 is the correlation time measured in steps ( i.e. ,  clusters flipped ) in the wolff algorithm .",
    "the conventional choice for the constant of proportionality is 1 .",
    "this makes the correlation times for the wolff and metropolis algorithms equal in the limits of low and high temperature , for the reasons discussed in the last section .",
    "this is not quite fair , since , as we pointed out earlier , the wolff algorithm is a little slower than the metropolis in these regimes because of its greater complexity .",
    "however , this difference is slight compared with the enormous difference in performance between the two algorithms near the phase transition which we will witness in a moment , so for all practical purposes we can write @xmath48    in our own simulations of the 2d ising model on a @xmath49 square lattice using the two algorithms , we measure the correlation time at @xmath35 of the wolff algorithm to be @xmath50 spin - flips per site , by contrast with the metropolis algorithm which has @xmath51 .",
    "a factor of a thousand certainly out - weighs any difference in the relative complexity of the two algorithms .",
    "it is this impressive performance on the part of the wolff algorithm which makes it a worthwhile one to use if we are interested in the behaviour of the model close to @xmath35 .",
    "[ taulw ]    in figure   we have plotted on logarithmic scales the correlation time of the wolff algorithm for the 2d ising model at the critical temperature , over a range of different system sizes . the slope of the line gives us an estimate of the dynamic exponent .",
    "our best fit , given the errors on the data points is @xmath52 .",
    "this was something of a rough calculation , though our result is competitive with other more thorough ones .",
    "the best available figure at the time of writing was that of coddington and baillie  ( 1992 ) who measured @xmath53 .",
    "this figure is clearly much lower than the @xmath54 of the metropolis algorithm , and gives us a quantitative measure of how much better the wolff algorithm really is .",
    "in fact in studies of the wolff algorithm for the 2d ising model one does not usually bother to make use of equation   to calculate @xmath2 .",
    "if we measure time in monte carlo steps ( that is , simple cluster flips ) , we can define the corresponding dynamic exponent @xmath55 in terms of the correlation time @xmath56 of equation   thus : @xmath57 the exponent @xmath55 is related to the real dynamic exponent @xmath4 for the algorithm by @xmath58 where @xmath59 and @xmath16 are the critical exponents governing the divergences of the magnetic susceptibility and the correlation length .",
    "if we know the values of @xmath16 and @xmath59 , as we do in the case of the 2d ising model , then we can use equation   to calculate @xmath4 without having to measure the mean cluster size in the algorithm , which eliminates one source of error in the measurement . directly , as we did in figure  . ]",
    "the first step in demonstrating equation   is to prove another useful result , about the magnetic susceptibility @xmath60 .",
    "it turns out that , for temperatures @xmath61 , the susceptibility is related to the mean size @xmath62 of the clusters flipped by the wolff algorithm thus : @xmath63 in many simulations using the wolff algorithm , the susceptibility is measured using the mean cluster size in this way .",
    "the demonstration of equation   goes like this .",
    "instead of implementing the wolff algorithm in the way described in this chapter , imagine instead doing it a slightly different way .",
    "imagine that at each step we look at the whole lattice and for every pair of neighbouring spins which are pointing in the same direction , we make a `` link '' between them with probability @xmath34 .",
    "when we are done , we will have divided the whole lattice into many different clusters of spins , as shown in figure  , each of which will be a correct wolff cluster ( since we have used the correct wolff probability @xmath13 to make the links ) . now we choose a single seed spin from the lattice at random , and flip the cluster to which it belongs .",
    "then we throw away all the links we have made and start again .",
    "the only difference between this algorithm and the wolff algorithm as we described it is that here we make the clusters first and choose the seed spin afterwards , rather than the other way around .",
    "this would not be a very efficient way of implementing the wolff algorithm , since it requires us to create clusters all over the lattice , almost all of which never get flipped , but it is a useful device for proving equation  .",
    "well , we can write the total magnetization @xmath64 of the lattice as a sum over all the clusters on the lattice thus : @xmath65 here @xmath66 labels the different clusters , @xmath67 is their size ( a positive integer ) , and @xmath68 depending on whether the cluster is pointing up or down , thereby making either a positive or a negative contribution to @xmath64 .",
    "the mean square magnetization is then @xmath69 now , since the average of the magnetization is zero under the wolff algorithm ( this is even true below @xmath35 , since the algorithm flips most of the spins on the lattice at every step at low temperature ) , the first term in this expression is an average over a large number of quantities which are randomly either positive or negative and which will therefore tend to average out to zero .",
    "the second term on the other hand is an average over only positive quantities and therefore is not zero . noting that @xmath70 for all @xmath66 ,",
    "we can then write @xmath71 or alternatively , in terms of the magnetization per spin @xmath17 : @xmath72    now consider the average @xmath62 of the size of the clusters which get flipped in the wolff algorithm . this is not quite the same thing as the average size @xmath73 of the clusters over the entire lattice , because when we choose our seed spin for the wolff algorithm it is chosen at random from the entire lattice , which means that the probability @xmath74 of it falling in a particular cluster @xmath66 , is proportional to the size of that cluster : @xmath75 the average cluster size in the wolff algorithm is then given by the average over the probability of the cluster being chosen times the size of that cluster : @xmath76 given that @xmath77 for @xmath61 , we now have our result for the magnetic susceptibility as promised : @xmath78    we can now use this equation to rewrite equation   thus : @xmath79 which implies in turn that @xmath80 near the critical point , this implies that @xmath81 and thus @xmath82 as we suggested earlier on .",
    "[ tausteps ]    in figure   we have replotted the results from our wolff algorithm simulations of the 2d ising model using the correlation time measured in monte carlo steps ( i.e. ,  cluster flips ) .",
    "the best fit to the data gives a figure of @xmath83 .",
    "using the accepted values @xmath84 and @xmath85 for the 2d ising model ( onsager  1944 ) , and setting @xmath86 , equation   then gives us @xmath53 , which is as good as the best published result for this exponent .",
    "we have looked in detail at the wolff algorithm for simulating the ising model . close to @xmath35 this algorithm",
    "is significantly more efficient than the metropolis one ; although it is more complex than the metropolis algorithm , the wolff algorithm has a very small dynamic exponent , which means that the time taken to perform a simulation scales roughly like the size of the system , which is the best that we can hope for in any algorithm .",
    "however , many other algorithms have been suggested for the simulation of the ising model . in this section",
    "we talk about a few of these alternative algorithms .",
    "after the metropolis and wolff algorithms , probably the most important other algorithm is the algorithm of swendsen and wang  ( 1987 ) .",
    "in fact this algorithm is very similar to the wolff algorithm and wolff took the idea for his algorithm directly from it .",
    "( swendsen and wang took the idea in turn from the work of fortuin and kasteleyn  ( 1972 ) and sweeny  ( 1983 ) . ) actually , we have seen the central idea behind the swendsen - wang algorithm already . in section",
    "we considered an alternative implementation of the wolff algorithm in which the entire lattice of spins is divided up into clusters by making `` links '' with probability @xmath87 between similarly - oriented neighbouring spins .",
    "we then imagined choosing a single spin from the lattice and flipping over the whole of the cluster to which it belongs .",
    "this procedure is clearly equivalent to the wolff algorithm , although in practice it would be an inefficient way of carrying it out .",
    "the swendsen - wang algorithm divides the entire lattice into clusters in exactly the same way , with this same probability @xmath13 of making a link .",
    "but then , instead of flipping just one cluster , each cluster is independently flipped with probability @xmath88 .",
    "we notice the following facts about this algorithm :    1 .",
    "the algorithm satisfies the condition of detailed balance .",
    "the proof of this fact is exactly the same as it was for the wolff algorithm .",
    "if the number of links broken and made in performing a move are @xmath17 and @xmath21 respectively ( and the reverse for the reverse move ) , then the energy change for the move is @xmath89 ( or @xmath90 for the reverse move ) .",
    "the selection probabilities for choosing a particular set of links differs between forward and reverse moves only at the places where bonds are made or broken , and so the ratio of the two selection probabilities is @xmath91 , just as it was before . by choosing @xmath92 ,",
    "we then ensure , just as before , that the acceptance probability is independent of @xmath17 and @xmath21 and everything else , so any choice which makes it the same in each direction , such as flipping all clusters with probability @xmath88 will make the algorithm correct .",
    "notice however that other choices would also work .",
    "it does nt matter how we choose to flip the clusters , though the choice made here is good because it minimizes the correlation between the direction of a cluster before an after a move , the new direction being chosen completely at random , regardless of the old one .",
    "the algorithm updates the entire lattice on each move . in measuring correlation times for this algorithm , one should therefore measure them simply in numbers of monte carlo steps , and not steps per site as with the metropolis algorithm .",
    "( in fact , on average , only half the spins get flipped on each move , but the number flipped scales like the size of the system , which is the important point . )",
    "the swendsen - wang algorithm is essentially the same as the wolff algorithm for low temperatures .",
    "well below @xmath35 , one of the clusters chosen by the algorithm will be a percolating cluster , and the rest will correspond to the `` excitations '' discussed in section  , which will be small . ignoring these small clusters then , the swendsen - wang algorithm will tend to turn over the percolating backbone of the lattice on average every two steps ( rather than every step as in the wolff algorithm  see figure  ) , but otherwise the two will behave almost identically .",
    "thus , as with the wolff algorithm , we can expect the performance of the swendsen - wang algorithm to be similar to that of the metropolis algorithm at low @xmath38 , though probably a little slower on average due to the complexity of the algorithm .",
    "4 .   at high temperatures , the swendsen - wang algorithm tends to divide the lattice into very small clusters because @xmath13 becomes small . as @xmath93 the clusters",
    "will just be one spin each , and the algorithm will just change all the spins to new random values on each move .",
    "this is also what the metropolis algorithm does at high temperatures in one sweep of the lattice , though again the metropolis algorithm can be expected to be a little more efficient in this regime , since it is a simpler algorithm which takes few operations on the computer to flip each spin .",
    "the combination of the last two points here implies that the only regime in which the swendsen - wang algorithm can be expected to out - perform the metropolis algorithm is the one close to the critical temperature .",
    "the best measurement of the dynamic exponent of the algorithm is that of coddington and baillie  ( 1992 ) , who found @xmath53 in two dimensions , which is clearly much better than the metropolis algorithm , and is in fact exactly the same as the result for the wolff algorithm .",
    "so the swendsen - wang algorithm is a pretty good algorithm for investigating the 2d ising model close to its critical point .",
    "however , as table   shows , for higher dimensions the swendsen - wang algorithm has a significantly higher dynamic exponent than the wolff algorithm , making it slower close to @xmath35 .",
    "the reason is that close to @xmath35 the properties of the ising model are dominated by the fluctuation of large clusters of spins .",
    "as the arguments of section   showed , the wolff algorithm preferentially flips larger clusters because the chance of the seed spin belonging to any particular cluster is proportional to the size of that cluster .",
    "the swendsen - wang algorithm on the other hand treats all clusters equally , regardless of their size , and therefore wastes a considerable amount of effort on small clusters which make vanishingly little contribution to the macroscopic properties of the system for large system sizes .",
    "this , coupled with the fact that the swendsen - wang algorithm is slightly more complicated to program than the wolff algorithm , makes the wolff algorithm the algorithm of choice for most people .",
    "it is worth noting however , that the work of ferrenberg , landau , and wong  ( 1992 ) appears to indicate that the wolff algorithm is unusually susceptible to imperfections in the random number generator used to implement the algorithm .",
    "although this result is probably highly sensitive to the way in which the algorithm is coded , it may be that the swendsen - wang algorithm would be a more sensible choice for those who are unsure of the quality of their random numbers .    [ cols=\"^,^,^,^\",options=\"header \" , ]     [ zval ]      another variation on the general cluster algorithm theme was proposed by niedermayer  ( 1988 ) .",
    "his suggestion is really just an extension of the ideas used in the wolff and swendsen - wang algorithms .",
    "in fact , niedermayer s methods are very general and can be applied to all sorts of models including glassy spin models . here",
    "we will just consider their application to the ordinary ising model .",
    "niedermayer pointed at that it is not necessary to constrain the `` links '' with which we make clusters to be only between spins which are pointing in the same direction .",
    "in general , we can define two different probabilities for putting links between sites  one for parallel spins and one for anti - parallel ones . the way niedermayer expressed it , he considered the energy contribution @xmath94 that a pair of spins @xmath66 and @xmath95 makes to the hamiltonian . in the case of the ising model , for example",
    ", @xmath96 he then wrote the probability for making a link between two neighbouring spins as a function of this energy @xmath97 . in the ising model @xmath94",
    "can only take two values @xmath98 , so the function @xmath99 only needs to be defined at these points , but for some of the more general models niedermayer considered it needs to be defined elsewhere as well .",
    "clearly , if for the ising model we make @xmath100 and @xmath101 , then we recover the wolff algorithm or the swendsen - wang algorithm , depending on whether we flip only a single cluster on each move , or many clusters over the entire lattice ",
    "niedermayer s formalism is applicable in either case . to be concrete about things ,",
    "let us look at the case of the single - cluster , wolff - type version of the algorithm .",
    "let us apply the condition of detailed balance to the algorithm .",
    "consider , as we did in the case of the wolff algorithm , two states of our system which differ by the flipping of a single cluster .",
    "( you can look again at figure   if you like , but bear in mind that , since we are now allowing links between anti - parallel spins , not all the spins in the cluster need be pointing in the same direction . ) as before , the probability of forming the cluster itself is exactly the same in the forward and reverse directions , except for the contributions which come from the borders . at the borders ,",
    "there are some pairs or spins which are parallel and some which are anti - parallel .",
    "suppose that in the forward direction there are @xmath17 pairs of parallel spins at the border ",
    "these correspond to bonds which have to be broken in flipping the cluster  and",
    "@xmath21 pairs which are anti - parallel  bonds which will be made when we flip . by definition",
    "no links are made between any of these border pairs , and the probability of that happening is @xmath102^m [ 1 - p_{\\rm    add}(-j)]^n$ ] . in the reverse direction",
    "the corresponding probability is @xmath102^n [ 1 - p_{\\rm add}(-j)]^m$ ] .",
    "just as in the wolff case , the energy cost of flipping the cluster from state @xmath15 to state @xmath16 is @xmath31 thus , the appropriate generalization of the acceptance ratio relation , equation  , is @xmath103^{n - m}. \\label{eqniedaccept}\\ ] ] any choice of acceptance ratios @xmath25 and @xmath26 which satisfies this relation will satisfy detailed balance . for the wolff choice of @xmath13",
    "we get acceptance ratios which are always unity , but niedermayer pointed out that there are other ways to achieve this .",
    "in fact , all we need to do is choose @xmath13 to satisfy @xmath104 and we will get acceptance ratios which are always one .",
    "niedermayer s solution to this equation was @xmath105 $ ] where @xmath106 is a free parameter whose value we can choose as we like .",
    "notice however that this quantity is supposed to be a probability for making a bond , so it is not allowed to be less than zero .",
    "thus the best expression we can write for the probability @xmath107 of adding a link between sites @xmath66 and @xmath95 is @xmath108 and this defines niedermayer s algorithm . in figure  , we have plotted @xmath13 as a function of @xmath109 for one particular choice of the constant @xmath106 .",
    "[ niedaccept ]    notice the following things about this algorithm :    1 .",
    "as long as all @xmath94 on the lattice are greater than or equal to @xmath106 , the right - hand side of equation   is always unity , so the two acceptance ratios can be chosen to be one for every move . since we are at liberty to choose @xmath106 however we like , we can always choose it to satisfy this condition by making it less than or equal to the smallest value that @xmath94 can take , which is @xmath110 in the case of the ising model .",
    "this gives us a whole spectrum of wolff - type algorithms for various values @xmath111 , which all have acceptance ratios of one .",
    "as @xmath106 gets more and more negative , the probabilities @xmath112 tend closer and closer to one , making the clusters formed larger and larger .",
    "this gives us a way of controlling the sizes of the clusters formed in our algorithm , all the way up to clusters which encompass ( almost ) every spin on the lattice at every move .",
    "if we choose @xmath106 be greater than the smallest possible value of @xmath94 ( which is @xmath110 in the ising case ) then the right - hand side of equation   is no longer equal to one , and we can no longer choose the acceptance ratios to be unity .",
    "instead we have @xmath113^{n - m } = [ \\e^{2\\beta(e_0 + j)}]^{n - m}.\\ ] ] just as with the metropolis algorithm , the optimal choice of the two acceptance ratios is then to make the larger of the two equal to one , and choose the smaller to satisfy this equation .",
    "if we do this , we again achieve detailed balance , and we now have an algorithm which , as @xmath106 is made larger and larger , produces smaller and smaller clusters , though it does so at the expense of an exponentially decreasing acceptance ratio for cluster moves which increase the energy of the system .",
    "if @xmath106 is chosen to be larger than the largest possible value of @xmath94 , which is @xmath114 , then @xmath115 for all pairs of spins @xmath66 , @xmath95 , so every cluster formed has only one spin in it and the acceptance ratios are given by @xmath116^{n - m}.\\ ] ] bearing in mind that @xmath17 is the number of neighbours of this single spin which are pointing in the same direction as it , and @xmath21 is the number which point in the opposite direction , we can see that this is exactly the same as the acceptance ratio for the metropolis algorithm .",
    "thus we see that by varying the parameter @xmath106 , niedermayer s cluster algorithm includes as special cases both the metropolis algorithm and the wolff algorithm , and interpolates smoothly from one to the other and beyond , varying the average cluster size from one spin all the way up to the entire lattice .",
    "the trouble with the algorithm is that no one really knows what value one should choose for @xmath106 .",
    "niedermayer himself conjectured that the wolff choice @xmath117 might not give the optimal correlation time and that shorter ones could be achieved by making other choices .",
    "he gave preliminary evidence in his 1988 paper that , in some cases at least , a larger value of @xmath106 than this ( i.e. ,  a value which produces smaller clusters on average , at the expense of a lowered acceptance probability ) gives a shorter correlation time .",
    "however , to our knowledge no one has performed an extensive study of the dynamic exponent of the algorithm as a function of @xmath106 , so for the moment at least , the algorithm remains an interesting extension of the wolff idea which has yet to find use in any large - scale simulations .",
    "an alternative technique for varying the sizes of the clusters flipped by the wolff algorithm has been studied by barkema and marko  ( 1993 ) , who proposed a general method for placing constraints on the clusters generated in the wolff algorithm . in their algorithm ,",
    "cluster growth proceeds just as in the normal wolff algorithm , except when the addition of a site would violate some condition which we wish to preserve .",
    "for example , we might want to place a limit on the total number of spins in the cluster , or limit the radius of the cluster around the initial seed spin .",
    "if the addition of a certain spin would violate such a condition , we simply make the probability of adding it to the cluster zero , instead of the usual @xmath118 . on its own",
    ", this procedure would violate detailed balance since it alters the selection probability for the move @xmath19 away from its carefully chosen wolff value . however , the requirement of detailed balance only places a condition on the product of the selection probability for a move and the acceptance ratio .",
    "thus it is possible to compensate for the change in @xmath19 by making an opposing change to the acceptance ratio @xmath25 . by doing this ,",
    "detailed balance is restored and the desired condition is met , albeit at the expense of changing the acceptance ratio from the optimal value of 1 which it has in the wolff algorithm .    in detail",
    "the method works like this .",
    "consider the two states @xmath15 and @xmath16 , and suppose that for a move which takes us from @xmath15 to @xmath16 there are @xmath17 bonds which have to be broken in order to flip the cluster .",
    "these broken bonds represent correctly - oriented spins which are not added to the cluster by the algorithm . for most of these broken bonds , the probability of not adding such a spin is @xmath119 as in the usual wolff algorithm , but there will now be some number @xmath120 that were rejected because of the applied constraint and thus had no chance of being added .",
    "thus the probability of not adding all the broken bonds , which is proportional to the selection probability @xmath19 for the forward move , is @xmath121 .",
    "if there are @xmath21 bonds which are broken in making the reverse move from @xmath16 to @xmath15 , @xmath122 of which had no chance of being added since they violated our constraint , then the selection probability @xmath23 will be proportional to @xmath123 .",
    "the ratio of the forward and backward selection probabilities is thus the same as in the wolff algorithm , except for a factor of @xmath124 . in order to restore detailed balance ,",
    "we compensate for this factor by introducing an acceptance ratio of @xmath125    what is the purpose of introducing constraints such as these ? in the context of the normal ising model , a constraint can be enforced to limit the cluster to a certain region .",
    "this allows us to divide the lattice into a number of regions in each of which cluster flips can be performed independently of the others .",
    "this leads to an efficient algorithm for simulating the model on a parallel ( distributed ) computer which shows near - linear speed - up in the number of processors employed ( barkema and macfarland  1994 ) .",
    "periodically , the division of the lattice has to be rearranged in order to obtain ergodicity .",
    "another application of the idea of cluster constraints can be found in the simulation of the conserved - order - parameter ising model : clusters of up- and down - pointing spins are grown with a hard constraint on their mass , and if both clusters reach the same mass , an exchange of the spins is proposed .",
    "size constraints have also proved useful in the simulation of the random - field ising model ( rfim ) .",
    "the rfim is normally simulated using a single - spin - flip metropolis algorithm .",
    "it is also possible to apply the wolff algorithm to the problem by growing wolff clusters ignoring the random fields and incorporating their energy contribution into an acceptance ratio for the cluster flip ( dotsenko  1991 ) .",
    "however in practice this algorithm works poorly in the critical region of the model because the rfim has a critical temperature lower than that of the normal ising model .",
    "this means that the clusters grown are nearly always large percolating clusters , which have little chance of being flipped over because the random fields pin them in one direction or the other .",
    "this problem can be overcome if the size of the cluster is limited to a radius @xmath126 , which is chosen at random on each step from a distribution as @xmath127 ( newman and barkema  1996 ) .",
    "also worthy of mention is a class of methods developed by kandel  ( 1989 , kandel  1991 ) , which are referred to as multigrid methods .",
    "these methods are also aimed at reducing critical slowing down and accelerating simulations at or close to the critical temperature .",
    "multigrid methods have not been used to a very great extent in large - scale monte carlo simulations because they are considerably more complex to program than the cluster algorithms discussed above .",
    "however , they may yet prove useful in some contexts because they appear to be faster than cluster algorithms for simulating very large systems .",
    "the fundamental idea behind these methods is the observation that , with the divergence of the correlation length at the critical temperature , we expect to see fluctuating domains of spins of all sizes up to the size of the entire lattice .",
    "the multigrid methods therefore split the cpu time of the simulation up , spending varying amounts of time flipping blocks of spins of various sizes .",
    "this idea certainly has something in common with the ideas behind the wolff and swendsen - wang algorithms , but the multigrid methods are more deliberate about flipping blocks of certain sizes , rather than allowing the sizes to be determined by the temperature and configuration of the lattice .",
    "kandel and his co - workers gave a number of different , similar algorithms , which all fall under the umbrella of multigrid methods .",
    "here , we describe one example , which is probably the simplest and most efficient such algorithm for the ising model .",
    "the algorithm works by grouping the spins on the lattice into blocks and then treating the blocks as single spins and flipping them using a metropolis algorithm . in detail",
    ", what one does is this .    in the swendsen - wang algorithm we made `` links '' between similarly - oriented spins , which effectively tied those spins together into a cluster , so that they flipped as one .",
    "any two spins which were not linked were free to flip separately  there was not even a ferromagnetic interaction between them to encourage them to point in the same direction . in the present multigrid method , two adjacent spins can be in three different configurations : they can be linked as in the swendsen - wang case so that they must flip together , they can have no connection between them at all so that they can flip however they like , or they can have a normal ising interaction between them of strength @xmath29 which encourages them energetically to point in the same direction , but does not force them to as our links do . by choosing one of these three states for every pair of spins , the lattice is divided up into clusters of linked spins which either have interactions between them , or which are free to flip however they like .",
    "the algorithm is contrived so that only clusters of one or two spins are created .",
    "no clusters larger than two spins appear .",
    "the procedure for dividing up the lattice goes like this .    1 .",
    "we take a spin on the lattice , and examine each of its neighbours in turn . if a neighbour is pointing in the opposite direction to the spin , then we leave it alone . in kandel",
    "s terminology , we `` delete '' the bond between the two spins , so that they are free to assume the same or different directions with no energy cost . if a neighbour is pointing in the same direction as our spin , then we make a link between the two with the same probability @xmath34 as we used in the wolff and swendsen - wang algorithms .",
    "kandel calls this `` freezing '' the bond between the spins .",
    "since we only want to create clusters of at most two spins , we stop looking at neighbours once we have created a link to any one of them .",
    "in fact , what we do is to keep the normal ising interactions between the spin and all the remaining neighbours that we have not yet looked at .",
    "we do this regardless of whether they are pointing in the same direction as our first spin or not .",
    "kandel describes this as `` leaving the bonds active '' .",
    "now we move onto another spin and do the same thing , and in this way cover the entire lattice . notice that , if we come to a spin which is adjacent to one we have considered before , then some of the spin s bonds will already have been frozen , deleted , or marked as active . in this case",
    "we leave those bonds as they are , and only go to work on the others which have not yet been considered .",
    "notice also that if we come to a spin and it has already been linked ( `` frozen '' ) to another spin , then we know immediately that we need to leave the interactions on all the remaining bonds to that spin active .    in this way , we decide the fate of all the spins on the lattice , dividing them into clusters of one or two , joined by bonds which may or may not have interactions associated with them .",
    "then we treat those clusters as single spins , and we carry out the metropolis algorithm on them , for a few sweeps of the lattice .    but",
    "this is not the end .",
    "now we do the whole procedure again , treating the clusters as spins , and joining them into bigger clusters of either one or two elements each , using exactly the same rules as before .",
    "( note that the lattice of clusters is not a regular lattice , as the original system was , but this does not stop us from carrying out the procedure just as before . )",
    "then we do a few metropolis sweeps of this coarser lattice too .",
    "and we keep repeating the whole thing until the size of the blocks reaches the size of the whole lattice . in this way",
    ", we get to flip blocks of spins of all sizes from single spins right up to the size of the entire system .",
    "then we start taking the blocks apart again into the blocks that made them up , and so forth until we get back to the lattice of single spins again .",
    "in fact , kandel and co - workers used a scheme where at each level in the blocking procedure they either went towards bigger blocks ( `` coarsening '' ) or smaller ones ( `` uncoarsening '' ) according to the following rule . at any particular level of the procedure",
    "we look back and see what we did the previous times we got to this level . if we coarsened the lattice the previous two times we got to this point , then on the third time , we uncoarsen .",
    "this choice has the effect of biasing the algorithm towards working more at the long length scales ( bigger , coarser blocks ) .",
    "well , perhaps you can see why the complexity of this algorithm has put people off using it .",
    "the proof that the algorithm satisfies detailed balance is even more involved , and , since you re probably not dying to hear about it right now , we ll refer you to the original paper for the details ( kandel  ( 1989 ) ) . in the same paper it is demonstrated that the dynamic exponent for the algorithm is in the region of @xmath128 for the two - dimensional ising model  a value similar to that of the wolff algorithm . before we dismiss the multigrid method out of hand , however , let us point out that the simulations do indicate that its performance is superior to cluster algorithms for large systems .",
    "these days , with increasing computer power , people are pushing simulations towards larger and larger lattices , and there may well come a point at which using a multigrid method could win us a significant speed advantage .      finally ,",
    "in our round - up of monte carlo algorithms for the ising model , we come to an unusual algorithm proposed by jon machta and co - workers called the invaded cluster algorithm ( machta  1995 ) .",
    "the thing which sets this algorithm apart from the others we have looked at so far is that it is not a general purpose algorithm for simulating the ising model at any temperature .",
    "in fact , the invaded cluster algorithm can only be used to simulate the model at the critical point ; it does not work at any other temperature .",
    "what s more , for a system at the critical point in the thermodynamic limit the algorithm is just the same as the swendsen - wang cluster algorithm of section  .",
    "so what s the point of the algorithm ?",
    "well , there are two points .",
    "first , the invaded cluster algorithm can find the critical point all on its own  we do nt need to know what the critical temperature is beforehand in order to use the algorithm . starting with a system at any temperature the algorithm will adjust its simulation temperature until it finds the critical value @xmath35 .",
    "this makes the algorithm very useful for actually measuring the critical temperature , something which normally requires either finite - size scaling or monte carlo rg .",
    "second , the invaded cluster algorithm equilibrates extremely quickly .",
    "although the algorithm is equivalent to the swendsen - wang algorithm once it reaches equilibrium at @xmath35 , its behaviour whilst getting there is very different , and in a direct comparison of equilibration times between the two , say for systems starting at @xmath129 , there is really no contest . for large system sizes , the invaded cluster algorithm can reach the critical point as much as a hundred times faster than the swendsen - wang algorithm .",
    "( a comparison with the wolff algorithm yields similar results ",
    "the performance of the wolff and swendsen - wang algorithms is comparable . )    so , how does the invaded cluster algorithm work ? basically , it is just a variation of the swendsen - wang algorithm in which the temperature is continually adjusted to look for the critical point .",
    "the algorithm finds the fraction of links at which a percolating backbone of spins first forms across the lattice and uses this measurement to make successively better approximations to the critical temperature at each monte carlo step . in detail , here s how it goes :    1 .",
    "we choose some starting configuration of the spins .",
    "it does nt matter what choice we make , but it could , for example , be the @xmath129 state in which all spins are aligned .",
    "we add links at random between pairs of similarly - oriented nearest - neighbour sites until one of the clusters formed reaches percolation .",
    "once the links are made , we flip each cluster on the lattice separately with probability @xmath88 , just as we do in the normal swendsen - wang algorithm .",
    "then the whole procedure is repeated from step  ( ii ) again .",
    "why does this algorithm work ?",
    "well , for any given configuration of the lattice , the maximum number of links which can be made between spins is equal to the number of pairs of similarly - oriented nearest - neighbour spins .",
    "let us call this number @xmath21",
    ". the number of links @xmath17 needed to achieve the percolation of one cluster will normally be smaller than this maximum value .",
    "a given step of the algorithm is equivalent to a single step of the swendsen - wang algorithm with @xmath130 .",
    "( the number of links @xmath17 required for percolation is fairly constant , and most of the variation in @xmath13 comes from @xmath21 . ) substituting this value into equation   and rearranging , we can then calculate the effective temperature of the monte carlo step : @xmath131    consider what happens if the system is below the critical temperature . in this case , the spins are more likely to be aligned with their neighbours than they are at the critical temperature , and @xmath21 is therefore large .",
    "this gives us a value of @xmath132 which is higher than @xmath35 .",
    "in other words , when the system is below the critical temperature , the algorithm automatically chooses a temperature @xmath133 for its swendsen - wang procedure .",
    "conversely , if the system is at a temperature above the critical point , neighbouring spins are less likely to be aligned with one another than they are at @xmath35 , giving a lower value of @xmath21 and a correspondingly lower value of @xmath132 .",
    "so , for a state of the system above the critical temperature , the algorithm automatically chooses a temperature @xmath134 for its swendsen - wang procedure .",
    "the algorithm therefore has a kind of negative feedback built into it , which always drives the system towards the critical point , and when it finally reaches @xmath35 it will stay there , performing swendsen - wang monte carlo steps at the critical temperature for the rest of the simulation .",
    "note that we do not need to know what the critical temperature is for the algorithm to work .",
    "it finds @xmath35 all on its own , and for that reason the algorithm is a good way of measuring @xmath35 . the same feedback mechanism also drives the algorithm towards @xmath35 faster than simply performing a string of swendsen - wang monte carlo steps exactly at @xmath35 .",
    "this point is discussed in more detail below .",
    "before we review the result of simulations using the invaded cluster algorithm , let us briefly examine its implementation . by and large ,",
    "the algorithm is straightforward to program , but some subtlety arises when we get to the part about testing for a percolating cluster . since we need to do this every time a link is added to the lattice , it is important that we find an efficient test .",
    "there are various criteria one can apply to test for percolation on a finite lattice . here",
    "we use one of the simplest : we assume a cluster to be percolating when it `` wraps around '' the periodic boundary conditions on the lattice . in other words the cluster percolates when there is a path from any site leading across the lattice and back to the same site which has an integrated displacement which is non - zero .",
    "this test can be efficiently implemented as follows .",
    "all sites belonging to the same cluster are organized into a tree structure in which there is one chosen `` reference site '' .",
    "all the other sites in the cluster are arranged in a hierarchy below this reference site .",
    "the immediate daughter sites of the reference site possess vectors which give their position relative to the reference site .",
    "other sites are arranged as grand - daughter sites , with vectors giving their position relative to one of the daughter sites , and so on until every site in the cluster is accounted for .",
    "the vector linking any site to the reference site can be calculated by simply ascending the tree from bottom to top , adding the vectors at each level until the reference site is reached .",
    "the particular way in which the sites in a cluster are arranged on the tree is dictated by the way in which the cluster grows . at the start of each monte",
    "carlo step no links exist and each site is a reference site . each time",
    "a link is added we must either join together two separate clusters to make one larger cluster , or join together two sites which already belong to the same cluster . since our criterion for percolation is the appearance of a cluster which wraps around the boundary conditions , it follows that percolation can only appear on moves of the latter type . in other words",
    "we need only check for percolation when we add a link joining two sites which are already members of the same cluster .",
    "the procedure is therefore as follows .",
    "each time we add a link , we use the tree structures to determine the reference site of each of the two sites involved .",
    "these reference sites tell us which clusters the sites belong to . when a link is made between two sites belonging to different clusters , we have to combine the two clusters into one , which we do by making one of the reference sites the daughter of the other .",
    "when we make a link between sites in the same cluster , we do nt need to combine two clusters , but we do need to check for percolation . to do this",
    "we take the lattice vectors from each of the two sites to the reference site of the cluster and subtract them .",
    "normally , this procedure should result in a difference vector of length one  the distance between two nearest - neighbour spins .",
    "however , if the addition of a link between these two spins causes the cluster to wrap around the boundary conditions , the difference of the two vectors will instead be a unit vector _ plus _ a vector of length the dimension of the lattice . in this case",
    "we declare the cluster to the percolating , and stop adding links .    [ ictc ]    the results for the invaded cluster algorithm are impressive .",
    "machta  ( 1995 ) found equilibration times 20 or more times faster than for the swendsen - wang algorithm at @xmath35 for the two- and three - dimensional ising systems they examined , and of course , the invaded cluster algorithm allowed them to measure the value of @xmath35 , which is not directly possible with the normal swendsen - wang algorithm .",
    "figure   shows their results for the critical temperature of a set of 2d ising systems of different sizes . because there is always a finite chance of producing a percolating cluster on a finite lattice with any value of @xmath13 no matter how small ( as long as it s not zero ) , the average estimate of @xmath35 which the algorithm makes on a finite system tends to be a little high .",
    "however , we can easily extrapolate to the limit @xmath135 using finite - size scaling . in this case",
    "we do this by plotting the measured @xmath35 as a function of @xmath136 and extrapolating to the @xmath137 axis .",
    "the result is @xmath138 , which compares favourably with the known exact result of @xmath139 , especially given the small amount of cpu time taken by the simulation .",
    "( the runs were @xmath140 monte carlo steps , for each system size . )",
    "the invaded cluster algorithm can also be used to measure other quantities at the critical temperature ",
    "magnetization for example , or internal energy . however , a word of caution is in order here . for lattices of finite size , which of course includes all the lattices in our monte carlo simulations ,",
    "the invaded cluster algorithm does not sample the boltzmann distribution exactly .",
    "in particular , the fluctuations in quantities measured using the algorithm are different from those you would get in the boltzmann distribution . to see this , consider what happens once the algorithm has equilibrated to the critical temperature . at this point , as we argued before , it should stop changing the temperature and just become equivalent to the swendsen - wang algorithm at @xmath35 , which , as we know , certainly samples the boltzmann distribution correctly . however , in actual fact , because the lattice is finite , statistical variations in the order in which we generate the links on the lattice will give rise to variations in the temperature @xmath38 of successive steps in the simulation . the negative feedback effect that we described above will ensure that @xmath38 always remains close to @xmath35 , but the size of fluctuations is very sensitive to small changes in temperature near @xmath35 and as a result the measured fluctuations are not a good approximation to those of the true boltzmann distribution .",
    "thus the invaded cluster algorithm is not suitable for measuring , for instance , the magnetic susceptibility @xmath60 or the specific heat @xmath141 of the ising model at @xmath35 , both of which are determined by measuring fluctuations . on the other hand , one could use the algorithm to determine the value of @xmath35 , and then use the normal wolff or swendsen - wang algorithm to perform a simulation at that temperature to measure @xmath60 or @xmath141 .",
    "cluster monte carlo algorithms are not only applicable to the ising model .",
    "they also work for other types of spin models . in this section",
    "we consider how they can be modified to work with potts models and continuous spin models .",
    "potts models suffer from critical slowing down in just the same way as the ising model .",
    "all of the algorithms discussed in this chapter can be generalized to potts models . here",
    "we discuss the example of the wolff algorithm , whose appropriate generalization is as follows :    1 .",
    "choose a seed spin at random from the lattice .",
    "2 .   look in turn at each of the neighbours of that spin .",
    "if they have the same value as the seed spin , add them to the cluster with probability @xmath142 .",
    "( note that the 2 has vanished from the exponent ; the 2-state potts model , for example , is equivalent to the ising model except for a factor two in the interaction constant @xmath29 . )",
    "3 .   for each spin that was added in the last step ,",
    "examine each of _ its _ neighbours to find which ones , if any , have the same value and add each of them to the cluster with the same probability @xmath13 .",
    "( as with the ising model , we notice that some of the neighbours may already be members of the cluster , in which case you do nt have to consider adding them again .",
    "also , some of the spins may have been considered for addition before , as neighbours of other spins in the cluster , but rejected . in this case , they get another chance to be added to the cluster on this step . )",
    "this step is repeated as many times as necessary until there are no spins left in the cluster whose neighbours have not been considered for inclusion in the cluster .",
    "4 .   choose at random a new value for the spins in the cluster , different from the present value , and set all the spins to that new value .",
    "the proof that this algorithm satisfies detailed balance is exactly the same as it was for the ising model .",
    "if we consider two states @xmath15 and @xmath16 of the system which differ by the changing of just one cluster , then the ratio @xmath143 of the selection probabilities for the moves between these states depends only the number of bonds broken @xmath17 and the number made @xmath21 around the edges of the cluster .",
    "this gives us an equation of detailed balance which reads @xmath144 just as in the ising case .",
    "the change in energy is also given by the same expression as before , except for a factor of two : @xmath145 and so the ratio of the acceptance ratios @xmath25 and @xmath26 for the two moves is @xmath146^{n - m}. \\label{contacc}\\ ] ] for the choice of @xmath13 given above , this is just equal to one .",
    "equation   is then satisfied by making the acceptance ratios equal to 1 , and with this choice the algorithm satisfies detailed balance .    as in the case of the ising model , the wolff algorithm gives an impressive improvement in performance near to the critical temperature .",
    "arguments similar to those of bausch  ( 1981 ) indicate that the dynamic exponent of the metropolis algorithm for a potts model should have a lower bound of 2 .",
    "by contrast , baillie and coddington  ( 1991 ) have measured a dynamic exponent of @xmath147 for the wolff algorithm in the 2d , @xmath148 case . as before , the single - spin - flip algorithms come into their own well away from the critical point , because critical slowing down ceases to be a problem and the relative simplicity of these algorithms over the wolff algorithm tends to give them the edge . ones choice of algorithm should therefore ( as always ) depend on exactly which properties of the model one wants to investigate , but the wolff algorithm is definitely a good choice for examining critical properties .",
    "the swendsen - wang algorithm can also be generalized for use with potts models in a very simple fashion , as can all of the other algorithms described in section  .",
    "cluster algorithms can also be generalized to models with continuous spins .",
    "let us take the xy model as our example .",
    "a version of the wolff algorithm for this model was given by wolff in his original paper in 1989 .",
    "it also works for similar models in higher dimensions .",
    "the idea is a simple one : one chooses at random a seed spin to start a cluster and a direction vector @xmath149 .",
    "to represent the direction of the vector . in three dimensions",
    "we need to choose both a @xmath150 and a @xmath151 .",
    "@xmath150 is again uniformly distributed between 0 and @xmath152 , and @xmath151 is given by @xmath153 where @xmath126 is random real number uniformly distributed between zero and one . ]",
    "then we treat the components @xmath154 of the spins in that direction roughly in the same way as we did the spins in the ising model .",
    "a neighbour of the seed spin whose component in this direction has the same sign as that of the seed spin can be added to cluster by making a link between it and the seed spin with some probability @xmath13 . if the components point in opposite directions then the spin is not added to the cluster .",
    "when the complete cluster has been built , it is `` flipped '' by reflecting all the spins in the plane perpendicular to @xmath149 .",
    "the only complicating factor is that , in order to satisfy detailed balance , the expression for @xmath13 has to depend on the values of the spins which are joined by links thus : @xmath155.\\ ] ] readers may like to demonstrate for themselves that , with this choice , the ratio of the selection probabilities @xmath19 and @xmath23 is equal to @xmath156 , where @xmath157 is the change in energy in going from a state @xmath15 to a state @xmath16 by flipping a single cluster . thus , detailed balance is obeyed as in the ising case by an algorithm for which the acceptance probability for the cluster flip is 1 .",
    "this algorithm has been used , for example , by gottlob and hasenbusch  ( 1993 ) to perform extensive studies of the critical properties of the heisenberg model .",
    "similar generalizations to continuous spins are possible for all the algorithms discussed in section  .",
    "we have discussed a number of recently developed cluster algorithms for the simulation of classical spin systems , including : the wolff algorithm , which flips a single cluster of spins at each update step and almost completely eliminates critical slowing down from the simulation of the ising model ; the swendsen - wang algorithm , which updates the entire lattice at each step and has performance similar to , though not quite as good as the wolff algorithm ; neidermayer s algorithm , a variation of the wolff algorithm which allows one to tune the size of clusters flipped ; the limited cluster algorithm , another technique for controlling the sizes of clusters ; multigrid methods , which may be even more efficient close to criticality than the wolff algorithm , although they achieve this at the expense of considerable programming complexity ; and the invaded cluster algorithm , which self - organizes to the critical point of the system studied and permits accurate measurements of @xmath35 to be made with little computational effort .",
    "we have also discussed briefly how algorithms such as these can be generalized to potts models and continuous spin models .",
    "the authors would like to thank eytan domany , jon machta and alan sokal for useful discussions about cluster algorithms , and daniel kandel for supplying a copy of his ph.d . thesis ."
  ],
  "abstract_text": [
    "<S> we describe a number of recently developed cluster - flipping algorithms for the efficient simulation of classical spin models near their critical temperature . </S>",
    "<S> these include the algorithms of wolff , swendsen and wang , and niedermeyer , as well as the limited cluster algorithm , the multigrid methods of kandel and co - workers , and the invaded cluster algorithm . </S>",
    "<S> we describe the application of these algorithms to ising , potts , and continuous spin models .    </S>",
    "<S> # 1([#1 ] ) # 1[#1 ] # 1[#1 ] # 1[#1 ] # 1#1 # 1@xmath0    # 1to    = # 1to    # 1to    # 1to    # 1#2#3#4#5#6#7#8#9to to to    # 1#2#3#4to    # 1#2to    monte carlo simulations of classical spin systems such as the ising model can usually be performed simply and very efficiently using local update algorithms such as the metropolis or heat - bath algorithms . however , in the vicinity of a continuous phase transition , these algorithms display dramatic critical slowing down , making them extremely poor tools for the study of critical phenomena . in the last ten years or so , a number of new monte carlo algorithms making use of non - local update moves have been developed to address this problem . in this chapter </S>",
    "<S> we examine a number of these algorithms in some detail , and discuss their strengths and weaknesses . </S>",
    "<S> the outline of the chapter is as follows . in section  </S>",
    "<S> , we briefly discuss the problem of critical slowing down . in section   </S>",
    "<S> we describe the wolff algorithm , which is probably the most successful of the algorithms developed so far . in section   </S>",
    "<S> we discuss a number of other algorithms , such as the swendsen - wang algorithm , niedermayer s algorithm , and the invaded cluster algorithm . </S>",
    "<S> each of these algorithms is introduced first in the context of the ising model , but in section   we discuss how they may be generalized to systems with many valued spins such as potts models or continuous valued spins such as the classical xy and heisenberg models . in section   </S>",
    "<S> we give our conclusions . </S>"
  ]
}