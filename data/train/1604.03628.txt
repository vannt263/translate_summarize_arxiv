{
  "article_text": [
    "we are witnessing an explosion in visual content .",
    "significant recent advances in machine learning and computer vision , especially via deep neural networks , have relied on supervised learning and availability of copious annotated data .",
    "however , manually labelling data is a time - consuming , laborious , and often expensive process . in order to make better use of available unlabeled images , clustering and/or unsupervised learning is a promising direction .    in this work",
    ", we aim to address image clustering and representation learning on unlabeled images in a unified framework .",
    "it is a natural idea to leverage cluster ids of images as supervisory signals to learn representations and in turn the representations would be beneficial to image clustering . at a high - level view , given a collection of @xmath0 unlabeled images @xmath1 , the global objective function for learning image representations and clusters can be written as : @xmath2 where @xmath3 is a loss function , @xmath4 denotes the cluster ids for all images , and @xmath5 denotes the parameters for representations .",
    "if we hold one in @xmath6 to be fixed , the optimization can be decomposed into two alternating steps :    @xmath7 @xmath8    0.32     0.32     0.32     intuitively , can be cast as a conventional clustering problem based on fixed representations , while is a standard supervised representation learning process .    in this paper , we propose an approach that alternates between the two steps  updating the cluster ids given the current representation parameters and updating the representation parameters given the current clustering result .",
    "specifically , we cluster images using agglomerative clustering@xcite and represent images via activations of a convolutional neural network ( cnn ) .    the reason to choose agglomerative",
    "clustering is three - fold : 1 ) it begins with an over - clustering , which is more reliable in the beginning when a good representation has not yet been learned .",
    "intuitively , clustering with representations from a cnn initialized with random weights are not reliable , but nearest neighbors and over - clusterings are often acceptable ; 2 ) these over - clusterings can be merged as better representations are learned ; 3 ) agglomerative clustering is a recurrent process and can naturally be interpreted in a recurrent framework .",
    "our final algorithm is farily intuitive .",
    "we start with an intial over - clustering , update cnn parameters ( 2b ) using image cluster labels as supervisory signals , then merge clusters ( 2a ) and iterate until we reach a stopping criterion .",
    "an outcome of the proposed framework is illustrated in fig .",
    "[ fig_introduction ] .",
    "initially , there are 1,762 clusters for mnist test set ( 10k samples ) , and the representations ( image intensities ) are not that discriminative . after several iterations",
    ", we obtain 17 clusters and more discriminative representations .",
    "finally , we obtain 10 clusters which are well - separated by the learned representations and interestingly correspond primarily to the groundtruth category labels in the dataset , even though the representation is learnt in an unsupervised manner . to summarize , the major contributions of our work are :    we propose a simple but effective end - to - end learning framework to jointly learn deep representations and image clusters from an unlabeled image set ;    we formulate the joint learning in a recurrent framework , where merging operations of agglomerative clustering",
    "are expressed as a forward pass , and representation learning of cnn as a backward pass ;    we derive _ a single loss function _ to guide agglomerative clustering and deep representation learning , which makes optimization over the two tasks seamless ;    our experimental results show that the proposed framework outperforms previous methods on image clustering and learns deep representations that can be transferred to other tasks and datasets .",
    "* clustering * clustering algorithms can be broadly categorized into hierarchical and partitional approaches @xcite .",
    "agglomerative clustering is a hierarchical clustering algorithm that begins with many small clusters , and then merges clusters gradually @xcite . as for partitional clustering methods ,",
    "the most well - known is k - means @xcite , which minimizes the sum of square errors between data points and their nearest cluster centers .",
    "related ideas form the basis of a number of methods , such as expectation maximization ( em ) @xcite , spectral clustering @xcite , and non - negative matrix factorization ( nmf ) based clustering @xcite .    * deep representation learning * many works use raw image intensity or hand - crafted features @xcite combined with conventional clustering methods .",
    "recently , representations learned using deep neural networks have presented significant improvements over hand - designed features on many computer vision tasks , such as image classification @xcite , object detection @xcite , etc .",
    "however , these approaches rely on supervised learning with large amounts of labeled data to learn rich representations .",
    "a number of works have focused on learning representations from unlabled image data .",
    "one class of approaches cater to reconstruction tasks , such as auto - encoders @xcite , deep belief networks ( dbn ) @xcite , etc .",
    "another group of techniques learn discriminative representations after fabricating supervisory signals for images , and then finetune them supervisedly for downstream applications @xcite . unlike our approach ,",
    "the fabricated supervisory signal in these previous works is not updated during representation learning .",
    "* combination * a number of works have explored combining image clustering with representation learning . in @xcite , the authors proposed to learn a non - linear embedding of the undirected affinity graph using stacked autoencoder , and then ran k - means in the embedding space to obtain clusters . in @xcite , a deep semi - nmf model was used to factorize the input into multiple stacking factors which are initialized and updated layer by layer . using the representations on the top layer ,",
    "k - means was implemented to get the final results .",
    "unlike our work , they do not jointly optimize for the representation learning and clustering .    to connect image clustering and representation learning more closely , @xcite conducted image clustering and codebook learning iteratively .",
    "however , they learned codebook over sift feature @xcite , and did not learn deep representations . instead of using hand - crafted features ,",
    "chen @xcite used dbn to learn representations , and then conducted a nonparametric maximum margin clustering upon the outputs of dbn .    afterwards , they fine - tuned the top layer of dbn based on clustering results . a more recent",
    "work on jointly optimizing two tasks is found in @xcite , where the authors trained a task - specific deep architecture for clustering .",
    "the deep architecture is composed of sparse coding modules which can be jointly trained through back propagation from a cluster - oriented loss .",
    "however , they used sparse coding to extract representations for images , while we use a cnn .",
    "instead of fixing the number of clusters to be the number of categories and predicted labels based on softmax outputs , we predict the labels using agglomerative clustering based on the learned representations . in our experiments",
    "we show that our approach outperforms @xcite .",
    "we denote an image set with @xmath0 images by @xmath9 .",
    "the cluster labels for this image set are @xmath10 .",
    "@xmath5 are the cnn parameters , based on which we obtain deep representations @xmath11 from @xmath12 .",
    "given the predicted image cluster labels , we organize them into @xmath13 clusters @xmath14 , where @xmath15 .",
    "@xmath16 are the @xmath17 nearest neighbours of @xmath18 , and @xmath19 is the set of @xmath20 nearest neighbour clusters of @xmath21 . for convenience ,",
    "we sort clusters in @xmath19 in descending order of affinity with @xmath21 so that the nearest neighbour @xmath22 is the first entry @xmath23 $ ] . here",
    ", @xmath24 is a function to measure the affinity ( or similarity ) between two clusters .",
    "we add a superscript @xmath25 to \\{@xmath5 , @xmath26 , @xmath4 , @xmath27 } to refer to their states at timestep @xmath25 .",
    "we use @xmath28 to denote the sequence @xmath29 with @xmath30 timesteps .      as background",
    ", we first briefly describe conventional agglomerative clustering @xcite .",
    "the core idea in agglomerative clustering is to merge two clusters at each step until some stopping conditions .",
    "mathematically , it tries to find two clusters @xmath31 and @xmath32 by @xmath33    there are many methods to compute the affinity between two clusters @xcite .",
    "more details can be found in @xcite .",
    "we now describe how the affinity is measured by @xmath24 in our approach .",
    "first , we build a directed graph @xmath34 , where @xmath35 is the set of vertices corresponding to deep representations @xmath26 for @xmath12 , and @xmath36 is the set of edges connecting vertices .",
    "we define an affinity matrix @xmath37 corresponding to the edge set . the weight from vertex @xmath18 to @xmath38 is defined by @xmath39 where @xmath40 .",
    "this way to build up a directed graph can be found in many previous works such as @xcite . here ,",
    "@xmath41 and @xmath17 are two predefined parameters ( their values are listed in table [ tb_ac_param ] ) . after constructing a directed graph for samples",
    ", we then adopt the graph degree linkage in @xcite to measure the affinity between cluster @xmath21 and @xmath42 , denoted by @xmath43 .",
    "our key insight is that agglomerative clustering can be interpreted as a recurrent process in the sense that it merges clusters over multiple timesteps .",
    "based on this insight , we propose a recurrent framework to combine the image clustering and representation learning processes .    as shown in fig .",
    "[ fig_framework ] , at the timestep @xmath25 , images @xmath12 are first fed into the cnn to get representations @xmath44 and then used in conjunction with previous hidden state @xmath45 to predict current hidden state @xmath46 , i.e , the image cluster labels at timestep @xmath25 . in our context , the output at timestep @xmath25 is @xmath47 .",
    "hence , at timestep @xmath25    @xmath48    @xmath49    @xmath50    where @xmath51 is a function to extract deep representations @xmath44 for input @xmath12 using the cnn parameterized by @xmath52 , and @xmath53 is a merging process for generating @xmath54 based on @xmath44 and @xmath55 .    in a typical recurrent neural network",
    ", one would unroll all timesteps at each training iteration . in our case , that would involve performing agglomerative clustering until we obtain the desired number of clusters , and then update the cnn parameters by back - propagation .    in this work",
    ", we introduce a _ partial unrolling _ strategy , , we split the overall @xmath30 timesteps into multiple periods , and unroll one period at a time .",
    "the intuitive reason we unroll partially is that the representation of the cnn at the beginning is not reliable .",
    "we need to update cnn parameters to obtain more discriminative representations for the following merging processes . in each period",
    ", we merge a number of clusters and update cnn parameters for a fixed number of iterations at the end of the period .",
    "an extreme case would be one timestep per period , but it involves updating the cnn parameters too frequently and is thus time - consuming .",
    "therefore , the number of timesteps per period ( and thus the number of clusters merged per period ) is determined by a parameter in our approach .",
    "we elaborate on this more in sec .",
    "[ sec_optimization ] .      in our recurrent framework",
    ", we accumulate the losses from all timesteps , which is formulated as @xmath56    here , @xmath57 takes each image as a cluster . at timestep @xmath25 , we find two clusters to merge given @xmath58 .",
    "in conventional agglomerative clustering , the two clusters are determined by finding the maximal affinity over all pairs of clusters . in this paper",
    ", we introduce a criterion that considers not only the affinity between two clusters but also the local structure surrounding the clusters .",
    "assume from @xmath58 to @xmath59 , we merged a cluster @xmath60 and its nearest neighbour .",
    "then the loss at timestep @xmath25 is a combination of negative affinities , that is ,    @xmath61 ) &   &   &   & & & & & & & & & & & \\end{aligned } \\label{eq_similarity_term_a_timestep_t}\\ ] ]    @xmath62 ) - \\bm{\\mathcal{a}}(\\mathcal{c}^t_i ,   \\mathcal{n}_{\\mathcal{c}^t_i}^{k_c}[k])\\right ) \\end{aligned } \\label{eq_similarity_term_b_timestep_t}\\ ] ]    where @xmath63 weighs and .",
    "note that @xmath59 , @xmath58 and @xmath52 are not explicitly presented at the right side , but they determine the loss via the image cluster labels and affinities among clusters . on the right side of the above equation , there are two terms : 1 ) measures the affinity between cluster @xmath21 and its nearest neighbour , which follows conventional agglomerative clustering ; 2 ) measures the difference between affinity of @xmath21 to its nearest neighbour cluster and affinities of @xmath21 to its other neighbour clusters .",
    "this term takes the local structure into account .",
    "see sec .",
    "[ sec_forward_pass ] for detailed explanation .",
    "it is hard to simultaneously derive the optimal @xmath29 and @xmath64 that minimize the overall loss in eq .  .",
    "as aforementioned , we optimize iteratively in a recurrent process .",
    "we divide @xmath30 timesteps into @xmath65 partially unrolled periods . in each period , we fix @xmath5 and search optimal @xmath4 in the forward pass , and then in the backward pass we derive optimal @xmath5 given the optimal @xmath4 .",
    "details will be explained in the following sections .",
    "in forward pass of the @xmath66-th ( @xmath67 ) partially unrolled period , we update the cluster labels with @xmath5 fixed to @xmath68 , and the overall loss in period @xmath66 is @xmath69 where @xmath70 is the sequence of image labels in period @xmath66 , and @xmath71 $ ] is the corresponding timesteps in period @xmath66 .    for optimization , we follow a greedy search similar to conventional agglomerative clustering . starting from the time step @xmath72",
    ", it finds one cluster and its nearest neighbour to merge so that @xmath73 is minimized over all possible cluster pairs .    in fig .",
    "[ fig_approach_toyexample ] , we present a toy example to explain the reason why we employ the term .",
    "as shown , it is often the case that the clusters are densely populated in some regions while sparse in some other regions . in conventional agglomerative clustering",
    ", it will choose two clusters with largest affinity ( or smallest loss ) at each time no mater where the clusters are located .",
    "in this specific case , it will choose cluster @xmath32 and its nearest neighbour to merge .",
    "in contrast , as shown in fig .  [ fig_approach_toyexample](b ) , our algorithm by adding will find cluster @xmath74 , because it is not only close to it nearest neighbour , but also relatively far away from its other neighbours , i.e. , the local structure is considered around one cluster .",
    "another merit of introducing is that it will allow us to write the loss in terms of triplets as explained next .",
    "0.48     0.48     in forward pass of the @xmath66-th partially unrolled period , we have merged a number of clusters .",
    "let the sequence of optimal image cluster labels be given by @xmath75 , and clusters merged in forward pass are denoted by @xmath76\\}$ ] , @xmath77 . in the backward pass ,",
    "we aim to derive the optimal @xmath5 to minimize the losses generated in forward pass .",
    "because the clustering in current period is conditioned on the clustering results of all previous periods , we accumulate the losses of all @xmath66 periods    , i.e. , @xmath78 minimizing w.r.t @xmath5 leads to representation learning on @xmath12 supervised by @xmath79 or @xmath80 . based on and , the loss in eq .",
    "[ eq_loss_overall_wst_theta ] is reformulated to @xmath81 ) - \\bm{\\mathcal{a}}(\\mathcal{c}^t _ * , \\mathcal{n}_{\\mathcal{c}^t_*}^{k_c}[k])\\right ) \\end{aligned } \\label{eq_loss_triplet_time_step}\\ ] ] where @xmath82 .",
    "is a loss defined on clusters of points , which needs the entire dataset to estimate , making it difficult to use batch - based optimization .",
    "however , we show that this loss can be approximated by a sample - based loss , enabling us to compute unbiased estimators for the gradients using batch - statistics .",
    "the intuition behind reformulation of the loss is that agglomerative clustering starts with each datapoint as a cluster , and clusters at a higher level in the hierarchy are formed by merging lower level clusters .",
    "thus , affinities between clusters can be expressed in terms of affinities between datapoints .",
    "we show in the supplement that the loss in can be approximately reformulated as @xmath83 where @xmath84 is a weight whose value depends on @xmath85 and how clusters are merged during the forward pass .",
    "@xmath18 and @xmath38 are from the same cluster , while @xmath86 is from the neighbouring clusters , and their cluster labels are merely determined by the final clustering result @xmath87 . to further simplify the optimization , we instead search @xmath86 in at most @xmath20 neighbour samples of @xmath88 from other clusters in a training batch . hence , the batch - wise optimization can be performed using conventional stochastic gradient descent method .",
    "note that such triplet losses have appeared in other works @xcite . because it is associated with a weight",
    ", we call the weighted triplet loss",
    ".       + @xmath12 : = collection of image data ; + @xmath89 : = target number of clusters ; +   + @xmath90 : = final image labels and cnn parameters ; + @xmath91 ; @xmath92 initialize @xmath5 and @xmath4 update @xmath59 to @xmath93 by merging two clusters @xmath94 @xmath95 cluster number reaches @xmath96 @xmath97 ; @xmath98    [ alg_optimization ]    given an image dataset with @xmath0 samples , we assume the number of desired clusters @xmath96 is given to us as is standard in clustering",
    ". then we can build up a recurrent process with @xmath99 timesteps , starting by regarding each sample as a cluster .",
    "however , such initialization makes the optimization time - consuming , especially when datasets contain a large number of samples . to address this problem",
    ", we can first run a fast clustering algorithm to get the initial clusters . here , we adopt the initialization algorithm proposed in @xcite for fair comparison with their experiment results .",
    "note that other kind of initializations can also be used , e.g. k - means .",
    "based on the algorithm in @xcite , we obtain a number of clusters which contain a few samples for each ( average is about 4 in our experiments ) .",
    "given these initial clusters , our optimization algorithm learns deep representations and clusters .",
    "the algorithm is outlined in alg .",
    "[ alg_optimization ] . in each partially unrolled period , we perform forward and backward passes to update @xmath4 and @xmath5 , respectively . specifically , in the forward pass , we merge two clusters at each timestep . in the backward pass ,",
    "we run about 20 epochs to update @xmath5 , and the affinity matrix @xmath100 is also updated based on the new representation .",
    "the duration of the @xmath66-th period is @xmath101 timesteps , where @xmath102 is the number of clusters at the beginning of current period , and @xmath103 is a parameter called _ unrolling rate _ to control the number of timesteps .",
    "the less @xmath103 is , the more frequently we update @xmath5 .",
    "we compare our approach with 12 clustering algorithms , including k - means @xcite , njw spectral clustering ( sc - njw ) @xcite , self - tuning spectral clustering ( sc - st)@xcite , large - scale spectral clustering ( sc - ls ) @xcite , agglomerative clustering with average linkage ( ac - link)@xcite , zeta function based agglomerative clustering ( ac - zell ) @xcite , graph degree linkage - based agglomerative clustering ( ac - gdl ) @xcite , agglomerative clustering via path integral ( ac - pic ) @xcite , normalized cuts ( n - cuts ) @xcite , locality preserving non - negative matrix factorization ( nmf - lp ) @xcite , nmf with deep model ( nmf - d ) @xcite , task - specific clustering with deep model ( tsc - d ) @xcite .",
    "we show the first three principle components of learned representations in fig .",
    "[ fig_pca_display_5 ] and fig .",
    "[ fig_pca_display_4 ] at different stages . for comparison , we show the image intensities at the first column .",
    "we use different colors for representing different clusters that we predict during the algorithm . at the bottom of each plot",
    ", we give the number of clusters at the corresponding stage . at the final stage ,",
    "the number of cluster is same to the number of categories in the dataset . after a number of iterations",
    ", we can learn more discriminative representations for the datasets , and thus facilitate more precise clustering results ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a recurrent framework for * * j**oint * * u**nsupervised * * le**arning ( * jule * ) of deep representations and image clusters . in our framework , </S>",
    "<S> successive operations in a clustering algorithm are expressed as _ steps in a recurrent process _ , stacked on top of representations output by a convolutional neural network ( cnn ) . during training , </S>",
    "<S> image clusters and representations are updated jointly : image clustering is conducted in the forward pass , while representation learning in the backward pass . </S>",
    "<S> our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning . by integrating two processes into a single model with a unified weighted triplet loss and optimizing it </S>",
    "<S> end - to - end , we can obtain not only more powerful representations , but also more precise image clusters . </S>",
    "<S> extensive experiments show that our method outperforms the state - of - the - art on image clustering across a variety of image datasets . </S>",
    "<S> moreover , the learned representations generalize well when transferred to other tasks . </S>",
    "<S> the source code can be downloaded from https://github.com/jwyang/joint-unsupervised-learning . </S>"
  ]
}