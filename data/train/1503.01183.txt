{
  "article_text": [
    "clustering is an unsupervised technique used to find underlying structure in a dataset by grouping data points into subsets that are as homogeneous as possible .",
    "clustering has many applications in a wide range of fields .",
    "no list of references can be complete , however , three important recent references are @xcite , @xcite and @xcite",
    ".    arguably , there are three main classes of clustering algorithm : centroid - based , hierarchical , and partitional .",
    "centroid - based refers to @xmath0-means and its variants .",
    "hierarchical comes in two main forms , divisive and agglomerative .",
    "partitional comes in three main forms graph - theoretic , spectral , and model - based . because the scope of clustering problems is so big , all of these procedures have limitations .",
    "so , each major class of clustering procedures has its strengths and weakness even if , in some cases , these are not mapped out very precisely .",
    "the justification for the new method presented here is that it combines two classes of methods ( centroid - based and agglomerative hierarchical ) with a careful treatment of influential data points and ( i ) is not limited by convexity or ( ii ) as dependent on subjective choices of quantities such as dissimilarities .",
    "that is , we combine several clustering techniques and principles in sequence so that one part of the technique may correct weaknesses in other parts giving a uniform improvement  not necesarily decisively better than other methods in particular cases , but rarely meaningfully outperformed .",
    "in particular , we have not found any examples in which our clustering method is outperformed to any meaningful extent .",
    "the examples presented here dramatize this since most of them are very diffucult for any clustering method .",
    "one consequence of this is that our procedure is well designed for non - convex clusterings as well convex ones .",
    "a further benefit of our clustering method is that we can give formal conditions ensuring that the clustering will be correct for special cases .",
    "that is , we prove a theorem ensuring that the basal sets cover the regions in a clustering problem , in the limit of large sample size and use this to establish a corollary ensuring that the final clustering from our method , at least in simple cases , will be correct .",
    "we also give formal results ensuring that the conditions of our main theorem can be satisfied in some simple but general cases . to the best of our knowledge ,",
    "there are no techniques , except for @xmath0-means , for which theoretical results such as ours can be established .    to fix notation",
    ", we assume @xmath9 independent and identical ( iid ) outcomes @xmath10 of a random variable @xmath11 .",
    "the @xmath12 s are assumed @xmath13-dimensional and written as @xmath14 when needed .",
    "we denote a clustering of size @xmath0 by @xmath15 ; effectively we assume that for each @xmath0 only one clustering will be generated .    for a given @xmath0",
    ", we start by drawing a random @xmath16 , @xmath17 from a distribution that ensures a variety of reasonable clustering sizes will be searched .",
    "then , the generic steps are as follows .    1",
    ".   _ hybrid clustering _ : create @xmath1 clusters by @xmath0-means . then use single linkage ( sl ) clustering to take unions of the @xmath1 cluster to get clusterings of size @xmath16 .",
    "2 .   _ stabilization _ : repeat stage one @xmath18 times ; the result is @xmath18 clusterings with sizes @xmath19 . from these clusterings ,",
    "form a pooled @xmath20 membership matrix @xmath21 .",
    "since each row of @xmath21 corresponds to an @xmath12 and is a vector of zeros and ones of length @xmath22 , the hamming distance @xmath23 between any two rows can be found and is between one and @xmath24 .",
    "these hamming distances give a dissimilarity so that sl clustering can again be applied .",
    "3 .   _ choosing a clustering _ : use a ` grow - and - prune ' approach on the dendrogram from stage 2 . cut the dendrogram at some dis - similarity value smaller than @xmath25 , the value of the dis - similarity that gives @xmath0 clusters .",
    "the @xmath3 clusters are then merged to form @xmath0 clusters after ignoring any clusters that are too small .",
    "the last stage involves possibly two reclusterings : one to merge the larger clusters down to @xmath0 clusters and a second to merge the small clusters into these @xmath0 clusters .",
    "the definition of small clusters requires the use of a cutoff value @xmath26 , here taken to be 0.05 ; details are in sec .",
    "[ presentation ] .    in step 1 ) , there is a range of choices for dis - similarity to be used in the sl clustering .",
    "the usual dis - similarity , namely , defining the minimum distance between two sets the be the shortest path length connecting them , is one valid choice . as seen below , however , it is most effective when the data are generated from a probability measure @xmath27 that has disjoint closed components each the closure of an open set .",
    "when the components of @xmath27 are not disjoint , we have found it advantageous to use a robustified form of the minimum distance between two sets , namely the 20th percentile of the distances between the points in the two sets .",
    "this is discussed further in subsec .",
    "[ slmerging1 ]    there are precedents for the kind of hybrid clustering described in stages 1 ) and 2 ) that combines two or more distinct clustering techniques .",
    "perhaps the closest is @xcite their central idea is to create many clusterings of different sizes ( by @xmath0-means ) that can be pooled via a ` co - association matrix ' that weights points in each clustering according their membership .",
    "this matrix can then be modified ( take one minus each entry ) to give a dis - similarity so that single - linkage clustering can be used to give a final clustering . @xcite",
    "refer to this as evidence accumulation clustering ( eac ) because they are pooling information over a range of clusterings .",
    "eac differs meaningfully from our technique in three ways .",
    "first , in our technique we choose a single @xmath1 while eac uses a range of cluster sizes .",
    "second , we ensemble ( and hence stabilize ) directly by membership in terms of hamming distance whereas eac ensembles by a co - association .",
    "third , our procdure uses an extra step of growing and pruning a dendrogram ( see step 5 in algoirthm # 1 ) that is akin to an optimization over ` main ' clusters .",
    "our ` fine - tuning ' of their technique seems to give better results .",
    "another technique that is conceptually similar to ours is due to chipman and tibshirani @xcite , hereafter ct .",
    "first , in a ` bottom - up stage ' , small sets of points that are not to be separated are replaced by their centroids .",
    "then , in a ` top - down stage ' the remaining points are clustered divisively to give big clusters . then , the bottom up and top - down stages are reconciled to give a final clustering .",
    "our proposed technique differs from ct in four key ways .",
    "first , we use @xmath0-means in place of ct s ` mutual clusters ' .",
    "second , we use single linkage where ct uses average linkage .",
    "third , our technique has a stabilization stage .",
    "fourth , our technique uses a ` grow and prune ' strategy , unlike ct .",
    "so , it is unclear how well ct performs when the true clusters are non - convex .",
    "a third technique , conceptually related to ours but nevertheless very different , is due to karypis et al .",
    "( @xcite ) .",
    "this technique , often called chameleon , rests on a graph theoretic analysis of the clustering problem and uses two passes over the data .",
    "the first is a graph partitioning based algorithm to divide the data set into a collection of small clusters .",
    "the second pass is an agglomerative hierarchical clustering based on connectivity ( a graph - theoretic concept ) to combine these clusters .",
    "our method differs from @xcite in four key ways .",
    "first , we use @xmath0-means instead of graph partitioning .",
    "second , we simply use single linkage whereas @xcite combines small clusters based on both closeness and relative interconnectivity .",
    "third , our technique has a stabilization stage to manage cluster boundary uncertainty .",
    "fourth , our technique explicitly uses a ` grow and prune ' strategy permitting a ` look ahead ' to more clusters than necessary .",
    "by contrast , chameleon has an elaborate optimization . on the other hand",
    ", both can find non - convex clusters .    to the best of our knowledge ,",
    "the earliest explicit proposal for hybrid methods is in @xcite who observed that using @xmath0-means with @xmath0 too large and single linkage may enable a technique to find nonconvex clusters .",
    "in addition to proposing a new hybrid clustering technique ( algorithm # 1 ) we present a way to estimate the correct value @xmath5 of @xmath0 in algorithm # 2 .",
    "essentially , we combine the first three steps of algorithm # 1 with a modification of @xcite .    the rest of this paper is organized as follow . in sec .",
    "[ presentation ] we present our two algorithms for clustering and estimating @xmath5 . in sec .",
    "[ justification ] we provide justifications for some of the steps in our algorithms . for the steps where we are unable to provide theory",
    ", we provide methodological interpretations as a motivation for their use . in sec .",
    "[ comparisons ] we present our numerical comparisons . our concluding remarks are in sec . [ conclusions ] .",
    "we begin with algorithm # 1 that formalizes our generation of clusterings .",
    "it has five steps and five inputs : the number @xmath0 of clusters to be in the final clustering , a number @xmath28 to be the largest number of clusters that we would consider reasonable , a number @xmath18 of iterations of our initial hybrid clustering technique , a number @xmath2 of smaller clusters that will be concatenated to larger clusters , and a value @xmath26 to serve as a cutoff for the size of a cluster as measured by the proportion of how many of the @xmath1 clusters had to be combined to create it . in practice , setting @xmath29 worked reasonably well ; however , @xmath30 is an arbitrary choice and we found that adding a layer of variability by choosing @xmath1 according to a @xmath31 $ ] gave improved results .",
    "separately , we also found that larger values of @xmath28 seemed to require larger values of @xmath18 to get good results .",
    "we address the choice of @xmath18 and @xmath28 later in sec .",
    "[ comparisons ] . in our work here , we merely set @xmath32 .",
    "this ensured that we got at least @xmath0 clusters in our examples .",
    "loosely , the more outliers or clusters there are , the smaller one should choose @xmath26 .",
    "so , effectively , given algorithm # 1 , only @xmath0 must be specified .",
    "the specification of @xmath0 is done separately in algorithm # 2 .",
    "we begin with our clustering algorithm given in the column to the right",
    "given @xmath0 , start by drawing a value of @xmath1 and then drawing a value of @xmath33 where @xmath34 , for @xmath35 . for each @xmath16 , do the following with randomly generated initial conditions to obtain @xmath36 : * use standard @xmath0-means clustering ( or any partitional technique ) to generate a clustering of size @xmath1 ` basal ' clusters . * next , use single linkage clustering ( or any agglomerative technique ) to merge the @xmath1 basal clusters to get a clustering @xmath37 .",
    "2 .   for @xmath38 ,",
    "let @xmath39 be the @xmath40 membership matrix with entries @xmath41 doing the same for the rest of the @xmath42 s generates membership matrices @xmath43 for clusterings @xmath44 , respectively .",
    "concatenating @xmath45 s gives the overall membership matrix @xmath46 $ ] .",
    "3 .   from the @xmath20 overall membership matrix",
    "@xmath47 we construct a dissimilarity matrix using hamming distance .",
    "let @xmath48 .",
    "that is , the @xmath49-th and @xmath50-th rows in @xmath47 are of the form @xmath51 and @xmath52 and so give dis - similarities @xmath53 where @xmath54 if @xmath55 and zero otherwise .",
    "so , @xmath56 is the number of entries in @xmath12 and @xmath57 that are different and @xmath58 .",
    "let @xmath59 be the resulting matrix .",
    "4 .   given @xmath60 , use sl clustering to generate a vertical dendrogram with leaves at the bottom and dis - similarity values on the @xmath61-axis .",
    "since @xmath0 is given , it corresponds to a dis - similarity value @xmath25 on the vertical axis , namely , @xmath25 is the maximum dis - similarity associated with @xmath0 clusters .",
    "now , there will be @xmath0 lines or branches on the dendrogram that cross @xmath25 .",
    "let the lengths of these lines from @xmath25 down to the next split be denoted @xmath62 .",
    "cut the dendrogram at @xmath63 and let @xmath3 be the number of clusters at that value .",
    "write @xmath64 with @xmath65 . if @xmath66 , the clustering from step 4 of size @xmath0 is the final clustering . if @xmath67 , write @xmath68 where @xmath69 is the number of clusters in the clustering @xmath70 for which @xmath71 . in the case",
    "that @xmath0 clusters of size at least @xmath26 do not exist , @xmath26 is adjusted downward until @xmath0 such clusters exist .",
    "ignore these @xmath69 clusters and using sl ( under the corresponding submatrix of @xmath60 ) recluster the points in the remaining @xmath72 clusters to reduce them to @xmath0 ` main ' clusters .",
    "then , use sl clustering again to assign the points in the @xmath69 clusters to the @xmath0 ` main ' clusters to give the final clustering of size @xmath0 .    for brevity",
    ", we refer to algorithm # 1 as shc .",
    "in shc , we have specified the use @xmath0-means in the first part of step 1 but left open which dissimilarity to use in step 2 .",
    "this is intentional because we can establish theory for our method that suggests the usual minimum distance dissimilarity is best when the components of @xmath27 are separated ( convex or not ) ; however , a dissimilarity between sets based on 20th percentile of the distance between their points works better when the separation is not clear or entirely absent . in our examples below",
    "we denote these dissimilarities by writing shcm ( minimal ) and shc20 ( 20th percentile ) .",
    "note that the number of clusters @xmath3 is defined internally to the algorithm in step 4 .",
    "the idea is to get a tree that is slightly larger than cutting at @xmath25 , i.e. , to let the algorithm search an extra few steps ahead for good clusters . in step 5 , any extra clusters that are found but not helpful are pruned away .",
    "the intuition behind the choice of @xmath3 is that the level of the dissimilarity it represents identifies the point at which chaining begins to affect the clustering procedure negatively .",
    "algorithm # 1 can serve as the basis for another algorithm to estimate @xmath5 . we add an extra step derived from the method for choosing @xmath0 in @xcite . recall that @xcite considered a set of ` lifetimes ' that were lengths in terms of the dissimilarity .",
    "these were the distances between the values on the vertical axis at which one could cut a dendrogram so as to get a collection of clusters with the property that at least one of the clusters emerges precisely at the value on the vertical axis at which the horizontal line was drawn .",
    "@xcite then cut the dendrogram at the dis - similarity that corresponds to the maximum of these vertical distances to choose the number of clusters .",
    "algorithm # 2 extends this method by using it once , removing some clusters , and then using it again .",
    "our general procedure is given in algorithm # [ alg4 ] , next page .",
    "use steps 1 - 3 from algorithm # 1 to obtain @xmath60 .",
    "form the dendrogram for the data under @xmath60 using sl .",
    "3 .   use the @xcite technique to find the two largest lifetimes .",
    "4 .   for each of the two largest lifetimes ,",
    "cut the dendrogram at that lifetime and examine the size of the clusters .",
    "remove clusters that are both small ( containing less that @xmath26100% of the data ) and split off at or just below @xmath25 .",
    "this gives two sub - dendrograms , one for each lifetime .",
    "5 .   for each of the sub - dendrograms , cut at @xmath25 .",
    "this gives two numbers of clusters .",
    "take the mean of these two numbers of clusters as the estimate of the correct number of clusters .    for brevity",
    ", we refer to algorithm # 2 as ek .",
    "in this section we provide motivation , interpretation , and properties of the steps in the two algorithms we have proposed .",
    "let the probability measure @xmath27 have density @xmath73 and assume that @xmath73 only takes values zero and a single , fixed constant .",
    "the places where @xmath73 assumes a nonzero value are the clusters of @xmath27 .",
    "our first result shows that the support of @xmath73 can be expressed as a disjoint union of small clusters in the limit of large @xmath9 .",
    "let @xmath74 denote the symmetric difference between sets @xmath75 and @xmath18 and for any set @xmath75 , let @xmath76 be the diameter of @xmath75 .",
    "now , given data @xmath77 write @xmath78 to be a clustering of @xmath79 into @xmath0 clusters .",
    "our result is the following .",
    "suppose the following assumptions are satisfied :    1 .",
    "@xmath80 so that @xmath81 in @xmath27-probability as @xmath82 .",
    "2 .   for any @xmath13 , @xmath83 as @xmath84 .",
    "3 .   for each @xmath13 and @xmath0 , @xmath85 .",
    "the support of @xmath73 , @xmath86 , consists of finitely many disjoint open sets with disjoint closures having smooth boundaries .",
    "5 .   the random variable @xmath11 generating @xmath79 is bounded .",
    "then for any fixed @xmath13 , along any sequence of sets @xmath87 with @xmath88 , there is a @xmath89 , the support of @xmath27 , so that @xmath90 as @xmath9 increases first and @xmath0 increases second , at suitable rates .    consider a sequence @xmath91 for which @xmath85 ; this is possible by items 1 ) and 3 ) . by item 2 ) , @xmath92 .",
    "step 1 : for such a sequence , @xmath93    begin by writing @xmath94",
    "since @xmath11 is bounded , term goes to zero as @xmath82 by the dominated convergence theorem since @xmath95 in @xmath27-probability under item 1 ) .    to deal with term ,",
    "write it as @xmath96    since @xmath11 is bounded by @xmath21 , say , the absolute value of term is bounded by    @xmath97    now , by assumption 1 , with probability at least @xmath98 , for any @xmath99 , as @xmath82 we have @xmath100 so , the factor in absolute value bars in can be made less than any pre - assigned positive number , for instance , @xmath101 , giving that can be made arbitrarily small as @xmath82 .",
    "consequently , @xmath102 and step 1 is complete .",
    "step 2 : by item 2 ) , @xmath103 such that @xmath104 .",
    "so , by step 1 , as @xmath82 @xmath105 in @xmath27-probability . now , to prove the theorem , it remains to show @xmath106 .    by way of contradiction , suppose @xmath107 .",
    "then , since @xmath108 is a closed set by item 4 ) , its complement is open and hence @xmath109 so that @xmath110 , where @xmath111 indicates a ball centered at @xmath112 of radius @xmath113 .",
    "however , consider a sequence of sets @xmath87 for some fixed @xmath13 with @xmath114 such a sequence must exist for some @xmath13 by item 3 ) . by item 2 ) , we have that @xmath115 so , @xmath116 such that @xmath117 , @xmath118 and therefore @xmath119 by letting @xmath9 and @xmath0 increase at appropriate rates , a contradiction .",
    "hence , @xmath120 , establishing the theorem .",
    "the utility of the theorem stems mostly from the following corollary .",
    "there exists a @xmath121 so that for @xmath122 , there are @xmath123 for sole @xmath124 with @xmath125 that is , there are rates at which @xmath126 , @xmath84 and @xmath127 , so that in a limiting sense @xmath128    this corollary gives conditions under which the procedure of choosing @xmath0 too large , in @xmath0-means for instance , ensures that the union of the clusters for that @xmath0 very closely approximates the support of @xmath11 , regardless of whether the support is convex or not . 5disjoint open sets .    since assumptions 3 ) , 4 ) and 5 ) are straightforward to assess , we provide sufficient conditions for assumptions 1 ) and 2 ) for the special case of @xmath0-means clustering .",
    "to do this for assumption 1 ) , recall that @xmath0-means uses the euclidean distance to define the dis - similarity @xmath129 for points @xmath130 and @xmath131 .",
    "formally , in the limit of large sample sizes , let @xmath132 be the means of unknown classes @xmath87 under clustering @xmath133 and let @xmath134 be the membership function that assigns data points @xmath12 to clusters i.e. , @xmath135 under the clustering @xmath136 .",
    "then the @xmath0-means clustering is the @xmath137 that achieves @xmath138 ( strictly speaking , the objective function in should be written in its limiting form @xmath139 with the constraints @xmath140 . )    under the @xmath0-means optimality criterion , given @xmath0 there are @xmath141 such that the minimum in ( or ) can be written as @xmath142 with the property that @xmath143 defining the centroid of @xmath87 as @xmath144 with corresponding estimate defined as @xmath145 we can quote the following result .    under various regularity conditions , as @xmath82 , the @xmath0-means clustering @xmath146 is consistent for @xmath136 . in particular , @xmath147    see pollard ( 1981 ) .",
    "since @xmath0-means is a centroid - based clustering , we have that @xmath148 so combining this with we get that @xmath149 i.e. , assumption 1 ) is satisfied .    turning to assumption 2 ) , consider the following example with @xmath0-means to understand the intuition behind it .",
    "suppose a data set is generated as two clusters of the same number of outcomes , one with high variance and one with low variance , see the upper left panel in fig .",
    "[ hk3 ] . then , applying @xmath0-means with increasing @xmath0 , e.g. , @xmath150 in fig .",
    "[ hk3 ] shows that @xmath0-means partitions the two clusters more and more finely but continually assigns more clusters to the high variance data . in this context ,",
    "assumption 2 ) means that as @xmath9 increases , the clusters will appear to ` fill in ' yielding @xmath0 regions with non - void interior for each @xmath151 even if the @xmath9 required for a given @xmath0 increases with @xmath0 .     to see how clustering divides the true clusters . ]    to begin formalizing this intuition , write @xmath152 recognizable as the ` within clusters ' sum of squares . in the one - dimensional case ,",
    "suppose @xmath153 data points are drawn , @xmath154 $ ] and @xmath155 $ ] .",
    "clearly , @xmath156 represents a low diameter component and @xmath157 represents a high diameter component . if we seek a @xmath0-means clustering for @xmath158 , it is clear that @xmath159 and @xmath160 should be found .",
    "however , consider @xmath161 .",
    "there are two natural clusterings .",
    "the first is to split @xmath159 into two clusters of equal size , say @xmath162 and @xmath163 letting @xmath164 .",
    "the other is the reverse : let @xmath165 and split @xmath160 into two clusters of equal size , say @xmath163 and @xmath166 .",
    "it is easy to verify that the population value of wss for the first clustering is @xmath167 while for the second clustering it is @xmath168 .",
    "this means the second clustering , splitting the high diameter component , gives a smaller wss . since @xmath0-means chooses the mean of wss over the number of clusters , in this case , @xmath0-means would choose the second clustering .",
    "the example can be continued for higher @xmath0 , higher @xmath5 , and other distributions continuing to show that @xmath0-means tends to split the largest cluster until it is worthwhile to split the smaller cluster and then resumes splitting the larger cluster , and so on .",
    "a consequence of this is that @xmath169 tends to decrease in size as @xmath0 increases and this suggests that @xmath87 will similarly decrease as assumed in item 2 ) as @xmath82 .",
    "we state a version of this in the following .",
    "suppose a clustering method continually splits the largest cluster on the population level as @xmath0 increases .",
    "then , given @xmath170 , there is a @xmath121 so that @xmath171    let @xmath172 be the centers of the optimal clusters and write @xmath173 then , for @xmath174 large enough , @xmath175 since this process can be repeated the proposition is established .    taken together , the results of this subsection justify the @xmath0-means part of step 1 ) of algorithm # 1 .",
    "next we turn to justifying the use of single linkage ( sl ) in the second part of step 1 in algorithm # 1 .",
    "recall , sl means that we merge sets that are closest i.e. , given a distance @xmath176 on , say , @xmath177 , sl clustering merges the two sets that achieve @xmath178 the question that remains is how to choose @xmath176 .",
    "here we use two choices .",
    "the first is to write @xmath176 as @xmath179 where @xmath176 is a metric e.g. , euclidean , i.e. , @xmath180 gives the distance between two sets as the minimum over the distances between their points .",
    "however , being an order statistic , can be affected by extreme values in the data set .",
    "so , we stabilize @xmath181 by replacing it with the 20th percentile of the distances between points in @xmath87 and @xmath182 . that is , for @xmath183 , we find the distances @xmath184 take their order statistics , and find the approximately @xmath185 order statistic .",
    "( finding a non - integer order statistic is done internally to the r program using linear interpolation . )",
    "we call the resulting dissimilarity @xmath186 , i.e. , @xmath187 to indicate it is based on the 20th percentile of the distances between points in the two sets .",
    "thus , with @xmath186 we are using single linkage with respect to a dissimilarity that should be robust against extreme values .",
    "other percentiles such as the fifth or tenth can also be used , but they gave values between @xmath180 and @xmath186 in the examples we studied .",
    "it seemed from our work that @xmath186 gave the best resuts in cases where @xmath180 did not .",
    "for the sake of completeness we next give conditions under which @xmath180 can be expected to perform well .",
    "we are unable to demonstrate this for @xmath186 but suggest there will be an analogous result since using @xmath186 gave results that were essentially never worse ( and sometimes better ) than @xmath180 .",
    "suppose @xmath188 consists of @xmath5 disjoint regions each being the closure of an open set , assumed disjoint from the other open sets .",
    "let @xmath189 be the minimum distance between points in disjoint components ,",
    "i.e. , @xmath190 if @xmath9 and @xmath0 are chosen so large that all the @xmath169 s for @xmath191 have @xmath192 ( any number strictly less than @xmath189 will suffice ) , choose a regular grid @xmath193 of points in @xmath188 so that the distance between two adjacent points on the same axis is less than @xmath194 .",
    "this ensures that each @xmath169 has at least one grid point in it .",
    "the points in @xmath193 are essentially a perfectly representative set of @xmath188 and hence of @xmath27 .",
    "now , if we apply sl with @xmath180 to the points in @xmath193 we will always put points or subsets in the same component together before we merge points or subsets of any two distinct components .",
    "that is , the metric on @xmath193 ensures that the closest point to any other point will always be in the same component if possible .",
    "so , we have proved the following theorem .",
    "if the components of @xmath188 are disjoint there is a cut point in the dendrogram of the sl merging under @xmath180 of the points in @xmath193 that separates the components perfectly .",
    "now , if @xmath9 is large enough , the data set can be taken as perfectly representative of @xmath188 and hence of @xmath27 , i.e. , it is a good approximation to @xmath193 in the sense of filling out all the components of @xmath27 . hence , it follows that sl using @xmath180 can perfectly separate the components of @xmath188 , and hence of @xmath27 , in a limiting sense .",
    "this does not require convexity of the components of @xmath27 , only that the data points can be regarded as essentially a perfect representation of @xmath27 .",
    "note that one of the key hypotheses of this theorem forces the components of @xmath27 to be separated .",
    "in fact , this is often not the case  components may touch each other at individual points or may be linked by a very thin short line . in these cases , the components of @xmath195 may not have disjoint closures or the closure of the components may not give @xmath188 , respectively . when assumption 4 of theorem 1 is satisfied , we have found that @xmath180 works well : in a limiting sense , two basal sets from the same component will always be joined before either is joined to another component . however ,",
    "when hypothesis 4 of theorem 1 is not satisfied , @xmath180 does not have this property .",
    "in these cases , we have found @xmath186 to work better ; this is seen in subsec .",
    "[ aggregation ] .",
    "it should be noted that the examples in subsecs .",
    "[ halfring ] and [ flamedata ] also do not seem to satisfy hypothesis 4 but for these cases @xmath180 and @xmath186 give comparable results .",
    "we regard this as a reflection of the fact that hypothesis 4 is necessary but not sufficient for the conclusion of theorem 1 .",
    "also , although not shown here , we examined our clustering technique using @xmath196 and @xmath186 in the sense of but found they were outperformed by at least one of @xmath180 or @xmath186 .",
    "one point in favor of @xmath180 is that it is interpretable in that two points are in the same cluster merely if they are close enough , unlike @xmath186 .",
    "why does the 20-th percentile work well in cases where hypothesis 4 is not satisfied ?",
    "while we do not have a formal argument , the intuition may be expressed as follows",
    ". if hypothesis 4 is not satisfied and the clusters are highly non - convex @xmath180 will be much more sensitive to the boundary values of the clusters than @xmath186 .",
    "consequently , there may be overly influential data points  data points that are valid but far from other data points  that will affect the sequence of merges of the basal clusters in ways that are not representative of the support of @xmath27 .",
    "using @xmath186 in place of @xmath180 reduces the influence of these data points eliminating distortions of the path by which basal clusters are merged . we do not have a rule for when to use @xmath180 versus @xmath186 , however , the presence of extreme points ( as opposed to outliers ) is a good indicator that @xmath186 should be preferred and this is consistent with all our examples .",
    "indeed , the examples in subsecs .",
    "[ halfring ] and [ flamedata ] , where @xmath180 and @xmath186 give equivalent results , do not have clusters with extreme points .      in steps 2 and 3 of algorithm # 1 , a composite membership matrix @xmath47 for @xmath18 clusterings is defined .",
    "then , single linkage clustering is applied to the rows of @xmath47 in step 4 . because @xmath60 is absed on @xmath47",
    "our results should be robust results because by using several random starts for the clustering and looking only at which cluster a data point is in , we are ensuring that the final clustering is a sort of ` consensus clustering ' representing what is invariant under two sorts of randomness  randomness of the clustering and random noise in the data points themselves .",
    "our use of the matrix @xmath47 means our method may be regarded as an ensemble approach .",
    "each set of columns in @xmath47 represents a clustering and pooling over clusterings in step 4 effectively means that we are analyzing @xmath18 different clustering structures for the data .",
    "the analog of ensembling the matrices is played by single linkage which groups similar clusterings together .",
    "the result is that the final clustering is stabilized .      in step 4 ,",
    "algorithm # 1 grows a dendrogram of size @xmath3 by single linkage .",
    "in fact , @xmath3 may be bigger than the size @xmath0 of the desired clustering . in such cases ,",
    "the dendrogram ` grown ' is too large and must be pruned back .",
    "this is done in step 5 .",
    "the benefit of this is that by growing a dendrogram a little larger than required , the method may look one or more splits further along so that outliers or other aberrant points may be removed .",
    "the outliers or other aberrant points are in the @xmath69 small clusters that are removed before the data are reclustered . leaving out the @xmath69 small clusters",
    "means that the resulting clustering should be more stable , and therefore hopefully more accurate .",
    "of course , the outliers and aberrant points in the @xmath69 small clusters must be merged back into the clustering as is done at the end of step 5 .",
    "however , they are merged back into clusters they were not used to form .",
    "hence , the final clustering may be more representative of @xmath27 than if the extra points were used to form the clusters in the first place .    at root ,",
    "steps 2 - 5 are designed to take advantage of the chaining property of single linkage .",
    "usually , the chaining property is a reason not to use single linkage ; here the chaining property is used only to fill out clusters but as far as possible not to merge them . in terms of filling out clusters ,",
    "the chaining property is desirable .",
    "it only becomes disadvantageous when it inappropriately joins clusters .",
    "steps 1 and 2 in algorithm 2 have been addressed in subsecs .",
    "[ largekell ] , [ slmerging1 ] , and [ membership ] .",
    "so , it remains to justify the use of lifetimes in steps 3 - 5 for estimating @xmath5 .    as can be seen in fig .",
    "3 of @xcite where they give an example of lifetimes for a dendrogram , defining clusters by the use of a maximum lifetime has the tendency to amplify the separation between so that points are usually only put in their final cluster near the leaves of the dendrogram .",
    "that is , there are often several long lifetimes that give reasonable places to cut the dendrogram such that the clusters at the bottom are well separated and homogeneous in the sense that further decreases in dissimilarity are small .",
    "this method seems to work well when the clusters are well separated , regardless of whether they are convex .",
    "our refinement of the @xcite technique is an effort to extend it to cases where the separation among clusters is not as clear . indeed , removing subsets of data that are too small before applying @xcite ensures that likely outliers or other aberrant points will not affect the collection of lifetimes .",
    "the benefit is that outliers and aberrant points will rarely be seen as separate clusters , yielding a more accurate number of clusters .",
    "in this section , we compare the performance of the two proposed techniques with the existing techniques described in sec .",
    "[ intro ] using eight data sets that are qualitatively different from each other .",
    "the first five are the 3-normals , aggregation , spiral , half - ring , flame data sets found at @xcite .",
    "these two dimensional data sets are ` shape data ' .",
    "3-normals is simulated from three normal distributions giving convex shapes that are not well separated .",
    "aggregation has one cluster that is non - convex and several others that are convex but not separated .",
    "spiral has three separated but nonconvex and ` intertwined ' clusters .",
    "half - ring has two separated clusters that are non - convex with different densities making it ambiguous whether one of the clusters should be split or not .",
    "flame has two clusters , one convex , the other non - convex .",
    "the two clusters are not well - separated and the convex cluster has some outliers .",
    "we did not examine other shape sets at @xcite because they were similar to data sets we had used or were too challenging for all methods .    for four of these five data sets",
    ", we considered seven different clustering techniques : @xmath0-means ( k - m ) , eac , ct , chameleon ( hereafter abbreviated to cha , spectral clustering specc , shcm , and shc20 .",
    "we applied all seven to aggregation , spiral , half - ring , and flame but only applied chameleon to one output from 3-normals because chameleon is intended for nonconvex data and is cumbersome when doing many repetitions with simulated data .",
    "the first five data sets are two dimensional , so it is enough to compare the output of the clustering techniques visually . however , because we can unambiguously assign a ` true ' clustering to these data sets , we can also calculate an accuracy index , ai , i.e. , the proportion of data points correctly assigned to their cluster .",
    "this was calculated using the software described in @xcite . since there is randomness built into k - m , eac , specc , and our method , where necessary i.e. , for non - synthetic data , we repeated the techniques and report the mean ai ( mai ) and its standard deviation ( sai ) .",
    "this was not necessary for ct since it does not vary over repetitions ( hence sai for ct is always zero ) .",
    "the results are in subsecs .",
    "[ 3_normals ] and [ nonconvex ] .",
    "the last three data sets have four or more dimensions .",
    "the first of these is fisher s familiar iris data that has four dimensions .",
    "the second is the garber microarray data set found at @xcite .",
    "it has 916 dimensions .",
    "the third is the wine quality data set found at @xcite .",
    "this data set has 11 variables and is really two data sets , one for red wine and one for white wine .",
    "we used all seven techniques on iris and garber but dropped chameleon for wine quality because it was hard to implement and , being graph - theoretic , it can not be expected to perform well on data that have many dimensions .",
    "we also dropped ct for wine quality since ct so rarely performed well . in these examples ,",
    "the data sets are from classification problems so we have assumed that a unique true clustering exists as defined by the classes .",
    "we can again calculate the mai and sai as described above .",
    "the results are in subsec .",
    "[ higherdims ] .    in all eight examples , we set @xmath197 for eac , shcm , and sch20 to ensure fairness .",
    "in the first seven examples we set @xmath198 in shc ; in the garber data set we chose @xmath199 because @xmath200 and @xmath201 so that it made sense to draw @xmath1 s from @xmath202 .",
    "the implication of our examples is that while other methods may equal or even perform slightly better than one or both of the shc methods in some cases , no competitor beats them consistently by a nontrivial amount .",
    "we begin with straightforward comparisons of the clustering techniques for fixed @xmath0 and then turn to comparing the techniques for choosing @xmath0 .",
    "specifically , for all eight data sets , we compare six techniques for choosing @xmath0 , namely , the silhouette distance , the gap statistic , bic , eac , and two methods based on algorithm # 2 ( ekm and ek20 ) .",
    "these results are given in subsec .",
    "[ estk ] .      in order to study this convex clustering problem , consider figure [ cov1 ] showing @xmath203 observations that clearly form three clusters .",
    "the data were generated by taking 40 independent and identical draws from each of three normal distributions .",
    "specifically , the there normals are @xmath204 , @xmath205 where @xmath206 @xmath207 and @xmath208    applying the seven clustering techniques to one set of the 3-normals data with @xmath209 gives fig .",
    "first , the upper left panel shows the true clusters .",
    "it appears that k - m , ct , specc , and shc20 do roughly equally well , although spectral clustering tends to enlarge the right hand cluster unduly .",
    "by contrast , cha , eac , and shcm do not give intuitively reasonable results .",
    "cha makes the lower left cluster too small ; eac and shcm effectively merge the right and bottom clusters .",
    "[ htp ! ]",
    "these observations are mostly but not thoroughly consistent with table [ tabcon ] in which mai values are rounded to two decimal places and sai values are rounded to three decimal places .",
    "table [ tabcon ] is the summary of the performance of the six methods over 200 simulated data sets ; cha is omitted as noted earlier . clearly , despite fig .",
    "[ cov1 ] , ct and eac are poor in an average sense ( mai ) while schm is comparable to shc20  suggesting the single example of shcm in fig .",
    "[ cov1 ] is unrepresentative of its general performance .",
    "the performance of spectral clustering is better than fig .",
    "[ cov1 ] suggests but not as good as @xmath0-means which is best as expected .",
    "loosely , k - m , specc .",
    "and the two shc techniques seem to be broadly equivalent .",
    "note that , usually , the size of the sai values are higher for poorer performing clustering techniques .",
    "this can be taken as an assessment of the quality of the clustering .",
    ".the comparison of the proposed methods on 3-normals data [ cols=\"<,^,^,^,^,^,^ \" , ]      + [ clustersize ]",
    "our approach leads to two natural techniques that differ in the dis - similarity used in the single linkage step of our clustering approach .",
    "one dis - similiarity is the usual euclidean distance between two points .",
    "the other is the 20-th percentile of the distances between points in two different clusters .",
    "we can establish formal results for the euclidean distance and it has a naural geometric interpretation .",
    "however , the 20-th percentile dissimilarity gives performance that is no worse and sometimes meaningfully better than the euclidean distance . in order to evalaute the performance of the proposed methods",
    ", we tested them on a wide variety of qualitatively different clusterings , including both real and simulated data , convex and nonconvex true clusterings , and clusterings in which the components are not well separated .",
    "our theory and examples suggest that our methods lead to accurate clusterings and that using @xmath210 to form the membership matrices is enough to provide satisfactory stability . of course , for more complicated data  irregular shapes , little separation between shapes , etc .",
    " more ensembling ( higher @xmath18 ) may lead to better results .    in our examples , our methods effectively equal or outperform many standard or related methods such as spectral clustering , @xmath0-means , eac @xcite , hybrid hierachical clustering @xcite , and chameleon @xcite . in fact , in all eight examples we presented here , one of the two forms of our approach always yielded robust and relatively accurate results .",
    "the authors gratefully acknowledge support from the nsf - dtra , grant no . nr66853w .                            c. fraley , a. e. raftery , t. b. murphy and l. scrucca , `` mclust version 4 for r : normal mixture modeling for model - based clustering , classification , and density estimation '' , technical report no .",
    "597 , department of statistics , university of washington , 2012 ."
  ],
  "abstract_text": [
    "<S> here , we propose a clustering technique for general clustering problems including those that have non - convex clusters . for a given desired number of clusters @xmath0 , we use three stages to find a clustering . </S>",
    "<S> the first stage uses a hybrid clustering technique to produce a series of clusterings of various sizes ( randomly selected ) . </S>",
    "<S> they key steps are to find a @xmath0-means clustering using @xmath1 clusters where @xmath2 and then joins these small clusters by using single linkage clustering . </S>",
    "<S> the second stage stabilizes the result of stage one by reclustering via the ` membership matrix ' under hamming distance to generate a dendrogram . </S>",
    "<S> the third stage is to cut the dendrogram to get @xmath3 clusters where @xmath4 and then prune back to @xmath0 to give a final clustering . </S>",
    "<S> a variant on our technique also gives a reasonable estimate for @xmath5 , the true number of clusters .    </S>",
    "<S> we provide a series of arguments to justify the steps in the stages of our methods and we provide numerous examples involving real and simulated data to compare our technique with other related techniques .    * a general hybrid clustering technique *    saeid amiri@xmath6 ,  bertrand clarke@xmath7 ,  jennifer clarke@xmath7  and   hoyt a. koepke@xmath8    _ @xmath7department of statistics , university of nebraska - lincoln , lincoln , nebraska , usa + @xmath8graphlab inc . </S>",
    "<S> seattle , wa , usa _    </S>",
    "<S> * keywords * : hybrid clustering , @xmath0-means , single linkage , non - convex clusters , stability </S>"
  ]
}