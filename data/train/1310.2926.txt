{
  "article_text": [
    "distance covariance ( dcov ) and distance correlation characterize multivariate independence for random vectors in arbitrary , not necessarily equal dimension . in this work we focus on the open problem of _",
    "partial distance correlation_. our intermediate results include methods for applying distance correlation to dissimilarity matrices .",
    "the distance covariance , denoted @xmath0 , of two random vectors @xmath1 and @xmath2 characterizes independence ; that is , @xmath3 with equality to zero if and only if @xmath1 and @xmath2 are independent .",
    "this coefficient is defined by a weighted @xmath4 norm measuring the distance between the joint characteristic function ( c.f . )",
    "@xmath5 of @xmath1 and @xmath2 , and the product @xmath6 of the marginal c.f.s of @xmath1 and @xmath2 .",
    "if @xmath1 and @xmath2 take values in @xmath7 and @xmath8 , respectively , @xmath0 is the non - negative square root of @xmath9 where @xmath10 .",
    "the integral exists provided that @xmath1 and @xmath2 have finite first moments .",
    "note that feuerverger ( 1993 ) @xcite proposed a bivariate test based on this idea and applied the same weight function @xmath11 , where it may have first appeared .",
    "the following identity is established in szkely and rizzo ( * ? ? ?",
    "* theorem 8 , p. 1250 ) .",
    "let @xmath12 , @xmath13 , and @xmath14 be independent and identically distributed ( iid ) , each with joint distribution @xmath12",
    ". then @xmath15 provided that @xmath1 and @xmath2 have finite first moments . in section [ s4 ] an alternate version of ( [ edcov ] )",
    "is defined for @xmath1 and @xmath2 taking values in a separable hilbert space .",
    "that definition and intermediate results lead to the definition of partial distance covariance .",
    "we summarize a few key properties and computing formulas below for easy reference .",
    "the distance correlation ( dcor ) @xmath16 is a standardized coefficient , @xmath17 , that also characterizes independence : @xmath18 for more details see @xcite and @xcite .",
    "properties , extensions , and applications of distance correlation have been discussed in the recent literature ; see e.g. dueck et al .",
    "@xcite , lyons @xcite , kong et al .",
    "@xcite , and li , zhong , and zhu @xcite . however , there is considerable interest among researchers and statisticians on the open problem of a suitable definition and supporting theory of partial distance correlation . among the many potential application areas of partial distance correlation are variable selection",
    "( see example [ ex7 ] ) and graphical models ; see e.g.  wermuth and cox @xcite for an example of work that motivated the question in that context .    in this work",
    "we introduce the definition of partial distance covariance ( pdcov ) and partial distance correlation ( pdcor ) statistics and population coefficients .",
    "first , let us see why it is not straightforward to generalize distance correlation to partial distance correlation in a meaningful way that preserves the essential properties one would require , and allows for interpretation and inference .    one could try to follow the definitions of the classical partial covariance and partial correlation that are based on orthogonal projections in a euclidean space , but there is a serious difficulty .",
    "orthogonality in case of partial distance covariance and partial distance correlation means independence , but when we compute the orthogonal projection of a random variable onto the condition variable , the `` remainder '' in the difference is typically not independent of the condition .    alternately , the form of sample distance covariance ( definition [ defvn ] ) may suggest an inner product , so one might think of working in the hilbert space of double centered distance matrices ( [ dcenter ] ) , where the inner product is the squared distance covariance statistic ( [ e : anxy ] ) .",
    "here we are facing another problem : what would the projections represent ?",
    "the difference @xmath19 of double centered distance matrices is typically not a double centered distance matrix of any sample .",
    "this does not affect formal computations , but if we can not interpret our formulas in terms of samples then inference becomes impossible .    to overcome these difficulties",
    "while preserving the essential properties of distance covariance , we finally arrived at an elegant solution which starts with defining an alternate type of double centering called `` @xmath20-centering '' ( see definition [ def : tildea ] and proposition [ p : unbiased ] below ) .",
    "the corresponding inner product is an unbiased estimator of squared population distance covariance . in the hilbert space of `` @xmath20-centered '' matrices , all linear combinations , and in particular projections ,",
    "are zero diagonal @xmath20-centered matrices .",
    "we prove a representation theorem that connects the orthogonal projections to random samples in euclidean space .",
    "methods for inference are outlined and implemented , including methods for non - euclidean dissimilarities .",
    "definitions and background for dcov and dcor are summarized in section 2 .",
    "the partial distance correlation statistic is introduced in section 3 , and section 4 covers the population partial distance covariance and inference for a test of the hypothesis of zero partial distance correlation .",
    "examples and applications are given in section 5 , followed by a summary in section 6 .",
    "appendix a contains proofs of statements .",
    "in this section we summarize the definitions for distance covariance and distance correlation statistics .",
    "random vectors are denoted with upper case letters and their sample realizations with lowercase letters . in certain contexts we need to work with a data matrix , which is denoted in boldface , such as @xmath21 or @xmath22 .",
    "for example , the sample distance correlation of a realization from the joint distribution of @xmath23 is denoted @xmath24 .",
    "the norm @xmath25 denotes the euclidean norm when its argument is in @xmath26 , and we omit the subscript if the meaning is clear in context . a primed random variable denotes",
    "an independent and identically distributed ( iid ) copy of the unprimed symbol ; so @xmath27 are iid , etc .",
    "the pair @xmath28 denotes two realizations of the random variable @xmath1 .",
    "the transpose of @xmath29 is denoted by @xmath30 .",
    "other notation will be introduced within the section where it is needed .    in this paper",
    "a _ random sample _ refers to independent and identically distributed ( iid ) realizations from the underlying joint or marginal distribution .",
    "the distance covariance and distance correlation statistics are functions of the double centered distance matrices of the samples . for an observed random sample @xmath31 from the joint distribution of random vectors @xmath32 and @xmath33 , compute the euclidean distance matrices @xmath34 and @xmath35 . define @xmath36 where @xmath37 similarly , define @xmath38 , for @xmath39 .    in previous work",
    ", we denoted the double centered distance matrix of a sample by a plain capital letter , so that @xmath40 and @xmath41 above were denoted by @xmath42 and @xmath43 . in this paper",
    "we work with two different methods of centering , so we have introduced notation to clearly distinguish the two methods .",
    "in the first centering above the centered versions @xmath44 have the property that all rows and columns sum to zero .",
    "another type of centering , which we will call unbiased or @xmath20-centering and denote by @xmath45 in definition [ def : tildea ] below , has the additional property that all expectations are zero : that is , @xmath46 = 0 $ ] for all @xmath47 .",
    "although this centering has a somewhat more complicated formula ( [ au ] ) , we will see in this paper the advantages of the new centering .",
    "[ defvn ] the sample distance covariance @xmath48 and sample distance correlation @xmath49 are defined by @xmath50 and @xmath51 respectively , where the squared sample distance variance is defined by @xmath52    although it is obvious , it is perhaps worth noting that @xmath53 and @xmath49 are rigid motion invariant , and they can be defined entirely in terms of the distance matrices , or equivalently in terms of the double centered distance matrices .",
    "this fact is important for what follows , because as we will see , one can compute partial distance covariance by operating on certain transformations of the distance matrices , without reference to the original data that generated the matrices .",
    "( these definitions are not invariant to monotone transformation of coordinates , such as ranks . )",
    "if @xmath1 and @xmath2 have finite first moments , the population distance covariance coefficient @xmath54 exists and equals zero if and only if the random vectors @xmath1 and @xmath2 are independent . some of the properties of distance covariance and distance correlation include :    1 .",
    "@xmath53 and @xmath24 converge almost surely to @xmath0 and @xmath16 , as @xmath55 .",
    "2 .   @xmath56 and @xmath57 if and only if every sample observation is identical .",
    "[ th4 - 1 ] @xmath58 .",
    "4 .   [ th4 - 2 ] if @xmath59 then there exists a vector @xmath60 , a non - zero real number b and an orthogonal matrix @xmath61 such that @xmath62 , for the data matrices @xmath63 and @xmath64 .",
    "a consistent test of multivariate independence is based on the sample distance covariance : large values of @xmath65 support the alternative hypothesis that @xmath1 and @xmath2 are dependent ( see @xcite ) . a high - dimensional distance correlation @xmath66-test of independence",
    "was introduced by szkely and rizzo @xcite ; the tests are implemented as ` dcov.test ` and ` dcor.ttest ` , respectively , in the _ energy _ package ( rizzo and szkely , @xcite ) for r ( r core team @xcite ) .    the definitions of sample distance covariance and sample distance correlation can be extended to random samples taking values in any , possibly different , metric spaces .",
    "for defining the population coefficient we need to suppose more ( see lyons @xcite ) .",
    "if @xmath1 and @xmath2 take values in possibly different separable hilbert spaces and both @xmath1 and @xmath2 have finite expectations then it remains true that @xmath67 , and equals zero if and only if @xmath1 and @xmath2 are independent .",
    "this implies that the results of this paper can be extended to separable hilbert space valued variables .",
    "extensions and population coefficients pdcov and pdcor are discussed in section [ s4 ] . extending lyons @xcite , sejdinovic et al . @xcite",
    "discuss the equivalence of distance based and rkhs - based statistics for testing dependence .    for theory , background , and further properties of the population and sample coefficients ,",
    "see szkely , et al .",
    "@xcite , szkely and rizzo @xcite , and lyons @xcite , and the references therein . on the weight function",
    "applied see also feuerverger @xcite and szkely and rizzo @xcite . for an overview of recent methods for measuring dependence and testing independence of random vectors , including distance covariance ,",
    "readers are referred to josse and holmes @xcite .",
    "we introduce partial distance correlation by first considering the sample coefficient .",
    "[ def : tildea ] let @xmath68 be a symmetric , real valued @xmath69 matrix with zero diagonal , @xmath70 .",
    "define the @xmath20-centered matrix @xmath71 as follows .",
    "let the @xmath72-th entry of @xmath71 be defined by @xmath73    here `` @xmath20-centered '' is so named because as shown below , the corresponding inner product ( [ vu ] ) defines an unbiased estimator of squared distance covariance .",
    "[ p : unbiased ] let @xmath74 denote a sample of observations from the joint distribution @xmath75 of random vectors @xmath1 and @xmath2 .",
    "let @xmath68 be the euclidean distance matrix of the sample @xmath76 @xmath77 from the distribution of @xmath1 , and @xmath78 be the euclidean distance matrix of the sample @xmath79 from the distribution of @xmath2 .",
    "then if @xmath80 , for @xmath81 , @xmath82 is an unbiased estimator of squared population distance covariance @xmath54 .    proposition [ p : unbiased ]",
    "is proved in appendix ( [ prfunbiased ] ) .",
    "it is obvious that @xmath83 if all of the sample observations are identical .",
    "more generally , @xmath84 if and only if the @xmath85 sample observations are equally distant or at least @xmath86 of the @xmath85 sample observations are identical .    for a fixed @xmath87 , we define a hilbert space generated by euclidean distance matrices of arbitrary sets ( samples ) of @xmath85 points in a euclidean space @xmath7 , @xmath88 .",
    "consider the linear span @xmath89 of all @xmath69 distance matrices of samples @xmath90 .",
    "let @xmath91 be an arbitrary element in @xmath89 .",
    "then @xmath42 is a real valued , symmetric matrix with zero diagonal .",
    "let @xmath92 and for each pair of elements @xmath93 , @xmath94 in the linear span of @xmath95 define their inner product @xmath96 if @xmath97 then @xmath98 for any @xmath99 .",
    "below in theorem [ thmmds ] it is shown that every matrix @xmath100 is the @xmath20-centered distance matrix of a configuration of @xmath85 points in a euclidean space @xmath7 , where @xmath101 .",
    "[ t : hilbert ] the linear span of all @xmath69 matrices @xmath102 is a hilbert space with inner product defined by ( [ vu ] ) .",
    "let @xmath103 denote the linear span of @xmath104 . if @xmath105 and @xmath106 , then @xmath107 so @xmath108 . it is also true that @xmath100 implies that @xmath109 , and the zero element is the @xmath69 zero matrix .    then since @xmath110 for the inner product , we only need to prove that for @xmath111 and real constants @xmath112 , the following statements hold :    1 .",
    "@xmath114 only if @xmath115 .",
    "3 .   @xmath116 4 .",
    "@xmath117 .    statements ( i ) and",
    "( ii ) hold because @xmath118 is proportional to a sum of squares . statements ( iii ) and ( iv ) follow easily from the definition of @xmath103 , @xmath71 , and ( [ vu ] ) .",
    "the space @xmath103 is finite dimensional because it is a subspace of the space of all symmetric , zero - diagonal @xmath69 matrices .    in what follows",
    ", @xmath103 denotes the hilbert space of theorem [ t : hilbert ] with inner product ( [ vu ] ) , and @xmath119 is the norm of @xmath71 .      from theorem [ t : hilbert ] a projection operator ( [ proj ] ) can be defined in the hilbert space @xmath103 , @xmath87 , and applied to define partial distance covariance and partial distance correlation for random vectors in euclidean spaces .",
    "let @xmath120 and @xmath121 be elements of @xmath103 corresponding to samples @xmath122 and @xmath123 , respectively , and let @xmath124 denote the orthogonal projection of @xmath125 onto @xmath126 and the orthogonal projection of @xmath127 onto @xmath126 , respectively . in case @xmath128",
    "the projections are defined @xmath129 and @xmath130 .",
    "clearly @xmath131 and @xmath132 are elements of @xmath103 , their dot product is defined by ( [ vu ] ) , and we can define an estimator of pdcov(@xmath133 ) via projections .",
    "[ def.pdcov ] let @xmath134 be a random sample observed from the joint distribution of @xmath135 .",
    "the sample partial distance covariance ( pdcov ) is defined by @xmath136 where @xmath131 , and @xmath132 are defined by ( [ proj ] ) , and @xmath137    let @xmath134 be a random sample observed from the joint distribution of @xmath135 .",
    "then sample partial distance correlation is defined as the cosine of the angle @xmath138 between the ` vectors ' @xmath131 and @xmath132 in the hilbert space @xmath103 : @xmath139 and otherwise @xmath140 .",
    "sample pdcov and pdcor have been defined via projections in the hilbert space generated by @xmath20-centered distance matrices . in this section ,",
    "we start to consider the interpretation of sample pdcov and sample pdcor .",
    "since pdcov is defined as the inner product ( [ pdcov ] ) of two @xmath20-centered matrices , and ( unbiased squared ) distance covariance ( [ vudcov ] ) is computed as inner product , a natural question is the following . are matrices @xmath131 and @xmath132 the @xmath20-centered _ euclidean _ distance matrices of samples of points @xmath141 and @xmath142 , respectively ?",
    "if so , then the sample partial distance covariance ( [ pdcov ] ) is distance covariance of @xmath21 and @xmath22 , as defined by ( [ vudcov ] ) .",
    "for every sample of points @xmath143 $ ] , @xmath144 , there is a @xmath20-centered matrix @xmath145 in @xmath103 .",
    "conversely , given an arbitrary element @xmath146 of @xmath103 , does there exist a configuration of points @xmath147 $ ] in some euclidean space @xmath8 , for some @xmath148 , such that the @xmath20-centered euclidean distance matrix of sample @xmath141 is exactly equal to the matrix @xmath146 ?",
    "in this section we prove that the answer is yes : @xmath131 , @xmath132 of ( [ pdcov ] ) , and in general every element in @xmath103 , is the @xmath20-centered distance matrix of some sample of @xmath85 points in a euclidean space .",
    "first a few properties of centered distance matrices are established in the following lemma .",
    "[ lemmau ] let @xmath71 be a @xmath20-centered distance matrix",
    ". then    1 .",
    "rows and columns of @xmath71 sum to zero . 2 .",
    "that is , if @xmath43 is the matrix obtained by @xmath20-centering an element @xmath150 , @xmath151 .",
    "@xmath71 is invariant to double centering .",
    "that is , if @xmath43 is the matrix obtained by double centering the matrix @xmath71 , then @xmath152 .",
    "[ l4 ] if @xmath112 is a constant and @xmath43 denotes the matrix obtained by adding @xmath112 to the off - diagonal elements of @xmath71 , then @xmath153 .",
    "see appendix [ prflemmau ] for proof of lemma [ lemmau ] .    as lemma [ lemmau](iv )",
    "is essential for our results , it becomes clear that we can not apply double centering as in the original ( biased ) definition of distance covariance here .",
    "the invariance with respect to the constant @xmath112 in ( iv ) holds for @xmath20-centered matrices but it does not hold for double centered matrices .    an @xmath69 matrix @xmath19 is called _ euclidean _ if there exist points @xmath154 in a euclidean space such that their euclidean distance matrix is exactly @xmath19 ; that is , @xmath155 , @xmath39 .",
    "necessary and sufficient conditions that @xmath19 is euclidean are well known results of classical ( metric ) mds . with the help of lemma [ lemmau ] , and certain results from the theory of mds , we can find for each element @xmath156 a configuration of points @xmath154 in euclidean space such that their @xmath20-centered distance matrix is exactly equal to @xmath146 .",
    "a solution to the underlying system of equations to solve for the points @xmath157 is found in schoenberg @xcite and young and householder @xcite .",
    "it is a classical result that is well known in ( metric ) multidimensional scaling .",
    "mardia , kent , and bibby @xcite summarize the result in theorem 14.2.1 , and provide a proof . for an overview of the methodology see also cox and cox @xcite , gower @xcite , mardia @xcite , and torgerson @xcite .",
    "we apply the converse ( b ) of theorem 14.2.1 as stated in mardia , kent , and bibby @xcite , summarized below .",
    "let @xmath158 be a dissimilarity matrix and define @xmath159 .",
    "form the double centered matrix @xmath160 .",
    "if @xmath40 is positive semi - definite ( p.s.d . ) of rank @xmath161 , then a configuration of points corresponding to @xmath162 can be constructed as follows .",
    "let @xmath163 be the positive eigenvalues of @xmath40 , with corresponding normalized eigenvectors @xmath164 , such that @xmath165 , @xmath166 .",
    "then if @xmath167 is the @xmath168 matrix of eigenvectors , the rows of @xmath167 are @xmath161-dimensional vectors that have interpoint distances equal to @xmath158 , and @xmath169 is the inner product matrix of this set of points .",
    "the solution is constrained such that the centroid of the points is the origin .",
    "there is at least one zero eigenvalue so @xmath170 .    when the matrix @xmath40 is not positive semi - definite , this leads us to the _ additive constant problem _ , which refers to the problem of finding a constant @xmath112 such that by adding the constant to all off - diagonal entries of @xmath158 to obtain a dissimilarity matrix @xmath171 , the resulting double centered matrix is p.s.d .",
    "let @xmath172 denote the double centered matrix obtained by double centering @xmath173 , where @xmath174 is the kronecker delta .",
    "let @xmath175 denote the matrix obtained by double centering @xmath176 .",
    "the smallest value of @xmath112 that makes @xmath172 p.s.d .",
    "is @xmath177 , where @xmath178 is the smallest eigenvalue of @xmath179 .",
    "then @xmath172 is p.s.d . for every @xmath180 . the number of positive eigenvalues of the p.s.d .",
    "double centered matrix @xmath172 determines the dimension required for the representation in euclidean space .    however , we require a constant to be added to the elements @xmath181 rather than @xmath182 . that is , we require a constant @xmath112 , such that the dissimilarities @xmath183 are euclidean .",
    "the solution by cailliez @xcite is @xmath184 , where @xmath184 is the largest eigenvalue of a @xmath185 block matrix @xmath186 where @xmath187 is the zero matrix and @xmath188 is the identity matrix of size @xmath85 ( see cailliez @xcite or cox and cox ( * ? ? ?",
    "2.2.8 ) for details ) .",
    "this result guarantees that there exists a constant @xmath184 such that the adjusted dissimilarities @xmath189 are euclidean . in this case at most @xmath190 dimensions are required ( cailliez ( * ? ? ?",
    "* theorem  1 ) ) .    finally , given an arbitrary element @xmath146 of @xmath103 ,",
    "the problem is to find a configuration of points @xmath191 $ ] such that the @xmath20-centered distance matrix of @xmath22 is exactly equal to the element @xmath146 .",
    "thus , if @xmath192 we are able to find points @xmath22 such that the euclidean distance matrix of @xmath22 equals @xmath193 , and we need @xmath194 . now since @xmath195 we can apply lemma [ lemmau ] to @xmath146 , and @xmath196 follows from lemma [ lemmau](ii ) and lemma [ lemmau](iv ) .",
    "hence , by applying classical mds with the additive constant theorem , and lemma [ lemmau ] ( ii ) and ( iv ) , we obtain the configuration of points @xmath22 such that their @xmath20-centered distances are exactly equal to the element @xmath156 .",
    "lemma [ lemmau](iv ) also shows that the inner product is invariant to the constant @xmath112 .",
    "this establishes our theorem on representation in euclidean space .",
    "[ thmmds ] let @xmath146 be an arbitrary element of the hilbert space @xmath103 of @xmath20-centered distance matrices .",
    "then there exists a sample @xmath154 in a euclidean space of dimension at most @xmath190 , such that the @xmath20-centered distance matrix of @xmath154 is exactly equal to @xmath146 .",
    "the above details also serve to illustrate why a hilbert space of double centered matrices ( as applied in the original , biased statistic @xmath197 ) is not applicable for a meaningful definition of partial distance covariance .",
    "the diagonals of double centered distance matrices are not zero , so we can not get an exact solution via mds , and the inner product would depend on @xmath112 .",
    "another problem is that while @xmath197 is always non - negative , the inner product of projections could easily be negative .    [ [ methods - for - dissimilarities ] ] methods for dissimilarities + + + + + + + + + + + + + + + + + + + + + + + + + + +    in community ecology and other fields of application , it is often the case that only the ( non - euclidean ) dissimilarity matrices are available ( see e.g.  the genetic distances of table [ t : maize7 ] ) .",
    "suppose that the dissimilarity matrices are symmetric with zero diagonal .",
    "an application of theorem [ thmmds ] provides methods for this class of non - euclidean dissimilarities . in this case ,",
    "theorem [ thmmds ] provides that there exists samples in euclidean space such that their @xmath20-centered euclidean distance matrices are equal to the dissimilarity matrices .",
    "thus , to apply distance correlation methods to this type of problem , one only needs to obtain the euclidean representation .",
    "existing software implementations of classical mds can be applied to obtain the representation derived above .",
    "for example , classical mds based on the method outlined in mardia @xcite is implemented in the r function ` cmdscale ` , which is in the _ stats _ package for r. the ` cmdscale ` function includes options to apply the additive constant of cailliez @xcite and to specify the dimension .",
    "the matrix of points @xmath22 is returned in the component ` points ` . for an exact representation",
    ", we can specify the dimension argument equal to @xmath190 .",
    "[ ex0 ] to illustrate application of theorem [ thmmds ] for non - euclidean dissimilarities , we computed the bray - curtis dissimilarity matrix of the ` iris ` _ setosa _ data , a four - dimensional data set available in r. the bray - curtis dissimilarity defined in cox and cox ( * ? ? ?",
    "* table  1.1 ) as @xmath198 is not a distance since it does not satisfy the triangle inequality .",
    "a bray - curtis method is available in the ` distance ` function of the _ ecodist _ package for r @xcite .",
    "we find a configuration of 50 points in @xmath199 that have @xmath20-centered distances equal to the @xmath20-centered dissimilarities .",
    "the mds computations are handled by the r function ` cmdscale ` .",
    "function ` ucenter ` , which implements @xmath20-centering , is in the r package _ pdcor _ @xcite .",
    ".... > x < - iris[1:50 , 1:4 ] >",
    "iris.d < - distance(x , method=\"bray - curtis \" ) > au < - ucenter(iris.d ) > v < - cmdscale(as.dist(au ) , k=48 , add = true)$points ....    the points ` v ` are a @xmath200 data matrix , of 50 points in @xmath199 .",
    "next we compare the @xmath20-centered distance matrix of the points ` v ` with the original object ` au ` from @xmath103 :    .... > vu < - ucenter(v ) > all.equal(au , vu ) [ 1 ] true ....    the last line of output shows that the points ` v ` returned by ` cmdscale ` have @xmath20-centered distance matrix ` vu ` equal to our original element ` au ` of the hilbert space .",
    "@xmath201    example [ ex0 ] shows that the sample distance covariance can be defined for dissimilarities via the inner product in @xmath103 .",
    "alternately one can compute @xmath202 , where @xmath21 , @xmath22 are the euclidean representations corresponding to the two @xmath20-centered dissimilarity matrices that exist by theorem [ thmmds ] . using the corresponding definitions of distance variance , sample distance correlation for dissimilarities",
    "is well defined by ( [ rcorrected ] ) or @xmath203 . similarly one can define pdcov and pdcor when one or more of the dissimilarity matrices of the samples is not euclidean distance . however , as in the case of euclidean distance , we need to define the corresponding population coefficients , and develop a test of independence . for the population definitions",
    "see section [ s4 ] .",
    "let @xmath204 where @xmath205 are the @xmath20-centered distance matrices of the samples @xmath29 and @xmath206 , and @xmath207 .",
    "the statistics @xmath208 and @xmath209 take values in @xmath210 $ ] , but they are measured in units comparable to the squared distance correlation @xmath211 .    [ p : pdcor2 ] if @xmath212 , a computing formula for @xmath213 in definition ( [ pdcor2 ] ) is @xmath214    see appendix [ prfrxy ] for a proof",
    ".    equation ( [ r.alt ] ) provides a simple and familiar form of computing formula for the partial distance correlation . the computational algorithm is easily implemented , as summarized below .",
    "_ algorithm to compute partial distance correlation @xmath215 from euclidean distance matrices @xmath216 , @xmath217 , and @xmath218 _ :    1 .",
    "compute the @xmath20-centered matrices @xmath71 , @xmath219 , and @xmath121 , using @xmath220 and @xmath221 .",
    "2 .   compute inner products and norms using @xmath222 and @xmath208 , @xmath223 , and @xmath224 using @xmath225 3 .   if @xmath226 and @xmath227 @xmath228    otherwise apply the definition ( [ pdcor2 ] ) .    note",
    "that it is not necessary to explicitly compute the projections , when ( [ r.alt ] ) is applied .",
    "the above algorithm has a straightforward translation to code ; see e.g.  the _ pdcor _ package @xcite for an implementation in r.",
    "the population distance covariance has been defined in terms of the joint and marginal characteristic functions of the random vectors . here",
    "we give an equivalent definition following lyons @xcite , who generalizes distance correlation to separable hilbert spaces . instead of starting with the distance matrices @xmath34 and @xmath229 the starting point of the population definition are the bivariate distance functions @xmath230 and @xmath231 where @xmath28 are realizations of the random variables @xmath1 and @xmath232 are realizations of the random variable @xmath2 .",
    "we can also consider the random versions .",
    "let @xmath233 and @xmath234 be random variables with finite expectations .",
    "the random distance functions are @xmath235 and @xmath236 . here",
    "the primed random variable @xmath1 denotes an independent and identically distributed ( iid ) copy of the variable @xmath1 , and similarly @xmath237 are iid .",
    "the population operations of double centering involves expected values with respect to the underlying population random variable . for a given random variable @xmath1 with cdf @xmath238 , we define the corresponding _",
    "double centering function with respect to _ @xmath1 as @xmath239 provided the integrals exist .    here",
    "@xmath240 is a real valued function of two realizations of @xmath1 , and the subscript @xmath1 references the underlying random variable .",
    "similarly for @xmath241 iid with cdf @xmath238 , we define the random variable @xmath242 as an abbreviation for @xmath243 , which is a random function of @xmath244 .",
    "similarly we define @xmath245 and the random function @xmath246 .",
    "now for @xmath247 iid , and @xmath237 iid , such that @xmath248 have finite expectations , the population distance covariance @xmath249 is defined by @xmath250.\\ ] ] the definition ( [ e : dcov2 ] ) of @xmath54 is equivalent to the original definition ( [ e : aw ] ) .",
    "however , as we will see in the next sections , ( [ e : dcov2 ] ) is an appropriate starting point to develop the corresponding definition of pdcov and pdcor population coefficients .",
    "more generally , we can consider _ dissimilarity functions _ @xmath251 . in this paper ,",
    "a dissimilarity function is a symmetric function @xmath252 with @xmath253 .",
    "the corresponding random dissimilarity functions @xmath254 are random variables such that @xmath255 , and @xmath256 .",
    "double centered dissimilarities are formally defined by the same equations as double centered distances in ( [ e : hatax ] ) .",
    "the following lemma establishes that linear combinations of double - centered dissimilarities are double - centered dissimilarities .",
    "[ lemma3 ] suppose that @xmath233 , @xmath234 , @xmath257 is a dissimilarity on @xmath258 , and @xmath259 is a dissimilarity on @xmath260 .",
    "let @xmath261 and @xmath262 denote the dissimilarity obtained by double - centering @xmath257 and @xmath259 , respectively",
    ". then if @xmath263 and @xmath264 are real scalars , @xmath265 where @xmath266 \\in \\mathbb r^p \\times \\mathbb r^q$ ] , and @xmath267 is the result of double - centering @xmath268 .",
    "see appendix [ prflemma3 ] for the proof .",
    "the linear span of double - centered distance functions @xmath261 is a subspace of the space of double - centered dissimilarity functions with the property @xmath269 . in this case",
    "all integrals in ( [ e : hatax ] ) and @xmath270 are finite if @xmath1 has finite expectation .",
    "the linear span of the random functions @xmath270 for random vectors @xmath1 with @xmath271 is clearly a linear space , such that the linear extension of ( [ e : dcov2 ] ) @xmath272 = \\mathcal v^2(x , y)\\ ] ] to the linear span is an inner product space or pre - hilbert space ; its completion with respect to the metric arising from its inner product @xmath273\\ ] ] ( and norm ) is a hilbert space which we denote by @xmath274 .",
    "[ defpdcov ] introduce the scalar coefficients @xmath275 if @xmath276 define @xmath277 .",
    "the double - centered projections of @xmath270 and @xmath278 onto the orthogonal complement of @xmath279 in hilbert space @xmath274 are defined @xmath280 or in short @xmath281 and @xmath282 , where @xmath279 denotes double - centered with respect to the random variable @xmath283 .",
    "the population partial distance covariance is defined by the inner product @xmath284.\\ ] ]    [ defpdcor ] population partial distance correlation is defined @xmath285 where @xmath286 if @xmath287 we define @xmath288 .    note that if @xmath277 , we have @xmath289 = \\mathcal v^2(x , y ) , $ ] and @xmath290 .",
    "the population coefficient of partial distance correlation can be evaluated in terms of the pairwise distance correlations using formula ( [ pdcor.pop ] ) below .",
    "theorem [ t : pdcor.pop ] establishes that ( [ pdcor.pop ] ) is equivalent to definition [ defpdcor ] , and therefore serves as an alternate definition of population pdcor .",
    "[ t : pdcor.pop ] the following definition of population partial distance correlation is equivalent to definition [ defpdcor ] .",
    "@xmath291 where @xmath16 denotes the population distance correlation .",
    "the proof of theorem [ t : pdcor.pop ] is given in appendix [ prfthm3 ]      partial distance correlation is a scalar quantity that captures dependence , while conditional inference ( conditional distance correlation ) is a more complex notion as it is a function of the condition . although one might hope that partial distance correlation @xmath292 if and only if @xmath1 , and @xmath2 are conditionally independent given @xmath283 , this is , however , not the case . both notions capture",
    "_ overlapping _ aspects of dependence , but mathematically they are not equivalent .",
    "the following examples illustrate that @xmath293 is not equivalent to conditional independence of @xmath1 and @xmath2 given @xmath283 .",
    "suppose that @xmath294 are iid standard normal variables , @xmath295 , @xmath296 , and @xmath297 .",
    "then @xmath298 , @xmath299 , @xmath300 , and @xmath1 and @xmath2 are conditionally independent of @xmath283",
    ". one can evaluate @xmath301 by applying ( [ pdcor.pop ] ) and the following result .",
    "suppose that @xmath75 are jointly bivariate normal with correlation @xmath302 .",
    "then by ( * ? ? ?",
    "* theorem 7(ii ) ) , @xmath303 in this example @xmath304 , @xmath305 , and @xmath306 .    in the other direction we can construct a trivariate normal @xmath135 such that @xmath307 but @xmath308 .",
    "according to baba , shibata and sibuya @xcite , if @xmath309 are trivariate normal , then pcor@xmath310 if and only if @xmath1 and @xmath2 are conditionally independent given @xmath283 .",
    "a specific numerical example is as follows . by inverting equation ( [ cor2dcor ] ) ,",
    "given any @xmath311 , one can solve for @xmath312 in @xmath313 $ ] .",
    "when @xmath314 , and @xmath315 , the nonnegative solutions are @xmath316 , @xmath317 .",
    "the corresponding @xmath318 correlation matrix is positive definite .",
    "let @xmath309 be trivariate normal with zero means , unit variances , and correlations @xmath316 , @xmath317 .",
    "then pcor@xmath319 @xmath320 and therefore conditional independence of @xmath1 and @xmath2 given @xmath283 does not hold , while @xmath321 is exactly zero .    in the non - gaussian case , it is not true that zero partial correlation is equivalent to conditional independence , while partial distance correlation has the advantages that it can capture non - linear dependence , and is applicable to vector valued random variables .",
    "we have defined partial distance correlation , which is bias corrected and has an informal interpretation , but we also want to determine whether @xmath322 is significantly different from zero , which is not a trivial problem .",
    "although theory and implementation for a consistent test of multivariate independence based on distance covariance is available @xcite , clearly the implementation of the test by randomization is not directly applicable to the problem of testing for zero partial distance correlation .    since @xmath323 if and only if @xmath324 , we develop a test for @xmath325 vs @xmath326 based on the inner product @xmath327 .",
    "unlike the problem of testing independence of @xmath1 and @xmath2 , however , we do not have the distance matrices or samples corresponding to the projections @xmath131 and @xmath132 . for a test",
    "we need to consider that @xmath131 and @xmath132 are arbitrary elements of @xmath103 .",
    "we have proved ( theorem [ thmmds ] ) that @xmath328 can be represented as the @xmath20-centered distance matrix of some configuration of @xmath85 points @xmath21 in a euclidean space @xmath7 , @xmath101 , and that @xmath329 has such a representation @xmath22 in @xmath8 , @xmath330 .",
    "hence , a test for @xmath324 can be defined by applying the distance covariance test statistic @xmath202 .",
    "the resulting test procedure is practical to apply , with similar computational complexity as the original dcov test of independence .",
    "one can apply the test of independence implemented by permutation bootstrap in the ` dcov.test ` function of the energy package @xcite for r. alternately a test based on the inner product ( [ pdcov ] ) is implemented in the _ pdcor _ @xcite package .",
    "in this section we summarize simulation results for tests of the null hypothesis of zero partial distance correlation and two applications .",
    "we compared our simulation results with two types of tests based on linear correlation .",
    "methods for dissimilarities are illustrated using data on genetic distances for samples of maize in example [ ex6 ] , and variable selection is discussed in example [ ex7 ] .",
    "the test for zero partial distance correlation is a test of whether the inner product @xmath331 of the projections is zero .",
    "we obtain the corresponding samples @xmath332 such that the @xmath20-centered distances of @xmath21 and @xmath22 are identical to @xmath328 and @xmath329 , respectively , using classical ( metric ) mds .",
    "we used the ` cmdscale ` function in r to obtain the samples @xmath21 and @xmath22 in both cases .",
    "the _ pdcov _",
    "test applies the inner product of the double centered distance matrices of @xmath21 and @xmath22 as the test statistic .",
    "alternately , one could apply the original _ dcov _ test , to the joint sample @xmath333 .",
    "these two test statistics are essentially equivalent , but the _ pdcov _ test applies an unbiased statistic defined by ( [ vu ] ) , while the _ dcov _ test applies a biased statistic , @xmath334 ( [ e : anxy ] ) . both tests are implemented as permutation tests , where for a test of @xmath335 the sample indices of the @xmath1 sample are randomized for each replicate to obtain the sampling distribution of the test statistic under the null hypothesis .",
    "the term ` permutation test ' is sometimes restricted to refer to the exact test , which is implemented by generating all possible permutations . except for very small samples ,",
    "it is not computationally feasible to generate all permutations , so a large number of randomly generated permutations are used .",
    "this approach , which we have implemented , is sometimes called a randomization test to distinguish it from an exact permutation test .    in our simulations",
    "the _ pdcov _ and _ dcov _ tests for zero partial distance correlation were equivalent under null or alternative hypotheses in the sense that the type 1 error rates and estimated power agreed to within one standard error . in power comparisons , therefore , we reported only the _ pdcov _ result .",
    "the linear partial correlation @xmath336 measures the partial correlation between one dimensional data vectors @xmath29 and @xmath206 with @xmath123 removed ( or controlling for @xmath123 ) .",
    "the sample partial correlation coefficient is @xmath337 where @xmath338 denotes the linear ( pearson ) sample correlation .",
    "the partial correlation test is usually implemented as a @xmath66 test ( see e.g.  legendre @xcite ) . in examples where @xmath29 , @xmath206 , @xmath123 are one dimensional , we have included the partial correlation @xmath66-test in comparisons .",
    "however , for small samples and some non - gaussian data , the type 1 error rate is inflated . in cases where type 1 error rate of _ pcor _ was not controlled",
    "we did not report power .",
    "the partial mantel test is a test of the hypothesis that there is a linear association between the distances of @xmath1 and @xmath2 , controlling for @xmath283 .",
    "this extension of the mantel test @xcite was proposed by smouse , et al .",
    "@xcite for a partial correlation analysis on three distance matrices .",
    "the mantel and partial mantel tests are commonly applied in community ecology ( see e.g.  legendre and legendre @xcite ) , population genetics , sociology , etc .",
    "let @xmath339 denote the upper triangles of the @xmath69 distance matrices of the @xmath1 , @xmath2 and @xmath283 samples , respectively .",
    "then the partial mantel test statistic is the sample linear partial correlation between the @xmath340 elements of @xmath341 and @xmath342 controlling for @xmath343 .",
    "let @xmath344 be the corresponding data vectors obtained by representing @xmath339 as vectors .",
    "the partial mantel statistic is @xmath345 , computed using formula ( [ pcor.stat ] ) .",
    "since @xmath346 are not iid samples , the usual @xmath66 test is not applicable , so the partial mantel test is usually implemented as a permutation ( randomization ) test .",
    "see legendre @xcite for a detailed algorithm and simulation study comparing different methods of computing a partial mantel test statistic . based on the results reported by legendre , we implemented the method of permutation of the raw data .",
    "the algorithm is given in detail on page 44 by legendre @xcite , and it is very similar to the algorithm we have applied for the _ energy _ tests ( _ pdcov _ and _ dcov _ tests for zero pdcor ) .",
    "this method ( permutation of raw data ) for the partial mantel test is implemented in the _ ecodist _ package @xcite and also the _ vegan _ package @xcite for r. for simulations , the ` mantel ` function in the _ ecodist _ package , which is implemented mainly in compiled code , is much faster than the ` mantel ` or ` mantel.partial ` functions in the _ vegan _ package , which are implemented in r.    [ covdist ] both the _ pdcov _ and partial mantel ( _ mantel _ ) tests are based on distances .",
    "one may ask `` is distance covariance different or more general than covariance of distances ? ''",
    "the answer is yes ; it can be shown that @xmath347 where @xmath348 and @xmath349 are iid .",
    "the _ dcov _",
    "tests are tests of independence of @xmath1 and @xmath2 ( @xmath350 ) , while the mantel test is a test of the hypothesis @xmath351 .",
    "an example of dependent data such that their distances are uncorrelated but @xmath352 is given e.g. by lyons @xcite .",
    "thus , distance covariance tests are more general than mantel tests , in the sense that distance covariance measures all types of departures from independence .      in each permutation",
    "test @xmath353 replicates are generated and the estimated @xmath161-value is computed as @xmath354 where @xmath355 is the indicator function , @xmath356 is the observed value of the test statistic , and @xmath357 is the statistic for the @xmath358-th sample . the test is rejected at significance level @xmath359 if @xmath360 . the partial correlation test ( _ pcor _ ) is also included for comparison",
    "it is implemented as a t - test @xcite .",
    "type 1 error rates and estimated power are determined by a simulation size of 10,000 tests in each case ; for @xmath361 the number of tests is 100,000 .",
    "the standard error is at most 0.005 ( 0.0016 for @xmath361 ) .",
    "[ ex1a ] this example is a comparison of type 1 error on uncorrelated standard normal data .",
    "the vectors @xmath1 , @xmath2 , and @xmath283 are each iid standard normal .",
    "results summarized in table [ t : ex1a ] show that type 1 error rates for _ pdcov _ , _ dcov _ , and _ partial mantel _ tests are within two se of the nominal significance level , while type 1 error rates for pcor are inflated for @xmath362 .",
    ".example [ ex1a ] : type 1 error rates at nominal significance level @xmath359 for uncorrelated standard trivariate normal data.[t : ex1a ] [ cols=\">,>,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]     here the goal was to fit a linear model to predict the response _ lpsa _ given one or more of the predictor variables above .",
    "the train / test set indicator is in the last column of the data set .",
    "for comparison with the discussion and analysis in hastie et al .",
    "@xcite , we standardized each variable , and used the training set of 67 observations for variable selection .",
    "feature screening by distance correlation has been investigated by li et al .",
    "@xcite . in this example",
    "we introduce a partial distance correlation criterion for variable selection . for simplicity",
    ", we implement a simple variant of forward selection .",
    "this criterion , when applied for a linear model , can help to identify possible important variables that have strong non - linear association with the response , and thus help researchers to improve a linear model by transforming variables or improve prediction by extending to a nonlinear model .    in the initial step of pdcor forward",
    "selection , the first variable to enter the model is the variable @xmath363 for which distance correlation @xmath364 with response @xmath206 is largest . after the initial step , we have a model with one predictor @xmath363 , and we compute pdcor@xmath365 , for the variables @xmath366 not in the model , then select the variable @xmath367 for which pdcor@xmath365 is largest .",
    "then continue , at each step computing @xmath368 for every variable @xmath363 not yet in the model , where @xmath11 is the vector of predictors currently in the model .",
    "the variable to enter next is the one that maximizes @xmath369 .    according to the pdcor criterion",
    ", the variables selected enter the model in the order : _ lcavol , lweight , svi , gleason , lbph , pgg45_.    if we set a stopping rule at 5% significance for the pdcor coefficient , then we stop after adding _ lbph _ ( or possibly after adding _ pgg45 _ ) . at the step where _ gleason _ and _ lbph _ enter , the @xmath161-values for significance of pdcor are 0.016 and 0.001 , respectively .",
    "the @xmath161-value for _ pgg45 _ is approximately 0.05 .",
    "the models selected by pdcor , best subset method , and lasso ( hastie et al .",
    "* table 3.3 , p. 63 ) ) are : @xmath370 the order of selection for ordinary forward stepwise selection ( cp ) is _ lcavol , lweight , svi , lbph , pgg45 , age , lcp , gleason_.    comparing pdcor forward selection with lasso and forward stepwise selection , we see that the results are similar , but _ gleason _ is not in the lasso model and enters last in the forward stepwise selection , while it is the fourth variable to enter the pdcor selected model .",
    "the raw gleason score is an integer from 2 to 10 which is used to measure how aggressive is the tumor , based on a prostate biopsy .",
    "plotting the data ( see figure [ f : prostate1 ] ) we can observe that there is a strong _ non - linear _ relation between _ gleason _ and _",
    "lpsa_.    this example illustrates that partial distance correlation has practical use in variable selection and in model checking .",
    "if we were using pdcor only to check the model selected by another procedure , it would show in this case that there is some nonlinear dependence remaining between the response and the predictors excluded from the lasso model or the best subsets model . using the pdcor selection approach",
    ", we also learn which of the remaining predictors may be important .",
    "finally , it is useful to recall that pdcor has more flexibility to handle predictors that are multi - dimensional .",
    "one may want groups of variables to enter or leave the model as a set .",
    "it is often the case that when dimension of the feature space is high , many of the predictor variables are highly correlated . in this case , methods such as partial least squares are sometimes applied , where a small set of derived predictors ( linear combinations of features ) become the predictor variables . using methods of partial distance correlation , one could evaluate the subsets ( as predictor sets ) without reducing the multivariate observation to a real number via linear combination .",
    "the pdcor coefficient can be computed for multivariate predictors ( and for multivariate response ) .",
    "partial distance covariance and partial distance correlation coefficients are measures of dependence of two random vectors @xmath1 , @xmath2 , controlling for a third random vector @xmath283 , where @xmath1 , @xmath2 , and @xmath283 are in arbitrary , not necessarily equal dimensions .",
    "these definitions are based on an unbiased distance covariance statistic , replacing the original double centered distance matrices of the original formula with @xmath20-centered distance matrices .",
    "the @xmath20-centered distance covariance is the inner product in the hilbert space @xmath103 of @xmath20-centered distance matrices for samples of size @xmath85 , and it is unbiased for the squared population distance covariance .",
    "each element in the hilbert @xmath103 is shown to be the @xmath20-centered distance matrix of some configuration of @xmath85 points in a euclidean space @xmath7 , @xmath101 .",
    "the proof and the solution are obtained through application of theory of classical mds .",
    "this allows one to define dcor and dcov for dissimilarity matrices , and provides a statistically consistent test of independence based on the inner product statistic .",
    "this methodology also provides a similar test of the hypothesis of zero partial distance correlation based on the inner product .",
    "all pdcor and pdcov methods have been implemented and simulation studies illustrate that the tests control the type 1 error rate at its nominal level .",
    "power performance was superior compared with power of partial correlation and partial mantel tests for association .",
    "methods have been implemented and illustrated for non - euclidean dissimilarity matrices .",
    "a ` pdcor forward selection ' method was applied to select variables for a linear model , with a significance test as the stopping rule .",
    "more sophisticated selection methods will be investigated in future work . with the flexibility to handle multivariate response and/or multivariate predictor",
    ", pdcor offers a new method to extend the variable selection toolbox .",
    "software is available in the _ energy _",
    "@xcite package for r , and the _ pdcor _ @xcite package for r.",
    "proposition [ p : unbiased ] asserts that @xmath371 is an unbiased estimator of the population coefficient @xmath54 . when the terms of @xmath371 are expanded , we have a linear combination of terms @xmath372 .",
    "the expected values of these terms differ according to the number of subscripts that agree .",
    "define @xmath373=e[|x - x'| ] , \\qquad \\beta : = e[b_{kl}]=e[|y - y'| ] , \\qquad k \\neq l,\\\\   \\delta & : = e[a_{kl}b_{kj } ] = e[|x - x'||y - y''| ] , \\qquad j , k , l \\ ; \\mathrm { distinct},\\\\   \\gamma & : = e[a_{kl}b_{kl}]=e[|x - x'||y - y'| ] , \\qquad k \\neq l,\\end{aligned}\\ ] ] where @xmath348 @xmath349 are iid .",
    "due to symmetry , the expected value of each term in the expanded expression @xmath371 is proportional to one of @xmath374 , @xmath375 , or @xmath376 , so the expected value of @xmath371 can be written as a linear combination of @xmath377 , @xmath375 , and @xmath376 .",
    "the population coefficient can be written as ( see ( * ? ? ?",
    "* theorem 7 ) ) @xmath378 + e[|x - x'|]\\,e[|y\\!-\\!y'| ] - 2e[|x - x'||y\\!-\\!y''| ] \\\\ & = \\gamma + \\alpha \\beta - 2 \\delta.\\end{aligned}\\ ] ]    let us adopt the notation @xmath379 , @xmath380 , and @xmath381 , where @xmath382 , @xmath383 , and @xmath384",
    ". similarly define @xmath385 , and @xmath386 .",
    "then @xmath387 @xmath388    let @xmath389 then @xmath390    it is easy to see that @xmath391=n(n-1)\\gamma$ ] . by expanding the terms of @xmath392 and @xmath393 , and combining terms that have equal expected values",
    ", one can obtain @xmath394 = n(n-2)\\{(n-2)(n-3)\\alpha \\beta + 2 \\gamma + 4(n-2)\\delta\\},\\ ] ] and @xmath395 = n(n-1)\\{(n-2)\\delta + \\gamma\\}.\\ ] ] then @xmath396 & = \\frac{1}{n(n-3 ) } e\\left [ t_1 - \\frac{t_2}{(n-1)(n-2)^2 } -\\frac{2t_3}{n-2}\\right ] \\\\ & = \\frac{1}{n(n-3 ) } \\left\\ { \\frac{n^3 - 5n^2 + 6n}{n-2 } \\gamma + n(n-3)\\alpha \\beta + ( 6n-2n^2)\\delta \\right \\ } \\\\ & = \\gamma + \\alpha \\beta - 2 \\delta = \\mathcal v^2(x , y).\\end{aligned}\\ ] ]      here the inner product is ( [ vu ] ) and the ` vectors ' are @xmath20-centered elements of the hilbert space @xmath397 .",
    "equation ( [ r.alt ] ) can be derived from ( [ pdcor2 ] ) using similar algebra with inner products as used to obtain the representation @xmath398 for the linear correlation @xmath399 ( see e.g.  huber @xcite ) .",
    "the details for @xmath400 are as follows .",
    "if either @xmath401 or @xmath402 then @xmath403 so by definition @xmath404 , @xmath405 , and ( [ r.alt ] ) is also zero .    if @xmath406 but @xmath407 , then @xmath408 . in this case @xmath409 and @xmath410 , so that @xmath411 which equals expression ( [ r.alt ] ) since @xmath408 .",
    "suppose that none of @xmath412 are zero .",
    "then @xmath413 similarly , in the denominator of ( [ pdcor2 ] ) we have @xmath414 thus , if the denominator of ( [ pdcor2 ] ) is not zero , the factor @xmath415 cancels from the numerator and denominator and we obtain ( [ r.alt ] ) .",
    "the sum of the first row of @xmath71 is @xmath416 similarly each of the rows of @xmath71 sum to zero , and by symmetry the columns also sum to zero , which proves statement ( i )",
    ".    statements ( ii ) and ( iii ) follow immediately from ( i ) . in (",
    "iv ) let @xmath417 be the @xmath418-th element of @xmath43 .",
    "then @xmath419 , and for @xmath420 , @xmath421 . hence @xmath422 , @xmath420 and @xmath423 .",
    "therefore @xmath424 which proves ( iv ) .",
    "it is clear that @xmath425 is identical to the double - centered dissimilarity @xmath426 .",
    "it remains to show that the sum of two arbitrary elements @xmath427 is a double - centered dissimilarity function .",
    "let @xmath266 \\in \\mathbb r^p \\times \\mathbb r^q$ ] .",
    "consider the dissimilarity function @xmath428 , where @xmath429 $ ] and @xmath430 $ ] . then @xmath431df_t(t ' ) \\\\ & \\qquad - \\int [ a(x , x')+b(y , y')]df_t(t ) \\\\ & \\qquad + \\iint [ a(x , x')+b(y , y')]df_t(t')df_t(t ) \\\\ & = a(x , x ' ) + b(y , y ' )   -e[a(x , x')+b(y , y ' ) ]   \\\\ & \\qquad - e[a(x , x')+b(y , y ' ) ] + e[a(x , x')+b(y , y ' ) ] \\\\&=    a_x(x , x ' ) +    b_y(y , y').\\end{aligned}\\ ] ]      it is straightforward to check the first three special cases .    case ( i ) : if @xmath283 is constant a.s .",
    "then @xmath432 , and @xmath433 . in this case both @xmath321 and ( [ pdcor.pop ] ) simplify to @xmath311 .",
    "case ( ii ) : if @xmath1 or @xmath2 is constant a.s .",
    "and @xmath283 is not a.s .",
    "constant , then we have zero in both definition [ defpdcor ] and ( [ pdcor.pop ] ) .    case ( iii ) : if none of the variables @xmath434 are a.s .",
    "constant , but @xmath435 or @xmath436 , then @xmath437 by definition and @xmath438 or @xmath439 .",
    "thus ( [ pdcor.pop ] ) is also zero by definition .",
    "case ( iv ) : in this case none of the variables @xmath434 are a.s .",
    "constant , and @xmath440 .",
    "thus @xmath441 = ( a_x-\\alpha c_z , b_y-\\beta c_z )   \\\\ \\notag & = ( a_x , b_y ) - \\alpha ( b_y , c_z )   - \\beta ( a_x , c_z ) + \\alpha \\beta ( c_z , c_z )   \\\\ & = \\notag   \\mathcal v^2(x , y ) - \\frac{\\mathcal v^2(x , z)\\mathcal v^2(y , z)}{\\mathcal v^2(z , z ) } \\\\ & \\notag \\qquad - \\frac{\\mathcal v^2(x , z)\\mathcal v^2(y , z)}{\\mathcal v^2(z , z ) }   + \\frac{\\mathcal v^2(x , z)\\mathcal v^2(y , z ) \\mathcal v^2(z , z ) }   { \\mathcal v^2(z , z ) \\mathcal v^2(z , z ) }    \\\\ \\notag & =   \\mathcal v^2(x , y ) - \\frac{\\mathcal v^2(x , z)\\mathcal v^2(y , z)}{\\mathcal v^2(z , z)}.\\end{aligned}\\ ] ] similarly @xmath442 and @xmath443 hence , using ( [ e : iv-1])([e : iv-3 ] ) @xmath444 thus , in all cases definition [ defpdcor ] and ( [ pdcor.pop ] ) coincide .",
    "the authors thank russell lyons for several helpful suggestions and comments on a preliminary draft of this paper .",
    "baba , k. , shibata , r. and sibuya , m. ( 2004 ) .",
    "partial correlation and conditional correlation as measures of conditional independence . _ australian and new zealand journal of statistics _ * 46 * ( 4 ) : 657664 .",
    "kong , j. , klein , b. e. k. , klein , r. , lee , k. , and wahba , g. ( 2012 ) . using distance correlation and ss - anova to assess associations of familial relationships , lifestyle factors , diseases , and mortality , _ proc . of the national acad . of sciences",
    "_ , * 109 * ( 50 ) , 2035220357 .",
    "doi:10.1073/pnas.1217269109                  jari oksanen , f. guillaume blanchet , roeland kindt , pierre legendre , peter r. minchin , r. b. ohara , gavin l. simpson , peter solymos , m. henry h. stevens and helene wagner ( 2013 ) .",
    "vegan : community ecology package .",
    "r package version 2.0 - 7 .",
    "http://cran.r-project.org/package=vegan        reif , j. c. , a. e. melchinger , x. c. xia , m. l. warburton , d. a. hoisington , s. k. vasal , g. srinivasan , m. bohn , and m. frisch ( 2003 ) .",
    "genetic distance based on simple sequence repeats and heterosis in tropical maize populations , _ crop science _ * 43 * , no .",
    "4 , 12751282 .        schoenberg , i. j. ( 1935 ) .",
    "remarks to maurice frchet s article `` sur la definition axiomatique dune classe despace distancis vectoriellement applicable sur lespace de hilbert . '' _ ann .",
    "_ , * 36 * , 724732 .",
    "stamey , t. a. , kabalin , j. n. , mcneal , j. e. , johnstone , i. m. , freiha , f. , redwine , e. a. and yang , n. ( 1989 ) .",
    "prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate : ii .",
    "radical prostatectomy treated patients , _ journal of urology _ * 141*(5 ) , 10761083 .",
    "wermuth , n. and cox , d. r. ( 2013 ) .",
    "concepts and a case study for a flexible class of graphical markov models . in becker ,",
    "c. , fried , r. and kuhnt , s. ( eds . ) _ robustness and complex data structures .",
    "festschrift in honour of ursula gather .",
    "_ springer , heidelberg , 331350 ."
  ],
  "abstract_text": [
    "<S> distance covariance and distance correlation are scalar coefficients that characterize independence of random vectors in arbitrary dimension . </S>",
    "<S> properties , extensions , and applications of distance correlation have been discussed in the recent literature , but the problem of defining the partial distance correlation has remained an open question of considerable interest . </S>",
    "<S> the problem of partial distance correlation is more complex than partial correlation partly because the squared distance covariance is not an inner product in the usual linear space . for the definition of partial distance correlation </S>",
    "<S> we introduce a new hilbert space where the squared distance covariance is the inner product . </S>",
    "<S> we define the partial distance correlation statistics with the help of this hilbert space , and develop and implement a test for zero partial distance correlation . </S>",
    "<S> our intermediate results provide an unbiased estimator of squared distance covariance , and a neat solution to the problem of distance correlation for dissimilarities rather than distances . </S>"
  ]
}