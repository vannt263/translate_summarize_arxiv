{
  "article_text": [
    "rapid advances in technology have empoweredresearchers to collect data on a large number of explanatory variables to predict many outcomes of interest @xcite . because the relationship between an outcome @xmath8 and its predictors @xmath9 may be highly nonlinear and involve interaction ,",
    "there is a practical need to investigate statistical estimation under multivariate regression models @xmath10 with minimal assumptions made on @xmath11 .",
    "the quality of estimation that may be achieved under an assumed model can be mathematically quantified by the minimax risk of estimating @xmath11 from @xmath12 data points . a classic result due to charles stone",
    "@xcite states that if no assumption is made on how @xmath11 depends on @xmath13 other than requiring it to be differentiable with a smoothness level @xmath14 ( definition below ) , then the associated minimax risk decays in @xmath12 at a rate @xmath15 .",
    "this rate is very slow when @xmath2 is large , which means a very large sample size is needed for high quality statistical estimation  a phenomenon that has been termed the `` curse of dimensionality . ''",
    "the curse of dimensionality becomes even more pronounced in the so - called large @xmath2 small @xmath12 setting , where the minimax risk decay rate is expressed as a function of both @xmath12 and @xmath2 , with @xmath2 growing faster than @xmath12 @xcite .",
    "practically motivated modeling assumptions must focus on nonparametric spaces of functions with lower inherent dimensions than the manifest dimension  @xmath2 .",
    "an example of such assumptions is    1 .",
    "@xmath11 potentially depends on all elements of @xmath16 , but @xmath17 itself lies in a low - dimensional manifold @xmath18 in the ambient space @xmath19 .",
    "it is well known that under m1 , the minimax rate is @xmath20 which is determined by the smoothness level @xmath21 of @xmath11 and the latent manifold dimension @xmath22 @xcite , but does not depend on the ambient dimension @xmath2 .",
    "various nonparametric regression techniques that operate on the ambient space and do not require estimation of the underlying manifold indeed achieve this minimax rate without any prior knowledge of @xmath22 or @xmath21 @xcite .",
    "however , for many high - dimensional applications , such as gene expression studies , a low - dimensional manifold assumption on @xmath17 may not be tenable or verifiable . in such cases ,",
    "one often uses the following sparsity inducing assumption :    1 .",
    "@xmath11 depends on a small subset of @xmath22 predictors with @xmath23 .",
    "m2 has served as the springboard for many widely used regression methods , including high - dimensional linear regression approaches , such as the lasso @xcite and the dantzig selector @xcite , and nonparametric regression methods with variable selection , such as the rodeo @xcite and the gaussian process regression @xcite .",
    "the latter two allow flexible shape estimation of @xmath11 and is able to capture interactions among the selected important predictors .",
    "however , in light of the classic result due to @xcite it is conceivable that when @xmath11 is allowed to be fully nonparametric , m2  should also suffer from the curse of dimensionality in a large @xmath2 small @xmath12 setting , unless @xmath22 is much much smaller than @xmath2 , that is , the regression function is assumed to be extremely sparse .",
    "a precise result that extends the work of @xcite to account for predictor selection is presented in section  [ sminimax ] .    to relax this assumption of extreme sparsity without having to completely give up on nonparametric shape flexibility ,",
    "we introduce a third modeling assumption :    1 .",
    "@xmath11 may depend on @xmath24 variables for some @xmath25 but admits an additive structure @xmath26 , where each component function @xmath27 depends on a small @xmath28 number of predictors .",
    "clearly , m3 subsumes m2 as a special case . in section  [ sminimax ] , we show m2 gives slowest minimax rates within m3 . at the opposite extreme",
    "is the modeling assumption that @xmath11 admits a completely additive structure with univariate components @xmath29 for which scalable algorithms have been devised @xcite and attractive minimax risk bounds have been derived albeit under the strong assumption that all component functions @xmath27 have the same smoothness level @xcite .    compared to either of these two extremes , m3 provides a much more practically attractive theory of large @xmath2 nonparametric regression . in theorem [ thmm3 ] , we derive sharp upper and lower bounds on the minimax @xmath0 estimation risk under m3 as a function of @xmath12 , @xmath2 , @xmath30 , component sizes @xmath31 and smoothness levels of @xmath32 which are allowed to have different levels of smoothness than one another .",
    "minimax rates under m2 and the completely additive structure of @xcite follow as corollaries to this general result .",
    "our calculations suggest that m3 offers a minimax risk that decays quickly in @xmath12 even when @xmath2 grows almost exponentially in @xmath12 , @xmath11 involves nearly @xmath33 many predictors and these predictors interact with each other .    in section  [ sgp ] , we demonstrate that a conceptually straightforward extension of the widely used gaussian process regression method ( see , e.g. , @xcite , for a review ) achieves the minimax rate adaptively across all subclasses of m3 under suitable large @xmath2 small @xmath12 asymptotics where @xmath2 grows almost exponentially in @xmath12 . in this paper",
    "we restrict only to a theoretical study of this new approach , which we name `` additive gaussian process regression . ''",
    "this approach appears entirely practicable with computational demands similar to those of the popular bayesian additive regression tree method @xcite .",
    "a full fledged methodological development of the same is underway and will be reported elsewhere .",
    "the rest of the paper is organized as follows .",
    "section  [ snotations ] introduces the notation and some basic assumptions .",
    "section  [ sminimax ] summarizes our main minimax results for high - dimensional nonparametric regression under m2 and m3 .",
    "section  [ sgp ] proves the adaptive minimax optimality of additive gaussian process regression .",
    "section  [ sproofs ] provides proofs of our main results in sections  [ sminimax ]  and  [ sgp ] . supporting technical results and proofs",
    "are presented in section  [ saux ] .",
    "[ snotations ] let @xmath34 , @xmath35 denote the observations on @xmath36 .",
    "we make a stochastic design assumption that @xmath37 are independent and identically distributed ( iid ) according to some compactly supported probability measure @xmath38 on @xmath19 and that @xmath39 , the linear space of real valued functions on @xmath19 equipped with inner product @xmath40 and norm @xmath41 .",
    "we do not need to know or estimate @xmath38 for the purpose of estimating @xmath11 , but it is a natural candidate to judge average prediction accuracy at future observations of @xmath17 drawn from @xmath38 , as will be the case under simple exchangeability assumptions . without loss of generality",
    "assume support  @xmath42^p$ ] .",
    "let @xmath43 stand for the @xmath0 norm under the lebesgue measure .",
    "the @xmath0 minimax risk of estimation associated with any function space @xmath44 is defined as @xmath45 where @xmath46 is the space of all measurable functions of data to @xmath47 and @xmath48 denotes expectation under the model : @xmath49 , @xmath50 , independently across @xmath35 . when no risk of ambiguity is present , we shorten the notation @xmath51 to @xmath52 and call @xmath52 the minimax risk or the minimax _ rate _ , when viewed as a function of the sample size @xmath12 .",
    "let @xmath53 denote the set of natural numbers and @xmath54 .",
    "for any dimensional multiindex @xmath55 define @xmath56 and let @xmath57 denote the mixed partial derivative operator @xmath58 . for any real number  @xmath59 , let @xmath60 denote the largest integer strictly smaller than @xmath59 .",
    "use the notation @xmath61 to denote the banach space of hlder @xmath21-smooth functions on @xmath62^d$ ] equipped  with the norm @xmath63^d } \\bigl{\\vert}d^{\\lfloor\\alpha\\rfloor}f(x ) - d^{\\lfloor\\alpha\\rfloor}(y)\\bigr{\\vert}/ { \\vert}x - y { \\vert}^{\\alpha - \\lfloor\\alpha \\rfloor}.\\ ] ] let @xmath64 denote the unit ball of @xmath61 .    for",
    "any @xmath65 and @xmath66 , let @xmath67 denote the vector of @xmath68 predictors picked by @xmath59 and let @xmath69 denote the mapping that takes an @xmath70 to @xmath71 .",
    "let @xmath72 denote the set of all @xmath73 with @xmath74 .",
    "we formalize the space of centered , @xmath2-variate , @xmath21-smooth functions of sparsity @xmath22 and bound @xmath75 as @xmath76 where @xmath77^p\\dvtx \\int f(x ) \\,dx=0\\}$ ] .",
    "the condition that @xmath11 is centered can be imposed without any loss of generality due to the presence of the overall mean parameter @xmath78 in our regression model .",
    "the function spaces @xmath79 make up  m2 . for m3 , we consider additive convolutions of multiple @xmath80 spaces with an additional restriction on the number of components a single predictor can appear in . for @xmath81 , @xmath82 define @xmath83 and @xmath84 in studying minimax rates for a fixed @xmath30 , one can set @xmath85 as large as @xmath30 .",
    "but in the more interesting large @xmath2 small @xmath12 scenario where @xmath30 increases with @xmath2 , the use of a fixed @xmath85 is crucial for interpreting our results .    for a metric space @xmath86 ,",
    "the covering @xmath87-entropy of a subset @xmath88 is the logarithm of the minimum number of @xmath89-balls of radius @xmath90 and centers in @xmath91 needed to cover @xmath92 , and is denoted @xmath93 .",
    "a finite subset @xmath94 is called @xmath90-packing in @xmath92 if any two elements of @xmath95 have a @xmath89-distance at least @xmath90 .",
    "the logarithm of the maximal cardinality of an @xmath87-packing set in @xmath92 is called the packing @xmath90-entropy of @xmath92 and is denoted @xmath96 .",
    "[ sminimax ] precise calculations of @xmath52 under m2 and m3 and theoretical results on whether these rates are achieved in practice are known only under additional simplifying assumptions on the shape of @xmath11 , or , for inference tasks that are simpler than prediction .",
    "we provide a brief overview of known results before presenting our main theorem on minimax @xmath0 risk for regression under m3 .      for linear regression where",
    "@xmath97 is taken as the set of functions @xmath98 with @xmath99 in an @xmath100 ( @xmath101 ) ball of @xmath19 and some additional regularity assumptions are made on the design matrix , @xcite shows that @xmath102$,}\\end{aligned}\\ ] ] up to some multiplicative constant , where @xmath22 is the number of important predictors .",
    "as shown in @xcite , these rates are the typical minimax risks associated with variable selection uncertainty . for @xmath103 , the @xmath100 norm",
    "precisely encodes the sparsity condition of @xmath104 .",
    "see @xcite and @xcite for additional results and overviews .",
    "many authors have established near minimax performance guarantees of various linear regression methods under the @xmath0 prediction loss ; see , for example , @xcite and  @xcite .    as a nonlinear , nonparametric generalization of the linear model",
    ", @xcite considers the completely additive special case of m3 where all @xmath30 components are univariate and have the same smoothness @xmath14 and shows @xmath105 clearly , the minimax risk decomposes into two terms , where the first term is the sum of minimax risks of estimating each component and the second term is the variable selection uncertainty .",
    "an entirely different generalization of the linear model is the sparse , fully nonparametric regression model m2 . to the best of our knowledge",
    ", the only minimax rates result in this context is @xcite , which analyzes minimax risks of support recovery where the objective is to identify the important predictor rather than estimation of @xmath11 itself .",
    "it is shown that if @xmath106 is lower bounded by some positive constant , then for some constant @xmath107 , @xmath108 where @xmath109 ranges over all variable selection estimators , that is , measurable maps of data to the space of all subsets of @xmath110 , @xmath97 is the space of all differentiable functions that depend on only @xmath22 many predictors and have squared integrable gradients , and @xmath111 is the index set of truly important predictors associated with @xmath11 . because of this result , we refer to the term @xmath106 as the risk associated with variable selection uncertainty . for large @xmath2 , @xmath112 is asymptotically of the same order as the logarithm of @xmath113 , the number of ways to select @xmath22 out of @xmath2 predictors .",
    "any estimation problem involving high - dimensional variable selection is likely to include a variable selection uncertainty term @xmath106 in its minimax rate .",
    "[ sem2m3 ] we calculate minimax @xmath0 risks under the following condition on the stochastic design :    [ assq ] @xmath38 admits a probability density function ( p.d.f . ) @xmath114 on @xmath62^p$ ] such that @xmath115 and @xmath116^p } q(x ) \\ge\\underline q$ ] for some @xmath117 and @xmath118 .",
    "the requirement of @xmath114 being lower bonded on some sub - hypercube inside @xmath62^p$ ] is crucial to obtaining sharp lower bounds on the minimax risk .",
    "this requirement is essentially equivalent to asking that @xmath17 can not be reduced to a lower dimension without some loss of information , for example , @xmath17 can not lie on a lower - dimensional subspace of manifold as assumed under .",
    "[ thmm3 ] under assumption [ assq ] , there exist @xmath119 , @xmath120 , all depending only on @xmath85 , @xmath121 , @xmath122 , @xmath123 , @xmath124 , @xmath125 , such that for all @xmath126 , @xmath127 where @xmath128 and @xmath129    by choosing @xmath130 and @xmath131 in theorem [ thmm3 ] , we obtain the minimax risk for m2 as a simple corollary , @xmath132    one can shed light on the scope and limitations of a model by investigating the conditions needed on the model parameters in order to bound the model s minimax risk by a given margin . from ( [ eqminimaxhsn ] ) ,",
    "the minimax risk of m2 consists of two terms .",
    "the second term is the typical risk associated with variable selection uncertainty @xcite which remains small as long as @xmath133 for some @xmath134 , which gives the standard large @xmath2 small @xmath12 dynamics between sample size and predictor count .",
    "the first term in ( [ eqminimaxhsn ] ) is the minimax risk of estimating a @xmath22-variate , @xmath21-smooth regression function @xmath135 when there is no variable selection uncertainty . for a fixed smoothness level @xmath21 ,",
    "this term remains small as long as @xmath136 under standard large @xmath2 small @xmath12 dynamics .",
    "in other words , meaningful statistical learning is possible under m2 only when the true number of important predictors is much much smaller than the total predictor count .",
    "m3 offers a platform to break away from such extreme sparsity conditions .",
    "we consider two special cases for illustration under a standard large @xmath2 small @xmath12 dynamic : @xmath137 for some @xmath134 , while allowing @xmath30 to depend on @xmath12 .",
    "first , suppose all additive components @xmath27 have the same dimension ( @xmath138 ) , smoothness ( @xmath139 ) and magnitude ( @xmath140 ) , all of which fixed as @xmath30 increases @xmath12 .",
    "this situation includes as a special case the completely additive framework of @xcite . from theorem [ thmm3 ] , the associated minimax risk @xmath141 which remains small as long as @xmath142 for some @xmath143 .",
    "thus , the total number of important predictors , which is of the order @xmath144 , could be as large as a fractional power of @xmath33 , a number that is much larger than what is allowed under m2 .",
    "in the second case , consider an unbalanced case where @xmath28 , @xmath145 vary with @xmath146 , but remain bounded as @xmath30 increases with @xmath12 , and the magnitudes diminish so that the series @xmath147 is convergent .",
    "theorem [ thmm3 ] suggests that a consistent estimator of @xmath11 exists in this case as long as @xmath148 , that is , the total number of important predictors is @xmath149 .",
    "consider another unbalanced scenario where @xmath30 is fixed and one additive component is much more complex than the rest , that is , @xmath150 for @xmath151 . in this case ,",
    "theorem [ thmm3 ] gives a minimax risk @xmath152 , where the first term is dominated by the largest risk of all additive components , while the second term is still determined by the overall variable selection uncertainty .",
    "therefore , the difficulty of estimating a function with an additive form is determined by the estimation difficulty of its `` hardest '' component .",
    "[ sgp ] a gaussian process ( gp ) on an euclidean set @xmath153 is a random element @xmath154 of the supremum - norm banach space of continuous functions over @xmath155 such that any linear functional of @xmath156 is univariate gaussian @xcite .",
    "the probability law of a gp @xmath156 is completely determined by the mean and covariance functions @xmath157 and @xmath158 and is denoted by @xmath159 . for any function @xmath160 and any nonnegative definite function @xmath161 , there exist a gp @xmath156 with law @xmath162 .",
    "adaptivity and near minimax optimality of bayesian gaussian process regression methods are known for low - dimensional applications @xcite . in gp regression , @xmath11 is assigned a @xmath162 prior and inference on @xmath11 is carried out by summarizing the resulting posterior distribution given data , which also remains a gp law  @xcite .",
    "theoretical treatments of gp regression have typically focused on @xmath163 and @xmath164 , the square exponential covariance function , with additional hyper - parameters inserted inside the covariance function @xcite . in particular , in order to achieve adaptation to unknown smoothness , @xcite considers as prior distribution the law of a rescaled process @xmath165 defined as @xmath166 where @xmath167 and @xmath168 follows a gamma distribution , and proves the resulting posterior distribution contracts to the true @xmath11 at the minimax rate @xmath169 up to a @xmath5 factor when @xmath11 is hlder @xmath21-smooth .",
    "extensions to anisotropic function spaces are carried out by @xcite .",
    "[ ssagp ] for a stochastic process @xmath170 , a scalar @xmath171 and a binary inclusion vector @xmath65 , define a _ selective - rescaled _ process @xmath172^p)$ ] by @xmath173 where @xmath174 is the elementwise product operator . toward a bayesian estimation of regression functions @xmath11 described by m3 , we consider the following additive gaussian process ( add - gp ) prior distribution on @xmath11 : @xmath175 where @xmath176 is a probability distribution on @xmath53 , and @xmath177 are iid copies of the process @xmath178 defined as : @xmath179 , @xmath180 and @xmath181 are mutually independent random elements distributed as @xmath182\\\\[-8pt]\\nonumber b & \\sim&\\biggl[\\bigotimes_{j = 1}^p \\operatorname{be}\\biggl(\\frac { 1}p\\biggr)\\biggr]\\bigg|_{{\\vert}b{\\vert}\\le d_0},\\qquad a^{{\\vert}b{\\vert } } \\bigr{\\vert}b \\sim\\operatorname{ga}(a_1,a_2),\\end{aligned}\\ ] ] where @xmath183 is a density function on @xmath184 and @xmath185 are prespecified , positive valued hyper - parameters .    to complete the add - gp prior specification , we need to specify a prior distribution on @xmath186 .",
    "we consider @xmath187 where @xmath188 is a gaussian distribution and @xmath189 admits density function on @xmath190 with a compact support inside @xmath191 .      for any @xmath192^p)^{\\infty}$ ] and any @xmath193 , let @xmath194 denote the conditional distribution of @xmath195 given @xmath196 , @xmath197 , under ( [ eqreg ] ) .",
    "let @xmath198 denote the posterior distribution of @xmath199 under the add - gp prior given @xmath200 , @xmath201 .",
    "following @xcite , the posterior contraction rate of the add - gp prior at any @xmath202 is said to be at least @xmath203 if for every @xmath204 , other than in a @xmath205-null set , @xmath206 as @xmath207 for some constant @xmath208 , where @xmath209 denotes an empirical version of the @xmath47 norm : @xmath210 .",
    "it is possible to replace @xmath211 with @xmath212 by appealing to the techniques developed for gp priors in section  2.4 in  @xcite , but we omit the details .",
    "[ thm2 ] under assumption [ assq ] , for any @xmath213 , @xmath214 and @xmath215 with @xmath216 and @xmath217 , the posterior contraction rate at @xmath218 is of the order @xmath219 where @xmath220 with @xmath221 , @xmath222 , provided @xmath223 .",
    "when @xmath2 grows with @xmath12 , add - gp regression essentially employs a sequence of priors changing with @xmath12 . in this case , it is possible and useful to also let @xmath224 grow with  @xmath12 and study posterior contraction rate at a sequence of @xmath225 changing with  @xmath12 .",
    "theorem [ thm2 ] remains valid as long as @xmath223 , the true number of components @xmath217 , @xmath226 are bounded from above and below and @xmath227 is bounded .",
    "related work on estimation of @xmath11 under m3 includes @xcite , where convergence rates are investigated for an @xmath208-estimator with a sparsity penalty on the number of additive components and smoothness penalties on each components .",
    "however , @xcite considers only univariate components . in @xcite",
    ", pac - bayesian bounds are derived for general additive regression with additive gp priors .",
    "however , @xcite assumes that the covariate vector @xmath17 is pre - divided into @xmath208 subsets @xmath228 and @xmath229 , with sparsity constraints on the component functions .",
    "both these studies assume that important predictors are not shared across components , which makes the studied methods somewhat restricted in application .",
    "a lack of overlap comes with the technical advantage that @xmath230 decomposes to @xmath231 if every @xmath27 has @xmath38-integral 0 . in the more general case where components are allowed to share predictors , a nave application of the cauchy ",
    "schwarz inequality gives @xmath232 , but the multiplication by @xmath30 results in sub - optimal rates unless @xmath224 grows extremely slowly in @xmath12 .",
    "our assumption that any predictor can appear in at most @xmath85 many components , for some fixed @xmath85 , overcomes this difficulty with the help of lemma [ lemoverlap ] .",
    "[ sear ] [ sproofs ]      [ lemsoln ] for every @xmath233 , @xmath234 there exist @xmath235 , @xmath236 , such that for any @xmath126 and all @xmath237 , the @xmath203 that solves @xmath238 satisfies @xmath239    let @xmath240 be as in lemma [ lemsparse ] . without loss of generality , @xmath241 .",
    "let @xmath242 and set @xmath243 large enough such that @xmath244 for all @xmath126 .",
    "for the remainder of this proof , abbreviate @xmath245 to @xmath246 .",
    "the arguments below mostly rest on the fact that @xmath90-packing entropy is nonincreasing in @xmath90 .",
    "note that @xmath247 where the second inequality follows by sticking in @xmath248 as @xmath90 in lemma [ lemsparse ] . hence , @xmath249 .",
    "also , by lemma [ lemsparse ] , @xmath250 & & \\qquad\\ge m_0 n \\max\\biggl\\{\\lambda\\biggl(\\frac{\\sqrt n \\lambda } { \\sigma } \\biggr)^{-2\\alpha/(2\\alpha+ d ) } , \\frac{\\sigma}{\\sqrt n } \\log ^{1/2 } \\pmatrix{p \\cr d } \\biggr \\}^2 \\big/ \\sigma^2\\end{aligned}\\ ] ] and hence @xmath251",
    ". this proves the result with @xmath252 and @xmath253 .",
    "proof of theorem [ thmm3 ] by theorem 6 of @xcite , the minimax risk @xmath52 is the solution to @xmath254 . for @xmath222 ,",
    "let @xmath255 be the solution to @xmath256 . from lemma [ lemsoln ] ,",
    "there are @xmath257 , @xmath258 , such that for all @xmath259 , @xmath260 .",
    "denote @xmath261 , @xmath262 .",
    "then , by theorem [ thmadd ] , with @xmath263 , @xmath264\\\\[-9pt]\\nonumber & \\ge&\\frac{1}{16 } \\frac{n{\\vert}\\delta_n { \\vert}^2}{\\sigma^2}\\end{aligned}\\ ] ] provided @xmath265 , and hence , @xmath266 for some @xmath267 .",
    "next , let @xmath268 where @xmath269 is the solution to @xmath270 , @xmath222 .",
    "by lemma [ lemsoln ] , there are @xmath257 , @xmath271 , such that for all @xmath259 , @xmath272 .",
    "set @xmath273 . by theorem [ thmadd ]",
    "again , @xmath274 , and hence @xmath275 completing the proof .      according to @xcite , theorem 1 and",
    "section  7.7 , and @xcite , the conclusion of theorem [ thm2 ] holds if for @xmath205-almost every @xmath204 there exist @xmath276 , @xmath277 , such that @xmath278 where @xmath279 and @xmath280 denotes the add - gp prior on @xmath281 .",
    "these conditions map to one to one to concentration properties of the selective - rescaled gaussian processes underlying the add - gp formulation . without loss of generality ,",
    "we assume the prior density @xmath183 on @xmath282 is a folded gaussian p.d.f . , and that .",
    "two important objects associated with any gaussian process are its reproducing kernel hilbert space ( rkhs ) and concentration function .",
    "the rkhs of any gp @xmath283 , with @xmath284 , is defined to be the set @xmath285 of all function @xmath286 that can be written as @xmath287 for some @xmath92 in the closure of the linear span of the collection of random variables @xmath288 in @xmath0 norm .",
    "the set @xmath285 is a hilbert space with @xmath289 . with @xmath156",
    "seen as an element in @xmath290 , its concentration function at any @xmath291 is defined as @xmath292 we make use of the following well - known inequalities involving the rkhs and the concentration function : @xmath293 see lemma 5.3 of @xcite for a proof of ( [ eqb1 ] ) .",
    "the inequality ( [ eqb2 ] ) is the well - known borell s inequality @xcite , and the right - hand side can be further bounded by @xmath294 when @xmath295 since @xmath296 for all @xmath297 .",
    "inequality ( [ eqb3 ] ) holds because the right - hand side gives an upper bound to @xmath298 , since , if @xmath299 are @xmath300-separated in @xmath301 then @xmath302 by ( [ eqb1 ] ) .    for any @xmath65 , @xmath171 , let @xmath303 and @xmath304 denote the rkhs and the concentration function of the selective - rescaled gp @xmath305 introduced in section  [ ssagp ] . by definition , @xmath305 is isomorphic to a @xmath22 dimensional , rescaled gp @xmath306 with @xmath307 on @xmath308 , whose rkhs and concentration function have been studied extensively in @xcite .",
    "the following results , which are direct consequences of lemmas 4.3 , 4.6 , 4.7 and 4.8 of @xcite , are of particular interest to us : @xmath309\\label{eqconc0 }   \\\\[-8pt ] \\eqntext{\\forall a \\ge a_0 , \\forall\\varepsilon < \\varepsilon_0 \\wedge{g_1 } { a^{-\\alpha } } , } \\\\",
    "\\label{eqcontain }   & \\displaystyle a_1^{{\\vert}b{\\vert}/2 } { \\mathbb{h}}^{a_1,b}_1 \\subset a_2^{{\\vert}b{\\vert}/2 } { \\mathbb{h}}^{a_2,b}_1\\qquad \\forall0 < a_1 < a_2 , & \\\\ \\label{eqconst }   & \\displaystyle h \\in{\\mathbb{h}}^{a , b}_1\\quad{\\longrightarrow}\\quad\\bigl{\\vert}h(0)\\bigr{\\vert}\\le1,\\qquad \\bigl{\\vert}h - h(0)\\bigr{\\vert}_\\infty\\le a { \\vert}b{\\vert}.&\\end{aligned}\\ ] ] in ( [ eqconc0 ] ) , the constants @xmath310 , @xmath311 depend only on @xmath312 and @xmath313 .    [ lecp ]",
    "suppose @xmath314 satisfies @xmath315 for some @xmath316 and @xmath223 .",
    "then there exists a sequence of sets @xmath317^d$ ] satisfying @xmath318 and @xmath319 with @xmath320 .",
    "let @xmath321 , where @xmath322 is a large constant to be determined later , and define @xmath323 for some constant @xmath324 . by ( [ eqconc0 ] ) , and",
    "the fact that @xmath325 is nondecreasing in @xmath326 , the constant @xmath324 can be chosen large enough so that @xmath327 for all large @xmath12 . set @xmath328 and take @xmath329 . for every @xmath330 , define @xmath331 \\cup\\{r\\ } \\setminus\\{0\\ } } 2 \\bar l_n m_n { \\mathbb{h}}^{a , b}_1 + \\frac{\\varepsilon _ n}{k_0 } { \\mathbb{b}}_1.\\ ] ] consider the sieves @xmath332 \\oplus\\bigcup _ { 1 \\le k \\le k_0 } \\mathop{\\mathop{\\bigcup_{b^1,\\ldots , b^k \\in\\bigcup _ { d \\le d_0 } \\mathcal{b}^{p , d},}}_{r_s \\in\\{0\\}\\cup\\delta_n(b^s ) , 1\\le s \\le k , } } _ { \\sum_s r_s^{{\\vert}b^s{\\vert } } \\le4r_n } \\mathcal{f}^{r_1,b^1}_n \\oplus\\cdots\\oplus\\mathcal{f}^{r_k , b^k}_n,\\ ] ] for @xmath277 .",
    "fix any @xmath333 , @xmath334 and @xmath335 satisfying @xmath336 . for @xmath222 ,",
    "if @xmath337 set @xmath338 , otherwise find @xmath339 such that @xmath340 .",
    "then @xmath341 and by ( [ eqcontain ] ) , @xmath342 for all @xmath343 .",
    "therefore , @xmath344 \\\\ & & \\qquad \\le e^{-n/2 } + k \\bigl\\{e^{-\\bar l^2_n } + e^{-m^2_n / 8}\\bigr\\ } \\\\ & & \\qquad \\le3ke^{-r_n}\\end{aligned}\\ ] ] for all large @xmath12 , by ( [ eqb2 ] ) and ( [ eqm2 ] ) and the fact @xmath345 . consequently , @xmath346 with @xmath347 .",
    "notice @xmath348 .",
    "therefore , by the assumption on @xmath224 , @xmath349 is bounded by @xmath350 for all large @xmath12 , provided @xmath322 is chosen suitably large .    by ( [ eqconst ] ) , when @xmath351 , @xmath352 + ( 2\\varepsilon_n / k_0 ) { \\mathbb{b}}_1 $ ] , and hence could be covered by @xmath353 many or fewer balls of supremum norm radius @xmath354 . when @xmath355 , by ( [ eqb3 ] ) , at most another @xmath356 many balls may be needed to maintain @xmath357 covering",
    "therefore , by ( [ eqconc0 ] ) , @xmath358 for every @xmath330 , for some constant @xmath359 that depends only on @xmath360 , as long as @xmath361 . consequently , @xmath362 where @xmath208 is the size of the finite set @xmath363 .",
    "this proves the result because of the assumption on @xmath224 , since @xmath364 \\le c_6 k_0 \\log p$ ] for some constant @xmath365 that depends only on @xmath360 .",
    "[ lepc ] under the conditions of theorem [ thm2 ] , for @xmath38-almost every @xmath204 , @xmath366 for all large @xmath12 where @xmath367 with @xmath368 , @xmath369 .    by lemma [ lenormc ] , with @xmath370 as in ( [ eqsieve2 ] ) , we only need to show @xmath371 . by inequality ( [ eqsieve1 ] ) and the fact @xmath372 it suffices to show that @xmath373 we can write @xmath374 where @xmath375 , @xmath376 , @xmath222 and @xmath377 . let @xmath378 , @xmath343 and @xmath379 . set @xmath380 .",
    "for any @xmath171 , @xmath65 define the gaussian variable @xmath381 .",
    "then the gaussian process @xmath382 satisfies @xmath383 , and is independent of the process @xmath384 where @xmath385 and @xmath386 , @xmath387^p$ ] . by cauchy ",
    "schwarz inequality , @xmath388 .",
    "clearly , @xmath305 decomposes as @xmath389 .",
    "therefore , for any @xmath390 and given @xmath391 , @xmath392 , @xmath222 , we can decompose the additive - gp process @xmath11 as @xmath393 , where @xmath394 are independent @xmath395 variables , @xmath396 are mutually independent with probability laws same as those of @xmath397 , and these two sets of random quantities are independent .",
    "consequently , for large enough @xmath12 , @xmath398 because of lemma [ lemoverlap ] , since by the assumption on @xmath399 and the construction of @xmath396 , we have for every @xmath222 , @xmath400 for at most @xmath401 many @xmath402 .",
    "if @xmath403 $ ] , then @xmath404 . when @xmath405 $ ] , where @xmath406 is as in ( [ eqconc0 ] ) , the last probability can be lower bounded by @xmath407 for some constant @xmath408 , for all large @xmath12 , by ( [ eqb1 ] ) and ( [ eqconc0 ] ) .",
    "for the same choices of @xmath409 , @xmath222 , @xmath410 for some constant @xmath411 , for all large @xmath12 .",
    "therefore , by the assumption on @xmath224 , @xmath412\\bigr)\\pi\\bigl(b^s = b^s\\bigr ) \\\\ & & \\hspace*{62pt } { } \\times\\pi \\bigl(a^{d_s}_s \\in({g_1}/ { \\delta_{ns}})^{d_s/\\alpha_s } \\cdot[1 , 2]{|}{\\vert}b_s { \\vert}= d_s\\bigr ) \\bigr\\ } \\\\ & & \\qquad \\ge g_5 \\exp\\biggl\\{-g_6 n\\biggl\\{{\\vert}\\delta_n { \\vert}^2 + \\frac{\\sigma^2 \\sum_s d_s}{n } \\log p\\biggr\\ } \\biggr\\}\\end{aligned}\\ ] ] for all large @xmath12 for some constants @xmath413 that depend only on @xmath121 , @xmath414 , @xmath227 , @xmath415 and @xmath123 .",
    "this proves the result since @xmath78 is independent of @xmath11 and @xmath416 for some constant @xmath417 .",
    "proof of theorem [ thm2 ] equations  ( [ eq1])([eq3 ] ) are implied by lemmas [ lepc ]  and  [ lecp ] with the @xmath203 given in theorem [ thm2 ] .",
    "[ saux ] in this section , we provide a number of auxiliary results on packing and covering entropies of regular , sparse and additive hlder spaces .",
    "[ lembasic ] for every @xmath14 , @xmath234 there exist @xmath418 , @xmath419 such that for all @xmath420 there are @xmath421 functions @xmath422 satisfying @xmath423 and @xmath424^d,\\qquad f_i { |}_{[0,1]^d } \\in c^{\\alpha ,",
    "d}_1,\\qquad0 \\leq i \\leq n , \\\\",
    "\\label{eqfzero }   \\qquad \\int_{\\mathbb{r}}f_i(u_1 , \\ldots , u_d ) \\,du_j & = & 0,\\qquad0 \\le i \\le n,1 \\le j \\le d , \\\\",
    "\\label{eqfsep}{\\vert}f_i - f_k { \\vert}&\\ge&\\varepsilon , \\qquad 0 \\le i < k \\le n.\\end{aligned}\\ ] ]    our proof follows the calculations in @xcite , section  2.6.2 , suitably adapted to handle @xmath0 norm and condition ( [ eqfzero ] ) .",
    "let @xmath425 such that @xmath426^d,\\qquad \\int\\mathcal{k } ( u_1 , \\ldots , u_d ) \\,du_j = 0 , j = 1 , \\ldots , d.\\ ] ] for example , one could take @xmath427 where @xmath428 , @xmath429 .",
    "fix an arbitrary @xmath430 and take @xmath431 , @xmath432 and a rectangular grid @xmath433 on @xmath62^d$ ] consisting of the @xmath208 grid points @xmath434 , @xmath435 .",
    "we assume @xmath183 is small enough so that @xmath436 . for each @xmath437 , the function @xmath438 defined as @xmath439^d\\ ] ]",
    "has support inside @xmath440^d$ ] and belongs to @xmath64 .",
    "let @xmath441 and for each @xmath442 define @xmath443 . clearly ,",
    "each @xmath444 is supported on @xmath62^d$ ] and @xmath445 for every @xmath446 .",
    "also , since @xmath438 s are shifted copies of each other with disjoint supports , each @xmath447 and @xmath448\\\\[-8pt]\\nonumber & = & h^{\\alpha+d/2}\\frac{{\\vert}\\mathcal{k } { \\vert}}{{\\vert}\\mathcal{k } { \\vert}_{c^{\\alpha , d } } } \\rho ^{1/2}\\bigl(\\omega , \\omega'\\bigr),\\end{aligned}\\ ] ] where @xmath449 denotes the hamming distance .    by the varshamov ",
    "gilbert bound @xcite , lemma 2.9 , there are @xmath450 binary strings @xmath451 , with @xmath452 , satisfying @xmath453 , @xmath454 .",
    "then @xmath455 , @xmath456 , satisfy ( [ eqfsupp])([eqfzero ] ) and @xmath457 where @xmath458 depends on only @xmath21 and @xmath22 .",
    "this proves the result since with @xmath459 , which could be arbitrarily small , we get @xmath460 where @xmath461 depends on only @xmath22 and @xmath21 .    [ lemsparse ] for every @xmath462 , @xmath234 there exist @xmath463 such that for any @xmath464 and all @xmath465 @xmath466 and , an @xmath90-packing set satisfying the above lower bound may be obtained entirely with @xmath467 functions .",
    "it suffices to prove for @xmath468 since @xmath469 for any set @xmath97 . by lemma",
    "[ lembasic ] there exist @xmath470 such that for any @xmath420 there are functions @xmath471 satisfying ( [ eqfsupp])([eqfsep ] ) with @xmath472 .",
    "therefore , the set @xmath473 is a subset of @xmath474 . by ( [ eqfzero ] ) , for any @xmath475 , @xmath476 for all @xmath477 .",
    "hence , @xmath478 is @xmath90-separated in @xmath479 since @xmath480 by ( [ eqfsep ] ) if @xmath481 and @xmath482 by ( [ eqfsep ] ) and the fact that @xmath423 .",
    "this gives the lower bound on @xmath483 since the cardinality of @xmath484 is @xmath485 .",
    "it is well known that for every @xmath14 , @xmath234 there exist @xmath486 such that for all @xmath487 , @xmath488 @xcite , section  2.6.1 , and @xcite . since a union of sets",
    "is covered by the union of their covers , it follows that @xmath489 for all @xmath490 .",
    "consequently , @xmath491 for all @xmath492 .",
    "this proves the result with @xmath493 and @xmath494 .",
    "[ lemadd ] let @xmath495 be mutually orthogonal subsets of a hilbert space @xmath496 .",
    "then , for any @xmath497 and @xmath498 @xmath499 where @xmath500 .    for every @xmath222 ,",
    "let @xmath501 denote a maximal @xmath502-packing set of @xmath503 with @xmath504 .",
    "take @xmath505 and let @xmath506 be a random element in @xmath507 with the uniform probability distribution .",
    "fix an @xmath508 such that @xmath509 and let @xmath510 , @xmath511 , be iid copies of @xmath512 . if @xmath513 then @xmath507 contains a subset @xmath514 with at least @xmath208 elements such that for any two , @xmath515 .",
    "this would prove the result .",
    "the probability value in ( [ eqplow ] ) is at least @xmath516 , and hence it suffices to show @xmath517 . define @xmath518 , @xmath369 , which are independent binary variables with @xmath519 . by orthogonality of @xmath495 , @xmath520 and",
    "hence it suffices to show @xmath521 by markov s inequality , for any @xmath522 , @xmath523 by the assumption on @xmath524 , @xmath525 for every @xmath526 , and hence , @xmath527 when we set @xmath528 .",
    "consequently , @xmath529 which completes the proof .",
    "[ thmadd ] suppose @xmath530 and set @xmath531 , @xmath222 . under assumption",
    "[ assq ] , for any @xmath497 , @xmath532 with @xmath533 and @xmath534 .",
    "fix @xmath30 mutually exclusive subsets @xmath535 of @xmath536 with @xmath537 , @xmath538 .",
    "let @xmath539 denote the space of norm @xmath540 , @xmath226-smooth regression functions that select @xmath28 predictors from @xmath541 and none from the other subsets , that is , @xmath542 .",
    "these subsets are mutually orthogonal since @xmath543 and @xmath544 , @xmath545 pick disjoint sets of predictors and @xmath546 .",
    "clearly , @xmath547 .",
    "let @xmath548 , @xmath549 , be a packing set of @xmath550 under @xmath551 .",
    "we must have @xmath552 by an application of lemma [ lemadd ] with @xmath553 , coupled with the fact that @xmath539 is isomorphic with @xmath554 .",
    "also , by lemma [ lembasic ] and the packing set construction used in the proof of lemma [ lemadd ] , each @xmath555 can be chosen to belong to @xmath556 .",
    "define @xmath557 as : @xmath558 where @xmath559 .",
    "then each @xmath560 and , @xmath561 , since every @xmath562 involve at most @xmath563 many variables and they are orthogonal across @xmath146 .",
    "this proves the first assertion of the theorem .    in light of the well - known relation @xmath564 between packing and covering entropies of subsets in a metric space , and",
    "the fact that @xmath565 , the second assertion can be established by showing @xmath566 for every @xmath222 , let @xmath567 be a minimal @xmath568-covering set of @xmath569 . for each @xmath146 , replace every element @xmath570 by its centered version @xmath571 .",
    "the new @xmath567 remains a @xmath572-covering set of @xmath573 .",
    "take @xmath574 any @xmath575 equals @xmath576 for some @xmath577 , @xmath222 and @xmath578 .",
    "find @xmath579 such that @xmath580 , @xmath222 and set @xmath581 .",
    "since every @xmath582 , we get @xmath583 whenever @xmath584 , that is , @xmath585 , @xmath586 have no shared selection . by assumption on @xmath587 , for every @xmath146 , there are at most @xmath588 many @xmath589 with shared selection .",
    "therefore , by lemma [ lemoverlap ] , @xmath590 .",
    "consequently , @xmath591 gives a @xmath592-covering of @xmath593 .",
    "this completes the proof since @xmath594 for every @xmath222 .",
    "[ lemoverlap ] suppose @xmath595 are elements of a hilbert space @xmath596 and for any @xmath222 , let @xmath597",
    ". then @xmath598 .",
    "since @xmath599 , we have @xmath600    [ lenormc ] suppose @xmath601 satisfies @xmath602 .",
    "then , for any sequence @xmath603 satisfying @xmath604 and @xmath605 , @xmath606    take @xmath607 and suppose that @xmath608 and @xmath609",
    ". let @xmath610 form an minimal @xmath603-covering of @xmath611 under the sup - norm with @xmath612 .",
    "then there exists some @xmath613 such that @xmath614 . by the assumptions on @xmath11",
    ", we have @xmath615 and @xmath616 , implying @xmath617 . by bernstein s inequality",
    ", we have @xmath618.\\ ] ] since there are at most @xmath619 choices for @xmath620 , we get @xmath621\\leq2\\exp\\biggl[-\\frac { 1}{8}n\\delta_n^2 \\biggr],\\end{aligned}\\ ] ] from which the results follows by the borel  cantelli lemma .",
    "the authors would like to thank the associate editor and two referees for their insightful comments and suggestions ."
  ],
  "abstract_text": [
    "<S> minimax @xmath0 risks for high - dimensional nonparametric regression are derived under two sparsity assumptions : ( 1 ) the true regression surface is a sparse function that depends only on @xmath1 important predictors among a list of @xmath2 predictors , with @xmath3 ; ( 2 ) the true regression surface depends on @xmath4 predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components . for either modeling assumption , a practicable extension of the widely used bayesian gaussian process regression method </S>",
    "<S> is shown to adaptively attain the optimal minimax rate ( up to @xmath5 terms ) asymptotically as both @xmath6 with @xmath7 . </S>"
  ]
}