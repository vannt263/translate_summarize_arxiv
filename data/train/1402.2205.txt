{
  "article_text": [
    "edwin t. jaynes wrote a beautiful article in 1957 @xcite advocating a reinterpretation of statistical mechanics in light of shannon s mathematical theory of communication @xcite . instead of working with ensembles of systems",
    ", jaynes posed the problem in the following terms : suppose we know the expected values of a set of functions @xmath0 of the microscopic state @xmath1 of a system , what is the best estimate for the average value of some other function @xmath2 ?",
    "without access to @xmath1 , which is never available in the lab , the best that can be done is to pick a probability distribution @xmath3 over the states and then calculate the expected value of @xmath4 as @xmath5 $ ] .",
    "but then we encounter the problem of which distribution @xmath6 to choose , because the average values @xmath7 do not provide enough information to determine @xmath6 uniquely .",
    "we need an additional criterion .",
    "therefore , argued jaynes , one should use the distribution that maximises the shannon entropy functional @xmath8 \\equiv -\\sum _ i\\rho(z_i ) \\ln(\\rho(z_i)),\\ ] ] subject to the constraints imposed by the information available .",
    "any other distribution would imply an unjustified bias in the probabilities .",
    "shannon s @xmath9 functional applies only to discrete distributions , but the gibbs - jaynes entropy functional @xmath10 is analogous to @xmath9 for continuous sets of states , as in the case of points in phase space @xcite .",
    "@xmath11 \\equiv -k_b \\int_{\\gamma}\\rho(z)\\ln\\left(\\frac{\\rho(z)}{m(z)}\\right)\\ dz    = -k_b \\mathrm{tr}\\left[\\rho \\ln\\left(\\frac{\\rho}{m}\\right)\\right].\\ ] ] in classical hamiltonian dynamics , the measure @xmath12 turns out to be @xmath13 ( @xmath14 is the number of particles and @xmath15 is planck s constant ) .",
    "@xmath16 stands for the whole phase space and @xmath17 for boltzmann s constant .",
    "jaynes s maximum entropy formalism allows us not only to derive equilibrium statistical mechanics from the point of view of statistical inference , but also to select probability distributions in more general situations , when the expected values of several arbitrary phase functions have been established , even if they are not dynamical invariants . working out such distributions",
    "constitutes the key step for projection operator techniques in the theory of transport processes @xcite .",
    "recent developments in nonequilibrium statistical mechanics @xcite have borrowed another tool from information theory known as the kullback - leibler divergence @xcite , @xmath18 which measures how `` different '' @xmath6 is from @xmath19 . a well - known result in information theory",
    "states that @xmath20 , with equality holding only when @xmath21 almost everywhere .    when an equilibrium ensemble is used as the reference distribution , @xmath22 , there is a simple connection between the gibbs - jaynes entropy ( [ s ] ) and the _ relative entropy _ , defined here as @xmath23",
    "\\equiv -k_b\\ d(\\rho\\|\\rho^{eq}).\\ ] ] the link can easily be established the moment we realise that the equilibrium ensemble is a stationary solution of liouville s equation , @xmath24 and that @xmath25 must therefore be a function of the dynamical invariants only .",
    "let @xmath26 stand for the invariants for the microstate @xmath1 , such as the energy , the linear momentum , and so forth .",
    "we then know that @xmath27 is some function @xmath28 .",
    "the integral of @xmath25 over all the states that satisfy @xmath29 should equal the probability @xmath30 of finding the system in a state compatible with these values of the invariants .",
    "@xmath31 =    \\phi(i)\\mathrm{tr}\\left[\\delta(i - i)\\right ] = p(i),\\ ] ] where @xmath32 is the dirac delta function .",
    "solving for @xmath33 , we end up with @xmath34},\\ ] ] which means that @xmath35 the function @xmath36 in the denominator may be thought of as the `` number of microstates '' that satisfy @xmath29 , @xmath37 when the expression for the equilibrium distribution ( [ rho_eq ] ) is substituted into the definition of the relative entropy ( [ def - relative_entropy ] ) , we find @xmath38 & = &   -k_b \\int_{\\gamma } \\rho(z ) \\ln\\left(\\frac{h^{3n}\\rho(z ) }                                            { \\frac{p(i(z ) ) }                                                        { \\omega(i(z))}}\\right)\\ dz \\\\ & = &    -k_b \\int_{\\gamma } \\rho(z ) \\ln\\left(h^{3n}\\rho(z)\\right)\\ dz    + k_b \\int_{\\gamma } \\rho(z ) \\ln\\left(\\frac{p(i(z))}{\\omega(i(z))}\\right)\\ dz \\nonumber \\\\     & = &    s[\\rho ] + k_b \\int \\mathrm{tr}\\left[\\rho \\delta(i - i)\\right ]                       \\ln\\left(\\frac{p(i)}{\\omega(i)}\\right)\\ di , \\nonumber\\end{aligned}\\ ] ] ( the latter integral is extended over all the values of @xmath39 ) . hence maximising",
    "the relative entropy functional @xmath40 is equivalent to maximising the gibbs - jaynes functional @xmath10 _ as long as this last integral is constant for the distributions allowed by the constraints_. a reasonable way to meet this condition requires @xmath41 = p(i).\\ ] ] in other words , the agreement between both maximisation strategies is based on the assumption that the unknown distribution generates the same probability distribution over the dynamical invariants as the equilibrium ensemble .",
    "if equation ( [ probability_assumption ] ) is conceded then , given the probability distribution at equilibrium , we can follow two different paths to calculate the least biased distribution compatible with our information about the system .",
    "an anonymous reviewer pointed out that condition ( [ probability_assumption ] ) could be relaxed when @xmath25 designates the canonical ensemble ( [ canonical_ensemble ] ) , for then we can simply assume that the expected energy calculated with @xmath6 leads to the same result as when it is calculated with @xmath25 and then prove that the final integral in equation ( [ delta_s_theorem ] ) becomes independent of @xmath6 .",
    "@xmath42\\ ] ] hence , in the canonical case condition ( [ probability_assumption ] ) may be relaxed to @xmath43 = \\mathrm{tr}[\\rho^{eq } i]\\ ] ] similarly , for a microcanonical @xmath25 , it is enough to require that @xmath6 vanishes whenever @xmath25 does .",
    "let us illustrate the two alternative maximisation routes by working out the classic example of the probability distribution for a system in contact with a heat bath at temperature @xmath44 .",
    "we shall start with the gibbs - jaynes functional and introduce two habitual assumptions .",
    "first , we will imagine that the system and the reservoir have been isolated from the rest of the universe , and second , we will disregard the interaction energy between them , which we assume is very small compared with their internal energies , so that the hamiltonian , which remains constant , may be expressed as a sum of two terms , @xmath45 the former corresponding to our system , and the latter to the reservoir .",
    "a typical setup might also include the measured values of several macroscopic variables , such as concentrations or hydrodynamic velocity fields @xcite .",
    "we denote these values @xmath46 and equate them to the averages for the corresponding functions @xmath7 , of the microstate @xmath47 @xmath48,\\ ] ] where @xmath6 is the unknown distribution . given the one - to - one correspondence between the equilibrium temperature @xmath44 and the total internal energy",
    ", we include an additional constraint for the expected value of @xmath49 , @xmath50,\\ ] ] although we do not yet know the actual number represented by @xmath51 . finally",
    ", we must ensure that @xmath6 is properly normalised , @xmath52 = 1.\\ ] ]      we now wish to find the distribution that maximises the entropy functional subject to all the constraints ( [ lambda_constraints])-([normalisation ] ) . following the standard method of lagrange multipliers ,",
    "we add constraint terms to the gibbs - jaynes functional ( [ s ] ) to obtain @xmath53 & = &      -k_b\\mathrm{tr}\\left[\\rho\\ \\ln\\left(h^{3n}\\rho\\right)\\right ]      -k_b\\sum_{i=1}^k\\lambda_i\\left(\\mathrm{tr}\\left[\\rho f_i\\right]-f_i\\right ) \\\\ &   &      -k_b\\beta\\left(\\mathrm{tr}\\left[\\rho h\\right ] - e\\right )      -k_b\\mu\\left(\\mathrm{tr}\\left[\\rho\\right ] - 1 \\right ) .",
    "\\nonumber\\end{aligned}\\ ] ] the @xmath54 , @xmath55 and @xmath56 are all lagrange multipliers",
    ". the functional derivative of @xmath57 with respect to @xmath6 should vanish for the least biased distribution , frequently called the _ relevant _ distribution , @xmath58 , @xmath59 and this equation leads us to @xmath60 substitution of @xmath58 into the constraints should allow us , in principle , to calculate the lagrange multipliers .",
    "equation ( [ normalisation ] ) , for example , determines the value of @xmath56 .",
    "when combined with ( [ rel_rho ] ) , we find that @xmath61 }              = \\frac{1}{z},\\ ] ] so our probability distribution becomes @xmath62 equation ( [ hamiltonian ] ) implies that the partition function @xmath63 factors into the product of an integral over @xmath47 and another over @xmath64 .",
    "we will refer to these factors as @xmath65 and @xmath66 .",
    "@xmath67 where @xmath68 and @xmath69 are the number of particles in the system and reservoir , respectively .",
    "we define the entropy @xmath70 as the maximum value of the gibbs - jaynes entropy functional . inserting",
    "the expression for @xmath58 into ( [ s ] ) reveals the following link between the partition function and the entropy : @xmath71          =    k_b\\ln(z ) + k_b\\sum_{i=1}^k \\lambda_i f_i                        + k_b\\beta e.\\ ] ] factoring @xmath63 according to ( [ z1z2 ] ) , the entropy separates neatly into two terms , @xmath72 where @xmath73 and @xmath74 stand for the expected values of @xmath75 and @xmath76 . on the right of equation ( [ s_is_extensive ] )",
    "we find the entropies of the system and reservoir considered separately .",
    "in other words , suppose we had isolated the system from the heat bath and had then calculated the entropies for both independently by maximising the gibbs - jaynes functional , with the same constraints on the average values of @xmath0 and the normalisation of @xmath6 , but changing the constraints on the expected energies to @xmath77 for the system and @xmath78 for the bath , where @xmath73 and @xmath74 are the same values as in equation ( [ s_is_extensive ] ) .",
    "then the expressions for the entropies , @xmath79 for the system and @xmath80 for the reservoir , would read @xmath81 we will show below that the sum @xmath82 equals @xmath10 in ( [ s_is_extensive ] ) because @xmath83 the set of equations ( [ s_is_extensive ] ) , ( [ ss_and_sr ] ) and ( [ thermal_equilibrium ] ) illustrates the well - known fact that entropy is an extensive quantity if the interaction between subsystems is small enough to be disregarded .",
    "the equality of the three lagrange multipliers in ( [ thermal_equilibrium ] ) can be verified by comparing the average values calculated using ( [ total_rho_bar ] ) to the averages for the system and reservoir considered independently , noting that @xmath84    the lagrange multiplier @xmath85 is related to the temperature of the heat bath according to @xmath86 therefore , equation ( [ thermal_equilibrium ] ) shows that the temperatures of the system and the bath must equal the same value @xmath44 for the relevant distribution , and @xmath87 equation ( [ total_rho_bar ] ) then becomes @xmath88 to calculate the probability distribution for @xmath47 , we can now integrate over the degrees of freedom of the reservoir , that is , @xmath89 the answer to our problem , known as the generalised canonical probability distribution @xcite , is the least biased probability distribution for a system at constant temperature @xmath44 that also satisfies the constraints ( [ lambda_constraints ] ) . carrying out the integral ( [ integral_over_z_r ] ) , we obtain @xmath90},\\ ] ] where the trace should now be interpreted as an integration over @xmath47 .",
    "now consider the derivation of the generalised canonical distribution ( [ gcd ] ) from the relative entropy ( [ def - relative_entropy ] ) . in that case , we do not need to pay attention to the reservoir , so we maximise the functional @xmath91 , which includes the relative entropy and constraints ( [ lambda_constraints ] ) and ( [ normalisation ] ) . @xmath92",
    "-k_b\\sum_{i=1}^k\\lambda_i\\left(\\mathrm{tr}\\left[\\rho f_i\\right]-f_i\\right )      -k_b\\mu\\left(\\mathrm{tr}\\left[\\rho\\right ] - 1 \\right).\\ ] ] as before , we calculate the functional derivative of @xmath91 with respect to @xmath6 and find that @xmath93 the equilibrium distribution @xmath25 for a system at constant temperature is the well - known canonical ensemble , @xmath94}.\\ ] ] by ensuring that @xmath58 is normalised , we determine @xmath56 @xmath95 }                      { \\mathrm{tr}\\left[e^{-\\sum_{i=1}^k \\lambda_i f_i(z_s )                                           -\\frac{1}{k_bt}h_s(z_s)}\\right]}.\\ ] ] and substituting ( [ canonical_ensemble ] ) and ( [ normalisation_factor ] ) into ( [ rel_rho_2 ] ) , we recover the generalised canonical probability distribution ( [ gcd ] ) .    jaynes s maximum entropy formalism guided us to the desired solution , but the path we had to follow was not as direct as the relative entropy route . furthermore ,",
    "in the former derivation , we found ourselves describing the effect of the reservoir in terms of the total energy @xmath96 . but",
    "knowledge of the energy in the reservoir , @xmath74 , clearly has no bearing on our problem because heat baths are characterised by their temperature , not their internal energy , and it is a good thing that @xmath74 eventually drops out of the equations .",
    "therefore , if we already know the equilibrium ensemble , perhaps it is easier to derive the relevant distribution from the relative entropy functional .",
    "nevertheless , the functional form of equation ( [ gcd ] ) could have been inferred from the gibbs - jaynes entropy ( [ s ] ) by a simpler procedure that does not contemplate the reservoir .",
    "the idea is to use the constraints for the average values @xmath97 ( [ lambda_constraints ] ) , normalisation ( [ normalisation ] ) and an extra constraint for the expected value of the unknown energy @xmath73 .",
    "the maximum entropy formalism then leads to an expression analogous to ( [ gcd ] ) , but with an unknown coefficient before @xmath98 .",
    "all the extra work with the reservoir in the subsection on the gibbs - jaynes derivation was carried out to establish that the temperature in equation ( [ gcd ] ) was equal to the temperature of the reservoir @xmath44 ( note that we have not assumed thermal equilibrium between the reservoir and system of interest ) . when the same result was derived from the relative entropy functional ( [ def - relative_entropy ] ) we did not have to do any of this extra work because the relevant information was already captured in the equilibrium distribution .",
    "the preceding discussion might give the impression that the relevant distribution may always be expressed as @xmath99 where @xmath100 stands for the appropriate normalisation factor .",
    "but this rule may lead to incorrect conclusions if applied carelessly . to see why ,",
    "let us examine a slightly more general problem .",
    "whenever we are dealing with macroscopic systems in experiments , the exact number of atoms or molecules remains unknown .",
    "let @xmath101 represent the coordinates and momenta of a system of @xmath14 particles .",
    "the probability distributions and functions will now depend on the dimensionality of phase space , so we will write them with a subindex @xmath14 to emphasise this dependence .",
    "the gibbs - jaynes entropy functional ( [ s ] ) can be generalised to @xmath102 and the constraints on the average values @xmath103 now read @xmath104 where @xmath105 represents the phase space for @xmath14 particles . similarly ,",
    "the expression for the relative entropy turns into @xmath106 the obvious generalisation of ( [ relevant_eq ] ) must be @xmath107 note that @xmath108 and @xmath109 represent joint probability densities for @xmath14 and @xmath101 , and are therefore not normalised to one , but rather @xmath110 where p(n ) represents the probability of @xmath14 particles in the system .",
    "imagine an isolated system for which we know the probability distribution @xmath111 for the total energy @xmath51 and number of particles @xmath14 .",
    "both @xmath51 and @xmath14 are dynamical invariants , and so is the probability @xmath111 .",
    "hence we should find the same probability for @xmath51 and @xmath14 at equilibrium .",
    "@xmath112 if we use lagrange s method to maximise ( [ jaynes_entropy ] ) subject to ( [ macro_f_constraints ] ) and ( [ probability_constraint ] ) , we derive the set of relevant distributions @xmath113 but in general these functions are formally different from our previous expression for @xmath114 ( [ macrocanonical_rho_eq ] ) .",
    "the disagreement between the two methods dissolves when we carry out the operations carefully .",
    "it might seem at first that there is no need to include the constraint ( [ probability_constraint ] ) when we maximise the functional for the relative entropy , because all the relevant information about @xmath111 should already be included in the equilibrium distribution .",
    "however , we do in fact have to specify that the distribution we are looking for must lead to the same value as the equilibrium distribution when both are integrated over a given constant energy manifold .",
    "when using the relative entropy ( [ relative_entropy ] ) , the correct functional to maximise is thus @xmath115 the last term above includes a lagrange multiplier @xmath56 for each pair of @xmath51 and @xmath14 , as required by equation ( [ probability_constraint ] ) .",
    "once again , we equate the derivatives of @xmath91 to zero and solve for the relevant distribution to find @xmath116 this distribution can be identified with ( [ macrocanonical_rho_eq ] ) as long as @xmath100 is interpreted as a function of @xmath51 and @xmath14 .",
    "in other words , if we define @xmath117 then we can simply insert ( [ relative_solution ] ) into ( [ probability_constraint ] ) and solve for @xmath100 , to determine @xmath118 recalling the expression for the equilibrium distribution ( [ rho_eq ] ) , @xmath119 equations ( [ def - mathcal_z])-([rho_eq_n ] ) can be used to convert ( [ relative_solution ] ) into ( [ jaynes_solution ] ) , so the two methods once again lead to the same result , as they should .",
    "in the literature , relative entropy has been applied mainly to the calculation of nonequilibrium free energy differences @xcite and dissipated work @xcite . in that context , @xmath6 and @xmath25 are both distributions that can be realised physically , such as the equilibrium ensembles of a given hamiltonian . by contrast , in the present paper we have used the relative entropy functional to determine _ relevant distributions _ , which need not be realised physically , because they represent the least - biased distribution that is consistent with the information available .",
    "double - well potential confining @xmath120 lennard - jones particles .",
    "the dashed line marks the average energy per particle @xmath121.,scaledwidth=40.0% ]    within the theory of mori - zwanzig transport processes , relevant distributions have become a crucial tool to derive generalised transport equations @xcite .",
    "consider the one - dimensional isolated double - well potential drawn in figure [ doublewell ] , which confines one hundred particles that interact with each other through the lennard - jones potential .",
    "let the relevant variable @xmath122 represent the number of particles on the right , @xmath123 where @xmath124 is the heaviside step function and @xmath125 the position of particle @xmath39 .",
    "the mori - zwanzig theory of nonequilibrium transport allows us to write exact transport equations for the average value of any phase function @xcite .",
    "in particular , for the relevant variable in ( [ relevant_variable ] ) , the theory produces the following transport equation for the average value @xmath126 , @xmath127 the first term on the right is known as the organized drift , @xmath128 , while @xmath129 is called the after - effect function . here",
    "we will concentrate only on the organized drift , defined as @xmath130 =    \\mathrm{tr}\\left[\\bar{\\rho}\\sum_{i=1}^n\\delta(q_i)\\frac{p_i }                                                           { m_i}\\right],\\ ] ] as a first crude approximation to the time rate of change for @xmath131 .",
    "that is to say , we are assuming that @xmath132 . in ( [ organised_drift ] )",
    "we have applied the liouville operator @xmath133 to @xmath134 to calculate how @xmath128 is related to the momenta @xmath135 and masses @xmath136 of the particles .",
    "@xmath137 if we follow jaynes s maximum entropy route to determine the relevant distribution @xmath58 for @xmath131 equal to some value @xmath69 , then we get @xmath138},\\ ] ] with the lagrange multiplier @xmath139 chosen to satisfy the constraint on the average value of @xmath134 .",
    "when we insert ( [ doublewell_rhobar ] ) in ( [ organised_drift ] ) and integrate , the organised drift vanishes because it is the integral of an odd function , due to the presence of the momenta @xmath135 .",
    "@xmath140    surprisingly , in many cases this conclusion ( [ no_drift ] ) is incorrect .",
    "suppose we choose an initial state with all the particles on the left .",
    "if the average energy per particle @xmath121 lies below the height of the potential barrier , then the system can never reach a state with all the particles on the right , simply because there is not enough energy to get them all over the barrier .",
    "note , though , that when we switch the signs of all the coordinates in our initial state we create a new inaccessible state with the same total energy as before .",
    "in other words , the system is not ergodic for some values of the energy @xmath51 , and so it does not explore the complete @xmath141 surface in phase space .",
    "let @xmath142 designate the final stationary distribution reached by the system , to distinguish it from the microcanonical @xmath25 implied by the maximum entropy approach . calculating the relevant distribution from the relative entropy functional ( [ def - relative_entropy ] ) , the expression for the organised drift becomes @xmath143 . }    { \\mathrm{tr}\\left[\\rho^{ref}e^{-\\lambda_{n_r}f}\\right]}\\ ] ] even though @xmath142 is unknown , we can sample it by means of a molecular dynamics simulation . after the simulation run",
    ", the system has traversed a set of points @xmath144 , so we estimate @xmath145 with @xmath146     number of particles with a positive coordinate value versus time for a system of @xmath147 lennard - jones particles initially to the left of the potential barrier in figure [ doublewell ] .",
    "a classic runge - kutta fourth order algorithm with time step equal to @xmath148 was used to integrate the equations of motion .",
    "the total energy remained constant to four significant figures .",
    "numerical integration with the organised drift from figure [ simulated_drift ] allowed us to estimate the average value of @xmath134 as a function of time ( dashed line ) . ]",
    "figure [ simulation_run ] represents the number of particles on the right of the double - well potential in figure [ doublewell ] as a function of time .",
    "we started the simulation with all the particles on the left and an average energy per particle @xmath121 below the height of the potential barrier .",
    "the average kinetic temperature calculated over the whole run ( @xmath149 time steps ) equals @xmath150 ( mean @xmath151 standard deviation ) .",
    "when the particles on either side of the barrier were considered separately , the average kinetic temperature remained the same , but the standard deviation doubled on the right of the well @xmath152 .",
    "organised drift versus average number of particles to the right of the potential barrier , @xmath69 , calculated with equation ( [ average_drift]).,scaledwidth=50.0% ]    figure [ simulated_drift ] shows the organised drift calculated with ( [ average_drift ] ) as a function of the average number of particles @xmath69 to the right of the potential barrier .",
    "numerical integration of equation ( [ dfdt ] ) by setting @xmath153 generated the dashed line shown in figure [ simulation_run ] , which agrees qualitatively with the general trend of the simulation .",
    "the deviations observed are not very surprising , considering that we have completely neglected the after - effect function .    in summary",
    ", even though we have neglected the memory effects in the equation for the organised drift ( [ organised_drift ] ) calculated with the relative entropy ( as opposed to the gibbs - jaynes entropy method ) , we have achieved a very good description of the transport over the energy barrier .",
    "the results are especially interesting because the dynamical evolution took place under non - ergodic conditions .",
    "the method presented here could also be applied in principle to the numerical calculation of the integral of @xmath154 in ( [ dfdt ] ) , but this would involve a much greater computational cost , so we have deferred these calcuations to future research .",
    "in the context of the maximum entropy formalism , relative entropy ( [ def - relative_entropy ] ) has pleasant features .",
    "its maximum value , @xmath155 , obviously corresponds to equilibrium , and it enables us to calculate the relevant distribution with less effort .",
    "furthermore , the relevant distribution turns into the equilibrium ensemble when the lagrange multipliers @xmath54 vanish . with expressions like ( [ rel_rho_2 ] ) or ( [ relative_solution ] )",
    "this fact lies in plain sight .",
    "given the equilibrium ensemble @xmath25 and a set of constraints on average values , @xmath156 , the relevant distribution can be calculated immediately by following these rules : first , write @xmath157 then ensure that @xmath58 is normalised by writing @xmath158 = 1\\ ] ] and solving for the partition function @xmath63 .",
    "finally , the @xmath54 lagrange multipliers can be calculated , at least in principle , by inserting @xmath58 into the constraints on the average values , @xmath156 , and solving for the @xmath54 .",
    "the constraints for isolated systems must be considered carefully , because knowledge of the equilibrium ensemble reveals the probability distribution @xmath30 over the whole set of values of the dynamical invariants through @xmath159 = p(i).\\ ] ] this information must be taken into account , so in this case we write @xmath160 then we ensure that @xmath58 is normalised by writing",
    "@xmath161 = p(i),\\ ] ] and solving for @xmath162 .",
    "the relevant ensemble can then be used in conjunction with the other constraints to find the value of the unknown lagrange multipliers .",
    "the relative entropy method relies on our knowledge of the equilibrium ensemble , and it provides no clues regarding how to calculate this probability distribution , unlike jaynes s method .",
    "however , this might also be interpreted as a virtue .",
    "when the system has concealed dynamical invariants which have not been taken into account , maximising the gibbs - jaynes entropy will not generally reproduce the measured average values faithfully .",
    "this would signal the existence of missing information .",
    "by contrast , if the equilibrium ensemble has been determined by other means or if we are able to sample it effectively ( with molecular dynamics , for example ) , then we have simultaneously determined the probability distribution for all the dynamical invariants .",
    "we may then simply write the relevant distribution in terms of the equilibrium ensemble and , in principle , use it to calculate nonequilibrium quantities like the coarse - grained free energy or green - kubo transport coefficients @xcite .",
    "we would like to express our gratitude to the anonymous reviewers of this article for their insightful comments .",
    "jaynes , e. t. : information theory and statistical mechanics . the physical review , * 106 * , 620 - 630 ( 1957 ) shannon , c. e. : a mathematical theory of communication",
    ". bell system technical journal , * 27 * , 379 - 423 and 623 - 656 ( 1948 ) jaynes , e. t. : information theory and statistical mechanics , in statistical physics , 181 - 218 .",
    "w. a. benjamin , inc .",
    ", new york ( 1963 ) kawasaki , k. , and gunton , j. d. : theory of nonlinear transport processes : nonlinear shear viscosity and normal stress effects , physical review a * 8 * , 20482064 ( 1973 ) grabert , h. : projection operator techniques in nonequilibrium statistical mechanics , 29 - 32 .",
    "springer - verlag , berlin - heidelberg - new york ( 1982 ) zubarev , d. : statistical mechanics of nonequilibrium processes , 89 - 98 .",
    "wiley , berlin ( 1996 ) gaveau , b. , schulman , l. s. : a general framework for non - equilibrium phenomena : the master equation and its formal consequences .",
    "physics letters a * 229 * 347 - 353 ( 1997 ) qian , h. : relative entropy : free energy associated with equilibrium fluctuations and nonequilibrium deviations .",
    "physical review e * 63 * , 042103 ( 2001 ) kawai , r. , parrondo , j. m. r. , and c. van der broeck : dissipation : the phase - space perspective , physical review letters * 98 * , 080602 ( 2007 ) shell , m. s. : the relative entropy is fundamental to multiscale and inverse thermodynamic problems , the journal of chemical physics * 129 * , 144108 ( 2008 ) vaikuntanathan , s. , and jarzynski , c. : dissipation and lag in irreversible processes .",
    "europhysics letters , * 87 * , 60005 ( 2009 ) horowitz , j. , and jarzynski , c. : illustrative example of the relationship between dissipation and relative entropy .",
    "physical review e * 79 * , 021106 ( 2009 ) roldn , e. , and parrondo , j. m. r. : entropy production and kullback - leibler divergence between stationary trajectories of discrete systems .",
    "physical review e , * 85 * , 031129 ( 2012 ) crooks , g. e. , and sivak , d. a. : measures of trajectory ensemble disparity in nonequilibrium statistical dynamics . journal of statistical mechanics : theory and experiment , p06003 ( 2012 ) crooks , g. e. : on thermodynamic and microscopic reversibility .",
    "journal of statistical mechanics : theory and experiment p07008 ( 2012 ) sivak , d. a. , and crooks , g. e. : near equilibrium measurements of nonequilibrium free energy .",
    "physical review letters , * 108 * 150601 ( 2012 ) kullback , s. , and leibler , r. a. : on information and sufficiency .",
    "annals of mathematical statistics * 22 * , 79 - 86 ( 1951 )"
  ],
  "abstract_text": [
    "<S> the maximum entropy formalism developed by jaynes determines the relevant ensemble in nonequilibrium statistical mechanics by maximising the entropy functional subject to the constraints imposed by the available information . </S>",
    "<S> we present an alternative derivation of the relevant ensemble based on the kullback - leibler divergence from equilibrium . </S>",
    "<S> if the equilibrium ensemble is already known , then calculation of the relevant ensemble is considerably simplified . </S>",
    "<S> the constraints must be chosen with care in order to avoid contradictions between the two alternative derivations . </S>",
    "<S> the relative entropy functional measures how much a distribution departs from equilibrium . </S>",
    "<S> therefore , it provides a distinct approach to the calculation of statistical ensembles that might be applicable to situations in which the formalism presented by jaynes performs poorly ( such as non - ergodic dynamical systems ) .    </S>",
    "<S> [ the final publication is available at springer via http://dx.doi.org/10.1007/s10955-014-0954-6 ] </S>"
  ]
}