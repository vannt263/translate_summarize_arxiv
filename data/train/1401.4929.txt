{
  "article_text": [
    "knowledge of steady - state quantities related to an ergodic stochastic chemical system can provide useful insights into its properties .",
    "moreover , steady - state values of cost functions are often easier to estimate with good accuracy compared to stationary distributions .",
    "when the system propensities are affine in the state , mean values of polynomial functions of the system state can be computed analytically , as the system of moments is closed . however ,",
    "when non - polynomial functions are considered , or the system propensities are not affine , analytic calculations are no longer possible and the only solution left is simulation .    while moment closure methods @xcite can be used to provide good approximations to system moments over a finite time interval , they commonly tend to diverge from the true solution over time , thus resulting in biased steady - state values .",
    "the solution presented in ref .",
    "@xcite works only when polynomial functions of the state are considered , and generalization to arbitrary functions is still very difficult .",
    "the finite state projection algorithm @xcite can be alternatively employed to provide moment estimates with guaranteed accuracy bounds , however the number of states required to attain a certain accuracy makes the method applicable to small problems .    on the other hand , stochastic simulation @xcite can always provide estimates for the stationary mean of any function of the state , however these estimates are inevitably noisy .",
    "brute - force noise reduction can only be achieved at an increased computational cost , either by simulating longer trajectories or by running many trajectories in parallel .",
    "another possibility for reducing the noise in the estimated quantities is the application of a variance reduction technique @xcite , provided the added computational cost of the reduced variance estimator is significantly smaller than the gain in computer time . in this work we present the application of such a variance reduction technique @xcite to systems of stochastic chemical kinetics",
    "the idea is based on so - called _ shadow functions _ and originated in the queueing systems simulation literature , a field where the range of analytically tractable systems in that field is much larger .",
    "we demonstrate how the same idea can be applied to steady - state simulation of stochastic chemical systems .",
    "we further test the capabilities of the reduced - variance estimators by performing parametric sensitivity calculations for two systems governed by nonlinear propensity functions .",
    "the paper is organized as follows : in sections ii and iii we define the steady - state estimation problem and define the nave and shadow function estimators . in sections",
    "iv and v we present one possible implementation of the variance reduction technique to stochastic chemical kinetics and its applicability to steady - state sensitivity analysis . the numerical examples in section",
    "vi serve to demonstrate the effectiveness of the shadow function method in practice and assess its computational cost in comparison to nave estimation .",
    "the conclusions of our study and some future research directions are finally summarized in section vii .",
    "assume an irreducible positive recurrent markov chain @xmath0 on a countable space @xmath1 .",
    "in the case of stochastic chemical kinetics , @xmath2 , where @xmath3 is the number of chemical species in the system . the chain moves according to a finite set of available transitions @xmath4 , with a corresponding set of propensity functions @xmath5 .",
    "the infinitesimal generator of @xmath6 is the operator @xmath7 satisfying @xmath8 for all @xmath9 such that @xmath10 .",
    "the discreteness of @xmath1 allows us to enumerate its elements and think of @xmath7 as an infinite matrix @xmath11 .",
    "similarly , any function @xmath12 on @xmath1 can be thought of as an infinite column vector , and distributions on @xmath1 can be defined as infinite row vectors .",
    "let @xmath9 be a @xmath13-integrable cost function associated with @xmath6 .",
    "the ergodic theorem for markov chains @xcite ascertains that for any initial condition @xmath14 where @xmath13 is the unique invariant distribution of the system and @xmath15 the steady - state mean value of @xmath12 .",
    "since the analytic calculation of @xmath15 is possible only in very special cases , its estimation from simulation is usually the only possibility .",
    "the most straightforward estimator of @xmath15 is @xmath16 which is also strongly consistent @xcite .    under some further general conditions on @xmath6 , and @xmath12",
    ", we also know that @xmath17 as @xmath18 , where @xmath19 denotes weak convergence , @xmath20 is the standard normal distribution and @xmath21 is called the _ time average variance constant ( tavc ) for @xmath22 _ @xcite .    the tavc can be expressed in terms of the integrated autocovariance function of the process @xmath23 , where @xmath24 , according to the formula @xcite : @xmath25\\,ds.\\ ] ] an alternative expression for @xmath26 can be derived from the functional central limit theorem for continuous - time markov chains @xcite : @xmath27 where @xmath28 is a solution to the so - called _ _ poisson s equation__@xcite ( note that solutions to the poisson equation are unique up to an additive constant , i.e. if @xmath28 is a solution , then @xmath29 is also a solution @xcite ) : @xmath30    a more general class of estimators for @xmath15 has the form @xmath31 where @xmath32 is chosen such that @xmath33 almost surely for all @xmath34 @xcite .",
    "the function @xmath35 offers an extra degree of freedom in the design of the estimator , which can be exploited to achieve variance reduction .",
    "in other words , @xmath35 can be chosen such that the tavc of @xmath36 , denoted by @xmath37 , is smaller than @xmath38 .",
    "the obvious choice @xmath39 is of course intractable , however it suggests that a function @xmath35 with a zero steady - state mean that is approximately equal to @xmath40 could also achieve variance reduction .",
    "such functions would result in a process @xmath41 that behaves almost antithetically from @xmath42 , thus making the variance of @xmath43 smaller than that of @xmath42 alone . in the steady - state simulation literature , a function @xmath32 that satisfies @xmath44",
    "is called a _ shadow function _ @xcite .",
    "the problem then becomes the selection of an appropriate shadow function @xmath35 , so that @xmath45 , with @xmath46 . from",
    "we see that a reduction of variance by a factor @xmath47 implies that the variance of @xmath48 is equal to the variance of @xmath49 . assuming that the computational cost of both estimators is dominated by the cost of simulating the process @xmath6 , @xmath50 can be used as an indicator of the efficiency of @xmath36 relative to @xmath51 .    the basic idea of the shadow function method of ref .",
    "@xcite , outlined in the next section , is to obtain such an @xmath35 by using analytical information from a second markov chain that approximates the original one and is mathematically tractable .",
    "a second alternative solution of more general applicability will be described after presenting the method in more detail .",
    "the basic idea to the shadow function method is to consider candidate functions of the form @xmath52 where @xmath7 is the generator matrix of the markov chain and @xmath28 is any @xmath13-integrable function ( so that the ergodic theorem holds for it as well ) . in this case , and under the assumption that @xmath53 ( that holds under some not - too - stringent conditions on @xmath28 @xcite ) , @xmath54 becomes a shadow function . we are then naturally led to consider the solution of the poisson equation , which could provide us with the appropriate function @xmath28 .",
    "solving is of course not possible , since the state space is countable and @xmath15 is unknown .",
    "however , we can look for so - called _ surrogate functions _ that approximate this solution to build a better estimator .",
    "following the analysis from @xcite , we consider another markov chain @xmath55 evolving on a countable space @xmath56 , with stationary distribution @xmath57 and generator @xmath58 .",
    "we also assume a map @xmath59 ( not necessarily one - to - one ) and a function @xmath60 that is somehow closely related to the original cost function @xmath12 . if @xmath60 is @xmath57-integrable , we further assume that we can compute the solution to the poisson equation @xmath61 through which we arrive at a surrogate function @xmath62    summing up , the approach outlined above is based on the fact that if @xmath55 is : 1 ) a relatively good approximation of @xmath6 and 2 ) tractable analytically , then we can derive a surrogate function @xmath28 and an estimator @xmath63 which is better than the original estimator in terms of tavc ( assuming that the extra calculation time needed for @xmath36 is not significant ) .",
    "the shadow function method was originally developed for steady - state simulation of queueing systems , for which a wide range of known and tractable approximations exists .",
    "the solution of the approximating poisson equation can thus be calculated explicitly in many cases , and the application of the method is straightforward .",
    "this is not the case for stochastic chemical kinetic systems , where explicit solutions are very hard or impossible to calculate .",
    "one thus has to resort to different types of approximation schemes , outlined below .",
    "the markov chains we are interested in satisfy the following properties :    1 .",
    "they have a finite number of bounded increments over each finite time interval 2 .",
    "each state leads to a finite number of states ( i.e. for every @xmath64 , @xmath65 for finitely many @xmath66 s )    for such chains , an obvious idea for obtaining an approximating process is to consider a chain evolving on a finite truncation of @xmath1 ( i.e. consider @xmath56 to be a finite subset of @xmath1 ) .",
    "actually , under quite weak assumptions and careful definition of @xmath58 , one can show that the invariant distribution of @xmath55 on @xmath56 approaches that of @xmath6 as the truncation size grows @xcite .",
    "this of course implies that @xmath67 also approaches @xmath68 . in this case",
    ", the function @xmath69 between the two state spaces can be intuitively defined to map every @xmath70 to itself , and every @xmath71 to some @xmath72 ( which may vary with @xmath73 ) . in this way , @xmath74 .    in order to arrive at a good approximation with this approach ,",
    "one first has to study a few simulations of @xmath6 , to determine a finite set that contains a good amount of its invariant mass and then perform the necessary calculation of the solution to the poisson equation on @xmath56 .",
    "the size of this set is determined in practice as a trade - off between tractability and approximation accuracy . however , the applicability of this approach is in general very limited due to the fact that the required truncations grow exponentially with the system dimension .",
    "another problem is that the approximation @xmath75 of @xmath28 ( the solution to the original intractable poisson equation ) will be very poor for states @xmath71 , because of the form of @xmath69 , which projects are states outside @xmath56 back into the set .",
    "this implies that significant variance reduction will be hard to achieve ( and in some cases variance may even increase ) , if the chain sample paths exit @xmath56 too frequently during simulation .      instead of searching for an approximating markov process",
    ", one may try to approximate the solution of directly , to arrive at a suitable shadow function @xmath35 .",
    "this approach is also followed in ref .",
    "@xcite , where the discrete - time steady - state simulation problem is considered . given a set of functions @xmath76 ,",
    "must satisfy a boundedness condition derived from a foster - lyapunov inequality . for more details , see ref .",
    "@xcite or ch.8 of ref .",
    "@xcite . in the sequel",
    "we will assume that all the functions considered satisfy this property .",
    "] one can define @xmath77 where @xmath78 and @xmath79 is a vector of weights .    in principle",
    "one could then try to calculate the value of @xmath80 that minimizes the tavc of @xmath36 .",
    "using and , this variance constant turns out to be ( see appendix [ app_a ] ) @xmath81,\\ ] ] where @xmath28 solves .",
    "thus , minimizing the tavc of @xmath82 requires knowledge of @xmath28 , which is unavailable .",
    "we thus have to resort to heuristic methods for obtaining a suboptimal estimate of @xmath80 , for example by determining the value of @xmath80 that minimizes @xmath83 for some suitable measure @xmath84 .",
    "this is a linear least squares regression problem , which can be solved approximately by generating a set of training data @xmath85 , @xmath86 , with weights @xmath87 .",
    "if we define the finite - sample version of @xmath88 by @xmath89 and similarly set @xmath90 we can then calculate the matrix @xmath91 corresponding to @xmath92 by using the explicitly known form of the markov chain generator and finally obtain @xmath93 where @xmath94 , as the ( weighted ) least squares minimizer of @xmath95 .",
    "putting together all the elements presented above , we summarize below the basic steps of the variance reduction algorithm implemented in this work :    * simulate a long path of the process @xmath6 using any preferred version of the stochastic simulation algorithm @xcite . *",
    "obtain a rough estimate of @xmath15 from the simulated trajectory using @xmath51 .",
    "* pick a set of functions @xmath96 and approximate the solution @xmath28 to the poisson equation by @xmath97 using the approach outlined above . *",
    "evaluate @xmath98 along the simulated sample path .",
    "* refine the estimate of @xmath15 using @xmath36 . *",
    "verify that variance reduction has been achieved .",
    "the last step is necessary to ensure that the variance has not actually increased due to the use of a suboptimal weight vector @xmath80 , and it can be carried out quite straightforwardly using the method of batch means @xcite and the simulated trajectory from step 1 . in all cases",
    "we have tested , steps 2 - 6 do not contribute more than a few seconds to the computational cost of this algorithm , which implies that the main computational bottleneck still lies at step 1 .      the estimate of @xmath99 obtained by weighted least squares is clearly suboptimal , however it may still yield a reduced - variance estimator .",
    "the choice of the weighting measure @xmath84 in the optimization problem above is completely free , and one could in principle try to optimize over both @xmath84 and @xmath80 for a given problem . in practice",
    "however , such an approach would increase computational cost of the reduced - variance estimator and possibly eliminate the benefit of variance reduction . to maintain estimator efficiency",
    ", one should thus consider a single ( or a few ) `` generic '' choices for @xmath84 , and preferably re - use the points generated at step 1 .",
    "a reasonable choice of weighting measure would be @xmath13 itself .",
    "the training set for regression would then consist of all distinct points visited by the process over the course of simulation in step 1 ( possibly after discarding the burn - in period ) , weighted according to the empirical distribution of the process .",
    "a more coarse approximation of @xmath13 would be to use the same sample with all weights being equal .",
    "yet another possibility consists of sampling from a uniform grid that is centered on the area containing the bulk of the invariant mass of the chain .",
    "this area can also be crudely determined from the sample of step 1 .",
    "all these approaches can achieve variance reduction , however the optimal choice remains problem - dependent .",
    "given that the calculation of least squares estimates can be carried out very efficiently using linear algebraic techniques , it is highly advisable to test several alternatives for the problem at hand .    in ch.11 of ref .",
    "@xcite , the problem of selecting an optimal @xmath80 is overcome by introducing a least - squares temporal difference learning ( lstd ) algorithm for the approximation of the value of @xmath80 that minimizes the variance of @xmath36 in the context of discrete - time chains . the same algorithm could in principle be applied to continuous - time chains using the embedded discrete - time markov chain and carrying out the necessary modifications to the original algorithm , based on the results of ref .",
    "while this solution is theoretically justified , it requires setting up and running an lstd estimator in parallel with the simulated chain that will asymptotically converge to the optimal value of @xmath80 . depending on the convergence properties of this estimator , the overall efficiency of the variance reduction scheme may be smaller than the efficiency achieved by using a sub - optimal value for @xmath80 , especially when several approximating functions @xmath100 are considered .",
    "another degree of freedom in the design of shadow function estimators is the choice of the approximating set @xmath101 . here",
    ", the probabilistic interpretation of poisson s equation may assist the selection of approximating functions by providing some useful intuition : assuming @xmath12 is @xmath13-integrable and @xmath6 ergodic , it holds that @xcite @xmath102,\\ ] ] where @xmath103 is the hitting time of some state @xmath104 ( changing @xmath104 simply shifts @xmath105 by a constant ) and @xmath106 denotes expectation given @xmath107 . from this equation one",
    "may infer some general properties of @xmath28 ( e.g. monotonicity , oscillatory behavior etc . )",
    "based on the form of the propensity functions .",
    "the same formula can be used to provide some crude simulation - based estimates of @xmath105 , which can be also helpful for the selection of @xmath108 .",
    "finally , a lyapunov - type analysis can be employed to infer the asymptotic behavior of @xmath28 @xcite .",
    "chemical reaction systems typically depend on several kinetic parameters , and the calculation of the output sensitivity with respect to these parameters is an essential step in the analysis of a given model .",
    "while there are several powerful parameter sensitivity methods available today @xcite , they are mostly appropriate for transient sensitivity analysis , as the variance of their estimates tends to grow with the simulation length .",
    "indeed , it can be shown that the variance of sensitivity methods based on the so - called likelihood ratio @xcite or the girsanov transformation @xcite grows linearly with time .",
    "on the other hand , the variance of estimators based on finite parametric perturbations can be shown to remain bounded under mild conditions on the propensity functions , provided the underlying process is ergodic",
    ". however , the stationary variance can be still quite large , which makes necessary the use of a variance reduction method , such as the one presented here . besides providing reduced - variance estimates of various steady - state functions of the chain ,",
    "the shadow function estimator can be also employed for sensitivity analysis using a finite difference scheme @xcite and the common random numbers ( crn ) estimator @xcite .",
    "more analytically , assuming that the propensity functions of @xmath6 are of the form @xmath109 , where @xmath110 is a parameter of interest , the finite difference method aims to characterize the sensitivity of the steady - state value of a given function @xmath12 to a small finite perturbation of @xmath111 of @xmath110 around a nominal value @xmath112 .",
    "if @xmath111 is small enough , we expect that @xmath113 will be approximately equal to @xmath114 .",
    "finite difference - based sensitivity analysis using shadow functions can be simply carried out by generating process trajectories for the nominal and perturbed parameter values , and estimating @xmath114 by @xmath115 . as shown in ref .",
    "@xcite , use of the same random number stream for the generation of both the nominal and perturbed trajectories can result in great variance decrease compared to using independent streams .",
    "to demonstrate the efficiency of shadow function estimators , we next present two applications of the method to steady - state sensitivity estimation .",
    "we compare our finite difference scheme that uses common random numbers and the shadow function estimator to the method of coupled finite differences ( cfd ) @xcite , which frequently outperforms finite - difference estimators based on common random numbers and the random time change representation @xcite .",
    "all numerical examples were generated using custom - written matlab scripts running on a 3.4 ghz quad - core pc with 8 gb of ram .",
    "as a first example , we consider the stochastic focusing model of @xcite , where an input signaling molecule @xmath116 inhibits the production of another molecule @xmath117 .",
    "stochastic focusing arises due to the presence of stochastic fluctuations in @xmath116 , that make the mean value of @xmath117 more sensitive to changes @xmath116 than predicted by the deterministic model of the system .",
    "the same system is treated in ref .",
    "@xcite using a more sophisticated method based on trajectory reweighting .",
    "the system reactions are given below : @xmath118 where @xmath119 .",
    "the parameters used are @xmath120 , @xmath121 and @xmath122 , while @xmath123 is varied between 200 and 900 to study the effect of varying @xmath124 $ ] on @xmath125 $ ] . more specifically ( and similarly to ref .",
    "@xcite ) , we want to calculate the gain @xmath126 to this end we estimate @xmath127 using finite differences with @xmath128 at several points between @xmath129 and @xmath130 . figure [ sf_gain ] shows the calculated confidence intervals for @xmath131 obtained by the common random number ( crn ) estimator , the crn estimator in conjunction with a shadow function and the cfd method . for each value of @xmath123 , a simulated sample path of length @xmath132 time units ( t.u . )",
    "was used to generate 19 batches of length 400 t.u .",
    "each , while the first 400 t.u . were discarded as burn - in .",
    "shadow functions consisted of linear combinations of all monomials in two variables up to order three ( that is , @xmath133 , with @xmath134 ) , together with can provide useful intuition for the selection of approximating functions . in the case at hand , @xmath135 , so @xmath28 ( the solution to the poisson equation ) is expected to grow only very slowly with @xmath116 , as the production rate of @xmath117 tends to zero as @xmath136 . ]",
    "this set of @xmath100 s was selected manually and is definitely not the `` optimal '' choice .",
    "the training set used for regression consisted of all unique points visited by the process sample paths after a burn - in period .",
    "two alternative weighting schemes were tested for each value of @xmath123 : according to the first , all points were assigned equal weight ( @xmath138 ) , while in the second one the points were weighted according to the empirical distribution of the process , calculated using the simulated sample paths ( @xmath139 . both schemes lead to variance reduction , and calculation of @xmath99 in each case can be performed very fast ( @xmath140 sec ) , given the small number of training points ( @xmath141 ) .",
    "post - processing of the trajectories for the evaluation of the shadow function over the different batches takes another 5 sec of cpu time . on the other hand ,",
    "ssa simulation takes on average 40 sec , which demonstrates that the overhead associated with the shadow function usage is relatively small , while the computational savings in the estimation of @xmath142 are significant , as table [ vr_ar ] demonstrates .",
    "finally , a cfd simulation of the same length requires 220 sec of cpu time on average , while achieving a smaller magnitude of variance reduction .     to @xmath142 ,",
    "estimated with the finite difference method . shown",
    "are 95% confidence intervals obtained with the method of batch means @xcite .",
    "green : crn estimator .",
    "blue : cfd estimator .",
    "red : crn combined with shadow functions . ]",
    ".variance reduction in the estimation of @xmath28 [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     the variance reduction method remains quite efficient computationally in this case as well : ssa simulation of a @xmath143 t.u .",
    "trajectory takes about 17 sec of cpu time , while calculation of @xmath80 requires 1 sec and post - processing of the sample path another 3 sec . at the same time",
    ", a cfd simulation of the same length requires 95 sec of cpu time on average , while failing to achieve a comparable level of variance reduction .",
    "we demonstrated the applicability of the powerful shadow function method to the problem of steady - state simulation of stochastic chemical kinetics .",
    "our results suggest that a significant increase in the efficiency of a steady - state estimator is possible by only a small increase in its computational cost .",
    "the method can be applied to the steady - state estimation of practically any function of the process , and can thus provide improved estimates of high order ( cross-)moments , as well as estimates of stationary probabilities for subsets of the process state space , by using set indicators as cost functions .",
    "the magnitude of variance reduction achieved by the shadow function method allows also the efficient and precise computation of steady - state parameter sensitivities using the finite difference method .",
    "the comparison of the efficiency of this approach for providing steady - state sensitivity estimates with the one presented in @xcite is the topic of our ongoing work",
    ". it would also be instructive to assess the relative strengths and weaknesses of the lstd approximation algorithm for optimizing the shadow function @xcite and test its scalability with system size and number of approximating functions ( note that only one - dimensional examples are treated in @xcite ) .",
    "the proposed workflow for arriving at a useful shadow function can be improved at several points , by drawing from the large literature on function approximation techniques , in order to enlarge its range of applicability and its accuracy . however , even a crude approach such as the one presented above seems to be sufficient for systems of practical interest .",
    "from and , @xmath144 , where @xmath145 solves the poisson equation @xmath146 .",
    "this implies that @xmath147 , where @xmath148 is the solution of the poisson equation @xmath149 .",
    "the variance of the shadow function estimator thus becomes @xmath150\\\\ & = \\sigma_1 ^ 2 - 2\\left[\\langle f_c,\\psi\\theta\\rangle -\\langle q(\\psi\\theta),g_1\\rangle + \\langle q(\\psi\\theta),\\psi\\theta\\rangle\\right].\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> we address the problem of estimating steady - state quantities associated to systems of stochastic chemical kinetics . in most cases of interest </S>",
    "<S> these systems are analytically intractable , and one has to resort to computational methods to estimate stationary values of cost functions . in this work </S>",
    "<S> we consider a previously introduced variance reduction method and present an algorithm for its application in the context of stochastic chemical kinetics . </S>",
    "<S> using two numerical examples , we test the efficiency of the method for the calculation of steady - state parametric sensitivities and evaluate its performance in comparison to other estimation methods . </S>"
  ]
}