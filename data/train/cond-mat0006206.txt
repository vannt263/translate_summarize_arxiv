{
  "article_text": [
    "the learning of noisy examples by a nonlinear perceptron is a _ frustrating _ process , in the sense that competing information extracted from the training examples needs to be processed @xcite .",
    "since learning usually involves minimizing a cost function , the learner often has to choose between interpolating the conflicting examples or sacrificing some in favor of satisfying others , so as to attain a minimum overall cost .",
    "this kind of competition is especially marked in nonlinear perceptrons .",
    "this sacrificial effect is an important issue . as we shall see",
    ", it will lead to a gap in the activation distribution of the examples .",
    "the activations of the sacrificed examples are separated from those of the satisfied ones by a wide margin , and different choices of partitioning the sacrificed and preferred examples correspond to different local minima in the energy landscape .",
    "the appearance of a multiplicity of local minima leads to the roughening of the energy landscape , rendering the learning processes prone to being trapped before reaching the global minimum .",
    "the effect is best understood using the cavity approach @xcite .",
    "it uses a self - consistency argument to consider what happens when a new example is added to the training set .",
    "when the learner adopts a sacrificial learning strategy , learning the added example may lead to different choices of the sacrificed examples in the background , causing a shift of the global energy minimum among a number of local minima .",
    "the cavity method yields identical macroscopic predictions with the replica method @xcite .",
    "the assumption of a smooth energy landscape corresponds to the replica symmetric ansatz in the replica approach .",
    "instability appears in the ansatz when the almeida - thouless condition is violated @xcite , beyond which replica symmetry breaking solutions have to be introduced , corresponding to a rough energy landscape .",
    "as we shall see , the appearance of the gap is closely related to the almeida - thouless line .    in this paper",
    ", we analyze the learning of noisy examples by a nonlinear perceptron using the cavity method .",
    "we study the activation distribution of the training examples and find parameter regimes with gaps in the distribution .",
    "simulation results show that the assumption of a smooth energy landscape works well when no gaps are present , but fails when gaps appear .",
    "the rule to be learned is generated by a teacher perceptron with @xmath0 weights @xmath1 , @xmath2 and @xmath3 .",
    "the student perceptron , with @xmath0 weights @xmath4 , @xmath2 , tries to model the teacher by learning from a set of @xmath5 examples .",
    "each example , labeled @xmath6 with @xmath7 , consists of an input vector @xmath8 and the noisy output @xmath9 of the teacher .",
    "the input components @xmath10 are random variables , with @xmath11 and @xmath12 .",
    "the teacher output @xmath9 is a nonlinear activation function of the teacher activation @xmath13 , corrupted by a gaussian noise @xmath14 with @xmath15 and @xmath16 .",
    "that is , @xmath17 , where @xmath18 is the noise level , and here we use the sigmoid function @xmath19^{-1}$ ] . correspondingly , the student models the teacher outputs by @xmath20 , where the student activation is @xmath21 .",
    "learning is attained by minimizing the energy function which consists of the errors of the student in reproducing the teacher s outputs for the training set , as well as the penalty term for excessive complexity .",
    "hence we use the energy function @xmath22 where @xmath23 is the weight decay strength . minimizing the energy function by gradient descent",
    ", the student reaches the equilibrium state @xmath24      if an example 0 is fed to the student , the activation @xmath25 is called cavity activation . since the student @xmath26 has no information about the example , the cavity activation is a gaussian variable for random inputs @xmath27 .",
    "its mean , variance and covariance with the teacher activation of example 0 are given by @xmath28 , @xmath29 and @xmath30 respectively , where @xmath31 denotes the ensemble average , and the parameters @xmath32 and @xmath33 are defined by @xmath34 hence in the large @xmath0 limit , the cavity activation can be expressed as @xmath35 , where @xmath36 is a gaussian variable with mean 0 and variance 1 .",
    "now compare the student @xmath26 with another one which incorporates example 0 in the training set , denoted by @xmath37 .",
    "the generic student activation @xmath38 is no longer a gaussian variable .",
    "nevertheless , it is reasonable to expect that the difference between the students @xmath26 and @xmath37 is small . following the perturbative analysis in @xcite ,",
    "we conclude that @xmath39 is a well defined function of @xmath40 , given by @xmath41 where @xmath42 is the local susceptibility given by @xmath43    for the nonlinear perceptron , it is possible that for sufficiently large @xmath42 , the generic activation @xmath39 is a multi - valued function of the cavity activation @xmath40 . in this case",
    "we have to choose the one which minimizes the energy function in ( [ energy ] ) .",
    "the cavity method shows that the energy increase on adding example 0 is @xmath44 the first term is the primary change due to the added example , and the second term is due to the adjustment of the background examples . in the multi - valued region",
    "one needs to compare those solutions whose values of @xmath39 are closer to @xmath40 ( therefore favorable in the background adjustment ) with those whose outputs @xmath45 are closer to the teacher s outputs @xmath46 ( therefore favorable in the primary cost ) .",
    "this competition leads to a discontinuity in the range of the activation @xmath39 when the cavity activation @xmath40 varies , accompanied by the appearance of gaps in the activation distribution .",
    "similarly , the cavity method can be used to analyze the changes when an input 0 is added to the rule . in this case , the teacher outputs are given by @xmath47 , where @xmath48 is the original teacher activation with @xmath0 inputs . if the student has inputs 1 to @xmath0 only , the resultant student perceptron is given by @xmath49 .",
    "we may construct the weight 0 for the student perceptron using the same prescription , namely @xmath50 however , this is not the generic weight since in the activation functions @xmath51 , the arguments @xmath52 do not contain the input 0 ; nor is the information fed from input 0 ever being utilized in the learning of @xmath52 .",
    "hence @xmath53 is called the cavity weight .",
    "now compare the student with another one with inputs 0 to @xmath0 , and which incorporates input 0 in all the training examples .",
    "its weights are denoted by @xmath54 for @xmath2 and @xmath55 for input 0 .",
    "@xmath55 is different from @xmath53 .",
    "nevertheless , it is also reasonable to expect that the difference between the students @xmath4 and @xmath54 is small . using the perturbative analysis",
    ", we conclude that @xmath55 is a well defined function of @xmath53 , given by @xmath56 the cavity weight distribution @xmath57 is a gaussian with mean and variance given by @xmath58}\\right\\rangle_\\mu b_0,\\ ] ] @xmath59      making use of the relation ( [ weight ] ) , we can obtain the self - consistent equations for the macroscopic parameters @xmath42 , @xmath33 and @xmath32 in ( [ sus ] ) and ( [ qr ] ) , @xmath60f''(x)}{1+\\gamma     \\{f'(x)^{2}-[f(\\sqrt{1+t^2}u)-f(x)]f''(x)\\}}{\\rm d } { \\it u}{\\rm d }          { \\it v } ,         \\\\",
    "r&=&\\alpha\\gamma\\int\\!\\!\\!\\!\\int\\frac{f'(\\sqrt{1+t^2}u)f'(x ) }          { 1+\\gamma\\{f'(x)^{2}-[f(\\sqrt{1+t^2}u)-f(x)]f''(x)\\}}{\\rm d }          { \\it u}{\\rm d } { \\it v } ,",
    "\\\\     q - r^2&=&\\alpha\\gamma^2\\int\\!\\!\\!\\!\\int          [ f(\\sqrt{1+t^2}u)-f(x)]^2 f'(x)^2{\\rm d }          { \\it u}{\\rm d } { \\it v } ,    \\end{aligned}\\ ] ] where d@xmath61d@xmath62/\\sqrt{2\\pi}$ ] and d@xmath63d@xmath64/\\sqrt{2\\pi}$ ] are two independent gaussian measures .",
    "both the noise - corrupted teacher activation and the cavity activation are gaussian distributed and determined from @xmath65 and @xmath66 via @xmath67 and the generic activation @xmath68 is given by the solution of @xmath69f'(x).\\ ] ]    the progress of learning is monitored by the training error @xmath70 and the generalization error @xmath71 , which are respectively the root mean square errors for a training example and an arbitrary example , @xmath72 ^ 2{\\rm d }          { \\it u}{\\rm d } { \\it v } ,    \\\\",
    "\\varepsilon_g^2&=&\\int\\!\\!\\!\\!\\int\\left[f(\\sqrt{1+t^2}\\,u)-f    \\left(\\frac{r}{\\sqrt{1+t^2}}u+\\sqrt{q-\\frac{r^2}{1+t^2}}\\,v\\right)\\right]^2     { \\rm d}{\\it u}{\\rm d}{\\it v}.     \\end{aligned}\\ ] ]      the validity of the perturbation approach can be checked by considering the stability condition of the equilibrium state .",
    "when example 0 is added , the amplitude of the change in the student vector is given by @xmath73 hence the stability condition is @xmath74 this is identical to the stability condition of the replica - symmetric ansatz in the replica approach , the so - called almeida - thouless ( at ) condition . in particular , we note that when a gap is present in the activation distribution , @xmath52 is a discontinuous function of @xmath75 and the system becomes unstable .",
    "gaps in the activation distribution appear for values of local susceptibility @xmath76 , when a single value of the cavity activation in ( [ act ] ) may map onto multiple values of the generic activation , corresponding to different energy minima .",
    "when the energy minimum favors the generic activation to take a value closer to the teacher activation than the cavity activation , the example is satisfied .",
    "otherwise , when the generic activation is closer to the cavity activation , the example is sacrificed .    as shown in fig .",
    "1(a ) , sacrificial learning first occurs at the extreme values of the teacher output @xmath77 .",
    "this is because in nonlinear perceptrons , changes in the student activation around these extreme values of @xmath77 do not result in significant changes in the training error of an example , and if the cavity activation is very different from the teacher s , it is more economical to keep the student activation close to the cavity activation , so that the background adjustment remains small .",
    "hence sacrificial learning is a unique consequence of the nonlinearity of the perceptron output .",
    "in contrast , no sacrificial learning is present in linear perceptrons , even when perfect learning is impossible @xcite .",
    "[ hbt ]    in fig .",
    "1(a ) , no values of the student output in the shaded region exist . for @xmath78 , student activations to the left of the shaded region",
    "correspond to the satisfied examples , whereas those to the right correspond to the sacrificed ones . for intermediate values of @xmath77 , the competitive effects are less , and there are no gaps developed .",
    "figure 1(b ) shows the regions for the existence of gapped activation distributions .",
    "they are closely related to the unstable regions which violate the condition ( [ stab ] ) .",
    "the gapped regions lie inside the unstable regions , since the development of a gap is already sufficient to cause an uncontrollable change when a new example is added .",
    "however , provided that @xmath79 is not too small , the boundaries of the gapped and unstable regions are very close to each other .",
    "figure 1(b ) shows that frustration is serious when the training set size is small , leading to gapped activation distributions .",
    "when the training examples are sufficient , the underlying rule can be extracted with confidence , thereby restoring the continuous distribution .",
    "furthermore , increasing the data noise broadens the gapped region .",
    "indeed , noisy data introduces competing information to be learned by the student , and hence increases the degree of frustration . on the other hand ,",
    "the gapped region narrows with increasing weight decay strength .",
    "arguably , weight decay restricts the flexibility in the weight space , thus reducing the tendency for multiple minima .",
    "[ hbt ]    figure 2(a ) shows a typical activation distribution in the region of continuous distribution , where @xmath80 and the stability condition ( [ stab ] ) is fulfilled . comparing with the simulation result",
    ", we see that the assumption of a smooth energy landscape used in the present work is valid in this region .",
    "the theoretical and simulational results of @xmath70 and @xmath71 also agree .",
    "in contrast , the gapped distributions in fig .",
    "2(b ) show that the assumption does not well describe the simulation result when @xmath81 and the stability condition ( [ stab ] ) is violated .",
    "there are prominent differences of @xmath70 and @xmath71 between theoretical and simulational results . to improve the agreement , a rough energy landscape as discussed in @xcite",
    "must be introduced .",
    "we have demonstrated the existence of band gaps in the activation distribution , and attributed them to frustrations arising from the competition of conflicting information inherent in noisy data , and the nonlinearity of the student perceptron .",
    "activations corresponding to sacrificed or satisfied examples during learning are seperated by band gaps .",
    "the existence of band gaps necessitates the picture of a rough energy landscape . in the picture of the replica approach",
    ", it corresponds to the replica symmetry breaking ansatz beyond the almeida - thouless line .",
    "we remark that the sacrificial effects are common in many other cases , such as multilayer perceptrons @xcite and weight pruning @xcite .",
    "it may also exist in support vector machines ( svm ) when examples are noisy and insufficient @xcite .",
    "they may create local minima which complicate the convergence of learning processes .",
    "hence it is an issue that should be considered both theoretically and practically ."
  ],
  "abstract_text": [
    "<S> using the cavity method we consider the learning of noisy teacher - generated examples by a nonlinear student perceptron . for insufficient examples and weak weight decay , </S>",
    "<S> the activation distribution of the training examples exhibits a gap for the more difficult examples . </S>",
    "<S> this illustrates that the outliers are sacrificed for the overall performance . </S>",
    "<S> simulation shows that the picture of the smooth energy landscape can not describe the gapped distributions well , implying that a rough energy landscape may complicate the learning process . </S>"
  ]
}