{
  "article_text": [
    "it is well known that the tsallis entropy and fisher information entropy ( matrix ) are very important quantities expressing information measures in nonextensive systems .",
    "the tsallis entropy for @xmath0-unit nonextensive system is defined by @xcite-@xcite @xmath1 with @xmath2^q \\:\\pi_i d x_i , \\label{eq : a2}\\end{aligned}\\ ] ] where @xmath3 is the entropic index ( @xmath4 ) , and @xmath5 denotes the probability distribution of @xmath0 variables @xmath6 . in the limit of @xmath7 , the tsallis entropy reduces to the boltzman - gibbs - shannon entropy given by @xmath8 the boltzman - gibbs - shannon entropy is extensive in the sense that for a system consisting @xmath0 independent but equivalent subsystems , the total entropy is a sum of constituent subsystems : @xmath9 .",
    "in contrast , the tsallis entropy is nonextensive : @xmath10 for @xmath11 , and @xmath12 expresses the degree of the nonextensivity of a given system .",
    "the tsallis entropy is a basis of the nonextensive statistical mechanics , which has been successfully applied to a wide class of systems including physics , chemistry , mathematics , biology , and others @xcite .",
    "the fisher information matrix provides us with an important measure on information .",
    "its inverse expresses the lower bound of decoding errors for unbiased estimator in the cramr - rao inequality .",
    "it denotes also the distance between the neighboring points in the rieman space spanned by probability distributions in the information geometry .",
    "the fisher information matrix expresses a local measure of positive amount of information whereas the boltzman - gibbs - shannon - tsallis entropy represents a global measure of ignorance @xcite . in recent years , many authors have investigated the fisher information in nonextensive systems @xcite-@xcite . in a previous paper @xcite ,",
    "we have pointed out that two types of _ generalized _ and _ extended _ fisher information matrices are necessary for nonextensive systems @xcite .",
    "the generalized fisher information matrix @xmath13 obtained from the generalized kullback - leibler divergence in conformity with the tsallis entropy , is expressed by @xmath14 , \\label{eq : a4}\\end{aligned}\\ ] ] where @xmath15 $ ] denotes the average over @xmath5 [ @xmath16 characterized by a set of parameters @xmath17 . on the contrary , the extended fisher information matrix @xmath18 derived from the cramr - rao inequality in nonextensive systems , is expressed by @xcite @xmath19 , \\label{eq : a5}\\end{aligned}\\ ] ] where @xmath20 $ ] expresses the average over the escort probability @xmath21 given by @xmath22^q}{c_q^{(n)}},\\end{aligned}\\ ] ] @xmath23 being given by eq .",
    "( [ eq : a2 ] ) . in the limit of @xmath24 , both the generalized and extended fisher information matrices reduce to the conventional fisher information matrix .",
    "studies on the information entropies have been made mainly for independent ( uncorrelated ) systems .",
    "effects of correlated noise and inputs on the fisher information matrix and shannon s mutual information have been extensively studied in neuronal ensembles ( for a recent review , see ref .",
    "@xcite ; related references therein ) .",
    "it is a fundamental problem in neuroscience to determine whether correlations in neural activity are important for decoding , and what is the impact of correlations on information transmission .",
    "when neurons fire independently , the fisher information increases proportionally to the population size . in ensembles with the limited - range correlations ,",
    "however , the fisher information is shown to saturate as a function of population size @xcite-@xcite . in recent years",
    "the interplay between fluctuations and correlations in nonextensive systems has been investigated @xcite-@xcite .",
    "it has been demonstrated that in some globally correlated systems , the tsallis entropy becomes extensive while the boltzman - gibbs - shannon entropy is nonextensive @xcite .",
    "thus the correlation plays important roles in discussing the properties of information entropies in nonextensive systems .",
    "it is the purpose of the present paper to study effects of the spatially - correlated variability on the tsallis entropy and fisher information in nonextensive systems . in sec .",
    "2 , we will discuss information entropies of correlated nonextensive systems , by using the probability distributions derived by the maximum entropy method ( mem ) . in sec . 3",
    ", we discuss the marginal distribution to study the properties of probability distributions obtained by the mem .",
    "previous related studies are critically discussed also . the final sec .",
    "4 is devoted to our conclusion . in appendix a , results of the mem for uncorrelated , nonextensive systems are briefly summarized @xcite .",
    "we consider correlated @xmath0-unit nonextensive systems , for which the probability distribution is derived with the use of the mem under the constraints given by @xmath26 ,   \\label{eq : c22}\\\\ \\sigma^2 & = & \\frac{1}{n } \\sum_i e_q\\left[(x_i-\\mu)^2 \\right ] ,   \\label{eq : c23 } \\\\",
    "s \\:\\sigma^2 & = & \\frac{1}{n(n-1)}\\sum_i \\sum_{j ( \\neq i ) } e_q\\left[(x_i-\\mu)(x_j-\\mu ) \\right ] , \\label{eq : c24}\\end{aligned}\\ ] ] @xmath27 , @xmath28 and @xmath29 expressing the mean , variance , and degree of the correlated variability , respectively .",
    "cases with @xmath25 and arbitrary @xmath0 will be separately discussed in secs .",
    "2.1 and 2.2 , respectively .    for a given correlated nonextensive system with @xmath25 , the mem with constraints given by eqs .",
    "( [ eq : c21])-([eq : c24 ] ) yields ( details being explained in appendix b ) @xmath30 , \\label{eq : c5}\\end{aligned}\\ ] ] with @xmath31 where @xmath32 denotes the beta function and @xmath33 expresses the @xmath3-exponential function defined by @xmath34^{1/(1-q)}. \\label{eq : c13}\\ ] ] the matrix @xmath35 with elements @xmath36 is expressed by the inverse of the covariant matrix @xmath37 given by @xmath38 with @xmath39 .",
    "\\hspace{1cm}\\mbox{for $ i , j=1,2 $ } \\label{eq : d2 } \\end{aligned}\\ ] ] in the limit of @xmath24 , the distribution @xmath40 reduces to @xmath41 , \\label{eq : d3 } \\end{aligned}\\ ] ] which is nothing but the gaussian distribution for @xmath25 .",
    "we have calculated information entropies , by using the distribution given by eq .",
    "( [ eq : c5 ] ) . * tsallis entropy *    we obtain @xmath42+\\log(r_q^{(2 ) } ) , \\hspace{1cm}\\mbox{for $ q=1 $ } \\\\ & = & \\frac{1-c_q^{(2)}}{q-1 } , \\hspace{4cm}\\mbox{for $ q \\neq 1 $ } \\label{eq : d6}\\end{aligned}\\ ] ] with @xmath43 where @xmath44 is given by eq .",
    "( [ eq : c9])-([eq : c11 ] ) . from @xmath45 given by eq .",
    "( [ eq : c8 ] ) , we may obtain the @xmath29 dependence of @xmath46 as given by @xmath47 , \\hspace{2cm}\\mbox{for $ \\mid s \\mid \\ll 1 $ } \\label{eq : d9}\\end{aligned}\\ ] ] which yields @xmath48    figure [ figh](a ) shows @xmath49 as a function of the correlation @xmath29 for @xmath25 ( values of @xmath50 and @xmath51 are hereafter adopted in model calculations shown in figs .",
    "[ figh]-[fige ] ) .",
    "we note that tsallis entropy is decreased with increasing absolute value of @xmath29 independently of its sign .",
    "* fisher information *    by using eqs . ( [ eq : a4 ] ) and ( [ eq : a5 ] ) for @xmath52 , we obtain the fisher information matrices given by @xmath53 which show that @xmath54 is independent of @xmath3 and that the inverses of both matrices are proportional to @xmath55 .",
    "figure [ figj ] shows the @xmath29 dependence of the extended fisher information for @xmath25 , whose inverse is increased ( decreased ) for a positive ( negative ) @xmath29 , depending on a sign of @xmath29 in contrast to @xmath56 .",
    "it is possible to extend our approach to the case of arbitrary @xmath0 , for which the mem with the constraints given by eqs .",
    "( [ eq : c21])-([eq : c24 ] ) lead to the distribution given by ( details being given in appendix b ) @xmath57 , \\label{eq : f1}\\end{aligned}\\ ] ] with @xmath58}{\\nu_q^{(n)}\\sigma^2(1-s)[1+(n-1)s ] } , \\\\ b & = & - \\:\\frac{s}{\\nu_q^{(n)}\\sigma^2(1-s)[1+(n-1)s ] } ,",
    "\\\\ z_q^{(n ) } & = & \\frac{(2 \\nu_q^{(n ) }   \\sigma^2)^{n/2 } \\ : r_q^{(n ) } } { ( q-1)^{n/2}}\\;\\ ; \\pi_{i=1}^n   \\:b\\left(\\frac{1}{2 } , \\frac{1}{q-1}-\\frac{i}{2 } \\right ) , \\hspace{1 cm } \\mbox{for $ 1 < q < 3$}\\\\ & = & ( 2 \\pi   \\sigma^2)^{n/2 } \\ : r_q^{(n ) } , \\hspace{6 cm } \\mbox{for $ q=1 $ } \\\\ & = & \\frac{(2 \\nu_q^{(n ) } \\sigma^2)^{n/2 } \\ : r_q^{(n ) } } { ( 1-q)^{n/2}}\\;\\ ; \\pi_{i=1}^n   \\:b\\left(\\frac{1}{2 } , \\frac{1}{1-q}+\\frac{(i+1)}{2 } \\right ) , \\hspace{0.5 cm } \\mbox{for $ 0 < q < 1 $ }   \\\\ r_q^{(n ) } & = & \\ : \\{(1-s)^{n-1}[1+(n-1)s ] \\}^{1/2 } , \\label{eq : f3 } \\\\",
    "\\nu_q^{(n ) } & = & \\frac{[(n+2)-nq]}{2}. \\label{eq : f4}\\end{aligned}\\ ] ] the matrix @xmath35 is expressed by the inverse of the covariant matrix @xmath37 whose elements are given by @xmath59 .",
    "\\end{aligned}\\ ] ] in the limit of @xmath24 , the distribution given by eq .",
    "( [ eq : f1 ] ) becomes the multivariate gaussian distribution given by @xmath60.\\end{aligned}\\ ] ]    it is necessary to note that there is the condition for a physically conceivable @xmath29 value given by [ see eq .",
    "( [ eq : z6 ] ) , details being discussed in appendix c ] @xmath61 where the lower and upper critical @xmath29 values are given by @xmath62 and @xmath63 , respectively . in the case of @xmath25 and @xmath64 , for example , we obtain @xmath65 and @xmath66 , respectively .    by using the probability distribution given by eq .",
    "( [ eq : f1 ] ) , we have calculated information entropies whose @xmath29 dependences are given as follows .",
    "* tsallis entropy *    we obtain @xmath67+\\log(r_q^{(n ) } ) , \\hspace{1cm}\\mbox{for $ q=1 $ } \\\\ & = & \\frac{1-c_q^{(n)}}{q-1 } , \\hspace{4cm}\\mbox{for $ q \\neq 1 $ } \\label{eq : f5b}\\end{aligned}\\ ] ] with @xmath68 , \\hspace{0.5cm}\\mbox{for $ \\mid s \\mid \\ll 2/\\sqrt{n(n-1 ) } $ } \\label{eq : f5c}\\end{aligned}\\ ] ] where the @xmath29 dependence of @xmath23 arises from a factor of @xmath69 in eq .",
    "( [ eq : f3 ] ) , and @xmath70 expresses the @xmath71 value of @xmath23 .",
    "equation ( [ eq : f5 ] ) yields the @xmath29 dependent @xmath72 given by @xmath73   s^2 ,   \\hspace{0.5cm}\\mbox{for $ \\mid s \\mid \\ll 2/\\sqrt{n(n-1 ) } $ }   \\label{eq : f6}\\end{aligned}\\ ] ] where @xmath74 stands for the tsallis entropy for @xmath71 .",
    "the region where eqs .",
    "( [ eq : f5c ] ) and ( [ eq : f6 ] ) hold becomes narrower for larger @xmath0 .",
    "the @xmath29 dependence of @xmath49 for @xmath64 is shown in fig . [",
    "figh](b ) , where @xmath49 has a peak at @xmath71 and it is decreased with increasing @xmath75 . comparing fig .",
    "[ figh](b ) with fig .",
    "[ figh](a ) , we notice that @xmath29-dependence of @xmath49 for @xmath64 is more significant than that for @xmath25 [ eq .",
    "( [ eq : f6 ] ) ] .",
    "circles in figs . [ figf](a ) and [ figf](b ) show @xmath49 with @xmath71 for @xmath76 and @xmath77 , respectively , which are calculated with the use of the expressions given by eqs .",
    "( [ eq : f5b ] ) and ( [ eq : f5 ] ) .",
    "they are in good agreement with dashed curves showing exact results which are given by eq .",
    "( [ eq : b5 ] ) and shown in figs .",
    "[ fige](a ) and [ fige](b ) in appendix a. squares show @xmath49 with @xmath78 calculated by using eqs .",
    "( [ eq : f5b ] ) and ( [ eq : f5 ] ) .",
    "the tsallis entropy is decreased by an introduced correlation . because of a computational difficulty @xcite , calculations using eqs .",
    "( [ eq : f5b ] ) and ( [ eq : f5 ] ) can not be performed for larger @xmath0 than those shown in figs . [ figf](a ) and [ figf](b ) .    * fisher information",
    "*    the generalized and extended fisher information matrices are given by @xmath79 } ,   \\label{eq : f10 } \\\\",
    "\\tilde{g}_q^{(n ) } & = & \\frac{nq(q+1 ) } { \\sigma^2(3-q)(2q-1 ) [ 1+(n-1 ) s]}. \\label{eq : f11}\\end{aligned}\\ ] ] the results for @xmath24 given by eqs . ( [ eq : f10 ] ) and ( [ eq : f11 ] ) are consistent with those derived with the use of the multivariate gaussian distribution @xcite . by using the fisher information matrices for @xmath80 , @xmath81 and @xmath82 , given by eqs . ( [ eq : b9a ] ) and ( [ eq : b10a ] ) , we obtain @xmath83 which holds independently of @xmath3 .",
    "inverses of the fisher information matrices approach the value of @xmath29 for @xmath84 , and are proportional to @xmath85 for @xmath71 .",
    "in particular , they vanish at @xmath86 .",
    "these features are clearly seen in fig .",
    "[ figd ] where the inverses of fisher information matrices , @xmath87 and @xmath88 , are plotted as a function of @xmath0 for various @xmath29 values .",
    "in the present study , we have obtained the probability distributions , applying the mem to spatially - correlated nonextensive systems .",
    "we will examine our probability distributions in more detail .",
    "the @xmath89 dependences of @xmath90 for @xmath25 given by eq .",
    "( [ eq : c5 ] ) with @xmath71 and @xmath78 are plotted in figs . [ figc](a ) and [ figc](b ) , respectively , where @xmath91 is treated as a parameter . when @xmath71 , the distribution is symmetric with respect of @xmath89 for all @xmath91 values .",
    "when the correlated variability of @xmath78 is introduced , peak positions of the distribution appear at finite @xmath89 for @xmath92 and 1.0 .    in the limit of @xmath71 ( _ i.e. _ no correlated variability ) , @xmath90 given by eq .",
    "( [ eq : c5 ] ) becomes @xmath93^{1/(1-q ) } , \\label{eq : g8}\\end{aligned}\\ ] ] which does not agree with the exact result ( except for @xmath24 ) , as given by @xmath94^{1/(1-q ) } , \\label{eq : g } \\\\ & \\neq & p^{(2)}(x_1,x_2),\\end{aligned}\\ ] ] because of the properties of the @xmath3-exponential function defined by eq .",
    "( [ eq : c13 ] ) : @xmath95 . by using the @xmath3-product @xmath96 defined by @xcite @xmath97^{1/(1-q ) } , \\end{aligned}\\ ] ] we may obtain the expression given by @xmath98^{1/(1-q ) } , \\label{eq : g16}\\end{aligned}\\ ] ] which coincides with @xmath40 given by eq .",
    "( [ eq : g8 ] ) besides a difference between @xmath99 and @xmath100 .    in order to study the properties of the probability distribution of @xmath40 in more details ,",
    "we have calculated its marginal probability ( with @xmath71 ) given by @xmath101^{1/(1-q)+1/2}. \\label{eq : g4}\\end{aligned}\\ ] ] dashed curves in figs . [ figb](a ) and [ figb](b ) show @xmath102 in linear and logarithmic scales , respectively .",
    "the marginal distributions are in good agreement with solid curves showing @xmath103 [ eq .",
    "( [ eq : x4 ] ) ] , @xmath104^{1/(1-q)}. \\label{eq : g7}\\end{aligned}\\ ] ] in the case of @xmath105 , the distribution given by eq .",
    "( [ eq : f1 ] ) yields its marginal distribution ( with @xmath71 ) given by @xmath106^{1/(1-q)+1}. \\label{eq : g6}\\end{aligned}\\ ] ] chain curves in figs . [",
    "figb](a ) and [ figb](b ) represent @xmath107 , which is again in good agreement with solid curves showing @xmath103 .",
    "these results justify , to some extent , the probability distribution adopted in our calculation .    the marginal distribution for an arbitrary @xmath0 ( with @xmath71 ) is given by @xmath108^{1/(1-q)+(n-1)/2 } , \\\\",
    "& \\propto & \\left [ 1- \\frac{(1-q_n ) x_1 ^ 2 } { 2 \\nu_n \\sigma^2 } \\right]^{1/(1-q_n ) } ,   \\label{eq : g11}\\end{aligned}\\ ] ] with @xmath109 equations ( [ eq : g11])-([eq : g13 ] ) show that in the limit of @xmath84 , we obtain @xmath110 , @xmath111 , and @xmath112 reduces to the gaussian distribution .",
    "one of typical microscopic nonextensive systems is the langevin model subjected to multiplicative noise , as given by @xcite-@xcite @xmath113 here @xmath114 expresses the relaxation rate , @xmath115 denotes a function of an external input @xmath116 , and @xmath117 and @xmath118 stand for magnitudes of multiplicative and additive noise , respectively , with zero - mean white noise given by @xmath119 and @xmath120 with the correlated variability , @xmath121 \\delta(t - t'),\\\\ \\langle \\xi_i(t)\\:\\xi_j(t ' ) \\rangle   & = & \\beta^2 [ \\delta_{ij } + c_a(1-\\delta_{ij})]\\delta(t - t'),\\\\ \\langle \\eta_i(t)\\:\\xi_j(t ' ) \\rangle & = & 0 , \\label{eq : h1}\\end{aligned}\\ ] ] where @xmath122 and @xmath123 express the degrees of correlated variabilities of additive and multiplicative noise , respectively .",
    "the fokker - planck equation ( fpe ) for the probability distribution @xmath124 ( @xmath125 ) is given by @xmath126   \\nonumber \\\\ & + & \\frac{\\beta^2}{2}\\sum_{i}\\sum_{j } [ \\delta_{ij } + c_a ( 1-\\delta_{ij } ) ] \\frac{\\partial^2}{\\partial x_{i } \\partial x_{j } } \\:p \\nonumber \\\\ & + & \\frac{\\alpha^2}{2}\\sum_{i } \\sum_j [ \\delta_{ij}+ c_m ( 1-\\delta_{ij } ) ]   \\frac{\\partial}{\\partial x_{i } } x_i \\frac{\\partial}{\\partial x_j } ( x_j \\:p ) , \\label{eq : h0}\\end{aligned}\\ ] ] in the stratonovich representation .",
    "for additive noise only ( @xmath127 ) , the stationary distribution is given by @xmath128,\\end{aligned}\\ ] ] where @xmath129 denotes the average of @xmath130 , and @xmath37 expresses the covariance matrix given by @xmath131.\\end{aligned}\\ ] ]    when multiplicative noise exists @xmath132 , the calculation of even stationary distribution becomes difficult , and it is generally not given by the gaussian .",
    "indeed , the stationary distribution for non - correlated multiplicative noise with @xmath133 , @xmath134 and @xmath135 is given by @xcite@xcite-@xcite @xmath136^{1/(1-q ) }   \\:e^{y(x_i ) } , \\label{eq : h3}\\end{aligned}\\ ] ] with @xmath137 the probability distribution given by eq .",
    "( [ eq : h3 ] ) for @xmath138 ( @xmath139 ) agrees with that derived by the mem for @xmath140 [ eq .",
    "( [ eq : x4 ] ) ] . for @xmath133 , @xmath141 and @xmath142 ( @xmath139 ) , eq .",
    "( [ eq : h3 ] ) becomes @xcite @xmath143 yielding the fisher information given by @xmath144 where @xmath145 and @xmath146 is the heaviside function .",
    "the probability distribution for correlated multiplicative noise ( @xmath133 , @xmath147 ) is also the non - gaussian , which is easily confirmed by direct simulations of the langevin model with @xmath25 @xcite . in some previous studies @xcite-@xcite , the stationary distribution of the langevin model subjected to correlated multiplicative noise with @xmath147 , @xmath141 and @xmath148",
    "is assumed to be expressed by the gaussian distribution with the covariance matrix given by @xmath149 .",
    "\\label{eq : h2}\\end{aligned}\\ ] ] this is equivalent to assume that @xmath150 & \\simeq & \\frac{\\partial}{\\partial x_i }   \\left [ \\langle x_i \\rangle   \\frac{\\partial } { \\partial x_j } ( \\langle x_j \\rangle \\:p ) \\right ] , \\nonumber \\\\",
    "& = & \\mu_i \\mu_j \\frac{\\partial^2 p } { \\partial x_i \\partial x_j } , \\label{eq : h3b}\\end{aligned}\\ ] ] in the fpe given by eq .",
    "( [ eq : h0 ] ) with @xmath151 and @xmath152 . by using such an approximation , abbott and dayan ( ad )",
    "@xcite calculated the fisher information matrix of a neuronal ensemble with the correlated variability , which is given by @xmath153 } + 2 n k ,   \\nonumber \\\\ & = & \\frac{n}{\\sigma_m^2 \\mu^2 [ 1+(n-1)c_m]}+ \\frac{2n}{\\mu^2 } ,   \\label{eq : h4 } \\end{aligned}\\ ] ] with a spurious second term ( @xmath154 ) , where @xmath155 ^ 2=1/\\mu^2 $ ] [ eq .",
    "( 4.7 ) of ref .",
    "@xcite in our notation ] .",
    "equation ( [ eq : h4 ] ) is not in agreement with either eq .",
    "( [ eq : f10 ] ) or ( [ eq : f11 ] ) derived by the mem .",
    "furthermore , the result of ad in the limit of @xmath156 , @xmath157 , does not agree with the exact result given by eq .",
    "( [ eq : h7 ] ) for the langevin model .",
    "this fact casts some doubt on the results of refs .",
    "@xcite-@xcite based on the gaussian approximation given by eq .",
    "( [ eq : h2 ] ) or @xmath158 , which has no physical or mathematical justification .",
    "the fisher information matrix depends on a detailed structure of the probability distribution because it is expressed by the derivative of the distribution with respect to @xmath159 , as given by @xmath160 .",
    "\\label{eq : h5 } \\end{aligned}\\ ] ] we must take into account the non - gaussian structure of the probability distribution in discussing the fisher information of nonextensive systems .",
    "we have discussed effects of the spatially - correlated variability on the tsallis entropy and fisher information matrix in nonextensive systems , by using the probability distribution derived by the mem . although the obtained analytical distribution in the limit of @xmath71",
    "does not hold the relation given by @xmath161 for @xmath11 , it numerically yields good results ( fig . [ figf ] ) , reducing to the multivariate gaussian distribution in the limit of @xmath24 .",
    "our calculations have shown that    \\(i ) the tsallis entropy is decreased by both positive and negative correlations , and    \\(ii ) the inverses of the fisher information matrices are increased ( decrease ) by a positive ( negative ) correlation .",
    "the difference between the @xmath29 dependences of @xmath56 and @xmath162 arises from the difference in their characteristics : @xmath56 provides us with a global measure of ignorance while @xmath162 a local measure of positive amount of information @xcite .",
    "the item ( ii ) implies that the accuracy of unbiased estimate of fluctuation is improved by the negative correlation . if there is known , and strong , negative correlation between successive pairs of data , estimating the unknown parameter as their simple average must reduce the error , as negatively correlated errors tend to cancel in taking the difference .    in connection with the discussion presented in sec .",
    "3 , it is interesting to make a detailed study on the properties of information entropies in the langevin model subjected to correlated as well as uncorrelated multiplicative noise .",
    "such a calculation is in progress and will be reported elsewhere @xcite .",
    "this work is partly supported by a grant - in - aid for scientific research from the japanese ministry of education , culture , sports , science and technology .",
    "we summarize results of the mem for nonextensive systems @xcite . in order to apply the mem to @xmath0-unit nonextensive systems",
    ", we impose the three constraints given by @xmath163 ,   \\label{eq : c2}\\\\ \\sigma^2 & = & \\frac{1}{n } \\sum_i e_q\\left[(x_i-\\mu)^2 \\right ] , \\label{eq : c3}\\end{aligned}\\ ] ] where the @xmath3-dependent @xmath27 and @xmath28 correspond to @xmath164 and @xmath165 , respectively , in @xcite . for a given nonextensive system , the variational condition for the tsallis entropy given by eq .",
    "( [ eq : a1 ] ) with the constraints ( [ eq : c1])-([eq : c3 ] ) yields the @xmath3-gaussian distribution given by @xcite      with @xmath167 , \\label{eq : x4}\\\\ z_q^{(1 ) } & = & \\int   \\exp_q\\left(-\\frac{(x-\\mu)^2}{2 \\nu_q^{(1 ) } \\sigma^2 } \\right ) \\:dx , \\\\ & = & \\left(\\frac{2 \\nu_q^{(1 ) }   \\sigma^2}{q-1 } \\right)^{1/2 } b\\left(\\frac{1}{2 } , \\frac{1}{q-1}-\\frac{1}{2 } \\right ) , \\hspace{1cm}\\mbox{for $ 1 < q < 3 $ } \\label{eq : x5 }   \\\\ & = & \\sqrt{2 \\pi } \\sigma , \\label{eq : x6 } \\hspace{5.5cm}\\mbox{for $ q=1.0 $ }   \\\\ & = & \\left(\\frac{2 \\nu_q^{(1 ) } \\sigma^2}{1-q } \\right)^{1/2 } b\\left(\\frac{1}{2 } , \\frac{1}{1-q}+1 \\right ) , \\hspace{1cm}\\mbox{for $ 0 < q < 1 $ } \\label{eq : x7 } \\\\ \\nu_q^{(1 ) } & = & \\frac{3-q}{2 } , \\label{eq : x9 } \\ ] ] where @xmath32 denotes the beta function .      substituting the probability distribution given by eq .",
    "( [ eq : b1 ] ) to eqs .",
    "( [ eq : a1 ] ) and ( [ eq : a2 ] ) , we obtain the tsallis entropy expressed by @xmath67 ,   \\hspace{1cm}\\mbox{for $ q=1 $ } \\\\ & = & \\frac{[1-(c_q^{(1)})^n]}{(q-1 ) } , \\hspace{2cm}\\mbox{for $ q \\neq 1 $ } \\label{eq : b3}\\end{aligned}\\ ] ] where @xmath168",
    ". we may express @xmath72 in terms of @xmath169 by @xcite @xmath170 where @xmath171 .",
    "equation ( [ eq : b6 ] ) clearly shows that the tsallis entropy is generally nonextensive except for @xmath24 for which @xmath172 .",
    "figures [ fige](a ) and [ fige](b ) show the @xmath0 dependence of the tsallis entropy per element , @xmath49 , of uncorrelated systems ( @xmath71 ) , which are calculated with the use of eq .",
    "( [ eq : b5 ] ) . with increasing @xmath0 , @xmath49 is decreased for @xmath77",
    "whereas it is significantly increased for @xmath76 .      with the use of eqs .",
    "( [ eq : a4 ] ) and ( [ eq : a5 ] ) for @xmath52 , the generalized and extended fisher information matrices are given by @xcite @xmath173 with @xmath174 which show that fisher information matrices are extensive .",
    "in the case of @xmath25 , the probability distribution @xmath40 given by eqs .",
    "( [ eq : c5 ] ) and ( [ eq : c6 ] ) is rewritten as @xmath175^{1/(1-q ) } , \\label{eq : y1}\\end{aligned}\\ ] ] with @xmath176,\\\\ & = & \\lambda_1 y_1 ^ 2+\\lambda_2 y_2 ^ 2 ,   \\label{eq : y2}\\end{aligned}\\ ] ] where @xmath177 and @xmath178 ( @xmath179 ) are eigenvalues and eigenvectors of @xmath180 given by @xmath181 the averages of @xmath182 $ ] and @xmath183 $ ] are given by @xmath184 & = & \\frac{1}{\\nu_q^{(2)}\\lambda_1 } , \\\\",
    "e_q[y_2 ^ 2 ] & = & \\frac{1}{\\nu_q^{(2)}\\lambda_2},\\end{aligned}\\ ] ] from which we obtain @xmath28 and @xmath185 as @xmath186   = \\frac{1}{2}e_q[y_1 ^ 2+y_2 ^ 2 ] , \\nonumber \\\\ & = & \\frac{a}{\\nu_q^{(2)}(a^2-b^2 ) } ,   \\label{eq : y3}\\\\ s\\ : \\sigma^2 & = & e_q[x_1 x_2 ] = \\frac{1}{2}e_q[y_1 ^ 2-y_2 ^ 2 ] , \\nonumber \\\\",
    "& = & - \\frac{b}{\\nu_q^{(2)}(a^2-b^2)}. \\label{eq : y4}\\end{aligned}\\ ] ] by using eqs .",
    "( [ eq : y3 ] ) and ( [ eq : y4 ] ) , @xmath187 and @xmath188 are expressed in terms of @xmath28 and @xmath29 as @xmath189 which yield the matrix of @xmath35 given by an inverse of the covariance matrix of @xmath37 [ eq .",
    "( [ eq : d2 ] ) ] .    a calculation for the case of an arbitrary @xmath0",
    "may be similarly performed as follows .",
    "the distribution given by eqs .",
    "( [ eq : f1 ] ) and ( [ eq : f0 ] ) is rewritten as @xmath190^{1/(1-q ) } , \\label{eq : w1}\\end{aligned}\\ ] ] with @xmath191,\\\\ & = & \\sum_i \\lambda_i y_i^2 ,   \\label{eq : w2}\\end{aligned}\\ ] ] where @xmath177 and @xmath178 are eigenvalues and eigenvectors , respectively . with the use of eigenvalues given by @xmath192 ,   \\hspace{1cm}\\mbox{for $ i=1 $ } \\\\ & = & \\frac{1}{2}(a - b ) ,   \\hspace{1cm}\\mbox{for $ 1 < i \\leq n$}\\end{aligned}\\ ] ] eqs .",
    "( [ eq : c23 ] ) and ( [ eq : c24 ] ) lead to @xmath193 ,   \\nonumber \\\\ & = & \\frac{1}{\\nu_q^{(n ) } n } \\sum_i \\left ( \\frac{1}{\\lambda_i } \\right ) , \\nonumber \\\\   & = & \\frac{[a+b(n-2)]}{\\nu_q^{(n ) } ( a - b)[a+(n-1)b ] } ,   \\label{eq : w3}\\\\ s\\:\\sigma^2 & = & \\frac{1}{n(n-1 ) } \\sum_{i < j}e_q[y_i^2-y_j^2 ] ,   \\nonumber\\\\ & = & \\left ( \\frac{1}{\\nu_q^{(n ) } n(n-1 ) } \\right ) \\sum_{i < j}\\left ( \\frac{1}{\\lambda_i}-\\frac{1}{\\lambda_j } \\right ) , \\nonumber\\\\ & = & -\\ : \\frac{b}{\\nu_q^{(n ) } ( a - b)[a+(n-1)b]}. \\label{eq : w4 } \\end{aligned}\\ ] ] from eqs .",
    "( [ eq : w3 ] ) and ( [ eq : w4 ] ) , @xmath187 and @xmath188 are expressed in terms of @xmath28 and @xmath29 , as given by @xmath194}{\\nu_q^{(n)}\\sigma^2(1-s)[1+(n-1)s ] } , \\\\ b & = & - \\:\\frac{s}{\\nu_q^{(n)}\\sigma^2(1-s)[1+(n-1)s]}.\\end{aligned}\\ ] ]      in order to discuss the condition for a physically conceivable @xmath29 value , we consider the global variable @xmath195 defined by @xmath196 the first and second @xmath3 moments of @xmath195 are given by @xmath197 & = & \\frac{1}{n } \\sum_i e_q[x_i(t ) ] = \\mu(t ) ,",
    "\\\\ \\label{eq : z2 } e_q[\\ { \\delta x(t ) \\}^2 ]   & = & \\frac{1}{n^2 } \\sum_i \\sum_j e_q[\\delta x_i(t)\\delta x_j(t)],\\\\ & = & \\frac{1}{n^2 } \\sum_i e_q[\\ { \\delta x_i(t ) \\}^2 ] + \\frac{1}{n^2 } \\sum_i \\sum_{j ( \\neq i ) }   e_q[\\delta x_i(t)\\delta x_j(t ) ] , \\\\ & = & \\frac{\\sigma(t)^2}{n}[1+(n-1)s(t ) ] , \\label{eq : z3}\\end{aligned}\\ ] ] where @xmath198 and @xmath199 .",
    "since global fluctuation in @xmath200 is smaller than the average of local fluctuation in @xmath6 , we obtain @xmath201 \\leq   \\frac{1}{n}\\sum_i e[\\ { \\delta x_i(t)\\}^2]=\\sigma(t)^2 .",
    "\\label{eq : z4}\\end{aligned}\\ ] ] equations ( [ eq : z3 ] ) and ( [ eq : z4 ] ) yield @xmath202}{n } \\leq 1.0 , \\label{eq : z5}\\ ] ] which leads to @xmath203 with @xmath204 and @xmath63 .",
    "a. plastino and a. r. plastino , physica a * 222 * , 347 ( 1995 ) . c. tsallis and d. j. bukman , phys .",
    "e * 54 * , r2197 ( 1996 ) .",
    "a. plastino , a. r. plastino , and h. g. miller , physica a * 235 * , 577 ( 1997 ) . f. pennini , a. r. plastino , and a. plastino , physica a * 258 * , 446 ( 1998 ) .",
    "l. borland , f. pennini , a. r. plastino , and a. plastino , eur .",
    "j. b. * 12 * , 285 ( 1999 ) .",
    "a. r. plastino , m. casas , and a. plastino , physica a * 280 * , 289 ( 2000 )",
    ". s. abe , phys .",
    "e * 68 * , 031101 ( 2003 ) .",
    "calculations of @xmath205 for large @xmath0 with the use of eqs .",
    "( [ eq : f5b ] ) and ( [ eq : f5 ] ) need a computer program of the gamma function with a negative ( real ) argument , which is not available in our facility ."
  ],
  "abstract_text": [
    "<S> we have calculated the tsallis entropy and fisher information matrix ( entropy ) of spatially - correlated nonextensive systems , by using an analytic non - gaussian distribution obtained by the maximum entropy method . </S>",
    "<S> effects of the correlated variability on the fisher information matrix are shown to be different from those on the tsallis entropy . </S>",
    "<S> the fisher information is increased ( decreased ) by a positive ( negative ) correlation , whereas the tsallis entropy is decreased with increasing an absolute magnitude of the correlation independently of its sign . </S>",
    "<S> this fact arises from the difference in their characteristics . </S>",
    "<S> it implies from the cramr - rao inequality that the accuracy of unbiased estimate of fluctuation is improved by the negative correlation . </S>",
    "<S> a critical comparison is made between the present study and previous ones employing the gaussian approximation for the correlated variability due to multiplicative noise .    </S>",
    "<S> = 1.333    * effects of correlated variability on information entropies + in nonextensive systems *    hideo hasegawa    _ department of physics , tokyo gakugei university + koganei , tokyo 184 - 8501 , japan _    </S>",
    "<S> ( )    pacs number(s ) : 05.70.-a,05.10.gg,05.45.-a    _ key words _ : tsallis entropy , fisher information , correlated variability , nonextensive systems </S>"
  ]
}