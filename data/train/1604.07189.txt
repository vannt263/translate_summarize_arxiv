{
  "article_text": [
    "before addressing the convergence theory , we would like to discuss stochastic noise modeling and its intrinsic conflict with the deterministic model . here and throughout the rest of the paper ,",
    "assume @xmath31 to be a complete probability space with a set @xmath32 of outcomes of the stochastic event , @xmath33 the corresponding @xmath34-algebra and @xmath35 a probability measure , @xmath36 $ ] .",
    "we restrict ourselves here to probability measures for the sake of simplicity .",
    "extensions to more general measures are straightforward . in the hilbert - space",
    "setting , the noise is typically modeled as follows , see for example @xcite .",
    "let @xmath37 be a stochastic process .",
    "then for @xmath1 @xmath38 defines a real - valued random variable . assuming that @xmath39 for all @xmath40 and that this expectation is continuous in @xmath41",
    ", @xmath42 defines a continuous , symmetric nonlinear bilinearform .",
    "in particular , there exists the _ covariance operator _",
    "@xmath43 with @xmath44    for the stochastic analysis of infinite dimensional problems via deterministic results , is problematic .",
    "namely , if @xmath45 is an orthonormal basis in @xmath46 , the set @xmath47 consists of infinitely many identically distributed random variables with @xmath48 @xcite .",
    "thus @xmath49 is almost surely infinite and a realization of the noise is an element of the hilbert space @xmath46 with probability zero .",
    "let us take the common example of gaussian white noise which can be modeled via the above construction .",
    "namely , with @xmath50 and the covariance operator @xmath51 where @xmath52 is the identity and @xmath53 the variance parameter , the gaussian white noise is described @xcite . as consequence of ( [ eq : eerror ] ) and",
    "explained for example in @xcite , a realization of such a gaussian random variable is with probability zero an element of an infinite dimensional @xmath54-space .",
    "it is therefore inappropriate to use an @xmath54-norm for the residual in case of an infinite dimensional problem . since in this case",
    "a realization of gaussian white noise only lies ( almost surely ) in any sobolev space @xmath55 with @xmath56 where @xmath57 is the dimension of the domain , one should adjust the norm for the residual accordingly . except for the paper @xcite",
    "this issue seems not to have been addressed in the literature . a main reason for this might be that for the practical solution of the inverse problem this is not a severe issue since in reality the measurements are finite dimensional and , in order to use a computer to solve the problem , a finite dimensional approximation of the unknown object has to be used .",
    "in this case the sum in is finite and the noise lies almost surely in the finite dimensional space .",
    "however , difficulties arise whenever one seeks to investigate convergence of the discretized problem to its underlying infinite dimensional problem",
    ". we will not address this issue and assume throughout the whole work that @xmath58 or use the slightly weaker bound on the ky - fan metric ( see section [ ssec : kyfan ] ) . in order to handle the ky fan metric we need to be able to evaluate probabilities @xmath59 , @xmath60 , which is only meaningful if @xmath61 .",
    "assuming that @xmath46 is finite dimensional , then this is clear . for infinite dimensional problems , however , we have to assume that the noise is smooth enough for the sum in ( [ eq : eerror ] ) to converge .",
    "examples for this are brownian noise ( @xmath62-noise ) or pink noise ( @xmath63-noise ) , see e.g. @xcite . at this point",
    "we would also like to mention that as a consequence of our rather generic noise model we might not make use of some specific properties of the noise as would be possible when focusing on a particular distribution of the noise .",
    "however , we are able to show convergence for a large variety of regularization methods .",
    "in order to measure the magnitude of the stochastic noise and the quality of the reconstructions , we need metrics that incorporate the stochastic nature of the problem",
    ". one such metric , which will be the the main tool for our stochastic convergence analysis , is the ky fan metric ( cf .",
    "it is defined as follows .",
    "let @xmath64 and @xmath65 be random variables in a probability space @xmath66 with values in a metric space @xmath67 .",
    "the distance between @xmath64 and @xmath65 in the _ ky fan metric _ is defined as @xmath68    we will often drop the explicit reference to @xmath69 . this metric essentially allows to lift results from a metric space to the space of random variables as the connection to the deterministic setting is inherent via the metric @xmath70 used in its definition .",
    "the deterministic metric is often induced by a norm @xmath71 .",
    "we will implicitly assume that equation is scaled appropriately since @xmath72 @xmath73 by definition .",
    "note that one can use definition also if @xmath74 is a more general distance function than a metric .",
    "then the construction itself is no longer a metric , however , the techniques used in later parts of the paper can readily be expanded to this setting .",
    "an immediate consequence of ( [ def : kyfan ] ) is that @xmath75 if and only if @xmath76 almost surely . convergence in the ky fan metric is equivalent to convergence in probability , i.e. , for a sequence @xmath77 and @xmath78 one has @xmath79 hence convergence in the ky fan metric also leads to pointwise ( almost sure ) convergence of certain subsequences in the metric @xmath70 @xcite .",
    "a somewhat more intuitive and more frequently used stochastic metric is the expectation , or more general , a ( stochastic ) @xmath80 metric .",
    "for random variables @xmath81 and @xmath82 with values in a metric space @xmath83 , @xmath84 defines the @xmath85-th moment of @xmath86 for @xmath87 , assuming the existence of the integral .",
    "we will use @xmath88 and refer to it as _ convergence in expectation_. note that since the variance is defined as @xmath89 one always has @xmath90 we will show later that for parameter choice rules the expectation of the noise has to be slightly overestimated , hence estimating @xmath91 via the popular and often easier to compute @xmath54-norm @xmath92 with is not problematic .    while the main part of our analysis is based on the description of the noise and the reconstruction quality in the ky fan metric",
    ", we will also allow the expectation as measure of the stochastic noise and partially show convergence of the reconstructed solutions in expectation . to this end",
    ", we comment in the following on the connection between those two metrics .",
    "it is well - known that convergence in expectation implies convergence in probability , see for example @xcite .",
    "hence , convergence in the ky fan metric is implied by convergence in expectation ( and also by convergence of higher moments ) .",
    "namely , with markovs inequality one has , for an arbitrary nonnegative random variable @xmath93 with @xmath94 and @xmath95 @xmath96 under an additional assumption one can show conversely that convergence in probability implies convergence in expectation .",
    "we have the following definition .",
    "[ def : uniform_int ] let @xmath66 be a complete probability space .",
    "a family @xmath97 is called _ uniformly integrable _",
    "if @xmath98    [ thm : uniform_int ] let @xmath99 be a sequence convergent almost everywhere ( or in probability ) to a function @xmath7 .",
    "if the sequence @xmath100 is uniformly integrable , then it converges to @xmath7 in the norm of @xmath101 .    from a practical point of view",
    ", uniform integrability of a sequence of regularized solutions to an inverse problem is a rather natural condition . since inverse problems typically arise from some real - world application , it is to be expected that the true solution is bounded .",
    "for example , in computer tomography , the density of the tissue inside the body can not be arbitrarily high .",
    "although for an inverse problem with a stochastic noise model , boundedness of the regularized solutions can not be guaranteed due to the possibly huge measurement error , one can enforce the condition from a priori knowledge of the solution .",
    "assume that the true solution @xmath102 fulfills @xmath103 and @xmath104 globally for some fixed @xmath105 . under this assumption ,",
    "let @xmath106 be a sequence of regularized solution for noisy data with variance @xmath107 .",
    "let @xmath108 and define @xmath109 then the sequence @xmath110 is uniformly integrable . in other words , by discarding solutions that must be far away from the true solution in regard of a priori knowledge , convergence in the ky fan metric implies convergence in expectation .    to close this section ,",
    "let us remark on the computation of the ky fan distance .",
    "it can be estimated via the moments of the noise .",
    "let @xmath111 be random variables in a complete probability space @xmath66 and @xmath112 for some @xmath113",
    ". then @xmath114{\\mathbb{e}(d_\\mathcal{y}(y_1,y_2)^s)}\\ ] ]    one has , due to markov s inequality and the monotonicity of the mapping @xmath115 for @xmath116 , @xmath117 for @xmath118 . solving @xmath119 for @xmath120 yields the claim .",
    "note that even if moments exist for all @xmath113 @xmath121{\\mathbb{e}(d_\\mathcal{y}(y_1,y_2)^s)}\\neq \\mathbb{e}(d_\\mathcal{y}(y_1,y_2)),\\ ] ] see @xcite , due to the tail of the distributions .",
    "in the gaussian case , a direct estimate has been derived in @xcite .",
    "we present it in proposition [ prop : kyfandistance ] .",
    "as mentioned previously , the intention of this paper is to show convergence for inverse problems under a stochastic noise model using results from the deterministic setting .",
    "assume we have at hand a deterministic regularization method of our liking for the solution of under the noisy data where now @xmath122 for some @xmath9 . by regularization method",
    "we understand ( possibly nonlinear ) mappings @xmath123 where @xmath124 is the regularized solution to the regularization parameter @xmath16 given the data @xmath22 .",
    "often , @xmath125 is obtained via the minimization of functionals of the type . in order to deserve the name regularization we require @xmath126 to fulfill @xmath127 under a certain choice of the regularization parameter @xmath16 chosen either a priori @xmath128 or a posteriori @xmath129 . in our notation @xmath102",
    "is the true solution , usually the minimum norm solution with respect to the penalty @xmath130 in , i.e. , @xmath131 note that , in particular for nonlinear problems , @xmath102 does not need to be unique . in @xcite",
    "it was pointed out that this is problematic for the lifting arguments . a standard argument in the deterministic theory is to prove convergence of subsequences to the desired solution , and then deduces convergence of the whole series of regularized solutions , if possible . in the stochastic",
    "setting , this is not possible in general since subsequences for different @xmath69 do not have to be related",
    ". a constructed example for this behavior can be found in section 4.1 . of @xcite . in order to lift general deterministic regularization methods into the stochastic setting",
    "we must therefore require that @xmath102 is unique .",
    "we formulate our convergence results assuming the noise to be bounded with respect to the ky fan metric or in expectation .",
    "as we will see , in the latter case we have to `` inflate '' the expectation for decreasing variance @xmath18 in order to obtain convergence . for the analysis we mainly use a lifting argument using deterministic theory . in (",
    "* ; * ? ? ?",
    "* theorem 4.1 ) , it was proved how by means of the ky fan metric deterministic results can be lifted to the space of random variables for nonlinear tikhonov regularization .",
    "since the theorem is based solely on the fact that there is a deterministic regularization theory and that the probability space @xmath32 can be decomposed into a part where the deterministic theory holds and a small part where it does not , it is easily generalized .",
    "before we state the theorem , we need the following lemmata .",
    "( @xcite , see also @xcite)[lem : egoroff ] let @xmath66 be a complete probability space .",
    "let @xmath132 and @xmath7 be measurable functions from @xmath32 into a metric space @xmath133 with metric @xmath70 .",
    "suppose @xmath134 for @xmath35-almost all @xmath135.then for any @xmath136 there is a set @xmath137 with @xmath138 such that @xmath139 uniformly on @xmath137 , that is @xmath140    [ prop : kyfan ] let @xmath100 be a sequence of random variables that converges to @xmath7 in the ky fan metric .",
    "then for any @xmath141 and @xmath136 there exist @xmath142 , @xmath143 , and a subsequence @xmath144 with @xmath145 furthermore there exists a subsequence that converges to @xmath7 almost surely .",
    "we give a sketch of the proof for the first statement taken from @xcite .",
    "+ set @xmath146 . by definition of the ky fan metric ( [ def : kyfan ] ) , for given @xmath147 , there exists a set @xmath148 with @xmath149 and @xmath150 such that @xmath151 . for arbitrary @xmath136 and @xmath152",
    "we pick a subsequence @xmath153 with @xmath154 and introduce the set @xmath155",
    ". one can check that @xmath156 . since @xmath137 is a subset of every @xmath157 we have @xmath158 which proves he first statement .",
    "the second one follows since convergence in ky fan metric is equivalent to convergence in probability , which itself implies almost - sure convergence of a subsequence , cf @xcite .    with this",
    ", we are ready for the convergence theorem which we shall split in two parts , one for the ky fan metric as error measure and one for the expectation .",
    "[ thm : lifting_convergence_kyfan ] let @xmath126 be a regularization method for the solution of in the deterministic setting under a suitable choice of the regularization parameter .",
    "let now @xmath159 where @xmath160 is a stochastic error such that @xmath161 as @xmath162 . then , assuming has a unique solution @xmath102 and all necessary assumptions for the deterministic theory ( except the bound on the noise ) hold with probability one , the regularization method @xmath126 fulfills @xmath163 under the same parameter choice rule as in the deterministic setting with @xmath164 replaced by @xmath165 .",
    "if the regularized solutions are defined by , then it holds that @xmath166    denote @xmath167 .",
    "define @xmath168 .",
    "( note that @xmath169 due to the properties of the ky fan metric ) .",
    "we show in the following that for arbitrary @xmath136 we have @xmath170 and hence @xmath171 as a first step we pick a  worst case \" subsequence @xmath172 of @xmath173 , a subsequence for which the corresponding solutions satisfy @xmath174 .",
    "we now show that even from this  worst case \" sequence we can pick a subsequence @xmath175 for which we have @xmath176 for arbitrary @xmath136 .",
    "+ let @xmath136 . according to lemma [ prop : kyfan ]",
    "we can pick a subsequence @xmath175 and a set @xmath137 with @xmath177 as well as @xmath178 , @xmath141 arbitrarily small , on @xmath137 . for all @xmath179",
    ", the noise tends to zero .",
    "we can therefore use the deterministic result with @xmath180 and deduce that @xmath181 converges to the unique solution @xmath182 for @xmath183 , @xmath179 where in the choice of the regularization parameter @xmath164 is substituted by @xmath184 .",
    "the convergence is not uniform in @xmath69 ; nevertheless , pointwise convergence implies uniform convergence except on sets of small measure according to lemma [ lem : egoroff ] .",
    "therefore there exist @xmath185 , @xmath186 and @xmath187 such that @xmath188 @xmath189 and @xmath190 .",
    "we thus have @xmath191 since we split @xmath192 with @xmath193 , @xmath194 we have shown existence of a subsequence @xmath195 such that @xmath196 for @xmath195 sufficiently small .",
    "this @xmath197 is by definition of the ky fan metric an upper bound for the distance between @xmath198 and @xmath102 .",
    "therefore we have @xmath199 on the other hand , the original sequence satisfied @xmath200 . since @xmath201 it follows @xmath170 . because @xmath136 was arbitrary , this implies @xmath202 , which concludes the proof of convergence in the ky fan metric .",
    "convergence in expectation follows from theorem [ thm : uniform_int ] noting that by the sequence of regularized solutions is uniformly integrable .",
    "[ thm : lifting_convergence_exp ] let @xmath126 be a regularization method for the solution of in the deterministic setting under a suitable choice of the regularization parameter .",
    "let now @xmath159 where @xmath160 is a stochastic error such that @xmath203 as @xmath162 . then",
    ", assuming has a unique solution @xmath102 and all necessary assumptions for the deterministic theory ( except the bound on the noise ) hold with probability one , the regularization method @xmath126 fulfills @xmath163 under the same parameter choice rule as in the deterministic setting with @xmath164 replaced by @xmath204 where @xmath205 fulfills @xmath206 if the regularized solutions are defined by then it holds that @xmath166    as previously we pick a ",
    "worst case \" subsequence @xmath172 of @xmath173 , a subsequence for which the corresponding solutions satisfy @xmath174 .",
    "+ let @xmath136 .",
    "we can now pick a subsequence which we again denote by @xmath175 fulfilling @xmath207 , where without loss of generality @xmath208 , such that @xmath209 this again defines , via the complement in @xmath32 , @xmath137 with @xmath177 on which @xmath210 .",
    "as before , we can now apply the deterministic theory by substituting @xmath164 with @xmath211 .",
    "the remainder of the proof is identical to the one of theorem [ thm : lifting_convergence_exp ] .",
    "the theorems justify the use of deterministic algorithms under a stochastic noise model .",
    "since the proof is solely based on relating the stochastic noise to a deterministic one on subsets of @xmath32 and does not use any specific properties of the regularization methods or the underlying spaces , it opens most of the deterministic methods for the a stochastic noise model .",
    "in particular , the parameter choice rules from the deterministic setting are easily adapted .",
    "as usual in deterministic literature , the general convergence theorem is followed by convergence rates which are obtained under additional assumptions .",
    "often these conditions ensure at least local uniqueness of the true solution . if not , we have to require such a property for the same reason as previously .",
    "[ thm : lifting_rates_kyfan ] let @xmath126 be a regularization method for the solution of in the deterministic setting such that , under a set of assumptions on the operator @xmath212 and the solutions @xmath102 and a suitable choice of the regularization parameter , @xmath213 with a monotonically increasing right - continuous function @xmath214 .",
    "let now @xmath159 where @xmath160 is a stochastic error such that    * @xmath161 or * @xmath203    as @xmath162 .",
    "then , assuming all necessary assumptions for the deterministic theory ( except the bound on the noise ) hold with probability one and that there is ( either by the deterministic conditions or by additional assumption ) a ( locally ) unique solution @xmath102 to , the regularization method @xmath126 fulfills @xmath215 in case a ) or , respectively , in case b ) , @xmath216 under the same parameter choice rule as in the deterministic setting with @xmath164 replaced by @xmath165 ( case a ) ) or @xmath204 where @xmath205 fulfills ( case b ) ) .",
    "we start again with the ky fan distance as noise measure .",
    "since we have the deterministic theory at hand , we know that @xmath217 whenever @xmath218 . with @xmath219",
    "we have , since @xmath214 is monotonically increasing and right continuous , @xmath220 and hence by definition @xmath221 .",
    "if the expectation is used as measure for the data error , we have @xmath222 by markovs inequality . hence , with probability @xmath223 we are in the deterministic setting with @xmath224 and @xmath225 the convergence rate follows by the definition of the ky fan metric .    for inverse problems ,",
    "the convergence rates are most often given by functions which decay at most linearly fast , i.e. , @xmath226 hence in this case the convergence rates are preserved in the ky fan metric . for the expectation",
    "this is not the case .",
    "we have to gradually inflate the expectation by the parameter @xmath227 in order to obtain convergence ( and rates ) .",
    "let us discuss the simple example of gaussian noise in the finite dimensional setting , i.e. @xmath5 from consists of @xmath228 i.i.d .",
    "random variables @xmath229 with zero mean and variance @xmath230 .",
    "then it has been shown in @xcite that for any @xmath231 @xmath232 with the gamma functions @xmath233 and @xmath234 defined as @xmath235 in particular , is independent of the variance @xmath230 . in order to to decrease the probability to zero",
    ", we therefore have to link @xmath227 with the variance . for gaussian noise of the above kind",
    "the following estimate for the ky fan distance between true and noisy data has been given in @xcite .",
    "[ prop : kyfandistance ] let @xmath236 be a random variable with values in @xmath237 .",
    "assume that the distribution of @xmath5 is @xmath238 with @xmath239 .",
    "then it holds in @xmath240 that @xmath241    recall that @xmath242 see e.g. @xcite . comparing and",
    ", one sees that @xmath243 and in particular the decay of @xmath244 slows down with decreasing @xmath18 . in other words ,",
    "the artificial inflation we had to impose on the expectation is automatically included in the ky fan distance which we suppose is the reason why the convergence theory carries over in such a direct fashion for the ky fan metric .    for many nonlinear inverse problems the requirement of a unique solution is too strong .",
    "often one has several solutions of the same quality , in particular there exists more than one minimum norm solution . in this case ,",
    "theorem [ thm : lifting_convergence_kyfan ] is not applicable . in the example (",
    "* example 4.3 and 4.5 ) with two minimum norm solutions the noise was constructed such that , while the error in the data converges to zero , for each fixed @xmath135 the regularized solutions jump between both solutions such that no converging subsequence can be found .",
    "the main problem there is that the ky fan distance can not incorporate the concept that all minimum norm solutions are equally acceptable .",
    "we will now define a pseudo metric that resolves this issue .",
    "let @xmath245 be a metric space . denote with @xmath246 the set of minimum - norm solutions to",
    ". then @xmath247 measures the distance between an element @xmath2 and the set @xmath246 , in particular it is @xmath248    with this , one can define a pseudometric on @xmath66 via @xmath249 obviously is positive , symmetric and fulfills the triangle inequality .",
    "however , @xmath250 does not imply @xmath251 a.e .",
    "but instead @xmath252 which fixes the aforementioned issue of the ky fan metric and allows the following theorems .    let @xmath126 be a regularization method for the solution of in the deterministic setting under a suitable choice of the regularization parameter .",
    "let now @xmath159 where @xmath160 is a stochastic error such that    * @xmath161 or * @xmath203    as @xmath162 . then , assuming all necessary assumptions for the deterministic theory ( except the bound on the noise ) hold with probability one , the regularization method @xmath126 fulfills @xmath253 under the same parameter choice rule as in the deterministic setting with @xmath164 replaced by @xmath165 ( case a ) ) or @xmath204 where @xmath205 fulfills ( case b ) ) .",
    "in particular , the series of regularized solutions fulfills @xmath254    the proof follows the lines of the one of theorem [ thm : lifting_convergence_kyfan ] with @xmath255 replaced by @xmath256 . also lemma [ lem : egoroff ]",
    "is easily adjusted to incorporate multiple solutions .",
    "so far we assumed that only the noise is stochastic whereas the operator @xmath212 and the unknown @xmath7 were assumed to be deterministic . in @xcite",
    "general stochastic inverse problems @xmath257 were considered .",
    "it was shown how deterministic conditions such as source conditions can be incorporated into the stochastic setting by assuming that the deterministic conditions hold with a certain probability . however , additional conditions may occur when lifting these in order to ensure the deterministic requirements up to a certain probability . since this is easier",
    "seen given an example , we move the discussion of the complete stochastic formulation in the next section . although we will address only one particular example , the technique can be applied to general approaches .",
    "due to the possible multiplicity of stochastic conditions which might appear in this context it seems not possible to develop a lifting strategy in such a general fashion as in the previous section .",
    "we will therefore consider two classical examples , namely nonlinear tikhonov regularization and landweber s method for nonlinear inverse problems .",
    "the theory is taken completely from @xcite .",
    "we seek the solution of a nonlinear ill - posed problem via the variational problem @xmath258 with a reference point @xmath259 and given noisy data @xmath22 according to where the stochastic distribution of the noise is assumed to be known .",
    "we shall skip the general convergence theorem ( which follows as in the previous section ) and move to convergence rates directly . in the deterministic theory ,",
    "i.e. when @xmath260 is the noisy data with @xmath261 , we have the following theorem from @xcite .",
    "[ thm : tikh_nonlin_det ] let @xmath262 be convex , @xmath263 such that @xmath261 and @xmath102 denote the @xmath264-minimum norm solution of .",
    "furthermore let the following conditions hold .",
    "* @xmath212 is frchet - differentiable * there exists @xmath265 such that @xmath266 in a sufficiently large ball @xmath267 * @xmath268 satisfies the source condition @xmath269 for some @xmath270 . *",
    "the source element satisfies @xmath271 .",
    "then for the choice @xmath272 with some fixed @xmath273 we obtain @xmath274    as given in theorem 4.6 of @xcite , the following stochastic formulation of theorem [ thm : tikh_nonlin_det ] holds .",
    "[ thm : tikh_nonlin_stoch ] let @xmath262 be convex , let @xmath22 be such that @xmath275 and @xmath102 denote the @xmath264-minimum norm solution of for almost all @xmath69 . furthermore let the following conditions hold .    * @xmath276 is frechet - differentiable for almost all @xmath69 * @xmath277 satisfies @xmath278 in a sufficiently large ball @xmath279 * ( smoothness ) @xmath280 where @xmath281 * ( closedness ) @xmath282 * ( decay ) @xmath283 .    then for the choice",
    "@xmath284 we obtain @xmath285    we have @xmath286 with probability @xmath287 . now fix @xmath288 and @xmath289 .",
    "then with probability @xmath290 conditions d ) and e ) are fulfilled .",
    "thus for the corresponding values of @xmath69 we can apply theorem [ thm : tikh_nonlin_det ] and obtain @xmath291 or , fixing the parameter @xmath284 , @xmath292 this estimate holds on a set with probability greater or equal @xmath293 .",
    "the ky fan distance can therefore be bounded as @xmath294 this estimate is valid for arbitrary choices of @xmath236 and @xmath227 above , therefore we may bound the ky fan distance of @xmath102 and @xmath295 by taking the infimum with respect to @xmath236 and @xmath227 .",
    "the core principle of the lifting strategy is to ensure that there exists a subset @xmath296 such that all deterministic assumptions hold with probability one on @xmath137 .",
    "this may lead to the introduction of new conditions such as the decay condition in theorem [ thm : tikh_nonlin_stoch ] .",
    "namely , since @xmath297 and @xmath298 may vary with @xmath69 , it may be possible that for a sequence @xmath299 @xmath300 and @xmath301 such that still for all @xmath302 @xmath303 . in this case",
    "the parameter @xmath227 can not be treated as a constant in the convergence rate , but it influences it to a significant degree . the decay condition had to be imposed in order to control the growth of @xmath227 .",
    "it is , however , possible to avoid condition e ) by imposing other conditions .",
    "for example , one could require that @xmath297 is bounded below by some @xmath304 . in this case condition",
    "d ) implies e ) . a more detailed discussion is given in @xcite .",
    "accordingly , in order to lift other deterministic convergence rate results into the fully stochastic setting , a careful examination of the conditions necessary for convergence in the stochastic setting , understanding their cross - connections and dependencies is important .",
    "however , once the conditions have been translated to the stochastic setting , convergence rates follow immediately using the ky fan metric .",
    "we will close this example by showing how particular choices of the stochastic parameters in theorem [ thm : tikh_nonlin_stoch ] influence the convergence rate . to this end , we cite remark 4.8 of @xcite .",
    "let in the first examples the operator be deterministic , i.e. , @xmath305 where @xmath306 .",
    "first consider the case that @xmath307 $ ] , i.e. , it is uniformly distributed on the interval @xmath308 $ ] .",
    "we therefore have @xmath309 , as well as @xmath310 for @xmath231 .",
    "thus theorem [ thm : tikh_nonlin_stoch ] implies @xmath311 which gives for @xmath284 the optimal rate @xmath312 for the second case suppose that @xmath313 for some exponent @xmath314 . since now we do not have @xmath315 , but @xmath316 we obtain @xmath317 since the right hand side does not converge to zero we do not obtain a convergence rate anymore .",
    "however , convergence itself still follows from theorem [ thm : lifting_convergence_kyfan ] .    finally , consider the case when both d ) and e ) from theorem [ thm : tikh_nonlin_stoch ] influence the convergence behavior , because @xmath212 is stochastic with varying @xmath297 .",
    "for instance in the case the for some @xmath318 $ ] we have @xmath319 and @xmath320 , we find that @xmath309 and @xmath321 are compatible realizations of @xmath322 and @xmath323",
    ". with this one can show @xmath324 under the parameter choice @xmath325 .",
    "from the given examples it is evident that the convergence speed is heavily influenced by the conditions d ) and e ) in theorem [ thm : tikh_nonlin_stoch ] .",
    "therefore , although the general formula for the convergence rate may suggest that the convergence rate is close to the deterministic one , it may be significantly slower due to the additional stochastic properties .      as before we seek the solution of a nonlinear ill - posed problem given noisy data @xmath22 according to where the stochastic distribution of the noise is assumed to be known .",
    "landweber s method can be seen as a descent algorithm for @xmath326 and is defined via the iteration @xmath327 where @xmath328 is an appropriately chosen stepsize and @xmath329 an initial guess .",
    "landweber s method constitutes a regularization method if it is stopped early enough @xcite . in the deterministic theory ,",
    "i.e. when @xmath260 is the noisy data with @xmath261 , we have the following theorem from @xcite for convergence rates of the landweber method .",
    "[ thm : lw_nonlin_det ] let @xmath262 be convex , @xmath263 such that @xmath261 and @xmath102 denote the @xmath264-minimum norm solution of",
    ". assume has a solution in @xmath330 .",
    "furthermore let the following conditions hold on @xmath331 .",
    "* @xmath212 is frechet - differentiable with @xmath332 and @xmath333 * @xmath334 where the bounded linear operators @xmath335 satisfy @xmath336 * @xmath268 satisfies the source condition @xmath337 for some @xmath270 and @xmath338 .",
    "let @xmath339 be sufficiently small . then",
    ", if the regularization parameter is stopped according to the discrepancy principle , i.e. , at the unique index @xmath340 for which for the first time @xmath341 with @xmath342 , we obtain @xmath343    we can obtain a stochastic version of theorem [ thm : lw_nonlin_det ] in the same way and with the same techniques used to show that theorem [ thm : tikh_nonlin_stoch ] followed from theorem [ thm : tikh_nonlin_det ] .",
    "[ thm : lw_nonlin_stoch ] let @xmath262 be convex , @xmath344 be given with @xmath165 and let @xmath182 denote the @xmath264-minimum norm solution of .",
    "assume has a solution in @xmath345 for almost all @xmath69 .",
    "furthermore let the following conditions hold on @xmath331 .",
    "* @xmath346 where for almost all @xmath69 the set @xmath347 describes a family of bounded linear operators with @xmath348 * @xmath268 satisfies the source condition @xmath349 for some @xmath350 and @xmath338 . *",
    "@xmath351 * @xmath352    then , if the regularization parameter is stopped according to the discrepancy principle , i.e. , at the unique index @xmath340 for which for the first time @xmath353 with @xmath354 , we obtain for @xmath355 sufficiently small the rate @xmath356 where the constant @xmath357 depends on @xmath358 only .    in the fully stochastic",
    "setting , the source condition b ) from theorem [ thm : lw_nonlin_stoch ] need not hold with constant exponent @xmath358 for all @xmath135 .",
    "there are at least two situations which lead to the power @xmath358 being a stochastic quantity as well , i.e. , it holds @xmath359 with @xmath360 .    in the first case all solutions @xmath182 come from some initial element @xmath361 , with small @xmath46-norm .",
    "some randomly smoothing operator is acting on this element and generates @xmath182 .",
    "( one could for instance think of some kind of evolution process , e.g. , a diffusion process that is applied to some initial value @xmath362 ) .",
    "the smoothness of @xmath182 is therefore random .",
    "secondly , @xmath102 may be a deterministic element satisfying a certain smoothness condition .",
    "the data @xmath363 is generated by applying a forward operator @xmath364 with random smoothness properties .",
    "if the realization of @xmath364 is strongly smoothing , this corresponds to a source condition with small @xmath365 , if @xmath364 is weakly smoothing we have the source condition with larger @xmath365 .",
    "the following proposition shows the convergence rate that results from the source condition for the case that @xmath365 is uniformly distributed on the interval @xmath366 $ ] .",
    "let all conditions of theorem [ thm : lw_nonlin_stoch ] hold except for b ) and d ) .",
    "let @xmath182 satisfy where @xmath298 is uniformly bounded and sufficiently small .",
    "let @xmath365 be uniformly distributed on the interval @xmath366 $ ] , i.e. , @xmath367 then the approximations @xmath368 obtained by landweber s method satisfy the convergence rate @xmath369 where @xmath370 denotes the lambert w - function , defined by @xmath371 , see @xcite .",
    "as can be seen from the proof of theorem 3.1 in @xcite , the requirement `` @xmath339 sufficiently small '' , becomes stronger , the larger @xmath358 is .",
    "supposing that @xmath339 in is sufficiently small for the case @xmath372 , implies therefore that also the convergence conditions for @xmath373 are satisfied .",
    "secondly we observe that the convergence rate in theorem [ thm : lw_nonlin_stoch ] contains a constant @xmath357 that depends on @xmath358 .",
    "although it is difficult to state an explicit formula for @xmath357 , investigation of @xcite shows , that @xmath374 attains its maximum value when @xmath372 .",
    "after these observations we start with the actual derivation of the convergence rate . for the sake of simplicity",
    "we assume that all appearing constants are just equal to 1 .",
    "furthermore we may assume that @xmath322 and @xmath323 both vanish .",
    "asymptotically , for given @xmath69 we therefore have the estimate @xmath375 measuring the distance in the ky fan metric we must , since we assumed that @xmath365 is as in , solve the equation @xmath376 for @xmath358 .",
    "we first consider the simplified equation @xmath377 which is solved by @xmath378 in the following we show that the above approximate solution is sufficiently accurate .",
    "therefore we construct a better estimate via the ansatz @xmath379 .",
    "the original equation then contains the term @xmath380 . neglecting the quadratic part",
    ", we can replace this term with @xmath381 , and obtain an equation that mathematica can solve for @xmath382 .",
    "the solution for the correction term is given as @xmath383 and tends to zero approximately linearly in @xmath384 .",
    "thus this correction becomes small rather quickly , and we can consider the asymptotic bound in as sufficiently accurate due to the asymptotics of the lambert w - function .",
    "we will now apply the theory developed in the previous section to selected deterministic regularization methods .",
    "let @xmath385 be a linear compact operator between hilbert spaces @xmath386 and @xmath46 with singular system @xmath387 , see e.g. @xcite .",
    "then , for @xmath388 , the generalized inverse @xmath389 to @xmath385 is given by @xmath390 since for compact operators the singular values approach zero , their inverse blows up and the generalized inverse yields a meaningless solution to for noisy data .",
    "a popular class of regularization methods is based on the filtering of the generalized inverse . introducing an appropriate filter function @xmath391 depending on the regularization parameter @xmath16 that controls the growth of @xmath392 ,",
    "the regularized solutions are defined by @xmath393 examples for filter based methods are for example the classical tikhonov regularization , truncated singular value decomposition or landwebers method @xcite .",
    "the regularization properties are fully determined by the filter functions . in the deterministic setting ,",
    "the conditions can be found in , e.g. , ( * ? ? ?",
    "* theorem 3.3.3 . ) .",
    "convergence rates can be obtained for a priori and a posteriori parameter choice rules under stricter conditions on the filter functions .",
    "we will only comment on an a priori choice here in order to keep this section short .",
    "an example of the discrepancy principle as a posteriori parameter choice is given in the next section in a different context .",
    "using the smoothness condition @xmath394 the following theorem can be obtained .",
    "* theorem 3.4.3)[thm : louis_filter_optimality ] let @xmath395 and @xmath396 .",
    "assume that it holds @xmath397 and for @xmath398 , @xmath399 where @xmath400 and @xmath401 are constants independent of @xmath164 .",
    "then with the a priori parameter choice @xmath402 the method induced by the filter @xmath403 is order optimal for all @xmath404 , i.e. , @xmath405 for some constant @xmath406 independent of @xmath164 and @xmath407 .",
    "now we use theorem [ thm : lifting_rates_kyfan ] and obtain convergence rates in the ky fan metric .",
    "let @xmath395 and @xmath165 be known .",
    "assume that it holds @xmath397 and for @xmath398 , and hold .",
    "then with the a priori parameter choice @xmath408 the method induced by the filter @xmath403 fulfills @xmath409 for some constant @xmath406 independent of @xmath164 and @xmath407 .",
    "more about filter methods in the stochastic setting including numerical examples can be found in @xcite .",
    "we consider an autoconvolution equation @xmath410(s)=\\int_0^s x(s - t)x(t)\\ , dt,\\qquad 0\\leq s\\leq 1\\ ] ] between hilbert spaces @xmath411 $ ] and @xmath412 $ ] where @xmath413 .",
    "such an equation is of great interest in , for example , stochastics or spectroscopy and has been analyzed in detail in @xcite .",
    "recently , a more complicated autoconvolution problem has emerged from a novel method to characterize ultra - short laser pulses @xcite . here",
    ", we want to show the transition from the deterministic setting to the stochastic setting in a numerical example .",
    "we base our results on the deterministic paper @xcite .    using the haar - wavelet basis , the authors of @xcite reformulate as an equation from @xmath414 to @xmath414 by switching to the space of coefficients in the haar basis . in order to stabilize the inversion , an @xmath415 penalty term",
    "is used such that the task is to minimize the functional @xmath416 the regularization parameter @xmath16 in is chosen according to the discrepancy principle . in @xcite ,",
    "the following formulation is used : for @xmath417 choose @xmath129 such that @xmath418 holds .",
    "the authors show that this leads to a convergence of the regularized solutions against a solution of with minimal @xmath415-norm of its coefficients .",
    "it was also shown that the regularization parameter fulfills @xmath419    by courtesy of stephan anzengruber we were allowed to use the original code for the numerical simulation in @xcite .",
    "we only changed the parts directly connected to the data noise .",
    "namely , we replaced the deterministic error @xmath420 with i.i.d gaussian noise , @xmath421 @xmath422 .",
    "the discretization is due to the truncation of the expansion of the functions in the haar - basis after @xmath423 elements .",
    "the parameter choice was realized with @xmath164 replaced by @xmath424 in accordance with theorem [ thm : lifting_convergence_kyfan ] . instead of the correct expectation @xmath425",
    "see @xcite , we used the upper bound @xmath426 since , as shown in this chapter , the expectation has to be `` blown up '' anyway . in a first experiment we let @xmath427 . in this case , the numerical results suggest that the regularization parameter decreases too fast , i.e. , @xmath428 does not converge to zero as the requirement in states ; see figure [ fig : conv_nonlin ] . for comparison ,",
    "in a second run we chose @xmath429 where @xmath423 is the amount of data points .",
    "this way , @xmath430 .",
    "now @xmath428 converges to zero as it should be the case from [ eq : alpha_props ] , see figure [ fig : conv_nonlin ] .    at this point",
    "we would like to mention that the discrepancy principle using the ky fan distance and the deterministic one are not completely equivalent since a different way of measuring the noise is used .",
    "typically the stochastic noise level will be smaller ( it need to bound 100% of the possible realizations ) and the iteration will be stopped later than in the deterministic setup .    .",
    "left : constant value of @xmath227 in the discrepancy principle with the expectation of the noise leads to the regularization parameter decreasing too fast .",
    "right : increasing @xmath227 appropriately with decreasing variance resolves this issue.,title=\"fig : \" ] . left : constant value of @xmath227 in the discrepancy principle with the expectation of the noise leads to the regularization parameter decreasing too fast .",
    "right : increasing @xmath227 appropriately with decreasing variance resolves this issue.,title=\"fig : \" ]      in @xcite the lifting strategy was used in a slightly different way . in particular , the ky fan metric was used to obtain a novel parameter choice rule .",
    "the convergence rates obtained there , however , can also be viewed in the framework of this work .",
    "the scope of that paper was to transfer the deterministic convergence results from @xcite into the stochastic setting .",
    "the seminal paper @xcite initiated the investigation of sparsity - promoting regularization for inverse problems .",
    "looking for the solution of the linear ill - posed problem @xmath431 between hilbert spaces @xmath386 and @xmath46 with given noisy data @xmath432 , the regularization strategy was to obtain an approximation @xmath125 to @xmath102 via @xmath433 where @xmath434 is an appropriate index set , @xmath435 @xmath436 , @xmath437 a dictionary ( typically an orthonormal basis or frame ) in @xmath386 and @xmath438 .",
    "choosing a sufficiently smooth wavelet basis for @xmath437 and setting @xmath439 with @xmath440 , the penalty term in corresponds to a norm in the besov space @xmath441 . formulating the problem of determining @xmath7 from noisy data @xmath442 , @xmath443 , in the bayesian setting with the distributions @xmath444 and @xmath445 and using the maximum a - posteriori solution lead to the formulation @xmath446 where @xmath18 is the variance of the noise and @xmath447",
    "can roughly be described as the inverse variance of the prior .",
    "the product @xmath448 gives the actual regularization parameter . in direct application of theorem [ thm : lifting_convergence_kyfan ] ,",
    "the deterministic condition @xmath449 with @xmath164 replaced by @xmath165 from translates to the conditions @xmath450 leading to convergence of @xmath451 to the unique ( in case @xmath88 the operator is assumed to be injective ) solution @xmath102 of minimal norm @xmath452 in the ky fan metric .",
    "the proof of convergence rates is based on two assumption : @xmath453 where @xmath454 and @xmath455 for some @xmath456 .",
    "combining proposition 4.5 , proposition 4.6 , proposition 4.7 from @xcite it is @xmath457 where with @xmath458 , @xmath459 and @xmath460 we know that the deterministic rate is an upper bound to the reconstruction error whenever @xmath461 and @xmath462 .",
    "hence , it is @xmath463 where @xmath464 and @xmath465 where the besov - space functions were truncated after the first @xmath466 basis functions . by definition of the ky fan",
    "metric , it follows immediately from that @xmath467 since @xmath447 is a free parameter , we can balance the terms in , i.e. solve the nonlinear equation @xmath468 for @xmath447 . with this parameter choice rule one obtains by construction @xmath469    we can also apply the theory developed in this work to this problem . in the deterministic setting ,",
    "see @xcite , it was proposed to chose the regularization parameter @xmath470",
    ". combining ( * ? ? ?",
    "* proposition 4.5 ) and ( * ? ? ?",
    "* proposition 4.7 ) then yields the rate @xmath471 with @xmath472 from and some @xmath95 . theorem [ thm : lifting_rates_kyfan ] then yields in the stochastic setting the parameter choice @xmath473 and @xmath474 in the notation of it is for gaussian noise @xmath475 @xmath476 see proposition .",
    "since @xmath477 , compare and , the convergence rate in is slightly slower than the one in , but they share the same order of convergence .",
    "our goal was to demonstrate how convergence results for inverse problems in the deterministic setting can be carried over into the stochastic setting . using the ky fan metric , we have shown that , when only the noise is assumed to be stochastic whereas the other quantities are deterministic , this is is possible in a straight - forward way .",
    "namely , assuming the knowledge of an estimate of @xmath165 , the convergence results and parameter choice follows from the deterministic setting by replacing @xmath164 , which originates from the basic deterministic assumption @xmath478 , with @xmath479 . we have shown that , under some slight modifications , it is possible to use the expectation as measure for the magnitude of the noise . in a fully stochastic situation , where additionally to the noise other objects might be of stochastic nature , the lifting of deterministic convergence results is possible as well .",
    "however , careful analysis is necessary in order to carry the deterministic conditions over into the stochastic setting ."
  ],
  "abstract_text": [
    "<S> both for the theoretical and practical treatment of inverse problems , the modeling of the noise is a crucial part . </S>",
    "<S> one either models the measurement via a deterministic worst - case error assumption or assumes a certain stochastic behavior of the noise . although some connections between both models are known , the communities develop rather independently . in this paper </S>",
    "<S> we seek to bridge the gap between the deterministic and the stochastic approach and show convergence and convergence rates for inverse problems with stochastic noise by lifting the theory established in the deterministic setting into the stochastic one . </S>",
    "<S> this opens the wide field of deterministic regularization methods for stochastic problems without having to do an individual stochastic analysis for each problem .    in inverse problems , </S>",
    "<S> the model of the inevitable data noise is of utmost importance . in most cases , an additive noise model @xmath0 </S>",
    "<S> is assumed . in </S>",
    "<S> , @xmath1 is the true data of the unknown @xmath2 under the action of the ( in general ) nonlinear operator @xmath3 , @xmath4 and @xmath5 in corresponds to the noise . </S>",
    "<S> the spaces @xmath6 are assumed to be banach- or hilbert spaces . when speaking of inverse problems , we assume that is ill - posed . </S>",
    "<S> in particular this means that solving for @xmath7 with noisy data is unstable in the sense that `` small '' errors in the data may lead to arbitrarily large errors in the solution . </S>",
    "<S> hence , is not a sufficient description of the noise . </S>",
    "<S> more information is needed in order to compute solutions from the data in a stable way . in the _ deterministic _ setting , one assumes @xmath8 for some @xmath9 where @xmath10 is an appropriate distance functional . </S>",
    "<S> typically , @xmath11 is induced by a norm such that reads @xmath12 . here and further on we use the superscript @xmath13 to indicate the deterministic setting . </S>",
    "<S> solutions of under the assumption , are often computed via a tikhonov - type variational approach @xmath14 where again @xmath11 is a distance function and @xmath15 is the penalty term used to stabilize the problem and to incorporate a - priori knowledge into the solution . the regularization parameter @xmath16 is used to balance between data misfit and the penalty and has to be chosen appropriately . </S>",
    "<S> the literature in the deterministic setting is rich , at this point we only refer to the monographs @xcite for an overview .    </S>",
    "<S> the deterministic worst - case error stands in contrast to _ </S>",
    "<S> stochastic _ noise models where a certain distribution of the noise @xmath5 in is assumed . </S>",
    "<S> we shall indicate the stochastic setting by the superscript @xmath17 . in this paper </S>",
    "<S> , @xmath18 will be the parameter controlling the variance of the noise . </S>",
    "<S> depending on the actual distribution of @xmath5 , @xmath19 may be arbitrarily large , but with low probability . </S>",
    "<S> a very popular approach to find a solution of is the bayesian method . for more detailed information </S>",
    "<S> , we refer to @xcite . in the bayesian </S>",
    "<S> setting , the solution of the inverse problem is given as a distribution of the random variable of interest , the _ posterior distribution _ </S>",
    "<S> @xmath20 , determined by bayes formula @xmath21 that is , roughly spoken , all values @xmath7 are assigned a probability of being a solution to given the noisy data @xmath22 . in , the _ likelihood function _ </S>",
    "<S> @xmath23 represents the model for the measurement noise whereas the _ prior distribution _ </S>",
    "<S> @xmath24 represents a - priori information about the unknown . </S>",
    "<S> the data distribution @xmath25 as well as the normalization constants are usually neglected since they only influence the normalization of the posterior distribution . in practice one is often more interested in finding a single representation as solution instead of the distribution itself . </S>",
    "<S> popular point estimates are the _ </S>",
    "<S> conditional expectation _ ( conditional mean , cm ) @xmath26 and the _ maximum a - posteriori ( map ) _ </S>",
    "<S> solution @xmath27 i.e. , the most likely value for @xmath7 under the prior distribution given the data @xmath22 . </S>",
    "<S> both point estimators are widely used . </S>",
    "<S> the computation of the cm - solution is often slow since it requires repeated sampling of stochastic quantities and the evaluation of high - dimensional integrals . </S>",
    "<S> the map - solution , however , essentially leads to a tikhonov - type problem . </S>",
    "<S> namely , assuming @xmath28 and @xmath29 , one has @xmath30 analogously to .    also non - bayesian approaches for inverse problems often seek to minimize a functional , see e.g. @xcite or use techniques known from deterministic theory such as filter methods @xcite . </S>",
    "<S> finally , inverse problems appear in the context of statistics . </S>",
    "<S> hence , the statistics community has developed methods to solve , partly again based on the minimization of . </S>",
    "<S> we refer to @xcite for an overview .    in summary , </S>",
    "<S> tikhonov - type functionals and other deterministic methods frequently appear also in the stochastic setting . from a practical point of view </S>",
    "<S> , one would expect to be able to use deterministic regularization methods for even when the noise is stochastic . </S>",
    "<S> indeed , the main question for the actual computation of the solution , given a particular sample of noisy data @xmath22 , is the choice of the regularization parameter . </S>",
    "<S> a second question , mostly coming from the deterministic point of view , is the one of convergence of the solutions when the noise approaches zero . in the stochastic setting these questions </S>",
    "<S> are answered often by a full stochastic analysis of the problem . in this paper </S>",
    "<S> we present a framework that allows to find appropriate regularization parameters , prove convergence of regularization methods and find convergence rates for inverse problems with a stochastic noise model by directly using existing results from the deterministic theory .    </S>",
    "<S> the paper takes several ideas from the dissertation @xcite , which is only publicly available as book @xcite and not published elsewhere . </S>",
    "<S> it is organized as follows . in section [ ssec : noisemodel ] </S>",
    "<S> we discuss an issue occurring in the transition from deterministic to stochastic noise for infinite dimensional problems . </S>",
    "<S> the ky fan metric , which will be the main ingredient of our analysis , and its relation to the expectation will be introduced in section [ ssec : kyfan ] . </S>",
    "<S> we present our framework to lift convergence results from the deterministic setting into the stochastic setting in section [ ssec : conv_stoch ] . </S>",
    "<S> examples for the lifting strategy are given in section [ sec : examples ] . </S>"
  ]
}