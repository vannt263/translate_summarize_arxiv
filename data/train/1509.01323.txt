{
  "article_text": [
    "one of the main aims in data modeling is good generalization , i.e. the model s capability to approximate accurately the system output for unseen data .",
    "sparse models can be constructed using the @xmath0-penalized cost function , e.g. the basis pursuit or least absolute shrinkage and selection operator ( lasso ) @xcite .",
    "based on a fixed single @xmath0-penalized regularization parameter , the lasso can be configured as a standard quadratic programming optimization problem . by exploiting piecewise linearity of the problem , the least angle regression ( lar )",
    "procedure @xcite is developed for solving the problem efficiently .",
    "note that the computational efficiency in lasso is facilitated by a _",
    "regularization parameter setting . for more complicated constraints , e.g. multiple regularizers , the cross validation by actually splitting data sets as the means of evaluating model generalization comes with considerably large overall computational overheads .",
    "alternatively the forward orthogonal least squares ( ols ) algorithm efficiently constructs parsimonious models @xcite .",
    "fundamental to the evaluation of model generalization capability is the concept of cross - validation @xcite , and one commonly used version of cross - validation is the leave - one - out ( loo ) cross validation . for the linear - in - the - parameters models ,",
    "the loo mean square error ( loomse ) can be calculated without actually splitting the training data set and estimating the associated models , by making use of the sherman - morrison - woodbury theorem . using the loomse as the model term selective criterion to seek the model generalization , an efficient orthogonal forward regression ( ofr ) procedure have been introduced @xcite .",
    "furthermore , the @xmath1-norm based regularization techniques @xcite have been incorporated into the ols algorithm to produce a regularized ols ( rols ) algorithm that carries out model term selection while reduces the variance of parameter estimate simultaneously @xcite .",
    "the optimization of @xmath0-norm regularizer with respect to model generalization analytically is however less studied .    in this contribution",
    ", we propose a @xmath0-norm penalized ofr ( @xmath0-pofr ) algorithm to carry out the regularizer optimization as well as model term selection and parameter estimation simultaneously in a forward regression manner .",
    "the algorithm is based on a new @xmath0-norm penalized cost function with multiple @xmath0 regularizers , each of which is associated with an orthogonal basis vector by orthogonal decomposition of the regression matrix of the selected model terms .",
    "we derive a closed form of the optimal regularization parameter in terms of minimal loomse . to save computational costs an inactive set",
    "is used along the ofr process by predicting whether any model terms will be unselectable in future regression steps .",
    "consider the general nonlinear system represented by the nonlinear model @xcite : @xmath2 where @xmath3^{\\rm t}\\in \\mathbb{r}^m$ ] denotes the @xmath4-dimensional input vector at sample time index @xmath5 and @xmath6 is the system output variable , respectively , while @xmath7 denotes the system white noise and @xmath8 is the unknown system mapping .",
    "the unknown system ( [ eq:1 ] ) is to be identified based on an observation data set @xmath9 using some suitable functional which can approximate @xmath8 with arbitrary accuracy . without loss of generality , we use @xmath10 to construct a radial basis function ( rbf ) network model of the form @xmath11 where @xmath12 is the model prediction output for @xmath13 based on the @xmath14-term rbf model , and @xmath14 is the total number of regressors or model terms , while @xmath15 are the model weights .",
    "the regressor @xmath16 is given by @xmath17 in which @xmath18^{\\rm t}$ ] is known as the center vector of the @xmath19th rbf unit and @xmath20 is an rbf width parameter .",
    "we assume that each rbf unit is placed on a training data , namely , all the rbf center vectors @xmath21 are selected from the training data @xmath22 , and the rbf width @xmath20 has been predetermined , for example , using cross validation .",
    "let us denote @xmath23 as the @xmath14-term modeling error for the input data @xmath13",
    ". over the training data set @xmath10 , further denote @xmath24^{\\rm t}$ ] , @xmath25^{\\rm t}$ ] , and @xmath26 $ ] with @xmath27^{\\rm t}$ ] , @xmath28 .",
    "we have the @xmath14-term model in the matrix form of @xmath29 where @xmath30^{\\rm t}$ ] .",
    "let an orthogonal decomposition of the regression matrix @xmath31 be @xmath32 where @xmath33\\ ] ] and @xmath34\\ ] ] with columns satisfying @xmath35 , if @xmath36 . the regression model ( [ eq:4 ] ) can alternatively be expressed as @xmath37 where the ` orthogonal ' model s weight vector @xmath38^{\\rm t}$ ] satisfies the triangular system @xmath39 , which can be used to determine the original model parameter vector @xmath40 , given @xmath41 and @xmath42 .    further consider the following weighted @xmath0-norm penalized ols criterion for the model ( [ eq:8 ] ) @xmath43 where @xmath44 , which contains the local regularization parameters @xmath45 , for @xmath46 , and @xmath47 is a predetermined lower bound for the regularization parameters . for a given @xmath48 , the solution for @xmath42 can be obtained by setting the subderivative vector of @xmath49 to zero , i.e. @xmath50 , yielding @xmath51 for @xmath46 , with the usual least squares solution given by @xmath52 , and the operator @xmath53 @xmath54    unlike the lasso @xcite , our objective @xmath55 is constructed on the orthogonal space and the @xmath0-norm parameter constraints are associated with the orthogonal bases @xmath56 , @xmath46 .",
    "since the cost function ( [ eq:9 ] ) contains sparsity inducing @xmath0 norm , some parameters @xmath57 will be returned as zeros , producing a sparse model in the orthogonal space spanned by the columns of @xmath58 , which corresponds to a sparse model in the original space spanned by the columns of @xmath31 .",
    "each ofr stage involves the joint regularization parameter optimization , model term selection and parameter estimation .",
    "the regularization parameters with respect to their associated candidate regressors are optimized using the approximate loomse formula that is derived in section  [ s3:2 ] , and the regressor with the smallest loomse is selected .",
    "consider the ofr modeling process that has produced the @xmath59-term model .",
    "let us denote the constructed @xmath59 columns of regressors as @xmath60 $ ] , with @xmath61^{\\rm t}$ ] .",
    "the model output vector of this @xmath59-term model is given by @xmath62 and the corresponding modeling error vector by @xmath63 .",
    "clearly , the @xmath59th ofr stage can be represented by @xmath64 the model form ( [ eq:13 ] ) illustrates the fact that the @xmath59th ofr stage is simply to fit a one - variable model using the current model residual produced after the @xmath65th stage as the desired system output . since @xmath66 , it is easy to verify that @xmath67 .",
    "the selection of one regressor from the candidate regressors involves initially generating candidate @xmath68 by making each candidate regressor to be orthogonal to the @xmath65 orthogonal basis vectors , @xmath56 for @xmath69 obtained in the previous @xmath65 ofr stages , followed by evaluating their contributions .",
    "consider the case of @xmath70 . applying ( [ eq:10 ] ) to ( [ eq:13 ] )",
    ", we note that clearly as @xmath71 decreases away from @xmath72 towards @xmath73 , @xmath74 increases its magnitude at a linear rate to @xmath71 , from zero to an upper bound @xmath75 with @xmath76 for any candidate regressor , it is vital that we evaluate its potential model generalization performance using the most suitable value of @xmath71 .",
    "the optimization of the loomse with respect to @xmath71 is detailed in section  [ s3:2 ] , based on the idea of the loo cross validation outlined below .",
    "suppose that we sequentially set aside each data point in the estimation set @xmath10 in turn and estimate a model using the remaining @xmath77 data points .",
    "the prediction error is calculated on the data point that has not been used in estimation .",
    "that is , for @xmath78 , the models are estimated based on @xmath79 , respectively , and the outputs are denoted as @xmath80 . then , the loo prediction error based on the @xmath5th data sample is calculated as @xmath81 the loomse is defined as the average of all these prediction errors , given by @xmath82 $ ] .",
    "thus the optimal regularization parameter for the @xmath59th stage is given by @xmath83 evaluation of @xmath84 by directly splitting the data set requires extensive computational efforts .",
    "instead , we show in section  [ s3:2 ] that @xmath84 can be approximately calculated without actually sequentially splitting the estimation data set .",
    "furthermore , we also show that the optimal value @xmath85 can be obtained in a closed - form expression .",
    "we notice from ( [ eq:10 ] ) that @xmath86 if @xmath87 , and thus a sufficient condition that a given @xmath68 may be excluded from the candidate pool without explicitly determining @xmath71 is @xmath88 , which is the regularizer s lower bound , a preset value indicating the correlation of the candidate regressor .",
    "hence , in the following we assume that @xmath89 , and we have @xmath90 where @xmath91^{\\rm t}$ ] , @xmath92 @xmath93^{\\rm t}$ ] , and @xmath94 .",
    "note that ( [ eq:17 ] ) is consistent to ( [ eq:10 ] ) for all terms with nonzero @xmath95 . in the ofr procedure , any candidate terms @xmath56 producing zero @xmath57 will not be selected since they will not contribute to any reduction in the loomse .",
    "the model residual is defined by @xmath96 where @xmath97 denotes the transpose of the @xmath5th row of @xmath98 . if the data sample indexed at @xmath5 is removed from the estimation data set , the loo parameter estimator obtained by using only the @xmath77 remaining data points is given by @xmath99 in which @xmath100 , @xmath101 and @xmath102 denote the resultant regression matrix and desired output vector , respectively .",
    "it follows that we have @xmath103 @xmath104 the loo error evaluated at @xmath5 is given by @xmath105    applying the matrix inversion lemma to ( [ eq:21 ] ) yields @xmath106 and @xmath107 substituting ( [ eq:22 ] ) and ( [ eq:24 ] ) into ( [ eq:20 ] ) yields @xmath108 assuming that @xmath109 holds for most data samples and then applying ( [ eq:18 ] ) to ( [ eq:25 ] ) , we have @xmath110 where @xmath111 , and @xmath112 is the @xmath5th element of @xmath56 . the loomse can then be calculated as @xmath113 we point out that in order for @xmath114 and @xmath115 to be different , each element in @xmath116 needs to be very close to zero , which is unlikely since only the model terms satisfying @xmath117 are considered .",
    "hence we can treat @xmath84 given in ( [ eq:27 ] ) as the exact loomse for any preset @xmath73 that is not too small .",
    "we further represent ( [ eq:18 ] ) as @xmath118 where @xmath119 is the model residual obtained based on the least square estimate at the @xmath59th step stage . by setting @xmath120",
    ", we obtain @xmath71 in the form of the weighted least square estimate @xmath121 where @xmath122 and @xmath123^{\\rm t}\\in \\mathbb{r}^n$ ] .",
    "finally we calculate @xmath124 in order to satisfy the constraint that @xmath125 . for @xmath85",
    "obtained using ( [ eq:30 ] ) , we consider the following two cases :    1 .   if @xmath126 , then @xmath86 , and this candidate regressor will not be selected .",
    "2 .   if @xmath127 , then calculate @xmath128 based on @xmath129 as the loomse for this candidate regressor .",
    "from section  [ s3:2 ] we noted that a candidate regressor satisfying @xmath130 does not need to be considered at the @xmath59th stage of selection .",
    "to save computational cost , we define the inactive set @xmath131 as the index set of the unselectable regressors removed from the pool of candidates .    in the @xmath59th ofr stage ,",
    "all the candidate regressors in the candidate pool are made orthogonal to the previously selected @xmath65 regressors , and the candidate with the smallest loomse value is selected as the @xmath59th model term @xmath68 .",
    "denote any other candidate regressor as @xmath132 .",
    "_ main results _",
    ": if @xmath133 , then this candidate regressor will never be selected in further regression stages , and hence it can be moved to @xmath131 .    _",
    "proof _ : at the @xmath134th ofr stage , consider making the regressor @xmath132 orthogonal to @xmath68 , and define @xmath135 clearly , @xmath136 the model residual vector after the selection of @xmath68 is @xmath137 where @xmath74 can be written as @xmath138 thus we have @xmath139 @xmath140 and @xmath141 substituting ( [ eq:36 ] ) and ( [ eq:37 ] ) into ( [ eq:35 ] ) yields @xmath142 due to the fact that @xmath143 . from ( [ eq:32 ] ) and ( [ eq:38 ] ) , it can be concluded that @xmath144 since @xmath145 is the upper bound of @xmath146 , this means that this regressor will not be selected at the @xmath134th stage . by induction , it will never be selected in further regression stages , and hence it can be moved to @xmath131 .",
    "the proposed @xmath0-pofr algorithm integrates ( i )  the model regressor selection based on minimizing the loomse ; ( ii )  regularization parameter optimization also based on minimizing the loomse ; and ( iii )  the mechanism of removing unproductive candidate regressors during the ofr procedure .",
    "define @xmath147\\in \\mathbb{r}^{n \\times m } , \\ ] ] with @xmath148 .",
    "if some of the columns in @xmath149 have been interchanged , this will still be referred as @xmath150 for notational simplicity .",
    "\\cap \\{j \\notin { \\mathcal s}\\}$ ] , denote the @xmath5th element of @xmath151 as @xmath152 and compute @xmath153 , and @xmath154 .    step  1 ) :  if @xmath155 , @xmath156 ; else if @xmath157 , set @xmath158 as a very large positive number so that it will not be selected in step  4 ) .",
    "otherwise goto step  2 ) .    step  2 ) :  calculate @xmath159    step  3 ) :  if @xmath160 , set @xmath158 as a very large positive number so that it will not be selected in step  4 ) ; otherwise calculate @xmath161 @xmath162    step  4 ) :  find @xmath163 then update @xmath164 and @xmath74 as @xmath165 and @xmath166 , respectively .",
    "the @xmath167th and the @xmath59th columns of @xmath150 are interchanged , while the @xmath167th column and the @xmath59th column of @xmath41 are interchanged up to the @xmath65th row .",
    "this effectively selects the @xmath59th regressor in the subset model . the modified gram - schmidt orthogonalisation procedure @xcite then calculates the @xmath59th row of the matrix @xmath41 and transfers @xmath150 into @xmath168 as follows @xmath169 then update @xmath170 for @xmath171 .",
    "+         ( a )    the initial conditions are as follows .",
    "preset @xmath47 as a very small value . set @xmath172 , @xmath173 for @xmath171 , and @xmath131 as the empty set @xmath174 .",
    "the @xmath59th stage of the selection procedure is listed in table  [ tab:1 ] .",
    "the ofr procedure is automatically terminated at the @xmath175th stage when the condition @xmath176 is detected , yielding a subset model with @xmath177 significant regressors .",
    "it is worth emphasizing that there always exists a model size @xmath177 , and for @xmath178 , the loomse @xmath179 decreases as @xmath59 increases , while the condition ( [ eq:51 ] ) holds @xcite .    note",
    "that the loomse is used not only for deriving the closed form of the optimal regularization parameter estimate @xmath85 but also for selecting the most significant model regressor .",
    "specifically , a regressor is selected as the one that produces the smallest loomse value as well as offering the reduction in the loomse .",
    "after the @xmath177 stage when there is no reduction in the loomse criterion for a few consecutive ofr stages , the model construction procedure can be terminated .",
    "thus , the @xmath0-pofr algorithm automatically constructs a sparse @xmath177-term model , where typically @xmath180 .    also note that it is assumed that @xmath73 should not be too small such that the loomse estimation formula can be considered to be accurate",
    "this means that if @xmath73 is set too low , many insignificant candidate regressors will have inaccurate loomse values for competition .",
    "however , we emphasize that these terms with inaccurate loomse values will not be selected as the winner to enter the model . hence in practice",
    "we only need to make sure that @xmath73 is not too large , which would introduce unnecessary bias to the model parameter estimates .",
    "clearly , a relatively larger @xmath73 will save computational costs by 1 )  resulting in a sparser model , and 2 )  producing a larger sized inactive set during the ofr process .",
    "finally , regarding the computational complexity of the @xmath0-pofr algorithm , if the unproductive regressors are not removed to the inactive set @xmath131 during the ofr procedure , it is well known that the computational cost is in the order of @xmath181 for evaluating each candidate regressor @xcite .",
    "the total computational cost then needs to be scaled by the number of evaluations in forward regression , which is @xmath182 . by removing unproductive regressors to @xmath131 during the ofr procedure",
    ", the computational cost can obviously be reduced significantly .",
    "it is not possible to exactly assess the computational cost saving due to removing the unproductive regressors , as this is problem dependent .",
    ".comparison of the modeling performance for engine data .",
    "the computational cost saving is based on the same size of model without removing unproductive regressors in the @xmath0-pofr . [ cols=\"^,^,^,^,^ \" , ]     _ example 2 _ : this regression benchmark data set , boston housing data , is available at the uci repository @xcite .",
    "the data set comprises 506 data points with 14 variables .",
    "the previous study @xcite performed the task of predicting the median house value from the remaining 13 attributes using the @xmath73-svm @xcite , the lrols - loo @xcite and the nonlinear ofr based on the loomse ( nonofr - loo ) @xcite .",
    "the nonofr - loo algorithm @xcite constructs a _ nonlinear _ rbf model in the ofr procedure , where each stage of the ofr determines one rbf node s center vector and diagonal covariance matrix by minimizing the loomse . in the experiment study presented in @xcite ,",
    "456 data points were randomly selected from the data set for training and the remaining 50 data points were used to form the test set .",
    "average results were given over 100 realizations .",
    "for each realization , 13 input attributes were normalized so that each attribute had zero mean and standard deviation of one .",
    "we also experimented with the lasso supplied by matlab _ lasso.m _ with option set as 10-fold cv to select the associated regularization parameter .",
    "for the lasso , a common kernel width @xmath20 was set for constructing the kernel model from the 456 candidate regressors of each realization , and a range of @xmath20 values were experimented .    for the @xmath0-pofr ,",
    "@xmath183 was empirically set for constructing 456 candidate gaussian rbf regressors of each realization .",
    "we experimented a range of the preset @xmath73 values for the @xmath0-pofr algorithm , and the results obtained are as summarized in table  [ tab:3 ] , in comparison with the results obtained by the @xmath73-svm and the lasso , as well as the lrols - loo and nonofr - loo , which are quoted from the study @xcite .",
    "we have developed an efficient data model algorithm , referred to as the @xmath0-norm penalized orthogonal forward regression ( @xmath0-pofr ) , for linear - in - the - parameters nonlinear models based on a new @xmath0-norm penalized cost function defined in the constructed orthogonal modeling space .",
    "the loomse is used for simultaneous model term selection and regularization parameter estimation in a highly efficient ofr procedure .",
    "additionally , we have proposed a lower bound of the regularisation parameters for robust loomse estimation as well as detecting and removing insignificant regressors to an inactive set along the ofr process , further enhancing the efficiency of the ofr procedure .",
    "numerical studies have been utilized to demonstrate the effectiveness of this new @xmath0-pofr approach .",
    "x.  hong , p.  m. sharkey , and k.  warwick , `` automatic nonlinear predictive model - construction using forward regression and the press statistic , '' _ iee proc .",
    "control theory applications _ , vol .  150 , no .  3 , pp .",
    "245254 , 2003 .",
    "s.  chen , x.  hong , and c.  j. harris , `` sparse kernel regression modelling using combined locally regularised orthogonal least squares and d - optimality experimental design , '' _ ieee trans .",
    "automatic control _",
    "48 , no .  6 , pp .",
    "10291036 , june 2003 .",
    "s.  chen , x.  hong , c.  j.  harris , and p.  m.  sharkey , `` sparse modelling using orthogonal forward regression with press statistic and regularization , '' _ ieee trans .",
    "systems , man and cybernetics , part b : cybernetics _ , vol .",
    "34 , no .  2 , pp",
    ".  898911 , apr .",
    "2004 .    s.  a.  billings , s.  chen , and r.  j.  backhouse , `` the identification of linear and non - linear models of a turbocharged automotive diesel engine , '' _ mechanical systems and signal processing _",
    ", vol .  3 , no .  2 , pp .",
    "123142 , 1989 .",
    "s.  chen , x.  hong , and c.  j. harris , `` construction of tunable radial basis function networks using orthogonal forward selection , '' _ ieee trans .",
    "trans . on systems ,",
    "man and cybernetics , part b : cybernetics _ , vol .",
    "39 , no .  2 , pp",
    ".  457466 , apr ."
  ],
  "abstract_text": [
    "<S> a @xmath0-norm penalized orthogonal forward regression ( @xmath0-pofr ) algorithm is proposed based on the concept of leave - one - out mean square error ( loomse ) . </S>",
    "<S> firstly , a new @xmath0-norm penalized cost function is defined in the constructed orthogonal space , and each orthogonal basis is associated with an individually tunable regularization parameter . </S>",
    "<S> secondly , due to orthogonal computation , the loomse can be analytically computed without actually splitting the data set , and moreover a closed form of the optimal regularization parameter in terms of minimal loomse is derived . </S>",
    "<S> thirdly , a lower bound for regularization parameters is proposed , which can be used for robust loomse estimation by adaptively detecting and removing regressors to an inactive set so that the computational cost of the algorithm is significantly reduced . </S>",
    "<S> illustrative examples are included to demonstrate the effectiveness of this new @xmath0-pofr approach .    </S>",
    "<S> cross validation , forward regression , leave - one - out errors , regularization </S>"
  ]
}