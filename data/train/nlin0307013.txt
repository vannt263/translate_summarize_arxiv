{
  "article_text": [
    "the possibility to predict future states of a system stands at the foundations of scientific knowledge with an obvious relevance both from a conceptual and applicative point of view .",
    "the perfect knowledge of the evolution law of a system may induce the conclusion that this aim could be attained .",
    "this classical deterministic point of view was claimed by laplace @xcite : once the evolution laws of the system are known , the state at a certain time @xmath0 completely determines the subsequent states for every time @xmath1 .",
    "however it is well established now that in some systems , full predictability can not be accomplished in practice because of the unavoidable uncertainty in the initial conditions .",
    "indeed , as already stated by poincar , long - time predictions are reliable only when the evolution law does not amplify the initial uncertainty too rapidly .",
    "therefore , from the point of view of predictability , we need to know how an error on the initial state of the system grows in time . in systems with great sensitive dependence on initial conditions ( deterministic chaotic systems ) errors",
    "grows exponentially fast in time , limiting the ability to predict the future states .",
    "a branch of the theory of dynamical systems has been developed with the aim of formalizing and characterizing the sensitivity to initial conditions .",
    "the lyapunov exponent and the kolmogorov - sinai entropy are the two main indicators for measuring the rate of error growth and information production during a deterministic system evolution .",
    "a complementary approach has been developed in the context of information theory , data compression and algorithmic complexity theory and it is rather clear that the latter point of view is closely related to the dynamical systems one .",
    "if a system is chaotic , then its predictability is limited up to a time which is related to the first lyapunov exponent , and the time sequence by which we encode one of its chaotic trajectories can not be compressed by an arbitrary factor , i.e. is algorithmically complex . on the contrary",
    ", the coding of a regular trajectory can be easily compressed ( e.g. , for a periodic trajectory it is sufficient to have the sequence for a period ) so it is `` simple '' .    in this paper",
    "we will discuss how unpredictability and algorithmic complexity are closely related and how information and chaos theory complete each other in giving a general understanding of complexity in dynamical processes .",
    "in particular , we shall consider the extension of this approach , nowadays well established in the context of low dimensional systems and for asymptotic regimes , to high dimensional systems with attention to situations far from asymptotic ( i.e. finite time and finite observational resolution ) @xcite .",
    "the characteristic lyapunov exponents are somehow an extension of the linear stability analysis to the case of aperiodic motions . roughly speaking , they measure the typical rate of exponential divergence of nearby trajectories and , thus , contain information on the growing rate of a very small error on the initial state of a system .    consider a dynamical system with an evolution law given , e.g. , by the differential equation @xmath2 we assume that @xmath3 is smooth enough that the evolution is well - defined for time intervals of arbitrary extension , and that the motion occurs in a bounded region of the phase space .",
    "we intend to study the separation between two trajectories , @xmath4 and @xmath5 , starting from two close initial conditions , @xmath6 and @xmath7 , respectively .    as long as the difference between the trajectories , @xmath8 , remains small ( infinitesimal , strictly speaking )",
    ", it can be regarded as a vector , @xmath9 , in the tangent space .",
    "the time evolution of @xmath9 is given by the linearized differential equations : @xmath10 under rather general hypothesis , oseledec @xcite proved that for almost all initial conditions @xmath6 , there exists an orthonormal basis @xmath11 in the tangent space such that , for large times , @xmath12 where the coefficients @xmath13 depend on @xmath14 .",
    "the exponents @xmath15 are called _ characteristic lyapunov exponents _ ( les ) .",
    "if the dynamical system has an ergodic invariant measure , the spectrum of les @xmath16 does not depend on the initial condition , except for a set of measure zero with respect to the natural invariant measure .",
    "equation  ( [ eq:1 - 5 ] ) describes how a @xmath17-dimensional spherical region of the phase space , with radius @xmath18 centered in @xmath19 , deforms , with time , into an ellipsoid of semi - axes @xmath20 , directed along the @xmath21 vectors .",
    "furthermore , for a generic small perturbation @xmath22 , the distance between the reference and the perturbed trajectory behaves as @xmath23.\\ ] ] if @xmath24 we have a rapid ( exponential ) amplification of an error on the initial condition . in such a case ,",
    "the system is chaotic and , _ de facto _ , unpredictable on the long times . indeed ,",
    "if the initial error amounts to @xmath25 , and we purpose to predict the states of the system with a certain tolerance @xmath26 ( not too large ) , then the prediction is reliable just up to a _ predictability time _ given by @xmath27 this equation shows that @xmath28 is basically determined by the largest lyapunov exponent , since its dependence on @xmath29 and @xmath26 is logarithmically weak .",
    "because of its preeminent role , @xmath30 is often referred as `` the lyapunov exponent '' , and denoted by @xmath31 .      in experimental investigations of physical processes ,",
    "the access to a system occurs only through a measuring device which produces a time record of a certain observable , i.e. a sequence of data . in this",
    "regard a system , whether or not chaotic , generates messages and may be regarded as a source of information whose properties can be analysed through the tools of information theory .",
    "the characterization of the information contained in a sequence can be approached in two very different frameworks .",
    "the first considers a specific message ( sequence ) as belonging to the ensemble of all the messages that can be emitted by a source , and defines an average information content by means of the average compressibility properties of the ensemble @xcite .",
    "the second considers the problem of characterizing the universal compressibility ( i.e. ensemble independent ) of a specific sequence and concerns the theory of algorithmic complexity and algorithmic information theory @xcite . for the sake of self - consistency we briefly recall the concepts and ideas about the shannon entropy@xcite , that is the basis of whole information theory",
    "consider a source that can output @xmath32 different symbols ; denote with @xmath33 the symbol emitted by the source at time @xmath34 and with @xmath35 the probability that a given word @xmath36 , of length @xmath37 , is emitted @xmath38 .",
    "we assume that the source is stationary , so that , for the sequences @xmath39 , the time translation invariance holds : @xmath40 .",
    "we introduce the @xmath37-block entropies @xmath41 for stationary sources the limit @xmath42 exists and defines the shannon entropy @xmath43 which quantifies the richness ( or `` complexity '' ) of the source emitting the sequence .",
    "this can be precisely expressed by the first theorem of shannon - mcmillan @xcite that applies to stationary ergodic sources : the ensemble of @xmath37-long subsequences , when @xmath37 is large enough , can be partitioned in two classes , @xmath44 and @xmath45 such that all the words @xmath46 have the same probability @xmath47 and @xmath48 the meaning of this theorem is the following .",
    "an @xmath32-states process admits , in principle , @xmath49 possible sequences of length @xmath37 .",
    "however the number of typical sequences , @xmath50 , effectively observable ( i.e. those belonging to @xmath44 ) is @xmath51 note that @xmath52 if @xmath53 .",
    "the entropy per symbol , @xmath43 , is a property of the source .",
    "however , because of the ergodicity @xmath43 can be obtained by analyzing just one single sequence in the ensemble of the typical ones , and it can also be viewed as a property of each typical sequence .    in information theory , expression  ( [ eq : wordstypical ] ) is somehow the equivalent of the boltzmann equation in statistical thermodynamics : @xmath54 , being @xmath55 the number of possible microscopic configurations and @xmath56 the thermodynamic entropy , this justifies the name `` entropy '' for @xmath43 .",
    "the relevance of the shannon entropy in information theory is given by the fact that @xmath43 sets the maximum compression rate of a sequence @xmath57 .",
    "indeed a theorem of shannon states that , if the length @xmath58 of a sequence is large enough , there exists no other sequence ( always using @xmath32 symbols ) , from which it is possible to reconstruct the original one , whose length is smaller than @xmath59 @xcite . in other words , @xmath60 represents the maximum allowed compression rate . the relation between shannon entropy and data compression problems",
    "is well illustrated by considering the optimal coding ( shannon - fano ) to map @xmath61 objects ( e.g. the @xmath37-words @xmath62 ) into sequences of binary digits @xmath63 @xcite . denoting with @xmath64 the binary length of the sequence specifying @xmath62 , we have @xmath65 i.e. , in a good coding , the mean length of a @xmath37-word is equal to @xmath37 times the shannon entropy , apart from a multiplicative factor , since in the definition ( [ eq : shannon ] ) of @xmath43 we used the natural logarithm and here we want to work with a two symbol code .      after the introduction of the shannon entropy we can easily define the kolmogorov - sinai entropy which is the analogous measure of complexity applied to dynamical systems .",
    "consider a trajectory , @xmath4 , generated by a deterministic system , sampled at the times @xmath66 , with @xmath67 .",
    "perform a finite partition @xmath68 of the phase space , with the finite number of symbols @xmath69 enumerating the cells of the partition . the time - discretized trajectory @xmath70 determines a sequence @xmath71 , whose meaning is clear : at the time @xmath72 the trajectory is in the cell labeled by @xmath73 .",
    "to each subsequence of length @xmath74 one can associate a word of length @xmath37 : @xmath75 .",
    "if the system is ergodic , as we suppose , from the frequencies of the words one obtains the probabilities by which the block entropies @xmath76 are calculated : @xmath77 the probabilities @xmath78 , computed by the frequencies of @xmath79 along a trajectory , are essentially dependent on the stationary measure selected by the trajectory .",
    "the entropy per unit time of the trajectory with respect to the partition @xmath68 , @xmath80 , is defined as follows : @xmath81 notice that , for the deterministic systems we are considering , the entropy per unit time does not depend on the sampling time @xmath82 @xcite .",
    "the ks - entropy ( @xmath83 ) , by definition , is the supremum of @xmath84 over all possible finite partitions @xcite @xmath85 the extremal character of @xmath83 makes every computation based on the definition  ( [ eq : ks ] ) , impossible in the majority of practical cases . in this respect",
    ", a useful tool would be the kolmogorov - sinai theorem , through which one is granted that @xmath86 if @xmath87 is a generating partition .",
    "a partition is said to be generating if every infinite sequence @xmath88 corresponds to a single initial point .",
    "however the difficulty now is that , with the exception of very simple cases , we do not know how to construct a generating partition .",
    "we only know that , according to the krieger theorem @xcite , there exists a generating partition with @xmath89 elements such that @xmath90 .",
    "then , a more tractable way to define @xmath83 is based upon considering the partition @xmath91 made up by a grid of cubic cells of edge @xmath18 , from which one has @xmath92 we expect that @xmath93 becomes independent of @xmath18 when @xmath94 is so fine to be `` contained '' in a generating partition .    for discrete time maps",
    "what has been exposed above is still valid , with @xmath95 ( however , krieger s theorem only applies to invertible maps ) .",
    "the important point to note is that , for a truly stochastic ( i.e. non - deterministic ) system , with continuous states , @xmath96 is not bounded and @xmath97 .",
    "the shannon entropy establishes a limit on how efficiently the ensemble of messages emitted by a source can be coded .",
    "however , we may wonder about the compressibility properties of a single sequence with no reference to its belonging to an ensemble . that is to say",
    ", we are looking for an universal characterization of its compressibility or , it is the same , an universal definition of its information content .",
    "this problem can be addressed through the notion of _ algorithmic complexity _ , that concerns the difficulty in reproducing a given string of symbols .",
    "everybody agrees that the binary digits sequence @xmath98 is , in some sense , more random than @xmath99 the notion of algorithmic complexity , independently introduced by kolmogorov @xcite , chaitin @xcite and solomonov @xcite , is a way to formalize the intuitive idea of randomness of a sequence .",
    "consider , for instance , a binary digit sequence ( this does not constitute a limitation ) of length @xmath37 , @xmath100 , generated by a certain computer code on a given machine @xmath101 .",
    "the algorithmic complexity ( or algorithmic information content ) @xmath102 of @xmath103 is the bit - length of the shortest computer program able to give @xmath103 and to stop afterward .",
    "of course , such a length depends not only on the sequence but also on the machine . however , kolmogorov @xcite proved the existence of a universal computer , @xmath104 , able to perform the same computation that a program @xmath105 makes on @xmath106 , with a modification of @xmath105 that depends only on @xmath106 .",
    "this implies that for all finite strings : @xmath107 where @xmath108 is the complexity with respect to the universal computer and @xmath109 depends only on the machine @xmath106 .",
    "we can consider the algorithmic complexity with respect to a universal computer dropping the @xmath106-dependence in the symbol for the algorithmic complexity , @xmath110 .",
    "the reason is that we are interested in the limit of very long sequences , @xmath111 , for which one defines the algorithmic complexity per unit symbol : @xmath112 that , because of ( [ eq : kolmocomplex ] ) , is an intrinsic quantity , i.e. independent of the machine .    now coming back to the @xmath37-sequences ( [ eq : seq1 ] ) and ( [ eq : seq2 ] )",
    ", it is obvious that the latter can be obtained with a minimal program of length @xmath113 and therefore when taking the limit @xmath111 in ( [ eq : acomplexity ] ) , one obtains @xmath114 . of course @xmath110",
    "can not exceed @xmath37 , since the sequence can always be generated by a trivial program ( of bit length @xmath37 ) @xmath115 therefore , in the case of a very irregular sequence , e.g. , ( [ eq : seq1 ] ) , one expects @xmath116",
    "( i.e. @xmath117 ) , and the sequence is named complex ( i.e. of non zero algorithmic complexity ) or random .",
    "algorithmic complexity can not be computed , and the un - computability of @xmath110 may be understood in terms of gdel s incompleteness theorem @xcite . beyond the problem of whether or not @xmath110 is computable in a specific case , the concept of algorithmic complexity brings an important improvement to clarify the vague and intuitive notion of randomness .    between the shannon entropy , @xmath43 , and the algorithmic complexity , there exists the straightforward relationship @xmath118 where @xmath119 , being @xmath120 the algorithmic complexity of the @xmath37-words , in the ensemble of sequences , @xmath62 , with a given distribution of probabilities , @xmath35 . therefore the expected complexity @xmath121 is asymptotically equal to the shannon entropy ( modulo the @xmath122 factor ) .",
    "it is important to stress again that , apart from the numerical coincidence of the values of @xmath123 and @xmath124 , there is a conceptual difference between the information theory and the algorithmic complexity theory .",
    "the shannon entropy essentially refers to the information content in a statistical sense , i.e. it refers to an ensemble of sequences generated by a certain source .",
    "the algorithmic complexity defines the information content of an individual sequence @xcite .",
    "the notion of algorithmic complexity can be also applied to the trajectories of a dynamical system .",
    "this requires the introduction of finite open coverings of the phase space , the corresponding encoding of trajectories into symbolic sequences , and the searching of the supremum of the algorithmic complexity per symbol at varying the coverings @xcite .",
    "brudno s and white s theorems @xcite state that the complexity @xmath125 for a trajectory starting from the point @xmath126 , is @xmath127 for almost all @xmath126 with respect to the natural invariant measure .",
    "the factor @xmath122 stems again from the conversion between natural logarithms and bits .",
    "this result indicates that the ks - entropy quantifies not only the richness of a dynamical system but also the difficulty of describing its typical sequences .",
    "let us consider a @xmath128 chaotic map @xmath129 the transmission of the sequence @xmath130 , accepting only errors smaller than a tolerance @xmath26 , is carried out by using the following strategy",
    "@xcite :    1 .   transmit the rule ( [ eq : mappa ] ) : for this task one has to use a number of bits independent of the sequence length @xmath58 .",
    "2 .   specify the initial condition @xmath131 with a precision @xmath29 using a finite number of bits which is independent of @xmath58 .",
    "3 .   let the system evolve till the first time @xmath132 such that the distance between two trajectories , that was initially @xmath133 , equals @xmath26 and then specify again the new initial condition @xmath134 with precision @xmath29 .",
    "4 .   let the system evolve and repeat the procedure ( 2 - 3 ) , i.e. each time the error acceptance tolerance is reached specify the initial conditions , @xmath135 , with precision @xmath29 . the times",
    "@xmath136 are defined as follows : putting @xmath137 , @xmath138 is given by the minimum time such that @xmath139 and so on .    following the steps @xmath140 , the receiver can reconstruct , with a precision @xmath26 , the sequence @xmath141 , by simply iterating on a computer the evolution law  ( [ eq : mappa ] ) between @xmath142 and @xmath143 , @xmath132 and @xmath144 , and so on .",
    "the amount of bits necessary to implement the above transmission ( 1 - 4 ) can be easily computed . for simplicity of notation",
    "we introduce the quantities @xmath145 which can be regarded as a sort of _ effective _ lyapunov exponents @xcite .",
    "the le @xmath31 can be written in terms of @xmath146 as follows @xmath147 where @xmath148 is the average time after which we have to transmit the new initial condition .",
    "note that to obtain @xmath31 from the @xmath149 s requires the average ( [ eq : liapt ] ) , because the transmission time , @xmath150 , is not constant . if @xmath58 is large enough",
    "the number of transmissions , @xmath37 , is @xmath151 .",
    "therefore , noting that in each transmission , a reduction of the error from @xmath26 to @xmath29 requires the employ of @xmath152 bits , the total amount of bits used in the transmission is @xmath153 in other words the number of bits for unit time is proportional to @xmath31 .    in more than one dimension , we have simply to replace @xmath31 with @xmath83 in ( [ eq : bits ] ) , because the above transmission procedure has to be repeated for each of the expanding directions .",
    "lyapunov exponents and ks - entropy are properly defined only in specific asymptotic limits : very long times and arbitrary accuracy . however , predictability problem in realistic situations entails considering finite time intervals and limited accuracy .",
    "the first obvious way for quantifying the predictability of a physical system is in terms of the _ predictability time _ @xmath28 , i.e. the time interval on which one can typically forecast the system .",
    "a simple argument suggests @xmath154 however , the above relation is too naive to be of practical relevance , in any realistic system . indeed , it does not take into account some basic features of dynamical systems . the lyapunov exponent is a global quantity , because it measures the average rate of divergence of nearby trajectories .",
    "in general there exist finite - time fluctuations and their probability distribution functions ( pdf ) is important for the characterization of predictability .",
    "generalized lyapunov exponents _ have been introduced with the purpose to take into account such fluctuations @xcite . moreover ,",
    "the lyapunov exponent is defined for the linearized dynamics , i.e. , by computing the rate of separation of two infinitesimally close trajectories . on the other hand , in measuring the predictability time ( [ eq:2.1 - 1 ] )",
    "one is interested in a finite tolerance @xmath26 , because the initial error @xmath29 is finite .",
    "a recent generalization of the lyapunov exponent to _ finite size _",
    "errors extends the study of the perturbation growth to the nonlinear regime , i.e. both @xmath29 and @xmath26 are not infinitesimal @xcite .",
    "we discuss now an example where the lyapunov exponent is of little relevance for characterizing the predictability .",
    "this problem can be illustrated by considering the following coupled map model : @xmath155 where @xmath156 , @xmath157 , @xmath158 is a rotation matrix of arbitrary angle @xmath159 , @xmath160 is a vector function and @xmath161 is a chaotic map . for simplicity",
    "we consider a linear coupling @xmath162 and the logistic map @xmath163 .    for @xmath164",
    "we have two independent systems : a regular and a chaotic one .",
    "thus the lyapunov exponent of the @xmath126 subsystem is @xmath165 , i.e. , it is completely predictable . on the contrary",
    ", the @xmath166 subsystem is chaotic with @xmath167 .",
    "the switching on of a small coupling ( @xmath168 ) yields a single three - dimensional chaotic system with a positive global lyapunov exponent @xmath169 a direct application of ( [ eq:2.1 - 1 ] ) would give @xmath170 but this result is clearly unacceptable : the predictability time for @xmath126 seems to be independent of the value of the coupling @xmath171 .",
    "this is not due to an artifact of the chosen example , indeed , the same argument applies to many physical situations @xcite .",
    "a well known example is the gravitational three body problem , with one body ( asteroid ) much smaller than the other two ( planets ) .",
    "when the gravitational feedback of the asteroid on the two planets is neglected ( restricted problem ) , one has a chaotic asteroid in the regular field of the planets . as soon as the feedback is taken into account ( i.e. @xmath168 in the example ) one has a non - separable three body system with a positive le .",
    "of course , intuition correctly suggests that , in the limit of small asteroid mass ( @xmath172 ) , a forecast of the planet motion should be possible even for very long times .",
    "the apparent paradox arises from the misuse of formula  ( [ eq:2.1 - 1 ] ) , strictly valid for tangent vectors , to the case of non infinitesimal regimes .",
    "as soon as the errors become large , the full nonlinear evolution of the three body system has to be taken into account .",
    "this situation is clearly illustrated by the model ( [ eq:2.3 - 1 ] ) in figure  [ fig:2.3 - 1 ] .",
    "the evolution of @xmath173 is given by @xmath174 where , with our choice , @xmath175 . at the beginning ,",
    "both @xmath176 and @xmath177 grow exponentially . however , the available phase space for @xmath166 is finite and the uncertainty reaches the saturation value @xmath178 in a time @xmath179 . at larger times",
    "the two realizations of the @xmath166 variable are completely uncorrelated and their difference @xmath177 in ( [ eq:2.3 - 4 ] ) acts as a noisy term . as a consequence ,",
    "the growth of the uncertainty on @xmath126 becomes diffusive with a diffusion coefficient proportional to @xmath180 @xcite @xmath181 so that : @xmath182    this example shows that , even in simple systems , the lyapunov exponent can be of little relevance for the characterization of the predictability .    in more complex systems , in which different scales are present , one is typically interested in forecasting the large scale motion , while the le is related to the small scale dynamics .",
    "a familiar example of that is weather forecast : despite the le of the atmosphere is indeed rather large , due to the small scale convective motion , large - scale weather predictions are possible for about @xmath183 days @xcite .",
    "it is thus natural to seek for a generalization of the le to finite perturbations from which one can obtain a more realistic estimation for the predictability time .",
    "it is worth underlining the important fact that finite errors are not confined in the tangent space but are governed by the complete nonlinear dynamics . in this sense",
    "the extension of the le to finite errors will give more information on the system .",
    "aiming to generalize the le to non infinitesimal perturbations let us now define the finite size lyapunov exponent ( fsle ) @xcite .",
    "consider a reference @xmath4 and a perturbed trajectory @xmath184 , such that @xmath185 .",
    "one integrates the two trajectories and computes the time @xmath186 necessary for the separation @xmath187 to grow from @xmath188 to @xmath189 . at time",
    "@xmath190 the distance between the trajectories is rescaled to @xmath188 and the procedure is repeated in order to compute @xmath191 .",
    "the threshold ratio @xmath192 must be @xmath193 , but not too large in order to avoid contributions from different scales in @xmath194 .",
    "a typical choice is @xmath195 ( for which @xmath194 is properly a `` doubling '' time ) or @xmath196 .",
    "in the same spirit of the discussion leading to eq.s ( [ eq : mars1 ] ) and ( [ eq : liapt ] ) , we may introduce an effective finite size growth rate : @xmath197    after having performed @xmath198 error - doubling experiments , we can define the fsle as @xmath199 where @xmath200 is @xmath201 see @xcite for details . in the infinitesimal limit , the fsle reduces to the standard lyapunov exponent @xmath202 in practice this limit means that @xmath203 displays a constant plateau at @xmath204 for sufficiently small @xmath188 ( fig .  [ fig:2.3 - 2 ] ) . for finite value of @xmath188 the behavior of @xmath203",
    "depends on the details of the non linear dynamics .",
    "for example , in the model ( [ eq:2.3 - 1 ] ) the diffusive behavior ( [ eq:2.3 - 5 ] ) , by simple dimensional arguments , corresponds to @xmath205 .",
    "@xmath203 as a function of @xmath188 for the coupled map ( [ eq:2.3 - 1 ] ) with @xmath206 .",
    "the perturbation has been initialized as in fig .",
    "[ fig:2.3 - 1 ] . for @xmath207 , @xmath208 ( horizontal line ) .",
    "the dashed line shows the behavior @xmath205 . ]",
    "since the fsle measures the rate of divergence of trajectories at finite errors , one might wonder whether it is just another way to look at the average response @xmath209 as a function of time .",
    "the answer is negative , because taking the average at fixed time is not the same as computing the average doubling time at _ fixed scale _ , as in ( [ eq:2.3 - 10 ] ) .",
    "this is particularly clear in the case of strongly intermittent system , in which @xmath210 can be very different in each realization . in",
    "the presence of intermittency , averaging over different realizations at fixed times can produce a spurious regime due to the superposition of exponential and diffusive contributions by different samples at the same time @xcite .",
    "the fsle method can be easily applied to data analysis @xcite . for other approaches addressing the problem of non - infinitesimal perturbations",
    "see @xcite .      for most systems ,",
    "the computation of kolmogorov - sinai entropy ( [ eq : ks ] ) is practically impossible , because it involves the limit on arbitrary fine resolution and infinite times .",
    "however , in the same philosophy of the fsle , by relaxing the requirement of arbitrary accuracy , one can introduce the @xmath18-entropy which measures the amount of information for reproducing a trajectory with finite accuracy @xmath18 in phase - space . roughly speaking",
    "the @xmath18-entropy can be considered the counterpart , in information theory , of the fsle .",
    "such a quantity was originally introduced by shannon @xcite , and by kolmogorov @xcite . recently",
    "gaspard and wang @xcite made use of this concept to characterize a large variety of processes .",
    "we start with a continuous - time variable @xmath211 , which represents the state of a @xmath17-dimensional system , we discretize the time by introducing an interval @xmath82 and we consider the new variable @xmath212 of course @xmath213 and it corresponds to the trajectory which lasts for a time @xmath214 .    in data analysis ,",
    "the space where the state of the system lives is unknown and usually only a scalar variable @xmath215 can be measured .",
    "then , one considers vectors @xmath216 , that live in @xmath217 and allow a reconstruction of the original phase space , known as delay embedding in the literature @xcite , and it is a special case of ( [ eq:2 - 1 ] ) .",
    "introduce now a partition of the phase space @xmath218 , using cells of edge @xmath18 in each of the @xmath17 directions .",
    "since the region where a bounded motion evolves contains a finite number of cells , each @xmath219 can be coded into a word of length @xmath32 , out of a finite alphabet : @xmath220 where @xmath221 labels the cell in @xmath218 containing @xmath222 . from the time evolution one",
    "obtains , under the hypothesis of ergodicity , the probabilities @xmath223 of the admissible words @xmath224 .",
    "we can now introduce the @xmath225-entropy per unit time , @xmath226 @xcite : @xmath227 where @xmath228 is the block entropy of blocks ( words ) with length @xmath32 : @xmath229 for the sake of simplicity , we ignored the dependence on details of the partition . to make @xmath230 partition - independent one has to consider a generic partition of the phase space @xmath231 and to evaluate the shannon entropy on this partition : @xmath232 .",
    "the @xmath171-entropy is thus defined as the infimum over all partitions for which the diameter of each cell is less than @xmath171 @xcite : @xmath233 note that the time dependence in ( [ def : eps ] ) is trivial for deterministic systems , and that in the limit @xmath234 one recovers the kolmogorov - sinai entropy @xmath235",
    "in the previous sections , we discussed the characterization of dynamical behaviors when the evolution laws are known either exactly or with some degree of uncertainty . in experimental investigations , however , only time records of some observable are available , while the equations of motion for the observable are generally unknown . the predictability problem of this latter case , at least from a conceptual point of view ,",
    "can be treated as if the evolution laws were known .",
    "indeed , in principle , the embedding technique allows for a reconstruction of the phase space @xcite .",
    "nevertheless there are rather severe limitations for high dimensional systems @xcite and even in low dimensional ones non trivial features appear in the presence of noise @xcite . in this section we show that an entropic analysis at different resolution scales provides a pragmatic classification of a signal and gives suggestions for modeling of systems . in particular we illustrate , using some examples , how quantities such as the @xmath18-entropy or the fsle can display a subtle transition from the large to the small scales .",
    "a negative consequence of this is the difficulty in distinguishing , only from data analysis , a genuine deterministic chaotic system from one with intrinsic randomness @xcite . on the other hand ,",
    "the way the @xmath18-entropy or fsle depends on the ( resolution ) scale , allows for a classification of the stochastic or chaotic character of a signal , and this gives some freedom in modeling the system .",
    "the `` true character '' of the number sequence @xmath236 obtained by a ( pseudo ) random number generator ( prng ) on a computer is an issue of paramount importance in computer simulations and modeling .",
    "one would like to have a sequence with a random character as much as possible , but is forced to use deterministic algorithms to generate @xmath236 .",
    "this subsection is mainly based on the paper @xcite .",
    "a simple and popular prng is the multiplicative congruent one : @xmath237 with an integer multiplier @xmath238 and modulus @xmath239 .",
    "the @xmath240 are integer numbers from which one hopes to generate sequence of random variables @xmath241 , which are uncorrelated and uniformly distributed in the unit interval .",
    "a first problem arises from the periodic nature of the rule  ( [ prng ] ) as a consequence of its discrete nature .",
    "note that the rule  ( [ prng ] ) can be interpreted also as a deterministic dynamical system , i.e. @xmath242 which has a uniform invariant measure and a ks entropy @xmath243 .",
    "when imposing the integer arithmetics of eq .",
    "( [ prng ] ) onto this system , we are , in the language of dynamical systems , considering an unstable periodic orbit of eq .",
    "( [ xdyn ] ) , with the particular constraint that , to achieve the period @xmath244 ( i.e.  all integers @xmath245 should belong to the orbit of eq .",
    "( [ prng ] ) ) , it has to contain all values @xmath246 , with @xmath247 . since the natural invariant measure of eq .",
    "( [ xdyn ] ) is uniform , such an orbit represents the measure of a chaotic solution in an optimal way .",
    "every sequence of a prng is characterized by two quantities : its period @xmath248 and its positive lyapunov exponent @xmath31 , which is identical to the entropy of a chaotic orbit of the equivalent dynamical system .",
    "of course a good random number generator must have a very large period , and as large as possible entropy .",
    "it is natural to ask how this apparent randomness can be reconciled with the facts that ( a ) the prng is a deterministic dynamical systems ( b ) it is a discrete state system .",
    "if the period is long enough , on shorter times only point ( a ) matters and it can be discussed in terms of the behavior of the @xmath18-entropy , @xmath249 . at high resolutions ( @xmath250 )",
    ", it seems rather reasonable to think that the true deterministic chaotic nature of the congruent rule shows up , and , therefore , @xmath251 . on the other hand , for @xmath252 , one expects to observe the `` apparent random '' behavior of the system , i.e. @xmath253 , see fig  [ fig : rng_entropy ] .",
    "the @xmath18-entropies , @xmath254 , at varying the embedding dimension @xmath32 for the multiplicative congruential random number generator eq .  [ prng ] for different choices of @xmath238 and @xmath239 .",
    "]      we discuss an example of high - dimensional system with a non - trivial behavior at varying the resolution scales , namely the emergence of nontrivial collective behavior .",
    "let us consider a globally coupled map ( gcm ) defined as follows @xmath255 where @xmath37 is the total number of elements , and @xmath256 is a chaotic map on the interval @xmath257 $ ] , depending on the control parameter @xmath258 .",
    "the evolution of a macroscopic variable , e.g. , the center of mass @xmath259 upon varying @xmath171 and @xmath258 in eq .",
    "( [ eq:3.38 ] ) , displays different behaviors @xcite :    \\(a ) _ standard chaos _",
    ": @xmath260 obeys a gaussian statistics with a standard deviation @xmath261 ;    \\(b ) _ macroscopic periodicity _ : @xmath260 is a superposition of a periodic function and small fluctuations @xmath262 ;    \\(c ) _ macroscopic chaos _ : @xmath260 exhibits an irregular motion , as seen by plotting @xmath260 vs. @xmath263 .",
    "the plot sketches a structured function ( with thickness @xmath264 ) , and suggests a chaotic motion for @xmath260 .    in the case of _",
    "macroscopic chaos _",
    ", the center of mass is expected to evolve with typical times longer than the characteristic time @xmath265 of the full dynamics ( microscopic dynamics ) ; @xmath204 being the lyapunov exponent of the gcm . indeed ,",
    "conceptually , macroscopic chaos for gcm can be thought of as the analogous of the hydro - dynamical chaos for molecular motion . in spite of a huge microscopic lyapunov exponent ( @xmath266",
    ", @xmath267 is the collision time ) , one can have rather different behaviors at a hydro - dynamical ( coarse grained ) level : regular motion ( @xmath268 ) or chaotic motion ( @xmath269 ) . in principle , if the hydrodynamic equations were known , a characterization of the macroscopic behavior would be possible by means of standard dynamical system techniques .",
    "however , in generic cml there are no general systematic methods to build up the macroscopic equations , apart from particular cases @xcite .",
    "we recall that for chaotic systems , in the limit of infinitesimal perturbations @xmath207 , one has @xmath270 , i.e. @xmath203 displays a plateau at the value @xmath271 for sufficiently small @xmath188 . however , for non infinitesimal @xmath188 , one can expect that the @xmath188-dependence of @xmath203 may give information on the characteristic time - scales governing the system , and , hence , it could be able to characterize the macroscopic motion .",
    "in particular , at large scales ( @xmath272 ) , the fast microscopic components saturate and @xmath273 , where @xmath274 can be fairly called the `` macroscopic '' lyapunov exponent .",
    "the fsle has been determined by looking at the evolution of @xmath275 , which has been initialized at the value @xmath276 by shifting all the elements of the unperturbed system by the quantity @xmath277 ( i.e. @xmath278 ) , for each realization .",
    "the computation has been performed by choosing the tent map as local map , but similar results can be obtained for other maps @xcite .",
    "the main result can be summarized as follows :    * at small @xmath279 , where @xmath37 is the number of elements , the `` microscopic '' lyapunov exponent is recovered , i.e. @xmath280 * at large @xmath281 , another plateau @xmath282 appears , which can be much smaller than the microscopic one .",
    "the emerging scenario is that , at a coarse - grained level , i.e. @xmath272 , the system can be described by an `` effective '' hydro - dynamical equation ( which in some cases can be low - dimensional ) , while the `` true '' high - dimensional character appears only at very high resolution , i.e. @xmath283      consider the following map which generates a diffusive behavior on the large scales @xcite : @xmath284 where @xmath285 indicates the integer part of @xmath286 and @xmath287 is given by : @xmath288\\ , .",
    "\\label{eq : mappaf}\\ ] ] the largest lyapunov exponent @xmath31 can be obtained immediately : @xmath289 , with @xmath290 .",
    "one expects the following scenario for @xmath249 : @xmath291 @xmath292 where @xmath293 is the diffusion coefficient , @xmath294 .",
    "the map @xmath295 ( [ eq : mappaf ] ) for @xmath296 is shown with superimposed the approximating ( regular ) map @xmath297 ( [ eq:3 - 5 ] ) obtained by using @xmath298 intervals of slope @xmath299 . ]",
    "consider now a stochastic system , namely a noisy map @xmath300 where @xmath301 , as shown in fig .  [ map ] , is a piece wise linear map which approximates the map @xmath287 , and @xmath302 is a stochastic process uniformly distributed in the interval @xmath303 , and no correlation in time . when @xmath304 , as is the case we consider , the map ( [ eq:3 - 5 ] ) , in the absence of noise , gives a non - chaotic time evolution .",
    "lyapunov exponent @xmath305 versus @xmath18 obtained for the map @xmath287 ( [ eq : mappaf ] ) with @xmath296 ( @xmath306 ) and for the noisy ( regular ) map ( [ eq:3 - 5 ] ) ( @xmath307 ) with @xmath308 intervals of slope @xmath309 and @xmath310 .",
    "straight lines indicate the lyapunov exponent @xmath311 and the diffusive behavior @xmath312    .",
    "now we compare the finite size lyapunov exponent for the chaotic map ( [ eq:3 - 1 ] ) and for the noisy one ( [ eq:3 - 5 ] ) . in the latter",
    "the fsle has been computed using two different realizations of the noise . in fig .",
    "[ fslediff ] we show @xmath305 versus @xmath18 for the two cases .",
    "the two curves are practically indistinguishable in the region @xmath313 .",
    "the differences appear only at very small scales @xmath314 where one has a @xmath305 which grows with @xmath18 for the noisy case , remaining at the same value for the chaotic deterministic case .",
    "both the fsle and the @xmath315-entropy analysis show that we can distinguish three different regimes observing the dynamics of ( [ eq:3 - 5 ] ) on different length scales . on the large length scales",
    "@xmath316 we observe diffusive behavior in both models .",
    "on length scales @xmath317 both models show chaotic deterministic behavior , because the entropy and the fsle are independent of @xmath18 and larger than zero .",
    "finally on the smallest length scales @xmath318 we see stochastic behavior for the system ( [ eq:3 - 5 ] ) , _ i.e. _ @xmath319 , while the system ( [ eq:3 - 1 ] ) still shows chaotic behavior .",
    "the above examples show that the distinction between chaos and noise can be a highly non trivial task , which makes sense only in very peculiar cases , e.g. , very low dimensional systems .",
    "nevertheless , even in this case , the entropic analysis can be unable to recognize the `` true '' character of the system due to the lack of resolution .",
    "again , the comparison between the diffusive map ( [ eq:3 - 1 ] ) and the noisy map ( [ eq:3 - 5 ] ) is an example of these difficulties . for @xmath320 both the system ( [ eq:3 - 1 ] ) and ( [ eq:3 - 5 ] ) , in spite of their `` true '' character , will be classified as chaotic , while for @xmath321 both can be considered as stochastic .    in high - dimensional chaotic systems , with @xmath37 degrees of freedom",
    ", one has typically @xmath322 for @xmath323 ( where @xmath324 as @xmath325 ) while for @xmath326 , @xmath249 decreases , often with a power law @xcite .",
    "since also in some stochastic processes the @xmath18-entropy obeys a power law , this can be a source of confusion .",
    "these kind of problems are not abstract ones , as a recent debate on `` microscopic chaos '' demonstrates @xcite .",
    "the detection of microscopic chaos by data analysis has been recently addressed in a work of gaspard et al .",
    "these authors , from an entropic analysis of an ingenious experiment on the position of a brownian particle in a liquid , claim to give an empirical evidence for microscopic chaos .",
    "in other words , they state that the diffusive behavior observed for a brownian particle is the consequence of chaos at a molecular level .",
    "their work can be briefly summarized as follows : from a long ( @xmath327 data ) record of the position of a brownian particle they compute the @xmath18-entropy with the cohen - procaccia method @xcite from which they obtain : @xmath328 where @xmath293 is the diffusion coefficient .",
    "then , _ assuming _ that the system is deterministic , and making use of the inequality @xmath329 , they conclude that the system is chaotic",
    ". however , their result does not give a direct evidence that the system is deterministic and chaotic .",
    "indeed , the power law ( [ eq : gasp ] ) can be produced with different mechanisms :    1 .   a genuine chaotic system with diffusive behavior , as the map ( [ eq : mappaf ] ) ; 2 .   a non chaotic system with some noise , as the map ( [",
    "eq:3 - 5 ] ) , or a genuine brownian system ; 3 .",
    "a deterministic linear non chaotic system with many degrees of freedom ( see for instance @xcite ) ; 4 .   a `` complicated '' non chaotic system as the ehrenfest wind - tree model where a particle diffuses in a plane due to collisions with randomly placed , fixed oriented square scatters , as discussed by cohen et al .",
    "@xcite in their comment to ref .",
    "@xcite .",
    "it seems to us that the weak points of the analysis in ref .",
    "@xcite are :    \\a ) the explicit assumption that the system is deterministic ;    \\b ) the limited number of data points and therefore limitations in both resolution and block length .",
    "the point ( a ) is crucial , without this assumption ( even with an enormous data set ) it is not possible to distinguish between 1 ) and 2 ) .",
    "one has to say that in the cases 3 ) and 4 ) at least in principle it is possible to understand that the systems are `` trivial '' ( i.e. not chaotic ) but for this one has to use a huge number of data .",
    "for example cohen et al .",
    "@xcite estimated that in order to distinguish between 1 ) and 4 ) using realistic parameters of a typical liquid , the number of data points required has to be at least @xmath330 .    concluding , we have the apparently paradoxical result that `` complexity '' helps in the construction of models .",
    "basically , in the case in which one has a variety of behaviors at varying the scale resolution , there is a certain freedom on the choice of the model to adopt .",
    "for some systems the behavior at large scales can be realized both with chaotic deterministic models or suitable stochastic processes . from a pragmatic point of view ,",
    "the fact that in certain stochastic processes @xmath331 can be indeed extremely useful for modeling such high - dimensional systems .",
    "perhaps , the most relevant case in which one can use this freedom in modeling is the fully developed turbulence whose non infinitesimal ( the so - called inertial range ) properties can be successfully mimicked in terms of multi - affine stochastic process ( see ref .",
    "the guideline of this paper has been _ the interpretation of different aspects of the predictability of a system as a way to characterize its complexity_.    we have discussed the relation between chaoticity , the kolmogorov - sinai entropy and algorithmic complexity . as clearly exposed in the seminal works of alekseev and yakobson @xcite and ford @xcite , the time sequences generated by a system with sensitive dependence on initial conditions have non - zero algorithmic complexity .",
    "a relation exists between the maximal compression of a sequence and its ks - entropy .",
    "therefore , one can give a definition of complexity , without referring to a specific description , as an intrinsic property of the system .",
    "the study of these different aspects of predictability constitutes a useful method for a quantitative characterization of `` complexity '' , suggesting the following equivalences : @xmath332 the above point of view , based on dynamical systems and information theory , quantifies the complexity of a sequence considering each symbol relevant but it does not capture the structural level .",
    "let us clarify this point with the following example .",
    "a binary sequence obtained with a coin tossing is , from the point of view adopted in this review , complex since it can not be compressed ( i.e. it is unpredictable ) . on the other hand",
    "such a sequence is somehow trivial , i.e. with low `` organizational '' complexity .",
    "it would be important to introduce a quantitative measure of this intuitive idea .",
    "the progresses of the research on this intriguing and difficult issue are still rather slow .",
    "we just mention some of the most promising proposals as the logical depth and the sophistication  @xcite .",
    "f.  takens , `` detecting strange attractors in turbulence '' in _ dynamical systems and turbulence ( warwick 1980 ) _ , vol .",
    "898 of _ lecture notes in mathematics _ ,",
    "rand and l .- s .",
    "young ( eds . ) , pg . 366 , springer - verlag , berlin ( 1980 ) ."
  ],
  "abstract_text": [
    "<S> some aspects of the predictability problem in dynamical systems are reviewed . </S>",
    "<S> the deep relation among lyapunov exponents , kolmogorov - sinai entropy , shannon entropy and algorithmic complexity is discussed . in particular , we emphasize how a characterization of the unpredictability of a system gives a measure of its complexity . </S>",
    "<S> a special attention is devoted to finite - resolution effects on predictability , which can be accounted with suitable generalization of the standard indicators . </S>",
    "<S> the problems involved in systems with intrinsic randomness is discussed , with emphasis on the important problems of distinguishing chaos from noise and of modeling the system .    </S>",
    "<S> _ all the simple systems are simple in the same way , each complex system has its own complexity _ ( freely inspired by _ anna karenina _ by lev n. tolstoy ) </S>"
  ]
}