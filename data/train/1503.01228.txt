{
  "article_text": [
    "learning the parameters of a markov random field ( mrf ) or a conditional random field ( crf ) is a ubiquitous problem in machine learning and related fields .",
    "often , the parameters are learned via regularized maximum likelihood estimation ( mle ) and then prediction is performed via maximum a - posteriori ( map ) or marginal inference given parameters @xmath0 , while mle learning refers to estimating @xmath0 given observations @xmath1 , optionally including quadratic regularization .",
    "] as the log - likelihood is concave , it can in principle be maximized by gradient ascent . however , this requires repeatedly computing gradients of the log - partition function , which in general is intractable .",
    "one can circumvent this difficulty by using surrogates for the log - partition function   or by approximating the partition function using sampling  .",
    "alternatively , one can avoid likelihoods entirely , and use methods such as the structured perceptron or structured support vector machines ( svm - struct ) that rely only on a map solver  .",
    "such methods can often be quite accurate and are typically faster than approximate mle , since map , or relaxations thereof , can be performed quickly using sophisticated combinatorial solvers . by using such solvers as black boxes , map - based training methods also offer users an attractive abstraction between the learning problem and the optimization algorithm . on the other hand",
    ", mle remains a primary goal for many practitioners , since it may yield superior predictive accuracy , offers parameter values with increased interpretability and statistical properties , and supports test - time marginal inference .    in this work ,",
    "we introduce mle - struct , a novel approximate mle algorithm that also only requires access to a map solver .",
    "we combine bethe - style convex free energies with the frank - wolfe ( fw ) method  . a naive application of fw for approximate mle would perform approximate marginal inference using repeated calls to map , as in the experiments of  , and then use these marginals to perform a single gradient step on the parameters .",
    "this double - loop algorithm requires a significant number of map solver calls , especially if very accurate answers are required .",
    "our approach achieves fast learning by avoiding this costly double loop structure .",
    "first , we employ a generic reweighted entropy approximation technique that yields convex bethe - style surrogate likelihoods for any underlying undirected graphical model .",
    "then , we construct a constrained , convex dual problem for this approximate maximum likelihood objective .",
    "we demonstrate that the approximate dual problem can be minimized efficiently using fw : each of the linear subproblems that are solved as part of the algorithm can be formulated as separate approximate or exact map inference tasks on each training example .",
    "finally , we introduce a technique to accelerate the line search subroutine of fw by precomputing certain data - dependent terms .",
    "we can also use fw to perform test - time marginal inference using the procedure of  .",
    "therefore , at both train and test time we can interact with our underlying problem structure using only a map routine .",
    "this allows us to design fast approximate learning and prediction algorithms for a wide variety of settings in which efficient approximate / exact map solvers exist : bipartite / general matching and b - matching problems ( via the max - flow and blossom algorithms ) , pairwise binary graphical models ( via qpbo ) , planar ising models with no external field ( via a reduction to matching )  , among others .",
    "we apply our method to learn pairwise binary crfs and distributions over matchings on both bipartite and general graphs .",
    "our method provides good predictive performance while often solving the approximate mle problem significantly faster with fewer numerical instabilities than other approximate mle methods .",
    "we also apply our method to a new dataset of housing preferences and roommate assignments of university students to predict good freshmen roommate assignments .",
    "we consider conditional random fields where , in addition to samples @xmath2 from some discrete space @xmath3 , we also observe feature vectors @xmath4 . in this case , the conditional probability of the @xmath5 sample has the form @xmath6 where @xmath7 is a vector of sufficient statistics , @xmath0 is a vector of parameters , and the partition function is given by @xmath8 in typical applications , the joint probability distribution factors over a hypergraph @xmath9 where @xmath10 is a collection of subsets of @xmath11 . for ease of presentation ,",
    "we assume that @xmath12 that is , for @xmath13 and @xmath14 , @xmath15 these models include both potts and ising models , as well as log - linear distributions over matchings  .",
    "given @xmath16 observations @xmath17 with corresponding feature vectors @xmath18 where @xmath19 is of the form , we would like to learn @xmath0 by maximizing the log - likelihood of the observations plus a quadratic regularizer .",
    "@xmath20 here , @xmath21      the central challenge in maximum likelihood estimation is computing the partition function @xmath22 for each sample @xmath23 at each iteration . in this work , we approximate the partition function in order to make the learning problem tractable . we begin with the bethe free energy , a standard approximation to the so - called gibbs free energy that is motivated by ideas from statistical physics .",
    "the approximation has been generalized to include different _ counting numbers _ that result in alternative entropy approximations .",
    "we focus on a restricted set of counting numbers that result in a family of _ convex _ reweighted free energies .",
    "the reweighted free energy at temperature @xmath24 is specified by a polytope approximation @xmath25 , the hypergraph @xmath9 , an entropy approximation @xmath26 , and a vector of counting numbers @xmath27 ( henceforth referred to as reweighting parameters ) . @xmath28 where the energy is given by @xmath29 the entropy approximation is given by @xmath30 and @xmath31 is restricted to lie in an outer bound of the marginal polytope known as the local ( marginal ) polytope , @xmath32 the reweighted partition function",
    "is then computed by minimizing over @xmath25 @xmath33 setting @xmath34 for each @xmath35 recovers the typical bethe free energy approximation .",
    "the reweighting parameters can always be chosen so that the approximate free energy is convex . for example , the tree - reweighted belief propagation algorithm ( trw ) chooses the reweighting parameters so that they correspond to ( hyper)edge appearance probabilities of a collection of spanning ( hyper)trees .",
    "we approximate the exact partition function in the mle objective with a reweighted free energy approximation of the form  .",
    "this results in the saddle - point problem @xmath36 - \\frac{\\lambda}{2 } \\|{\\theta}\\|^2\\bigg].\\hspace{-.5 cm } \\label{eq : saddle } \\end{aligned}\\ ] ] investigated unregularized likelihoods of this form for mrfs , and demonstrated that convexity of the bethe free energy guarantees that the empirical marginals satisfy a moment matching condition : the empirical marginals minimize the bethe free energy at the @xmath0 that maximizes the approximate log - likelihood .",
    "moment matching is not necessarily achieved for general mrfs when the reweighted approximation is not convex .",
    "investigated the use of trw for learning in pairwise binary graphical models .",
    "they observed that the parameters learned via trw were more robust to the addition of new data than those learned by bp .",
    "this robustness of convex free energy approximations for learning can be made theoretically precise .",
    "if we compute the partition function via an iterative procedure , then solving   necessarily requires a double - loop algorithm , which can be expensive for large datasets .",
    "the existing work on _ bethe learning _ has sought to design more efficient approximate learning algorithms .",
    "proposed a piecewise training scheme whereby the graph is divided into smaller subgraphs over which the partition function can be efficiently computed exactly or approximately .",
    "these results are then combined to approximate the true partition function .",
    "because the subproblems are typically much smaller , the procedure is quite fast but can be inaccurate if the pieces are too small . for bipartite matchings , one can obtain an unbiased but noisy gradient of the log - likelihood by utilizing an @xmath37 perfect sampler algorithm due to . use this approach for ranking and graph matching problems , but limited themselves to 20 vertices , and each observation required its own set of samples . proposed performing mle using a small , fixed number of trw iterations as part of a procedure to estimate the gradient .",
    "however , if trw is not converging quickly ( i.e. , a reasonable solution is not obtained after running for a fixed number of iterations ) , the resulting procedure can fail to converge .",
    "proposed improving the convergence in the outer loop using accelerated gradient methods .",
    "all of the above methods rely on a double loop .",
    "we now consider a convex _ dual _ reformulation of   that applies to convex free energies and yields a new , fast learning algorithm .",
    "we first note that   is concave in the variables being maximized and convex in the variables being minimized , and one set of variables ( the @xmath31 ) are constrained to a compact domain .",
    "we can thus invoke sion s minimax theorem to reverse the @xmath38 and @xmath39 operators .",
    "next , we can analytically solve for the optimal @xmath0 in terms of fixed @xmath40 . setting the gradient with respect to @xmath0 equal to zero in yields @xmath41\\bigg )",
    "\\label{eq : tau2theta_a } \\\\",
    "\\theta^*_\\alpha(\\tau^{(1:m ) } ) = \\frac{1}{\\lambda}\\bigg ( & \\sum_m \\big[\\phi_\\alpha(x^{(m ) } , y^{(m)}_\\alpha ) - \\nonumber\\\\      & \\sum_{y_\\alpha } \\tau^{(m)}_\\alpha(y_\\alpha ) \\phi_\\alpha(x^{(m ) } , y_\\alpha)\\big]\\bigg ) .",
    "\\label{eq : tau2theta_b } \\end{aligned}\\ ] ] finally , substituting these back into yields the following optimization problem over the local marginal polytope . @xmath42",
    "the linearly - constrained convex objective   can be minimized via general convex optimization techniques , such as the ellipsoid method  though this can be slow in practice . in the sequel",
    ", we minimize this objective with the frank - wolfe algorithm ( fw ) .",
    "convex free energies can be obtained using the reweighting techniques above , and whenever the graph is a tree the standard bethe free energy is both convex and exact . in principle",
    ", a similar argument can be made for any convex approximation of the partition function , though we only focus on convex bethe - style approximations in this work .",
    "the mle objective is dual to the maximum entropy problem , and recent work on approximate mle has focused on different families of entropy approximations",
    ". also followed a maximum entropy approach to the approximate mle problem .",
    "they proposed approximating the entropy objective using the bethe entropy approximation ( i.e. , @xmath43 ) , but specifically avoided convex entropy approximations .",
    "unfortunately , this results in a non - convex optimization problem in general , for which the authors use the concave - convex procedure .",
    "other recent work approximated the mle problem using convex free energies , but did not consider the maximum entropy approach .",
    "following , fw minimizes a general convex function @xmath44 over a convex set @xmath45 via a sequence of iterates defined by @xmath46 where the step - size , @xmath47 , is either selected using line search or is fixed at @xmath48 .    for the objective function in",
    ", each step requires minimizing a linear objective over a linear set of constraints .",
    "@xmath49 since the constraints are separable across training examples , decouples into @xmath16 independent linear programs ( lps ) that can be solved in parallel .",
    "depending on the specific application , purely combinatorial methods or reweighted message - passing algorithms may provide faster and more space efficient alternatives to generic lp solvers .",
    "note that   could not be solved with projected gradient algorithms efficiently , since projection onto the local or marginal polytopes is not tractable .    despite the ability to perform   in parallel , it can still be prohibitive for large sample sizes . for convex optimization problems over separable constraint spaces , propose a block - coordinate fw algorithm ( bcfw ) .",
    "the bcfw procedure performs the fw iteration over a randomly selected @xmath50 and leaves the remaining coordinates untouched .",
    "bcfw requires less work at each iteration , but the asymptotic rate of convergence remains the same as that for the standard fw algorithm .",
    "this block coordinate approach is known to outperform fw for the svm - struct problem .",
    "technical details concerning the convergence of fw for this problem , including methods to bound the convergence rate , can be found in appendix [ app : fw ] .",
    "both the fw and bcfw versions of our algorithm , mle - struct , are described in algorithm  [ alg : fw - learning].line search can be accelerated by precomputing quadratic terms as discussed in  [ app : line - search ] .",
    "training examples @xmath51 , reweighting parameters @xmath52^n$ ] , regularizer @xmath53 + approximate maximum likelihood @xmath0 .",
    "set each @xmath54 uniformly .",
    "@xmath55 set @xmath56 for batch and @xmath57 for block or use line search .",
    "@xmath58 set @xmath0 using   and  .",
    "[ alg : fw - learning ]      in many applications , computing marginals is useful at test time , as well as during learning .",
    "fortunately , we can use fw to perform marginal inference , thus maintaining our ability to interact with the underlying model only through a map solver .",
    "specifically , approximate reweighted bethe marginals can be obtained by minimizing   with respect to @xmath31 , which is a convex problem suitable for fw .",
    "the technique was first used in  , using a generic lp solver for map .    in appendix",
    "[ sec : fw - inf ] , we provide experiments on crfs defined over bipartite matchings ( see section  [ sec : perms ] ) , demonstrating the favorable accuracy and speed of fw - based inference versus the bp algorithm of   and an instance of the perturb - and - map framework designed specifically for matchings  . we find that fw outperforms perturb - and - map in terms of both accuracy and convergence speed .",
    "fw and bp minimize the same objective , since the bethe entropy is convex for matchings , so we compare them purely in terms of speed .",
    "we find that fw is preferable to bp in most regimes , except when extremely precise optimization is required .",
    "we apply the mle - struct framework to a variety of exponential family models defined over different combinatorial structures , including grid crfs for image segmentation , bipartite matchings in vision applications , and general perfect matchings for a university roommate assignment problem . for crfs , map inference is intractable , but we can efficiently solve the lp relaxation , which is equivalent to map inference over the local polytope with qpbo  .",
    "this means our estimated pseudomarginals will not be globally consistent , but the procedure can still yield accurate predictions  . for the matching problem , we use efficient max - flow solvers to obtain exact map solutions ( i.e. , over the marginal polytope )  . in this case , our estimated pseudomarginals will be globally consistent .",
    "appendix  [ app : expt - details ] details the data sources , feature extraction , and machine setup .",
    "we first consider the problem of learning distributions over perfect matchings of a given graph . for a graph @xmath59 and edge weights @xmath60 , the probability of observing a particular",
    "matching is @xmath61 where @xmath62 is the adjacency matrix of a perfect matching in @xmath63 , @xmath64 is a weighted adjacency matrix of @xmath63 , and @xmath65 is the partition function .",
    "each entry of @xmath64 is a function of edge - wise features .",
    "our formulation can be relaxed to distributions over all matchings by allowing @xmath62 to correspond to the adjacency matrix of any ( not necessarily perfect ) matching .    when @xmath63 is bipartite , the partition function is the permanent of the matrix of edge weights and is thus # p - hard to compute  .",
    "although the partition function can be computed to any given accuracy using a fully polynomial randomized approximation scheme , such algorithms are impractical for graphs of any significant size .    in practice",
    ", @xmath64 is unknown and must be learned from data .",
    "we can learn a generative model by estimating @xmath64 directly , or a conditional model by first assuming that @xmath64 is the linear combination of some feature maps and then learning the weights .",
    "for concreteness , suppose we have @xmath66 features , and for the @xmath67 feature we have a @xmath68 matrix @xmath69 .",
    "let @xmath70 be our model parameters , so that the weight on edge @xmath71 is @xmath72",
    ". then the conditional likelihood is @xmath73      theorem  [ thm : conv ] is proven in appendix  [ app : conv ] . by inclusion",
    ", it implies   is also convex over the marginal polytope .",
    "this generalizes earlier known results of convexity for bipartite perfect matchings . due to the convexity of the bethe entropy and the availability of high - quality maximum - weight matching solvers , algorithm",
    "[ alg : fw - learning ] is well - suited to the approximate mle task .",
    "a derivation of the specific form of   for matchings and a technique for making the associated line search particularly efficient by precomputing certain data - dependent terms can be found in appendix  [ app : fwmatch ] .",
    "we begin with a synthetic experiment using the flexibility of mle - struct to analyze the accuracy of various entropy approximations for matchings .",
    "we sample @xmath74 bipartite matchings from the distribution  .",
    "we explore two choices of the weight matrix @xmath64 : one in the _ high snr regime _ with @xmath75 on the off - diagonals and @xmath76 on the diagonals , and one in the _ low snr regime _ @xmath77 off - diagonal and @xmath76 on - diagonal .",
    "our problems are small enough that we can compute exact partition functions and their gradients with ryser s algorithm  .",
    "hence , we can perform exact mle with gradient descent .",
    "we can also evaluate the true ( regularized ) likelihood of our estimates .",
    "we ran algorithm  [ alg : fw - learning ] with @xmath78 and called this result the _ bethe estimator .",
    "_ in addition , setting @xmath79 for this problem guarantees a concave entropy approximation and an upper bound on the partition function .",
    "we also ran algorithm  [ alg : fw - learning ] at this setting and denote the result as the _ rw estimator_.    figure  [ fig : synmat](a ) displays the average regularized log - likelihood of each estimator , higher being better and the _ exact _ curve being an upper bound . in both low and high snr regimes , the bethe estimator is superior to the rw estimator . reweighted entropies such as the one chosen here are known to perform poorly as estimators of the true partition function as compared to belief propagation .",
    "interestingly , although the objective values of the estimates are different in each case , in the low snr regime , all estimation methods produce about the same likelihood .",
    "our framework can also be used to bound the value of the true likelihood .",
    "first , since @xmath80 provides an upper bound on @xmath81  , we have @xmath82 for any @xmath64 .",
    "second , for matchings , we have @xmath83 .",
    "therefore , we have      for all @xmath64 .",
    "moreover , @xmath85 and @xmath86 are _ global _ bounds on the maximum likelihood , so the inequalities also hold at their respective optima .",
    "that is , @xmath87 where @xmath88 is the rw estimator , @xmath89 is the regularized mle , and @xmath90 is the bethe estimator .",
    "we plot the quantities of   in figure  [ fig : synmat](b ) .",
    "we can also use   to obtain upper and lower bounds of @xmath91 and @xmath92 by using fw for inference to compute @xmath93 and @xmath94 , since @xmath95 and @xmath96 will already be available upon convergence of algorithm  [ alg : fw - learning ] .",
    "appendix  [ app : expt - details ] shows these results .",
    "we now apply the bipartite matching model to a graph matching problem arising in computer vision over the cmu _ house _ and _ hotel _ image sequences .",
    "we follow the setup of .",
    "the data consist of 111 frames of a toy house and 101 separate frames of a toy hotel , each rotated a fixed angle from its predecessor .",
    "each frame was hand - labeled with the same 30 landmark points .",
    "we consider pairs of images at a fixed number of frames apart ( the _ gap _ ) , which we divide into training , validation , and testing sets following the same splits as .",
    "we measure the average hamming error between the predicted matching ( map estimate using our learned parameters ) and the ground truth .",
    "we compare our algorithm against the linear+learning method of , which fits the parameters of a linear model using the same features as our algorithm but with a hinge loss objective .",
    "the results of the experiments with reweighting parameters @xmath97 are described in figure [ fig : hotels ] . for each subsequence , we chose the regularization parameter via cross - validation .",
    "both methods perform comparably , with our method doing slightly better on the houses and the method of doing slightly better on the hotels .",
    "we also compared the performance of our algorithm with different reweighting parameters @xmath27 .",
    "figure  [ fig : rho ] shows the results for the house data when the gap is @xmath98 for various choices of @xmath27 .",
    "we observed little difference in test error as @xmath27 varies : this was confirmed over synthetic as well as real data .",
    "as a result , we did not tune @xmath27 for different data / problem setups .",
    "figure  [ fig : mismatch ] illustrates one advantage of learning a probabilistic model over a discriminative model : the pseudomarginals indicate the model s confidence in a prediction . in many cases ,",
    "when the algorithm made the wrong prediction , two edges incident to a specific node had relatively high pseudomarginal probabilities .",
    "in these cases , the errors were not completely unfounded .",
    "similar image parts were matched , albeit incorrectly .",
    "algorithm  [ alg : fw - learning ] permits fast and simple approximate mle in problems where it was previously quite difficult .",
    "namely , we found that using standard bp approaches   to compute marginals for the standard double - loop mle approach was very unstable for this problem because bp often failed to converge after taking several gradient steps .",
    "in later sections , we juxtapose algorithm  [ alg : fw - learning ] with alternative approximate mle approaches ."
  ],
  "abstract_text": [
    "<S> many machine learning tasks can be formulated in terms of predicting structured outputs . in frameworks such as the structured support vector machine ( svm - struct ) and the structured perceptron , </S>",
    "<S> discriminative functions are learned by iteratively applying efficient maximum a posteriori ( map ) decoding . </S>",
    "<S> however , maximum likelihood estimation ( mle ) of probabilistic models over these same structured spaces requires computing partition functions , which is generally intractable . </S>",
    "<S> this paper presents a method for learning discrete exponential family models using the bethe approximation to the mle . </S>",
    "<S> remarkably , this problem also reduces to iterative ( map ) decoding . </S>",
    "<S> this connection emerges by combining the bethe approximation with a frank - wolfe ( fw ) algorithm on a convex dual objective which circumvents the intractable partition function . </S>",
    "<S> the result is a new single loop algorithm mle - struct , which is substantially more efficient than previous double - loop methods for approximate maximum likelihood estimation . </S>",
    "<S> our algorithm outperforms existing methods in experiments involving image segmentation , matching problems from vision , and a new dataset of university roommate assignments . </S>"
  ]
}