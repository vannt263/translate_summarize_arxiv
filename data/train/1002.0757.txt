{
  "article_text": [
    "we resolve two open problems from @xcite concerning universal codes of the predictive plug - in type , also known as `` prequential '' codes .",
    "these codes were introduced independently by rissanen @xcite in the context of mdl learning and by dawid @xcite , who proposed them as probability forecasting strategies rather than directly as codes .",
    "roughly , the plug - in codes relative to parametric model @xmath4 work by sequentially coding each outcome @xmath5 based on an an estimator @xmath6 for all previous outcomes @xmath7 , leading to codelength ( log loss ) @xmath8 , where @xmath9 denotes the probability density or mass function indexed by @xmath10 . if we take @xmath11 equal to the ml ( maximum likelihood ) estimator , we call the resulting code the `` ml plug - in code ''",
    ".    there are many papers about the redundancy and/or expected regret for the ml plug - in codes , for a large variety of models including multivariate exponential families , arma processes , regression models and so on .",
    "examples are @xcite . in all these papers the ml plug - in code is shown to achieve an asymptotic expected regret or redundancy of @xmath12 , where @xmath13 is the number of parameters of the model and @xmath14 is the sample size .",
    "this matches the behaviour of the shtarkov , bayesian and two - part universal codes and is optimal in several ways , see @xcite ; since the ml plug - in codes are often easier to calculate than any of these other three codes , this appears to be a strong argument for using them in practical data compression and mdl - style model selection . yet",
    ", more recently @xcite , it was shown that , at least for single - parameter exponential family models , when the data are generated i.i.d .",
    "@xmath15 , the redundancy in fact grows as @xmath16 , where @xmath17 is the distribution in @xmath0 that is closest to @xmath18 in kullback - leibler divergence , i.e. it minimizes @xmath19 ; a related result for linear regression is in @xcite .",
    "in contrast to the other cited works , @xcite do not assume that @xmath20 : the model may be _ misspecified_. yet _ if _",
    "@xmath21 , then we have @xmath22 so that the redundancy grows like it does in the other universal models .",
    "but when @xmath23 , the shtarkov , bayes and universal codes typically still achieve asymptotic expected regret @xmath24 , whereas the plug - in codes behave differently .",
    "@xcite show that this leads to substantially inferior performance of the plug - in codes in practical mdl model selection .",
    "in general , the estimator for @xmath3 based on @xmath25 need not be an element of the parametric model @xmath3 ; for example , we may think of the bayesian predictive distribution as an estimator relative to @xmath3 , even though it is `` out - model '' : rather than a single element of @xmath26 , it is a mixture of distributions in @xmath3 , each weighted by their posterior density ( see section  [ sec : squashed ] for an example ) .",
    "we may thus re - interpret bayesian universal codes as prequential codes based on `` out - model '' estimators .",
    "from now on , we reserve the term `` prequential plug-_in _ code '' , abbreviated to just `` plug - in code '' , for codes based on `` _ in_-model '' estimators , i.e. estimators required to lie within @xmath26 . when we call a code just `` prequential '' , it may be sequentially constructed from either in - model or out - model estimators .",
    "@xcite established a nonstandard redundancy , different from @xmath27 , only for ml and closely related plug - in codes .",
    "? * open problem nr .",
    "2 ) conjectured that a similar result should hold for _ all _ plug - in codes , even if they are based on in - model estimators very different from the ml estimator : the conjecture was that _ no _ plug - in code can achieve guaranteed redundancy of @xmath27 if data are i.i.d . @xmath15 and @xmath28 .",
    "our first main result , theorem  [ thm : no_plugin ] below , shows that , essentially , this conjecture is true for general one - parameter exponential families @xmath29 . specifically , the redundancy can become much larger than @xmath30 if @xmath31 .    the second related conjecture ( * ? ? ?",
    "* open problem nr .",
    "3 ) concerned the fact that for the normal location family with constant variance @xmath32 , the bayesian predictive distribution based on data @xmath25 and a normal prior looks `` almost '' like an in - model estimator for @xmath25 , and hence the resulting code looks `` almost '' like a plug - in code : the bayes predictive distribution is equal to the normal distribution for @xmath33 with mean equal to the ml estimator @xmath34 but with a variance of order @xmath35 , i.e. slightly larger than the variance @xmath32 of @xmath36 ( see section  [ sec : squashed ] for details ) .",
    "since the bayesian predictive distribution does achieve the redundancy @xmath30 even if @xmath37 , this means that if @xmath26 is the normal location family , then there does exist an `` almost '' in - model estimator ( i.e. a slight modification of the ml estimator ) that does achieve @xmath30 even if @xmath38 .",
    "although this example does not extend straightforwardly to other exponential families , @xcite conjectured that there should nevertheless be some general definition for `` almost '' in - model estimators that achieve @xmath27 redundancy even if @xmath31 . here",
    "we show that this conjecture is true , at least if @xmath39 : we propose the _ slightly squashed _",
    "ml estimator , a modification of the ml estimator that puts it slightly outside model @xmath3 , and in theorem  [ thm : robustml ] we show that this estimator achieves @xmath30 redundancy even if @xmath37 .",
    "this result is important in practice since , in contrast to the bayesian predictive distribution , the slightly squashed ml estimator is in general just as easy to compute as the ml estimator itself .",
    "throughout this text we use nats rather than bits as units of information . a sequence of outcomes @xmath40 is abbreviated to @xmath41 .",
    "we write @xmath42 as a shorthand for @xmath43 , the expectation of @xmath44 under distribution @xmath18 .",
    "when we consider a sequence of @xmath14 outcomes independently distributed @xmath15 , we use @xmath42 even as a shorthand for the expectation of @xmath45 under the @xmath14-fold product distribution of @xmath18 .",
    "finally , @xmath46 denotes the probability mass function of @xmath18 in case @xmath44 is discrete - valued , and it denotes the density of @xmath18 , in case @xmath44 takes its value in a continuum .",
    "when we write ` density function of @xmath44 ' , then , if @xmath44 is discrete - valued , this should be read as ` probability mass function of @xmath44 ' .",
    "note however that in our second main result , theorem  [ thm : robustml ] we do not assume that the data - generating distribution @xmath18 admits a density .",
    "let @xmath47 be a set of outcomes , taking values either in a finite or countable set , or in a subset of @xmath13-dimensional euclidean space for some @xmath48 .",
    "let @xmath49 be a random variable on @xmath47 , and let @xmath50 be the range of @xmath51 .",
    "exponential family models are families of distributions on @xmath47 defined relative to a random variable @xmath51 ( called ` sufficient statistic ' ) as defined above , and a function @xmath52 .",
    "let @xmath53 ( the integral to be replaced by a sum for countable @xmath47 ) , and @xmath54 .",
    "[ def : expfam ] the _ single parameter exponential family _ @xcite with _ sufficient statistic _ @xmath51 and _ carrier _",
    "@xmath55 is the family of distributions with densities @xmath56 , where @xmath57 .",
    "@xmath58 is called the _",
    "natural parameter space_. the family is called _ regular _ if @xmath58 is an open interval of @xmath59 .    in the remainder of this text we only consider single parameter , regular exponential families , but this qualification will henceforth be omitted .",
    "examples include the poisson , geometric and multinomial families , and the model of all gaussian distributions with a fixed variance or mean .",
    "the statistic @xmath60 is sufficient for @xmath61 @xcite .",
    "this suggests reparameterizing the distribution by the expected value of @xmath51 , which is called the _",
    "mean value parameterization_. the function @xmath62 $ ] maps parameters in the natural parameterization to the mean value parameterization .",
    "it is a diffeomorphism ( it is one - to - one , onto , infinitely often differentiable and has an infinitely often differentiable inverse ) @xcite .",
    "therefore the mean value parameter space @xmath63 is also an open interval of @xmath59 .",
    "we write @xmath64 where @xmath65 is the distribution with mean value parameter @xmath66 .",
    "we are now ready to define the plug - in universal model .",
    "this is a distribution on infinite sequences @xmath67 , recursively defined in terms of the distributions of @xmath68 conditioned on @xmath69 , for all @xmath70 . in the definition ,",
    "we use the notation @xmath71 . note that we use the term `` model '' both for a single distribution ( `` plug - in universal model '' , a common phrase in information theory ) and for a family of distributions ( `` statistical model '' , a common phrase in statistics ) .",
    "[ def : preq ] let @xmath72 be an exponential family with mean value parameter domain @xmath63 . given @xmath0 , constant @xmath73 and a sequence of functions @xmath74 , such that @xmath75 , we define the _ plug - in universal model _ ( or _ plug - in model _ for short ) @xmath76 by setting , for all @xmath14 , all @xmath77 : @xmath78 where @xmath79 is the density / mass function of @xmath80 conditional on @xmath69 .",
    "we usually refer to plug - in universal model in terms of the codelength function of the corresponding plug - in universal code : @xmath81    the most important plug - in model is the ml ( _ maximum likelihood _ ) plug - in model , defined as follows :    [ def : preqb ] given @xmath0 and constants @xmath82 and @xmath83 , we define the _ ml plug - in model _",
    "@xmath84 by setting , for all @xmath14 , all @xmath77 : @xmath85 where @xmath86    to understand this definition , note that for exponential families , for any sequence of data , the ordinary maximum likelihood parameter is given by the average @xmath87 of the observed values of @xmath51 @xcite . here",
    "we define our plug - in model in terms of a slightly modified maximum likelihood estimator that introduces a ` fake initial outcome ' @xmath88 with multiplicity @xmath89 in order to avoid infinite code lengths for the first few outcomes ( a well - known problem sometimes called the `` inherent singularity '' of predictive coding @xcite ) and to ensure that the plug - in ml code of the first outcome is well - defined . in practice we can take @xmath90 but our result holds for any @xmath91 .",
    "[ def : red ] following @xcite , we define _ relative redundancy _ with respect to @xmath18 of a code @xmath76 that is universal on a model @xmath0 , as : @xmath92 - \\inf_{\\mu \\in { \\ensuremath{\\theta_{\\text{mean}}}}}e_p [ -    \\ln { { m_\\mu}}(z^n)],\\ ] ] where @xmath93 is the length function of @xmath76 .",
    "we use the term _ relative redundancy _ rather than just _",
    "redundancy _ to emphasize that it measures redundancy relative to the element of the model that minimizes the codelength rather than to @xmath18 , which is not necessarily an element of the model . from now on ,",
    "we only consider @xmath18 under which the data are i.i.d . under this condition ,",
    "let @xmath94 be the element of @xmath0 that minimizes kl divergence to @xmath18 : @xmath95,\\ ] ] where the equality follows from the definition of kl divergence .",
    "if @xmath94 exists , it is unique , and if @xmath96 \\in { \\ensuremath{\\theta_{\\text{mean}}}}$ ] , then @xmath97 $ ] ( * ? ? ?",
    "17 ) , and the relative redundancy satisfies @xmath98 -   e_{p}[- \\ln { { m_{{\\mu^*}}}}(z^n)].\\ ] ]",
    "the three major types of universal codes , bayes , nml and 2-part , achieve relative redundancies that are ( in an appropriate sense ) close to optimal . specifically , under the conditions on @xmath0 described above , and if data are i.i.d .",
    "@xmath15 , then , under some mild conditions on @xmath18 , these universal codes satisfy : @xmath99 ( where the @xmath100 may depend on @xmath66 and the universal code used ) , whenever @xmath20 or @xmath101 .",
    "( [ eq : bic ] ) is the famous ` @xmath13 over @xmath102 log @xmath14 formula ' ( @xmath39 in our case ) , refinements of which lie at the basis of practical approximations to mdl learning @xcite .",
    "while it is known that for @xmath20 , the fourth major type of universal code , the ml plug - in code , satisfies ( [ eq : bic ] ) as well , it was shown by @xcite that when @xmath18 is not in the model , the ml plug - in code may behave suboptimally .",
    "specifically , its relative redundancy satisfies : @xmath103 and can be significantly larger than ( [ eq : bic ] ) , when the variance of @xmath18 is large .    in this paper",
    ", we show that not only the ml plug - in code , but _ every _ plug - in code may behave suboptimally , when @xmath104 . in other words , modifying the ml estimator @xmath105 or introducing any other sequence of estimators @xmath106 , and constructing the plug - in code based on that sequence will not help to satisfy ( [ eq : bic ] ) .",
    "thus the optimal redundancy can only be achieved by codes outside @xmath0 , unless @xmath0 is the bernoulli family ( since we assume the data are i.i.d .",
    ", in the bernoulli case we must have that @xmath107 ; but the bernoulli case is the _ only _ case in which we must have @xmath107 ) .    our main result , theorem [ thm : no_plugin ] , concerns the case in which @xmath18 is itself a member of some exponential family @xmath108 , but @xmath108 is in general different than @xmath0 .",
    "then , the suboptimal behavior of plug - in codes follows immediately as corollary [ coll : no_plugin ] , stated further below .",
    "[ thm : no_plugin ] let @xmath109 and @xmath110 be single parameter exponential families with the same sufficient statistic @xmath51 and mean - value parameter space @xmath63 .",
    "let @xmath76 denote any plug - in model with respect to @xmath0 based on the sequence of estimators @xmath111 .",
    "then , for lebesgue almost all @xmath112 ( i.e. all apart from a lebesgue measure zero set ) , for @xmath113 i.i.d .",
    "@xmath114 : @xmath115    _ ( rough sketch ; a detailed proof is in the appendix ) _ the proof is based on a theorem stated by rissanen @xcite ( see also @xcite , theorem 14.2 ) , a special case of which says the following .",
    "let @xmath116 be a closed , non - degenerate interval , @xmath108 be defined as above , @xmath117 be a joint distribution of @xmath14 outcomes generated i.i.d . from @xmath118 , @xmath119 be an arbitrary probabilistic source , i.e. a distribution on infinite sequences @xmath120 , and let @xmath121 be its restriction to the first @xmath14 outcomes .",
    "define : @xmath122 then for lebesgue almost all @xmath123 , @xmath124 .",
    "we apply rissanen s theorem by constructing a source @xmath119 , specifying the conditional probabilities @xmath125 , for every @xmath126 .",
    "we now have : @xmath127 \\nonumber \\\\",
    "\\sum_{i=1}^{n-1 } e_{p_{{\\mu^ * } } } \\left [ d(p_{{{\\mu^ * } } } \\| p_{{{{\\bar\\mu}_i } } } ) \\right].\\end{aligned}\\ ] ] to see how ( [ eq : kl_deriv ] ) is related to our case , let us first rewrite the redundancy in a more convenient form : @xmath128.\\ ] ] the derivation of ( [ eq : redundancy_convienient ] ) make use of a standard result in the theory of exponential families and can be found e.g. in @xcite .    comparing ( [ eq : kl_deriv ] ) and ( [ eq : redundancy_convienient ] )",
    ", we see that although in both expressions , the expectation is taken with respect to @xmath129 , ( [ eq : kl_deriv ] ) is a statement about kl divergence between the members of @xmath108 , while ( [ eq : redundancy_convienient ] ) speaks about the members of @xmath0 .",
    "the trick , which allows us to relate both expressions , is to examine their second - order behavior . by expanding @xmath130 into a taylor series around @xmath131 ,",
    "we get : @xmath132 where we abbreviated @xmath133 .",
    "the term @xmath134 is zero , since @xmath135 as a function of @xmath66 has its minimum at @xmath136 @xcite . as is well - known @xcite , for exponential families the term @xmath137 coincides precisely with the fisher information @xmath138 evaluated at @xmath66 .",
    "another standard result @xcite for the mean - value parameterization says that for all @xmath66 , @xmath139 .",
    "therefore , we get @xmath140 , and similarly , @xmath141 , so that @xmath142 , and using ( [ eq : kl_deriv ] ) and ( [ eq : redundancy_convienient ] ) : @xmath143 the last step of the proof is to use rissanen s theorem and conclude that @xmath144 is equal to @xmath145 for lebesgue almost all @xmath123 , and thus for lebesgue almost all @xmath112 .",
    "we now use theorem  [ thm : no_plugin ] to show that the redundancy of plug - in codes is suboptimal for all exponential families which satisfy the following very weak condition :    [ cnd : dispersion_model ] let @xmath109 be a single parameter exponential family with sufficient statistic @xmath51 and mean - value parameter space @xmath63 .",
    "we require that there exists another single - parameter exponential family @xmath110 with the same mean - value parameter space as @xmath0 , but with strictly larger variance than @xmath0 for every @xmath146 .",
    "the condition [ cnd : dispersion_model ] is widely satisfied among known exponential families .",
    "when @xmath147 $ ] , we define @xmath118 to be a `` scaled '' bernoulli model , by putting all probability mass on @xmath148 in such a way that @xmath149 .",
    "it is easy to show , that such distribution has the highest variance among all distributions defined on @xmath150 $ ] with a given mean value @xmath66 ; therefore @xmath151 , unless @xmath0 is a `` scaled '' bernoulli itself . when @xmath152 , @xmath108 can be chosen to be a normal family with fixed , sufficiently large variance @xmath32 . for @xmath153 ,",
    "@xmath108 can be taken to be a gamma family with sufficiently large scale parameter . when @xmath154 , @xmath108 can be taken to be negative binomial ( with expected `` number of successes '' sufficiently small ) .",
    "thus , we see that for all commonly used exponential families , except for bernoulli , condition [ cnd : dispersion_model ] holds . on the other hand if @xmath0 is bernoulli , corollary [ coll : no_plugin ] is no longer relevant anyway , since then @xmath18 must lie in @xmath3 .",
    "[ coll : no_plugin ] let @xmath109 a single parameter exponential family with sufficient statistic @xmath51 and mean - value parameter space @xmath63 , satisfying condition [ cnd : dispersion_model ] .",
    "let @xmath76 denote any plug - in model with respect to @xmath0 based on any sequence of estimators @xmath155 .",
    "then , there exists a family of distributions @xmath156 , such that for lebesgue almost all @xmath112 , for @xmath113 i.i.d .",
    "@xmath157 : @xmath158 so that the set of @xmath131 for which @xmath76 achieves the regret @xmath159 is a set of lebesgue measure zero .",
    "immediate from theorem [ thm : no_plugin ] .",
    "we showed that every plug - in code , including the ml plug - in code , behaves suboptimally for 1-parameter families @xmath0 unless @xmath0 is bernoulli .",
    "this fact does not , however , exclude the possibility that a small modification of the ml plug - in code , which puts the predictions slightly outside @xmath0 , will lead to the optimal redundancy ( [ eq : bic ] ) . an argument supporting this claim comes from considering the bayesian predictive distribution when @xmath0 is the normal family with fixed variance @xmath32 . in this case , the bayesian code based on prior @xmath160 has a simple form @xcite : @xmath161 where @xmath162 is the density of normal distribution @xmath163 , @xmath164 thus , the bayesian predictive distribution is itself a gaussian with mean equal to the modified maximum likelihood estimator ( with @xmath165 ) , albeit with a slightly larger variance @xmath35 .",
    "this shows that for the normal family with fixed variance , there exists an `` almost '' in - model code , which satisfies ( [ eq : bic ] ) .",
    "this led @xcite to conjecture that something similar holds for general exponential families . here",
    "we show that this is indeed the case : we propose a simple modification of the ml plug - in universal model , obtained by predicting @xmath80 using a slightly `` squashed '' version @xmath166 of the ml estimator @xmath167 , defined as : @xmath168 where @xmath105 is defined as in ( [ eq : ml_estimator ] ) and @xmath169 is the fisher information for model @xmath0 . note that @xmath170 represents a valid probability density : it is non - negative due to @xmath171 ( property of exponential families ) , and it is properly normalized : @xmath172 where the final equality follows because for exponential families , @xmath173 . while @xmath174 , we have @xmath175 , i.e. @xmath176 is `` almost '' in - model estimator .",
    "[ def : preqc ] given @xmath0 , constants @xmath82 and @xmath83 , we define the _ slightly squashed ml prequential model _ @xmath76 by setting , for all @xmath14 , all @xmath177 : @xmath178    where @xmath176 is the slightly squashed ml estimator as above .",
    "the codelengths of the corresponding slightly squashed ml prequential code are not harder to calculate than those of the ordinary ml plug - in model and in some cases they are easier to calculate than the lengths of the bayesian universal code . on the other hand , we show below that the slightly squashed ml code always achieves the optimal redundancy , satisfying ( [ eq : bic ] ) .",
    "[ thm : robustml ] let @xmath113 be i.i.d.@xmath15 , with @xmath96 = { { \\mu^*}}$ ] .",
    "let @xmath0 be a single parameter exponential family with sufficient statistic @xmath51 and @xmath131 an element of the mean value parameter space .",
    "let @xmath76 denote the slightly squashed ml model with respect to @xmath0 .",
    "if @xmath0 and @xmath18 satisfy condition [ condition ] below , then : @xmath179    [ condition ]",
    "we require that the following holds both for @xmath180 and @xmath181 :    * if @xmath182 is unbounded from above then there is a @xmath183 such that the first @xmath13 moments of @xmath182 exist under @xmath18 , that @xmath184 , @xmath185 and that either @xmath169 is constant or @xmath186 . *",
    "if @xmath182 is bounded from above by a constant @xmath187 then @xmath188 , @xmath189 , and @xmath169 are polynomial in @xmath190 .",
    "the usefulness of theorem [ thm : robustml ] depends on the validity of condition [ condition ] among commonly used exponential families . as can be seen from figure [ fig : condition ] , for some standard exponential families ,",
    "our condition applies whenever the fourth moment of @xmath18 exists .",
    "_ ( of theorem  [ thm : robustml ] ; rough sketch  a detailed proof is in the appendix ) _ we express the relative redundancy of the slightly squashed ml plug - in code @xmath76 by the sum of the relative redundancy of the ordinary ml plug - in code @xmath84 and the difference in expected codelengths between @xmath76 and @xmath84 : @xmath191 -   e_{p}[- \\ln { { m_{{\\mu^*}}}}(z^n ) ] = \\\\",
    "e_{p}[l_u(z^n ) - l_{{\\hat{u}}}(z^n ) ] + { { \\mathcal{r}}}_{{\\hat{u}}}(n ) = \\\\",
    "e_{p}[l_u(z^n ) - l_{{\\hat{u}}}(z^n ) ] + \\frac{1}{2 } \\frac{{{\\textnormal{var}}}_p x}{{{\\textnormal{var}}}_{m_{\\mu^ * } } x } \\ln n + o(1 ) , \\end{array } \\vspace*{-3pt}\\ ] ] where the last equality follows from ( [ eq : redundancyml ] ) .",
    "we have : @xmath192 since @xmath193 , we get @xmath194 . denoting @xmath195 , we also get @xmath196 .",
    "next , we consider @xmath197 $ ] : @xmath198 = \\frac{1}{2i } e_p \\left [   i_{{\\cal m}}({{{\\hat\\mu}_i}})(x_{i+1 } - { { \\mu^*}}+ { { \\mu^*}}- { { { \\hat\\mu}_i}})^2 \\right ] = \\\\ \\frac{1}{2i } e_p \\left[i_{{\\cal m}}({{{\\hat\\mu}_i}})\\left ( { { \\textnormal{var}}}_{p_{{\\mu^*}}}x + ( { { \\mu^*}}- { { { \\hat\\mu}_i}})^2   \\right ) \\right ]   = \\\\ \\frac{1}{2i } \\left({{\\textnormal{var}}}_{p_{{\\mu^*}}}x e_p \\left[i_{{\\cal m}}({{{\\hat\\mu}_i}})\\right ] + e_p \\left[i_{{\\cal m}}({{{\\hat\\mu}_i } } ) ( { { \\mu^*}}- { { { \\hat\\mu}_i}})^2\\right ] \\right ) . \\end{array}\\ ] ] the second term @xmath199 $ ] is @xmath200 as @xmath201 = o(i^{-1})$ ] and @xmath202 = i_{{\\cal m}}({{\\mu^ * } } ) + o(i^{-1})$ ] ( follows from expanding @xmath203 up to the first order around @xmath131 ) . similarly , the first term is @xmath204 .",
    "thus , using @xmath205 , we finally get : @xmath206 = -e_p[v_i ] + o(i^{-2 } ) = \\frac{1}{2i } \\frac{{{\\textnormal{var}}}_{p_{{\\mu^*}}}x}{{{\\textnormal{var}}}_{m_{{\\mu^*}}}x } + o(i^{-2}).\\ ] ] taking all together , we see that the terms @xmath207 cancel and we finally get @xmath208 condition [ condition ] is necessary to ensure that all taylor expansions above hold .",
    "[ fig : condition ]    l@c@c@c distribution & @xmath209 & @xmath210 & @xmath189 +   + bernoulli & @xmath211 & @xmath212 & @xmath213 + poisson & @xmath214 & @xmath215 & @xmath216 + geometric & @xmath217 & @xmath218 & @xmath219 + gamma ( fixed @xmath13 ) & @xmath220 & @xmath221 & @xmath222 + normal ( fixed mean ) & @xmath223 & @xmath224 & @xmath225 + normal ( fixed variance ) & @xmath32 & @xmath226 & @xmath226 +",
    "in future work , we hope to extend our results concerning the slightly squashed ml estimator to the multi - parameter case and establish almost - sure variation of theorem  [ thm : robustml ] .",
    "we also plan to analyze the estimator in the individual sequence framework , along the lines of @xcite .",
    "10 [ 1]#1 url@rmstyle [ 2]#2    p.  grnwald , _ the minimum description length principle_.1em plus 0.5em minus 0.4emcambridge , ma : mit press , 2007 .",
    "j.  rissanen , `` universal coding , information , prediction and estimation , '' _ ieee transactions on information theory _ , vol .",
    "30 , pp . 629636 , 1984 .",
    "a.  dawid , `` present position and potential developments : some personal views , statistical theory , the prequential approach , '' _ j. royal stat.soc . , ser .",
    "a _ , vol .",
    "147 , no .  2 ,",
    "pp . 278292 , 1984 .",
    "j.  rissanen , `` a predictive least squares principle , '' _ i m a journal of mathematical control and information _ , vol .  3 , pp .",
    "211222 , 1986 .",
    "l.  gerenscer , `` order estimation of stationary gaussian arma processes using rissanen s complexity , '' computer and automation institute of the hungaian academy of sciences , tech .",
    "rep . , 1987 .",
    "l.  li and b.  yu , `` iterated logarithmic expansions of the pathwise code lengths for exponential families , '' _ ieee transactions on information theory _ , vol .  46 , no .  7 , pp . 26832689 , 2000 .",
    "a.  barron , j.  rissanen , and b.  yu , `` the minimum description length principle in coding and modeling , '' _ ieee transactions on information theory _ ,",
    "44 , no .  6 , pp . 27432760 , 1998 , special commemorative issue : information theory : 1948 - 1998 .    s.  de rooij and p.  d. grnwald , `` mdl model selection using the ml plug - in code , '' in _ proceedings of the 2005 ieee international symposium on information theory ( isit 2005 ) _ , adelaide , australia , 2005",
    ".    p.  d. grnwald and s.  de  rooij , `` asymptotic log - loss of prequential maximum likelihood codes , '' in _ proc .",
    "of the 18th annual conference on computational learning theory ( colt 2005 ) _ , 2005 , pp .",
    "652667 .",
    "s.  de rooij and p.  d. grnwald , `` an empirical study of mdl model selection with infinite parametric complexity , '' _ journal of mathematical psychology _ , vol .",
    "50 , no .  2 ,",
    "180192 , 2006 .",
    "c.  wei , `` on predictive least squares principles , '' _ the annals of statistics _ , vol .",
    "20 , no .  1 ,",
    "pp . 142 , 1990 .",
    "o.  barndorff - nielsen , _ information and exponential families in statistical theory_.1em plus 0.5em minus 0.4emchichester , uk : wiley , 1978 .",
    "j.  takeuchi and a.  r. barron , `` robustly minimax codes for universal data compression , '' in _ proceedings of the twenty - first symposium on information theory and its applications ( sita 98 ) _ , gifu , japan , 1998 .",
    "j.  rissanen , `` stochastic complexity and modeling , '' _ the annals of statistics _ , vol .",
    "14 , pp . 10801100 , 1986 .",
    "n.  cesa - bianchi and g.  lugosi , _ prediction , learning and games_.1em plus 0.5em minus 0.4emcambridge university press , 2006 .",
    "m.  raginsky , r.  f. marcia , s.  jorge , and r.  willett , `` sequential probability assignment via online onvex programming using exponential families , '' in _ proceedings of the 2009 ieee international symposium on information theory _ ,",
    "seoul , korea , 2009 .",
    "before we show the main result , we need to prove the following lemmas .",
    "[ lem : rissanen_based ] let @xmath109 and @xmath110 be single parameter exponential families with the same sufficient statistic @xmath51 and mean - value parameter space @xmath63 .",
    "let @xmath116 be any non - degenerate closed interval .",
    "let @xmath113 be i.i.d .",
    "@xmath227 for some @xmath228 .",
    "let @xmath229 be a sequence of estimators , such that @xmath230 and @xmath231 for all @xmath232 .",
    "then , for lebesgue almost all @xmath123 : @xmath233 } { \\ln n } \\geq \\underline{v}_{{{\\cal p}}}\\ ] ] where @xmath234 .",
    "the proof is based on a theorem stated by rissanen @xcite ( see also @xcite , theorem 14.2 ) , a special case of which says the following .",
    "let @xmath108 and @xmath235 be defined as above , @xmath117 be a joint distribution of @xmath14 outcomes generated i.i.d . from @xmath118 , @xmath119 be an arbitrary probabilistic source , i.e. a distribution on infinite sequences @xmath120 , and let @xmath121 be its restriction to the first @xmath14 outcomes ( marginalized over @xmath236 ) .",
    "define : @xmath237 then for lebesgue almost all @xmath123 , @xmath238 .",
    "we construct the source @xmath119 by specifying the conditional probabilities : @xmath239 for every @xmath126 .",
    "this definition is valid , because @xmath106 depends only on @xmath240 .",
    "now , we have : @xmath241 \\\\   & = & \\sum_{i=0}^{n-1 } e_{z^{i } \\sim p_{{\\mu^*}}^{(i ) } } \\left [ \\ln p_{{{\\mu^*}}}(z_{i+1 } ) - \\ln q(z_{i+1}|z^{i } ) \\right ] \\\\   & = & \\sum_{i=1}^{n-1 } e_{z^{i } \\sim p_{{\\mu^*}}^{(i ) } } \\left [ d(p_{{{\\mu^ * } } } \\| p_{{{{\\bar\\mu}_i } } } ) \\right ] . \\\\\\end{aligned}\\ ] ] expanding @xmath242 into a taylor series around @xmath131 yeilds : @xmath243 for some @xmath66 between @xmath244 and @xmath131 , where we abbreviated @xmath133 .",
    "the term @xmath134 is zero , since @xmath135 as a function of @xmath66 has its minimum at @xmath136 @xcite . as is well - known @xcite , for exponential families the term @xmath137 coincides precisely with the fisher information @xmath138 evaluated at @xmath66 .",
    "another standard result @xcite for the mean - value parameterization says that for all @xmath66 , @xmath245 therefore ( using shorter notation @xmath246 for @xmath247 ) : @xmath248 \\\\ & \\leq   \\frac{1}{2 } \\frac{1}{\\underline{v}_{{{\\cal p } } } } \\sum_{i=0}^{n-1 } e_{p_{{\\mu^ * } } } \\left [ ( { { { \\bar\\mu}_i}}- { { \\mu^*}})^2 \\right ] .",
    "\\label{eq : squared_approx } \\end{array}\\ ] ] note , that @xmath249 is an infimum of a continuous and positive function on a compact set . from ( [ eq : rissanen_thm ] ) and ( [ eq : squared_approx ] ) we have : @xmath250}{\\frac{1}{2 } \\ln n ' } \\right\\ } \\geq   g_n({{\\mu^ * } } ) \\underline{v}_{{{\\cal p}}},\\ ] ] and thus rissanen s theorem proves the lemma .",
    "[ lemm : variance_ratio ] let @xmath251 , be defined as in lemma [ lem : rissanen_based ] .",
    "let @xmath76 denote any plug - in model with respect to @xmath0 based on a sequence of estimators @xmath155 ( notice that now we do not restrict @xmath244 to be in @xmath235 , as in lemma [ lem : rissanen_based ] ) .",
    "then , for lebesgue almost all @xmath123 : @xmath252}{\\ln n } \\geq \\frac{1}{2 } \\frac{\\underline{v}_{{\\cal p}}}{\\overline{v}_{{\\cal m}}},\\ ] ] for @xmath234 and @xmath253 .",
    "let us denote @xmath254 $ ] .",
    "we define a truncated sequence of estimators @xmath255 as follows :    @xmath256 so that @xmath257 .",
    "note , that @xmath258 , as there exists @xmath259 $ ] such that we can express @xmath260 and @xmath261 is strictly decreasing in @xmath262 @xcite . using this fact and expanding @xmath263 into taylor series as in lemma [ lem : rissanen_based ] , we get : @xmath264 \\geq   e_{p_{{{\\mu^*}}}}[d(m_{{{\\mu^*}}}\\|m_{{{\\bar\\mu}_i } } ' ) ] \\\\   & = & \\frac{1}{2 } e_{p_{{\\mu^ * } } } \\left [ \\frac{({{{\\bar\\mu}_i}}- { { \\mu^*}})^2}{{{\\textnormal{var}}}_{m_\\mu } x } \\right ]   \\geq \\frac{1}{2 } \\frac{1}{\\overline{v}_{{{\\cal m } } } } e_{p_{{\\mu^ * } } } \\left [ ( { { { \\bar\\mu}_i}}- { { \\mu^*}})^2 \\right ] . \\end{aligned}\\ ] ] summing over @xmath265 and using lemma [ lem : rissanen_based ] finishes the proof .",
    "before we prove theorem  [ thm : no_plugin ] , we further need a simple lemma to rewrite the redundancy in a more convenient form :    let @xmath76 and @xmath0 be defined as in theorem  [ thm : no_plugin ] .",
    "we have : @xmath266.\\ ] ] [ lem : redundancy ]    the usefulness of this lemma comes from the fact that the kl divergence @xmath267 is defined as an expectation over @xmath268 rather than @xmath269 .",
    "the proof makes use of a standard result in the theory of exponential families and can be found e.g. in @xcite ( see also related lemma 1 in @xcite ) .    _",
    "( of theorem  [ thm : no_plugin ] ) _ choose any @xmath270 and span around it a non - degenerate closed interval @xmath271 , so that @xmath272 . fix some @xmath273 .",
    "it follows from general properties of exponential families ( see , e.g. , @xcite ) that @xmath274 and @xmath275 are continuous ( with respect to @xmath66 ) , therefore if we choose the interval @xmath276 small enough , we will have @xmath277 , with @xmath278 and @xmath279 .",
    "using lemma [ lemm : variance_ratio ] with @xmath280 , and lemma [ lem : redundancy ] , we have for lebesgue almost all @xmath281 .",
    "@xmath282 note , that w.l.o.g .",
    "@xmath283 can be chosen to have rational ends .",
    "the family of all intervals @xmath284 with rational ends and rational @xmath131 , i.e. @xmath285 \\mid { { \\mu^ * } } , \\mu_0 , \\mu_1 \\in { \\ensuremath{\\theta_{\\text{mean}}}}\\cap { { \\mathbb{q}}}\\}$ ] , is countable and covers @xmath63 , @xmath286 .",
    "therefore , @xmath287 since this holds for every @xmath273 , this also means that @xmath288 for lebesgue almost all @xmath112 . to show this ,",
    "assume the contrary , that the set @xmath289 has positive lebesgue measure , @xmath290 .",
    "let @xmath291 be any sequence of positive numbers converging to @xmath226 and let us define @xmath292 . obviously , @xmath293 , and @xmath294 . from continuity of measure",
    ", we must have @xmath295 for @xmath296 large enough , which is a contradiction with ( [ eq : snow ] ) .",
    "the theorem is proved .",
    "we will make use of the following two theorems , proofs of which can be found in @xcite .            *",
    "if @xmath182 is unbounded from above then there is a @xmath307 such that the first @xmath13 moments of @xmath182 exist under @xmath18 and that @xmath308 . *",
    "if @xmath182 is bounded from above by a constant @xmath187 then @xmath304 is polynomial in @xmath309 .",
    "let us denote @xmath313 .",
    "we distinguish a number of regions in the value space of @xmath314 : let @xmath315 and let @xmath316 for some constant value @xmath317 . if the individual outcomes @xmath51 are bounded on the right hand side by a value @xmath187 then we require that @xmath318 and we define @xmath319 ; otherwise we define @xmath320 for @xmath321 .",
    "now we want to analyze asymptotic behavior of : @xmath322   = \\sum_j p({{\\delta_i}}\\in\\delta_j ) e_p \\left[f(\\mu ) { { \\delta_i}}^s\\mid{{\\delta_i}}\\in\\delta_j\\right].\\ ] ] if we can establish the proper asymptotic behavior @xmath312 for all regions @xmath323 for @xmath324 , then we can use a symmetrical argument to establish the behavior for @xmath325 as well , so it suffices if we restrict ourselves to @xmath324 .",
    "first we show it for @xmath326 . in this case",
    ", the basic idea is that since the remainder @xmath304 is well - defined over the interval @xmath327 , we can bound it by its extremum on that interval , namely @xmath328 .",
    "now we get : @xmath329\\right| ~\\le~ 1\\cdot    e\\left[{{\\delta_i}}^s\\left|f(\\mu)\\right|\\right],\\ ] ] which is less or equal than @xmath330 $ ] . using theorem  [ thm : devavasym ]",
    "we find that @xmath331 $ ] is @xmath312 , which is what we want .",
    "theorem  [ thm : devavasym ] requires that the first four moments of @xmath18 exist , but this is guaranteed to be the case : either the outcomes are bounded from both sides , in which case all moments necessarily exist , or the existence of the required moments is part of the condition on the main theorem .",
    "now we distinguish between the unbounded and bounded cases .",
    "first we assume @xmath51 is unbounded from above . in this case",
    ", we must show , hat : @xmath332 = o(i^{-s/2})\\ ] ] we bound this expression from above .",
    "the @xmath314 in the expectation is at most @xmath333 .",
    "furthermore @xmath334 by assumption , where @xmath335 . depending on @xmath13 and @xmath336",
    ", both boundaries could maximize this function , but it is easy to check that in both cases the resulting function is @xmath337 .",
    "so we bound ( [ eq : unbounded_case ] ) from the above by : @xmath338 since we know from the condition on the main theorem that the first @xmath339 moments exist , we can apply theorem  [ thm : boundprob ] to find that @xmath340 ( since @xmath13 has to be even ) ; plugging this into the equation and simplifying we obtain @xmath341 , which is of order @xmath312 , since the sum @xmath342 converges and @xmath343 .",
    "now we consider the case where the outcomes are bounded from above by @xmath187 .",
    "this case is more complicated , since now we have made no extra assumptions as to existence of the moments of @xmath18 . of course ,",
    "if the outcomes are bounded from both sides , then all moments necessarily exist , but if the outcomes are unbounded from below this may not be true",
    ". to remedy this , we map all outcomes into a new domain in such a way that all moments of the transformed variables are guaranteed to exist .",
    "any constant @xmath344 defines a mapping @xmath345 .",
    "we define the random variables @xmath346 , the initial outcome @xmath347 and the mapped analogues of @xmath131 and @xmath311 , respectively : @xmath348 is defined as the mean of @xmath349 under @xmath18 and @xmath350 . since @xmath351 , we can bound : @xmath352 \\right| \\\\   & \\le & p({{{\\hat\\mu}_i}}-{{\\mu^*}}\\ge a ) \\sup_{{{\\delta_i}}\\in\\delta_1}\\left|f(\\mu ) { { \\delta_i}}^s \\right| \\\\    & \\le & p(|{{\\tilde\\mu}_i}-{{\\mu^\\dagger}}|\\ge    a+{{\\mu^*}}-{{\\mu^\\dagger}})g^s\\sup_{{{\\delta_i}}\\in\\delta_1}\\left|f(\\mu)\\right|\\ ] ] by choosing @xmath344 small enough , we can bring @xmath348 and @xmath131 arbitrarily close together ; in particular we can choose @xmath344 such that @xmath353 so that application of theorem  [ thm : boundprob ] is safe .",
    "it reveals that the summed probability is @xmath354 for any even @xmath355 .",
    "now we bound @xmath304 which is @xmath356 for some @xmath357 by the condition on the main theorem .",
    "here we use that @xmath358 ; the latter is maximized if all outcomes equal the bound @xmath187 , in which case the estimator equals @xmath359 . putting all of this together , we get @xmath360 ; if we plug this into the equation we obtain : @xmath361 this is of order @xmath312 if we choose @xmath362 .",
    "we can do this because the construction of @xmath363 ensures that all moments exist , and therefore certainly the first @xmath364 moments .      _",
    "( of theorem [ thm : robustml ] ) _ we express the relative redundancy of the slightly squashed ml plug - in code @xmath76 by the sum of the relative redundancy of the ordinary ml plug - in code @xmath84 and the difference in expected codelengths between @xmath76 and @xmath84 : @xmath365 -   e_{p}[- \\ln { { m_{{\\mu^*}}}}(z^n ) ] .",
    "\\\\ & = & e_{p}[l_u(z^n ) - l_{{\\hat{u}}}(z^n ) ] + { { \\mathcal{r}}}_{{\\hat{u}}}(n ) \\\\ & = & e_{p}[l_u(z^n ) - l_{{\\hat{u}}}(z^n ) ] + \\frac{1}{2 } \\frac{{{\\textnormal{var}}}_p x}{{{\\textnormal{var}}}_{m_{\\mu^ * } } x } \\ln n + o(1),\\end{aligned}\\ ] ] where the last equality follows from ( [ eq : redundancyml ] ) , which is valid under the conditions imposed on @xmath189 ( see condition 1 in @xcite for details ) .",
    "we have : @xmath366 since @xmath193 , we have : @xmath367 to analyze the second term in the sum , we use the fact that for arbitary @xmath368 : @xmath369 which follows e.g. from expanding the logarithm into taylor expansion up to the second order . in our case , @xmath370 .",
    "we will show that @xmath371 $ ] is @xmath372 , and then @xmath373 = -e_p[v_i ] + o(i^{-2})$ ] .",
    "we have : @xmath374 = \\frac{1}{4i^2 } e_p \\left [ i_{{\\cal m}}^2({{{\\hat\\mu}_i } } ) \\left ( x_{i+1 } - { { { \\hat\\mu}_i}}\\right)^4   \\right ] \\\\   = & \\frac{1}{4i^2 }   e_{x^i \\sim p } \\left [ i_{{\\cal m}}^2 ( { { { \\hat\\mu}_i}})e_{x_{i+1 } \\sim p } \\left[\\left ( x_{i+1 } - { { \\mu^*}}+ { { \\mu^*}}- { { { \\hat\\mu}_i}}\\right)^4\\right ] \\right ] \\\\",
    "= & \\frac{1}{4i^2 }   e_p \\left [ i_{{\\cal m}}^2 ( { { { \\hat\\mu}_i } } ) \\left ( { \\textnormal{m}_p^{(4 ) } } - 4\\delta_i{\\textnormal{m}_p^{(3 ) } }   + 6   \\delta_i^2{{\\textnormal{var}}}_{p_{{\\mu^*}}}x + \\delta_i^4 \\right ) \\right ] , \\end{array}\\ ] ] where @xmath375 is @xmath376 $ ] , the @xmath13-th central moment of @xmath377 , and @xmath378 .",
    "we will show that the terms under expectation are bounded .",
    "if @xmath203 is constant , then we apply theorem [ thm : devavasym ] with @xmath39 , @xmath379 and @xmath380 to the second , third and fourth term , respectively and thus all the terms under expectation are @xmath100 .",
    "if @xmath203 is not constant , then by condition [ condition ] the assumptions of lemma [ lemm : bounding ] are satisfied with @xmath381 and @xmath382 . applying the lemma subsequently to the first , third and fourth term ( with @xmath382 , respectively ) , we see that all those terms are @xmath100 .",
    "the second term is also @xmath100 by applying lemma [ lemm : bounding ] once again with @xmath383 and @xmath384 ( assumptions are again satisfied by condition [ condition ] ) .",
    "thus , we showed that @xmath371 = o(i^{-2})$ ] .",
    "next , we consider @xmath197 $ ] : @xmath385 = \\frac{1}{2i } e_p \\left [   i_{{\\cal m}}({{{\\hat\\mu}_i}})(x_{i+1 } - { { \\mu^*}}+ { { \\mu^*}}- { { { \\hat\\mu}_i}})^2 \\right ] \\\\ & = & \\frac{1}{2i } e_p \\left[i_{{\\cal m}}({{{\\hat\\mu}_i}})\\left ( { { \\textnormal{var}}}_{p_{{\\mu^*}}}x + \\delta_i^2   \\right ) \\right ] \\\\ & = & \\frac{1}{2i } \\left({{\\textnormal{var}}}_{p_{{\\mu^*}}}x e_p \\left[i_{{\\cal m}}({{{\\hat\\mu}_i}})\\right ] + e_p \\left[i_{{\\cal m}}({{{\\hat\\mu}_i } } ) \\delta_i^2\\right ] \\right).\\end{aligned}\\ ] ] the second term @xmath386 $ ] is @xmath200 by lemma [ lemm : bounding ] applied with @xmath387 and @xmath388 . to analyze the first term we expand @xmath203 into taylor series around @xmath131 : @xmath389 = i_{{\\cal m}}({{\\mu^ * } } ) + e_p\\left[\\frac{d}{d\\mu } i_{{\\cal m}}({{\\mu^ * } } )",
    "\\delta_i + \\frac{d^2}{d^2\\mu } i_{{\\cal m}}(\\mu ) \\delta^2_i\\right],\\ ] ] for some @xmath66 between @xmath131 and @xmath311 .",
    "the linear term in the expansion is @xmath200 by theorem [ thm : devavasym ] applied with @xmath39 .",
    "the quadratic term is @xmath200 by applying lemma [ lemm : bounding ] with @xmath390 and @xmath388 ; condition [ condition ] guarantees that assumptions of the lemma are satisfied .",
    "thus , using ( [ eq : efficient ] ) : @xmath391 \\!=\\ ! \\frac{1}{2i }",
    "i_{{{\\cal m}}}({{\\mu^ * } } ) { { \\textnormal{var}}}_{p_{{\\mu^*}}}\\!x + o(i^{-2 } ) \\!=\\ !",
    "\\frac{1}{2i } \\frac{{{\\textnormal{var}}}_{p_{{\\mu^*}}}\\!x}{{{\\textnormal{var}}}_{m_{{\\mu^*}}}\\!x } + o(i^{-2}),\\ ] ] so that : @xmath392 = - \\frac{1}{2i } \\frac{{{\\textnormal{var}}}_{p_{{\\mu^*}}}x}{{{\\textnormal{var}}}_{m_{{\\mu^*}}}x } + o(i^{-2}),\\ ] ] taking together ( [ eq : ln_v ] ) and ( [ eq : logarithm ] ) we have : @xmath393 and thus : @xmath394"
  ],
  "abstract_text": [
    "<S> we analyse the prequential plug - in codes relative to one - parameter exponential families @xmath0 . </S>",
    "<S> we show that if data are sampled i.i.d . from some distribution outside @xmath0 </S>",
    "<S> , then the redundancy of any plug - in prequential code grows at rate larger than @xmath1 in the worst case . </S>",
    "<S> this means that plug - in codes , such as the rissanen - dawid ml code , may behave inferior to other important universal codes such as the 2-part mdl , shtarkov and bayes codes , for which the redundancy is always @xmath2 . </S>",
    "<S> however , we also show that a slight modification of the ml plug - in code , `` almost '' in the model , does achieve the optimal redundancy even if the the true distribution is outside @xmath3 .    </S>",
    "<S> [ theorem]lemma [ theorem]proposition [ theorem]conjecture </S>"
  ]
}