{
  "article_text": [
    "a bayesian data analysis specifies joint probability distributions to describe the relationship between the prior information , the model or hypotheses , and the data . using bayes theorem , the posterior distribution is uniquely determined from the conditional probability distribution of the unknowns given the observed data .",
    "the posterior probability is usually stated as follows : @xmath0 where @xmath1 is the marginal likelihood .",
    "the symbol @xmath2 denotes the assumption of a particular model and the parameter vector @xmath3 . for physical models , the sample space @xmath4 is most often a continuous space",
    ". in words , equation ( [ eq : bayes ] ) says : the probability of the model parameters given the data and the model is proportional to the prior probability of the model parameters and the probability of the data given the model parameters .",
    "the posterior may be used , for example , to infer the distribution of model parameters or to discriminate between competing hypotheses or models .",
    "the latter is particularly valuable given the wide variety of astronomical problems where diverse hypotheses describing heterogeneous physical systems is the norm ( see * ? ? ?",
    "* for a thorough discussion of bayesian data analysis ) .    for parameter estimation ,",
    "one often considers @xmath5 to be an uninteresting normalization constant",
    ". however , equation ( [ eq : evidence ] ) clearly admits a meaningful interpretation : it is the support or _ evidence _ for a model given the data .",
    "this see this , assume that the prior probability of some model , @xmath6 say , is @xmath7 . then by bayes theorem ,",
    "the probability of the model given the data is @xmath8 .",
    "the posterior odds of model @xmath9 relative to model @xmath10 is then : @xmath11 if we have information about the ratio of prior odds , @xmath12 , we should use it , but more often than not our lack of knowledge forces a choice of @xmath13 .",
    "then , we estimate the relative probability of the models given @xmath14 over their prior odds by the bayes factor @xmath15 ( see * ? ? ?",
    "* for a discussion of additional concerns ) .",
    "when there is no ambiguity , we will omit the explicit dependence on @xmath16 of the prior distribution , likelihood function , and marginal likelihood for notational convenience .",
    "the bayes factor has a number of attractive advantages for model selection @xcite : ( 1 ) it is a consistent selector ; that is , the ratio will increasingly favor the true model in the limit of large data ; ( 2 ) bayes factors act as an occam s razor , preferring the simpler model if the `` fits '' are similar ; and ( 3 ) bayes factors do not require the models to be nested in any way ; that is , the models and their parameters need not be equivalent in any limit . there is a catch : direct computation of the marginal likelihood ( eq . [ eq : evidence ] ) is intractable for most problems of interest .",
    "however , recent advances in computing technology together with developments in markov chain monte carlo ( mcmc ) algorithms have the promise to compute the posterior distribution for problems that have been previously infeasible owing to dimensionality or complexity .",
    "the posterior distribution is central to bayesian inference : it summarizes all of our knowledge about the parameters of our model and is the basis for all subsequent inference and prediction for a given problem .",
    "for example , current astronomical datasets are very large , the proposed models may be high - dimensional , and therefore , the posterior sample is expensive to compute . however ,",
    "once obtained , the posterior sample may be exploited for a wide variety of tasks . although dimension - switching algorithms , such as reversible - jump mcmc @xcite incorporate model selection automatically without need for bayes factors , these simulations appear slow to converge for some of our real - world applications .",
    "moreover , the marginal likelihood may be used for an endless variety of tests , ex post facto .",
    "@xcite presented a formula for estimating @xmath17 from a posterior distribution of parameters .",
    "they noted that a mcmc simulation of the posterior selects values of @xmath3 distributed as @xmath18 and , therefore , @xmath19 or @xmath20_{p({\\boldsymbol{\\theta}}|{{\\bf d } } ) } , \\label{eq : hmean}\\ ] ] having suppressed the explicit dependence on @xmath16 for notational clarity .",
    "this latter equation says that the marginal likelihood is the harmonic mean of the likelihood with respect to the posterior distribution .",
    "it follows that the harmonic mean computed from a sampled posterior distribution is an estimator for the marginal likelihood , e.g. : @xmath21^{-1}. \\label{eq : hsamp}\\ ] ] unfortunately , this estimator is prone to domination by a few outlying terms with abnormally small values of @xmath22 ( e.g. see * ? ? ?",
    "* and references therein ) .",
    "@xcite describes convergence criteria for equation ( [ eq : hsamp ] ) and @xcite present augmented approaches with error estimates .",
    "alternative approaches to computing the marginal likelihood from the posterior distribution have been described at length by @xcite . of these",
    ", the laplace approximation , which approximates the posterior distribution by a multidimensional gaussian distribution and uses this approximation to compute equation ( [ eq : evidence ] ) directly , is the most widely used .",
    "this seems to be favored over equation ( [ eq : harmonic ] ) because of the problem with outliers and hence because of convergence and stability . in many cases , however , the laplace approximation is far from adequate in two ways .",
    "first , one must identify all the dominant modes , and second , modes may not be well - represented by a multidimensional gaussian distribution for problems of practical interest , although many promising improvements have been suggested ( e.g. * ? ? ? * ) .",
    "@xcite explored the use of the savage - dickey density ratio for cosmological model selection ( see also * ? ? ?",
    "* for a full review of the model selection problem for cosmology ) .    finally , we may consider evaluation of equation ( [ eq : evidence ] ) directly .",
    "the mcmc simulation samples the posterior distribution by design , and therefore , can be used to construct volume elements in @xmath23-dimensional parameter space , @xmath24 , e.g. when @xmath25 .",
    "although the volume will be sparsely sampled in regions of relatively low likelihood , these same volumes will make little contribution to equation ( [ eq : evidence ] ) . the often - used approach from computational geometry , delaney triangulation , maximizes the minimum angle of the facets and thereby yields the `` roundest '' volumes .",
    "unfortunately , the standard procedure scales as @xmath26 for a sample of @xmath27 points .",
    "this can be reduced to @xmath28 using the flip algorithm with iterative construction @xcite but this scaling is prohibitive for large @xmath27 and @xmath23 typical of many problems .",
    "rather , in this paper , we consider the less optimal but tractable kd - tree for space partitioning .    in part ,",
    "the difficulty in computing the marginal likelihood from the sampled posterior has recently led ( * ? ? ?",
    "*  nesting sampling  ) to suggest an algorithm to simulate the marginal likelihood rather than the posterior distribution .",
    "this idea has been adopted and extended by cosmological modelers @xcite .",
    "the core idea of nesting sampling follows by rewriting equation ( [ eq : zdef0 ] ) as a double integral and swapping the order of integration , e.g. @xmath29 the nested sampler is a monte carlo sampler for the likelihood function @xmath30 with respect to the prior distrbution @xmath31 so that @xmath32 .",
    "the generalization of the construction in equation ( [ eq : nest ] ) for general distributions and multiple dimensions is the lebesgue integral ( see  [ sec : intgr ] ) .",
    "clearly , this procedure has no problems with outliers with small values of @xmath33 .",
    "of course , any algorithm implementing nested sampling must still thoroughly sample the multidimensional posterior distribution and so retains all of the intendant difficulties that mcmc has been designed to solve .    in many ways ,",
    "the derivation of the nested sampler bears a strong resemblance to the derivation of the harmonic mean but without any obvious numerical difficulty .",
    "this led me to a careful study of equations ( [ eq : evidence ] ) and ( [ eq : zdef0 ] ) to see if the divergence for small value of likelihood could be addressed .",
    "indeed they can , and the following sections describe two algorithms based on each of these equations . these new algorithms retain the main advantage of the harmonic mean approximation ( hma ) : direct incorporation of the sampled posterior without any assumption of a functional form . in this sense , they are fully and automatically adaptive to any degree multimodality given a sufficiently large sample .",
    "we begin in  [ sec : intgr ] with a background discussion of lebesgue integration applied to probability distributions and monte carlo ( mc ) estimates .",
    "we apply this in  [ sec : evid ] to the marginal likelihood computation .",
    "this development both illuminates the arbitrariness in the hma from the numerical standpoint and leads to an improved approach outlined in  [ sec : algo ] . in short ,",
    "the proposed approach is motivated by methods of numerical quadrature rather than sample statistics .",
    "examples in  [ sec : examples ] compare the application of the new algorithms to the hma and the laplace approximation .",
    "the overall results are discussed and summarized in  [ sec : summary ] .",
    "using riemann and lebesgue integration . for riemann integration , we sum thin vertical rectangles of height @xmath34 about the abscissa point @xmath35 for some width @xmath36 . for lebesgue integration , we sum thin horizontal rectangles of width @xmath37 about the ordinate point @xmath38 of height @xmath39 . in both cases , we sum the area under the curve @xmath40 . in the lebesgue case , we must add in the rectangle of width @xmath41 and height @xmath42.,scaledwidth=50.0% ]    assume that we have a mc generated sample of random variates with density @xmath40 .",
    "recall that any moment or expectation with respect to this density may be computed as a single sum , independent of the parameter - space dimension !",
    "this powerful and seemingly innocuous property follows from the power of lebesgue integration ( e.g. * ? ? ?",
    "* ) . to see this ,",
    "let us begin by considering a one - dimensional integral @xmath43 where @xmath40 is non - negative , finite and bounded , that is : @xmath44 for @xmath45 $ ] .",
    "more formally , we may define the lebesgue integral @xmath46 over the measure of sets of points @xmath47 $ ] with measure @xmath48 as follows .",
    "we assume that @xmath40 is measurable over @xmath4 and , following @xcite , define the measure function @xmath49 . clearly , @xmath50 is monotonic with @xmath51 and @xmath52 .",
    "let @xmath53 be a partition of @xmath54 $ ] with @xmath55 and @xmath56 . in our case , the partition @xmath57 is our mc sample .",
    "the choice of @xmath57 induces a partition of @xmath58 through @xmath59 although the sequence in @xmath58 will no longer be monotonic . for convenience ,",
    "reorder the @xmath58 such that @xmath60 . now consider the lebesgue integral of @xmath40 over @xmath4 , @xmath61 we interpret this geometrically as the area under the curve @xmath62 ; in other words , we have swapped the abscissa and ordinate . to see this ,",
    "define @xmath63 for the partition @xmath57 .",
    "define the interval @xmath64 .",
    "clearly @xmath65\\sup(\\lambda_j ) = \\left[m(b ) - m(a)\\right]\\sup(\\lambda_j)\\ ] ] and , therefore , @xmath66 since @xmath50 is monotonic and @xmath67 . using this",
    ", we may evaluate the integral in equation ( [ eq : int2 ] ) as follows : @xmath68 the sums in equation ( [ eq : lun ] ) have the form of a rectangular quadrature of the measure over the _ range _ of @xmath40 .",
    "this geometry is illustrated in figure [ fig : geom ] .",
    "although the lebesgue integration theory is general , the equivalence of the one - dimensional integral and equation ( [ eq : lebesgue2 ] ) is easily seen by rewriting equation ( [ eq : int1 ] ) as a two dimensional integral and changing the order of integration using elementary techniques as follows : @xmath69 where @xmath70 and @xmath71 are the minimum and maximum values of @xmath40 in @xmath54 $ ] .",
    "an average of the sums in equation ( [ eq : lun ] ) gives us a trapezoidal rule analog : @xmath72(y_{i } - y_{i-1 } ) .",
    "\\label{eq : tn}\\ ] ] further generalization is supported by the lebesgue theory of differentiation .",
    "a central result of the theory is that a continuous , monotonic function in some interval is differentiable almost everywhere in the interval @xcite .",
    "this applies to the measure function @xmath37 .",
    "this result may be intuitively motivated in the context of our marginal likelihood calculation as follows .",
    "our measure function describes the amount of density with likelihood smaller than some particular value .",
    "a typical likelihood surface for a physical model is smooth , continuous , and typically consists of several discrete modes .",
    "consider constructing @xmath37 by increasing the value of @xmath38 beginning at the point of maximum likelihood peak .",
    "since @xmath73 , this is equivalent to beginning at @xmath74 and decreasing @xmath30 . recall that @xmath37 decreases from 1 to 0 as @xmath38 increases from @xmath75 to @xmath76 .",
    "therefore , we construct @xmath37 from @xmath77 by finding the level set corresponding to some value of @xmath38 and subtracting off the area of the likelihood surface constructed from the perimeter of the set times @xmath78 .",
    "the function @xmath37 will decrease smoothly from unity at @xmath75 until @xmath38 reaches the peak of the second mode . at this point",
    ", there may be a discontinuity in the derivative of @xmath37 as another piece of perimeter joins the level set , but it will be smooth thereafter until the peak of the third mode is reached , and so on . since we expect the contribution to @xmath17 to be dominated by a few primary modes , this suggests that we can evaluate the integral @xmath46 numerically using the quadrature implied by equation ( [ eq : lebesgue2 ] ) and possibly even higher - order quadrature rules .",
    "these arguments further suggest that partitioning @xmath4 into separate domains supporting individual modes would improve the numerics by removing discontinuities in the derivative of @xmath37 and explicitly permitting the use of higher - order quadrature rules",
    ". this partition may be difficult to construct automatically , however .    to better control the truncation error for this quadrature , we might like to choose a uniform partition in @xmath58 , @xmath79 , to evaluate the sums @xmath80 and @xmath81 . for mc integration ,",
    "this is not possible .",
    "rather , mc selects @xmath57 with irregular spacings and this induces a partition of @xmath58 .",
    "motivated by kernel density estimation , we may then approximate @xmath50 by @xmath82 where @xmath83 monotonically increases from 0 to 1 in the vicinity of @xmath84 .",
    "for example , we may choose @xmath85 to be the heaviside function @xmath86 which assigns a `` step '' to the upper value of the range in @xmath58 for each @xmath87 .",
    "alternatively , we may consider smoothing functions such as @xmath88    \\label{eq : nummeas3}\\ ] ] where @xmath89 denotes the error function .",
    "then , upon substituting equation ( [ eq : nummeas ] ) into equation ( [ eq : lebesgue2 ] ) for @xmath90 , we get : @xmath91 where @xmath92 and @xmath93 by construction and the final equality follows by gathering common factors of @xmath94 in the earlier summation .    for integration over probability distributions",
    ", we desire @xmath95 distributed according some probability density @xmath96 , @xmath97 = \\int_{\\mathbb{r } } f(x ) g(x)\\,dx ,    \\label{eq : probint}\\ ] ] which yields @xmath98 with the normalization @xmath99 , and therefore , @xmath100 $ ] . despite the appearance of the density @xmath101 in equation ( [ eq : probint ] ) , only the value of measure has changed , not the formalism , i.e. the points @xmath87 are now sampled from @xmath96 rather than uniformly .",
    "let us now consider the connection between equation ( [ eq : itilden ] ) and a classic mc integration , although we do not need this for later sections of this paper . for a classic mc integration , we choose @xmath102 by construction , and with this substitution @xmath103",
    "becomes the mc approximation : @xmath104 for integration over probability distributions , we assign @xmath105 to its expectation value @xmath106 and the mc integral becomes @xmath107 although intuition suggests that the explicit form from equation ( [ eq : itilden ] ) will yield a better result .",
    "the development leading to equation ( [ eq : itilden ] ) remains nearly unchanged for a multidimensional integral : @xmath108 as in the one - dimensional case , the lebesgue integral becomes @xmath109 where the only difference is that @xmath50 is now the measure of the set of points with @xmath110 with @xmath111 .",
    "perhaps more remarkable than the similarity of equation ( [ eq : lebesgue2 ] ) with equation ( [ eq : lebesgue3 ] ) is that the numerical lebesgue integral is one - dimensional independent of the dimensionality @xmath23 .",
    "however , this does not simplify the computational work ; one still needs to sample a subset of @xmath112 to evaluate equation ( [ eq : lebesgue3 ] ) .",
    "the mc version proceeds similarly as well : replace @xmath87 by @xmath113 in equation ( [ eq : mci ] ) .    in summary ,",
    "monte carlo integration is most often posed as the expectation over a distribution , which , more generally , is a lebesgue integral .",
    "lebesgue integration and differentiation theory suggests alternative computational approaches akin to traditional riemann - based numerical analysis , if the underlying likelihood function and prior probability density are well - behaved functions .",
    "we will see in the next section that a truncation - error criterion applied to the marginal likelihood integral in the form of equation ( [ eq : lebesgue3 ] ) can improve the hma .",
    "now , given a mc - computed sample from the posterior distribution @xmath114 with prior distribution @xmath115 and likelihood function @xmath33 , how does one compute the marginal likelihood ?",
    "the integral in equation ( [ eq : evidence ] ) states that marginal likelihood is the expectation of the likelihood with respect to the prior distribution .",
    "this is the same as equation ( [ eq : lebesgue3 ] ) with @xmath116 replacing @xmath117 , @xmath33 replacing @xmath118 , @xmath115 replacing @xmath119 . alternatively , returning to equation ( [ eq : zdef0 ] ) , the integral @xmath120 is implicitly defined by @xmath121 the value @xmath122 is the probability of @xmath31 over @xmath123 .",
    "we will assume that @xmath124 ; this implies that @xmath125 since @xmath126 .",
    "in addition , the existence of equation ( [ eq : zdef ] ) implies that @xmath127 almost everywhere in @xmath4 . defining @xmath128",
    ", it follows that the lebesgue integral of the integral on the left - hand - side of equation ( [ eq : zdef ] ) is @xmath129 with measure @xmath130 intuitively , one may interpret this construction as follows : divide up the parameter space @xmath116 into volume elements sufficiently small that @xmath114 is approximately constant .",
    "then , sort these volume elements by their value of @xmath131 .",
    "the probability element @xmath132 is the prior probability of the volume between @xmath38 and @xmath133 .",
    "clearly @xmath134 $ ] and may be trivially computed from a mcmc - generated posterior distribution .",
    "using our finite mc - sampled distributed as the posterior probability , @xmath135 , and converting the integral to a sum , we have the following simple estimate for @xmath136 : @xmath137 } \\equiv \\frac1n \\sum_{j=1}^n \\mathbf{1}_{\\{y_j > y_i\\ } } ,    \\qquad    m_i^{[u ] } \\equiv \\frac1n \\sum_{j=1}^n \\mathbf{1}_{\\{y_j\\ge y_i\\ } } ,    \\qquad    m_i \\equiv \\frac{m_i^{[l ] } + m_i^{[u]}}{2 } ,    \\label{eq : measurex3}\\ ] ] where we have defined the left and right end points from equation ( [ eq : nummeas2 ] ) and the mean separately so that @xmath138 } \\le m_i \\le m_i^{[u]}$ ] .",
    "the indicator function @xmath139 enforces the inclusion of a contribution @xmath140 for index @xmath141 only if @xmath142 or @xmath143 for the lower and upper form , respectively .",
    "alternatively , these sums may be expressed using equations ( [ eq : nummeas])([eq : itilden ] ) .",
    "we may now estimate the marginal likelihood from equation ( [ eq : zdef ] ) using the second part of equation ( [ eq : lebesgue3 ] ) for finite @xmath27 by gathering terms in @xmath144 to get @xmath145 in deriving equation ( [ eq : harmonic0 ] ) , the leading term @xmath146 from equation ( [ eq : lebesgue3 ] ) is absorbed into the sum and @xmath147 . assuming that @xmath148 in equation ( [ eq : zdef ] ) and using equation ( [ eq : harmonic0 ] ) yields @xmath149 this is an alternative derivation for the `` harmonic mean '' approximation ( hma ) to the marginal likelihood .",
    "the evaluation of the integral @xmath150 ( eq . [ eq : intk ] ) may fail both due to insufficient sampling and intrinsic divergence . as an example of the former , a sparse sampling may lead to large intervals @xmath151 and inflated truncation error ( eq .",
    "[ eq : numerick ] ) .",
    "we will consider this further in the next section .",
    "as an example of the latter , consider the textbook inference of an unknown mean @xmath152 from a sample of @xmath27 normally distributed points @xmath153 .",
    "the likelihood function is @xmath154 where @xmath155 and @xmath156 is the sample mean .",
    "let the prior distribution for @xmath152 be @xmath157 .",
    "we use an mcmc algorithm to sample the posterior distribution @xmath152 .",
    "now , let us evaluate @xmath150 using lebesgue integration for this example . to begin , we need the measure function @xmath158 with @xmath73 we may use equation ( [ eq : likeex ] ) to solve for @xmath159 , noting that the solution has two branches . after some algebra , we have : @xmath160    \\label{eq : myg}\\ ] ] where @xmath161 @xmath162 the value @xmath163 is the variance weighted mean of the prior mean and the sample mean , and @xmath164 is the harmonic mean of the variance of the prior distribution and the variance of the sample mean .",
    "the value @xmath75 is the minimum value for @xmath38 and @xmath165 describes the offset of @xmath152 with increasing @xmath38 .",
    "note that @xmath166 .    since the values of @xmath38 are obtained by a sampling , the sample will not cover @xmath167 but will be limited from above by the smallest sampled value of the likelihood @xmath168 .",
    "we define this limited value of the integral @xmath150 as @xmath169 where the last equality uses @xmath170 .",
    "clearly @xmath171 .",
    "the magnitude of the truncation due to finite sampling , @xmath172 , depends critically on the width of the likelihood distribution relative to the prior distribution .",
    "we describe this ratio of widths by @xmath173 .",
    "the convergence condition @xmath174\\rightarrow0 $ ] requires that @xmath37 decreases faster than @xmath175 for some @xmath176 . for @xmath177 and large @xmath38 , @xmath178 increases as @xmath179 . for @xmath180 ,",
    "@xmath178 decreases at least as fast @xmath181 .",
    "figure [ fig : gauss_ex1 ] shows @xmath182 as a function of @xmath183 and suggests that @xmath184 is sufficient to obtain convergence for practical values of @xmath27 .",
    "qualitatively , a prior distribution that limits @xmath38 from above ( or , equivalently , @xmath30 from below ) will prevent the divergence .",
    "the integral @xmath185 is shown as a function of @xmath186 for various values of the ratio @xmath173 .",
    "for an uninformative prior distribution @xmath187 and @xmath188 and @xmath189 diverges with increasing @xmath186 . for an informative prior distribution @xmath190 and @xmath189 convergences",
    "quickly with @xmath186.,scaledwidth=60.0% ]    similar asymptotic expressions for @xmath189 may be derived for multivariate normal distributions . for simplicity , assume that data is distributed identically in each of @xmath23 dimensions and , for ease of evaluation , take @xmath191 .",
    "then @xmath192 where @xmath193 is the upper incomplete gamma function and @xmath194 is the complete gamma function . using the standard asymptotic formula for @xmath193 in the limit of large @xmath38",
    ", one finds that @xmath195^{k/2 - 1 }    \\left(\\frac{y}{y_0}\\right)^{-1-b } \\qquad \\mbox{for\\ } y\\gg k/2 .",
    "\\label{eq : mulimygasymp}\\ ] ] this expression reduces to equation ( [ eq : myg ] ) when @xmath196 , but more importantly , this shows that the tail magnitude of @xmath37 increases with dimension @xmath23 . figure [ fig : gauss_ex2 ] illustrates this for various values of @xmath23 and @xmath183 .",
    "the divergence of @xmath150 in the limit @xmath197 shows that normally distributed likelihood function with an uninformative prior is divergent .",
    "moreover , figures [ fig : gauss_ex1 ] and [ fig : gauss_ex2 ] further demonstrate that weakly informative prior with very small @xmath183 is likely to be numerically divergent even if @xmath150 formally converges .",
    "intuitively , the cause is clear : if the markov chain never samples the wings of the prior distribution that still make significant contribution to @xmath150 , then @xmath150 will increase with sample size .",
    "analytically , the failure is caused by the measure decreasing too slowly as @xmath38 increases ( as described at the beginning of  [ sec : evid ] ) .",
    "empirically , the convergence of the integral @xmath150 may be tested by examining the run of @xmath189 for increasing @xmath38 .",
    "overall ,  [ sec : evid ] highlights the marginal likelihood as a numerical quadrature .",
    "we have considered the path of approximations leading from the quadrature to standard expression for the hma .",
    "we have also considered the intrinsic requirements on the prior distribution so that the meausure @xmath37 is convergent .",
    "this development suggests that there are two sources of error in evaluating equation ( [ eq : zdef ] ) using equations ( [ eq : evidencex ] ) and ( [ eq : numerick ] ) .",
    "the first is a truncation error of order @xmath198 .",
    "the second is a bias error resulting from specifying the computational grid by a monte carlo procedure .",
    "we will consider each in turn .",
    "thinking again in terms of numerical quadrature , we can trust the sum in equation ( [ eq : numerick ] ) when adjacent values of @xmath22 are close .",
    "the error in @xmath199 will be dominated by the extremely small values of @xmath200 that lead to @xmath201 .",
    "specifically , the error for such a term will be @xmath202 , and a small number of such terms can dominate the error for @xmath199 .",
    "numerical analysis is based on functional approximation of smooth , continuous , and differentiable functions .",
    "this truncation error estimate assumes that @xmath37 is such a function .",
    "although not true everywhere , we have argued in  [ sec : intgr ] ) that this assumption should be valid over a countable number of intervals in practice .",
    "in addition , we expect the sample to be strongly clustered about the value of the posterior mode . by the central limit theorem for a likelihood dominated posterior , the distribution of @xmath152 tends toward a normal distribution",
    ". therefore , larger @xmath27 will yield more extremal values of @xmath30 and _ increase _ the divergence of @xmath203 .",
    "eventually , for proper prior distributions and sufficiently large @xmath27 , the smallest possible value of @xmath30 will be realized as long as @xmath204 ( see eq . [ eq : zdef ] and following discussion ) .",
    "further sampling will reduce the largest intervals , and this will lead to the decrease of large @xmath205 , and finally , convergence .    the second source of error is closely related to the first . after eliminating the divergent samples with @xmath206 , the sampled domain @xmath123 will be a subset of the originally defined domain @xmath207 .",
    "that is , the mcmc sample will not cover all possible values of the parameter vector @xmath208 .",
    "this implies that the numerical quadrature of equation ( [ eq : zdef ] ) will yield @xmath209 .",
    "identification of these error sources immediately suggests solutions .",
    "note that this observation does not change the problem definition in some new way , but rather , allows us to exploit the mcmc - chosen domain @xmath123 to eliminate the divergence for small @xmath183 described in ",
    "[ sec : gauss_ex ] .",
    "first , we may decrease the truncation error in @xmath199 by ordering the posterior sample by increasing values of @xmath38 and truncating the sequence at the point where @xmath210 for some choice @xmath211 .",
    "next , we need a consistent evaluation for @xmath212",
    ". we may use the sampled posterior distribution itself to estimate the sampled volume in @xmath207 .",
    "this may be done straightforwardly using a space partitioning structure .",
    "a computationally efficient structure is a binary space partition ( bsp ) tree , which divides a region of parameter space into two subregions at each node .",
    "the most easily implemented tree of this type for arbitrary dimension is the kd - tree ( short for k - dimensional tree ) .",
    "the computational complexity for building the tree from the @xmath27 sampled points in parameter space scales as @xmath213 using the quicksort algorithm at each successive level ( this may be improved , see * ? ? ?",
    "each leaf has zero volume .",
    "each non - leaf node has the minimum volume enclosing the points in the node by coordinate planes . assigning the volume containing a fixed number of leaves @xmath214 ( e.g. @xmath215 or @xmath216 ) , and some representative value of the prior probability in each node ( such as a @xmath217-quantile or mean value ) , one may immediately sum product of each volume and value to yield an estimate of @xmath122 . for modest values",
    "@xmath27 , we will almost certainly find that @xmath209 .",
    "since the mcmc chain provides the values of @xmath115 and @xmath218 , we may use the same tree to evaluate both @xmath219 and @xmath212 over the sampled volume @xmath123 .",
    "the example in ",
    "[ sec : gauss_ex ] suggests that evaluation of @xmath150 may stymied by poor convergence unless the prior distribution is restrictive .",
    "therefore , if the value of @xmath150 is divergent or very slowly convergent , the evaluation of @xmath17 using @xmath220 will fail whether or not we use the improved truncation criteria .",
    "direct evaluation of the @xmath17 is free from this divergence and remains an practical option in this case .",
    "the advantage of a direct evaluation is clear : the converged markov chain samples the domain @xmath4 proportional to the integrand of equation ( [ eq : evidence ] ) , and therefore , we expect @xmath221 for large sample size by construction .",
    "we propose a hybrid of cubature and monte carlo integration .",
    "the bsp tree provides a _ tiling _ of multidimensional volume by using the posterior distribution to define volume elements , @xmath222 .",
    "we use a @xmath217-quantile ( such as the @xmath223 median ) or mean value of the posterior probability or the prior probability to assign a probability value to each volume element .",
    "an approximation to the integrals @xmath17 and @xmath122 follow from summing the field values over the volume elements , analogous to a multidimensional riemann rule .",
    "although @xmath224 for a _ infinite _ posterior sample ,",
    "there are several sources of error in practice .",
    "first , the variance in the tessellated parameter - space volume will increase with increasing volume and decreasing posterior probability .",
    "this variance may be estimated by bootstrap .",
    "secondly , the truncation error of the cubature increases with the number of points per element . as usual , there is a variance  bias trade off choosing the resolution of the tiling : the bias of the probability value estimate increases and the variance decreases as the number of sample points per volume element increases .",
    "the prior probability value will be slowing varying over the posterior sample for a typical likelihood - dominated posterior distribution , so the bias will be small .",
    "this suggests that larger numbers of points per cell will be better for the evaluation of @xmath122 and a smaller number will be better for @xmath17 .",
    "some practical examples suggest that the resulting estimates are not strongly sensitive to the number of points per cell ( @xmath225 or 32 appears to be a good compromise ) .",
    "almost certainly , there will be a bias toward larger volume and therefore larger values of @xmath219 and this bias will increase with dimension most likely .    to summarize ,",
    "we have described two approaches for numerically computing @xmath17 from a mcmc posterior simulation .",
    "the first evaluates of the integral @xmath150 by numerical lebesgue integration , and the second evaluates @xmath17 directly by a parameter space partition obtained from the sampled posterior distribution .",
    "the first is closely related to the hma .",
    "it applies ideas of numerical analysis the integral that defines the hma .",
    "the second is more closely related to the laplace approximation .",
    "in some sense , laplace approximation is an integral of a parametric fit to the posterior distribution .",
    "the tree integration described above is , in essence , an integral of a non - parametric fit to the posterior distribution .",
    "the advantage of the first method its amenability to analysis .",
    "the disadvantage is the limitation on convergence as illustrated in  [ sec : gauss_ex ] .",
    "the advantage of the second method is its guaranteed convergence .",
    "the disadvantage is its clear , intrinsic bias and variance .",
    "the variance could be decreased , presumably , using high - dimensional voronoi triangulation but not without dramatic computational cost .",
    "the hma is treated as an expectation value in the literature .",
    "one of the failures pointed out by @xcite and others is that the hma is particularly awful when the sample is a single datum . in the context of the numerical arguments here ,",
    "this is no surprise : one can not accurately evaluate a quadrature with a single point ! even for larger samples ,",
    "the hma is formally unstable in the limit of a thin - tailed likelihood function owing to the divergence of the variance of the hma .",
    "@xcite address this failure of the statistic directly , proposing methods for stabilizing the harmonic mean estimator by reducing the parameter space to yield heavier - tailed densities .",
    "this is a good alternative to the analysis presented here when such stabilization is feasible .",
    "as previously mentioned , @xcite presents conditions on the posterior distribution for the consistency of the hma .",
    "intuitively , there is a duality with the current approach . trimming the lebesgue quadrature sum so that the interval @xmath226 is equivalent to lopping off the poorly sampled tail of the posterior distribution .",
    "this truncation will be one - sided in the estimate of the marginal likelihood @xmath219 since it removes some of the sample space .",
    "however , this may be compensated by an appropriate estimation of @xmath212 .",
    "the exposition and development in  [ sec : evid ] identifies the culprits in the failure of the hma : ( 1 ) truncation error in the evaluation of the measure @xmath37 ; and ( 2 ) the erroneous assumption that @xmath148 when @xmath207 in practice .",
    "we now present two new algorithms , the _ numerical lebesgue algorithm _ ( nla ) and the _ volume tessellation algorithm _ ( vta ) , that implement the strategies described in  [ sec : newevid ] to diagnose and mitigate this error .",
    "nla computes @xmath199 and vta computes @xmath212 and , optionally , @xmath219 directly from equation ( [ eq : evidence ] ) . in the following sections , we assume that @xmath25 .",
    "we begin with a converged mcmc sample from the posterior distribution .",
    "after the initial sort in the values of @xmath22 , the nla computes the difference @xmath227 with @xmath228 to find the first value of @xmath229 satisfying @xmath230 .",
    "the algorithm then computes the @xmath231 for @xmath232 using equation ( [ eq : measurex3 ] ) . for completeness , we compute the @xmath231 using both the restriction @xmath233 and @xmath234 to obtain lower and upper estimate for @xmath37 .",
    "then , these may be combined with @xmath235 and @xmath236 from  [ sec : intgr ] to riemann - like upper and lower bounds on @xmath199 .",
    "see the listing below for details . excepting the sort , the work required to implement this algorithm is only slightly harder than the hma .",
    "the vta uses a kd - tree to partition the @xmath27 samples from posterior distribution into a spatial regions .",
    "these tree algorithms split @xmath112 on planes perpendicular to one of the coordinate system axes .",
    "the implementation described here uses the median value along one of axes ( a _ balanced _ kd - tree ) .",
    "this differs from general bsp trees , in which arbitrary splitting planes can be used .",
    "there are , no doubt , better choices for space partitioning such as voronoi tessellation as previously discussed , but the kd - tree is fast , easy to implement , and published libraries for arbitrary dimensionality are available .",
    "traditionally , every node of a kd - tree , from the root to the leaves , stores a point . in the implementation used here , the points are stored in leaf nodes only , although each splitting plane still goes through one of the points .",
    "this choice facilitates the computation of the volume spanned by the points for each node as follows .",
    "let @xmath237 be the number of parameter - space points @xmath238 } , n=1,\\ldots,{\\bar m}_j$ ] in the @xmath239 node .",
    "let @xmath240}$ ] denote the field quantities at each point @xmath238}$ ] .",
    "some relevant field quantities include the values of the unnormalized posterior probability and the prior probability .",
    "the volume for node @xmath141 is @xmath241}_i,\\ldots,\\theta^{[{{\\bar m}}_j]}_i ) -      \\min(\\theta^{[1]}_i,\\ldots,\\theta^{[{{\\bar m}}_j]}_i ) \\right ] .",
    "\\label{eq : volj}\\ ] ] the set of nodes with @xmath242 for some fixed integer @xmath243 , determines an exclusive volume partition of the parameter space spanned by the point set , the _ frontier_. the value of @xmath243 is chosen large enough to limit the sampling bias of field quantities in the volume but small enough resolve the posterior modes of interest .",
    "the values @xmath244 $ ] seem to be good choices for many applications .",
    "each node in the frontier is assigned a representative value @xmath245 .",
    "i use @xmath217-quantiles with @xmath246 for tests here .",
    "the resulting estimate of the integrals @xmath212 and/or @xmath219 follow from summing the product of the frontier volumes with their values @xmath245 .",
    "both the nla and the vta begin with a sort of the likelihood sequence @xmath30 and this scales as @xmath247 . in the nla ,",
    "the computation of the @xmath248 followed by the computation of @xmath219 is @xmath249 .",
    "the sequence @xmath250 is useful also for diagnosis as we will explore in the next section .",
    "however , in many cases , we do not need the individual @xmath231 but only need the differential value @xmath251 to compute @xmath219 , which contains a single term .",
    "the values of likelihood may range over many orders of magnitude .",
    "owing to the finite mantissa , the differential value be necessary to achieve adequate precision for large @xmath27 , and the nla may be modified accordingly .",
    "the algorithm computes the lower , upper , and trapezoid - rule sums ( eqns .",
    "[ eq : lun][eq : tn ] ) for the final integral @xmath219 . for large posterior samples , e.g. @xmath252 ,",
    "the differences between @xmath253 and @xmath254 are small .",
    "indeed , a more useful error estimate may be obtained by a random partitioning and subsampling of the original sequence @xmath255 to estimate the distribution of @xmath219 ( see the examples in  [ sec : examples ] ) . in practice ,",
    "computing the marginal likelihood from a posterior sample with @xmath256 takes 0.2 cpu seconds on a single 2ghz opteron processor .",
    "although nla could be easily parallelized over @xmath257 processors to reduce the total runtime by @xmath258",
    "this seems unnecessary .",
    "the kd - tree construction in vta scales as @xmath259 followed by a tree walk to sum over differential node volumes to obtain the final integral estimates that scales as @xmath260 .",
    "this scaling was confirmed empirically using the multidimensional example described in ",
    "[ sec : highd ] with dimension @xmath261 $ ] and sample size @xmath262 $ ] .",
    "computing the marginal likelihood from a posterior sample with @xmath256 and @xmath263 takes 4.4 cpu seconds on a single 2ghz opteron processor , and , therefore , the computation is unlikely to be an analysis bottleneck , even when resampling to produce a variance estimate .",
    "the leading coefficient appears to vary quite weakly the distribution , although there may be undiscovered pathological cases that significantly degrade performance .",
    "the required value of @xmath27 increases with parameter dimension @xmath23 ; @xmath256 is barely sufficient for @xmath264 in tests below .",
    "subsampling recommends the use of even larger chains to mitigate dependence of the samples .",
    "therefore , the first practical hardware limitation is likely to be sufficient ram to keep the data in core .",
    "likelihood values @xmath265 from the simulated posterior distribution sort the sequence so that @xmath266 @xmath267 @xmath268 @xmath269 @xmath270 and/or @xmath271 save the values @xmath231",
    "@xmath272 @xmath273 @xmath274 save the estimated marginal likelihood , @xmath219 and algorithmic error    likelihood values @xmath265 from the simulated posterior distribution change variables @xmath275 sort the sequence so that @xmath276 create an empty point set @xmath277 @xmath278 @xmath279 add @xmath280 } , { \\bf f}^{[n]})$ ] to @xmath277 @xmath281 find the set of frontier nodes @xmath282 with the desired number of points @xmath214 @xmath283 compute the median value of @xmath240}$ ] among the @xmath214 points @xmath284 save the estimated value of the integrals @xmath285    buildkd(@xmath277 )    a set posterior points and values @xmath280 } , { \\bf f}^{[n]})\\in{\\cal p}$ ] compute and store the range for each coordinate and the volume for this node locate the coordinate dimension @xmath286 with maximum variance determine the median value of @xmath287}$ ] ( e.g. by quicksort ) split @xmath277 into two subsets by the hyperplane defined by @xmath288 : @xmath289 .",
    "to estimate the marginal likelihood using the methods of the previous section , we may use either the nla to estimate @xmath150 and the vta to estimate @xmath122 or use the vta alone to estimate @xmath17 .",
    "examples below explore the performance of these strategies .",
    "the mcmc posterior simulations are all computed using the umass bayesian inference engine ( bie , * ? ? ?",
    "* ) , a general - purpose parallel software platform for bayesian computation .",
    "all examples except that in ",
    "[ sec : highd ] simulate the posterior distribution using the parallel tempering scheme @xcite with @xmath292 and 20 temperature levels .",
    "convergence was assessed using the subsampling algorithm described in @xcite , a generalization of the @xcite test .      for",
    "a simple initial example , let us compute the marginal likelihood for a data sample @xmath14 of 100 points @xmath293 modelled by @xmath294 with prior distribution for @xmath295 .",
    "the marginal likelihood @xmath17 can be computed analytically from @xmath14 for this simple example .",
    "the final 200,000 converged states of the mcmc - generated chain were retained .",
    "application of the nla for @xmath199 and the vta for @xmath212 gives a value of @xmath296 ( 95% confidence interval ) , close to but systematically smaller than the analytic result : @xmath297 .",
    "a value of @xmath298 seems appropriate from numerical considerations , although experiments suggest that the algorithm is not sensitive to this choice as long as @xmath299 is not so small to decimate the sample or so large that error - prone outliers are included .",
    "it is prudent to check a range of @xmath299 to determine the appropriate value of each problem .",
    "the vta yields @xmath300 , consistent with the analytic result .",
    "the bias in the first estimate appears to be caused by an overestimate of @xmath301 produced by the vta .",
    "this might be improved by a space partition whose cells have smaller surface to volumes ratios (  [ sec : highd ] for a graphical example ) .",
    "the bias is much less pronounced in the direct estimate of @xmath219 by the vta owing to smallness of the posterior probability in the extrema of the sample .",
    "these extrema result in anomalously small value of @xmath302 for the hma .",
    "figure [ fig : mk ] illustrates the details of the nla applied to this computation .",
    "panel ( a ) plots @xmath303 from equation ( [ eq : measurex3 ] ) .",
    "the run of @xmath303 with @xmath38 rises rapidly near the posterior mode and drops rapidly to zero for small likelihood values .",
    "the inset in this figure shows @xmath303 in linear scale .",
    "the measure function @xmath303 , and hence the integral @xmath304 , is dominated by large values of @xmath30 as long as @xmath305 decreases sufficiently fast ( see  [ sec : gauss_ex ] ) .",
    "panel ( b ) plots the accumulating sum defining the quadrature of @xmath219 in equations ( [ eq : lun])([eq : tn ] ) , beginning with the largest values of likelihood first .",
    "the contribution to @xmath219 is dominated at the likelihood peak , corresponding to the steeply rising region of @xmath303 in panel ( a ) . in other words , the samples with small values of @xmath30 that degrade the hma",
    "make a negligible contribution to the marginal likelihood computation as long as @xmath306 .",
    "in addition , nla provides upper and lower bounds , and thereby some warning when the estimate is poorly conditioned , e.g. owing to an inappropriate choice for @xmath299 . the plot in figure [ fig",
    ": mk]b will readily reveal such failures .",
    "a more realistic error assessment can be obtained by subsampling the sequence @xmath255 .",
    "the cpu time for these algorithms is sufficiently small that this procedure should be practical in general .",
    "consider the following experiment : ( 1 ) the posterior is simulated by mcmc ( as described above ) to obtain a chain of 400,000 states ; ( 2 ) the first half of the chain is discarded ; ( 3 ) the second - half is randomly subsampled with replacement to obtain 128 samples of 10,000 states ; ( 4 ) the marginal likelihood for each is computed using the nla , vta , the laplace approximation and the hma ( approximately 2 cpu minute in total ) . for all but",
    "the hma , increasing the number of states decreases the variance for each distribution ; samples with 10,000 states best revealed the differences between the algorithms with a single scale .",
    "figure [ fig : subsample ] illustrates the relative performance with different prior distributions .",
    "figure [ fig : subsample]a is the model described at the beginning of this section ; the range of the prior distribution is much larger than the values sampled from the posterior distribution .",
    "the prior distributions for each successive panel have smaller ranges as indicated .",
    "the colors are composited over @xmath307 yields the new value @xmath308 . ] with @xmath309 ( e.g. hma over vta is brown , hma over nla is purple , laplace over hma is blue - grey , laplace over vta is blue - green ) . in panel ( d ) , the range is within the range of values sampled by the posterior in panel ( a ) .",
    "the overall trends are as follows : 1 ) the hma has unacceptably large variance unless the domain of the prior roughly coincides with the domain sampled by the mcmc algorithm ; 2 ) the vta and laplace approximation have the smallest variances , followed by hma ; 3 ) the nla is consistently biased below the analytic value ; and 4 ) the vta and laplace approximation are closed to the expected analytic value .",
    "indeed , the laplace approximation is an ideal match to and should do well for this simple unimodal model . in the final panel ,",
    "there are no outlier values of @xmath30 and the harmonic mean approximation is comparable to the others . these tests also demonstrate that the same outliers that wreck the hma have much less affect on nla and vta .",
    "further experimentation reveals that the results are very insensitive to the threshold value @xmath299 .",
    "in fact , one needs an absurdly large value of @xmath299 , @xmath310 , to produce failure .      here , we test these algorithms on the radiata pine compressive strength data analyzed by han and carlin ( 2001 ) and a number of previous authors .",
    "we use the data tabled by han and carlin from williams ( 1959 ) .",
    "these data describe the maximum compressive strength parallel to the grain @xmath311 , the density @xmath312 , and the resin - adjusted density @xmath313 for @xmath314 specimens .",
    "carlin and chib ( 1995 ) use these data to compare the two linear regression models : @xmath315 with @xmath316 , and @xmath317 .",
    "we follow han and carlin ( 2001 ) and carlin and chib ( 1995 ) , adopting @xmath318 priors on @xmath319 and @xmath320 , and @xmath321^{-1}\\right)$ ] priors on @xmath322 and @xmath323 , where @xmath324 is the inverse gamma distribution with density function @xmath325 where @xmath326 and @xmath327 .",
    "han and carlin point out these priors are approximately centered on the least - squares solution but are otherwise rather vague . using direct integration , green and ohagan ( 1998 ) find a bayes factor of about 4862 in favor of model 2 .",
    ".marginal likelihood for non - nested linear regression models [ cols=\"<,<,<,<,<\",options=\"header \" , ]     for each model of dimension @xmath23 , we compute a markov chain using the differential evolution algorithm ( de , * ? ? ?",
    "this algorithm evolves an ensemble of chains with initial conditions sampled from the prior distribution .",
    "a proposal is computing by randomly selecting pairs of states from the ensemble and using a multiple of their difference ; this automatically ` tunes ' the proposal width .",
    "we have further augmented this algorithm by including a tempered simulation step @xcite after every 20 de steps ( see * ? ? ?",
    "* for more details ) .",
    "each row describes of table [ tab : highd ] describes the application of the nla , vta , and laplace approximation to a model of dimension @xmath23 .",
    "the mcmc simulations produce approximately 1.4 million converged states .",
    "convergence is testing using the gelman - rubin statistic ( op .",
    "each converged chain is resampled with replacement to provide 1024 subsamples of @xmath257 states .",
    "the value @xmath328 $ ] is chosen to achieve 95% confidence intervals approximately 1% of @xmath219 or smaller .",
    "the 95% confidence intervals on @xmath219 are indicated as sub- and super - scripts .",
    "recall that the standard vta determines volume spanned @xmath214 samples and approximates the integral by multiplying the volume by the median value of the sample . to assess the variance inherent in this choice , i quote the results for two other p - quantiles , @xmath329 and @xmath330 .",
    "finally , for each algorithm , the table presents the relative error : @xmath331 .",
    "both the nla and vta results are very encouraging : the relative error is within a few percent for @xmath332 . for @xmath264 ,",
    "i computed @xmath219 with samples sizes of 400,000 states .",
    "both the nla and vta tend to slightly overestimate @xmath17 for large @xmath23 .",
    "the laplace approximation results are disappointing for small @xmath23 and improve for large @xmath23 , but still are less precise than either the nla or vta .    .",
    ", scaledwidth=70.0% ]    figure [ fig : k2d ] illustrates the kd - tree construction for a single @xmath333 sample .",
    "each two - dimensional cell is colored by the median value of the posterior probability for the @xmath334 points in each cell and scaled to the peak value of posterior probability @xmath335 for the entire sample .",
    "a careful by - eye examination of the cell shape reveals a preponderance of large axis - ratio rectangles ; this is a well - known artifact of the kd - tree algorithm . for large values of @xmath335 , the volume elements are small , and with a sufficiently large sample , the gradient in @xmath335 across the volume are small . for small values of @xmath335 ,",
    "the volume elements are large , the gradients are large , and the large - axis ratio rectangles distort the reconstructed shape of the true posterior .",
    "however , as described in ",
    "[ sec : newevid ] , the values of @xmath224 for an infinite sample , so a small number of distorted rectangles will not compromise the end result .",
    "moreover , the values of @xmath336 at large volumes are smaller than those at small volume for these tests , and this further decreases the importance of the kd - tree cell - shape artifact .",
    "but for a sample from the cauchy distribution from fig . .",
    "the box shows the quantiles and median , the whisker shows the ( 10% , 90% ) intervals , followed by outlying points .",
    "the three distributions are ( 1 ) the hma ; ( 2 ) nla for @xmath199 and vta for @xmath212 ; and ( 3 ) vta for both @xmath337 and @xmath219.,scaledwidth=50.0% ]    as an example of model selection , we first compute the marginal likelihood for the same data @xmath338 as in the first example of  [ sec : test ] but assuming a cauchy - lorentz distribution , @xmath339^{-1},\\ ] ] as the model with unknown location @xmath340 and scale @xmath183 parameters . for prior distributions , we take @xmath341 and @xmath342 where @xmath343 is the weibull distribution with scale parameter @xmath344 and shape parameter @xmath23 .",
    "nla yields @xmath345 , vta yields @xmath346 and the hma yields @xmath347 .",
    "the data and fits are shown in figure [ fig : cauchy]a .",
    "there should be no surprise that the true model ( with @xmath348)is strongly preferred .",
    "let us now repeat the experiment using 100 data points selected from the cauchy - lorentz distribution ( @xmath349 ) and compare the marginal likelihood values for a cauchy - lorentz distribution and a mixture of two normal distributions ( @xmath350 ) .",
    "nla and vta , respectively , yield @xmath351 and @xmath352",
    ". the hma yields @xmath353 and @xmath354 .",
    "regardless of the algorithm performing the test , the bayes factor reveals strong evidence in favor of the true model .",
    "note from figure [ fig : cauchy]b that both models are reasonable fits `` by eye '' .",
    "however , the bayes factor overwhelmingly _ prefers _ the simpler ( in this case , true ) model . as expected , the distribution of @xmath219 for the heavy - tailed cauchy distribution is much better behaved ( see fig .",
    "[ fig : subsamplec ] ) .",
    "the results for nla and vta are consistent and the hma is systematically larger , but non enough to misguide a decision .",
    "in summary , much of the general measure - theoretic underpinning of probability and statistics naturally leads naturally to the evaluation of expectation values .",
    "for example , the harmonic mean approximation ( hma , * ? ? ?",
    "* ) for the marginal likelihood has large variance and is slow to converge ( e.g. * ? ? ?",
    "on the other hand , the use of analytic density functions for the likelihood and prior permits us to take advantage of less general but possibly more powerful computational techniques . in   [ sec : intro][sec : algo ] we diagnose the numerical origin of the insufficiencies of the hma using lebesgue integrals .",
    "there are two culprits : 1 ) the integral on the left - hand side of equation ( [ eq : zdef0 ] ) may diverge if the measure function @xmath355 from equation ( [ eq : measurex ] ) decreases too slowly ; and 2 ) truncation error may dominate the quadrature of the left - hand side of equation ( [ eq : zdef0 ] ) unless the sample is appropriately truncated . using numerical quadrature for the marginal likelihood integral ( eqns . [",
    "eq : evidence ] and [ eq : measurex ] ) leads to improved algorithms : the _ numerical lebesgue algorithm _ ( nla ) and the _ volume tessellation algorithm _ ( vta ) .",
    "our proposed algorithms are a bit more difficult to implement and have higher computational complexity than the simple hma , but the overall cpu time is rather modest compared to the computational investment required to produce the mcmc - sampled posterior distribution itself . for a sample of size",
    "@xmath27 , the sorting required by nla and vta has computational complexity of @xmath247 and @xmath356 , respectively , rather than @xmath249 for the harmonic mean . nonetheless , the computational time is a fraction of second to minutes for typical values of @xmath357 ( see  [ sec : algo ] ) .",
    "the geometric picture behind nla is exactly that for lebesgue integration .",
    "consider integrating a function over a two - dimensional domain .",
    "in standard riemann quadrature , one chops the domain into rectangles and adds up their area .",
    "the sum can be refined by subdividing the rectangles ; in the limit of infinitesimal area , the resulting sum is the desired integral . in the lebesgue approach ,",
    "one draws horizontal slices through the surface and adds up the area of the horizontal rectangles formed from the width of the slice and the vertical distance between slices .",
    "the sum can be refined by making the slices thinner when needed ; in the limit of slices of infinitesimal height , the resulting sum is the desired integral . in the riemann case ,",
    "we multiply the box area in the domain , @xmath358 , by the function height , @xmath359 . in the lebesgue",
    ", we multiply the slice height in the range , @xmath360 , by the domain area , @xmath361 ( see fig .",
    "[ fig : geom ] ) .",
    "both algorithms easily generalize to higher dimension . for the lebesgue integral , the slices become level sets on the hypersurface implied by the integrand",
    ". therefore the lebesgue approach always looks one - dimensional in the level - set value ; the dimensionality @xmath23 is ` hidden ' in the area of domain ( _ hypervolume _",
    "@xmath361 for @xmath362 ) computed by the measure function @xmath37 .",
    "the level - set value for the nla is @xmath363 .",
    "once determined , nla applies the trapezoidal rule to the sum over slices and compute the upper and lower rectangle sums as bounds .",
    "clearly , the error control on this algorithm might be improved by using more of the information about the run of @xmath361 with @xmath359 .",
    "having realized that the practical failure of the harmonic mean approximation is a consequence of the sparsely sampled parameter - space domain , nla addresses the problem by determining a well - sampled subset @xmath207 from the mcmc sample , ex post facto .",
    "restricted to this subset , @xmath123 , the value of the integral @xmath122 on the right - hand side of equation ( [ eq : zdef0 ] ) is less than unity .",
    "we determine @xmath123 by a binary space partitioning ( bsp ) tree and compute @xmath122 from this partition .",
    "a bsp tree recursively partitions a the k - dimensional parameter space into convex subspaces .",
    "the vta is implemented with a kd - tree @xcite for simplicity .",
    "in addition , one may use vta by itself to compute equation ( [ eq : evidence ] ) directly",
    ".    judged by bias and variance , the test examples do not suggest a strong preference for either the nla or the vta .",
    "however , both are clearly better than the hma or the laplace approximation .",
    "conversely , because these algorithms exploit the additional structure implied by smooth , well - behaved likelihood and prior distribution functions , the algorithms developed here will be inaccurately and possibly fail miserably for _ wild _ density functions .",
    "the nla and the vta are not completely independent since the nla uses the tessellation from the vta to estimate the integral @xmath122 .",
    "however , the value of the integral @xmath150 tends to dominate @xmath17 , that is @xmath364 , and the contributions are easily checked .",
    "based on current results , i tentatively recommend relying preferentially on vta for the following reasons : 1 ) there is no intrinsic divergence ; 2 ) it appears to do as well as vta even in a high - dimensional space ; and 3 ) there is no truncation threshold @xmath299 .",
    "figure [ fig : k2d ] illustrates the potential for volume artifacts that could lead to both bias and variance .",
    "this error source affects both the vta and nla ( through the computation of @xmath212 ) but the affect on the nla may be larger (  [ sec : test ] ) .",
    "additional real - world testing , especially on high - dimensional multimodal posteriors , will provide more insight .",
    "in test problems described in this paper , i explored the effects of varying the threshold @xmath299 and the kd - tree bucket size @xmath214 .",
    "these parameters interact the sample distribution , and therefore , are likely to vary for each problem .",
    "i also recommend implementing both the nla , vta , htm , laplace approximation and comparing the four for each problem .",
    "we are currently testing these algorithms for astronomical inference problems too complex for a simple example ; the results will be reported in future papers .",
    "an implementation of these algorithms will be provided in the next release of the umass bayesian inference engine ( bie , * ? ? ?",
    "there are several natural algorithmic extensions and improvements not explored here . ",
    "[ sec : intgr ] describes a smoothed approximation to the computation of @xmath37 ( eqns .",
    "[ eq : nummeas][eq : itilden ] ) rather than the step function used in ",
    "[ sec : algo ] .",
    "the direct integration of equation ( [ eq : evidence ] ) currently ignores the location of field values in each cell volume . at the expense of cpu time",
    ", the accuracy might be improved by fitting the sampled points with low - order multinomials and using the fits to derive a cubature algorithm for each cell .",
    "in addition , a more sophisticated tree structure may decrease the potential for bias by providing a tessellation with `` rounder '' cells .    in conclusion ,",
    "the marginal likelihood @xmath365 may be reliably computed from a monte carlo posterior sample though careful attention to the numerics .",
    "we have demonstrated that the error in the hma is due to samples with very low likelihood values but significant prior probability .",
    "it follows that their posterior probability also very low , and these states tend to be outliers . on the other hand ,",
    "the converged posterior sample is a good representation of the posterior probability density by construction .",
    "the proposed algorithms define the subdomain @xmath207 dominated by and well - sampled by the posterior distribution and perform the integrals in equation ( [ eq : zdef0 ] ) over @xmath123 rather than @xmath4 .",
    "although more testing is needed , these new algorithms promise more reliable estimates for @xmath17 from an mcmc simulated posterior distribution with more general models than previous algorithms can deliver .",
    "i thank michael lavine for thoughtful discussion and both neal katz and michael lavine for comments on the original manuscript .",
    "it is also a pleasure to acknowledge the thoughtful and helpful comments of two anonymous referees and the associate editor of the journal .",
    "this work was supported in part by nsf iis program through award 0611948 and by nasa aisr program through award nng06gf25 g ."
  ],
  "abstract_text": [
    "<S> computation of the marginal likelihood from a simulated posterior distribution is central to bayesian model selection but is computationally difficult . </S>",
    "<S> the often - used harmonic mean approximation uses the posterior directly but is unstably sensitive to samples with anomalously small values of the likelihood . </S>",
    "<S> the laplace approximation is stable but makes strong , and often inappropriate , assumptions about the shape of the posterior distribution . </S>",
    "<S> it is useful , but not general . </S>",
    "<S> we need algorithms that apply to general distributions , like the harmonic mean approximation , but do not suffer from convergence and instability issues . here </S>",
    "<S> , i argue that the marginal likelihood can be reliably computed from a posterior sample by careful attention to the numerics of the probability integral . posing the expression for the marginal likelihood as a lebesgue integral </S>",
    "<S> , we may convert the harmonic mean approximation from a sample statistic to a quadrature rule . as a quadrature , </S>",
    "<S> the harmonic mean approximation suffers from enormous truncation error as consequence . </S>",
    "<S> this error is a direct consequence of poor coverage of the sample space ; the posterior sample required for accurate computation of the marginal likelihood is much larger than that required to characterize the posterior distribution when using the harmonic mean approximation . </S>",
    "<S> in addition , i demonstrate that the integral expression for the harmonic - mean approximation converges slowly at best for high - dimensional problems with uninformative prior distributions . </S>",
    "<S> these observations lead to two computationally - modest families of quadrature algorithms that use the full generality sample posterior but without the instability . </S>",
    "<S> the first algorithm automatically eliminates the part of the sample that contributes large truncation error . </S>",
    "<S> the second algorithm uses the posterior sample to assign probability to a partition of the sample space and performs the marginal likelihood integral directly . </S>",
    "<S> this eliminates convergence issues . </S>",
    "<S> the first algorithm is analogous to standard quadrature but can only be applied for convergent problems . </S>",
    "<S> the second is a hybrid of cubature : it uses the posterior to discover and tessellate the subset of that sample space was explored and uses quantiles to compute a representative field value . </S>",
    "<S> qualitatively , the first algorithm improves the harmonic mean approximation using numerical analysis , and the second algorithm is an adaptive version of the laplace approximation . </S>",
    "<S> neither algorithm makes strong assumptions about the shape of the posterior distribution and neither is sensitive to outliers . based on numerical tests , we recommend a combined application of both algorithms as consistency check to achieve a reliable estimate of the marginal likelihood from a simulated posterior distribution .     </S>",
    "<S> * keywords : * bayesian computation , marginal likelihood , algorithm , bayes factors , model selection </S>"
  ]
}