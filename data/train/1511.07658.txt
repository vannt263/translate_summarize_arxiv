{
  "article_text": [
    "recent years have seen the proliferation of graphics processing units ( gpus ) as application accelerators in high performance computing ( hpc ) systems , due to the rapid advancements in graphic processing technology over the past few years and the introduction of programmable processors in graphics processing units ( gpus ) , which is also known as gpgpu , or general - purpose computation on graphic processing units @xcite . as a result ,",
    "a wide range of hpc systems have incorporated gpus to accelerate applications by utilizing the unprecedented floating point performance and massively parallel processor architectures of modern gpus , which can achieve unparalleled floating point performance in terms of flops ( floating - point operations per second ) up to the teraflop barrier @xcite @xcite .",
    "such systems range from clusters of compute nodes to parallel supercomputers . while examples of gpu - based computer clusters can be found in academia for research purpose , such as @xcite and @xcite .",
    "the contemporary offerings from supercomputer vendors have begun to incorporate professional gpu computing cards into the compute blades of their parallel computer products ; example include the latest cray xk7 @xcite and sgi altix uv @xcite supercomputers .",
    "yet another notable example is the titan supercomputer @xcite currently ranking the 2^nd^ in the top 500 supercomputer list @xcite .",
    "titan is equipped with 18,688 nvidia tesla gpus and is thereby able to achieve a sustained 17.59 pflops linpack performance @xcite .",
    ".gpu - based supercomputers in the top 30 list [ cols=\"<,<,<,<\",options=\"header \" , ]     our previous execution models depict inter - process parallelism and overlapping under the sharing scheme achieved through gpu virtualization . as we use two kernel cases to analyze the overlapping behaviors in the execution model , here",
    "we first utilize two extreme benchmark cases ( highly compute - intensive and highly i / o - intensive ) as experimental evaluation of potential overlapping behavior .",
    "the purpose is to demonstrate the different overlapping behavior for c - i and io - i kernel cases and compare the actual performance gain with non - virtualization solution .",
    "the i / o - intensive application we use is a very large vector addition benchmark while the compute - intensive benchmark is the gpu version @xcite of ep ( problem size : m=30 ) from nas parallel benchmarks ( npb ) @xcite .",
    "the ep kernel grid size is designed small merely to show the effectiveness of concurrency under virtualization , while the actual grid size decides the overlapping and concurrency extent in real applications .",
    "we list all benchmark kernel profiles used in this section in table [ tb : apps ] .    our experiment with ep(m30 ) and",
    "vecadd primarily focuses on evaluating process turnaround time by emulating a process - level parallel spmd program for both benchmarks , while launching multiple processes with the same benchmark kernel simultaneously . as an spmd program generally requires _",
    "_ n~process~__@xmath0_n~processor~ _ and our computing node consists of 8 microprocessor cores , we varied the number of spmd parallel processes from 1 to the maximum of 8 .",
    "figure [ fig : vecadd ] and [ fig : ep ] demonstrate the effectiveness of gpu virtualization in reducing the process turnaround time with the increasing number of processes for both c - i and io - i cases .",
    "for the i / o - intensive benchmark in figure [ fig : vecadd ] , when the number of parallel processes increases , without virtualization , the turnaround time increase sharply due to the zero overlapping and context - switch overheads .",
    "with virtualization , the turnaround time still increases but much slowly comparatively .",
    "this is because i / o - intensive application can not achieve complete overlapping as explained in the model earlier , but can still partially overlap i / o as well as eliminate context - switch and initialization overheads . for the compute - intensive benchmark in figure [ fig : ep ] , with virtualization ,",
    "the turnaround time increases very little with the increasing number of processes , which clearly shows that our gpu virtualization implementation can achieve the expected execution concurrencies for smaller kernels only using a portion of the gpu resource in the case of compute - intensive kernel .",
    "cuda s current concurrent kernel execution support heavily depends on kernel profiles .",
    "in other words , blocks from multiple kernels are concurrently executed on separated sms inside gpu to achieve the concurrency when cuda streams are used .",
    "thus small kernels ( small number of blocks ) can achieve better kernel execution concurrency compared with large kernels . in the previous modeling analysis ,",
    "we assume kernel execution overlapping is complete since we are focused on studying overlapping behaviors . thus in order to verify our previous modeling analysis",
    ", we utilize ep(m24 ) and vecmult shown in table [ tb : apps ] as the benchmark kernels to verify c - i and io - i models , respectively . for both kernels ,",
    "we carry out initial profiling analysis to empirically derive _",
    "t~data_in~ _ , _ t~comp~ _ and _ t~data_out~_. as both execution models are to estimate the total execution time of _",
    "n~process~ _ kernels sharing the gpu under virtualization .",
    "the theoretical time can be derived using equation and respectively with the profiling results .",
    "experimentally , we here launch the emulated spmd kernel programs while varying _",
    "n~process~ _ from 1 to 8 .",
    "instead of measuring process turnaround time , we here only measure the time all kernels spend on sharing the gpu inside the gvm of our virtualization infrastructure .",
    "thus , we are able to avoid bringing unnecessary virtualization overheads into the model validation . comparisons of experimental results and modeling results are shown in figure [ fig : ep_model ] for c - i model and in figure [ fig : vecm_model ] for io - i model , both of which demonstrate accurate modeling results .",
    "we also note that for c - i model validation , utilizing ep(m24 ) with kernel size of one is merely to guarantee that all kernels are executed on separated sms ( up to 8 kernels in our case ) . in other words , complete overlapping of actual kernel computation",
    "can be achieved with kernels executed on separated sms .",
    "the comparisons in both figures validate our previous execution model results with an average model deviation of 0.42% for ep(m24 ) and 4.76% for vecmult .",
    "considering the virtualization infrastructure is an add - on layer , possible overhead can be added on top of the theoretical modeling results .",
    "as our implementation mainly uses posix shared memory and message queue , the vast majority overhead comes from data movement and message synchronization between the api layer and base layer .",
    "we here conduct another micro benchmark using the i / o - intensive vecadd benchmark with multiple data sizes .",
    "we measure the overhead by launching a single process and compare the time purely spent on the gpu in the base layer with the process turnaround time .",
    "as shown in figure [ fig : overhead ] , the overhead , which is the differences between the turnaround time and pure gpu time , increases with the size of data being transfered as expected . even in the case when the data size is very large ( 400 mb in our case ) , the virtualization overhead is measured around 20% , which demonstrates that our virtualization implementation incurs comparatively low overhead , especially considering that an add - on virtualization layer generally brings much more overhead .    as a further step",
    ", we conduct several additional benchmarks to demonstrate the efficiency of the proposed virtualization approach in addressing real - life applications with different profiles .",
    "as table [ tb : apps ] shows , mm refers to the 2048x2048 single precision floating - point matrix multiplication .",
    "mg and cg refer to gpu versions @xcite of npb @xcite kernel mg and cg , respectively , with the problem size of class s. black scholes @xcite is a european option pricing benchmark used in financial area , adapted from nvidia s cuda sdk .",
    "we set option prices over 512 iterations as default .",
    "electrostatics refers to fast molecular electrostatics algorithm as a part of the molecular visualization program vmd @xcite and we set the problem size to be 100k atoms with 25 iterations . by evaluating i / o and computing time ratio , we further profile the class of each benchmark as shown in table [ tb : apps ] .",
    "experimentally , we emulate process - level spmd execution of each benchmark kernel with multiple processes and compare the process turnaround time between virtualization and non - virtualization scenario .",
    "figure [ fig : mm ] , [ fig : mg ] , [ fig : bs ] , [ fig : cg ] and [ fig : es ] respectively compare the turnaround time with and without gpu virtualization .",
    "it is worth mentioning that the performance improvement using one process is due to the elimination of initialization overheads by the virtualization implementation , even with the add - on virtualization overhead . since mm is profiled as intermediate and the grid size is large enough to occupy the whole gpu , it partially benefits from both i / o and kernel computing overlapping with virtualization .",
    "both mg and cg are compute - intensive benchmarks and class s problem sizes ( small kernel sizes ) only make mg and cg utilize partial gpu resource .",
    "thus mg and cg can achieve more overlapping by concurrent kernel execution under virtualization . with the default problem size and a grid size of 480 ,",
    "a single black scholes benchmark can utilize full gpu resource and can hardly be concurrently executed under virtualization . since it is also i / o - intensive application , it is only able to achieve limited overlapping between the i / o and kernel - computing as described earlier . as for electrostatic benchmark ,",
    "since it is compute - intensive while the grid size of 288 making it occupy the whole gpu , the overlapping potential is small using virtualization .",
    "however , it still benefits from zero context - switch and initialization overhead due to virtualization .",
    "therefore , within the five application benchmarks , as each achieves certain amount of performance gain through virtualization due to overlapping and elimination of overheads , mg and cg achieve better performance gains .",
    "figure [ fig : sp ] summarizes an example speedup comparison scenario utilizing all available system processors ( 8 processes ) . including ep(m30 ) and",
    "vecadd as two extreme cases along with the five real - life benchmarks , all seven benchmarks we conduct achieve speedups from 1.4 to 7.4 with 8 process - level parallelism under gpu virtualization . therefore , while all benchmarks can achieve certain amount of performance gains , the efficiency of the virtualization approach also depends on the profiles of the applications , including the i / o and computing time ratio as well the gpu resource usage . to summarize from figure [ fig : sp ] ,",
    "small compute - intensive kernels can achieve the best performance improvement as ep(m30 ) , mg and cg show .",
    "intermediate kernels can achieve reasonable speedups with partial i / o and compute overlapping as shown from mm .",
    "i / o - intensive kernels ( bs and vecadd ) can only achieve i / o overlapping , while large compute - intensive kernels ( es ) can overlap i / o ( small portion ) and very limited kernel execution .",
    "thus they achieve relatively less performance gain .",
    "however , the elimination of context - switch and initialization overhead plus well - increased gpu utilization from gpu virtualization allow considerable speedup for application in general .",
    "in fact , our virtualization experimental results show a good agreement with the proposed analytical model , and demonstrate that our gpu virtualization implementation is an effective approach allowing multi - processes to share the gpu resource efficiently under spmd model , while incurring comparatively low overhead .",
    "in this paper , we proposed a gpu virtualization approach which enables efficient sharing of gpu resources among microprocessors in heterogeneous hpc systems under spmd execution model . in achieving the desired objective of making",
    "each microprocessors effectively utilize shared resources , we investigated the concurrency and overlapping potentials that can be exploited on the gpu device - level .",
    "we also analyzed the performance and overheads of direct gpu access and sharing from multiple microprocessors as a comparison baseline .",
    "we further provided an analytical execution model as a theoretical performance estimate of our proposed virtualization approach .",
    "the analytical model also provided us with better understanding of the methodologies in implementing our virtualization concept .",
    "based on these concepts and analyses , we implemented our virtualization infrastructure as a run - time layer running in the user space of the os .",
    "the virtualization layer manages requests from all microprocessors and provides necessary gpu resources to the microprocessors .",
    "it also exposes a vgpu view to all the microprocessors as if each microprocessor has its own gpu resource . inside the virtualization layer",
    ", we managed to eliminate unnecessary overheads and achieve possible overlapping and concurrency of executions . in the experiments",
    ", we utilized our nvidia fermi gpu computing node as the test bed .",
    "we used initial i / o - intensive and compute - intensive benchmarks as well as application benchmarks with multiple folds of analyses in our experiments .",
    "our experimental results showed that we were able to achieve considerable performance gains in terms of speedups with our virtualization infrastructure with low overhead .",
    "the experimental results also demonstrate an agreement with our theoretical analysis .",
    "proposed as a solution for microprocessor resource underutilization by providing a virtual spmd execution scenario , our approach proves to be effective and efficient and can be deployed to any heterogeneous gpu clusters with imbalanced cpu / gpu resources .",
    "37 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , , , , in : , , , pp . . .",
    ", , , , , in : , , , p.  .",
    ", , , , , , , , , in : , , , pp . . , , .",
    ", , . , . ,",
    ", , , , volume  , , . , . , .",
    ", , , , , , , , in : , , , pp . . , , , , in : , , , pp . . , , , , , ( ) . , , , , , , , in : , , , pp . . , , , , , in : , , .",
    ", , , ( ) . , , , , in : , , , pp . . .",
    ", , , , in : , cf 14 , , , , pp . .",
    "http://doi.acm.org/10.1145/2597917.2597925 . .        ,",
    "thesis , the george washington university , .",
    ", , , ( ) . , , , , , ( ) . , , , , in : , , , pp . . , , .",
    ", , , , , , , ( ) . http://dx.doi.org/10.1002/cpe.1860 . .",
    ", , , , , , , , , , et  al .",
    ", , , ( ) . , ."
  ],
  "abstract_text": [
    "<S> the high performance computing ( hpc ) field is witnessing a widespread adoption of graphics processing units ( gpus ) as co - processors for conventional homogeneous clusters . </S>",
    "<S> the adoption of prevalent single - program multiple - data ( spmd ) programming paradigm for gpu - based parallel processing brings in the challenge of resource underutilization , with the asymmetrical processor / co - processor distribution . in other words , under spmd , balanced cpu / gpu distribution is required to ensure full resource utilization . in this paper </S>",
    "<S> , we propose a gpu resource virtualization approach to allow underutilized microprocessors to efficiently share the gpus . </S>",
    "<S> we propose an efficient gpu sharing scenario achieved through gpu virtualization and analyze the performance potentials through execution models . </S>",
    "<S> we further present the implementation details of the virtualization infrastructure , followed by the experimental analyses . </S>",
    "<S> the results demonstrate considerable performance gains with gpu virtualization . </S>",
    "<S> furthermore , the proposed solution enables full utilization of asymmetrical resources , through efficient gpu sharing among microprocessors , while incurring low overhead due to the added virtualization layer .    </S>",
    "<S> gpu , virtualization , resource sharing , spmd , heterogeneous computing , high performance computing </S>"
  ]
}