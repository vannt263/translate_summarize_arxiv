{
  "article_text": [
    "perhaps the most basic estimation problem in statistics is the canonical problem of estimating a multivariate normal mean .",
    "based on the observation of a  @xmath0-dimensional multivariate normal random @xmath1 the problem is to find a suitable estimator @xmath2 of  @xmath3 .",
    "the celebrated result of stein ( @xcite ) dethroned @xmath4 , the maximum likelihood and best location invariant estimator for this problem , by showing that , when @xmath5 , @xmath6 is inadmissible under quadratic loss @xmath7 from a decision theory point of view , an important part of the appeal of @xmath6 was the protection offered by its minimax property .",
    "the worst possible risk  @xmath8 incurred by @xmath6 was no worse than the worst possible risk of any other estimator .",
    "stein s result implied the existence of even better estimators that offered the same minimax protection .",
    "he had begun the hunt for these better minimax estimators .    in a remarkable series of follow - up papers",
    "stein proceeded to set the stage for this hunt .",
    "james and stein ( @xcite ) proposed a new closed - form minimax shrinkage estimator @xmath9 the now well - known james  stein estimator , andshowed explicitly that its risk was less than @xmath10 for every value of @xmath3 when @xmath5 , that is , it uniformly dominated @xmath11 .",
    "the appeal of @xmath12 under  @xmath8 was compelling .",
    "it offered the same guaranteed minimax protection as @xmath6 while also offering the possibility of doing much better .",
    "stein ( @xcite ) , though primarily concerned with improved confidence regions , described a parametric empirical bayes motivation for ( [ jsestimator ] ) , describinghow  @xmath13 could be seen as a data - based approximation to the posterior mean @xmath14 the bayes rule which minimizes the average risk @xmath15 when @xmath16 .",
    "he here also proposed the positive - part james  stein estimator@xmath17 , a dominating improvementover  @xmath13 , and commented that `` it would be even better to use the bayes estimate with respect to a  reasonable prior distribution . ''",
    "these observations served as a clear indication that the bayesian paradigm was to play a major role in the hunt for these new shrinkage estimators , opening up a new direction that was to be ultimately successful for establishing large new classes of shrinkage estimators .",
    "dominating fully bayes shrinkage estimators soon emerged .",
    "strawderman ( @xcite ) proposed @xmath18 , a class of bayes shrinkage estimators obtained as posterior means under priors @xmath19 for  which @xmath20 strawderman explicitly showed that @xmath21 uniformly dominated @xmath6 and was proper bayes , when @xmath22 and @xmath23 or when @xmath24 and @xmath25 .",
    "this was especially interesting because any proper bayes was necessarily admissible and so could not be improved upon .",
    "then , stein ( @xcite ) showed that @xmath26 , the bayes estimator under the harmonic prior @xmath27 dominated @xmath6 when @xmath5 . a special case of @xmath28 when @xmath29 , @xmath30 was only formal bayes because @xmath31 is improper .",
    "undeterred , stein pointed out that the admissibility of @xmath30 followed immediately from the general conditions for the admissibility of generalized bayes estimators laid out by brown ( @xcite ) .",
    "a  further key element of the story was brown s ( @xcite ) powerful result that all such generalized bayes rules ( including the proper ones of course ) constituted a  complete class for the problem of estimating multivariate normal mean under quadratic loss .",
    "it was now clear that the hunt for new minimax shrinkage estimators was to focus on procedures with at least some bayesian motivation .",
    "perhaps even more impressive than the factthat  @xmath30 dominated @xmath6 was the way stein proved it . making further use of the rich results in brown ( @xcite ) , the key to his proof was the fact that any posterior mean bayes estimator under a prior @xmath32 can be expressed as @xmath33 where @xmath34 is the marginal distribution of @xmath35 under @xmath32 .",
    "[ here @xmath36 is the familiar gradient . ]    at first glance it would appear that ( [ keyrep1 ] ) has little to do with the risk . however , stein noted that insertion of ( [ keyrep1 ] ) into @xmath8 , followed by expansion and an integration - by - parts identity , now known as one of stein s lemmas , yields the following general expression for the difference between the risks of @xmath37 and  @xmath6 : @xmath38 \\\\[-8.5pt ] & & \\quad = e_\\mu \\biggl[\\|\\nabla\\log m_\\pi(x)\\|^2 - 2 \\frac{\\nabla^2 m_\\pi(x)}{m_\\pi ( x ) } \\biggr ] \\nonumber\\\\[-0.5pt ] \\label{uber1b } & & \\quad   =   e_\\mu \\bigl[-4\\nabla^2 \\sqrt{m_\\pi(x)}\\big/\\sqrt{m_\\pi ( x ) } \\bigr].\\end{aligned}\\ ] ] ( here @xmath39 is the familiar laplacian . )    because the bracketed terms in ( [ uber1a ] ) and ( [ uber1b ] ) do not depend on @xmath3 ( they are unbiased estimators of the risk difference ) , the domination of @xmath6 by @xmath37 would follow whenever @xmath40 was such that these bracketed terms were nonnegative .",
    "as stein noted , this would be the case in ( [ uber1a ] ) whenever @xmath40 was superharmonic , @xmath41 , and in ( [ uber1b ] ) whenever @xmath42  was superharmonic , @xmath43 , a weaker condition .",
    "the domination of @xmath6 by @xmath30 was seen now to be attributable directly to the fact that the marginal  ( [ margx ] ) under @xmath44 , a mixture of harmonic functions , is superharmonic when @xmath5 .",
    "however , such an explanation would not work for the domination of @xmath6 by  @xmath21 , because the marginal ( [ margx ] ) under  @xmath45 in ( [ pi_a ] ) is not superharmonic for any @xmath46 .",
    "indeed , as was shown later by fourdrinier , strawderman and wells ( @xcite ) , a superharmonic marginal can not be obtained with any proper prior .",
    "more importantly , however , they were able to establish that the domination by @xmath21 was attributable to the superharmonicity of @xmath47 under @xmath45 when @xmath48 ( and strawderman s conditions on @xmath49 ) .",
    "in fact , it also followed from their results that @xmath47 is superharmonic when @xmath50 and @xmath5 , further broadening the class of minimax improper bayes estimators .",
    "prior to the appearance of ( [ uber1a ] ) and ( [ uber1b ] ) , minimaxity proofs , though ingenious , had all been tailored to suit the specific estimators at hand .",
    "the sheer generality of this new approach was daunting in its scope . by restricting attention to priors that gave rise to marginal distributions with particular properties ,",
    "the minimax properties of the implied bayes rules would be guaranteed .",
    "[ sec : pred - emerges ]    the seminal work of stein concerned the canonical problem of how to estimate @xmath3 based on an observation of @xmath51 .",
    "a more ambitious problem is how to use such an @xmath35 to estimate the entire probability distribution of a future @xmath52 from a normal distribution with this same unknown mean @xmath3 , the so - called predictive density of @xmath52 .",
    "such a predictive density offers a complete description of predictive uncertainty .    to conveniently treat the possibility of different variances for @xmath35 and @xmath52",
    ", we formulate the predictive problem as follows .",
    "suppose @xmath53 and @xmath54 are independent @xmath0-dimensional multivariate normal vectors with common unknownmean @xmath3 but known variances @xmath55 and @xmath56 .",
    "letting @xmath57 denote the density of @xmath52 , the problem is to find an estimator @xmath58 of @xmath59 based on the observation of @xmath60 only .",
    "such a problem arises naturally , for example , for predicting @xmath61 based on the observation of @xmath62 which is equivalent to observing @xmath63 .",
    "this is exactly our formulation with @xmath64 and @xmath65 .    for the evaluation of @xmath58 as an estimator of @xmath66 ,",
    "the analogue of quadratic risk @xmath8 for the mean estimation problem is the kullback ",
    "leibler ( kl ) risk @xmath67 where @xmath68 denotes the density of @xmath35 , and @xmath69 is the familiar kl loss .    for a ( possibly improper ) prior distribution @xmath70 on  @xmath3 ,",
    "the average risk @xmath71 is minimized by the bayes rule @xmath72\\nonumber \\\\[-8pt ] \\\\[-8pt ] & = & \\int p(y { |}\\mu ) \\pi(\\mu{|}x)\\,d\\mu , \\nonumber\\end{aligned}\\ ] ] the posterior mean of @xmath59 under @xmath70 ( aitchison , @xcite ) .",
    "it follows from ( [ eq : bayes ] ) that @xmath73 is a proper probability distribution over @xmath74 whenever the marginal density of @xmath75 is finite for all @xmath76 ( integrate w.r.t .",
    "@xmath74 and switch the order of integration ) .",
    "furthermore , the mean of @xmath77 ( when it exists ) is equal to @xmath78 , the bayes rule for estimating @xmath3 under quadratic loss , namely the posterior mean of @xmath3 .",
    "thus , @xmath79 also carries the necessary information for that estimation problem .",
    "note also that unless @xmath70 is a trivial point prior , such @xmath77 will not be of the form of @xmath59 for any @xmath3 .",
    "the range of the bayes rules here falls outside the target space of the densities which are being estimated .",
    "a tempting initial approach to this predictive density estimation problem is to use the simple plug - in estimator @xmath80 to estimate @xmath59 , the so - called estimative approach .",
    "this was the conventional wisdom until the appearance of aitchison ( @xcite ) .",
    "he showed that the plug - in estimator @xmath81 is uniformly dominated under @xmath82 by @xmath83\\hspace*{-17pt}\\nonumber \\\\[-7pt ] \\\\[-8pt ]     & = & \\frac{1}{\\ { 2\\pi(v_x + v_y)\\}^{{{p}/{2 } } } }    \\exp \\biggl\\ { -\\frac{\\|y - x\\| ^2}{2(v_x + v_y ) }   \\biggr\\},\\hspace*{-17pt } \\nonumber\\end{aligned}\\ ] ] the posterior mean of @xmath59 with respect to the uniform prior @xmath84 , the so - called predictive approach . in a related vein ,",
    "akaike ( @xcite ) pointed out that , by jensen s inequality , the bayes rule @xmath77 would dominate the random plug - in estimator@xmath85 when @xmath86 is a random draw from @xmath70 .",
    "strategies for averaging over @xmath3 were looking better than plug - in strategies .",
    "the hunt for predictive shrinkage estimators had turned to bayes procedures .",
    "distinct from @xmath81 , @xmath87 was soon shown to be the best location invariant predictive density estimator ; see murray ( @xcite ) and ng ( @xcite ) . that @xmath87 is best invariant and minimax also follows from the more recent general results of liang and barron ( @xcite ) , who also showed that @xmath87 is admissible when @xmath88 .",
    "the minimaxity of @xmath87 was also shown directly by george , liang and xu ( @xcite ) .",
    "thus , @xmath87 , rather than @xmath81 , here plays the role played by @xmath6 in the mean estimation context .",
    "not surprisingly , @xmath89 , the posterior mean under the uniform prior @xmath90 is identical to @xmath6 in that context .",
    "the parallels between the mean estimation problem and the predictive estimation problem came into sharp focus with the stunning breakthrough result of komaki ( @xcite ) .",
    "he proved that when @xmath5 , @xmath91 itself is dominated by the bayes rule @xmath92,\\ ] ] under the harmonic prior @xmath93 in ( [ pi_h ] ) used by stein ( @xcite ) . shortly thereafter",
    "liang ( @xcite ) showedthat  @xmath94 is dominated by the proper bayes rule  @xmath95 under  @xmath96 for which @xmath97 when @xmath98 , and when @xmath22 and @xmath99 or and @xmath25 , the same conditions that strawderman had obtained for his estimator .",
    "notethat  @xmath19 in ( [ eq : pa ] ) is an extension of ( [ pi_a ] ) which depends on the constant @xmath100 .",
    "as before , @xmath93 is the special case of  @xmath19 when @xmath101 .",
    "note that @xmath87 is now playing the `` straw - man '' role that was played by @xmath6 in the mean estimation problem .",
    "[ sec : theory ]    the proofs of the domination of @xmath87 by @xmath102 in komaki ( @xcite ) and by @xmath103 in liang ( @xcite ) were both tailored to the specific forms of the dominating estimators .",
    "they did not make direct use of the properties of the induced marginal distributions of @xmath35 and  @xmath52 . from the theory developed by brown ( @xcite ) and stein ( @xcite ) for the mean estimation problem",
    ", it was natural to ask if there was a theory analogous to ( [ keyrep1])([uber1b ] ) which would similarly unify the domination results in the predictive density estimation problem .",
    "as it turned out , just such a theory was established in george , liang and xu ( @xcite ) , the main results of which we now proceed to describe .",
    "the story begins with a representation , analogous to brown s representation @xmath104 in ( [ keyrep1 ] ) , that is available for posterior mean bayes rules in the predictive density estimation problem .",
    "a key element of the representation is the form of the marginal distributions for our context which we denote by @xmath105 for @xmath106 and a prior @xmath32 . in terms of our previous notation ( [ margx ] ) , @xmath107 .",
    "[ thm : pform ] the bayes rule @xmath77 in ( [ eq : bayes ] ) can be expressed as @xmath108 where @xmath94 is the bayes rule under @xmath109 given by ( [ eq : piu ] ) , @xmath110 is the marginal distribution of @xmath35 , and @xmath111 , where @xmath112 , is the marginal distribution of @xmath113 for independent @xmath53 and @xmath54 .",
    "lemma [ thm : pform ] shows how the form of @xmath77 is determined entirely by @xmath94 and the form of @xmath114 and @xmath111 .",
    "the essential step in its derivation is to factor the joint distribution of @xmath75 and @xmath74 into terms including a function of the sufficient statistic @xmath115 . inserting the representation ( [ eq : mform ] ) into the risk",
    "@xmath82 leads immediately to the following unbiased estimate for the @xmath116 risk difference between @xmath117 and  @xmath77 : @xmath118    as one can see from ( [ eq : uber3 ] ) and the fact that @xmath119 , @xmath94 would be uniformly dominated by @xmath77 whenever @xmath120 is decreasing in @xmath121 . as if by magic , the sign of @xmath122 turned out to be directly linked to the same unbiased risk difference estimates ( [ uber1a ] ) and ( [ uber1b ] ) of stein ( @xcite ) .",
    "[ thm : rderiv ] @xmath123 \\\\[-8pt ] & &   \\quad \\quad   =   e_{\\mu , v }   \\biggl[\\frac { \\nabla^2 m_\\pi(z ; v)}{m_\\pi(z ; v ) } - \\frac{1}{2 } \\|\\nabla\\log m_\\pi(z ; v)\\|^2   \\biggr ] \\nonumber \\\\ \\label{uber2a } & &   \\quad \\quad   =   e_{\\mu , v } \\bigl [ 2\\nabla^2 \\sqrt{m_\\pi(z ; v)}\\big/\\sqrt{m_\\pi(z ; v ) } \\bigr].\\end{aligned}\\ ] ]    the proof of lemma [ thm : rderiv ] relies on brown s representation , stein s lemma , and the fact that any normal marginal distribution @xmath124 satisfies @xmath125 the well - known heat equation which has a long history in science and engineering ; for example , see steele ( @xcite )",
    ". combining ( [ eq : uber3 ] ) and lemma [ thm : rderiv ] with the fact that @xmath117 is minimax yields the following general conditions for the minimaxity of a predictive density estimator , conditions analogous to those obtained by stein for the minimaxity of a normal mean estimator .",
    "[ theo1 ] if @xmath126 is finite for all @xmath76 , then @xmath73 will be minimax if either of the following hold for all @xmath127 :    @xmath128 is superharmonic .",
    "@xmath129 is superharmonic .",
    "although condition ( i ) implies the weaker condition  ( ii ) above , it is included because of its convenience when it is available . since a superharmonic prior always yields a superharmonic @xmath128 for all  @xmath121 , the following corollary is immediate .",
    "[ cor1 ] if @xmath126 is finite for all @xmath76 ,  then @xmath73 will be minimax if @xmath32 is superharmonic .    because @xmath130 is superharmonic , it is immediate from corollary  [ cor1 ] that @xmath102 is minimax . because @xmath131 is superharmonic for all @xmath121 ( under suitable conditions on @xmath49 ) , it is immediate from theorem  [ theo1 ] that  @xmath103 is  minimax .",
    "it similarly follows that any of the improper superharmonic @xmath132-priors of faith ( @xcite ) or any of the proper generalized @xmath132-priors of fourdrinier , strawderman and wells ( @xcite ) yield minimax bayes rules .",
    "the connections between the unbiased risk difference estimates for the kl risk and quadratic risk problems ultimately yields the following identity : @xmath133 \\\\[-8pt ] & &   \\quad \\quad= \\frac{1}{2 } \\int_{v_w}^{v_x } \\frac{1}{v^2 } [ r_q(\\mu , \\hat\\mu _ u ) - r_q(\\mu , \\hat\\mu_\\pi )   ] _ v \\,dv , \\nonumber\\end{aligned}\\ ] ] explaining the parallel minimax conditions in both problems .",
    "brown , george and xu ( @xcite ) used this identity to further draw out connections to establish sufficient conditions for the admissibility of bayes rules under kl loss , conditions analogous to those of brown ( @xcite ) and brown and hwang ( @xcite ) , and to show that all admissible procedures for the kl risk problems are bayes rules , a direct parallel of the complete class theorem of brown ( @xcite ) for quadratic risk .",
    "the james  stein estimator @xmath13 in ( [ jsestimator ] ) provided an explicit example of how risk improvements for estimating @xmath3 are obtained by shrinking @xmath35 toward 0 by the adaptive multiplicative factor @xmath134 .",
    "similarly , under unimodal priors , posterior mean bayes rules @xmath135 shrink @xmath75 toward the center of  @xmath136 , the mean of @xmath32 when it exists .",
    "( section  [ sec : multshrink ] will describe how multimodal priors yield multiple shrinkage estimators . ) as we saw earlier , @xmath75 here plays the role both of @xmath137 and of the formal bayes estimator @xmath138 .",
    "the representation ( [ eq : mform ] ) reveals how @xmath77 analogously `` shrinks '' the formal bayes estimator @xmath94 , but not @xmath139 , by an adaptive multiplicative factor @xmath140 however , because @xmath77 must be a proper probability distribution ( whenever @xmath40 is always finite ) , it can not be the case that @xmath141 for all @xmath74 at any  @xmath75 .",
    "thus , `` shrinkage '' here really refers to a  reconcentration of the probability distribution of  @xmath94 .",
    "furthermore , since the mean of  @xmath142 is  @xmath78 , this reconcentration , under unimodal priors , is toward the center of @xmath32 , as in the mean estimation case .",
    "consider , for example , what happens under @xmath130 which is symmetric and unimodal about 0 .",
    "figure  [ fig1 ] illustrates how this shrinkage occurs for @xmath143 for various values of @xmath75 when @xmath22 . figure  [ fig1 ] plots @xmath117 and @xmath144 as functions of @xmath145 when @xmath146 and @xmath147 .",
    "note first that @xmath117 is always the same symmetric shape centered at @xmath75 .",
    "when @xmath148 , shrinkage occurs by pushing the concentration of @xmath144 = @xmath149 toward 0 . as @xmath75 moves further from @xmath150 to @xmath151 and @xmath152 this shrinkage diminishes as @xmath144 becomes more and more similar to @xmath94 .",
    "as in the problem of mean estimation , the shrinkage by @xmath102 manifests itself in risk reduction over  @xmath87 . to illustrate this , figure  [ fig2 ] displays the risk difference",
    "@xmath153 $ ] at @xmath154 , @xmath155 when @xmath146 and @xmath147 for dimensions @xmath156 . paralleling the risk reduction offered by  @xmath157 in the mean estimation problem ,",
    "the largest risk reduction offered by @xmath102 occurs close to @xmath158 and decreases rapidly to 0 as @xmath159 increases .",
    "[ @xmath160 is constant as a function of @xmath3 .",
    "] at the same time , the risk reduction by @xmath102 is larger for larger @xmath0 at each fixed @xmath159 .",
    "[ sec : targets ]    by a simple shift of coordinates , the modified james ",
    "stein estimator , @xmath161 remains minimax , but now shrinks @xmath75 toward @xmath162 where its risk function is smallest .",
    "similarly , minimax bayes shrinkage estimators of a mean or of a  predictive density can be shifted to shrink toward  @xmath163 , by recentering the prior @xmath32 to @xmath164 .",
    "these shifted estimators are easily obtained by inserting the     and @xmath165 when @xmath166 , @xmath167 , @xmath168 . ]",
    "corresponding translated marginal @xmath169 into ( [ keyrep1 ] ) to obtain @xmath170 and into ( [ eq : mform ] ) to obtain @xmath171 recentered unimodal priors such as @xmath172 and @xmath173 yield estimators that now shrink @xmath75 and @xmath94 toward @xmath163 rather than toward 0 .",
    "since the superharmonic properties of @xmath40 are inherited by @xmath174 , the minimaxity of such estimators will be preserved .    in his discussion of stein ( @xcite ) ,",
    "lindley ( @xcite ) noted that the james  stein estimator could be modified to shrink toward @xmath175 ( @xmath176 is the mean of the components of @xmath75 ) , by replacing @xmath163 and in  ( [ jsb ] ) by @xmath177 and @xmath178 , respectively .",
    "the resulting estimator remains minimax as long as and offers smallest risk when @xmath179 is close to the subspace of @xmath3 with identical coordinates , the subspace spanned by the vector @xmath180 .",
    "note that @xmath177 is the projection of @xmath75 into this subspace .",
    "more generally , minimax bayes shrinkage estimators of a mean or of a predictive density can be similarly modified to obtain shrinkage toward any ( possibly affine ) subspace @xmath181 , whenever they correspond to spherically symmetric priors .",
    "such priors , which include @xmath130 and @xmath45 , are functions of @xmath3 only through @xmath159 .",
    "such a modification is obtained by recentering the prior @xmath32 around @xmath182 via @xmath183 where @xmath184 is the projection of  @xmath3 onto @xmath182 .",
    "effectively , @xmath185 puts a uniform prior on  @xmath186 and applies a suitably modified version of  @xmath70 to @xmath187 .",
    "note that the dimension of @xmath187 , namely @xmath188 , must be taken into account when determining the appropriate modification for  @xmath70 .",
    "for example , recentering the harmonic prior @xmath189 around the subspace spanned by @xmath190 yields @xmath191 where @xmath192 . here ,",
    "the uniform prior is put on @xmath193 , and the harmonic prior in dimension @xmath194 ( which is different from the harmonic prior in @xmath195 ) is put on @xmath196 , the orthogonal complement of @xmath182 .",
    "the marginal @xmath197 corresponding to the recentered  @xmath198 in ( [ eq : phb ] ) can be directly obtained by recentering the spherically symmetric marginal @xmath40 corresponding to @xmath70 , that is , @xmath199 where @xmath200 is the projection of @xmath76 onto @xmath182 .",
    "analogously to @xmath201 , @xmath202 is uniform on @xmath200 and applies a suitably modified version of @xmath40 to @xmath203 . here ,",
    "too , the dimension of @xmath203 , namely @xmath188 , must be taken into account when determining the appropriate modification for @xmath40 .",
    "for example , recentering the marginal @xmath40 around the subspace spanned by @xmath190 would entail replacing @xmath204 by @xmath205 , where @xmath206 , and appropriately modifying @xmath40 to apply to @xmath207 .    applying the recentering ( [ eq : phb ] ) to priors such as  @xmath130 and @xmath45 ,",
    "which are unimodal around 0 , yields priors  @xmath208 and @xmath209 and hence marginals @xmath210 and @xmath211 , which are unimodal around @xmath182 . such recentered marginals yield mean estimators @xmath212 and predictive density estimators @xmath213 that now shrink @xmath75 and @xmath94 toward @xmath182 rather than toward 0 .",
    "shrinkage will be largest when @xmath214 , and will diminish as @xmath75 moves away from @xmath182 .",
    "these estimators offer smallest risk when @xmath215 , but do not improve in any important way over @xmath75 and @xmath216 when @xmath3 is far from @xmath182 .",
    "a superharmonic @xmath40 will lead to a superharmonic  @xmath197 as long as @xmath188 is large enough .",
    "for example , the recentered marginal @xmath210 will be superharmonic only when @xmath217 . in such cases ,",
    "the minimaxity of both @xmath218 and @xmath219 will be preserved .",
    "[ sec : multshrink ]    stein s discovery of the existence of minimax shrinkage estimators such as @xmath220 in ( [ jsb ] ) demonstrated that costless improvements over the minimax @xmath6 were available near any target preselected by the statistician .",
    "as stein ( @xcite ) put it when referring to the use of such an estimator to center a confidence region , the target `` should be chosen   as one s",
    "best guess '' of @xmath179 . that frequentist considerations had demonstrated the folly of ignoring subjective input was quite a shock to the perceived `` objectivity '' of the frequentist perspective .",
    "although the advent of minimax shrinkage estimators of the form @xmath221 in ( [ keyrepb ] ) and @xmath219 in ( [ eq : bform ] ) opened up the possibility of small risk near any preselected ( affine ) subspace @xmath181 ( this includes the possibility that @xmath182 is a single point ) , it also opened up a  challenging new problem , how to best choose such a @xmath182 . from the vast number of possible choices , the goal was to choose @xmath182 close to the unknown @xmath3 , otherwise risk reduction would be negligible . to add to the difficulties , low - dimensional @xmath182 , which offered the greatest risk reduction ,",
    "were also the most difficult to get close to @xmath3 .    when faced with a number of potentially good target choices ,",
    "say @xmath222 , rather than choose one of them and proceed with @xmath218 or @xmath219 , an attractive alternative is to use a minimax multiple shrinkage estimator ; see george ( @xcite ) .",
    "such estimators incorporate all the potential targets by combining them into an adaptive convex combination of @xmath223 for mean estimation , and of @xmath224 for predictive density estimation . by adaptively shrinking toward the more promising targets , the region of potential risk reduction",
    "is vastly enlarged while at the same time retaining the safety of minimaxity .",
    "the construction of these minimax multiple shrinkage estimators proceeds as follows , again making fundamental use of the bayesian formulation . for a  spherically symmetric prior @xmath32 , a set of subspaces @xmath222 of @xmath195 , and a set of nonnegative weights @xmath225 such that @xmath226 , consider the mixture prior @xmath227 where each @xmath228 is a recentered prior as in ( [ eq : phb ] ) . to simplify notation",
    ", we consider the case whereeach  @xmath228 is a recentering of the same @xmath70 , although in principle such a construction could be applied with different priors .",
    "the marginal @xmath229 corresponding to the mixture prior @xmath230 in ( [ eq : priorstar ] ) is then simply @xmath231 where @xmath232 are the recentered marginals corresponding to the @xmath228 as given by ( [ eq : mb ] ) .",
    "applying brown s representation @xmath233 from ( [ keyrep1 ] ) with @xmath229 in ( [ eq : mstar ] ) immediately yields the multiple shrinkage estimator of @xmath3 , @xmath234 where @xmath235    similarly , applying the representation @xmath236 from ( [ eq : mform ] ) with @xmath229 immediatelyyields the multiple shrinkage estimator of @xmath59 , @xmath237 where @xmath238    the forms ( [ pstar1 ] ) and ( [ pstar2 ] ) reveal @xmath239 and @xmath240 to be adaptive convex combination of the individual posterior mean estimators @xmath241 and @xmath242 , respectively .",
    "the adaptive weights @xmath243 in ( [ pprob1 ] ) and ( [ pprob2 ] ) are the posterior probabilities that @xmath179 is contained in each of the @xmath244 , effectively putting increased weight on those individual estimators which are shrinking most .",
    "note that the uniform prior estimates  @xmath245and  @xmath87 are here doubly shrunk by @xmath239 and @xmath246 ; in addition to the individual estimator shrinkage they are further shrunk by the posterior probability @xmath247 .",
    "the key to obtaining @xmath239 and @xmath246 which are minimax is simply to use priors which yield superharmonic @xmath248 .",
    "if such is the case , then trivially from ( [ eq : mstar ] ) @xmath249 so that @xmath229 will be superharmonic , and the minimaxity of @xmath250 and @xmath246 will follow immediately . note that marginals whose square root is superharmonic will not be adequate , as this argument will fail .     and",
    "multiple shrinkage @xmath251 when @xmath252 , @xmath167 , @xmath168 , @xmath253 , @xmath254 , and @xmath255 . ]",
    "the adaptive shrinkage behavior of @xmath239 and @xmath240 manifests itself as substantial risk reduction whenever @xmath3 is near any of @xmath256 .",
    "let us illustrate how that happens for the predictive density estimator @xmath257 , the multiple shrinkage version of @xmath258 .",
    "figure  [ fig3 ] illustrates the risk reduction @xmath259 $ ] at various @xmath154 obtained by @xmath257 which adaptively shrinks @xmath94 toward the closer of the two points @xmath260 and @xmath261 using equal weights @xmath262 .",
    "as in figure  [ fig2 ] , we considered the case @xmath263 for @xmath156 .",
    "as the plot shows , maximum risk reduction occurs when @xmath3 is close to @xmath264 or @xmath265 , and goes to 0 as @xmath3 moves away from either of these points . at the same time , for each fixed @xmath266 , risk reduction by @xmath257 is larger for larger @xmath0 .",
    "it is impressive that the size of the risk reduction offered by @xmath267 is nearly the same as each of its single target counterparts .",
    "the cost of multiple shrinkage enhancement seems negligible , especially compared to the benefits .",
    "beyond their attractive risk properties , the james  stein estimator @xmath12 and its positive - part counterpart @xmath268 are especially appealing because of their simple closed forms which are easy to compute .",
    "as shown by xu and zhou ( @xcite ) , similarly appealing simple closed - form predictive density shrinkage estimators can be obtained by the same empirical bayes considerations that motivate @xmath269 and @xmath268 .",
    "the empirical bayes motivation of @xmath12 , alluded to in section [ sec1 ] , simply entails replacing @xmath270 in ( [ conjugate - coef ] ) by @xmath271 , its unbiased estimate under the marginal distribution of @xmath272 when @xmath273 .",
    "the positive - part @xmath268 is obtained by using the truncated estimate @xmath274 which avoids an implicitly negative estimate of the prior variance @xmath275 .    proceeding analogously , xu and zhou considered the bayesian predictive density estimate , @xmath276 \\\\[-8pt ]   & & \\hspace*{65pt}\\frac{v_x}{v_x + \\nu}v_y +   \\biggl(1-\\frac{v_x}{v_x + \\nu } \\biggr)(v_x+v_y )   \\biggr ) , \\nonumber\\end{aligned}\\ ] ] when @xmath53 and @xmath277 are independent , and @xmath278 . replacing @xmath279 by its truncated unbiased estimate @xmath280 under the marginal distribution of @xmath35",
    ", they obtained the empirical bayes predictive density estimate @xmath281 \\\\[-8pt ]    & & \\hphantom{\\hat p_{p-2}(y { |}x ) \\sim",
    "n_p   \\biggl ( } v_y +   \\biggl(1-\\frac{(p-2)v_x}{\\|x\\|^2 } \\biggr)_+ v_x   \\biggr ) \\nonumber\\end{aligned}\\ ] ] where @xmath282 , an appealing simple closed form .",
    "centered at @xmath268 , @xmath283 converges to the best invariant procedure @xmath284 as @xmath285 , and converges to @xmath286 as @xmath287 .",
    "thus , @xmath283 can be viewed as a shrinkage predictive density estimator that `` pulls '' @xmath288 toward @xmath289 , its shrinkage adaptively determined by the data .    to assess the kl risk properties of such empirical bayes estimators",
    ", xu and zhou considered the class of estimators @xmath290 of the form ( [ eq : positive_js_hp ] ) with @xmath291 replaced by a constant @xmath292 , a class of simple normal forms centered at shrinkage estimators of @xmath3 with data - dependent variances to incorporate estimation uncertainty .",
    "for this class , they provided general sufficient conditions on @xmath292 and the dimension @xmath0 for  @xmath290 to dominate the best invariant predictive density  @xmath288 and thus be minimax .",
    "going further , they also established an `` oracle '' inequality which suggests that the empirical bayes predictive density estimator is asymptotically minimax in infinite - dimensional parameter spaces and can potentially be used to construct adaptive minimax estimators .",
    "it appears that these minimax empirical bayes predictive densities may play the same role as the james  stein estimator in such problems .",
    "it may be of interest to note that a particular pseudo - marginal empirical bayes construction that works fine for the mean estimation problem appears not to work for the predictive density estimation problem .",
    "for instance , the positive - part james  stein estimator @xmath293 can be expressed as @xmath294 , where @xmath295 is the function @xmath296 with @xmath297 ( see stein , @xcite ) .",
    "we refer to @xmath298 as a pseudo - marginal because it is not a bona fide marginal obtained by a real prior .",
    "nonetheless , it plays the formal role of a marginal in the mean estimation problem , and can be used to generate further innovations such as minimax multiple shrinkage james  stein estimators ( see george , @xcite ) .",
    "proceeding by analogy , it would seem that @xmath298 could be inserted into the representation ( [ eq : mform ] ) from lemma [ thm : pform ] to obtain similar results under kl loss .",
    "unfortunately , this does not yield a suitable minimax predictive estimator because @xmath299 is not a  proper probability distribution .",
    "indeed , @xmath300 and varies with @xmath75 .",
    "what has gone wrong ? because they do not correspond to real priors , such pseudo - marginals are ultimately at odds with the probabilistic coherence of a valid bayesian approach .",
    "in contrast to the mean estimation framework , the predictive density estimation framework apparently requires stronger fidelity to the bayesian paradigm .",
    "moving into the multiple regression setting , stein ( @xcite ) considered the estimation of a @xmath0-dimensional coefficient vector under suitably rescaled quadratic loss .",
    "he there established the minimaxity of the maximum likelihood estimators , and then proved its inadmissibility when @xmath301 , by demonstrating the existence of a dominating shrinkage estimator .    in a similar vein , as one might expect , the theory of predictive density estimation presented in sections [ sec : pred - emerges ] and [ sec : theory ] can also be extended to the multiple regression framework .",
    "we here describe the main ideas of the development of this extension which appeared in george and xu ( @xcite ) .",
    "similar results , developed independently from a slightly different perspective , appeared at the same time in kobayashi and komaki ( @xcite ) .",
    "consider the canonical normal linear regression setup : @xmath302 where @xmath303 is a full rank , fixed @xmath304 , @xmath182 is a fixed @xmath305 matrix , and @xmath306 is a common @xmath307 unknown regression coefficient .",
    "the error variance @xmath308 is assumed to be known , and set to be @xmath309 without loss of generality .",
    "the problem is to find an estimator of @xmath58 of the predictive density @xmath310 , evaluating its performance by kl risk @xmath311 where @xmath312 is the kl loss between the density @xmath313 and its estimator @xmath314    the story begins with the result , analogous to aitchison s ( @xcite ) for the normal mean problem , that the plug - in estimator @xmath315 , where @xmath316 is the least squares estimate of @xmath306 based on @xmath75 , is dominated under kl risk by the posterior mean of @xmath313 , the bayes rule under the uniform prior @xmath317 \\\\[-8pt ] & & { } \\times\\exp \\biggl\\ { - \\frac { { \\mathit{rss}}_{x , y } - { \\mathit{rss}}_{x}}{2 }   \\biggr\\}. \\nonumber\\end{aligned}\\ ] ] here , too , @xmath87 is minimax ( liang , @xcite ; liang and barron , @xcite ) and plays the straw - man role of the estimator to beat .",
    "the challenge was to determine which priors @xmath70 would lead to bayes rules which dominated @xmath87 , and hence would be minimax .",
    "analogously to the representation ( [ eq : mform ] ) in lemma [ thm : pform ] for the normal mean problem , the following representation for a bayes rule @xmath77 here , was the key to meeting this challenge .",
    "[ thm : pform : reg ] the bayes rule @xmath318 can be expressed as @xmath319 where @xmath320 @xmath321 @xmath322 , @xmath316 is the least squares estimates of @xmath306 based on @xmath75 , and  @xmath323 based on @xmath75 and @xmath74 , and @xmath324 is the marginal distribution of @xmath325 under @xmath326 .",
    "the representation ( [ eq : mform : reg ] ) leads immediately to the following analogue of ( [ eq : uber3 ] ) for the kl risk difference between @xmath94 and @xmath77 : @xmath327 the challenge thus became that of finding conditions on @xmath40 to make this difference positive , a challenge made more difficult than the previous one for ( [ eq : uber3 ] ) because of the complexity of @xmath328 and @xmath329 .",
    "fortunately this could be resolved by rotating the problem as follows to obtain diagonal forms .",
    "since @xmath330 and @xmath329 are both symmetric and positive definite , there exists a full rank @xmath331 matrix @xmath332 , such that @xmath333 \\\\[-8pt ] d&= & \\operatorname{diag}(d_1,\\ldots , d_p ) .",
    "\\nonumber\\end{aligned}\\ ] ] because @xmath334 where @xmath335 is nonnegative definite , it follows that @xmath336 $ ] for all @xmath337 with at least one @xmath338 .",
    "thus , the parameters for the rotated problem become @xmath339 \\\\[-8pt]\\hat{\\mu}_{x , y } & = & w^{-1 } \\hat{\\beta}_{x , y } \\sim n_p(\\mu , d ) .",
    "\\nonumber\\end{aligned}\\ ] ] letting @xmath340 for @xmath341 $ ] , the risk difference ( [ eq : uber3:reg ] ) could be reexpressed as @xmath342 \\\\[-8pt ] & & { } \\qquad - e_{\\mu , i } \\log m_{\\pi_w } ( \\hat{\\mu}_{x } ; i ) \\nonumber\\\\ & & \\quad = h_{\\mu}(v_0 ) - h_{\\mu}(v_1 ) , \\nonumber\\end{aligned}\\ ] ] where @xmath343 and @xmath344 .",
    "the minimaxity of @xmath79 would now follow from conditions on @xmath345 such that @xmath346 for all @xmath3 and @xmath341.$ ] the following substantial generalizations of theorem  [ theo1 ] and corollary  [ cor1 ] provide exactly those conditions .",
    "suppose @xmath347 is finite for all @xmath76 with the invertible matrix @xmath332 defined as in ( [ eq : w : reg ] ) .",
    "let @xmath348 be the hessian matrix of @xmath349 .    if @xmath350   \\ } \\le0$]for all @xmath341 $ ] , then @xmath351 is minimax .    if @xmath352   \\ } \\le0$]for all @xmath341 $ ] , then @xmath353 is minimax .",
    "suppose @xmath347 is finitefor all @xmath76 .",
    "then @xmath351 is minimax if @xmath354   \\ } \\le0    \\quad a.e.\\ ] ]    as a consequence of corollary 2 , the scaled harmonic prior @xmath355 can be shown to yield minimax predictive density estimators for the regression setting .",
    "going further , george and xu ( @xcite ) went on to show that the minimax bayes estimators here can be modified to shrink toward different points and subspaces as in section  [ sec5 ] , and that the minimax multiple shrinkage constructions of section  [ sec6 ] apply as well .",
    "in particular , they obtained minimax multiple shrinkage estimators that naturally accommodate variable selection uncertainty .",
    "moving in another direction , xu and liang ( @xcite ) considered predictive density estimation in the context of modern nonparametric regression , a context in which the james  stein estimator has turned out to play an important asymptotic minimaxity role ; see wasserman ( @xcite ) .",
    "their results pertain to the canonical setup for nonparametric regression : @xmath356 where @xmath349 is an unknown smooth function in @xmath357 $ ] , @xmath358 , and @xmath359 s are i.i.d .",
    "@xmath360 . a central problem here is to estimate @xmath349 or various functionals of  @xmath349 based on observing @xmath361 . transforming the problem with an orthonormal basis , ( [ eq : x : nonpa ] )",
    "is equivalent to estimating the @xmath362 s in @xmath363 known as the gaussian sequence model .",
    "the model above is different from the ordinary multivariate normal model in two aspects : ( 1 ) the model is increasing with the sample size , and ( 2 )  under function space assumptions on @xmath349 , the @xmath362 s lie in a  constrained space , for example , an ellipsoid @xmath364 .",
    "a large body of literature has been devoted to minimax estimation of @xmath349 under @xmath365 risk over certain function spaces ; see , for example , johnstone ( @xcite ) , efromovich ( @xcite ) , and the references therein . as opposed to the ordinary multivariate normal mean problem",
    ", exact minimax analysis is difficult for the gaussian sequence model ( [ seq : model : nonpa ] ) when a constraint on the parameters is considered .",
    "this difficulty has been overcome by first obtaining the minimax risk of a subclass of estimators of a simple form , and then showing that the overall minimax risk is asymptotically equivalent to the minimax risk of the subclass .",
    "for example , an important result from pinsker ( @xcite ) is that when the parameter space is constrained to an ellipsoid , the nonlinear minimax risk is asymptotically equivalent to the linear minimax risk , namely the minimax risk of the subclass of linear estimators of the form @xmath366 .    for nonparametric regression , the following analogue between estimation under @xmath365 risk and predictive density estimation under kl risk was established in xu and liang ( @xcite ) .",
    "the prediction problem for nonparametric regression is formulated as follows .",
    "let @xmath367 be future observations arising at a set of dense ( @xmath368 ) and equally spaced locations @xmath369 .",
    "given @xmath349 , the predictive density @xmath370 is just a product of gaussians .",
    "the problem is to find an estimator @xmath371 of @xmath372 , where performance is measured by the averaged kl risk @xmath373 in this formulation , densities are estimated at the  @xmath374 locations simultaneously by @xmath371 . as it turned out",
    ", the kl risk based on the simultaneous formula - tion  ( [ risk : simultaneous : nonpa ] ) is the analog of the @xmath365 risk for estimation .",
    "indeed , under the kl risk ( [ risk : simultaneous : nonpa ] ) , the prediction problem for a nonparametric regression model can be converted to the one for a gaussian sequence model .",
    "based on this formulation of the problem , minimax analysis proceeds as in the general framework for the minimax study of function estimation used by , for example , pinsker ( @xcite ) and belitser and  levit ( @xcite , @xcite ) .",
    "the linear estimators there , which play a central role in their minimax analysis , take the same form as posterior means under normal priors .",
    "analogously , predictive density estimates under the same normal priors turned out to play the corresponding role in the minimax analysis for prediction .",
    "( the same family of bayes rules arises from the empirical bayes approach in section  [ sec7 ] . )",
    "thus , xu and liang ( @xcite ) were ultimately able to show that the overall minimax kl risk is asymptotically equivalent to the minimax kl risk of this subclass of bayes rules , a direct analogue of pinker s theorem for predictive density estimation in nonparametric regression .",
    "stein s ( @xcite ) discovery of the existence of shrinkage estimators that uniformly dominate the minimax maximum likelihood estimator of the mean of a multivariate normal distribution under quadratic risk when @xmath375 was the beginning of a major research effort to develop improved minimax shrinkage estimation . in subsequent papers",
    "stein guided this effort toward the bayesian paradigm by providing explicit examples of minimax empirical bayes and fully bayes rules .",
    "making use of the fundamental results of brown ( @xcite ) , he developed a general theory for establishing minimaxity based on the superharmonic properties of the marginal distributions induced by the priors .",
    "the problem of predictive density estimation of a  multivariate normal distribution under kl risk has more recently seen a series of remarkably parallel developments . with a focus on bayes rules catalyzed by aitchison ( @xcite ) , komaki ( @xcite ) provided a fundamental breakthrough by demonstrating that the harmonic prior bayes rule dominated the best invariant uniform prior bayes rule .",
    "these results suggested the existence of a theory for minimax estimation based on the superharmonic properties of marginals , a theory that was then established in george , liang and xu ( @xcite ) .",
    "further developments of new minimax shrinkage predictive density estimators now abound , including , as described in this article , multiple shrinkage estimators , empirical bayes estimators , normal linear model regression estimators , and nonparametric regression estimators .",
    "examples of promising further new directions for predictive density estimation can be found in the work of komaki ( @xcite ) which included results for poisson distributions , for general location - scale models and for wishart distributions , in the work of ghosh , mergel and datta ( @xcite ) which developed estimation under alternative divergence losses , and in the work of kato ( @xcite ) which established improved minimax predictive domination for the multivariate normal distribution under kl risk when both the mean and the variance are unknown .",
    "minimax predictive density estimation is now beginning to flourish .",
    "this work was supported by nsf grants dms-07 - 32276 and dms-09 - 07070 .",
    "the authors are grateful for the helpful comments and clarifications of an anonymous referee ."
  ],
  "abstract_text": [
    "<S> in a remarkable series of papers beginning in 1956 , charles stein set the stage for the future development of minimax shrinkage estimators of a multivariate normal mean under quadratic loss . </S>",
    "<S> more recently , parallel developments have seen the emergence of minimax shrinkage estimators of multivariate normal predictive densities under kullback  leibler risk . </S>",
    "<S> we here describe these parallels emphasizing the focus on bayes procedures and the derivation of the superharmonic conditions for minimaxity as well as further developments of new minimax shrinkage predictive density estimators including multiple shrinkage estimators , empirical bayes estimators , normal linear model regression estimators and nonparametric regression estimators .    ,    . </S>"
  ]
}