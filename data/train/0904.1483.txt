{
  "article_text": [
    "setting appropriate claims reserves to meet future claims payment cash flows is one of the main tasks of non - life insurance actuaries .",
    "there is a wide range of models , methods and algorithms used to set appropriate claims reserves . among the most popular methods there is the chain - ladder method , the bornhuetter - ferguson method and the generalized linear model methods . for an overview , see wthrich and merz ( 2008 ) and england and verrall ( 2002 ) .",
    "setting claims reserves includes two tasks : estimate the mean of future payments and quantify the uncertainty in this prediction for future payments . typically , quantifying the uncertainty includes two terms , namely the so - called process variance and the ( parameter ) estimation error .",
    "the process variance reflects that we predict random variables , i.e. it describes the pure process uncertainty .",
    "the estimation error reflects that the true model parameters need to be estimated and hence there is an uncertainty in the reliability of these estimates . in this paper , in addition to these two terms",
    ", we consider a third source of error / uncertainty , namely , we analyze the fact that we could have chosen the wrong model .",
    "that is , we select a family of claims reserving models and quantify the uncertainty coming from a possibly wrong model choice within this family of models .",
    "such an analysis is especially important when answering solvency questions .",
    "a poor model choice may result in a severe shortfall in the balance sheet of an insurance company , which requires under a risk - adjusted solvency regime an adequate risk capital charge .",
    "we analyze typical sizes of such risk capital charges within the family of tweedie s compound poisson models * * , * * see tweedie ( 1984 ) , smyth and jrgensen ( 2002 ) and wthrich ( 2003 ) .",
    "assume that @xmath0 are incremental claims payments with indices @xmath1 where @xmath2 denotes the accident year and @xmath3 denotes the development year . at time @xmath4",
    ", we have observations @xmath5 and for claims reserving at time @xmath4 we need to predict the future payments @xmath6 see table [ tab1 ] .",
    "hence , the outstanding claims payment at time @xmath4 is given by @xmath7 its conditional expectation at time @xmath4 is given by @xmath8   = \\sum_{i=1}^{i}e\\left [ \\left .",
    "r_{i}\\right\\vert \\mathcal{d}_{i}\\right ]   = \\sum_{i+j > i}e\\left [   \\left .",
    "y_{i , j}\\right\\vert \\mathcal{d}_{i}\\right ]   .\\ ] ] hereafter , the summation @xmath9 is for @xmath10 .",
    "therefore , we need to predict @xmath11 and to estimate @xmath12   $ ] .",
    "assume that @xmath13 is an appropriate @xmath14-measurable predictor for @xmath11 and @xmath14-measurable estimator for @xmath15 $ ] .",
    "then , @xmath13 is used to predict the future payments and is the amount that is put aside in the balance sheet of the insurance company for these payments .",
    "prediction uncertainty is then often studied with the help of the ( conditional ) mean square error of prediction ( msep ) which is defined by @xmath16   .\\ ] ] if @xmath13 is @xmath14-measurable , the conditional msep can easily be decoupled as follows * * , * * see wthrich and merz ( 2008 ) , section 3.1 : @xmath17   -\\widehat{r}\\right ) ^{2}\\label{vardecomp}\\\\ &   = \\text { process variance } + \\text { estimation error.}\\nonumber\\end{aligned}\\ ] ] it is clear that the consistent estimator @xmath13 which minimizes the conditional msep is given by @xmath18   $ ] and is used , hereafter , as the `` best estimate '' for reserves .",
    "assuming the model is parameterized by the parameter vector @xmath19 , @xmath20 can be decomposed as @xmath21 + \\mathrm{var}\\left(\\left . e\\left",
    "r\\right\\vert \\bm{\\theta } , \\mathcal{d}_{i}\\right ] \\right\\vert \\mathcal{d}_{i}\\right ) \\label{decompvar}\\\\ &   = \\text{average process variance } + \\text { parameter estimation error}.\\nonumber\\end{aligned}\\ ] ] these are the two terms that are usually studied when quantifying prediction uncertainties in a bayesian context , where the unknown parameters @xmath22 are modelled stochastically .",
    "that is , we obtain in the bayesian context a similar decomposition as in the frequentist estimation ( [ vardecomp ] ) . in the frequentist approach , the second term in ( [ vardecomp ] )",
    "is often estimated by @xmath23 , see for example section 6.4.3 in wthrich and merz ( 2008 ) .",
    "as discussed in cairns ( 2000 ) , in full generality one could consider several sources of model uncertainty , however unlike cairns ( 2000 ) we focus on a specific class of models .",
    "we consider the setting discussed in bernardo and smith ( 1994 ) termed m complete modelling .",
    "in such a setting the premise is that one considers a set of models in which the `` truth '' exists but is unknown _ a priori_. in this setting we demonstrate the risk associated with the model uncertainty which we analyze jointly as a decomposition into two main parts .",
    "the first involves the uncertainty in the parameterization of the model , this is a variable selection problem within a nested model structure in the same vein as discussed in cairns ( 2000 ) .",
    "it relates to finding a trade - off between parsimony and accuracy in the estimation .",
    "the second source of model uncertainty that we study involves the choice of a parameter which determines membership from a spectrum of possible models within the tweedie s compound poisson family of models .",
    "we restrict the analysis to tweedie s compound poisson models and justify this by assuming we are working in the m complete setting . if we relaxed this assumption and therefore consider competing models not in this family , then the analysis would be difficult to interpret and analyze in the manner we develop in this paper .",
    "the second source of model uncertainty will be considered under both a model selection and a model averaging setting , given the first `` variable selection '' uncertainty is resolved . as mentioned in cairns ( 2000 )",
    "achieving such an analysis requires advanced simulation methodology .",
    "note , in future work we would also consider the m open modeling framework of bernardo and smith ( 1994 ) which relaxes the belief that the truth lies in the set of models considered and hence introduces additional uncertainty associated with the family of models considered .",
    "the advanced sampling methodology required to study the m open model setting will be briefly discussed .",
    "the paper is organised as follows . in section 2",
    ", we present tweedie s compound poisson model and section 3 considers parameter estimation in the model , using the maximum likelihood and bayesian markov chain monte carlo approaches for a real data set .",
    "having addressed the variable selection question in section 4 , we then analyze claims reserve estimation and model uncertainty in both a frequentist and bayesian setting in section 5 .",
    "we finish with conclusions from our findings .",
    "we assume that @xmath0 belongs to the family of tweedie s compound poisson models .",
    "below we provide three different parameterizations for tweedie s compound poisson models , for rigorous derivations we refer to jrgensen and de souza ( 1994 ) , smyth and jrgensen ( 2002 ) and wthrich ( 2003 ) .    [ 1st representation][model ass1 ] @xmath24 as an indicator function .       * 2nd representation . *",
    "the random variable @xmath0 given in belongs to the family of tweedie s compound poisson models , see tweedie ( 1984 ) .",
    "the distribution of @xmath0 can be reparameterized in such a way that it takes a form of the exponential dispersion family , see e.g. formula ( 3.5 ) and appendix a in wthrich ( 2003 ) : @xmath0 has a probability weight at @xmath25 given by @xmath26   = p\\left [   n_{i , j}=0\\right ]   = \\exp\\left\\ { -\\phi_{i , j}^{-1}\\kappa_{p}(\\theta_{i , j})\\right\\ }   \\label{tweedie}\\ ] ] and for @xmath27 the random variable @xmath0 has continuous density @xmath28    here @xmath29 , @xmath30 , the normalizing constant is given by @xmath31 and the cummulant generating function @xmath32 is given by@xmath33 ^{\\gamma},\\ ] ] where @xmath34 and @xmath35    the parameters , in terms of the 1st representation quantities , are : @xmath36    then the mean and variance of @xmath0 are given by @xmath37    &   = \\frac{\\partial}{\\partial\\theta_{i , j}}\\kappa _",
    "{ p}(\\theta_{i , j})=\\kappa_{p}^{\\prime}(\\theta_{i , j})=\\left [   ( 1-p)\\theta _ { i , j}\\right ]   ^{1/(1-p)}=\\mu_{i , j},\\\\ \\text{var}\\left (   y_{i , j}\\right )    &   = \\phi_{i , j}\\kappa_{p}^{\\prime\\prime } ( \\theta_{i , j})=\\phi_{i , j}~\\mu_{i , j}^{p}.\\end{aligned}\\ ] ] that is , @xmath0 has the mean @xmath38 , dispersion @xmath39 and variance function with the variance parameter @xmath40 . the extreme cases @xmath41 and @xmath42 correspond to the overdispersed poisson and the gamma models , respectively . hence , in this spirit , tweedie s compound poisson model with @xmath43 closes the gap between the poisson and the gamma models . often in practice , @xmath40 is assumed to be known and fixed by the modeller .",
    "the aim of this paper is to study _ model uncertainty _ , that is , we would like to study the sensitivity of the claims reserves within this subfamily , i.e.  tweedie s compound poisson models ( which are now parameterized through @xmath40 ) .",
    "this answers model uncertainty questions within the family of tweedie s compound poisson models . in this paper",
    "the restriction on @xmath44 is taken in the context of practical application of these models to claims reserving , wthrich ( 2003 ) comments that the majority of claims reserving problems will be captured under this assumption .",
    "however , in general , in the exponential dispersion family @xmath40 can be outside of the @xmath45 range , e.g. @xmath46 produces a gaussian density and @xmath47 leads to an inverse gaussian model",
    ".       * 3rd representation . * utilizing the above definitions , the distribution of @xmath0 can be rewritten in terms of @xmath38 , @xmath40 and @xmath39 as @xmath26   = p\\left [   n_{i , j}=0\\right ]   = \\exp\\left\\ { -\\phi_{i , j}^{-1}\\frac{\\mu_{i , j}^{2-p}}{2-p}\\right\\}\\ ] ] and for @xmath27@xmath48 \\right\\ }   .\\ ] ]",
    "our goal is to estimate the parameters @xmath38 , @xmath40 and @xmath39 based on the observations @xmath14 . in order to estimate these parameters we need to introduce additional structure in the form of a multiplicative model .",
    "@xmath49,@xmath50    in addition , we impose the normalizing condition @xmath51 , so that the estimation problem is well - defined .",
    "that is we have @xmath52 unknown parameters @xmath53 that have to be estimated from the data @xmath14 .",
    "next we present the likelihood function for this model and then develop the methodology for parameter estimation using the maximum likelihood and bayesian inference methods .",
    "define the parameter vector @xmath54 .",
    "then the likelihood function for @xmath0 , @xmath55 , is given by @xmath56   \\right\\ } , \\label{likelihoodfunction}\\end{aligned}\\ ] ] where we set @xmath57 for @xmath58 .",
    "the difficulty in the evaluation of the likelihood function is the calculation of @xmath59 which contains an infinite sum @xmath60 where @xmath61 .",
    "tweedie ( 1984 ) identified this summation as wright s ( 1935 ) generalized bessel function , which can not be expressed in terms of more common bessel functions . to evaluate this summation",
    "we follow the approach of dunn and smyth ( 2005 ) which directly sums the infinite series , including only terms which significantly contribute to the summation .",
    "consider the term @xmath62 where @xmath63 replacing the gamma functions using stirling s approximation and approximating @xmath64 by @xmath65 we get @xmath66 which is also a reasonable approximation for small @xmath67 .",
    "treating @xmath67 as continuous and taking the partial derivative w.r.t .",
    "@xmath67 gives@xmath68 hence , the sequence @xmath69 is unimodal in @xmath70 solving @xmath71 , to find ( approximately ) the maximum of @xmath69 , results in the approximate maximum lying close to @xmath72 this gives a surprisingly accurate approximation to the true maximum of @xmath69 , @xmath73 . finally , the aim is to find @xmath74 such that the following approximation is sufficiently accurate for the use in the evaluation of the likelihood terms,@xmath75 the fact that @xmath76 is monotonic and decreasing implies that @xmath77 is strictly convex in @xmath67 and hence the terms in @xmath69 decay at a faster rate than geometric on either side of @xmath78 .",
    "dunn and smyth ( 2005 ) derive the following bounds , @xmath79 with @xmath80 these bounds are typically too conservative since the decay is much faster than geometric . in practice ,",
    "an adaptive approach balancing accuracy and efficiency is to continue adding terms either side of the maximum until the lower and upper terms satisfy the double precision constraints @xmath81 ( or @xmath82 ) and @xmath83 .",
    "when evaluating the summation for @xmath84 , it was important to utilize the following identity to perform the summation in the log scale to avoid numerical overflow problems , @xmath85    we made an additional observation when analyzing this model . for our data",
    "set , as @xmath40 approaches @xmath86 ( i.e. when  the distribution approaches the overdispersed poisson model ) the likelihood may become multimodal . therefore , to avoid numerical complications in actual calculations , we restrict to @xmath87 . at the other extreme , when @xmath88 the number of terms required to evaluate @xmath59 may become very large , hence to manage the computation burden , we restrict @xmath89 . these limitations are also discussed in dunn and smyth ( 2005 ) . for our data set , we checked that this restriction did not have a material impact on the results .",
    "the maximum likelihood estimator ( mle ) for the parameters is given by maximizing @xmath90 in @xmath91 under the constraints @xmath49 , @xmath92 , @xmath93 and @xmath94 .",
    "this leads to the mles @xmath95 and to the best estimate reserves for @xmath11 , given @xmath14 , @xmath96    a convenient practical approach to obtain the mles is to use the fact that at the maximum of the likelihood , @xmath97 are expressed through @xmath98 and @xmath40 according to the following set of equations , @xmath34:@xmath99 obtained by setting partial derivatives @xmath100 equal to zero . hence , after maximizing the likelihood in @xmath101 one then calculates the set of equations ( [ betaml2 ] ) for the remaining parameters utilizing the normalization condition @xmath51 .    under an asymptotic gaussian approximation ,",
    "the distribution of the mles is gaussian with the covariance matrix elements @xmath102 where @xmath103 is fisher s information matrix that can be estimated by the observed information matrix @xmath104    it is interesting to note that , @xmath105 . also , it is easy to show ( using ( [ loglhwrtbeta ] ) and ( [ mlecorr ] ) ) that @xmath106 is orthogonal to all other parameters , i.e. @xmath107    the next step is to estimate the parameter estimation error in the reserve as a function of the parameter uncertainty .",
    "we do this via propagation of error by forming a taylor expansion around the mles , see england and verrall ( 2002 ) formulae ( 7.6)-(7.8 ) and wthrich ( 2003 ) formulae ( 5.1)-(5.2 ) , @xmath108 additionally , using the independence assumption on @xmath0 and ( 2.11 ) , the process variance is estimated as @xmath109    then the conditional msep ( [ vardecomp ] ) is estimated by @xmath110    note that , in practice , typically mle is done for a fixed @xmath40 ( expert choice ) and hence model selection questions are neglected . in our context",
    "it means that the expert chooses @xmath40 and then estimates @xmath111 , @xmath112 and @xmath113 ( see also wthrich ( 2003 ) , section 4.1 ) .",
    "the case @xmath114 corresponds to the overdispersed poisson model and provides the chain - ladder estimate for the claims reserves ( see wthrich and merz ( 2008 ) , section 2.4 ) .",
    "it is important to note that , often the dispersion parameter @xmath115 is estimated using pearson s residuals as @xmath116 where @xmath117 is the number of observations @xmath0 in @xmath118 and @xmath119 is the number of estimated parameters @xmath120 , @xmath121 ( see e.g. wthrich and merz ( 2008 ) , formula ( 6.58 ) ) .",
    "also note that for a given @xmath40 , @xmath122 given by ( [ mle_er ] ) does not depend on @xmath115 and the estimators for the process variance ( [ mle_pv ] ) and estimation error ( [ mle_ee ] ) are proportional to @xmath115 .",
    "next we present the bayesian model which provides the posterior distribution of the parameters given the data .",
    "this will be used to analyze the model uncertainty within tweedie s compound poisson models .      in a bayesian context",
    "all parameters , @xmath40 , @xmath115 , @xmath49 and @xmath92 , are treated as random .",
    "using bayesian inference we adjust our _ a priori _ beliefs about the parameters of the model utilizing the information from the observations . through the bayesian paradigm we are able to learn more about the distribution of @xmath40 , @xmath115 , @xmath98 and @xmath97 after having observed @xmath14 .",
    "our _ a priori _ beliefs about the parameters of the model are encoded in the form of a prior distribution on the parameters @xmath123 then the joint density of @xmath124 and @xmath125 is given by @xmath126    now applying bayes law , the posterior distribution of the model parameters , given the data @xmath14 , is@xmath127 usually , there are two problems that arise in this context , the normalizing constant of this posterior is not known in closed form . additionally , generating samples from this posterior is typically not possible using simple inversion or rejection sampling approaches . in such cases",
    "it is usual to adopt techniques such as markov chain monte carlo ( mcmc ) methods , see for example gilks _",
    "( 1996 ) and robert and casella ( 2004 ) for detailed expositions of such approaches .",
    "the bayesian estimators typically considered are the maximum a postiori ( map ) estimator and the minimum mean square estimator ( mmse ) , that is the mode and mean of the posterior , defined as follows : @xmath128   , \\\\",
    "mmse   &   : \\text { \\ \\ \\ } \\hat{\\bm{\\theta}}^{mmse}=e\\left [ \\bm{\\theta}~|~\\mathcal{d}_{i}\\right ]   .\\end{aligned}\\ ] ] we mention here that if the prior @xmath129 is constant and the parameter range includes the mle , then the map of the posterior is the same as the mle . additionally , one can approximate the posterior using a second order taylor series expansion around the map estimate as @xmath130 this corresponds to @xmath131 approximated by the gaussian distribution with the mean @xmath132 and covariance matrix calculated as the inverse of the matrix @xmath133 which in the case of diffuse priors ( or constant priors defined on a large range ) compares with the gaussian approximation for the mles ( [ mlecorr])-([observedinformationmatrix ] ) .    in the bayesian context , the conditionally expected future payment , for model assumptions 3.1 ,",
    "is given by @xmath134 = \\sum_{i+j > i}e\\left [ \\left .",
    "\\alpha_{i}\\beta_{j}\\right\\vert \\mathcal{d}_{i}\\right ]   .",
    "\\label{gl3}\\ ] ] denote the expected reserves , given the parameters @xmath135 by @xmath136   = \\sum_{i+j > i}\\alpha_{i}\\beta_{j}. \\label{rtilda}\\ ] ] then , the best consistent estimate of reserves ( er ) is given by @xmath137   = \\sum_{i+j > i}e\\left [   \\left .",
    "\\alpha_{i}\\beta _ { j}\\right\\vert \\mathcal{d}_{i}\\right]=e\\left .",
    "\\left [   r\\right\\vert \\mathcal{d}_{i}\\right ] , \\ ] ] which is , of course , a @xmath14-measurable predictor . hence",
    ", the conditional msep is simply @xmath138   ~=~\\mathrm{var}\\left (   \\left .",
    "r\\right\\vert \\mathcal{d}_{i}\\right )   .",
    "\\label{msep}\\ ] ] this term , in the bayesian approach for tweedie s compound poisson model , is decomposed as , see also ( [ decompvar ] ) , @xmath139   + \\mathrm{var}\\left ( \\left .",
    "\\widetilde{r}~\\right\\vert \\mathcal{d}_{i}\\right ) .\\end{aligned}\\ ] ] hence , we obtain the familiar decoupling into average process variance and estimation error .",
    "however , in addition we incorporate model uncertainty within tweedie s compound poisson model , which enters the calculation by the averaging over all possible values of the variance parameter @xmath40 .      in this section",
    "we describe an mcmc method to be used to sample from the posterior distribution ( [ posterior ] ) .",
    "the following notations are used : @xmath140 is the vector of parameters ; @xmath141 is the uniform distribution on the interval @xmath142 ; @xmath143 and @xmath144 are the gaussian density and distribution correspondingly with the mean @xmath145 and standard deviation @xmath146 at position @xmath147 .",
    "* prior structure : * we assume that all parameters are independent under the prior distribution @xmath129 and all distributed uniformly with @xmath148 .",
    "the prior domains we used for our analysis were @xmath149 , @xmath150 , @xmath151 and @xmath152 .",
    "these are reasonable ranges for the priors in view of our data in table [ tab2 ] and corresponding to the mles in table [ tab3 ] .",
    "other priors such as diffuse priors can be applied with no additional difficulty .",
    "the choice of very wide prior supports was made with the aim of performing inference in the setting where the posterior is largely implied by the data .",
    "subsequently , we checked that making the ranges wider does not affect the results .",
    "next we outline a random walk metropolis - hastings ( rw - mh ) within gibbs algorithm .",
    "this creates a reversible markov chain with the stationary distribution corresponding to our target posterior distribution ( [ posterior ] ) .",
    "that is , we will run the chain until it has sufficiently converged to the stationary distribution ( = posterior distribution ) and in doing so we obtain samples from that posterior distribution .",
    "it should be noted that the gibbs sampler creates a markov chain in which each iteration of the chain involves scanning either deterministically or randomly over the variables that comprise the target stationary distribution of the chain .",
    "this process involves sampling each proposed parameter update from the corresponding full conditional posterior distribution .",
    "the algorithm we present generates a markov chain that will explore the parameter space of the model in accordance with the posterior mass in that region of the parameter space .",
    "the state of the chain at iteration @xmath153 will be denoted by @xmath154 and the chain will be run for a length of @xmath155 iterations .",
    "the manner in which mcmc samplers proceed is by proposing to move the @xmath2th parameter from state @xmath156 to a new proposed state @xmath157 the latter will be sampled from an mcmc proposal transition kernel ( [ transkernel ] ) .",
    "then the proposed move is accepted according to a rejection rule which is derived from a reversibility condition .",
    "this makes the acceptance probability a function of the transition kernel and the posterior distribution as shown in ( [ acceptprob ] ) .",
    "if under the rejection rule one accepts the move then the new state of the @xmath2th parameter at iteration @xmath153 is given by @xmath158 , otherwise the parameter remains in the current state @xmath159 and an attempt to move that parameter is repeated at the next iteration . in following this procedure ,",
    "one builds a set of correlated samples from the target posterior distribution which have several asymptotic properties .",
    "one of the most useful of these properties is the convergence of ergodic averages constructed using the markov chain samples to the averages obtained under the posterior distribution .",
    "next we present the algorithm and then some references that will guide further investigation into this class of simulation methodology . properties of this algorithm , including convergence results can be found in the following references casella and george ( 1992 ) , robert and casella ( 2004 ) , gelman _",
    "( 1995 ) , gilks _",
    "( 1996 ) and smith and roberts ( 1993 ) .    *",
    "random walk metropolis hastings ( rw - mh ) within gibbs algorithm .",
    "initialize randomly or deterministically for @xmath160 the parameter vector @xmath161 ( e.g.  mles ) .",
    "\\2 . for @xmath162",
    "\\a ) set @xmath163    \\b ) for @xmath164    sample proposal @xmath165 from gaussian distribution whose density is truncated below @xmath166 and above @xmath167 and given by    @xmath168    to obtain @xmath169 .",
    "accept proposal with acceptance probability @xmath170    where @xmath171 is given by ( [ posterior ] ) .",
    "that is , simulate @xmath172 and set @xmath173 if @xmath174    * note that in ( [ acceptprob ] ) the normalizing constant of the posterior @xmath175 from ( [ posterior ] ) is not needed .",
    "* the rw - mh algorithm is simple in nature and easily implemented .",
    "however , if one does not choose the proposal distribution carefully , then the algorithm only gives a very slow convergence to the stationary distribution .",
    "there have been several studies regarding the optimal scaling of proposal distributions to ensure optimal convergence rates .",
    "_  ( 1997 ) , bedard and rosenthal ( 2007 )  and roberts and rosenthal ( 2001 ) were the first authors to publish theoretical results for the optimal scaling problem in rw - mh algorithms with gaussian proposals .",
    "for @xmath176-dimensional target distributions with i.i.d .",
    "components , the asymptotic acceptance rate optimizing the efficiency of the process is 0.234 independent of the target density . in this case",
    "we recommend that the selection of @xmath177 are chosen to ensure that the acceptance probability is roughly close to 0.234 .",
    "this number is the acceptance probability obtained for asymptotically optimal acceptance rates for rw - mh algorithms when applied to multidimensional target distributions with scaling terms possibly depending on the dimension . to obtain this acceptance rate , one is required to perform some tuning of the proposal variance prior to final simulations",
    "an alternative approach is to utilize a new class of adaptive mcmc algorithms recently proposed in the literature , see atchade and rosenthal ( 2005 ) and rosenthal ( 2007 ) , but these are beyond the scope of this paper .",
    "this section presents the results comparing both mle and bayesian estimates for the parameters of tweedie s compound poisson model .",
    "it is also demonstrated how additional information in a bayesian framework can be obtained through the complete knowledge of the target posterior distribution obtained from the mcmc algorithm described above . in this",
    "regard we demonstrate how this additional information can be exploited in the claims reserving setting to provide alternative statistical analysis not obtainable if one just considers point estimators .",
    "we also analyze model averaging solutions in section 5 .",
    "these can be obtained by forming estimates using the information given by the full posterior distribution @xmath178 that we find empirically from the mcmc samples .",
    "the maximum likelihood and mcmc algorithms were implemented in fortran .",
    "the maximization routine for the mles utilizes the direct search algorithm dbcpol ( that requires function evaluation only ) from the imsl numerical library .",
    "note that , gradient based optimization routines such as the bfgs algorithm can be more efficient , but the direct search algorithm we used was sufficient for our problem in terms of computing time ( @xmath179 seconds on a typical desktop pc ) .",
    "the algorithm was analyzed on synthetic data and found to provide correct estimates .",
    "in particular with uniform priors the map estimates of the parameters are the same as the mles , up to numerical errors .",
    "this was confirmed for different sized claims triangles .",
    "the actual data set studied in this paper is presented in table [ tab2 ] .",
    "the data we study is the standard data set used in wthrich and merz ( 2008 ) scaled by 10,000 .",
    "the results presented for the bayesian approach were obtained after pretuning the markov chain random walk standard deviations , @xmath180 to produce average acceptance probabilities of @xmath181 then the final simulation was for @xmath182 iterations from a markov chain ( @xmath183min ) in which the first @xmath184 iterations were discarded as burnin when forming the estimates .",
    "the pretuned proposal standard deviations @xmath185 are presented in table [ tab3 ] .",
    "the first set of results in table [ tab3 ] demonstrates the mle versus the bayesian posterior estimator mmse for all model parameters .",
    "included are the [ 5% , 95% ] predictive intervals for the bayesian posterior distribution .",
    "the mle standard deviations are calculated using ( [ mlecorr ] ) .",
    "the numerical standard errors ( due to a finite number of mcmc iterations ) in the bayesian estimates are obtained by blocking the mcmc samples post burnin into blocks of length 5000 and using the estimates on each block to form the standard error ( these are given in brackets next to the estimates ) .",
    "the next set of analysis demonstrates the performance of the mcmc approach in converging to the stationary distribution given by the target posterior @xmath175 . to analyze this , in figure [ fig1 ] ,",
    "we present the trace plots for the markov chain for the parameters , @xmath186 .",
    "also , in figure [ fig2 ] , we demonstrate the marginal posterior distribution histograms and pair - wise posterior scatter plots for @xmath187 .",
    "the lower panels in figure [ fig2 ] are the scatter plots for the pair - wise marginal posteriors , the diagonal contains the marginal posteriors and the upper panels contains the correlations between parameters .",
    "these plots demonstrate strong linear correlations between several parameters .",
    "some of these correlations are similar to mle correlations calculated using ( [ mlecorr ] ) .",
    "for example , we found that under the posterior distribution @xmath188 and @xmath189 , see figure [ fig2 ] , are similar to @xmath190 and @xmath191 correspondingly .",
    "however , we also observed that under the posterior distribution @xmath192 and @xmath193 , see figure [ fig2 ] , while corresponding mle correlations are zero , see ( [ zerobetaicorr ] ) .",
    "in the development so far it has been assumed that variable selection is not being performed , that is we are assuming that the model is known and we require parameter estimates for this model .",
    "this is equivalent to specifying that the number of @xmath194 and @xmath195 parameters is fixed and known in advance .",
    "we now relax this assumption and will demonstrate how the variable selection problem can be incorporated into our framework .",
    "the procedure we utilize for the variable selection is based on recent work of congdon ( 2006 ) and specifies the joint support of the posterior distribution for the models and parameters under the product space formulation of carlin and chib ( 1995 ) .    in this section",
    "we consider the subset of nested models which create homogenous blocks in the claims reserving triangle @xmath196 for the data set in table 2 .    *",
    "@xmath197}=\\left(p,\\phi,\\widetilde { { \\alpha}}_{0}={\\alpha}_{0},\\ldots,\\widetilde{{\\alpha}}_{i}={\\alpha } _ { i},\\widetilde{{\\beta}}_{0}={\\beta}_{0},\\ldots,\\widetilde{{\\beta}}_{i}={\\beta } _ { i}\\right )   $ ] -",
    "* saturated model*. * @xmath198}=\\left(p,\\phi,\\widetilde{{\\beta}}_{0}\\right ) $ ] with @xmath199 * @xmath200}=\\left(p,\\phi,\\widetilde{{\\alpha}}_{1},\\widetilde { { \\beta}}_{0},\\widetilde{{\\beta}}_{1}\\right )   $ ] with @xmath201 , @xmath202,@xmath203 , @xmath204 . * @xmath205}=\\left(p,\\phi,\\widetilde{{\\alpha}}_{1},\\widetilde { { \\alpha}}_{2},\\widetilde{{\\beta}}_{0},\\widetilde{{\\beta}}_{1},\\widetilde { { \\beta}}_{2}\\right )   $ ] with @xmath206 , @xmath207,@xmath208 , @xmath209 , @xmath210 , @xmath211 * @xmath212}=\\left(p,\\phi,\\widetilde{{\\alpha}}_{1},\\widetilde { { \\alpha}}_{2},\\widetilde{{\\alpha}}_{3},\\widetilde{{\\beta}}_{0},\\widetilde { { \\beta}}_{1},\\widetilde{{\\beta}}_{2},\\widetilde{{\\beta}}_{3}\\right ) $ ] with @xmath213 , @xmath214 , + @xmath215 , @xmath216 , @xmath217 , @xmath218 , @xmath219 , @xmath220 * @xmath221}=\\left(p,\\phi,\\widetilde{{\\alpha}}_{1},\\widetilde { { \\alpha}}_{2},\\widetilde{{\\alpha}}_{3},\\widetilde{{\\alpha}}_{4 } , \\widetilde{{\\beta}}_{0},\\widetilde{{\\beta}}_{1},\\widetilde{{\\beta } } _ { 2},\\widetilde{{\\beta}}_{3},\\widetilde{{\\beta}}_{4}\\right ) $ ] with @xmath213 , @xmath222 , @xmath223 , @xmath224 , @xmath225 , @xmath226 , @xmath227 , @xmath228 , @xmath229 , @xmath230 * @xmath231}=\\left(p,\\phi,{\\alpha}_{0},\\widetilde{{\\alpha } } _ { 1},{\\beta}_{0},{\\beta}_{1},\\ldots,{\\beta}_{i}\\right )   $ ] with @xmath232    now , to determine the optimal model , we first consider the joint posterior distribution for the model probability and the model parameters denoted @xmath233}~|~\\mathcal{d}_{i}),$ ] where @xmath234 } = \\left ( \\widetilde{{\\theta}}_{1,[k]},\\widetilde{{\\theta}}_{2,[k]},\\ldots,\\widetilde { { \\theta}}_{n_{\\left[k\\right]},[k]}\\right )   $ ] is the parameter vector for model @xmath235.$ ] additionally we denote the prior bounds for @xmath236}$ ] as @xmath237}},b_{\\widetilde{{\\theta } } _ { i,[k]}}\\right ]   .$ ] we assume a prior distribution @xmath238 for the model selection and a prior for the parameters conditional on the model @xmath239}~|~m_{k}\\right ) $ ] .",
    "it is no longer possible to run the standard mcmc procedure we described in section 3.4 for this variable selection setting .",
    "this is because the posterior is now defined on either a support consisting of disjoint unions of subspaces or a product space of all such subspaces , one for each model considered .",
    "a popular approach to run markov chains in such a situation is to develop a more advanced sampler than that presented above , typically in the disjoint union setting .",
    "this involves developing a reversible jump rj - mcmc framework , see green ( 1995 ) and the references therein .",
    "this type of markov chain sampler is complicated to develop and analyze .",
    "hence , we propose as an alternative in this paper to utilize a recent procedure that will allow us to use the above mcmc sampler we have already developed for a model @xmath240 the process we must follow involves first running the sampler in the simulation technique described in section 3.4 for each model considered .",
    "then the calculation of the posterior model probabilities @xmath241 is performed using the samples from the markov chain in each model to estimate ( [ modprobspost ] ) .",
    "furthermore , our approach here removes the assumption on the priors across models , made by congdon ( 2006 ) , p.348 , @xmath242   } ~|~m_{k}\\right )   = 1,m\\neq k\\ ] ] and instead we work with the prior @xmath243 } ~|~m_{k})={\\textstyle\\prod\\limits_{i=1 } ^{n_{\\left [   m\\right ] } } } \\left [   b_{\\widetilde{{\\theta}}_{i,[m ] } } -a_{\\widetilde{{\\theta}}_{i,[m]}}\\right ]   ^{-1},m\\neq k.\\ ] ]    that is , instead we use a class of priors where specification of priors for a model @xmath244 automatically specifies priors for any other model .",
    "this is a sensible set of priors to consider given our product space formulation and it has a clear interpretation in our setting where we specify our models through a series of constraints , relative to each other . in doing this we also achieve our goal of having posterior model selection insensitive to the choice of the prior and being data driven . the modified version of congdon s ( 2006 ) , formula a.3",
    ", we obtain after relaxing congdon s assumption , allows the calculation of the posterior model probabilities @xmath241 using the samples from the markov chain in each model to estimate@xmath245 } ~|~\\mathcal{d}_{i})d\\bm{\\theta}_{[k ] } = \\int\\pi(m_{k}~|~\\bm{\\theta}_{[k]},\\mathcal{d}_{i})\\pi(\\bm{\\theta}_{[k ] } ~|~\\mathcal{d}_{i})d\\bm{\\theta}_{[k]}\\nonumber\\\\ & \\approx\\frac{1}{t - t_{b}}\\sum\\limits_{j = t_{b}+1}^{t}\\pi(m_{k}~|~\\mathcal{d } _ { i},\\bm{\\theta}_{j,[k]})\\nonumber\\\\ & = \\frac{1}{t - t_{b}}\\sum\\limits_{j = t_{b}+1}^{t}\\frac{l_{\\mathcal{d}_{i } } ( m_{k},\\bm{\\theta}_{j,[k ] } ) { \\textstyle\\prod\\limits_{k=0}^{k } } \\pi(\\bm{\\theta}_{j,[k]}~|~m_{k})\\pi(m_{k})}{\\sum\\nolimits_{m=0}^{k } l_{\\mathcal{d}_{i}}(m_{m},\\bm{\\theta } _ { j,[m ] } ) { \\textstyle\\prod\\limits_{k=0}^{k } } \\pi(\\bm{\\theta}_{j,[k]}~|~m_{m})\\pi(m_{m})}\\nonumber\\\\ & = \\frac{1}{t - t_{b}}\\sum\\limits_{j = t_{b}+1}^{t}\\frac{l_{\\mathcal{d}_{i } } ( m_{k},\\bm{\\theta}_{j,[k]})}{\\sum\\nolimits_{m=0}^{k}l_{\\mathcal{d}_{i } } ( m_{m},\\bm{\\theta } _ { j,[m]})}. \\label{modprobspost}\\end{aligned}\\ ] ] here @xmath246 and for a proof , see congdon ( 2006 ) , formula a.3 . note that , the prior of parameters ( given model ) contributes in the above implicitly as @xmath247}$ ] are mcmc samples from the @xmath248 models posterior distribution . in the actual implementation we used @xmath249 and the burnin period @xmath250 note , the prior probabilities for each model",
    "are considered diffuse and are set such that all models _ a priori _ are equiprobable , hence @xmath251 and @xmath252}~|~m_{k})$ ] is the prior for model @xmath244 s parameters evaluated at the @xmath253 markov chain iteration .",
    "once we have the posterior model probabilities we can then take the map estimate for the optimal model ( variable selection ) for the given data set . in this paper we do not consider the notion of model averaging over different parameterized models in the variable selection context .",
    "instead we simply utilize these results for optimal variable selection from a map perspective for the marginal posterior @xmath241 .",
    "in addition to this model selection criterion we also consider in the bayesian framework the deviance information criterion ( dic ) , see bernardo and smith ( 1994 ) . from a classical maximum likelihood perspective we present the likelihood ratio ( lhr ) p - values .",
    "application of this technique to the simulated mcmc samples for each of the considered models produced the posterior model probabilities given in table [ tab4 ] .",
    "this suggests that within this subset of models considered , the saturated model @xmath254 was the optimal model to utilize in the analysis of the claims reserving problem , @xmath255 .",
    "it is followed by model @xmath256 with @xmath257 .",
    "additionally , the choice of @xmath254 was also supported by the other criteria we considered : dic and lhr .    in future research it would be interesting to extend to the full model space which considers all models in the power set @xmath258 } \\right\\vert $ ] .",
    "this is a large set of models including all combinatorial combinations of model parameters for @xmath259 and @xmath260 . in such cases",
    "it is no longer feasible to run standard mcmc algorithms in each model since this will involve an impractical number of simulations .",
    "hence , more sophisticated model exploration techniques will be required such as rj - mcmc , see green ( 1995 ) or the product space samplers of carlin and chib ( 1995 ) .",
    "we note here that we do not claim @xmath254 is the optimal model in all possible models , only in the subset we consider in this section . in saying this we acknowledge that we aim to work in the saturated model but consider it important to illustrate how variable selection",
    "can be performed in this class of models and also raise awareness that this will impact the model uncertainty analysis subsequently performed .",
    "hence , using these findings and the analysis of the mcmc results for model @xmath254 provided above , we may now proceed to analyze the claims reserving problem .",
    "of  interest to the aim of this paper is the sensitivity of the model choice parameter @xmath40 to the parameterization of the claims reserving triangle .",
    "this is particularly evident when one considers the mmse estimate of the model specification parameter @xmath40 estimated under each model . in the most parsimonious ,",
    "yet inflexible model @xmath261 the estimate obtained was @xmath262 , a very similar estimate was obtained in models @xmath263 and @xmath264 however , interestingly in the saturated model the estimate was @xmath265 which is almost at the other extreme of the considered range for which the parameter @xmath40 is defined .",
    "we now demonstrate the results for several quantities in the claims reserving setting , utilizing the mcmc simulation results we obtained for the bayesian posterior distribution under the variable selection model @xmath266 ( saturated model ) .",
    "in particular , we start by noting that we use uniform prior distributions with a very wide ranges to perform inference implied by the data only . in this case , theoretically , the bayesian map ( the posterior mode ) and mles for the parameters should be identical up to numerical error due to the finite number of mcmc iterations . a large number of mcmc iterations was performed so that the numerical error is not material . in general",
    ", the use of more informative priors will lead to the differences between the map and mle .",
    "some of the mmse estimates ( the posterior mean ) were close to the map estimates , indicating that the marginal posterior distributions are close to symmetric .",
    "when the posterior is not symmetric , mmse and map can be very different .",
    "also , note that the uncertainties in the parameter mles are estimated using the asymptotic gaussian approximation ( [ mlecorr])-([observedinformationmatrix ] ) . in the case of constant priors",
    ", this should lead to the same inferences as corresponding bayesian estimators if the posterior distributions are close to the gaussian approximation , see ( [ posteriorgaussianapprox1])-([posteriorgaussianapprox2 ] ) .",
    "in addition , the mles for the reserves , estimation error and process variance , see section 3.2 , are based on a taylor expansion around parameter mles assuming small errors . in many cases",
    "the posterior is materially different from the gaussian distribution , has significant skewness and large standard deviation leading to the differences between the mles and corresponding bayesian estimators .",
    "having mentioned this , we now focus on the main point of this paper which involves analysis of the quantities in table [ tab5 ] related to the model uncertainty within tweedie s compound poisson models ( introduced by fixing model parameter @xmath40 ) in a bayesian setting .",
    "it is worth noting that point estimates of model parameters are either in the frequentists approach mles or in a bayesian approach the map or mmse estimates .",
    "these are under the auspice that we wish to perform model selection ( i.e.  selection of @xmath40 ) .",
    "the focus of this paper is to demonstrate the difference in results obtained for reserve estimates that can arise by performing model averaging instead of the typical approach of model selection , using _ a priori _ chosen @xmath40 . in this regard",
    "we perform estimation utilizing the full posterior distribution of the parameters and not just point estimators .",
    "this allows us to capture the influence of the model uncertainty ( uncertainty in @xmath40 ) , since in a bayesian setting we can account for this uncertainty using the posterior distribution . in particular , the bayesian analysis specifies the optimal @xmath40 ( either in the map or the mmse context ) and it also provides a confidence interval for the choice of @xmath40 ( see figure [ fig7 ] ) , which corresponds to the choice of the optimal model within tweedie s compound poisson models .",
    "moreover , we demonstrate the impact on the claims reserve by varying @xmath40 from 1.1 to 1.9 ( i.e.  for a fixed model choice ) .",
    "initially it is worth considering the predicted reserve distribution for the estimator @xmath267 .",
    "this is obtained by taking the samples @xmath268 to @xmath269 from the mcmc simulation @xmath270 and calculating @xmath271 via ( [ rtilda ] ) .",
    "the histogram estimate is presented in figure [ fig3 ] . in the same manner",
    ", we also estimate the distributions of @xmath272 for the individual cells of the @xmath273 claims matrix , presented as subplots in figure [ fig4 ] . note that the total observed loss in the upper triangle ( @xmath274 ) is consistent with @xmath275 $ ] and @xmath276^{1/2}$ ] estimated using the mcmc samples as ( @xmath277 ) and ( @xmath278 ) respectively .",
    "the maximum likelihood approach results in @xmath279 with standard deviation @xmath280 also conforming with the observed total loss .",
    "now we focus on quantities associated with the estimated distribution for @xmath281 to calculate the results , see table [ tab5 ] , which can only be estimated once the entire posterior distribution is considered .",
    "these quantities are the key focus of this paper since they allow assessment of the conditional msep as specified in ( [ msep ] ) .",
    "in particular , we may now easily use the posterior probability samples obtained from the mcmc algorithm to evaluate the estimated reserve ( er ) , the process variance ( pv ) and the estimation error ( ee ) in the conditional msep .",
    "this provides an understanding and analysis of the behaviour of the proposed model in both the model averaging and model selection ( i.e.  selection of @xmath40 ) contexts whilst considering the issue of model uncertainty , the goal of this paper .",
    "the bayesian estimates for er , pv , ee and msep are presented in table [ tab6 ] .",
    "the corresponding mles were calculated using ( [ mle_er ] ) , ( [ mle_pv ] ) , ( [ mle_ee ] ) and ( [ decompvarmle ] ) respectively and presented in table [ tab6 ] for comparison .",
    "the results demonstrate the following :    * claims reserves mle , @xmath282 is less than bayesian estimate @xmath283 by approximately 3% , which is the estimation bias of the claims reserve mle ( see also wthrich and merz ( 2008 ) , remarks 6.15 .",
    "* @xmath284 and @xmath285 are of the same magnitude , approximately 6 - 7% of the total claims reserves .",
    "* mles for @xmath284 and @xmath285 are less than corresponding bayesian estimates by approximately 37% and 30% , respectively .",
    "* the difference between @xmath122 and @xmath286 is of the same order of magnitude as @xmath284 and @xmath285 and thus is significant .",
    "note that we use constant priors with very wide ranges , the mle uncertainties are calculated using an asymptotic gaussian approximation and numerical error due to the finite number of mcmc iterations is not material ( also see the 1st paragraph , section 5 ) .",
    "the observed significant differences between the mles and corresponding bayesian estimators suggest that our posterior distributions are skewed and materially different from the gaussian distribution .",
    "we conclude this section with the distribution of @xmath11 , the total outstanding claims payment , see figure [ fig5 ] .",
    "this is obtained from the mcmc samples of the parameters @xmath287 which we then transform to parameters @xmath288 from model representation 1 , section 2 , and simulate annual losses in @xmath9 .",
    "that is , these samples of @xmath11 are obtained from the full predictive distribution @xmath289 where @xmath290 is the distribution of @xmath11 given by ( [ r ] ) and ( [ tweedie model ] ) .",
    "it takes into account both process uncertainty and parameter uncertainty .",
    "we note that while reserving by some measure of centrality such as @xmath283 may be robust , it will not take into account the distributional shape of @xmath11 . a viable alternative may be value - at - risk ( var ) or a coherent risk measure such as expected shortfall . in table",
    "[ tab7 ] we demonstrate estimates of the var for @xmath267 and @xmath11 at the @xmath291 and @xmath292 quantiles .",
    "as part of the model uncertainty analysis , it is useful to present plots of the relevant quantities in the model selection ( selection of @xmath40 ) settings , see figure [ fig6 ] , where we present @xmath293 $ ] , @xmath294 $ ] and @xmath295 as a function of @xmath40 .",
    "figure [ fig6 ] shows :    * mle of @xmath296 is almost constant , varying approximately from a maximum of @xmath297 @xmath298 to a minimum of @xmath299 @xmath300 while the mle for @xmath301 was 602.63 . * the bayesian estimates for @xmath296 change as a function of @xmath302 approximately , it ranged from a maximum of @xmath303 @xmath300 to a minimum of @xmath304 @xmath305 while the bayesian estimator for @xmath301 was @xmath306 .",
    "hence , the difference ( estimation bias ) within this possible model range is @xmath307 which is of a similar order as the process uncertainty and the estimation error .",
    "* bayesian estimators for @xmath308 and @xmath309 increase as @xmath40 increases approximately from @xmath310 to @xmath311 and from @xmath312 to @xmath313 respectively , while the bayesian estimators for @xmath285 and @xmath284 are @xmath314 and @xmath315 correspondingly .",
    "hence , the resulting risk measure strongly varies in @xmath40 which has a large influence on quantitative solvency requirements .",
    "the mles for @xmath316 and @xmath317 are significantly less than the corresponding bayesian estimators . also , the difference between the mle and the bayesian estimators increases as @xmath40 increases .    for interpretation purposes of the above results it is helpful to use the following relations between model averaging and model selection quantities ( easily derived from their definitions in table [ tab5 ] )",
    ": @xmath318 , \\\\ pv & = e[pv_p|\\mathcal{d}_{i } ] , \\\\ ee & = e[ee_p|\\mathcal{d}_{i}]+\\mathrm{var}(er_p|\\mathcal{d}_{i}).\\end{aligned}\\ ] ] here , the expectations are calculated with respect to the posterior distribution of @xmath40 .",
    "the histogram estimate of the later is presented in figure [ fig7 ] and highlights significant uncertainty in @xmath40 ( model uncertainty within tweedie s compound poisson model ) .",
    "we also provide figure [ fig8 ] demonstrating a box and whisker summary of the distributions of @xmath319 for a range of values of @xmath320this plot provides the first , second and third quartiles as the box .",
    "the notch represents uncertainty in the median estimate for model comparison , across values of @xmath321 and the whiskers demonstrate the smallest and largest data points not considered as outliers .",
    "the outliers are included as crosses and the decision rule to determine if a point is an outlier was taken as the default procedure from the statistical software package r.    the conclusion from this section is that if model selection is performed ( i.e.  @xmath40 is fixed by the modeller ) , the conditional msep will increase significantly if a poor choice of the model parameter @xmath40 is made . in particular , though the median is fairly constant for the entire range of @xmath44 the shape of the distribution of @xmath319 is clearly becoming more diffuse as @xmath42 .",
    "this will lead to significantly larger variance in the reserve estimate .",
    "if risk measures such as value - at - risk are used in place of the mean , it will result in reserves which are too conservative ( if a poor choice of @xmath40 is made ) .",
    "also , using the maximum likelihood approach may significantly underestimate the claims reserves and associated uncertainties .",
    "there are several popular claims reserving models , however we restrict our comparison to the overdispersed poisson and gamma models since they fit into tweedie s compound poisson framework when @xmath114 and @xmath88 respectively .",
    "note that the overdispersed poisson model and several other stochastic models lead to the same reserves as the chain ladder method but different in higher moments .",
    "the detailed treatment of these models can be found in e.g. england and verrall ( 2002 ) or wthrich and merz ( 2008 ) , section 3.2 .",
    "the mles for the reserves and associated uncertainties within the overdispersed poisson and gamma models are provided in table [ tab8 ] .",
    "these results are obtained when the dispersion @xmath115 is estimated by @xmath322 using pearson s residuals ( [ dispersionviapearson ] ) and when @xmath115 is estimated by @xmath113 obtained from the maximization of the likelihood .",
    "the results for the first case are also presented in wthrich and merz ( 2008 ) , table 6.4 .",
    "firstly note that , the values of @xmath323 and @xmath113 are significantly different both for the overdispersed poisson and gamma models . as we mentioned in section 3.2 , for a fixed @xmath40 , the mle for the reserves does not depend on @xmath115 while the estimation error , process variance and msep are proportional to @xmath115 .",
    "as one can see from table [ tab8 ] , different estimators for the dispersion @xmath115 lead to the same estimators for the reserves but very different estimators for the uncertainties . also note that , our mle calculations for tweedie s distribution conditional on @xmath40 , i.e. figure [ fig6 ] , are obtained using @xmath113 and are consistent with the corresponding results for the overdispersed poisson and gamma models when @xmath324 and @xmath325 respectively .",
    "though , in the case of the overdispersed poisson we had to use an extended quasi - likelihood to estimate @xmath113 . in figure [ fig6 ] , we do not show the results based on @xmath322 but would like to mention that these are always above the mles and below the bayesian estimators for the process variance and estimation error and are consistent with corresponding overdispersed poisson and gamma model limits .",
    "interestingly , the ratio @xmath326 is approximately @xmath327 for all considered cases of @xmath40 within a range@xmath328 $ ] .",
    "the mles obtained using both @xmath113 and @xmath323 underestimate the uncertainties compared to the bayesian analysis . note that , while the mles for the uncertainties are proportional to the dispersion estimator , the corresponding bayesian estimators are averages over all possible values of @xmath115 according to its posterior distribution .",
    "the uncertainty in the estimate for the dispersion is large which is also highlighted by a bootstrap analysis in wthrich and merz ( 2008 ) , section 7.3 .",
    "this indicates that @xmath115 should also depend on the individual cells @xmath329 .",
    "however , in this case overparameterization needs to be considered with care and bayesian framework should be preferred .",
    "the results demonstrate the development of a bayesian model for the claims reserving problem when considering tweedie s compound poisson model . the sampling methodology of a gibbs sampler is applied to the problem to study the model sensitivity for a real data set .",
    "the problem of variable selection is addressed in a manner commensurate with the mcmc sampling procedure developed in this paper and the most probable model under the posterior marginal model probability is then considered in further analysis . under this model",
    "we then consider two aspects , model selection and model averaging with respect to model parameter @xmath40 .",
    "the outcomes from these comparisons demonstrate that the model uncertainty due to fixing @xmath40 plays a significant role in the evaluation of the claims reserves and its conditional msep .",
    "it is clear that whilst the frequentist mle approach is not sensitive to a poor model selection , the bayesian estimates demonstrate more dependence on poor model choice , with respect to model parameter @xmath40 .",
    "we use constant priors with very wide ranges to perform inference in the setting where the posterior is largely implied by data only .",
    "also , we run a large number of mcmc iterations so that numerical error in the bayesian estimators is very small . in the case of the data we studied , the mles for the claims reserve ,",
    "process variance and estimation error were all significantly different ( less ) than corresponding bayesian estimators .",
    "this is due to the fact that the posterior distribution implied by the data and estimated using mcmc is materially different from gaussian , i.e. more skewed .",
    "future research will examine variable selection aspects of this model in a bayesian context considering the entire set of possible parameterizations .",
    "this requires development of advanced approaches such as reversible jump mcmc and variable selection stochastic optimization methodology to determine if a more parsimonious model can be selected under assumptions of homogeneity in adjacent columns / rows in the claims triangle .    * * + the first author is thankful to the department of mathematics and statistics at the university of nsw for support through an australian postgraduate award and to csiro for support through a postgraduate research top up scholarship .",
    "thank you also goes to robert kohn for discussions .       *",
    "*   + _ csiro mathematical and information sciences , sydney , locked bag 17 , north ryde , nsw , 1670 , australia _ + and + _ unsw mathematics and statistics department , sydney , 2052 , australia . + email : peterga@maths.unsw.edu.au_ + * * ( corresponding author ) + _ csiro mathematical and information sciences , sydney , locked bag 17 , north ryde , nsw , 1670 , australia .",
    "+ email : pavel.shevchenko@csiro.au_ + * * + _ eth zurich , department of mathematics , ch-8092 zurich , switzerland . +",
    "email : wueth@math.ethz.ch_ +    [ c]|c||rrrrrrrrrr|accident & + year @xmath2 & 0 & 1 & & & &  & @xmath3 &   &  & @xmath4 + @xmath25 & + @xmath86 & & + @xmath330 & & + & & + @xmath2 & & + & & + & & + @xmath330 & & + @xmath331 & & + @xmath4 & & +"
  ],
  "abstract_text": [
    "<S> in this paper we examine the claims reserving problem using tweedie s compound poisson model . </S>",
    "<S> we develop the maximum likelihood and bayesian markov chain monte carlo simulation approaches to fit the model and then compare the estimated models under different scenarios . </S>",
    "<S> the key point we demonstrate relates to the comparison of reserving quantities with and without model uncertainty incorporated into the prediction . </S>",
    "<S> we consider both the model selection problem and the model averaging solutions for the predicted reserves . as a part of this process we also consider the sub problem of variable selection to obtain a parsimonious representation of the model being fitted .    </S>",
    "<S> * keywords : * claims reserving , model uncertainty , tweedie s compound poisson model , bayesian analysis , model selection , model averaging , markov chain monte carlo .    </S>",
    "<S> this is a preprint of an article to appear in + astin bulletin 39(1 ) , pp.1 - 33 , 2009 . </S>"
  ]
}