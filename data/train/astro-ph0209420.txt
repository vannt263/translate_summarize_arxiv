{
  "article_text": [
    "the last few years saw a shift in high performance computing from the paradigm of vector computing to massively parallel systems which allow to solve increasingly complex , physical problems .",
    "we want to concentrate on general concepts to write parallel code which may be helpful to get started and to estimate / choose the _ right ( ! ) _ number of cpus for a given numerical problem . due to the rapid advances in the hardware ,",
    "any number for the performance or the hardware mentioned ( e.g. myrinet , gbit - ethernet ) will be outdated long before this contribution goes into print . to judge the performance of parallel codes ,",
    "the relevant quantity is the ratio between communication and cpu - speed and , thus , the numbers may still be useful .    ideally , code for massively parallel computers should be designed and written from scratch , and it should not be based on any legacy code . however , this approach is both costly and very time consuming .",
    "development cycles may be longer than changes in the computational landscape , and the flexibility may be reduced to answer problems in astronomy .",
    "sometimes , it may be beneficial to adopt a current code base because of faster development cycles , higher flexibility and , more important , to use well tested codes . here , we want to report our attempt of the latter approach to demonstrate problems and possible solutions .",
    "most of our code is written in fortran with some c routines .",
    "our hyddrodynamical radiation transport code is based on a number of individual programs which have been used to carry out many of the analyzes of snia and core collapse supernovae ( @xcite , ... ,",
    "@xcite , @xcite , ... ) .",
    "all components have been written or adopted to a modular form with well defined interfaces which allows an easy coupling ( see fig .",
    "1 ) and code verification by exchanging modules .",
    "the modules consist of physical units to provide a solution for e.g. the nuclear network , the statistical equations to determine the atomic level population , equation of states , the opacities , the hydro or the radiation transport problem .",
    "the individual modules are coupled explicitly .",
    "consistency between the solutions is achieved iteratively by perturbation methods ( see hflich 2002 , this volume ) .",
    "-0.5 cm 0.5 cm   -0.02",
    "cm    the goal of this paper is to provide some help in the transition from existing scalar or vector to parallel codes . after some basics and the general concept , we will discuss various approaches to create parallel code , available tools , and limitations of our approach .    the terms _ scalability _ and _ efficiency _ characterize how well the execution time decrease with the number of processors n(cpu ) and the effective speed per processor compared to n(cpu ) one - processor systems .",
    "the actual numbers have been obtained using a 20 node pc - cluster with dual - pentium iis with 400mhz , 512 mb per node and fast ethernet interconnections .",
    "in addition , we used a 16 node ibm - sp3 system with 4 cpus and 1 gb per node , interconnected by a cross - bar .",
    "three basic approaches to computational resources can be distinguished namely shared memory systems ( e.g. cray t90 , nec ) , distributed memory systems ( beowulf - clusters ) and distributed systems ( grid - computing over the internet ) which represent a sequence from configurations with highly to very loosely coupled cpus corresponding the trend from expensive to inexpensive computers .",
    "main stream computing uses either shared or distributed memory systems , and we will concentrate on those . recently ,",
    "hybrid systems have been introduced which use multiple cpu - nodes to combine the advantages of both .",
    "the basic limitations for multiple - cpus is the communication namely its _ latency _ and _ bandwidth_. the former measures the time it takes to establish a communication link between cpus , the latter provides a measure for the amount of data per time which can be transfered .    in the following ,",
    "we want to discuss the concepts and tools .",
    "the strategy for parallization depends on the kind of numerical problem .",
    "we use either an algorithm based approach , subdivide the computational domain , or use a module based approach .    before discussing details",
    "we want to refer to some web - sides which we found useful to get started :    * ` www.links2go.com/topic/parallel_computing ` * provides links to a wide variety of resources .",
    "* wotug.ukc.ac.uk/parallel * and * www.cs.rit.edu/  ncs / parallel.html * are archives for information on parallel computing .    * www.openmp.org * and * www.crpc.rihce.edu/hpff/home.html * provide a link to communications tools for shared memory systems , namely for openmp and highperformancefortran , respectively .",
    "* www.mcs.anl.gov * and * ` www.epm.ornl.gov/pvm_home.html ` * provide a link to communication tools for distributed memory systems , namely mpi and pvm .",
    "* www.cs.utk.edu/",
    "browne / perftools - review * provides a good overview about tools to analyze mpi and pvm codes .",
    "* www.netlib.org * provides an excellent resource to math - libraries optimized for parallel machines including scalapack , blacs , linpack , clapack , slatec , sparce , etc .",
    "the two basic paradigms / concepts of parallel computing are the _ shared _ and _ distributed _ memory model . in shared memory systems ,",
    "all processors have direct access to a unified memory using high speed buses .",
    "this approach avoids the need for a software based data exchange . on the downside , though , the number of processors for a computer system using a shared memory is rather small , and the stringent requirements on the bus - system result in very expensive computers .",
    "the distributed memory system allows to connect an unlimited number of nodes to gain large computing power at a low cost using standard network technology or relatively inexpensive and , comparably , slow options such as ethernet , cross - bars or myrinet .",
    "each processor has direct access only to the memory on the local node .",
    "data communication has to be handled explicitly by the user .",
    "this puts the burden on the user to manage rather sparse resources but gives more control to the user .      in shared memory systems ,",
    "the communication channels are sufficiently fast to allow parallization for moderate number of cpus on a loop - level , very similar to classical vector computers .",
    "parallization can be achieved by compiler options ( auto - tasking , or hpf ) or , more efficiently , by directives inserted into the code by the user , e.g. by using openmp ( openmp review board , 2000 ) . in distributed memory systems ,",
    "data communication is achieved by explicit message passing using specialized communication libraries , namely pvm ( parallel virtual machine ) ( geist et al . 1994 ) , the message passing interface ( mpi ) ( e.g. mpi , 1994 , @xcite ) , or the new standard mpi2 ( @xcite ) .",
    "pvm has been designed at the oak ridge national laboratory to connect inhomogeneous clusters of workstations .",
    "pvm supports fault tolerance and includes a notification scheme , dynamic hosts and it is very portable due to its pd implementation .",
    "dynamical host allocation means that the number of tasks can be changed during a run .",
    "this is particular useful in complex calculations to adjust the number of parallel tasks to specific problems , and it may be useful on multi - processor systems used by a large number of users because it allows to free resources .",
    "a further advantage of dynamical host allocation is its `` fail - save '' feature .",
    "if a task terminates due to a failing hardware or a numerical problem , a new task can be started on another cpu or another algorithm to continue the calculation .",
    "mpi has been developed to provide fast communication on homogeneous clusters , and it has been adopted as the standard for parallel computing ( e.g. snir et al . 1995 ) .",
    "tasks are static but , as for pvm , the resulting codes are highly portable due to excellent pd implementations ( mpich , 2002 ; lam , byrnes et al .",
    "static task allocation implies that the number of cpu s has to be requested at the beginning and can not be adjusted on the run and that the entire job terminates if a task fails .",
    "mpi is an excellent choice for domain - based parallization and , numerically , very stable algorithm as commonly used in hydro calculations whereas pvm has its virtue if elaborated iteration schemes ( e.g. ali ) are used to solve complex systems of non - linear integro - differential equations such as radiation - hydrodynamical problems including statistical equations which , sometimes , fail to converge ( see hflich , this volume ) .",
    "more precisely , a more aggressive strategy can increase the convergence rates dramatically but , often , on the expense of stability .",
    "even a failor - rate of 1 % of ali becomes a problem in lc - calculations which require several thousands of time steps ( see below ) . in conclusion",
    ", both mpi and pvm are targeted towards distributed memory systems but their design concepts are very different .    to overcome the problems created by two separated communication libraries ,",
    "the new mpi2 standard has been defined in 1997 by the mpi2-forum .",
    "this new standard combines both the advantages of mpi and pvm , and it includes important enhancements such as single sided communication which greatly decrease the latency for small messages and , thus , boost the overall scalability .",
    "unfortunately , no reference implementation has been released which hurts portability of codes .",
    "moreover , most vendor - based implementations are restricted to subset of mpi2 without dynamical task allocation and single sided communication ( e.g. ibm ) .",
    "we use a combination of pvm and openmp but hope to migrate to mpi2 in the near future . using pvm ,",
    "our code is parallized on the level of large modules , e.g. for the equation of state , statistical equation , or packages of photons for radiation transport reducing the communication overhead . to increase the scalability ,",
    "openmp is employed to distribute the computations on the loop level within time consuming routines and to take advantage of the on - board shared memory of multi - processor nodes .",
    ".limitations for distributed memory machines due to the communication latency and bandwidth .",
    "the break - even point is defined by the situation in which an equal amount of time is spent for computations and communications , namely the the number of floating point operations ( flops ) per communication and the number of in local vs. global memory accesses per communication ( mac ) . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ table1 ]      in theory , multi - processing on n processors may reduce the execution time of a code by a factor of n ( or more ) .",
    "however , in reality , the typical gain is much lower and , sometimes , multiple cpus may even cause an increase of the execution time because the speed of non - parallel parts stays the same but communication and administrative overhead increases with the number of processors .",
    "distributed memory systems have a large latency and low communication bandwidth compared to the processing speed of individual cpus .",
    "it may be useful to ask for the break - even - point bep when the communication between two processors requires as much time as the calculations .",
    "some typical examples are given in table 1 .",
    "obviously , parallization is only useful if , per communication , about @xmath0 to @xmath1 floating point operations are performed and about 10 times more local than global data are accessed . in reality ,",
    "an increase in speed by parallization can be expected if the workload per task is significantly larger ( @xmath2 ) .",
    "parallization is very inefficient on an inner - loop level for all but very large vectors ,      for both the domain and ( physical ) module based parallization , we distribute the work by dividing the computational load nt - times according to the spatial or frequency coordinates , use groups of photons or individual elements .    in a direct approach",
    ", one may distribute packages of size @xmath3 as following :    do i= 1,n    send package of size n to task i    enddo    collect the results and wait till the last task is finished    this approach works reasonable well for domain - based parallization if the number of cells are about the same in all sub - domains ( see section 3.2 ) .",
    "however , it becomes very inefficient when a complex system of equations has to be solved .",
    "examples are the solution of implicit equations such as nuclear and atomic networks , or radiation transport by monte - carlo type methods for which the actual work depends critical on the optical depth . in our example",
    ", all processes have to wait for the slowest task .",
    "the direct approach must be modified , and the _ dynamical range _ dr must be taken into account .",
    "we define the dr as the ratio between maximum to minimum cpu time required for a task . in practice , we use package sizes of @xmath4 .",
    "dr must be estimated and optimization of the code means improving the estimator for dr .",
    "small packages increase the overhead in the communication ( see table 1 ) .",
    "the package sizes and the corresponding work loads are reduced with increasing number of cpus .",
    "eventually , this limits the scalability of the problem . often , the performance levels out at 20 to 50 cpus on our beowulf cluster . for improvements ,",
    "we employ auto - tasking or openmp for parallel algorithms and secondary masters on multi - processor nodes with share memory ( see below ) .",
    "programs may use the master / slave approach and/or symmetric multi - processing ( fig .",
    "2 ) . in the master / slave configurations ,",
    "a master - process is used as hub for the communication between different slave processes and to balance the load , i.e. to distribute the work .",
    "in addition , the master may be used for sequential calculations during its idle time .",
    "a centralized approach makes it easy to keep track of the timing and synchronization of tasks , data coherency etc .",
    "however , the entire communication load is handled by one master process running on a specific cpu .",
    "this produces a bottle - neck which limits the scalability of the code .",
    "direct communication between equal slaves , the symmetric multi - tasking , avoids this problem and allows to take full advantage of the available bandwidth of the system .",
    "however , sequential calculations have to be done by each slave , and data coherency may be a problem . in practice",
    ", we use a hybrid approach based on a tree scheme ( fig .",
    "2 ) .    -3.04 cm   + 2.194 cm",
    "the most effective way to parallize code depends largely on the kind of numerical problem . in the following ,",
    "we want to describe some concepts used in our code hydra ( hflich , this volume ) and show some results for illustration .",
    "basically we distinguish between three kind of numerical problems : 1 ) algorithm , 2 ) domain and 3 ) ( physical ) module based parallization .      during the last few years , very sophisticated mathematical algorithm",
    "have been developed specially designed for parallel systems , in particular for linear algebra problems , fourier transformations etc . .",
    "we use this approach for matrix operations and to solve linear equations .",
    "an effective implementation requires a rewrite of the corresponding routines , extensive testing / optimization and , often , is very hardware dependent .",
    "therefore , we use subroutines available on the net namely scalapack , a parallel implementation of lapack .",
    "-0.04 cm    as basic building blocks , scalapack uses basiclinearalgebrasubroutines ( blas ) . in fig .",
    "3 , the execution speed is given as a function of the rank of a matrix .",
    "graphs are given for a single cpu , 2,4 and 8 processor pc - clusters with fast ethernet interconnections using tcp - stacks or directly addressing the memory on each node ( gamma ) .",
    "obviously , there is no gain in speed for a rank @xmath5 .",
    "2 cpus are faster than 4 cpus for @xmath6 .",
    "our tests on an ibm - sp3 showed similar results for a matrix of rank 2000 . by increasing the number of processors by a factor of 16 ( @xmath7 processors )",
    "the speedup was very moderate ( factor 2.2 ) .",
    "therefore , we do not use algorithm based parallization on pc - clusters or distributed memory systems but restrict this approach to multi - processor , shared memory nodes .",
    "when using the distributed memory model ( pvm or mpi ) on a dual - pii - node and matrices of rank 100 and 1000 , we achieve a speedup of 0.9 and 1.6 , respectively . using openmp ,",
    "the corresponding numbers are 1.3 and 1.5 .",
    "therefore , openmp is employed for ranks @xmath8 and , for larger matrices , we use pvm or mpi .    as a general rule for linear algebra problems with of moderate rank ( @xmath9 )",
    ", parallization should be restricted to shared memory systems / nodes . optimized algorithm and machine specific libraries should be used . for small matrices ( @xmath10 ) , openmp seems to give better performance than explicit message parsing .    0.5 cm   with a resolution of @xmath11 .",
    "the snapshot shows the density distribution ( @xmath12 , in cgs ) after about 10 seconds for the explosion of a star with 2.5 @xmath13 triggered by low velocity jet ( @xmath14 ) ( @xcite , hflich et al .",
    ", title=\"fig:\",width=359 ] -0.04 cm      here , the computational domain is subdivided and the workload is distributed over several cpus .",
    "we use this approach for 3-d hydro ( @xcite ) and for the spherical radiation transport in the comoving frame ( mihalas , kunacz & hummer 1975 ) . for the hydro ,",
    "each processor solves the set of equations in a specific spatial domain .",
    "the hydro code solves the compressible reactive flow equations in eulerian frame using an explicit piecewise parabolic method ( ppm ) ( @xcite ) . for adaptive mesh refinement ( amr ) ,",
    "it uses a fully threaded tree ( ftt ) .",
    "communication between neighboring domains is needed to exchange the results at the boundary .",
    "the scalability is limited by the latency . for typical problems with an effective resolution of @xmath15 to @xmath16",
    "( see fig .",
    "4 ) , the code scales well up to 20 and 100 processors with an efficiency of 0.5 to 0.2 on systems with a low latency ( e.g. ibm - sp3 , table 1 ) . on our beowulf cluster with fast - ethernet",
    ", the absolute peak performance is already reached on 8 processors with an efficiency of @xmath17 due to the large latency of the cluster ( see table 1 ) .",
    "another example is the spherical radiation transport module for expanding atmospheres .",
    "the equations are discretized by nd depth and nfr frequency points .",
    "angular resolution is achieved by integrating along nd rays parallel to a main axis with impact parameters according to the radial grid .",
    "typically , we use between 90 and 900 depth points and @xmath18 to @xmath1 frequency points . because the velocity field is monotonic",
    ", the intensity @xmath19 at the frequency ifr depends on the results at the previous frequency ifr-1 making , on a first glance , the frequency space not suitable for parallization despite the large workload which includes the integration over all rays . in principle",
    ", parallization could be done along individual rays . however , the number of operations depends on the number of shells touched by a ray . as a consequence ,",
    "the workload of individual tasks will vary by a factor of nd and some tasks would have very little work per communication . both results in a very low efficiency .",
    "instead , we distribute the workload using frequency groups , and employ load balancing as described above . as pay - off , the boundary conditions of each frequency group depends on the neighbors and the solution has to be iterated to obtain the correct intensity @xmath19 at the boundary . in practice ,",
    "the solution for @xmath19 is pretty well known from the model at a previous time step or the nlte iteration .",
    "typically the efficiency is 30 to 50 % even on our beowulf cluster with a fast - ethernet connection .",
    "we found it very useful to employ parallization which is based on physical modules , e.g. to solve the statistical equations for all elements at a specific grid point ( see fig .",
    "each physical module is designed to solve a specific problem such as the equation of state , nuclear networks , the radiation transport problem etc .. in figure 1 , we indicate the basic variable used to distribute the computational load .",
    "the advantages are obvious .",
    "firstly , each of the modules consists of complex codes ( with several thousands of lines ) and the solution of the problem requires a fair amount of computations .",
    "secondly , only the results have to be communicated although the amount of local data can be significant keeping small the the communication overhead .",
    "e.g. for the nuclear network or the rate equations , a huge amount of atomic and nuclear data are required but only the resulting abundances and level populations have to be communicated .",
    "both these properties are the basic recipe for good scalability and efficiency . on the down - side",
    ", this approach requires relatively large memory per node , typically 512 mb to 1 gb per node .    for the user , this approach is beneficial too .",
    "moderate changes are required in course of the transition from scalar or vector code to massively parallel systems .",
    "this is particular true for the master / slave approach .",
    "the modular approach with well defined interfaces allows easy code verification ( see above ) .",
    "the modules can be based on well tested tested scalar and vector codes , and modules can be parallized and tested one at a time . by using a distributed memory model , the memory space is well separated .",
    "data are only seen globally if they are communicated , all but avoiding memory leaks . of course",
    ", all programmers keep control over all local and global variables and their names .",
    "however , by mistake , the same variable name may be used e.g. in the hydro and the nuclear reaction network which , historically , are based on separate codes .",
    "for the data communication between various modules , we use dedicated subroutines .",
    "e.g. , for the nuclear network , all data relevant are collected by the master and communicated to a dedicated subroutine .",
    "subsequently , this subroutine uses calls to the communication library ( pvm or mpi2 ) to transfer the data to the dedicated routine of the nuclear network which makes the data visible to the module e.g. via common blocks .",
    "as example , we want to discuss the budget for a spherical nlte - calculation of sne ia light curves . for results ,",
    "see hflich ( 2002 , this volume ) .",
    "not all parts of our code have been parallized and , consequently , scaling is far from ideal .",
    "in particular , the 1-d hydro module has not yet been adopted for shared memory computers . however",
    ", the solution of nlte - rate equations and the radiation transport equations are fully parallel . in radiation - hydro problems ,",
    "the solution of the boltzmann equation for the radiation transport and rate equations for the level population is more cpu intensive than the hydro by factors of 50 to 100 .",
    "-2.2 cm 0.3 cm   + 2.2 cm    the calculations have been done on our 40 processor beowulf cluster using spherical hydro in lagrangian coordinates , and detailed equation of state for the atomic level population and expansion opacities .",
    "the radiation transport is solved using the moment equations .",
    "the eddington factors are calculated by solving the time - independent radiation transport equation in comoving frame .",
    "@xmath20-ray transport is done using our monte carlo scheme ( fig .",
    "1 ) . for the atomic level populations ,",
    "we consider a total of 59 super - levels .",
    "radiation transport has been performed using 2500 frequency groups .",
    "the envelope has been discretize by 272 depth points . to calculate the eddington factors , the hydro - grid has been re - mapped to properly resolve the photosphere by 90 radial points .",
    "a ) the rate equations and the radiation transport equations are fully parallized .",
    "the dynamical range dr has been set to 3 for both .",
    "depending on the phase of the light curve , the efficiency @xmath21 was between 30 to 50 % .",
    "b ) as described above , the comoving frame equations are parallized using the frequency grid .",
    "formally , the problem scales almost perfect .",
    "however , we need about 2 to 5 iteration for the boundary conditions , reducing the effective efficiency to @xmath22 .",
    "c ) the @xmath20-ray transport by mc is used to determine the energy deposition functions .",
    "the energy deposition varies very slowly and hardly increases the total cpu time .",
    "d ) finally , the 1-d hydro , moment and energy equations are tuned for shared memory systems ( i.e. nodes ) .",
    "the cpu - time requirements for a ) , b ) and d ) are @xmath23 200 , 800 and 3.8 cpu - seconds per nlte - iteration , respectively . at each time step , we need about 5 to 10 nlte - iteration .",
    "2000 time steps are required for an entire light curve up to day 60 .",
    "our lc computations take about 8 to 16 days which is rather favorable to the 110 ... 220 days on a single processor system .",
    "0.6 cm -rays in a thermonuclear explosion at day 1 ( left ) and 23 ( right ) .",
    "the results have been obtained with our full 3-d radiation transport ( upper panels ) ( hflich 2001 ) .",
    "the diameter of the wd is normalized to 100 . in the lower panels , the @xmath20 spectra at day 23",
    "are given as seen from various @xmath24 of 90 , 60 , 30 and 0@xmath25 ( red , blue , green , pink ) and @xmath26 .",
    "this fluctuations may be produced by instabilities in nuclear burning fronts , and they may be seen by forthcoming @xmath20-ray satellites such as integral . ,",
    "title=\"fig:\",width=173 ] 0.6 cm -rays in a thermonuclear explosion at day 1 ( left ) and 23 ( right ) .",
    "the results have been obtained with our full 3-d radiation transport ( upper panels ) ( hflich 2001 ) .",
    "the diameter of the wd is normalized to 100 . in the lower panels ,",
    "the @xmath20 spectra at day 23 are given as seen from various @xmath24 of 90 , 60 , 30 and 0@xmath25 ( red , blue , green , pink ) and @xmath26 .",
    "this fluctuations may be produced by instabilities in nuclear burning fronts , and they may be seen by forthcoming @xmath20-ray satellites such as integral . , title=\"fig:\",width=181 ]      the final example has been chosen as a warning not to extrapolate scalability of a code and to demonstrate the importance of testing and fine tuning .",
    "monte carlo methods are well known for excellent scalability which , often , is called trivial. in fig .",
    "5 , the increase of performance and efficiency is shows as a function of cpus .",
    "apparently , scaling is nearly perfect for up to 10 processors in all cases which include photon packages of @xmath1 and @xmath27 photons , and the use of homogeneous and heterogeneous cluster of workstations .",
    "however , with increasing number of cpus , the efficency drops rapidly for heterogeneous clusters of workstations .",
    "the translation of data into processor independent formats for the communication is extremely costly and heterogeneous systems should be avoided .",
    "in addition , this example clearly shows the importance to use large work loads and the influence of the latency. often the _ latency _ of a system is more important for the scalability of a problem than _",
    "parallelization may allow to solve problems previously not feasible .",
    "stellar atmospheric programs and radiation hydro problems are most suitable because they use very complex physics which , in general allow a ( physical ) module based approach .",
    "this enables to the use of legacy code",
    ". nevertheless , the efforts to change from scalar and vector - computers are non - trivial and time - consuming .",
    "currently , three architectures are used for parallel computing : 1 ) shared memory and 2 ) distributed memory systems , and 3 ) distributed systems . for most problems , only the first two are a valuable option .",
    "inhomogeneous clusters of workstations ( e.g. pcs + suns ) should be avoided .",
    "often , the scalability of a parallel code can be increased significantly by employing both the shared and distributed memory model .",
    "nowadays , systems become rather common which bundle clusters of multi - processor nodes .",
    "parallization tools on shared memory systems are directive based ( e.g. open mp ) or or done by the compiler .",
    "typically , communication within multi - processor nodes is sufficient fast for task distribution on a loop level .",
    "these systems are highly preferred for algorithm based problems such a linear equations , matrix inversions etc .",
    "the domain and physical module based approach is very suitable for distributed memory systems , and load balancing can be implemented in a straight forward way .",
    "in particular , parallization on a module basis shows a very good scalability and it is easy to implement .",
    "explicit message passing may be a bonus in large program packages because the memory space is well splitted which greatly reduces the problem of memory leaks between physical modules .",
    "dynamical task allocation as a crucial feature in radiation - hydro problems which include statistical equations because it provides a fail - save mode ( see sect .",
    "fortunately , it is realized in both pvm and the new standard mpi2 .",
    "new projects should use the communication libraries based on the mpi2 standard as soon as full implementations become common place .    for high performance ,",
    "it is critical to use the right number of processors .",
    "communication overhead may actually decrease the overall performance significantly .",
    "optimization and dynamical load balancing is critical for good performance .",
    "this research is supported in part by nasa grant lsta-98 - 022 .",
    "the calculations have been performed on the beowulf cluster of the department for astronomy and the highperformancecenter at the university of texas at austin .",
    "byrnes et al .",
    "2002 , the lam foudation , http://www.lam.uc.edu colella , p. ; woodward , p.r . 1984 ,",
    "54 , 174 geist a. , beguelin a. dongorra j. , jiang w. , mancheck r. , sunderam v. 1994 , pvm , mit - press hauschild p. , 2002",
    ", this volume hflich , p. 1995",
    ", apj 443 , 89 hflich , p. 1988",
    ", pasp 7 , 434 h \" oflich p. 2002",
    ", new astronomy , in press & astro - ph/0110098 on relativistic astrophysics , eds .",
    "wheeler & martel , aip conference proceedings 586 , p. 459",
    "hflich p. 2002",
    ", this volume howell , d.  a. , h \" oflich , p. , wang , l. , & wheeler , j.  c.  2001 , , 556 , 302 khokhlov a. , hflich p. , oran e.s . , wheeler j.c .",
    ", p. wang l. , 1999 , apj 524 , l107 khokhlov , a.m. 1998 , j.comput.phys .",
    ", 143 , 519 khokhlov , a. 2001 , apj , submitted & astro - ph/0008463 mihalas d. , kunasz r.b . , hummer d.g .",
    "1975 mpi 1994 , a message passing interface standard , international journal of supercomputer applications 8 , mpich , 2002 , a portable implementation of the message passing interface , ` www-unix.mcs.anl.gov/mpi/mpich/ ` message passing interface forum , 1997 , documents can be found on ` http://www-unix.mcs.anl.gov/mpi/index.html ` openmp architecture review board , 2000 ` http://www.openmp.org ` snir m. , otto s. , huss - lederman s. , walker d. , dongarra j. , 1995 mit press and ` www.netlib.org/utk/papers/mpi-book/mpi-book.html ` sunderam v.s .",
    "1990 , in concurrency : practice and experience , p. 315 , and ` www.epm.ornl.gov/pvm/ `"
  ],
  "abstract_text": [
    "<S> parallel computing has turned out to be the enabling technology to solve complex physical systems . </S>",
    "<S> however , the transition from shared memory , vector computers to massively parallel , distributed memory systems and , recently , to hybrid systems poses new challenges to the scientist . </S>",
    "<S> we want to present a cook - book ( with a very strong , personal bias ) based on our experience with parallization of our existing codes . </S>",
    "<S> some of the general tools and communication libraries are discussed . </S>",
    "<S> our approach includes a mixture of algorithm , domain and physical module based parallization . </S>",
    "<S> the advantages , scalability and limitations of each are discussed at some examples . </S>",
    "<S> we want show that it becomes easier to write parallel code with increasing complexity of the physical problem making stellar atmosphere codes beyond the classical assumptions very suitable .    </S>",
    "<S> # 1_#1 _ # 1_#1 _ = = cmmi9 = cmr10 = cmr7    # 1 1.25 in .125 in .25 in </S>"
  ]
}