{
  "article_text": [
    "despite the rapid progress in designing fully autonomous systems , many systems still require human s expertise to handle tasks which autonomous controllers can not handle or which they have poor performance .",
    "therefore , shared autonomy systems have been developed to bridge the gap between fully autonomous and fully human operated systems . in this paper",
    ", we examine a class of shared autonomy systems , featured by switching control between a human operator and an autonomous controller to collectively achieve a given control objective .",
    "examples of such shared autonomy systems include robotic mobile manipulation @xcite , remote tele - operated mobile robots @xcite , human - in - the - loop autonomous driving vehicle @xcite .",
    "in particular , we consider control under temporal logic specifications .",
    "one major challenge for designing shared autonomy policies under temporal logic specifications is making trade - offs between two possibly competing objectives : achieving the optimal performance for satisfying temporal logic constraints and minimizing human s effort .",
    "moreover , human s cognition is an inseparable factor in synthesizing shared autonomy systems since it directly influences human s performance , for example , a human may have limited time span of attention and possible delays in response to a request . although finding an accurate model of human cognition is an ongoing challenging topic within cognitive science , markov models have been proposed to model and predict human behaviors in various decision making tasks @xcite . adopting this modeling paradigm for human s cognition",
    ", we propose a formalism for shared autonomy systems capturing three important components : the operator , the autonomous controller and the cognitive model of the human operator , into a stochastic _ shared - autonomy system_. precisely , the three components includes a markov model representing the fully - autonomous system , a markov model for the fully human - operated system , and a markov model representing the evolution of human s cognitive states under requests from autonomous controller to human , or other external events .",
    "the uncertainty in the composed system comes from the stochastic nature of the underlying dynamical system and its environment as well as the inherent uncertainty in the operator s cognition .",
    "switching from the autonomous controller to the operator can occur only at a particular set of human s cognitive states , influenced by requests from the autonomous controller to the operator , such as , pay more attention , be prepared for a possible future control action .    under this mathematical formulation ,",
    "we transform the problem of synthesizing a shared autonomy policy that coordinates the operator and the autonomous controller into solving a with temporal logic constraints : one objective is to optimize the probability of satisfying the given temporal logic formula , and another objective is to minimize the human s effort over an infinite horizon , measured by a given cost function . the trade - off between multiple objectives is then made through computing the pareto optimal set .",
    "given a policy in this set , there is no other policy that can make it better for one objective than this policy without making it worse for another objective . in literature ,",
    "pareto optimal policies for s have been studied for the cases of long - run discounted and average rewards @xcite .",
    "the authors in @xcite proposed the weighted - sum method for s with multiple temporal logic constraints by solving pareto optimal policies for undiscounted time - bounded reachability or accumulated rewards .",
    "these aforementioned methods are not directly applicable in our problem due to the time unboundness in both satisfying these temporal logic constraints and the accumulated cost / reward . to this end",
    ", we develop a novel two - stage optimization method to handle the multiple objectives and adopt the so - called _ tchebychev scalarization method _",
    "@xcite for finding a uniform coverage of all pareto optimal points in the policy space , which can not be computed via weighted - sum ( linear scalarization ) methods @xcite as the latter only allows pareto optimal solutions to be found amongst the convex area of the pareto front .",
    "finally , we conclude the paper with an algorithm that generates a pareto - optimal policy achieving the desired trade - off from user - defined weights for coordinating the switching control between an operator and an autonomous controller for a stochastic system with temporal logic constraints .",
    "we provide necessary background for presenting the results in this paper .",
    "a vector in @xmath0 is denoted @xmath1 where @xmath2 are the components of @xmath3 .",
    "we denote the set of probability distributions on a set @xmath4 by @xmath5 . given a probability distribution @xmath6 $ ] , let @xmath7 be the set of elements with non - zero probabilities in @xmath8 .",
    "[ def : labeledmdp ] a _ labeled _ is a tuple @xmath9 where @xmath4 and @xmath10 are finite state and action sets .",
    "@xmath11 is the initial probability distribution over states .",
    "the transition probability function @xmath12 $ ] is defined such that given a state @xmath13 and an action @xmath14 , @xmath15 gives the probability of reaching the next state @xmath16 .",
    "@xmath17 is a finite set of atomic propositions and @xmath18 is a labeling function which assigns to each state @xmath19 a set of atomic propositions @xmath20 that are valid at the state @xmath21 .",
    "@xmath22 is a reward function giving the immediate reward @xmath23 for reaching the state @xmath16 after taking action @xmath24 at the state @xmath21 and @xmath25 is the reward discount factor .",
    "@xmath26    in this context , @xmath27 gives a probability distribution over the set of states .",
    "@xmath28 and @xmath29 both express the transition probability from state @xmath21 to state @xmath16 under action @xmath24 in @xmath30 .",
    "a _ path _ is an infinite sequence @xmath31 of states such that for all @xmath32 , there exists @xmath33 , @xmath34 .",
    "we denote @xmath35 to be a set of actions enabled at the state @xmath21 .",
    "that is , for each @xmath36 , @xmath37 .",
    "a _ randomized policy _ in @xmath30 is a function @xmath38 that maps a finite path into a probability distribution over actions .",
    "a deterministic policy is a special case of randomized policies that maps a path into a single action .",
    "given a policy @xmath39 , for a measurable function @xmath40 that maps paths into reals , we write @xmath41 $ ] ( resp .",
    "$ ] ) for the expected value of @xmath40 when the starts in state @xmath21 ( resp .",
    "an initial distribution of states @xmath43 ) and the policy @xmath39 is used .",
    "a policy @xmath39 induces a probability distribution over paths in @xmath30 .",
    "the state reached at step @xmath44 is a random variable @xmath45 and the action being taken at state @xmath45 is also a random variable , denoted @xmath46 .",
    "we use @xcite to specify a set of desired system properties such as safety , liveness , persistence and stability . in the following , we present some basic preliminaries for specifications and introduce a product operation for synthesizing policies in s under constraints .",
    "a formula in is built from a finite set of atomic propositions @xmath17 , @xmath47 , @xmath48 and the boolean and temporal connectives @xmath49 and @xmath50 ( always ) , @xmath51 ( until ) , @xmath52 ( eventually ) , @xmath53 ( next ) .",
    "given an formula @xmath54 as the system specification , one can always represent it by a @xmath55 where @xmath56 is a finite state set , @xmath57 is the alphabet , @xmath58 is the initial state , and @xmath59 is the transition function .",
    "the acceptance condition @xmath60 is a set of tuples @xmath61 .",
    "the run for an infinite word @xmath62w[1]\\ldots \\in ( 2^{{\\mathcal{ap}}})^\\omega$ ] is an infinite sequence of states @xmath63 where @xmath64 and @xmath65)$ ] .",
    "a run @xmath66 is accepted in @xmath67 if there exists at least one pair @xmath68 such that @xmath69 and @xmath70 where @xmath71 is the set of states that appear infinitely often in @xmath72 .",
    "we define a product operation between a labeled and a .",
    "[ def : product ] given a labeled @xmath73 and the @xmath74 , the _ product _ is @xmath75 , with components defined as follows : @xmath76 is the set of states .",
    "@xmath10 is the set of actions .",
    "@xmath77 $ ] is the initial distribution , defined by @xmath78 where @xmath79 .",
    "@xmath80 $ ] is the transition probability function .",
    "given @xmath81 , @xmath82 , @xmath83 and @xmath84 , let @xmath85 .",
    "the reward function is defined as @xmath86 where given @xmath87 , @xmath83 , @xmath33 , @xmath88 for @xmath33 .",
    "the acceptance condition is @xmath89 . @xmath26",
    "the problem of maximizing the probability of satisfying the formula @xmath54 in @xmath30 is transformed into a problem of maximizing the probability of reaching a particular set in the product @xmath90 , which is defined next .",
    "@xcite the _ end component _ for the product @xmath91 is a pair @xmath92 where @xmath93 is a non - empty set of states and @xmath94 is a randomized policy .",
    "moreover , the policy @xmath39 is defined such that for any @xmath95 , for any @xmath96 , @xmath97 ; and the induced directed graph @xmath98 is strongly connected . here",
    ", @xmath99 is an edge in the directed graph if @xmath100 for some @xmath101 .",
    "an is an end component such that @xmath102 and @xmath103 for some @xmath104 .",
    "@xmath26    let the set of s in @xmath90 be denoted @xmath105 and the set of _ accepting end states _ be denoted by @xmath106 .",
    "note that , by definition , for each @xmath107 , by exercising the associated policy @xmath39 , the probability of reaching any state in @xmath108 is 1 . due to this property , once we enter some state @xmath109 , we can find at least one accepting end component @xmath107 such that @xmath95 , and initiate the policy @xmath39 such that for some @xmath110 , all states in @xmath111 will be visited only a finite number of times and some state in @xmath112 will be visited infinitely often .",
    "the set @xmath105 can be computed by algorithms @xcite in polynomial time in the size of @xmath90 .",
    "we aim to synthesize a shared autonomy policy that switches control between an operator and an autonomous controller .",
    "the stochastic system controlled by the human operator and the autonomous controller , gives rise to two different s with the same set of states @xmath4 , the same set @xmath17 of atomic propositions and the same labeling function @xmath113 , but possibly different sets of actions and transition probability functions .",
    "* autonomous controller : @xmath114 where @xmath115 $ ] is the transition probability function under autonomous controller . * human operator : @xmath116 where @xmath117 $ ] is the transition probability function under human operator .",
    "let @xmath118 $ ] be the initial distribution of states , same for both @xmath119 and @xmath120 .",
    "for the same system , the set of physical actions can be the same for both the autonomous controller and the human .",
    "we can add subscript to distinguish whose action it is .",
    "the models @xmath119 and @xmath120 can be constructed either from prior knowledge or from experiments by applying a policy that samples each action from each state a sufficient amount of times @xcite .    in the shared autonomy system",
    ", the interaction between the autonomous controller and the operator is often made through a dialogue system @xcite .",
    "the controller may send a request of attention , or some other signal to the operator .",
    "the operator may grant the request , or respond to signals , depending on his current workload , level of attention .",
    "admitting that it is not possible to capture all aspects of an operator s cognitive states , we have the following model to capture the evolution of the modeled cognitive state .",
    "the operator s cognition in the shared autonomy system is modeled as an @xmath121 where @xmath122 represents a finite set of cognitive states .",
    "@xmath123 is a finite set of events that trigger changes in cognitive state .",
    "$ ] is the initial distribution .",
    "@xmath125 $ ] is the transition probability function .",
    "@xmath126 is the cost function .",
    "@xmath127 is the cost of human effort for the transition from @xmath128 to @xmath129 under event @xmath130 .",
    "@xmath131 is the discount factor .",
    "@xmath132 is a subset of states at which the operator can take over control .",
    "@xmath26    this cognitive model can be generalized to accomodate different model of operator s interaction with the autonomous controller .",
    "the set @xmath123 of events can be requests sent by the autonomous controller to the operator , a workload that the operator assigns to himself , or any other external event that influences the operator s cognitive state .",
    "this model generalizes the model of operator s cognition in @xcite , in which an event is a request to increase , decrease , or maintain the operator s attention in the control task . in particular",
    ", it is assumed that in a particular set of states , transitions from the autonomous controller to the operator can happen .",
    "for instance , for tele - operated robotic arm or semi - autonomous vehicle , operator may take over control only when he is aware of the system s state and not occupied by other tasks @xcite .",
    "the model is flexible and can be extended to other cognitive models in shared autonomy . in this paper , we assume the model of operator for the given task is given .",
    "one can obtain such a model by statistical learning @xcite .",
    "we illustrate the concepts using the robotic arm example .",
    "consider a robot manipulator having to pick up the objects on a table and place it into a box .",
    "there are two types of objects , small and large . for small and large objects ,",
    "the probabilities of a successful pick - and - place maneuver performed by the autonomous controller is @xmath133 and @xmath134 respectively .",
    "the for the controller is shown in figure  1a . with an operator",
    "tele - operating the robot , the probabilities of a successful pick - and - place maneuver is @xmath135 and @xmath136 respectively .",
    "the operator s cognitive model includes two cognitive states : @xmath137 represents the state when the human does not pay any attention to the system ( at the attention level @xmath137 ) , and @xmath138 represents the state when he pays full attention ( at the attention level @xmath138 ) .",
    "the set of events in @xmath139 is the requests of human attention to the task , @xmath140 where @xmath141 represents the current requested attention level is @xmath130 . for any @xmath142 , @xmath141 , let @xmath143 for @xmath144 , otherwise @xmath145 .",
    "the transition probability function of @xmath139 is shown in figure  1b .",
    "represents there are @xmath146 small objects and @xmath147 large objects remaining to be picked .",
    "the available actions are @xmath24 and @xmath148 for picking up small and large objects , respectively .",
    "the for the robotic arm tele - operated by the human can be obtained by changing the probabilities on the transitions .",
    "( b ) the @xmath139 for modeling the dynamics of human s attention changes.[],scaledwidth=45.0% ]    given two s , @xmath119 for the controller and @xmath120 for the operator , and a cognitive model for the operator @xmath139 , we construct a _ shared autonomy stochastic systems _ as an as follows .",
    "@xmath149 where @xmath150 is the set of states .",
    "a state @xmath151 includes a state @xmath152 of the system and a cognitive state @xmath128 of human .",
    "@xmath153 is the set of actions . if @xmath154 , the system is controlled by the autonomous controller and the event affecting human s cognition is @xmath130 . if @xmath155 , the system is controlled by human operator and the event affecting human s cognition is @xmath130 .",
    "@xmath156 $ ] is the transition probability function , defined as follows . given a state @xmath157 and action @xmath158 , @xmath159 , which expresses that the controller acts and triggers an event that affects the operator s cognitive state . given a state @xmath151 for @xmath160 , and action @xmath161 , @xmath162 , which expresses that the operator controls the system and an event @xmath130 happens and may affect the cognitive state .",
    "$ ] is the initial distribution .",
    "@xmath164 , for all @xmath13 , @xmath165 .",
    "@xmath166 is the labeling function such that @xmath167 .",
    "@xmath168 is a cost function for human effort defined over the state and action spaces and @xmath169 .",
    "@xmath131 is the discount factor , the same in @xmath139 . slightly abusing the notation",
    ", we denote the cost function in @xmath170 the same as the cost function in @xmath139 and the labeling function in @xmath170 the same as the labeling function in @xmath30 .    note",
    "that , although the cost of human effort only contains the cost in his cognitive model , it is straightforward to incorporate the cost of human s actions into the cost function .",
    "we construct @xmath170 in figure  [ fig : samdp ] for the robotic arm example .",
    "for example , @xmath171 , which means the probability of the robot successfully picking up a small object and placing it into the box while the human changes his cognitive state to 1 ( fully focused ) upon the robot s request is @xmath172 . also it is noted that from the states @xmath173 and @xmath174 , no human s action is enabled .",
    "the cost function is defined such that @xmath175 if @xmath144 , otherwise @xmath145 .     for robotic arm example ( note",
    "only a subset of states and transitions are shown ) .",
    "subscripts @xmath176 and @xmath122 distinguish actions performed by the autonomous controller ( @xmath176 ) and the human ( @xmath122 ) , respectively .",
    ", scaledwidth=45.0% ]    the main problem we solve is the following .",
    "[ def : problem ] given a stochastic system under shared autonomy control between an operator and an autonomous controller , modeled as s @xmath120 and @xmath119 , a model of human s cognition @xmath139 , and an specification @xmath54 , compute a policy that is pareto optimal with respect to two objectives :    maximizing the discounted probability of satisfying the specification @xmath54 and    minimizing the discounted total cost of human effort over an infinite horizon .",
    "the definition of pareto optimality in this context is given formally at the beginning of section  [ sec : multiobj ] . by following a pareto optimal policy ,",
    "we achieve a balance between two objectives : it is impossible to make one better off without making the other one worse off .",
    "given an @xmath177 and a @xmath178 , the product following definition  [ def : product ] is @xmath179 . recall that the policy maximizing the probability of satisfying the specification is obtained by first computing the set of s in @xmath90 and then finding a policy that maximizing the probability of hitting the set @xmath180 of states contained in s ( see section  [ sub : mdpltl ] ) .    for quantitative objectives , for example , maximizing the probability of satisfying an formula , or a discounted reward objective over an infinite horizon , a memoryless policy in the product suffices for optimality @xcite . in the following , by policies , we mean memoryless ones in the product .",
    "problem  [ def : problem ] is in fact a multi - objective optimization problem for which we need to balance the cost of human s effort and satisfaction for constraints .",
    "however , the solutions for multi - objective s can not be directly applied due to the constraint that once the system runs into an of @xmath90 , the policy should be constrained such that all states in that are visited infinitely often .",
    "based on the particular constraint , we divide the original problem into a two - stage optimization problem : the policy synthesis for s is separated from solving a multi - objective formulated before reaching a state in an .",
    "the first stage is to balance between a quantitative criterion for a temporal logic objective and a criterion with respect to the cost of human effort before a state in the set @xmath180 is reached .",
    "remind that @xmath180 is the union of states in the accepting end components of @xmath90 .",
    "we formulate it as an . however , for objectives of different types , such as , discounted , undiscounted , and limit - average . the scalarization method for solving s",
    "does not apply .",
    "thus , we consider to use the discounted reachability property @xcite for the given specification , as well as discounted costs for the human attention , with the same discount factor @xmath181 specifying the relative importance of immediate rewards .    for an specification , discounting in the state sequence before reaching",
    "the set @xmath180 means that the number of steps for reaching @xmath180 is concerned @xcite . without discounting , as long as two policies have the same probability of reaching the set @xmath180",
    ", they are equivalent regardless of their expected numbers of steps to reach @xmath180 . with discounting though ,",
    "a policy has smaller expected number of steps in reaching @xmath180 is considered to be better than the other .",
    "[ def : momdpvaluefunc ] given the product @xmath90 , for a state @xmath182 in @xmath90 , the _ discounted probability _ for reaching the set @xmath180 under policy @xmath183 is @xmath184\\ ] ] where the reward function @xmath185 is defined such that @xmath186 if and only if @xmath187 and @xmath188 , otherwise @xmath189 . the discounted total reward with respect to human attention for a policy @xmath190 and a state @xmath182",
    "is @xmath191,\\ ] ] where the reward function @xmath192 is defined such that @xmath193 if and only if @xmath194 and @xmath195 , @xmath196 if @xmath187 and @xmath188 , and @xmath197 otherwise . here",
    ", @xmath198 is the discounted cost of human attention for remaining in an accepting end components under the optimal policy for the second stage . @xmath26",
    "the _ discounted value profile _ , at @xmath182 for policy @xmath39 , is defined as @xmath199 .",
    "we denote @xmath200 as the vector of reward functions . the function @xmath198 is computed in the next section .",
    "@xcite given an @xmath201 and a vector of reward functions @xmath202 , for a given state @xmath203 , policy @xmath39 _ pareto - dominates _",
    "policy @xmath204 at state @xmath182 if and only if @xmath205 and for all @xmath206 .",
    "a policy @xmath39 is _ pareto optimal _ in a state @xmath207 if there is no other policy @xmath204 pareto - dominating @xmath39 . for a pareto - optimal policy @xmath39 at state @xmath182 , the corresponding value profile @xmath208 is referred to as a _ pareto - optimal point ( or an efficient point)_. the set of pareto - optimal point are called the _ pareto",
    "set_. @xmath26    a pareto optimal policy @xmath39 for a given initial distribution is defined analogously by comparing the expectations of value functions under the initial distribution .",
    "we employ tchebycheff scalarization method @xcite to find pareto optimal policies for user specified weights .",
    "first , we solve a set of single objective s , one for each reward function .",
    "let @xmath209 be the value function of the optimal policy @xmath210 with respect to the @xmath211-th reward function .",
    "the ideal point @xmath212 is then computed as follows : for @xmath213 , @xmath214 . given a weight vector @xmath215 where @xmath216 is the weight for the @xmath211-th criterion such that @xmath217 , a pareto optimal policy associated with the weight vector @xmath218 can be found with the following nonlinear program :    @xmath219    where @xmath220 is a small positive real that can be chosen arbitrarily ,",
    "@xmath221 is interpreted as the expected discounted frequency of reaching the state @xmath182 and then choosing action @xmath24 , @xmath222 , and @xmath223 is a positive weighting vector computed from a weight @xmath218 , the ideal points and the nadir points @xcite for all reward functions ( detailed in appendix ) .",
    "the nonlinear programming problem can then be formulated into a linear programming problem in the standard way by setting a new variable @xmath224 .",
    "the pareto optimal policy @xmath225 is defined such that @xmath226 which selects action @xmath24 with probability @xmath227 from the state @xmath182 , for all @xmath203 , @xmath228 .",
    "continue with the robot arm example .",
    "given the discount factor @xmath229 , for the simple objective ( @xmath138st objective ) as quickly as possible of reaching a state at which all objects are in the box , the optimal strategy @xmath230 is shown in the first row of table  [ tbl : pickplacepareto ] .",
    "intuitively , the robot starts by requesting the operator to increase his level of attention and wants to switch control to human as soon as possible as the latter has higher probability of success for a pick - and - place maneuver .",
    "alternatively , the optimal policy with respect to minimizing the cost of human effort ( @xmath231nd objective ) , is to let the robot pick up all the objects since by doing so , eventually all the objects will be collected into the box .",
    "the strategy @xmath232 is shown in the second row of table  [ tbl : pickplacepareto ] .",
    "now suppose that a user gives a weight @xmath233 for the first objective and @xmath234 for the second objective , through normalization , the new weight vector @xmath235 , is obtained with the method in appendix . by solving the linear programming problem in ,",
    "we obtain a pareto - optimal policy @xmath236 shown in the third row of table  [ tbl : pickplacepareto ] . noting that the difference of @xmath236 and @xmath237 is that when it comes to the small object , if the current human attention is high , the robot will request the human to decrease his attention level and therefore , if the object fails to be picked up through tele - operation , the autonomous controller will take over for picking up the small object .",
    "whileas in @xmath237 , the robot prefers the human operator to pick up all objects , no matter it is a big one or a small one .",
    "figure  [ fig : pickplacepareto ] shows the state value for the initial state @xmath238 with respect to reward functions @xmath239 , under the policies @xmath237 , @xmath232 and a subset of pareto optimal policies , one for each weight vector @xmath218 in the set @xmath240 .",
    "[ tbl : pickplacepareto ]    , under policies @xmath237 , @xmath232 and a set of pareto optimal policies @xmath236 , one for each weight vectors in the set @xmath241 .",
    "the @xmath242-axis and @xmath243-axis represent the values of the initial state under the 1st and 2nd criteria , respectively.,scaledwidth=50.0% ]    though the pareto optimal policy for @xmath244 is deterministic in this example .",
    "it may generally need to be randomized for a given weight vector .",
    "so far we have introduced a method for synthesizing pareto optimal policies before reaching a state in one of the accepting end components .",
    "next , we introduce a constrained optimization for synthesizing a policy that minimize the expected discounted cost of staying in an and visiting all the states in that infinitely often .      for a state @xmath182 in @xmath180 , one can identify at least one @xmath107 such that @xmath95 .",
    "it is noted that the policy @xmath245 is a randomized policy that ensures every state in @xmath108 is visited infinitely often with probability 1 @xcite .",
    "however , there might be more than one that contains a state @xmath182 , and we need to decide which to stay in such that the expected discounted cost of human effort for the control execution over an infinite horizon is minimized .",
    "we consider a constrained optimization problem : for each @xmath107 where @xmath246 and @xmath247 , solve for a policy @xmath248 such that the cost of human effort for staying in that is minimized .",
    "the constrained optimization problem is formulated as follows .",
    "@xmath249\\\\ &   \\text{subject to : } \\forall v\\in w , { \\mathrm{pr}}^g(\\forall t , \\exists t'>t , x_{t'}=v ) = 1 , \\text { and } \\\\    & \\forall v \\in w , \\forall a \\notin { \\gamma}(v ) , g(v)(a)=0 , \\end{split}\\end{aligned}\\ ] ] where the term @xmath250 measures the probability of infinitely revisiting state @xmath182 under policy @xmath251 .    the linear program formulated for solving can be obtained as follows : @xmath252\\\\ & \\text{subject to :   for }   v\\in w , \\\\   & \\sum_{a\\in { \\gamma}(v ) } x(v , a)= \\eta(v ) + \\gamma \\sum_{v'\\in v } \\sum_{a'\\in { \\gamma}(v ' ) } \\delta(v',a ' , v ) \\cdot x(v ' , a'),\\\\ & \\forall v\\in w , \\forall a \\in \\sigma , x(v , a ) \\ge 0,\\\\ & \\forall v \\in w , \\sum_{a\\in { \\gamma}(v ) } x(v , a ) > = \\varepsilon , \\text { and } \\\\ &   \\forall v \\in w , \\forall a \\notin { \\gamma}(v ) , x(v , a ) = 0 , \\end{split}\\end{aligned}\\ ] ] where @xmath253 is an arbitrarily small positive real .",
    "@xmath254 $ ] is the initial distribution of states when entering the set @xmath108 . because for single objective optimization the optimal state value does not depend on the initial distribution @xcite , @xmath255 can be chosen arbitrarily from the set of distributions over @xmath108 .",
    "the physical meaning of @xmath256 is the discounted frequency of visiting the state @xmath182 , which is strictly smaller than the frequency of visiting the state @xmath182 as long as @xmath257 . by enforcing the constraints @xmath258 ,",
    "we ensure that the frequency of visiting every state in @xmath108 is non - zero , i.e. , all states in @xmath108 will be visited infinitely often .",
    "the solution to produces a memoryless policy @xmath259 that chooses action @xmath24 at a state @xmath182 with probability @xmath260 . using policy evaluation @xcite , the state value @xmath261 for each @xmath95 under the optimal policy @xmath262 can be computed .",
    "then , the terminal cost @xmath263 is defined as follows . @xmath264 and",
    "the policy after hitting the state @xmath182 is @xmath251 such that @xmath265 .",
    "we now present algorithm  [ alg : twostage ] to conclude the two - state optimization procedure .",
    "[ [ remark ] ] remark + + + + + +     although in this paper we only considered two objectives , the methods can be easily extended to more than two objectives for handling specifications and different reward / cost structures in synthesis for stochastic systems , for example , the objective of balancing between the probability of satisfying an formula , the discounted total cost of human effort , and the discounted total cost of energy consumption .",
    "we apply algorithm  [ alg : twostage ] to a robotic motion planning problem in a stochastic environment .",
    "the implementations are in python and matlab on a desktop with intel(r ) core(tm ) processor and 16 gb of memory .",
    "figure  4a shows a gridworld environment of four different terrains : pavement , grass , gravel and sand . in each terrain",
    ", the mobile robot can move in four directions ( heading north `",
    "n ' , south ` s ' , east ` e ' , and west ` w ' ) .",
    "there is onboard feedback controller that implements these four maneuver , which are motion primitives . using the onboard controller ,",
    "the probability of arriving at the correct cell is @xmath135 for pavement , @xmath266 for grass , @xmath136 for gravel and @xmath267 for sand . alternatively ,",
    "if the robot is operated a human , it can implement the four actions with a better performance for terrains grass , sand and gravel .",
    "the probability of arriving at the correct cell under human s operation is @xmath135 for pavement , @xmath268 for grass , @xmath133 for gravel and @xmath266 for sand .",
    "the objective is that either the robot has to visit region @xmath269 and then @xmath270 , in this order , or it needs to visit region @xmath271 infinitely often , while avoiding all the obstacles .",
    "formally , the specification is expressed with an formula @xmath272 .",
    "figure  4b is the cognitive model of the operator , including three states : @xmath273 , @xmath30 and @xmath122 represent that human pays low , moderate , and high attention to the system respectively .",
    "the costs of paying low , moderate and high attention to the system are @xmath138 , @xmath145 , and @xmath274 , respectively .",
    "action ` @xmath275 ' ( resp .",
    "@xmath276 ) means a request to increase ( resp .",
    "decrease ) the attention and action @xmath277 means a request to maintain the current attention .",
    "the operator takes over control at state @xmath122 .",
    "gridworld , where the disk represents the robot , the cells @xmath269 , @xmath270 , and @xmath271 are the interested regions , the crossed cells are obstacles .",
    "we assume that if the robot hits the wall ( edges ) , it will be bounced back to the previous cell .",
    "different grey scales represents different terrains : from the darkest to the lightest , these are `` sand , '' `` grass , '' `` pavement '' and `` gravel . ''",
    "( b ) the @xmath139 of the human operator.,scaledwidth=45.0% ]    during control execution , we aim to design a policy that coordinates the switching of control between the operator and the autonomous controller , i.e. , onboard software controller",
    ". the policy should be pareto optimal in order to balance between maximizing the expected discounted probability of satisfying the formula @xmath54 , and minimizing the expected discounted total cost of human efforts .",
    "figure  [ fig : gridworldpareto ] shows the state value for the initial state with respect to reward functions @xmath278 for the formula and @xmath279 for the cost of human effort , under the single objective optimal policy @xmath237 and @xmath232 , and a subset of pareto optimal policies , one for each weight vectors @xmath218 in the set @xmath280 .",
    "for the specification , all policies are randomized .",
    ", under policies @xmath237 , @xmath232 and a set of pareto optimal policies @xmath236 , for each @xmath281 .",
    "the @xmath242-axis represents the values of the initial state for discounted probability of satisfying the specification .",
    "the @xmath243-axis represents the values of the initial state with respect to the cost of human effort.,scaledwidth=40.0% ]",
    "we developed a synthesis method for a class of shared autonomy systems featured by switching control between a human operator and an autonomous controller . in the presence of inherent uncertainties in the systems dynamics and the evolution of humans cognitive states , we proposed a two - stage optimization method to trade - off the human effort for the system s performance in satisfying a given temporal logic specification .",
    "moreover , the solution method can also be extended for solving multi - objective s with temporal logic constraints . in the following ,",
    "we discuss some of the limitations in both modeling and solution approach in this paper and possible directions for future work .",
    "we employed two s for modeling the system operated by the human and for representing the evolution of cognitive states triggered by external events such as workload , fatigue and requests for attention .",
    "we assumed that these models are given .",
    "however , in practice , we might need to learn such models through experiments and then design adaptive shared autonomy policies based on the knowledge accumulated over the learning phase . in this respect ,",
    "a possible solution is to incorporate joint learning and control policy synthesis , for instance , pac - mdp methods @xcite , into multi - objective s with temporal logic constraints .",
    "another limitation in modeling is that the current cognitive model can not capture all possible influences of human s cognition on his performance .",
    "consider , for instance , when the operator is bored or tired , his performance in some tasks can be degraded , and therefore the transition probabilities in @xmath120 are dependent on the operator s cognitive states . in this case , we will need to develop a different product operation for combining the three factors : @xmath119 , a set of @xmath120 s for different cognitive states , and @xmath139 , into the shared autonomy system . despite the change in modeling the shared autonomy system ,",
    "the method for solving pareto optimal policies developed in this paper can be easily extended .",
    "consider a multiobjective @xmath282 where @xmath283 is a vector of reward functions and @xmath284 is the discount factor , let @xmath285 be the vectorial value function optimal for the @xmath211-th criterion , specified with the reward function @xmath286 .",
    "an approximation of the nadir point for the @xmath211-th criterion is computed as follows , @xmath287 where @xmath288 is a vector value function obtained by evaluating the optimal policy for the @xmath289-th criterion with respect to the @xmath211-th reward function .",
    "the weight vector after normalization is defined as @xmath290      b.  pitzer , m.  styer , c.  bersch , c.  duhadway , and j.  becker , `` towards perceptual shared autonomy for robotic mobile manipulation , '' in _ ieee international conference on robotics and automation _ , may 2011 , pp . 62456251 .",
    "k.  kinugawa and h.  noborio , `` a shared autonomy of multiple mobile robots in teleoperation , '' in _ proceedings of ieee international workshop on robot and human interactive communication _ , 2001 , pp .",
    "319325 .",
    "s.  gnatzig , f.  schuller , and m.  lienkamp , `` human - machine interaction as key technology for driverless driving - a trajectory - based shared autonomy control approach , '' in _ ieee international symposium on robot and human interactive communication _ , sept 2012 , pp .",
    "913918 .",
    "w.  li , d.  sadigh , s.  sastry , and s.  seshia , `` , '' in _ _ , ser .",
    "lecture notes in computer science , e.  brahm and k.  havelund , eds.1em plus 0.5em minus 0.4emspringer berlin heidelberg , 2014 , vol .",
    "8413 , pp . 470484 .",
    "k.  chatterjee , r.  majumdar , and t.  a. henzinger , `` markov decision processes with multiple objectives , '' in _",
    "symposium on theoretical aspects of computer science_.1em plus 0.5em minus 0.4emspringer , 2006 , pp .",
    "325336 .",
    "v.  forejt , m.  kwiatkowska , and d.  parker , `` pareto curves for probabilistic model checking , '' in _ proceedings of 10th international symposium on automated technology for verification and analysis _ , ser .",
    "lncs , s.  chakraborty and m.  mukund , eds .",
    "7561.1em plus 0.5em minus 0.4emspringer , 2012 , pp .",
    "317332 .",
    "p.  perny and p.  weng , `` on finding compromise solutions in multiobjective markov decision processes , '' in _ proceedings of the 19th european conference on artificial intelligence_.1em plus 0.5em minus 0.4emios press , 2010 , pp .",
    "969970 .",
    "i.  das and j.  e. dennis , `` a closer look at drawbacks of minimizing weighted sums of objectives for pareto set generation in multicriteria optimization problems , '' _ structural optimization _",
    "14 , no .  1 ,",
    "pp . 6369 , 1997 .",
    "k.  chatterjee , m.  henzinger , m.  joglekar , and n.  shah , `` symbolic algorithms for qualitative analysis of markov decision processes with bchi objectives , '' _ formal methods in system design _ , vol .",
    "42 , no .  3 , pp .",
    "301327 , 2013 .",
    "d.  henriques , j.  g. martins , p.  zuliani , a.  platzer , and e.  m. clarke , `` statistical model checking for markov decision processes , '' in _",
    "9th international conference on quantitative evaluation of systems _ , 2012 , pp .",
    "8493 .",
    "mouaddib , s.  zilberstein , a.  beynier , l.  jeanpierre , _",
    "et  al . _ , `` a decision - theoretic approach to cooperative control and adjustable autonomy . '' in _",
    "european conference on artificial intelligence _ , 2010 , pp ."
  ],
  "abstract_text": [
    "<S> in systems in which control authority is shared by an autonomous controller and a human operator , it is important to find solutions that achieve a desirable system performance with a reasonable workload for the human operator . </S>",
    "<S> we formulate a shared autonomy system capable of capturing the interaction and switching control between an autonomous controller and a human operator , as well as the evolution of the operator s cognitive state during control execution . to trade - off human s effort and the performance level , e.g. , measured by the probability of satisfying the underlying temporal logic specification , </S>",
    "<S> a two - stage policy synthesis algorithm is proposed for generating pareto efficient coordination and control policies with respect to user specified weights . </S>",
    "<S> we integrate the tchebychev scalarization method for multi - objective optimization methods to obtain a better coverage of the set of pareto efficient solutions than linear scalarization methods . </S>"
  ]
}