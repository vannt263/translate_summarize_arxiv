{
  "article_text": [
    "an unprecedented volume of observational data and information regarding the distribution and properties of optical sources in the universe is becoming available from ongoing and future sky surveys , both imaging and spectroscopic . these include boss ( baryon oscillation spectroscopic survey ;  @xcite ) , des ( dark energy survey ; @xcite ) , kids ( kilo - degree survey ; @xcite ) , sumire , desi ( dark energy spectroscopic instrument ; @xcite ) , 4most ( 4-meter multi - object spectroscopic telescope ; @xcite ) , j - pas ( javalambre - physics of the accelerated universe astrophysical survey ; @xcite ) , lsst ( large synoptic survey telescope ;  @xcite ) , euclid  @xcite , and wfirst ( wide - field infrared survey telescope ; @xcite ) .",
    "combined with microwave background observations from the planck satellite and ground - based telescopes , such as act ( atacama cosmology telescope ) and spt ( south pole telescope ) , the mining of these and other datasets , such as resulting from new radio surveys , e.g. , vlass ( very large array sky survey ) and x - ray surveys , are expected to yield a host of cosmological and astrophysical insights .",
    "there are several foundational cosmological questions that the datasets will directly address .",
    "perhaps the most pressing is the mysterious cause of the accelerated expansion of the universe  whether it is due to dark energy or a modification of general relativity .",
    "in addition , the observations will also bear on the ultimate nature of dark matter , provide information on the physics of the early universe by probing primordial fluctuations , and enhance our knowledge of the neutrino family , the lightest known massive particles in the universe . aside from these basic cosmological questions ,",
    "the survey data provides an enormous resource for attacking a very large number of astrophysical problems related primarily to understanding the formation of complex structure in the universe .    in order to extract the full extent of information from these remarkable surveys , a similar level of effort",
    "must be undertaken in the realm of theory and modeling . to attain the necessary realism and accuracy , sophisticated , large scale simulations of structure formation",
    "must be carried out .",
    "these simulations address a large variety of tasks : providing predictions for many different cosmological models to solve the inverse problem related to determining cosmological parameters , investigating astrophysical and observational systematics that could mimic physics effects of the dark sector , enabling careful calibration of errors ( by providing precision covariance estimates ) , testing , optimizing , and validating observational strategies with synthetic catalogs , and finally , exploring new and exciting ideas that could either explain puzzling aspects of the observations ( such as cosmic acceleration ) or help to motivate and design the implementation of new types of cosmological probes .",
    "the simulations have to be large enough in volume to cover the observed universe ( or at least a large part of it ) and at the same time possess sufficient mass and force resolution ( a spatial dynamic range of roughly a million ) to resolve objects down to the smallest relevant scales .",
    "as one example , a recently constructed synthetic galaxy and quasar catalog for desi is shown in figure  [ syn_cat ] .",
    "this catalog was based on a 1.1  trillion - particle hacc simulation , with a box - size of @xmath0  gpc ( @xcite ) .",
    "the diverse uses of simulations outlined above also demand fast turn - around times , as not one such simulation is required but , eventually , many hundreds to hundreds of thousands .",
    "the simulation codes , therefore , have to be capable of exploiting the largest supercomputing platforms available today and into the future .",
    "viewed as a single entity , the field of ` modern ' computational cosmology ( @xcite , @xcite ) has largely kept pace with the growth of computational power , but new challenges will need to be faced over the next decade .",
    "this is due to the failure of dennard scaling  ( @xcite ) , which underlay the success of moore s law for about two decades . as a consequence of the fact that single - core clock rate and performance have stalled since 2004/5",
    ", the design of future microprocessors is branching into several new directions , to overcome the related performance bottlenecks  ( @xcite )  the key constraint being set by electrical power requirements . the resulting impact on large supercomputers",
    "is already being seen ; aside from the familiar large multi - core processor clusters , two major approaches can be easily identified .    the first approach is the large homogeneous system , built around a ` system on chip ' ( soc ) design or around many - core nodes , with concurrencies in the multi - million range  the ibm blue gene / q is an excellent example of such an approach ; specific examples include sequoia ( 20  pflops peak ) at lawrence livermore national laboratory and mira ( 10  pflops peak ) at argonne national laboratory , supporting up to @xmath1 million - way concurrency ( sequoia ; half of that on mira ) . the second route is to take a conventional cluster but to attach computational accelerators to the cpu nodes .",
    "the accelerators can range from the ibm cell processor ( as on los alamos national laboratory s roadrunner , first to break the petaflop barrier ) , to gpus as on titan ( 27  pflops peak ) at oak ridge national laboratory , to the intel xeon phi coprocessor as on stampede ( 10  pflops peak ) at the texas advanced computing center .",
    "future evolution of supercomputer systems will almost certainly involve further branching in the space of possible architectures .    because development of the supercomputing software environment is likely to lag significantly behind the pace of hardware evolution , it appears prudent , if not essential , to develop a code design philosophy and implementation plan for cosmological simulations that can be reconfigured relatively quickly for new architectures , has support for multiple programming paradigms , and at the same time , can extract high levels of sustained performance . with this challenge in mind ,",
    "we have designed hacc ( hardware / hybrid accelerated cosmology codes ) , an n - body cosmology code framework that takes full advantage of all available architectures .",
    "this paper presents an overview of hacc ` theory and practice ' by covering the methods employed , as well as describing important components of the implementation strategy .",
    "an example of hacc s capabilities is shown in figure  [ zoom ] , a recent large cosmological run on mira , evolving more than 1 trillion particles ; this is the same simulation on which the results of figure  [ syn_cat ] are based .     box - size simulation with hacc on 32 blue gene / q racks .",
    "the force resolution is @xmath2  kpc and the particle mass , @xmath3  m@xmath4 .",
    "the image , taken during a late stage of the evolution , illustrates the global spatial dynamic range covered by the simulation , @xmath5 , although the finer details are not resolved in this visualization.,width=278 ]      cosmological simulations can be broadly divided into two classes : gravity - only n - body simulations and ` hydrodynamic ' simulations that incorporate gasdynamics and models of astrophysical processes .",
    "since gravity dominates at large scales , and dark matter outweighs baryons by roughly a factor of five , n - body simulations are an essential component in modeling the formation of structure .",
    "several post - processing strategies can be used to incorporate missing physics in gravity - only codes , such as the halo occupation distribution ( hod ) approach ( @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) , subhalo / halo abundance matching ( s / ham ) ( @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) or semi - analytic modeling ( sam ) ( @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) for adding galaxies to the simulations .",
    "whenever a more detailed understanding of the dynamics of baryons is required , gasdynamic , thermal , and radiative processes ( among others ) must be modeled , as well as processes such as star formation and local feedback mechanisms ( outflows , agn / sn feedback ) .",
    "a compact review of cosmological simulation techniques and applications can be found in @xcite .",
    "a concise review of phenomenological galaxy modeling is given in @xcite .",
    "carrying out a fully realistic first principles simulation program for all aspects of modeling cosmological surveys will not be possible for quite some time .",
    "the required gasdynamic simulations are very expensive and there is considerable uncertainty in the modeling and physics inputs .",
    "progress is nevertheless possible by combining high - resolution , large - volume n - body simulations and post - processing inputs from simulations that include gas physics to build robust phenomenological models .",
    "the parameters of these models would be determined by a set of observational constraints ; yet other observations would then function as validation tests .",
    "the hacc approach assumes this starting point as an initial design constraint , but one that can be relaxed in the future .",
    "the overall structure of hacc is based on the realization that a large - scale computational framework must not only meet the challenges of spatial dynamic range , mass resolution , accuracy , and throughput , but , as already discussed , be cognizant of disruptive changes in computational architectures .",
    "as an early validation of its design philosophy , hacc was among the pioneering applications proven on the heterogeneous architecture of roadrunner  ( @xcite ) , the first computer to break the petaflop barrier . with its multi - algorithmic structure , hacc allows the coupling of mpi with a variety of local programming models  mpi+`x '  to readily adapt to different platforms .",
    "currently , hacc is implemented on conventional and cell / gpu - accelerated clusters , on the blue gene / q architecture ( @xcite ) , and has been run on prototype intel xeon phi hardware .",
    "hacc is the first , and currently the only large - scale cosmology code suite world - wide , that has been demonstrated to run at full scale on _ all _ available supercomputer architectures .",
    "another key aspect of the hacc code suite is an inbuilt capability for fast `` on the fly '' or _ in situ _ data analysis . because the raw data from each run can easily be at the petabyte ( pb ) scale or larger , it is essential that data compression and data analysis be maximized to the extent possible , before the code output is dumped to the file system . in order to comply with storage and data transfer bandwidth limitations ,",
    "the amount of data reduction required is roughly two orders of magnitude .",
    "hacc s _ in situ _ data analysis system is designed to incorporate a number of parallel tools such as tools for generating clustering statistics ( power spectra , correlation functions ) , tessellation - based density estimators , a fast halo finder  @xcite with an associated merger tree capability , real - time visualization , etc .",
    "the hacc framework has been used to generate a number of large simulations to carry out a variety of scientific projects .",
    "these include a suite of 64  billion particle runs for predicting the baryon acoustic oscillation signal in the quasar ly-@xmath6 forest , as observed by boss  ( @xcite ) , a high - statistics study of galaxy group and cluster profiles , to better establish the halo concentration - mass relation  ( @xcite ) , tests of a new matter power spectrum emulator  ( @xcite ) , and a study of the effect of neutrino mass and dynamical dark energy on the matter power spectrum  ( @xcite ) .",
    "results from other simulation runs will be available shortly .",
    "these range from simulation campaigns in the @xmath7  billion particle range per simulation , e.g. , simulations for determining sampling covariance for boss  ( @xcite ) , to individual simulations in the @xmath8 billion particle class . as an example of the latter , a recently completed simulation on titan used 550 billion particles to evolve a 1.3  gpc periodic box , with mass resolution , @xmath9  m@xmath10 , and force smoothing scale of @xmath11  kpc  ( @xcite ) .",
    "the paper is organized as follows . in section  [ sec : feat ] we introduce the basic hacc design and algorithms , focusing on how portability and scaling on very large supercomputers is achieved , including discussions of practical matters such as memory management and parallel i / o . a significant aspect of supercomputer architecture is diversity at the level of the computational nodes .",
    "as mentioned above , the hacc design adopts different short - range solvers for different nodal architectures , and this feature is discussed separately in section  [ sec : specs ] .",
    "section  [ sec : verif ] presents some results from our extensive code verification program , showcasing a comparison test with gadget-2  @xcite , one of the most widely used cosmology codes today .",
    "the _ in situ _ analysis tool suite is covered in section  [ sec : tools ] , and selected performance results are given in section  [ sec : perf ] .",
    "we conclude in section  [ sec : conc ] with a recap and a discussion of future evolution paths for hacc .",
    "in the standard model of cosmology , structure formation at large scales is described by the gravitational vlasov - poisson equation ( @xcite ) , a 6-d partial differential equation for the liouville flow ( [ le ] ) of the one - particle phase space distribution , arising from the non - relativistic limit of the vlasov - einstein set of equations , @xmath12 where the poisson equation encodes the self - consistency of the evolution : @xmath13 the expansion history of the universe is given by the time - dependence of the scale factor @xmath14 governed by the specifics of the cosmological model , the hubble parameter , @xmath15 , @xmath16 is newton s constant , @xmath17 is the critical density , @xmath18 , the average mass density as a fraction of @xmath17 , @xmath19 is the local mass density , and @xmath20 is the dimensionless density contrast , @xmath21 @xmath22 in general , the vlasov - poisson equation is computationally very difficult to solve as a partial differential equation because of its high dimensionality and the development of nonlinear structure  including complex multistreaming  on ever finer scales , driven by the gravitational jeans instability .",
    "consequently , n - body methods , using tracer particles to sample @xmath23 are used ; the particles follow newton s equations , with the forces given by the gradient of the scalar potential @xmath24  ( cf .",
    "@xcite ; for an early comparison of direct and particle methods in a non - gravitational context , see @xcite ) .",
    "the cosmological n - body problem is characterized by a very large value of the number of interacting particles and a very large spatial dynamic range . if one wishes to track billions of galaxy - hosting halos at a reasonable mass resolution , then hundreds of billions to trillions of tracer particles",
    "are required .",
    "since gravity can not be shielded , this obviously precludes the use of brute - force direct particle - particle algorithms for the particle force computation .",
    "popular alternatives include pure particle - based methods ( tree codes ) or multi - scale grid - based methods ( amr codes ) , or hybrids of the two ( treepm , particle - particle particle - mesh , p@xmath25 m ) .",
    "it is not our purpose here to go into many details of the algorithms and their implementations ; good coverage of the background material can be found in @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "the hacc design approach acknowledges that , as a general rule , particle and grid - based methods both have their limitations . for physics , algorithmic , and data structure reasons ,",
    "grid - based techniques are better suited to larger ( ` smooth ' ) lengthscales , with particle methods possessing the opposite property .",
    "this suggests that higher levels of code organization should be grid - based , interacting with particle information at a lower level of the computational hierarchy .    following this central idea",
    ", hacc uses a hybrid parallel algorithmic structure , splitting the gravitational force calculation into a specially designed grid - based long / medium range spectral particle - mesh ( pm ) component that is retained on all computational architectures , and an architecture - tunable particle - based short / close - range solver .",
    "the spectral pm component can be viewed as an upper layer that is implemented using c++/c / mpi , essentially independent of the target architecture , whereas , the bottom or node level is where the short / close - range solvers reside .",
    "these are chosen and optimized depending on the target architecture and use different local programming models as appropriate .",
    "of the 6 orders of magnitude required for the spatial dynamic range for the force solver , the grid is responsible for 4 orders of magnitude , while the particle methods handle the critical 2 orders of magnitude at the shortest scales where particle clustering is maximal and the bulk of the time - stepping computation takes place .",
    "the short - range solvers can employ direct particle - particle interactions , i.e. , a p@xmath25 m algorithm  @xcite , as on some accelerated systems , or use both tree and particle - particle methods as on the ibm blue gene / q ( ` pptreepm ' with a recursive coordinate bisection ( rcb ) tree ) . as two extreme cases , in non - accelerated systems , the tree solver provides very good performance but has some complexity in the data structure , whereas for accelerated systems , the local @xmath26 approach is more compute - intensive but has a very simple data structure , better - suited for computational accelerators such as cells and gpus .",
    "the availability of multiple algorithms within the hacc framework also allows us to carry out careful error analyses , for example , the p@xmath25 m and the pptreepm versions agree to within @xmath27 for the nonlinear power spectrum test in the code comparison suite of  @xcite ( see section  [ sec : verif ] for more details ) .",
    "this level of error control easily meets the minimal requirements set by the increased statistical power of next - generation survey observations .    in the following",
    "we first describe the long / medium - range force solver employed by hacc .",
    "as mentioned above , this solver remains unchanged across all architectures . after doing this , we provide details of the architecture - specific short - range solvers , and the sub - cycled time - stepping scheme used by the code suite .",
    "an important aspect of large - volume cosmological simulations is that the density distribution is very highly clustered , with an overall topology descriptively referred to as the `` cosmic web '' .",
    "the clustering is such that the maximum distance moved by a particle is roughly 30  mpc , very much smaller than the overall scale of the simulation box ( @xmath28gpc ) . with a 3-d domain decomposition , each ( non - cubic ) nodal volume ( mpi rank ) is roughly of linear size 50 - 500  mpc , depending on the simulation run size .",
    "the idea behind particle overloading is to ` overload ' the node with particles belonging also to a zone of size roughly 3 - 10  mpc extending out from the nominal spatial boundary for that node ( so - called `` passive '' particles ) .",
    "note that copies of these particles  essentially a replicated particle cache , roughly analogous to ghost zones in pde solvers  will also be held by other processors , in only one of which will they be `` active '' , hence the use of the term ` overloading ' . because more than one copy of these particles is held by neighboring domains ,",
    "overloading is not the same as the guard zone conventionally used to reduce communication in particle codes .",
    "the point of having this particle cache is two - fold .",
    "first , for a number of time steps no particle communication across nodes is required .",
    "additionally , the cached particle ` skin ' allows particle deposition and force interpolation for the spectral particle - mesh method to be done using information entirely local to a node , thus grid communication is also reduced .",
    "the particle cache is refreshed ( replacement of passive particles in each computational domain by active particles from neighboring domains ) at some given number of time steps .",
    "this involves only nearest neighbor communication and the penalty is a trivial fraction of the time spent in a single global time step .",
    "the second advantage of overloading is that when a short - range solver is added to each computational domain , no communication infrastructure is associated with this step .",
    "thus , in principle , short - range solvers can be developed completely independently at the node level , and then inserted into the code as desired .",
    "consequently , hacc s weak scaling is purely a function of the properties of the long - range solver .    the overloaded pm solver is formally exact ",
    "each node sends its local density field ( computed with active particles only ) to the global spectral poisson solver , which then returns the force for both active and passive particles to each node . the short - range force calculation for passive particles",
    "is computed in the same way as for the active particles , except that passive particles , which are closer to the outer domain boundary than the short - range force - matching scale , @xmath29 ( defined in the following section ) , do not have their short - range forces computed , and are subject to only long - range forces .",
    "this avoids force anisotropy near the overloaded zone boundary , at the expense of a force error on the passive particles that are close to the edge of the boundary . since the role of the passive particles is primarily to provide a buffer",
    "` boundary condition ' zone for the active particles near the nodal domain s active zone boundary , consequences of this error are easy to control .",
    "overloading has two associated costs : ( i ) loss of memory efficiency because of the overloading zone , and ( ii ) the just - discussed numerical error for the short - range force that slowly leaks in from the edge of the outer ( passive particle ) domain boundary . in cosmology applications ,",
    "the memory inefficiency can be tolerated to the point where the memory in passive and active particles is roughly equal , but this is not a limitation in the majority of the cases of interest .",
    "the second problem is easily mitigated by balancing the boundary thickness against the frequency of the particle cache refresh , `` recycling '' the passive particles after some finite number of time - steps , chosen to be such that the refresh time is smaller than the error diffusion time : each domain gets rid of all of its passive particles and then refreshes the passive zone with ( high accuracy ) active particles exchanged from nearest - neighbor domains .",
    "conventional particle - based codes employ a combination of spatial and spectral techniques .",
    "the cloud - in - cell ( cic ) scheme used for particle deposition is an example of a real space operation , whereas the fast fourier transform ( fft)-based poisson solver is a @xmath30-space or spectral operation .",
    "the spatial operations in a typical particle code are the particle deposition , the force interpolation , and the finite - differences for computing field derivatives .",
    "the spectral operations include the influence ( or pseudo - green ) function fft solve , digital filtering , and spectral differentiation techniques .",
    "spatial techniques are often less flexible and more tedious to implement than their spectral counterparts . also , higher - order spatial operations can be complicated and lead to messy and dense communication patterns ( e.g. , indirection ) .",
    "accurate p@xmath25 m codes are usually run with triangle - shaped - cloud ( tsc ) , a high - order spatial deposition / filtering scheme , as well as with high - order spatial differencing templates . in terms of grid units ,",
    "the cic deposition kernel filters roughly at the level of two grid cells or less with a large amount of associated `` anisotropy noise '' ; tsc filters at about three grid cells with much reduced noise .",
    "the resulting long - range / short - range force matching is usually done at four / five grid cells or so ( but can be as small as three grid cells ) .",
    "treepm codes can move the matching point to a larger number of grid cells because of the inherent speed of the tree algorithm , so they can continue to use cic deposition ( with some additional gaussian filtering ) .",
    "hacc allows the use of both p@xmath25 m and treepm algorithms , using a shorter matching scale than most treepm codes .",
    "one advantage of the shorter matching scale is that a low - order polynomial expression can be used in the force representation , which greatly speeds up evaluation of the force kernel .",
    "behavior is tuned to occur at a separation of three grid cells , which sets the force - matching scale , @xmath29 .",
    "the low level of the force anisotropy noise is shown by the bracketing red lines representing the 1-@xmath31 deviation .",
    "the solid curve is the fitting formula of eq .",
    "[ fitform].,width=321 ]    the hacc long - range force solver uses only cic mass deposition and force interpolation with a gauss - sinc spectral filter to mimic tsc - like behavior .",
    "in addition , a fourth - order super - lanczos spectral differentiator  ( @xcite ) is used , along with a sixth - order influence function .",
    "this approach allows the data motion to be simplified as no complicated spatial differentiation is needed .",
    "also , by moving more of the poisson - solve to the spectral domain , the inherent flexibility of fourier space can be exploited .",
    "for example , the filtering is flexible and tunable , allowing careful force matching at only three grid cells .",
    "figure  [ ppforce ] shows the force - matching with the spectral techniques using a pair of test particles . in this test",
    ", multiple realizations of particle pairs were taken at fixed distances , but with random orientations of the separation vector in order to sample the anisotropy imposed by the grid - based calculation of the force .",
    "the solution of the poisson equation in hacc s long range force - solver is carried out using large ffts ; the corresponding spectral representation of the inverse ( discrete ) laplacian is the influence function .",
    "hacc employs the following three - dimensional , sixth - order , periodic , influence function : @xmath32^{-1 } ,   \\label{greenf}\\end{aligned}\\ ] ] where the sum is over the three spatial dimensions , @xmath33 is the grid spacing , and @xmath34 the physical box size .",
    "as mentioned earlier , the cic - deposited density field is spectrally filtered using a sinc - gaussian filter : @xmath35^{n_s}.   \\label{filter}\\ ] ] the aim of the filter is to reduce the anisotropy noise as well as control the matching scale where the short - range and long - range forces are matched ; the nominal choices in hacc are @xmath36 and @xmath37 .",
    "this filtering reduces the anisotropy `` noise '' of the basic cic scheme by better than an order of magnitude , and allows for the use of the higher - order spectral differencing scheme .    instead of solving for the scalar potential , and then using a spatial stencil - based differentiation",
    ", hacc uses spectral differentiation within the poisson - solve itself , using fourth - order spectral lanczos derivatives , as previously mentioned .",
    "the ( one - dimensional ) fourth - order super - lanczos spectral differentiation for a function , @xmath38 , given at a discrete set of points is @xmath39 where @xmath40 are coefficients in the fourier expansion of @xmath38 .",
    "the `` poisson - solve '' in the hacc code is the composition of all the kernels above in one single fourier transform .",
    "note that each component of the field gradient requires an independent fft .",
    "this entails some extra work , but is a very small fraction of the total force computation , the bulk of which is dominated by the short - range solver .",
    "an efficient and scalable parallel fast fourier transform ( fft ) is an essential component of hacc s design , and determines its weak scaling properties .",
    "although parallel fft libraries are available , hacc uses its own portable parallel fft implementation optimized for low memory overhead and high performance .",
    "since slab - decomposed parallel ffts are not scalable beyond a certain point ( restricted to @xmath41 ) , the hacc fft implementation uses data partitioning across a two - dimensional subgrid , allowing @xmath42 , where @xmath43 is the number of mpi ranks and @xmath44 is the linear size of the 3-d array .",
    "the resulting scalable performance is sufficient for use in any supercomputer in the foreseeable future  ( @xcite ) .",
    "the implementation consists of a data partitioning algorithm which allows an fft to be taken in each dimension separately .",
    "the data structure of the computing nodes prior to the fft is such as to divide the total space into regular three - dimensional domains .",
    "therefore , to employ a two - dimensionally decomposed fft , the distribution code reallocates the data from small ` cubes ' , where each cube represents the data of one mpi rank , to thin two - dimensional ` pencil ' shapes , as depicted schematically in figure  [ pencil ] .",
    "once the distribution code has formed the pencil data decomposition , a one - dimensional fft can be taken along the long dimension of the pencil .",
    "moreover , the same distribution algorithm is employed to carry out the remaining two transforms by redistributing the domain into pencils along those respective dimensions .",
    "the transposition and fft steps are overlapped and pipelined , with a reduction in communication hotspots in the interconnect .",
    "lastly , the dataset is returned to the three - dimensional decomposition , but now in the spectral domain .",
    "pairwise communication is employed to redistribute the data , and has proven to scale well in our larger simulations .",
    "a demonstration of this is provided by the blue gene / q sytems , where we have run on up to @xmath45 million mpi ranks ( @xcite ) . as the grid size",
    "is increased on a given number of processors , the communication efficiency ( i.e. , the fraction of time spent communicating data between processors ) , remains unchanged .",
    "this is an important validation of our implementation design , as the communication cost of the algorithm must not outpace the increase in local computation performance when scaling up in size .",
    "further details of the parallel fft implementation will be presented elsewhere .",
    "the total force on a particle is given by the vector sum of two components : the long - range force and the short - range force .",
    "at distances greater than the force - matching scale , only the long - range force is needed ( at these scales , the ( filtered ) pm calculation is an excellent approximation to the desired newtonian limit , see figure  [ ppforce ] ) . at distances less than the force - matching scale , @xmath29 ,",
    "the short - range force is given by subtracting the residual filtered grid force from the exact newtonian force .    to find the residual filtered pm force , we compute it numerically using a pair of test particles ( since in our case no analytic expression is available ) , evaluating the force at many different distances at a large number of random orientations .",
    "the results are fit to an expression that has the correct asymptotic behaviors at small and large separation distances ( cf .",
    "we used the particular form : @xmath46 which , with @xmath47 , @xmath48 , @xmath49 , @xmath50 , @xmath51 , @xmath52 , @xmath53 , and @xmath54 , provides an excellent match to the data from the test code ( figure  [ ppforce ] ) , with errors much below @xmath27 . at very small distance scales ,",
    "the gravitational force must be softened , and this can be implemented using plummer or spline kernels ( see , e.g. , @xcite ) .    the force expression , eq .",
    "( [ fitform ] ) , is complex and to implement faster force evaluations one can either employ look - ups based on interpolation or a simpler polynomial expression",
    ". the communication penalty of look - ups can be quite high , whereas an extended dynamic range is difficult to fit with sufficiently low - order polynomials . in our case ,",
    "the choice of a short matching scale , @xmath29 , enables the use of a fifth - order polynomial approximation , which can be vectorized for high performance .    depending on the target architecture , hacc uses two different short - range solvers , one based on a tree algorithm , the other based on a direct particle - particle interaction ( p@xmath25 m ) .",
    "tree methods are employed on non - accelerated systems , while both p@xmath25 m and tree methods can be used on accelerated systems .",
    "hacc uses an rcb tree in conjunction with a highly - tuned short - range polynomial force kernel .",
    "( an oct - tree implementation also exists , but is not the current production version . ) the implementation of the rcb tree , although not the force evaluation scheme , generally follows the discussion in  @xcite .",
    "( multiple rcb trees are used per rank to enhance parallelism , as described in section  [ rcb ] . )",
    "two core principles underlie the high performance of the rcb tree s design ( @xcite ) .        _ spatial locality . _",
    "the rcb tree is built by recursively dividing particles into two groups .",
    "the dividing line is placed at the center of mass coordinate perpendicular to the longest side of the box ( figure  [ rcb ] ) .",
    "once the line of division is chosen , the particles are partitioned such that particles in each group occupy disjoint memory buffers .",
    "local forces are then computed one leaf node at a time .",
    "the net result is that the particle data exhibits a high degree of spatial locality after the tree build ; because the computation of the short - range force on the particles in any given leaf node , by construction , deals with particles only in nearby leaf nodes , the cache miss rate during the force computation is very low .    _",
    "walk minimization .",
    "_ in a traditional tree code , an interaction list is built and evaluated for each particle . while the interaction list size scales only logarithmically with the total number of particles ( hence the overall @xmath55 complexity ) ,",
    "the tree walk necessary to build the interaction list is a relatively slow operation .",
    "this is because it involves the evaluation of complex conditional statements and requires `` pointer chasing '' operations .",
    "a direct @xmath26 force calculation scales poorly as @xmath56 grows , but for a small number of particles , a thoughtfully - constructed kernel can still finish the computation in a small number of cycles .",
    "the rcb tree exploits our highly - tuned short - range force kernels to decrease the overall force evaluation time by shifting workload away from the slow tree - walking and into the force kernel .",
    "up to a point , doing this actually speeds up the overall calculation : the time spent in the force kernel goes up but the walk time decreases faster .",
    "obviously , at some point this breaks down , but on many systems , tens or hundreds of particles can be in each leaf node before the crossover is reached .",
    "we point out that the force kernel is generally more efficient as the size of the interaction list grows : the relative loop overhead is smaller , and more of the computation can be done using unrolled vectorized code .",
    "in addition to the performance benefits of grouping multiple particles in each leaf node , doing so also increases the accuracy of the resulting force calculation : the local force is dominated by nearby particles , and as more particles are retained in each leaf node , more of the force from those nearby particles is calculated exactly . in highly - clustered regions ( with very many nearby particles ) , the accuracy can increase by several orders of magnitude when keeping over 100 particles per leaf node .",
    "the p@xmath25 m implementation within hacc follows the standard method of building a chaining mesh to control the number of particle - particle interactions ( @xcite ) .",
    "this algorithm is used within hacc when working with accelerated systems .",
    "we defer further details regarding the architecture - specific implementation of the short - range force to section  [ sec : specs ] , where the different alternatives are covered separately .",
    "the time - stepping in hacc is based on the widely employed symplectic scheme , as used , e.g. , in the impact code ( @xcite ) , the forerunner of mc@xmath57  ( mesh - based cosmology code , see , e.g. , @xcite ) , in turn the pm precursor of hacc .",
    "the basic idea here is not to finite - difference the equations of motion , but to view evolution as a symplectic map on phase space .",
    "symplectic integration in hacc approximates the full evolution to second order in the time - step by composing elementary maps using the campbell - baker - hausdorff series expansion ( @xcite ) . in pm mode ,",
    "the elementary maps are the ` stream ' and ` kick ' maps @xmath58 and @xmath59 corresponding to the free particle ( kinetic ) piece and the one - particle effective potential in the hamiltonian , respectively . in the stream map ,",
    "the particle position is drifted using its known velocity , which remains unchanged ; in the kick map , the velocity is updated using the force evaluation , while the position remains unchanged .",
    "a symmetric ` split - operator ' symplectic step @xmath60 is termed sks ( stream - kick - stream ) ; a ksk step is another way to implement a second - order symplectic integrator .",
    "( in the presence of explicitly time - dependent hamiltonan pieces , the map evaluations have to be implemented at the correct times to maintain second - order accuracy . )    in the presence of both short and long - range forces , we split the hamiltonian into two parts , @xmath61 where @xmath62 contains the kinetic and particle - particle force interaction ( with an associated map @xmath63 ) , whereas , @xmath64 is just the long range force , corresponding to the map @xmath65 . since the long range force varies relatively slowly , we construct a single time - step map by subcycling @xmath63 : @xmath66 the total map @xmath63 being a usual second - order symplectic integrator .",
    "this corresponds to a ksk step , where the s is not an exact stream step as in the pm case , but has enough @xmath63 steps composed together to obtain the required accuracy . for typical problems",
    "the number , @xmath67 , of short time steps for each long time step will range between 3 - 10 , depending on accuracy requirements .",
    "because the late - time distribution of particles is highly clustered , there can be a substantial advantage in using different ( synchronized ) local time steps down to the single - particle level . although hacc is currently designed for a regime where extreme dynamic range",
    "is not needed , as in treating the innermost part of galaxy halos or in tracking orbits around black holes  where this advantage is most felt ( e.g. , @xcite )  the automatic density information available in the short - range force solvers is used to enable multi - level time - stepping , resulting in speed - ups by a factor of 2 - 3 , with only small effects on the accuracy .",
    "more on this topic can be found in section  [ sec : specs ] .",
    "hacc uses comoving coordinates for positions and velocities .",
    "the actual internal representation of all variables is in the dimensionless form : @xmath68 where the fundamental scaling length , @xmath69 , is the length of a single grid cell of the long - range force pm - solver , @xmath70 , where @xmath34 is the box - size and @xmath71 is the number of grid points in a single dimension , @xmath72 is the current value of the hubble parameter , and the background mass density , @xmath73 .",
    "hacc uses powers of the scale factor , @xmath74 , as the actual evolution variable , with a nominal default value of @xmath75 ; time - stepping is performed using the variable @xmath76 , @xmath77 .",
    "besides easy portability between different architectures , another very important feature of hacc is its highly optimized memory footprint .",
    "pushing the simulation limits in large - scale structure formation problems means running simulations with as many particles as possible , and this often implies running as close as possible to the memory limit of the machine . as a result , memory fragmentation becomes a serious problem .",
    "to make matters worse , hacc is required to allocate and free different data structures during different parts of each time step because there is not enough available memory to hold all such structures at the same time .",
    "furthermore , many of these data structures , such as the rcb tree used for the short - range force calculation , have sizes that change dynamically with each new time step .",
    "this , combined with other allocations from the mpi implementation , message printing , file i / o , etc . with lifetimes that might outlast a time - step phase ( e.g. long - range force computation , short - range force computation , _ in situ _",
    "analysis ) , is a recipe for fatal memory fragmentation problems  problems that we actually encountered on the blue gene / q systems .    to mitigate this difficulty",
    "we have implemented a specialized pool allocator called bigchunk .",
    "this allocator grabs a large chunk of memory , and then distributes it to various other subsystems .",
    "during the first time step , bigchunk acts only as a wrapper of the system s memory allocator , except that it keeps track of the total amount of memory used during each phase of the time step . before the second time step begins",
    ", bigchunk allocates an amount of memory equal to the maximum used during any phase of the previous time step plus some safety factor .",
    "subsequent allocations are satisfied using memory from the ` big chunk ' , and all such memory is internally marked as free after the completion of each time - step phase .",
    "this en - masse deallocation , the only kind of deallocation supported by bigchunk , allows for an implementation that has minimal overhead , and the time it takes to allocate memory from bigchunk is very small compared to the speed of the system allocator . because the bigchunk memory is not released back to the system , memory fragmentation no longer fatally affects the ability of the time - step phases to allocate their necessarily - large data structures .",
    "the persistent state information in the simulation is carried by the particle attributes .",
    "while the number of particles in each mpi rank s ( overloaded ) spatial sub - volume is similar , structure formation implies some variance . once the overload cache is filled , the ( total ) number of particles on a rank is fixed until the cache is emptied and refreshed , at which point the number of particles on each rank can change . in order to avoid memory fragmentation with persistent information",
    "we must anticipate the maximum number of particles any rank will need during the course of a simulation run and allocate that amount of memory for particle information at the start , before any other significant memory allocation occurs .",
    "we estimate the maximum number of particles by choosing a maximum representative volume from which bulk motion could move all of the particles into a rank s ( overloaded ) sub - volume and multiply that volume by the average particle density .",
    "the actual memory allocations are monitored during runtime , allowing for adjustments to be made while the code is running . in practice for the sub - volumes with side lengths that are at least several tens of mpc",
    "this results in allocating memory for an additional skin of particles that is 6 - 10  mpc thick .",
    "hacc prints a memory diagnostic at each time step to indicate the extrema of particle memory usage across all ranks , and the amount of extra memory can be adjusted when restarting the code if the initial estimate appears to be insufficient for later times .",
    "in addition to the space required for the actual particle information , we also allocate an array of integers to represent a permuted ordering of the particles and a scratch array large enough to hold any single particle attribute .",
    "these enable out - of - place re - ordering of the particle information one attribute at a time without additional memory allocation .      a plurality of i / o strategies are needed for different use cases , machine architectures , and data sizes because no single i / o strategy works well under all of these conditions .",
    "our three main approaches are described below .",
    "_ one file per process .",
    "_ using one output file per process ( i.e. mpi rank ) is the simplest i / o strategy , and continues to provide the best write bandwidth compared to any other strategy .",
    "because every process writes into a separate file , after file creation , there is no locking or synchronization needed in between processes .",
    "unfortunately , while simple and portable , one file per process only works for a modest number of processes ( typically less than 10,000 ) .",
    "no file system can manage hundreds of thousands of files that would result from checkpointing a large - scale run with one file per process .",
    "additionally , as a practical matter , managing hundreds of thousands of files is cumbersome and error - prone . finally , even when the number of files is reasonable , reading the stored data back into a different number of processes than used to write the data requires redistribution .",
    "this happens when the output is used for analysis on a smaller cluster ( or machine partition ) , or for visualization . in such cases ,",
    "the required reshuffling of all of the data in memory is equivalent to the aggregation done by more complex collective i / o strategies and cancels out the simplicity of the one file per process approach . for improved scalability and flexibility , hacc supports the following additional i / o strategies .        _ many processes per file . _",
    "the default i / o strategy used by hacc , called genericio , partitions the processes in a system - specific manner , and each partition writes data into a custom self - describing file format .",
    "each process writes its data into a distinct region of the file in order to reduce contention for file - system - level page locks . within the region assigned to each process ,",
    "each variable is written contiguously .",
    "on modern supercomputers , such as ibm blue gene and cray x series machines , dedicated i / o nodes execute special i / o forwarding system software on behalf of a set of compute nodes .",
    "for example , on ibm blue gene / q systems , one i / o node is assigned to 128 compute nodes . by writing one file per i / o node ,",
    "the total number of files is reduced by at least the ratio of compute nodes to i / o nodes , and a high percentage of the peak available bandwidth can be captured by the reading and writing processes . by partitioning the processes by i / o - node assignment and providing each process with a disjoint data region",
    ", we are taking advantage of the technique successfully used by the glean i / o library  ( @xcite ) on the blue gene / p and blue gene / q systems .",
    "the i / o implementation can use mpi i / o , in either collective or non - collective mode , or ( non - collective ) posix - level i / o routines .",
    "importantly , 64-bit cyclic - redundancy - check ( crc ) codes are computed for each variable for each rank , and this provides a way to validate data integrity .",
    "this detects corruption that occurs while the data is stored on disk , while files are being transferred in between systems , and while being transmitted within the storage subsystem(s ) . during a recent run which generated  100  tb of checkpoint files , a single error in a single variable from one writing process",
    "was detected during one checkpoint - reading process , and we were able to roll - back to a previous checkpoint and continue the simulation with valid data .",
    "furthermore , over the past two years , crc validation has detected corrupted data from malfunctioning memory hardware on storage servers , misconfigured raid controllers and bugs in ( or miscompiled ) compression libraries .",
    "the probability of seeing corrupt data from any of these conditions is small ( even while they exist ) , and overall , modern storage subsystems are highly reliable , but when writing and reading many petabytes of data at many facilities the probability of experiencing faults is still significant .",
    "the crc64 code is in the process of being transformed into an open - source project .",
    "an example of genericio performance under production conditions on the blue gene / q system mira is given in table  [ table : genio - perf ] . in tests , i / o performance very close to the peak achievable has been recorded . under production conditions ,",
    "we still achieve about two - thirds of the peak performance .    _ a single file using parallel netcdf .",
    "_ when peak i / o performance is not required , and the system s mpi i / o implementation can deliver acceptable performance , we can make use of a parallel - netcdf - based i / o implementation ( @xcite ) . the netcdf format is used by simulation codes from many different science domains , and readers for netcdf have been integrated with many visualization and analysis tools .",
    "because netcdf has an established user community , and will likely be supported into the foreseable future , writing data into a netcdf - based format should make distributing data generated by hacc to other outside groups easier than if only custom file formats were supported .",
    "the file schema that we developed for the parallel netcdf format is shown in figure [ fig : pnetcdf ] .",
    "as the figure shows , particles are organized and indexed according to the spatial subdomains ( blocks ) of the simulation , one block per process .",
    "the spatial extents of each block are also indexed .",
    "currently , three modes of reading the parallel netcdf files are supported .",
    "first , given some number of processes that need not be the same as the number of blocks , particles can be redistributed uniformly among the new number of processes while being read collectively .",
    "second , the particles in a single block can be retrieved given the block i d .",
    "third , particles in a desired bounding box can be retrieved .",
    "the queried bounding box need not match the extents of any one block and particles overlapping several blocks may be retrieved in this manner .",
    "the performance for writing the parallel netcdf output is shown in table [ table : pnetcdf - perf ] .",
    "these tests were run on hopper ( cray xe6 ) at the national energy research scientific computing center ( nersc ) with a lustre file system . because the file system is shared by all running jobs",
    ", performance results will vary due to resource contention ; the values in table [ table : pnetcdf - perf ] are the means of four runs for each configuration .    to put these results in perspective , consider the last row of table  [ table : pnetcdf - perf ] .",
    "the peak performance expected for the number of object storage targets ( osts , 128 in this case ) is approximately 26.7  gib / s ( 1 gib = @xmath78 = 1,073,741,824 bytes ) .",
    "this value represents ideal conditions of writing large amounts of data to one file per ost .",
    "in contrast , our strategy uses one shared file with collective i / o aggregation and a high - level format with index data in addition to raw particle arrays .",
    "even so , we achieve 56% of the peak ideal bandwidth .",
    "b0.5 in b0.5 in b0.5 in b0.5 in b0.3 in no .",
    "particles & no .",
    "processes & file size ( gib ) & write time ( s ) & write bandwidth ( gib / s ) + @xmath79 & 512 & 43.8 & 22.0 & 1.90 + @xmath80 & 16384 & 1332.4 & 99.0 & 12.88 + @xmath81 & 262144 & 43821.6 & 380.5 & 109.9 +    [ table : genio - perf ]    b0.5 in b0.5 in b0.5 in b0.5 in b0.3 in no .",
    "particles & no .",
    "processes & file size ( gib ) & write time ( s ) & write bandwidth ( gib / s ) + @xmath82 & 64 & 4.6 & 3.54 & 1.30 + @xmath79 & 512 & 36 & 14.34 & 2.51 + @xmath83 & 4096 & 288 & 19.10 & 15.1 +    [ table : pnetcdf - perf ]",
    "in this section we go over the choice of algorithms deployed as a function of nodal architecture , as well as the corresponding optimizations implemented so far  performance optimization is a continuous process .      in order to evaluate the short - range force on non - accelerated systems , such as the blue gene / q",
    ", hacc uses an rcb tree in conjunction with a highly - tuned short - range polynomial force kernel , as has been discussed in section  [ srf ] .",
    "an important consideration in this implementation is the tree - node partitioning step , which is the most expensive part of the tree build .",
    "the particle data is stored as a collection of arrays  the so - called structure - of - arrays format .",
    "there are three arrays for the three spatial coordinates , three arrays for the velocity components , in addition to arrays for mass , a particle identifier , etc .",
    "our implementation in hacc divides the partitioning operation into three phases .",
    "the first phase loops over the coordinate being used to divide the particles , recording which particles will need to be swapped .",
    "next , these prerecorded swapping operations are performed on six of the arrays .",
    "the remaining arrays are identically handled in the third phase .",
    "dividing the work in this way allows the blue gene / q hardware prefetcher to effectively hide the memory transfer latency during the particle partitioning operation and reduces expensive read - after - write dependencies .",
    "the premise underlying the multilevel timestepping scheme ( section  [ time - stepper ] ) is that particles in higher density regions will require finer short - range time steps .",
    "a local density estimate can be trivially extracted from the same rcb tree constructed for evaluating the short - range interparticle forces .",
    "each `` leaf node '' in the rcb tree holds some number of particles , the bounding box for those particles has already been computed , and a constant density estimate is used for all particles within the leaf node s bounding box . because the bounding box , and thus the density estimate , changes as the particles are moved , the timestepping level assigned to each leaf node is fixed when the tree is constructed .",
    "this implies that , after particles have been moved , the leaf - node bounding boxes may overlap .",
    "the force - evaluation algorithm is insensitive to these small overlaps , and the effect on the efficiency of the force calculation is apparently negligible .",
    "[ see eq .",
    "( [ kicks ] ) ] versus 5 sub - cycle steps for each particle .",
    "shown is the ratio of the final power spectra for a small test problem ( 256  @xmath84mpc , 256@xmath25 particles ) , going out to the particle nyquist wave number .",
    "the multi - level time step approach leads to a speed - up of the full simulation by a factor of @xmath282 , with only a negligible change in the error.,width=302 ]    in between consecutive long - range force calculations , each particle is operated on by the kick ( velocity update ) and stream ( position update ) operators of the short - range force .",
    "if a particle , based on the density of its leaf node at the beginning of the subcycle , is evolved using @xmath85 kicks , then it needs to be acted on by @xmath86 stream operators each evolving the particle by @xmath87 . to ensure time synchronization , these stream operators are further split such that all particles are acted on by the same number of stream operators",
    "the number of kicks used for particles in a leaf node is determined by : @xmath88 where @xmath89 is an adjustable linear scaling parameter . in addition , the maximum level is capped by an additional user - provided parameter .",
    "we show examples of accuracy control in the multi - level time stepping scheme in figures  [ time1 ]  [ time3_dens ] .    in figure",
    "[ time1 ] , we compare the power spectrum obtained from a simulation with 5 sub - cycle steps for each particle with a result that was obtained in the following way : 5 sub - cycles per step per particle are used until @xmath90 ; since the clustering at that point is still modest , this point is reached relatively quickly .",
    "after @xmath90 we evolve each particle with at least 2 sub - cycles and allow ",
    "depending on the density  two more levels of sub - cycling . in this test , setting the scaling parameter @xmath89 to 20 or 10 leads to accurate results , better than 0.2% out to the particle nyquist wavenumber , @xmath91 , where @xmath33 is the initial inter - particle separation ( the test case used 256@xmath25 particles in a 256  @xmath84mpc volume with 500  pm steps . ) in precision cosmology applications , one desires better than @xmath92 accuracy , and this is attained at wavenumbers less than @xmath93  ( @xcite ) .",
    "consequently , the current error limits are comfortably wihtin the required bounds , the error at @xmath93 being only @xmath94 .",
    "using the multi - level stepping can speed up the simulation by a factor of two ( changing @xmath89 in the range shown leaves the performance unaffected ) .",
    "we have carried out a suite of convergence tests , concluding that the setting with @xmath95 satisfies our accuracy requirements .    at much smaller length scales ,",
    "the power spectrum test above can be augmented by checking the stability of small scale structures in the halos as the adaptive time - stepping parameters are varied . as typical examples thereof ,",
    "we show results for the largest and second - largest halos ( identified using a ` friends - of - friends ' or fof algorithm ) in the same simulation discussed above in figures  [ time2 ]  [ time3_dens ] .",
    "the halo density field is computed via a tessellation - based method in three dimensions and then projected onto a two - dimensional grid ; details of this implementation will be presented elsewhere  ( @xcite ) .",
    "the angle - averaged ( spherical ) density profiles are also shown in figures  [ time2_dens ] and [ time3_dens ] . as can be seen from these results , aside from trivial differences due to the fof linking noise , the halo substructure depends relatively mildly on the choice of the values of the scaling parameter , @xmath89 , over the chosen ranges used .    )",
    "halo at @xmath96 for the same run as in figure  [ time1 ] , with different values for @xmath89 .",
    "minor variations in the outskirts of the halos are due to fof linking ` noise'.,width=264 ]     at corresponding values of the scaling parameter , @xmath89.,width=302 ]     for the second largest halo.,width=283 ]     for the second - largest halo.,width=302 ]    aside from optimizing the number of force evaluations , one also has to minimize the time spent in evaluating the force kernel .",
    "this is a function of the design of the compute nodes .",
    "here we provide a description of the blue gene / q - specific short - range force kernel and how it is optimized .",
    "( very similar implementations were carried out for cray xe6 and xc30 systems . ) as mentioned earlier , the compactness of the short - range interaction ( cf .",
    "section  [ sec : lrange ] ) , allows the kernel to be represented as @xmath97 where @xmath98 , @xmath99 is a 5-th order polynomial in @xmath100 , and @xmath101 is a short - distance cutoff ( plummer softening ) .",
    "this computation must be vectorized to attain high performance ; we do this by computing the force for every neighbor of each particle at once .",
    "the list of neighbors is generated such that each coordinate and the mass of each neighboring particle is pre - generated into a contiguous array .",
    "this guarantees that 1 ) every particle has an independent list of particles and can be processed within a separate thread ; and 2 ) every neighbor list can be accessed with vector memory operations , because contiguity and alignment restrictions are taken care of in advance .",
    "every particle on a leaf node shares the interaction list , therefore all particles have lists of the same size , and the computational threads are automatically balanced .",
    "the filtering of @xmath100 , i.e. , checking the short - range condition , can be processed during the generation of the neighbor list or during the force evaluation itself ; since the condition is likely violated only in a number of `` corner '' cases , it is advantageous to include it into the force evaluation in a form where ternary operators can be combined to remove the need of storing a value during the force computation .",
    "each ternary operator can be implemented with the help of the instruction , which also has a vector equivalent .",
    "even though these alterations introduce an ( insignificant ) increase in instruction count , the entire force evaluation routine becomes fully vectorizable .",
    "there is significant flexibility in choosing the number of mpi ranks versus the number of threads on an individual blue gene / q node . because of the excellent performance of the memory sub - system , a large number of openmp threads  up to 32 per node",
    " can be run to optimize performance .",
    "concurrency in the short - range force evaluation is exposed by , first , building a work queue of leaf - node - vs - tree interactions , and second , executing those interactions in parallel using openmp s dynamic scheduling capability .",
    "each work item incorporates both the interaction - list building and the force calculation itself for each leaf node s particles .    to further increase the amount of parallel work",
    ", hacc builds multiple rcb trees per rank .",
    "first , the particles are sorted into fixed bins , where the linear size of each bin is roughly the scale of the short - range force .",
    "an rcb tree is then constructed within each bin , and because this process in each bin is independent of all other bins , this is done in parallel .",
    "this parallelization of the tree - building process provides a significant performance boost to the overall force computation . when the force on the particles in each leaf node is computed , not only must the parent tree be searched , but so must the other 26 neighboring trees . because of the limited range of the short - range force , only nearest neighbors need to be considered . while searching many neighboring trees",
    "adds extra expense , the trees are individually not as deep , and so the resulting walks are less expensive . also , because we distribute ` ( leaf node , neighboring tree ) ' pairs among the threads , this scheme also increases the amount of available parallelism post - build ( which helps with thread - level load balancing ) .",
    "all told , using multiple trees in this fashion provides a significant performance advantage over using one large tree for the entire domain .",
    "the first version of hacc was originally written for the ibm powerxcell 8i - accelerated hardware of roadrunner , the first machine to break the petaflop barrier . this architecture posed three critical challenges , all of which continue to be faced in one way or the other on all accelerated systems .",
    "a more detailed description of the cell implementation and the roadrunner architecture is given in @xcite .",
    "( see also @xcite . )",
    "the three challenges for a roadrunner - style architecture are as follows .",
    "( i ) _ memory balance . _",
    "the machine architecture ( figure  [ rr_arch ] ) has a top layer of conventional multi - core processors ( in this case , two dual - core amd opterons ) to which are attached ibm powerxcell 8i cell broadband engines ( cell  bes ) via an eight - lane pci - e bus .",
    "the relative performance of the opterons is small compared to that of the cell  bes , by roughly a factor of 1:20 , but they carry half the memory and possess access to a communication fabric that is balanced to their level of computational performance . for large - scale n - body codes",
    ", memory is a key limiting factor , therefore the code design must make the best use possible of the cpu layer ( this situation continues in current and future accelerated systems , as discussed further below ) .",
    "( ii ) _ communication balance .",
    "_ the cell  bes dominate the computational resource , but are starved for communication , due to the relatively slow pci - e link to the host cpu ( figure  [ rr_arch ] ) . from the point of view of the compute / communication ratio , such a machine is 50 - 100 times out of balance .",
    "we note that this situation also continues to hold in the current generation of accelerated systems such as cpu / gpu or cpu / xeon phi machines : the computational approach taken must therefore maximize computation for a given amount of data motion .",
    "( iii ) _ multiple programming models . _ accelerated systems have a multi - layer programming model . on the cpu level ,",
    "standard languages and interfaces can be used ( hacc uses c / c++/mpi / openmp ) but the accelerator units often have a hardware - specific programming paradigm .",
    "( although attempts to overcome this gap now exist , the results  in actual practice  are not yet compelling . ) for this reason , it is desirable to keep the code on the accelerator ( the cell  be in this case ) as simple as possible and avoid elaborate coding .",
    "in addition , it also proved advantageous to keep the data structures and communication patterns on the cell  be as straightforward as possible to optimize computational efficiency .        with these challenges in mind ,",
    "hacc is matched to the machine architecture as follows : at the first level of code organization , the medium / long range force is handled by the fft - based method that operates at the opteron layer , as for all other architectures . at this layer ,",
    "only grid information is stored and manipulated ( except when carrying out analysis steps ) .",
    "particles live only at the cell layer .",
    "there is a rough memory balance between the grid and particle data , matching well to the memory organization on the machine , and combating the first challenge mentioned above .",
    "the particle - grid deposition and grid - particle interpolation steps are performed at the cell layer with only grid information passing between the two layers .",
    "this compensates for the limited bandwidth available between the cell  bes and the opterons .",
    "the local force calculations reside at the cell level .",
    "this addresses the second challenge , as aided by the particle overloading discussed in section  [ sec : overload ] . because implementing complicated data structures at the cell level is difficult , and conditionals are best avoided , our choice for the local force solve is a direct particle - particle interaction . to make this interaction more efficient , we use a chaining mesh to control the number of interactions  ( see , e.g. , @xcite )",
    "this leads to an efficient , hardware - accelerated p@xmath25 m algorithm , thereby overcoming the third challenge ( @xcite ) .",
    "as already discussed above , some of the challenges for gpu - accelerated systems are very similar in spirit to those for cell - accelerated systems .",
    "the low - level gpu programming model ( opencl or cuda ) adds another layer of complexity and the compute to communication balance is heavily skewed towards computing .",
    "one major difference between the two architectures is the memory balance : while on the cell - accelerated systems the cell layer has the same amount of memory as the cpu layer , this is generally not the case for cpu / gpu systems .",
    "for example , the cray xk7 system , titan , at oak ridge , has 32  gb of host - side memory on a single node , with only 6  gb of gpu memory .",
    "this adds yet another challenge , that of memory imbalance .",
    "we have overcome this by partitioning the local data into smaller overlapping blocks , which can fit on device memory . very similar in spirit to particle overloading ,",
    "the boundaries of the partitions are duplicated , such that each block can be evolved independently .",
    "we emphasize again that the long / medium range calculations on this architecture remain unchanged , and only the short - range force kernel needs to be optimized .",
    "the data partitioning is illustrated in figure  [ data_part ] .",
    "we utilize a two - dimensional decomposition of data blocks , which are in turn composed of slabs that are spatially separated by the extent of the short - range force  roughly 3 grid cells ( see figure  [ ppforce ] ) . in complete analogy to particle overloading , the data blocks are composed of ` active ' particles ( green ) that are updated utilizing the ` passive ' particles ( yellow and red ) from the boundary .",
    "the ( red ) edge slabs are solely streamed , as opposed to performing the full sks time stepping described in section  [ time - stepper ] .",
    "this mediates the error inflow on these passive particles , as they can only `` see '' the interior particles within the domain .",
    "we note that since the data has been decomposed into smaller independent work items , these blocks can now be communicated to any nodes that have the available resources to handle the extra workload .",
    "hence , this scheme provides for a straightforward load - balancing algorithm by construction .",
    "details of the error analysis and load balancing schemes will be described in an upcoming paper devoted to the gpu implementation of hacc  @xcite .",
    "m version ( blue short - dashed ) .",
    "we also show the comparison with a gadget-2 run .",
    "the agreement is very good  the treepm runs agree with the gpu version of hacc to better than 0.1% up to the particle nyquist wavenumber .",
    "the level of agreement with gadget-2 is noteworthy because it is a completely independent code .",
    ", width=307 ]",
    "[ cols=\"^,^,^,^,^\",options=\"header \" , ]         . the solid curve is the prediction from the extended coyote emulator of @xcite .",
    "the agreement across the two runs is at the fraction of a percent level , while the agreement with the emulator is at the 2% level , which is the estimated emulator accuracy .",
    ", width=302 ]    hacc has been subjected to a large number of standard convergence tests ( second - order time - stepping , halo profiles , power spectrum measurements , etc . ) . in this section",
    "we focus mostly on a subset of hacc test results using the setup of the code comparison project , originally carried out in  @xcite . in that work , a set of initial conditions was created for different problems ( mainly different volumes ) and a number of cosmological codes were run on those , all at their nominal default settings .",
    "the final outputs were compared by measuring a variety of statistics , including matter fluctuation power spectra , halo positions and profiles , and halo mass functions .",
    "the initial conditions and final results from the tests are publicly available and have been used subsequently by other groups for code verification , e.g for gadget-2  @xcite , and most recently for nyx  @xcite .",
    "in addition , we will also show some results from recently carried out large - scale simulations .",
    "we first restrict attention to the larger volume simulation ( 256  @xmath84mpc ) and compare hacc results with those found for gadget-2 , as published in  @xcite . while the simulation is only modest in size ( 256@xmath25 particles ) it does present a relatively sensitive challenge and is capable of detecting subtle errors in the code under test .",
    "not only are statistical measures such as the power spectrum robust indicators of code accuracy , but visual inspection of the particle data itself presents a quick qualitative check on code behavior and correctness ; it is particularly valuable in identifying problems at early stages of code development .",
    "we use paraview for this purpose ( @xcite ) .",
    "the code comparison test was run with a force resolution of @xmath102kpc , very similar to what was used in the gadget-2 simulation .",
    "we compare results from different hacc versions ( pptreepm on the blue gene / q and hopper , p@xmath25 m on a cell - accelerated and a gpu - accelerated system ) with those from gadget-2 .",
    "the result for the matter power spectrum is shown in figure  [ comp_pk ] , where , as in figure  [ time1 ] , we show results up to @xmath103 .",
    "all the code results are very close to each other ( note the scale on the y - axis ) .",
    "the agreement over the full @xmath30-range is better than 0.5% , including gadget-2 . considering the different implementations of force solvers and time steppers ,",
    "this closeness is very reassuring , particularly as no effort was made to fine - tune the level of agreement .",
    "the treepm versions of hacc and the cpu / gpu version agree to better than a tenth of a percent .",
    "next we present results from a more qualitative , but nevertheless , very detailed comparison , shown in figure  [ comp_vis ] . in this test , we identify all particles that belong to halos with at least 100 particles per halo .",
    "this is done within paraview , using an fof halo finder with linking length @xmath104 .",
    "the particles are colored with respect to halo mass .",
    "the most massive halo in the image ( colored in red ) has a mass of @xmath105m@xmath10 .",
    "while there are differences in the images  as is to be expected  the overall agreement is striking .",
    "almost all small halos exist in all images ( the ones that are missing are just below the cut of 100 particles , but they do actually exist ) and many of the fine details within the halo structures of the larger halos are well - matched .",
    "the mass profiles of the three largest halos in the simulation are shown in figure  [ comp_prof ] .",
    "the binning shot noise dominates the comparison at small radii , but beyond that the agreement is very good .",
    "other quantitative halo comparison statistics are given in table  [ tab1 ] , to further illustrate the close match of the results from all of the different algorithms .",
    "we illustrate the dynamic range and accuracy of the hacc approach by comparing results for the matter power spectrum from the ` outer rim ' and ` q continuum ' runs in figure  [ big_pk ] .",
    "the q continuum run on titan had @xmath106 billion particles , a 1.3  gpc box , with a mass resolution , @xmath107  m@xmath10 , and the outer rim run on mira had @xmath108 trillion particles , a 4.225  gpc box , with a mass resolution , @xmath109  m@xmath10 .",
    "the numerical results ( run with different short - range force algorithms ) agree to fractions of a percent , and agree at the 2% level with the ( extended ) coyote emulator predictions of @xcite , which is at the level of accuracy expected from the emulator .",
    "finally , we show results for the halo mass function at very large simulation scales illustrating excellent agreement across different sized simulations carried out using different short - range force implementations .",
    "figure  [ mf_big ] shows the ( fof , @xmath104 ) halo mass function at @xmath90 resulting from three different simulations with the same cosmology , ( i ) a run from the mira universe suite ( @xmath110 billion particles , 2.1  gpc box , mass resolution , @xmath111  m@xmath10 ) , ( ii ) the q continuum run on titan , and ( iii ) the outer rim run on mira .    )",
    "halo mass function for three large simulations run with hacc , measured at @xmath90 .",
    "the box sizes range from @xmath112  gpc and the particle number in each simulation ranges from @xmath110 billion to over one trillion ( mass resolutions range from @xmath113  m@xmath10 to @xmath114  m@xmath10 ; for details , see text).,width=321 ]",
    "an entire suite of _ in situ _ analysis tools ( cosmotools ) for hacc is under continuous development , driven by evolving science goals ; cosmotools exists in both _ in situ _ and stand - alone versions ( to be used for off - line processing ) . the overall structure of the _ in situ _",
    "framework is shown in figure [ fig : framework ] .",
    "output from the analysis is available either at run time or for post - processing . in the former case ,",
    "a paraview server can be launched and connected to the simulation through a tool called catalyst . in the latter case ,",
    "results of the _ in situ _ analysis are written to a parallel file system and later retrieved .",
    "several standard tools for the analysis of cosmological simulations are part of the _ in situ _",
    "framework , such as halo finders  ( @xcite ) and merger tree constructors .",
    "the first tool to be part of this framework that works on the full particle data to produce field information is a parallel voronoi tessellation that computes a polyhedral mesh whose cell volume is inversely proportional to the distance between particles .",
    "such a mesh representation acts as a continuous density field that affords accurate sampling of both high- and low - density regions .",
    "connected components of cells above or below a certain density can also approximate large - scale structures .",
    "two important criteria for _ in situ _ analysis filters are that they should scale similarly as the simulation and have minimal memory overhead .",
    "the parallel tessellation approach meets these criteria ; full details are given in @xcite .",
    "the various tools can be turned on through the configuration file for hacc , and the frequency of their execution is also adjustable .",
    "hacc runs on a variety of supercomputing platforms and has scaled to the maximum size of some of the fastest machines in the world , including roadrunner at los alamos , sequoia at livermore , titan at oak ridge , mira at argonne , and hopper at nersc .",
    "we have carried out detailed scaling and performance studies on the blue gene / q systems  ( @xcite ) and on titan ; a sample of our results is presented below .    for both systems",
    ", we carried out weak and strong scaling tests .",
    "for the weak scaling tests we fix a physical volume and number of particles per node . when scaling up to more nodes , the volume and particle loading therefore increases , while the mass resolution stays constant .",
    "the wall - clock time for a run should hence stay constant if the code scales or , equivalently , the time to solution per particle per step should decrease . the absolute performance measured in tflops per seconds will rise while the percentage per peak will stay constant . for our weak scaling tests ,",
    "the particle mass is @xmath115m@xmath10 and the force resolution , 6  kpc .",
    "all simulations are for a @xmath116cdm model .",
    "simulations of cosmological surveys focus on large problem sizes , therefore the weak scaling properties are of primary interest .    for the weak scaling test on the blue gene / q systems , we ran with 2 million particles in a @xmath28(100  @xmath84mpc)@xmath25 volume per core , using a typical particle loading in actual large - scale simulations on these systems . tests with 4 million particles per core produce very similar results . as demonstrated in figure  [ comp ] ( right panel ) , weak scaling is ideal up to 1,572,864 cores ( 96 racks , all of sequioa ) , where hacc attains a peak performance of 13.94  pflops and parallel efficiency of @xmath117 . the largest test simulation on sequoia evolved @xmath283.6 trillion particles and a ( very high accuracy ) particle substep took @xmath118  ns for the full high - resolution code .",
    "the scaling results were obtained by averaging over 50 sub - cycles .    on titan we ran with 32 million particles per node in a fixed ( nodal ) physical volume of @xmath119 , representative of the particle loading in actual large - scale runs ( the gpu version was run with one mpi rank per node ) .",
    "the results are shown in the left panel of figure  [ comp ] .",
    "in addition ( not shown ) we have timing results for a 1.1  trillion particle run , where we have kept the volume per node the same but increased the number of particles per node by a factor of two to 64.5 million . as for the blue gene / q systems , hacc weak - scales essentially perfectly up to the full machine .",
    "for the strong scaling tests , we chose the same problem size on both systems , a ( 1000 @xmath84mpc)@xmath25 volume with 1024@xmath25 particles .",
    "this is a rather small problem and strong scaling is expected to break down at some point .",
    "the results for both systems are shown in figure  [ comp ]  the strong scaling regime is remarkably large .",
    "on the blue gene / q system , we demonstrate strong scaling between 512 and 16384 cores ( with somewhat degraded performance at the largest scale ) . for titan",
    ", we increase the number of nodes for this problem from 32 to 8192 , almost half of the machine .",
    "as can be seen in the left panel in figure  [ comp ] , strong scaling only starts degrading after 2048 nodes .",
    "( the results for titan have been improved since these earlier tests , they will be reported in @xcite ) .",
    "the significance of the strong scaling tests is in showing how well a code can perform as the effective memory per core reduces  hacc can run very effectively at values as low as 100mb / core , which is roughly equivalent to the memory per core available in next - generation many - core systems .",
    "the impressive scale and quality of data from sky survey observations requires a correspondingly strong response in theory and modeling .",
    "it is widely recognized that large - scale computing must play an essential role in shaping this response , not only in interpreting the data , but also in optimizing survey strategies and validating data and analysis pipelines .",
    "the hacc simulation framework is designed to produce synthetic catalogs and to run large campaigns for precision predictions of cosmological observables .",
    "the evolution of hacc continues to proceed in two broad directions : ( i ) the further development of algorithms ( and their optimization ) for future - looking supercomputing architectures , including areas such as power management , fault - tolerance , exploitation of local non - volatile random - access memory ( nvram ) and investigation of alternative programming models , and ( ii ) addition of new physics capabilities in both modeling and simulation ( e.g. , gas physics , feedback processes , etc . ) and in the analysis part of the framework ( e.g. , increased sophistication in semi - analytic galaxy modeling , and an associated validation program ) .",
    "the use of hacc for large - scale simulation campaigns covers applications such as those required to construct cosmological emulators ( @xcite ) , to determine covariance matrices ( @xcite ) , to help optimize survey design and test associated pipelines with synthetic catalogs ( @xcite ) , and , finally , to carry out mcmc - based parameter estimation across multiple cosmological probes ( @xcite ) .",
    "a major component of the future use of hacc is therefore related to the production and exploitation of large simulation databases , including public access and analysis ; work in this area is in progress with a number of collaborators .",
    "for useful interactions and discussions over time , the authors thank jim ahrens , viktor decyk , nehal desai , wu feng , chung - hsing hsu , rob latham , pat mccormick , rob ross , robert ryne , paul sathre , sergei shandarin , volker springel , joachim stadel , and martin white .",
    "we acknowledge early contributions to hacc by nehal desai and paul sathre . running on a number of supercomputers ,",
    "a large fraction of which were in their acceptance phase , required the generous assistance and support of many people .",
    "we record our indebtedness to susan coghlan , kalyan kumaran , joe insley , ray loy , paul messina , mike papka , paul rich , adam scovel , tisha stacey , william scullin , rick stevens , and tim williams ( argonne national laboratory ) , brian carnes , kim cupps , david fox , and michel mccoy ( lawrence livermore national laboratory ) , lee ankeny , sam gutierrez , scott pakin , sriram swaminarayan , andy white , and cornell wright ( los alamos national laboratory ) , and bronson messer and jack wells ( oak ridge national laboratory ) .",
    "argonne national laboratory s work was supported under u.s .",
    "department of energy contract de - ac02 - 06ch11357 .",
    "part of this research was supported by the doe under contract w-7405-eng-36 .",
    "partial support for this work was provided by the scientific discovery through advanced computing ( scidac ) program funded by the u.s .",
    "department of energy , office of science , jointly by advanced scientific computing research and high energy physics .",
    "this research used resources of the alcf , which is supported by doe / sc under contract de - ac02 - 06ch11357 and resources of the olcf , which is supported by doe / sc under contract de - ac05 - 00or22725 . some of the results presented here result from awards of computer time provided by the innovative and novel computational impact on theory and experiment ( incite ) and ascr leadership computing challenge ( alcc ) programs at alcf and olcf ."
  ],
  "abstract_text": [
    "<S> current and future surveys of large - scale cosmic structure are associated with a massive and complex datastream to study , characterize , and ultimately understand the physics behind the two major components of the ` dark universe ' , dark energy and dark matter . </S>",
    "<S> in addition , the surveys also probe primordial perturbations and carry out fundamental measurements , such as determining the sum of neutrino masses . </S>",
    "<S> large - scale simulations of structure formation in the universe play a critical role in the interpretation of the data and extraction of the physics of interest . just as survey instruments continue to grow in size and complexity , so do the supercomputers that enable these simulations . </S>",
    "<S> here we report on hacc ( hardware / hybrid accelerated cosmology code ) , a recently developed and evolving cosmology n - body code framework , designed to run efficiently on diverse computing architectures and to scale to millions of cores and beyond . </S>",
    "<S> hacc can run on all current supercomputer architectures and supports a variety of programming models and algorithms . </S>",
    "<S> it has been demonstrated at scale on cell- and gpu - accelerated systems , standard multi - core node clusters , and blue gene systems . </S>",
    "<S> hacc s design allows for ease of portability , and at the same time , high levels of sustained performance on the fastest supercomputers available . </S>",
    "<S> we present a description of the design philosophy of hacc , the underlying algorithms and code structure , and outline implementation details for several specific architectures . </S>",
    "<S> we show selected accuracy and performance results from some of the largest high resolution cosmological simulations so far performed , including benchmarks evolving more than 3.6 trillion particles .    </S>",
    "<S> cosmology  large scale structure of the universe ; n - body simulations </S>"
  ]
}