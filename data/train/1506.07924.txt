{
  "article_text": [
    "this paper aims at developing new learning algorithms with desirable convergence properties for certain classes of stochastic games , which are discrete - time dynamic games in which the history can be summarized by a `` state '' @xcite .",
    "more specifically , we focus on weakly acyclic stochastic games that can be used to model cooperative systems .",
    "the chief merit of the paper lies in the fact that learning takes place in stochastic games , which are truly dynamic games , as opposed to learning in repeated games in which the same single - stage game is played in every stage . in stochastic games , the policies selected by the decision makers not only impact their immediate cost but",
    "also alter the stage - games to be played in the future through the state dynamics .",
    "hence , our results are applicable to a significantly broader set of applications .",
    "the existing literature on learning in stochastic games is very small in comparison with the literature on learning in repeated games .",
    "as the method of reinforcement learning gained popularity in the context of markov decision problems , a surge of interest in generalizing the method of reinforcement learning , in particular q - learning algorithm @xcite , to stochastic games has led to a set of publications primarily in the computer science literature ; see @xcite and the references therein . in",
    "many of these publications , the authors tend to assume that the real objective of the agents is for some reason to find and play an equilibrium strategy ( and sometimes this even requires agents to somehow agree on a particular equilibrium strategy ) , and not necessarily to pursue their own objectives .",
    "another serious issue is that the multi - agent algorithms introduced in many of these recent papers are not scalable since each agent needs to maintain estimates of its q - factors for each state / joint action pair and compute an equilibrium at each step of the algorithm using the updated estimates , assuming that the actions and objectives are exchanged between all agents .",
    "standard q - learning , which enables an agent to learn how to play optimally in a single - agent environment , has also been applied to very specific multi agent applications @xcite . here",
    ", each agent runs a standard q - learning algorithm by ignoring the other agents , and hence information exchange between agents and computational burden on each agent are substantially lower than aforementioned multi - agent extensions of q - learning algorithm . also , standard q - learning in a multi - agent environment makes sense from individual bounded rationality point of view . however , no analytical results exist regarding the properties of standard q - learning in a stochastic game setting .",
    "we should also mention several attempts to extend a well - known learning algorithm called fictitious play ( fp ) @xcite to stochastic games @xcite .",
    "the joint action learning algorithm presented in @xcite would be computationally prohibitive quickly as the number of agents / states / actions grow .",
    "the algorithms presented in @xcite are claimed to be convergent to an equilibrium in single - state single - stage common interest games but without a proof .",
    "the extension of fp considered in @xcite requires each agent to calculate a stationary policy at each step in response to the empirical frequencies of the stationary policies calculated and announced by other agents in the past .",
    "the main contribution of @xcite is to show that such fp algorithm is not convergent even in the simplest 2x2x2 stochastic game where there are two states and two agents with two moves for each agent .",
    "the version of fp used in @xcite is applicable only to zero - sum games ( strictly adversarial games ) .",
    "other related work includes @xcite . in @xcite , a multi - agent version of an actor - critic algorithm @xcite is shown to be convergent to generalized equilibria in a weak sense of convergence , whereas in @xcite a policy iteration algorithm is presented without rigorous results for stochastic games . the algorithms given in @xcite",
    "are rational from individual agent perspective , however they require higher level of data storing and processing than standard q - learning .",
    "the paper @xcite uses the policy iteration algorithm given in @xcite in conjunction with certain approximation methods to deal with a large state - space in a specific card - game without rigorous results .",
    "we should emphasize that our viewpoint is individual bounded rationality and strategic decision making , that is , agents should act to pursue their own objectives even in the short term using localized information and reasonable algorithms .",
    "it is also desired that agent strategies converge to an agreeable solution in cooperative situations where agent objectives are aligned with system designer s objective even though agents do not necessarily strive for converging to a particular strategy .",
    "the rest of the paper is organized as follows . in  [ se : sg ] , the model is introduced . in   [ se : lph ] , the specifics of the learning paradigm and the standard q - learning algorithm is discussed , followed by the presentation of our first q - learning algorithm for stochastic games and its convergence properties .",
    "generalizations of our main results in ",
    "[ se : lph ] are presented in ",
    "[ se : ext ] .",
    "this is followed by a simulation study in  [ simulationsection ] .",
    "the paper is concluded with some final remarks in  [ conclusionsection ] .",
    "appendices contain the proofs of the technical results in the paper .",
    "consider the ( discrete - time ) networked control system illustrated in figure  [ fig1 ] where @xmath0 is the state of the system at time @xmath1 , @xmath2 is the input generated by controller @xmath3 at time @xmath1 , and @xmath4 is the random disturbance input at time @xmath1 .",
    "suppose that each controller @xmath3 is an autonomous decision maker ( dm ) interested in minimizing its own long - term cost @xmath5\\ ] ] where @xmath6 is the cost incurred by controller @xmath3 at time @xmath1 , and @xmath7 $ ] denotes the expectation given a collection of control policies ( which will be specified later in the paper ) on a probability space @xmath8 .",
    "although controller @xmath3 can only choose its own decisions @xmath9 , its cost generally depends on the decisions of all controllers through its single - stage cost as well as the state dynamics .",
    "this dynamic coupling between self - interested dms with long - term objectives naturally lead to the framework of stochastic games @xcite which generalize markov decision problems .    over the past half - century , there have been many applications of stochastic games on control problems ; see chapter  xiv in @xcite as an early reference . at the present time",
    ", the control theory literature includes a large number of papers employing the theory of stochastic games and their continuous - time counterparts called `` differential games '' @xcite .",
    "many papers in this body of work study a zero - sum game between a controller which aims to optimize the system performance and an adversary which controls certain system parameters and inputs to make the system performance as poor as possible .",
    "we selectively cite @xcite for robust control and minimax estimation problems , @xcite for flow control in queueing networks , @xcite for control of hybrid systems , and @xcite for robustness , security , and resilience of cyber - physical control systems .",
    "the case of nonzero - sum games in which the decision makers do not always have diametrically opposed objectives has also received significant attention ; see for example @xcite on admission , service , and routing control in queueing systems , @xcite on transmission control in cognitive radio systems , @xcite on network security , and @xcite on formation control .",
    "we should also mention the work on team decision problems where all dms share a common long - term objective albeit with access to different information variables ; see e.g. , @xcite . in this paper , differently from the usual team decision problems in the literature , even though each dm has access to the state information , it does not have access to global information on the other dms , and even their presence .",
    "we also note that the emergence of distributed control systems requires the formulation of `` team problems '' within a game - theoretic framework where local controllers are tasked to achieve one system level objective without centralized coordination ; see for example @xcite on distributed model predictive control .",
    "this type of team problems and its generalizations where the objectives of dms are aligned in some sense with a team objective are the primary focus of our work though the class of games considered in this paper is more general and it even includes some zero - sum stochastic games .",
    "a ( finite ) discounted stochastic game has the following ingredients ; see @xcite .    *",
    "a finite set of dms with the @xmath10th dm referred to as dm@xmath11 for @xmath12 * a finite set @xmath13 of states * a finite set @xmath14 of control decisions for each dm@xmath11 * a cost function @xmath15 for each dm@xmath11 determining dm@xmath11 s cost @xmath16 at each state @xmath17 and for each joint decision @xmath18 * a discount factor @xmath19 for each dm@xmath11 * a random initial state @xmath20 * a transition kernel for the probability @xmath21 $ ] of each state transition from @xmath17 to @xmath22 for each joint decision @xmath23    such a stochastic game induces a discrete - time controlled markov process where the state at time @xmath1 is denoted by @xmath24 starting with the initial state @xmath25 . at any time @xmath26",
    ", each dm@xmath11 makes a control decision @xmath27 ( possibly randomly ) based on the available information .",
    "the state @xmath0 and the joint decisions @xmath28 together determine each dm@xmath11 s cost @xmath29 at time @xmath1 as well as the probability distribution @xmath30 $ ] with which the next state @xmath31 is selected .",
    "a policy for a dm is a rule of choosing an appropriate control decision at any time based on the dm s history of observations .",
    "we will focus on stationary policies of the form where a dm s decision at time @xmath1 is determined solely based on the state @xmath0 .",
    "such policies for each dm@xmath11 are identified by mappings from the state space @xmath13 to the set @xmath32 of probability distributions on @xmath14 .",
    "the interpretation is that a dm@xmath11 using such a policy @xmath33 makes its decision @xmath2 at any time @xmath1 by choosing randomly from @xmath14 according to @xmath34 .",
    "we will denote the set of such policies by @xmath35 for each dm@xmath11 .",
    "we will primarily be interested in deterministic ( stationary ) policies denoted by @xmath36 for each dm@xmath11 , where each policy @xmath37 is identified by a mapping from @xmath13 to @xmath14 .",
    "the objective of each dm@xmath11 is to find a policy @xmath38 that minimizes its expected discounted cost @xmath39 \\label{eq : dc}\\ ] ] for all @xmath17 , where @xmath40 denotes the conditional expectation given @xmath41 . since dms",
    "have possibly different cost functions and each dm s cost may depend on the control decisions of the other dms , we adopt the notion of equilibrium to represent those policies that are _ person - by - person optimal_. for ease of notation , we denote the policies of all dms other than dm@xmath11 by @xmath42 . for future reference",
    ", we also define @xmath43 and @xmath44 as well as @xmath45 and @xmath46 . using this notation , we write a joint policy @xmath47 as @xmath48 and @xmath49 as @xmath50 .",
    "a joint policy @xmath51 constitutes an ( markov perfect ) equilibrium if , for all @xmath3 , @xmath52 , @xmath53    it is known that any finite discounted stochastic game possesses an equilibrium policy as defined above @xcite .",
    "although the minimum above can always be achieved by a deterministic policy in @xmath36 ( since each dm@xmath11 s problem is a stationary markov decision problem when the policies of the other dms are fixed at @xmath54 ) , a deterministic equilibrium policy may not exist in general .",
    "however , many interesting classes of games do possess equilibrium in deterministic policies .",
    "in particular , large classes of games arising from applications where all dms benefit from cooperation possess equilibrium in deterministic policies .",
    "the primary examples of such games of cooperation are team problems where all dms have the same cost function . in team problems ,",
    "the deterministic policies minimizing the common cost function are clearly equilibrium policies although non - optimal deterministic equilibrium policies may also exist . a more general set of games of cooperation are those in which some function , called the potential function , decreases whenever a single dm decreases its own cost by unilaterally switching from one deterministic policy to another one . in this class of games ,",
    "the deterministic policies minimizing the potential function are equilibrium policies . as such , we are primarily interested in the set of deterministic equilibrium policies denoted by @xmath55 , where @xmath56 .",
    "we next formally introduce the set of games considered in this paper .",
    "let @xmath57 denote dm@xmath11 s set of ( deterministic ) best replies to any @xmath58 , i.e. , @xmath59 dm@xmath11 s best replies to any @xmath58 can be characterized by its optimal q - factors @xmath60 satisfying the fixed point equation @xmath61\\min_{v^i\\in\\mathbb{u}^i } q_{\\pi^{-i}}^i(x^{\\prime},v^i ) \\big ] \\label{eq : qfp}\\end{aligned}\\ ] ] for all @xmath62 , where @xmath63 denotes the expectation with respect to the joint distribution of @xmath64 given by @xmath65 .",
    "the optimal q - factor @xmath66 represents dm@xmath11 s expected discounted cost to go from the initial state @xmath52 assuming that dm@xmath11 initially chooses @xmath67 and uses an optimal policy thereafter while the other dms use @xmath42 .",
    "one can then write @xmath68 as @xmath69 the set of ( deterministic ) joint best replies is denoted by @xmath70",
    ". any best reply @xmath71 of dm@xmath11 is called a _ strict best reply _ with respect to @xmath72 if @xmath73 such a strict best reply @xmath74 achieves dm@xmath11 s minimum cost given @xmath42 for all initial states and results in a strict improvement over @xmath75 for at least one initial state .",
    "we call a ( possibly finite ) sequence of deterministic joint policies @xmath76 a _ strict best reply path _ if , for each @xmath77 , @xmath78 and @xmath79 differ in exactly one dm position , say dm@xmath11 , and @xmath80 is a strict best reply with respect to @xmath81 .",
    "[ def : best ] a discounted stochastic game is called _ weakly acyclic _ under strict best replies if there is a strict best reply path starting from each deterministic joint policy and ending at a deterministic equilibrium policy .        figure  [ fig3 ] shows the strict best reply graph of a game where the nodes represent the deterministic joint policies and the directed edges represent the single - dm strict best replies .",
    "each deterministic equilibrium policy is represented by a sink , i.e. , a node with no outgoing edges , in such a graph .",
    "note that the game illustrated in figure  [ fig3 ] is weakly acyclic under strict best replies since there is a path from every node to a sink ( @xmath82 or @xmath83 ) .",
    "note also that a weakly acyclic game may have cycles in its strict best reply graph , for example , @xmath84 in figure  [ fig3 ] .",
    "weakly acyclic games constitute a fairly large class of games . in the case of single - stage games , all potential games as well as dominance solvable games are examples of weakly acyclic games ; see @xcite .",
    "we note that the concept of weak acyclicity introduced in this paper is with respect to the stationary markov policies for stochastic games , and constitutes a generalization of weak acyclicity introduced in @xcite for single - stage games .",
    "the primary examples of weakly acyclic games in the case of stochastic games are the team problems with finite state and control sets where dms have identical cost functions and discount factors .",
    "clearly , many other classes of stochastic games are weakly acyclic , e.g. , appropriate multi - stage generalizations of potential games and dominance solvable games restricted to the stationary markov policies are weakly acyclic for the same reason that the single - stage versions of these games are weakly acyclic @xcite .",
    "consider a policy adjustment process in which only one dm updates its policy at each step by switching to one of its strict best replies .",
    "such a process would terminate at an equilibrium policy if the game has no cycles in its strict best reply graph and the process continues until no dm has strict best replies .",
    "a weakly acyclic game may contain cycles in its strict best reply graph but there must be some edges leaving each cycle because otherwise there would not be a path from each node to a sink . therefore ,",
    "as long as each updating dm considers each of its strict best replies with positive probability , the adjustment process would terminate at an equilibrium policy in a weakly acyclic game with probability ( w.p . )",
    "this adjustment process would require a criterion to determine the updating dm at each step and the dms would have to a priori agree to this criterion .",
    "an equilibrium policy can be reached through a similar adjustment process without a pre - game agreement on the selection of the updating dm , if all dms update their policies at each step but with some inertia .",
    "consider now the following policy adjustment process , which is the best reply process with memory length of one and inertia introduced in sections 6.4 - 6.5 of @xcite .",
    "_ best reply process with inertia ( for dm@xmath11 ) : _    set parameters + @xmath85 : inertia + initialize @xmath86 ( arbitrary ) + iterate @xmath87 + if @xmath88 + @xmath89 + else + @xmath90 + end + on the one hand , if the joint policy @xmath91 is an equilibrium policy at any step @xmath77 , then the policies will never change in the subsequent steps . on the other hand , regardless of what the joint policy @xmath91 is at any step @xmath77 , the joint policy @xmath92 in @xmath93 steps later will be an equilibrium policy with positive probability @xmath94 where @xmath93 is the maximum length of the shortest strict best reply path from any policy to an equilibrium policy and @xmath95 depends only on the inertias @xmath96 , and @xmath93 .",
    "this readily implies that the best reply process with inertia will reach an equilibrium policy in finite number of steps w.p .",
    "@xmath97 @xcite , i.e. , @xmath98=1.\\ ] ]    we now note that each updating dm@xmath11 at step @xmath77 needs to compute its best replies @xmath99 , which can be done by first solving the fixed point equation ( [ eq : qfp ] ) for @xmath100 .",
    "dm@xmath11 can solve ( [ eq : qfp ] ) , for example through value iterations , provided that dm@xmath11 knows the state transition probabilities @xmath101 and the policies @xmath102 of the other dms to evaluate the expectations in ( [ eq : qfp ] ) . in most realistic situations",
    ", dms would not have access to such information and therefore would not be able to compute their best replies directly . in the next section ,",
    "we introduce our learning paradigm in which dms would be able to learn their near best replies with minimal information and adjust their policies ( approximately ) along the strict best reply paths as in the best reply process with inertia .",
    "the learning setup involves specifying the information that dms have access to .",
    "we assume that each dm@xmath11 knows its own set @xmath14 of decisions and its own discount factor @xmath103 .",
    "in addition , before choosing its decision @xmath2 at any time @xmath1 , each dm@xmath11 has the knowledge of    * its own past decisions @xmath104 , and * past and current state realizations @xmath105 , and * its own past cost realizations @xmath106    each dm@xmath11 has access to no other information such as the state transition probabilities or any information regarding the other dms ( not even the existence of the other dms ) . in effect",
    ", the problem of decision making from the perspective of each dm@xmath11 appears to be a stationary markov decision problem .",
    "it is reasonable that each dm@xmath11 with this view of its environment would use the standard q - learning algorithm @xcite to learn its optimal q - factors and its optimal decisions .",
    "this would lead to the following q - learning dynamics for each dm@xmath11 : @xmath107\\end{aligned}\\ ] ] where @xmath108 $ ] denotes dm@xmath11 s step size at time @xmath1 .",
    "if only one dm , say dm@xmath11 , were to use q - learning and the other dms used constant policies @xmath42 , then dm@xmath11 would asymptotically learn its corresponding optimal q - factors , i.e. , @xmath109=1\\ ] ] provided that all state - control pairs @xmath62 are visited infinitely often and the step sizes are reduced at a proper rate .",
    "this follows from the well - known convergence of q - learning in a stationary environment ; see @xcite . to exploit the learnt q - factors",
    "while maintaining exploration , the actual decisions are often selected with very high probability as @xmath110 and with some small probability any decision in @xmath14 is experimented .",
    "one common way of achieving this for dm@xmath11 is to select any decision @xmath111 randomly according to ( boltzman action selection ) @xmath112=",
    "\\frac{e^{- q_t^i(x_t , u^i)/\\tau}}{\\sum_{v^i\\in\\mathbb{u}^i } e^{-q_t^i(x_t , v^i)/\\tau}}\\ ] ] where @xmath113 is a small constant called the temperature parameter , and @xmath114 is the history of the random events realized up to the point just before the selection of @xmath115 .",
    "however , when all dms use q - learning and select their decisions as described above , the environment is non - stationary for all dms , and there is no reason to expect convergence in that case .",
    "in fact , one can construct examples where dms using q - learning are caught up in persistent oscillations ; see section  4 in @xcite for the non - convergence of q - learning in shapley s game .",
    "however , the convergence of q - learning may still be possible in team problems , coordination - type games , or more generally in weakly - acyclic games .",
    "it is instructive to first consider the repeated games .    here",
    ", there is no state dynamics ( the set @xmath13 of states is a singleton ) and the dms have no look - ahead ( @xmath116 ) .",
    "the only dynamics in this case is due to q - learning which reduces to the averaging dynamics @xmath117 \\label{eq : qlrosg1}\\end{aligned}\\ ] ] where @xmath112= \\frac{e^{- q_t^i(u^i)/\\tau}}{\\sum_{v^i\\in\\mathbb{u}^i } e^{-q_t^i(v^i)/\\tau}}. \\label{eq : qlrosg3}\\ ] ] the long - term behavior of these averaging dynamics is analyzed in @xcite and strongly connected to the long - term behavior of the well - known stochastic fictitious play ( sfp ) dynamics @xcite in the case of two dms ; see lemma  4.1 in @xcite . in two - dm sfp , each dm@xmath11 tracks the empirical frequencies of the past decisions of its opponent dm@xmath118 and chooses a nearly optimal response ( with some experimentation ) based on the incorrect assumption that dm@xmath118 will choose its decisions according to the empirical frequencies of its past decisions @xmath119 where @xmath120 is the indicator function and @xmath121   = & \\frac{e^{- m_t^i(u^i)/\\tau}}{\\sum_{v^i\\in\\mathbb{u}^i } e^{-m_t^i(v^i)/\\tau } } \\\\",
    "m_t^i(u^i )   : = & \\sum_{u^{-i } } q_{t}^{-i}(u^{-i } ) c^i(u^i , u^{-i}).\\end{aligned}\\ ] ]    using the connection between q - learning dynamics ( [ eq : qlrosg1])-([eq : qlrosg3 ] ) and sfp dynamics , the convergence of q - learning ( [ eq : qlrosg1])-([eq : qlrosg3 ] ) is established in zero - sum games as well as in partnership games with two dms ; see proposition  4.2 in @xcite .",
    "it may be possible to extend this convergence result to multi - dm potential games @xcite , but this is currently unresolved .",
    "however , given the nonconvergence of fp ( where dms choose exact optimal responses with no experimentation , i.e. , @xmath122 ) in some coordination games @xcite , the prospect of establishing the convergence of q - learning even in all two - dm weakly acyclic games does not seem promising .",
    "it is possible to employ additional features such as the truncation of the observation history or multi - time - scale learning to obtain learning dynamics that are convergent in all repeated weakly acyclic games ; see our own previous work @xcite and the others @xcite .",
    "however , the question of learning an equilibrium policy in stochastic games is an open question .",
    "the only relevant reference considering the stochastic games is @xcite where each dm uses value learning coupled with policy search at a slower time - scale .",
    "the results in @xcite apply to all stochastic games and therefore they are necessarily quite weak . loosely speaking , the main result in @xcite shows that the limit points of certain empirical measures ( weighted with the step sizes ) in the policy space constitute `` generalized nash equilibria '' , which in particular does not imply convergence of learning to an equilibrium policy . in the next subsection ,",
    "we propose a simple variation of q - learning which converges to an equilibrium policy in all weakly acyclic stochastic games .",
    "the discussion in the previous subsection reveals that the standard q - learning ( [ eq : qlrosg1])-([eq : qlrosg3 ] ) can lead to robust oscillations even in repeated coordination games .",
    "the main obstacle to convergence of q - learning in games is due to the presence of multiple active learners leading to a non - stationary environment for all learners . to overcome this obstacle",
    ", we use some inspiration from our previous work @xcite on repeated games and modify the q - learning for stochastic games as follows . in our variation of q - learning , we allow dms to use constant policies for extended periods of time called _",
    "exploration phases_.    th exploration phase . ]",
    "as illustrated in figure  [ fig5 ] , the @xmath123th exploration phase runs through times @xmath124 , where @xmath125 for some integer @xmath126 denoting the length of the @xmath123th exploration phase . during the @xmath123th exploration phase , dms use some constant policies @xmath127 as their baseline policies with occasional experimentation",
    "the essence of the main idea is to create a stationary environment over each exploration phase so that dms can accurately learn their optimal q - factors corresponding to the constant policies used during each exploration phase . before arguing why this would lead to an equilibrium policy in all weakly acyclic stochastic games ,",
    "let us introduce our variation of q - learning more precisely .",
    "[ al:1 ]    set parameters + @xmath128 : some compact subset of the euclidian space @xmath129 + where @xmath130 is the number of pairs @xmath131 + @xmath132 : sequence of integers in @xmath133 + @xmath134 : experimentation probability + @xmath85 : inertia + @xmath135 : tolerance level for sub - optimality + @xmath136 : sequence of step sizes where + @xmath137 $ ] , @xmath138 , @xmath139 + ( e.g. , @xmath140 where @xmath141 $ ] )    initialize @xmath86 ( arbitrary ) , @xmath142 ( arbitrary ) + receive @xmath25 + iterate @xmath87 + ( @xmath123th exploration phase ) + iterate @xmath143 + @xmath144 + receive @xmath145 + receive @xmath31 ( selected according to @xmath146 $ ] ) + @xmath147 the number of visits to @xmath148 in the @xmath123th + exploration phase up to @xmath1 + @xmath149 + @xmath150 $ ] + @xmath151 , for all @xmath152 + end + @xmath153 + @xmath154 + if @xmath155 + @xmath156 + else + @xmath157 + end + reset @xmath158 to any @xmath159 ( e.g. , project @xmath158 onto @xmath128 ) + end    algorithm  [ al:1 ] mimics the best reply process with inertia in  [ ss : pap ] arbitrarily closely with arbitrarily high probability under certain conditions .",
    "the key difference here is that each dm using algorithm  [ al:1 ] approximately learns its optimal q - factors during each exploration phase with limited observations .",
    "accordingly , each dm updates its ( baseline ) policy to one of its near best replies with inertia based on its learnt q - factors .",
    "hence , algorithm  [ al:1 ] can be regarded as an approximation to the best reply process with inertia in  [ ss : pap ] ; see @xcite where best replies are obtained based on rewards that must be estimated using noisy observations .    [ as : alpha ] for all @xmath160 , there exists a finite integer @xmath161 and joint decisions @xmath162 such that @xmath163>0.\\ ] ]    assumption  [ as : alpha ] ensures that the step sizes satisfy the well - known conditions of the stochastic approximation theory @xcite during each exploration phase .",
    "[ as : qp ] for all @xmath3 , @xmath164 and @xmath165 , where @xmath166 and @xmath167 ( which depend only on the parameters of the game at hand ) are defined in appendix  [ qsection ] .    assumption  [ as : qp ] requires that the tolerance levels for sub - optimality used in the computation of near best replies as well as the experimentation probabilities be nonzero but sufficiently small .",
    "[ normalq ] consider a discounted stochastic game that is weakly acyclic under strict best replies .",
    "suppose that each dm@xmath11 updates its policies by algorithm  [ al:1 ] .",
    "let assumption  [ as : alpha ]  and  [ as : qp ] hold .    1 .   for any @xmath168 , there exist @xmath169 , @xmath170 such that if @xmath171 , then @xmath172 \\geq 1-\\epsilon , \\qquad \\mbox{for all } \\ k\\geq\\tilde{k}.\\ ] ] 2 .",
    "if @xmath173 , then @xmath172 \\rightarrow 1.\\ ] ] 3 .",
    "there exists finite integers @xmath174 such that if @xmath175 , for all @xmath77 , then @xmath176 = 1.\\ ] ]    see appendix  [ qsection ] .",
    "let us discuss the main idea behind this result .",
    "since all dms use constant policies throughout any particular exploration phase , each dm indeed faces a stationary markov decision problem in each exploration phase .",
    "therefore , if the length of each exploration phase is long enough and the experimentation probabilities @xmath177 are small enough ( but non - zero ) , each dm@xmath11 can learn its corresponding optimal q - factors in each exploration phase with arbitrary accuracy with arbitrarily high probability .",
    "this allows each dm@xmath11 to accurately compute its near best replies to the other dms policies @xmath102 at the end of the @xmath123th exploration phase . intuitively , allowing each dm@xmath11 to update its policy @xmath178 to its near best replies ( to @xmath102 ) at the end of the @xmath123th exploration phase with some inertia",
    "@xmath85 results in a policy adjustment process that approximates the best reply process with inertia in  [ ss : pap ] .",
    "one may also wish to find explicit lower - bounds on @xmath179 to achieve almost sure convergence based on the convergence rates of the standard q - learning with a single dm ; we refer the reader to @xcite for bounds on the convergence rates for standard q - learning .",
    "we present another q - learning algorithm with provable convergence to equilibrium in discounted stochastic games that are weakly acyclic under strict better replies . for this , we first introduce the notion of weak acyclicity under strict better replies . given any @xmath180 , let @xmath181 denote dm@xmath11 s set of ( deterministic ) better replies with respect to @xmath182 , i.e. , @xmath183 any better reply @xmath184 of dm@xmath11 is called a _ strict better reply _ ( with respect to @xmath182 ) if @xmath185    we call a ( possibly finite ) sequence of deterministic joint policies @xmath76 a _ strict better reply path _ if , for each @xmath77 , @xmath78 and @xmath79 differ in exactly one dm position , say dm@xmath11 , and @xmath80 is a strict better reply with respect to @xmath81 .    [ def : better ] a discounted stochastic game is called weakly acyclic under strict better replies if there is a strict better reply path starting from each deterministic joint policy and ending at a deterministic equilibrium policy .",
    "since every strict best reply path is also a strict better reply path , the set of games weakly acyclic under better replies contain ( in fact , strictly ) the set of games weakly acyclic under best replies .",
    "it is straightforward to introduce a policy adjustment process analogous to the one in  [ ss : pap ] where , at each step , each dm@xmath11 switches to one of its strict better replies with some inertia ; see sections 6.4 - 6.5 in @xcite .",
    "such a process would clearly converge to an equilibrium in games that are weakly acyclic under strict better replies .",
    "we next introduce a learning algorithm which allows each dm to learn the q - factors corresponding to two policies , a baseline policy and a randomly selected experimental policy , during each exploration phase .",
    "if the learnt q - factors indicate that the experimental policy is better than the baseline policy within a certain tolerance level , then the baseline policy is updated to the experimental policy with some inertia at the end of each exploration phase .",
    "this learning algorithm enables dms to adjust their policies with much less information ( as in  [ se : lp ] ) , and follow ( approximately ) along the strict better reply paths that the adjustment process follows .",
    "[ al:2 ]    set parameters as in algorithm  [ al:1 ] + initialize @xmath186 ( arbitrary except @xmath187 ) , @xmath188 ( arbitrary ) + receive @xmath25 + iterate @xmath87 + ( @xmath123th exploration phase ) + iterate @xmath143 + @xmath189 + receive @xmath145 + receive @xmath31 ( selected according to @xmath146 $ ] ) + @xmath147 the number of visits to @xmath148 in the @xmath123th + exploration phase up to @xmath1 + @xmath149 + @xmath190 $ ] + @xmath191 + @xmath192 $ ] + @xmath151 , for all @xmath152 + @xmath193 , for all @xmath152 + end + if @xmath194 + and +  @xmath195 + @xmath196 + else + @xmath156 + end + @xmath197 any policy @xmath198 with equal + probability + reset @xmath158 , @xmath199 to any @xmath200 + end    since any policy except the baseline policy can be chosen as an experimental policy ( with equal probability ) , each dm can switch to any of its strict better replies with positive probability .",
    "in contrast , each dm using algorithm  [ al:1 ] can only switch to one of its strict best replies . as a result",
    ", each dm using algorithm  [ al:2 ] can escape a strict best reply cycle by switching to a strict better reply ( if one exists ) ; whereas , any dm using algorithm  [ al:1 ] can not .",
    "this flexibility comes at the cost of running two q - learning recursions , one for the baseline policy and the other for the experimental policy , instead of one .",
    "however , this flexibility also leads to convergent behavior in a strictly larger set of games .",
    "we cite @xcite as a reference to an earlier use of the idea of comparing two strategies and selecting one according to the boltzman distribution .",
    "the counterpart of theorem  [ normalq ] can be obtained for algorithm  [ al:2 ] in games that are weakly acyclic under strict better replies .",
    "[ as : qp2 ] for all @xmath3 , @xmath201 and @xmath202 , where @xmath203 and @xmath204 ( which depend only on the parameters of the game at hand ) are defined in appendix  [ qsection2 ] .",
    "[ normalq2 ] consider a discounted stochastic game that is weakly acyclic under strict better replies .",
    "suppose that each dm@xmath11 updates its policies by algorithm  [ al:2 ] .",
    "let assumption  [ as : alpha ]  and  [ as : qp2 ] hold .    1 .   for any @xmath168 , there exist @xmath169 , @xmath170 such that if @xmath171 , then @xmath172 \\geq 1-\\epsilon , \\qquad   k\\geq\\tilde{k}.\\ ] ] 2 .",
    "if @xmath173 , then @xmath172 \\rightarrow   1.\\ ] ] 3 .",
    "there exists finite integers @xmath174 such that if @xmath175 , for all @xmath77 , then @xmath205 = 1.\\ ] ]    see appendix  [ qsection2 ] .",
    "the notion of weak acyclicity can be generalized by allowing multiple dms to simultaneously update their policies in a strict best or better reply path .",
    "we call a ( possibly finite ) sequence of deterministic joint policies @xmath76 a _ multi - dm strict best ( better ) reply path _",
    "if , for each @xmath77 , @xmath78 and @xmath79 differ for at least one dm and , for each deviating dm@xmath11 , @xmath80 is a strict best ( better ) reply with respect to @xmath81 .",
    "a discounted stochastic game is called weakly acyclic under multi - dm strict best ( better ) replies if there is a multi - dm strict best ( better ) reply path starting from each deterministic joint policy and ending at a deterministic equilibrium policy .",
    "this generalization leads to a strictly larger set of games that are weakly acyclic . to see this , consider a single - stage game characterized by the cost matrices in figure  [ fig : ewa ] where dm@xmath206 chooses a row , dm@xmath207 chooses a column , and dm@xmath208 chooses a matrix , simultaneously .",
    "33[@xmath97 ] & @xmath97 & @xmath210 & @xmath211 + @xmath97 & @xmath212 & @xmath213 & @xmath214 + @xmath210 & @xmath215 & @xmath216 & @xmath215 + @xmath211 & @xmath214 & @xmath213 & @xmath217    33[@xmath210 ] & @xmath97 & @xmath210 & @xmath211 + @xmath97 & @xmath214 & @xmath219 & @xmath219 + @xmath210 & @xmath215 & @xmath217 & @xmath220 + @xmath211 & @xmath216 & @xmath219 & @xmath219    assume @xmath221 .",
    "there is no strict best ( or better ) reply path to an equilibrium from the joint decisions @xmath222 , @xmath223 , @xmath224 , @xmath225 , @xmath226 , @xmath227 , if only a single dm can update its decision at a time . therefore , this game is not weakly acyclic under strict best ( or better ) replies in the sense of definition  [ def : best ] ( or definition  [ def : better ] ) .",
    "however , if multiple dms are allowed to switch to their strict best ( or better ) replies simultaneously , then it becomes possible to reach the equilibrium @xmath228 from any joint decision .",
    "for example , if dm@xmath207 and dm@xmath208 switch to their strict best ( or better ) replies simultaneously from the joint decision @xmath222 , then the resulting joint decision would be @xmath229 .",
    "this would subsequently lead to the equilibrium @xmath228 if dm@xmath206 switches to its strict best ( or better ) reply from @xmath229 .",
    "all learning algorithms introduced in the paper allow multiple dms to simultaneously update their policies with positive probability .",
    "in view of this , it is straightforward to see that our main convergence results theorem  [ normalq ] ( theorem  [ normalq2 ] ) hold in games that are weakly acyclic under multi - dm strict best ( better ) replies .",
    "we consider a discounted stochastic game with two dms where @xmath230 .",
    "each dm@xmath11 s utility ( to be maximized ) at each time @xmath26 depends only on the joint decisions @xmath231 of both dms as    32[dm@xmath11:][dm@xmath118 : ] & 1 & 2 + 1 & @xmath232 & @xmath233 + 2 & @xmath234 & @xmath235    we assume @xmath236 .",
    "the state evolves as @xmath237   & = 1-\\gamma \\\\ p\\big [ x_{t+1}= 2 \\ |   \\ ( u_t^1,u_t^2)\\not=(1,1)\\big ] & = 1-\\gamma\\end{aligned}\\ ] ] where @xmath238 and @xmath239=1/2 $ ] .",
    "the single - stage game corresponds to the well - known prisoner s dilemma where the @xmath10th prisoner ( dm@xmath11 ) cooperates ( defects ) at time @xmath1 by choosing @xmath240 ( @xmath241 ) .",
    "the single - stage game has a unique equilibrium @xmath242 , i.e. , both dms defect , leading to rewards @xmath243 .",
    "the dilemma is that each dm can do strictly better by cooperating , i.e. , @xmath244 ( not an equilibrium ) .    in the multi - stage game , the state @xmath0 indicates , w.p .",
    "@xmath245 , whether or not both dms cooperated in the previous stage .",
    "it turns out that cooperation can be obtained as an equilibrium of the multi - stage game if the dms are patient , i.e. , the discount factors are sufficiently high , and the error probability @xmath246 is sufficiently small .",
    "note that each dm@xmath11 has four different policies of the form @xmath247 .",
    "for large enough @xmath248 , and small enough @xmath246 , the multi - stage game has two ( markov perfect ) equilibria . in one equilibrium",
    ", called the cooperation equilibrium , each dm cooperates if @xmath249 and defects otherwise . in the other equilibrium , called the defection equilibrium , both dms always defect . furthermore , from any joint policy in @xmath250 , there is a strict best reply path to one of these two equilibria , which implies that the multi - stage game is weakly acyclic under strict best replies .",
    "we set @xmath251 , @xmath252 , @xmath253 , @xmath254 .",
    "we simulate algorithm  [ al:1 ] with the following parameter values : @xmath255 , @xmath256 , @xmath257 , @xmath258 , for all @xmath259 .",
    "we keep the lengths of the exploration phases constants , i.e , @xmath260 , for all @xmath77 .",
    "we consider different values for @xmath261 since the lengths of the exploration phases appear to be most critical for the behavior of the learning process .",
    "for each value of @xmath261 , we run algorithm  [ al:1 ] and the best reply process with inertia ( in  [ ss : pap ] ) in parallel , with @xmath262 policy updates starting from each of the @xmath263 initial joint policies in @xmath264 .",
    "we initialize all the learnt q - factors at @xmath235 for each simulation run ; however , we do not reset the learnt q - factors at the end of any exploration phase during any simulation run .",
    "we let @xmath78 and @xmath265 denote the policies generated by algorithm  [ al:1 ] and the best reply process with inertia in  [ ss : pap ] , respectively . for each value of @xmath261 , table  [ tb:1 ]",
    "shows the fraction of times at which @xmath78 visits an equilibrium and the fraction of times at which @xmath78 agrees with @xmath265 , during the @xmath262 policy updates ( averaged uniformly over all @xmath263 initial policies in @xmath264 ) .",
    "the results in table  [ tb:1 ] reveals that , as @xmath261 increases , @xmath78 visits an equilibrium and agrees with @xmath265 more often .",
    "this is consistent with theorem  [ normalq ] since dms are expected to learn their q - factors more accurately with higher probability for larger values of @xmath261 .",
    "when @xmath261 is sufficiently large , the polices @xmath78 are at equilibrium and agrees with @xmath265 nearly all of the time regardless of the initial policy . in a typical simulation run ( with a large enough @xmath261 ) , the polices @xmath78 and @xmath265 transition to an equilibrium in few steps and stay at equilibrium thereafter .     | c | c | c | @xmath261 &    .the fraction of times at which @xmath78 visits an equilibrium and + the fraction of times at which @xmath78 agrees with @xmath265 . [ cols=\"^ \" , ]      + @xmath266 & @xmath267 & @xmath268 + @xmath269 & @xmath270 & @xmath271 + @xmath272 & @xmath273 & @xmath274 + @xmath275 & @xmath276 & @xmath277 + @xmath262 & @xmath278 & @xmath279 + @xmath280 & @xmath281 & @xmath282 + @xmath283 & @xmath284 & @xmath285 +    [ tb:1 ]",
    "in this paper , we develop decentralized q - learning algorithms and present their convergence properties for stochastic games under weak acyclicity .",
    "this is the first paper , to our knowledge , that presents learning algorithms with convergence to equilibria in large classes of stochastic games .",
    "the decision makers observe only their own decisions and cost realizations , and the state transitions ; they need not even know the presence of the other decision makers .",
    "our approach has a two - time scale flavor ; however , unlike the existing work on multi - time - scale learning , it does not depend on the stochastic approximation theory .",
    "note that the existing work on multi - time - scale learning , e.g. , @xcite , require the stability analysis of some ordinary differential equations ( ode ) describing the mean behavior of the learning algorithms . aside from the difficulty of choosing the step sizes running at multiple time scales , the existing work involves nonlinear odes",
    "whose analysis does not seem to be within reach even for dynamic team problems .",
    "in contrast , our approach leads to a considerably simpler analysis for all weakly acyclic stochastic games .",
    "convergence of the standard q - learning algorithm with a single dm is well known @xcite . however , to prove the results of this paper , we need the sample paths generated by the standard q - learning algorithm to well behave with respect to the initial conditions .",
    "let us now consider a single - dm version of the setup introduced in ",
    "[ se : sg ] where the dm index @xmath3 ( in the superscript ) is dropped ( only in appendix  [ se : ap3 ] ) and @xmath286 representing the one - stage cost for applying control @xmath287 at @xmath52 is an exogenous random variable with finite variance .",
    "let us assume that a single dm using a stationary random policy @xmath288 updates its q - factors as : for @xmath26 , @xmath289 where the initial condition @xmath290 is given , @xmath291 is chosen according to @xmath292 , the state @xmath0 evolves according to @xmath293 $ ] starting at @xmath25 , @xmath294 is the number of visits to @xmath295 up to time @xmath1 , and @xmath296 is a sequence of step sizes satisfying @xmath297 , \\quad \\sum_{n } \\alpha_n = \\infty , \\quad   \\quad \\sum_{n } \\alpha_n^2 < \\infty.\\ ] ]    [ lm : ql ] assume that each @xmath298 is visited infinitely often w.p .",
    "@xmath97 . for any @xmath168 and compact @xmath299",
    ", there exists @xmath300 such that , for any @xmath301 , @xmath302 \\geq 1-\\epsilon\\ ] ] where @xmath303 denotes the maximum norm and @xmath304 is the unique fixed point of the mapping @xmath305 defined by @xmath306+\\beta\\sum_{x^{\\prime } } p[x^{\\prime}|x , u ] \\min_v q(x^{\\prime},v)\\ ] ] for all @xmath52 , @xmath287 .",
    "let @xmath307 and @xmath308 be the trajectories for the initial conditions @xmath309 and @xmath310 , respectively , corresponding to a sample path @xmath311 .",
    "it is easy to see that , for all @xmath26 , @xmath312 this implies that @xmath313 is non - increasing and therefore convergent .",
    "suppose that @xmath314 .",
    "there exists some @xmath315 such that @xmath316 .",
    "hence , we have , for all @xmath317 , @xmath318 this leads to : for all @xmath298 and @xmath317 , @xmath319 \\beta \\bar{m}(1 + 1/\\beta)/2      \\end{aligned}\\ ] ] where @xmath320 is the number of visits to @xmath298 in @xmath321 $ ] .",
    "since each @xmath298 is visited infinitely often w.p . @xmath97 and @xmath322",
    ", we have , for each @xmath298 , @xmath323 as @xmath324 w.p . @xmath97 .",
    "this implies that @xmath325 w.p .",
    "@xmath97 , which is a contradiction . therefore , @xmath326 , w.p . @xmath97 .",
    "theorem  4 in @xcite shows that , for any initial condition @xmath290 , @xmath327 , w.p .",
    "hence , for any @xmath328 , we have @xmath329 , w.p .",
    "therefore , @xmath330 , w.p .",
    "this leads to the desired result , i.e. , for any @xmath168 and compact @xmath299 , there exists @xmath300 such that @xmath331 \\geq 1-\\epsilon.\\ ] ]    [ rm : uc ] the q - factors corresponding to a certain deterministic policy @xmath332 can be learnt by modifying the recursion ( [ eq : sq1])-([eq : sq2 ] ) as follows : for @xmath26 , @xmath333 where the initial condition @xmath334 is given and @xmath291 is chosen according to @xmath292 .",
    "hence , the uniform convergence result in lemma 1 also holds for the this recursion .",
    "for any @xmath58 , let @xmath335 denote the self - mapping of @xmath336 defined by @xmath337 \\min_{v^i } q^i(x^{\\prime},v^i ) \\big]\\end{aligned}\\ ] ] for all @xmath62 .",
    "it is well - known that @xmath335 is a contraction mapping with the lipschitz constant @xmath103 with respect to the maximum norm .",
    "recall from ( [ eq : qfp ] ) that each dm@xmath11 s optimal q - factors @xmath60 is the unique fixed point of @xmath335 .",
    "we also note that , during the @xmath123th exploration phase , each dm@xmath11 actually uses the random policy @xmath338 defined as @xmath339 where @xmath340 is the random policy that assigns the uniform distribution on @xmath341 to each @xmath52 .",
    "[ lm : qappx ] for any @xmath168 , there exists @xmath342 such that , if @xmath343 , then @xmath344 \\geq 1-\\epsilon.\\ ] ]    note that the @xmath123th exploration phase starts with @xmath345 , which belongs to the finite state space @xmath13 , and @xmath346 , where @xmath128 is compact , for all @xmath3 .",
    "note also that , during each exploration phase , dms use stationary random policies of the form ( [ eq : pibar ] ) and there are finitely many such joint policies .",
    "hence , the desired result follows from lemma  [ lm : ql ] in appendix  [ se : ap3 ] .",
    "[ lm : qexp ] for any @xmath168 , there exists @xmath347 such that , if @xmath348 , for all @xmath3 , then @xmath349    we have @xmath350 where @xmath351 is some convex combination of the policies in @xmath352 of the form where each dm@xmath353 , @xmath354 , either uses its baseline policy @xmath355 or the uniform distribution where @xmath356 and @xmath357 is a policy such that @xmath358 for @xmath359 and @xmath360 for @xmath361 . ] . because @xmath362 belongs to a finite subset of @xmath363 , an upper bound @xmath364 on @xmath365 exists , which is uniform in @xmath362 .",
    "this results in @xmath366 which proves the lemma .",
    "let @xmath166 denote the minimum separation between the entries of dms optimal q - factors ( with respect to the deterministic policies ) , defined as for some @xmath3 , @xmath52 , @xmath367 , @xmath368 , @xmath369 . ]",
    "@xmath370 we consider @xmath166 to be an upper bound on the tolerance levels for sub - optimality , i.e. , @xmath371 , for all @xmath3 . in that case",
    ", we also introduce an upper bound @xmath372 on the experimentation rates such that , if @xmath373 , for all @xmath3 , then @xmath374 such an upper bound @xmath372 exists due to lemma  [ lm : qexp ] .",
    "[ lm : pi ] suppose @xmath371 , @xmath375 , for all @xmath3 . for any @xmath168 , there exist @xmath376 , such that , if @xmath377 , then @xmath378 \\geq 1-\\epsilon\\ ] ] where @xmath379 , @xmath87 , is the random event defined as @xmath380    the desired result follows from lemma  [ lm : qappx ] and ( [ eq : condrho ] ) .      note that @xmath381 therefore , we have @xmath382 = 1 , \\quad\\mbox{for all } \\ k.\\ ] ] since we have a weakly acyclic game at hand , for each @xmath383 , there exists a strict best reply path of minimum length @xmath384 starting at @xmath182 and ending at an equilibrium policy .",
    "let @xmath385 .",
    "there exists @xmath386 ( which depends only on @xmath96 , and @xmath93 ) such that , for all @xmath77 , @xmath387 \\geq p_{\\min}.\\label{eq : ne2e}\\end{aligned}\\ ] ] pick @xmath388 satisfying @xmath389 lemma  [ lm : pi ] implies the existence of @xmath169 such that , if @xmath390 , then @xmath391 \\geq 1-\\tilde{\\epsilon } ,   \\quad\\mbox{for all } \\ k.\\ ] ] for the rest of this part , we assume @xmath392 . from ( [ eq : e2e ] ) , ( [ eq : ne2e ] ) , ( [ eq : pi ] ) , we obtain @xmath393 \\geq p_{\\min}(1-\\tilde{\\epsilon } ) , \\quad\\mbox{for all } \\",
    "k\\ ] ] and @xmath394 \\geq 1-\\tilde{\\epsilon } , \\quad\\mbox{for all } \\ k.\\ ] ] this leads to the recursive inequalities @xmath395 \\label{geometricconv}\\end{aligned}\\ ] ] where @xmath396 $ ] , for all @xmath77 .",
    "note that we have , for all @xmath397 , @xmath398 we rewrite ( [ geometricconv ] ) as @xmath399 \\left[\\frac{(1-\\tilde{\\epsilon } )   p_{\\min}}{\\tilde{\\epsilon}+(1-\\tilde{\\epsilon } )   p_{\\min}}-p_{nl}\\right].\\ ] ] this shows that if @xmath400 we have @xmath401 .",
    "therefore , whenever @xmath402 satisfies ( [ eq : ineqxyzw ] ) , it will increase by at least @xmath403 until it exceeds the right hand side of ( [ eq : ineqxyzw ] ) , which will happen in a finite number of steps .",
    "in fact , @xmath402 would increase as long as @xmath404 . on the other hand , if @xmath405 , @xmath402 can not decrease more than @xmath406 ; recall ( [ eq : lwb ] ) .",
    "therefore , there exists @xmath407 such that , for all @xmath408 , @xmath409 finally , due to ( [ eq : neeqx ] ) , we have , for all @xmath408 , @xmath410 , @xmath411      for any @xmath168 , let @xmath169 , @xmath170 be as in part ( i ) .",
    "let @xmath412 be such that @xmath413 .",
    "it is straightforward to see from the proof of part ( i ) that , for all @xmath414 , we have @xmath415 \\geq 1 -\\epsilon.$ ]      pick a sequence @xmath416 satisfying @xmath417 , for all @xmath397 , and @xmath418 where @xmath95 is as in ( [ eq : ne2e ] ) .",
    "lemma  [ lm : pi ] implies the existence of a sequence @xmath419 of finite integers such that if @xmath420 then @xmath421   \\geq 1-\\tilde{\\epsilon}_n.\\label{eq : xyz}\\end{aligned}\\ ] ] we assume ( [ eq : xyzz ] ) ( therefore ( [ eq : xyz ] ) ) holds for all @xmath397 .",
    "this leads to @xmath422   \\leq   ( 1-p_{\\min } ) p\\left [   \\pi_{nl } \\not\\in \\pi_{\\rm eq }   \\right ]   + \\tilde{\\epsilon}_n.\\end{aligned}\\ ] ] from this , it is straightforward to obtain @xmath423   \\\\ & \\qquad \\leq   ( 1-p_{\\min})^n \\left ( 1   + \\sum_{s=0}^n ( 1-p_{\\min})^{-s } \\tilde{\\epsilon}_s \\right).\\end{aligned}\\ ] ] due to ( [ eq : xyz ] ) , we have , for @xmath424 , @xmath425   \\geq ( 1-\\tilde{\\epsilon}_n ) p\\left [   \\pi_{nl } \\in \\pi_{\\rm eq }   \\right].\\ ] ] therefore , for @xmath424 , @xmath426 \\\\ & \\qquad \\leq ( 1-p_{\\min})^n \\left ( 1   + \\sum_{s=0}^n ( 1-p_{\\min})^{-s } \\tilde{\\epsilon}_s \\right)+\\tilde{\\epsilon}_{n+1}.\\end{aligned}\\ ] ] from this and ( [ eq : condet ] ) , we obtain @xmath427 \\\\ & \\ \\leq l \\sum_{n\\geq0 } \\left[(1-p_{\\min})^n \\left ( 1   + \\sum_{s=0}^n ( 1-p_{\\min})^{-s } \\tilde{\\epsilon}_s \\right)+\\tilde{\\epsilon}_{n+1}\\right ] \\\\ & \\ \\",
    "< \\infty.\\end{aligned}\\ ] ] borel - cantelli lemma implies @xmath428=0.\\end{aligned}\\ ] ] from ( [ eq : condet ] ) and ( [ eq : xyz ] ) , we obtain @xmath429 < \\infty$ ] .",
    "borel - cantelli lemma again implies @xmath430=0.\\end{aligned}\\ ] ] finally , ( [ eq : bc1 ] ) and ( [ eq : bc2 ] ) imply the desired result .",
    "for any @xmath431 , let @xmath432 denote the self - mapping of @xmath336 defined by @xmath433 q^i(x^{\\prime},\\pi^i(x^{\\prime } ) ) \\big]\\end{aligned}\\ ] ] for all @xmath62 .",
    "it is well - known that @xmath432 is a contraction mapping with the lipschitz constant @xmath103 with respect to the maximum norm .",
    "let us denote the unique fixed point of @xmath432 by @xmath434 .",
    "we also note that , during the @xmath123th exploration phase , each dm@xmath11 actually uses the random policy @xmath338 defined as @xmath435 where @xmath340 is the random policy that assigns the uniform distribution on @xmath341 to each @xmath52 .",
    "note that each exploration phase starts with @xmath345 , which belongs to a finite state space , and @xmath438 , where @xmath128 is compact , for all @xmath3 .",
    "note also that , during each exploration phase , dms use stationary random policies of the form ( [ eq : pibar2 ] ) and there are finitely many such joint policies .",
    "hence , the desired result follows from lemma  [ lm : ql ] in appendix  [ se : ap3 ] ; see remark  [ rm : uc ] .",
    "we have @xmath440 where @xmath351 is some convex combination of the joint policies of the form where each dm@xmath353 , @xmath354 , either uses its baseline policy @xmath355 or the uniform distribution ( as in appendix  [ qsection ] ) . because @xmath441 belongs to a finite subset of @xmath442 , an upper bound @xmath443 on @xmath444 exists , which is uniform in @xmath441 .",
    "this results in @xmath445 which leads to the first bound .",
    "the second bound can be obtained similarly .",
    "let @xmath203 denote the minimum separation between the entries of dms q - factors ( for deterministic policies ) , defined as , for some @xmath3 , @xmath52 , @xmath446 , @xmath369 , to avoid trivial cases . ]",
    "@xmath447 we consider @xmath203 to be an upper bound on the tolerance levels for sub - optimality , i.e. , @xmath448 , for all @xmath3 . in that case , we also introduce an upper bound @xmath449 on the experimentation rates such that , if @xmath450 , for all @xmath3 , then @xmath451 for all @xmath3 , @xmath77 .",
    "such an upper bound @xmath449 exists due to lemma  [ lm : qexp2 ] .",
    "[ lm : pi2 ] suppose @xmath201 , @xmath202 , for all @xmath3 . for any @xmath168",
    ", there exist @xmath376 , such that , if @xmath377 , then @xmath452 \\geq 1-\\epsilon\\ ] ] where @xmath453 , @xmath87 , is the random event defined as @xmath454      we have @xmath455 = 1 , \\qquad\\mbox{for all } \\ k.\\ ] ] since we have a weakly acyclic game at hand , for each @xmath383 , there exists a strict better reply path of minimum length @xmath456 starting at @xmath182 and ending at an equilibrium policy .",
    "let @xmath457 .",
    "there exists @xmath458 ( which depends only on @xmath96 , and @xmath93 ) such that , for all @xmath77 , @xmath459 \\geq \\check{p}_{\\min}. \\label{eq : ne2e2}\\end{aligned}\\ ] ] pick @xmath460 satisfying @xmath461 lemma  [ lm : pi2 ] implies the existence of @xmath462 such that , if @xmath463 , then @xmath464 \\geq 1-\\check{\\epsilon } , \\quad \\mbox{for all } \\",
    "k.\\ ] ] for the rest of the proof , we assume @xmath463 . from ( [ eq : e2e2 ] ) , ( [ eq : ne2e2 ] ) , ( [ eq : pi2 ] ) , we obtain , for all @xmath77 , @xmath465 \\geq \\check{p}_{\\min}(1-\\check{\\epsilon } ) \\\\ & \\mbox{and } \\",
    "p\\left [   \\pi_{k+\\check{l } } = \\cdots = \\pi_k   |   \\pi_k\\in\\pi_{\\rm eq }   \\right ] \\geq 1-\\check{\\epsilon}.\\end{aligned}\\ ] ] this leads to the recursive inequalities @xmath466 ,",
    "\\quad n\\geq0 \\label{geometricconv2}\\end{aligned}\\ ] ] where @xmath396 $ ] .",
    "note that these inequalities are similar to ( [ geometricconv ] ) and by similar reasoning , there exists @xmath467 such that , for all @xmath468 and @xmath410 , @xmath469 this proves part ( i ) .",
    "the proofs of part ( ii)-(iii ) are analogous to the proofs of part ( ii)-(iii ) of theorem  [ normalq ] , respectively .                    c.  claus and c.  boutilier , `` the dynamics of reinforcement learning in cooperative multiagent systems , '' in _ proceedings of the tenth innovative applications of artificial intelligence conference , madison , wisconsin _ , july 1998 , pp",
    ". 746752 .",
    "j.  ding , m.  kamgarpour , s.  summers , a.  abate , j.  lygeros , and c.  tomlin , `` a stochastic games framework for verification and control of discrete time stochastic hybrid systems , '' _ automatica _ , vol .",
    "49 , no .  9 , pp .",
    "26652674 , 2013 .",
    "q.  zhu and t.  baar , `` game - theoretic methods for robustness , security , and resilience of cyberphysical control systems : games - in - games principle for optimal cross - layer resilient control systems , '' _ control systems , ieee _ , vol .",
    "35 , no .  1 ,",
    "pp . 4665 , 2015 .",
    "j.  w. huang and v.  krishnamurthy , `` transmission control in cognitive radio as a markovian dynamic game : structural result on randomized threshold policies , '' _ ieee transactions on communications _",
    "58 , no .  1 , pp . 301310 , 2010 .",
    "j.  r. marden , g.  arslan , and j.  s. shamma , `` cooperative control and potential games , '' _ ieee transaction on systems , man , and cybernetics - part b : cybernetics _ , vol .",
    "* 39 * , no .  6 , pp .",
    "13931407 , 2009 .",
    "j.  r. marden , h.  p. young , g.  arslan , and j.  s. shamma , `` payoff based dynamics for multi - player weakly acyclic games , '' _ siam journal on control and optimization _",
    "* 48 * , no .  1 ,",
    "pp . 373396 , 2009 .",
    "a.  c. chapman , d.  s. leslie , a.  rogers , and n.  r. jennings , `` convergent learning algorithms for unknown reward games , '' _ siam journal on control and optimization _ , vol .",
    "51 , no .  4 , pp . 31543180 , 2013 .",
    "g.  arslan , m.  f. demirkol , and y.  song , `` equilibrium efficiency improvement in mimo interference systems : a decentralized stream control approach , '' _ ieee transactions on wireless communications _",
    ", vol .  6 , no .  8 , pp .",
    "29842993 , 2007 ."
  ],
  "abstract_text": [
    "<S> there are only a few learning algorithms applicable to stochastic dynamic teams and games which generalize markov decision processes to decentralized stochastic control problems involving possibly self - interested decision makers . </S>",
    "<S> learning in games is generally difficult because of the non - stationary environment in which each decision maker aims to learn its optimal decisions with minimal information in the presence of the other decision makers who are also learning . in stochastic dynamic games , </S>",
    "<S> learning is more challenging because , while learning , the decision makers alter the state of the system and hence the future cost . in this paper , we present decentralized q - learning algorithms for stochastic games , and study their convergence for the weakly acyclic case which includes team problems as an important special case . </S>",
    "<S> the algorithm is decentralized in that each decision maker has access to only its local information , the state information , and the local cost realizations ; furthermore , it is completely oblivious to the presence of other decision makers . we show that these algorithms converge to equilibrium policies almost surely in large classes of stochastic games . </S>"
  ]
}