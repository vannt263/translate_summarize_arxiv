{
  "article_text": [
    "wikipedia has become the most used internet encyclopedia and , indeed , one of the most popular websites overall.[multiblock footnote omitted ] in addition , due to wikipedia s inclusion into widely used applications such as google knowledgegraph or apple s siri system , its content will influence the knowledge and , potentially , the behavior of millions of users , even if they do not visit the wikipedia site directly .",
    "therefore , it is essential that its content is accurate and reliable .",
    "in contrast to traditional encyclopedias , wikipedia is not authored mainly by experts .",
    "also , the articles are authored collaboratively by more than just a small number of contributors and the identity and expertise of authors is hard to verify .",
    "this leaves wikipedia articles open to addition of inaccurate content , spamming or vandalism , and calls into question its reliability .",
    "a substantial number of reliability studies have compared wikipedia against other reference works ( such as the _ encyclopedia britannica _ or drug package information ) or subjected them to expert review : the exhaustive survey in @xcite concludes that the results of these studies have overall been favourable to wikipedia when it comes to accuracy of facts , although some works ( especially on medical articles ) found errors of omission.[multiblock footnote omitted ]    these surprisingly favorable results on the reliability of wikipedia can in all probability be traced to a small number of wikipedia editorial policies , one of which we are concerned with in this paper .",
    "the _ verifiability _ policy requires wikipedia contributors to support their additions with citations from authoritative external sources . in particular , wikipedia policy states that `` articles should be based on reliable , third - party , published sources with a reputation for fact - checking and accuracy . '' ] this policy , on the one hand , guides contributors towards both neutrality and the importance of authoritative assessment and , on the other hand , allows wikipedia core editors to identify unreliable articles more easily via a lack of such citations .",
    "citations therefore play a crucial role in ensuring and upholding wikipedia reliability .    for current and recent events ,",
    "news citations are one of the most - used sources @xcite .",
    "again , wikipedia encourages the use of news outlets as citations : `` news reporting from well - established news outlets is generally considered to be reliable for statements of fact''@xdefthefnmark[reliability]footnotemark . as we show in section  [ sec : datasets_groundtruth ] , news are indeed the second - most widely used citation category in wikipedia ( with 1.88 million citations in our english wikipedia snapshot )  however , around 26% of these are no longer available due to dead or redirected links .",
    "in addition , new information is added all the time and will need verification .",
    "for both these purposes , an automatic way of finding an authoritative news citation for any fact(s ) one might wish to update , locate again or add would greatly facilitate wikipedia editing and improve its reliability .",
    "moreover , if no such citation can be found , it can guide contributors or core editors towards questioning their edits .    in this paper",
    ", we suggest such a method for automatic news citation discovery for wikipedia . in particular , we make the following contributions : ( i ) we analyze for which type of wikipedia statements a news citation is appropriate ( in contrast to , for example , a scientific journal citation ) , taking into account the type and structure of entity the statement is about , as well as the language the statement is written in .",
    "we provide a supervised learning algorithm for statement classification into citation categories .",
    "( ii ) we then develop a citation discovery algorithm which formalizes three properties of a good citation , namely that it entails the statement it supports , that it is from an authoritative source and that the statement it supports is central to it .",
    "( iii ) we establish a large - scale evaluation framework for citation discovery which uses crowdsourcing for measuring our approach s precision .",
    "to the best of our knowledge , this is the first work that automatically discovers citations for fine - grained wikipedia statements .",
    "we show that news citations can be discovered with high precision , in large contemporary news collections .",
    "in particular , we with high accuracy recover the same or very similar citations as the ones originally given by wikipedia contributors in the presence of numerous strong distractors or even find citations which are preferable to the original ones ( as established via crowdsourcing ) .",
    "in this section , we describe the terminology and problem definition for finding _ news citations _ for wikipedia .",
    "we operate on a specific snapshot of wikipedia @xmath0 where the text in each wikipedia page @xmath1 is organized into _ sections _ denoted by @xmath2 .",
    "additionally , entity pages are organized into a _",
    "structure , which is a _ directed - acyclic - graph _ ( dag ) induced by the wikipedia categories .",
    "this is routinely exploited by knowledge bases like yago ( e.g. _ barack_obama _ ` isa person ` ) and we leverage this type structure where each page @xmath3 belongs to a set of types @xmath4 .",
    "we , however , modify the original yago type structure to make it _ depth consistent _ as explained in section  [ subsec : learning_framework ] .",
    "* citation : * in wikipedia pages , any _ piece of text _ can be supported by a _",
    "citation_. the citation points to an external information source , such as a news article , blog , book or journal , that is considered as _ evidence _ for the fact mentioned in the text . citations in wikipedia are categorized into a predefined set of _ 16 citation categories _ viz .",
    "@xmath5`web ` , ` news ` , ` books ` , ` journal ` , ` map ` , ` comic ` , ` court ` , ` press release ` , @xmath6 .",
    "the distribution of the citation types is given in figure  [ fig : cite_type_dist ] .",
    "* statement : * we will refer to the _ piece of text _ from a wikipedia page that has or needs a citation as a _ wikipedia statement _ or simply a _ statement_. in this work , we restrict statements to a single sentence or a sequence of sentences that occur between two consecutive citation markers or a citation marker and paragraph beginning / end .",
    "a citation marker is either an actual citation or a placeholder ` citation needed`[multiblock footnote omitted ] .",
    "we therefore leave the identification of statements to future work .",
    "we also do not consider finding evidence for partial sentences or clauses .",
    "each statement @xmath7 in a page @xmath3 belongs to a section @xmath8 , and the set of statements extracted from a section @xmath9 of @xmath3 is represented as @xmath10 .",
    "* anchors and entities : * typically words or phrases in statements link to other wikipedia pages which represent entities through _ anchors_. we denote these links to other pages or entities starting from a statement @xmath7 as @xmath11 , and @xmath12 the corresponding entity types .",
    "we posit that the following two tasks are integral to finding a citation for a wikipedia statement .",
    "* statement categorization .",
    "* for a statement @xmath7 from a page @xmath3 of an unknown citation category , the task aims to determine the correct citation category for @xmath7 .",
    "@xmath13    we want to categorize @xmath7 as a _ news statement _ if it requires a news citation .",
    "this is based on the hypothesis that each statement typically has a preferred citation category , which we need to determine before making a high precision citation recommendation",
    ".    * citation discovery . * given a ( i ) statement @xmath7 found in page @xmath3 and of category @xmath14`news ` , and ( ii ) an external news collection @xmath15 , we define the citation discovery task as finding articles @xmath16 that serve as evidence for @xmath7 .",
    "we define the function @xmath17 which for @xmath7 outputs the subset of articles that can be suggested for citation .",
    "@xmath18      figure  [ fig : approach_overview ] shows an overview of our approach . for an entity , we extract entity and type structure , and its statements and finally run the steps of statement categorization and citation discovery .",
    "* _ statement categorization  sc .",
    "_ * in the first step , we predict the citation category of a wikipedia statement @xmath7 via supervised machine learning .",
    "we train a multi - class classification model , where the classes correspond to the citation categories @xmath19 .    *",
    "_ citation discovery ",
    "* in the second step , for all _ news statements _ we find evidence for them via news articles .",
    "we retrieve candidate news articles from a news collection @xmath15 through standard _",
    "information retrieval _ methods with @xmath7 serving as our query , and classify each candidate as either an appropriate citation for @xmath7 or not .",
    "from a wikipedia snapshot @xmath0 ( 2015 - 07 - 01 ) we extract all statements and all citations associated with that statement .",
    "we extract 6.9 million statements with 8.8 million citations , from 1.65 million entities and 668k section types .",
    "citations are categorized into one of the categories @xmath19 by the wikipedia editors . however ,",
    "sometimes the editors do not categorize a citation as ` news ` although they should do so .",
    "for example , in * w * , its top3 news domains _ bbc _ , _ nytimes _ , _ guardian _ , are often cited in categories other than ` news ` .",
    "most of such _ violations _ by the editors occur when citing news under the category ` web ` , which often is a catch - all for almost any type of resource ( news , book , etc . ) .    in most cases",
    "such violations can be accurately corrected by applying two simple heuristics :    * majority voting . *",
    "citations from the same domain url are tagged with different categories .",
    "we resolve such cases based on _",
    "majority voting_. in case a domain is cited more often under the ` news ` category , then all citations to the same domain are changed to ` news ` .    * url patterns . * in this heuristic we look for patterns in the url , specifically for ` _ _ /news/ _ _ ' and `",
    "_ _ http://news .",
    "this rule is applied to ` web ` statements , and in case the url matches one of the patterns , we change its category to ` news ` .",
    "table  [ tbl : gt_cite_changes ] shows the top4 most frequent citation categories and the impact of our ground - truth curation rules .",
    "rule application changes the citation category for 1,652,619 citations , approximately 18% of all citations in * w*. the cells in the table show the number of statements that are changed from the category in the row to the category in the column table .",
    "we say that a statement is a _",
    "news statement _ if it contains at least one ` news ` citation ( after ground - truth curation ) .",
    "figure  [ fig : cite_type_dist ] shows the statement distribution across the categories .",
    "it is evident that ` web ` and ` news ` are the two most popular categories , with 5.3 and 1.88 million citations , coming from 1.2 million and 436k entities , respectively .          from the _ news statements",
    "_ , we extract the cited news articles and construct the wikipedia news collection @xmath20 , which serves as our ground - truth for the _ citation discovery _ task .",
    "we define @xmath21 as the set of articles cited from statements @xmath7 which come from entities of type @xmath22 .",
    "with @xmath23 we denote the set of articles cited by @xmath7 .    from the collection of news statements , we have 1.88 million citations to news articles ( see above ) .",
    "we successfully crawled 1.5 million articles.the remaining 19% of citations point to non - existent articles ( dead links , moved content etc . ) .",
    "furthermore , some of the successfully crawled urls point to the _ index pages_. this can be noticed when we consider the article length ( in terms of characters ) in figure  [ fig : news_length ] .",
    "filtering out articles that are below 200 characters , we are left with with 1.39 million articles , a decrease of 26% from the original 1.88 million news citations .",
    "an additional issue we notice in @xmath24 are citations to non - english news articles .",
    "we find that 23% of articles in @xmath25 are in languages other than english , using apache tika[multiblock footnote omitted ] for language detection .",
    "in the statement categorization task , we are given a statement @xmath7 and the entity @xmath3 from which it is extracted .",
    "we compute features that exploit the language style of @xmath7 and the type and section structure of @xmath3 to categorize @xmath7 into one of the citation categories @xmath19 .",
    "we learn a multi - class classifier ( section  [ subsec : learning_framework ] ) with classes corresponding to citation categories @xmath19 and optimize for predicting news statements .",
    "table  [ tbl : sc_feature_list ] shows an overview of the feature list .",
    "we hypothesize that wikipedia statements with news citations are similar to the language style of news , as they often paraphrase cited news articles .",
    "different genres ( such as _ news , recipes , sermons , faqs , fiction  _ ) differ in their linguistic properties as the different functions they fulfill influence linguistic form @xcite .",
    "for example , we expect news reports ( which center mostly on past events ) to contain more past tense verbs than a recipe which gives instructions via verbs in the imperative .",
    "we use features that were successful in automatic genre classification including structural features via parts - of - speech as well as lexical surface cues @xcite .",
    "* part of speech density .",
    "* frequency of part - of - speech ( pos ) tags , determined via the stanford tagger , allows us to capture some of the structural properties of text .",
    "for example , news statements can be characterized by a high number of past tense verbs as well as proper names .",
    "we normalize the pos tag frequency w.r.t the sum of all tags in a statement , to account for varying statement length .",
    "* verbs of attribution and quotation marks .",
    "* news articles often report statements by persons of repute , witnesses or other sources .",
    "we approximate this by two features : firstly , we count _ verbs of attribution _ in @xmath7 , via a list of 92 such verbs ( _ claim , tell etc _ ) with pos tag ` vb * ` and normalize w.r.t the total number of ` vb * ` .",
    "secondly , we use * quotation marks * as a potential indicator of _ paraphrasing_. the feature simply counts the number of quotation marks in @xmath7 , normalized w.r.t the statement length .    *",
    "temporal proximity @xmath26 .",
    "* most wikipedia statements with news citations refer to relatively recent events , i.e. events close to the time of the wikipedia snapshot .",
    "therefore , we use _ temporal expressions _ such as dates and years as distinguishing features for news statements .",
    "we use a set of hand - crafted regular expression rules to extract temporal expressions .. we use the following rules : ( 1 ) ` dd month yyyy ` , ( 2 ) ` dd mm yyyy ` , ( 3 ) ` mm dd yy(yy ) ` , ( 4 ) ` yyyy ` , with different delimiters ( whitespace , ` - ' , ` . ' ) .",
    "we then compute @xmath27 .",
    "* discourse analysis .",
    "* we use discourse connectives to annotate the statements @xmath7 with _ explicit discourse relations _ based on an approach proposed by pitler and church  @xcite .",
    "the annotations belong to the categories @xmath28__temporal , contingency , comparison , expansion__@xmath29 , following the penn discourse treebank annotation @xcite .",
    "some of the explicit discourse relations are particularly interesting ( i.e. , _ temporal _ ) as they represent a common language construct used in news articles that report event sequences .",
    "the features are boolean indicators on whether @xmath7 contains a specific explicit discourse relation .",
    "* language model and topic model scoring . * as surface lexical features have been shown to be efficient in genre recognition @xcite , we compute n  gram ( up to n=3 ) language models with _ kneser - ney _ smoothing ( lm ) from news articles @xmath30 and compute the score @xmath31 .",
    "this score shows how likely @xmath7 can be constructed from the lm .",
    "similarly , we compute topic models using the lda framework  @xcite , where the score is the jaccard similarity between @xmath7 and the topic terms from @xmath30 .      determining if a statement requires a news citation solely on language style is not always feasible .",
    "we exploit the entity structure of @xmath3 and compute the probability of statements having a news citation given its types @xmath4 and sections @xmath2 .    * section - type probability . * a good indicator of the likelihood that a statement @xmath7 requires a news citation is the entity type it belongs to and the section type that it appears in .",
    "for instance , for type ` politician ` , news statements have higher density in section _ ` early life and career ' _ as these tend to be more reflected in news . to avoid over - fitting we filter out entity types with fewer than 10 statements .",
    "similarly , we filter out section with fewer than 10 statements , and in which they belong to the same citation category .",
    "we compute the _ conditional probability _ of having a news citation for @xmath7 for an entity type @xmath32 given a section type @xmath9 . @xmath33",
    "the @xmath34 probability is likely to be a sparse feature , so we compute type and section news - priors .",
    "we compute section @xmath35 and type news - priors @xmath36 based on the news statement ratio that belong to a section or type , respectively .    since @xmath7 is associated with an entity @xmath3 , which has a set of types @xmath4 , we aggregate the computed type news - priors and the section - type joint probability into their _ min , max _ and _ avg _ probabilities",
    ".    * type co - occurrence .",
    "* from the entity types @xmath37 and @xmath4 we measure the likelihood of type co - occurrence in news .",
    "the probability simply counts the co - occurrence between @xmath22 and @xmath38 in news statements with respect to their total co - occurrence .",
    "examples of highly co - occurring types in news are ` politician ` and ` organization ` .",
    "@xmath39      * learning setup .",
    "* wikipedia consists of a highly diverse set of entities .",
    "a model trained on all entities is unlikely to work .",
    "for example , the types ` location ` and ` politician ` represent two highly divergent groups with regard to entity page structure , the statements they contain and the way they are reported in news .",
    "we therefore learn @xmath40 for individual types in the yago type taxonomy .",
    "the advantages of type specific functions @xmath40 is that they are trained on homogeneous entities , which helps the models predict with greater accuracy . we take only types that have more than 1000 entity instances , resulting in 672 types .",
    "the types are organized from very broad types such as ( ` owl : thing ` ) to very specific types like ` serie_a_players ` .    to utilize the specialization and generalization in a principled manner we transform the yago type taxonomy ( dag ) into a hierarchical dag .",
    "this is utilized later on in order to find the right level of type granularity for learning @xmath40 .",
    "we assume that the hierarchy is rooted at ` owl : thing ` and all internal nodes are _ depth - consistent _",
    ", i.e. all paths from the root to the node are of the same length .",
    "we obtain this by a simple heuristic whereby for every _ child type _",
    "@xmath41 _ parent type _ we remove edges where the parent s _ depth level _ in the taxonomy is higher than the _ minimum level _ from other parent nodes .    with this hierarchical type - taxonomy",
    ", we can determine the _ optimal level _ of type granularity such that we have optimal performance in categorizing statements .",
    "for learning the type specific @xmath40 , we keep 10% of entity instances for evaluation and the remainder for training .",
    "it is important to note that when we learn @xmath40 for a given type , the training instances are sampled through _ stratified sampling _ from all its _ children types_.    * learning model . *",
    "the functions @xmath40 represent _ multi - class _ classifiers with classes corresponding to the citation categories .",
    "since we want to predict the news category @xmath14`__news _ _ ' with high accuracy , one question is why we do not pose this as a _ binary _",
    "classification problem , where a statement is categorized as news or not .",
    "we used the _ multi - class _ classifiers because they give us a more balanced distribution when compared to merging all non - news statements into a single category .",
    "finally , we opt for _ random forests _ ( rf )  @xcite as our supervised machine learning model .",
    "we experimented with other models , but the differences in performance are marginal , and rf have superior learning time .",
    "we train the models on the full feature set in table  [ tbl : sc_feature_list ] .",
    "for the citation discovery task , we follow the _ _ citation policy__[multiblock footnote omitted ] guidelines in wikipedia and single out three key properties on what makes a _",
    "good citation_.    1",
    ".   the statement should be _ entailed _ by the cited news article 2 .",
    "the statement should be _ central _ in the cited news article 3",
    ".   the cited news article should be from an _ authoritative _ source    we approach the citation discovery for news statements as follows .",
    "we use statement @xmath7 as a query ( see section  [ subsec : query_construction ] ) to retrieve the top@xmath42 news articles from @xmath15 as citation candidates for @xmath7 .",
    "we then classify the candidate citations as either ` _ _ correct _ _ ' or ` _ _ incorrect _ _ ' , depending on whether they meet the above criteria of a _",
    "good citation_.    in order to do so , we compute features for each pair @xmath43 , w.r.t the individual sentences of a news article @xmath44 .",
    "the feature vectors become the following @xmath45\\rangle$ ] , where @xmath46 represents the @xmath47-th sentence from @xmath44 .    since the number of sentences @xmath48 varies across news articles , we aggregate the individually computed features at sentence level into the corresponding _ min , max _ , _ average _ , _ weighted average _ , and _ exponential decay function _ scores as shown below .",
    "@xmath49 where @xmath50 is a feature from the complete feature list in table  [ tbl : missing_citations_feature_list ] .",
    "we use the _ statement text _ as query which can vary from a sentence to a paragraph .    one way to improve the likelihood of obtaining good citation candidates from top@xmath42 articles is through _ query construction _ approaches ( qc ) .",
    "it has been shown that in similar cases where the query corresponds to a sentence or paragraph , qc approaches are necessary to increase the accuracy of ir models .",
    "henzinger et al .",
    "@xcite propose several qc approaches that weigh query terms based on the _ tf  idf _ score .",
    "we experimented with different qc approaches from @xcite and their impact on finding news articles in @xmath25 .",
    "we found that _",
    "qca1base _ performed best and use it in the remainder of the paper . in _",
    "qca1base _ , the terms extracted from the statements are weighted based on _ tf  idf _ , with @xmath51 and @xmath52 are computed w.r.t the other statements under consideration .    in principle",
    ", one should consider all retrieved articles from the result set .",
    "however , this is not only computationally expensive for our subsequent learning step but also unbalances our training set . to determine a reasonable retrieval depth , we experimented with 1000 randomly chosen statement queries with qc and determined the hit - rate at retrieval depth @xmath42 , i.e. whether the cited article is retrieved in the top@xmath42 articles .    figure  [ fig : index_coverage ] shows the hit - rate in top1000 with top 50 ranked query terms and with _ divergence from randomness _ query similarity measure  @xcite for our _ random sample _ of 1000 news statements .",
    "we focus on the top100 retrieved news articles as potential citations for @xmath7 , as the achieved hit - rate beyond the top100 shows only minor improvement . in figure",
    "[ fig : index_coverage ] , we also note that the hit - rate does not go beyond 50% .",
    "we found that most of the news articles that are not retrieved are either missing or non - english articles in @xmath24 .",
    "as the citation is supposed to give close evidence for the statement s content , in the ideal case the cited news article should fully _ entail _ the statement , i.e. the statement should be derivable from the news article .",
    "the recognition of textual entailment has been the study of extensive research in the last 10 years ; cf @xcite for an overview .",
    "a full treatment of entailment needs extensive world knowledge and inference rules ; we here restrict ourselves to much simpler lexical and syntactic similarity methods used in baseline entailment systems and leave the extensions to future work.[multiblock footnote omitted ]    * ir baseline features .",
    "* we use the retrieval model as a pre - filter to find candidate news articles as citations for @xmath7 .",
    "the retrieval model also provides us with two possible features for the learning model : firstly , a _ matching score _ of @xmath44 for query @xmath7 , where the score corresponds to the _ divergence from randomness _ query similarity measure  @xcite .",
    "secondly , the retrieval rank of @xmath44 .",
    "we use the ir model as our _ baseline _ and hence refer to them as _",
    "baselines features_.    * tree kernel similarity . * lexical similarity measures in many cases fail to capture the joint _",
    "semantic _ and _ syntactic _ similarity . for this purpose",
    ", we consider the _ tree kernel _",
    "similarity measure proposed in  @xcite .",
    "we first compute the _ dependency parse trees _ of @xmath7 and @xmath46 using the stanford tagger  @xcite , and then compute the tree kernel , @xmath53 .",
    "tree kernel similarity through the dependency parse tree measures the maximum matching subtrees between @xmath7 and @xmath46 , where the matching subtrees have the same syntactic and semantic meaning .",
    "we refer the reader to @xcite for details .    * lm & topic model scoring . * from an article @xmath44 we compute a _ unigram lm _ and compute @xmath54 as the likelihood of @xmath7 being generated from the computed lm .",
    "in addition , we compute _",
    "n  gram _ lm ( with @xmath55 up to 3 ) from articles in @xmath30 , and compute the score @xmath56 accordingly .    similarly , we compute _ lda topic models _  @xcite for entity types , specifically from articles in @xmath30 .",
    "this follows the intuition that content usually is clustered around specific topics , i.e. for type ` politician ` most discussions are centered around politics , career , etc . the topic score is the jaccard similarity between @xmath44 and the topic terms .",
    "* similarity to most central news sentence .",
    "* as described above we compute similarity features between @xmath7 and sentences in @xmath44 .",
    "however , some sentences in @xmath44 are more central than others . hence , the computed features between the pairs @xmath57\\rangle$ ] , do not have uniform weight .",
    "therefore , we find the most _ central sentence _",
    "@xmath58 in @xmath44 and distinguish the computed entailment / similarity features between @xmath7 and @xmath58 .",
    "we compute centrality of a sentence in @xmath44 through the textrank approach introduced in  @xcite .",
    "we first construct a graph @xmath59 from @xmath44 , where @xmath60 corresponds to the sentences of @xmath44 , with edges in @xmath61 weighted with the jaccard similarity between any two sentences , in this case @xmath62 .",
    "computation of centrality for any vertex @xmath46 is similar to that of pagerank , with slight changes accounting for the weighted edges between vertices .",
    "@xmath63 where @xmath64 is the damping factor ( @xmath65 ) , a common value in pagerank computation .",
    "the computation converges if the difference in the score of @xmath66 in two consecutive iterations is small .",
    "* relative entity frequency . *",
    "the importance of @xmath3 in @xmath44 is crucial when finding citations for @xmath7 .",
    "this importance is partially mirrored simply in how often @xmath3 is mentioned in @xmath44 .",
    "however , another genre - typical property of news is its inverted pyramid structure , i.e. the most important information is mentioned at the beginning of the article .",
    "we therefore measure relative entity frequency of @xmath3 in @xmath44 based on an approach described in  @xcite .",
    "it attributes higher weight to entities appearing in the top paragraphs of @xmath44 , where the weight follows an exponential decay function .    @xmath67    where @xmath68 represents a news paragraph from @xmath55 and @xmath69 indicates the set of all paragraphs .",
    "@xmath70 indicates the frequency of @xmath3 in @xmath68 . with @xmath71 and @xmath72",
    "we indicate the number of paragraphs in which entity @xmath3 occurs and the total number of paragraphs .",
    "additionally we consider the relative entity frequency for entities in @xmath73 and measure the _ minimum , maximum _ and _ average _ relative entity frequency scores .",
    "wikipedia s editing policy distinguishes clearly between more and less - established news outlets and prefers the former ( see the introduction ) .",
    "we therefore compute the authority of news domains w.r.t entity types and sections .",
    "we will denote the _ domain _ of the news article referred from @xmath7 as @xmath74 $ ] , and with @xmath75 any arbitrary domain .",
    "* type - domain authority . * authority of news domains is non - uniformly distributed across types . for types such as ` politician ` the authority of domains like _ bbc _ is higher than for types such as ` athletes ` , where a domain specialized in sports news is more likely to be authoritative .",
    "we capture the _ type - domain authority _ as follows : @xmath76}}{\\sum_{e\\in\\mathbf{w}\\wedge t\\in t(e)}\\sum_{s \\in s(e)}d[s]}\\ ] ]    * section - domain authority .",
    "* we measure the authority of domains associated to certain entity sections .",
    "the density of news references across sections varies heavily .",
    "therefore , it is natural to consider the authority of news domains for a given section . @xmath77}}{\\sum_{e\\in\\mathbf{w}}\\sum_{s \\in s(e,\\psi)}d[s]}\\ ] ]    note that these features compute news outlet authority with regard to current wikipedia usage , which we seek to re - create .",
    "an alternative we intend to look at in future work is to measure authoritativeness via wikipedia - external measures of news outlets , such as page visits or interlinkage .",
    "here we describe the evaluation of our approach for sc . since we consider a type taxonomy , we have a hierarchy of models .",
    "each statement belongs to an entity , which in turn is a child to a type ( node ) in the hierarchy .",
    "consequently , we construct each model from training instances ( statements ) that are its children .",
    "we focus on two aspects of our approach ( i ) performance of models at varying depths , and ( ii ) performance of various feature classes .",
    "we provide the detailed results for the statement categorization task and the corresponding ground - truth data at the paper urlhttp://l3s.de/~fetahu / cikm2016/ ]",
    ".      * setup .",
    "* we consider 672 entity types from our yago taxonomy , for which we learn individual @xmath40 models .",
    "we consider types that have more than 1000 entity instances .",
    "the level of granularity in the yago taxonomy has a maximum depth of 20 , while the root type is ` owl : thing ` containing all possible entities",
    ".    * train / test .",
    "* we learn the @xmath40 models using up to 90% of the entity instances of a type @xmath22 as training set , and the remainder of 10% for evaluation .",
    "we use _ stratified sampling _ to pick entities of type @xmath22 and its subtypes for the train and test set .",
    "we train and test @xmath40 models over 6 million statements coming from 1.3 million entities .",
    "* metrics .",
    "* we evaluate the performance of @xmath40 with _ precision _",
    "@xmath78 , _ recall _ @xmath79 and @xmath80 .",
    "a statement is categorized correctly if the predicted category corresponds to the ground - truth .",
    "the following discussion focuses on the results for the statement categorization task for the ` news ` category .",
    "due to space constraints we report the first three type levels in the yago taxonomy , specifically the immediate child ` legal actor geo ` of ` owl : thing ` .",
    "the results for the remainder of the types are accessible at the url@xdefthefnmark[paper_url]footnotemark .",
    "table  [ tbl : yagolegalactorgeo_t1 ] shows the results for @xmath40 models evaluated over 61k entities and trained with up to 550k entities , depending on the training sample size @xmath81 $ ] .",
    "the results for this type represent more than 47% of the total set of entities in our evaluation dataset .    the overall performance of @xmath40 for all types for @xmath82 measured through _ micro - average _ precision is 0.57 . since a statement belongs to multiple types @xmath37 , we decide the category of @xmath7 based on majority as categorized from the individual @xmath40 models .",
    "as expected , we observe that model performance depends on the type level ( cf .",
    "table  [ tbl : yagolegalactorgeo_t1 ] ) .",
    "a unified model from heterogeneous training instances performs poorly : the @xmath40 model for the main type ` legal actor geo ` achieves a precision p=0.527 with high variance across its subtypes .",
    "comparing the types at depth level 3 , the difference in terms of precision can go as high as 15% between ` legal actor geo ` and the best performing subtype ` preserver ` .    at higher depths , performance of the @xmath40 models",
    "often improves significantly as the instances belonging to a given type become more homogeneous .",
    "for example , the fine grained entity type ` wcat italian footballers ` has a precision of p=0.87 and recall of r=0.58 , which constitutes a 50% precision and a 26% recall improvement over its parent type ` person ` .",
    "however , the performance improvement is not monotonically increasing . in some fine - grained types , there is in fact a performance reduction which can be attributed to over - fitting .",
    "this suggests that there is indeed a sweet spot in terms of choice of the best performing model for an instance .",
    "we observed that the instances that are children of ` person ` showed best performances between levels 5 and 8 .",
    "our models perform poorly for types such as ` location ` since location pages have a lower news density .",
    "we again observe that news articles are usually centered around people and its instances benefit the most from our approach .",
    "we also observe that the performance of our approach is sensitive to the type hierarchy .",
    "the choice of yago as a taxonomy is due its fine - grained types . however , there exist many long - tail entities that are direct descendants from the higher levels and fail to leverage the homogeneity of fine - grained types .",
    "we also perform poorly on such instances .    in the yago taxonomy ,",
    "the entities are distributed normally with a mean at depth level 8 , which contains around 36% of entities . the long tail with types lower than depth level 8 accounts for 28% of entities in the yago taxonomy .",
    "we focussed on the category ` news ` in our discussion and in table  [ tbl : yagolegalactorgeo_t1 ] .",
    "performance of @xmath40 models for the categories @xmath5`web ` , ` book ` , ` journal`@xmath29 and type ` person ` is p=0.62 and r=0.59 , p=0.29 and r=0.69 , and p=0.25 and r=0.26 , respectively .",
    "the relatively high score for the ` web ` category can be attributed to the high density of statements of category ` web ` , accounting for more than 54% of the total statements .",
    "hence , by always choosing ` web ` as the category of a statement we get an average precision of 0.54 .",
    "* convergence .",
    "* we measure the amount of training data required for the models to converge to optimal performance .",
    "figure  [ fig : t1_learning_curve ] shows the learning curve for some of the types reported in table  [ tbl : yagolegalactorgeo_t1 ] . we see that @xmath40 models converge and achieve optimal performance early on with a sample around 7% to 10% .",
    "* ablation . *",
    "we apply a feature ablation test for the different different features groups from table  [ tbl : sc_feature_list ] .",
    "figure  [ fig : feature_ablation_yagolegalactorgeo ] shows the results for the feature groups _ language style _ , and _",
    "entity structure_. the highest gain is achieved with the feature group _ entity structure _ , which reveals the challenging nature of the task where language style features can not be applied alone .",
    "in this section , we evaluate the _ citation discovery _ task for _ news statements_. we perform an extensive evaluation for approximately 22k news statements and discover citations from a real - word news collection with 20 million articles in a timespan of two years .",
    "we limit ourselves to the subset of news statements with citations to news articles in @xmath24 from 2013 to 2015 .",
    "the resulting set contains 22k news statements with 27k news article citations in @xmath24.[multiblock footnote omitted ] we denote this temporal slice of news articles in @xmath24 by @xmath83 .    as finding",
    "the right citation from this preselected collection is easier than the realistic scenario of finding a citation among all possible news , we also collected all english news articles from the period [ 2013 - 08 , 2015 - 08 ] from the gdelt project[multiblock footnote omitted ] .",
    "we call the resulting high - coverage dataset @xmath84 .",
    "we merge @xmath84 with @xmath83 and call the resulting dataset @xmath85 . the set @xmath15 contains around 20 million news articles .",
    "@xmath83 accounts for less than 1% in @xmath15 , making the correct articles hard to find .",
    "* evaluation strategy ` e1 ` : * in this scenario , we , for each news statement @xmath7 , only consider the pairs @xmath86 , where @xmath87 as correct and all other possible citations as incorrect . this allows for fully automatic evaluation but is only a _",
    "lower bound _ for @xmath17 , as there can be additional articles that are relevant for @xmath7 but do not exist in @xmath23 .",
    "we therefore also consider a variant ` e1+fp ` , where we consider @xmath88 as additional correct citations if the similarity ( based on the _ jaccard _ similarity ) to one of the articles in @xmath23 is above 0.8 .",
    "* evaluation strategy ` e2 ` : * ` e2 ` assesses the true performance of @xmath17 . in this case , apart from already existing citations for @xmath7 from @xmath23 , we assess through _ crowd - sourcing _ the appropriateness as citations of articles @xmath89 .",
    "we set up the crowd - sourcing experiment for ` e2 ` as follows . for a statement @xmath7 and",
    "an article @xmath90 marked as correct by @xmath17 , we ask the crowd to compare @xmath44 with the ground - truth article @xmath91 and answer the question _ ` which of the two shown news articles is an appropriate citation for the statement?'_. the workers are shown @xmath7 as well as @xmath44 and the ground truth article in random order without an indication which one is the ground truth .",
    "we provide the following response options : ( i ) _ first _ , ( ii ) _ second _ , ( iii ) _ both _ , ( iv ) _ none _ , and ( v ) _ insufficient info_. we deployed the experiment in crowdflower[multiblock footnote omitted ] and chose only high quality workers to ensure the reliability of our experiments .",
    "furthermore , we removed workers who did not spend the minimum amount of two minutes to assess the appropriateness of a citation .",
    "we collect three judgments per question .",
    "we count citations as correct which are ground - truth articles or articles which the majority of workers judge as appropriate citations .      * retrieval model .",
    "* we use the retrieval model in @xcite via the implementation provided by solr[multiblock footnote omitted ] .",
    "we use the top-100 retrieved news articles for a statement as candidate citations , from which we perform feature extraction and learn our sc models .",
    "* learning setup .",
    "* we learn classifiers specific to entity types for a total of 83 types .",
    "we limit ourselves to types that have news statements in the date range 2013 - 2015 and with at least 100 entity instances . from our set of 22k statements",
    ", we randomly sample statements from each entity type if they have more than 1000 instances , otherwise we take all statements .",
    "training and testing data consist of the pairs @xmath92 , where @xmath7 is a news statement , and @xmath44 is one of the top100 citation candidates which we retrieve from @xmath15 .",
    "we split training and testing data per statement @xmath7 , where each @xmath7 and all its candidates are included _",
    "completely _ either in the training or test set",
    ".    * learning approach .",
    "* we learn the @xmath17 models as supervised binary classification models using random forests rf@xcite .",
    "we predict @xmath93__`correct ' , ` incorrect ' _ _ , i.e. if a candidate news article is an appropriate citation for @xmath7 or not .",
    "we optimize for the _ ",
    "correct_ class .",
    "the correct labels in training and automatic evaluation @xmath94 are all part of @xmath83 , which makes up less than 1% of our news collection @xmath95 . therefore , we learn @xmath17 as a _ cost - sensitive _ classifier",
    ".    * metrics . *",
    "we evaluate performance of @xmath17 models via precision @xmath78 , recall @xmath79 , and @xmath80 score .",
    "* baselines . *",
    "we consider two baselines ( * b1 * and * b2 * ) for this task . for @xmath96 , we use the _ divergence from randomness _ model  @xcite to retrieve news articles from @xmath15 for @xmath7 and simply suggest the top1 article as citation .",
    "in @xmath97 we learn a supervised model based on the _ ir baseline features _ ( see table  [ tbl : missing_citations_feature_list ] ) .",
    "table  [ tbl : t2_results_e1 ] shows the results for all evaluation strategies for the citation discovery task .",
    "we only display detailed results for the top10 best performing entity types out of the 83 types in our evaluation .",
    "the results in each row in table  [ tbl : t2_results_e1 ] show the best performance we achieve for the individual types , while varying the variables such as the _ training sample size _ and _ feature number_. we show results with a maximum of 60% training sample size .",
    "we report additionally the overall performance of @xmath17 models across all 83 types through _ micro - average _ in the last row in table  [ tbl : t2_results_e1 ] .",
    "the detailed results are accessible at the paper url@xdefthefnmark[paper_url]footnotemark .      in table",
    "[ tbl : t2_results_e1 ] , in the third column , we show the evaluation results for the strategy ` e1 ` .",
    "results for ` e1 ` are encouraging given the fact that in top100 news candidates retrieved from @xmath98 only 1% of the news are ` _ _ correct _ _ ' ( on average one relevant citation in @xmath83 per statement ) .",
    "furthermore , as shown in figure  [ fig : index_coverage ] the highest recall we get at top100 is on average around 45% .",
    "we achieve the best performance in terms of precision for the entity type ` football ` ` player ` , with precision p=0.80 and a recall of r=0.30 .",
    "for f1 the best performing type in this setup is the entity type ` player ` with f1=0.57 .    using the evaluation strategy ` e1+fp `",
    ", we consider as relevant all _ false positive _ ( fp ) articles which are highly similar to the ground - truth articles @xmath23 ( above 0.8 similarity ) .",
    "even though the fp articles do not exist in our ground - truth , the high similarity to the ground - truth article is a strong indicator for them being relevant citations . using this strategy ,",
    "the results improve for some of the types with up to 8% in terms of precision . for type `",
    "entertainer ` we have an increase of 11% . in absolute numbers , by considering the highly - similar fp articles as relevant we gain an additional 757 news articles out of 12,877 , i.e. an additional 6% news citations .",
    "baselines * b1 * and * b2",
    "* show the difficulty of the citation discovery task .",
    "in particular , we show that standard ir models struggle with this task . choosing only the top1 article for citation (",
    "* b1 * ) achieves only up to p=0.37 . on the other hand , for * b2 * , we see that we can not learn well using only the ir baseline features , and perform even worse than using * b1*.      for ` e2 ` , we report results after re - evaluating performance of @xmath17 models via gathering judgements for false positive ( fp ) news articles suggested as citations for @xmath7 .",
    "we evaluate 11,803 false positive news article citation candidates for the top10 entity types in table  [ tbl : t2_results_e1 ] , from 6.9k news statements .",
    "as reported above , crowd - workers could choose between both ground truth and our suggestion being correct , one of them or neither .",
    "the inter - rater agreement between workers was 64% .",
    "table  [ tbl : e2_eval ] shows how these false positives were assessed .    .",
    "[ cols= \" < , > \" , ]     we see that in many cases our suggestion was equal to ( 38.2% ) or even preferred ( 19.4% ) over the ground - truth suggestion .",
    "hence , our method can even improve citation quality in wikipedia .    in the `",
    "e2 ` column in table  [ tbl : t2_results_e1 ] we show the updated results for @xmath17 after collecting judgments for false positive news articles . we see that for most of the types we have an average gain of 18% in terms of precision .",
    "we achieve the biggest gain of 28% for the entity type ` location ` . for the types ` football player , creator , entertainer ` , we can suggest news citations with 90 - 91% precision .",
    "please note that we do not report the recall score for ` e2 ` , since assessing the appropriateness of every article in @xmath15 as a citation for @xmath7 is not feasible .",
    "the recall score is only reported w.r.t the ground - truth articles in @xmath83 .",
    "for the evaluation of both tasks in a pipeline scenario , we randomly sample 1000 statements from all categories and ran the process of citation discovery through both steps .",
    "each statement is associated with multiple entity types , as they are extracted from @xmath3 where @xmath4 is a set of types . for the _ statement categorization _",
    "task we perform the evaluation based on our ground - truth ; for the _ citation discovery _ we evaluate the suggested citations as in evaluation strategy ` e2 ` .",
    "note , that here in the evaluation pair we have a news article ( that we suggest ) and a resource that can be of any type including _ book , web , journal_.    * statement categorization .",
    "* we set up _",
    "statement categorization _ as a _ majority voting _ categorization . for each statement and the type specific classifiers",
    "@xmath40 we predict the category and pick the category that has the majority of votes .",
    "in contrast to the _ statement categorization _ in section  [ sec : statement_classification ] , where the original task aimed at showing for which types this task can be performed accurately , we now aim to set up citation discovery in an automated manner .    based on the ground - truth , 340 out of the 1000 statements were _",
    "news statements_. we categorize 368 as news statements , out of which 263 are correct , i.e. p=0.72 and r=0.77 .",
    "it is interesting to see that we can leverage additional information through _ majority voting _",
    ", where for the same statement and its associated types we can predict with high accuracy the citation category label of @xmath7 .",
    "* citation discovery .",
    "* for the _ citation discovery _",
    "task we ran it based on the generic @xmath17 model trained on statements belonging to all types , namely ` owl : thing ` .",
    "we could use the type specific @xmath17 , with additional costs for computing type specific features .    in the second task , from the 368 statements classified as news statements , we ran the citation discovery model @xmath17 .",
    "we are able to suggest 549 news citations for 78 statements . based on crowd - sourcing evaluation",
    ", we suggest 346 relevant citations , i.e. a precision of p=0.63 , out of which 200 citations are citations that were preferred over existing ones in the ground - truth . for 146 cases",
    "the citations we suggest are considered to be equally appropriate as the existing ones in the ground - truth , for 116 citations the ground - truth ones were preferred over the ones we suggested .",
    "note that our @xmath17 models suggest citations for @xmath7 only in case they fulfill the criteria in section  [ sec : missing_citations ] , thus , enforcing high accuracy .",
    "* citation sources . * ford et al .",
    "@xcite analyze the citation behavior of wikipedia editors with respect to their adherence to the citation guidelines .",
    "they investigate what types of sources are most often cited , i.e. _ primary , secondary _ and _ tertiary _ as defined in wikipedia@xdefthefnmark[reliability]footnotemark .",
    "they conclude that news are one of the top cited source in the _ secondary _ type , while they see a growing trend of _ primary _ sources due to their persistence on the web , contrary to the policies of preferring secondary sources .",
    "luyt and tan  @xcite analyze a subset of history entity pages and show that citations are biased towards a specific group of sources .",
    "@xcite emphasize the importance of citations in wikipedia as a means to ensure the quality of entity pages .",
    "* wikipedia quality .",
    "* anderka et al .",
    "@xcite propose an approach to predict quality flaws in wikipedia pages . a quality flaw in wikipedia is usually annotated with specific _ cleanup _ tags .",
    "they train a model to predict quality flaws , where among the top10 quality flaws they identify _ unreferenced , refimprove , primary sources _ as some of the most serious flaws .",
    "our work is complementary to theirs since we aim at finding appropriate citations for wikipedia statements , thereby improving the quality of wikipedia pages .",
    "* wikipedia enrichment . * sauper and barzilay @xcite propose an approach to automatically generate complete entity pages for a specific entity type .",
    "the approach is trained on already - populated entity pages of a specific type by learning templates about the section structure at the type level . for a new entity page ,",
    "they extract documents through web search ( with entity and section title as a query ) and identify the most relevant paragraphs to add in a section .",
    "fetahu et al . in  @xcite",
    "proposed an approach for suggesting news articles for a wikipedia entity and entity section .",
    "they first identify news articles that are important to an entity and in which the entity is salient , and further identify the most appropriate section to suggest the article . in case of a missing section ,",
    "a new section is added by exploiting the section structure from the entity type .",
    "this work differs from @xcite as we do not add content or suggest news articles to a complete section in an entity page , but rather provide citations to already existing statements .    *",
    "cumulative citation recommendation ( ccr ) .",
    "* trec introduced the ccr track in the knowledge base acceleration track in 2012 . for a stream of news and social media content and a target entity from a knowledge base ( wikipedia ) ,",
    "the goal of the task is to generate a score for each document based on how pertinent it is to the input entity .",
    "balog et al .",
    "@xcite propose approaches that find entity mentions in the document collection and rank them according to how central the entity is in the respective documents .",
    "this however is a filtering task for documents towards checking if they are relevant for a pre - defined set of entities .",
    "in contrast , in our task we aim at finding news citations as evidence for wikipedia statements .",
    "in this work we define and attempt to solve the automatic news citation discovery problem for wikipedia . we define two tasks  _ sentence categorization _ and the _ citation discovery _  towards finding the correct news citation for a given wikipedia statement . for the sentence categorization task",
    ", we learn a multi - class classifier to predict if a statement requires a news statement . for the news citation discovery problem",
    ", we first find the likely candidates by a retrieval model over a real - world news collection followed by a binary classification for the top - ranked candidates .",
    "we find that statement categorization is a hard problem due to lack of context for the nlp - based features to perform well .",
    "however , the wikipedia page and its type structure provide important cues towards accurate classification . on the other hand , we perform well on the citation discovery task with 67% precision ( for top - categories ) using the automated evaluation , which further improves to over 80% when crowd - sourced .",
    "this shows that we not only identify the correct ground truth articles present in wikipedia , but in some cases our suggestions are a better fit compared to the sources in wikipedia .",
    "this work is funded by the erc advanced grant alexandria ( grant no ."
  ],
  "abstract_text": [
    "<S> an important editing policy in wikipedia is to provide citations for added statements in wikipedia pages , where statements can be arbitrary pieces of text , ranging from a sentence to a paragraph . in many cases citations </S>",
    "<S> are either outdated or missing altogether .    in this work </S>",
    "<S> we address the problem of finding and updating news citations for statements in entity pages . </S>",
    "<S> we propose a two - stage supervised approach for this problem . in the first step , we construct a classifier to find out whether statements need a news citation or other kinds of citations ( web , book , journal , etc . ) . in the second step , </S>",
    "<S> we develop a news citation algorithm for wikipedia statements , which recommends appropriate citations from a given news collection . </S>",
    "<S> apart from ir techniques that use the statement to query the news collection , we also formalize three properties of an appropriate citation , namely : ( i ) the citation should entail the wikipedia statement , ( ii ) the statement should be central to the citation , and ( iii ) the citation should be from an authoritative source .    </S>",
    "<S> we perform an extensive evaluation of both steps , using 20 million articles from a real - world news collection . </S>",
    "<S> our results are quite promising , and show that we can perform this task with high precision and at scale .    </S>",
    "<S> [ 1]>p#1 </S>"
  ]
}