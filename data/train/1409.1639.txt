{
  "article_text": [
    "in data warehousing , etl technology is a collection of tools responsible for extracting , cleansing , customization , reformatting , integration and loading data from different sources into a data warehouse .",
    "dataflow is a term used in computing architectures where a number of computing units are organized logically in order to do the required computations in terms of user - defined rules @xcite .",
    "the concept of dataflow is introduced in the etl process to define the data movement and transformation logic from sources to a central data warehouse .",
    "figure  [ fig : etldataflow ] shows a typical etl dataflow in a data warehouse .",
    "it consists of the following four components : 1 ) data sources , e.g. , operational databases , text files and so on ; 2 ) data processing activities ; 3 ) the data warehouse ; and 4 ) the dependency of the activities which is denoted by the dashed arrows .        today",
    ", with the ever - growing volume of data , data warehousing is facing the increasing pressure to enhance their data processing capabilities in that the etl scenarios in many enterprises may take several hours or even days to complete .",
    "the delay in data processing might lead to in - accurate business decision - making .",
    "business intelligence ( bi ) developers therefore tend to improve their data warehousing system , e.g. , to optimize their etl dataflow which is one of the most commonly used techniques under existing it infrastructures .",
    "the dataflow optimization techniques include dataflow designing , caching , parallelizing , job scheduling and so on . with an optimized etl dataflow",
    ", the changes to data in source systems can be synchronized with the data warehouse in a timely manner . in the past few years",
    ", etl dataflow optimization has received a growing attention from the research community , such as @xcite . nevertheless , optimizing an etl dataflow is a non - trivial task as it involves multiple aforementioned aspects .",
    "for example , in order to find an optimal dataflow design , the search space could grow exponentially @xcite .",
    "most existing etl tools only provide the basic functionality for the optimization ( such as re - ordering of the activities in etl dataflow ) , however they do not provide more advanced features . in an etl dataflow , data is transfered and processed through a number of connected etl components . in order to explain the etl dataflow , we consider a simple scenario consisting of two connected components ( see a and b in figure  [ fig : separatecache ] ) .",
    "the component a processes the data and saves the results into an output cache .",
    "the results from the output cache are then transferred to an input cache of the component b by data copying operation . since",
    "copying involves physical data movement and memory synchronization , it is relatively expensive in the dataflow .",
    "furthermore , the activity thread of the component b might be inactive during the copying process , which may delay the data processing overall . in this paper , we propose the optimization framework that uses a shared cache to remedy the data copying operation . in term of the characteristics of the activities , the optimization framework partitions a dataflow into several subsets , each of which contains one or more components .",
    "the framework optimizes a subset by re - using a single cache to transfer the data , and processes the components in the cache in a sequential order . to reduce the processing time of the activities",
    ", the optimization framework makes use of the parallelization techniques including pipelining and multi - threading based on different granularities ( subset - level or component - level within a subset ) .",
    "this proposed technique greatly maximizes the usage of the computing resources for a dataflow , e.g. , running on a powerful machine .",
    "the proposed framework applies the optimization techniques at the transformation layer of etl , whilst it remains transparent to bi developers .    in this paper",
    ", we make the following contributions .",
    "first , we classify the components in an etl dataflow into three different categories based on their data processing characteristics : row - synchronized component , semi - block component and block component .",
    "second , we propose the partitioning algorithm to divide the dataflow based on these categories .",
    "the partitioning granularities ( subset - level or component - level within a subset ) provide the foundation to determine an appropriate parallelization method .",
    "third , the proposed framework provides the built - in support for selecting appropriate optimization techniques including pipelining and multi - threading at different granularities .",
    "fourth , we propose the concept of shared caching scheme ( used within multiple etl components ) to reduce memory footprint and the algorithm to estimate the optimal degree of parallelization . finally , we show the empirical evidence to verify the effectiveness of using the proposed optimization framework .",
    "the paper is organized as follows .",
    "section  [ sec : problem ] describes the problems and the optimization framework .",
    "section  [ sec : optanddataflow ] proposes the optimization caching scheme and categorizes the etl components in a dataflow .",
    "section  [ sec : parallelstrategies ] presents the dataflow partitioning and the parallelization methods used for the partitioned dataflows .",
    "section  [ sec : evaluation ] presents the implementation and the evaluation .",
    "section  [ sec : relatedwork ] discusses related work .",
    "section  [ sec : conclusionandfuturework ] concludes the paper and presents the future work .",
    "an etl dataflow is formalized as a directed acyclic graph ( dag ) @xcite , @xmath0 , where @xmath1 is a list of activities @xmath2 over the row set @xmath3 , i.e. , @xmath4 , and @xmath5 represents as the set of logical transitions from an activity to the other activities .",
    "[ fig : optframework ]    in the dataflow , the row sets @xmath3 go through the components , and are processed by the activity on each component , i.e. , does data transformation ( this resembles to the work flow in the assemble line of a factory ) . in the dataflow ,",
    "a cache is used to hold the data temporarily between two continuous neighbor components . in the entire dataflow",
    ", the caches could cause a lot of memory consumption for holding the intermediate data .",
    "moreover , since the data in a cache is passed to a downstream component as the input , it triggers the copying operation , which requires significant cpu resources .",
    "the proposed optimization framework remedies this by using shared caching scheme , where a single cache is re - used by the neighbor components .",
    "therefore , no cache copying is needed and the consumption of cpu resource for copying is also saved . in shared caching scheme , an etl dataflow is first partitioned based on the types of components ( we will discuss the partitioning in section  [ sec : parallelstrategies ] ) ,",
    "then shared caches are used in each of the partitions .",
    "the system architecture of the optimization framework is presented in figure  [ fig : optframework ] .",
    "it consists of the following five functional modules :    * _ dataflow designer _ :",
    "this module is used to design an etl dataflow with the toolkits offered by the system .",
    "the toolkits include the utilities for designing work such as update , edit , search and replace , etc . , and the components for building a dataflow including different type of data sources , targets , and the components for different etl transformation operators .",
    "the dataflow designer provides a graphic user interface in which programmers can conveniently implement a dataflow according to user requirements . * _ dataflow partitioner _ : this module is responsible for partitioning a dataflow based on the types of dataflow components .",
    "the optimization techniques are applied in each partition to improve the execution concurrency of the dataflow tasks . * _ dataflow task planner _ :",
    "this module is responsible for planning dataflow tasks automatically .",
    "when a dataflow is partitioned , the job for dataflow is thus generated into multiple tasks , and task planner will plan the execution order according to the dependency of the generated tasks . *",
    "_ dataflow component manager _ : this module is responsible for data processing and data source component management .",
    "it is in charge of component life cycle management and component specification management . *",
    "_ metadata store _ : the store is responsible for the metadata management .",
    "the metadata includes the schema information of data sources and data processing components , the specifications of etl dataflows and the information of job and task planning .",
    "metadata can be imported from or exported to xml files .",
    "this section illustrates the traditional approach of transferring data between two components shown as a and b in figure  [ fig : separatecache ] .",
    "if we use separate caches for the output and input , the extra memory and cpu resources are needed to transfer the data ( shown as ` copy ` ) . however",
    ", in some cases the cache can be re - used for optimization , even by a number of connected components .",
    "we call this as _ shared caching scheme _ ( see figure  [ fig : sharedcache ] which only shows two components for simplicity ) .",
    "before we apply the shared caching scheme , we first classify the components into the following three categories ( the types of the components are saved in the metadata store ) :    * * row - synchronized components : * this component performs row - based data processing , such as ` filter ` , ` lookup`,`splitter ` , ` data format converter ` , etc .",
    "it processes rows one after the other . in this component ,",
    "the shared caching scheme is applied to optimize the component activities using a single thread space ( see figure  [ fig : sharedcache ] ) .",
    "* * block components : * this component receives the rows from a single upstream component .",
    "data processing can not be started until all the rows have been received , e.g. , the component b in figure  [ fig : separatecache ]",
    ". the output of component a is added into the input cache by copying operation .",
    "the components in this category are mainly consists of aggregation operators , such as ` avg ` , , ` min ` and ` max ` , etc .",
    "each activity of a block component runs its own thread .",
    "block component has to accumulate all the rows that is why the block components are the least efficient . *",
    "* semi - block components : * this component receives the rows from multiple upstream components .",
    "some what similar to the previous component , data processing can not be started until all the rows that satisfy a certain condition have been received , e.g. , the component c in figure  [ fig : semiblockcomp ] . the rows from its upstream components are added into the input cache by copying operation .",
    "the semi - block components consist of ` union ` , ` merge ` and ` alike ` .",
    "in order to optimize , we first perform the vertical partitioning by dividing a dataflow into multiple subsets , or _ sub - dataflows _ , then we use parallelization technology in each of the subsets .",
    "when we do the partitioning , the partitioning granularity should not be either too coarse or too fine .",
    "if the granularity is too coarse , the execution concurrency of the sub - dataflows will become very low , which is not able to take full advantage of the computing resources , if running on a powerful machine . on the contrary , if the granularity is too fine , it will lead to a lot of concurrent threads , and cause overall performance loss for competing of the computing resources .",
    "in addition , data synchronization between two neighbor sub - dataflows could also lead to some overhead , and so does thread switching when multi - threading is used . based on these considerations",
    ", we therefore propose the _ 3-level _ partitioning method : first vertically partition a dataflow into multiple sub - dataflows , called _ execution trees _ ( the _ coarse - level _ ) , then horizontally partition the input of an execution tree ( _ medium - level _ ) , and finally do a further partitioning in the staggering components of an execution tree ( _ fine - level _ ) . for each partition with a different granularity",
    ", we choose an appropriate parallelization method to optimize it ( if applicable ) .",
    "an execution tree is a directed acyclic graph , defined as @xmath6 .",
    "it is a subgraph of the dataflow graph @xmath0 , where @xmath7 is a nonempty subset of @xmath1 and @xmath8 is a subset of @xmath5 . in @xmath9 ,",
    "the vertex with indegree equal to zero is the @xmath10 , and the vertex with zero outdegree is the leaf .",
    "suppose that we have done the coarse - level partitioning for the etl dataflow graph @xmath11 , which results in @xmath12 execution trees , @xmath13 @xmath14 .",
    "the generated execution trees also form a graph , defined as @xmath15 , where @xmath16 @xmath17 , and @xmath18 is the set of the edge of any two connected execution trees .",
    "@xmath19 is a directed acyclic graph .",
    "we now illustrate the coarse - level partitioning using the example shown in figure  [ fig : paritionexetree ] .",
    "the example depicts the etl dataflow of loading data from a single data source into two target tables through a number of etl transformation operators . as discussed earlier ( section  [ sec : optanddataflow ] ) that block and semi - block components require their own caches to accumulate the data before processing ,",
    "this dataflow can be partitioned into four execution trees , @xmath20 and @xmath21 , each of which starts from the root , such as a data source , semi - block or block component . in an execution tree , the output of the root is horizontally partitioned into multiple splits , each of which is hold in a shared cache , and passed to the downstream row - synchronized components .",
    "for any two connected execution trees , a new cache is needed , and the data is transfered to the new cache by copy , e.g. , @xmath22 , @xmath23 , and @xmath24 .",
    "algorithm [ alg : workflowpartitioning ] shows how to partition a dataflow .",
    "it takes a dataflow graph , @xmath11 , as the input , and returns an execution tree graph , @xmath19 , as the output .",
    "the algorithm does the partitioning using depth - first searching ( dfs ) , which starts from the data source components , i.e. , the vertexes whose in - degree is equal to zero in @xmath11 ( see line 69 ) .",
    "an execution tree is created taking a data source component as the _ root _ ( see line 7 ) .",
    "the algorithm then does the depth - first searching the downstream components , and adds them as the children of the tree .",
    "if a component is neither block type nor semi - block type , it will be regarded as a child added to the tree ( see line 15 ) ; otherwise , the searching for downstream components will finish",
    ". a new execution tree is created , which takes this block or semi - block component as the root ( see line 17 ) , and the tree is added into the execution tree graph , @xmath19 ( see line 1819 ) . at the end of the algorithm , the graph , @xmath19 , is returned , containing one or several partitioned execution trees ( the vertexes ) . for each of the execution trees , we optimize it using the shared caching scheme . all the threads for the activities use a single cache , thus data copying is not needed any more .",
    "@xmath25 @xmath26 @xmath27 \\leftarrow 0 $ ] @xmath28 @xmath29 @xmath30@xmath31 ( @xmath32 ) @xmath19",
    "@xmath27 \\gets 1 $ ] @xmath33@xmath34 @xmath35 @xmath34 @xmath30@xmath36 @xmath37@xmath38    @xmath39 \\gets 1 $ ] @xmath40    [ alg : workflowpartitioning ]      the output of the root of an execution tree is horizontally partitioned , and pipeline parallelization is used within the execution tree to process the partitions .",
    "we now give the formalization .",
    "the etl process of an execution tree is a 3-tuple @xmath41 where @xmath42 represents the input , @xmath43 represents the etl activities of the components , and @xmath44 represents the output of the execution tree ( see figure  [ fig : sharedcacheinexetree ] ) .",
    "we use the tuple , @xmath45 , to denote the activities of the components in an execution tree .",
    "the function @xmath46 is used to denote the activity of the first component , @xmath47 , where @xmath48 represents the input of the execution tree , and @xmath49 represents the output of @xmath47 .",
    "we horizontally partition @xmath49 into @xmath50 even splits ( the value of @xmath50 is configurable ) , i.e. , @xmath51 , where @xmath52 disjoints the subsets of @xmath49 . here",
    ", we use , @xmath53 , to denote the @xmath54th horizontal split as the input for the @xmath55th activity , and use @xmath56 to denote the number of rows for the split .",
    "the final output of an execution tree is @xmath57 where @xmath58 .",
    "an activity , @xmath59 , can be expressed as a function , i.e. , @xmath60 where @xmath61 and @xmath62 .",
    "thus , the activities @xmath63 @xmath64 can be formalized as @xmath65 , @xmath66 , which is made of the activity functions recursively .",
    "a shared cache is created to hold each of the splits , and the same cache will be pass through all the downstream activities , each of which runs a separate thread . therefore ,",
    "@xmath12 activity threads run in parallel in an execution tree ( see figure  [ fig : sharedcacheinexetree ] ) . in pipeline parallelization ,",
    "when a component has finished processing a shared cache , it passes it to the next immediately .",
    "algorithm  [ alg : pipelineparallelism ] describes the pipeline parallelization with a degree @xmath67 . in order to limit the memory usage , we set the value of @xmath68 .",
    "we exploit a fix - sized blocking queue , and a house keeping thread to maintain the maximal number of parallel threads , and the memory usage ( see line 14 - 15 ) .",
    "a shared cache is created to hold each of the splits from the first component ( of block or semi - block type , see line 1718 ) .",
    "the _ pipeline consumer thread _ is created to process the shared cache ( see line 1819 ) .",
    "the thread will be blocked if there is no space available in the queue .",
    "the shared cache is processed sequentially through all the activity threads ( see line 111 ) . for each component , it has to process @xmath50 shared caches in a sequential order . therefore ,",
    "if a component is processing a shared cache , and a pipeline consumer thread passes it a new shared cache , the pipeline consumer thread will be halted by ` wait ` ( see line 7 ) .",
    "it will not be waken up until the component has finished its processing ( by ` notifyall ` , see line 11 ) .",
    "when a shared cache is processed by the component , @xmath69 , the total processing time , denoted by @xmath70 , consists of the time for processing all rows , and miscellaneous time , denoted by @xmath71 .",
    "the miscellaneous time is consumed by any other necessary actions , such as create and clean temporary tables , and hand over a shared cache to other components , etc .",
    ", typically , @xmath72 .",
    "we call @xmath73 as net time for processing the rows in activity @xmath69 . therefore ,",
    "when a shared cache goes through all the @xmath12 sequential activities , the total time is @xmath74 ( see figure  [ fig : assemblyprocessing ] ) . for simplicity",
    ", we now consider when the @xmath50 splits can fit into the memory , and use the same number of pipelines to process the splits , i.e. , @xmath75 .",
    "suppose that the activity @xmath59 has the maximal time , i.e. , @xmath76 , according to the pipeline workflow algorithm in @xcite , the total time is @xmath77 .",
    "[ 2][unknown]*class * # 1    @xmath78 @xmath79 @xmath80 @xmath81 @xmath82 @xmath83    q @xmath84 new blockingqueue(@xmath67 ) new housekeepingthread(@xmath85).start ( ) @xmath86 create a new shared cache read a split of the output of the first component into @xmath87 @xmath88 * new * pipelineconsumerthread(@xmath89 ) @xmath90",
    "@xmath91    [ alg : pipelineparallelism ]    if we process all the shared caches in a sequential order , the total time , denoted by @xmath92 , will become @xmath93 .",
    "if we exclude the miscellaneous time of each activity , the total net time can be regarded as a constant value , i.e. , @xmath94 since the number of rows processed in each activity is a constant .",
    "therefore , @xmath95 can be represented as : @xmath96    for a fix - sized input of an execution tree , if the degree of pipeline parallelization is set to @xmath97 , the cost is minimal .",
    "_ suppose the total number of rows processed by the staggering activity , @xmath59 , is @xmath98 ( note that @xmath98 is not necessary to equal to the size of the execution tree input @xmath99",
    ". for example , the upstream component of @xmath59 could be a _ filter _",
    "operator for screening noisy data ) .",
    "the size of each horizontal split in @xmath59 is @xmath100 , and we assume that the time used is linear to the split size .",
    "@xmath101 can be represented as @xmath102 where @xmath103 is an coefficient of the split size .",
    "therefore , _    & t_p = + ( m-1)(t_0 + ) + nt_0 + & = + t_0 m + n + ( n-1)t_0 + & 2 + n + ( n-1)t_0    when @xmath104 , i.e. , @xmath97 .",
    "the total time @xmath95 has the minimal value @xmath105 .    from the discussion above",
    ", we can see the degree of parallelization is relevant to the size of the staggering component .",
    "but , it is obvious to know the range of the value , @xmath106 .",
    "if @xmath107 , it means that we pipeline for every single row .",
    "it has the highest degree of parallelization .",
    "however , this case will lead to frequent transferring of the shared caches between activity threads , which dominate the overall time . on the contrary , if @xmath108 , the etl workflow will degenerate to non - pipeline fashion ( or sequential execution ) since the input will be processed once for all through all the activities .",
    "to make an optimal optimization , we need to find staggering component , compute the degree of parallelization , then adjust all the activities .",
    "algorithm  [ alg : findoptimal ] describes how to find the staggering activity , e.g. , the grayed one @xmath59 in figure  [ fig : mosttimconsumingactivity ] , and compute the optimal degree of parallelization .    ,",
    "scaledwidth=40.0% ]    the sample data set @xmath109 ( with @xmath67 horizontal splits ) , and the number of activities in an execution tree , @xmath12 .",
    "measure the total miscellaneous time , @xmath110 .",
    "run an execution tree in non - pipeline fashion to process @xmath67 splits , record the time of each activities , and measure the total processing time @xmath92 . find the staggering activity , @xmath59 , compute the constant @xmath111 , and the average miscellaneous time , @xmath112 .",
    "run an execution tree in pipeline parallelization , and process the @xmath67 splits , and compute the coefficient @xmath103 .    according the formula in _ theorem 1 _ ,",
    "compute the degree of parallelization , @xmath50 .",
    "[ alg : findoptimal ]    in this algorithm , we first measure the total time without giving any input to an execution tree .",
    "this is to approximate the miscellaneous time when processing data , such as the time for initializing and thread switching , etc .",
    "( see line  1 ) .",
    "we , then , use the sample data ( random selection on rows from the data to be processed ) as the input to execute the dataflow in non - pipeline and pipeline fashion , respectively , and finally compute the value of @xmath50 according to the formula ( line  25 ) .",
    "it refers to using multi - threading technology to parallelize data processing in a heavy - computation component of the dataflow .    in an execution tree ,",
    "the computation task of some components is much heavier than others , and forms the bottleneck .",
    "we make the further optimization of using multi - threading to parallelize data processing in a component .",
    "multi - threading is applied to the non - block type components , e.g. , the ` filter rows ` component in @xmath113 of figure  [ fig : paritionexetree ] .",
    "figure  [ fig : insidecompparallel ] shows the scenario . for a shared cache , @xmath114 , delivered to it ,",
    "the system firstly divides the rows in @xmath114 evenly into multiple splits , i.e. , @xmath115 .",
    "then , the component spawns a number of threads to process the @xmath12 splits in parallel ( the value of @xmath12 can be set in the system configuration file ) , each of which results in an output @xmath116 . finally , the outputs @xmath117 are merged by the ` row order synchronizer ` , which maintains the row order of the output to be the same of the input ( in some cases such as the activities ` sort - filter - merge ` , the input and output row order of ` filter ` should not be changed since the downstream activity is ` merge ` ) .",
    "the merged rows are saved into another shared cache @xmath118 , and delivered to the downstream component .",
    "we implement the optimization framework based on the open source etl tool , talend @xcite , also known as talend open studio for data integration with an intuitive , graphical , drag and drop design environment .",
    "we add the extension , _ dataflow partitioner _",
    ", to the transformation engine , and make it support the shared caching and pipeline parallelization .",
    "the degree of pipeline parallelization of execution trees can be configured in the configuration file . for the non - block type components including the ` row filter , splitter , exploder , expression ` , ` lookup ` , etc .",
    ", they are extended to support the inside - component parallelization using multi - threading .",
    "the number of threads is also configurable .",
    "if the number is not set , the system uses one as the default value , which means that the inside - component parallelization is disabled .",
    "in addition , xml is used as the metadata repository ( rdbms could also be configured to use ) , and the repository manager is extended to support managing the partitioning information of a dataflow .    the host of the experiments is a hp proliant dl380 g5 server with two intel xeon quad - core processors ( 2.4ghz ) , 8 gb ram and a sata hard disk ( 350 gb , 3 gb / s , 16 mb cache and 7200 rpm ) , running ubuntu 12.04 lts with 64bit linux 2.6.32 kernel .",
    "jvm 1.6.0.29 , and the option `` -xms2500 m -xmx6000 m '' are used to run the program .",
    "postgresql 8.4 is used as the data warehouse dbms with the settings  shared_buffers=512 mb , temp_buffers= 128 mb , work_mem=56 mb , checkpoint_segments=20 \" and default values for other configuration parameters .",
    "we use the star schema benchmark ( ssb ) based on tpc - h @xcite for the evaluation .",
    "the star schema consists of one fact table , ` lineorder ` , and four dimension tables including ` customer ` , ` part ` , ` supplier ` and ` date ` ( see @xcite for the detailed schema information ) .",
    "in the experiments , we use the fix - sized data sets for the dimension tables : ` customer ` ( 150,000 rows , 725 m ) , ` part ` ( 24,000 rows , 323 m ) , ` supplier ` ( 231,000 rows , 150 m ) and ` date ` ( 10,000 rows , 134k ) , but vary the size of fact data . in all tests , we first load the data sets into the data warehouse , then execute the dataflow corresponding to a ssb query , and finally write the query results into a text file .",
    "the evaluation divides into two parts : the evaluation of the proposed optimization techniques , and comparing with other etl tools .",
    "we first use the query , q4.1 ( see the script bellow ) to evaluate the optimization framework itself .",
    ".... select d_year ,          c_nation ,          sum(lo_revenue - lo_supplycost ) as profit   from date , customer , supplier , part , lineorder   where lo_custkey = c_custkey and lo_suppkey = s_suppkey and        lo_partkey = p_partkey and lo_orderdate = d_datekey and        c_region = ' america ' and s_region = ' america ' and        ( p_mfgr = ' mfgr#1 ' or p_mfgr = ' mfgr#2 ' )   group by d_year , c_nation   order by d_year , c_nation   ....    figure  [ fig : ssdexecutiontree ] shows the corresponding dataflow , which is partitioned into three execution trees using algorithm  [ alg : workflowpartitioning ] .",
    "the dataflow reads the rows from the fact table ` lineorder ` , then joined with the four dimension tables respectively by ` lookup ` .",
    "the join conditions are ` lo_custkey = c_custkey and c_region = america ` , ` lo_suppkey = s_suppkey and s_region = america ` , ` lo_partkey = p_partkey and ( p_mfgr=mfgr#1 or p_mfgr=mfgr#2 ) ` , and ` lo_orderdate = d_datekey ` .",
    "component 6 filters the rows that fail to join with any of the dimensions ( whose returned key values is equal to the default value , -1 ) . component 7 selects the necessary field values for the query by projection .",
    "component 8 computes the value of the expression ` ( lo_revenue - lo_supplycost ) ` of each row .",
    "component 9 groups the computed values by ` sum ` operator , then component 10 sorts the values in each group , and component 11 writes the final results into a text file .",
    "the partitioned dataflow comprises three execution trees , @xmath119 and @xmath120 .",
    "@xmath113 consists of eight components , whilst @xmath121 and @xmath120 contain one and two components , respectively .",
    "@xmath113 has four ` lookup`s , which are the relatively expensive operators in terms of the computing resource consumptions .",
    "we first evaluate the speedup when the pipeline parallelization is applied to @xmath113 .",
    "we execute the dataflow when the fact table is loaded with 2 , 4 , and 8 gb data sets , respectively .",
    "figure  [ fig : speedup_pipeline ] shows the experimental results .",
    "the speedup value is computed by dividing the sequential execution time ( no pipeline ) by the time of using pipelines . as shown ,",
    "for each data set the speedup scales nearly linear when the number of the pipelines is less than eight , but the speedup decreases dramatically when more pipelines are added .",
    "it is because more threads have been generated when the degree of pipelines is increased .",
    "this causes by cpu bound ( we show this in the next experiment ) . from figure",
    "[ fig : speedup_pipeline ] , we also see that the speedup is more significant to a bigger sized data set .",
    "the optimal number of pipelines is about eight , where the speedups are 4.7x , 3.9x and 3.7x for the fact table loaded 2 , 4 , and 8 gb data , respectively .",
    "figure  [ fig : cpu_threads ] shows the cpu usage of executing the dataflow when the fact table is loaded 8 gb data .",
    "we vary the number of cores by booting the system with the kernel option @xmath122 where @xmath123 and 8 . as shown , when the degree of pipelines increases , the cpu usage grows as well , due to the increased number of threads .",
    "we now evaluate the inside - component parallelization using multi - threading . for testing purpose",
    ", we remove the index on the attribute ` s_nation ` of the ` supplier ` table , and also disable the pipeline parallelization .",
    "this leads to the lookup operation on the ` supplier ` table be the bottleneck of the whole dataflow .",
    "we execute the dataflow when the fact table is loaded with 4 gb data , and the system is configured with 2 , 4 , 6 and 8 cores .",
    "fig  [ fig : scalepipelinedegree ] shows the speedup lines when the number of the threads used by the lookup component of ` supplier ` is scaled from 1 to 16 .",
    "the speedup of each test grows at the beginning but decreases when the number of threads is increased .",
    "the performance deterioration is also due to the computing resource competition for over threading .",
    "the results also indicate that the speedup is better if multi - core is used .",
    "we now measure the performance improvement after applying the proposed optimization techniques .",
    "we first test the sequential execution with and without using shared caching scheme , then test the pipeline parallelization .",
    "the number of pipelines is set to eight , which showed the best performance in the previous experiment .",
    "the size of fact data is scaled from 1 to 8 gb , and figure  [ fig : total_optimized ] shows the results . as shown , the sequential execution of using shared caching scheme has made @xmath124 performance improvement , due to the removal of data copying operation in the execution trees .",
    "thus , it was more efficient . when the pipeline parallelization is enabled , the execution is 4.7x and 3.9x faster than the sequential executions without and with using the shared caching scheme , respectively .",
    "we now compare the optimization framework ( abbreviated as _ opt . frm . _ ) with the open source etl tool , kettle @xcite .",
    "we select the first query in each of the four ssb query categories ( note that the queries in each category only differentiate in the query conditions ) , denoted by q1 , q2 , q3 and q4 . we run the test with the fact table loaded 8 gb data set",
    ". kettle also supports multi - threading parallelization within a component . to make the comparison fair , we use the same number of threads and the same step for both tools ( eight threads for each query ) .",
    "we first compare the sequential execution when the multi - threading parallelization is enabled .",
    "the results in figure  [ fig : compared_others_seq ] have shown the proposed optimization framework outperforms kettle for all the four queries , due to the use of shared caching scheme .",
    "we now compare the pipeline parallelization to kettle .",
    "since kettle does not provide the built - in supports for pipeline parallelization as ours , we have to make the workaround , i.e. , for the connected row - synchronized components we horizontally split the dataflow into multiple parallel sub - flows by a splittor , then merge the results of all the sub - flows .",
    "the number of sub - flows is set to the same size of the pipelines of our framework ( also set to eight ) .",
    "the results in figure  [ fig : compared_others_parallel ] provide the further evidence that the pipeline parallelization for partitioned dataflows is effective .",
    "the optimization of etl process is of particular importance , since etl processes have to complete their tasks within specific time frames @xcite .",
    "some research works are found to optimize the etl process .",
    "simitsis et al .",
    "propose the theoretical framework @xcite that formalizes an etl scenario as a directed acyclic graph ( dag ) , and handles etl optimization as a state - space problem .",
    "when given an etl scenario , the proposed framework can find an equivalent logical scenario that has the best execution time by searching the state space .",
    "this formalized dag provides the theoretical foundation to the dataflow partitioning of our work .",
    "tziovara et al .",
    "propose the approach @xcite of producing the physical scenario with a logical etl template as the input .",
    "but , unfortunately the the above research works do not have any open source implementations available for our comparison .",
    "li and zhan determine the critical path of a workflow with regard to the execution time .",
    "they analyzes the dependencies of tasks in a workflow and optimize the tasks without dependencies using parallelization @xcite .",
    "behrend and jrg use rule - based optimization for the incremental etl flows @xcite .",
    "the rule - based optimization rewrites the rules with regards to the algebraic equivalences , however the rules need to be hard - coded during initial deployment @xcite .",
    "the qox - driven approach @xcite is proposed to deal with the quality and the optimization objectives of etl design . in comparison to the works above",
    ", we use the partitioning techniques based on the component types ( in addition to parallelization ) in order to optimize etl dataflow . to the best of our knowledge ,",
    "this is the first work to optimize etl technology based on the partitioning techniques with respect to the component types existing in etl dataflow . in the market",
    "a plethora of commercial etl tools exist , such as ibm s data warehouse manager @xcite , informatica s powercenter @xcite , microsoft s data transformation services @xcite and oracle s warehouse builder @xcite .",
    "the commercial tools provide the advanced gui to ease etl design , however most of them only provide the black - box optimization at logical level , e.g. , re - ordering of the activities in the etl dataflow .",
    "some etl engines such as powercenter @xcite support ",
    "push down \" optimization , which pushes the operators that can be expressed in sql from the etl flow down to the source or target dbms engine @xcite .",
    "the remaining transformations are executed in the etl data integration server .",
    "in contrast , our framework optimizes an etl dataflow at three levels of granularity and uses pipelining and multi - threading based parallelization .",
    "this technique takes full advantage of computing resources , if the dataflow runs on a powerful machine .",
    "one of the latest trends of etl technologies is handling big data .",
    "one answer to this is pig @xcite , an open source data flow system for analyzing large data sets @xcite .",
    "pig offers the sql - style programming language , pig latin , which provides data manipulation constructs assembled in a dataflow .",
    "another similar system is hive @xcite , built on top of hadoop and also offering sql - style language , hiveql . in both systems ,",
    "the programs are compiled into sequences of mapreduce jobs , and executed in the hadoop mapreduce environment . since they are designed to analyze big data for that reason they only have a limited etl capabilities , somewhat like dbmss rather than full - fledged etl tools . to complement this , liu et al .",
    "propose a scalable dimensional etl programming framework , etlmr @xcite using mapreduce , extended from @xcite .",
    "etlmr aims at the traditional rdbms - based data warehousing system .",
    "furthermore , a dimensional etl , cloudetl @xcite , is implemented for hive as the warehouse system .",
    "these two above mentioned etl frameworks provide the built - in support for different data warehouse schemas including star , snowflake , and slowly changing dimensions .",
    "only a few lines of code are needed to create a parallel etl program @xcite . as opposed to the cloud - based etl",
    ", the proposed framework aims at improving the efficiency of traditional etl processes that typical run on a single server , whilst the cloud - based etl improve the scalability , scaling out to many commodity servers and aims at dealing with the big data for today s data warehousing .",
    "the cloud - based etl could be extended by using the proposed optimization techniques to improve the performance .",
    "we beleive that the use of the computing resource on each node could also be maximized by using the proposed optimization techniques , shared caching and execution tree parallelization .",
    "in order to improve the efficiency of an etl dataflow , optimization is one of commonly used techniques , however , it is a non - trivial task due to the complexity of etl technologies . in this paper",
    ", we have proposed a framework to optimize etl dataflow .",
    "the proposed framework first classifies the components based on their characteristics in the etl dataflow , after that it partitions the dataflow at different granularities that are the foundations for the optimization .",
    "subsequently , we have proposed the concept of shared caching scheme .",
    "the use of shared cache not only minimizes the memory footprint of etl dataflow but also reduces the cpu consumption to copy data , which is one of most frequently used operations in a dataflow .",
    "furthermore , we have also presented the techniques to optimize the dataflow at different granularities by making use of parallelization technologies , such as pipelining in execution trees and multi - threading at staggering components .",
    "in addition , we have proposed the formula and algorithm to estimate the optimal degree of pipelines in terms of the cost .",
    "finally , we have evaluated the proposed optimization framework comprehensively .",
    "the results show that the proposed framework is 4.7 times faster than the ordinary etl dataflows ( without using the proposed optimization techniques ) , and outperforms the similar tool ( kettle ) .    in the future",
    ", the proposed optimization framework could be extended in several different directions .",
    "first , the self - adapt configuration is desirable such that system can automatically configure the number of pipelines needed , based on the statistics data of the previous runs .",
    "second , it is interesting to add a cost model to the system , based on the cost model the system generates the optimal execution plan for etl dataflow .",
    "third , due to the growing interest of moving etl to the cloud platform today , it is also interesting to improve the system so that it could be deployed in a clustering environment , whereas , the proposed optimization methods are used to take full advantage of the computing resources at each node .",
    "gates , a. , natkovich , o. , chopra , s. , kamath , p. , narayanam , s. , olston , c. , reed , b. , srinivasan , s. , and srivastava , u. : building a high level dataow system on top of mapreduce : the pig experience .",
    "pvldb , 2(2):14141425 , 2009 ."
  ],
  "abstract_text": [
    "<S> extract - transform - load ( etl ) handles large amount of data and manages workload through dataflows . </S>",
    "<S> etl dataflows are widely regarded as complex and expensive operations in terms of time and system resources . in order to minimize the time and the resources required by etl dataflows </S>",
    "<S> , this paper presents a framework to optimize dataflows using shared cache and parallelization techniques . </S>",
    "<S> the framework classifies the components in an etl dataflow into different categories based on their data operation properties . </S>",
    "<S> the framework then partitions the dataflow based on the classification at different granularities . </S>",
    "<S> furthermore , the framework applies optimization techniques such as cache re - using , pipelining and multi - threading to the already - partitioned dataflows . </S>",
    "<S> the proposed techniques reduce system memory footprint and the frequency of copying data between different components , and also take full advantage of the computing power of multi - core processors . </S>",
    "<S> the experimental results show that the proposed optimization framework is 4.7 times faster than the ordinary etl dataflows ( without using the proposed optimization techniques ) , and outperforms the similar tool ( kettle ) . </S>"
  ]
}