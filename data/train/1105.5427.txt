{
  "article_text": [
    "large - scale convex optimization problems appear in many areas of science such as graph theory , networks , transportation , distributed model predictive control , distributed estimation and multistage stochastic optimization @xcite .",
    "solving large - scale optimization problems is still a challenge in many applications @xcite . over the years , thanks to the development of parallel and distributed computer systems , the chances for solving large - scale problems have been increased .",
    "however , methods and algorithms for solving this type of problems are limited @xcite .",
    "convex minimization problems with a separable objective function form a class of problems which is relevant in many applications .",
    "this class of problems is also known as separable convex minimization problems , see , e.g. @xcite . without loss of generality",
    ", a separable convex optimization problem can be written in the form of a convex program with separable objective function and coupled linear constraints @xcite .",
    "in addition , decoupling convex constraints may also be considered .",
    "mathematically , this problem can be formulated in the following form : @xmath2 where @xmath3 is convex , @xmath4 is a nonempty , closed convex set , @xmath5 , @xmath6 for all @xmath7 , and @xmath8 .",
    "the last constraint is called _ coupling linear constraint_. in principle , many convex problems can be written in this separable form by doubling the variables , i.e. introducing new variables @xmath9 and imposing the constraint @xmath10 . despite the increased number of variables , treating convex problems by doubling variables may be useful in some situations , see , e.g. @xcite .    in the literature ,",
    "numerous approaches have been proposed for solving problem .",
    "for example , ( augmented ) lagrangian relaxation and subgradient methods of multipliers @xcite , fenchel s dual decomposition @xcite , alternating linearization @xcite , proximal point - type methods @xcite , interior point methods @xcite , mean value cross decomposition @xcite and partial inverse method @xcite among many others have been proposed .",
    "our motivation in this paper is to develop a numerical algorithm for solving which can be implemented in a parallel or distributed fashion .",
    "note that the approach presented in the present paper is different from splitting methods and alternating methods considered in the literature , see , e.g. @xcite .",
    "one of the classical approaches for solving is lagrangian dual decomposition .",
    "the main idea of this approach is to solve the dual problem by means of a subgradient method .",
    "it has been recognized in practice that subgradient methods are usually slow and numerically sensitive to the step size parameters . in the special case of a strongly convex objective function",
    ", the dual function is differentiable .",
    "consequently , gradient schemes can be applied to solve the dual problem .",
    "recently , nesterov @xcite developed smoothing techniques for solving nonsmooth convex optimization problems based on the fast gradient scheme which was introduced in his early work @xcite .",
    "the fast gradient schemes have been used in numerous applications including image processing , compressed sensing , networks and system identification @xcite .",
    "exploiting nesterov s idea in @xcite , necoara and suykens @xcite applied a smoothing technique to the dual problem in the framework of lagrangian dual decomposition and then used the fast gradient scheme to maximize the smoothed function of the dual problem .",
    "this resulted in a new variant of dual decomposition algorithms for solving separable convex optimization .",
    "the authors proved that the rate of convergence of their algorithm is @xmath0 which is much better than @xmath11 in the subgradient methods of multipliers , where @xmath1 is the iteration counter .",
    "a main disadvantage of this scheme is that the smoothness parameter requires to be given _",
    "a priori_. moreover , this parameter crucially depends on the given desired accuracy .",
    "since the lipschitz constant of the gradient of the objective function in the dual problem is inversely proportional to the smoothness parameter , the algorithm usually generates short steps towards a solution of the problem although the rate of convergence is @xmath0 .    to overcome this drawback , in this paper",
    ", we propose a new algorithm which combines three techniques : smoothing @xcite , excessive gap @xcite and lagrangian dual decomposition @xcite techniques .",
    "instead of fixing the smoothness parameters , we update them dynamically at every iteration . even though the worst case complexity is @xmath12 , where @xmath13 is a given tolerance , the algorithms developed in this paper work better than the one in @xcite and are more numerically robust in practice .",
    "note that the computational cost of the proposed algorithms remains almost the same as in the proximal - center - based decomposition algorithm proposed in ( * ? ? ?",
    "* algorithm 3.2 ) .",
    "( algorithm 3.2 in @xcite requires to compute an additional dual step ) .",
    "this algorithm is called ( algorithm [ alg : a1 ] ) .",
    "alternatively , we apply the switching strategy of @xcite to obtain a decomposition algorithm with switching primal - dual update for solving problem .",
    "this algorithm differs from the one in @xcite at two points .",
    "first , the smoothness parameter is dynamically updated with an exact formula and second the proximal - based mappings are used to handle the nonsmoothness of the objective function .",
    "the second point is more significant since , in practice , estimating the lipschitz constants is not an easy task even if the objective function is differentiable .",
    "the switching algorithm balances the disadvantage of the decomposition methods using the primal update ( algorithm [ alg : a1 ] ) and the dual update ( algorithm 3.2 @xcite ) .",
    "proximal - based mapping only plays a role of handling the nonsmoothness of the objective function .",
    "therefore , the algorithms developed in this paper do not belong to any proximal - point algorithm class considered in the literature .",
    "note also that all algorithms developed in this paper are first order methods which can be highly distributed .",
    "0.1 cm * contribution .",
    "* the contribution of this paper is the following :    1 .",
    "we apply the lagrangian relaxation , smoothing and excessive gap techniques to large - scale separable convex optimization problems which are not necessarily smooth .",
    "note that the excessive gap condition that we use in this paper is different from the one in @xcite , where not only the duality gap is measured but also the feasibility gap is used in the framework of constrained optimization , see . 2 .",
    "we propose two algorithms for solving general separable convex optimization problems .",
    "the first algorithm is new , while the second one is a new variant of the first algorithm proposed in ( * ? ? ?",
    "* algorithm 1 ) applied to lagrangian dual decomposition . a special case of the algorithms , where the objective is strongly convex is considered .",
    "all the algorithms are highly parallelizable and distributed .",
    "the convergence of the algorithms is proved and the rate of convergence is estimated .",
    "implementation details are discussed and a theoretical and numerical comparison is made .",
    "the rest of this paper is organized as follows . in the next section ,",
    "we briefly describe the lagrangian dual decomposition method @xcite for separable convex optimization , the smoothing technique via prox - functions as well as excessive gap techniques @xcite .",
    "we also provide several technical lemmas which will be used in the sequel .",
    "section [ sec : decom_primal_acceleration ] presents a new algorithm called _ decomposition algorithm with primal update _ and estimates its worst - case complexity .",
    "section [ sec : decom_primal_dual_acceleration ] is a combination of the primal and the dual step update schemes which is called _ decomposition algorithm with primal - dual update_. section [ sec : strongly_convex_case ] is an application of the dual scheme to the strongly convex case of problem .",
    "we also discuss the implementation issues of the proposed algorithms and a theoretical comparison of algorithms [ alg : a1 ] and [ alg : a2 ] in section [ sec : discussion ] .",
    "numerical examples are presented in section [ sec : num_results ] to examine the performance of the proposed algorithms and to compare different methods .    0.1 cm * notation .",
    "* throughout the paper , we shall consider the euclidean space @xmath14 endowed with an inner product @xmath15 for @xmath16 and the norm @xmath17 .",
    "associated with @xmath18 , @xmath19 defines its dual norm .",
    "for simplicity of discussion , we use the euclidean norm in the whole paper .",
    "hence , @xmath20 is equivalent to @xmath18 .",
    "the notation @xmath21 represents a column vector in @xmath14 , where @xmath22 is a subvector in @xmath23 , @xmath7 and @xmath24 .",
    "a classical technique to address coupling constraints in optimization is lagrangian relaxation @xcite .",
    "however , this technique often leads to a nonsmooth optimization problem in the dual form . to overcome this situation , we combine the lagrangian dual decomposition and smoothing technique in @xcite to obtain a smoothly approximate dual problem .    for simplicity of discussion",
    ", we consider problem with @xmath25 . however , the methods presented in the next sections can be directly applied to the case @xmath26 ( see section [ sec : discussion ] ) .",
    "the problem can be rewritten as follows : @xmath27 where @xmath28 is convex , @xmath29 is a nonempty , closed , convex and bounded subset in @xmath23 , @xmath5 and @xmath6 ( @xmath30 ) .",
    "problem is said to satisfy the slater constraint qualification condition if @xmath31 , where @xmath32 is the relative interior of the convex set @xmath33 .",
    "let us denote by @xmath34 the solution set of this problem . throughout the paper",
    ", we assume that :    [ as : a1 ] the solution set @xmath34 is nonempty and either the slater qualification condition for problem holds or @xmath29 is polyhedral .",
    "the function @xmath35 is proper , lower semicontinuous and convex in @xmath14 , @xmath30 .    since @xmath33 is convex and bounded , @xmath34 is also convex and bounded .",
    "note that the objective function @xmath36 is not necessarily smooth .",
    "for example , @xmath37 , which is is nonsmooth and separable .",
    "let us define the lagrange function of the problem with respect to the coupling constraint @xmath38 as : @xmath39 where @xmath40 is the multiplier associated with the coupling constraint @xmath38 .",
    "a triplet @xmath41 is called a saddle point of @xmath42 if : @xmath43 next , we define the lagrange dual function @xmath44 of the problem as : @xmath45 and then write down the dual problem of : @xmath46 let @xmath47 $ ] . due to assumption",
    "a.[as : a1 ] _ strong duality _ holds and we have : @xmath48 let us denote by @xmath49 the solution set of the dual problem .",
    "it is well - known that @xmath49 is bounded due to assumption a.[as : a1 ] .",
    "now , let us consider the dual function @xmath44 defined by .",
    "it is important to note that the dual function @xmath50 can be computed separately as : @xmath51 where @xmath52 we denote by @xmath53 a solution of the minimization problem in ( @xmath30 ) and @xmath54 .",
    "since @xmath35 is continuous and @xmath29 is closed and bounded , this problem has a solution .",
    "note that if @xmath53 is not uniques for a given @xmath55 then @xmath56 is not differentiable at the point @xmath55 ( @xmath30 ) .",
    "consequently , @xmath44 is not differentiable at @xmath55 .",
    "the representation - is called a _ dual decomposition _ of the dual function @xmath44 .      by assumption that @xmath29 is bounded , instead of considering the nonsmooth function @xmath44 , we smooth the dual function @xmath44 by means of prox - functions . a function @xmath57 is called a proximity function ( prox - function ) of a given nonempty , closed and bounded convex set @xmath58 if @xmath57 is continuous , strongly convex with convexity parameter @xmath59 and @xmath60 .",
    "suppose that @xmath57 is a prox - function of @xmath29 and @xmath59 is its convexity parameter ( @xmath30 ) .",
    "let us consider the following functions : @xmath61 here , @xmath62 is a given parameter called smoothness parameter .",
    "we denote by @xmath63 the solution of , i.e. : @xmath64 note that it is possible to use different parameters @xmath65 for ( @xmath30 ) .",
    "let @xmath66 be the prox - center of @xmath29 which is defined as : @xmath67 without loss of generality , we can assume that @xmath68 . since @xmath29 is bounded , the quantity @xmath69 is well - defined and @xmath70 for @xmath30 .",
    "the following lemma shows the main properties of @xmath71 , whose proof can be found , e.g. , in @xcite .",
    "[ le : smoothing_estimate ] for any @xmath72 , the function @xmath73 defined by is well - defined and continuously differentiable on @xmath74 .",
    "moreover , this function is concave and its gradient w.r.t @xmath55 is given as : @xmath75 which is lipschitz continuous with a lipschitz constant @xmath76 ( @xmath30 ) .",
    "the following estimates hold : @xmath77 consequently , the function @xmath78 defined by is concave and differentiable and its gradient is given by @xmath79 which is lipschitz continuous with a lipschitz constant @xmath80 .",
    "moreover , it holds that : @xmath81    the inequalities show that @xmath71 is an approximation of @xmath44 .",
    "moreover , @xmath71 converges to @xmath44 as @xmath82 tends to zero .",
    "[ re : bound_xy ] even without the assumption that @xmath33 is bounded , if the solution set @xmath34 of is bounded then , in principle , we can bound the feasible set @xmath33 by a large compact set which contains all the sampling points generated by the algorithms ( see section [ sec : decom_primal_dual_acceleration ] below ) .",
    "however , in the following algorithms we do not use @xmath83 , @xmath30 ( defined by ) in any computational step .",
    "they only appear in the theoretical complexity estimates .",
    "next , for a given @xmath84 , we define a mapping @xmath85 from @xmath33 to @xmath86 by : @xmath87 this function can be considered as an approximate version of @xmath88 using the prox - function @xmath89 .",
    "it is easy to show that the unique solution of the maximization problem in is given explicitly as @xmath90 and @xmath91 .",
    "therefore , @xmath85 is well - defined and differentiable on @xmath33 .",
    "let @xmath92 the next lemma summarizes the properties of @xmath85 .",
    "[ le : lipschitz_diff_psi ] for any @xmath84 , the function @xmath93 defined by is continuously differentiable on @xmath33 and its gradient is given by : @xmath94 which is lipschitz continuous with a lipschitz constant @xmath95 .",
    "moreover , the following estimate holds for all @xmath96 : @xmath97\\\\[-1.5ex ] & & + \\frac{l_1^{\\psi}(\\beta_2)}{2}{\\|x_{1 } \\!-\\",
    "! \\hat{x}_{1}\\|}^2 \\!+\\ !",
    "\\frac{l_2^{\\psi}(\\beta_2)}{2}{\\|x_{2 } \\!-\\ !",
    "\\hat{x}_{2}\\|}^2 , \\nonumber\\end{aligned}\\ ] ] where @xmath98 and @xmath99 .",
    "since @xmath100 by the definition and @xmath101 , it is easy to compute directly @xmath102",
    ". moreover , we have : @xmath103\\\\[-1.5ex ] & & \\leq \\frac{1}{\\beta_2}{\\|a_{1}\\|}^2{\\|x_{1 } - \\hat{x}_{1}\\|}^2 + \\frac{1}{\\beta_2}{\\|a_{2}\\|}^2{\\|x_{2 } - \\hat{x}_{2}\\|}^2.\\nonumber\\end{aligned}\\ ] ] this inequality is indeed .",
    "@xmath104    from the definition of @xmath105 , we obtain : @xmath106 note that @xmath105 is an upper bound of @xmath107 instead of a lower bound as in @xcite .",
    "note that the lipschitz constants in are roughly estimated .",
    "these quantities can be quantified carefully by taking into account the problem structure to trade - off the computational effort in each component subproblem .      since the primal - dual gap of the primal and dual problems - is measured by @xmath108 ,",
    "if the gap @xmath109 is equal to zero for some feasible point @xmath110 and @xmath55 then this point is an optimal solution of - . in this section ,",
    "we apply to the lagrangian dual decomposition framework a technique called _ excessive gap _ proposed by nesterov in @xcite .",
    "let us consider @xmath111 .",
    "it follows from and that @xmath112 is an underestimate of @xmath113 , while @xmath105 is an overestimate of @xmath107 . therefore , @xmath114 . let us recall the following excessive gap condition introduced in @xcite .",
    "[ de : excessive_gap ] we say that a point @xmath115 satisfies the _ excessive gap _ condition with respect to two smoothness parameters @xmath72 and @xmath84 if : @xmath116 where @xmath117 and @xmath78 are defined by and , respectively .",
    "the following lemma provides an upper bound estimate for the duality gap and the feasibility gap of problem .",
    "[ le : excessive_gap ] suppose that @xmath118 satisfies the excessive gap condition .",
    "then for any @xmath119 , we have : @xmath120 \\textrm{and } ~~ & & \\nonumber\\\\ [ -1.5ex ] & { \\|a\\bar{x } - b\\| } & \\leq \\beta_2\\left[{\\|y^{*}\\| } + \\sqrt{{\\|y^{*}\\|}^2+\\frac{2\\beta_1}{\\beta_2}(d_1 + d_2)}\\right ] .",
    "\\label{eq : feasibility}\\end{aligned}\\ ] ]    suppose that @xmath121 and @xmath122 satisfy condition . for a given @xmath123 , one has : @xmath124 which implies the first inequality of . by using lemma [ le : smoothing_estimate ] and we have : @xmath125 now , by substituting the condition into this inequality , we obtain the second inequality of .",
    "let @xmath126 .",
    "it follows from that @xmath127 .",
    "the estimate follows from this inequality after few simple calculations . @xmath104",
    "in this section , we derive an iterative decomposition algorithm for solving based on the excessive gap technique .",
    "this method is called a _ decomposition algorithm with primal update_. the aim is to generate a point @xmath115 at each iteration such that this point maintains the excessive gap condition while the algorithm drives the parameters @xmath82 and @xmath128 to zero .      as assumed earlier ,",
    "the function @xmath35 is convex but not necessarily differentiable .",
    "therefore , we can not use the gradient information of these functions .",
    "we consider the following mappings ( @xmath30 ) : @xmath129 where @xmath130 .",
    "since @xmath131 defined in lemma [ le : lipschitz_diff_psi ] is positive , @xmath132 is well - defined .",
    "this mapping is called _ proximal operator _ @xcite .",
    "let @xmath133 .",
    "first , we state that the excessive gap condition is well - defined by showing that there exists a point @xmath134 that satisfies .",
    "this point will be used as a starting point in algorithm [ alg : a1 ] described below .",
    "[ le : intial_point ] suppose that @xmath135 is the prox - center of @xmath33 .",
    "for a given @xmath84 , let us define : @xmath136 if the parameter @xmath82 is chosen such that : @xmath137 then @xmath138 , @xmath139 satisfies the excessive gap condition .",
    "the proof of lemma [ le : intial_point ] can be found in the appendix .",
    "suppose that @xmath115 satisfies the excessive gap condition .",
    "we generate a new point @xmath140 and by applying the following update scheme : @xmath141 where @xmath142 and @xmath143 will be chosen appropriately .",
    "[ re : parallel ] in the scheme , the points @xmath144 , @xmath145 and @xmath146 can be computed _ in parallel_. to compute @xmath147 and @xmath148 we need to solve the corresponding convex programs in @xmath149 and @xmath150 , respectively .",
    "the following theorem shows that the update rule maintains the excessive gap condition .",
    "[ th : main_rule ] suppose that @xmath151 satisfies with respect to two values @xmath72 and @xmath84",
    ". then @xmath152 generated by scheme - is in @xmath153 and maintains the excessive gap condition with respect to two smoothness parameter values @xmath154 and @xmath155 provided that : @xmath156    the last line of shows that @xmath157 .",
    "let us denote by @xmath158 .",
    "then , by using the definition of @xmath71 , the second line of and @xmath159 , we have : @xmath160\\right\\ } \\nonumber\\\\ & & \\overset{\\scriptsize\\textrm{line $ 2 $ \\eqref{eq : main_update_rule}}}{=}\\min_{x\\in x}\\left\\ { \\phi(x ) + ( 1-\\tau)(ax - b)^t\\bar{y } + \\tau(ax - b)^t\\hat{y}\\right . \\nonumber\\\\   & & + \\left .",
    "( 1-\\tau)\\beta_1[p_1(x_{1 } ) + p_2(x_{2})]\\right\\ } \\\\ & & = \\min_{x\\in x}\\left\\ { ( 1-\\tau)\\left[\\phi(x ) + ( ax - b)^t\\bar{y } + \\beta_1[p_1(x_{1})+p_2(x_{2})]\\right]\\right.\\nonumber\\\\ & & + \\left .",
    "\\tau\\left[\\phi(x ) + ( ax - b)^t\\hat{y}\\right]\\right\\}.\\nonumber \\end{aligned}\\ ] ] now , we estimate the first term in the last line of . since @xmath161 ,",
    "one has : @xmath162 moreover , if we denote by @xmath163 then , by the strong convexity of @xmath164 and @xmath165 , and @xmath166 , we have : @xmath167 \\nonumber\\\\ & & \\geq \\min_{x\\in x}\\left\\{\\phi(x ) + ( ax \\!-\\ ! b)^t\\bar{y } \\!+\\ ! \\beta_1[p_1(x_{1 } ) \\!+\\ ! p_2(x_{2})]\\right\\",
    "\\frac{1}{2}\\beta_1\\!\\!\\left[\\sigma_1{\\|x_{1 } \\!-\\ ! x_{1}^1\\|}^2 \\!+\\ !",
    "\\sigma_2{\\|x_{2 } \\!-\\ !",
    "x_{2}^1\\|}^2\\!\\right]\\nonumber\\\\ & & = d(\\bar{y } ; \\beta_1 ) + \\frac{1}{2}\\beta_1\\left[\\sigma_1{\\|x_{1 } - x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2\\right]\\nonumber\\\\ [ -1.5ex]\\\\[-1.5ex ] & & \\overset{\\scriptsize\\eqref{eq : excessive_gap}}{\\geq } f(\\bar{x } ; \\beta_2 ) +   \\frac{1}{2}\\beta_1\\left[\\sigma_1{\\|x_{1 } - x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2\\right]\\nonumber\\\\ & & \\overset{\\scriptsize\\textrm{def.~}f(\\cdot ; \\beta_2)}{= } \\phi(\\bar{x } ) + \\psi(\\bar{x } ; \\beta_2 ) +   \\frac{1}{2}\\beta_1\\left[\\sigma_1{\\|x_{1 } - x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2\\right]\\nonumber\\\\ & & \\overset{\\scriptsize\\eqref{eq : th31_est1b}}{= } \\phi(\\bar{x } ) + \\psi(\\bar{x } ; \\beta_2^{+ } ) +   \\frac{1}{2}\\beta_1\\left[\\sigma_1{\\|x_{1 } - x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2\\right ] - \\tau\\psi(\\bar{z};\\beta_2^{+})\\nonumber\\\\ & & \\overset{\\scriptsize\\eqref{eq : convexity_psi}}{= } \\phi(\\bar{x } ) + \\psi(\\hat{x } ; \\beta_2^{+ } ) + \\nabla\\psi(\\hat{x } ; \\beta_2^{+})^t(\\bar{x } -\\hat{x } ) + \\frac{1}{2}\\beta_1\\left[\\sigma_1{\\|x_{1 } - x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2\\right]\\nonumber\\\\ & & + \\frac{1}{2\\beta_2^{+}}{\\|a(\\bar{x}-\\hat{x})\\|}^2 - \\tau\\psi(\\bar{x } ; \\beta_2^{+}).\\nonumber   \\end{aligned}\\ ] ] for the second term in the last line of , we use the fact that @xmath168 and @xmath169 to obtain : @xmath170\\\\[-1.5ex ] & & \\overset{\\tiny\\textrm{def.~}\\hat{y}+\\eqref{eq : d_psi}}{= } \\phi(x ) + \\nabla\\psi(\\hat{x } ; \\beta_2^{+})^t(x - \\hat{x } ) + \\frac{1}{\\beta_2^{+}}{\\|a\\hat{x } - b\\|}^2 \\nonumber\\\\ & & \\overset{\\tiny\\textrm{def.~}\\hat{\\psi}}{= } \\phi(x ) + \\psi(\\hat{x } ; \\beta_2^{+ } ) + \\nabla\\psi(\\hat{x } ; \\beta_2^{+})^t(x - \\hat{x } ) + \\psi(\\hat{x } ; \\beta_2^{+ } ) .",
    "\\nonumber\\end{aligned}\\ ] ] substituting and into and noting that @xmath171 due to the first line of , we obtain : @xmath172 \\big ] \\nonumber\\\\ & & + \\tau\\left[\\phi(x ) + \\psi(\\hat{x } ; \\beta_2^{+ } ) + \\nabla\\psi(\\hat{x } ; \\beta_2^{+})^t(x-\\hat{x})\\right]\\big\\ } \\nonumber\\\\ & & - \\tau(1-\\tau)\\psi(\\bar{x } ; \\beta_2^{+ } ) + \\frac{(1-\\tau)}{2\\beta_2^{+}}{\\|a(\\bar{x}-\\hat{x})\\|}^2 + \\tau\\psi(\\hat{x } ; \\beta^{+}_2 ) \\\\ & & = \\min_{x\\in x}\\big\\ { ( 1-\\tau)\\phi(\\bar{x } ) + \\tau\\phi(x ) + \\psi(\\hat{x } ; \\beta_2^{+ } ) + \\nabla\\psi(\\hat{x } ; \\beta_2^{+})^t\\left[(1-\\tau)(\\bar{x}-\\hat{x } ) + \\tau(x-\\hat{x})\\right ] \\nonumber\\\\ & & + \\frac{1}{2}(1-\\tau)\\beta_1\\left[\\sigma_1{\\|x_{1}-x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2 \\right ] \\big\\ } + \\mathrm{\\textbf{t}}_3\\nonumber\\\\ & & \\overset{\\tiny\\phi-\\mathrm{convex}}{\\geq } \\min_{x\\in x}\\big\\ { \\phi((1-\\tau)\\bar{x } + \\tau x ) + \\psi(\\hat{x } ; \\beta_2^{+ } ) + \\tau\\nabla\\psi(\\hat{x } ; \\beta_2^{+})^t(x - x^1 ) \\nonumber\\\\ & & + \\frac{1}{2}(1-\\tau)\\beta_1\\left[\\sigma_1{\\|x_{1}-x_{1}^1\\|}^2 + \\sigma_2{\\|x_{2 } - x_{2}^1\\|}^2 \\right ] \\big\\ } + \\mathrm{\\textbf{t}}_3,\\nonumber\\end{aligned}\\ ] ] where @xmath173 .",
    "next , we note that the condition is equivalent to : @xmath174 moreover , if we denote by @xmath175 then : @xmath176 now , by using lemma [ le : lipschitz_diff_psi ] , the condition and , the estimation becomes : @xmath177 to complete the proof , we show that @xmath178 .",
    "indeed , let us define @xmath179 and @xmath180 , then @xmath181 .",
    "we have : @xmath182   \\nonumber\\\\ & & = \\frac{1}{2\\beta_2^{+}}\\left[\\tau{\\|\\hat{u}\\|}^2 - \\tau(1-\\tau){\\|\\bar{u}\\|}^2 + ( 1-\\tau){\\|\\hat{u}\\|}^2 + ( 1-\\tau){\\|\\bar{u}\\|}^2 - 2(1-\\tau)\\hat{u}^t\\bar{u}\\right ] \\\\ & & = \\frac{1}{2\\beta_2^{+}}\\left[{\\|\\hat{u}\\|}^2 + ( 1-\\tau)^2{\\|\\bar{u}\\|}^2 - 2(1-\\tau)\\hat{u}^t\\bar{u}\\right ] \\nonumber\\\\ & & = \\frac{1}{2\\beta_2^{+}}{\\|\\hat{u } - ( 1-\\tau)\\bar{u}\\|}^2 \\geq 0 . \\nonumber\\end{aligned}\\ ] ] substituting into we obtain the inequality @xmath183 .",
    "@xmath104    [ re : lipschit_diff_phi ] if @xmath35 is convex and differentiable and its gradient is lipschitz continuous with a lipschitz constant @xmath184 for some @xmath30 , then instead of using the proximal mapping @xmath132 in we can use the gradient mapping which is defined as : @xmath185 where @xmath186 .",
    "indeed , let us prove the condition @xmath187 , where @xmath188 and @xmath189 .",
    "first , by using the convexity of @xmath35 and the lipschitz continuity of its gradient , we have : @xmath190 next , by summing up the second inequality from @xmath191 to @xmath192 and adding to we have : @xmath193^t(u - \\hat{x } ) \\nonumber\\\\ [ -1.5ex]\\\\[-1.5ex ] & & +   \\frac{\\hat{l}_1^{\\psi}(\\beta_2^{+})}{2}{\\|u_1 -\\hat{x}_1\\|}^2 +   \\frac{\\hat{l}_2^{\\psi}(\\beta_2^{+})}{2}{\\|u_2 - \\hat{x}_2\\|}^2 . \\nonumber\\end{aligned}\\ ] ] finally , from the second inequality of we have : @xmath194^t(\\hat{\\bar{x}}^{+}-\\hat{x } ) \\nonumber\\\\ & & + \\frac{\\hat{l}_1^{\\psi}(\\beta_2^{+})}{2}{\\|\\hat{\\bar{x}}_{1}^{+ } -\\hat{x}_{1}\\|}^2 + \\frac{\\hat{l}_2^{\\psi}(\\beta_2^{+})}{2}{\\|\\hat{\\bar{x}}_{2}^{+ } - \\hat{x}_{2}\\|}^2\\nonumber\\\\ & & \\overset{\\tiny\\eqref{eq : rm3_est2}}{\\geq } \\phi(\\hat{\\bar{x}}^{+ } ) + \\psi(\\hat{\\bar{x}}^{+ } ; \\beta_2^{+ } ) =   f(\\hat{\\bar{x}}^{+ } ; \\beta_2^{+ } ) .",
    "\\nonumber \\end{aligned}\\ ] ] in this case , the conclusion of theorem [ th : main_rule ] is still valid for the substitution @xmath195 provided that : @xmath196 if @xmath197 is polytopic then problem becomes a convex quadratic programming problem .",
    "now , let us show how to update the parameter @xmath198 such that the condition holds for @xmath154 and @xmath155 . from the update rule we have",
    "suppose that @xmath82 and @xmath128 satisfy the condition , i.e. : @xmath200 if we substitute @xmath82 and @xmath128 by @xmath154 and @xmath155 , respectively , in this inequality then we have @xmath201",
    ". however , since @xmath199 , it implies @xmath202 .",
    "therefore , if @xmath203 then @xmath154 and @xmath155 satisfy .",
    "this condition leads to @xmath204 .",
    "since @xmath205 , the last inequality implies @xmath206 and @xmath207 hence , - are well - defined .",
    "now , we define a rule to update the step size parameter @xmath198 .",
    "[ le : choice_of_tau ] suppose that @xmath208 is arbitrarily chosen in @xmath209 . then the sequence @xmath210 generated by : @xmath211 satisfies the following equality : @xmath212 moreover ,",
    "the sequence @xmath213 generated by @xmath214 for fixed @xmath215 satisfies : @xmath216    if we denote by @xmath217 and consider the function @xmath218 then the sequence @xmath219 generated by the rule @xmath220 satisfies @xmath221 for all @xmath222 .",
    "hence @xmath223 for @xmath222 . to prove",
    ", we observe that @xmath224 .",
    "hence , by substituting into the last equality and carrying out a simple calculations , we get .",
    "@xmath104    [ re : choice_of_tau ] since @xmath225 , from lemma [ le : choice_of_tau ] we see that with @xmath226 ( e.g. , @xmath227 ) the right - hand side estimate of is minimized .      before presenting the algorithm",
    ", we assume that the prox - center @xmath228 of @xmath29 is given a priori for ( @xmath30 ) .",
    "moreover , the parameter sequence @xmath229 is updated by .",
    "the algorithm is presented in detail as follows :    ' '' ''       -0.2cm[alg : a1 ]  ( _ decomposition algorithm with primal update _ )    -0.3 cm    ' '' ''    * initialization : *    1 .   set @xmath230 .",
    "choose @xmath231 and @xmath232 as follows : @xmath233 2 .",
    "compute @xmath234 and @xmath235 from as : @xmath236    * iteration : * ` for ` @xmath237 ` do `    1 .",
    "if a given stopping criterion is satisfied then terminate .",
    "update the smoothness parameter @xmath238 .",
    "3 .   compute @xmath239 _ in parallel _ for @xmath30 and @xmath240 by the scheme : @xmath241 4 .   update the smoothness parameter : @xmath242 .",
    "5 .   update the step size parameter @xmath243 by : @xmath244 .    `",
    "end of for ` .",
    "-0.2 cm    ' '' ''    as mentioned in remark [ re : parallel ] , there are two steps of the scheme @xmath245 at step 3 of algorithm [ alg : a1 ] that can be parallelized .",
    "the first step is finding @xmath246 and the second one is computing @xmath247 . in general , both steps require solving two convex programming problems in parallel .",
    "the stopping criterion of algorithm [ alg : a1 ] at step 1 will be discussed in section [ sec : discussion ] .",
    "the following theorem provides the worst - case complexity estimate for algorithm [ alg : a1 ] .      by the choice of @xmath251 and steps 1 in the initialization phase of algorithm [ alg : a1 ] we see that @xmath252 for all @xmath253 .",
    "moreover , since @xmath227 , by lemma [ le : choice_of_tau ] , we have @xmath254 .",
    "now , by applying lemma [ le : excessive_gap ] with @xmath82 and @xmath128 equal to @xmath255 and @xmath256 respectively , we obtain the estimates and .",
    "@xmath104    [ re : alg1 ] the worst case complexity of algorithm [ alg : a1 ] is @xmath12 . however , the constants in the estimations and also depend on the choices of @xmath257 and @xmath258 , which satisfy the condition . the values of @xmath257 and @xmath258 will affect the accuracy of the duality and feasibility gaps .    in algorithm [ alg : a1 ] we can use a simple update rule @xmath259 , where @xmath260 is arbitrarily chosen such that the condition @xmath261 holds .",
    "however , the rule is the tightest one .",
    "in this section , we apply the switching strategy to obtain a new variant of the first algorithm proposed in ( * ? ? ? * algorithm 1 ) for solving problem .",
    "this scheme alternately switches between the primal and dual step depending on the iteration counter @xmath1 being even or odd .",
    "apart from its application to lagrangian dual decomposition , this variant is still different from the one in @xcite at two points .",
    "first , since we assume that the objective function is not necessarily smooth , instead of using the gradient mapping in the primal scheme , we use the proximal mapping defined by to construct the primal step .",
    "in contrast , since the objective function in the dual scheme is lipschitz continuously differentiable , we can directly use the gradient mapping to compute @xmath262 ( see ) .",
    "second , we use the exact update rule for @xmath198 instead of the simplified one as in @xcite .      since the smoothed dual function @xmath71 is lipschitz continuously differentiable on @xmath74 ( see lemma [ le : smoothing_estimate ] ) .",
    "we define the following mapping : @xmath263 where @xmath264 and @xmath265 .",
    "this problem can explicitly be solved to get the unique solution : @xmath266 + \\hat{y}.\\ ] ] the mapping @xmath267 is called gradient mapping of the function @xmath71 ( see @xcite ) .",
    "first , we adapt the scheme - in the framework of primal and dual variant .",
    "suppose that the pair @xmath115 satisfies the excessive gap condition .",
    "the primal step is computed as follows : @xmath268 and then we update @xmath269 , where @xmath270 and @xmath271 is defined in .",
    "the difference between schemes @xmath245 and @xmath272 is that the parameter @xmath128 is fixed in @xmath272 .",
    "symmetrically , the dual step is computed as : @xmath273 where @xmath143 .",
    "the parameter @xmath82 is kept unchanged , while @xmath128 is updated by @xmath274 .",
    "the following result shows that @xmath152 generated either by @xmath272 or by @xmath275 maintains the excessive gap condition .",
    "[ le : dual_main ] suppose that @xmath115 satisfy with respect to two values @xmath82 and @xmath128 .",
    "then @xmath152 generated either by scheme @xmath272 or by @xmath275 is in @xmath153 and maintains the excessive gap condition with respect to either two new values @xmath154 and @xmath128 or @xmath82 and @xmath155 provided that the following condition holds : @xmath276    the proof of this lemma is quite similar to ( * ? ? ?",
    "* theorem 4.2 . ) that we omit here .",
    "[ re : dual_init ] given @xmath72 , we can choose @xmath84 such that the condition holds .",
    "let @xmath277 , we compute a point @xmath278 as : @xmath279 then , similar to , the point @xmath278 satisfies",
    ". therefore , we can use this point as a starting point for algorithm [ alg : a2 ] below .    in algorithm",
    "[ alg : a2 ] below we apply either the primal scheme @xmath272 or the dual scheme @xmath275 by using the following rule :    * rule a. * _ if the iteration counter @xmath1 is even then apply @xmath272 .",
    "otherwise , @xmath275 is used .",
    "_    now , we provide an update rule to generate a sequence @xmath229 such that the condition holds .",
    "let @xmath280 .",
    "suppose that at the iteration @xmath1 the condition holds , i.e. : @xmath281 since at the iteration @xmath282 , we either update @xmath255 or @xmath256 .",
    "thus we have @xmath283 .",
    "however , as the condition holds , we have @xmath284 .",
    "now , we suppose that the condition is satisfied with @xmath285 and @xmath286 , i.e. : @xmath287 this condition holds if @xmath288 , which leads to @xmath289 .",
    "since @xmath290 , we obtain : @xmath291 < \\tau_k.\\ ] ] the tightest rule for updating @xmath243 is : @xmath292,\\ ] ] for all @xmath222 and @xmath293 given . associated with @xmath229 ,",
    "we generate two sequences @xmath294 and @xmath295 as : @xmath296 where @xmath297 are fixed .",
    "[ le : update_tau ] let @xmath229 , @xmath294 and @xmath295 be three sequences generated by and , respectively . then",
    ": @xmath298 for all @xmath299 .",
    "the proof of this lemma can be found in the appendix .",
    "[ re : choice_of_tau2 ] we can see that the right - hand side @xmath300 of is decreasing in @xmath301 for @xmath299 .",
    "therefore , we can choose @xmath208 as large as possible to minimize @xmath302 in @xmath303 .",
    "for instance , we can choose @xmath304 in algorithm [ alg : a2 ] .",
    "note that lemma [ le : update_tau ] shows that @xmath305 .",
    "hence , in algorithm [ alg : a2 ] , we can also use a simple updating rule for @xmath243 as @xmath306 , where @xmath307 and @xmath308 .",
    "this update satisfies .",
    "suppose that the initial point @xmath278 is computed by .",
    "then , we can choose @xmath309 which satisfy .",
    "the algorithm is now presented in detail as follows :    0.3 cm    ' '' ''       -0.2cm[alg : a2 ]  ( _ decomposition algorithm with primal - dual update _ )    -0.3 cm    ' '' ''    * initialization : *    1 .   choose @xmath304 and set @xmath310 .",
    "2 .   compute @xmath234 and @xmath235 as : @xmath311    * iteration : * ` for ` @xmath237 ` do `    1 .",
    "if a given stopping criterion is satisfied then terminate .",
    "if @xmath1 is even then : * compute @xmath312 as : @xmath313 * update the smoothness parameter @xmath255 as @xmath242 .",
    "3 .   otherwise , i.e. if @xmath1 is odd then : * compute @xmath312 as : @xmath314 * update the smoothness parameter @xmath256 as @xmath238 .",
    "4 .   update the step size parameter @xmath243 as : @xmath315 $ ] .",
    "` end of for ` .",
    "-0.2 cm    ' '' ''    the main steps of algorithm [ alg : a2 ] are steps 2a and 2b , which requires us to compute either a primal step or a dual step . in the primal step ,",
    "we need to solve two convex problem pairs in parallel , while in the dual step , it only requires to solve two convex problems in parallel .",
    "the following theorem shows the convergence of this algorithm .",
    "[ th : convergence2 ] let the sequence @xmath316 be generated by algorithm [ alg : a2 ] .",
    "then the duality and feasibility gaps satisfy : @xmath317 \\textrm{and}~ & & \\nonumber\\\\ [ -1.5ex ] & { \\|a\\bar{x}^k - b\\| } & \\leq \\frac{2\\sqrt{\\bar{l}}}{0.998k}\\left[{\\|y^{*}\\| } + \\sqrt{{\\|y^{*}\\|}^2 + 2(d_1 + d_2)}\\right]\\label{eq : feasible_gap2},\\end{aligned}\\ ] ] where @xmath250 and @xmath299 .",
    "the conclusion of this theorem follows directly from lemmas [ le : excessive_gap ] and [ le : choice_of_tau ] , the condition @xmath318 , @xmath319 and the fact that @xmath320 . @xmath104",
    "[ re : complexity2 ] note that the worst - case complexity of algorithm [ alg : a2 ] is still @xmath12 .",
    "the constants in the complexity estimates and are similar to the one in and , respectively . as we discuss in section [ sec : discussion ] below , the rate of decrease of @xmath243 in algorithm [ alg : a2 ] is smaller than two times of @xmath243 in algorithm [ alg : a1 ] .",
    "consequently , the sequences @xmath294 and @xmath295 generated by algorithm [ alg : a1 ] approach zero faster than the ones generated by algorithm [ alg : a2 ] .",
    "[ re : b_rule ] note that the role of the schemes @xmath272 and @xmath275 in algorithm [ alg : a2 ] can be exchanged .",
    "therefore , algorithm [ alg : a2 ] can be modified at three steps to obtain a symmetric variant as follows :    1 .   at step 2 of the initialization phase , to compute @xmath234 and @xmath235 instead of .",
    "2 .   at steps 2a",
    ", @xmath272 is used if the iteration counter @xmath1 is odd .",
    "otherwise , we use @xmath275 at step 3a .",
    "3 .   at steps 2b",
    ", @xmath256 is updated if @xmath1 is odd .",
    "otherwise , @xmath255 is updated at step 3b .",
    "if @xmath321 ( @xmath30 ) in is strongly convex then the convergence rate of the dual scheme can be accelerated up to @xmath322 .",
    "suppose that @xmath35 is strongly convex with a convexity parameters @xmath59 ( @xmath30 ) .",
    "then the function @xmath44 defined by is well - defined , concave and differentiable .",
    "moreover , its gradient is given by : @xmath323 which is lipschitz continuous with a lipschitz constant @xmath324 .",
    "the excessive gap condition in this case becomes : @xmath325 for given @xmath326 , @xmath327 and @xmath84 . from lemma",
    "[ le : excessive_gap ] we conclude that if the point @xmath328 satisfies then , for a given @xmath123 , the following estimates hold : @xmath329 \\mathrm{and } & & \\nonumber\\\\ [ -1.0ex ] & { \\|a\\bar{x } - b\\| } & \\leq 2\\beta_2{\\|y^{*}\\|}\\label{eq : sec5_feasible_gap}. \\end{aligned}\\ ] ] we now adapt the dual scheme to this special case .",
    "suppose @xmath330 satisfies , we generate a new pair @xmath331 as @xmath332 where @xmath333 , and @xmath334 is the solution of the minimization problem in .",
    "the parameter @xmath128 is updated by @xmath335 and @xmath270 will appropriately be chosen .",
    "the following lemma shows that @xmath152 generated by satisfies whose proof can be found in @xcite .",
    "[ le : sec5_main_rule ] suppose that the point @xmath330 satisfies the excessive gap condition with the value @xmath128 .",
    "then the new point @xmath331 computed by is in @xmath153 and also satisfies with a new parameter value @xmath155 provided that @xmath336    now , let us derive the rule to update the parameter @xmath198 .",
    "suppose that @xmath128 satisfies .",
    "since @xmath337 , the condition holds for @xmath155 if @xmath338 . therefore ,",
    "similar to algorithm [ alg : a2 ] , we update the parameter @xmath198 by using the rule .",
    "the conclusion of lemma [ le : update_tau ] still holds for this case .    before presenting the algorithm ,",
    "it is necessary to find a starting point @xmath278 which satisfies .",
    "let @xmath339 and @xmath340 . we compute @xmath341 , @xmath342 as @xmath343 it follows from lemma 7.4 @xcite that @xmath278 satisfies the excessive gap condition .",
    "finally , the decomposition algorithm for solving the strongly convex programming problem of the form is described in detail as follows :    0.3 cm    ' '' ''       -0.2cm[alg : a3 ]   ( _ decomposition algorithm for strongly convex objective function _ )    -0.3 cm    ' '' ''    * initialization : *    1 .",
    "choose @xmath344 .",
    "set @xmath345 .",
    "2 .   compute @xmath234 and @xmath235 as : @xmath346    * iteration : * ` for ` @xmath237 ` do `    1 .   if a given stopping criterion is satisfied then terminate .",
    "compute @xmath347 using scheme : @xmath348 3 .",
    "update the smoothness parameter as : @xmath238 .",
    "4 .   update the step size parameter @xmath243 as : @xmath315 $ ] .    `",
    "end of for ` .",
    "-0.2 cm    ' '' ''    the convergence and the worst - case complexity of algorithm [ alg : a3 ] are stated as in theorem [ th : convergence3 ] below .",
    "[ th : convergence3 ] let @xmath349 be a sequence generated by algorithm [ alg : a3 ]",
    ". then the following duality and feasibility gaps are satisfied : @xmath350 \\textrm{and}~ & & \\nonumber\\\\ [ -1.5ex ] & { \\|a\\bar{x}^k - b\\| } & \\leq \\frac{8l^{\\phi}{\\|y^{*}\\|}}{(k+4)^2 } , \\label{eq : sec5_feasible_gap2}\\end{aligned}\\ ] ] where @xmath324 .    from the update rule of @xmath351",
    ", we have @xmath352 .",
    "moreover , since @xmath353 , it implies that @xmath354 . by using the inequalities and @xmath355",
    ", we have @xmath356 . with @xmath357 ,",
    "one has @xmath358 . by substituting this inequality into and ,",
    "we obtain and , respectively .",
    "@xmath104    theorem [ th : convergence3 ] shows that the worst - case complexity of algorithm [ alg : a3 ] is @xmath359 .",
    "moreover , at each iteration of this algorithm , only two convex problems need to be solved _ in parallel_.",
    "algorithms [ alg : a1 ] and [ alg : a2 ] require to build a prox - function for each feasible set @xmath29 for @xmath30 . for a nonempty , closed and bounded convex set @xmath29 , the simplest prox - function is @xmath360 , for a given @xmath361 and @xmath362 .",
    "this function is strongly convex with the parameter @xmath363 and the prox - center is @xmath364 , ( @xmath30 ) . in implementation",
    ", it is worth to investigate the structure of the feasible set @xmath29 in order to choose an appropriate prox - function and its scaling factor @xmath365 for each feasible subset @xmath29 ( @xmath30 ) .    in",
    ", we have used the euclidean distance to construct the proximal terms .",
    "it is possible to use a generalized bregman distance in these problems which is compatible to the prox - function @xmath57 and the feasible subset @xmath197 ( @xmath30 ) .",
    "moreover , a proper choice of the norms in the implementation may lead to a better performance of the algorithms , see @xcite for more details .",
    "the algorithms developed in the previous sections can be directly applied to solve problem in the case @xmath26 .",
    "first , we provide the following formulas to compute the parameters of algorithms [ alg : a1]-[alg : a3 ] .    1 .   the constant @xmath366 in theorems [ th : convergence ] and [ th : convergence2 ] is replaced by @xmath367 .",
    "2 .   the initial values of @xmath257 and @xmath258 in algorithms [ alg : a2 ] and [ alg : a3 ] are @xmath368 .",
    "the lipschitz constant @xmath131 in lemma [ le : lipschitz_diff_psi ] is @xmath369 ( @xmath7 ) .",
    "the lipschitz constant @xmath370 in lemma [ le : smoothing_estimate ] is @xmath371 .",
    "the lipschitz constant @xmath372 in algorithm [ alg : a3 ] is @xmath373 .",
    "note that these constants depend linearly on @xmath374 and the structure of matrix @xmath375 ( @xmath7 ) .",
    "next , we rewrite the smoothed dual function @xmath376 defined by for the case @xmath26 as follows : @xmath377 where @xmath374 function values @xmath378 can be computed in parallel as : @xmath379 note that the term @xmath380 is also computed locally for each component subproblem instead of computing separately as in .",
    "the quantities @xmath381 and @xmath382 defined in and can respectively be expressed as : @xmath383.\\nonumber\\end{aligned}\\ ] ] these formulas show that each component of @xmath381 and @xmath384 can be computed by only using the local information and its neighborhood information .",
    "therefore , both algorithms are highly distributed .",
    "finally , we note that if there exists a component @xmath35 of the objective function @xmath36 which is lipschitz continuously differentiable then the gradient projection mapping @xmath385 defined by corresponding to the primal convex subproblem of this component can be used instead of the proximity mapping @xmath386 defined by .",
    "this modification can reduce the computational cost of the algorithms . note that the sequence @xmath210 generated by the rule still maintains the condition in remark [ re : lipschit_diff_phi ] .      in practice",
    ", we do not often encounter a problem which reaches the worst - case complexity bound .",
    "therefore , it is necessary to provide a stopping criterion for the implementation of algorithms [ alg : a1 ] , [ alg : a2 ] and [ alg : a3 ] to terminate earlier than using the worst - case bound . in principle , we can use the kkt condition to terminate the algorithms .",
    "however , evaluating the global kkt tolerance in a distributed manner is impractical .    from theorems",
    "[ th : convergence ] and [ th : convergence2 ] we see that the upper bound of the duality and feasibility gaps do not only depend on the iteration counter @xmath1 but also on the constants @xmath366 , @xmath83 and @xmath123 .",
    "the constant @xmath366 can be explicitly computed based on matrix @xmath387 and the choice of the prox - functions .",
    "we now discuss on the evaluations of @xmath83 and @xmath388 in the case @xmath197 is unbounded .",
    "let sequence @xmath389 be generated by algorithm [ alg : a1 ] ( or algorithm [ alg : a2 ] ) .",
    "suppose that @xmath390 converges to @xmath391 .",
    "thus , for @xmath1 sufficiently large , the sequence @xmath389 is contained in a neighborhood of @xmath392 . given @xmath393 ,",
    "let us define @xmath394 we can use these constants to construct a stopping criterion in algorithms [ alg : a1 ] and [ alg : a2 ] .",
    "more precisely , for a given tolerance @xmath395 , we compute @xmath396,\\ ] ] at each iteration .",
    "we terminate algorithm [ alg : a1 ] if @xmath397 and @xmath398 .",
    "a similar strategy can also be applied to algorithms [ alg : a2 ] and [ alg : a3 ] .",
    "firstly , we compare algorithms [ alg : a1 ] and [ alg : a2 ] . from lemma",
    "[ le : excessive_gap ] and the proof of theorems [ th : convergence ] and [ th : convergence2 ] we see that the rate of convergence of both algorithms is as same as of @xmath255 and @xmath256 . at each iteration ,",
    "algorithm [ alg : a1 ] updates simultaneously @xmath255 and @xmath256 by using the same value of @xmath243 , while algorithm [ alg : a2 ] updates only one parameter .",
    "therefore , to update both parameters @xmath255 and @xmath256 , algorithm [ alg : a2 ] needs two iterations .",
    "we analyze the update rule of @xmath243 in algorithms [ alg : a1 ] and [ alg : a2 ] to compare the rate of convergence of both algorithms .",
    "let us define @xmath399.\\ ] ] the function @xmath400 can be rewritten as @xmath401 .",
    "therefore , we can easily show that : @xmath402 if we denote by @xmath403 and @xmath404 the two sequences generated by algorithms [ alg : a1 ] and [ alg : a2 ] , respectively then we have @xmath405 for all @xmath1 provided that @xmath406 . since algorithm [ alg : a1 ] updates @xmath407 and @xmath256 simultaneously while algorithm [ alg : a2 ] updates each of them at each iteration .",
    "if we choose @xmath408 and @xmath409 in algorithms [ alg : a1 ] and [ alg : a2 ] , respectively , then , by directly computing the value of @xmath410 and @xmath411 , we can see that @xmath412 for all @xmath299 . consequently , the sequences @xmath294 and @xmath295 in algorithm [ alg : a1 ] converge to zero faster than in algorithm [ alg : a2 ] . in other words ,",
    "algorithm [ alg : a1 ] is faster than algorithm [ alg : a2 ] .",
    "now , we compare algorithm [ alg : a1 ] , algorithm [ alg : a2 ] and algorithm 3.2 . in @xcite",
    "( see also @xcite ) .",
    "note that the smoothness parameter @xmath82 which is also denoted by @xmath413 is fixed in algorithm 3.2 of @xcite . moreover ,",
    "this parameter is proportional to the given desired accuracy @xmath13 , which is often very small .",
    "thus , the lipschitz constant @xmath414 is very large .",
    "consequently , algorithm 3.2 . of @xcite",
    "makes a slow progress at the very early iterations . in algorithms",
    "[ alg : a1 ] and [ alg : a2 ] , the parameters @xmath82 and @xmath128 are dynamically updated starting from given values . besides , the cost per iteration of algorithm 3.2 @xcite is more expensive than algorithms [ alg : a1 ] and [ alg : a2 ] since it requires to solve two convex problem pairs in parallel and two dual steps",
    "in this section , we verify the performance of the proposed algorithms by applying them to solve the following separable convex optimization problem : @xmath415 where @xmath416 is convex , @xmath417 , @xmath418 and @xmath419 are given for @xmath7 .",
    "the problem arises in many applications including resource allocation problems @xcite and dsl dynamic spectrum management problems @xcite . in the case of inequality coupling constraints",
    ", we can bring the problem in to the form of by adding a slack variable @xmath420 as a new component .",
    "we implement algorithms [ alg : a1 ] and [ alg : a2 ] proposed in the previous sections to solve .",
    "the implementation is carried out in c++ running on a @xmath421 cores workstation intelxeron @xmath422ghz and @xmath423 gb of ram . to solve general convex programming subproblems",
    ", we implement a primal - dual predictor - corrector interior point method .",
    "all the algorithms are parallelized by using ` openmp ` .",
    "the prox - functions @xmath424 are used , where @xmath66 is the center of the box @xmath425 $ ] and @xmath426 for all @xmath7 .",
    "we terminate algorithms [ alg : a1 ] and [ alg : a2 ] if @xmath427 and either @xmath428 or the value of the objective function does not significantly change in @xmath429 successive iterations , i.e. @xmath430 for @xmath431 , where @xmath432 , @xmath433 and @xmath434 are given tolerances .",
    "note that the quantity ` rdfgap ` is computed in the worst - case complexity , see lemma [ le : excessive_gap ] .    to compare the performance of the algorithms",
    ", we also implement the proximal - center - based decomposition algorithm proposed in ( * ? ? ?",
    "* algorithm 3.2 . ) and an exact variant of the proximal - based decomposition in ( * ? ? ? * algorithm i ) for solving which we name ` pcbd ` and ` epbd ` , respectively .",
    "the prox - function of the dual problem is chosen as @xmath435 with @xmath436 and the smoothness parameter @xmath413 of ` pcbd ` is set to @xmath437 , where @xmath438 is defined by .",
    "we terminate ` pcbd ` if the relative feasibility gap @xmath439 and either the objective value reaches the one reported by algorithm [ alg : a1 ] or the maximum number of iterations @xmath440 is reached .",
    "we test the above algorithms for three examples .",
    "the two first examples are resource allocation problems and the last one is a dsl dynamic spectrum management problem .",
    "the first example was considered in @xcite , while the problem formulation and the data of the third example are obtained from @xcite .    0.2 cm _ 7.2.1 .",
    "resource allocation problems .",
    "_ let us consider a resource allocation problem in the form of where the coupling constraint @xmath441 is tackled .",
    "@xmath442 nonsmooth convex optimization problems . _ in the first numerical example , we choose @xmath443 , @xmath444 , the objective function @xmath445 which is nonsmooth and @xmath446 as in @xcite .",
    "the lower bound @xmath418 is set to @xmath447 and the upper bound @xmath448 is @xmath449 for @xmath7 . with these choices ,",
    "the optimal solution of this problem is @xmath450 .",
    "we use four different algorithms which consist of algorithm [ alg : a1 ] , algorithm [ alg : a2 ] , ` pcbd ` in @xcite and ` pcbd ` in ( * ? ? ?",
    "* algorithm i ) to solved problem .",
    "the approximate solutions reported by these algorithms after @xmath451 iterations are @xmath452 ,  @xmath453 ,  @xmath454 and @xmath455 , respectively .",
    "the corresponding objective values are @xmath456 ,  @xmath457 , @xmath458 and @xmath459 , respectively .",
    "the convergence behaviour of four algorithms is shown in figure [ fig : error_exam1 ] , where the relative error of the objective function @xmath460 is plotted on the left and the relative error of the solution @xmath461 is on the right .    -0.5 cm        -0.4",
    "cm    as we can see from these figures that the relative errors in algorithm [ alg : a2 ] , ` pcbd ` and ` epbd ` oscillate with respect to the iteration counter while they are decreasing monotonously in algorithm [ alg : a1 ] . the relative errors in algorithms [ alg : a1 ] and [ alg : a2 ] are approaching zero earlier than the ones in ` pcbd ` and ` epbd ` . note that in this example a nonmonotone variant of the ` pcbd ` algorithm @xcite is used .",
    "_ @xmath462 nonlinear resource allocation problems . _ in order to compare the efficiency of algorithm [ alg : a1 ] , algorithm [ alg : a2 ] and ` pcbd ` , we build two performance profiles of these algorithms in terms of total iterations and total computational time .    in this case",
    ", the objective function @xmath35 is chosen as @xmath463 , where the linear cost vector @xmath464 , vector @xmath465 and the weighting vector @xmath466 are generated randomly in the intervals @xmath467 $ ] , @xmath468 $ ] and @xmath467 $ ] , respectively .",
    "the lower bound and the upper bound are set to @xmath469 and @xmath470 , respectively .",
    "note that the objective function @xmath35 is linear if @xmath471 and strictly convex if @xmath472 .",
    "we carry out three algorithms for solving a collection of @xmath473 random test problems with the size varying from @xmath474 to @xmath475 components , @xmath476 to @xmath477 coupling constraints and @xmath478 to @xmath479 variables .",
    "the performance profiles are plotted in figure [ fig : perf_scp ] which include the total number of iterations ( left ) and total computational time ( right ) .",
    "-0.5 cm     scale : left - number of iterations , right - cpu time.,width=464,height=151 ]    -0.5 cm    the numerical test on this collection of problems shows that algorithm [ alg : a1 ] solves all the problems and algorithm [ alg : a2 ] solve @xmath480 problems , i.e. @xmath481 of the collection . `",
    "pcbd ` only solves @xmath482 problems , i.e. @xmath483 of the collection .",
    "however , algorithms [ alg : a1 ] is the most efficient .",
    "it solves up to more than @xmath484 problems with the best performance . `",
    "pcbd ` is rather slow and exceeds the maximum number of iterations in many of the test problems ( @xmath485 problems ) .",
    "moreover , it is rather sensitive to the smoothness parameter .    0.2 cm _ 7.2.2 .",
    "dsl dynamic spectrum management problem .",
    "_ in this example , we apply the proposed algorithms to solve a separable convex programming problem arising in dsl dynamic spectrum management .",
    "this problem is a convex relaxation of the original dsl dynamic spectrum management formulation considered in @xcite .",
    "since the formulation given in @xcite has an inequality coupling constraint @xmath486 , by adding a new slack variable @xmath420 such that @xmath487 and @xmath488 , we can transform this problem into .",
    "the objective function of the resulting problem becomes : @xmath489 here , @xmath490 , @xmath491 and @xmath492 , ( @xmath7 ) .",
    "the function @xmath35 is convex ( but not strongly convex ) for all @xmath493 . as described in @xcite that the variable @xmath22 is referred to as transmit power spectral density , @xmath494 for all @xmath7",
    "is the number of users , @xmath374 is the number of frequency tones which is usually large and @xmath35 is a convex approximation of a desired ber function , the coding gain and noise margin .",
    "a detail model and parameter descriptions of this problem can be found in @xcite .",
    "we test three algorithms for the case of @xmath495 tones and @xmath496 users .",
    "the other parameters are selected as in @xcite .",
    "algorithm [ alg : a1 ] requires @xmath497 iterations , algorithm [ alg : a2 ] needs @xmath498 iterations , while ` pcbd ` reaches the maximum number of iterations @xmath499 .",
    "the relative feasibility gaps @xmath500 reported by the three algorithms are @xmath501 , @xmath502 and @xmath503 , respectively .",
    "the obtained approximate solutions of three algorithms and the optimal solution are plotted in figure [ fig : dsl_frequency ] which represent the transmit power with respect to the frequency tones .",
    "-0.5 cm        the relative errors of the approximation @xmath504 to the optimal solution @xmath505 , @xmath506 , are @xmath507 , @xmath508 and @xmath509 , respectively .",
    "the corresponding objective values are @xmath510 , @xmath511 and @xmath512 , respectively , while the optimal value is @xmath513 .",
    "figure [ fig : dsl_frequency ] shows that the solutions reported by three algorithms are consistently close to the optimal one . as claimed in @xcite , ` pcbd ` works much better than subgradient methods .",
    "however , we can see from this application that algorithms [ alg : a1 ] and [ alg : a2 ] require fewer iterations than ` pcbd ` to reach a relatively similar approximate solution .",
    "in this paper , two new algorithms for large scale separable convex optimization have been proposed .",
    "their convergence has been proved and complexity bound has been given .",
    "the main advantage of these algorithms is their ability to dynamically update the smoothness parameters .",
    "this allows the algorithms to control the step - size of the search direction at each iteration .",
    "consequently , they generate a larger step at the first iterations instead of remaining fixed for all iterations as in the algorithm proposed in @xcite .",
    "the convergence behavior and the performance of these algorithms have been illustrated through numerical examples .",
    "although the global convergence rate is still sub - linear , the computational results are remarkable , especially when the number of variables as well as the number of nodes increase . from a theoretical point of view , the algorithms possess a good performance behavior , due to their numerical robustness and reliability .",
    "currently , the numerical results are still preliminary , however we believe that the theory presented in this paper is useful and may provide guidance for practitioners .",
    "moreover , the steps of the algorithms are rather simple so they can easily be implemented in practice .",
    "future research directions include the dual update scheme and extensions of the algorithms to inexact variants as well as applications .    0.3 cm    * acknowledgments .",
    "* the authors would like to thank dr .",
    "ion necoara and dr .",
    "michel baes for useful comments on the text and for pointing out some interesting references .",
    "furthermore , the authors are grateful to dr .",
    "paschalis tsiaflakis for providing the reality data in the second numerical example .",
    "research supported by research council kul : coe ef/05/006 optimization in engineering(optec ) , iof - scores4chem , goa/10/009 ( manet ) , goa /10/11 , several phd / postdoc and fellow grants ; flemish government : fwo : phd / postdoc grants , projects g.0452.04 , g.0499.04 , g.0211.05 , g.0226.06 , g.0321.06 , g.0302.07 , g.0320.08 , g.0558.08 , g.0557.08 , g.0588.09 , g.0377.09 , g.0712.11 , research communities ( iccos , anmmm , mldm ) ; iwt : phd grants , belgian federal science policy office : iuap p6/04 ; eu : ernsi ; fp7-hdmpc , fp7-embocon , erc - highwind , contract research : aminal .",
    "other : helmholtz - vicerp , comet - accm .",
    "this appendix provides the proofs of two technical lemmas stated in the previous sections .",
    "let @xmath514 .",
    "then it follows from that : @xmath515\\\\[-1.5ex ] & & \\overset{\\tiny\\textrm{def.}~\\psi(\\cdot;\\beta_2)}{\\!\\!\\!\\!=\\!\\!\\ ! } \\!\\!\\frac{1}{2\\beta_2}{\\|a\\hat{x } - b\\|}^2 \\!+\\ !",
    "\\hat{y}^ta_{1}(x_{1 } \\!-\\ !",
    "\\hat{x}_{1 } )",
    "\\hat{y}^ta_{2}(x_{2 }   \\!-\\ ! \\hat{x}_{2 } )   \\!+\\ !",
    "\\frac{l_1^{\\psi}(\\beta_2)}{2}{\\|x_{1 } \\!-\\ ! \\hat{x}_{1}\\|}^2 \\!+\\ !",
    "\\frac{l_2^{\\psi}(\\beta_2)}{2}{\\|x_{2 } \\!-\\!\\hat{x}_{2}\\|}^2.\\nonumber\\\\ & & = \\hat{y}^t(ax - b ) - \\frac{1}{2\\beta_2}{\\|a\\hat{x}-b\\|}^2 + \\frac{l_1^{\\psi}(\\beta_2)}{2}{\\|x_{1 } \\!-\\",
    "! \\hat{x}_{1}\\|}^2 \\!+\\ ! \\frac{l_2^{\\psi}(\\beta_2)}{2}{\\|x_{2}-\\hat{x}_{2}\\|}^2 . \\nonumber\\end{aligned}\\ ] ] by using the expression @xmath516 , the definition of @xmath121 , the condition and we have : @xmath517 \\right\\ } - \\frac{1}{2\\beta_2}{\\|ax^c - b\\|}^2\\nonumber\\\\ & = d(\\bar{y } ; \\beta_1)- \\frac{1}{2\\beta_2}{\\|ax^c - b\\|}^2 \\leq d(\\bar{y } ; \\beta_1),\\end{aligned}\\ ] ] which is indeed the condition .",
    "@xmath104      let us define @xmath518 .",
    "it is easy to show that @xmath519 is increasing in @xmath301 .",
    "moreover , @xmath520 for all @xmath222 .",
    "let us introduce @xmath521 .",
    "then , we can show that @xmath522 . by using this inequalities and the increase of @xmath519 in @xmath301",
    ", we have : @xmath523 now , by the update rule , at each iteration @xmath1 , we only either update @xmath255 or @xmath256 .",
    "hence , it implies that : @xmath524\\label{eq : lm42_est2}\\\\[-1.5ex ] & & \\beta_2^k = ( 1-\\tau_1)(1-\\tau_3)\\cdots(1-\\tau_{2\\lfloor{k/2\\rfloor}-1})\\beta_2 ^ 0,\\nonumber\\end{aligned}\\ ] ] where @xmath525 is the largest integer number which is less than or equal to the positive real number @xmath110 . on the other hand , since @xmath526 for @xmath527 , for any @xmath528 , it implies : @xmath529 ^ 2 <",
    "\\prod_{i=0}^{2l+1}(1-\\tau_i ) , \\nonumber\\\\ [ -1.5ex]\\label{eq : lm42_est3}\\\\[-1.5ex ] \\textrm{and}~ & \\prod_{i=0}^{2l-1}(1-\\tau_i ) < \\left[(1-\\tau_1)(1-\\tau_3)\\cdots(1-\\tau_{2l-1})\\right]^2 <   ( 1-\\tau_0)^{-1}\\prod_{i=0}^{2l}(1-\\tau_i ) . \\nonumber\\end{aligned}\\ ] ] note that @xmath530 , it follows from and for @xmath299 that : @xmath531 by combining these inequalities and , and noting that @xmath532 , we obtain .",
    "@xmath104                boyd , s. , parikh , n. , chu , e. , peleato , b. and eckstein , j. : distributed optimization and statistical learning via the alternating direction method of multipliers . foundations and trends in machine learning ,",
    "* 3*:1 , 1 - 122 ( 2011 ) .",
    "kojima , m. , megiddo , n. and mizuno , s. et al : horizontal and vertical decomposition in interior point methods for linear programs . technical report .",
    "information sciences , tokyo institute of technology ( 1993 ) .",
    "tran dinh , q. , necoara , i. , savorgnan , c. and diehl , m. : an inexact perturbed path - following method for lagrangian decomposition in large - scale separable convex optimization .",
    "report , 137 , ( 2011 ) , url : http://arxiv.org/abs/1109.3323 .",
    "tsiaflakis p. , necoara i. , suykens j.a.k .",
    ", moonen m. : improved dual decomposition based optimization for dsl dynamic spectrum management .",
    "ieee transactions on signal processing , * 58*(4 ) , 22302245 , ( 2010 ) .",
    "venkat , a. , hiskens , i. , rawlings , j. , and wright , s. : distributed mpc strategies with application to power system automatic generation control .",
    "ieee trans .",
    ". technol . * 16*(6 ) , 119212 - 6 ( 2008 ) ."
  ],
  "abstract_text": [
    "<S> a new algorithm for solving large - scale convex optimization problems with a separable objective function is proposed . </S>",
    "<S> the basic idea is to combine three techniques : lagrangian dual decomposition , excessive gap and smoothing . </S>",
    "<S> the main advantage of this algorithm is that it dynamically updates the smoothness parameters which leads to numerically robust performance . </S>",
    "<S> the convergence of the algorithm is proved under weak conditions imposed on the original problem . </S>",
    "<S> the rate of convergence is @xmath0 , where @xmath1 is the iteration counter . in the second part of the paper , the algorithm </S>",
    "<S> is coupled with a dual scheme to construct a switching variant of the dual decomposition . </S>",
    "<S> we discuss implementation issues and make a theoretical comparison . </S>",
    "<S> numerical examples confirm the theoretical results . </S>"
  ]
}