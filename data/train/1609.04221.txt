{
  "article_text": [
    "dynamic games with symmetric information among agents has been studied well in game theory literature  @xcite .",
    "the appropriate equilibrium concept is subgame perfect equilibrium ( spe ) which is a refinement of nash equilibrium .",
    "a strategy profile is spe if its restriction to any subgame of the original game is also an spe of the subgame .",
    "however there are models of interest where strategic agents interact repeatedly whilst observing private signals .",
    "for instance , relay scheduling with private queue length information at different relays  @xcite .",
    "another example is that of bayesian learning games  @xcite such as when customers post reviews on websites such as amazon ; clearly agents posses their own experience of the object they bought and this constitutes private information .",
    "these examples lead to a model with asymmetric information , where the notion of spe is ineffective .",
    "appropriate equilibrium concepts in such a case consist of strategy profiles and beliefs .",
    "these include weak perfect bayesian equilibrium , perfect bayesian equilibrium ( pbe ) and sequential equilibrium .",
    "pbe is the most commonly used notion .",
    "the requirement of pbe is sequential rationality of strategy as well as belief consistency . due to the fact that future beliefs depend on past strategy and strategies are sequentially rational based on specific beliefs , finding a pair of strategy and belief that satisfy pbe requirement is a difficult problem and usually reduces to a large fixed - point equation over the entire time horizon . to date",
    ", there is no universal algorithm that provides simplification by decomposing the aforementioned fixed - point equation for calculating pbes .",
    "our motivation stems from the work of nayyar et al .",
    "authors in  @xcite consider a decentralized dynamic team problem ( non - strategic agents ) with asymmetric information .",
    "each agents observes a private type which evolves with time as a controlled markov process .",
    "they introduce a common information based approach , whereby each agent calculates a belief on every agents current private type .",
    "this belief is defined in such a manner that despite having asymmetric information , agents can agree on it .",
    "they proceed to show optimality of policies that depend on private history only through this belief and respective agents current private type .",
    "the common information based approach has been studied for finite horizon dynamic games with asymmetric information in  @xcite . of these ,  @xcite consider models where the aforementioned belief from the common information approach is updated independent of strategy .",
    "this implies that the simultaneous requirements of sequential rationality and belief consistency can be decoupled , resulting in a simplification in calculating pbes .",
    "however these models produce non - signaling pbes . for the models studied in",
    "@xcite belief updates depend on strategies .",
    "consequently the resulting pbes are signaling equilibria .",
    "the problem of finding them though , becomes more complicated .",
    "other than the common information based approach , li et al .",
    "@xcite consider a finite horizon zero - sum dynamic game , where at each time only one agent out of the two knows the state of the system .",
    "the value of the game is calculated by formulating an appropriate linear program .",
    "all works listed above for dynamic games consider a finite horizon . in this paper",
    ", we deal with an infinite horizon discounted reward dynamic game .",
    "cole et al .",
    "@xcite consider an infinite horizon discounted reward dynamic game where actions are only privately observable .",
    "they provide a fixed - point equation for calculating a subset of sequential equilibrium , which is referred to as markov private equilibrium ( mpre ) . in mpre strategies depend on history only through the latest private observation .      in this work",
    "we follow the models of  @xcite where beliefs are updated dependent on strategy and thus signaling pbes are considered .",
    "this paper provides a one - shot fixed - point equation and a forward recursive algorithm that together generate a subset of perfect bayesian equilibria for the infinite horizon discounted reward dynamic game with asymmetric information .",
    "the common information based approach of  @xcite is used and the strategies are such that they depend on the public history only through the common belief ( which summarizes the relevant information ) and on private history only through respective agents current private type .",
    "the pbe thus generated are referred to as structured pbe or spbe and posses the property that the mapping between belief and strategy is stationary ( w.r.t .",
    "time ) .",
    "the provided methodology ( consisting of solving a one - shot fixed - point equation ) provides a decomposition of the interdependence between belief and strategy in pbes and enables a systematic evaluation of spbes .",
    "the model allows for signaling amongst agents as beliefs depend on strategies .",
    "this is because the source of asymmetry of information in our model is that each agent has a privately observed type that affects the utility function of all the agents .",
    "these types evolve as a controlled markov process and depend on actions of all agents ( the actions are assumed to be common knowledge ) .",
    "our methodology for proving our results is as follows : first we extend the finite horizon model of  @xcite to include belief - based terminal reward .",
    "this does not change the proofs significantly but helps us simplify the exposition and proofs of the infinite horizon case . in the next step ,",
    "the infinite horizon results are obtained with the help of the finite horizon ones through continuity arguments as horizon @xmath0 .",
    "we demonstrate our methodology of finding spbes through a concrete public goods game .",
    "the remainder of this paper is organized as follows : section  [ secmodel ] introduces the model for the dynamic game with private types and discounted reward .",
    "we consider two versions of the problem , finite and infinite horizon .",
    "section  [ secfh ] defines the backward recursive algorithm for calculating spbes in the finite horizon model .",
    "the results in this section are an adaptation of the finite horizon results from  @xcite .",
    "this is done so that the finite horizon results can be appropriately used in section  [ secih ] .",
    "section  [ secih ] contains the central result of this paper , namely the fixed - point equation that defines the set of spbe strategy for the infinite horizon game .",
    "finally , section  [ secexample ] discusses a concrete example of a public goods game with two agents .",
    "this example is an infinite horizon version of the finite horizon public goods example from  ( * ? ? ?",
    "8 , example  8.3 ) .",
    "we consider a dynamical system with @xmath1 strategic agents , denoted by @xmath2 .",
    "associated with each agent @xmath3 at any time @xmath4 , is a type @xmath5 .",
    "the set of types is denoted by @xmath6 and is assumed to be finite .",
    "we assume that each agent @xmath3 can observe their own type @xmath7 but not that of others .",
    "denote the profile of types at time @xmath8 by @xmath9 .",
    "each agent @xmath3 at any time @xmath4 , after observing their private type @xmath7 takes action @xmath10 .",
    "the set of available actions to agent @xmath3 is denoted by @xmath11 and is assumed to be finite .",
    "it is assumed that the action profile is publicly observed and is common knowledge .",
    "the type of each agent evolves over time as a controlled markov process independent of other agents given the action profile @xmath12 .",
    "we assume a time - homogeneous kernel @xmath13 for evolution of agent @xmath3 s type i.e. , @xmath14 .",
    "the notation we use is @xmath15 .",
    "initial belief , at time @xmath16 , is assumed as @xmath17 .",
    "associated with each agent @xmath3 is a reward function @xmath18 that depends on the type and action profiles i.e. , reward at time @xmath8 for agent @xmath3 is @xmath19 .",
    "furthermore , rewards are accumulated over time in a discounted manner with a common discount factor @xmath20 .",
    "we consider two versions of the problem - finite horizon @xmath21 and infinite horizon .",
    "for the finite horizon problem we introduce a terminal reward .",
    "we first define beliefs @xmath22 since the terminal reward is defined as a function of beliefs .",
    "define the public history @xmath23 at time @xmath8 as the set of publicly observed actions i.e. , @xmath24 .",
    "denote the set of public histories by @xmath25 .",
    "define private history @xmath26 of any agent @xmath3 at time @xmath8 as the set of privately observed types @xmath27 of agent @xmath3 and publicly observed action profiles @xmath28 i.e. , @xmath29 .",
    "denote the set of private histories by @xmath30 .",
    "note that private history @xmath26 contains public history @xmath23 . also denote the overall history at time @xmath8 by @xmath31 and the set of all such histories by @xmath32 .    any strategy used by agent @xmath3",
    "can be represented as @xmath33 with @xmath34 .",
    "the notation we use is : @xmath35 is the set of all probability distributions over the elements of the set @xmath36 .",
    "the belief @xmath37 , at any time @xmath8 , depends on the strategy profile @xmath38 . given a strategy profile @xmath39",
    ", we define the belief as follows : @xmath40 , with @xmath41 if @xmath42 , else @xmath43 initial belief is @xmath44 .",
    "the above definition ensures that the belief random variable @xmath45 , at any time @xmath8 , is measurable , even if the set of histories resulting in any instance @xmath46 has zero measure under strategy @xmath47 .",
    "finally , in the finite horizon game , for each agent @xmath3 there is a terminal reward @xmath48 that depends on the terminal type of agent @xmath3 and the terminal belief .",
    "it is assumed that @xmath49 is absolutely bounded .",
    "in this section , we state and prove properties of the finite horizon backwards recursive algorithm for calculating spbe , adopted from  @xcite with minor modifications to accommodate for terminal rewards ( specifically belief - based ) .",
    "the results from this section are used in section  [ secih ] to prove the infinite horizon equilibrium result .    to distinguish quantities defined in this section with infinite horizon",
    ", we add a superscript @xmath50 to all the quantities .",
    "consider a finite horizon , @xmath51 , problem in the above dynamic game model .",
    "define the value functions @xmath52 and strategy @xmath53 backwards inductively as follows .",
    "1 .   @xmath54 @xmath55 , set @xmath56 .",
    "2 .   for any @xmath57 and @xmath22 , solve the following fixed - point equation in @xmath58 .",
    "@xmath59 @xmath55 , @xmath60 , @xmath61_{j=1}^n , x_{t+1}^i \\big ) \\mid { \\underline{\\pi}}_t , x_t^i \\big ]      \\end{gathered}\\ ] ] ( see below for the various quantities involved in the above expression ) .",
    "3 .   then define , @xmath62_{j=1}^n , x_{t+1}^i \\big ) \\mid { \\underline{\\pi}}_t , x_t^i \\big ]      \\end{gathered}\\ ] ]    denote by @xmath63 the mapping @xmath64 i.e. , @xmath65 $ ] .    in  , the expectation is with the following distribution @xmath66 and @xmath67 \\sum_{x^j \\in { \\mathcal{x}}^j } \\pi^j(x^j ) q^j(x^{\\prime j } \\mid x^j , a ) \\!\\!\\ ! & \\mbox{if } \\text{den . }",
    "= 0 \\end{array } \\right.\\end{gathered}\\ ] ]    below we define strategy - belief pair @xmath68 ,    @xmath69    based on the mapping @xmath70 produced by the above algorithm .",
    "define belief @xmath71 inductively as follows : set @xmath72 .",
    "then for @xmath57 ,    [ eqbelfh ] @xmath73       & = f\\big ( \\mu_{t}^{i,\\star}[h_t^c ] , \\theta_t^i\\big[\\mu_{t}^{\\star}[h_t^c]\\big ] , a_{t } \\big )      \\\\",
    "\\mu_{t+1}^\\star\\big [ h_{t+1}^c \\big](x_{t+1 } )       & = \\prod_{i=1}^n \\mu_{t+1}^{i,\\star}\\big [ h_{t+1}^c \\big](x_{t+1}^i )      \\end{aligned}\\ ] ]    denote the strategy arising out of @xmath74 by @xmath75 i.e. , @xmath76\\big](a_t^i \\mid x_t^i)\\end{gathered}\\ ] ]      in this section , we presented three lemmas .",
    "the first two are technical results needed in the proof of the third .",
    "the result of the third lemma is used in section  [ secih ] .    define the reward - to -",
    "go @xmath77 for any agent @xmath3 and strategy @xmath78 as @xmath79 } \\big [ \\sum_{n = t}^t \\delta^{n - t } r^i(x_n , a_n )",
    "\\\\ + \\delta^{t+1-t } g^{i}(\\underline{\\pi}_{t+1},x^i_{t+1 } ) \\mid h_t^i \\big].\\end{gathered}\\ ] ] here agent @xmath3 s strategy is @xmath78 whereas all other agents use strategy @xmath80 defined above . since @xmath81 are assumed to be finite and @xmath82 absolutely bounded , the reward - to - go is finite @xmath59 @xmath83 .",
    "[ thmfh1 ] for any @xmath57 , @xmath55 , @xmath26 and @xmath78 , @xmath84,x_t^i ) \\ge { \\mathbb{e}}^{\\beta^i,\\beta^{-i,\\star},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t )       \\\\",
    "+ \\delta v_{t+1}^{i , t}\\big ( \\big[f(\\mu_t^{j,\\star}[h_t^c],\\beta_t^{j,\\star},a_t)\\big]_{j=1}^n , x_{t+1}^i \\big ) \\mid h_t^i \\big ]      \\end{gathered}\\ ] ]    we use proof by contradiction .",
    "suppose @xmath85 @xmath86 such that   is violated .",
    "construct strategy @xmath87 as @xmath88 then    @xmath89,{\\widehat}{x}_t^i )           \\\\          & \\ge { \\mathbb{e}}^{{\\widehat}{\\gamma}_t^i(\\cdot \\mid { \\widehat}{x}_t^i),\\beta_t^{-i,\\star},\\mu_t^\\star[{\\widehat}{h}_t^c ] } \\big [ r^i(x_t , a_t )   \\delta v_{t+1}^{i , t }          \\\\",
    "\\nonumber          & + \\big ( \\big [ f(\\mu_t^{j,\\star}[{\\widehat}{h}_t^c],\\beta_t^{j,\\star}(\\cdot \\mid { \\widehat}{h}_t^c , \\cdot),a_t ) \\big]_{j=1}^n , x_{t+1}^{i } \\big ) \\mid \\mu_t^\\star[{\\widehat}{h}_t^c],{\\widehat}{x}_t^i \\big ]          \\\\          & = { \\mathbb{e}}^{{\\widehat}{\\beta}_t^i,\\beta_t^{-i,\\star},\\mu_t^\\star[{\\widehat}{h}_t^c ] } \\big [ r^i(x_t , a_t )   + \\delta v_{t+1}^{i , t }          \\\\",
    "\\nonumber           & \\big ( \\big [ f(\\mu_t^{j,\\star}[{\\widehat}{h}_t^c],\\beta_t^{j,\\star}(\\cdot \\mid { \\widehat}{h}_t^c , \\cdot),a_t ) \\big]_{j=1}^n , x_{t+1}^{i } \\big ) \\mid { \\widehat}{h}_t^i \\big ]          \\\\          & > v_t^{i , t}(\\mu_t^\\star[{\\widehat}{h}_t^c],{\\widehat}{x}_t^i )          \\end{aligned}\\ ] ]    the first inequality above follows from the algorithm definition in   and  , the second equality follows from the definition above in   and finally the last inequality follows from the assumption at the beginning of this proof .",
    "since the above is clearly a contradiction , the result follows .",
    "[ lemcond ] @xmath90 } \\big [ \\sum_{n = t+1}^t \\delta^{n-(t+1 ) } r^i(x_n , a_n )       \\\\      + \\delta^{t+1-t } g^i(\\pi_{t+1},x_{t+1}^i ) \\mid h_t^i , a_t , x_{t+1}^i \\big ]      \\\\",
    "= { \\mathbb{e}}^{\\beta^i_{t : t},\\beta_{t : t}^{-i,\\star},\\mu_{t}^\\star[h_t^c ] } \\big [ \\sum_{n = t+1}^t \\delta^{n-(t+1 ) } r^i(x_n , a_n )       \\\\      + \\delta^{t+1-t }",
    "g^i(\\pi_{t+1},x_{t+1}^i ) \\mid h_t^i , a_t , x_{t+1}^i \\big ]      \\end{gathered}\\ ] ]    this result relies on the structure of the update in  , specifically that @xmath91 $ ] is a deterministic function of @xmath92 $ ] , @xmath93 , @xmath94 and does not depend on @xmath78 .",
    "consider the joint pmf - pdf of random variables involved in the expectation @xmath95 } \\big ( x_{t+1}^{-i},a_{t+1:t } , x_{t+2:t},x_{t+1}^i ,      \\\\      \\pi_{t+1 } \\mid h_t^i , a_t , x_{t+1}^i \\big )       \\end{gathered}\\ ] ] this can be written as @xmath96 with    [ eqnumden ] @xmath97 } \\big ( x_t^{-i},a_t , x_{t+1},a_{t+1:t},x_{t+2:t } ,          \\\\",
    "x_{t+1}^i,\\pi_{t+1 } \\mid h_t^i \\big )           \\end{gathered}\\ ] ] @xmath98}\\big ( \\tilde{x}_t^{-i } , a_t , x_{t+1}^i \\mid h_t^i \\big )            \\end{gathered}\\ ] ]    using causal decomposition we can write    @xmath97}(x_t^{-i } \\mid h_t^{i } ) \\beta_t^i(a_t^i \\mid h_t^i )           \\\\",
    "\\beta_t^{-i,\\star}(a_t^{-i } \\mid h_t^c , x_t^{-i } ) q(x_{t+1 } \\mid x_t , a_t )           \\\\          { \\mathbb{p}}^{\\beta_{t : t}^i,\\beta_{t : t}^{-i,\\star},\\mu_t^\\star[h_t^c ] } ( a_{t+1:t},x_{t+2:t},x_{t+1}^i,\\pi_{t+1 }           \\\\          \\mid h_t^i , a_t , x_t^{-i},x_{t+1 } )          \\end{gathered}\\ ] ]    @xmath99(x_t^{-i } ) \\beta_t^i(a_t^i \\mid h_t^i ) \\beta_t^{-i,\\star}(a_t^{-i } \\mid h_t^c , x_t^{-i } )           \\\\",
    "q^i(x_{t+1}^i \\mid x_t^i , a_t ) q^{-i}(x_{t+1}^{-i } \\mid x_t^{-i},a_t )          \\\\          { \\mathbb{p}}^{\\beta_{t+1:t}^i,\\beta_{t+1:t}^{-i,\\star},\\mu_{t+1}^\\star[h_{t+1}^c ] } ( a_{t+1:t},x_{t+2:t},x_{t+1}^i,\\pi_{t+1 }           \\\\",
    "\\mid h_t^i , a_t , x_t^{-i},x_{t+1 } )          \\end{gathered}\\ ] ]    where the second equality follows from the fact that given @xmath100 and @xmath101 $ ] , the probability of @xmath102 depends on @xmath103 $ ] only through @xmath104 .",
    "also the second equality above uses the fact that types evolve conditionally independent given action .",
    "performing similar decomposition of the denominator @xmath105 and substituting back in the expression from   allows us to cancel the terms @xmath106 and @xmath107 . using the belief update from",
    ", this gives that the expression in   is    @xmath108(x_{t+1}^{-i } ) { \\mathbb{p}}^{\\beta_{t+1:t}^i,\\beta_{t+1:t}^{-i,\\star},\\mu_{t+1}^\\star[h_{t+1}^c ] } ( a_{t+1:t},x_{t+2:t } ,          \\\\",
    "x_{t+1}^i,\\pi_{t+1 } \\mid h_t^i , a_t , x_{t+1 } )          \\\\",
    "= { \\mathbb{p}}^{\\beta_{t+1:t}^i,\\beta_{t+1:t}^{-i,\\star},\\mu_{t+1}^\\star[h_{t+1}^c ] } ( x_{t+1}^{-i } , a_{t+1:t},x_{t+2:t } ,          \\\\          x_{t+1}^i,\\pi_{t+1 } \\mid h_t^i , a_t , x_{t+1}^i )          \\end{gathered}\\ ] ]    the above equality follows directly from definition .",
    "this completes the proof .",
    "the result below shows that the value function from the backwards recursive algorithm is higher than any reward - to - go .",
    "[ thmfh2 ] for any @xmath57 , @xmath55 , @xmath26 and @xmath78 , @xmath109,x_t^i ) \\ge w_t^{i,\\beta^i , t}(h_t^i )          \\end{gathered}\\ ] ]    we use backward induction for this . at time @xmath50 , using the maximization property from  ,    @xmath110,x_t^i )           \\\\          & \\triangleq { \\mathbb{e}}^{\\tilde{\\gamma}_t^{i , t}(\\cdot \\mid x_t^i),\\tilde{\\gamma}_t^{-i , t},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t )           \\\\",
    "\\nonumber           & + \\delta g^i\\big ( \\big[f(\\mu_t^{j,\\star}[h_t^c],\\tilde{\\gamma}_t^{j , t},a_t ) \\big]_{j=1}^n , x_{t+1}^i \\big ) \\mid \\mu_t^\\star[h_t^c],x_t^i \\big ]          \\\\          & \\ge { \\mathbb{e}}^{{\\gamma}_t^{i , t}(\\cdot \\mid x_t^i),\\tilde{\\gamma}_t^{-i , t},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t )           \\\\",
    "\\nonumber           & + \\delta g^i\\big ( \\big[f(\\mu_t^{j,\\star}[h_t^c],\\tilde{\\gamma}_t^{j , t},a_t ) \\big]_{j=1}^n , x_{t+1}^i \\big ) \\mid \\mu_t^\\star[h_t^c],x_t^i \\big ]          \\\\          & = w_t^{i,\\beta^i , t}(h_t^i )          \\end{aligned}\\ ] ]    here the second inequality follows from   and   and the final equality is by definition in  .",
    "assume that the result holds for all @xmath111 , then at time @xmath8 we have    @xmath112,x_t^i )           \\\\          & \\ge { \\mathbb{e}}^{\\beta_t^i,\\beta_t^{-i,\\star},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t )           \\\\",
    "\\nonumber           & + \\delta v_{t+1}^{i , t}\\big ( \\big[f(\\mu_t^{j,\\star}[h_t^c],\\beta_t^{j,\\star},a_t)\\big]_{j=1}^n , x_{t+1}^i \\big ) \\mid h_t^i \\big ]          \\\\          & \\ge { \\mathbb{e}}^{\\beta_t^i,\\beta_t^{-i,\\star},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t )           \\\\ \\nonumber           & + \\delta { \\mathbb{e}}^{\\beta^i_{t+1:t},\\beta_{t+1:t}^{-i,\\star},\\mu_{t+1}^\\star[h_t^c , a_t ] } \\big [ \\sum_{n = t+1}^t \\delta^{n-(t+1 ) } r^i(x_n , a_n )           \\\\",
    "\\nonumber           & + \\delta^{t - t } g^i(\\pi_{t+1},x_{t+1}^i ) \\mid h_t^i , a_t , x_{t+1}^i \\big ] \\mid h_t^i \\big ]          \\\\          & = { \\mathbb{e}}^{\\beta^i_{t : t},\\beta^{-i,\\star}_{t : t},\\mu_t^\\star[h_t^c ] } \\big [ \\sum_{n = t}^t \\delta^{n - t } r^i(x_n , a_n )           \\\\",
    "\\nonumber           & + \\delta^{t+1-t}g^i(\\pi_{t+1},x_{t+1}^i ) \\mid h_t^i \\big ]          \\\\          & = w_t^{i,\\beta^i , t}(h_t^i )          \\end{aligned}\\ ] ]    here the first inequality follows from lemma  [ thmfh1 ] , the second inequality from the induction hypothesis , the third equality follows from lemma  [ lemcond ] and the final equality by definition  .",
    "in this section we consider the infinite horizon dynamic game , with naturally no terminal reward .",
    "a perfect bayesian equilibrium is the pair of strategy and belief @xmath68 , where    @xmath113    such that sequential rationality is satisfied : @xmath59 @xmath55 , @xmath78 , @xmath4 , @xmath114 , @xmath115 } \\big [ \\sum_{n = t}^\\infty \\delta^{n - t } r^i(x_n , a_n ) \\mid h_t^i \\big ]   \\\\ \\ge { \\mathbb{e}}^{\\beta^{i},\\beta^{-i,\\star},\\mu_t^\\star[h_t^c ] } \\big [ \\sum_{n = t}^\\infty \\delta^{n - t } r^i(x_n , a_n ) \\mid h_t^i \\big]\\end{gathered}\\ ] ] and beliefs satisfy certain consistency conditions ( please refer  @xcite for the exact conditions ) .      in this section , we state the fixed - point equation that defines the value function and strategy mapping for the infinite horizon problem .",
    "this is analogous to the backwards recursion ( and  ) that defined the value function and @xmath70 mapping for the finite horizon problem .",
    "define the set of functions @xmath116 and strategies @xmath117 ( which is generated formally as @xmath118 $ ] for given @xmath119 ) via the following fixed - point equation : @xmath59 @xmath55 , @xmath60 ,    [ eqihfpe ] @xmath120_{j=1}^n , x^{i,\\prime}\\big ) \\mid { \\underline{\\pi}},x^i \\big],\\end{gathered}\\ ] ] @xmath121_{j=1}^n , x^{i,\\prime}\\big ) \\mid { \\underline{\\pi}},x^i \\big].\\end{gathered}\\ ] ]    note that the above is a joint fixed - point equation in @xmath122 , unlike the backwards recursive algorithm earlier which required solving a fixed - point equation only in @xmath123 . here",
    "the unknown quantity is distributed as @xmath124 and @xmath125 is as defined in  .",
    "define belief @xmath71 inductively as follows : set @xmath72 .",
    "then for @xmath4 ,    [ eqmuih ] @xmath126   & = f\\big ( \\mu_{t}^{i,\\star}[h_t^c ] , \\theta^i\\big[\\mu_{t}^{\\star}[h_t^c]\\big ] , a_{t } \\big ) \\\\",
    "\\mu_{t+1}^\\star\\big [ h_{t+1}^c \\big](x_{t+1 } )   & = \\prod_{i=1}^n \\mu_{t+1}^{i,\\star}\\big [ h_{t+1}^c \\big](x_{t+1}^i)\\end{aligned}\\ ] ]    by construction",
    "the belief defined above satisfies the consistency condition needed for a perfect bayesian equilibrium .",
    "denote the stationary strategy arising out of @xmath123 by @xmath127 i.e. , @xmath128\\big](a_t^i \\mid x_t^i)\\end{gathered}\\ ] ]      the following result highlights the similarities between the fixed - point equation in infinite horizon and the backwards recursion in the finite horizon .    [ lemfh ] consider the finite horizon game with @xmath129 .",
    "then @xmath130 , @xmath59 @xmath55 , @xmath57 satisfies the backwards recursive construction   and  .",
    "use backward induction for this .",
    "consider the finite horizon algorithm at time @xmath131 , noting that @xmath132 ,    [ eqfht ] @xmath133_{j=1}^n , x_{t+1}^i \\big ) \\mid { \\underline{\\pi}}_t , x_t^i \\big ]          \\end{gathered}\\ ] ] @xmath134_{j=1}^n , x_{t+1}^i \\big ) \\mid { \\underline{\\pi}}_t , x_t^i \\big ]          \\end{gathered}\\ ] ]    comparing the above set of equations with  , we can see that the pair @xmath122 arising out of   satisfies the above .",
    "now assume that @xmath135 for all @xmath111 . at time @xmath8 , in the finite horizon construction from  ,  , substituting @xmath136 in place of @xmath137 from the induction hypothesis , we get the same set of equations as  .",
    "thus @xmath138 satisfies it .",
    "below we state the central result of this paper .",
    "it states that the strategy - belief pair @xmath68 constructed from the solution of the fixed - point equation   and the forward recursion of   and   indeed constitutes a pbe .",
    "[ thih ] assuming that the fixed - point equation   admits an absolutely bounded solution @xmath136 ( for all @xmath55 ) , the strategy - belief pair @xmath68 defined in   and   is a pbe of the infinite horizon discounted reward dynamic game i.e. , @xmath59 @xmath55 , @xmath78 , @xmath4 , @xmath114 , @xmath139 } \\big [ \\sum_{n = t}^\\infty \\delta^{n - t } r^i(x_n , a_n ) \\mid h_t^i \\big ]       \\\\      \\ge      { \\mathbb{e}}^{\\beta^{i},\\beta^{-i,\\star},\\mu_t^\\star[h_t^c ] } \\big [ \\sum_{n = t}^\\infty \\delta^{n - t } r^i(x_n ,",
    "a_n ) \\mid h_t^i \\big ]      \\end{gathered}\\ ] ]    [ [ remark ] ] remark + + + + + +    note that by definition in  , @xmath71 already satisfies the consistency conditions required for perfect bayesian equilibrium .",
    "we divide the proof into two parts : first we show that the value function @xmath136 is at least as big as any reward - to - go function ; secondly we show that under the strategy @xmath140 , reward - to - go is @xmath136 .    [ [ part-1 ] ] part 1 + + + + + +    for any @xmath55 , @xmath78 define the following reward - to - go functions    [ eqihr2 g ] @xmath141 } \\big [ \\sum_{n = t}^\\infty \\delta^{n - t } r^i(x_n , a_n ) \\mid h_t^i \\big ] \\end{gathered}\\ ] ] @xmath142 } \\big [ \\sum_{n = t}^t \\delta^{n - t } r^i(x_n , a_n )   \\\\ + \\delta^{t+1-t } v^{i}(\\underline{\\pi}_{t+1},x^i_{t+1 } ) \\mid h_t^i \\big]\\end{gathered}\\ ] ]    since @xmath81 are finite sets the reward @xmath18 is absolutely bounded , the reward - to - go @xmath143 is finite @xmath59 @xmath83 .    for any @xmath55 , @xmath114 , @xmath144,x_t^i\\big ) - w_t^{i,\\beta^i}(h_t^i )",
    "=   \\\\ \\big ( v^i\\big(\\mu_t^\\star[h_t^c],x_t^i\\big ) - w_t^{i,\\beta^i , t}(h_t^i ) \\big )   + \\big ( w_t^{i,\\beta^i , t}(h_t^i ) - w_t^{i,\\beta^i}(h_t^i ) \\big)\\end{gathered}\\ ] ] combining results from lemma  [ thmfh2 ] and  [ lemfh ] , the term in the first bracket in rhs of   is non - negative .",
    "using  , the term in the second bracket is @xmath145 } \\big[- \\sum_{n = t+1}^\\infty \\delta^{n-(t+1 ) } r^i(x_n , a_n )   \\\\ +   v^{i}(\\underline{\\pi}_{t+1},x^i_{t+1 } ) \\mid h_t^i \\big].\\end{gathered}\\ ] ] the summation in the expression above is bounded by a convergent geometric series .",
    "also , @xmath136 is bounded .",
    "hence the above quantity can be made arbitrarily small by choosing @xmath50 appropriately large .",
    "since the lhs of   does not depend on @xmath50 , this gives that @xmath146,x_t^i\\big ) \\ge w_t^{i,\\beta^i}(h_t^i ) \\end{gathered}\\ ] ]    [ [ part-2 ] ] part 2 + + + + + +    since the strategy @xmath127 generated in   is such that @xmath147 depends on @xmath26 only through @xmath101 $ ] and @xmath7 , the reward - to - go @xmath148 , at strategy @xmath127 , can be written ( with abuse of notation ) as @xmath149,x_t^i )   \\\\ = { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big [ \\sum_{n = t}^\\infty \\delta^{n - t } r^i(x_n , a_n ) \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ]    for any @xmath114 ,    @xmath150,x_t^i )   = { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t ) + \\delta w_{t+1}^{i,\\beta^{i,\\star } } \\\\",
    "\\big(\\big[f(\\mu_t^{j,\\star}[h_t^c],\\theta^i[\\mu_t^\\star[h_t^c]],a_{t+1}^j)\\big]_{j=1}^n , x_{t+1}^i\\big )   \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ]    @xmath151,x_t^i )   = { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big [ r^i(x_t , a_t ) + \\delta v^{i } \\\\",
    "\\big(\\big[f(\\mu_t^{j,\\star}[h_t^c],\\theta^i[\\mu_t^\\star[h_t^c]],a_{t+1}^j)\\big]_{j=1}^n , x_{t+1}^i\\big )   \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ]    repeated application of the above for the first @xmath152 time periods gives    @xmath150,x_t^i )   = { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big [ \\sum_{m = t}^{t+n-1 } \\delta^{m - t } r^i(x_t , a_t )    \\\\",
    "+ \\delta^{n }   w_{t+n}^{i,\\beta^{i,\\star}}\\big(\\pi_{t+n},x_{t+n}^i\\big )   \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ]    @xmath151,x_t^i )   = { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big [ \\sum_{m = t}^{t+n-1 } \\delta^{m - t } r^i(x_t , a_t )    \\\\ + \\delta^{n }   v^{i}\\big(\\pi_{t+n},x_{t+n}^i\\big )   \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ]    here @xmath153 is the @xmath154step belief update under strategy and belief prescribed by @xmath155 .",
    "taking difference gives    @xmath150,x_t^i )   - v^{i}(\\mu_t^\\star[h_t^c],x_t^i )   \\\\",
    "= \\delta^n { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big [ w_{t+n}^{i,\\beta^{i,\\star}}\\big(\\pi_{t+n},x_{t+n}^i\\big )   \\\\ - v^{i}\\big(\\pi_{t+n},x_{t+n}^i\\big ) \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ]    taking absolute value of both sides then using jensen s inequality for @xmath156 and finally taking supremum over @xmath26 gives us @xmath157,x_t^i )   - v^{i}(\\mu_t^\\star[h_t^c],x_t^i ) \\big\\vert    \\\\ \\le   \\delta^n \\sup_{h_t^i }   { \\mathbb{e}}^{\\beta^{\\star},\\mu_t^\\star[h_t^c ] } \\big[\\big\\vert w_{t+n}^{i,\\beta^{i,\\star}}(\\pi_{t+n},x_{t+n}^i )    \\\\ - v^{i}(\\mu_t^\\star[h_t^c],x_t^i ) \\big\\vert   \\mid \\mu_t^\\star[h_t^c],x_t^i \\big]\\end{gathered}\\ ] ] now using the fact that @xmath158 are bounded and that we can choose @xmath152 arbitrarily large , we get @xmath159,x_t^i )   - v^{i}(\\mu_t^\\star[h_t^c],x_t^i ) \\vert = 0 $ ] .",
    "in this section , we consider an infinite horizon version of the public goods example from  ( * ? ? ? * ch .  8 ,",
    "example 8.3 ) .",
    "we solve the corresponding fixed point equation ( arising out of  ) numerically to calculate the mapping @xmath70 ( which in turn generates the perfect bayesian equilibrium @xmath68 ) .",
    "the example consists of two symmetric agents .",
    "the type space and action sets are @xmath160 and @xmath161 .",
    "each agents type is static and does not vary with time .",
    "the actions represents whether agents are willing to contribute for a common public good .",
    "if at least one agent contributes then both agents receive utility @xmath16 and the agent(s ) that contributed receive cost equal to their type .",
    "if no one contributes then both agents receive utility @xmath162 .",
    "thus the reward function is @xmath163 where @xmath164 represents the action taken by the agent other than @xmath3 .",
    "we use the following values @xmath165 , @xmath166 and consider three values @xmath167 . since type sets have two elements we can represent the distribution @xmath168 with only @xmath169 $ ] , similarly for agent @xmath170 . for any @xmath171 ^ 2 $ ]",
    ", the mapping @xmath172 $ ] produces @xmath173 for every @xmath174 and @xmath175 .",
    "since the action space contains two elements , we can represent the distribution @xmath173 by @xmath176 i.e. , the probability of taking action @xmath16 .",
    "we solve the fixed - point equation by discretizing the @xmath177space @xmath178 ^ 2 $ ] and all solutions that we find are symmetric w.r.t .",
    "agents i.e. , @xmath179 for @xmath180 is the same as @xmath181 for @xmath182 and similarly for type @xmath183 .    for @xmath184 ,",
    "the game is instantaneous and for the values considered , we have @xmath185 .",
    "this implies that whenever agent @xmath16 s type is @xmath183 , it is instantaneously profitable not to contribute .",
    "this gives @xmath186 , for all @xmath119 .",
    "thus we only plot @xmath187 ; in fig .  [",
    "fig : del0_p1l ] . for @xmath188",
    "the fixed - point equation   is only for the variable @xmath123 and not @xmath189 , and can be solved analytically .",
    "refer to  ( * ? ? ?",
    "* eq .  ( 20 ) and fig .  ( 1 ) ) , where this solution is stated .",
    "there are multiple solutions to the fixed - point equation and our result from fig .",
    "[ fig : del0_p1l ] matches with the one of the results in  @xcite .",
    "intuitively , with type @xmath190 the only value of @xmath119 for which agent 1 would not wish to contribute is if he anticipates agent @xmath170 s type to be @xmath190 with high probability and rely on agent 2 to contribute .",
    "this is why for lower values of @xmath191 ( i.e. , agent @xmath170 s type likely to be @xmath190 ) we see @xmath192 in fig .",
    "[ fig : del0_p1l ] .    now consider @xmath187 plotted in fig .",
    "[ fig : del0_p1l ] ,  [ fig : del05_p1l ] and  [ fig : del095_p1l ] . as @xmath193 increases ,",
    "future rewards attain more priority and signaling comes into play .",
    "so while taking an action , agents not only look for their instantaneous reward but also how their action affects the future public belief @xmath194 about their private type .",
    "it is evident in the figures that as @xmath193 increases , at high @xmath195 , up to larger values of @xmath191 agent @xmath16 chooses not to contribute when his type is @xmath190 . this way he intends to send a `` wrong '' signal to agent @xmath170 i.e. , that his type is @xmath183 and subsequently force agent @xmath170 to invest .",
    "this way agent 1 can free - ride on agent @xmath170 s investment .",
    "now consider fig .",
    "[ fig : del05_p1h ] and  [ fig : del095_p1h ] , where @xmath196 is plotted .",
    "coordination via signaling is evident here .",
    "although it is instantaneously not profitable to contribute if agent @xmath16 s type is @xmath183 , by contributing at higher values of @xmath191 ( i.e. , agent @xmath170 s type is likely @xmath183 ) and low @xmath195 , agent @xmath16 coordinates with agent @xmath170 to achieve net profit greater than @xmath162 ( reward when no one contributes ) . this can be done since the loss of contributing is @xmath197 whereas profit from free - riding on agent @xmath170 s contribution is @xmath16 .    under the equilibrium strategy ,",
    "beliefs @xmath198 form a markov chain .",
    "one can trace this markov chain to study the signaling effect at equilibrium . on numerically simulating this markov chain for the above example ( at @xmath199 )",
    "we observe that for almost all initial beliefs , within a few rounds agents completely learn each other s private type truthfully ( or at least with very high probability ) . in other words , agents manage to reveal their private type via their actions at equilibrium and to such an extent that it negates any possibly incorrect initial belief about their type .    as a measure of cooperative coordination at equilibrium one can perform the following calculation .",
    "compare the value function @xmath200 of agent @xmath16 arising out of the fixed - point equation , for @xmath201 and @xmath202 ( normalize it by multiplying with @xmath203 so that it represents per - round value ) with the best possible attainable single - round reward under a symmetric mixed strategy with a ) full coordination and b ) no coordination . note that the two cases need not be equilibrium themselves , which is why this will result in a bound on the efficiency of the evaluated equilibria . in case",
    "a ) , assuming both agents have the same type @xmath204 , full coordination can lead to the best possible reward of @xmath205 i.e. , agent @xmath16 contributes with probability @xmath206 and agent @xmath170 contributes with probability @xmath206 but in a coordinated manner so that it does nt overlap with agent @xmath16 contributing .    in case",
    "b ) when agents do not coordinate and invest with probability @xmath207 each",
    ", then the expected single - round reward is @xmath208 .",
    "the maximum possible value of this expression is @xmath209 .    for @xmath210 ,",
    "the range of values of @xmath211 over @xmath212 ^ 2 $ ] is @xmath213 $ ] . whereas full coordination produces @xmath214 and no coordination @xmath215 .",
    "it is thus evident that agents at equilibrium end up achieving reward close to the best possible and gain significantly compared to the strategy of no coordination .",
    "similarly for @xmath216 the range is @xmath217 $ ] . whereas full coordination produces @xmath218 and no coordination @xmath219 .",
    "the gain via coordination is evident here too .",
    "vs. @xmath220 at @xmath184.,scaledwidth=50.0% ]     vs. @xmath220 at @xmath221.,scaledwidth=50.0% ]     vs. @xmath220 at @xmath221.,scaledwidth=50.0% ]     vs. @xmath220 at @xmath201.,scaledwidth=50.0% ]     vs. @xmath220 at @xmath201.,scaledwidth=50.0% ]",
    "this paper considers the infinite horizon discounted reward dynamic game with private types i.e. , where each agent can only observe their own type .",
    "the types evolve as a controlled markov process and are conditionally independent across agents given the action profile .",
    "asymmetry of information between agents exists in this model , since each agent only knows their own private type .    to date",
    ", there exists no universal algorithm for calculating pbe in models with asymmetry of information that decouples , w.r.t .",
    "time , the calculation of strategy .",
    "section  [ secih ] provides a single - shot fixed - point equation for calculating the equilibrium generating function @xmath70 , which in conjunction with the forward recursion in   and   gives a subset of pbes @xmath68 of this game .",
    "the method proposed in this paper finds pbe of a certain type i.e. , where for any agent @xmath3 , his strategy at equilibrium depends on the public history only through the common belief @xmath22 and on private history only through agent @xmath3 s current private type @xmath7 .",
    "finally , we demonstrate our methodology by a concrete example of a two agent symmetric public goods game and observe the signaling effect in agents strategies at equilibrium as discount factor is increased .",
    "the signaling effect implies that agents take into account how their actions affect future public beliefs @xmath119 about their private type .    one important direction for future work is characterization of games where the proposed spbe exists .",
    "this boils down to existence of a solution to the fixed - point equation in section  [ secih ] , as any spbe must satisfy this equation",
    ".                    t.  n. le , v.  g. subramanian , and r.  a. berry , `` the impact of observation and action errors on informational cascades , '' in _",
    "53rd ieee conference on decision and control_.1em plus 0.5em minus 0.4emieee , 2014 , pp .",
    "19171922 .",
    "d.  vasal and a.  anastasopoulos .",
    "( 2016 ) decentralized bayesian learning in dynamic games .",
    "submitted to ieee allerton conference 2016 .",
    "[ online ] .",
    "available : http://web.eecs.umich.edu/~anastas/docs/netecon16.pdf    a.  nayyar , a.  mahajan , and d.  teneketzis , `` decentralized stochastic control with partial history sharing : a common information approach , '' _ ieee transactions on automatic control _",
    "58 , no .  7 , pp .",
    "16441658 , july 2013 .",
    "a.  gupta , a.  nayyar , c.  langbort , and t.  baar , `` common information based markov perfect equilibria for linear - gaussian games with asymmetric information , '' _ siam journal on control and optimization _ , vol .",
    "52 , no .  5 ,",
    "32283260 , 2014 .",
    "a.  nayyar , a.  gupta , c.  langbort , and t.  baar , `` common information based markov perfect equilibria for stochastic games with asymmetric information : finite games , '' _ ieee transactions on automatic control _ ,",
    "59 , no .  3 , pp .",
    "555570 , march 2014 .",
    "d.  vasal and a.  anastasopoulos , `` a systematic process for evaluating structured perfect bayesian equilibria in dynamic games with asymmetric information , '' in _ american control conference _",
    ", july 2016 .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1508.06269    d.  vasal and a.  anastasopoulos .",
    "( 2015 ) signaling equilibria for dynamic lqg games with asymmetric information .",
    "submitted to ieee cdc 2016 .",
    "[ online ] .",
    "available : http://www-personal.umich.edu/~dvasal/papers/cdc16.pdf"
  ],
  "abstract_text": [
    "<S> in dynamic games with asymmetric information structure , the widely used concept of equilibrium is perfect bayesian equilibrium ( pbe ) . </S>",
    "<S> this is expressed as a strategy and belief pair that simultaneously satisfy sequential rationality and belief consistency . unlike symmetric information dynamic games , where subgame perfect equilibrium ( spe ) is the natural equilibrium concept , to date there </S>",
    "<S> does not exist a universal algorithm that decouples the interdependence of strategies and beliefs over time in calculating pbe . in this paper </S>",
    "<S> we find a subset of pbe for an infinite horizon discounted reward asymmetric information dynamic game . </S>",
    "<S> we refer to it as structured pbe or spbe ; in spbe , any agents strategy depends on the public history only through a common public belief and on private history only through the respective agents latest private information ( his private type ) . </S>",
    "<S> the public belief acts as a summary of all the relevant past information and it s dimension does not increase with time . </S>",
    "<S> the motivation for this comes the common information approach proposed in nayyar et al . </S>",
    "<S> ( 2013 ) for solving decentralized team ( non - strategic ) resource allocation problems with asymmetric information . </S>",
    "<S> we calculate spbe by solving a single - shot fixed - point equation and a corresponding forward recursive algorithm . </S>",
    "<S> we demonstrate our methodology by means of a public goods example . </S>"
  ]
}