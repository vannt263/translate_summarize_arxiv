{
  "article_text": [
    "since the terminology was coined in 1999 @xcite , cognitive radio ( cr ) has been developed for more than fifteen years , which has drawn attention from both academic and industrial communities since it is intended to enable smart use of the scarce spectrum resource with the initial objective of maximizing spectrum utilization . recently , cognitive network design goes beyond spectrum utilization and target at broader network objectives such as higher quality of service , lower energy cost , etc . to achieve such new objectives ,",
    "the statistical knowledge on the primary network status becomes necessary @xcite for resource management and system control , which gets us closer to the ideal cr operation that integrates spectrum sensing , environment learning , statistical reasoning , and predictive acting .",
    "this will go beyond most of the existing cr sensing literature , which usually focus on detecting the presence of primary users only @xcite .    in practice ,",
    "the network behavior is impacted by many factors which may change dynamically , such that the uncertainty of a network can not be priorly represented by a certain statistical distribution . to cope with such an issue",
    ", the statistical machine learning methodology becomes a feasible solution for understanding the network activity pattern . in this paper",
    ", we introduce the bayesian network ( bn ) @xcite structure learning method to obtain the statistical primary networking pattern , via observing the on / off status of primary base stations .",
    "the bayesian model has been well known in the field of artificial intelligence ( ai ) @xcite . when considering the probability and uncertainty , bn is a distinct technique for modeling the complex interaction among real world facts @xcite . in particular ,",
    "the bn structure learning is an effective new modeling tool in both spatial and temporal domains .",
    "however , the associated computational complexity is high since it needs to evaluate the dependence between each pair of variables in a target system and compute the corresponding conditional probability table @xcite , which becomes the major drawback in applying bn structure learning . in this paper",
    ", we focus on reducing the computational load for efficiently learning the statistical behavior patterns of primary users .    for bn structure learning",
    ", the related algorithms could be sorted into two categories .",
    "one is to use heuristic searching to construct a probable model and then evaluate it by a scoring function @xcite .",
    "the structure with the highest score is preferred as the learning outcome .",
    "the score - based approach of learning bns has been proven as a np - hard problem @xcite .",
    "the other one is to use conditional independence test to measure every possible dependence relationships one by one , and then determine the structure based on the evaluated dependence . in @xcite ,",
    "the authors show a much more efficient approach to learn the ordered bn by using mutual information to check the dependence of any possible pairs of nodes .",
    "however , this approach can not be adaptively adjusted when the number of variable changes . to overcome the drawbacks of the current learning methods , we propose a structure learning algorithm based on a completely connected graph and the conditional mutual information , which could efficiently learn both the structure and the corresponding conditional probability table .",
    "in particular , each pair of variables in the bn is defined with a generalized relationship where the independence case is unified as the weakest dependence case . as a result , the bn network structure is completely connected .",
    "accordingly , the network structure becomes very regular , such that the conditional mutual information based learning could be formulated as a sequence of closed - form function evaluations . with this",
    ", our learning algorithm not only has the same computational overhead as that in @xcite , but can also dynamically adapt to different numbers of variables .",
    "moreover , for further reducing the computational complexity , we simplify the conditional mutual information function and explore the prior knowledge that the cr sensing results are binary .    besides the complexity problem , there are some special issues in learning bn structure in the context of cr , due to the lack of collaboration from the primary user side . in conventional learning cases ,",
    "many existing works assume the number of variables and their related observations are usually prior knowledge and hence only focus on learning the structure among the variables .",
    "however , in cr , the observations are usually collected without recording the correct time epoch , causing the observation to be unidentifiable on which time that it belongs to .",
    "this is mainly due to the fact that the statistical period of the bn structure , which reflects the temporal pattern , is not known _ a priori_. in essence , the above issues belong to the unsupervised classification problem , with a new challenge to consider the missing time period . to address this ,",
    "we propose a blind variable identification algorithm , combining with the proposed structure learning algorithm , to learn the period via the fast fourier transformation ( fft ) . in conclusion ,",
    "the proposed structure and period learning algorithms constitute a complete bn structure learning scheme , by which cr could understand the statistical pattern of primary base stations in both spatial and temporal domains .",
    "the remainder of the paper is organized as follows . in section",
    "ii , the system model is presented in details .",
    "section iii briefly introduces the relationship between the bn method and the wireless communication problem in cr . in section iv",
    ", an efficient algorithm is proposed for jointly learning the structure and the corresponding conditional probability table . in section",
    "v , an algorithm is proposed to obtain the statistical period of bn structure . in section",
    "vi , the simulation and learning results are presented to validate the proposed scheme .",
    "finally , we conclude the paper in section vii .",
    "our system model consists of a primary cellular network and multiple secondary sensors over an observation area , of which the detailed specifications are given as follows .",
    "the primary cellular network consists of several primary base stations and multiple mobile primary users .",
    "the observation area consists of the cells and a road as illustrated in fig .",
    "[ f1 ] , where we consider the mobile primary users only moving along a one - way ( from right to left ) road for our case of study .",
    "the arrival of the users at the entrance of the road follows a poisson process .",
    "the arrived mobile users pass along the road at a speed generated by a uniform distribution .",
    "once the mobile users move out the road , they are no longer observed .",
    "additionally , the primary network obeys the following setup : 1 ) the primary base stations are located based on a pre - designed network deployment plan ( i.e. , at the centers of the cells ) ; 2 ) the primary users share a single channel and access a primary base station depending on which cell they are located geographically ; 3 ) an ideal tdma - based multiple access ( mac ) scheme is adopted , and the arrival and departure of primary data traffic at each user follow a poisson process and an exponential service law , respectively .",
    "hence , the on / off status of a primary base station is determined by the overall data traffic generated from the mobile primary users in its cell ( here we only consider the uplink transmissions ) .",
    "a cognitive secondary sensor is installed very close to each primary base station , which implies that the accuracy of sensing could be assumed perfect , such that no sensing errors are considered in this paper .",
    "the secondary sensors sense the on / off status of primary base stations periodically in a synchronous fashion .",
    "let set @xmath0 denote the observed primary base stations , and set @xmath1 denote the sequence of time epoch @xmath2 , @xmath3 .",
    "in addition , let @xmath4 be a variable denoting the state of the @xmath5-th primary base station at time @xmath2 , and @xmath6 , where @xmath7 and @xmath8 represent the off and on statuses respectively .",
    "in cr , as we argued before , it is valuable to know the statistical behavior of the primary network . by exploring such knowledge , the secondary network could exploit the idle spectrum resource more efficiently and more broadly . in our setup , each secondary sensor could obtain a number of observations ( or samples ) about the on / off status of the observed primary base station , which is the key element of the primary network . the existing works with respect to cr sensing have been mainly focused on the busy / idle status of a particular spectrum hole by observing a specific primary base station .",
    "contrastingly , our objective is to learn the statistical pattern of the spatial and temporal behavior of multiple networked primary base stations by mining the obtained observations over the whole network across different time epochs . to achieve our objective ,",
    "bn structure learning is deployed as the key methodology .",
    "the bn framework has been known in the field of artificial intelligence and exploited in different expert systems to model complex interactions among causes and consequences , while bn structure learning aims to derive and quantify the complex interaction from data .      with bn ,",
    "the spatial and temporal interactions among primary base stations are expressed by a directed graph @xmath9 , where @xmath10 is a finite , nonempty set whose elements are called the nodes denoting the variable @xmath4 , and @xmath11 is a set of directed lines called the edges connecting the pairs of distinct elements in @xmath10 . if there is a directed edge from @xmath12 to @xmath13 where @xmath14 , it means that @xmath13 is impacted by @xmath12 i.e. , @xmath13 depends on @xmath12 . in bn analysis ,",
    "such a directional relationship is generally expressed by a conditional probability @xmath15 .",
    "for the graphical bn , it has a distinguishing feature that : for an arbitrary @xmath16 , it is conditionally independent of the set of all other indirectly connected nodes given the set of all directly connected nodes .",
    "apparently , if we have the complete knowledge on @xmath17 together with all the values of @xmath15 , the statistical pattern of the network behavior is readily available .      as explained above , for quantifying the bn and the related graph from observations , we need to determine both the variables ( nodes ) and the dependence of each pair of variables ( edge )",
    "in many applications , the number of variables is priorly known .",
    "hence , most of learning results mainly focus on how to learn the edges efficiently . in our cr scenario , since the observations are collected from the deployed sensors , it is easy to identify @xmath18 for the range of @xmath5 in @xmath4 .",
    "however , due to the randomness of the primary user number , speed , and traffic , the temporal information about @xmath19 , which defines the range for @xmath2 of @xmath4 , is not directly known .",
    "thus , bn learning in cr not only needs to efficiently mine the relationship and interaction among the variables , but also has to identify the temporal scale @xmath20 of bn .",
    "since @xmath20 is not known , we can not simply sort the observations into the corresponding nodes .",
    "hence , the unknown time scale @xmath20 becomes a critical issue in the proposed cr bn learning . in machine learning language",
    ", such an issue belongs to the unsupervised classification problem , which is widely considered difficult .    .",
    "the dashed line means that the value of an edge has not been determined.,width=328 ]    for edge learning , the traditional learning methods are based on scoring or dependence checking functions . given a scoring function , the computational complexity of determining the bn structure increases exponentially when the number of variables increases .",
    "the score - based approach for learning bayesian networks has been shown np - hard @xcite , which is a key challenge in the learning community .",
    "recently , a relatively efficient way to learn an ordered bn is given in @xcite by using the conditional mutual information to check the dependence between any possible pair of nodes .",
    "formally , the conditional mutual information is defined as @xmath21 which is a measure of the mutual dependence between variables @xmath22 and @xmath23 given variable @xmath24 , where the marginal , joint , and/or conditional probability mass functions are denoted by @xmath25 with appropriate subscripts .",
    "evidently , this method demands for multiple nested for - loops for implementation , and the number of for - loops is determined by the number of variables in the bn .",
    "hence , this method is not directly applicable for the online learning case where the number of variables may be time - varying .",
    "moreover , in @xcite , the conditional mutual information checking is performed for every combination of all possible parent nodes of @xmath22 . in other words , @xmath26 is computed for all @xmath27 where @xmath27 denotes the @xmath28-combination of set @xmath29 , @xmath30 .",
    "the combination based checking introduces the huge computational overhead .      considering the unknown @xmath20 and the high computational complexity issues , we propose a bn model for our cr sensing case , and",
    "term it as cognitive bn ( cbn ) .",
    "the cbn has four characteristics : _ 1 _ ) it is a first - order bn and its nodes are ordered in the temporal domain ; _ 2 _ ) its structure is completely connected which means @xmath31 , there exists an edge between any @xmath32 and @xmath33 ; _ 3 _ ) the observation of each variable is binary ; _ 4 _ ) it has an unknown operation period . here , ",
    "ordered in the temporal domain \" means that for a given @xmath5 , @xmath34 are ordered in @xmath35",
    ". characteristics _ 3 _ and _ 4 _ are two direct outcomes from the system model .",
    "thus we only explain characteristics _ 1 _ and _ 2 _ below .    in our system model , the user movement and data service behaviors of primary users at the current time epoch @xmath2",
    "could be solely determined by the system states in the former time epoch @xmath36 . in other words ,",
    "our system model has the first - order markov property , which has been adopted previously @xcite . in @xcite",
    ", it is shown that a wireless communication network could be represented by a markov state transition system .",
    "hence , we have characteristic _ 1 _ in our model , which leads to huge complexity reduction in checking the multiple ordered nodes .",
    "next , we explain characteristic _ 2 _ in detail , which is unique and critical in further reducing the computational burden .    when learning the edges , the conventional approaches usually consider the edges either existing or absent , by analyzing the directed dependence that is based on the empirical probabilities generated from observations , where the edge weight could be considered as a bi - level quantization of the directed dependence @xcite . in contrast , this paper first considers the existence of every possible edge in the first - order bn model , and then quantifies the existence with an analog value to reflect the dependence level between any two nodes . in other words ,",
    "we consider independence as an extreme case of dependence with edge value @xmath37 , which implies that each pair of variables in the cbn has a generalized relationship .",
    "based on such an approach , the cbn has a completely connected structure that is highly regular , as shown in fig .",
    "[ f3 ] . in the next section ,",
    "we show that , by exploring the regularity of the cbn structure , both the analog - valued edges and the conditional probability table could be learned efficiently .",
    "in this section , we propose an efficient learning algorithm which could correctly work under an arbitrary period @xmath20 , while the problem of an unknown @xmath20 is studied in the next section .",
    "as explained before , high computational complexity is a critical issue in bn structure learning .",
    "it has been shown @xcite that the mutual information check based approach leads to the desired efficiency ; but it can not handle a dynamic number of variables . here , we proposed an efficient learning algorithm of the same complexity as the @xmath38 based method , and can cope with the varying number of variables .    when employing the conditional mutual information , every possible edge will be checked in turn . in other words ,",
    "the number of possible edges affects the learning overhead .",
    "recall that our cbn is a first - order bn and ordered in the temporal domain .",
    "it means that , in cbn , the direction of edges is known and the edge only connects the adjacent nodes for a given @xmath5 in @xmath4 as shown in fig . [ f3 ] .",
    "hence , the computational complexity of learning such a cbn is proportional to learning a subgraph enclosed by the bold - line rectangle in fig .",
    "if we do not consider the direction of edges , this subgraph is called a clique in a markov network . for the clarity of expression",
    ", we here call the targeted subgraph as a c - clique . according to characteristic",
    "_ 1 _ , the number of edges in a cbn is @xmath20 times as that in a c - clique .",
    "apparently , the computational complexity of learning cbn is linearly proportional to the overhead of learning a c - clique .",
    "hence , in the following study , we focus on how to efficiently learn a c - clique .      before introducing our idea ,",
    "we need to show how the current approaches learn a cbn by using the conditional mutual information check , which is helpful for us to understand the computational complexity of learning a c - clique with the proposed algorithm .    in conventional methods , when learning the edges in a c - clique , we need to determine the existence of possible edges one by one .",
    "for example , to check the edge between @xmath39 and @xmath40 with a realization of @xmath41 as shown in fig .",
    "[ f3 ] , the corresponding conditional mutual information @xmath42 is performed by three times for @xmath43 , @xmath44 , and @xmath45 .    actually , according to information theory , there exists @xmath46 where @xmath47 is a set containing all nodes at @xmath2 , and @xmath48 is a set containing all parent nodes of @xmath49 at @xmath2 .",
    "( [ 1.a.2 - 1 ] ) means that it is not necessary to check the mutual information conditioned on every possible @xmath50 . to utilize such results , the proposed completely connected structure shows a distinct merit that we only need perform the following checking once , @xmath51 where each probability is estimated by an empirical probability .    on the other hand ,    the computational load of calculating @xmath52",
    "is mainly determined by using the related observations to calculate both the empirical probability @xmath53 and the structure of @xmath52 function itself .",
    "apparently , the related computation requires all the realizations of @xmath54 , e.g. @xmath55 , @xmath56 , @xmath57 , @xmath58 . as a result",
    ", the computational complexity of measuring an edge depends on computing @xmath59 times of @xmath60 . for - loops . ] in general , since there are @xmath61 edges in a c - clique , the computational complexity of learning a c - clique is determined by running @xmath62 times of @xmath60 .",
    "such outcome implies that the computational complexity of learning a c - clique could be reduced by improving the dependency checking function . on the other hand , each term in ( [ 1.a.2 ] ) is actually based on the conditional probability table considering that the c - clique is completely connected .",
    "when each term in ( [ 1.a.2 ] ) is estimated by the empirical probabilities , the computational complexity of ( [ 1.a.2 ] ) is proportional to that of computing the conditional probability table .",
    "evidently , if we could efficiently learn the complete conditional probability table and quantify each edge by closed - form expressions and without nested for - loops , the issues of varying variables and high computational complexity will be solved and released , respectively .",
    "based on the above two ideas , we next propose an efficient algorithm based on analyzing the completely connected structure and exploring the fact of binary observations .",
    "as explained before , the efficiency of the mutual information based cbn learning is determined by two factors : the measurement method of dependence and the adaptation to the number of variables . among the two factors , the latter one plays a leading role . in this subsection",
    ", we show our effort to handle the second factor by utilizing characteristic _ 2 _ of our cbn .      in a completely connected bn",
    ", we need to compute @xmath63 , @xmath64 and @xmath65 , to obtain the conditional probability table , where @xmath66 means @xmath4 s parents that are the nodes * connecting to * @xmath4 directly . from a glance ,",
    "the completely connected bn is of high computational complexity because of the large number of edges .",
    "however , the fact is just the opposite .",
    "since the structure of a completely connected bn is perfectly symmetry , the algorithm of computing the conditional probability table could be designed efficiently , which will be discussed next .    for a variable @xmath4 ,",
    "consider its related observations contained in a column vector @xmath67 .",
    "for brevity , we take @xmath68 along with binary observations to explain our idea , and then extend the related results to a general case with @xmath69 . from the perspective of frequentist probability , the empirical conditional probability table of @xmath63 in a c - clique",
    "is given by @xmath70 where @xmath71 is the hadamard product , @xmath72 where @xmath73 , @xmath74 where @xmath75 , @xmath76 @xmath77 @xmath78 @xmath79 and @xmath80 it is easy to see that the empirical conditional probability could be calculated as multiplying the observation @xmath81 by a regular arithmetic operator denoted by @xmath82 .",
    "we add an index @xmath83 to @xmath82 to express the condition , e.g. , @xmath84 with @xmath85 stands for the arithmetic operator under @xmath86 and @xmath87 , such that we have @xmath88 .    from ( [ 2.a.1])-([2.a.4 ] ) and ( [ 2.a.5])-([2.a.8 ] )",
    ", we see that the computation of the conditional probability table is transformed to obtaining the arithmetic operator @xmath84 with every realization of index @xmath83 , where the structure of @xmath84 is very regular , which benefits from characteristic _ 2 _ of our cbn . based on the regularity of @xmath84 ,",
    "the edges no longer need to be learned one by one , which is explained as follows .      in cr ,",
    "the on / off behavior of a primary base station is expressed by a binary value , which leads to our learning algorithm exploring this fact .",
    "given binary observations , the number of realizations of @xmath83 is @xmath89 , based on which we define @xmath90 .",
    "accordingly , for a given @xmath91 , we could use ( [ q3.1.2.2.0 ] ) defined below to generate a corresponding vector @xmath92 leading to @xmath93\\times \\boldsymbol c_x[2]\\times\\cdots \\times\\boldsymbol c_x[m]$ ] , with @xmath94=\\left\\lfloor \\frac{x}{2^{i-1}}\\right\\rfloor\\backslash 2 \\end{aligned}\\ ] ] where @xmath95 denotes the modulo operation and @xmath96 .",
    "for example ,",
    "when @xmath68 and @xmath97 , @xmath98 $ ] , which means @xmath99 .",
    "further , we could generate a matrix @xmath100 where each row contains a realization of @xmath83 given @xmath91 : @xmath101=\\left\\lfloor \\frac{x}{2^{i-1}}\\right\\rfloor\\backslash 2 , i\\in\\mathbb m \\end{aligned}.\\ ] ]    on the other hand , when the observation is binary and we take @xmath68 as in the previous subsection , the numerators of @xmath102 , @xmath103 , @xmath104 , and @xmath105 could be reformulated as @xmath106+(1-\\boldsymbol c_3)(1-[\\boldsymbol o_{1,t-1}^t , \\boldsymbol o_{2,t-1}^t])}{2}\\right\\rfloor , \\end{aligned}\\ ] ] @xmath107+(1-\\boldsymbol c_2)(1-[\\boldsymbol o_{1,t-1}^t , \\boldsymbol o_{2,t-1}^t])}{2}\\right\\rfloor , \\end{aligned}\\ ] ] @xmath108+(1-\\boldsymbol c_1)(1-[\\boldsymbol o_{1,t-1}^t , \\boldsymbol o_{2,t-1}^t])}{2}\\right\\rfloor , \\end{aligned}\\ ] ] @xmath109+(1-\\boldsymbol c_0)(1-[\\boldsymbol o_{1,t-1}^t , \\boldsymbol o_{2,t-1}^t])}{2}\\right\\rfloor , \\end{aligned}\\ ] ] respectively .",
    "let @xmath110 and @xmath111 respectively denote the numerator and denominator of @xmath91 , and @xmath112 be a matrix containing all @xmath84 s .",
    "then , we arrive at a simple form for each realization of @xmath84 as @xmath113\\\\ = & \\left\\lfloor \\frac{\\boldsymbol c\\boldsymbol o_{t-1}+(1-\\boldsymbol c)(1-\\boldsymbol o_{t-1})}{2}\\right\\rfloor \\end{aligned}\\ ] ] where @xmath114^t , \\end{aligned}\\ ] ] @xmath115 with @xmath100 obtained from ( [ q3.1.2.2.1 ] ) .    on the other hand ,",
    "since @xmath116 is a matrix , we have @xmath117 , where @xmath118 is a column vector of @xmath119 ones .",
    "thus , the denominator of @xmath112 is given by @xmath120    extending ( [ q3.1.2.2.6 ] ) to an arbitrary value of @xmath121 yields a general arithmetic operator @xmath112 , @xmath122 where @xmath123 denotes the kronecker product , and @xmath124}{m}\\right\\rfloor , \\end{aligned}\\ ] ] @xmath125^t .",
    "\\end{aligned}\\ ] ] hence , the empirical conditional probability table could be calculated as @xmath126 & = \\boldsymbol{\\mathcal f}\\boldsymbol o_{t}^t , \\end{aligned}\\ ] ] where @xmath127 is a column vector with size @xmath89 containing every probability that @xmath128 holds conditioned on @xmath83 .",
    "consequently , the bbcpt learning algorithm is given by ( [ q3.1.2.2.1 ] ) and ( [ q3.1.2.2.10])-([q3.1.2.2.13 ] ) . when compared with conventional methods ,",
    "the proposed bbcpt algorithm effectively reduces the number of for - loops to just two for - loops ( with matrix computation ) , and the overall empirical conditional probability table could be obtained by a sequence of closed - form function evaluations efficiently .    at the beginning of this subsection",
    ", we stated that the total computational overhead is affected by two factors , with the latter one already discussed in this subsection .",
    "next , we study the first one to show how to efficiently measure the dependence between any pair of two variables in a c - clique .",
    "given discrete random variables @xmath22 with support @xmath129 and @xmath23 with support @xmath130 , the conditional entropy between @xmath22 and @xmath23 is given by @xmath131 if @xmath23 is completely determined by @xmath22 , we have @xmath132 ; if @xmath23 is independent of @xmath22 , we have @xmath133 . hence , @xmath134 $ ] reflects the dependence of @xmath23 on @xmath22 . according to information theory",
    ", @xmath135 is equivalent to @xmath136 when measuring the dependence between @xmath22 and @xmath23 is a measure between variables for evaluating the variation of information or shared information distance . ] .    for the cr case where @xmath137 and @xmath138 , the conditional entropy is given by @xmath139 which is always smaller or equal to @xmath140 over all possible @xmath141 .",
    "according to ( [ q3.1.2.3.2 ] ) , there is @xmath133 , i.e. , @xmath23 independs of @xmath22 , when the following holds .",
    "@xmath142 for @xmath143 or @xmath8 .",
    "similarly , we could conclude the same observations for @xmath144 .",
    "evidently , given any values of @xmath141 and @xmath145 , smaller values of @xmath146 and @xmath147 imply larger values of @xmath148 or @xmath144 , which further implies weaker dependence between @xmath23 and @xmath22 .",
    "therefore , we arrive at the following dependence metric , where @xmath149 is termed as the conditional probability based dependence ( cpbd ) , @xmath150 which is symmetric with respect to @xmath151 and @xmath152 . the proposed cpbd @xmath153 is not affected by @xmath145 and @xmath141 , and the value range of @xmath153 is @xmath154 . for multiple - variable cases ,",
    "the cpbd between @xmath91 and @xmath155 conditioned on @xmath156 is given by @xmath157    taking the example of @xmath68 in cbn , the cpbd between @xmath158 and @xmath159 conditional on @xmath160 is given by @xmath161 where each term inside the @xmath162 operation could be directly obtained from the conditional probability table derived in section [ efficient cpt ] .",
    "moreover , each term has a regular relative index @xmath83 corresponding to the conditional probability table .",
    "as shown in the previous subsection , the conditional probability table with respect to @xmath159 is given as @xmath163^t$ ] .",
    "let @xmath164 ; we have @xmath165^t$ ] and @xmath166 .",
    "then @xmath167 and @xmath168 can be calculated as @xmath169 @xmath170 where @xmath171    for every possible edge pointing to @xmath159 , the corresponding cpbd is given by @xmath172\\\\ = & ( \\left|\\log\\boldsymbol b_{1 , t-1}^t\\boldsymbol l\\right| + \\left|\\log\\bar{\\boldsymbol b}_{1 , t-1}^t\\boldsymbol l\\right| ) \\boldsymbol s \\end{aligned}\\ ] ] where @xmath173 , \\end{aligned}\\ ] ] @xmath174    similarly as the above , it is easy to check that @xmath175 and @xmath176 obtained for @xmath159 is still suitable for @xmath177 .",
    "thus , when @xmath68 , the cpbd of the c - clique at @xmath36 is given by @xmath178 where @xmath179 @xmath180 , \\end{aligned}\\ ] ] @xmath181 , \\end{aligned}\\ ] ] @xmath182    in our system model , the behavior of each primary station follows a same birth - death process , which can be formulated by a bayesian graph containing two nodes .",
    "it means that if we only study the statistical pattern of one base station , the related bayesian structure consists of one edges and two nodes which means @xmath183 .",
    "therefore , the value of @xmath167 and @xmath184 are same and keep constant over every c - clique of a whole bn graph which expresses the pattern of @xmath121 primary base stations . on the other hand , the limited number of observations leads that the empirical @xmath167 and @xmath184 are different .",
    "hence , we normalize the entries in @xmath185 normalized as follows for reflecting the spatial relationship clearly .",
    "@xmath186    where @xmath187 denotes the diagonal matrix with the same main diagonal as @xmath185 .",
    "consequently , for an arbitrary @xmath121 , the normalized cpbd of c - clique at @xmath36 is given by @xmath188 where @xmath189 @xmath190 , \\end{aligned}\\ ] ] @xmath191    it is worth emphasizing that matrix @xmath175 does not depend on the number of c - cliques , but only on the number of variables in one c - clique .",
    "hence , matrix @xmath175 only needs to be computed once for a given @xmath121 .",
    "it could be computed offline prior to the online cbn learning .",
    "this paper does not study the optimal approach to obtain @xmath175 ; alternatively , we provide a feasible method to arrive at @xmath175 , given in table [ l ] .",
    ".algorithm of obtaining @xmath175 .",
    "[ cols=\"^ , < \" , ]     next , we show the learning results of cpbd . for clarity of expression , we only show the results in the case of @xmath192 $ ] and @xmath193 .",
    "when @xmath194 , the learned statistical pattern @xmath195 is given as follows , @xmath196    apparently , the entries in the upper triangular part of @xmath197 is generally larger than the entries in the lower triangular part , which implies that the status of base stations @xmath198 and @xmath199 is heavily impacted by base station @xmath200 , but the reverse does not hold .",
    "in addition , we use the learned cpbd @xmath195 to draw a cbn shown in fig .",
    "[ f6.3 ] , where the line width is proportional to the magnitude of dependence and we see that the trend of dependence between base station @xmath200 and base station @xmath199 increases as @xmath2 increases during a period .",
    "it is due to the fact that the users served by base station @xmath200 at the beginning of a period will arrive in the range of base station @xmath199 at the end of the period .",
    "note that if we add a binary thresholding to make decisions on the existence of edges , which is used for binary testing of each edge in @xcite , the learning outcomes will be similar to those in @xcite .",
    "the estimated conditional probability table @xmath201 is also reported in ( [ f56])-([f62 ] ) .",
    "based on the learned conditional probability table , we could generate the joint distribution of the on / off behaviors of the three primary base stations and predict the status of network for the future , to serve broader cr applications .    in brief ,",
    "the simulated outcomes show that the proposed cbn structure learning algorithm not only can quantify the strength of dependence but also correctly reflect the unidirectional statistical pattern caused by the mobile user s one - way movement , indicating that our learning scheme can efficiently learn the cbn structure to represent the true statistical behavior of the underlying network .",
    "in this paper , we proposed a learning scheme to obtain the statistical pattern of a primary network s activity in both spatial and temporal domains simultaneously .",
    "the proposed scheme incurs significantly lower computational complexity when compared with the traditional ones .",
    "additionally , it is capable of learning the statistical period of the cbn structure . by simulations ,",
    "we show that the learning results also correctly reflect certain network behavior beyond spectrum usage , which could be useful for broader cr network control applications .",
    "s.  dudley , w.  headley , m.  lichtman , e.  imana , x.  ma , m.  abdelbar , a.  padaki , a.  ullah , m.  sohul , t.  yang , and j.  reed , `` practical issues for spectrum management with cognitive radios , '' _ proceedings of the ieee _",
    "102 , no .  3 , pp .",
    "242264 , march 2014 .",
    "z.  quan , s.  cui , and a.  sayed , `` optimal linear cooperation for spectrum sensing in cognitive radio networks , '' _ selected topics in signal processing , ieee journal of _ , vol .  2 , no .  1 , pp . 2840 , feb 2008 .",
    "p.  shaughnessy and g.  livingston , `` evaluating the causal explanatory value of bayesian network structure learning algorithms , '' _ research paper , department of computer science , university of massachusetts lowell .",
    "_ , 2005 .",
    "w.  han , j.  li , z.  li , j.  si , and y.  zhang , `` efficient soft decision fusion rule in cooperative spectrum sensing , '' _ signal processing , ieee transactions on _ , vol .",
    "61 , no .  8 , pp .",
    "19311943 , april 2013 ."
  ],
  "abstract_text": [
    "<S> in cognitive radio ( cr ) technology , the trend of sensing is no longer to only detect the presence of active primary users . </S>",
    "<S> a large number of applications demand for more comprehensive knowledge on primary user behaviors in spatial , temporal , and frequency domains . </S>",
    "<S> to satisfy such requirements , we study the statistical relationship among primary users by introducing a bayesian network ( bn ) based framework . how to learn such a bn structure is a long standing issue , not fully understood even in the statistical learning community . besides , another key problem in this learning scenario is that the cr has to identify how many variables are in the bn , which is usually considered as prior knowledge in statistical learning applications . </S>",
    "<S> to solve such two issues simultaneously , this paper proposes a bn structure learning scheme consisting of an efficient structure learning algorithm and a blind variable identification scheme . </S>",
    "<S> the proposed approach incurs significantly lower computational complexity compared with previous ones , and is capable of determining the structure without assuming much prior knowledge about variables . with this result </S>",
    "<S> , cognitive users could efficiently understand the statistical pattern of primary networks , such that more efficient cognitive protocols could be designed across different network layers .    cognitive radio , bayesian network learning , network structure learning . </S>"
  ]
}