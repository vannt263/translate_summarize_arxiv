{
  "article_text": [
    "nonnegative matrix factorization ( nmf ) is a recent data analysis technique with applications in image processing , text mining , spectral unmixing , air emission control , computational biology , clustering , etc .",
    "( see @xcite and references therein ) .",
    "nmf can be described as follows : given a nonnegative input matrix @xmath0 and an integer @xmath1 , find two nonnegative matrices @xmath2 and @xmath3 whose product approximates the input matrix as closely as possible : @xmath4 so that @xmath5 is a low - rank approximation of @xmath6 .",
    "matrix factorization can be interpreted as a linear factor model : assuming that each column of the input matrix @xmath6 represents an element of a data set , decomposition   can be written as stands for the @xmath7-entry of a matrix @xmath8 , @xmath9 for its @xmath10 column and @xmath11 for its @xmath12 row . ]",
    "@xmath13 i.e. , each input column @xmath14 is a linear combination of a set of @xmath15 basis elements @xmath16 with corresponding weights @xmath17 .",
    "in contrast with standard linear factor model techniques such as principal component analysis , nmf considers nonnegativity of the input columns to be an important feature and consequently requires the basis elements to be also nonnegative , so that they can be interpreted in the same way ( e.g. , these columns can correspond to images described by nonnegative pixel intensities or to texts represented by vectors of nonnegative word counts ) .",
    "furthermore , nmf imposes nonnegativity of the weights , leading to an essentially additive reconstruction of the input columns by the basis elements .",
    "this representation is then _ part - based _ : basis elements @xmath16 will represent common parts of the columns of @xmath14 . for example , if each column of @xmath6 represents a face using pixel intensities , the basis elements generated by nmf can be facial features , such as eyes , noses and lips , as shown in figure  [ facto ] .",
    "this low - rank approximation technique with nonnegativity constraints was introduced in 1994 by paatero and tapper  @xcite and started to be extensively studied after the publication of an article by lee and seung  @xcite in 1999 . since an exact representation of the input matrix can not be obtained in general , the quality of the approximation is measured by some criterion , typically the sum of the squares of the errors on the entries , which leads to the following minimization problem denotes the frobenius norm of matrix @xmath8 . ] : @xmath18    an important feature of nmf is that its nonnegativity constraints typically induce _ sparse factors _ , i.e. , factors with relatively many zero entries .",
    "intuitively , decomposition into parts requires the basis elements to be sparse , cf .",
    "figure  [ facto ] .",
    "more formally , the reason for this behavior is that stationary points @xmath19 of nmf will be typically located at the boundary of the feasible domain @xmath20 , hence will feature zero components .",
    "this can be explained with the first - order optimality conditions : because the set of stationary points of a problem of the type @xmath21 is given by the following expression ( where @xmath22 is the gradient of @xmath23 ) @xmath24_i = 0 } \\",
    ", \\forall i \\},\\ ] ] some components of the solution can be expected to be equal to zero .",
    "sparsity of the factors is an important consideration in practice : in addition to reducing memory requirements to store the basis elements and their weights , sparsity improves interpretation of the factors , especially when dealing with classification / clustering problems , e.g. , in text mining @xcite and computational biology @xcite .",
    "by contrast , unconstrained low - rank approximations such as principal component analysis ( pca ) do not naturally generate sparse factors ( for that reason , low - rank approximations techniques with additional sparsity constraints have been recently introduced ; this is referred to as sparse principal component analysis , sparse pca or spca , see , e.g. , @xcite and references therein ) .",
    "although solutions of nmf typically display some level of sparsity , some applications require even sparser solutions , leading to variants of nmf called sparse nonnegative matrix factorization .",
    "they are in general developed in two different ways : some authors define a priori a desired sparsity level and adapt the main iteration of their method in order to guarantee that the factors satisfy that level of sparsity throughout the application of the algorithm , see , e.g. , @xcite .",
    "alternatively , a penalty term can be added to the objective function to prevent the algorithm from considering dense solutions , see  @xcite . in particular , it is well - known that @xmath25-norm penalty terms induce sparser solutions ( see , e.g. , @xcite ) .",
    "more details about these techniques are given at the beginning of section  [ appl ] .",
    "unfortunately the advantages of nmf ( part - based representation and sparsity ) over pca come at a certain price .",
    "first , because of the additional nonnegativity constraints , the approximation error of the input data for a given factorization rank @xmath15 will always be higher for nmf than in the unconstrained case .",
    "second , optimization problem   is more difficult to solve than its unconstrained counterpart : while pca problems can be solved in polynomial time ( e.g. , using a singular value decomposition technique @xcite ) , nmf problems belong to the class of np - hard problems , as recently showed by vavasis @xcite .",
    "however , it should also be pointed out that these drawbacks ( higher error , np - hardness ) are also present for competing techniques emphasizing sparsity , such as spca .",
    "because of its np - hardness , practical algorithms can not be expected to find provably optimal global solutions for in a reasonable amount of time and aim instead at finding locally optimal solutions .",
    "most methods start from some initial guess factors @xmath19 and improve them iteratively using nonlinear optimization schemes such as projected gradient methods  @xcite , newton - like methods  @xcite , ( block-)coordinate descent ( also called alternating nonnegative least squares ",
    "nnls )  @xcite , multiplicative updates  @xcite , etc .",
    "( see also  @xcite and references therein ) .    in this paper , we introduce a novel approach to solve nmf problem based on the use of an underapproximation technique and show its effectiveness to obtain sparse solutions .",
    "section  [ nmus ] introduces our underapproximation problem , motivated by a recursive technique to solve nmf , studies the sparsity of its solutions and proves that it is np - hard for any fixed factorization rank .",
    "nevertheless , section  [ lag ] describes an algorithm to solve it approximately using a technique based on lagrangian relaxation .",
    "finally , in the last section , we test this approach on several standard image datasets , and show both qualitatively and quantitatively that it provides part - based and sparse representations that are comparable and sometimes superior to those obtained with standard sparse nonnegative matrix factorization techniques .",
    "finding a rank - one nonnegative matrix factorization , i.e. , solving with @xmath26 is notably easier than for higher factorization ranks : while the general problem is np - hard , computing a globally optimal rank - one approximation can be done in polynomial time .",
    "more specifically , the first rank - one factor of the singular value decomposition ( svd ) of the input matrix is an optimal solution : indeed , the perron - frobenius theorem implies that the dominant left and right singular vectors of a nonnegative matrix are nonnegative , while the eckart - young theorem states that the outer product of these dominant singular vectors is the best possible rank - one approximation in the frobenius norm .    in principle , we might try exploit this result to find factorizations of higher ranks by applying it recursively : after identification of an optimal rank - one nmf solution @xmath27 , one could subtract the @xmath28 factor from @xmath6 and apply the same technique to @xmath29 to recover the next rank - one factor .",
    "unfortunately , this idea can not work : the difference between @xmath6 and its rank - one approximation may contain negative values ( typically roughly half of them ) , so that the next svd factor will no longer provide a nonnegative solution .",
    "moreover , there is no hope of replacing svd by another efficient technique for this step since @xcite shows that it is np - hard to find the optimal nonnegative rank - one approximation to a matrix which is not nonnegative .",
    "if we wish to keep the principle of a recursive algorithm finding one rank - one factor at a time , we have to add a constraint ensuring that the @xmath28 factor , when subtracted from @xmath6 , gives a nonnegative remainder , i.e. , we need to have @xmath30 .",
    "therefore we introduce a similar _ upper bound constraint _",
    "@xmath31 to the general ( nmf ) problem and obtain a new problem we call _ nonnegative matrix underapproximation _ ( nmu ) : given @xmath0and @xmath32 , the nmu optimization problem is defined as @xmath33 assuming we are able to solve it for @xmath26 , an underapproximation of any rank can then be built by following the recursive procedure outlined above .",
    "more precisely , if @xmath34 is a rank - one underapproximation for @xmath6 , i.e. , @xmath35 and @xmath36 , we have that @xmath37 is nonnegative .",
    "@xmath38 can then be underapproximated @xmath39 , leading to @xmath40 , and so on .",
    "after @xmath15 steps , we get an underapproximation of rank @xmath15 @xmath41 [ w_{1 : } ; \\ ; w_{2 : } ; \\ , \\dots \\ , ; w_{r : } ] \\\\    &    =   & v w.\\end{aligned}\\ ] ]    besides enabling this recursive procedure , we notice that nmu leads to a more _ localized _ part - based decomposition , in the sense that different basis elements tend to describe disjoint parts of the input data ( i.e. , involving different nonzero entries ) .",
    "this is a consequence of the underapproximation constraints which impose the extracted parts ( the basis elements @xmath16 ) to really be common features of the columns of @xmath6 since @xmath42 basis elements can only be combined to approximate a column of @xmath6 if each of them represents a part of this column , i.e. , none of the parts selected with a positive weight can involve a nonzero entry corresponding to a zero entry in the input column @xmath14 .",
    "the following example demonstrates this behavior .",
    "[ swimex ] the swimmer image dataset consists of 256 binary images of a body with 4 limbs which can be each in 4 different positions .",
    "nmf is expected to find a part - based decomposition of these images , i.e. , isolate different constitutive parts of the images ( the body and the limbs ) in each of its basis elements .",
    "figure  [ swim ] displays a sample of such images along with the basis elements obtained with nmf and nmu .",
    "while nmf elements are rather sparse , they are mixtures of several limbs . by contrast , nmu returns a even sparser solution and is able to extract a single body part for each of its elements .",
    "the fact that nmu decompositions naturally generate sparser solutions than nmf can be explained as follows : since the zero entries of @xmath6 can only be underapproximated by zeros , we have @xmath43 which shows that when the input matrix is sparse , many components of the nmu factors will have to be equal to zero .",
    "this observation can be made more formal : defining the sparsity @xmath44 of a @xmath45 by @xmath46 matrix @xmath6 as the proportion of its zero entries , i.e. , @xmath47,\\ ] ] we have the following theorem relating sparsity of @xmath6 and its nmu factors .    [ sumsvsw ] for any nonnegative rank - one underapproximation @xmath48 of @xmath0 we have @xmath49    for a rank - one matrix @xmath28 , the number of nonzeros is exactly equal to the product of the number of nonzeros in vectors @xmath50 and @xmath51 .",
    "therefore we have that @xmath52 which implies @xmath53 .",
    "since underapproximation @xmath28 satisfies @xmath54 , it must have more zeros than @xmath6 and we have @xmath55 proving our claim .",
    "recall the recursive definition of the residuals @xmath56 and @xmath57 .",
    "the following corollary relates their sparsity and the sparsity of the whole rank-@xmath15 approximation with that of the nmu factors .",
    "[ corosp ] for any nonnegative underapproximation @xmath58 of @xmath0 we have for each factor @xmath59 and @xmath60 .",
    "we have @xmath61 , which implies by the previous theorem the first set of inequalities . observing",
    "that @xmath62 and @xmath63 is sufficient to prove the second inequality .",
    "sparsity of the residuals @xmath64 is monotonically nondecreasing at each step , since @xmath65 .",
    "moreover , the following theorem can guarantee an increase in sparsity at each step .",
    "[ zerores ] for any locally optimal nonnegative rank - one underapproximation @xmath48 of @xmath0 , define sets @xmath66 and @xmath67 ( supports of vectors @xmath50 and @xmath51 ) by @xmath68 and define matrix @xmath69 to be the submatrix of residual @xmath70 whose row and column indices belong respectively to @xmath66 and @xmath67 ( corresponding to the submatrix of @xmath6 that is not approximated by zeros ) .",
    "then there is at least one zero in each row and each column of submatrix @xmath69 .",
    "simply observe that if @xmath71 ( resp .",
    "@xmath72 ) for some @xmath73 ( resp .",
    "@xmath74 ) , @xmath75 ( resp .",
    "@xmath76 ) can be increased to obtain a strictly better solution , which contradicts the local optimality assumption .",
    "this ability of nmu to generate sparse part - based decomposition will be experimentally confirmed in section  [ appl ] .      the problem of rank - one underapproximation has been first introduced by levin in @xcite in the case of _ positive stochastic _ matrices .",
    "he introduced a specific objective function different from the frobenius norm and used a logarithmic change of variables in order to design an iterative method based on the corresponding optimality conditions .    in @xcite",
    ", the rank - one underapproximation problem is cast as a convex problem ( hence efficiently solvable ) using again different objective functions .",
    "solutions are then used to initialize standard nmf algorithms in order to accelerate their convergence and , in general , find better final solutions as compared to those obtained with random initializations .",
    "similar behavior was observed for other judicious initializations in  @xcite .",
    "more recently , dong et al",
    ". @xcite studied the same problem with the additional constraint that the rank of the residual must be strictly smaller than the rank of the factorized matrix . using",
    "the wedderburn rank reduction formula , they proposed a numerical procedure which is able to compute the maximum rank splitting of a nonnegative matrix .",
    "however , the underlying optimization problem is np - hard @xcite and their algorithm is not guaranteed to find a solution in all cases .",
    "biggs et al .",
    "@xcite also introduced a recursive procedure to solve nmf problems : their idea is to locate and then approximate nearly rank - one submatrices of @xmath6 .",
    "however , the problem of locating maximum rank - one submatrices is also shown to be np - hard , and their algorithm is not globally optimal .",
    "we now prove that is np - hard , even in the rank - one case ( unlike , which is polynomially solvable in the rank - one case ) . in order to do this , the rank - one version of the problem is proved to be equivalent to the biclique problem , which is np - hard .",
    "the result is then generalized to with arbitrary factorization rank @xmath15 using a simple construction . + a _ bipartite graph _",
    "@xmath77 is a graph whose vertices can be divided into two disjoint sets such that there is no edge between two vertices in the same set .",
    "a _ biclique _",
    "@xmath78 is a complete bipartite graph , i.e. , a bipartite graph where all the vertices from different sets are connected by an edge .",
    "finally , the so - called maximum edge biclique problem ( the biclique problem for short ) in a bipartite graph @xmath79 is the problem of finding a biclique @xmath80 in @xmath77 ( i.e. , @xmath81 and @xmath82 ) with a maximum number of edges @xmath83 .",
    "letting @xmath84 be the adjacency matrix of @xmath77 with @xmath85 and @xmath86 , i.e. , @xmath87 and introducing indicator binary variables @xmath88 ( resp .",
    "@xmath89 ) to denote whether @xmath90 ( resp .",
    "@xmath91 ) belongs to the biclique @xmath78 , the maximum edge biclique problem in a bipartite graph can be formulated as follows @xmath92 one can check that this objective is equivalent to @xmath93 .",
    "in fact , @xmath94 since @xmath6 , @xmath50 and @xmath51 are binary and @xmath95 .",
    "+ the corresponding decision problem `` _ _ given @xmath96 , does @xmath77 contain a biclique with at least @xmath96 edges ? _ _ '' has been shown to be np - complete  @xcite .",
    "therefore , the corresponding optimization problem is at least np - hard .",
    "+ for @xmath97 , can be written as @xmath98 which is very close to : the difference is that vectors @xmath50 and @xmath51 are required to be _ binary _ for and _ nonnegative _ for .",
    "the next lemma proves that the two problems are actually equivalent .",
    "[ lembin ] for @xmath84 , every optimal solution @xmath99 of is such that @xmath28 is binary , i.e. , @xmath100 , and can then be trivially transformed into a binary optimal solution @xmath101 of .    for @xmath102 ,",
    "this is trivial .",
    "otherwise , suppose @xmath99 is an optimal solution of .",
    "let define @xmath103 as @xmath104 and analyze the different possibilities : as @xmath105 , we have either    * @xmath106 and @xmath107 ; * @xmath106 and @xmath108 ; * @xmath109 .    therefore , @xmath110 which implies @xmath111 by optimality of @xmath99 , we must have @xmath112 .",
    "therefore , @xmath113 is an optimal binary solution of which is then also an optimal solution of ( note that we must have @xmath114 ) .",
    "[ nmu1np ] is np - hard .",
    "[ nmu1np ]    we now generalize corollary  [ nmu1np ] to the more general case of with @xmath115 .    is np - hard .",
    "[ nmunp ]    let @xmath84 be the adjacency matrix of a bipartite graph @xmath77 .",
    "we define the matrix @xmath8 as @xmath116 which is the adjacency matrix of another bipartite graph @xmath117 which is the graph @xmath77 repeated @xmath15 times .",
    "let @xmath19 be an optimal solution of .",
    "since @xmath118 , we have @xmath119 .",
    "therefore @xmath120 is a feasible solution of for the matrix @xmath8 , i.e. , for the graph @xmath117 .",
    "hence , each @xmath120 corresponds to a biclique @xmath121 of @xmath117 with @xmath122 by optimality of @xmath19 and since there are at least @xmath15 independent maximum biclique in @xmath117 , each @xmath120 must coincide with a maximum biclique of @xmath117 which corresponds to a maximum biclique of @xmath77 .",
    "this is due to the fact that , because @xmath117 is the graph @xmath77 repeated @xmath15 times , a biclique clearly can not span two disjoint subgraphs of @xmath117 .",
    "therefore , is np - hard since any instance of can be polynomially reduced to an instance of .",
    "since , like , is a np - hard problem , we can not expect to solve it up to guaranteed global optimality in a reasonable ( e.g. , polynomial ) computational time ( unless @xmath123 ) . in this section , we propose a nonlinear optimization scheme based on lagrangian relaxation in order to compute approximate solutions of .",
    "+ drop the @xmath124 underapproximation constraints @xmath31 of and add them into the objective function with the corresponding lagrange multipliers ( dual variables , forming a matrix ) @xmath125 , to obtain the lagrangian function @xmath126 @xmath127 where a factor of @xmath128 was introduced to make the presentation nicer .",
    "the lagrangian relaxation subproblem [ lagrel ] consists in minimizing @xmath129 for a fixed value of the @xmath130 multipliers , leading to the corresponding lagrangian dual function @xmath131 @xmath132 where @xmath131 is well - defined because the minimum of @xmath126 is always attained , due to the fact that @xmath23 is bounded below and the search space can be restricted to a compact set .",
    "indeed , considering each rank - one factor individually @xmath120 and imposing w.l.o.g .",
    "@xmath133 , we have @xmath134 where we have used the trivial solution @xmath135 to bound @xmath136 ( cf .",
    "derivations of section  [ vwup ] ) .",
    "standard application of lagrangian duality tells us that @xmath137 where the problem on the left of the inequality is equivalent to our original nmu formulation and the problem on the right is its lagrangian dual , whose solution will provide a ( hopefully tight ) lower bound on the optimal .",
    "this new problem is a nondifferentiable optimization problem with the nice property that its objective @xmath138 is concave and its maximization ( over a convex set ) is then a convex problem ( see @xcite and references therein ) .",
    "we describe in the next section a general solution technique , which consists in repeatedly applying the following two steps :    1 .",
    ": :    given multipliers @xmath130 , compute @xmath19    to ( approximately ) minimize @xmath126 , i.e. , solve    ; this is discussed in section  [ vwup ] ; 2 .",
    ": :    given solution @xmath19 , update multipliers    @xmath130 ; this is described in section  [ lamup ] .",
    "the following derivations @xmath139 show that minimizing @xmath126 for a fixed @xmath130 is equivalent to minimizing @xmath140 .",
    "matrix @xmath141 is not necessarily nonnegative , therefore finding @xmath142 and @xmath143 such that @xmath144 is a more general problem than nmf .",
    "it is actually studied in detail in  @xcite ( see also @xcite ) where it is called nonnegative factorization  ( nf ) , is formulated as @xmath145 with @xmath146 and @xmath32 and is shown to be np - hard for any factorization rank ( including @xmath26 ) .",
    "some standard algorithms for nmf can easily adapted to handle an input matrix that is not nonnegative , i.e. , solve a nf problem .",
    "for this work , we decided to use a recent technique called hierarchical alternating least squares ( hals ) , proposed in @xcite , which alternatively updates each column of @xmath147 and each row of @xmath148 with the following optimal closed - form solutions : @xmath149 with @xmath150 and @xmath151 , and @xmath152 with @xmath153 and @xmath154 .",
    "this can be viewed as a simple method of ( block-)coordinate descent ( also called alternating variables ) , which has been shown to perform strikingly well in practice , and much better than the popular multiplicative updates of lee and seung ( see @xcite ) . under some mild assumptions , every limit point of",
    "the above alternating scheme is a stationary point @xcite .",
    "the main computational cost of one hals iteration is the evaluation of @xmath8 and @xmath155 : they each require @xmath156 ( floating point ) operations .",
    "one can check that the resulting total number of operations is @xmath157 .",
    "hals is sensitive to the scaling of the initial matrices .",
    "for example , if the initial matrices @xmath147 and @xmath148 are chosen such that @xmath158 , optimal columns of @xmath147 and optimal rows of @xmath148 computed by formulas and at the first step will most likely be equal to zero .",
    "this will lead to rank deficient approximations ( @xmath159 for some @xmath160 ) and numerical problems ( for @xmath161 , update of @xmath162 is not well defined and vice versa ) .",
    "if the initial matrices @xmath19 are scaled  @xcite , i.e. , by ensuring that @xmath163 where @xmath164 , this behavior is in general avoided .",
    "all initial matrices used in the following have been scaled .",
    "the second step of our algorithm consists in updating @xmath130 in order to find better ( i.e. , higher ) solutions to the lagrangian dual problem . using the knowledge that any optimal solution @xmath165 of the lagrangian dual must satisfy the following complementarity slackness conditions @xmath166",
    "we see that the update rule for the multipliers @xmath130 should satisfy the following :    * if @xmath167 , @xmath168 should be decreased and eventually reach zero if @xmath169 , * if @xmath170 , @xmath168 should be increased to give more importance to @xmath171 in the cost function , hopefully in order to get a feasible solution such that @xmath172 .",
    "in the sequel , we use the following rule to update @xmath130 , which satisfies the above requirements : @xmath173 where @xmath174 is a predefined sequence of step lengths decreasing to zero ; @xmath130 can be initialized to zero .",
    "this update is inspired from the concept of subgradient methods @xcite ; in fact , one can easily check that the quantity @xmath175 is a subgradient of @xmath176 with respect to @xmath130 , i.e. , if @xmath177 , we have @xmath178 two questions now arise    * since an iterative algorithm is used to solve ( approximately ) the lagrangian relaxation problem ( cf .",
    "section  [ vwup ] ) , after how many of these hals iterations do we stop and proceed to update the multipliers @xmath130 ?",
    "* how do we choose the sequence of step lengths @xmath174 ?",
    "subgradient methods usually assume that the lagrangian relaxation problem   can be solved exactly and can guarantee their convergence to an optimal solution provided an appropriate sequence of step sizes is selected ( see , e.g. , @xcite ) , for example @xmath179 satisfying the conditions @xmath180 in the sequel , we choose to use @xmath181 , which is such a suitable sequence .",
    "however , in our case , we can not expect to solve in a reasonable amount of time since the problem is np - hard .",
    "it would even probably be too expensive to wait for the stabilization of @xmath19 ( e.g. , getting close to a stationary but not necessarily optimal point ) .",
    "we therefore suggest to update @xmath19 only a constant number of times @xmath182 between each update of @xmath130 , which leads to algorithm l - nmu .",
    "note that because we do not solve exactly , algorithm l - nmu is not guaranteed to converge to an optimal solution of the lagrangian dual but , as we will see , it produces satisfactory solutions in practice .",
    "@xmath0 , @xmath183 , @xmath2 , @xmath3 , maxiter , @xmath182 .",
    "@xmath19 s.t .",
    "@xmath184 . + @xmath185 ; + update @xmath19 using @xmath182 iterations of hals - ; update @xmath186 ;    the additional computational cost of one iteration of algorithm l - nmu when compared with one iteration of hals for nmf consists in the computation of @xmath187 ( needed in step  3 ) and the update of @xmath130 ( at step  4 ) , which require @xmath188 operations ( and , in the special case @xmath26 , @xmath189 operations ) .",
    "because convergence is not theoretically guaranteed , algorithm l - nmu may end up with solutions that do not completely satisfy the underapproximation constraint .",
    "although our numerical experiments show that this has no detrimental influence on the quality of the obtained sparse part - based representations ( see section  [ appl ] ) , we give here a simple technique to transform such a solution into a feasible solution .",
    "indeed , it is enough to consider the following qp problem ( convex quadratic objective function , linear inequality constraints ) which only involves the @xmath147 factor @xmath190 because of its convexity , this problem can be solved up to global optimality in a very efficient manner , and replacing the original @xmath147 factor by the optimal solution @xmath191 leads to a feasible solution @xmath192 to .",
    "because update rule   is exact and computable in practice , it would be natural to consider a simpler algorithm based on its alternative application to the @xmath147 and @xmath148 factors , without using the lagrangian relaxation technique , hoping to converge to a solution of .",
    "unfortunately , we observed that this is quite inefficient in practice .",
    "in fact ,    * it is relatively computationally expensive to solve these linearly constrained quadratic programs ( with @xmath193 and @xmath194 inequalities ) , at least compared to the hals closed - form update rules - ; * since the underapproximation constraint is imposed at each step , this algorithm has much less freedom to converge to good solutions : iterates rapidly get stuck on the boundary of the feasible domain , typically with ( too ) many zeros and a lower rank .",
    "for example , assuming @xmath6 has one zero in each column , we have that for any positive matrix @xmath147 the corresponding optimal @xmath148 is equal to 0 : @xmath195 therefore , such an algorithm can only work if we decide a priori which values in @xmath147 and @xmath148 should be equal to zero , i.e. if we find a good sparsity pattern for the solution , which is precisely where the difficulty of the problem lies .",
    "note that the same behavior is observed if a hals - type algorithm is used instead of   ( i.e. , updating columns of @xmath147 and rows of @xmath148 alternatively ) : after the update of one column of @xmath147 , the residual will have one zero in each row ( cf .",
    "theorem  [ zerores ] ) which will prevent the other columns of @xmath147 to be nonzero ( except if the sparsity pattern is chosen a priori ) .",
    "the l - nmu algorithm described above is not particularly well - suited to deal with very sparse input matrices .",
    "in fact , one has to store a potentially dense @xmath124 matrix with the lagrangian variables @xmath130 .",
    "nevertheless , berry et al .",
    "@xcite have obtained encouraging results when applying nmu to sparse anomaly detection problems in text mining .",
    "moreover , it is possible to take advantage of the input sparsity pattern and design a computationally cheaper method .",
    "first note that the lagrangian variables associated with a zero of @xmath6 will be nondecreasing in the course of the algorithm , since @xmath196 where superscript @xmath197 denotes the solution at step @xmath160 .",
    "therefore one can significantly reduce the computational cost by defining @xmath198 where @xmath199 is an arbitrary positive nondecreasing function , e.g. , @xmath200 with @xmath201 , as is implicitly done in @xcite .",
    "we have argued in section  [ nmus ] that nmu is potentially able to extract a better part - based representation of the data and that its factors should be sparser than those of the standard nmf , at the detriment of the approximation error . in this section",
    ", we support these claims by reporting results of computational experiments involving two variants of algorithm l - nmu on several image datasets .",
    "a direct comparison between nmu and nmf is not very informative in itself : while the former will provide a sparser part - based representation , the latter will feature a lower approximation error .",
    "this does not really tell us whether the improvements in the part - based representation and sparsity are worth the increase in approximation error .",
    "for that reason , we chose to compare nmu with two other sparse nonnegative matrix factorizations techniques , described below , in order to better assess whether the increase in sparsity achieved by nmu is worth the loss in reconstruction accuracy .",
    "we selected and tested the following two sparse nonnegative matrix factorization techniques that are frequently used in the literature .",
    "* hoyer describes in @xcite an algorithm relying on additional explicit sparsity constraints on the factors , enforced at each iteration by means of a projection .",
    "the approximation error is reduced via a combination of projected gradient and multiplicative updates . for our experiments",
    ", we use the code provided by the author . +",
    "it should be pointed out that hoyer is using a different definition of sparsity : for any nonzero @xmath46 dimensional vector @xmath202 , his measure of sparsity @xmath203 is defined as @xmath204.\\ ] ] hence , a vector with a single nonzero entry is perfectly sparse @xmath205 ) = 1 , \\quad \\forall k \\neq 0,\\ ] ] while a vector with all entries equal to each other is completely dense @xmath206 ) = 0,\\quad \\forall k \\neq 0.\\ ] ] in our experiments , we report sparsity using both the standard @xmath207 indicator and hoyer s @xmath208 measure . * instead of enforcing sparsity at every iteration ,",
    "a sparsity - inducing penalty term can be introduced in the objective function  @xcite . in particular , it is well - known that adding @xmath25-norm penalty terms induce sparser solutions ( see , e.g. , @xcite ) , and we therefore solve the following problem : @xmath209 where @xmath210 and @xmath211 and @xmath212 are two positive parameters controlling the sparsity of @xmath147 and @xmath148 . in order to solve",
    ", we use the hals algorithm which can easily be adapted to handle the additional @xmath25-norm penalty terms ( see , e.g. , @xcite ) .",
    "this algorithm will be referred to as snmf .",
    "technical details for the first technique are more complicated , but it allows the sparsity of the factors to be chosen a priori .",
    "the second technique is conceptually simpler but requires the determination of appropriate penalizing parameters by other means .",
    "algorithm  l - nmu proposed in section  [ lag ] can be used to compute underapproximations for any given factorization rank @xmath15 .",
    "this opens the possibility of building a rank-@xmath15 underapproximation in several different ways : one simple option consists in applying algorithm l - nmu directly to the rank-@xmath15 problem  we call this method _ global nmu _",
    "( g - nmu ) .",
    "another option consists in applying the recursive technique outlined in the introduction , used to motivate the introduction of underapproximations .",
    "more specifically , this means running algorithm l - nmu successively @xmath15 times to compute @xmath15 rank - one approximations , subtracting each approximation from the input matrix before computing the next one ",
    "we call this method _ recursive nmu _ ( r - nmu ) . note that many other variants are possible ( e.g. , computing two rank-@xmath213 approximations , computing @xmath213 successive rank - two approximations , etc . ) but we only tested the two above - mentioned variants , which represent two extreme cases ( no recursion and maximum recursion ) .    in both cases ,",
    "our implementation of algorithm l - nmu computes two hals steps between each update of the multipliers @xmath130 ( i.e. , we fixed @xmath214 ) .",
    "most of the computational work done in one iteration of l - nmu consists in computing @xmath187 , performing the two hals steps and updating @xmath130 ; more specifically , one can estimate the computational cost of one iteration of g - nmu to @xmath215 operations , while an r - nmu iteration takes @xmath216 operations ( repeated @xmath15 times in the recursive procedure ) .    for each dataset",
    ", we test five nonnegative factorization algorithms : nmf based on hals updates ( nmf ) , global nmu ( g - nmu ) , recursive nmu ( r - nmu ) , sparse nmf with @xmath25-penalty terms ( snmf ) and the algorithm of hoyer .",
    "we also report the results of a standard principal component analysis ( pca ) to serve as a reference ( recall that the approximation error of this unconstrained low - rank approximation , computed here with a singular value decomposition , is globally minimal for the given rank , but that its factors are neither nonnegative , nor sparse ) .",
    "each of the five iterative algorithms described above requires a limit on the number of its iterations ; these limits were chosen in order to roughly allocate the same cpu time to each algorithm .",
    "more specifically , the standard nmf was given a 600-iterations limit , which corresponds to the computation of 600 hals updates .",
    "the sparse snmf , based on a slightly modified hals update , was also allowed 600 iterations .",
    "because a hals update involves @xmath157 operations , we can deduce the following iteration budgets for g - nmu and r - nmu from the leading terms in their corresponding operation counts : g - nmu is allowed @xmath217 l - nmu iterations while r - nmu can take @xmath218 iterations .",
    "an exception to the equal cpu time rule was made for the algorithm of hoyer .",
    "results obtained after an amount of cpu time similar to that of the other algorithms were too poor to be compared in a meaningful way . indeed , because this method is based on a projected gradient method and multiplicative updates ( both @xmath219 operations per iteration ) , which are known to converge at a typically much slower rate , a relatively high limit of 1000 iterations had to be fixed , although the resulting cpu time is then much larger than for the other methods ( for example , on the cbcl dataset , 600 iterations of hals took @xmath220 while 1000 iterations of the algorithm of hoyer needed @xmath221 ) .",
    "recall we decided to test algorithms snmf and hoyer to assess the quality of the sparsity - accuracy compromise proposed by our nmu approaches . to achieve this",
    ", we decided to pit each nmu variant against a solution of snmf / hoyer featuring the same level sparsity , and compare the resulting approximation errors .",
    "we therefore report results for @xmath222 algorithms on each dataset : pca , nmf , g - nmu , snmf with the same sparsity as g - nmu , which we denote by snmf\\{g - nmu } , hoyer\\{g - nmu } , r - nmu , snmf\\{r - nmu } and hoyer\\{r - nmu}.    in order to enforce a sparsity similar to the nmu solution in hoyer s code , we compute the @xmath223 measure of the nmu factors and input it as a parameter of the method ( see description in subsection  [ twosnmf ] ) ; note however that we could only enforce this for the sparsest of the two nmu factors . in the case of snmf",
    ", sparsity can not be directly controlled , and penalty parameters are found using the following adaptive procedure , which proved to work well in practice : @xmath211 and @xmath212 are initialized to 0.1 and , after each iteration , @xmath211 ( resp .",
    "@xmath212 ) is increased by 5 percent if @xmath224 ( resp .",
    "@xmath225 ) is below the target sparsity , and is decreased by 5 percent otherwise .",
    "all algorithms were run @xmath226 times with the same initial random matrices and only the best solution with respect to the frobenius norm of the error is reported .",
    "when testing with gray - level images , the input matrices @xmath6 where normalized to have their entries varying between 0 and 1 , with 0 representing white and 1 representing black ( when trying to decompose @xmath6 as a sum of parts , this make more sense than the opposite convention , since the dark regions are the constitutive parts of the objects in the image datasets we analyze ) .",
    "finally , before computing reported sparsity measures of the factors , any sufficiently small of the largest entry in its column .",
    "] entry is rounded to zero ( indeed , because algorithms are stopped by the iteration limit before convergence , true zeros are typically not all reached ) .",
    "all tests were run within the 7.1 ( r14 ) version , on a 3ghz intel@xmath227 core2 dual cpu pc .",
    "the cbcl face image dataset was used for the illustrative example of figure  [ facto ] and is made of 2429 gray - level images of faces represented with @xmath228 pixels .",
    "we look for an approximation of rank @xmath229 .",
    "figure  [ cbclb ] displays the basis elements for nmf , g - nmu , r - nmu and snmf\\{g - nmu } ( which was the best solution obtained in term of sparsity vs. error among all four snmf and hoyer variants ) .",
    "both g - nmu , r - nmu and snmf achieve a better part - based representation than nmf , generating sparser solutions . an interesting feature of r - nmu is that it extracts parts successively in order of `` importance '' : the first basis element is a mean face ( which is dense ) while the next ones describe different complementary parts ( which become sparser as the recursion moves on , cf .",
    "corollary  [ corosp ] and theorem  [ zerores ] ) .",
    "a more quantitative assessment is provided in table  [ cbcle ] , reporting for the @xmath222 algorithms tested the relative error ( in percent ) of their solutions @xmath230 in the second column ( `` plain '' ) and the corresponding sparsity measures ( in percent ) of factors @xmath147 and @xmath148 in the last four columns .",
    "[ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "in order to solve the nmf problem in a recursive way , we have introduced a new problem , namely nonnegative matrix underapproximation ( nmu ) , which was shown to be np - hard using its equivalence with the maximum - edge biclique problem .",
    "the additional constraints of nmu are shown to induce sparser factors and to lead naturally to a better part - based representation of the data , while keeping a fairly good reconstruction .",
    "we proposed an algorithm based on lagrangian relaxation to find approximate solutions to nmu .",
    "we tested two factorization methods based on this algorithm , one with full recursion ( r - nmu ) , the other without recursion ( g - nmu ) , on several standard image datasets .",
    "after suitable post - processing , we observed that the factors computed by these methods indeed offer a good compromise between their achieved sparsity and the resulting approximation error , comparable or sometimes superior to that of two standard sparse matrix factorization techniques .",
    "these two variants can be contrasted in the following way : where g - nmu mainly focuses on finding sparse factors with small reconstruction error , in the same spirit as snmf and hoyer , r - nmu typically computes an even sparser factorization corresponding to a better part - based representation , albeit with a moderate increase in the reconstruction error ( due to the greedy approach ) . moreover ,",
    "this second variant is useful in situations where the factorization rank is not fixed a priori : the fact that it is recursive allows the user to stop the procedure as soon as the reconstruction error becomes satisfactory , without having to recompute a completely different solution from scratch every time a higher - rank factorization needs to be considered .",
    "the authors would like to thank the anonymous reviewers for their insightful comments which helped improve the paper .",
    "d.  chen , r.  plemmons , nonnegativity constraints in numerical analysis , 2007 , paper presented at the symposium on the birth of numerical analysis , leuven belgium . to appear in the conference proceedings , to be published by world scientific press , a. bultheel and r. cools , eds .",
    "r.  albright , j.  cox , d.  duling , a.  langville , c.  meyer , initializations and convergence for the nonnegative matrix factorization , in : 12th acm sigkdd int .",
    "knowledge discovery and data mining , 2006 .",
    "m.  berry , n.  gillis , f.  glineur , document classification using nonnegative matrix factorization and underapproximation , in : proc . of the ieee international symposium on circuits and systems ( iscas ) , 2009 , pp .",
    "27822785 , isbn : 978 - 1 - 4244 - 3828 - 0 ."
  ],
  "abstract_text": [
    "<S> nonnegative matrix factorization consists in ( approximately ) factorizing a nonnegative data matrix by the product of two low - rank nonnegative matrices . </S>",
    "<S> it has been successfully applied as a data analysis technique in numerous domains , e.g. , text mining , image processing , microarray data analysis , collaborative filtering , etc .    </S>",
    "<S> we introduce a novel approach to solve nmf problems , based on the use of an underapproximation technique , and show its effectiveness to obtain sparse solutions . </S>",
    "<S> this approach , based on lagrangian relaxation , allows the resolution of nmf problems in a recursive fashion . </S>",
    "<S> we also prove that the underapproximation problem is np - hard for any fixed factorization rank , using a reduction of the maximum edge biclique problem in bipartite graphs .    </S>",
    "<S> we test two variants of our underapproximation approach on several standard image datasets and show that they provide sparse part - based representations with low reconstruction error . </S>",
    "<S> our results are comparable and sometimes superior to those obtained by two standard sparse nonnegative matrix factorization techniques . </S>",
    "<S> +    * keywords : * nonnegative matrix factorization , underapproximation , maximum edge biclique problem , sparsity , image processing . </S>"
  ]
}