{
  "article_text": [
    "let @xmath4 and @xmath5 be random vectors in @xmath1 .",
    "we ask the following question : if the distributions of @xmath4 and @xmath5 are close in certain sense , can we guarantee that their differential entropies are close as well ?",
    "for example , one can ask whether @xmath6 one motivation comes from multi - user information theory , where frequently one user causes interference to the other and in proving the converse one wants to replace the complicated non - i.i.d .",
    "interference by a simpler i.i.d . approximation . as a concrete example",
    ", we consider the so - called `` missing corner point '' problem in the capacity region of the two - user gaussian interference channels ( gic ) @xcite . perhaps due to the explosion in the number of interfering radio devices , this problem has attracted renewed attention recently  @xcite . for further information on capacity region of gic and especially the problem of corner points",
    ", we refer to a comprehensive account just published by igal sason  @xcite .",
    "mathematically , the key question for settling `` missing corner point '' is the following : given independent @xmath7-dimensional random vectors @xmath8 with the latter two being gaussian , is it true that @xmath9    to illustrate the nature of the problem , we first note that the answer to is in fact negative as the counterexample of @xmath10 and @xmath11 demonstrates , in which case the divergence is @xmath12 but the differential entropies differ by @xmath13 .",
    "therefore even for very smooth densities the difference in entropies is not controlled by the divergence .",
    "the situation for discrete alphabets is very similar , in the sense that the gap of shannon entropies can not be bounded by divergence in general ( with essentially the same counterexample as that in the continuous case : @xmath4 and @xmath5 being uniform on one and two hamming spheres respectively ) .",
    "the rationale of the above discussion is two - fold : a ) certain regularity conditions of the distributions must be imposed ; b ) distances other than kl divergence might be more suited for bounding the entropy difference .",
    "correspondingly , the main contribution of this paper is the following : under suitable regularity conditions , the difference in entropy ( in both continuous and discrete cases ) can in fact be bounded by the _ wasserstein distance _ , a notion originating from optimal transportation theory which turns out to be the main tool of this paper .",
    "we start with the definition of the wasserstein distance on the euclidean space .",
    "given probability measures @xmath14 on @xmath1 , define their @xmath15-wasserstein distance ( @xmath16 ) as @xmath17)^{1/p } ,      \\label{eq : wp}\\ ] ] where @xmath18 denotes the euclidean distance and the infimum is taken over all couplings of @xmath19 and @xmath20 , i.e. , joint distributions @xmath21 whose marginals satisfy @xmath22 and @xmath23 .",
    "the following dual representation of the @xmath24 distance is useful : @xmath25    similar to , it is easy to see that in order to control @xmath26 by means of @xmath27 , one necessarily needs to assume some regularity properties of @xmath28 and @xmath29 ; otherwise , choosing one to be a fine quantization of the other creates infinite gap between differential entropies , while keeping the @xmath30 distance arbitrarily small .",
    "our main result in shows that under moment constraints and certain conditions on the densities ( which are in particular satisfied by convolutions with gaussians ) , various information measures such as differential entropy and mutual information on @xmath1 are in fact @xmath31-lipschitz continuous with respect to the @xmath30-distance .",
    "these results have natural counterparts in the discrete case where the euclidean distance is replaced by hamming distance ( ) .",
    "furthermore , _ transportation - information inequalities _ , such as those due to marton @xcite and talagrand @xcite , allow us to bound the wasserstein distance by the kl divergence ( see , e.g. , @xcite for a review ) .",
    "for example , talagrand s inequality states that if @xmath32 , then @xmath33 where @xmath34 denotes the maximal singular value of @xmath35 . invoking  ( [ eq : talagrand ] ) in conjunction with the wasserstein continuity of the differential entropy",
    ", we establish  ( [ eq : costa_req ] ) and prove a new outer bound for the capacity region of the two - user gic , finally settling the missing corner point in @xcite . see for details .",
    "one interesting by - product is an estimate that goes in the reverse direction of  ( [ eq : talagrand ] ) .",
    "namely , under regularity conditions on @xmath19 and @xmath20 we have , denote @xmath36 if @xmath37 is at most some universal constant . ]",
    "@xmath38 see   and in the next section .",
    "we want to emphasize that there are a number of estimates of the form @xmath39 where @xmath40 are independent of a standard gaussian vector @xmath41 , cf .",
    "* chapter 9 , remark 9.4 ) .",
    "the key difference of these estimates from  ( [ eq : dgg ] ) is that the @xmath30 distance is measured _ after _ convolving with @xmath42 .",
    "[ [ notations ] ] notations + + + + + + + + +    throughout this paper @xmath43 is with respect to an arbitrary base , which also specifies the units of differential entropy @xmath44 , shannon entropy @xmath45 , mutual information @xmath46 and divergence @xmath47 .",
    "the natural logarithm is denoted by @xmath48 .",
    "the norm of @xmath49 is denoted by @xmath50 . for random variables @xmath4 and @xmath51 , let @xmath52 denote their independence .",
    "we say that a probability density function @xmath15 on @xmath53 is @xmath54-regular if @xmath55 and @xmath56 notice that in particular , regular density is never zero and furthermore @xmath57 therefore , if @xmath4 has a regular density and finite second moment then @xmath58 + { c_1\\over2 } \\ee[\\|x\\|^2 ] < \\infty\\,.\\ ] ]    [ prop : ppr ] let @xmath59 and @xmath60 be random vectors with finite second moments . if @xmath60 has a @xmath54-regular density @xmath61 , then there exists a coupling @xmath62 , such that @xmath63 \\le   \\delta \\,,\\ ] ] where @xmath64 } + \\frac{c_1}{2}\\sqrt{\\ee[\\|v\\|^2]}+c_2\\big ) w_2(p_{u } , p_{v})\\,.\\ ] ] consequently , @xmath65 if both @xmath59 and @xmath60 are @xmath54-regular , then @xmath66    first notice : @xmath67 where follows from cauchy - schwartz inequality and the @xmath54-regularity of @xmath61 .",
    "taking expectation of  ( [ eq : ppr1a ] ) with respect to @xmath68 distributed according to the optimal @xmath30-coupling of @xmath69 and @xmath70 and then applying cauchy - schwartz and triangle inequality for @xmath71-norm , we obtain .    to show  ( [ eq : ppr2 ] ) notice that by finiteness of second moment @xmath72 .",
    "if @xmath73 then there is nothing to prove .",
    "so assume otherwise , then in identity @xmath74\\ ] ] all terms are finite and hence  ( [ eq : ppr2 ] ) follows . clearly ,  ( [ eq : ppr2 ] ) implies  ( [ eq : ppr3 ] ) ( when applied with @xmath59 and @xmath60 interchanged ) .",
    "finally , for  ( [ eq : ppr4 ] ) just add the identity  ( [ eq : ppr2a ] ) to itself with @xmath59 and @xmath60 interchanged to obtain @xmath75   + \\ee\\left[\\log { p_{u}(u)\\over p_{u}(v)}\\right]\\ ] ] and estimate both terms via  ( [ eq : ppr1 ] ) .",
    "the key question now is what densities are regular .",
    "it turns out that convolution with sufficiently smooth density , such as gaussians , produces a regular density .",
    "[ prop : pqr ] let @xmath76 where @xmath77 and @xmath78 < \\infty$ ] .",
    "then the density of @xmath60 is @xmath54-regular with @xmath79 and @xmath80 $ ] .",
    "first notice that whenever density @xmath81 of @xmath41 is differentiable and non - vanishing , we have : @xmath82}{p_v(v ) } =   \\ee[\\nabla \\log p_{z}(v - b)|v = v]\\,,\\ ] ] where @xmath83 $ ] is the density of @xmath60 . for @xmath84",
    ", we have @xmath85 so the proof is completed by showing @xmath86 \\le 3\\|v\\| + 4\\ee[\\|b\\|]\\,.\\ ] ] for this , we mirror the proof in  ( * ? ? ? * lemma 4 ) .",
    "indeed , we have @xmath87 & = \\ee{\\left [ \\|b - v\\| { p_{z}(b - v)\\over p_{v}(v ) } \\right ] } \\\\             & \\le 2\\ee[\\|b - v\\|1\\{a(b , v)\\le 2\\ } ] + \\ee[\\|b - v\\|a(b , v ) 1\\{a(b , v)>2\\}]\\ , , \\label{eq : pqr1}\\end{aligned}\\ ] ] where we denoted @xmath88 next , notice that @xmath89 thus since @xmath90 = p_{v}(v)$ ] we have an upper bound for the second term in  ( [ eq : pqr1 ] ) as follows @xmath91 \\le \\sqrt{2}\\sigma \\sqrt{\\ln^+{1\\over(2\\pi\\sigma^2)^{{n\\over2 } } 2 p_{v}(v)}}\\,,\\ ] ] where @xmath92 . from markov inequality",
    "we have @xmath93 \\ge 1/2 $ ] and therefore @xmath94)^2\\over 2\\sigma^2}}\\,.\\ ] ] using this estimate in  ( [ eq : pqr2 ] ) we get @xmath95 \\le \\|v\\| + 2 \\ee[\\|b\\|]\\,.\\ ] ] upper - bounding the first term in  ( [ eq : pqr1 ] ) by @xmath96 + 2\\|v\\|$ ] we finish the proof of  ( [ eq : pqr0 ] ) .",
    "another useful criterion for regularity is the following :    if @xmath97 has @xmath54-regular density and @xmath98 satisfies @xmath99 then @xmath100 has @xmath101-regular density .    apply  ( [ eq : pqrx ] ) and the estimate : @xmath102 \\le c_1(\\|v\\| + \\sqrt{np } ) + c_2 . \\qedhere\\ ] ]    as a consequence of regularity , we show that when smoothed by gaussian noise , mutual information , differential entropy and divergence are lipschitz with respect to the @xmath30-distance under average power constraints :    [ cor : w2lip ] assume that @xmath103 , with @xmath104 , \\ee[\\|{{\\tilde{x}}}\\|^2 ]   \\leq { np } $ ] and @xmath105 .",
    "then @xmath106 where @xmath107 .",
    "since @xmath108 \\leq \\sqrt{np}$ ] , by proposition  [ prop : pqr ] , the densities of @xmath109 and @xmath110 are both @xmath111-regular .",
    "the desired statement then follows from applying  ( [ eq : ppr3])-  ( [ eq : ppr4 ] ) to @xmath112 and @xmath113 .",
    "the lipschitz constant @xmath31 is order - optimal as the example of gaussian @xmath4 and @xmath114 with different variances ( one of them could be zero ) demonstrates .",
    "the linear dependence on @xmath30 is also optimal . to see this ,",
    "consider @xmath115 and @xmath116 in one dimension .",
    "then @xmath117 and @xmath118 , as @xmath119 .    in fact , to get the best constants for applications to interference channels it is best to forgo the notion of regular density and deal directly with  ( [ eq : pqrx ] ) .",
    "indeed , when the inputs has bounded norms , the next result gives a sharpened version of what can be obtained by combining with [ prop : pqr ] .",
    "[ prop : best ] let @xmath120 satisfying and @xmath121 be independent .",
    "let @xmath122 .",
    "then for any @xmath59 , @xmath123 - \\ee[\\|v\\|^2 ] + 2 \\sqrt{np } w_1(p_{u } , p_{v})\\right)\\,.\\label{eq : bd_best}\\end{aligned}\\ ] ]    plugging gaussian density @xmath124 into  ( [ eq : pqrx ] ) we get @xmath125 where @xmath126 = \\frac{{\\mathbb{e}}[b p_g(v - b)]}{{\\mathbb{e}}[p_g(v - b)]}$ ] satisfies @xmath127 since @xmath128 almost surely .",
    "next we use @xmath129 taking expectation of the last equation under the @xmath24-optimal coupling and in view of , we obtain .    to get slightly better constants in one - sided version of  ( [ eq : tpr1 ] ) we apply proposition  [ prop : best ] :    [ cor : best ] let @xmath130 be independent , with @xmath131 , @xmath132 and @xmath120 satisfying  ( [ eq : power_as ] ) .",
    "then for every @xmath133 $ ] we have : @xmath134 + 2 { \\left \\langle { \\mathbb{e}}[a ] , { \\mathbb{e}}[b ] \\right\\rangle}- \\ee[\\|g\\|^2]\\right ) +           { \\sqrt{2 n p ( \\sigma_g^2 + c^2\\sigma_z^2)\\log e}\\over \\sigma_g^2 + \\sigma_z^2 } \\sqrt { d(p_{a + c z } \\| p_{g + c          z})}\\end{aligned}\\ ] ]    first , notice that by definition wasserstein distance is non - increasing under convolutions , i.e. , @xmath135 . since @xmath136 and gaussian distribution is stable",
    ", we have @xmath137 which , in turn , can be bounded via talagrand s inequality by @xmath138 from here we apply proposition  [ prop : best ] with @xmath139 replaced by @xmath140 ( and @xmath141 by @xmath142 ) .",
    "in this section we assume that all random variables have bounded norms : @xmath143 \\simleq n\\,.\\ ] ]    first , an interesting question is whether for @xmath144 we can prove @xmath145 the answer is negative .",
    "a counterexample can be given when @xmath10 and @xmath11 .",
    "then divergence is @xmath146 but the differential entropies differ by @xmath13 .",
    "so even for very smooth densities the difference in entropies is not controlled by the divergence .    however , it turns out that @xmath30 distance does work :    mutual information , differential entropy and divergence are @xmath31-lipschitz with respect to @xmath30 : @xmath147    notice that @xmath148 \\simleq \\sqrt{n}\\ ] ] and thus by proposition  [ prop : pqr ] the density of @xmath109 is @xmath149-regular",
    ". then the statement follows from  ( [ eq : ppr3])-  ( [ eq : ppr4 ] ) .",
    "the assumption of regularity is not superfluous . indeed , to have inequality of the sort @xmath150 one necessarily needs to assume some regularity properties of @xmath151 and @xmath152 .",
    "otherwise , choosing one to be a fine quantization of another creates infinite gap between differential entropies , while keeping the @xmath30 distance arbitrarily small .",
    "the lipschitz constant @xmath31 is order - optimal as the example of gaussian @xmath4 and @xmath114 with different variances ( one of them could be zero ) demonstrates .",
    "let @xmath153 and @xmath41 be jointly independent and @xmath154 iid gaussian .",
    "then @xmath155    by talagrand s transportation inequality @xcite : @xmath156 thus , by coupling we also have @xmath157 then , the statement follows from  ( [ eq : tpr1 ] ) .",
    "consider the two - user gaussian interference channel ( gic ) : @xmath158 with @xmath159 , @xmath160 and a power constraint on the @xmath7-letter codebooks : either @xmath161 or @xmath162 \\le { np_1 } , \\quad { \\mathbb{e}}[\\|x_2\\|^2 ] \\le np_2 .",
    "\\label{eq : power2}\\ ] ] denote by @xmath163 the capacity region of the gic . as an application of the results developed in , we prove an outer bound for the capacity region .",
    "[ thm : corner2 ] let @xmath164 .",
    "let @xmath165 and @xmath166 .",
    "assume the almost sure power constraint .",
    "then for any @xmath167 and @xmath168 , any rate pair @xmath169 satisfies @xmath170 where @xmath171 assume the average power constraint . then holds with @xmath172 replaced by @xmath173 consequently , in both cases , @xmath174 implies that @xmath175 where @xmath176 as @xmath177 .    without loss of generality ,",
    "assume that all random variables have zero mean .",
    "first of all , setting @xmath178 ( which is equivalent to granting the first user access to @xmath179 ) will not shrink the capacity region of the interference channel . therefore to prove the desired outer bound it suffices to focus on the following z - interference channel henceforth : @xmath180 let @xmath181 be @xmath7-dimensional random variables corresponding to the encoder output of the first and second user , which are uniformly distributed on the respective codebook . for @xmath182 define @xmath183 by fano s inequality there is no difference asymptotically between this definition of rate and the operational one .",
    "define the entropy - power function of the @xmath184-codebook : @xmath185 we know the following general properties of @xmath186 :    * @xmath187 is monotonically increasing . *",
    "@xmath188 ( since @xmath184 is uniform over the codebook ) .",
    "* @xmath189 ( since @xmath190 by entropy power inequality ) .",
    "* @xmath186 is concave ( costa s entropy power inequality  @xcite ) .",
    "* @xmath191 ( gaussian maximizes differential entropy ) .",
    "we can then express @xmath192 in terms of the entropy power function as @xmath193 it remains to upper bound @xmath194 .",
    "note that @xmath195 and therefore @xmath196 where @xmath197 is defined in .",
    "this in conjunction with the slope property @xmath198 yields @xmath199 which , in view of , yields the first part of the bound .    to obtain the second bound ,",
    "let @xmath200 .",
    "using @xmath201 \\leq n p_2 $ ] and @xmath202 , we obtain @xmath203 that is , @xmath204 furthermore , @xmath205 note that the second term is precisely @xmath206 .",
    "the first term can be bounded by applying and with @xmath207 , @xmath208 , @xmath209 and @xmath210 : @xmath211 combining  yields @xmath212 where @xmath172 is defined in . from the concavity of @xmath186 and  ( [ eq : ct2 ] ) @xmath213",
    "where @xmath214 . in view of ,",
    "upper bounding @xmath215 in  ( [ eq : ct5 ] ) via  ( [ eq : ct3 ] ) we get after some simplifications the second part of  ( [ eq : corner2 ] ) .",
    "the outer bound for average power constraint follows analogously with replaced by below : by , the density of @xmath216 is @xmath217-regular . applying to , we have @xmath218 , where @xmath219 again using the fact that @xmath30 distance is non - decreasing under convolutions and invoking talagrand s inequality , we have @xmath220 which yields @xmath221 this yields the outer bound with @xmath222 defined in .    finally , in both cases , when @xmath223 , we have @xmath224 and @xmath225 and hence from @xmath226 .",
    "the first part of the bound  ( [ eq : corner2 ] ) coincides with sato s outer bound @xcite and ( * ? ? ?",
    "* theorem 2 ) by kramer , which ( * ? ? ?",
    "* theorem 2 ) was obtained by reducing the z - interference channel to the degraded broadcast channel ; the second part of  ( [ eq : corner2 ] ) is new , which settles the missing corner point of the capacity region ( see for discussions ) .",
    "note that our estimates on @xmath194 in the proof of theorem  [ thm : corner2 ] are tight in the sense that there exists a concave function @xmath186 satisfying the listed general properties , estimates  ( [ eq : ct2 ] ) and  ( [ eq : ct3 ] ) as well as attaining the minimum of  ( [ eq : ct4 ] ) and  ( [ eq : ct5 ] ) at @xmath194 . hence",
    ", tightening the bound via this method would require inferring more information about @xmath186 .",
    "the outer bound relies on costa s epi . to establish the second statement about corner point ,",
    "it is sufficient to invoke the concavity of @xmath227 ( * ? ? ?",
    "* corollary 1 ) , which is strictly weaker than costa s epi .",
    "[ rmk : igamma ]    the outer bound is evaluated on fig .",
    "[ fig : eval ] for the case of @xmath178 ( z - interference ) , where we also plot ( just for reference ) the simple han - kobayashi inner bound for the z - gic attained by choosing @xmath228 with @xmath229 jointly gaussian .",
    "this achieves rates : @xmath230 for more sophisticated han - kobayashi bounds see  @xcite .     establishes the location of the upper corner point , as conjectured by costa  @xcite .",
    "the bottom corner point has been established by sato @xcite . ]",
    "the two corner points of the capacity region are defined as follows : @xmath231 where @xmath232 . as a corollary , completes the picture of the corner points for the capacity region of gic for all values of @xmath233 under the average power constraint .",
    "we note that the new result here is the proof of @xmath234 for @xmath235 and @xmath167 .",
    "the interpretation is that if one user desires to achieve its own interference - free capacity , then the other user must guarantee that its message is decodable at both receivers .",
    "the achievability of this corner point was previously known , while the converse was previously considered by costa  @xcite but with a flawed proof , as pointed out in  @xcite .",
    "the high - level difference between our proof and that of  @xcite is the replacement of pinsker s inequality by talagrand s and the use of a coupling argument .",
    "below we present a brief account of the corner points in various cases ; for an extensive discussion see  @xcite .",
    "we start with a few simple observations about the capacity region @xmath236 :    * any rate pair satisfying the following belongs to @xmath236 : @xmath237 which corresponds to the intersection of two gaussian multiple - access ( mac ) capacity regions , namely , @xmath238 and @xmath239 .",
    "these rate pairs correspond to the case when each receiver decodes both messages . * for @xmath240 and @xmath241 ( strong interference ) the capacity region",
    "is known to coincide with @xcite .",
    "so , without loss of generality we assume @xmath242 henceforth .",
    "* replacing either @xmath243 or @xmath244 with zero can only enlarge the region ( genie argument ) . * if @xmath245 then for any @xmath246 we have @xcite @xmath247 this follows from the observation that in this case @xmath248 , since conditioned on @xmath184 , @xmath249 is a noisier observation of @xmath179 than @xmath250 .    for the top corner",
    ", we have the following : @xmath251 note that for any @xmath167 , @xmath252 is discontinuous as @xmath253 . to verify  ( [ eq : bt2 ] ) we consider each case separately :    1 .",
    "for @xmath254 the converse bound follows from .",
    "for achievability , we consider two cases .",
    "when @xmath255 , we have @xmath256 and therefore treating interference @xmath179 as noise at the first receiver and using a gaussian mac - code for @xmath257 works . for @xmath258 ,",
    "the achievability follows from the mac inner bound .",
    "note that since @xmath259 , a gaussian mac - code that works for @xmath257 will also work for @xmath260 .",
    "alternatively , the achievability also follows from han - kobayashi inner bound ( see , e.g. , ( * ? ? ?",
    "* theorem 6.4 ) with @xmath261 for @xmath262 and @xmath263 for @xmath264 ) .",
    "2 .   for @xmath265 and @xmath266",
    "the converse is obvious , while for achievability we have that @xmath267 and therefore @xmath179 is decodable at @xmath250 .",
    "3 .   for @xmath265 and @xmath268",
    "the converse is  ( [ eq : bt1 ] ) and the achievability is just the mac code @xmath260 with rate @xmath269 .",
    "4 .   for @xmath265 and @xmath270",
    "the result follows from the treatment of @xmath271 below by interchanging @xmath272 and @xmath273 .",
    "the bottom corner point is given by the following : @xmath274 which is discontinuous as @xmath275 for any fixed @xmath276 $ ] .",
    "we treat each case separately :    1 .",
    "the case of @xmath277 is due to sato @xcite ( see also ( * ? ? ?",
    "* theorem 2 ) ) .",
    "the converse part also follows from   ( for @xmath265 there is nothing to prove ) . for the achievability",
    ", we notice that under @xmath278 we have @xmath279 and thus @xmath179 at rate @xmath277 can be decoded and canceled from @xmath250 by simply treating @xmath184 as gaussian noise ( as usual , we assume gaussian random codebooks ) .",
    "thus the problem reduces to that of @xmath178 .",
    "for @xmath178 , the gaussian random coding achieves the claimed result if the second receiver treats @xmath184 as gaussian noise .",
    "2 .   the converse follows from  ( [ eq : bt1 ] ) and for the achievability we use the gaussian mac - code @xmath260 and treat @xmath184 as gaussian interference at @xmath249 .",
    "if @xmath280 $ ] , we apply results on @xmath281 in by interchanging @xmath282 and @xmath273 .",
    "fix a finite alphabet @xmath283 and an integer @xmath7 . on the product space",
    "@xmath284 we define the hamming distance @xmath285 and consider the corresponding wasserstein distance @xmath24 .",
    "in fact , @xmath286 is known as ornstein s @xmath3-distance @xcite , namely , @xmath287 ,      \\label{eq : dbar}\\ ] ] where the infimum is taken over all couplings @xmath21 of @xmath19 and @xmath20 . for @xmath288 ,",
    "this coincides with the total variation , which is also expressible as @xmath289 for @xmath14 on @xmath290 .    for a pair of distributions @xmath14 on @xmath284 we may ask the following questions :    1 .",
    "does @xmath291 control the entropy difference @xmath292 ?",
    "2 .   does @xmath293 control the entropy difference @xmath292 ?",
    "recall that in the euclidean space the answer to both questions was negative unless the distributions satisfy certain regularity conditions . for discrete alphabets",
    "the answer to the first question is still negative in general ( see for a counterexample ) ; nevertheless , the answer to the second one turns out to be positive :    [ prop : dew ] let @xmath19 and @xmath20 be distributions on @xmath284 and let @xmath294 then @xmath295    in fact , the statement holds for any translation - invariant distance @xmath296 on @xmath290 extended additively to @xmath297 , i.e. , @xmath298 for any @xmath299 . indeed ,",
    "define @xmath300 \\le n s \\right\\}\\,,\\ ] ] where @xmath301 is an arbitrary fixed string .",
    "it is easy to see that @xmath302 is concave since @xmath303 is .",
    "furthermore , writing @xmath304 and applying chain - rule for entropy we get @xmath305 thus , letting @xmath306 be distributed according to the @xmath3-optimal coupling of @xmath19 and @xmath20 , we get @xmath307)\\right ] \\label{eq : jpt1}\\\\          & \\le n f_n(\\bar d(p , q))\\,,\\label{eq : jpt2}\\end{aligned}\\ ] ] where  ( [ eq : jpt1 ] ) is by definition of @xmath308 and  ( [ eq : jpt2 ] ) is by jensen s inequality .",
    "finally , for the hamming distance we have @xmath309 by fano s inequality .",
    "notice that the right - hand side of  ( [ eq : jpt ] ) behaves like @xmath310 when @xmath293 is small .",
    "this super - linear dependence is in fact sharp . and choose @xmath19 to be the output distribution of the optimal lossy compressor for @xmath20 at average distortion @xmath311 . by definition , @xmath312 . on the other hand , @xmath313 as @xmath314 and hence @xmath315 , which asymptotically meets the upper bound with equality . ] nevertheless , if certain regularity of distributions is assumed , the estimate can be improved to be linear in @xmath293 .",
    "the next result is the analog of in the discrete space .",
    "we formulate it in a form convenient for applications in multi - user information theory .",
    "[ prop : dbar ] let @xmath316 be a two - input blocklength-@xmath7 memoryless channel , namely @xmath317 where @xmath318 is a stochastic matrix and @xmath319 .",
    "let @xmath320 be independent @xmath7-dimensional discrete random vectors .",
    "let @xmath51 and @xmath321 be the outputs generated by @xmath322 and @xmath323 , respectively",
    ". then @xmath324 \\label{eq : dbari}\\end{aligned}\\ ] ] where @xmath325 & ~\\eqdef \\sum_{x\\in { \\ensuremath{\\mathcal{x}}}^n } p_x(x ) \\bar d(p_{y|x = x } , p_{{{\\tilde{y}}}|x = x } ) . \\ ] ]    given any stochastic matrix @xmath139 , define @xmath326 .",
    "recall the following fact from ( * ? ? ?",
    "( 58 ) ) about mixtures of product distributions : let @xmath59 and @xmath60 be @xmath7-dimensional discrete random vector connected by a product channel , that is , @xmath327 . then the mapping @xmath328 is @xmath329-lipschitz with respect to the hamming distance , where @xmath330 } l(p_{v_i|u_i})$ ] . consider another pair @xmath331 connected by the same channel , i.e. , @xmath332 .",
    "then lipschitz continuity implies that @xmath333 \\le l \\ee[d_h(v,{{\\tilde{v}}})]$ ] for any coupling @xmath334 .",
    "optimizing over the coupling and in view of , we obtain @xmath335 \\le",
    "l n \\bar d(p_{v } , p_{{{\\tilde{v}}}})\\,.\\ ] ] repeating the proof of  ( [ eq : ppr2])  ( [ eq : ppr4 ] ) , we have @xmath336 applying and to @xmath51 and @xmath337 gives and with @xmath338 defined in .",
    "to bound the mutual information , we first notice @xmath339 applying  ( [ eq : hstab ] ) conditioned on @xmath340 we get @xmath341 where @xmath342 } \\max_{y , y',a } \\log \\frac{w(y|x_j , a)}{w(y'|x_j , a)}$ ] . note that @xmath343 for any @xmath344 , averaging over @xmath345 gives @xmath346\\,.\\ ] ] from the convexity of @xmath347 , which holds for any wasserstein distance , we have @xmath348 $ ] and so the left - hand side of  ( [ eq : rgf1 ] ) also bounds @xmath349 from above .      in this section we discuss how previous bounds ( and [ prop : dbar ] ) in terms of the @xmath350-distance can be converted to bounds in terms of kl divergence .",
    "this is possible when @xmath20 is a product distribution , thanks to marton s transportation inequality  ( * ? ? ?",
    "* lemma 1 ) .",
    "we formulate this together with a few other properties of the @xmath351-distance in the following lemma proved in .",
    "[ lmm : dbar ]     1 .",
    "( marton s transportation inequality  @xcite ) : for any pair of distributions @xmath19 and @xmath352 on @xmath284 , @xmath353 2 .",
    "( tensorization ) @xmath354 .",
    "3 .   ( contraction ) for @xmath21 and @xmath355 such that @xmath356 , @xmath357 } { \\eta_{\\rm tv}}(p_{y_i|x_i } )    \\bar{d}(p_x , q_x ) .",
    "\\label{eq : dbar - contract}\\ ] ] where @xmath358 is dobrushin s contraction coefficient of a markov kernel @xmath97 defined as @xmath359 .",
    "if we assume that @xmath360 for some small @xmath361 , then combining and gives @xmath362 where the right - hand side behaves as @xmath363 when @xmath177 . this estimate has a one - sided improvement ( here again @xmath20 must be a product distribution ) : @xmath364 ( see  @xcite for @xmath288 and  ( * ? ? ?",
    "* appendix h ) for the general case ) .    switching to the setting in ,",
    "let us consider the case where @xmath365 has i.i.d .",
    "components , i.e. , @xmath366 . define @xmath367 which is the maximal dobrushin contraction coefficients among all channels @xmath368 indexed by @xmath369",
    ". then @xmath370 \\leq      { \\eta_{\\rm tv}}\\bar d(p_a , p_{{\\tilde{a } } } ) \\le { \\eta_{\\rm tv}}\\sqrt{\\frac{d(p_{a } \\| p_{{{\\tilde{a}}}})}{2 n \\log e}}\\ , ,      \\label{eq : dbar - etatv}\\ ] ] where the left inequality is by convexity of the @xmath3-distance as a wasserstein distance , the middle inequality is by , and the right inequality is via  ( [ eq : marton1 ] ) .",
    "an alternative to the estimate is the following : @xmath371 \\label{eq : gb1}\\\\             & \\le { \\mathbb{e}}{\\left [   \\sqrt{{1 \\over2n\\log e } d(p_{y|x } \\| p_{{{\\tilde{y}}}|x } ) } \\right ] }              \\label{eq : gb2}\\\\             & \\le \\sqrt{{1\\over2n\\log e } d(p_{y|x } \\| p_{{{\\tilde{y}}}|x}|p_x ) }              \\label{eq : gb3}\\\\             & \\le \\sqrt{{1\\over2n\\log e } { \\eta_{\\rm kl}}d(p_{a } \\| p_{{{\\tilde{a } } } } ) }              \\label{eq : gb4}\\end{aligned}\\ ] ] where  ( [ eq : gb2 ] ) is by  ( [ eq : marton1 ] ) since @xmath372 is a product distribution as @xmath365 has a product distribution ,  ( [ eq : gb3 ] ) is by jensen s inequality , and  ( [ eq : gb4 ] ) is by the tensorization property of the strong data - processing constant for divergence  @xcite : @xmath373 to conclude , in the regime of @xmath374 for some small @xmath361 our main yields @xmath375 matching the behavior of  ( [ eq : chang ] ) .",
    "however , the estimate is stronger , because a ) it is two - sided and b ) @xmath376 can be a mixture of product distributions ( since @xmath4 in may be arbitrary ) .      in order to apply   to determine corner points of capacity regions of discrete memoryless interference channels ( dmic )",
    "we will need an auxiliary tensorization result .",
    "this result appears to be a rather standard exercise for degraded channels and so we defer the proof to .    [",
    "prop : tensoh ] given channels @xmath377 and @xmath378 on finite alphabets , define @xmath379 then the following hold :    1 .",
    "( property of @xmath380 ) the function @xmath381 is concave , non - decreasing and @xmath382 . furthermore , @xmath383 for all @xmath384 , provided that @xmath378 and @xmath377 satisfy @xmath385 and @xmath386 respectively .",
    "2 .   ( tensorization ) for any blocklength-@xmath7 markov chain @xmath387 , where @xmath388 and @xmath389 are @xmath7-letter memoryless channels , we have @xmath390    neither of the sufficient condition and for strict inequality is superfluous , as can be seen from the example @xmath391 and @xmath392 , respectively ; in both cases @xmath393 .",
    "the important consequence of   is the following implication : and @xmath394 we have @xmath395 this also follows from the concavity of @xmath396 . ]    [ cor : tensox ] let @xmath397 , where the memoryless channels @xmath377 and @xmath378 of blocklength @xmath7 satisfy the conditions and .",
    "then there exists a continuous function @xmath398 satisfying @xmath399 , such that for all @xmath7 @xmath400    by , we have @xmath383 for all @xmath384 .",
    "this together with the concavity of @xmath380 implies that @xmath401 is convex , strictly increasing and strictly positive on @xmath402 .",
    "define @xmath403 as the inverse of @xmath401 , which is increasing and concave and satisfies @xmath399 . since @xmath404 , the tensorization result yields @xmath405 i.e. , @xmath406 , where @xmath407",
    ". then @xmath408 by definition , completing the proof .",
    "we are now ready to state a non - trivial example of corner points for the capacity region of dmic .",
    "the proof strategy mirrors that of , with and costa s epi replaced by and , respectively .",
    "[ thm : corner - discrete ] consider the two - user dmic : @xmath409 where @xmath410 , @xmath411 are independent and @xmath412 is i.i.d .  for some non - uniform @xmath413 containing no zeros .",
    "the maximal rate achievable by user 2 is @xmath414 at this rate the maximal rate of user 1 is @xmath415    as an example , consider @xmath416 $ ] where @xmath417 .",
    "then the maximum in is achieved by @xmath418 $ ] .",
    "therefore @xmath419 and @xmath420 , where @xmath421 $ ] .",
    "note that in the case of @xmath422 , where is not applicable , we simply have @xmath423 and @xmath424 since @xmath425 .",
    "therefore the corner point is discontinuous in @xmath172",
    ".    continues to hold even if cost constraints are imposed .",
    "indeed , if @xmath426 is required to satisfy @xmath427 for some cost function @xmath428 .",
    "then the maximum in and is taken over all @xmath20 such that @xmath429 \\leq b$ ] . note that taking @xmath430 is equivalent to dropping the constraint @xmath431 in . in this case , @xmath432 which can be shown by a simpler argument not involving .",
    "we start with the converse . given a sequence of codes with vanishing probability of error and rate pairs @xmath433 , where @xmath434",
    ", we show that @xmath435 , where @xmath436 as @xmath177 .",
    "let @xmath437 be the maximizer of , i.e. , the capacity - achieving distribution of the channel @xmath438 .",
    "let @xmath439 be distributed according to @xmath440 .",
    "then @xmath441 , where @xmath442 . by fano",
    "s inequality , @xmath443 that is , @xmath444 since @xmath445 is a product distribution , marton s inequality yields @xmath446 applying in and in view of the translation invariance of the @xmath350-distance , we obtain @xmath447   \\nonumber \\\\ = & ~ 2 c n \\bar{d}(p_{x_2+z_2 } , p_{{{\\tilde{x}}}_2 + z_2 } )    \\nonumber \\\\ \\leq & ~     ( \\alpha \\sqrt{\\epsilon } + o(1 ) ) n , \\nonumber \\end{aligned}\\ ] ] where @xmath448 and @xmath449 are finite since @xmath413 contains no zeros by assumption . on the other hand , @xmath450 where @xmath451 by fano s inequality . combining the last two displays",
    ", we have @xmath452 next we apply , with @xmath453 . to verify the conditions , note that the channel @xmath377 is memoryless and additive with non - uniform noise distribution @xmath413 , which satisfies the condition .",
    "similar , the channel @xmath378 is memoryless and additive with noise distribution @xmath437 , which is the maximizer of .",
    "since @xmath413 is not uniform , @xmath437 is not a point mass",
    ". therefore @xmath378 satisfies . then yields @xmath454 where the last inequality follows from the fact that @xmath455 attained by @xmath184 uniform on @xmath456 .",
    "finally , note that the rate pair @xmath457 is achievable by a random mac - code for @xmath239 , with @xmath184 uniform on @xmath456 and @xmath458 .",
    "explaining that the `` missing corner point '' requires proving of  ( [ eq : costa_req ] ) , as well as the majority of our knowledge on interference channels were provided by prof .",
    "chandra nair .",
    "we acknowledge his scholarship and patience deeply .    the research of y.p . has been supported in part by the center for science of information ( csoi ) , an nsf science and technology center , under grant agreement ccf-09 - 39370 and by the nsf career award under grant agreement ccf-12 - 53205 .",
    "the research of y.w . has been supported in part by nsf grants iis-14 - 47879 , ccf-14 - 23088 and ccf-15 - 27105 .",
    "this work would not be possible without the generous support of the simons institute for the theory of computing and california sb-420 .",
    "to prove the tensorization inequality , let @xmath459 be independent and individually distributed as the optimal coupling of @xmath460",
    ". then @xmath461=\\sum_{i=1}^n { \\mathbb{p}\\left[x_i\\neq y_i\\right ] } = \\sum_{i=1}^n { d_{\\rm tv}}(p_i , q_i)$ ] .    to show ,",
    "let @xmath462 be an arbitrary coupling of @xmath21 and @xmath355 so that @xmath463 is distributed according to the optimal coupling of @xmath464 , that is , @xmath465=n{\\bar{d}}(p_x , q_x)$ ] . by the first inequality we just proved , for any @xmath466 , @xmath467 where @xmath468 } { \\eta_{\\rm tv}}(p_{y_i|x_i})$ ] and the middle inequality follows from dobrushin s contraction coefficient . applying dobrushin s contractoin @xcite ( see ( * ? ? ?",
    "* proposition 18 ) , with @xmath469 and @xmath470 ) , there exists a coupling @xmath471 of @xmath21 and @xmath355 , so that @xmath472 and @xmath473 \\leq \\eta { \\mathbb{e}}_{\\pi}[d_h(x,{{\\tilde{x } } } ) ] = n \\eta { \\bar{d}}(p_x , q_x)$ ] , concluding the proof .",
    "basic properties of @xmath380 follow from standard arguments .",
    "to show the strict inequality @xmath383 under the conditions and , we first notice that @xmath380 is simply the concave envelope of the set of achievable pairs @xmath474 obtained by iterating over all @xmath345 . by caratheodory",
    "s theorem , it is sufficient to consider a ternary - valued @xmath59 in the optimization defining @xmath475",
    ". then the set of achievable pairs @xmath476 is convex and compact ( as the continuous image of the compact set of distributions @xmath477 ) . consequently , to have @xmath393 there must exist a distribution @xmath477 , such that @xmath478 we next show that under the extra conditions on @xmath378 and @xmath377 we must have @xmath479 .",
    "indeed , guarantees the channel @xmath378 satisfies the strong data processing inequality ( see , e.g. , ( * ? ? ?",
    "* exercise 15.12 ( b ) ) and ( * ? ? ? * section 1.2 ) for a survey ) that there exists  @xmath480 such that @xmath481 from  ( [ eq : th0 ] ) and  ( [ eq : th1 ] ) we infer that @xmath482 , or equivalently @xmath483 on the other hand , the condition ensures that then we must have @xmath484 .",
    "clearly , this implies @xmath479 in  ( [ eq : th0 ] ) .    to show the single - letterization statement  ( [ eq : sl ] ) , we only consider the case of @xmath485 since the generalization is straightforward by induction .",
    "let @xmath486 be a markov chain with blocklength-@xmath487 memoryless channel in between .",
    "we have @xmath488 where  ( [ eq : th2 ] ) is because @xmath489 and hence @xmath490 , and  ( [ eq : th3 ] ) is because @xmath491 .",
    "next consider the chain @xmath492 where  ( [ eq : th4 ] ) is by @xmath493 and hence @xmath494 ,  ( [ eq : th5 ] ) is by the definition of @xmath380 and since we have both @xmath495 and @xmath496 ,  ( [ eq : th6 ] ) is by the concavity of @xmath380 , and finally  ( [ eq : th7 ] ) is by the monotonicity of @xmath380 and  ( [ eq : th3 ] ) .",
    "ronit bustin , h  vincent poor , and shlomo shamai .",
    "the effect of maximal rate codes on the interfering message rate . in _ proc .",
    "2014 ieee int .",
    "inf . theory ( isit ) _ , pages 9195 , honolulu , hi , usa , july 2014 .",
    "max  h.m .",
    "costa and olivier rioul . from almost gaussian to gaussian : bounding differences of differential entropies . in _ proc .",
    "information theory and applications workshop ( ita ) _ , san diego , ca , february 2015 ."
  ],
  "abstract_text": [
    "<S> it is shown that under suitable regularity conditions , differential entropy is @xmath0-lipschitz as a function of probability distributions on @xmath1 with respect to the quadratic wasserstein distance . under similar conditions , </S>",
    "<S> ( discrete ) shannon entropy is shown to be @xmath2-lipschitz in distributions over the product space with respect to ornstein s @xmath3-distance ( wasserstein distance corresponding to the hamming distance ) . </S>",
    "<S> these results together with talagrand s and marton s transportation - information inequalities allow one to replace the unknown multi - user interference with its i.i.d .  </S>",
    "<S> approximations . as an application , a new outer bound for the two - user gaussian interference channel is proved , which , in particular , settles the `` missing corner point '' problem of costa ( 1985 ) . </S>"
  ]
}