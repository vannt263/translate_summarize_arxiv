{
  "article_text": [
    "machine - learning classifiers have proven to be very successful for several cognitive applications such as search , classification , recognition @xcite among others and are being increasingly deployed across a wide range of computing platforms from data centers to mobile devices . while the classifiers are modeled to mimic brain - like cognitive abilities , they lack the remarkable energy - efficient processing capability of the brain .",
    "for instance , supervision @xcite , a state of the art deep learning neural network ( nn ) for image classification tasks , demands compute energy in the order of 2 - 4 giga - ops ( multiply and accumulate operations ( ops ) ) per classification @xcite , which is nearly 8@xmath09 orders of magnitude larger than the human brain . with energy efficiency becoming a primary concern across the computing spectrum ,",
    "energy - efficient realization of large - scale neural networks is of great importance .",
    "it is well known that the visual cortical system is arranged in a hierarchical fashion with different areas responsible for processing different features ( for example , color and shape ) of visual information @xcite . for a given input ,",
    "the visual information is decomposed into representative features and only those areas of the brain that are instrumental to the recognition of the input are activated .",
    "the innate ability to simplify complex visual tasks into characteristic features and the selective activation of different areas based on the feature information in the input , enables the brain to perform cognition with extremely low power consumption . in this paper , we build upon this biological concept of feature selective processing to introduce feature driven selective classification ( falcon ) for faster and energy - efficient image recognition with competitive classification accuracy .",
    "interestingly , we note that there is a significant consensus among features of images across multiple classes in a real world dataset .",
    "consider the simple classification problem of recognizing 4 different objects : strawberry , sunflower , tennis ball and stop sign .",
    "all 4 objects belong to completely different classes . however , strawberry and stop sign have a feature i.e. the red color as representative information common across all images of the 2 objects .",
    "similarly , sunflower and tennis ball have the characteristic yellow color as a common feature . here",
    ", we utilize the feature consensus to break up the classification problem and use a cluster of classifiers to perform smaller classification tasks .",
    "we achieve this by constructing a hierarchical tree of classifiers wherein the initial nodes ( or classifiers ) are trained first to classify the image into general feature categories : red and yellow ( for the above example ) , while the deeper nodes categorize them into the 4 specific classes . the generic - to - specific transition in the classification hierarchy enable us to selectively process only those branches and nodes that are relevant to the input .",
    "1 illustrates our methodology . in the traditional approach shown in fig .",
    "1(a ) , a single classifier is responsible for classifying the inputs into the 4 distinct classes ( a , b , c , d ) .",
    "hence , the network clearly needs to be highly complex ( with more neurons and synapses ) in order to classify the objects with high accuracy .",
    "however , this model x does not take into account the common features across classes and thus expends constant computational effort on all inputs activating each and every connection / neuron to determine the output .",
    "in contrast , fig .",
    "1(b ) shows our proposed falcon approach wherein we build a hierarchical tree of classifiers based on the feature consensus between classes ( a , b and c , d ) .",
    "the initial node ( model y ) in the tree is trained to distinguish between the features ( 1 & 2 ) .",
    "the latter nodes ( model z1 , z2 ) perform the final classification task of separating the objects into classes a , b ( model z1 ) and c , d ( model z2 ) .",
    "since these models ( y , z1 , z2 ) are trained to classify between two different classes , they will be less complex than the traditional model x. it can be clearly seen that the classification task is now broken down into a 2-step process which involves two different paths comprising of separate nodes . due to the 2-step classification , z1 and z2",
    "need to be trained only on a subset of the training dataset as shown in fig 1(b ) , resulting in significant reduction in the training time of these nodes . for a given input instance , if model y gives a high confidence at output neuron p ( q ) , then , only path 1 ( 2 ) and the corresponding model z1 ( z2 ) is enabled while keeping z2 ( z1 ) idle .",
    "hence , our approach is both time and energy efficient , since it involves selective activation of nodes depending upon the input instance .",
    "another significant contribution of our work is the design of a scalable neuromorphic engine ( neue ) that provides a programmable hardware platform for executing falcon models with various nodes and weights .",
    "the neuromorphic engine features a 1d array of neural units ( nus ) followed by an activation unit ( au ) that process the basic computational elements of neural networks .",
    "we enable the neue with appropriate hardware mechanisms to effectively implement selective activation of nodes for energy benefits at run - time .    in summary ,",
    "the key contributions of this work are as follows :    * given any machine learning classifier , we propose a systematic methodology to construct a feature driven selective classification framework that exploits the consensus in the characteristic features ( color / texture ) across images in a dataset to perform faster and energy - efficient classification .",
    "the methodology is independent of the network topology , network parameters and training dataset .",
    "* we develop a design methodology to construct a tree of classifiers ( or nodes ) with a generic - to - specific transition in the classification hierarchy invoking multi - step classification .",
    "the initial nodes of the tree separate the instances based on feature information and selectively enable the latter nodes to perform object specific classification . * in this work , we use color and texture as the distinctive features to implement falcon .",
    "we also present an algorithm to select the optimal color / textures common across multiple classes of objects .",
    "* we design a programmable and scalable neuromorphic engine ( neue ) that can be used to efficiently execute falcon models on artificial neural networks ( anns ) . *",
    "we demonstrate the efficacy of our proposed approach on two natural image datasets : caltech101/ cifar10 .",
    "we construct the falcon based hierarchical tree of anns using the proposed design methodology and execute them on the neue platform to demonstrate significant improvements in energy for negligible loss in output quality .",
    "the rest of the paper is organized as follows . in section",
    "ii , we discuss related work . in section iii , we present the structured approach to construct falcon models .",
    "section iv details the architecture of neue .",
    "section v describes the experimental methodology and the benchmarks .",
    "we discuss the results in section vi and conclude in section vii .",
    "the widespread use of machine learning across computing platforms from data centers to mobile devices has renewed interest in forming efficient methodologies for classification that expend low compute effort . on the algorithmic front ,",
    "substantial work for increasing accuracy in machine - learning classification has been done @xcite .",
    "using semantics or feature information for improving the accuracy of content based image retreival systems has been an active area of research @xcite . in @xcite ,",
    "a comprehensive review of various techniques geared towards extracting global image features ( color , texture , local geometry ) for accurate image retreival has been discussed .",
    "the key idea of using high - level semantic features in our proposed falcon methodology is inspired from content based systems .",
    "however , the novelty of our work arises from the fact that we leverage the similarity in the features across various classes for clustering several classes into one and thus decomposing a large classifcation problem into smaller tasks organised in a tree - fashion to obtain efficiency in training as well as testing complexity .    in the recent past",
    ", there has been significant work employing approximate computing techniques to obtain efficient neural computations relying on the error resilient properties of recognition applications @xcite . in @xcite ,",
    "the authors have considered domain specific insights to introduce hardware approximations in neuromorphic applications . in @xcite ,",
    "the authors have utilized the inherent feature variability across input instances in a dataset to introduce software techniques for designing energy - efficient scalable classification framework .    in the context of efficient neuromorphic systems ,",
    "two major directions have been explored .",
    "the first is accelerator based computing where custom architectures for specific computation of nns are designed . in @xcite , application - specific nn designs and programmable neuromorphic processors have been proposed .",
    "also , nn implementations on programmable accelerators such as gpus have also been explored @xcite .",
    "the second is the use of emerging post - cmos device such as resistive ram @xcite , memristive crossbars @xcite and spintronics @xcite , to realize the individual computational elements : neurons and synapses more efficiently .    in this work ,",
    "we propose a new avenue for energy efficiency in neuromorphic systems by using representative features across images in a real - world dataset .",
    "the main focus of this paper is in developing an automatic design methodology to generate falcon models to lower the testing complexity in traditional classification problems .",
    "in contrast to the approximate techniques @xcite that usually provide an explicit tradeoff between efficiency and quality of results , our approach maintains classification accuracy while providing energy savings .",
    "in addition , our design methodology provides the opportunity to reuse nodes ( discussed in section iii ) enabling the classification framework to be more scalable .",
    "note that the efforts on efficient neuromorphic systems mentioned earlier can be employed with our proposed design methodology to further enhance the efficiency .",
    "also , our methodology improves the training time for large classification tasks which is one of the major challenges in machine learning at present .",
    "in this section , we present our structured approach to construct falcon based hierarchical tree of classifiers . while there exists a suite of machine - learning classifiers like support vector machines , decision trees , neural networks etc .",
    "suitable for classification , we will focus on a particular class : artificial neural network ( anns ) to validate the proposed methodology for image recognition .",
    "please note that the falcon tree can be applied on other machine - learning algorithms as well to lower the compute energy .",
    "falcon employs the features , representative of the input image data , to construct the nodes of the hierarchical tree . referring to fig .",
    "1 , model y is trained to classify the inputs based on the feature information .",
    "hence , the appropriate selection of features is crucial . while there can be several image features that can be used to discriminate the first step of selective classification , in this work , we use color and texture as our distinctive features to implement falcon .",
    "in fact , texture and color are the most widely usedrepresentative features for characterizing low - level image information @xcite . in this work ,",
    "we use hue - saturation - value ( hsv ) transformation @xcite and gabor filtering @xcite to extract the color and texture features of an image , respectively . applying hsv or gabor filtering onto an image",
    "results in dimensionality reduction of the original image .",
    "the reduced feature vector contains the relevant feature information , which is sufficient to characterize and classify an image .",
    "traditionally , images are transformed with appropriate feature extraction techniques to get a lower dimensional input vector @xcite .",
    "a machine - learning classifier yields better classification accuracy and converges to global minima faster when trained on the feature vector as opposed to the original input image .",
    "since falcon invokes multi - step classification , it therefore , enables the latter nodes in the tree ( model z1 , z2 in fig .",
    "1 ) to be trained on feature vectors alone , instead of real pixel valued images . due to the significant reduction in the input vector size , the models",
    "z1 and z2 are much simpler ( fewer neurons and connections ) as compared to the traditional model x. please note that we need to take into account the additional computational cost of hsv and gabor filtering for calculating energy costs @xcite .",
    "hsv gives rise to feature vectors corresponding to 8 color components per image .",
    "similarly , gabor filters corresponding to ` m ' scales and ` n ' orientations give rise to m x n texture components per image @xcite . in this work ,",
    "we use gabor filters with scales : 4@xmath1 * i \\{i= 1,2,4,8 } and 4 orientations : 0 , 45 , 90 , 135 degrees , which are adequate for characterizing the basic texture components of an image @xcite . for each orientation ,",
    "the texture features across all scales ( 4@xmath1 * i \\{i= 1,2,4,8 } ) are concatenated into a single feature vector .",
    "so , the feature selection methodology identifies the most probable orientation across the set of concatenated texture vectors .",
    "the most important question that needs to be answered is how we select the optimal features ( color / texture ) to categorize the images in a dataset into the general feature classes .",
    "we employ a simple search - based method to obtain the features common across multiple classes of objects .",
    "2 gives an overview of the feature selection methodology for a dataset with 4 distinct classes .",
    "for each class of objects in a dataset , we train a nn ( model @xmath2 ) based on a particular feature ( feature vector i ) with the target labels provided with the dataset .",
    "this is done for all four texture ( corresponding to the 4 orientations with scales concatenated ) and the eight color components . in each case , the nn s size and the number of iterations remain fixed .",
    "once the models corresponding to each feature are trained , we pass a single input image for a given class through each model . the feature that gives the highest confidence value ( @xmath3 ) at the output",
    "is chosen as the optimum one for that particular class , given that the confidence value is above a certain user - defined threshold @xmath4 .",
    "for instance , in the sunflower / strawberry / tennis/ stop - sign classification problem , applying the above method across all 4 classes we obtain that red feature produces a confidence value of 0.9 and 0.8 for strawberry and stop - sign while 0.3 and 0.2 for tennis and sunflower , respectively .",
    "thus , strawberry and stop - sign will be categorized under the red category by the initial node ( model y from fig .",
    "@xmath4 is chosen to be around 0.6 - 0.8 to get the most accurate feature selection .",
    "3 shows the conceptual view of the framework for a 4-object classification problem .",
    "3(a ) shows the baseline classifier with a single nn that has 4 output neurons corresponding to each class ( a , b , c , d ) .",
    "3 ( b ) illustrates the proposed falcon based tree with three nodes ( not considering node 4 for now ) .",
    "each node is a nn classifier trained using the standard backpropagation algorithm .",
    "first , the feature selection methodology discussed in section iii(a ) is employed to obtain the general features that are used as training labels ( r , y ) for the initial node ( node 1 ) .",
    "node 1 is responsible for classifying the input into the two broad feature categories and thus has two output neurons .",
    "node 2 and 3 then separate the inputs with feature consensus into the corresponding classes ( a , b and c , d ) . thus , the class labels produced at node 2 and 3 are expressed as the final output of the falcon framework .",
    "node 2 ( 3 ) is selectively activated only if the class label produced from node 1 is r ( y ) .",
    "node 2 and 3 are trained on the reduced feature vectors as input . in contrast , the original rgb pixel values are fed as input to node 1 to obtain a competitive classification accuracy with respect to the baseline classifier . the multi - step classification process enables the nodes in the falcon tree to be less complex than the baseline nn resulting in overall energy - efficiency .      in falcon , each node of the tree is trained separately on the input instances as discussed above . during test time , data is processed through each node in the tree to produce a class label .",
    "it is evident that the initial node ( node 1 in fig .",
    "3(b ) ) of the falcon tree would be the main bottleneck for achieving iso - accuracy with that of the baseline classifier .",
    "for an input instance belonging to class r , if node 1 produces a higher confidence value for class y , the input instance is not passed to the latter nodes and is misclassified at the first stage itself , resulting in a decline in accuracy .",
    "this would arise when the input instance has characteristics pertaining to both features ( r and y ) .",
    "for example , an image of a strawberry might have some yellow objects in the background .",
    "in such cases , the difference in the confidence of the two output neurons at node 1 would be low . as a result",
    ", the instance will get misclassified . to avoid this",
    ", we add the baseline classifier as a 4th node in the falcon tree that is enabled by the divergence module ( triangle in fig .",
    "the divergence module activates the 4th node if the confidence difference at the outputs of initial node is below a certain _ divergence value _ , @xmath5 .",
    "in that case , the paths 1 and 2 of the tree are disabled .",
    "this is in accordance with the selective processing concept .",
    "later , in section v(a ) , it is shown that accuracy degradation with respect to baseline in the absence of the divergence module ( or the baseline node ) in the falcon tree is around 2 - 4% for most classification problems .",
    "thus , for applications where the slight degradation in accuracy is permissible , it is not required to append the baseline classifier to the falcon tree .",
    "falcon facilitates the reuse of nodes ( or classifiers ) from one classification tree to another when we want to incorporate additional classes with new feature information into a given task .",
    "consider a 6-object classification problem wherein 4 classes are the same as that of fig .",
    "3(b ) and the remaining two new classes ( e , f ) have a common feature g. there are two different ways of constructing the falcon tree for the given problem as shown in fig .",
    "4 . we have not shown the divergence module for the sake of convenience in representation .",
    "it is evident that the last nodes ( c , d , e ) which provide the final output of the classifier are the same in both fig .",
    "4(a ) , ( b ) .",
    "additionally , the nodes c , d , b are the same as that of nodes 1 , 2 , 3 ( fig .",
    "3(b ) ) , respectively . hence ,",
    "these nodes ( 1 , 2 , 3 ) from fig .",
    "3(b ) can be reused for the 6-object problem where learning the weights for these nodes is not required .",
    "falcon allows us to create reusable models ( trained for a particular classification problem ) and use the same for different classification problems .",
    "reusability is one of the major benefits that falcon provides over conventional algorithms . in the conventional approach",
    ", the nn has to be retrained whenever a new class or object is added to the classification problem . for instance , the baseline nn in fig .",
    "3(a ) needs to incorporate 6 neurons at the output layer in this case .",
    "as the networks are fully connected , the weights have to be learnt all over again to achieve a nominal accuracy . in a resource - constrained environment ,",
    "reusability with falcon would enable us to realize large - scale classification frameworks in hardware , addressing more challenging problems in an energy - efficient manner .",
    "node reusability thus provides the falcon methodology with the added advantage of scalability .",
    "* input : * training dataset with the target labels ( @xmath6 ) for each class ( @xmath7 ) , baseline classifier ( @xmath8 ) + * output : * falcon tree ( @xmath9 )    obtain the relevant features associated with each class / object ( @xmath7 ) in the dataset with the feature selection methodology described in section iii ( a ) .",
    "group the objects and the corresponding training labels ( @xmath6 ) with feature consensus under one label ( @xmath10 ) .",
    "the labels ( @xmath10 ) serve as training labels for the initial node .",
    "* initialize * count= # of labels ( @xmath10 ) obtained , @xmath11= # of classes ( @xmath7 ) grouped under @xmath10 train the initial node ( @xmath12 ) of the falcon tree based on the labels ( @xmath10 ) to classify the objects based on their features . # of output neurons in @xmath12= count . _",
    "the input vector at @xmath12 is the original rgb pixel values of the image . _ * initialize * # of final nodes ( @xmath13 ) in the tree = @xmath14 . * for * @xmath15 // _ for each node based on the feature concensus _",
    "train @xmath16 with target labels ( @xmath6 ) corresponding to classes with feature consensus .",
    "# of output neurons in @xmath16 = @xmath17 .",
    "_ the input vector at @xmath16 is the feature vector of the image . _ * end for * append @xmath8 as the last node to @xmath9 depending upon the accuracy requirement .",
    "please note that each node of the falcon tree is trained to achieve iso - accuracy with that of the baseline .",
    "_ please note that each node of the falcon tree is trained to achieve iso - accuracy with that of the baseline .",
    "_      there are different ways of constructing a falcon tree for a given classification task .",
    "however , we need to select the configuration that yields higher energy savings . referring to the 6-object classification problem described above , both configurations in fig .",
    "4 will yield computational savings with respect to the baseline nn as it invokes selective activation of various nodes in the tree .",
    "however , the configuration in fig .",
    "4(a ) ( _ config1 _ ) would yield less energy savings than that of fig .",
    "4(b ) ( _ config2 _ ) . to explain this ,",
    "let us consider the best - case scenario where all nodes of the tree have 100% accuracy in both configurations .",
    "_ config1 _ differs from _ config2 _ in the initial node classifier output .",
    "node _ a _ in _",
    "config1 _ identifies the objects with feature r or y and classifies them under one class and the ones with feature g under another class .",
    "on the other hand , node _",
    "a _ in _ config2 _ distinguishes objects into the 3 distinct feature classes .",
    "then , selecting path 1 in _ config1 _ would enable 3 nodes ( _ b , c , d _ ) in order to classify the objects that have r or y as common feature .",
    "in contrast , _",
    "config2 _ would only enable 2 nodes ( _ c , d _ ) .",
    "the activation of an additional node would result in an increase in computational effort for _",
    "config1_. thus , for a given classification problem , falcon tree with initial node for feature classification and final nodes for object - specific classification ( as in _ config2 _ ) would yield maximum benefits .",
    "the systematic methodology to construct the falcon tree is given in algorithm 1 .",
    "the process takes a pre - trained baseline classifier ( single nn , @xmath8 ) , its corresponding training dataset with the target labels ( @xmath6 ) as input , and produces a falcon tree ( @xmath9 ) as output .",
    "once the falcon tree is constructed , we input the test data to the tree to obtain accuracy and efficiency results .",
    "the overall testing methodology is shown in algorithm 2 . given a test instance @xmath18",
    ", the process obtains the class label @xmath19 for it using the falcon tree ( @xmath9 ) .",
    "the output from the initial node is monitored by the divergence module to decide if a path of the tree corresponding to a final node ( @xmath13 ) or the baseline classifier @xmath8 ) is to be activated .    *",
    "* input:**test instance @xmath18 , falcon tree ( @xmath9 ) * output : * class label . @xmath19    obtain the feature vectors for @xmath20 corresponding to the labels ( @xmath10 ) obtained for the initial node ( @xmath12 ) . obtain the output of @xmath12 and compute the difference between the maximum ( @xmath21 ) and minimum ( @xmath22 ) confidence values across all output neurons of @xmath12 . *",
    "if * @xmath23 - @xmath24 ( user - defined divergence value ) * then * enable baseline classifier ( @xmath8 ) .",
    "class label @xmath19= class label given by @xmath8 . //",
    "_ in case the divergence module ( or the baseline node ) is not present in the falcon , the falcon produces an error for the instance @xmath18 .",
    "class label @xmath19= not found and the classification process is terminated at the initial node without activating other nodes . _ * if * @xmath23 - @xmath25 * then * final node ( @xmath13 ) corresponding to the path activated by output neuron @xmath21 is enabled .",
    "class label @xmath19= class label given by final node ( * @xmath13 * ) .    in summary",
    ", the design methodology implicitly obtains the relevant features representative of the classes in the dataset and utilizes the feature consensus across classes to construct a multi - step classification tree .",
    "the divergence value @xmath5 can be adjusted during runtime to achieve the best tradeoff between accuracy and efficiency with falcon .",
    "we believe that the proposed approach is systematic and can be applied across all classification applications .",
    "in this section , we describe the proposed neuromorphic engine ( neue ) that provides a hardware framework to execute anns .",
    "neue is a specialized many - core architecture for energy efficient processing of falcon classification technique .",
    "neue delivers state - of - the art accuracy with energy efficiency by using the following two approaches : ( 1 ) hardware support for efficient data movement by spatial and temporal data reuse ( fifo , t - buffer ) to minimize the number of sram accesses ; ( 2 ) hardware support for data gating to prevent unwanted memory reads and `` multiply and accumulate '' ( mac ) operations thereby allowing input - aware data processing .",
    "additionally , the control unit supports selective path activation to enable falcon .",
    "5 shows the block diagram of the neue architecture with arrows depicting the logical dataflow between the constituent units .",
    "the sram memory stores the input data ( image pixel values and weights ) for the trained neural network .",
    "efficient data movement is achieved by buffering the input data - image data ( i m ) and weight data ( wt ) in fifos and temporary output traces ( t - trace ) in the t - buffer .",
    "image data and weight data are read from sram memory into the fifos and streamed into the array of neuron units ( nus ) .",
    "temporary output traces computed in nus are buffered into t - buffer instead of being written back into the sram and read from the buffer when needed by the nus for further processing .",
    "the nus compute the product between the image data and weight data and keep accumulating it until all the inputs for a particular neuron are processed .",
    "after this , the activation unit ( au ) processes the value in the nu and the output is returned to the sram .",
    "let s discuss the mapping of a generic neural network ( fully connected ) into neue .",
    "the neuron computations are done layer wise  read the inputs and weights from sram , compute all the outputs corresponding to the first layer , store back the outputs in sram and then proceed to the next layer . within a layer , neurons are temporally scheduled in the nus  the output computations for the first set of ` n ' neurons are done .",
    "then , the next set of ` n ' neurons from the same layer are scheduled in the nu and the process continues until all the neurons in the current layer have been evaluated .",
    "hence , we temporally map the different layers of the neural network and different neurons within a layer to compute the entire neural network for a given input data .",
    "thus , neue is a temporally scalable architecture capable of implementing all fully connected artificial neural networks .    the logical dataflow between different components of the neue is also shown in fig .",
    "n ' ( 16 in our case ) input data are read from the sram into the input fifo .",
    "each nu receives weights from its dedicated weight fifo . corresponding to the data in input fifo , ` n",
    "' weights are read from the sram into each nu with each nu corresponding to a neuron .",
    "the input fifo is flushed ( new set of ` n ' data read from and put in input fifo ) after all the computations for the first layer neurons is done .",
    "inputs are streamed from the input fifo into the nu array as all the neurons in a layer share the same inputs .",
    "once all the computations ( that can be done with the current data in input fifo ) for the first set of ` n ' neurons scheduled into the nu array is complete , the t - traces are stored into t - buffer .",
    "the t - trace will be read back into the nu when the input fifo gets flushed to read the new set of inputs .",
    "after , the t - trace has been written to the t - buffer , the next set of ` n ' neurons are scheduled into the nus , corresponding weights read from sram into their respective weight fifos and the logical flow continues as described .",
    "input fifo and t - buffer facilitate efficient data movement .",
    "data in input fifo is shared by all neurons scheduled in the nus that allows spatial reuse of input data .",
    "additionally , temporary output traces are stored in the t - buffer and hence allowing temporal reuse of the data in input fifo for successive set of ` n ' neurons in the same layer .",
    "the data in t - buffer is also temporally reused by nus which otherwise would be written back and fetched from the sram .",
    "the falcon algorithm decomposes a bigger neural network into smaller ones thereby allowing effective t - buffer utilization as the number of intervening trace storages before a trace buffer entry is reutilized for further accumulation are less , hence preventing them from being evicted before getting reutilized .",
    "efficient data movement translates to @xmath07 % energy saving on an average across all datasets . for larger networks that can not store all the t - traces in the t - buffer for a layer ,",
    "the t - trace is evicted and written to the sram memory .",
    "the control unit holds control registers which store information about the topology of the falcon tree i.e. connections and size of anns in it .",
    "it also has the selective - path activation unit ( sau ) .",
    "the sau keeps track of network execution , gathers the outputs and selectively activates the correct path based on the output from the previous stage .",
    "each nu is a multiply and accumulate ( mac ) unit .",
    "the nus are connected in a serial fashion to allow data streaming from input fifo to the rightmost nu .",
    "the au implements a piecewise linear approximation of the sigmoid function .",
    "once , the nus have finished the weighted summation of all inputs , the au streams in the data from the nus in a cyclical fashion and sends the output back to the nus as shown in fig .",
    "data gating is achieved by input aware weight fetching .",
    "the zero input checker disables the corresponding weight fetches for all the neurons in the layer being processed currently if the input pixel value is zero .",
    "this translates to energy saving by skipping weight reads from sram and corresponding multiply and accumulate computation in nus . on an average , data gating translates to significant savings across the datasets further decreasing the overall energy consuption",
    "in this section , we describe the experimental setup used to evaluate the performance of falcon approach .",
    "we note that our methodology is generic and can be applied to any give n - object classification task .",
    "it is apparent that images in all real - world datasets do share common features across classes which can be utilized to implement our design strategy . as an example , we have implemented a standard ann based 12-class image recognition platform for the caltech101 dataset @xcite and 10-class platform for cifar10 dataset @xcite .",
    "we have used these datasets as for our proposed methodology , the images need to be characterized with appropriate features .",
    "caltech101/cifar10 have good resolution colored images that can be characterized with color / texture . for caltech ,",
    "each image is roughly around 300x200 pixels that are scaled to 75x50 pixels for hardware implementation . for cifar10",
    ", we used the original resolution of 32x32 pixels for evaluation . for the 12-class caltech recognition ,",
    "first we built a 4-object/8-object classifier ( fig . 6 ( a , b ,",
    "c ) ) using the design methodology discussed in section iii(c ) .",
    "then , the nodes of the smaller classifiers were reused to construct a 12-object classifier as shown in fig . 6 ( d ) .",
    "each node / classifier in the falcon tree is trained using stochastic gradient descent with backpropagation @xcite .",
    "for ease of representation , the divergence module with the baseline classifier for each falcon configuration is not shown .",
    "we can see that the initial node for each configuration is trained for different feature classes ( color : fig .",
    "6 ( a , b ) and texture : fig . 6 ( c ) ) as deemed optimum by the feature selection methodology .",
    "r , y , w , b are the broad color features that were obtained for classes ( a - h ) while g1 , g3 are the texture features for classes ( i - l ) .",
    "please note that the nodes that were reused to build the larger classifiers ( _ config rywb , config 12-class _ ) did not have to be retrained at all .",
    "the falcon shown in fig . 6 ( d ) reuses the nodes in fig . 6 ( b , c ) and has two initial nodes ( x1 , x2 ) . during the test phase for falcon in fig .",
    "6 ( d ) , the input image is fed to both x1 , x2 and the output neuron with the maximum confidence across x1 , x2 is used to select the corresponding path to the final node . in case of the 10-class image recognition for cifar10",
    ", we applied the same procedure as caltech where we built 6-object/4-object falcon classifier configurations and reused their nodes to build the 10-object falcon model as shown in fig .",
    "6 ( e ) . for convenience in representation , we have not shown the modular representation of the smaller falcon configurations for cifar10 .    for hardware implementation",
    ", we implemented the neue at the register - transfer - level ( rtl ) and mapped to the ibm 45 nm technology using synopsys design compiler .",
    "we used synopsys power compiler to estimate energy consumption of the implementation .",
    "the key micro - architectural parameters and implementation metrics for the core of the neue are shown in fig .",
    "7 . each of the configurations in fig . 6 for caltech101 and cifar10",
    "were ported manually to the neue platform and the baseline ( corresponding single nn classifier for each falcon config in fig .",
    "6 ) was well optimized for energy .",
    "the neue operates at 1ghz core clock resulting in an average total power consumption of 72.68 mw across the 12-class caltech/10-class cifar recognition implementations .",
    "the execution core and the memory consume 78.92% and 21.07% of the total power , respectively .",
    "to minimize leakage power and better optimize the energy of baseline classifiers for fare comparison with falcon , we used a supply voltage of 0.8v for memory and that of 1v for execution core operation in the neue .",
    "for runtime analysis , we implemented each of the configurations of fig .",
    "6 in matlab and measured runtime for the applications using performance counters on intel core i7 3.60 ghz processor with 16 gb ram .",
    "please note that the software baseline implementation was aggressively optimized for performance .",
    "in this section , we present the experimental results that demonstrate the benefits of our approach .",
    "we use caltech101 as our primary benchmark to evaluate the benefits with selective classification .",
    "( a ) shows the improvement in efficiency with respect to the traditional single nn classifier ( which forms the baseline ) for each configuration of fig . 6 ( a - d ) with and without the divergence module for caltech101 .",
    "we quantify efficiency in terms of two metrics : ( i ) the average number of operations ( or mac computations ) per input ( ops ) , ( ii ) energy of hardware implementation on neue .",
    "the ops and energy of each falcon _ config _ is normalized to a neue implementation of the corresponding baseline classifier . note that this is already a highly optimized baseline since the neue architecture is customized to the characteristics of anns .",
    "we observe that while our proposed falcon approach yields 1.51x-5.97x ( average : 3.74x ) improvement in average ops / input compared to the baseline in the case without divergence , the benefits are slightly lower 1.24x-4.59x ( average : 2.92x ) with divergence .",
    "this is obvious because the baseline classifier is not present as a final node in the falcon tree in the case without divergence .",
    "it is clearly seen in fig .",
    "8 ( a ) that the benefits observed increases by almost 1.5x each time we scale up from a 4-object classification ( _ config ry , gabor _ ) to an 8- object ( _ config rywb _ )",
    "/12-object ( _ config 12-class _ ) problem .",
    "this can be attributed to the fact that the complexity of the baseline classifier increases substantially in order to get a reasonable classification accuracy for a given n - object classification problem .",
    "in contrast , falcon invokes multi - step classification based on feature information in the input data .",
    "thus , the decomposition of the classification problem into simpler tasks allows us to use a cluster of less complex nodes ( with lower dimensional feature vector as input to final nodes ) that combined with selective activation yields larger benefits .",
    "additionally , the reuse of nodes contributes further to the increased benefits while scaling up from small to larger classification problems . please note that the benefits shown include the additional cost of hsv and gabor filtering for the falcon implementation . in case of hardware execution on neue , the energy improvements obtained are 3.66x/5.91x for the 12-object classification with / without divergence respectively as illustrated in fig .",
    "similarly , fig . 8 ( b ) shows the normalized benefits ( ops and energy ) observed for the falcon implementation of cifar10 with the three configurations from fig . 6 ( e ) . on an average",
    ", falcon achieves 3.05x/4.55x improvement in energy and 3.82x/4.26x improvement in ops with _",
    "config 10-class _ ( fig . 6 ( e ) ) for 10-object classification .",
    "we also show the fraction of total energy savings observed in the hardware platform neue due to other standard architectural design techniques besides selective activation for each of the datasets ( caltech101 , cifar10 ) in fig .",
    "it is clearly seen that while data gating and data movement techniques provide @xmath020% of the total savings in each case , the majority of savings is observed due to falcon methodology that invokes selective activation .",
    "a noteworthy obsevation here is that data gating / movement provides more benefits for caltech101 than cifar10 .",
    "this can be attributed to the fact that input size dimensions for caltech101 ( 75x50 ) is greater than cifar10 ( 32x32 ) that results in more near - zero pixels for the former and thus more data gating . also , in caltech101 ( fig . 6 ( d ) )",
    "the number of decomposed classifiers obtained from falcon is greater than that of cifar10 ( fig .",
    "the t - buffer reutilization is more in the former case resulting in larger % of savings due to efficient data movement than the latter .",
    "9 shows the normalized accuracy of each configuration in fig .",
    "6 ( a - d ) for caltech101 with / without the divergence module with respect to the corresponding baseline classifier .",
    "the accuracies of the falcon _",
    "configs _ are normalized with respect to the corresponding baseline .",
    "for example , the accuracy of the baseline for the 12-class problem is 94.2% that is set to 1 and the corresponding falcon ( _ config 12-class _ ) is normalized against it .",
    "it is evident that while the configuration with divergence module yields iso - accuracy as that of the baseline , the absence of the module results in a decline in accuracy by 1.7%-3.9% .",
    "for cifar10 , the falcon _",
    "config 10-class _ yields a 2.8% accuracy decline without the divergence module with respect to the baseline for the 10-class recognition problem . as discussed in section iii ( b.2 ) , this degradation is due to the errors given out at the initial node for those test instances that have more than one feature as representative information .",
    "however , for hardware implementations where energy - efficiency is crucial , 2 - 4% decline in accuracy may be permissible .",
    "one of the big challenges in machine learning is the time needed to train neural networks to obtain a reasonable accuracy for large classification tasks .",
    "in fact , the software based implementation of large scale problems require accelerators like gpus that use cuda for faster and high performance neural network training @xcite .",
    "since feature based classification enables the nodes in the falcon tree to be trained for simpler tasks , we can conjecture that there should be reduction in training time with falcon .",
    "for example , referring to fig . 6 ( b ) , @xmath26 is originally an 8-object classifier decomposed into a 4-object ( initial node @xmath27 ) and cluster of small 2-object classifiers ( node @xmath28 ) .",
    "hence , these nodes will converge to the global error minima much faster than the baseline classifier .",
    "however , it is understood from the design methodology that prior to constructing the falcon tree , the feature selection methodology has to be invoked to obtain the appropriate feature vectors .",
    "this would add extra overhead on the training time .",
    "10 illustrates the normalized training time observed for each configuration of fig .",
    "6 ( without the divergence module ) with respect to the baseline .",
    "the additional overhead of feature selection is more pronounced for smaller tasks ( @xmath29 ) due to which the time for training the falcon in these cases is slightly more ( 1.17x/1.13x ) than that of the baseline .",
    "however , as we scale to larger problems , we observe that there is a significant improvement ( 1.14x for config rywb/ 1.96x for config 12-class ) in training time with falcon even when node reusability is not taken into account .",
    "this is because the baseline classifier becomes increasingly complex and difficult to train for complex tasks .",
    "in contrast , falcon in spite of the overhead trains easily because of problem decomposition .",
    "now , if we take into account node reusability , then , scaling up the problem from @xmath30 ( 4-object ) to @xmath26 ( 8-object ) does nt require training of the nodes @xmath31 and @xmath32 .",
    "thus , reuse of nodes will cause the training time to further reduce that is evident in fig . 10 .",
    "since the 12-object falcon ( @xmath33 ) is built reusing the nodes from @xmath26 and @xmath34 , it should ideally require no extra training time that is seen from fig .",
    "it is very evident that with falcon , the classifier architecture is optimized such that it can be easily mapped to gpu/ cuda framework , in software simulations , giving ultra - high performance on enormous datasets .",
    "this shows the effectiveness of falcon .",
    "the divergence module discussed in section iii ( b.2 ) enables the baseline node in the falcon tree depending upon the divergence value , @xmath5 , set by the user .",
    "11 shows the variation in normalized energy ( with respect to baseline ) and the accuracy for the falcon ( _ config ry _ in fig .",
    "6(a ) ) with different @xmath5 . setting @xmath5 to a low value implies that the baseline node will be activated few times and more inputs will be passed to the final nodes ( node _ r , y _ : fig .",
    "6 ( a ) ) for classification .",
    "thus , initially we observe more reduction in energy as compared to the baseline .    however , in such cases , the difference between the confidences at the output neurons of the initial node ( node _ y1 _ ) is also low . there is a high probability that the initial node does not activate the final nodes accurately i.e. it wrongly activates the path to final node",
    "_ r _ when the test instance originally should be classified by node _",
    "y_. thus , we see that the accuracy of the falcon is lower than that of the baseline .",
    "increasing @xmath5 improves accuracy at the cost of increase in energy as the baseline is enabled more now .",
    "however , beyond a particular @xmath5 , the falcon achieves iso - accuracy with that baseline .",
    "this value of @xmath5 corresponds to the maximum efficiency that can be achieved for the given falcon configuration . in fig .",
    "11 , we observe that iso - accuracy is attained for @xmath5 = 0.7",
    ". the energy would still continue to increase beyond this point .",
    "so , we can regulate @xmath5 during runtime to trade accuracy for efficiency .",
    "till now , we have discussed reusing nodes from smaller classification tasks to scale up to larger problems when the new classes have different feature information ( like _ config ry _ to _ config rywb _ in fig .",
    "6 required incorporating classes with features white , black ) .",
    "consider a case where we need to extend the config ry in fig .",
    "6 ( a ) to incorporate new classes that have red as a representative feature . in this case",
    ", we need to retrain node _ y1 _",
    "6 ( a ) ) with the additional classes and also modify the final node corresponding to the path activated by r. hence , we have two options as shown in fig",
    ". 12 ( b ) : i ) retrain the final node r with new classes ( config retrain ) and ii ) add a new node ( node _ r _ ) to the path ( _ config new _ ) .",
    "however , the option that gives the maximum benefits depends on the number of new classes to be added .",
    "12 ( a ) shows the normalized ops ( that also quantifies efficiency ) for both options as the number of new classes ( to be added ) is increased .",
    "it is evident that both _",
    "config new _ and _ retrain _ will have higher # ops than the initial _ config ry _ ( which forms the baseline here ) due to the presence of new classes . in option ( i ) ,",
    "addition of a new node implies that both nodes ( _ r , r _ ) have to be activated to obtain the final classification result .",
    "in contrast , with option ( ii ) , only the retrained node _ r _ needs to be enabled .",
    "thus , as long as the complexity of retrained node _",
    "r _ in _ config retrain _ is less than the combined complexity of node _",
    "r _ and _ r _ in _ config new _ , option ( i ) yields more computational benefits .",
    "thus , initially we observe higher # ops with _",
    "config new_. however , as we increase the number of new classes , the complexity of retrained node _",
    "r _ also increases in order to maintain competitive classification accuracy . at some point",
    ", this complexity would overcome the cost penalty that activating two nodes ( _ r , r _ ) imposes . beyond this point , option ( ii ) yields more benefits . in fig",
    "12 ( b ) , for # of new classes > 2 , adding new nodes is preferred .",
    "this behavior is taken into account while constructing the falcon tree to get maximum savings .",
    "a similar analysis was done to construct _ config rywb _",
    "( b ) ) with a single initial node ( _ x1 _ ) as opposed to multiple initial nodes .",
    "_ config 12-class _ ( fig .",
    "6 ( d ) ) also has two initial nodes _ x1 , x2 _ due to the given analysis .",
    "in this paper , we propose falcon : feature driven selective classification , based on the biological visual recognition process , for energy - efficient realization of neural networks for multi - object classification .",
    "we utilize the feature similarity ( or concensus ) across multiple classes of images in a real - world dataset to break down the classification problem into simpler tasks organized in a tree - fashion .",
    "we developed a systematic methodology to select the appropriate features ( color and texture for images ) and construct the falcon tree for a given n - object classification task .",
    "the structure of falcon provides us with a significant advantage of reusing tree nodes from smaller classification tasks to implement large - scale problems thereby contributing to the reduction in training time as we scale to larger tasks .",
    "falcon invokes selective activation of only those nodes and branches relevant to a particular input , while keeping remaining nodes idle , resulting in an energy - efficient classification process .",
    "it is noteworthy to mention that the current falcon methdology employs a feature selction process that clusters classes at the root node based on a single feature similar to a group of classes .",
    "thus , we add the divergence module ( or baseline classifier as an extra node ) to maintain the accuracy of the falcon tree for those classes that have more than one feature in common .",
    "for such cases ( with divergence module ) , we observe lesser energy benefits .",
    "a feature selection algorithm that searches for more distinctive features similar across classes will prevent the use of the divergence module , yielding higher energy savings while maintaining iso - accuracy with that of baseline .",
    "hence , further research can be done to improvise the feature selection process .",
    "this work was supported in part by c - spin , one of the six centers of starnet , a semiconductor research corporation program , sponsored by marco and darpa , by the semiconductor research corporation , the national science foundation , intel corporation and by the national security science and engineering faculty fellowship .",
    "s.  g. ramasubramanian , r.  venkatesan , m.  sharad , k.  roy , and a.  raghunathan , `` spindle : spintronic deep learning engine for large - scale neuromorphic computing , '' in _ proceedings of the 2014 international symposium on low power electronics and design_.1em plus 0.5em minus 0.4emacm , 2014 , pp . 1520 .",
    "y.  sun , x.  wang , and x.  tang , `` deep convolutional network cascade for facial point detection , '' in _ proceedings of the ieee conference on computer vision and pattern recognition _",
    ", 2013 , pp . 34763483 .",
    "a.  w. smeulders , m.  worring , s.  santini , a.  gupta , and r.  jain , `` content - based image retrieval at the end of the early years , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "22 , no .  12 , pp . 13491380 , 2000 .",
    "s.  venkataramani , s.  t. chakradhar , k.  roy , and a.  raghunathan , `` approximate computing and the quest for computing efficiency , '' in _ proceedings of the 52nd annual design automation conference_.1em plus 0.5em minus 0.4emacm , 2015 , p. 120 .",
    "s.  venkataramani , a.  ranjan , k.  roy , and a.  raghunathan , `` axnn : energy - efficient neuromorphic systems using approximate computing , '' in _ proceedings of the 2014 international symposium on low power electronics and design_.1em plus 0.5em minus 0.4emacm , 2014 , pp .",
    "2732 .",
    "p.  panda , a.  sengupta , and k.  roy , `` conditional deep learning for energy - efficient and enhanced pattern recognition , '' in _ 2016 design , automation & test in europe conference & exhibition ( date)_.1em plus 0.5em minus 0.4emieee , 2016 , pp .",
    "475480 .",
    "s.  chakradhar , m.  sankaradas , v.  jakkula , and s.  cadambi , `` a dynamically configurable coprocessor for convolutional neural networks , '' in _ acm sigarch computer architecture news _ , vol .",
    "38 , no .  3.1em plus 0.5em minus 0.4emacm , 2010 , pp .",
    "247257 .",
    "chen , t.  krishna , j.  emer , and v.  sze , `` 14.5 eyeriss : an energy - efficient reconfigurable accelerator for deep convolutional neural networks , '' in _ 2016 ieee international solid - state circuits conference ( isscc)_.1em plus 0.5em minus 0.4emieee , 2016 , pp .",
    "262263 .",
    "j.  ngiam , a.  coates , a.  lahiri , b.  prochnow , q.  v. le , and a.  y. ng , `` on optimization methods for deep learning , '' in _ proceedings of the 28th international conference on machine learning ( icml-11 ) _ , 2011 , pp . 265272 .",
    "b.  rajendran , y.  liu , j .- s .",
    "seo , k.  gopalakrishnan , l.  chang , d.  j. friedman , and m.  b. ritter , `` specifications of nanoscale devices and circuits for neuromorphic computational systems , '' _ ieee transactions on electron devices _",
    ", vol .  60 , no .  1 , pp . 246253 , 2013 .",
    "k.  roy , m.  sharad , d.  fan , and k.  yogendra , `` beyond charge - based computation : boolean and non - boolean computing with spin torque devices , '' in _ low power electronics and design ( islped ) , 2013 ieee international symposium on_.1em plus 0.5em minus 0.4emieee , 2013 , pp . 139142 .",
    "chen , y .- l . chen , and s .- y .",
    "chien , `` fast image segmentation based on k - means clustering with histograms in hsv color space , '' in _ multimedia signal processing , 2008 ieee 10th workshop on_.1em plus 0.5em minus 0.4emieee , 2008 , pp .",
    "322325 .",
    "a.  bernardino and j.  santos - victor , `` a real - time gabor primal sketch for visual attention , '' in _ iberian conference on pattern recognition and image analysis_.1em plus 0.5em minus 0.4emspringer , 2005 , pp .",
    "335342 .",
    "m.  haghighat , s.  zonouz , and m.  abdel - mottaleb , `` identification using encrypted biometrics , '' in _ international conference on computer analysis of images and patterns_.1em plus 0.5em minus 0.4em springer , 2013 , pp . 440448 .",
    "l.  fei - fei , r.  fergus , and p.  perona , `` learning generative visual models from few training examples : an incremental bayesian approach tested on 101 object categories , '' _ computer vision and image understanding _ , vol .",
    "106 , no .  1 ,",
    "pp . 5970 , 2007 ."
  ],
  "abstract_text": [
    "<S> machine - learning algorithms have shown outstanding image recognition / classification performance for computer vision applications . </S>",
    "<S> however , the compute and energy requirement for implementing such classifier models for large - scale problems is quite high . in this paper , we propose eture driven seective lassificati ( falcon ) inspired by the biological visual attention mechanism in the brain to optimize the energy - efficiency of machine - learning classifiers . </S>",
    "<S> we use the consensus in the characteristic features ( color / texture ) across images in a dataset to decompose the original classification problem and construct a tree of classifiers ( nodes ) with a generic - to - specific transition in the classification hierarchy . </S>",
    "<S> the initial nodes of the tree separate the instances based on feature information and selectively enable the latter nodes to perform object specific classification . </S>",
    "<S> the proposed methodology allows selective activation of only those branches and nodes of the classification tree that are relevant to the input while keeping the remaining nodes idle . </S>",
    "<S> additionally , we propose a programmable and scalable neuromorphic engine ( neue ) that utilizes arrays of specialized neural computational elements to execute the falcon based classifier models for diverse datasets . the structure of falcon facilitates the reuse of nodes while scaling up from small classification problems to larger ones thus allowing us to construct classifier implementations that are significantly more efficient . </S>",
    "<S> we evaluate our approach for a 12-object classification task on the caltech101 dataset and 10-object task on cifar-10 dataset by constructing falcon models on the neue platform in 45 nm technology . </S>",
    "<S> our results demonstrate up to 3.66x improvement in energy - efficiency for no loss in output quality , and even higher improvements of up to 5.91x with 3.9% accuracy loss compared to an optimised baseline network . </S>",
    "<S> in addition , falcon shows an improvement in training time of up to 1.96x as compared to the traditional classification approach .    </S>",
    "<S> shell : bare demo of ieeetran.cls for ieee journals    machine learning classifiers , feature selective classification , energy - efficiency , neuromorphic architecture </S>"
  ]
}