{
  "article_text": [
    "principal component analysis ( pca ) is among the most popular tools in machine learning , statistics , and data analysis more generally .",
    "pca is the basis of many techniques in data mining and information retrieval , including the latent semantic analysis of large databases of text and html documents described in  @xcite . in this paper , we compute pcas of very large data sets via a randomized version of the block lanczos method , summarized in section  [ summary ] below .",
    "the proofs in  @xcite and  @xcite show that this method requires only a couple of iterations to produce nearly optimal accuracy , with overwhelmingly high probability ( the probability is independent of the data being analyzed , and is typically @xmath0 or greater ) .",
    "the randomized algorithm has many advantages , as shown in  @xcite and  @xcite ; the present article adapts the algorithm for use with data sets that are too large to be stored in the random - access memory ( ram ) of a typical computer system .    computing a pca of a data set amounts to constructing a singular value decomposition ( svd ) that accurately approximates the matrix @xmath1 containing the data being analyzed ( possibly after suitably `` normalizing '' @xmath1 , say by subtracting from each column its mean ) .",
    "that is , if @xmath1 is @xmath2 , then we must find a positive integer @xmath3 and construct matrices @xmath4 , @xmath5 , and @xmath6 such that @xmath7 with @xmath4 being an @xmath8 matrix whose columns are orthonormal , @xmath6 being an @xmath9 matrix whose columns are orthonormal , and @xmath5 being a diagonal @xmath10 matrix whose entries are all nonnegative .",
    "the algorithm summarized in section  [ summary ] below is most efficient when @xmath11 is substantially less than @xmath12 ; in typical real - world applications , @xmath13 .",
    "most often , the relevant measure of the quality of the approximation in  ( [ svd_approx ] ) is the spectral norm of the discrepancy @xmath14 ; see , for example , section  [ summary ] below .",
    "the present article focuses on the spectral norm , though our methods produce similar accuracy in the frobenius / hilbert - schmidt norm ( see , for example ,  @xcite ) .",
    "the procedure of the present article works to minimize the total number of times that the algorithm has to access each entry of the matrix @xmath1 being approximated .",
    "a related strategy is to minimize the total number of disk seeks and to maximize the dimensions of the approximation that can be constructed with a given amount of ram ; the algorithm in  @xcite takes this latter approach .    in the present paper ,",
    "the entries of all matrices are real valued ; our techniques extend trivially to matrices whose entries are complex valued .",
    "the remainder of the article has the following structure : section  [ informal ] explains the motivation behind the algorithm .",
    "section  [ summary ] outlines the algorithm .",
    "section  [ outs ] details the implementation for very large matrices .",
    "section  [ ccosts ] quantifies the main factors influencing the running - time of the algorithm .",
    "section  [ numexs ] illustrates the performance of the algorithm via several numerical examples .",
    "section  [ application ] applies the algorithm to a data set of interest in biochemical imaging .",
    "section  [ conclusion ] draws some conclusions and proposes directions for further research .",
    "in this section , we provide a brief , heuristic description .",
    "section  [ summary ] below provides more details on the algorithm described intuitively in the present section .",
    "suppose that @xmath11 , @xmath15 , and @xmath16 are positive integers with @xmath17 and @xmath18 , and @xmath1 is a real @xmath2 matrix .",
    "we will construct an approximation to @xmath1 such that @xmath19 where @xmath4 is a real @xmath8 matrix whose columns are orthonormal , @xmath6 is a real @xmath9 matrix whose columns are orthonormal , @xmath5 is a diagonal real @xmath10 matrix whose entries are all nonnegative , @xmath20 is the spectral ( @xmath21-operator ) norm of @xmath14 , and @xmath22 is the @xmath23st greatest singular value of @xmath1 . to do so",
    ", we select nonnegative integers @xmath24 and @xmath25 such that @xmath26 and @xmath27 ( for most applications , @xmath28 and @xmath29 is sufficient ; @xmath20 will decrease as @xmath24 and @xmath25 increase ) , and then identify an orthonormal basis for `` most '' of the range of @xmath1 via the following two steps :    1 .   using a random number generator ,",
    "form a real @xmath30 matrix @xmath31 whose entries are independent and identically distributed gaussian random variables of zero mean and unit variance , and compute the @xmath32 matrix @xmath33 2 .   using a pivoted @xmath34-decomposition ,",
    "form a real @xmath32 matrix @xmath35 whose columns are orthonormal , such that there exists a real @xmath36 matrix @xmath37 for which @xmath38 ( see , for example , chapter  5 in  @xcite for details concerning the construction of such a matrix @xmath35 . )    intuitively , the columns of @xmath35 in  ( [ good_approx20 ] ) constitute an orthonormal basis for most of the range of @xmath1 . moreover , the somewhat simplified algorithm with @xmath39 is sufficient except when the singular values of @xmath1 decay slowly ; see , for example , @xcite .",
    "notice that @xmath35 may have many fewer columns than @xmath1 , that is , @xmath11 may be substantially less than @xmath16 ( this is the case for most applications of principal component analysis ) .",
    "this is the key to the efficiency of the algorithm .",
    "having identified a good approximation to the range of @xmath1 , we perform some simple linear algebraic manipulations in order to obtain a good approximation to @xmath1 , via the following four steps :    1 .",
    "compute the @xmath40 product matrix @xmath41 2 .",
    "form an svd of @xmath42 , @xmath43 where @xmath44 is a real @xmath40 matrix whose columns are orthonormal , @xmath45 is a real @xmath36 matrix whose columns are orthonormal , and @xmath46 is a real diagonal @xmath36 matrix such that @xmath47 .",
    "( see , for example , chapter  8 in  @xcite for details concerning the construction of such an svd . )",
    "compute the @xmath32 product matrix @xmath48 4 .   retrieve the leftmost @xmath8 block @xmath4 of @xmath49 , the leftmost @xmath9 block @xmath6 of @xmath44 , and the leftmost uppermost @xmath10 block @xmath5 of @xmath46 .",
    "the matrices @xmath4 , @xmath5 , and @xmath6 obtained via steps  16 above satisfy  ( [ sort_of_svd0 ] ) ; in fact , they satisfy the more detailed bound  ( [ sort_of_svd ] ) described below .",
    "in this section , we will construct a low - rank ( say , rank @xmath11 ) approximation @xmath50 to any given real matrix @xmath1 , such that @xmath51 with high probability ( independent of @xmath1 ) , where @xmath15 and @xmath16 are the dimensions of the given @xmath2 matrix @xmath1 , @xmath4 is a real @xmath8 matrix whose columns are orthonormal , @xmath6 is a real @xmath9 matrix whose columns are orthonormal , @xmath5 is a real diagonal @xmath10 matrix whose entries are all nonnegative , @xmath22 is the @xmath23st greatest singular value of @xmath1 , and @xmath52 is a constant determining the probability of failure ( the probability of failure is small when @xmath53 , negligible when @xmath54 ) . in  ( [ sort_of_svd ] ) , @xmath24 is any nonnegative integer such that @xmath27 ( for most applications , @xmath55 or @xmath56 is sufficient ; the algorithm becomes less efficient as @xmath24 increases ) , and @xmath20 is the spectral ( @xmath21-operator ) norm of @xmath14 , that is , @xmath57 @xmath58 to simplify the presentation , we will assume that @xmath59 ( if @xmath60 , then the user can apply the algorithm to @xmath61 ) . in this section ,",
    "we summarize the algorithm ; see  @xcite and  @xcite for an in - depth discussion , including proofs of more detailed variants of  ( [ sort_of_svd ] ) .",
    "the minimal value of the spectral norm @xmath62 , minimized over all rank-@xmath11 matrices @xmath63 , is @xmath22 ( see , for example , theorem  2.5.3 in  @xcite ) .",
    "hence , ( [ sort_of_svd ] ) guarantees that the algorithm summarized below produces approximations of nearly optimal accuracy .    to construct a rank-@xmath11 approximation to @xmath1",
    ", we could apply @xmath1 to about @xmath11 random vectors , in order to identify the part of its range corresponding to the larger singular values . to help suppress the smaller singular values , we apply @xmath64 , too .",
    "once we have identified `` most '' of the range of @xmath1 , we perform some linear - algebraic manipulations in order to recover an approximation satisfying  ( [ sort_of_svd ] ) .",
    "a numerically stable realization of the scheme outlined in the preceding paragraph is the following .",
    "we choose an integer @xmath26 such that @xmath65 ( it is generally sufficient to choose @xmath28 ; increasing @xmath25 can improve the accuracy marginally , but increases computational costs ) , and make the following six steps :    1 .   using a random number generator , form a real @xmath30 matrix @xmath31 whose entries are independent and identically distributed gaussian random variables of zero mean and unit variance , and compute the @xmath66 matrices @xmath67 , @xmath68 , ",
    "@xmath69 , @xmath70 defined via the formulae @xmath71",
    "@xmath72 @xmath73 @xmath74 @xmath75 form the @xmath32 matrix @xmath76 2 .   using a pivoted @xmath34-decomposition ,",
    "form a real @xmath32 matrix @xmath35 whose columns are orthonormal , such that there exists a real @xmath36 matrix @xmath37 for which @xmath77 ( see , for example , chapter  5 in  @xcite for details concerning the construction of such a matrix @xmath35 . ) 3 .",
    "compute the @xmath40 product matrix @xmath78 4 .",
    "form an svd of @xmath42 , @xmath79 where @xmath44 is a real @xmath40 matrix whose columns are orthonormal , @xmath45 is a real @xmath36 matrix whose columns are orthonormal , and @xmath46 is a real diagonal @xmath36 matrix such that @xmath47 .",
    "( see , for example , chapter  8 in  @xcite for details concerning the construction of such an svd . )",
    "compute the @xmath32 product matrix @xmath80 6 .",
    "retrieve the leftmost @xmath8 block @xmath4 of @xmath49 , the leftmost @xmath9 block @xmath6 of @xmath44 , and the leftmost uppermost @xmath10 block @xmath5 of @xmath46 .",
    "the product @xmath50 then approximates @xmath1 as in  ( [ sort_of_svd ] ) ( we omit the proof ; see  @xcite for proofs of similar , more general bounds ) .    in the present paper ,",
    "we assume that the user specifies the rank @xmath11 of the approximation @xmath50 being constructed .",
    "see  @xcite for techniques for determining the rank @xmath11 adaptively , such that the accuracy @xmath20 satisfying  ( [ sort_of_svd ] ) also meets a user - specified threshold .",
    "variants of the fast fourier transform ( fft ) permit additional accelerations ; see  @xcite , @xcite , and  @xcite . however , these accelerations have negligible effect on the algorithm running out - of - core . for out - of - core computations , the simpler techniques of the present paper are preferable .",
    "the algorithm described in the present section can underflow or overflow when the range of the floating - point exponent is inadequate for representing simultaneously both the spectral norm @xmath81 and its @xmath82st power @xmath83 .",
    "a convenient alternative is the algorithm described in  @xcite ; another solution is to process @xmath84 rather than @xmath1 .",
    "with suitably large matrices , some steps in section  [ summary ] above require either storage on disk , or on - the - fly computations obviating the need for storing all the entries of the @xmath2 matrix @xmath1 being approximated . conveniently , steps  2 , 4 , 5 , and 6 involve only matrices having @xmath85 entries ; we perform these steps using only storage in random - access memory ( ram ) .",
    "however , steps  1 and  3 involve @xmath1 , which has @xmath86 entries ; we perform steps  1 and  3 differently depending on how @xmath1 is provided , as detailed below in subsections  [ on - the - fly ] and  [ on - disk ] .      if @xmath1 does not fit in memory , but we have access to a computational routine that can evaluate each entry ( or row or column ) of @xmath1 individually , then obviously we can perform steps  1 and  3 using only storage in ram . every time we evaluate an entry ( or row or column ) of @xmath1 in order to compute part of a matrix product involving @xmath1 or @xmath61",
    ", we immediately perform all computations associated with this particular entry ( or row or column ) that contribute to the matrix product .      if @xmath1 does not fit in memory , but is provided as a file on disk , then steps  1 and  3 require access to the disk . we assume for definiteness that @xmath1 is provided in row - major format on disk ( if @xmath1 is provided in column - major format , then we apply the algorithm to @xmath61 instead ) . to construct the matrix product in  ( [ first_prod ] ) , we retrieve as many rows of @xmath1 from disk as will fit in memory , form their inner products with the appropriate columns of @xmath31 , store the results in @xmath67 , and then repeat with the remaining rows of @xmath1 . to construct the matrix product in  ( [ product_t3 ] ) , we initialize all entries of @xmath42 to zeros , retrieve as many rows of @xmath1 from disk as will fit in memory , add to @xmath42 the transposes of these rows , weighted by the appropriate entries of @xmath35 , and then repeat with the remaining rows of @xmath1 .",
    "we construct the matrix product in  ( [ second_prod ] ) similarly , forming @xmath87 first , and @xmath88 second . constructing the matrix products in  ( [ third_prod])([last_prod ] )",
    "is analogous .",
    "in this section , we tabulate the computational costs of the algorithm described in section  [ summary ] , for the particular out - of - core implementations described in subsections  [ on - the - fly ] and  [ on - disk ] .",
    "we will be using the notation from section  [ summary ] , including the integers @xmath24 , @xmath11 , @xmath25 , @xmath15 , and @xmath16 , and the @xmath2 matrix @xmath1 .    for most applications , @xmath29 suffices . in contrast , the classical lanczos algorithm generally requires many iterations in order to yield adequate accuracy , making the computational costs of the classical algorithm prohibitive for out - of - core ( or parallel ) computations ( see , for example , chapter  9 in  @xcite ) .",
    "we denote by @xmath89 the number of floating - point operations ( flops ) required to evaluate all nonzero entries in @xmath1 .",
    "we denote by @xmath90 the number of nonzero entries in @xmath1 . with on - the - fly evaluation of the entries of @xmath1 ,",
    "the six steps of the algorithm described in section  [ summary ] have the following costs :    1 .",
    "forming  @xmath67 in  ( [ first_prod ] ) costs @xmath91 flops .",
    "forming any of the matrix products in  ( [ second_prod])([last_prod ] ) costs @xmath92 flops .",
    "forming  @xmath93 in  ( [ product23 ] ) costs @xmath94 flops .",
    "all together , step  1 costs @xmath95 flops .",
    "2 .   forming  @xmath35 in  ( [ good_approx23 ] ) costs @xmath96 flops .",
    "3 .   forming  @xmath42 in  ( [ product_t3 ] ) costs @xmath97 flops .",
    "4 .   forming the svd of @xmath42 in  ( [ svd_small3 ] )",
    "costs @xmath98 flops .",
    "5 .   forming @xmath49 in  ( [ product33 ] )",
    "costs @xmath96 flops .",
    "forming @xmath4 , @xmath5 , and @xmath6 in step  6 costs @xmath99 flops .",
    "summing up the costs for the six steps above , and using the fact that @xmath100 , we see that the full algorithm requires @xmath101 flops , where @xmath89 is the number of flops required to evaluate all nonzero entries in @xmath1 , and @xmath90 is the number of nonzero entries in @xmath1 . in practice , we choose @xmath102 ( usually a good choice is @xmath28 ) .",
    "we denote by @xmath103 the number of floating - point words of random - access memory ( ram ) available to the algorithm . with @xmath1",
    "stored on disk , the six steps of the algorithm described in section  [ summary ] have the following costs ( assuming for convenience that @xmath104 ) :    1 .   forming  @xmath67 in  ( [ first_prod ] ) requires at most @xmath105 floating - point operations ( flops ) , @xmath106 disk accesses / seeks , and a total data transfer of @xmath107 floating - point words . forming any of the matrix products in  ( [ second_prod])([last_prod ] ) also requires @xmath105 flops , @xmath106 disk accesses / seeks , and a total data transfer of @xmath107 floating - point words .",
    "forming  @xmath93 in  ( [ product23 ] ) costs @xmath94 flops .",
    "all together , step  1 requires @xmath108 flops , @xmath109 disk accesses / seeks , and a total data transfer of @xmath110 floating - point words .",
    "2 .   forming  @xmath35 in  ( [ good_approx23 ] ) costs @xmath96 flops .",
    "3 .   forming  @xmath42 in  ( [ product_t3 ] ) requires @xmath108 floating - point operations , @xmath106 disk accesses / seeks , and a total data transfer of @xmath107 floating - point words .",
    "4 .   forming the svd of @xmath42 in  ( [ svd_small3 ] )",
    "costs @xmath98 flops .",
    "5 .   forming @xmath49 in  ( [ product33 ] )",
    "costs @xmath96 flops .",
    "forming @xmath4 , @xmath5 , and @xmath6 in step  6 costs @xmath99 flops .    in practice , we choose @xmath102 ( usually a good choice is @xmath28 ) . summing up the costs for the six steps above , and using the fact that @xmath100 , we see that the full algorithm requires @xmath111 flops ,",
    "@xmath112 disk accesses / seeks ( where @xmath103 is the number of floating - point words of ram available to the algorithm ) , and a total data transfer of @xmath113 floating - point words ( more specifically , @xmath114 ) .",
    "in this section , we describe the results of several numerical tests of the algorithm of the present paper .",
    "we set @xmath28 for all examples , setting @xmath115 for the first two examples , and @xmath55 for the last two , where @xmath24 , @xmath11 , and @xmath25 are the parameters from section  [ summary ] above .",
    "we ran all examples on a laptop with 1.5  gb of random - access memory ( ram ) , connected to an external hard drive via usb 2.0 .",
    "the processor was a single - core 32-bit 2-ghz intel pentium m , with 2  mb of l2 cache .",
    "we ran all examples in matlab  7.4.0 , storing floating - point numbers in ram using ieee standard double - precision variables ( requiring 8 bytes per real number ) , and on disk using ieee standard single - precision variables ( requiring 4 bytes per real number ) .",
    "all our numerical experiments indicate that the quality and distribution of the pseudorandom numbers have little effect on the accuracy of the algorithm of the present paper .",
    "we used matlab s built - in pseudorandom number generator for all results reported below .      in this subsection",
    ", we illustrate the performance of the algorithm with the principal component analysis of three examples , including a computational simulation .",
    "for the first example , we apply the algorithm to the @xmath2 matrix @xmath116 where @xmath117 and @xmath118 are @xmath119 and @xmath120 unitary discrete cosine transforms of the second type ( dct - ii ) , and @xmath121 is an @xmath2 matrix whose entries are zero off the main diagonal , with @xmath122 clearly , @xmath123 ,  @xmath124 ,  , @xmath125 ,  @xmath126 are the singular values of @xmath1 .    for the second example , we apply the algorithm to the @xmath2 matrix @xmath116 where @xmath117 and @xmath118 are @xmath119 and @xmath120 unitary discrete cosine transforms of the second type ( dct - ii ) , and @xmath121 is an @xmath2 matrix whose entries are zero off the main diagonal , with @xmath127 clearly , @xmath123 ,  @xmath124 ,  , @xmath125 ,  @xmath126 are the singular values of @xmath1 .",
    "table  1a summarizes results of applying the algorithm to the first example , storing on disk the matrix being approximated .",
    "table  1b summarizes results of applying the algorithm to the first example , generating on - the - fly the columns of the matrix being approximated .",
    "table  2a summarizes results of applying the algorithm to the second example , storing on disk the matrix being approximated .",
    "table  2b summarizes results of applying the algorithm to the second example , generating on - the - fly the columns of the matrix being approximated .",
    "the following list describes the headings of the tables :    * @xmath15 is the number of rows in the matrix @xmath1 being approximated .",
    "* @xmath16 is the number of columns in the matrix @xmath1 being approximated .",
    "* @xmath11 is the parameter from section  [ summary ] above ; @xmath11 is the rank of the approximation being constructed .",
    "* @xmath128 is the time in seconds required to generate and store on disk the matrix @xmath1 being approximated .",
    "* @xmath129 is the time in seconds required to compute the rank-@xmath11 approximation ( the pca ) provided by the algorithm of the present paper .",
    "* @xmath130 is the spectral norm of the difference between the matrix @xmath1 being approximated and its best rank-@xmath11 approximation .",
    "* @xmath131 is an estimate of the spectral norm of the difference between the matrix @xmath1 being approximated and the rank-@xmath11 approximation produced by the algorithm of the present paper .",
    "the estimate @xmath131 of the error is accurate to within a factor of two with extraordinarily high probability ; the expected accuracy of the estimate @xmath131 of the error is about 10% , relative to the best possible error @xmath130 ( see  @xcite ) .",
    "the appendix below details the construction of the estimate @xmath131 of the spectral norm of @xmath132 , where @xmath1 is the matrix being approximated , and @xmath133 is the rank-@xmath11 approximation produced by the algorithm of the present paper .     +   +    [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     for the third example , we apply the algorithm with @xmath134 to an @xmath135 matrix whose rows are independent and identically distributed ( i.i.d . )  realizations of the random vector @xmath136 where @xmath137 ,  @xmath138 , and  @xmath139 are orthonormal @xmath140 vectors , @xmath141 is a @xmath140 vector whose entries are i.i.d .",
    "gaussian random variables of mean zero and standard deviation @xmath142 , and @xmath143 is drawn at random from inside an ellipsoid with axes of lengths @xmath144 ,  @xmath145 , and  @xmath146 , specifically , @xmath147 @xmath148 @xmath149 with @xmath150 drawn uniformly at random from @xmath151 $ ] , @xmath152 drawn uniformly at random from @xmath153 $ ] , and @xmath154 drawn uniformly at random from @xmath155 $ ] .",
    "we obtained @xmath137 , @xmath138 , and  @xmath139 by applying the gram - schmidt process to three vectors whose entries were i.i.d .  centered gaussian random variables ; @xmath137 , @xmath138 , and  @xmath139 are exactly the same in every row , whereas the realizations of @xmath156 , @xmath157 , @xmath158 , and @xmath141 in the various rows are independent .",
    "we generated all the random numbers on - the - fly using a high - quality pseudorandom number generator ; whenever we had to regenerate exactly the same matrix ( as the algorithm requires with @xmath159 ) , we restarted the pseudorandom number generator with the original seed .",
    "figure  1a plots the inner product ( _ i.e. _ , correlation ) of  @xmath137 in  ( [ simulation ] ) and the ( normalized ) right singular vector associated with the greatest singular value produced by the algorithm of the present article .",
    "figure  1a also plots the inner product of  @xmath138 in  ( [ simulation ] ) and the ( normalized ) right singular vector associated with the second greatest singular value , as well as the inner product of  @xmath139 and the ( normalized ) right singular vector associated with the third greatest singular value .",
    "needless to say , the inner products ( _ i.e. _ , correlations ) all tend to 1 , as @xmath15 increases  as they should . figure  1b plots the time required to run the algorithm of the present paper , generating on - the - fly the entries of the matrix being processed . the running - time is roughly proportional to @xmath15 , in accordance with  ( [ on - the - fly_cost ] ) .",
    "+     +      in this subsection , we illustrate the performance of the algorithm with the principal component analysis of images of faces .",
    "we apply the algorithm with @xmath160 to the 393,216 @xmath161 102,042 matrix whose columns consist of images from the feret database of faces described in  @xcite and  @xcite , with each image duplicated three times . for each duplicate , we set the values of a random choice of 10% of the pixels to numbers chosen uniformly at random from the integers 0 ,  1 ,  , 254 ,  255 ; all pixel values are integers from 0 ,  1 ,  , 254 ,  255 .",
    "before processing with the algorithm of the present article , we `` normalized '' the matrix by subtracting from each column its mean , then dividing the resulting column by its euclidean norm .",
    "the algorithm of the present paper required 12.3 hours to process all 150  gb of this data set stored on disk , using the laptop computer with 1.5  gb of ram described earlier ( at the beginning of section  [ numexs ] ) .",
    "figure  2a plots the computed singular values .",
    "figure  2b displays the computed `` eigenfaces '' ( that is , the left singular vectors ) corresponding to the five greatest singular values .",
    "while this example does not directly provide a reasonable means for performing face recognition or any other task of image processing , it does indicate that the sheer brute force of linear algebra ( that is , computing a low - rank approximation ) can be used directly for processing ( or preprocessing ) a very large data set . when used alone ,",
    "this kind of brute force is inadequate for face recognition and other tasks of image processing ; most tasks of image processing can benefit from more specialized methods ( see , for example , @xcite , @xcite , and  @xcite ) .",
    "nonetheless , the ability to compute principal component analyses of very large data sets could prove helpful , or at least convenient .",
    "in this section , we apply the algorithm of the present paper to a data set of interest in a currently developing imaging modality known as single - particle cryo - electron microscopy . for an overview of the field , see  @xcite , @xcite , and their compilations of references .    the data set consists of 10,000 two - dimensional images of the ( three - dimensional ) charge density map of the e.  coli 50s ribosomal subunit , projected from uniformly random orientations , then added to white gaussian noise whose magnitude is 32 times larger than the original images , and finally rotated by 0 ,  1 ,  2 ,  , 358 ,  359 degrees .",
    "the entire data set thus consists of 3,600,000 images , each 129 pixels wide and 129 pixels high ; the matrix being processed is 3,600,000 @xmath161 @xmath162 .",
    "we set @xmath55 , @xmath163 , and @xmath28 , where @xmath24 , @xmath11 , and @xmath25 are the parameters from section  [ summary ] above . processing the data set required 5.5  hours on two 2.8  ghz quad - core intel xeon x5560 microprocessors with 48  gb of random - access memory",
    ".    figure  3a displays the 250 computed singular values .",
    "figure  3b displays the computed right singular vectors corresponding to the 25 greatest computed singular values .",
    "figure  3c displays several noisy projections , their versions before adding the white gaussian noise , and their denoised versions .",
    "each denoised image is the projection of the corresponding noisy image on the computed right singular vectors associated with the 150 greatest computed singular values .",
    "the denoising is clearly satisfactory .",
    "[ fig : singular values ] +             +          +          +          +          +    ccc noisy & clean & denoised +   +   &   &   +   +   +   &   &   +   +   +   &   &   +   +   +   &   &   +   +   +   &   &   +   +   +     +",
    "the present article describes techniques for the principal component analysis of data sets that are too large to be stored in random - access memory ( ram ) , and illustrates the performance of the methods on data from various sources , including standard test sets , numerical simulations , and physical measurements .",
    "several of our data sets stored on disk were so large that less than a hundredth of any of them could fit in our computer s ram ; nevertheless , the scheme always succeeded .",
    "theorems , their rigorous proofs , and their numerical validations all demonstrate that the algorithm of the present paper produces nearly optimal spectral - norm accuracy .",
    "moreover , similar results are available for the frobenius / hilbert - schmidt norm .",
    "finally , the core steps of the procedures parallelize easily ; with the advent of widespread multicore and distributed processing , exciting opportunities for further development and deployment abound .",
    "in this appendix , we describe a method for estimating the spectral norm @xmath164 of a matrix @xmath165 .",
    "this procedure is particularly useful for checking whether an algorithm has produced a good approximation to a matrix ( for this purpose , we choose @xmath165 to be the difference between the matrix being approximated and its approximation ) .",
    "the procedure is a version of the classic power method , and so requires the application of @xmath165 and @xmath166 to vectors , but does not use @xmath165 in any other way . though the method is classical , its probabilistic analysis summarized below was introduced fairly recently in  @xcite and  @xcite ( see also section  3.4 of  @xcite ) .",
    "suppose that @xmath15 and @xmath16 are positive integers , and @xmath165 is a real @xmath2 matrix .",
    "we define @xmath167 , @xmath168 , @xmath169 ,  to be real @xmath170 column vectors with independent and identically distributed entries , each distributed as a gaussian random variable of zero mean and unit variance . for any positive integers @xmath103 and @xmath11",
    ", we define @xmath171 which is the best estimate of the spectral norm of @xmath165 produced by @xmath103 steps of the power method , started with @xmath11 independent random vectors ( see , for example ,  @xcite ) .",
    "naturally , when computing @xmath172 , we do not form @xmath173 explicitly , but instead apply @xmath165 and @xmath166 successively to vectors .",
    "needless to say , @xmath174 for any positive @xmath103 and @xmath11 .",
    "a somewhat involved analysis shows that the probability that @xmath175 is greater than @xmath176 the probability in  ( [ high_prob ] ) tends to 1 very quickly as @xmath103 increases .",
    "thus , even for fairly small @xmath103 , the estimate @xmath172 of the value of @xmath164 is accurate to within a factor of two , with very high probability ; we used @xmath177 for all numerical examples in this paper .",
    "we used the procedure of this appendix to estimate the spectral norm in  ( [ sort_of_svd ] ) , choosing @xmath178 , where @xmath1 , @xmath4 , @xmath5 , and @xmath6 are the matrices from  ( [ sort_of_svd ] ) .",
    "we set @xmath11 for @xmath172 to be equal to the rank of the approximation @xmath179 being constructed .    for more information ,",
    "see  @xcite , @xcite , or section  3.4 of  @xcite .",
    "we would like to thank the mathematics departments of ucla and yale , especially for their support during the development of this paper and its methods .",
    "nathan halko and per - gunnar martinsson were supported in part by nsf grants dms0748488 and dms0610097 .",
    "yoel shkolnisky was supported in part by israel science foundation grant 485/10 .",
    "mark tygert was supported in part by an alfred p. sloan research fellowship .",
    "portions of the research in this paper use the feret database of facial images collected under the feret program , sponsored by the dod counterdrug technology development program office .",
    ", _ normalized power iterations for the computation of svd _ , proceedings of the neural and information processing systems ( nips ) workshop on low - rank methods for large - scale machine learning , vancouver , canada ( 2011 ) , available at http://www.math.ucla.edu/@xmath180aszlam/npisvdnipsshort.pdf .          , _ computing steerable principal components of a large set of images and their rotations _ , technical report , princeton applied and computational mathematics , 2010 .",
    "available at http://math.princeton.edu/@xmath180amits/publications/largesetpca.pdf ."
  ],
  "abstract_text": [
    "<S> recently popularized randomized methods for principal component analysis ( pca ) efficiently and reliably produce nearly optimal accuracy  even on parallel processors  unlike the classical ( deterministic ) alternatives . </S>",
    "<S> we adapt one of these randomized methods for use with data sets that are too large to be stored in random - access memory ( ram ) . </S>",
    "<S> ( the traditional terminology is that our procedure works efficiently _ out - of - core_. ) we illustrate the performance of the algorithm via several numerical examples . </S>",
    "<S> for example , we report on the pca of a data set stored on disk that is so large that less than a hundredth of it can fit in our computer s ram .    algorithm , principal component analysis , pca , svd , singular value decomposition , low rank    65f15 , 65c60 , 68w20 </S>"
  ]
}