{
  "article_text": [
    "in many machine learning problems , the distance metric used over the input data has critical impact on the success of a learning algorithm .",
    "for instance , @xmath2-nearest neighbor ( @xmath3-nn ) classification @xcite , and clustering algorithms such as @xmath0-means rely on if an appropriate distance metric is used to faithfully model the underlying relationships between the input data points .",
    "a more concrete example is visual object recognition .",
    "many visual recognition tasks can be viewed as inferring a distance metric that is able to measure the ( dis)similarity of the input visual data , ideally being consistent with human perception .",
    "typical examples include object categorization @xcite and content - based image retrieval @xcite , in which a similarity metric is needed to discriminate different object classes or relevant and irrelevant images against a given query . as one of the most classic and simplest classifiers , @xmath0-nn has been applied to a wide range of vision tasks and it is the classifier that directly depends on a predefined distance metric . an appropriate distance metric is usually needed for achieving a promising accuracy .",
    "previous work ( , @xcite ) has shown that compared to using the standard euclidean distance , applying an well - designed distance often can significantly boost the classification accuracy of a @xmath0-nn classifier . in this work ,",
    "we propose a scalable and fast algorithm to learn a mahalanobis distance metric .",
    "mahalanobis metric removes the main limitation of the euclidean metric in that it corrects for correlation between the different features .",
    "recently , much research effort has been spent on learning a mahalanobis distance metric from labeled data @xcite .",
    "typically , a convex cost function is defined such that a global optimum can be achieved in polynomial time .",
    "it has been shown in the statistical learning theory @xcite that increasing the margin between different classes helps to reduce the generalization error .",
    "inspired by the work of @xcite , we directly learn the mahalanobis matrix from a set of _ distance comparisons _ , and optimize it via margin maximization .",
    "the intuition is that such a learned mahalanobis distance metric may achieve sufficient separation at the boundaries between different classes .",
    "more importantly , we address the scalability problem of learning the mahalanobis distance matrix in the presence of high - dimensional feature vectors , which is a critical issue of distance metric learning .",
    "as indicated in a theorem in @xcite , a positive semidefinite trace - one matrix can always be decomposed as a convex combination of a set of rank - one matrices .",
    "this theorem has inspired us to develop a fast optimization algorithm that works in the style of gradient descent . at each iteration",
    ", it only needs to find the principal eigenvector of a matrix of size @xmath4 ( @xmath5 is the dimensionality of the input data ) and a simple matrix update .",
    "this process incurs much less computational overhead than the metric learning algorithms in the literature @xcite .",
    "moreover , thanks to the above theorem , this process automatically preserves the property of the mahalanobis matrix . to verify its effectiveness and efficiency ,",
    "the proposed algorithm is tested on a few benchmark data sets and is compared with the state - of - the - art distance metric learning algorithms . as experimentally demonstrated , @xmath0-nn with the mahalanobis distance learned by our algorithms attains comparable ( sometimes slightly better ) classification accuracy .",
    "meanwhile , in terms of the computation time , the proposed algorithm has much better scalability in terms of the dimensionality of input feature vectors .",
    "we briefly review some related work before we present our work . given a classification task",
    ", some previous work on learning a distance metric aims to find a metric that makes the data in the same class close and separates those in different classes from each other as far as possible .",
    "xing @xcite proposed an approach to learn a mahalanobis distance for supervised clustering .",
    "it minimizes the sum of the distances among data in the same class while maximizing the sum of the distances among data in different classes .",
    "their work shows that the learned metric could improve clustering performance significantly .",
    "however , to maintain the property , they have used projected gradient descent and their approach has to perform a _ full _ eigen - decomposition of the mahalanobis matrix at each iteration .",
    "its computational cost rises rapidly when the number of features increases , and this makes it less efficient in coping with high - dimensional data .",
    "goldberger @xcite developed an algorithm termed neighborhood component analysis ( nca ) , which learns a mahalanobis distance by minimizing the leave - one - out cross - validation error of the @xmath0-nn classifier on the training set .",
    "nca needs to solve a non - convex optimization problem , which might have many local optima .",
    "thus it is critically important to start the search from a reasonable initial point .",
    "goldberger have used the result of linear discriminant analysis as the initial point . in nca , the variable to optimize is the projection matrix .",
    "the work closest to ours is large margin nearest neighbor ( lmnn ) @xcite in the sense that it also learns a mahalanobis distance in the large margin framework . in their approach ,",
    "the distances between each sample and its `` target neighbors '' are minimized while the distances among the data with different labels are maximized .",
    "a convex objective function is obtained and the resulting problem is a semidefinite program ( sdp ) . since conventional interior - point based sdp solvers can only solve problems of up to a few thousand variables , lmnn has adopted an alternating projection algorithm for solving the sdp problem . at each iteration ,",
    "similar to @xcite , also a full eigen - decomposition is needed .",
    "our approach is largely inspired by their work .",
    "our work differs lmnn @xcite in the following : ( 1 ) lmnn learns the metric from the pairwise distance information . in contrast , our algorithm uses examples of proximity comparisons among triples of objects ( , example @xmath6 is closer to example @xmath7 than example @xmath0 ) . in some applications like image retrieval",
    ", this type of information could be easier to obtain than to tag the actual class label of each training image .",
    "rosales and fung @xcite have used similar ideas on metric learning ; ( 2 ) more importantly , we design an optimization method that has a clear advantage on computational efficiency ( we only need to compute the leading eigenvector at each iteration ) .",
    "the optimization problems of @xcite and @xcite are both sdps , which are computationally heavy .",
    "linear programs ( lps ) are used in @xcite to approximate the sdp problem .",
    "it remains unclear how well this approximation is .",
    "the problem of learning a kernel from a set of labeled data shares similarities with metric learning because the optimization involved has similar formulations .",
    "lanckriet @xcite and kulis @xcite considered learning kernels subject to some pre - defined constraints .",
    "an appropriate kernel can often offer algorithmic improvements .",
    "it is possible to apply the proposed gradient descent optimization technique to solve the kernel learning problems .",
    "we leave this topic for future study .",
    "the rest of the paper is organized as follows .",
    "section  [ sec : knnmm ] presents the convex formulation of learning a mahalanobis metric . in section  [ sec : knnmm - sdp ] , we show how to efficiently solve the optimization problem by a specialized gradient descent procedure , which is the main contribution of this work .",
    "the performance of our approach is experimentally demonstrated in section  [ sec : experiments ] .",
    "finally , we conclude this work in section  [ sec : conclusion ] .",
    "in this section , we propose our distance metric learning approach as follows .",
    "the intuition is to find a particular distance metric for which the margin of separation between the classes is maximized . in particular",
    ", we are interested in learning a quadratic mahalanobis metric .    let @xmath8 denote a training sample where @xmath9 is the number of training samples and @xmath10 is the number of features . to learn a mahalanobis distance",
    ", we create a set @xmath11 that contains a group of training triplets as @xmath12 , where @xmath13 and @xmath14 come from the same class and @xmath15 belongs to different classes .",
    "a mahalanobis distance is defined as follows .",
    "let @xmath16 denote a linear transformation and @xmath17 be the squared euclidean distance in the transformed space .",
    "the squared distance between the projections of @xmath13 and @xmath14 writes : @xmath18 according to the class memberships of @xmath13 , @xmath14 and @xmath15 , we wish to achieve @xmath19 and it can be obtained as @xmath20 it is not difficult to see that this inequality is generally not a convex constrain in @xmath21 because the difference of quadratic terms in @xmath21 is involved . in order to make this inequality constrain convex ,",
    "a new variable @xmath22 is introduced and used throughout the whole learning process . learning a mahalanobis distance is essentially learning the mahalanobis matrix @xmath23 .",
    "becomes linear in @xmath24 .",
    "this is a typical technique to _ convexify _ a problem in convex optimization @xcite .      in our algorithm ,",
    "a _ margin _ is defined as the difference between @xmath25 and @xmath26 , that is , @xmath27 similar to the large margin principle that has been widely used in machine learning algorithms such as support vector machines and boosting , here we maximize this margin to obtain the optimal mahalanobis matrix @xmath23 .",
    "clearly , the larger is the margin @xmath28 , the better metric might be achieved . to enable some flexibility , , to allow some inequalities of not to be satisfied ,",
    "a soft - margin criterion is needed .",
    "considering these factors , we could define the objective function for learning @xmath23 as @xmath29 where @xmath30 constrains @xmath23 to be a matrix and @xmath31 denotes the trace of @xmath23 .",
    "@xmath32 indexes the training set @xmath11 and @xmath33 denotes the size of @xmath11 .",
    "@xmath34 is an algorithmic parameter that balances the violation of and the margin maximization .",
    "@xmath35 is the slack variable similar to that used in support vector machines and it corresponds to the soft - margin hinge loss . enforcing @xmath36 removes the scale ambiguity because the inequality constrains are scale invariant . to simplify exposition",
    ", we define @xmath37ssss therefore , the last constraint in can be written as @xmath38 note that this is a linear constrain on @xmath23 .",
    "problem is thus a typical sdp problem since it has a linear objective function and linear constraints plus a conic constraint .",
    "one may solve it using off - the - shelf sdp solvers like csdp @xcite .",
    "however , directly solving the problem using those standard interior - point sdp solvers would quickly become computationally intractable with the increasing dimensionality of feature vectors .",
    "we show how to efficiently solve in a fashion of first - order gradient descent .",
    "it is proved in  @xcite that _ a matrix can always be decomposed as a linear convex combination of a set of rank - one matrices_. in the context of our problem , this means that @xmath39 , where @xmath40 is a rank - one matrix and @xmath41 .",
    "this important result inspires us to develop a gradient descent based optimization algorithm . in each iteration",
    ", @xmath23 can be updated as @xmath42 where @xmath43 is a rank - one and trace - one matrix .",
    "@xmath44 is the search direction .",
    "it is straightforward to verify that @xmath45 , and @xmath46 hold .",
    "this is the starting point of our gradient descent algorithm . with this update strategy , the trace - one and positive semidefinteness of @xmath24",
    "is always retained .",
    "we show how to calculate this search direction in algorithm  [ alg:2 ] .",
    "although it is possible to use subgradient methods to optimize non - smooth objective functions , we use a differentiable objective function instead so that the optimization procedure is simplified ( standard gradient descent can be applied ) .",
    "so , we need to ensure that the objective function is differentiable with respect to the variables @xmath47 and @xmath23 .",
    "let @xmath48 denote the objective function and @xmath49 be a loss function .",
    "our objective function can be rewritten as @xmath50 the above problem adopts the hinge loss function that is defined as @xmath51 .",
    "however , the hinge loss is not differentiable at the point of @xmath52 , and standard gradient - based optimization cam be applied directly . in order to make standard gradient descent methods applicable",
    ", we propose to use differentiable loss functions , for example , the squared hinge loss or huber loss functions as discussed below .",
    "+    the squared hinge loss function can be represented as @xmath53 as shown in fig .",
    "[ fig : loss ] , this function connects the positive and zero segments smoothly and it is differentiable everywhere including the point @xmath52 .",
    "we also consider the huber loss function in this work : @xmath54 where @xmath55 is a parameter whose value is usually between @xmath56 and @xmath57 . a huber loss function with @xmath58",
    "is plotted in fig .",
    "[ fig : loss ] .",
    "there are three different parts in the huber loss function , and they together form a continuous and differentiable function .",
    "this loss function approaches the hinge loss curve when @xmath59 .",
    "although the huber loss is more complicated than the squared hinge loss , its function value increases linearly with the value of @xmath60 .",
    "hence , when a training set contains outliers or samples heavily contaminated by noise , the huber loss might give a more reasonable ( milder ) penalty than the squared hinge loss does .",
    "we discuss both loss functions in our experimental study .",
    "again , we highlight that by using these two loss functions , the cost function @xmath61 that we are going to optimization becomes differentiable with respect to both @xmath23 and @xmath47 .",
    "* initialize * : @xmath62 such that @xmath63    [ alg:0 ]    [ alg:2 ]",
    "the proposed algorithm maximizes the objective function iteratively , and in each iteration the two variables @xmath23 and @xmath47 are optimized alternatively . note that the optimization in this alternative strategy retains the global optimum because @xmath64 is a convex function in both variables @xmath65 and @xmath66 are not coupled together .",
    "we summarize the proposed algorithm in algorithm  [ alg:0 ] .",
    "note that @xmath67 is a scalar and line 3 in algorithm [ alg:0 ] can be solved directly by a simple one - dimensional maximization process .",
    "however , @xmath23 is a matrix with size of @xmath68 . recall that @xmath10 is the dimensionality of feature vectors .",
    "the following section presents how @xmath23 is efficiently optimized in our algorithm .",
    "let @xmath70 be the domain in which a feasible @xmath71 lies .",
    "note that @xmath72 is a convex set of @xmath71 .",
    "as shown in line 4 in algorithm  [ alg:0 ] , we need to solve the following maximization problem : @xmath73 where @xmath67 is the output of line 3 in algorithm  [ alg:0 ] .",
    "our algorithm offers a simple and efficient way for solving this problem by explicitly maintaining the positive semidefiniteness property of the matrix @xmath71 .",
    "it needs only compute the largest eigenvalue and the corresponding eigenvector whereas most previous approaches such as the method of @xcite require a full eigen - decomposition of @xmath71 .",
    "their computational complexities are @xmath74 and @xmath75 , respectively .",
    "when @xmath76 is large , this computational complexity difference could be significant .",
    "let @xmath77 be the gradient matrix of @xmath78 with respect to @xmath71 and @xmath79 be the step size for updating @xmath71 . recall that we update @xmath23 in such a way that @xmath80 , where @xmath81 and @xmath82 . to find the @xmath43 that satisfies these constraints and in the meantime",
    "can best approximate the gradient matrix @xmath77 , we need to solve the following optimization problem : @xmath83 the optimal @xmath84 is exactly @xmath85 where @xmath86 is the eigenvector of @xmath77 that corresponds to the largest eigenvalue .",
    "the constraints says that @xmath87 is a outer product of a unit vector : @xmath88 with @xmath89 .",
    "here @xmath90 is the euclidean norm .",
    "problem can then be written as : @xmath91 \\bf v   $ ] , subject to @xmath89 .",
    "it is clear now that an eigen - decomposition gives the solution to the above problem .    hence , to solve the above optimization",
    ", we only need to compute the leading eigenvector of the matrix @xmath77 .",
    "note that @xmath71 still retains the properties of @xmath92 after applying this process .    clearly , a key parameter of this optimization process is @xmath79 which implicitly decides the total number of iterations .",
    "the computational overhead of our algorithm is proportional to the number of iterations .",
    "hence , to achieve a fast optimization process , we need to ensure that in each iteration the @xmath79 can lead to a sufficient reduction on the value of @xmath93 .",
    "this is discussed in the following part .",
    "we employ the backtracking line search algorithm in  @xcite to identify a suitable @xmath79 .",
    "it reduces the value of @xmath79 until the wolfe conditions are satisfied .",
    "as shown in algorithm  [ alg:2 ] , the search direction is @xmath94 .",
    "the wolfe conditions that we use are @xmath95 @xmath96where @xmath97 .",
    "the result of backtracking line search is an acceptable @xmath79 which can give rise to sufficient reduction on the function value of @xmath48 .",
    "we show in the experiments that with this setting our optimization algorithm can achieve higher computational efficiency than some of the existing solvers .",
    "the goal of these experiments is to verify the efficiency of our algorithm in achieving comparable ( or sometimes even better ) classification performances with a reduced computational cost .",
    "we perform experiments on @xmath98 data sets described in table  [ table : dataset ] . for some data sets",
    ", pca is performed to remove noises and reduce the dimensionality .",
    "the metric learning algorithms are then run on the data sets pre - processed by pca .",
    "the wine , balance , vehicle , breast - cancer and diabetes data sets are obtained from uci machine learning repository @xcite , and usps , mnist and letter are from libsvm @xcite for mnist , we only use its test data in our experiment .",
    "the orlface data is from att research and twin - peaks is downloaded from l. van der maaten s website .",
    "the face and background classes ( 435 and 520 images respectively ) in the image retrieval experiment are obtained from the caltech-101 object database @xcite . in order to perform statistics analysis ,",
    "the orlface , twin - peaks , wine , balance , vehicle , diabetes and face - background data sets are randomly split as 10 pairs of train / validation / test subsets and experiments on those data set are repeated 10 times on each split .    [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "we have proposed a new algorithm to demonstrate how to efficiently learn a mahalanobis distance metric with the principle of margin maximization .",
    "enlightened by the important theorem on matrix decomposition in @xcite , we have designed a gradient descent method to update the mahalanobis matrix with cheap computational loads and at the same time , the property of the learned matrix is maintained during the whole optimization process .",
    "experiments on benchmark data sets and the retrieval problem verify the superior classification performance and computational efficiency of the proposed distance metric learning algorithm .",
    "chang and c .- j .",
    "lin , `` libsvm : a library for support vector machines , '' 2001 .",
    "[ online ] .",
    "available : http : // www .",
    "tw/ cjlin /    libsvmtools / datasets/[http : // www . csie .",
    "tw/ cjlin /    libsvmtools / datasets/ ]        l.  fei - fei , r.  fergus , and p.  perona , `` learning generative visual models from few training examples : an incremental bayesian approach tested on 101 object categories , '' in",
    "_ workshop on generative - model based vision , in conjunction with ieee conf .",
    "_ , washington , d.c . ,",
    "july 2004 .",
    "g.  r.  g. lanckriet , n.  cristianini , p.  bartlett , l.  el ghaoui , and m.  i. jordan , `` learning the kernel matrix with semidefinite programming , '' _ j. mach .",
    "_ , vol .  5 , no .  1 ,",
    "pp . 2772 , dec .",
    "2004 .",
    "a.  w.  m. s. , m.  worring , s.  santini , a.  gupta , and r.  jain , `` content - based image retrieval at the end of the early years , '' _ ieee trans .",
    "pattern anal .",
    "_ , vol .  22 , no .  12 , pp .",
    "13491380 , dec . 2000 .    c.  shen , a.  welsh , and l.  wang , `` psdboost : matrix - generation linear programming for positive semidefinite matrices learning , '' in _ _ proc .",
    "neural inf . process .",
    "syst.__1em plus 0.5em minus 0.4em vancouver , canada : mit press , dec .",
    "2008 , pp . 14731480 .",
    "k.  q. weinberger , j.  blitzer , and l.  k. saul , `` distance metric learning for large margin nearest neighbor classification , '' in _ proc . adv . neural inf . process .",
    "_ , vancouver , canada , dec .",
    "2006 , pp . 14751482 .",
    "e.  p. xing , a.  y. ng , m.  i. jordan , and s.  russell , `` distance metric learning , with application to clustering with side - information , '' in _ _ proc . adv .",
    "neural inf .",
    "syst.__1em plus 0.5em minus 0.4emvancouver , canada : mit press , dec .",
    "2003 , pp . 505512 .",
    "l.  yang , r.  sukthankar , and s.  c.  h. hoi , `` a boosting framework for visuality - preserving distance metric learning and its application to medical image retrieval , '' _ ieee trans . pattern anal .",
    "_ , vol .",
    "32 , no .  1 , jan ."
  ],
  "abstract_text": [
    "<S> for many machine learning algorithms such as @xmath0-nearest neighbor ( @xmath0-nn ) classifiers and @xmath1-means clustering , often their success heavily depends on the metric used to calculate distances between different data points . </S>",
    "<S> an effective solution for defining such a metric is to learn it from a set of labeled training samples . in this work , </S>",
    "<S> we propose a fast and scalable algorithm to learn a mahalanobis distance metric . </S>",
    "<S> the mahalanobis metric can be viewed as the euclidean distance metric on the input data that have been linearly transformed . by employing the principle of margin maximization to achieve better generalization performances </S>",
    "<S> , this algorithm formulates the metric learning as a convex optimization problem and a positive semidefinite ( ) matrix is the unknown variable . based on an important theorem that a trace - one matrix can always be represented as a convex combination of multiple rank - one matrices , our algorithm accommodates any differentiable loss function and solves the resulting optimization problem using a specialized gradient descent procedure . </S>",
    "<S> during the course of optimization , the proposed algorithm maintains the positive semidefiniteness of the matrix variable that is essential for a mahalanobis metric . </S>",
    "<S> compared with conventional methods like standard interior - point algorithms @xcite or the special solver used in large margin nearest neighbor ( lmnn ) @xcite , our algorithm is much more efficient and has a better performance in scalability . </S>",
    "<S> experiments on benchmark data sets suggest that , compared with state - of - the - art metric learning algorithms , our algorithm can achieve a comparable classification accuracy with reduced computational complexity .    large - margin nearest neighbor , distance metric learning , mahalanobis distance , semidefinite optimization . </S>"
  ]
}