{
  "article_text": [
    "consider a two - class classification problem , where we have  @xmath3 labeled training samples @xmath4 . here , @xmath5 are @xmath6-dimensional feature vectors and @xmath7 are the corresponding class labels .",
    "for simplicity , we assume two classes are _ equally likely _ , and the data are centered so that @xmath8 where @xmath9 is the contrast mean vector between two classes , and @xmath10 is the @xmath11 covariance matrix . given a fresh feature vector @xmath12",
    "the goal is to train @xmath13 to decide whether @xmath14 or @xmath15 .",
    "we denote @xmath16 by @xmath17 , and whenever there is no confusion , we drop the subscripts `` @xmath18 '' ( and also that of any estimator of it , say , @xmath19 ) .",
    "we are primarily interested in the so - called `` @xmath20 '' regime . in many applications where @xmath20 ( e.g. , genomics ) , we observe the following aspects .    *",
    "_ signals are rare_. due to large @xmath6 , the useful features ( i.e. , the nonzero coordinates of @xmath9 ) are rare . for example ,",
    "for a given type of cancer or disease , there are usually only a small number of relevant features ( i.e. , genes or proteins ) . when we measure increasingly more features , we tend to include increasingly more _ irrelevant _ ones . *",
    "_ signals are individually weak_. the training data can be summarized by the @xmath21-vector @xmath22 due to the small @xmath3 , signals are weak in the sense that , individually , the nonzero coordinates of @xmath23 are small or moderately large at most . * _ precision matrix @xmath2 is sparse_. take genetic regulatory network ( grn ) for example .",
    "the feature vector @xmath24 represents the expression levels of @xmath6 different genes , and is approximately distributed as @xmath25 . for any @xmath26 , it is believed that for all except a few @xmath27 , @xmath28 , the gene pair @xmath29 are conditionally independent given all other genes . in other words , each row of @xmath2",
    "has only a few nonzero entries , so @xmath2 is sparse @xcite .    in many applications , @xmath2 is unknown and has to be estimated . in many other applications such as complicate disease or cancer , decades of biomedical studies",
    "have accumulated huge databases which are sometimes referred to as `` data - for - data ''  @xcite .",
    "such databases can be used to accurately estimate @xmath2 independently of the data at hand , so @xmath2 can be assumed as known . in this paper , we investigate both the case where @xmath2 is known and the case where @xmath2 is unknown . in either case , we assume @xmath2 has unit diagonals : @xmath30 such an assumption is only for simplicity , and we do not use such information for inference",
    ".      fisher s linear discriminant analysis ( lda ) @xcite is a well - known method for classification , which utilizes a weighted average of the test features @xmath31 , and predicts @xmath32 if @xmath33 .",
    "here , @xmath34 is a preselected weight vector .",
    "fisher showed that the optimal weight vector satisfies @xmath35 in the classical setting where @xmath36 , @xmath9 and @xmath2 can be conveniently estimated and fisher s lda is approachable . unfortunately , in the modern regime where @xmath20 , fisher s lda faces immediate challenges .",
    "* it is challenging to estimate @xmath2 simply because there are @xmath37 unknown parameters but we have only @xmath38 different measurements . * even in the simplest case where @xmath39 , challenges remain , as the signals are rare and weak .",
    "see @xcite for the delicacy of the problem .",
    "the paper is largely focused on addressing the second challenge .",
    "it shows that successful classification can be achieved by simultaneously exploiting the sparsity of @xmath9 ( aka . signal sparsity ) and the sparsity of @xmath40 ( aka .",
    "graph sparsity ) . for the first challenge ,",
    "encouraging progresses have been made recently ( e.g. , @xcite ) , and the problem is more or less settled .",
    "still , the paper has a two - fold contribution along this line .",
    "first , we show that the performances of the methods in @xcite can be substantially improved if we add an additional re - fitting step ; see details in section  [ secsimul ] .",
    "second , we carefully analyze how the errors in estimating @xmath2 may affect the classification results .",
    "we wish to adapt fisher s lda to the current setting .",
    "recall that the optimal choice of weight vector is @xmath41 .",
    "if we have a reasonably good estimate of @xmath40 ( see section  [ subsecomega2 ] for more discussion on estimating  @xmath2 ) , say , @xmath42 , all we need is a good estimate of @xmath9 .    when @xmath9 is sparse , one usually estimates it with some type of thresholding @xcite .",
    "let @xmath43 be the training @xmath21-vector as in ( [ definez ] ) .",
    "for some threshold @xmath44 to be determined , there are three obvious approaches to thresholding :    * _ brute - force thresholding _",
    "we apply thresholding to @xmath43 directly using the so - called clipping rule @xcite : @xmath45 .",
    "alternatively , one may use soft thresholding or hard thresholding .",
    "however , numeric studies ( e.g. ,  @xcite ) suggest that different thresholding schemes only have small differences in classification errors , provided that these schemes use the same threshold picked from the range of interest . for this reason",
    ", we only study the clipping rule ; same below . * _ whitened thresholding _ ( wt ) .",
    "we first whiten the noise by the transformation @xmath46 , and then apply the thresholding to the vector @xmath47 in a similar fashion . *",
    "_ innovated thresholding _ ( it ) .",
    "we first take the transformation @xmath48 and then apply the thresholding by @xmath49    the transformation @xmath50 is connected to the term of _ innovation _ in the literature of time series @xcite , and so the name of innovated thresholding .",
    "it turns out that , among the three approaches , it is the best . to see the point , note that for any @xmath11 nonsingular matrix @xmath51 , one could always estimate @xmath9 by applying the thresholding to @xmath52 entry - wise ( in bt , wt , and it , @xmath53 , and @xmath2 approximately ) .",
    "the deal is , what is the best @xmath51 ?    toward this end , write @xmath54'$ ] . for any @xmath55",
    ", it is seen that @xmath56 .",
    "therefore , if we bet on @xmath57 , we should choose @xmath58 to optimize the signal - to - noise ratio ( snr ) of @xmath59 . by the cauchy ",
    "schwarz inequality , the optimal @xmath58 satisfies that @xmath60 .",
    "writing @xmath61 $ ] , it is seen that @xmath62 when we bet on @xmath57 , @xmath63 which is accessible to us .",
    "however , @xmath64 is a very noisy vector and is inaccessible to us , estimating which is equally hard as estimating @xmath9 itself .    in summary ,",
    "if we bet on @xmath57 , then the `` best '' accessible choice is @xmath65 .",
    "as this holds for all @xmath66 and we do not know where the signals are , the optimal choice for @xmath51 is @xmath67 .",
    "this says that it is not only the best among the three choices above , but is also the best choice in more general situations .",
    "the heuristics above are consolidated in sections  [ subsecomega1][subsecbtwt ] , where we show that it based classifiers achieve the optimal phase diagram for classification , while bt or wt based classifiers do not , even in very simple settings .",
    "the advantage of it over wt and bt can be illustrated with the following example , which is further discussed later in section  [ subsecbtwt ] where we compare the phase diagrams of it , wt , and bt .",
    "suppose @xmath2 is a block diagonal matrix where for @xmath68 and @xmath69 , @xmath70 \\\\[-8pt ] \\nonumber & & { } + h \\cdot1\\ { i - j = 1 , \\mbox{$i$ is even } \\}.\\end{aligned}\\ ] ] according to the block structure of @xmath2 , we also partition the vector @xmath9 into @xmath71 blocks , and each block has two entries .",
    "for simplicity , we suppose each block of @xmath9 has either no signal , or a single signal with a strength @xmath72 .",
    "bt , wt , and it apply thresholding to @xmath43 , @xmath73 , and @xmath74 , correspondingly , where @xmath75 is the training @xmath21-vector as above . in this simple example ,",
    "the snr for @xmath43 , @xmath73 , and @xmath76 are @xmath77 , @xmath78 \\tau/2 $ ] and @xmath79 correspondingly , with the last one being the largest ( for the mean vector of @xmath73 or @xmath76 , the nonzero coordinates have two different magnitudes ; the snr is computed based on the larger magnitude ) .    in ( [ omegaterms ] ) ,",
    "the point that ( ii ) is generally noninformative in designing the best @xmath58 can be further elaborated as follows : since we do not know the locations of other nonzero coordinates of @xmath9 , it makes sense to model @xmath80 as i.i.d .",
    "samples from @xmath81 where @xmath82 is the point mass at @xmath83 and @xmath84 is some distribution with no mass at @xmath83 . under general `` rare and weak '' conditions for @xmath85 and sparsity condition for @xmath2 , entries of @xmath86",
    "$ ] are uniformly small .    in the literature of variable selection ,",
    "it is also called _ marginal regression _  @xcite .",
    "the connection is not surprising , as approximately , @xmath87 which is a regression model .",
    "both methods apply thresholding to @xmath76 entry - wise , but marginal regression uses the hard thresholding rule , and it uses the clipping thresholding rule @xcite .    with that being said , challenges remain on how to set the threshold @xmath44 of it [ see  ( [ itthreshold ] ) ] .",
    "if we set @xmath44 too small or too large , the resultant estimator @xmath88 has too many or too few nonzeros .",
    "our proposal is to set the threshold in a data driven fashion by using the recent innovation of higher criticism thresholding ( hct ) .",
    "higher criticism ( hc ) is a notion mentioned in passing by tukey @xcite . in recent years , hc was found to be useful in sparse signal detection @xcite , large - scale multiple testing @xcite , goodness - of - fit  @xcite , and was applied to nongaussian detection in cosmic microwave background @xcite and genomics @xcite .",
    "hc as a method for threshold choice in feature selection was first introduced in @xcite ( see also @xcite ) , but the study has been focused on the case where @xmath2 is the identity matrix .",
    "the case we consider in the current paper is much more complicated , where how to use hc for threshold choice is a nontrivial problem .",
    "our proposal is as follows .",
    "let @xmath42 be a reasonably good estimate of @xmath2 and let @xmath43 be the training @xmath21-vector as in ( [ definez ] ) .",
    "as in ( [ itthreshold ] ) , denote for short @xmath89 the proposed approach contains three simple steps .    * for each @xmath28 , obtain a @xmath6-value by @xmath90 .",
    "* sort all the @xmath6-values in the ascending order @xmath91 . *",
    "define the hc functional @xmath92/ \\sqrt{(1 - j / p ) j / p}$ ] , @xmath93 .",
    "let @xmath94 be the index at which @xmath95 takes the maximum .",
    "the higher criticism threshold ( hct)denoted by @xmath96is defined as the @xmath94th largest coordinate of @xmath97 .",
    "moreover , for stability , we need the following refinement . define @xmath98 it is well - understood ( e.g. , @xcite ) that the threshold should not be larger than  @xmath99 . at the same time",
    ", the threshold should not be too small , especially when @xmath3 is small .",
    "the hct we use in this paper is @xmath100 see sections  [ subsecmodel ] and  [ sechct ] for more detailed discussion .",
    "we are now ready for classification .",
    "let @xmath42 be as above , and let @xmath101 be defined as @xmath102 compared to @xmath88 in ( [ itthreshold ] ) , the only difference is that we have replaced @xmath44 by @xmath103 .",
    "introduce the hct classification statistic @xmath104 the hct trained classifier ( or hct classifier for short ) is then the decision rule that decides @xmath32 according to @xmath105 .",
    "the innovation of the procedure is two - fold : using it for feature selection and using hct for threshold choice in the more complicated case where @xmath2 is unknown and is nonidentity .",
    "the work is connected to other works on hc @xcite , but the procedure and the delicate theory it entails are new .    a question is whether it has any advantages over exsiting variable selection methods ( e.g. , the lasso @xcite , scad @xcite , dantzig selector @xcite ) .",
    "the answer is yes , for the following reasons .",
    "first , compared to these methods , it is computationally much faster and much more approachable for delicate analysis .",
    "second , our goal is classification , not variable selection . for classification ,",
    "especially when features are rare and weak , the choice of different variable selection methods is secondary , while the choice of the tuning parameter is crucial .",
    "the threshold of it can be conveniently set by hct , but how to set the tuning parameter of the lasso , scad , or dantzig selector remains an open problem , at least in theory .",
    "how does the hct classifier behave ? in sections  [ subsecmodel][subseclb ] , we set up a theoretic framework and derive a lower bound for classification errors . in sections  [ subsecomega1][subsecomega2 ] , we investigate the hct classifier for the cases where @xmath2 is known and unknown separately , and show that the hct classifier yields optimal phase diagram in classification .",
    "motivated by the application examples aforementioned , we use a _ rare and weak _ signal model as follows .",
    "we model the scaled contrast mean vector @xmath23 as @xmath106 where as in ( [ eqmixture ] ) , @xmath82 is the point mass at @xmath83 , @xmath84 is some distribution with no mass at  @xmath83 , and @xmath107 is small [ note that @xmath108 depend on @xmath6 but not on @xmath27 ] .",
    "we use @xmath6 as the driving asymptotic parameter , and link parameters @xmath110 to @xmath6 through some fixed parameters . in detail",
    ", fixing parameters @xmath111 , we model @xmath112 as @xmath6 tends to @xmath113 , the sample size @xmath114 grows to @xmath113 but in a slower rate than that of  @xmath6 ; the signals get increasingly sparser but the number of signals tends to @xmath113 . the interesting range of parameters @xmath115 partitions into three regimes , according to the sparsity level .",
    "* _ relatively dense _",
    "in this regime , @xmath116 .",
    "the signals are relatively dense and successful classification is possible even when signals are very faint [ e.g. , @xmath84 concentrates its mass around a term @xmath117 . in such cases , (",
    "a ) successful feature selection is impossible as signals are too weak , and ( b ) feature selection is unnecessary for the signals are relatively dense . * _ rare and weak _ ( rw ) . in this regime , @xmath118 , and the signals",
    "are moderately sparse . for successful classification , we need moderately strong signals [ i.e. , nonzero coordinates of @xmath119 . in this case ,",
    "feature selection is subtle but could be substantially helpful .",
    "in contrast , classification is impossible if signals are much weaker than @xmath120 , and consistent feature selection is possible ( and so the problem of classification is much less challenging ) if the signals are much stronger than @xmath120 . * _ rare and strong _ ( rs ) . in this regime , @xmath121 , and the signals are very sparse . for successful classification , we need very strong signals [ signal strength @xmath122 . in this case ,",
    "feature selection is comparably easier to carry out ( but substantially helpful ) since the signals are strong enough to stand out for themselves .",
    "while the statements hold broadly , the most transparent way to understand them is probably to consider the case where @xmath84 is a point mass at @xmath123 ( say ) : in the above three regimes , the minimum @xmath124 required for successful classification ( up to some multi-@xmath125 factors in the first and last regimes ) are @xmath126 , @xmath120 , and @xmath127 correspondingly ; the proof is elementary so is omitted .    in summary ,",
    "feature selection is impossible in the rd regime and is relatively easy in the rs regime . for these reasons , we are primarily interested in the rw regime where we assume @xmath128 the rd / rs regimes are further discussed in section  [ subsecdiscu ] , where we address the connection between our work and @xcite . for @xmath129 in this range ,",
    "the most interesting range for the signal strength is when @xmath84 concentrates its mass at the scale of @xmath120 . in light of this",
    ", we fix @xmath130 and calibrate the signal strength parameter  @xmath123 by @xmath131    except in section  [ subseclb ] where we address the lower bound arguments , we assume  @xmath84 is a point mass [ compare ( [ defineh ] ) ] : @xmath132 we focus on the case @xmath133 , as the case @xmath134 corresponds to rs regime where the classification is comparably easier .",
    "this models a setting where the signal strengths are equal .",
    "the case where the signal strengths are unequal is discussed in section  [ subsecsummary ] .",
    "next , we model @xmath2 . motivated by the previous example on genetic regulatory network , we assume each row of @xmath2 has relatively few nonzeros .",
    "such a matrix naturally induces a sparse graph @xmath135 , where @xmath136 and there is an edge between nodes @xmath66 and @xmath27 if and only if @xmath137 ; see @xcite for basic terminology in graph theory .",
    "[ de1.1 ] fix @xmath6 and @xmath138 .",
    "we call a @xmath11 positive definite matrix @xmath2 @xmath139-sparse if each row of @xmath2 has at most @xmath139 nonzeros . for any graph @xmath140 , we call @xmath140 @xmath139-sparse if the degree of each node @xmath141 .    when @xmath2 is @xmath139-sparse , the induced graph @xmath142 is @xmath143 sparse , since by convention , there is no edge between a node and itself .",
    "the class of @xmath139-sparse graphs is much broader than the class of banded graphs ( we call @xmath140 a banded graph with bandwidth @xmath139 if nodes @xmath66 and @xmath27 are not connected whenever @xmath144 ) .",
    "in fact , even when @xmath140 is @xmath139-sparse with @xmath145 , we can not always shuffle the nodes of @xmath140 and make it a banded graph with a small bandwidth .",
    "let @xmath146 be the class of all @xmath11 positive definite correlation matrices .",
    "fixing @xmath147 , @xmath148 , and a sequence of integers @xmath149 , introduce @xmath150 and @xmath151 where @xmath152 is the spectral norm . in comparison",
    ", @xmath153 is slightly smaller than @xmath154 .",
    "the following short - hand notation is frequently used in this paper .",
    "[ de1.2 ] we use @xmath155 to denote a strictly positive generic multi-@xmath125 term that may vary from occurrence to occurrence but always satisfies that for any fixed @xmath156 , @xmath157 and @xmath158 .    in this paper , we are primarily interested in the case where @xmath149 is at most multi - logarithmically large unless stated otherwise : @xmath159 the first requirement is only for convenience . in our classification setting , @xmath160 , @xmath161 , and @xmath32 with equal probabilities .",
    "the following notation is frequently used in the paper .",
    "[ de1.3 ] we say the classification problem ( [ model])([testsample ] ) satisfies the asymptotic rare weak model @xmath162 if ( [ defineh])([defineepsandn ] ) , ( [ pointmass ] ) and ( [ definek ] ) hold .",
    "the normalization in arw is different from that in conventional asymptotic settings . in the latter ,",
    "we usually fix @xmath9 and let @xmath3 increase , so the classification problem becomes increasingly easier as @xmath3 increase . in arw , to focus on the `` most interesting parameter regime '' , we fix @xmath23 and let @xmath3 increase",
    ". as a result , the snr in the summarizing training @xmath43-vector remain the same , but the snr in the testing vector @xmath163 decrease rapidly as @xmath3 increase .",
    "therefore , the classification problem becomes increasingly harder as @xmath3 increase .",
    "introduce the _ standard phase boundary _",
    "function @xmath164 and let @xmath165 the function @xmath166 has appeared before in determining phase boundaries in a seemingly unrelated problem of multiple hypothesis testing @xcite .",
    "the following theorem is proved in the supplementary material @xcite .",
    "[ thmmlb ] fix @xmath167 such that @xmath118 and @xmath168 .",
    "suppose ( [ defineh])([defineepsandn ] ) , ( [ definetau ] ) , and ( [ definek ] ) hold and that for sufficiently large @xmath6 , @xmath169 and the support of @xmath84 is contained in @xmath170 $ ] .",
    "then as @xmath171 , for any sequence of trained classifiers , the misclassification error @xmath172 .    note that in theorem  [ thmmlb ] , we do not require the signals to have the same strength .",
    "also , recall that in our classification setting ( [ model])([testsample ] ) , two classes are assumed as equally likely ; extension to the case where two classes are unequally likely is straightforward .",
    "theorem  [ thmmlb ] was discovered before in @xcite , but the study has been focused on the case where @xmath173 and @xmath84 is the point mass at @xmath123 .",
    "the proof in the current case is much more difficult and needs a few tricks , where graph theory on vertex coloring plays a key role .",
    "the following lemma is adapted from @xcite , section v.1 .",
    "[ lemmapartition ] fix @xmath174 . for any graph @xmath175 that is @xmath139-sparse",
    ", the chromatic number of @xmath140 is no greater than @xmath176 .",
    "recall that when @xmath2 is @xmath139 sparse , then the induced graph @xmath177 is @xmath178 sparse , and so the chromatic number of @xmath142 @xmath141 . as a result , we can color the nodes of @xmath142 with no more than @xmath139 different colors , where there is no edge between any pair of nodes with the same color .    despite its seemingly simplicity , lemma  [ lemmapartition ] has far - reaching implications .",
    "lemma  [ lemmapartition ] is the corner stone for proving the lower bound and for analyzing the hct classifier ( where we need tight convergence rate of empirical processes for data with unconventional correlation structures ) .",
    "one noteworthy aspect of hct classifier is that it achieves the optimal phase diagram . in this section ,",
    "we show this for the case where @xmath2 is known . in this case , the hct classifier @xmath179 reduces to @xmath180 ( the term formed by replacing @xmath42 by @xmath2 everywhere in the definition of former ) .",
    "since we predict the label associated with @xmath163 as @xmath181 according to @xmath182 , the predicted label is correct if and only if @xmath183 . the following theorem is proved in section  [ subsecclassify ] .",
    "[ thmmidealized ] fix @xmath184 such that @xmath185 and @xmath186 .",
    "consider a sequence of classification problems @xmath187 with @xmath188 for sufficiently large @xmath6 .",
    "then as @xmath6 tends to @xmath113 , @xmath189 .",
    "when @xmath190 , the condition on @xmath2 can be relaxed to that of @xmath191 .",
    "call the two - dimensional space @xmath192 the phase space .",
    "theorems [ thmmlb][thmmidealized ] say that the phase space partitions into two separate regions , _ region of impossibility _ and _ region of possibility _ , where the classification problem is distinctly different .",
    "* _ region of impossibility_. @xmath193 .",
    "fix @xmath194 in the interior of this region and consider a sequence of classification problems with @xmath195 signals where each signal @xmath196 in strength . then for any sequence of `` sparse '' @xmath2 , successful classification is impossible .",
    "this is the most difficult case where not much can be done for classification aside from random guessing .",
    "* _ region of possibility_. @xmath197 .",
    "fix @xmath194 in the interior of this region and suppose signals have equal strength of @xmath198 .",
    "hct classifier @xmath199 yields successful classification ( the results hold much more broadly where equal signal strength assumption can be largely relaxed ) .",
    "we call the curve @xmath200 the _ separating boundary_. somewhat surprisingly , the separating boundary does not depend on the off - diagonals of @xmath2 .",
    "the partition of phase diagram was discovered by @xcite , and independently by @xcite , but the focus was on the case where @xmath39 .",
    "see also @xcite .",
    "the study in the current case is much more difficult .",
    "similar phase diagrams are also found in sparse signal detection @xcite , variable selection @xcite , and spectral clustering @xcite .",
    "why hct works ?",
    "the key insight is that there is an intimate relationship between the hc functional and fisher s separation ; the latter plays a key role in determining the optimal classification behavior , but is , unfortunately , an _ oracle _ quantity which depends on unknown parameters . in sections",
    "[ secidealhc][sechct ] , we outline a series of theoretic results , explaining why the hct classifier is the right approach and how it achieves the optimality .",
    "when @xmath2 is unknown , we first estimate it with the training data .",
    "[ de1.4 ] for any sequence of @xmath201 , we say an estimator @xmath19 is acceptable if it is symmetric and independent of the test feature vector  @xmath163 , and that there is a constant @xmath202 such that for sufficiently large @xmath6 , @xmath19 is @xmath203-sparse where @xmath204 , and @xmath205 for all @xmath69 .    usually , the @xmath206-rate can not be improved , even when @xmath2 is diagonal . for @xmath149-sparse @xmath2 satisfying ( [ definek ] ) ,",
    "acceptable estimators can be constructed based on the recent clime approach in @xcite .",
    "if additionally @xmath2 satisfies the mutual incoherence condition @xcite , assumption  1 , then the glasso @xcite is also acceptable , provided the tuning parameters are properly set .",
    "if @xmath2 is banded , then the bickel and levina thresholding ( blt ) method @xcite is also acceptable , up to some modifications .    with that being said , the numeric performances of all these estimators can be improved with an additional step of _ re - fitting_. see section  [ secsimul ] for details .    naturally , the estimation error of @xmath42 has some negative effects on the hct classifier .",
    "fortunately , for a large fraction of parameters @xmath194 in region of possibility , such effects are negligible and hct continues to yield successful classification . in detail , recalling that @xmath207 , @xmath208 , we suppose :    * _ condition _ ( a ) .",
    "@xmath209 , * _ condition _ ( b ) . when @xmath210 and @xmath211 , @xmath212 .",
    "the following theorem is proved in section  [ subsecclassify ] .",
    "[ thmmub1 ] fix @xmath184 such that @xmath185 , and conditions  hold . consider a sequence of classification problems @xmath187 such that @xmath213 when @xmath190 and @xmath188 when @xmath214 .",
    "for the hct classifier @xmath215 , if @xmath42 is acceptable , then as @xmath6 tends to @xmath113 , @xmath216 .",
    "we remark that , first , when @xmath217 and @xmath218 , condition ( a ) can be relaxed to that of @xmath219 .",
    "second , when @xmath220 , conditions ( a)(b ) automatically hold when @xmath221 . as a result",
    ", we have the following corollary , the proof of which is omitted .",
    "[ corub ] when @xmath220 , theorem  [ thmmub1 ] holds with conditions  replaced by that of @xmath186 .",
    "this says that as long as @xmath222 , the estimation errors of any acceptable estimator @xmath42 have negligible effects over the classification decision .      in disguise ,",
    "many methods are what we called `` brute - forth thresholding '' or `` bt , '' including but not limited to @xcite . since @xmath2 is hard to estimate , bickel and levina @xcite , fan and fan @xcite , and tibshirani et al .",
    "@xcite neglect the off - diagonals in @xmath0 for classification . in a seemingly different spirit ,",
    "efron @xcite proposes a procedure where he first selects features by neglecting the off - diagonals in @xmath0 and then estimates the correlation structures among selected features . however , under the rare and weak model , selected features tend to be uncorrelated .",
    "therefore , at least for many cases , the approach fails to exploit the `` local '' graphic structure of the data and is `` bt '' in disguise .",
    "it is also noteworthy that @xcite proposes to set the threshold of bt by cross validation , which is unstable , especially when @xmath114 is small .",
    "when we replace it by either bt or wt in hct classifier , the phase diagram associated with the resultant procedure is no longer optimal .",
    "while the claim holds very broadly , it can be conveniently illustrated with a simple example as follows .",
    "consider the same setting as in theorem  [ thmmidealized ] , except that @xmath2 is the matrix defined in ( [ blockwiseadd ] ) .",
    "that is , @xmath2 is the diagonal block - wise matrix where each diagonal block is the @xmath223 matrix with @xmath224 on the diagonals and @xmath225 on the off - diagonals , @xmath226 . in this simple case , by theorem  [ thmmidealized ] ,",
    "hct classifier gives successful classification when @xmath186 , and fails when @xmath227 . in comparison ,",
    "if we use bt ( which treats @xmath0 as diagonal and does not incorporate correlations for classification ) , the separating function for success and failure becomes @xmath228 , which is higher than @xmath200 in the @xmath129-@xmath229 plane ( a similar claim holds for wt , but the separating function is @xmath230 $ ] ; note @xmath231 > 1 $ ] for all @xmath232 ) .",
    "recall that when @xmath2 is given , the _ only _ difference between the hct classifier built over it and the hct classifier built over bt is that , for any threshold @xmath44 , bt and it estimate @xmath9 by @xmath233 respectively , where @xmath234 ; see section  [ subsecitadd ] for details .",
    "we have the following theorem , the proof of which is elementary so is omitted .",
    "[ thmmbickel ] fix @xmath235 such that @xmath118 .",
    "consider a sequence of classification problems @xmath187 where @xmath2 is the diagonal block - wise matrix defined in ( [ blockwiseadd ] ) .",
    "suppose we apply hct classifier built over the brute - force thresholding ( bt ) as in section  [ subsecitadd ] . as @xmath171 , the classification error @xmath236 if @xmath237 , and the classification error @xmath238 if @xmath239 .      the work is closely related to the recent approach by shao et al .",
    "@xcite , the road approach by fan et al .",
    "@xcite , and the lpd approach by cai and liu @xcite .",
    "while all approaches attempt to mimic fisher s lda , the difference lies in how we estimate the `` ideal weight vector '' @xmath240 prescribed in ( [ fisherw ] ) . in our notation , shao et al .",
    "@xcite estimates @xmath240 by @xmath241 , where @xmath242 is the regularized estimation of @xmath0 as in bickel and levina @xcite for an appropriate threshold , and @xmath243 is the estimation of @xmath9 by brute - force thresholding .",
    "road estimates @xmath240 by minimizing @xmath244 , and lpd estimates @xmath240 by minimizing @xmath245 subject to the constraint of @xmath246 , where @xmath247 and @xmath248 are tuning parameters .    in disguise , these works focused on the `` rare and strong '' regime according to our terminology .",
    "in fact , shao et al . @xcite assumes the minimum signal strength ( smallest coordinate in magnitude of @xmath249 ) is of the order of @xmath250 , and the main results of fan et al . @xcite and cai and liu @xcite ( i.e. , @xcite , theorem  3 , @xcite , theorem  1 ) assume a sparsity constraint that can be roughly translated to @xmath251 in our notation .",
    "seemingly , this concerns the rs regime we mentioned earlier .",
    "compared to these works , our work focuses on the most challenging regime where the signals are rare and weak , and we need much more sophisticated methods for feature selection and for threshold choices .",
    "hct classifier also has advantages over other well - known classifiers such as the support vector machine ( svm ) @xcite , random forest @xcite , and boosting @xcite .",
    "these methods need tuning parameters and are internally very complicated , but they do not outperform hct classifier even when we replace the it by bt ; see details in @xcite , where we have compared all these methods with three well - known gene microarray data sets in the context of cancer classification .",
    "hct is also closely related to pam @xcite , but is different in some important aspects .",
    "first , hct exploits the correlation structure while pam does not .",
    "second , while both methods perform feature selection , pam sets the threshold by cross validations ( cvt ) , and hct sets the threshold by higher criticism .",
    "when @xmath3 is small , cvt is usually unstable . in @xcite",
    ", we have shown that hct outperforms cvt when analyzing three microarray data sets aforementioned . in section  [ secsimul ]",
    ", we further compare hct with cvt with simulated data .",
    "we propose hct classifier for two - class classification , where the major methodological innovation is the use of it for feature selection and the use of hc for threshold choice .",
    "it is based on an `` optimal '' linear transform that maximizes snr in all signal locations , and has advantages over bt and wt .",
    "it also has a three - fold advantage over the well - known variable selection methods such as the lasso , scad , and dantzig selector : ( a ) it is computationally faster , ( b ) it is more approachable in terms of delicate analysis , and ( c ) the tuning parameter of it can be conveniently set , but how to set the tuning parameters of the other methods remains an open problem .",
    "the idea of using hc for threshold choice goes back to @xcite , where the focus is on the case where @xmath2 is the identity matrix ( see also @xcite ) . in this paper , with considerable efforts , we extend the ideas to the case where @xmath40 is unknown but is presumably sparse , and show that hc achieves the optimal phase diagram in classification .",
    "the optimality of hc is not coincidental , and the underlying reason is the intimate relationship between the hc functional and fisher s separation .",
    "this is explained in sections  [ secidealhc][sechct ] with details .    in theorems [ thmmidealized][thmmub1 ] and sections  [ secidealhc][sechct ]",
    ", we assume the signals have the same signs and strengths .",
    "the first assumption is largely for simplicity and can be removed .",
    "the second assumption can be largely relaxed , and both theorems [ thmmidealized][thmmub1 ] and the intimate relationship between hc and fisher s separation continue to hold to some extent if the signal strengths are unequal .",
    "one such example is where the signal distribution @xmath84 , after scaled by a factor of @xmath252 , has a continuous density over a closed interval contained in @xmath253 which does not depend on @xmath6 .    in the paper , we require @xmath2 to be @xmath149-sparse where @xmath254 ( see definition  [ de1.2 ] ) and does not exceed a multi-@xmath125 term .",
    "this assumption is mainly used to control the chromatic number of the induced graph @xmath142 . since",
    "the chromatic number of a graph could be much smaller than its maximum degree , the assumption on @xmath2 can be relaxed to that of the chromatic number of @xmath255 does not exceed a multi-@xmath125 term .",
    "also , when @xmath2 has many small nonzero coordinates , we can always regularize it first with a threshold @xmath256 : @xmath257 , and the main results continue to hold if @xmath258 is @xmath139-sparse and the difference between two matrices is `` sufficiently small . ''",
    "the remaining part of the paper is organized as follows . in section  [ secidealhc ] , we introduce two functionals : fisher s separation and ideal hc , and show that the two functionals are intimately connected to each other . in section  [ sechct ] , we derive a large - deviation bound on the empirical c.d.f . , and then use it to characterize the stochastic fluctuation of the hc functional and that of fisher s separation .",
    "theorems [ thmmidealized][thmmub1 ] are proved at the end of this section .",
    "all other claims ( theorems and lemmas ) are proved in the supplementary material @xcite .",
    "section  [ secsimul ] contains numeric examples .      in this paper ,",
    "@xmath202 and @xmath259 denote a generic constant and a generic multi-@xmath125 term respectively , which may vary from occurrence to occurrence . for two positive sequences @xmath260 and",
    "@xmath261 , we say that @xmath262 ( or @xmath263 ) if there is a sequence @xmath264 such that @xmath265 and @xmath266 [ or @xmath267 .",
    "we say that @xmath268 if @xmath262 and @xmath263 , and we say that @xmath269 if there is a constant @xmath270 such that for sufficiently large @xmath6 , @xmath271 .    the notation @xmath2 and @xmath0 are always associated with each other by @xmath1 , and @xmath13 represents a training sample while @xmath272 represents a test sample .",
    "the summarizing @xmath21-vector for the training data set is denoted by @xmath43 , with @xmath234 and @xmath273 , where @xmath42 is some estimate of @xmath2 .",
    "in sections  [ secidealhc][sechct ] , we discuss the behavior of hct classifier .",
    "we limit our discussion to the @xmath187 model , but the key ideas are valid beyond the @xmath274 model and extensions are possible ; see discussions in section  [ subsecsummary ] .",
    "the key insight behind the hct methodology is that in a broad context , @xmath275 the ideal hct is the nonstochastic counterpart of hct , and the ideal threshold is the threshold one would choose if the underlying signal structure were known .    in this section , we elaborate the intimate connection between the ideal hct and the ideal threshold , and their connections to fisher s separation . we also investigate the performance of `` ideal classifier '' where we assume @xmath2 is known and the threshold is set ideally .",
    "the connection between hct and ideal hct is addressed in section  [ sechct ] , which is new even in the case of @xmath39 ; compare @xcite .",
    "theorems [ thmmidealized][thmmub1 ] are also proved in section  [ sechct ] .",
    "fix a threshold @xmath256 and let @xmath42 be an acceptable estimator of @xmath2 .",
    "we are interested in the classifier that estimates @xmath32 according to @xmath276 , whereas in ( [ hctclassadd1])([hctclassadd2 ] ) , @xmath277 for any fixed @xmath278 vector @xmath43 and @xmath11 positive definite matrix @xmath279 , we introduce @xmath280 and @xmath281 where loosely , `` @xmath51 '' and `` @xmath282 '' stand for the mean and variance , respectively . in our model , given @xmath283 , the test sample @xmath284 ; see ( [ testsample ] ) and note that @xmath42 is independent of @xmath163 since it is acceptable .",
    "it follows that @xmath285 and the misclassification error rate of @xmath286 is @xmath287 where @xmath288 denotes the survival function of @xmath289 .",
    "the right - hand side of ( [ misclassadd1 ] ) is closely related to the well - known fisher s separation ( sep ) @xcite , which measures the standardized interclass distance @xmath290 : @xmath291 - e[l_t(x , \\hat{\\omega } ) | y = -1 ] } { sd(l_t(x , \\hat { \\omega}))}.\\ ] ] in fact , it is seen that @xmath292 , and ( [ misclassadd1 ] ) can be rewritten as @xmath293 by ( [ defineh ] ) and ( [ pointmass ] ) , the overall misclassification error rate is then @xmath294,\\ ] ] where @xmath295 is the expectation with respect to the law of @xmath296 , and @xmath297 is the expectation with respect to the law of @xmath85 ; see ( [ defineh ] ) and ( [ pointmass ] ) .",
    "we introduce two proxies for fisher s separation . throughout this paper ,",
    "@xmath298 for the first proxy , recall that @xmath299 [ e.g. , ( [ definehatz ] ) ] .",
    "heuristically , @xmath300 and so @xmath301 . we expect that @xmath302 ; the latter is fisher s separation for the idealized case where @xmath2 is known and is defined as @xmath303 for the second proxy , we note that when @xmath6 is large , some regularity appears , and we expect that @xmath304 and @xmath305 , where @xmath306 , \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber v_p(t , { \\varepsilon}_p , \\tau_p , \\omega ) & = & e \\bigl[v_p(t , \\tilde{z } , \\omega)\\bigr].\\end{aligned}\\ ] ] in light of this , a second proxy separation is the _ population @xmath307 _ : @xmath308 in summary , we expect to see that @xmath309 and that @xmath310 in section  [ sechct ] , we solidify the above connections . but",
    "before we do that , we study the ideal threshold  the threshold that maximizes @xmath311 .",
    "ideally , one would choose @xmath44 to minimize the classification error of @xmath286 . in light of ( [ errorapprox ] ) , this is almost equivalent to choosing @xmath44 as the ideal threshold .",
    "[ de2.1 ] the ideal threshold @xmath312 is the maximizing point of the second proxy : @xmath313 .",
    "in general , @xmath314 and @xmath312 may depend on @xmath2 in a complicated way .",
    "fortunately , it turns out that for large @xmath6 and all @xmath2 in @xmath154 [ see ( [ eqdefmset0 ] ) ] , the leading terms of @xmath311 and @xmath315 do not depend on the off - diagonals of @xmath2 and have rather simple forms .    [ de2.2 ] denote @xmath316 .",
    "when @xmath317 , we drop the subscript and write @xmath318 . also , denote @xmath319 and @xmath320 .    in detail , let @xmath321 and @xmath322 elementary calculus shows that for large @xmath6 , @xmath323 it turns out that there is an intimate relationship between @xmath324 and @xmath325 , where the latter does not depend on the off - diagonals of @xmath2 .",
    "to see the point , we discuss the cases @xmath326 and @xmath214 separately .    in the first case , for @xmath327 as in @xmath154 ,",
    "we let @xmath328 \\\\[-8pt ] \\nonumber \\tilde c_0(\\beta , r , a ) & = & \\tilde c_1(\\beta , r , a ) - \\delta(\\beta , r),\\end{aligned}\\ ] ] where @xmath329 for @xmath330 ; if @xmath331 , @xmath332 , and otherwise , @xmath333 the following lemma is proved in the supplementary material @xcite .",
    "[ lemmaidealsep ] fix @xmath184 such that @xmath334 and @xmath335 . in the @xmath336 model , as @xmath171 , @xmath337\\\\ & & \\qquad\\quad{}\\times \\sup_{\\{0 < t < \\infty \\ } } { \\widetilde{w}}_0(t , { \\varepsilon}_p , \\tau_p).\\end{aligned}\\ ] ]    note that @xmath338 . as a result , approximately , @xmath339 for all @xmath340 . combining thiswith  ( [ eqoptimizew0 ] )",
    ", we expect to have @xmath341 \\\\[-8pt ] \\nonumber \\sup_{0 < t < \\infty } \\widetilde{\\operatorname{sep}}(t , { \\varepsilon}_p , \\tau_p , \\omega ) & = & l_p p^ { { ( 1-\\theta)}/{2}-\\delta(\\beta , r)}.\\end{aligned}\\ ] ]    next , consider the case @xmath214 .",
    "the lemma below is proved in the supplementary material @xcite .",
    "[ lemmaidealsepb ] fix @xmath184 such that @xmath214 and @xmath342 .",
    "let @xmath343 and @xmath344 , where @xmath345 is some constant . in the @xmath187 model with @xmath346 , as @xmath171 ,    @xmath347 ,    @xmath348 ,    @xmath349 and + @xmath350 .",
    "a direct result of lemma  [ lemmaidealsepb ] is that , for all @xmath351 [ see ( [ eqdefmset0 ] ) ] , @xmath352 \\\\[-8pt ] \\nonumber \\sup_{\\ { 0 < t < \\infty\\ } } \\bigl\\ { \\widetilde{\\operatorname{sep}}(t ) \\bigr \\ } & \\asymp & l_pp^{(1-\\theta-\\beta)/2},\\end{aligned}\\ ] ] where @xmath353 and @xmath354 for short . in this case , the function @xmath311 sharply increases and decreases in the intervals @xmath355 and @xmath356 , respectively , but is relatively flat in the interval @xmath357 ; in this interval , the function reaches the maximum but varies slowly at the magnitude of @xmath358 . in the current case , on one hand , it is not critical to pin down @xmath359 , as @xmath360 for all  @xmath44 in the whole interval . on the other hand ,",
    "it is hard to pin down @xmath359 uniformly for all @xmath2 under consideration , if possible at all .",
    "ideal hct is a counterpart of hct and a nonstochastic threshold that hct tries to estimate .",
    "introduce a functional which is defined over all survival functions associated with a positive random variable : @xmath361 / \\sqrt{g(t ) \\bigl(1 - g(t ) \\bigr)},\\qquad t > 0.\\ ] ] we are primarily interested in thresholds that are neither too small or too large as far as hct concerns ; see ( [ definesnp ] ) . in light of this",
    ", we introduce the hct functional @xmath362 where the term @xmath363 is chosen for convenience , and can be replaced by some other positive constants . recall that @xmath234 and @xmath299 [ e.g. , ( [ definetildez ] ) and ( [ definehatz ] ) ] . for any @xmath256 ,",
    "let @xmath364 and @xmath365 \\\\[-8pt ] \\nonumber { \\widetilde{f}}(t ) & = & { \\widetilde{f}}(t , { \\varepsilon}_p , \\pi_p , \\omega ) = e_{{\\varepsilon}_p , \\pi_p } \\bigl [ { \\widetilde{f}}_p(t)\\bigr].\\end{aligned}\\ ] ] note that the only difference between @xmath366 and @xmath367 is the subscript @xmath6 .",
    "heuristically , for large @xmath6 , we expect to have @xmath368 . as a result",
    ", we expect that @xmath369 where @xmath370 is the hct where @xmath2 is unknown and has to be estimated , @xmath371 is the hct when @xmath2 is known , and @xmath372 is a nonstochastic counterpart of @xmath371 .",
    "note that in disguise , @xmath370 is the same as @xmath373 , the hct defined in ( [ hctrefined ] ) .",
    "[ de2.3 ] we call @xmath372 the ideal higher criticism threshold ( ideal hct ) .",
    "similarly , the leading term of ideal hct has a simple form that is easy to analyze .",
    "fix @xmath28 .",
    "let @xmath374 , and let @xmath375 the following is a counterpart of @xmath376 defined in ( [ tw0 ] ) and can be well approximated by the latter : @xmath377 the following lemmas are proved in the supplementary material @xcite .",
    "[ lemmaidealhct ] fix @xmath378 such that @xmath379 and @xmath380 . in the @xmath381 model , as @xmath171 , @xmath382    [ lemmaidealhct2 ] fix @xmath378 such that @xmath379 and @xmath380 . in the @xmath381 model , as @xmath171 , we have @xmath383.\\end{aligned}\\ ] ] if additionally @xmath214 , then :    @xmath384 ,    @xmath385 ,    @xmath386 ,    where @xmath387 is defined in lemma  [ lemmaidealsepb ] .",
    "lemmas [ lemmaidealhct][lemmaidealhct2 ] say that , approximately , @xmath388 , and that two functions @xmath376 and @xmath389 are generally close .",
    "together , lemmas [ lemmaidealsep][lemmaidealhct2 ] consolidate the intimate relationship between the ideal threshold and the ideal hct . to see the point",
    ", we discuss the cases @xmath326 and @xmath214 separately .",
    "for the first case , write @xmath390 and @xmath391 for short as before .",
    "the following theorem is proved in the supplementary material  @xcite .",
    "[ thmmidealhcta ] fix @xmath184 such that @xmath392 and @xmath380 . in the @xmath393 model with @xmath394 , as @xmath171 , there is a constant @xmath395 such that @xmath396 , and so @xmath397 .",
    "consider the second case .",
    "lemma  [ lemmaidealhct2 ] says that @xmath398 .",
    "while it is hard to further elaborate how close two ideal thresholds are , in light of ( [ eqidealsep ] ) , hc classification with any @xmath44 in this range is successful , so it is not critical to pin down the ideal hct .",
    "the following theorem is proved in the supplementary material @xcite .",
    "[ thmmidealhctb ] fix @xmath184 such that @xmath214 and @xmath380 . in the @xmath399 model where @xmath400 , as @xmath401",
    ", we have that @xmath402 .",
    "to conclude this section , we investigate the `` ideal '' classifier @xmath403 , where @xmath2 is known to us .",
    "note that for each fixed @xmath44 , the misclassification error of @xmath403 is @xmath404 $ ] .",
    "the following theorem is proved in the supplementary material @xcite .",
    "[ thmmsep ] fix @xmath184 such that @xmath185 and @xmath186 . in the @xmath399 model with @xmath405 , as @xmath171 , @xmath406 when @xmath190 , the condition @xmath405 can be relaxed to that of @xmath213 .    combining theorem  [ thmmsep ] with theorems [ thmmidealhcta][thmmidealhctb ] , @xmath407 where @xmath408 satisfies @xmath409 when @xmath326 and @xmath410 when @xmath214 . recall that in both cases , @xmath411 , where the exponent @xmath412 is strictly positive by the assumption of @xmath186 . therefore",
    ", if @xmath194 fall in region of possibility and if we set @xmath44 as either of the two ideal thresholds , then @xmath413 not only gives successful classification , but the classification error converges to @xmath83 very fast .",
    "in the preceding section , we have been focusing on two ideal thresholds . in this section ,",
    "we study the empirical quantities , and characterize the stochastic fluctuation of hct and sep defined in ( [ definesep ] ) .",
    "we conclude the section by proving theorems [ thmmidealized][thmmub1 ] .",
    "the main results in this section are new , even in the idealized case where @xmath39 .",
    "recall that @xmath414 / \\sqrt{{\\bar{f}}_p(t ) \\bigl(1 - { \\bar{f}}_p(t)\\bigr)}.\\ ] ] when @xmath415 , the above is not well defined , and we modify the definition slightly by replacing @xmath416 with @xmath417 .",
    "the change does not affect the proof of the results .",
    "the stochastic fluctuation of hct comes from that of @xmath416 , which consists of two components : that of estimating @xmath2 and that of the data",
    ". this is captured in the following triangle inequality [ see ( [ definecf])([definetf ] ) ] : @xmath418    consider @xmath419 first .",
    "the key is to study @xmath420 when @xmath39 , this is the _ standard uniform stochastic processes _ @xcite and much is known about its stochastic fluctuation . in the more general case",
    "where @xmath421 , it is usually hard to derive a tight bound on the tail probability of this process .",
    "fortunately , when @xmath2 is @xmath149-sparse , tight bounds are possible , and the key is graph theory on the chromatic number introduced in lemma  [ lemmapartition ]",
    ".    recall that @xmath422 [ e.g. , ( [ definesnp ] ) ] .",
    "the following lemma is the direct result of lemma  [ lemmapartition ] and the well - known bennet s inequality @xcite , and is proved in the supplementary material @xcite .",
    "[ lemmastochunif ] fix @xmath184 and consider an @xmath423 model with @xmath213 . as @xmath401",
    ", there is a constant @xmath202 such that with probability at least @xmath424 , for all @xmath44 satisfying @xmath425 , @xmath426    next , consider @xmath427 .",
    "recall that @xmath207 . by definition , if @xmath42 is an acceptable estimator of @xmath40 , then there is a constant @xmath202 such that with probability at least @xmath424 , @xmath428 as a result , we have the following lemma , whose proof is straightforward and thus omitted . recall that @xmath429 and @xmath234 [ e.g. , ( [ definehatz ] ) and ( [ definetildez ] ) ] .",
    "[ lemmahatomega ] for any acceptable estimator @xmath42 , @xmath430 with probability at least @xmath431 .",
    "write for short @xmath432 . by lemma  [ lemmahatomega ] , with probability at least @xmath431 , for all @xmath28 , @xmath433 . as a result ,",
    "@xmath434 where we note that heuristically , @xmath435 combining these , with probability at least @xmath431 , for any @xmath436 , @xmath437 recall @xmath422 .",
    "the above heuristic is captured in the following lemma , which is proved in the supplementary material @xcite .",
    "[ lemmahcbias ] fix @xmath184 . in the @xmath438 model with @xmath439 ,",
    "there exists a constant @xmath440 such that with probability at least @xmath441 , for all @xmath44 such that @xmath442 , @xmath443^{-1/2 } \\leq l_p \\max \\bigl\\ { \\bigl ( p^{(1 - \\theta ) } { \\widetilde{f}}(t ) \\bigr)^{1/2 } , 1\\bigr\\}.\\ ] ]    combining lemmas [ lemmastochunif ] and [ lemmahcbias ] , we have the following theorem , which is proved in the supplementary material @xcite .",
    "[ thmmhcfunction ] fix @xmath184 . in the @xmath438 model with @xmath439 , as @xmath171 , with probability at least @xmath444 , @xmath445\\qquad \\forall{\\bar{\\psi}}^{-1}\\bigl(\\tfrac{1}{2}\\bigr ) < t < s_p^*.\\ ] ]    by theorem  [ thmmhcfunction ] , in order for @xmath446 to be small , we must have that for all @xmath44 in the vicinity of @xmath372 , @xmath447 when @xmath220 , this holds for all @xmath194 in region of possibility since it can be checked that @xmath448 \\ll \\operatorname{hc}(t,{\\widetilde{f}})$ ] .",
    "when @xmath449 , this might not hold for all @xmath194 in this region , as the estimation error of @xmath42 is simply too large .",
    "this explains why we need to restrict hct to be no less than @xmath450 as in ( [ definesnp ] ) .",
    "this also explains why we need conditions ( a)(b ) in theorem  [ thmmub1 ] , but we do not need such conditions in theorem  [ thmmidealized ] and corollary  [ corub ] .    in the @xmath187 model , @xmath207 .",
    "therefore , @xmath451 see ( [ definesnp ] ) .",
    "accordingly , the hct defined in ( [ hctrefined ] ) can be rewritten as @xmath452 it is worthy to note here that the ideal threshold always falls below @xmath99 , which is defined as @xmath453 ; see section  [ subsecidealtadd ] and especially ( [ eqdeftp^ ] ) .",
    "it is also worthy to note that when @xmath454 and when @xmath2 is unknown , the estimation error of @xmath2 may have a major effect over the classification error , especially when the threshold is small . to alleviate such an effect ,",
    "one possible approach is to set a number @xmath455 ( say ) , and never allow the threshold to be smaller than @xmath455 .",
    "since @xmath194 are unknown to us [ but @xmath456 is known to us ] , so from a practical perspective , we must select @xmath455 in a way so that it does not depend on @xmath194 .",
    "our calculations show that @xmath457 is one of such choices .",
    "the main result in this section is as follows , which is proved in the supplementary material @xcite .",
    "[ thmmhct ] fix @xmath378 such that @xmath458 and @xmath186 . in the @xmath336 model with @xmath439 ,    if @xmath459 , then as @xmath171 , there are positive constants @xmath460 and @xmath461 such that with probability at least @xmath441 , @xmath462 when @xmath326 , and @xmath463 when @xmath464 , where @xmath465 .",
    "if @xmath466 and @xmath467 satisfy the conditions in theorem  [ thmmub1 ] , then with probability at least @xmath441 , @xmath468 for some constant @xmath469 when @xmath326 , and @xmath470 for @xmath471 when @xmath214 , where @xmath472 is a constant .      similarly , the stochastic fluctuation of @xmath473 contains two parts : that from @xmath234 , and that from the estimation @xmath474 . in detail , @xmath475 where @xmath476 and @xmath477 .",
    "consider @xmath478 first . recall that @xmath479 heuristically , @xmath480 and@xmath481 ; see ( [ definesmallmv ] ) . combining these with the definitions , we expect that @xmath482,\\nonumber\\end{aligned}\\ ] ] where in the square brackets , the second term is much smaller than @xmath224 .",
    "this is elaborated in the following lemma which is proved in the supplementary material  @xcite . in detail , let @xmath483    [ lemmasepstoch ] fix @xmath184 such that @xmath379 and @xmath380 . in the @xmath336 model with @xmath484 , as @xmath171 , with probability at least @xmath431 , @xmath485.\\ ] ] when @xmath326 , the condition on @xmath2 can be relaxed to that of @xmath213 .",
    "next , we consider @xmath486",
    ". the following lemma , which is proved in the supplementary material @xcite , characterizes the order of @xmath486 .",
    "[ lemmasepbias ] under the same conditions as in lemma  [ lemmasepstoch ] , as @xmath401 , with probability at least @xmath431 , for all @xmath44 such that @xmath487 , @xmath488 $ ] . when @xmath190 , the condition on @xmath2 can be relaxed to that of @xmath191 .",
    "combining lemmas [ lemmasepstoch][lemmasepbias ] gives the following theorem , the proof of which is omitted ( note that theorem  [ thmmsepfunction ] is parallel to theorem  [ thmmhcfunction ] ) .",
    "[ thmmsepfunction ] under the same conditions as in lemma  [ lemmasepstoch ] , as @xmath401 , with probability at least @xmath444 , for all @xmath44 such that @xmath489 , @xmath490.\\ ] ] when @xmath190 , the condition on @xmath2 can be relaxed to that of @xmath191 .",
    "we are now ready to prove theorems [ thmmidealized][thmmub1 ] , where @xmath2 is assumed as known and unknown , respectively .",
    "the proofs are similar , so we only show theorem  [ thmmub1 ] .",
    "consider @xmath215 , where @xmath42 is an acceptable estimator .",
    "the misclassification error is @xmath491.\\ ] ] we now prove for the case of @xmath326 and @xmath214 separately .    in the first case , we note that @xmath492\\leq l_pp^{\\min\\{0 , { 1}/{2}-\\theta\\}}$ ] for @xmath493 .",
    "write @xmath494 and @xmath391 for short as before . by theorem  [ thmmsepfunction ] , with probability @xmath495 , @xmath496 \\\\[-8pt ] \\nonumber & & \\qquad \\leq l_p \\bigl [ p^{\\min\\{0 , { 1}/{2}-\\theta\\ } } + p^{{(1-\\theta)}/{2 } - \\max\\{\\beta-{r}/{2 } , { ( 3\\beta+r)}/{4}\\}}\\bigr].\\end{aligned}\\ ] ] at the same time , by theorem  [ thmmhct ] , with probability @xmath431 , @xmath497 is algebraically small .",
    "note that @xmath311 is a nonstochastic function , and that in the vicinity of @xmath359 , the second derivative of @xmath498 at @xmath44 has the same magnitude as that of @xmath311 , up to a multi-@xmath499 term ( the first derivative is @xmath83 at @xmath500 ) . by taylor s expansion and lemma  [ lemmaidealsep ] , @xmath501 where @xmath502 is as in ( [ eqdefdelta ] ) . by definitions ,",
    "@xmath503 . inserting ( [ eqclasserr2])([eqclasserr3 ] ) into ( [ eqclasserr1 ] ) gives @xmath504 and the claim follows since @xmath505 .",
    "in the second case , @xmath506 with probability at least @xmath431 . combining this with theorem  [ thmmsepfunction ] , with probability at least @xmath431 , @xmath507 at the same time , by similar argument as that of the proof of theorem  [ thmmidealhctb ] , @xmath508 combining this with ( [ eqclasserr1 ] ) and ( [ eqclasserr2add ] ) gives @xmath509 and the claim follows since @xmath510 .",
    "this proves theorem  [ thmmub1 ] .",
    "we conclude this section by a remark on the convergence rate . at the end of section  [ secidealhc ] , we show that the `` ideal '' classifier @xmath413 has very fast convergence rate with @xmath44 being either the ideal threshold or the ideal hct . in comparison ,",
    "the convergence rate of @xmath215 is unfortunately much slower ( but is still algebraically fast ) . to explain this",
    ", we note that the rate of convergence of @xmath103 to @xmath372 and the rate of convergence of @xmath42 to @xmath2 are both algebraically fast ; if these convergence rates can be improved , then the misclassification error rate of @xmath215 can be improved as well .",
    "we have conducted a small - scale numerical study .",
    "the idea is to select a few sets of representative parameters for experiments , and compare the performance of hct classifier ( hct ) with three other methods : ordinary hct ( ohct ) , pseudo hct ( phct ) , and cvt .",
    "all these methods are very similar to hct , except that ( a ) in phct , we assume @xmath2 is known to us , ( b ) in cvt , we set the threshold of it by a @xmath511-fold cross validation , and ( c ) in ohct , we pretend @xmath0 is diagonal , and estimate @xmath2 accordingly .",
    "note that cvt reduces to pam @xcite if we do not utilize the correlation structure ; see more discussion in @xcite .      for some of the procedures ,",
    "we need to estimate @xmath2 .",
    "we use bickel and levina s thresholding ( blt ) procedure @xcite .",
    "alternatively , one could use the glasso @xcite or the clime @xcite .",
    "but since the main goal is to investigate the performance of hct , we do not include glasso and clime in the study : if hct performs well with @xmath40 estimated by blt , we expect it to perform even better if @xmath2 is estimated more accurately .    at the same time , each of these methods can be improved numerically with an additional _ re - fitting _ stage .",
    "take the blt for example .",
    "for the training data @xmath512 , let @xmath513 , and let @xmath514 be the empirical covariance matrix .",
    "blt starts by obtaining an estimate of @xmath0 using thresholding : @xmath515 and then estimate @xmath2 by @xmath516 . here",
    ", @xmath517 is a tuning parameter .",
    "we propose the following refitting stage to improve the estimator . fixing a tuning parameter @xmath518 ,",
    "we further improve @xmath519 via coordinate - wise thresholding and call the resultant estimator @xmath520 : @xmath521 for each @xmath26 , let @xmath522 , and let @xmath523 be the sub - matrix of @xmath524 formed by restricting the rows / columns of @xmath524 to @xmath525 .",
    "denote the final estimate of @xmath2 by @xmath526 $ ] .",
    "we define @xmath527 as follows .",
    "write @xmath528 , where @xmath529 .",
    "let @xmath530 be the @xmath278 vector such that @xmath531 , @xmath532 , and let @xmath533 be the @xmath534 vector formed by restricting the rows of @xmath530 to @xmath525 .",
    "define @xmath535 .",
    "we let @xmath536 , @xmath537 , and let @xmath538 if @xmath539 .",
    "the resultant estimation of the refitting procedure is a symmetric matrix , which is also positive definite , provided that @xmath149 is sufficiently small ( say , @xmath540 ) and that the smallest eigenvalue of @xmath2 is bounded from below by a constant @xmath202 ; recall that @xmath149 is the maximum of the number of nonzeros in each row of  @xmath2 .                    in experiment 1a , we fix @xmath551 , and let @xmath84 be the point mass at @xmath123 . also , we choose @xmath2 to be the tridiagonal matrix @xmath552 where @xmath327 takes values from @xmath553 .",
    "the results are reported in figure  [ figfig1 ] .",
    "the tuning parameter @xmath554 in ( [ bl ] ) , which varies with the values of @xmath327 , @xmath3 and @xmath6 , is calculated from trials of comparing @xmath555 with the true  @xmath2 .",
    "the tuning parameter @xmath556 in ( [ zeta ] ) , which also varies with the values of @xmath327 , @xmath3 and  @xmath6 , is chosen so that there are only @xmath557 nonzero coordinates in each row of @xmath520 after thresholding of @xmath558 .",
    "we let @xmath559 if @xmath2 is tridiagonal and @xmath560 if @xmath2 is five - diagonal ( see experiments below ) . in this experiment",
    ", @xmath554 is set accordingly from @xmath561 and @xmath556 is from @xmath562 .",
    "the results suggest that hct outperforms ohct , but is slightly inferior to phct since we have to pay a price for estimating @xmath2 .",
    "as @xmath327 increases , the correlation structure becomes increasingly influential , so the advantage of hct over ohct becomes increasingly prominent ( but differences between hct and phct remain almost the same ) .      in experiment 1b , for various @xmath563",
    ", we choose @xmath564 and let @xmath2 be either of the following tridiagonal matrix or five - diagonal matrix . in the first case , @xmath2 is a @xmath11 tridiagonal matrix with @xmath224 on the diagonal and @xmath327 on the off - diagonal . in the second case , @xmath2 is a @xmath11 five - diagonal matrix with @xmath224 on the diagonal , @xmath565 on the first off - diagonal , and @xmath566 on the second off - diagonal .",
    "experiment 1c uses a very similar setting , except that we take @xmath84 as the uniform distribution over @xmath567 $ ] .",
    "we select @xmath556 and @xmath554 similarly as in experiment 1a .",
    "the results based on 25 repetitions for experiment 1b1c are reported in table  [ tabletab1 ] , which suggest that hct outperforms ohct and that phct slightly outperforms hct .",
    "@lccc@ & & & + & & & + & & & + ohct & 0.0508 & 0.2818 & 0.1492 + phct & 0.0384 & 0.0698 & 0.1015 + hct & 0.0523 & 0.0742 & 0.1053 +   + & & & + & & & + & & & + ohct & 0.0560 & 0.2629 & 0.2183 + phct & 0.0571 & 0.1398 & 0.1893 + hct & 0.0572 & 0.1438 & 0.1959 +   + & & & + & & & @xmath568 + & @xmath568 & @xmath569 & @xmath570 + & @xmath571 & @xmath572 & @xmath573 + ohct & 0.0444 & 0.2672 & 0.1648 + phct & 0.0522 & 0.0733 & 0.1159 + hct & 0.0508 & 0.0843 & 0.0977 +    _ experiment _ 2 . in this experiment",
    ", we compare the phct with the cvt assuming @xmath2 is known ( the case @xmath2 is unknown is discussed in experiment 3 ) .",
    "experiment  2 contains two sub - experiments , 2a and 2b .    in experiment 2a",
    ", we consider @xmath574 different combinations of @xmath575 with @xmath564 , and let @xmath2 be the tridiagonal matrix as in ( [ eqtridiag ] ) with @xmath576 .",
    "averages of the selected thresholds and classification errors across different replications are reported in table  2 .",
    "the results over 25 repetitions suggest that the threshold choices by hc and cross validations are considerably different , with the former being more accurate and more stable .",
    "note that hct is also computationally much more efficient than the cvt .",
    "@ld1.2d1.3d1.2d1.3d1.2d1.2@ & & & & & & + phct & 1.9 & 0.05 & 2.16 & 0.002 & 1.99 & 0 + cvt & 2.5 & 0.08 & 1 & 0.018 & 1 & 0 + phct & 2.39 & 0.18 & 2.06 & 0.10 & 2.13 & 0.02 + cvt & 1.9 & 0.224 & 2.00 & 0.14 & 1.1 & 0.09 +    in experiment 2b , we set @xmath577 , @xmath578 , and let @xmath2 be the same as in experiment 2a .",
    "we let @xmath123 range from @xmath224 to @xmath579 with an increment of @xmath580 .",
    "the classification errors over 25 repetitions by phct and cvt are in figure  [ figfig2 ] , where a conclusion similar to that in experiment 2a can be drawn .",
    "_ experiment _ 3 .",
    "we compare the performance of hct with cvt for the case where @xmath2 is unknown and needs to be estimated .",
    "note that for small @xmath3 ( say , less than 500 ) we might not have reasonable accuracy on estimating @xmath2 using blt . for small @xmath6 ,",
    "say @xmath582@xmath583 , the cvt is computationally very slow and it is very likely that the refitting procedure for blt would not have decent performance .",
    "we take @xmath584 and let @xmath2 be the block diagonal matrix consisting @xmath585 diagonal blocks , each is a big five - diagonal matrix @xmath586 , where @xmath587 , @xmath588 , and @xmath589 , @xmath590 .",
    "we let @xmath123 range from @xmath224 to @xmath591 with an increment of 0.2 .",
    "the tuning parameters @xmath556 and @xmath554 are set in the same way as in experiment 1 .",
    "the results are reported in figure  [ figfig3 ] .",
    "due to high computational cost , we only conduct @xmath592 repetitions , so the results are a bit noisy . still , it is seen that hct outperforms cvt .      in summary , for a reasonably large sample size @xmath3 ,",
    "hct outperforms ohct and is only slightly inferior to phct .",
    "the reason we need a relatively large @xmath3 is mainly due to that we need to estimate @xmath2 .",
    "the relative performance of phct , hct , and ohct is intuitive , since phct utilizes the true correlation structure among the features , hct estimates the correlation structure , while ohct ignores it .",
    "the comparisons of phct with cvt in experiments 2a2b suggest that if @xmath2 is known , then hct dominates cvt .",
    "experiment 3 shows that when @xmath6 is several times larger than @xmath3 ( e.g. , 10 times larger ) , hct has smaller classification errors than cvt does , and the precision matrix @xmath2 can bez estimated reasonably well .    for larger @xmath6 , the advantages of the hct are even more prominent than those considered here .",
    "we skip the comparisons for larger @xmath6 due to high computational cost , which mainly comes from the blt procedure ( we must run the algorithm many times to select a good tuning parameter @xmath554 ) . in the future ,",
    "if we could find a more efficient method for estimating @xmath2 , then hct will be both more effective and more convenient to use for large @xmath6 ."
  ],
  "abstract_text": [
    "<S> consider a two - class classification problem where the number of features is much larger than the sample size . </S>",
    "<S> the features are masked by gaussian noise with mean zero and covariance matrix @xmath0 , where the precision matrix @xmath1 is unknown but is presumably sparse . </S>",
    "<S> the useful features , also unknown , are sparse and each contributes weakly ( i.e. , rare and weak ) to the classification decision .    by obtaining a reasonably good estimate of @xmath2 </S>",
    "<S> , we formulate the setting as a linear regression model . </S>",
    "<S> we propose a two - stage classification method where we first select features by the method of _ innovated thresholding _ ( it ) , and then use the retained features and fisher s lda for classification . in this approach , </S>",
    "<S> a crucial problem is how to set the threshold of it . </S>",
    "<S> we approach this problem by adapting the recent innovation of higher criticism thresholding ( hct ) .    </S>",
    "<S> we find that when useful features are rare and weak , the limiting behavior of hct is essentially just as good as the limiting behavior of ideal threshold , the threshold one would choose if the underlying distribution of the signals is known ( if only ) . somewhat surprisingly , when @xmath2 is sufficiently sparse , its off - diagonal coordinates usually do not have a major influence over the classification decision .    </S>",
    "<S> compared to recent work in the case where @xmath2 is the identity matrix [ _ proc . </S>",
    "<S> natl . </S>",
    "<S> acad . </S>",
    "<S> sci . </S>",
    "<S> usa _ </S>",
    "<S> * 105 * ( 2008 ) 1479014795 ; _ philos . </S>",
    "<S> trans . </S>",
    "<S> r. soc . </S>",
    "<S> lond . </S>",
    "<S> ser . a math . </S>",
    "<S> phys . </S>",
    "<S> eng . </S>",
    "<S> sci . _ </S>",
    "<S> * 367 * ( 2009 ) 44494470 ] , the current setting is much more general , which needs a new approach and much more sophisticated analysis . </S>",
    "<S> one key component of the analysis is the intimate relationship between hct and fisher s separation . another key component is the tight large - deviation bounds for empirical processes for data with unconventional correlation structures , where graph theory on vertex coloring plays an important role .    , </S>"
  ]
}