{
  "article_text": [
    "in  @xcite , faugre and lachartre presented a specialized linear algebra for grbner basiscomputation ( gbreduction ) .",
    "the benefit of their approach is due to the very special structure the corresponding matrices have . using algorithms like the tasks of _ searching for reducers _ and _ reducing the input elements _ are isolated . in the so - called _ symbolic preprocessing _ ( see  @xcite ) all possible reducers for all terms of a predefined subset of currently available",
    "s - polynomials are collected . out of this data a matrix @xmath5 is generated whose rows correspond to the coefficients of the polynomials whereas the columns represent all appearing monomials sorted by the given monomial order on the polynomial ring .",
    "new elements for the ongoing grbner basiscomputation are computed via gaussian elimination of @xmath5 , the reduction process of several s - polynomials at once .",
    "@xmath5 always has a structure like presented in figure  [ fig : input - matrix ] , where black dots correspond to nonzero coefficients of the associated polynomials .",
    "faugre and lachartre s idea is to take advantage of @xmath5 s nearly in triangular shape , already before starting the reduction process .     for homogeneous katsura-12,scaledwidth=60.0% ]",
    "this is a system paper introducing in detail our new open source plain c parallel library gbla .",
    "this library includes efficient implementations not only of the gbreductionbut also new algorithmic improvements .",
    "here we present new ways of exploiting the underlying structure of the matrices , introducing new matrix storage formats and various attempts to improve the reduction process , especially for parallel computations .",
    "we discuss different experimental results showing the benefits of our new attempt .",
    "the paper is structured as follows : in section  [ sec : implementations ] we give an overview of the structure of our library .",
    "section  [ s : fmt ] discusses the special matrix structure and presents a new efficient storage format .",
    "this is important for testing and benchmarking purposes .",
    "we also recall the general process used for reducing these matrices . in section  [ s : seq ]",
    "we first review the main steps of the gbreduction .",
    "afterwards we propose improvements to the sequential algorithm by further exploiting patterns in the matrices .",
    "this is implemented in our new library by specialized data structures and a rearrangement of the order of steps of the gbreduction .",
    "section  [ s : par ] is dedicated to ideas for efficient parallelization of our library that also takes into account the improvements discussed beforehand . in section  [ sec : er ] we show gbla s efficiency by giving experimental results comparing it to several other specialized linear algebra implementations for grbner basiscomputations .",
    "our library is called gbla(grbner basislinear algebra ) and is the first plain _ c _ open source package available for specialized linear algebra in grbner basis - like computations .",
    "it is based on a first _ c++ _ implementation of fayssal martani in lela , which is a fork of linbox  @xcite and which is no longer actively developed .",
    "the sources of our library are hosted at : http://hpac.imag.fr / gbla/. under this website a database of our input matrices in different formats ( see section  [ s : fmt ] ) is available as well as the routines for converting matrices in our special format .",
    "the general structure of the library is presented in table  [ tab : gbla - description ] .",
    "input can come from files on disk or the standard input .",
    "the latter is especially useful because we can use a pipe form ` zcat ` and never uncompress the matrices to the disk .",
    "uncompressed , our library of matrices would reprensent hundreds of gigabytes of data .",
    "gblasupports the following data representations :    1 .",
    "the code is optimized for prime fields @xmath6 with @xmath7 using simd vectorization  @xcite and storing coefficients as ` uint16 ` .",
    "the library also supports a cofficient representation using ` float ` such that we can use the better optimized simd instructions for floating point arithmetic .",
    "@xmath1-bit floating points can be used for exact computations over @xmath6 with @xmath8 .",
    "3 .   there is also a version for @xmath1-bit field characteristic using ` uint32 ` data types that needs further optimization .",
    "note that whereas it is true that vectorization in cpus is faster for floating point arithmetic compared to exact one we show in section  [ sec : er ] that for @xmath9-bit computations memory usage can become a bottleneck : representing data via ` uint16 ` can make matrices manageable that are not feasible when using ` float ` data type .    for parallelization gblais based on openmp .",
    "current versions of xkaapican interpret openmpmacros , so one can also easily use gblawith xkaapias scheduler .    in order to assure cache locality we use blocks , all of them of dimension @xmath10 by default .",
    "the user has the freedom to set this to any power of @xmath11 , but in all of our experiments the preset size is advantageous due to l1 cache size limitations .    at the moment we have two different types of implementations of the usual gbreduction(see section  [ s : intro ] ) and",
    "the new order of operations for rank computations ( see section  [ sec : new - order ] ) each :    1 .",
    "the first type of implementations is completely based on the multiline data structure , denoted gbla - v0.1 .",
    "the second type is nearly always faster and denoted gbla - v0.2 in this paper .",
    "there we use multilines only in a very specific block situation where we can nearly guarantee in advance that they give a speedup due to cache locality .",
    "otherwise we use usual sparse resp .",
    "dense row representations that are advantageous when sorting rows by pivots .    note that gbla - v0.2is able to reduce matrices that gbla - v0.1cannot due to it s smaller memory footprint not introducing too many zeroes in multilines ( see also section  [ sec : er ] ) .",
    "input matrices in the gbreductionhave some nearly - triangular structure and patterns in the coefficients that we take advantage of .",
    "we describe gbmatrices and an efficient way to store them .",
    "matrices coming from grbner basiscomputations represent a set of polynomials in a polynomial ring w.r.t . to",
    "some given monomial order .",
    "this order sorts the columns of the matrix , each column represents a monomial .",
    "each row represents a polynomial whereas the entries are just the coefficients of the polynomial for the corresponding monomial in the appropriate column . due to this , gbmatrices are sparse .",
    "we can assume that the matrix has been sorted by weight ( number of non zero elements ) with row @xmath12 the heaviest .",
    "pivoting the rows corresponds to reordering of the polynomials ; permuting non pivot columns is allowed once before the gbreductionand re - done after the elimination steps .    the first non zero element on each row is a @xmath13 ( each polynomial is monic ) , and this element will be called _",
    "pivoting candidate_. every such pivoting candidate lies below the main diagonal .",
    "columns whose last non zero element is not a pivoting candidate can be permuted in order to separate them from the pivot ones .",
    "now , the first @xmath14 columns contain pivoting candidates , called _",
    "pivot columns_. among the pivoting candidates of a given column , one row is selected , the _ pivot row_. this selection tries to keep @xmath15 ( a @xmath16 matrix ) as sparse as possible .      in standard matrix market file format , gbmatrices are huge ( hundreds of gb ) and slow to read .",
    "we compress them to a csr - like ( compressed storage row ) format and store them in _ binary _ format ( streams of bytes rather than a text file ) .",
    "we propose two different formats , see table  [ tab : bin_formats ] .",
    "the files consist in consecutive sequences of elements of type _ size _ repeated _ length _ times .    in format  1",
    ", ` m ` , ` n ` , ` p ` , ` nnz ` represent the number of rows , columns , the modulo and the number of non zeros in the sparse matrix . `",
    "rows[i ] ` represents the length of the @xmath17row .",
    "if @xmath18 is the sum @xmath19}+\\dots+\\texttt{row[i-1]}$ ] , then on row @xmath17 , there is an element at column ` cols[j+r ] ` with value ` data[j+r ] ` for all @xmath20 in @xmath21}-1\\right\\}}$ ] . in format  2",
    ", we separate the location of the non zero entries and the data .",
    "we store the data of the polynomials separately since there is redundancy : many lines will be of the form @xmath22 where @xmath23 is some monomial and @xmath24 is a polynomial in the intermediate grbner basis .",
    "hence the coefficients in all lines of this type correspond to the same polynomial @xmath24 and represent the same data , only the location on the basis changes .",
    "we allow to store the data on different machine types to adapt to the size of @xmath25 .",
    "data is blocked , so we utilize the fact that several non zero elements on a row may be adjacents , allowing compression of their consecutive column numbers . in this format matrices",
    "must have less than @xmath26 rows .",
    "first , the lowest 3 bits of the first element ` b ` represent the value of ` x ` and ` y ` in ` xinty_t ` , namely @xmath27 where @xmath28 is @xmath13 iff the type is signed and @xmath29 corresponds to a type on @xmath30 bits ( for instance @xmath31 represents type . on the highest bits a mask is used to store a file format version .",
    "the @xmath32 row has ` rows[i ] ` elements .",
    "we prefer storing the row length since it fits on @xmath1 bits while pointers ( the accumulated row length ) would fit on @xmath33 bits .",
    "we compress the column indices : if @xmath34 several non zero elements are consecutive on a row and if @xmath35 is the first one , then we store @xmath35 @xmath36 in the format .",
    "if @xmath37 then we use a mask and store @xmath38 . here",
    "we lose a bit for the number of rows .",
    "so far , we have stored the locations of the non zero elements .",
    "the polynomial data on a row is stored in ` pdata ` in the following fashion .",
    "$ ] gives the @xmath32 polynomial number of elements ( its support ) .",
    "there are ` pnb ` polynomials .",
    "@xmath40}$ ] maps the polynomial number @xmath18 on row @xmath17 .",
    "the polynomial data is laid out contiguously in ` pdata ` , polynomial @xmath12 finishes at @xmath41}$ ] , polynomial @xmath13 finishes at @xmath41+prow[1]}$ ] , and so on .    in table",
    "[ tab : compress ] we show the raw size ( in gigabits ) of a few sparse matrices in their binary format , compressed with ( default options ) and the time it takes ( in seconds ) . compressing format 2 yields an @xmath42 time improvement on the original uncompressed binary format 1 storage and over @xmath4 times better than compressed format 1 , in a much shorter time . the compressed format 2 is hence much faster to load and it makes it easier to perform tests on .",
    "in this section we present new developments in the implementation of the gbreductionthat can be found in our library ( see section  [ sec : implementations ] ) .",
    "section  [ sec : multiline ] presents ideas to exploit the structure of the input gbmatrix @xmath5 further with dedicated data structures , and section  [ sec : new - order ] gives an alternative ordering of the steps of the gbreductionif a non - reduced row echelon form of @xmath5 is sufficient .",
    "there are @xmath4 main steps in the gbreduction :    1 .",
    "the input matrix @xmath5 already reveals a lot of its pivots even before the gaussian elimination starts . for exploiting this fact we rearrange the rows and the columns : we reach a cutting of @xmath5 into @xmath43 . after this rearrangement",
    "one can see @xmath4 different parts of @xmath5 : a very sparse , upper triangular unit matrix on the top left ( @xmath15 ) representing the already known pivots . a denser , but still sparse top right part ( @xmath44 ) of the same number of rows .",
    "moreover , there are two bottom parts , a left one which is still rather sparse ( @xmath45 ) and a right one , which tends to be denser ( @xmath46 ) . whereas @xmath15 represents already known leading terms in the intermediate grbner basis",
    ", @xmath46 corresponds to the new polynomials added to the basis after the reduction step .",
    "if @xmath5 is of dimensions @xmath47 and if @xmath14 denotes the number of known pivots the characteristics of the four splices of @xmath5 are given in table  [ tab : fl - splices ] . + in general , @xmath48 and @xmath49 .",
    "2 .   in the second step of the gbreductionthe known pivot rows",
    "are reduced with each other , we perform an trsm .",
    "algebraically , this is equivalent to computing @xmath50 .",
    "@xmath15 is invertible due to being upper triangular with @xmath13s on the diagonal . from an implementational point of view one only reads @xmath15 and writes to @xmath44 .",
    "after this step , we receive a representation of @xmath5 in the format @xmath51 .",
    "3 .   in the third step",
    ", we reduce @xmath45 to zero using the identity matrix @xmath52 performing axpy . doing this",
    "we also have to carry out the corresponding operations induced by @xmath44 on @xmath46 .",
    "we get @xmath53 4 .",
    "the fourth step now reveals the data we are searching for : via computing a gaussian elimination on @xmath54 we receive new pivots reaching an upper triangular matrix @xmath55 .",
    "those new pivots correspond to new leading terms in our grbner basis , thus the corresponding rows represent new polynomials to be added to the basis . on the other hand , rows reducing to zero correspond to zero reductions in the grbner basiscomputation . 5 .",
    "as the last step we rearrange the columns of the echelon form of @xmath5 and read off polynomials whose monomials are sorted correctly w.r.t . the monomial order .",
    "if one is interested in a reduced row echelon form of @xmath5 we have to perform the gbreductiona second time , but only on the right part @xmath56 . from the grbner basispoint of view a fully reduced row echelon form",
    "has the advantage that also the multiples of polynomials already in the basis , elements representing the rows @xmath57 are reduced .",
    "thus , reusing them in later reduction steps of the grbner basiscomputation can be beneficial ; we refer to section  @xmath58 in  @xcite discussing the ` simplify ` procedure .",
    "as already seen in section  [ sec : intro - fl ] , matrices coming from grbner basiscomputations are structured in a way that can be exploited for a specialized gaussian elimination . furthermore , there are even more patterns in such matrices that we use in order to speed up the computations . in figure",
    "[ fig : input - matrix ] we can see that the nonzero entries are , in general , grouped in blocks . in other words , if there is a nonzero element @xmath59 at position @xmath18 in row @xmath17 then also @xmath60 ( horizontal pattern ) and @xmath61 ( vertical pattern ) tend to be nonzero , too .",
    "this fact can be used , for example , to optimize the axpy trsm computations in the second step of the gbreductionas illustrated in figure  [ fig : multiline - illustration ] : assuming that @xmath62 and @xmath63 are both not zero ( horizontal pattern ) , element @xmath64 is updated by both nonzero elements @xmath65 and @xmath66 ( vertical pattern ) .",
    "whereas the horizontal patterns are canonically taken care ofstoring blocks row - wise , we have to pack the vertical pattern in a dedicated data structure .",
    "\\(m ) & & & & & & + & 1 & & * & * & * & * + & & 1 & & a_i , j & a_i , j+1 & * + & & & & & & + & & & & 1 & & * + & & & & & 1 & * + & & & & & & 1 + ; ; ( m-3-5.north west )  ( m-3-6.north east )  ( m-3-6.south east )  ( m-3-5.south west )  ( m-3-5.north west ) ; ( m-5-5.north west )  ( m-5-5.north east )  ( m-5-5.south east )  ( m-5-5.south west )  ( m-5-5.north west ) ; ( m-6-6.north west )  ( m-6-6.north east )  ( m-6-6.south east )  ( m-6-6.south west )  ( m-6-6.north west ) ; ( m-5-5.north )  ( m-3-5.south ) ; ( m-6-6.north )  ( m-3-6.south ) ;    \\(n ) & & & & & & + * & * & & * & & * & * + * & * & & b_i , & * & & * + & & & & & & + * & * & & b_k , & * & & * + * & * & & b_k+1 , & * & & * + * & * & & * & & * & * + ; ; ( n-3-4.north west )  ( n-3-4.north east )",
    " ( n-3-4.south east ) ",
    "( n-3-4.south west ) ",
    "( n-3-4.north west ) ; ( n-5-4.north west)+(-0.2,0 ) rectangle ( n-6-4.south east ) ; ( n-5-4.north )  ( n-3-4.south ) ;    ( m-5-5.east ) edge[out=15,in=165 ] ( n-5-4.west ) ; ( m-6-6.east ) edge[out=-15,in=-165 ] ( n-6-4.west ) ;    an _ @xmath67-multiline vector _",
    "@xmath68 is a data structure consisting of two vectors in a sparse representation :    1 .",
    "a position vector @xmath69 of column indices such that at each index at least one of @xmath67 rows of elements has a nonzero element .",
    "2 .   a value vector @xmath70 of entries of @xmath5 .",
    "the entries of all @xmath67 rows in column @xmath71 $ ] are stored consecutively , afterwards the @xmath67 entries at position @xmath72 $ ] are stored .",
    "note that @xmath70 may have zero elements .",
    "if @xmath69 has a length @xmath73 , @xmath70 has length @xmath74 .",
    "in this situation we say that @xmath68 has length @xmath73 . for a @xmath11-multiline vector",
    "we use the shorthand notation _",
    "multiline vector_.    consider the following two rows :    @xmath75{rcccccccccc } r_1 & = & [ & 2 & 0 & 0 & 1 & 0 & 0 & 5 & ] , \\\\",
    "r_2 & = & [ & 1 & 7 & 0 & 0 & 0 & 1 & 0 & ] .",
    "\\end{array } $ ]    a sparse representation is given by @xmath76 ( values ) and @xmath77 ( positions ) :    @xmath75{rcccccccccccccc } v_1 & = & [ & 2 & 1 & 5 & ] & \\;\\ ; & v_2 & = & [ & 1 & 7 & 1 & ] , \\\\",
    "p_1 & = & [ & 0 & 3 & 6 & ] & \\;\\ ; & p_2 & = & [ & 0 & 1 & 5 & ] .\\\\ \\end{array } $ ]    a @xmath11-multiline vector representation of @xmath78 and @xmath79 is given by    @xmath75{lccccccccccccccc } { \\ensuremath{\\text{\\tt ml}}}.{\\ensuremath{\\text{\\tt val } } } & = & [ & 2 & 1 & \\color{myredd}{0 } & 7 & 1 & \\color{myredd}{0 } & \\color{myredd}{0 } & 1 & 5 & \\color{myredd}{0 } & ] , \\\\",
    "{ \\ensuremath{\\text{\\tt ml}}}.{\\ensuremath{\\text{\\tt pos } } } & = & [ & 0 & 1 & 3 & 5 & 6 & ] .\\\\",
    "\\end{array } $ ]    four zero values are added to @xmath80 , two from @xmath78 and @xmath79 resp .",
    "we do not add column @xmath11 since there both , @xmath78 and @xmath79 have zero entries .",
    "multiline vectors are especially useful when performing axpy . in the following we use multiline vectors to illustrate the reduction of two temporarily dense rows @xmath81 and @xmath82 with one multiline vector @xmath68 of length @xmath73 . for the entries in @xmath80 two situations",
    "are possible : either there is only one of @xmath83 $ ] and @xmath84 $ ] nonzero , or both are nonzero . due to the vertical pattern of gbmatrices",
    "very often both entries are nonzero .",
    "we can perform a specialized @xmath85 operation on @xmath81 and @xmath82 with scalars @xmath86 coming from column @xmath18 and @xmath87 from column @xmath88 where @xmath18 is the loop step in the corresponding @xmath89 opteration :    @xmath81 , @xmath82 , @xmath90 , @xmath91 , @xmath92 , @xmath93 , @xmath68 .",
    "@xmath94 , @xmath95 , @xmath17 , @xmath96 @xmath97 $ ] @xmath98 $ ] @xmath99 $ ] @xmath100 \\gets \\lambda_{1,1 } v_1 + \\lambda_{1,2 } v_2 $ ] @xmath101 \\gets \\lambda_{2,1 } v_1 + \\lambda_{2,2 } v_2 $ ]    the benefit of algorithm  [ alg : ml - axpy ] is clear : we perform @xmath4 reductions ( each dense row is reduced by two rows ) in one step . on the other hand , if the horizontal pattern does not lead to two successive nonzero entries ( for example if @xmath102 is zero in figure  [ fig : multiline - illustration ] ) , then algorithm  [ alg : ml - axpy ] would not use @xmath84 $ ] .",
    "this would introduce an disadvantage due to using only every other element of @xmath80 . in our implementation",
    "we take care of this situation and have a specialized @xmath85 implementation for that .",
    "still , we are performing two reductions ( each dense row is reduced by one row ) in one step .",
    "assuming general @xmath67-multiline vectors the problem of introducing useless operations on zero elements appears .",
    "for multiline vectors , @xmath103 , we can perform lightweight tests before the actual loop to ensure execution only on nonzero @xmath104 ( for single @xmath85 ) @xmath105 ( for algorithm  [ alg : ml - axpy ] ) . for general @xmath67",
    "we can not predict every possible configuration of the @xmath106 scalars @xmath107 . moreover , for @xmath67-multiline vectors the memory overhead can get problematic , too . for @xmath103 we can lose at most @xmath108 bytes per column index ,",
    "but for arbitrary @xmath67 this increases to @xmath109 bytes .",
    "all in all , we note the following fact that is also based on practical experimental results .",
    "based on cache efficiency as well as memory overhead due to adding zero entries to the @xmath110 vector @xmath11-multiline vector data structures are the most efficient .    as already mentioned in  @xcite ,",
    "representing the matrices @xmath15 , @xmath44 , @xmath45 and @xmath46 in blocks has several benefits : firstly , we can pack data in small blocks that fit into cache and thus we increase spatial and temporal locality . secondly , separating the data into column blocks we can perform operations on @xmath44 and @xmath46 rather naturally in parallel .",
    "thus we are combining the multiline vector data structure with a block representation in our implementation . in the following ,",
    "presented pseudo code is independent of the corresponding row block representation , standard row representation is used .",
    "multiline representations impede the readability of the algorithms , if there is an impact on switching to multilines , we point this out in the text .    using multilines is useful in situations where we can predict horizontal _ and _ vertical patterns with a high probability , in order to see advantages and drawbacks we have two different implementations , gbla - v0.1and gbla - v0.2 , which use multilines in different ways ( see also section  [ sec : implementations ] ) .      if the number of initially known pivots ( the number of rows of @xmath15 and @xmath44 ) is large compared to the number of rows of @xmath45 and @xmath46 , then most work of the gbreductionis spent in reducing @xmath15 , the trsmstep @xmath111 .",
    "for the grbner basisthe new information for updating the basis is strictly in @xmath46 .",
    "thus , if we are not required to compute a reduced echelon form of the input matrix @xmath5 , but if we are only interested in the reduction of @xmath46 the rank of @xmath5 we can omit the trsmstep .",
    "whereas in  @xcite the original gbreductionremoves nonzero entries above ( @xmath112 ) and below ( deleting @xmath45 ) the known pivots , it is enough to reduce elements below the pivots .",
    "thus , after splicing the input matrix @xmath5 of dimension @xmath47 we can directly reduce @xmath45 with @xmath15 while reflecting the corresponding operations with @xmath44 on @xmath46 with the following steps .",
    "submatrices @xmath113 , @xmath114 , @xmath115 , @xmath116 .",
    "@xmath117 , @xmath118 , @xmath17 , @xmath18 @xmath119)$ ] @xmath120)$ ] @xmath121 , a[j,*]\\right)$ ] @xmath122 , b[j,*]\\right)$ ] @xmath123 \\gets \\textrm{copy\\_dense\\_row\\_to\\_sparse}({\\ensuremath{\\text{dense}}}_d)$ ]    whereas algorithm  [ alg : new - red - order ] describes the idea of reducing @xmath45 and @xmath46 from a mathematical point of view , in practice one would want to use a block representation for the data in order to improve cache locality and also parallelization .",
    "strangely , it turned out that this is not optimal for efficient computations : in order for a block representation to make sense one needs to completely reduce all rows multilines in a given block before reducing the next block .",
    "that is not a problem for @xmath44 and @xmath46 since their blocks do not depend on the columns , but it is disadvantageous for @xmath15 and @xmath45 . assuming an operation on a lefthand side block of @xmath45 due to a reduction from a block from @xmath15 .",
    "any row operation on @xmath45 must be carried out through all blocks on the right .",
    "even worse , whenever we would try to handle @xmath45 per row multiline and @xmath46 per block at the same time this would introduce a lot of writing to @xmath46 .",
    "thus , in our implementation we found the most efficient solution to be the following :    1 .",
    "store @xmath15 and @xmath45 in multiline representation and @xmath44 and @xmath46 in block multiline representation as defined in section  [ sec : multiline ] .",
    "2 .   carry out the reduction of @xmath45 by @xmath15 , but store the corresponding coefficients needed for the reduction of @xmath46 by @xmath44 later on .",
    "3 .   transform @xmath45 to block multiline representation @xmath124 .",
    "4 .   reduce @xmath46 by @xmath44 using thecoefficients stored in @xmath124 .",
    "thus we have an optimal reduction of @xmath45 and an optimal reduction of @xmath46 .",
    "the only overhead we have to pay for this is the transformation from @xmath45 to @xmath124 . but copying @xmath45 into block format is negligible compared to the reduction operations done in @xmath45 and @xmath46 .    in section  [ sec : er ] we see that this new order of operations is faster than the standard gbreductionfor full rank matrices from .",
    "the density of the row echelon form of @xmath5 does not vary too much from @xmath5 s initial density which leads in less memory footprint .",
    "computing the row echelon form of @xmath46 the original fl implementationused a sequential structured gaussian elimination . here",
    "we use a modified variant that can be easily parallelized .",
    "submatrix @xmath116 .",
    "@xmath125 , rank of @xmath46 @xmath118 , @xmath17 , @xmath18 @xmath126 @xmath127)$ ] @xmath120)$ ] @xmath128\\right)$ ] @xmath123 \\gets \\textrm{copy\\_dense\\_row\\_to\\_sparse}({\\ensuremath{\\text{dense}}}_d)$ ] @xmath127)$ ] @xmath129 @xmath125    in algorithm  [ alg : sge ] we do a structured gaussian elimination on the rows of @xmath46 . note that @xmath46 is not a unitary matrix , so normalization and inversions are required . at the very end",
    "the rank of @xmath46 is returned .",
    "the modification lies mainly in the fact that once we have found a new pivot row , we do not sort the list of known pivot rows , but just add the new one .",
    "this is due to the usage of multilines in our implementation . storing two ( or more ) rows in this packed format",
    "it is inefficient to sort pivots by column index .",
    "possibly we would need to open a multiline row and move the second row to another position .",
    "for this , all intermediate multiline rows need to be recalculated .",
    "thus we decided to relinquish the sorting at this point of the computation and sort later on when reconstructing the row echelon form of the initial matrix @xmath5 .",
    "note that whereas we use multilines everywhere in gbla - v0.1 , in gbla - v0.2(see section  [ sec : er ] ) we restrict the usage of multilines to specific block situations and no longer use them for the dense gaussian elimination of @xmath46 .",
    "thus we are able to perform a sorting of the pivots .",
    "in this section we discuss improvements concerning parallelizing the gbreductiontaking the new ideas presented in the last section into account . for this",
    "we have experimented with different parallel schedulers such as openmp , xkaapiand pthreads . moreover , whereas the initial implementation of faugre and lachartre used a sequential gaussian elimination of @xmath46 we are now able to use a parallel version of algorithm  [ alg : sge ] .      as already discussed in section  [ sec : sge ] we use a modified structured gaussian elimination for multilines which omits sorting the list of known pivots , postponed to the reconstruction of the echelon form of the input matrix @xmath5 . in our library gblathere is also a non - multiline version with sorting , see section  [ sec : er ] for more information .    assuming that we have already found @xmath96 pivots in algorithm  [ alg : sge ] , we are able to reduce several rows of index @xmath130 in parallel .",
    "the @xmath96 pivots are already in their normalized form , they are readonly , thus we can easily update @xmath131 \\gets d[\\ell , * ] + \\sum_{i=0}^k \\lambda_i d[i , * ] $ ] for all @xmath132 and corresponding multiples @xmath133 . clearly , this introduces some bookkeeping : whereas in the above situation @xmath134 $ ] is fully reduced with the @xmath96 known pivots , the rows @xmath135 $ ] for @xmath136 are not .",
    "thus we can add @xmath134 $ ] to the list of known pivots , but not @xmath135 $ ] .",
    "we handle this by using a global waiting list @xmath137 which keeps the rows not fully reduced and the indices of the last pivot row up to which we have already updated the corresponding row .",
    "different threads share a global variable @xmath138 : the last known pivot .",
    "each thread performs the following operations :    1 .",
    "[ step : fetch ] fetch the next available row @xmath139 $ ] which was not updated up to this point or which is already in the waiting list @xmath137 .",
    "2 .   reduce it with all pivots not applied until now , up to @xmath138 .",
    "[ step : add - piv ] if @xmath140 , @xmath139 $ ] is a new pivot and @xmath138 is incremented .",
    "[ step : add - waiting ] if @xmath141 , @xmath139 $ ] is added to @xmath137 keeping track that @xmath138 is the index of the last row @xmath139 $ ] is already reduced with .",
    "naturally , the above description leaves some freedom for the decision which row to fetch and reduce next in step  [ step : fetch ] .",
    "we found the following choice to be the most efficient for a wide range of examples : when a thread fetches a row to be further reduced it prefers a row that was already previously reduced .",
    "this often leads to a faster recognition of new known pivots in step  [ step : add - piv ] .",
    "synchronization is needed in steps  [ step : add - piv ] and  [ step : add - waiting ] , besides this the threads can work independent of each other .",
    "we handle the communication between the threads using spin locks whose implementation w.r.t .",
    "a given used different parallel scheduler ( see section  [ sec : schedulers ] ) might differ slightly .",
    "talking about load balancing it can happen that one thread gets stuck in reducing already earlier reduced rows further , whereas other threads fetch pristine rows and fill up @xmath137 more and more . in order to avoid this",
    "we use the following techniques :    * if a thread has just fully reduced a row @xmath20 and thus adds a new known pivot , this thread prefers to take an already reduced row from @xmath137 possibly waiting for @xmath20 to become a known pivot . *",
    "if a thread has added @xmath142 new rows to @xmath137 consecutively , it is triggered to further reduce elements from @xmath137 instead of starting with until now untouched rows from @xmath46 .    for efficiency reasons we do not directly start with the discussed parallel elimination , but we do a sequential elimination on the first @xmath96 rows multiline rows of @xmath46 . in this way we can avoid high increasing on the waiting list @xmath137 at the beginning , which would lead to tasks too small to benefit from the available number of cores executing in parallel . thus @xmath96 depends on the number of threads used , in practice we found that @xmath143 is a good choice .",
    "clearly , the efficiency of this choice depends on how many of the first @xmath96 rows multiline rows of @xmath46 reduce to zero in this step .",
    "this is not a problem for full rank matrices coming from grbner basiscomputations .",
    "we did some research on which parallel schedulers to be used in our library .",
    "for this we tested not only well known schedulers like openmp  @xcite and intel tbb  @xcite but also xkaapi  @xcite and starpu  @xcite .",
    "we also did experiments with pthreadsand own implementations for scheduling .",
    "most of the schedulers have advantages and disadvantages in different situations like depending on sparsity , blocksizes or relying on locking for the structured gaussian elimination .",
    "moreover , all those packages are actively developed and further improved , thus we realized different behavoiour for different versions of the same scheduler . in the end we decided to choose openmpfor the current state of the library .    1 .",
    "it is in different situations usually not the fastest scheduler , but often tends to be the fastest for the overall computation .",
    "2 .   our library should be plain c as much as possible , thus we discarded the usage of intel tbbwhich is based on high - level c++ features for optimal usage .",
    "current versions of xkaapiare able to interpret openmppragmas .",
    "thus one can use our library together with xkaapiby changing the linker call : instead of libgomp one has to link against libkomp ( see also section  [ sec : er ] ) .",
    "4 .   using pthreadsnatively is error - prone and leads to code that is not portable ( it is not trivial to get them work on windows machines ) .",
    "openmp s locking mechanism boils down to pthreadson unix and their pendants on windows without having to deal with different implementations .",
    "starpu s performance depends highly on the used data structures . since the representation of our data is special ( see sections  [ s : fmt ] and  [ s : seq ] )",
    "we need further investigations on how to get data and scheduler playing together efficiently .",
    "moreover , the fact that starpucan be used for task scheduling even on heterogeneous multicore architectures like cpu / gpu combinations makes it a good candidate for further experiments .",
    "the following experiments were performed on http://hpac.imag.fr/ which is a numa architecture of 4x8 processors .",
    "each of the 32 non hyper - threaded intel(r ) xeon(r ) cpus cores clocks at 2.20ghz ( maximal turbo frequency on single core 2.60ghz ) .",
    "each of the 4 nodes has 96 gb of memory , so we have 384 gb of ram in total . the compiler is ` gcc-4.9.2 ` .",
    "the timings do not include the time spent on reading the files from disk .",
    "we state matrix characteristics of our example set in table  [ tab : expe : carac ] .",
    "we use various example sets : there are well known benchmarks like ` katsura ` , ` eco ` and ` cyclic ` , see  @xcite . ] .",
    "moreover , we use matrices from minrank problems arising in cryptography .",
    "furthermore we have random dense systems ` randx - d2-y - mat * ` in ` x ` variables , all input polynomials are of degree @xmath11 .",
    "then we deleted ` y ` polynomials to achieve higher - dimensional benchmarks .",
    "all examples are done over the biggest @xmath9-bit prime field , @xmath144 .",
    "we use the ` uint16 ` coefficient representation in gbla .",
    "if not otherwise stated gbla s timings are done using openmpas parallel scheduler .",
    "we show in table  [ tab : expe : fl ] a comparison with faugre and lachartre s fl implementationfrom @xcite and gbla .",
    "timings are in seconds , using 1 , 16 or 32 threads .",
    "this is done for matrices , thus we can use gbla s new order of operations ( see section  [ sec : new - order ] ) to compute a echelon form and to verify that the matrices have full rank .",
    "@xmath145    usually gbla - v0.1is faster on one core than fl implementation , gbla - v0.2is even faster than gbla - v0.1 . both gblaimplementations have a much better scaling than fl implementation , where gbla - v0.2preforms better than gbla - v0.1 , even scaling rather good for smaller examples , where the overhead of scheduling different threads starts to become a bottleneck .",
    "the only example where fl implementationis faster than gblais ` mr-9 - 10 - 7-mat3 ` , a very dense ( @xmath146 ) matrix .",
    "this good behaviour for fl implementationmight be triggered from the fact that fl implementationallocates all the memory needed for the computation in advance .",
    "usually the user does not know how much memory the computation might need , so this approach is a bit error - prone .",
    "still , fl implementationis faster than gblaonly on one core , starting to use several cpu cores the better scaling of gblawins ( already at @xmath11 cores the timings are nearly identical ) . moreover , for dense matrices like the minrank ones we can see a benefit of the multiline structure , at least for fewer cores .",
    "once the number of cores increases the better scaling of gbla - v0.2is favourable .    for `",
    "the speedup between @xmath9 and @xmath1 is quite small . due to the applied symmetry",
    "the matrix is already nearly reduced , so the scheduling overhead has a higher impact than the gain during reduction for anything greater than @xmath9 cores .    for the higher - dimensional random examples the row dimension of @xmath45 and @xmath46 is very small ( @xmath147 ) .",
    "our new order of operations ( section  [ sec : new - order ] ) enables gblato reduce matrices the fl implementationis not able to handle .",
    "even gbla - v0.2reaches for ` rand18-d2 - 9-mat7 ` the memory limit of the machine , but it is still able to reduce the matrix .",
    "memory overhead due to multilines hinders gbla - v0.1to compute ` rand16-d2 - 3-mat11 ` , but is more efficient on ` rand16-d2 - 3-mat10 ` for one core .      in table",
    "[ tab : expe : gbla ] we compare magma-2.19  @xcite to gbla . since there is no implementation in magmawe can only compare matrices coming from computations .",
    "since magmais closed source we are not able to access the specialized linear algebra for grbner basiscomputations directly .",
    "thus , we are comparing the same problem sets with the same degrees running magma s implementation .",
    "note that magmagenerates matrices that are , for the same problem and same degree , slightly larger , usually @xmath148 to @xmath149% .",
    "note that we use only magma s cpu implementation of , but not the rather new gpu one .",
    "we think that it is not really useful to compare gpu and cpu parallelized code . furthermore , most of our examples are too big to fit into the ram of a gpu , so data copying between cpu and gpu might be problematic for an accurate comparison .    for small examples",
    "magma , not splicing the matrices , has an advantage .",
    "but already for examples in the range of @xmath149 seconds gbla , especially gbla - v0.2gets faster on single core .",
    "the difference between magmaand gbla - v0.1is rather small , whereas gbla - v0.2becomes more than twice as fast .",
    "moreover , gbla - v0.1and gbla - v0.2scale very well on @xmath9 and @xmath1 cores . due to lack of space",
    "we do not state timings for the fl implementation .",
    "it behaves in nearly all examples like expected : due to preallocation of all memory it is very fast on sequential computations ( nearly as fast as gbla - v0.2 ) , but it scales rather bad .",
    "for example , for ` kat14-mat8 ` fl implementationruns in @xmath150s , @xmath151s and @xmath152s for @xmath13 , @xmath9 and @xmath1 cores , respectively . also note that fl implementation s memory consumption is higher than gbla s .",
    "for the random , higher dimensional examples magmacannot reduce matrices starting from ` rand16-d2 - 3-mat9 ` due to the ` float ` representation of the matrix entries and resulting higher memory usage on the given machine . for ` rand16-d2 - 3-mat11 ` even gbla - v0.1consumes too much memory by using multilines and thus introducing too many zeros ( see section  [ sec : multiline ] ) .",
    "even gbla - v0.2comes to the limit of our chosen compute server , but it can still reduce the matrix : at the end of the computation the process consumed @xmath153 of the machine s ram .",
    "we compare the different behaviour of the parallel schedulers that can be used in gbla(see also section  [ sec : schedulers ] ) : the default scheduler in gblais omp , here we use the latest stable version @xmath154 .",
    "xkcan interpret omppragmas , too , so we are able to run gblawith xkby just changing the linker call from ` libgomp ` to ` libkomp ` .",
    "the latest stable version of xkwe use is @xmath155 . in table",
    "[ tab : gbla - omp - kaapi ] we compare both schedulers on representative benchmarks in gbla - v0.1and gbla - v0.2on @xmath9 and @xmath1 cores .",
    "the timings show that in many examples both schedulers are on par .",
    "xktends to be a bit more efficient on @xmath1 cores , but that is not always the case . `",
    "f4-kat15-mat8 ` and ` f4-kat15-mat9 ` are cases where xkhas problems on @xmath1 cores for gbla - v0.1 .",
    "this comes from the last step , the structured gaussian elimination of @xmath46 where gbla - v0.1 , using multilines , can not sort the pivots which seems to become a bottleneck for xk s scheduling . for the same examples in gbla - v0.2(now with sorting of pivots )",
    "we see that xkis even a bit faster than omp .",
    "all in all , in our setting both schedulers behave nearly equal .",
    "we presented the first open - source , plain c library for linear algebra specialized for matrices coming from grbner basiscomputations including various new ideas exploiting underlying structures .",
    "this led to more efficient ways of performing the gbreductionand improved parallel scaling .",
    "moreover , the library uses a new compressed file format that enables us to generate matrices not feasible beforehand .",
    "corresponding routines for dumping and converting own matrices are included such that researchers are able to use their own data in our new format in gbla .    also the time needed to reduce @xmath46 during gbreductionis in general very small",
    "compared to the overall reduction , we plan to investigate our parallel structured gaussian elimination implementation in the future .",
    "for this we may again copy @xmath55 first to a different data representation and use external libraries for fast exact linear algebra such as fflas - ffpack  @xcite in given situations .",
    "b.  boyer , j .-",
    "dumas , p.  giorgi , c.  pernet , and b.  saunders . elements of design for containers and solutions in the linbox library . in h.  hong and c.  yap , editors , _ mathematical software ",
    "icms 2014 _ , volume 8592 of _ lecture notes in computer science _ , pages 654662 .",
    "springer berlin heidelberg , 2014 .",
    "j.  dumas , t.  gautier , c.  pernet , and z.  sultan .",
    "parallel computation of echelon forms . in f.",
    "m.  a. silva , i.  de  castro  dutra , and v.  s. costa , editors , _ euro - par 2014 parallel processing - 20th international conference , porto , portugal , august 25 - 29 , 2014 .",
    "proceedings _ ,",
    "volume 8632 of _ lecture notes in computer science _ , pages 499510 .",
    "springer , 2014 .",
    "dumas , t.  gautier , m.  giesbrecht , p.  giorgi , b.  hovinen , e.  l. kaltofen , b.  d. saunders , w.  j. turner , and g.  villard .",
    "linbox : a generic library for exact linear algebra . in _ proceedings of the 2002 international congress of mathematical software , beijing , china_. world scientific pub , aug .",
    "2002 .",
    "faugre and s.  lachartre . .",
    "in m.  moreno - maza and j.  l. roch , editors , _ proceedings of the 4th international workshop on parallel and symbolic computation _ , pasco 10 , pages 8997 , new york , ny , usa , july 2010 .",
    "faugre and s.  lachartre . .",
    "in m.  moreno - maza and j.  l. roch , editors , _ proceedings of the 4th international workshop on parallel and symbolic computation _ , pasco 10 , pages 8997 , new york , ny , usa , july 2010 .",
    "t.  gautier , x.  besseron , and l.  pigeon .",
    "kaapi : a thread scheduling runtime system for data flow computations on cluster of multi - processors . in _ proceedings of the 2007 international workshop on parallel symbolic computation _ , pasco 07 , pages 1523 , new york , ny , usa , 2007 ."
  ],
  "abstract_text": [
    "<S> this is a system paper about a new gplv2 open source c library gblaimplementing and improving the idea  @xcite of faugre and lachartre ( gbreduction ) . </S>",
    "<S> we further exploit underlying structures in matrices generated during grbner basiscomputations in algorithms like or taking advantage of block patterns by using a special data structure called _ </S>",
    "<S> multilines_. moreover , we discuss a new order of operations for the reduction process . in various different experimental results </S>",
    "<S> we show that gblaperforms better than gbreductionor magmain sequential computations ( up to @xmath0 faster ) and scales much better than gbreductionfor a higher number of cores : on @xmath1 cores we reach a scaling of up to @xmath2 . </S>",
    "<S> gblais up to @xmath3 times faster than gbreduction . </S>",
    "<S> further , we compare different parallel schedulers gblacan be used with . </S>",
    "<S> we also developed a new advanced storage format that exploits the fact that our matrices are coming from grbner basiscomputations , shrinking storage by a factor of up to @xmath4 . </S>",
    "<S> a huge database of our matrices is freely available with gbla . </S>"
  ]
}