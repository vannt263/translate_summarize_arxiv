{
  "article_text": [
    "it is common to use a carefully chosen representation of the problem at hand as a basis for machine learning  @xcite .",
    "for example , molecules can be represented as coulomb matrices  @xcite , scattering transforms  @xcite , bags of bonds ( bob )  @xcite , smooth overlap of atomic positions ( soap )  @xcite , or generalized symmetry functions  @xcite .",
    "kernel - based learning of molecular properties transforms these representations non - linearly by virtue of kernel functions .",
    "in contrast , deep neural networks  @xcite are able to infer the underlying regularities and learn an efficient representation in a layer - wise fashion  @xcite .",
    "molecular properties are governed by the laws of quantum mechanics , which yield the remarkable flexibility of chemical systems , but also impose constraints on the behavior of bonding in molecules .",
    "the approach presented here utilizes the many - body hamiltonian concept for the construction of the dtnn architecture ( see fig .  1 ) , embracing the principles of quantum chemistry , while maintaining the full flexibility of a complex data - driven learning machine .",
    "dtnn receives molecular structures through a vector of nuclear charges @xmath0 and a matrix of atomic distances @xmath1 ensuring rotational and translational invariance by construction ( fig .",
    "the distances are expanded in a gaussian basis , yielding a feature vector @xmath2 , which accounts for the different nature of interactions at various distance regimes .",
    "the total energy @xmath3 for the molecule @xmath4 composed of @xmath5 atoms is written as a sum over @xmath5 atomic energy contributions @xmath6 , thus satisfying permutational invariance with respect to atom indexing .",
    "each atom @xmath7 is represented by a coefficient vector @xmath8 , where @xmath9 is the number of basis functions , or features .",
    "motivated by quantum - chemical atomic basis set expansions , we assign an atom type - specific descriptor vector @xmath10 to these coefficients @xmath11 .",
    "subsequently , this atomic expansion is repeatedly refined by pairwise interactions with the surrounding atoms @xmath12 where the interaction term @xmath13 reflects the influence of atom @xmath14 at a distance @xmath15 on atom @xmath7 .",
    "note that this refinement step is seamlessly integrated into the architecture of the molecular dtnn , and is therefore adapted throughout the learning process . from the point of view of many - body interatomic interactions , subsequent refinement steps",
    "@xmath16 correlate atomic neighborhoods with increasing complexity . considering a molecule as a graph ,",
    "@xmath17 refinements of the coefficient vectors are comprised of all walks of length @xmath17 through the molecule ending at the corresponding atom  @xcite .",
    "non - linear coupling between the atomic vector features and the interatomic distances is achieved by a tensor layer  @xcite , such that the coefficient @xmath18 of the refinement is given by @xmath19 where @xmath20 is the bias of feature @xmath18 and @xmath21 and @xmath22 are the weights of atom representation and distance , respectively .",
    "the slice @xmath23 of the parameter tensor @xmath24 combines the inputs multiplicatively . since @xmath25 incorporates many parameters",
    ", using this kind of layer is both computationally expensive as well as prone to overfitting .",
    "therefore , we employ a low - rank tensor factorization , as described in @xcite , such that @xmath26,\\ ] ] where @xmath27 represents element - wise multiplication while @xmath28 , @xmath29 , @xmath30 , @xmath31 and @xmath32 are the weight matrices and corresponding biases of atom representations , distances and resulting factors , respectively . as the dimensionality of @xmath33 and @xmath34 corresponds to the number of factors , choosing only a few drastically decreases the number of parameters , thus solving both issues of the tensor layer at once .",
    "arriving at the final embedding after a given number of interaction refinements , two fully - connected layers predict an energy contribution from each atomic coefficient vector , such that their sum corresponds to the total molecular energy @xmath3 .",
    "therefore , the dtnn architecture scales with the number of atoms in a molecule , fully capturing the extensive nature of the energy .",
    "all weights , biases as well as the atom type - specific descriptors were initialized randomly and trained using stochastic gradient descent  @xcite .",
    "to demonstrate the versatility of the proposed dtnn , we train models with up to three interaction passes @xmath35 for both compositional and configurational degrees of freedom in molecular systems .",
    "the dtnn accuracy saturates at @xmath35 , and leads to a strong correlation between atoms in molecules , as can be visualized by the complexity of the potential learned by the network ( see fig .",
    "1e ) . for training",
    ", we employ chemically diverse datasets of equilibrium molecular structures , as well as molecular dynamics ( md ) trajectories for small molecules  @xcite .",
    "we employ two subsets of the gdb-13 database  @xcite referred to as gdb-7 , including more than 7,000 molecules with up to 7 heavy ( c , n , o , f ) atoms , and gdb-9 , consisting of 129,000 molecules with up to 9 heavy atoms  @xcite . in both cases , the learning task is to predict the molecular total energy calculated with density - functional theory ( dft ) .",
    "all gdb molecules are stable and synthetically accessible according to organic chemistry rules  @xcite .",
    "molecular features such as functional groups or signatures include single , double and triple bonds ; ( hetero- ) cycles , carboxy , cyanide , amide , amine , alcohol , epoxy , sulfide , ether , ester , chloride , aliphatic and aromatic groups . for each of the many possible stoichiometries ,",
    "many constitutional isomers are considered , each being represented only by a low - energy conformational isomer .",
    "as table s2 demonstrates , dtnn achieves a mean absolute error ( maes ) of 1.0 kcal / mol on both gdb datasets , training on 5.8k gdb-7 ( 80% ) and 25k ( 20% ) gdb-9 reference calculations , respectively  @xcite .",
    "1c shows the performance on gdb-9 depending on the size of the molecule .",
    "we observe that larger molecules have lower errors because of their abundance in the training data .",
    "additionally , we trained the network on a restricted subset of 5k molecules with more than 20 atoms . by adding smaller molecules to the training set , we are able to reduce the test error from 2.1 kcal / mol to less than 1.5 kcal / mol ( see inset in fig",
    "this result demonstrates that our model is able to transfer knowledge learned from small molecules to larger molecules with diverse functional groups .    while only encompassing conformations of a single molecule , reproducing md simulation trajectories poses a radically different challenge to predicting energies of purely equilibrium structures .",
    "we learned potential energies for md trajectories of benzene , toluene , malonaldehyde and salicylic acid , carried out at a rather high temperature of 500 k to achieve exhaustive exploration of the potential - energy surface of such small molecules .",
    "the neural network yields mean absolute errors of 0.05 kcal / mol , 0.18 kcal / mol , 0.17 kcal / mol and 0.39 kcal / mol for these molecules , respectively ( see table  s2 ) .",
    "1d shows the excellent agreement between the dft and dtnn md trajectory of toluene as well as the corresponding energy distributions .",
    "the dtnn errors are much smaller than the energy of thermal fluctuations at room temperature ( @xmath360.6 kcal / mol ) , meaning that dtnn potential - energy surfaces can be utilized to calculate accurate molecular thermodynamic properties by virtue of monte carlo simulations .    the ability of dtnn to accurately describe equilibrium structures within the gdb-9 database and md trajectories of selected molecules of chemical relevance demonstrates the feasibility of developing a universal machine learning model that can capture both compositional and configurational degrees of freedom in the vast chemical space . while the employed architecture of the dtnn is universal , the learned coefficients are different for gdb-9 and md trajectories of single molecules .     for @xmath37 atoms for benzene , toluene , salicylic acid , and malondehyde .",
    "* the isosurface was generated for @xmath38 = 3.8 @xmath39 ( the index @xmath7 is used to sum over all atoms of the corresponding molecule ) . , scaledwidth=48.0% ]        o@xmath40h@xmath41 .",
    "* dtnn trained on the gdb-9 database is able to acurately discriminate between 6095 different isomers of c@xmath42o@xmath40h@xmath41 , which exhibit a non - trivial spectrum of relative energies .",
    ", scaledwidth=48.0% ]",
    "beyond predicting accurate energies , the true power of dtnn lies in its ability to provide novel quantum - chemical insights . in the context of dtnn",
    ", we define a _ local chemical potential _ @xmath43 as an energy of a certain atom type @xmath44 , located at a position @xmath45 in the molecule @xmath4 . while the dtnn models the interatomic interactions , we only allow the atoms of the molecule act on the probe atom , while the probe does not influence the molecule  @xcite . the spatial and chemical sensitivity provided by our dtnn approach",
    "is shown in fig .",
    "1e for a variety of fundamental molecular building blocks . in this case",
    ", we employed hydrogen as a test charge , while the results for @xmath46 are shown in fig .  2 .",
    "despite being trained only on total energies of molecules , the dtnn approach clearly grasps fundamental chemical concepts such as bond saturation and different degrees of aromaticity . for example",
    ", the dtnn model predicts the c@xmath47o@xmath48h@xmath47 molecule to be `` more aromatic '' than benzene or toluene ( see fig .",
    "remarkably , it turns out that c@xmath47o@xmath48h@xmath47 does have higher ring stability than both benzene and toluene and dtnn predicts it to be the molecule with the most stable aromatic carbon ring among _ all _ molecules in the gdb-9 database ( see fig .  3 ) .",
    "further chemical effects learned by the dtnn model are shown in fig .  2 that demonstrates the differences in the chemical potential distribution of h , c , n , and o atoms in benzene , toluene , salicylic acid , and malonaldehyde .",
    "for example , the chemical potentials of different atoms over an aromatic ring are qualitatively different for h , c , n , and o atoms  an evident fact for a trained chemist . however , the subtle chemical differences described by dtnn are accompanied by chemically accurate predictions  a challenging task for humans .",
    "because dtnn provides atomic energies by construction , it allows us to classify molecules by the stability of different building blocks , for example aromatic rings or methyl groups .",
    "an example of such classification is shown in fig .  3 , where we plot the molecules with most stable and least stable carbon aromatic rings in gdb-9 . the distribution of atomic energies is shown in fig .",
    "s4 , while fig .",
    "s5 lists the full stability ranking .",
    "the dtnn classification leads to interesting stability trends , notwithstanding the intrinsic non - uniqueness of atomic energy partitioning .",
    "however , unlike atomic projections employed in electronic - structure calculations , the dtnn approach has a firm foundation in statistical learning theory . in quantum - chemical calculations , every molecule would correspond to a different partitioning depending on its self - consistent electron density .",
    "in contrast , the dtnn approach learns the partitioning on a large molecular dataset , generating a transferable and global `` dressed atom '' representation of molecules in chemical space . recalling that dtnn exhibits errors below 1 kcal / mol , the classification shown in fig .",
    "3 can provide useful guidance for the chemical discovery of molecules with desired properties .",
    "analytical gradients of the dtnn model with respect to chemical composition or @xmath43 could also aid in the exploration of chemical compound space  @xcite .",
    "the quantitative accuracy achieved by dtnn and its size extensivity paves the way to the calculation of configurational and conformational energy differences  a long - standing challenge for machine learning approaches  @xcite .",
    "the reliability of dtnn for isomer energy predictions is demonstrated by the energy distribution in fig .  4 for molecular isomers with c@xmath42o@xmath40h@xmath41 chemical formula ( a total of 6095 isomers in the gdb-9 dataset ) .",
    "obviously , more work is required to extend this predictive power for larger molecules , where the dtnn model will have to be combined with a reliable model for long - range interatomic ( van der waals ) interactions .",
    "the intrinsic interpolation smoothness achieved by the dtnn model can also be used to identify molecules with peculiar electronic structure .",
    "s6 shows a list of molecules with the largest dtnn errors compared to reference dft calculations .",
    "it is noteworthy that most molecules in this figure are characterized by unconventional bonding and the electronic structure of these molecules has potential multi - reference character .",
    "hence dtnn predictions might turn out to be closer to the correct answer due to its smooth interpolation in chemical space .",
    "higher - level quantum - chemical calculations would be required to investigate this interesting hypothesis in the future .",
    "we have proposed and developed a deep tensor neural network that enables understanding of quantum - chemical many - body systems beyond properties contained in the training dataset .",
    "the dtnn model is scalable with molecular size , efficient , and achieves uniform accuracy of 1 kcal / mol throughout compositional and configuration space for molecules of intermediate size .",
    "the dtnn model leads to novel insights into chemical systems , a fact that we illustrated on the example of relative aromatic ring stability , local molecular chemical potentials , relative isomer energies , and the identification of molecules with peculiar electronic structure .",
    "however , many avenues remain for improving the dtnn model on multiple fronts . among these we mention the extension of the model to increasingly larger molecules , predicting atomic forces and frequencies , and non - extensive electronic and optical properties .",
    "we propose the dtnn model as a versatile framework for understanding complex quantum - mechanical systems based on high - throughput electronic structure calculations .    * *",
    "we employ two subsets of the gdb database  @xcite , referred to in this paper as gdb-7 and gdb-9 .",
    "gdb-7 contains 7211 molecules with up to 7 heavy atoms out of the elements c , n , o , s and cl , saturated with hydrogen  @xcite .",
    "similarly , gdb-9 includes 133,885 molecules with up to 9 heavy atoms out of c , o , n , f  @xcite .",
    "both data sets include calculations of atomization energies employing density functional theory  @xcite with the pbe0  @xcite and b3lyp  @xcite exchange - correlation potential , respectively .",
    "the molecular dynamics trajectories are calculated at a temperature of 500  k and resolution of 0.5fs using density functional theory with the pbe exchange - correlation potential  @xcite .",
    "the data sets for benzene , toluene , malonaldehyde and salicylic acid consist of 627k , 442k , 993k and 320k time steps , respectively . in the presented experiments ,",
    "we predict the potential energy of the md geometries .      the molecular energies of the various data sets",
    "are predicted using a deep tensor neural network .",
    "the core idea is to represent atoms in the molecule as vectors depending on their type and to subsequently refine the representation by embedding the atoms in their neighborhood .",
    "this is done in a sequence of interaction passes where the atom representations influence each other in a pair - wise fashion .",
    "while each of these refinements depends only on the pair - wise atomic distances , multiple passes enable the architecture to also take angular information into account .",
    "due to this decomposition of atomic interactions , an efficient representation of embedded atoms is learned following quantum chemical principles .    in the following ,",
    "we describe the deep tensor neural network step - by - step , including hyper - parameters used in our experiments .    1 .",
    "* assign initial atomic descriptors * + we assign an initial coefficient vector to each atom @xmath7 of the molecule according to its nuclear charge @xmath0 : @xmath49 where b is the number of basis functions .",
    "all presented models use atomic descriptors with 30 coefficients .",
    "we initialize each coefficient randomly following @xmath50 .",
    "gaussian feature expansion of the inter - atomic distances * + the inter - atomic distances @xmath51 are spread across many dimensions by a uniform grid of gaussians @xmath52_{0 \\leq k",
    "\\leq \\mu_{max}/\\delta \\mu},\\ ] ] with @xmath53 being the gap between two gaussians of width @xmath54 .",
    "+ in our experiments , we set both to @xmath55 . the center of the first gaussian @xmath56 was set to @xmath57 , while @xmath58 was chosen depending on the range of distances in the data ( @xmath59 for gdb-7 and benzene , @xmath60 for toluene , malonaldehyde and salicylic acid and @xmath61 for gdb-9 ) .",
    "3 .   * perform @xmath17 interaction passes * each coefficient vector @xmath62 , corresponding to atom @xmath7 after @xmath16 passes , is corrected by the interactions with the other atoms of the molecule : @xmath63 here , we model the interaction @xmath64 as follows : @xmath65 where the circle ( @xmath27 ) represents the element - wise matrix product .",
    "the factor representation in the presented models employs 60 neurons .",
    "predict energy contributions * + finally , we predict the energy contributions @xmath6 from each atom @xmath7 . employing two fully - connected layers , for each atom a scaled energy contribution @xmath66 is predicted : @xmath67 @xmath68 in our experiments , the hidden layer @xmath69 possesses 15 neurons . to obtain the final contributions ,",
    "@xmath66 is shifted to the mean @xmath70 and scaled by the standard deviation @xmath71 of the energy per atom estimated on the training set .",
    "@xmath72 this procedure ensures a good starting point for the training .",
    "obtain the molecular energy * @xmath73    the bias parameters as well as @xmath74 are initially set to zero .",
    "all other weight matrices are initialized drawing from a uniform distribution according to  @xcite .",
    "the deep tensor neural networks have been trained for 3000 epochs minimizing the squared error , using stochastic gradient descent with 0.9 momentum and a constant learning rate  @xcite .",
    "the final results are taken from the models with the best validation error in early stopping .",
    "all dtnn models were trained and executed on an nvidia tesla k40 gpu .",
    "the computational cost of the employed models depends on the number of reference calculations , the number of interaction passes as well as the number of atoms per molecule .",
    "the training times for all models and data sets are shown in table [ tab : traintime ] , ranging from 6 hours for 5.768 reference calculations of gdb-7 with one interaction pass , to 162 hours for 100.000 reference calculations of the gdb-9 data set with 3 interaction passes .",
    "on the other hand , the prediction is instantaneous : all models predict examples from the employed data sets in less than 1 ms .",
    "[ fig : predtime ] shows the scaling of the prediction time with the number of atoms and interaction layers . even for a molecule with 100 atoms ,",
    "a dtnn with 3 interaction layers requires less than 5 ms for a prediction .",
    "the prediction as well as the training steps scale linearly with the number of interaction passes and quadratically with the number of atoms , since the pairwise atomic distances are required for the interactions . for large molecules",
    "it is reasonable to introduce a distance cutoff ( future work ) . in that case",
    ", the dtnn will also scale linearly with the number of atoms .",
    "given a trained neural network as described in the previous section , one can extract the coefficients vectors @xmath62 for each atom @xmath7 and each interaction pass @xmath16 for a molecule of interest . from each final representation @xmath75",
    ", the energy contribution @xmath6 of the corresponding atom to the molecular energy can be obtained .",
    "instead , we let the molecule act on a probe atom , described by its charge @xmath76 and the pairwise distances @xmath77 to the atoms of the molecule : @xmath78 with @xmath79 . while this is equivalent to how the coefficient vectors of the molecule are corrected , here , the molecule does not get to be influenced by the probe .",
    "now , the energy of the probe atom is predicted as usual from the final representation @xmath80 . interpreting this as a local potential @xmath81 generated by the molecule",
    ", we can use the neural network to visualize the learned interactions as illustrated in fig .",
    "[ fig : explanationpot ] .",
    "the presented energy surfaces show the potential for different probe atoms plotted on an isosurface of @xmath82 .",
    "we used mayavi  @xcite for the visualization of the surfaces .      the alchemical paths in fig .",
    "[ fig : alchemy ] were generated by gradually moving the atoms as well as interpolating between the initial coefficient vectors for changes of atom types .",
    "given two nuclear charges @xmath83 , the coefficient vector for any charge @xmath84 with @xmath85 is given by @xmath86 similarly , in order to add or remove atoms , we introduce fading factors @xmath87 $ ] for each atom .",
    "this way , influences on other atoms @xmath88 as well as energy contributions to the molecular energy @xmath89 can be faded out .",
    "table [ tab : results ] shows the mean absolute ( mae ) and root mean squared errors ( rmse ) as well as standard errors over five randomly drawn training sets . for gdb-9 and the md data sets ,",
    "1k reference calculations were used as validation set for early stopping , while the remaining data was used for testing . in case of gdb-7 ,",
    "validation and test set contained 10% of the reference calculations each .",
    "with respect to the mean absolute error , chemical accuracy can be achieved on all employed data sets using models with two or three interactions passes .    figs .",
    "[ fig : ccslc ] and [ fig : mdlc ] show the dependence of the performance on the number of training examples for the benzene md data set and gdb-9 , respectively . in both learning curves ( a ) ,",
    "an increase from 1.000 to 10.000 training examples reduces the error drastically while another increase to 100.000 examples yields comparatively small improvement .",
    "the error distributions ( b ) show that models with two and three interaction passes trained on at least 25.000 gdb-9 references calculations predict 95% of the unknown molecules with an error of 3.0 kcal / mol or lower .",
    "correspondingly , the same models trained on 25.000 or more md reference calculations of benzene predict 95% of the unknown benzene configurations with a maximum error lower than 1.3 kcal / mol .    beyond a certain number of reference calculations , the models with one interaction",
    "pass perform significantly worse in all theses respects .",
    "thus , multiple interaction passes indeed enrich the learned feature representation as demonstrated by the increased predictability of previously unseen molecules .",
    "deep learning has lead to major advances in computer vision , language processing , speech recognition and other applications  @xcite . in our model , we embed the atom type in a vector space @xmath90 .",
    "this idea is inspired by word embeddings ( word2vec ) employed in natural language processing  @xcite . in order to model inter - atomic effects",
    ", we need to represent the influence of an atom represented by @xmath91 at the distance @xmath51 . to account for multiple regimes of atomic distances as well as different dimensionality of the two inputs ,",
    "we apply the gaussian feature mapping described above .",
    "similar approaches have been applied to the entries of the coulomb matrix for the prediction of molecular properties before  @xcite . a natural way to connect distance and",
    "atom representation is a tensor layer as used in text generation  @xcite , reasoning  @xcite or sentiment analysis  @xcite . for an efficient computation as well as regularization",
    ", we employ a factorized tensor layer , corresponding to a low - rank approximation of the tensor product  @xcite .",
    "convolutional neural networks have been applied to images , speech and text with great success due to their ability to capture local structure  @xcite . in a convolution layer ,",
    "local filters are applied to local environments , e.g. , image patches , extracting features relevant to the classification task .",
    "similarly , local correlations of atoms may be exploited in a chemistry setting . the atom interaction in our model",
    "can indeed be regarded as a non - linear generalization of a convolution .",
    "in contrast to images however , atoms of molecules are not arranged on a grid .",
    "therefore , the convolution kernels need to be continuous . we define a function @xmath92 yielding @xmath93 at the atom positions .",
    "now , we can rewrite the interactions as @xmath94 with @xmath95 @xmath96 @xmath97    for @xmath98 being the identity , the sum is equivalent to a discrete convolution .",
    "created by the atoms of the molecule .",
    "putting a probe atom a with nuclear charge @xmath76 at a position @xmath45 described by the distances to the atoms of the molecule @xmath99 yields an energy @xmath100.,scaledwidth=80.0% ]     interaction passes trained on gdb-9 . * ( a ) mean absolute error of neural networks depending on the number of training examples .",
    "error bars correspond to standard errors over five repetitions . for more than 5k examples ,",
    "the error bars vanish due to standard errors below 0.05 kcal / mol .",
    "( b ) error distribution for models trained on 10k , 25k , 50k and 100k training examples .",
    "the box spans between the 25% and 75% quantiles , while the whiskers mark the 5% and 95% quantiles.,scaledwidth=70.0% ]     interaction passes trained on the benzene data set . *",
    "( a ) mean absolute error of neural networks depending on the number of training examples .",
    "error bars correspond to standard errors over five repetitions . for more than 10k examples ,",
    "the error bars vanish due to standard errors below 0.01 kcal / mol .",
    "( b ) error distribution for models trained on 10k , 25k , 50k and 100k training examples .",
    "the box spans between the 25% and 75% quantiles , while the whiskers mark the 5% and 95% quantiles.,scaledwidth=70.0% ]     in the gdb-9 data set .",
    "* the energy contributions were predicted using the gdb-9 model with two interaction passes trained on 50k reference calculations . ]            .",
    "* the dtnn model is able to smoothly create , remove and move atoms as well as continuously change their element - specific characteristics .",
    "a path leading from benzene to s - triazine was computed by only changing , removing and changing types of atoms ( blue ) . in the second path ( orange ) , atoms were also moved to the new equilibrium positions .",
    "the black dots mark the energy of dft reference calculations.,scaledwidth=60.0% ]     of the employed dtnn . *",
    "all predictions were computed on an nvidia tesla k40 gpu . ]",
    ".*training duration for the presented neural networks with up to three interaction passes ( @xmath101 ) in hours . *",
    "all models were trained with stochastic gradient descent with momentum for 3.000 epochs on an nvidia tesla k40 gpu . [ cols= \"",
    "< , > , > , > , > \" , ]      lrrrrrrrrrr data set & # training examples & & & + & & mae & rmse & mae & rmse & mae & rmse + & & & & & & & + [ -1.5ex ] gdb-7 & 5768 & @xmath102 & @xmath103 & @xmath104 & @xmath105 & @xmath106 & @xmath107 + & & & & & & & + [ -1.5ex ] gdb-9 & 25k & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath104 & @xmath112 & + & 50k & @xmath113 & @xmath114 & @xmath115 & @xmath116 & @xmath117 & @xmath118 & + & 100k & @xmath119 & @xmath120 & @xmath121 & @xmath122 & @xmath123 & @xmath124 & + & & & & & & & + [ -1.5ex ] benzene & 25k & @xmath125 & @xmath126 & @xmath127 & @xmath128 & @xmath129 & @xmath128 + & 50k & @xmath130 & @xmath131 & @xmath129 & @xmath132 & @xmath129 & @xmath132 + & 100k & @xmath125 & @xmath126 & @xmath132 & @xmath128 & @xmath132 & @xmath128 + & & & & & & & + [ -1.5ex ] toluene & 25k & @xmath133 & @xmath134 & @xmath135 & @xmath136 & @xmath137 & @xmath138 + & 50k & @xmath139 & @xmath140 & @xmath141 & @xmath142 & @xmath141 & @xmath142 + & 100k & @xmath143 & @xmath144 & @xmath145 & @xmath146 & @xmath147 & @xmath148 + & & & & & & & + [ -1.5ex ] malonaldehyde & 25k & @xmath149 & @xmath150 & @xmath151 & @xmath152 & @xmath151 & @xmath153 + & 50k & @xmath154 & @xmath155 & @xmath156 & @xmath157 & @xmath158 & @xmath159 + & 100k & @xmath160 & @xmath161 & @xmath162 & @xmath163 & @xmath164 & @xmath142 + & & & & & & & + [ -1.5ex ] salicylic acid & 25k & @xmath165 & @xmath166 & @xmath167 & @xmath168 & @xmath169 & @xmath170 + & 50k & @xmath171 & @xmath172 & @xmath173 & @xmath174 & @xmath175 & @xmath176 + & 100k & @xmath177 & @xmath178 & @xmath179 & @xmath180 & @xmath143 & @xmath181 +"
  ],
  "abstract_text": [
    "<S> learning from data has led to paradigm shifts in a multitude of disciplines , including web , text , and image search , speech recognition , as well as bioinformatics . </S>",
    "<S> can machine learning enable similar breakthroughs in understanding quantum many - body systems ? </S>",
    "<S> here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum - mechanical observables of molecular systems . </S>",
    "<S> we unify concepts from many - body hamiltonians with purpose - designed deep tensor neural networks ( dtnn ) , which leads to size - extensive and uniformly accurate ( 1 kcal / mol ) predictions in compositional and configurational chemical space for molecules of intermediate size . as an example of chemical relevance </S>",
    "<S> , the dtnn model reveals a classification of aromatic rings with respect to their stability  a useful property that is not contained as such in the training dataset . </S>",
    "<S> further applications of dtnn for predicting atomic energies and local chemical potentials in molecules , reliable isomer energies , and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum - chemical systems .        </S>",
    "<S> chemistry permeates all aspects of our life , from the development of new drugs to the food that we consume and materials we use on a daily basis . </S>",
    "<S> chemists rely on empirical observations based on creative and painstaking experimentation that leads to eventual discoveries of molecules and materials with desired properties and mechanisms to synthesize them . </S>",
    "<S> many discoveries in chemistry can be guided by searching large databases of experimental or computational molecular structures and properties by using concepts based on chemical similarity . because the structure and properties of molecules are determined by the laws of quantum mechanics , ultimately chemical discovery must be based on fundamental quantum principles . </S>",
    "<S> indeed , electronic structure calculations and intelligent data analysis ( machine learning , ml ) have recently been combined aiming towards the goal of accelerated discovery of chemicals with desired properties  @xcite . however , so far the majority of these pioneering efforts have focused on the construction of reduced models trained on large datasets of density - functional theory calculations . in this work , </S>",
    "<S> we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum - mechanical properties of molecular systems beyond those trivially contained in the training dataset . </S>",
    "<S> obviously , computational models are not predictive if they lack accuracy . </S>",
    "<S> in addition to being interpretable , size extensive and efficient , our deep tensor neural netwok ( dtnn ) approach is uniformly accurate ( 1 kcal / mol ) throughout compositional and configurational chemical space . on the more fundamental side , </S>",
    "<S> the mathematical construction of the dtnn model provides statistically rigorous partitioning of extensive molecular properties into atomic contributions  a long - standing challenge for quantum - mechanical calculations of molecules . </S>"
  ]
}