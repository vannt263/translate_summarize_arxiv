{
  "article_text": [
    "the last decade has seen a renewed interest in the problem of solving an underdetermined system of equations @xmath4 , @xmath5 , where @xmath6 , by regularizing its solution to be sparse , i.e. , having very few non - zero entries .",
    "specifically , if one aims to find @xmath7 with the least number of nonzero entries that solves the linear system , the problem is known as @xmath0-minimization:@xmath8    the problem @xmath9 is intended to seek _ entry - wise sparsity _ in @xmath7 and is known to be np - hard in general . in compressive sensing ( cs ) literature , it has been shown that the solution to often can be obtained by solving a more tractable linear program , namely , @xmath1-minimization @xcite:@xmath10 this unconventional equivalence relation between @xmath9 and @xmath11 and the more recent numerical solutions @xcite to efficiently recover high - dimensional sparse signal have been a very competitive research area in cs .",
    "its broad applications have included sparse error correction @xcite , compressive imaging @xcite , image denoising and restoration @xcite , and face recognition @xcite , to name a few .",
    "in addition to enforcing entry - wise sparsity in a linear system of equations , the notion of _ group sparsity _ has attracted increasing attention in recent years @xcite . in this case",
    ", one assumes that the matrix @xmath12 has some underlying structure , and can be grouped into blocks : @xmath13 , where @xmath14 and @xmath15 .",
    "accordingly , the vector @xmath7 is split into several blocks as @xmath16 , where @xmath17 . in this case , it is of interest to estimate @xmath7 with the least number of blocks containing non - zero entries .",
    "the group sparsity minimization problem is posed as @xmath18 where @xmath19 is the indicator function .",
    "since the expression @xmath20 can be written as @xmath21 , it is also denoted as @xmath22 , the @xmath23-norm of @xmath7 .",
    "enforcing group sparsity exploits the problem s underlying structure and can improve the solution s interpretability .",
    "for example , in a sparsity - based classification ( sbc ) framework applied to face recognition , the columns of @xmath12 are vectorized training images of human faces that can be naturally grouped into blocks corresponding to different subject classes , @xmath24 is a vectorized query image , and the entries in @xmath7 represent the coefficients of linear combination of all the training images for reconstructing @xmath24 .",
    "group sparsity lends itself naturally to this problem since it is desirable to use images of the smallest number of subject classes to reconstruct and subsequently classify a query image .",
    "furthermore , the problem of robust face recognition has considered an interesting modification known as the _ cross - and - bouquet _ ( cab ) model : @xmath25 , where @xmath26 represents possible sparse error corruption on the observation @xmath24 @xcite .",
    "it can be argued that the cab model can be solved as a group sparsity problem in , where the coefficients of @xmath27 would be the @xmath28 group .",
    "however , this problem has a trivial solution for @xmath29 and @xmath30 , which would have the smallest possible group sparsity .",
    "hence , it is necessary to further regularize the entry - wise sparsity in @xmath27 .    to this effect ,",
    "one considers a mixture of the previous two cases , where one aims to enforce entry - wise sparsity as well as group sparsity such that @xmath7 has very few number of non - zero blocks _ and _ the reconstruction error @xmath27 is also sparse .",
    "the _ mixed sparsity _ minimization problem can be posed as @xmath31 where @xmath32 controls the tradeoff between the entry - wise sparsity and group sparsity . due to the use of the counting norm ,",
    "the optimization problems in and are also np - hard in general .",
    "hence , several recent works have focused on developing tractable convex relaxations for these problems . in the case of group sparsity ,",
    "the relaxation involves replacing the @xmath23-norm with the @xmath33-norm , where @xmath34 .",
    "these relaxations are also used for the mixed sparsity case @xcite .    in this work ,",
    "we are interested in deriving and analyzing convex relaxations for general sparsity minimization problems . in the entry - wise case ,",
    "the main theoretical understanding of the link between the original np - hard problem in and its convex relaxation has been given by the simple fact that the @xmath1-norm is a convex surrogate of the @xmath0-norm .",
    "however , in the group sparsity case , a similar relaxation produces a family of convex surrogates , i.e. , @xmath35 , whose value depends on @xmath36 .",
    "this raises the question whether there is a preferable value of @xmath36 for the relaxation of the group sparsity minimization problem ?",
    "in fact , we consider the following more important question :    _ is there a unified framework for deriving convex relaxations of general sparsity recovery problems ?",
    "_      we present a new optimization - theoretic framework based on lagrangian duality for deriving convex relaxations of sparsity minimization problems .",
    "specifically , we introduce a new class of equivalent optimization problems for @xmath9 , @xmath37 and @xmath38 , and derive the lagrangian duals of the original np - hard problems .",
    "we then consider the lagrangian dual of the lagrangian dual to get a new optimization problem that we term as the _ lagrangian bidual _ of the primal problem .",
    "we show that the lagrangian biduals are convex relaxations of the original sparsity minimization problems .",
    "importantly , we show that the lagrangian biduals for the @xmath9 and @xmath37 problems correspond to minimizing the @xmath1-norm and the @xmath3-norm , respectively .    since the lagrangian duals for @xmath9 , @xmath37 and @xmath38 are linear programs",
    ", there is no duality gap between the lagrangian duals and the corresponding lagrangian biduals .",
    "therefore , the bidual based convex relaxations can be interpreted as maximizing the lagrangian duals of the original sparsity minimization problems .",
    "this provides new interpretations for the relaxations of sparsity minimization problems .",
    "moreover , since the lagrangian dual of a minimization problem provides a lower bound for the optimal value of the primal problem , we show that the optimal objective value of the convex relaxation provides a non - trivial lower bound on the sparsity of the true solution to the primal problem .",
    "in what follows , we will derive the lagrangian bidual for the mixed sparsity minimization problem , which generalizes the entry - wise sparsity and group sparsity cases ( also see section [ sec : results ] ) .",
    "specifically , we will derive the lagrangian bidual for the following optimization problem : @xmath39 , ~~ { \\mbox{s.t.}}~~ \\begin{bmatrix } a_1 & \\cdots & a_k \\end{bmatrix } \\begin{bmatrix } { { \\boldsymbol}{x}}_1 \\\\ \\vdots \\\\ { { \\boldsymbol}{x}}_k \\end{bmatrix } = { { \\boldsymbol}{b } } , \\end{split } \\label{eq : primal0}\\ ] ] where @xmath40 and @xmath41 .",
    "given any unique , finite solution @xmath42 to , there exists a constant @xmath43 such that the absolute values of the entries of @xmath42 are less than @xmath44 , namely , @xmath45 .",
    "note that if does not have a unique solution , it might not be possible to choose a finite - valued @xmath44 that upper bounds all the solutions . in this case",
    ", a finite - valued @xmath44 may be viewed as a regularization term for the desired solution . to this effect , we consider the following modified version of where we introduce the box constraint that @xmath46 : @xmath47 , ~~ { \\mbox{s.t.}}~~ a{{\\boldsymbol}{x}}= { { \\boldsymbol}{b}}\\text { and } \\|{{\\boldsymbol}{x}}\\|_\\infty \\le m , \\end{split } \\label{eq : primal1}\\ ] ] where @xmath44 is chosen as described above to ensure that the optimal values of and are the same .",
    "* primal problem .",
    "*    we will now frame an equivalent optimization problem for , for which we introduce some new notation .",
    "let @xmath48 be an entry - based sparsity indicator for @xmath7 , namely , @xmath49 if @xmath50 and @xmath51 otherwise .",
    "we also introduce a group - based sparsity indicator vector @xmath52 , whose @xmath53 entry @xmath54 denotes whether the @xmath53 block @xmath55 contains non - zero entries or not , namely , @xmath56 if @xmath57 and @xmath58 otherwise . to express this constraint ,",
    "we introduce a matrix @xmath59 , such that @xmath60 if the @xmath61 entry of @xmath7 belongs to the @xmath62 block and @xmath63 otherwise . finally , we denote the positive component and negative component of @xmath7 as @xmath64 and @xmath65 , respectively , such that @xmath66 .    given these definitions",
    ", we see that can be reformulated as @xmath67 , ~ { \\mbox{s.t.}}\\text { ( a ) } { { \\boldsymbol}{x}}_+ \\ge 0 , \\text { ( b ) } { { \\boldsymbol}{x}}_- \\ge 0 , \\text { ( c ) } { { \\boldsymbol}{g}}\\in \\{0,1\\}^k , \\!\\!\\\\ &",
    "\\text{(d ) } { { \\boldsymbol}{z}}\\in \\{0,1\\}^n   \\text { ( e ) } a({{\\boldsymbol}{x}}_+ - { { \\boldsymbol}{x}}_- ) = { { \\boldsymbol}{b } } ,   \\text { ( f ) } \\pi{{\\boldsymbol}{g}}\\ge \\frac{1}{m}({{\\boldsymbol}{x}}_+ + { { \\boldsymbol}{x}}_- ) ,    \\text { and ( g ) }   { { \\boldsymbol}{z}}\\ge \\frac{1}{m}({{\\boldsymbol}{x}}_+ + { { \\boldsymbol}{x}}_- ) , \\end{split}\\ ] ] where @xmath68 and @xmath69^\\top \\in { { \\mathbb{r}}}^n$ ] .",
    "constraints ( a)(d ) are used to enforce the aforementioned conditions on the values of the solution . while constraint ( e ) enforces the condition that the original system of linear equations is satisfied , the constraints ( f ) and ( g )",
    "ensure that the group sparsity indicator @xmath70 and the entry - wise sparsity indicator @xmath71 are consistent with the entries of @xmath7 .",
    "* lagrangial dual .",
    "*    the lagrangian function for is given as @xmath72 where @xmath73 , @xmath74 , @xmath75 , and @xmath76 . in order to obtain the lagrangian dual function ,",
    "we need to minimize @xmath77 with respect to @xmath78 , @xmath79 , @xmath70 and @xmath71@xcite .",
    "notice that if the coefficients of @xmath78 and @xmath79 , i.e. , @xmath80 and @xmath81 are non - zero , the minimization of @xmath77 with respect to @xmath78 and @xmath79 is unbounded below .",
    "to this effect , the constraints that these coefficients are equal to @xmath82 form constraints on the dual variables .",
    "next , consider the minimization of @xmath77 with respect to @xmath70 .",
    "since each entry @xmath54 only takes values @xmath82 or @xmath83 , its optimal value @xmath84 that minimizes @xmath77 is given as @xmath85 a similar expression can be computed for the minimization with respect to @xmath71 . as a consequence , the lagrangian dual problem can be derived as @xmath86 , { \\mbox{s.t.}}\\\\ & \\text{(a ) } \\forall i=1,2,4,5 : { { \\boldsymbol}{\\lambda}}_i \\ge { { \\boldsymbol}{0 } } , \\text { ( b ) } \\frac{1}{m}({{\\boldsymbol}{\\lambda}}_4+{{\\boldsymbol}{\\lambda}}_5)- a^\\top{{\\boldsymbol}{\\lambda}}_3 -{{\\boldsymbol}{\\lambda}}_1 = { { \\boldsymbol}{0}}\\\\ & \\text{and ( c ) } \\frac{1}{m}({{\\boldsymbol}{\\lambda}}_4+{{\\boldsymbol}{\\lambda}}_5)+ a^\\top{{\\boldsymbol}{\\lambda}}_3 -{{\\boldsymbol}{\\lambda}}_2 = { { \\boldsymbol}{0}}. \\end{split}\\ ] ]    this can be further simplified by rewriting it as the following linear program : @xmath87 , { \\mbox{s.t.}}~ \\text{(a ) } { { \\boldsymbol}{\\lambda}}_4 \\ge { { \\boldsymbol}{0 } } ,   \\text { ( b ) } { { \\boldsymbol}{\\lambda}}_5 \\ge { { \\boldsymbol}{0 } } , \\text { ( c ) } { { \\boldsymbol}{\\lambda}}_6 \\le 0 , \\text { ( d ) } { { \\boldsymbol}{\\lambda}}_7 \\le 0 , \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\\ & \\!\\!\\!\\!\\text { ( e ) } { { \\boldsymbol}{\\lambda}}_6 \\le { { \\boldsymbol}{\\alpha}}-\\pi^\\top{{\\boldsymbol}{\\lambda}}_4 , \\text { ( f ) } { { \\boldsymbol}{\\lambda}}_7 \\le { { \\boldsymbol}{\\beta}}-{{\\boldsymbol}{\\lambda}}_5 \\text { and ( g ) } -\\frac{1}{m}({{\\boldsymbol}{\\lambda}}_4+{{\\boldsymbol}{\\lambda}}_5 ) \\le a^\\top{{\\boldsymbol}{\\lambda}}_3 \\le \\frac{1}{m}({{\\boldsymbol}{\\lambda}}_4+{{\\boldsymbol}{\\lambda}}_5 ) .   \\end{split}\\ ] ]    notice that we have made two changes in going from to .",
    "first , we have replaced constraints ( b ) and ( c ) in with the constraint ( g ) in and eliminated @xmath88 and @xmath89 from .",
    "second , we have introduced variables @xmath90 and @xmath91 to encode the  min \" operator in the objective function of .    * lagrangian bidual .",
    "*    we will now consider the lagrangian dual of , which will be referred to as the _ lagrangian bidual _ of .",
    "it can be verified that the lagrangian dual of is given as @xmath92^k , \\!\\!\\\\ & \\!\\!\\!\\!\\!\\ !",
    "\\text{(d ) } { { \\boldsymbol}{z}}\\in [ 0,1]^n   \\text { ( e ) } a({{\\boldsymbol}{x}}_+ - { { \\boldsymbol}{x}}_- ) = { { \\boldsymbol}{b } } ,   \\text { ( f ) } \\pi{{\\boldsymbol}{g}}\\ge \\frac{1}{m}({{\\boldsymbol}{x}}_+ + { { \\boldsymbol}{x}}_- )    \\text { and ( g ) }   { { \\boldsymbol}{z}}\\ge \\frac{1}{m}({{\\boldsymbol}{x}}_+ + { { \\boldsymbol}{x}}_- ) .",
    "\\end{split}\\ ] ]    notice that in going from to , the discrete valued variables @xmath71 and @xmath70 have been relaxed to take real values between @xmath82 and @xmath83 .",
    "given that @xmath93 and noting that @xmath7 can be represented as @xmath94 , we can conclude from constraint ( g ) in that the solution @xmath42 satisfies @xmath45 .",
    "moreover , given that @xmath70 and @xmath71 are relaxed to take real values , we see that the optimal values for @xmath95 and @xmath96 are @xmath97 and @xmath98 , respectively .",
    "hence , we can eliminate constraints ( f ) and ( g ) by replacing @xmath71 and @xmath70 by these optimal values .",
    "it can then be verified that solving is equivalent to solving the problem : @xmath99",
    "\\quad { \\mbox{s.t.}}\\text { ( a ) }   a{{\\boldsymbol}{x}}= { { \\boldsymbol}{b}}\\text { and ( b ) } \\|{{\\boldsymbol}{x}}\\|_\\infty \\le m.\\ ] ]    this is the lagrangian bidual for .",
    "in this section , we first describe some properties of the biduality framework in general . we will then focus on some important results for the special cases of entry - wise sparsity and group sparsity .",
    "the optimal value of the lagrangian bidual in is a lower bound on the optimal value of the np - hard primal problem in .",
    "[ thm : optval ]    since there is no duality gap between a linear program and its lagrangian dual @xcite , the optimal values of the lagrangian dual in and the lagrangian bidual in are the same .",
    "moreover , we know that the optimal value of a primal minimization problem is always bounded below by the optimal value of its lagrangian dual @xcite . we hence have the required result .    since the original primal problem in is np - hard , we note that the duality gap between the primal and its dual in is non - zero in general .",
    "moreover , we notice that as we increase @xmath44 ( i.e. , a more conservative estimate ) , the optimal value of the primal is unchanged , but the optimal value of the bidual in decreases .",
    "hence , the duality gap increases as @xmath44 increases .",
    "@xmath44 in should preferably be equal to @xmath100 , which may not be possible to estimate accurately in practice .",
    "therefore , it is of interest to analyze the effect of taking a very conservative estimate of @xmath44 , i.e. , choosing a large value for @xmath44 . in what follows",
    ", we show that taking a conservative estimate of @xmath44 is equivalent to dropping the box constraint in the bidual .    for this purpose ,",
    "consider the following modification of the bidual : @xmath101   \\quad { \\mbox{s.t.}}\\quad   a{{\\boldsymbol}{x}}= { { \\boldsymbol}{b}},\\ ] ] where we have essentially dropped the box constraint ( b ) in .",
    "it is easy to verify that @xmath102 , we have that @xmath103",
    ". therefore , we see that taking a conservative value of @xmath44 is equivalent to solving the modified bidual in .      notice that by substituting @xmath104 and @xmath105 , the optimization problem in reduces to the entry - wise sparsity minimization problem in .",
    "hence , the lagrangian bidual to the @xmath44-regularized entry - wise sparsity problem @xmath9 is : @xmath106    more importantly , we can also conclude from that solving the lagrangian bidual to the entry - wise sparsity problem with a conservative estimate of @xmath44 is equivalent to solving the problem : @xmath107 which is precisely the well - known @xmath1-norm relaxation for @xmath9 .",
    "our framework therefore provides a new interpretation for this relaxation :    the @xmath1-norm minimization problem in is the lagrangian bidual of the @xmath0-norm minimization problem in , and solving is equivalent to maximizing the dual of .",
    "we further note that we can now use the solution of to derive a non - trivial lower bound for the primal objective function which is precisely the sparsity of the desired solution .",
    "more specifically , we can use theorem [ thm : optval ] to conclude the following result :    [ corr : l0l1 ] let @xmath108 be the solution to .",
    "we have that @xmath109 , the sparsity of @xmath108 , i.e. , @xmath110 is bounded below by @xmath111 .    due to the non - zero duality gap in the primal entry - wise sparsity minimization problem ,",
    "the above lower bound provided by corollary [ corr : l0l1 ] is not tight in general .",
    "notice that by substituting @xmath112 and @xmath113 , the optimization problem in reduces to the group sparsity minimization problem in .",
    "hence , the lagrangian bidual of the group sparsity problem is : @xmath114    as in the case of entry - wise sparsity above , solving the bidual to the group sparsity problem with a conservative estimate of @xmath44 is equivalent to solving : @xmath115 which is the convex @xmath3-norm relaxation of the @xmath23-min problem . in other words ,",
    "the biduality framework selects the @xmath3-norm out of the entire family of @xmath33-norms as the convex surrogate of the @xmath23-norm .",
    "finally , we use theorem [ thm : optval ] to show that the solution obtained by minimizing the @xmath3-norm provides a lower bound for the group sparsity .",
    "[ corr : grp ] let @xmath116 be the solution to . for any @xmath117",
    ", the group sparsity of @xmath116 , i.e. , @xmath118 , is bounded below by @xmath119 .",
    "the @xmath3-norm seems to be an interesting choice for computing the lower bound of the group sparsity , as compared to other @xmath33-norms for finite @xmath120 . for example , consider the case when @xmath121 , where the @xmath33-norm is equivalent to the @xmath1-norm .",
    "assume that @xmath12 consists of a single block with several columns so that the maximum number of non - zero blocks is @xmath83 .",
    "denote the solution to the @xmath1-minimization problem as @xmath122 .",
    "it is possible to construct examples ( also see figure [ fig : entry ] ) where @xmath123 .",
    "hence , it is unclear in general if the solutions obtained by minimizing @xmath33-norms for finite - valued @xmath120 can help provide lower bounds for the group sparsity .",
    "we now present experiments to evaluate the bidual framework for minimizing entry - wise sparsity and mixed sparsity .",
    "we present experiments on synthetic data to show that our framework can be used to compute non - trivial lower bounds for the entry - wise sparsity minimization problem .",
    "we then consider the face recognition problem where we compare the performance of the bidual - based @xmath3-norm relaxation with that of the @xmath124-norm relaxation for mixed sparsity minimization .",
    "we use boxplots to provide a concise representation of our results statistics .",
    "the top and bottom edge of a boxplot for a set of values indicates the maximum and minimum of the values .",
    "the bottom and top extents of the box indicate the @xmath125 and @xmath126 percentile mark .",
    "the red mark in the box indicates the median and the red crosses outside the boxes indicate potential outliers .",
    "* entry - wise sparsity .",
    "*    we now explore the practical implications of corollary  [ corr : l0l1 ] through synthetic experiments .",
    "we randomly generate entries of @xmath127 and @xmath128 from a gaussian distribution with unit variance .",
    "the sparsity of @xmath129 is varied from @xmath83 to @xmath130 in steps of @xmath131 .",
    "we solve with @xmath132 using @xmath133 and @xmath134 , where @xmath135 .",
    "we use corollary [ corr : l0l1 ] to compute lower bounds on the true sparsity , i.e. , @xmath136 .",
    "we repeat this experiment @xmath137 times for each sparsity level and figure  [ fig : entry ] shows the boxplots for the bounds computed from these experiments .",
    "we first analyze the lower bounds computed when @xmath138 , in figure [ fig : m0 ] . as explained in section [ sec : entry - results ] , the bounds are not expected to be tight due to the duality gap .",
    "notice that for extremely sparse solutions , the maximum of the computed bounds is close to the true sparsity but this diverges as the sparsity of @xmath129 reduces .",
    "the median value of the bounds is much looser and we see that the median also diverges as the sparsity of @xmath129 reduces .",
    "furthermore , the computed lower bounds seem to grow linearly as a function of the true sparsity .",
    "similar trends are observed for @xmath139 and @xmath134 in figures [ fig:2m0 ] and [ fig:5m0 ] , respectively . as expected from the discussion in section [ sec : entry - results ] , the bounds become very loose as @xmath44 increases .    in theory",
    ", we would like to have _ per - instance certificates - of - optimality _ of the computed solution , where the lower bound is equal to the true sparsity @xmath136 .",
    "nonetheless , we note that this ability to compute a per - instance non - trivial lower bound on the sparsity of the desired solution is an important step forward with respect to the previous approaches that require pre - computing optimality conditions for equivalence of solutions to the @xmath0-norm and @xmath1-norm minimization problems .",
    "we have performed a similar experiment for the group sparsity case , and observed that the bidual framework is able to provide non - trivial lower bounds for the group sparsity also .",
    "+    * mixed sparsity .",
    "*    we now evaluate the results of mixed sparsity minimization for the sparsity - based face recognition problem , where the columns of @xmath12 represent training images from the @xmath140 face classes : @xmath141 and @xmath142 represents a query image .",
    "we assume that a subset of pixel values in the query image may be corrupted or disguised .",
    "hence , the error in the image space is modeled by a sparse error term @xmath27 : @xmath143 , where @xmath144 is the uncorrupted image . a linear representation of the query image forms the following linear system of equations : @xmath145 where @xmath146 is the @xmath147 identity matrix .",
    "the goal of sparsity - based classification ( sbc ) is to minimize the group sparsity in @xmath7 and the sparsity of @xmath27 such that the dominant non - zero coefficients in @xmath7 reveal the membership of the ground - truth observation @xmath148 @xcite . in our experiments ,",
    "we solve for @xmath7 and @xmath27 by solving the following optimization problem : @xmath149    notice that for @xmath150 , this reduces to solving a special case of the problem in , i.e. , the bidual relaxation of the mixed sparsity problem with a conservative estimate of @xmath44 . in our experiments , we set @xmath151 and compare the solutions to obtained using @xmath152 and @xmath150 .",
    "we evaluate the algorithms on a subset of the ar dataset @xcite which has manually aligned frontal face images of size @xmath153 for 50 male and 50 female subjects , i.e. , @xmath154 and @xmath155 .",
    "each individual contributes 7 un - occluded training images , 7 un - occluded testing images and 12 occluded testing images .",
    "hence , we have 700 training images and 1900 testing images . to compute the number of non - zero blocks in the coefficient @xmath7 estimated for a testing image",
    ", we find the number of blocks whose energy @xmath156 is greater than a specified threshold .",
    "the results of our experiments are presented in figure [ fig : mixed ] .",
    "the solution obtained with @xmath152 gives better group sparsity of @xmath7 .",
    "however , a sparser error @xmath27 is estimated with @xmath150 .",
    "the number of non - zero entities in a solution to , i.e. , the number of non - zero blocks plus the number of non - zero error entries , is lower for the solution obtained using @xmath157 rather than that obtained using @xmath152 .",
    "however , the primal mixed - sparsity objective value @xmath158 ( see ) is lower for the solution obtained using @xmath152 .",
    "+   +    we now compare the classification results obtained with the solutions @xmath7 computed in our experiments . for classification",
    ", we consider the non - zero blocks in @xmath7 and then assign the query image to the block , i.e. , subject class , for which it gives the least @xmath159 residual @xmath160 .",
    "the results are presented in table [ tab : mixed ] .",
    "notice that the classification results obtained with @xmath150 ( the bidual relaxation ) are better than those obtained using @xmath152 .",
    "since the classification of un - occluded images is already very good using @xmath152 , classification with @xmath150 gives only a minor improvement in this case .",
    "however , a more tangible improvement is noticed in the classification of the occluded images .",
    "therefore the classification with @xmath150 is in general better than that obtained with @xmath152 , which is considered the state - of - the - art for sparsity - based classification @xcite .",
    ".classification results on the ar dataset using the solutions obtained by minimizing mixed sparsity .",
    "the test set consists of 700 un - occluded images and 1200 occluded images . [ cols=\"^,^,^,^,^ \" , ]",
    "we have presented a novel analysis of several sparsity minimization problems which allows us to interpret several convex relaxations of the original np - hard primal problems as being equivalent to maximizing their lagrangian duals .",
    "the pivotal point of this analysis is the formulation of mixed - integer programs which are equivalent to the original primal problems . while we have derived the biduals for only a few sparsity minimization problems , the same techniques",
    "can also be used to easily derive convex relaxations for other sparsity minimization problems @xcite .",
    "an interesting result of our biduality framework is the ability to compute a per - instance certificate of optimality by providing a lower bound for the primal objective function .",
    "this is in contrast to most previous research which aims to characterize either the subset of solutions or the set of conditions for perfect sparsity recovery using the convex relaxations @xcite . in most cases ,",
    "the conditions are either weak or hard to verify .",
    "more importantly , these conditions needed to be pre - computed as opposed to verifying the correctness of a solution at run - time . in lieu of this , we hope that our proposed framework will prove an important step towards per - instance verification of the solutions .",
    "specifically , it is of interest in the future to explore tighter relaxations for the verification of the solutions .",
    "this research was supported in part by aro muri w911nf-06 - 1 - 0076 , arl mast - cta w911nf-08 - 2 - 0004 , nsf cns-0931805 , nsf cns-0941463 and nsf grant 0834470 . the views and conclusions contained in this document",
    "are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the army research laboratory or the u.s .",
    "government is authorized to reproduce and distribute for government purposes notwithstanding any copyright notation herein ."
  ],
  "abstract_text": [
    "<S> recent results in compressive sensing have shown that , under certain conditions , the solution to an underdetermined system of linear equations with sparsity - based regularization can be accurately recovered by solving convex relaxations of the original problem . in this work , we present a novel primal - dual analysis on a class of sparsity minimization problems . </S>",
    "<S> we show that the lagrangian bidual ( i.e. , the lagrangian dual of the lagrangian dual ) of the sparsity minimization problems can be used to derive interesting convex relaxations : the bidual of the @xmath0-minimization problem is the @xmath1-minimization problem ; and the bidual of the @xmath2-minimization problem for enforcing group sparsity on structured data is the @xmath3-minimization problem . </S>",
    "<S> the analysis provides a means to compute per - instance non - trivial lower bounds on the ( group ) sparsity of the desired solutions . in a real - world application , </S>",
    "<S> the bidual relaxation improves the performance of a sparsity - based classification framework applied to robust face recognition . </S>"
  ]
}