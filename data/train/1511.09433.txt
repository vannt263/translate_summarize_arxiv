{
  "article_text": [
    "this paper concerns a fundamental question in high - dimensional stochastic geometry :    1 .",
    "is it likely that a random subspace of fixed dimension does _ not _ intersect a given set ?",
    "this problem has its roots in the earliest research on spherical integral geometry  @xcite , and it also arises in asymptotic convex geometry  @xcite . in recent years , this question has attracted fresh attention  @xcite because it is central to the analysis of randomized dimension reduction .",
    "this paper establishes that a striking universality phenomenon takes place in the stochastic geometry problem ( q1 ) . for a given set ,",
    "the answer to this question is essentially the same for every distribution on random subspaces that is induced by a natural model for random projectors .",
    "universality also manifests itself in metric variants of ( q1 ) , where we ask how far the random subspace lies from the set .",
    "we discuss the implications of these results in high - dimensional geometry , random matrix theory , numerical analysis , optimization , statistics , signal processing , and beyond .",
    "dimension reduction is the operation of mapping a set from a large space into a smaller space .",
    "ideally , this action distills the `` information '' in the set , and it allows us to develop more efficient algorithms for processing that information . in the setting of euclidean spaces , a fundamental method for dimension reduction is to apply a random linear map , often called a random projector , to each point in the set .",
    "it is important that the random projector preserve geometric features of the set . in particular ,",
    "we do _ not _ want the projector to map a point in the set to the origin .",
    "equivalently , the null space of the random projector should _ not _ intersect the set .",
    "we see that ( q1 ) emerges naturally in the context of randomized dimension reduction .",
    "let us introduce a framework in which to study this problem .",
    "it is natural to treat ( q1 ) as a question in spherical geometry because it is scale invariant .",
    "fix the ambient dimension @xmath0 , and consider a closed subset @xmath1 of the euclidean unit sphere in @xmath2 .",
    "for the moment , we also assume that @xmath1 is spherically convex ; that is , @xmath1 is the intersection of a convex cone that satisfies @xmath3 for all @xmath4 . ] with the unit sphere .",
    "construct a random linear map @xmath5 , where the embedding dimension @xmath6 does not exceed the ambient dimension @xmath0 . as we vary the distribution of the random projector @xmath7 , the map @xmath8 induces different distributions on the subspaces in @xmath2 with codimension at most @xmath6 .",
    "we may now reformulate ( q1 ) in this language :    1 .   for a given embedding dimension @xmath6 ,",
    "what is the probability that @xmath9 ?",
    "equivalently , what is the probability that @xmath10 ?",
    "we say that the random projection succeeds when @xmath10 .",
    "conversely , when @xmath11 , we say that the random projection fails .",
    "see figure  [ fig : intro - geometry ] for an illustration .",
    "we have the intuition that , for a fixed choice of @xmath1 , the projection is more likely to succeed as the embedding dimension @xmath6 increases .",
    "furthermore , a random projector @xmath7 with fixed embedding dimension @xmath6 is less likely to succeed as the size of the set increases .",
    "we will justify these heuristics in complete detail .",
    "( 0,0 ) circle ( 4pc ) ;    ( 0,0 ) + ( 75:5pc )  + ( -105:5pc ) ; ( 0,0 ) + ( -105:4.5pc ) node[below right]@xmath12 ;    ( 0,0 ) + ( -15:5pc )  + ( 165:5pc ) ; ( 0,0 ) + ( 165:4.5pc ) node[below left]@xmath13 ;    ( 0,0 ) circle ( 1pt ) ;    ( 0,0 ) + ( -15:1pc )  + + ( -15:3.46pc )  + +",
    "( 75:2pc ) arc ( 15:60:4pc )  cycle ; ( 0,0 ) + ( -15:1pc )  + +",
    "( -15:3.46pc )  + + ( 75:2pc ) arc ( 15:60:4pc )  cycle ;    ( 0,0 ) + ( 15:4pc ) arc ( 15:60:4pc ) ; ( 0,0 ) + ( 30:4.25pc ) node[right]@xmath1 ;    ( 0,0 ) + ( -15:1pc )  ( -15:3.46pc ) ; ( 0,-0.25pc ) + ( -15:2.25pc ) node[below]@xmath14 ;    ( 5.5,0 ) circle ( 4pc ) ;    ( 5.5,0 ) + ( 45:5pc ) ",
    "+ ( -135:5pc ) ; ( 5.5,0 ) + ( -135:4.5pc ) node[above left]@xmath12 ;    ( 5.5,0 ) + ( -45:5pc )  + ( 135:5pc ) ; ( 5.5,0 ) + ( 135:4.5pc ) node[below left]@xmath13 ;    ( 5.5,0 ) circle ( 1pt ) ;    ( 5.5,0 ) + ( 135:1pc )  + ( -45:2pc )  + +",
    "( 15:4pc ) arc ( 15:60:4pc )  cycle ; ( 5.5,0 ) + ( 135:1pc ) ",
    "+ ( -45:2pc )  + +",
    "( 15:4pc ) arc ( 15:60:4pc )  cycle ;    ( 5.5,0 ) + ( 15:4pc ) arc ( 15:60:4pc ) ; ( 5.5,0 ) + ( 30:4.25pc ) node[right]@xmath1 ;    ( 5.5,0 ) + ( 135:1pc )  + ( -45:2pc ) ; ( 5.5,0.2pc ) + ( -45:1.75pc ) node[below left]@xmath14 ;      we begin with a case where the literature already contains a comprehensive answer to the question ( q2 ) .",
    "the most natural type of random embedding is a uniformly random projector .",
    "that is , @xmath5 is a partial unitary map satisfies the condition @xmath15 , where @xmath16 is the transpose operation and @xmath17 is the identity map . ] whose null space , @xmath18 , is drawn uniformly at random from the haar measure on the grassmann manifold of subspaces in @xmath2 with codimension @xmath6 .",
    "the invariance properties of the distribution of @xmath7 allow for a complete analysis of its action on @xmath1 , the spherically convex set  ( * ? ? ? * chap .",
    "recent research  @xcite has shown how to convert the complicated exact formulas into interpretable results .",
    "the modern theory is expressed in terms of a geometric functional @xmath19 , called the statistical dimension : @xmath20 , \\quad\\text{where $ { { \\bm{g}}}$ is $ { \\textsc{normal}}({{\\bm{0 } } } , { \\mathbf{i}})$.}\\ ] ] the statistical dimension is increasing with respect to set inclusion , and its values range from zero ( for the empty set ) up to @xmath0 ( for the whole sphere ) .",
    "furthermore , the functional can be computed accurately in many cases of interest .",
    "see section  [ sec : stat - dim ] for more details .",
    "the statistical dimension demarcates a phase transition in the behavior of a uniformly random projector @xmath7 as the embedding dimension @xmath6 varies . for a closed , spherically",
    "convex set @xmath1 , the results  ( * ? ? ?",
    "i and prop .",
    "10.2 ) demonstrate that @xmath21 the number @xmath22 is a positive universal constant . in other terms",
    ", a uniformly random projection @xmath23 of a spherically convex set @xmath1 is likely to succeed precisely when the embedding dimension @xmath6 is larger than the statistical dimension @xmath19 .",
    "see figure  [ fig : intro - orthant - universal ] for a plot of the exact probability that a uniformly random projector annihilates a point in a specific set @xmath1 .",
    "[ rem : intro - phase - details ] the results  ( * ? ? ?",
    "7.1 ) and  ( * ? ? ?",
    "a ) contain good bounds for the probabilities in  .",
    "the probabilities can be approximated more precisely by introducing a second geometric functional  @xcite .",
    "these estimates depend on the spherical crofton formula  ( * ? ? ?",
    "( 6.62 ) , ( 6.63 ) ) , which gives the _ exact _ probabilities in a less interpretable form .",
    "related phase transition results can also be obtained via the gaussian minimax theorem ; see  ( * ? ? ?",
    "3.4 ) ,  ( * ? ? ?",
    "2.9 ) , @xcite , or  ( * ? ?",
    "ii.1 ) . see  @xcite for other results on uniformly random projectors .",
    "the research outlined in section  [ sec : intro - phase - transition ] delivers a complete account of how a uniformly random projector performs in the presence of some convexity .",
    "in contrast , the literature contains almost no precise information about the behavior of non - uniform random projectors .    nevertheless , in applications",
    ", we may prefer  or be forced  to work with a non - uniform random projector .",
    "here is a motivating example .",
    "many algorithms for numerical linear algebra now depend on randomized dimension reduction . in this context ,",
    "uniformly random projectors are expensive to construct , to store , and to perform arithmetic with .",
    "it is more appealing to implement a random projector that is discrete , or sparse , or structured .",
    "the lack of detailed theoretical information about how these projectors behave makes it difficult to design numerical methods with guaranteed performance .",
    "this plot describes the behavior of four types of random projectors applied to the set of unit vectors with nonnegative coordinates : @xmath24 .",
    "the * dashed line * marks the statistical dimension of the set : @xmath25 .",
    "the * gray curve * interpolates the exact probability that a uniformly random projector @xmath26 succeeds ( i.e. , @xmath10 ) as a function of the embedding dimension @xmath6 .",
    "the * markers * indicate the empirical probability ( over 100 trials ) that dimension reduction succeeds for a random projector with the specified distribution .",
    "see sections  [ sec : intro - other - projs ] and  [ sec : repro - research ] for further details . ]",
    "we can , however , use computation to investigate the behavior of non - uniform random projectors .",
    "figure  [ fig : intro - orthant - universal ] presents the results of the following experiment .",
    "consider the set @xmath1 of unit vectors in @xmath27 with nonnegative coordinates : @xmath28 according to  , below , the statistical dimension @xmath25 , so the formula   tells us to expect a phase transition in the behavior of a uniformly random projector when the embedding dimension @xmath29 . using  ( * ? ? ?",
    "5.3 and eqn .",
    "( 5.10 ) ) , we can compute the exact probability that a uniformly random projector @xmath26 succeeds as a function of @xmath6 .",
    "against this baseline , we compare the empirical probability ( over 100 trials ) that a random projector with independent rademacher with equal probability . ]",
    "entries succeeds .",
    "we also display experiments for a 20% nonzero rademacher projector and for a projector with student @xmath30 entries .",
    "see section  [ sec : repro - research ] for more information .    from this experiment",
    ", we discover that all three projectors act almost exactly the same way as a uniformly random projector",
    "! this universality phenomenon is remarkable because the four projectors have rather different distributions . at present",
    ", the literature contains no information about when  or why  this phenomenon occurs .",
    "the central goal of this paper is to show that there is a substantial class of random projectors for which the phase transition in the embedding dimension is universal .",
    "here is a rough statement of the main result .",
    "let @xmath1 be a closed , spherically convex set in @xmath2 .",
    "suppose that the entries of the matrix of the random projector @xmath5 are independent , standardized , and symmetric , has the same distribution as its negation @xmath31 . ] with a modest amount of regularity .",
    "have five uniformly bounded moments . ] in particular , we may consider random projectors that have an arbitrarily small , but constant , proportion of nonzero entries .",
    "for this class of random projectors , we will demonstrate that @xmath32 the little - o notation suppresses constants that depend only on the regularity of the random variables that populate @xmath7 .",
    "see theorem  [ thm : univ - embed ] in section  [ sec : thm - univ - embed ] for a more complete statement .",
    "the result   states that a random projector @xmath7 is likely to succeed for a spherically convex set @xmath1 precisely when the embedding dimension @xmath6 exceeds the statistical dimension @xmath19 of the set .",
    "we learn that the phase transition in the embedding dimension is universal over our class of random projectors , provided that @xmath1 is not too small as compared with the ambient dimension @xmath0 .",
    "note that a random projector @xmath33 with standard normal entries has the same behavior as a @xmath34 uniformly random projector because , almost surely , the null space of @xmath35 is a uniformly random subspace of @xmath2 with codimension @xmath6 .",
    "this analysis explains the dominant features of the experiment in figure  [ fig : intro - orthant - universal ] !    ( 0,-0.5 )  ( 0,2.75 ) ; ( 0,2.75 ) node[below left]@xmath12 ;    ( -1.5,0 ) ",
    "( 6,0 ) ; ( -1,0 ) node[above]@xmath13 ;    ( 0,0 ) circle ( 1pt ) ;    ( 3,0 )  ( 3,2 )  ( 5,2 )  ( 5,0 )  cycle ; ( 3,0 ) ",
    "( 5,0 )  cycle ;    ( 3,2 ) to[out=90,in=180 ] ( 4,2.5 ) to[out=0,in=90 ] ( 5,2 ) to[out=-90,in=0 ] ( 3.5,1 ) to[out=180,in=-90 ] ( 3,2 ) ; ( 3,2 ) to[out=90,in=180 ] ( 4,2.5 ) to[out=0,in=90 ] ( 5,2 ) to[out=-90,in=0 ] ( 3.5,1 ) to[out=180,in=-90 ] ( 3,2 ) ; ( 4,2 ) node@xmath36 ;    ( 3,0 )  ( 5,0 ) ; ( 4,-0.1 ) node[below]@xmath37 ;    ( 0,.1 )  ( 3,.1 ) ; ( 1.5,0.1 ) node[above ] @xmath38 ;      it is also a matter of significant interest to understand the _ stability _ of randomized dimension reduction .",
    "we quantify the stability of the random projector @xmath5 on a compact , convex set @xmath36 in @xmath2 using the restricted minimum singular value : @xmath39 when the restricted minimum singular value @xmath40 is large , the random projection @xmath41 is far from the origin , so the embedding is very stable . that is , we can deform either the projector @xmath7 or the set @xmath36 and still be sure that the embedding succeeds .",
    "when the restricted minimum singular value is small , the random projection is unstable .",
    "when it is zero , the random projection fails .",
    "see figure  [ fig : intro - rsv ] for a diagram .",
    "our second major theorem is a universality law for the restricted minimum singular values of a random projector .",
    "this result is expressed in terms of a geometric functional @xmath42 , called the @xmath6-excess width of @xmath36 : @xmath43 the @xmath6-excess width increases with the parameter @xmath6 , and it decreases with respect to set inclusion .",
    "the typical scale of @xmath42 is @xmath44 .",
    "in addition , the excess width can be evaluated precisely in many situations of interest .",
    "see section  [ sec : excess - width ] for more details .    now",
    ", suppose that the entries of the matrix of @xmath5 are independent , standardized , and symmetric , with a modest amount of regularity .",
    "for a compact , convex subset @xmath36 of the unit ball in @xmath2 , we will establish that @xmath45 the little - o notation suppresses constants that depend only on the regularity of the random variables .",
    "theorem  [ thm : univ - rsv ] in section  [ sec : thm - univ - rsv ] contains a more detailed statement of  .    in summary ,",
    "provided that the set @xmath36 is not too small , the restricted minimum singular value @xmath46 depends primarily on the geometry of the set @xmath36 and the embedding dimension @xmath6 , rather than on the distribution of the random projector @xmath7 .",
    "see figure  [ fig : intro - orthant - rsv ] for a numerical illustration of this fact .",
    "this plot describes the behavior of three types of random projectors applied to the probability simplex @xmath47 .",
    "the * dashed line * marks the minimum dimension @xmath29 where uniformly random embedding of the set @xmath48 is likely to succeed .",
    "the * gray curve * interpolates the value of the positive part @xmath49 of the @xmath6-excess width of @xmath48 , obtained from the asymptotic calculation  .",
    "the * markers * give an empirical estimate ( over 100 trials ) for the restricted minimum singular value @xmath50 of a random projector @xmath26 drawn from the specified distribution .",
    "see section  [ sec : repro - research ] for more information . ]      randomized dimension reduction and , more generally , random matrices have become ubiquitous in the information sciences . as a consequence , the universality laws that we outlined in section  [ sec : intro - univ - law1 ] and  [ sec : intro - univ - law2 ]",
    "have a wide range of implications .",
    "signal processing : :    the main idea in the field of compressed sensing is that we can    acquire information about a structured signal by taking random linear    measurements .",
    "the literature contains extensive empirical evidence    that many types of random measurements behave in an indistinguishable    fashion .",
    "our work gives the first explanation of this phenomenon .",
    "( section  [ sec : signal - recovery ] ) stochastic geometry : :    our results also indicate that the facial structure of the convex hull    of independent random vectors , drawn from an appropriate class , does    not depend heavily on the distribution .",
    "( section  [ sec : faces ] ) coding theory : :    random linear codes provide an efficient way to protect transmissions    against error .",
    "we demonstrate that a class of random codes is    resilient against sparse corruptions .",
    "the number of errors that can be    corrected does not depend on the specific choice of codebook .",
    "( section  [ sec : coding ] ) numerical analysis : :    our research provides an engineering design principle for numerical    algorithms based on randomized dimension reduction .",
    "we can select the    random projector that is most favorable for implementation and still    be confident about the detailed behavior of the algorithm .",
    "this    approach allows us to develop efficient numerical methods that also    have rigorous performance guarantees .",
    "( section  [ sec : rand - nla ] ) random matrix theory : :    our work leads to a new proof of the bai  yin law for the minimum    singular value of a random matrix with independent entries .",
    "( section  [ sec : subspace - embed ] ) high - dimensional statistics : :    the lasso is a widely used method for performing regression and    variable selection . we demonstrate that the prediction error    associated with a lasso estimator is universal across a large class of    random designs and statistical error models . we also show that    least - absolute - deviation ( lad ) regression can correct a small number    of arbitrary statistical errors for a wide class of random designs .",
    "( section  [ sec : lasso - err ] and remark  [ rem : lad ] ) neuroscience : :    our universality laws may even have broader scientific significance .",
    "it has been conjectured , with some experimental evidence , that the    brain may use dimension reduction to compress information  @xcite .",
    "our    universality laws suggest that many types of uncoordinated ( i.e. ,    random ) activity lead to dimension reduction methods with the same    behavior .",
    "this result indicates that the hypothesis of neural    dimensionality reduction may be biologically plausible .",
    "the universality phenomenon developed in this paper extends beyond the results that we establish , but there are some ( apparently ) related problems where universality does not hold .",
    "let us say a few words about these examples and non - examples .",
    "first , it does not appear important that the random projector @xmath7 has independent entries .",
    "there is extensive evidence that structured random projectors also have some universality properties ; for example , see  @xcite .",
    "second , the restricted minimum singular value is not the only type of functional where universality is visible .",
    "for instance , suppose that @xmath51 is a convex , lipschitz function .",
    "consider the quantity @xmath52 optimization problems of this form play a central role in contemporary statistics and machine learning .",
    "it is likely that the value of this optimization problem is universal over a wide class of random projectors .",
    "furthermore , we believe that our techniques can be adapted to address this question . on the other hand , geometric functionals involving non - euclidean norms need not exhibit universality .",
    "consider the @xmath53 restricted minimum singular value @xmath54 there are nontrivial sets @xmath36 where the value of the optimization problem   varies a lot with the choice of the random projector @xmath7 .",
    "for instance , let @xmath55 be the first standard basis vector , and define the shifted euclidean ball @xmath56 using theorem  [ thm : univ - embed ] and the calculation  ( * ? ? ?",
    "3.4 ) of the statistical dimension of a circular cone , we can verify that there is a universal phase transition for successful embedding of the set @xmath57 when the embedding dimension @xmath58 .",
    "the result   implies that the minimum restricted singular value of @xmath57 also takes a universal value . at the same time",
    ", figure  [ fig : l1-rsv - nonuniversal ] illustrates that the functional   is not universal for the set @xmath57 .",
    "finally , functionals involving maximization do not necessarily exhibit universality .",
    "the restricted maximum singular value is defined as @xmath59 it is not hard to produce examples where the restricted maximum singular value depends on the choice of the random matrix @xmath7 .",
    "for instance , figure  [ fig : max - rsv - nonuniversal ] demonstrates that the random projector @xmath7 has a substantial impact on the maximum singular value @xmath60 restricted to the probability simplex @xmath61 in @xmath2 .",
    "this observation may surprise researchers in random matrix theory because the ordinary maximum singular value is universal over the class of random matrices with independent entries  ( * ? ? ?",
    "* thm .  3.10 ) .",
    "restricted minimum singular value . _",
    "[ fig : l1-rsv - nonuniversal ] this plot describes the behavior of four types of random projectors applied to the set @xmath62 , where @xmath63 is the first standard basis vector .",
    "the * dashed line * stands at the phase transition @xmath64 for the embedding dimension of the set @xmath65 .",
    "the * markers * give an empirical estimate ( over 100 trials ) for the @xmath53 restricted minimum singular value @xmath66 of a random projector @xmath67 with the specified distribution as a function of the embedding dimension @xmath6 .",
    "see section  [ sec : repro - research ] for more details . ]",
    "this plot describes the behavior of four types of random projectors applied to the probability simplex @xmath47 .",
    "the * markers * give an empirical estimate ( over 100 trials ) for the restricted maximum singular value @xmath68 of a random projector @xmath26 with the specified distribution .",
    "see section  [ sec : repro - research ] for more details . ]",
    "this paper is accompanied by ` matlab ` code  @xcite that reproduces each figure from stored data .",
    "this software can also repeat the numerical experiments to obtain new instances of each figure . by modifying the parameters in the code",
    ", the reader may explore how changes affect the universality phenomenon .",
    "we omit meticulous descriptions of the numerical experiments from the text because these recitations are tiresome for the reader and the code offers superior documentation .",
    "this paper is divided into five parts .",
    "part  [ part : main ] offers a complete presentation of our universality laws , some comments about the proofs , and some prospects for further research .",
    "part  [ part : applications ] outlines the applications of universality in several disciplines , and it contains more empirical confirmation of our analysis . part  [ part : rsv ] presents the lengthy proof that the restricted minimum singular value exhibits universal behavior ; this argument also yields the condition in which randomized embedding is likely to succeed .",
    "part  [ part : rap ] contains the proof of the condition in which randomized embedding is likely to fail . finally , part  [ part : back - matter ] includes background results that are used throughout the paper , the acknowledgments , and the list of works cited .",
    "let us summarize our notation .",
    "we use italic lowercase letters ( for example , @xmath69 ) for scalars , boldface lowercase letters ( @xmath70 ) for vectors , and boldface uppercase letters ( @xmath71 ) for matrices .",
    "uppercase italic letters ( @xmath72 ) may denote scalars , sets , or random variables , depending on the context .",
    "roman letters ( @xmath73 , @xmath22 ) denote universal constants that may change from appearance to appearance .",
    "we sometimes delineate specific constant values with subscripts ( @xmath74 ) .    given a vector @xmath70 and a set @xmath75 of indices , we write @xmath76 for the vector restricted to those indices . in particular , @xmath77 is the @xmath78th coordinate of the vector . given a matrix @xmath71 and sets @xmath79 and @xmath75 of row and column indices , we write @xmath80 for the submatrix indexed by @xmath79 and @xmath75 . in particular , @xmath81 is the component in the @xmath82 position of @xmath71 .",
    "if there is a single index @xmath83 , it always refers to the _ column _ submatrix indexed by @xmath75 .",
    "we always work in a real euclidean space . the symbol @xmath84 is the unit ball in @xmath85 , and @xmath86 is the unit sphere in @xmath85 .",
    "the unadorned norm @xmath87 refers to the @xmath88 norm of a vector or the @xmath88 operator norm of a matrix .",
    "we use the notation @xmath89 for the standard inner product of vectors @xmath90 and @xmath91 with the same length .",
    "we write @xmath16 for the transpose of a vector or a matrix .    for a real number @xmath69 ,",
    "we define the positive - part and negative - part functions : @xmath92 these functions bind before powers , so @xmath93 is the square of the positive part of @xmath69 .    the symbols @xmath94 and @xmath95 refer to the expectation and variance of a random variable , and @xmath96 returns the probability of an event .",
    "we use the convention that powers bind before the expectation , so @xmath97 returns the expectation of the square .",
    "we write @xmath98 for the 01 indicator random variable of the event @xmath72",
    ".    a standardized random variable has mean zero and variance one .",
    "a symmetric random variable @xmath99 has the same distribution as its negation @xmath31 .",
    "we reserve the letter @xmath100 for a standard normal random variable ; the boldface letters @xmath101 are always standard normal vectors ; and @xmath35 is a standard normal matrix .",
    "the dimensions are determined by context .",
    "[ part : main ]    this part of the paper introduces two new universality laws , one for the phase transition in the embedding dimension and a second one for the restricted minimum singular values of a random projector",
    ". we also include some high - level remarks about the proofs , but we must postpone the difficult arguments to parts  [ part : rsv ] and  [ part : rap ] .    in section  [ sec : rdm - mtx ] , we introduce two models for random projectors that we use throughout the paper . section  [ sec : univ - embed ] presents the universality result for the embedding dimension , and section  [ sec : univ - rsv ] presents the result for restricted singular values .",
    "to begin , we present two models for random projectors that arise in our study of universality .",
    "one model includes bounded random matrices with independent entries , while the second allows random matrices with heavy - tailed entries .",
    "our first model contains matrices whose entries are uniformly bounded .",
    "this model is useful for some applications , and it plays a central role in the proofs of our universality results .    [",
    "mod : bdd - mtx ] fix a parameter @xmath102 .",
    "a random matrix in this model has the following properties :    * * independence .",
    "* the entries are stochastically independent random variables . *",
    "* standardization . *",
    "each entry has mean zero and variance one . *",
    "* symmetry .",
    "* each entry has a symmetric distribution . *",
    "* boundedness .",
    "* each entry @xmath99 of the matrix is uniformly bounded : @xmath103 .",
    "identical distribution of entries is not required . in some cases , which we will note , the symmetry requirement can be dropped .",
    "this model includes several types of random matrices that appear frequently in practice .",
    "consider a random matrix whose entries are independent , rademacher random variables .",
    "this type of random matrix meets the requirements of model  [ mod : bdd - mtx ] with @xmath104 .",
    "rademacher matrices provide the simplest example of a random projector .",
    "they are appealing for many applications because they are discrete .",
    "let @xmath105 $ ] be a thinning parameter .",
    "consider a random variable @xmath99 with distribution @xmath106 a random matrix whose entries are independent copies of @xmath99 satisfies model  [ mod : bdd - mtx ] with @xmath107 .",
    "these random matrices are useful because we can control the sparsity .",
    "next , we introduce a much more general class of random matrices that includes heavy - tailed examples .",
    "our main results concern random projectors from this model .",
    "[ mod : p - mom - mtx ] fix parameters @xmath108 and @xmath109 .",
    "a random matrix in this model has the following properties :    * * independence .",
    "* the entries are stochastically independent random variables . *",
    "* standardization . *",
    "each entry has mean zero and variance one .",
    "* * symmetry . *",
    "each entry has a symmetric distribution . *",
    "* bounded moments . *",
    "each entry @xmath99 has a uniformly bounded @xmath110th moment : @xmath111 .",
    "identical distribution of entries is not required .",
    "model  [ mod : p - mom - mtx ] subsumes model  [ mod : bdd - mtx ] , but it also encompasses many other types of random matrices .",
    "consider an @xmath112 random matrix @xmath35 whose entries are independent , standard normal random variables .",
    "the matrix @xmath35 satisfies the requirements of model  [ mod : p - mom - mtx ] for each @xmath108 with @xmath113 .    in some contexts",
    ", we can use a gaussian random matrix to study the behavior of a uniformly random projector .",
    "indeed , the null space , @xmath114 , of the standard normal matrix is a uniformly random subspace of @xmath85 with codimension @xmath115 , almost surely .",
    "model  [ mod : p - mom - mtx ] contains several well - studied classes of random matrices .",
    "suppose that the entries of the random matrix are independent , and each entry @xmath99 is symmetric , standardized , and uniformly subgaussian .",
    "that is , there is a parameter @xmath4 where @xmath116 these matrices are included in model  [ mod : p - mom - mtx ] for each @xmath108 with @xmath117 .",
    "rademacher , sparse rademacher , and gaussian matrices fall in this category .",
    "suppose that the entries of the random matrix are independent , and each entry @xmath99 is a symmetric , standardized , log - concave random variable .",
    "recall that a real log - concave random variable @xmath99 has a density @xmath51 of the form @xmath118 where @xmath119 is convex and @xmath120 is a normalizing constant .",
    "it can be shown  ( * ? ? ?",
    "2.4.6 ) that these matrices are included in model  [ mod : p - mom - mtx ] for any @xmath108 .",
    "in contrast with most research on randomized dimension reduction , we allow the random projector to have entries with heavy tails . here",
    "is one such example .",
    "suppose that each entry of the random matrix is an independent student @xmath121 random variable with @xmath122 degrees of freedom , for @xmath123 .",
    "this matrix also follows model  [ mod : p - mom - mtx ] for each @xmath124 .    finally , we present a general construction that takes a matrix from model  [ mod : p - mom - mtx ] and produces a sparse matrix that also satisfies the model , albeit with a larger value of the parameter @xmath125 .",
    "let @xmath126 be a random matrix that satisfies model  [ mod : p - mom - mtx ] for some value @xmath108 and @xmath109 .",
    "let @xmath105 $ ] be a thinning parameter , and construct a new random matrix @xmath127 whose entries @xmath128 are independent random variables with the distribution @xmath129 then the sparsified random matrix @xmath127 still follows model  [ mod : p - mom - mtx ] with the same value of @xmath110 and with a modified value @xmath130 of the other parameter : @xmath131 .",
    "in this section , we present detailed results which show that , for a large class of sets , the embedding dimension is universal over a large class of projectors .",
    "let us begin with a more rigorous statement of the problem .",
    "* fix the ambient dimension @xmath0 .",
    "* let @xmath36 be a nonempty , compact subset of @xmath2 that does not contain the origin .",
    "* let @xmath5 be a random projector with embedding dimension @xmath6 .",
    "we are interested in understanding the probability that the random projection @xmath41 does not contain the origin .",
    "that is , we want to study the following predicate : @xmath132 we say that the random projection succeeds when the property   holds ; otherwise , the random projection fails .",
    "our goal is to argue that there is a large class of sets and a large class of random projectors where the probability that   holds depends primarily on the choice of the embedding dimension @xmath6 and an appropriate measure of the size of the set @xmath36 . in particular , the probability does _ not _ depend significantly on the distribution of the random projector @xmath7 .",
    "the property   does not reflect the scale of the points in the set @xmath36 . as a consequence ,",
    "it is appropriate to translate the problem into a question about spherical geometry .",
    "we begin with a definition .    the spherical retraction map @xmath133 is defined as @xmath134    for every set @xmath36 in @xmath2",
    ", we have the equivalence @xmath135 therefore , we may pass to the spherical retraction @xmath136 of the set @xmath36 without loss of generality .",
    "to obtain a complete analysis of when random projection succeeds or fails , we must restrict our attention to sets that have a convexity property .",
    "a nonempty subset @xmath1 of the unit sphere in @xmath2 is spherically convex when the pre - image @xmath137 is a convex cone . by convention , the empty set is also spherically convex .",
    "in particular , suppose that @xmath138 is a compact , convex set that does not contain the origin .",
    "then the retraction @xmath139 is compact and spherically convex .",
    "next , we introduce a polarity operation for spherical sets that supports some crucial duality arguments .",
    "let @xmath1 be a subset of the unit sphere @xmath140 in @xmath2 .",
    "the polar of @xmath1 is the set @xmath141 by convention , the polar of the empty set is the whole sphere : @xmath142 .",
    "this definition is simply the spherical analog of polarity for cones .",
    "it can be verified that @xmath143 is always closed and spherically convex .",
    "furthermore , the polarity operation is an involution on the class of closed , spherically convex sets in @xmath140 .",
    "we have the intuition that , for a given compact subset @xmath36 of @xmath2 , the probability that a random projector @xmath5 succeeds decreases with the `` content '' of the set @xmath36 . in the present context ,",
    "the correct notion of content involves a geometric functional called the statistical dimension .",
    "[ def : stat - dim ] let @xmath1 be a nonempty subset of the unit sphere in @xmath2 .",
    "the statistical dimension @xmath19 is defined as @xmath144\\ ] ] in addition , define @xmath145 .",
    "we extend the statistical dimension to a general subset @xmath138 in @xmath2 using the spherical retraction : @xmath146 recall that the random vector @xmath147 has the standard normal distribution .",
    "the statistical dimension has a number of striking properties .",
    "we include a short summary ; see  ( * ? ? ?",
    "* prop .  3.1 ) and the citations there for further information .    * for a set @xmath138 in @xmath2 , the statistical dimension @xmath148 takes values in the range @xmath149 $ ] .",
    "* the statistical dimension is increasing with respect to set inclusion : @xmath150 implies that @xmath151 .",
    "* the statistical dimension agrees with the linear dimension on subspaces : @xmath152 * the statistical dimension interacts nicely with polarity : @xmath153 the same relation holds if we replace @xmath1 by a convex cone @xmath154 in @xmath2 and use conic polarity .    as a specific example of",
    ", we can evaluate the statistical dimension of the nonnegative orthant @xmath155 .",
    "indeed , the orthant is a self - dual cone , so @xmath156 there is also powerful machinery , developed in  @xcite , for computing the statistical dimension of a descent cone of a convex function .",
    "finally , we mention that the statistical dimension can often be evaluated by sampling gaussian vectors and approximating the expectation in   with an empirical average .",
    "the statistical dimension is related to the gaussian width functional . for a bounded set @xmath138 in @xmath2 ,",
    "the gaussian width @xmath157 is defined as @xmath158 for a subset @xmath1 of the unit sphere in @xmath2 , we have the inequalities @xmath159 see  ( * ? ? ?",
    "10.3 ) for a proof .",
    "the relation   allows us to pass between the squared width and the statistical dimension .",
    "the gaussian width of a set is closely related to its mean width  ( * ? ? ?",
    "the mean width is a canonical functional in the euclidean setting  ( * ? ? ?",
    "7.3 ) , but it is not quite the right choice for spherical geometry . we have chosen to work with the statistical dimension because it has many geometric properties that the width lacks in the spherical setting .    the papers  @xcite define the statistical dimension of a convex cone using intrinsic volumes . it can be shown that the statistical dimension is the canonical ( additive , continuous ) extension of the linear dimension to the class of closed convex cones  ( * ? ? ?",
    "our general definition   agrees with the original definition on this class .      we are now prepared to state our main result on the probability that a random projector succeeds or fails for a given set .",
    "[ thm : univ - embed ] fix the parameters @xmath108 and @xmath109 for model  [ mod : p - mom - mtx ] .",
    "choose parameters @xmath160 and @xmath161 .",
    "there is a number @xmath162 for which the following statement holds .",
    "suppose that    * the ambient dimension @xmath163 .",
    "* @xmath36 is a nonempty , compact subset of @xmath2 that does not contain the origin . *",
    "the statistical dimension of @xmath36 is proportional to the ambient dimension : @xmath164 . *",
    "the @xmath34 random projector @xmath7 obeys model  [ mod : p - mom - mtx ] with parameters @xmath110 and @xmath125 .    then @xmath165 the constant @xmath166 depends only on the parameter @xmath110 in the random matrix model .",
    "section  [ sec : thm1-strategy ] summarizes our strategy for establishing theorem  [ thm : univ - embed ] .",
    "the detailed proof of theorem  [ thm : univ - embed ] appears in section  [ sec : rsv - four - to - embed - univ ] ; the detailed proof of theorem  [ thm : univ - embed ] appears in section  [ sec : car - to - embed ] .",
    "let us mention that stronger probability bounds hold when the random projector is drawn from model  [ mod : bdd - mtx ]",
    ".    for any random projector @xmath7 drawn from model  [ mod : p - mom - mtx ] , theorem  [ thm : univ - embed ] ensures that the random projection @xmath41 is likely to succeed when the embedding dimension @xmath6 exceeds the statistical dimension @xmath167 .",
    "similarly , theorem  [ thm : univ - embed ] shows that @xmath41 is likely to fail when the embedding dimension @xmath6 is smaller than the statistical dimension @xmath167 , provided that @xmath136 is spherically convex .",
    "note that both of these interpretations require that the statistical dimension @xmath167 is not too small as compared with the ambient dimension @xmath0 .",
    "we have already seen a concrete example of theorem  [ thm : univ - embed ] at work . in view of the calculation   of the statistical dimension of the orthant",
    ", the theorem provides a satisfying explanation of figure  [ fig : intro - orthant - universal ] !",
    "when the random projector @xmath7 is gaussian , the result theorem  [ thm : univ - embed ] follows from gordon  ( * ? ? ?",
    "3.4 ) , while the conclusion   seems to be more recent  ( * ? ? ?",
    "i ) . to our knowledge",
    ", the literature contains no precedent for theorem  [ thm : univ - embed ] for general sets and for non - gaussian projectors .",
    "we can identify only a few sporadic special cases .",
    "donoho & tanner  @xcite studied the problem of recovering a `` saturated vector '' from random measurements via @xmath168 minimization .",
    "their work can be interpreted as a statement about random embeddings of the set of unit vectors with nonnegative coordinates .",
    "they demonstrate that the phase transition in the embedding dimension is universal when the rows of the projector matrix are independent , symmetric random vectors with a density ; this result actually follows from classical work of schfli  @xcite and wendel  @xcite .",
    "let us emphasize that this result does not apply to discrete random projectors .",
    "bayati et al .",
    "@xcite have studied the problem of recovering a sparse vector from random measurements via @xmath53 minimization .",
    "their work can be interpreted as a result on the embedding dimension of the set of descent directions of the @xmath53 norm at a sparse vector .",
    "they showed that , asymptotically , the phase transition in the embedding dimension is universal .",
    "their result requires the projector matrix to contain independent , subgaussian entries that are absolutely continuous with respect to the gaussian density .",
    "see section  [ sec : l1-min ] for more discussion of this result .",
    "there are also many papers in asymptotic convex geometry and mathematical signal processing that contain theory about the order of the embedding dimension for projectors from model  [ mod : p - mom - mtx ] ; see  @xcite .",
    "these results do not allow us to reach any conclusions about the existence of a phase transition or its location .",
    "there is also an extensive amount of empirical work , such as  @xcite , that suggests that phase transitions are ubiquitous in high - dimensional signal processing problems , but there has been no theoretical explanation of the universality phenomenon until now .",
    "the proof of theorem  [ thm : univ - embed ] depends on converting the geometric question to an analytic problem .",
    "first , recall the equivalence  , which allows us to pass from the compact set @xmath36 to its spherical retraction @xmath169 .",
    "next , we identify two analytic quantities that determine whether a linear map annihilates a point in the set @xmath1 .",
    "[ prop : annihilate ] let @xmath1 be a closed subset of the unit sphere @xmath140 in @xmath2 , and let @xmath170 be a linear map . then @xmath171 recall that @xmath172 is the smallest convex cone containing the set @xmath173 .",
    "the implication   is quite easy to check . at each point @xmath174",
    ", we have @xmath175 , which implies that @xmath176 . in other words , @xmath177 .",
    "the second implication   follows from a spherical duality principle .",
    "suppose that @xmath1 and @xmath178 are closed and spherically convex .",
    "if @xmath179 and @xmath180 do not intersect , then @xmath1 and @xmath178 must intersect ; see  ( * ? ?",
    "* thm  ( 2.7 ) ) .",
    "the analytic condition @xmath181 ensures that @xmath182 lies at a positive distance from @xmath183 .",
    "it follows that @xmath179 does not intersect @xmath183 . by duality",
    ", we conclude that @xmath184 , which yields  .    in view of proposition  [ prop : annihilate ] , we can establish theorem  [ thm : univ - embed ] by showing that @xmath185 this condition follows from a universality result , corollary  [ cor : rsv - four ] , for the restricted minimum singular value .",
    "the proof appears in section  [ sec : rsv - four - to - embed - univ ] .",
    "similarly , we can establish theorem  [ thm : univ - embed ] by showing that @xmath186 this condition follows from a specialized argument that culminates in corollary  [ cor : car - four ] .",
    "the proof appears in section  [ sec : car - to - embed ] .",
    "it is an interesting challenge to delineate the scope of the universality phenomenon described in theorem  [ thm : univ - embed ] .",
    "we believe that there remain many opportunities for improving on this result .",
    "* theorem  [ thm : univ - embed ] only shows that the width of the phase transition is @xmath187 .",
    "it is known  ( * ? ? ?",
    "7.1 ) that the width of the phase transition has order @xmath188 for a projector with standard normal entries .",
    "how wide is the phase transition for more general random projectors ? *",
    "a related question is whether theorem  [ thm : univ - embed ] holds for those sets @xmath36 whose statistical dimension @xmath167 is much smaller than the ambient dimension .",
    "* there is empirical evidence that the location of the phase transition is universal over a class wider than model  [ mod : p - mom - mtx ] . in particular",
    ", results like theorem  [ thm : univ - embed ] may be valid for structured random projectors .",
    "* figure  [ fig : intro - orthant - universal ] suggests that the _ probability _ of successful embedding is universal . under",
    "what conditions can this observation be formalized ?    in summary ,",
    "theorem  [ thm : univ - embed ] is just the first step toward a broader theory of universality in high - dimensional stochastic geometry .",
    "this section describes a quantitative universality law for random projectors .",
    "we show that the restricted minimum singular value of a random projector takes the same value for every projector in a substantial class .",
    "this type of result provides information about the stability of randomized dimension reduction .",
    "let us frame our assumptions :    * fix the ambient dimension @xmath0 .",
    "* let @xmath36 be a nonempty , compact subset of the unit ball @xmath189 .",
    "* let @xmath5 be a random projector with embedding dimension @xmath6 .    in this section , our goal is to understand the distance from the random projection @xmath41 to the origin .",
    "the following definition captures this property .",
    "let @xmath170 be a linear map , and let @xmath138 be a nonempty subset of @xmath2 .",
    "the restricted minimum singular value of @xmath71 with respect to the set @xmath138 is the quantity @xmath190 more briefly , we write restricted singular value or rsv .",
    "proposition  [ prop : annihilate ] shows that the restricted minimum singular value is a quantity of interest when studying the embedding dimension of a projector .",
    "it is also productive to think about @xmath191 as a measure of the stability of inverting the map @xmath71 on the image @xmath192 .",
    "in particular , note that the restricted singular value @xmath191 is a generalization of the ordinary minimum singular value @xmath193 , which we obtain from the selection @xmath194 .",
    "it is clear that the restricted singular value @xmath195 decreases with respect to set inclusion . in other words ,",
    "the restricted singular value depends on the `` content '' of the set @xmath138 . in the case of a random projector ,",
    "the following geometric functional provides the correct notion of content .",
    "[ def : excess - width ] let @xmath196 be a positive number , and let @xmath138 be a nonempty , bounded subset of @xmath2 .",
    "the @xmath196-excess width of @xmath138 is the quantity @xmath197    versions of the excess width appear as early as the work of gordon  ( * ? ? ?",
    "it has also come up in recent papers  @xcite on the analysis of gaussian random projectors .",
    "the excess width has a number of useful properties .",
    "these results are immediate consequences of the definition .    * for a subset @xmath138 of the unit ball in @xmath2 , the @xmath196-excess width satisfies the bounds @xmath198 in particular , the excess width can be positive or negative . *",
    "the @xmath196-excess width is weakly increasing in @xmath196 .",
    "that is , @xmath199 implies @xmath200 .",
    "* the @xmath196-excess width is decreasing with respect to set inclusion : @xmath150 implies @xmath201 . *",
    "the @xmath196-excess width is absolutely homogeneous : @xmath202 for @xmath203 . *",
    "the excess width   is related to the gaussian width  : @xmath204 using  , we can also relate the excess to the statistical dimension : @xmath205    the term `` excess width '' is not standard , but the formula   suggests that this moniker is appropriate . according to theorem  [ thm : univ - embed ] , the sign @xmath206 of the excess width @xmath207 indicates that a random projection @xmath23 with embedding dimension @xmath6 succeeds @xmath208 or fails @xmath209 with high probability .",
    "the papers  @xcite develop methods for computing the excess width in a variety of situations .",
    "for instance , if @xmath210 is a @xmath211-dimensional subspace of @xmath2 , then @xmath212 for a more sophisticated example , consider the probability simplex @xmath213 we can develop an asymptotic formula for its excess width : @xmath214.\\ ] ] the proof of the expression   is involved , so we must omit the details ; see  @xcite for a framework for making such computations .",
    "see part  [ part : applications ] for some other examples .",
    "it is also possible to estimate the excess width numerically by approximating the expectation in   with an empirical average .      with this preparation",
    ", we can present our main result about the universality properties of the restricted minimum singular value of a random projector .",
    "[ thm : univ - rsv ] fix the parameters @xmath110 and @xmath125 for model  [ mod : p - mom - mtx ] .",
    "choose parameters @xmath215 and @xmath216 and @xmath217 .",
    "there is a number @xmath218 for which the following statement holds .",
    "suppose that    * the ambient dimension @xmath163 .",
    "* @xmath36 is a nonempty , compact subset of the unit ball @xmath189 in @xmath2 . *",
    "the embedding dimension @xmath6 is in the range @xmath219 . *",
    "the @xmath6-excess width of @xmath36 is not too small : @xmath220 . *",
    "the random projector @xmath5 obeys model  [ mod : p - mom - mtx ] with parameters @xmath110 and @xmath125 .    then @xmath221 the constant @xmath166 depends only on the parameter @xmath110 in the random matrix model .",
    "section  [ sec : thm2-strategy ] contains an overview of the argument .",
    "the detailed proof of theorem  [ thm : univ - rsv ] appears in section  [ sec : rsv - four - to - rsv - univ ] .",
    "stronger probability bounds hold when the random projector is drawn from model  [ mod : bdd - mtx ] .    for any random projector @xmath7 drawn from model  [ mod : p - mom - mtx ] , theorem  [ thm : univ - rsv ]",
    "asserts that the distance of the random projection @xmath41 from the origin is typically not much smaller than the @xmath6-excess width @xmath42 .",
    "similarly , when @xmath36 is convex , the distance of the random projection from the origin is typically not much larger than the @xmath6-excess width .",
    "these conclusions require that the excess width @xmath42 is not too small as compared with the root @xmath222 of the embedding dimension",
    ".    theorem  [ thm : univ - rsv ] is vacuous when @xmath223 .",
    "nevertheless , in case @xmath136 is spherically convex , the condition @xmath224 implies that @xmath225 with high probability because of   and theorem  [ thm : univ - embed ] .",
    "we have already seen an illustration of theorem  [ thm : univ - rsv ] in action . in view of the expression   for the excess width of the probability simplex",
    ", the theorem explains the experiment documented in figure  [ fig : intro - orthant - rsv ] !",
    "when the random projector @xmath7 is gaussian , gordon s work  ( * ? ? ?",
    "1.1 ) yields the conclusion theorem  [ thm : univ - rsv ] , while the second conclusion   appears to be more recent ; see  @xcite .    for other types of random projectors , we are not aware of any research that yields the precise value of the restricted singular value , which is required to assert universality .",
    "the literature on asymptotic convex geometry and mathematical signal processing , e.g. ,  @xcite , contains many results on the restricted singular value that provide bounds of the correct order with unspecified multiplicative constants .      one standard template for proving a universality law factors the problem into two parts .",
    "first , we compare a general random structure with a canonical structure that has additional algebraic properties .",
    "then we use these extra algebraic properties to obtain a complete analysis of the canonical structure . because of the comparison , we learn that the general structure inherits some of the behavior of the canonical structure . in particular ,",
    "this recipe summarizes lindeberg s approach  @xcite to the central limit theorem ; see  ( * ? ? ?",
    "more recently , similar methods have been directed at harder problems , such as the universality of local spectral statistics of a non - symmetric random matrix  @xcite .",
    "let us imagine how we could apply this technique to prove theorem  [ thm : univ - rsv ] .",
    "suppose that @xmath36 is a compact , convex set .",
    "after performing standard discretization and smoothing steps , we might invoke the lindeberg exchange principle to compare the restricted singular value of the @xmath34 random projector @xmath7 with that of a @xmath34 standard normal matrix @xmath35 : @xmath226 to evaluate the restricted minimum singular value of a standard normal matrix , we can use the gaussian minimax theorem , as in  ( * ? ? ?",
    "1.1 ) : @xmath227 last , we can incorporate a convex duality argument , as in  @xcite , to obtain the reverse inequality : @xmath228 unfortunately , this simple approach is not adequate .",
    "our argument ultimately follows a related pattern , but we have to overcome a number of significant technical obstacles .",
    "let us explain the most serious issue and our mechanism for handling it .",
    "we would like to apply the lindeberg exchange principle to @xmath40 to replace each entry of @xmath7 with a standard normal variable .",
    "the problem is that @xmath36 may contain a vector @xmath229 with large entries .",
    "if we try to replace the columns of @xmath7 associated with the large components of @xmath229 , we incur an intolerably large error .",
    "moreover , for any given coordinate @xmath78 , the set @xmath36 may contain a vector @xmath230 that takes a large value in the distinguished coordinate @xmath78 .",
    "this fact seems to foreclose the possibility of replacing any part of the matrix @xmath7 .",
    "we address this challenge by dissecting the index set @xmath36 into pieces .",
    "for each ( small ) subset @xmath75 of @xmath231 , we define the set @xmath232 of vectors in @xmath36 whose components in @xmath233 are small .",
    "first , we argue that the restricted singular value of a subset does not differ much from the restricted singular value of the entire set : @xmath234 we may now use the lindeberg method to make the comparison @xmath235 where the matrix @xmath236 is a hybrid of the form @xmath237 and @xmath238 .",
    "that is , we only replace the columns of @xmath7 listed in the set @xmath233 with independent standard normal variables . at this point",
    ", we need to compare the minimum singular value of @xmath236 restricted to the subset @xmath232 against the excess width of the full set : @xmath239 in other words , the remaining part @xmath240 of the original matrix plays a negligible role in determining the restricted singular value .",
    "we perform this estimate using the gaussian minimax theorem  @xcite , some convex duality arguments  @xcite , and some coarse results from nonasymptotic random matrix theory  @xcite .",
    "see part  [ part : rsv ] for detailed statements of the main technical results that support theorem  [ thm : univ - rsv ] and the proofs of these results .",
    "most of the ingredients are standard tools from modern applied probability , but we have combined them in a subtle way . to make the long argument clearer ,",
    "we have attempted to present the proof in a multi - resolution fashion where pieces from each level are combined explicitly at the level above .",
    "we expect that there are a number of avenues for extending theorem  [ thm : univ - rsv ] .    *",
    "our analysis shows that the error in estimating @xmath40 by the excess width @xmath42 is at most @xmath241 . in case @xmath7",
    "is a standard normal matrix , the error actually has constant order  ( * ? ? ?",
    "can we improve the error bound for more general random projectors ? *",
    "a related question is whether the universality of @xmath40 persists when the excess width @xmath42 is very small in comparison with @xmath222 .",
    "* there is some empirical evidence that the restricted minimum singular value may have universality properties for a class of random projectors wider than model  [ mod : p - mom - mtx ] . * our proof",
    "can probably be adapted to show that other types of functionals exhibit universal behavior .",
    "for example , we can study @xmath242 where @xmath51 is a convex , lipschitz function .",
    "this type of optimization problem plays an important role in statistics and machine learning .    at the same time , we know that many natural functionals do _ not _ exhibit universal behavior .",
    "in particular , consider the restricted _ maximum _ singular value : @xmath243 there are many sets @xmath138 where the maximum singular value @xmath244 does not take a universal value for projectors in model  [ mod : p - mom - mtx ] ; for example , see figure  [ fig : max - rsv - nonuniversal ] .",
    "it is an interesting challenge to determine the full scope of the universality principle uncovered in theorem  [ thm : univ - rsv ] .",
    "[ part : applications ]    in this part of the paper , we outline some of the implications of theorems  [ thm : univ - embed ] and  [ thm : univ - rsv ] in the information sciences .",
    "we focus on problems involving least squares and @xmath53 minimization to reduce the number of distinct calculations that we need to perform ; nevertheless the techniques apply broadly . in an effort to make the presentation shorter and more intuitive ,",
    "we have also chosen to sacrifice some precision .",
    "section  [ sec : signal - recovery ] discusses structured signal recovery and , in particular , the compressed sensing problem .",
    "section  [ sec : coding ] presents an application in coding theory .",
    "section  [ sec : rand - nla ] describes how the results allow us to analyze randomized algorithms for numerical linear algebra .",
    "finally , section  [ sec : lasso - err ] gives a universal formula for the prediction error in a sparse regression problem .",
    "in signal processing , dimension reduction arises as a mechanism for signal acquisition .",
    "the idea is that the number of measurements we need to recover a structured signal is comparable with the number of degrees of freedom in the signal , rather than the ambient dimension . given the measurements , we can reconstruct the signal using nonlinear algorithms that take into account our prior knowledge about the structure .",
    "the field of compressed sensing is based on the idea that _ random _ linear measurements offer an efficient way to acquire structured signals . the practical challenge in implementing this proposal is to find technologies that can perform random sampling .",
    "our universality laws have a significant implication for compressed sensing .",
    "we prove that the number of random measurements required to recover a structured signal does not have a strong dependence on the distribution of the random measurements .",
    "this result is important because most applications offer limited flexibility in the type of measurement that we can take .",
    "our theory justifies the use of a broad class of measurement ensembles .",
    "in this section , we study the phase transition that appears when we reconstruct a sparse signal from random measurements via @xmath53 minimization . for a large class of random measurements , we prove that the distribution does not affect the location of the phase transition .",
    "this result resolves a major open question  @xcite in the theory of compressed sensing .",
    "let us give a more precise description of the problem .",
    "suppose that @xmath245 is a fixed vector with precisely @xmath246 nonzero entries .",
    "let @xmath247 be a random linear map , and suppose that we have access to the image @xmath248 .",
    "we interpret this data as a list of @xmath196 samples of the unknown signal .",
    "a standard approach  @xcite for reconstructing the sparse signal @xmath249 from the data is to solve a convex optimization problem : @xmath250 minimizing the @xmath53 norm promotes sparsity in the optimization variable @xmath251 , and the constraint ensures that the virtual measurements @xmath252 are consistent with the observed data @xmath253 .",
    "we say that the optimization problem   succeeds if it has a unique solution @xmath254 that coincides with @xmath249 .",
    "otherwise , it fails . in this setting",
    ", the following challenge arises .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * the compressed sensing problem : * is the optimization problem   likely to succeed or to fail to recover @xmath249 as a function of the sparsity @xmath246 , the number @xmath196 of measurements , the ambient dimension @xmath255 , and the distribution of the random measurement matrix @xmath126 ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this question has been a subject of inquiry in thousands of papers over the last 10 years ; see the books  @xcite for more background and references .    in a series of recent papers  @xcite , the compressed sensing problem has been solved completely in the case where the random measurement matrix @xmath126 follows the standard normal distribution .",
    "see remark  [ rem : l1-prior ] for a narrative of who did what when . in brief , there exists a phase transition function @xmath256 \\to [ 0,1]$ ] defined by @xmath257 the phase transition function is increasing and convex , and it satisfies @xmath258 and @xmath259 .",
    "when the measurement matrix @xmath126 is standard normal , @xmath260 in other words , as the number @xmath196 of measurements increases , the probability of success jumps from zero to one at the point @xmath261 over a range of @xmath262 measurements .",
    "the error terms in   can be improved substantially , but this presentation suffices for our purposes .",
    "donoho & tanner  @xcite have performed an extensive empirical investigation of the phase transition in the @xmath53 minimization problem  .",
    "their work suggests that the gaussian phase transition   persists for many other types of random measurements .",
    "see figure  [ fig : l1-univ ] for a small illustration .",
    "our universality results provide the first rigorous explanation of this phenomenon for measurement matrices drawn from model  [ mod : p - mom - mtx ] .     recovery phase transition . _",
    "[ fig : l1-univ ] these plots depict the empirical probability that the @xmath53 minimization problem   recovers a vector @xmath263 with @xmath246 nonzero entries from a vector of random measurements @xmath264 .",
    "the * heatmap * indicates the empirical probability , computed over 100 trials .",
    "the * white curve * is the phase transition @xmath261 promised by proposition  [ prop : l1-univ ] .",
    "left : the random measurement matrix @xmath126 is a sparse rademacher matrix with an average of 20% nonzero entries .",
    "right : the random measurement matrix @xmath126 has independent student @xmath30 entries .",
    "see section  [ sec : repro - research ] for more details . ]",
    "[ prop : l1-univ ] assume that    * the ambient dimension @xmath255 and the number @xmath196 of measurements satisfy @xmath199 .",
    "* the vector @xmath245 has exactly @xmath246 nonzero entries . *",
    "the random measurement matrix @xmath247 follows model  [ mod : p - mom - mtx ] with parameters @xmath110 and @xmath125 .",
    "* we observe the vector @xmath248 .",
    "then , as the ambient dimension @xmath265 , @xmath266 the little - o suppresses constants that depend only on @xmath110 and @xmath125 .",
    "proposition  [ prop : l1-univ ] follows directly from our universality result , theorem  [ thm : univ - embed ] , and the established calculation   of the phase transition in the standard normal setting .",
    "the approach is quite standard .",
    "let @xmath1 be the set of unit - norm descent directions of the @xmath53 norm at the point @xmath267 .",
    "that is , @xmath268 the primal optimality condition for   demonstrates that the reconstruction succeeds if and only if @xmath9 . since the @xmath53 norm is a closed convex function , the set @xmath269 of all descent directions forms a closed , convex cone .",
    "therefore , the set @xmath1 is closed and spherically convex .",
    "it follows from theorem  [ thm : univ - embed ] that the behavior of   undergoes a phase transition at the statistical dimension @xmath19 for any random projector drawn from model  [ mod : p - mom - mtx ] .",
    "the result  ( * ? ? ? * prop .",
    "4.5 ) contains the first complete and accurate computation of the statistical dimension of a descent cone of the @xmath53 norm : @xmath270 see also  ( * ? ? ?",
    "* prop .  1 ) .",
    "this fact completes the proof .",
    "note that proposition  [ prop : l1-univ ] requires the sparsity level @xmath246 to be proportional to the ambient dimension @xmath255 before it provides any information . by refining our argument",
    ", we can address the case when @xmath246 is proportional to @xmath271 for a small number @xmath272 that depends on the regularity of the random projector .",
    "the empirical work  @xcite of donoho & tanner is unable to provide statistical evidence for the universality hypothesis in the regime where @xmath246 is very small .",
    "it remains an open problem to understand how rapidly @xmath273 can vanish before the universality phenomenon fails .",
    "the paper  @xcite also contains numerical evidence that the @xmath53 phase transition persists for random measurement systems that have more structure than model  [ mod : p - mom - mtx ] .",
    "it remains an intriguing open question to understand these experiments .",
    "[ rem : l1-prior ] in early 2005 , donoho  @xcite and donoho & tanner  @xcite observed that there is a phase transition in the number of standard normal measurements needed to reconstruct a sparse signal via @xmath53 minimization .",
    "using methods from integral geometry , they were able to perform a heuristic computation of the location of the phase transition function  . in subsequent work  @xcite",
    ", they proved that the transition   is valid in some parameter regimes .",
    "they later reported extensive empirical evidence  @xcite that the distribution of the random measurement map has little effect on the location of the phase transition .    in early 2005 ,",
    "rudelson & vershynin  @xcite proposed a different approach to studying @xmath53 minimization by adapting results of gordon  @xcite that depend on gaussian process theory .",
    "stojnic  @xcite refined this argument to obtain an empirically sharp success condition for standard normal projectors , but his work did not establish a matching failure condition .",
    "stojnic s calculations were clarified and extended to other signal recovery problems in the papers  @xcite .",
    "bayati et al .",
    "@xcite is the first paper to rigorously demonstrate that the phase transition   is valid for standard normal measurements .",
    "the argument is based on a state evolution framework for an iterative algorithm inspired by statistical physics .",
    "this work also gives the striking conclusion that the @xmath53 phase transition is universal over a class of random measurement maps .",
    "this result requires the measurement matrix to have independent , standardized , subgaussian entries that are absolutely continuous with respect to the gaussian distribution . as a consequence ,",
    "the paper  @xcite excludes discrete and heavy - tailed models .",
    "furthermore , it only applies to @xmath53 minimization .",
    "in contrast , proposition  [ prop : l1-univ ] and its proof have a much wider compass .",
    "the paper  @xcite of amelunxen et al .",
    "contains the first complete integral - geometric proof of   for standard normal measurements .",
    "this work is significant because it established for the first time that phase transitions are ubiquitous in signal reconstruction problems .",
    "it also forged the first links between the approach to phase transitions based on integral geometry and those based on gaussian process theory .",
    "the papers  @xcite build on these ideas to obtain precise estimates of the success probability in  .",
    "subsequently , stojnic  @xcite refined the gaussian process methods to obtain more detailed information about the behavior of the errors in noisy variants of the @xmath53 minimization problem .",
    "his work has been refined and extended in a series  @xcite of papers by abbasi , oymak , thrampoulidis , hassibi , and their collaborators .",
    "because of this research , we now have a very detailed understanding of the behavior of convex signal recovery methods with gaussian measurements .    to our knowledge",
    ", the current paper is the first work that extends the type of general analysis in  @xcite beyond the confines of the standard normal model .",
    "the compressed sensing problem is the most prominent example from a large class of related questions .",
    "our universality results have implications for this entire class of problems .",
    "we include a brief explanation .",
    "let @xmath274 be a proper . ]",
    "convex function whose value increases with the `` complexity '' of its argument .",
    "the @xmath53 norm is an example of a complexity measure that is appropriate for sparse signals  @xcite .",
    "similarly , the schatten 1-norm is a good complexity measure for low - rank matrices  @xcite .",
    "let @xmath245 be a vector with `` low complexity . ''",
    "draw a random linear map @xmath247 , and suppose we have access to @xmath267 only through the measurements @xmath248 .",
    "we can attempt to reconstruct @xmath267 by solving the convex optimization problem @xmath275 in other words , we find the vector with minimum complexity that is consistent with the observed data .",
    "we say that   succeeds when it has a unique optimal point that coincides with @xmath267 ; otherwise , it fails .",
    "the paper  @xcite proves that there is a phase transition in the behavior of   when @xmath126 is standard normal .",
    "our universality law , theorem  [ thm : univ - embed ] , allows us to extend this result to include every random projector from model  [ mod : p - mom - mtx ] .",
    "define the set @xmath1 of unit - norm descent directions of @xmath51 at the point @xmath267 : @xmath276 then , as the ambient dimension @xmath265 , @xmath277 in other words , there is a phase transition in the behavior of   when the number @xmath196 of measurements equals the statistical dimension @xmath19 of the set of descent directions of @xmath51 at the point @xmath267 .",
    "see the papers  @xcite for some general methods for computing the statistical dimension of a descent cone .",
    "we conclude this section with a few additional remarks about the scope of our results on signal recovery .",
    "first , we discuss some geometric applications .",
    "second , we mention some other signal processing problems that can be studied using the same methods .      proposition  [ prop : l1-univ ] can be understood as a statement about the facial structure of a random projection of the @xmath53 ball .",
    "equivalently , it provides information about the facial structure of the convex hull of random points .",
    "suppose that @xmath278 is the @xmath255-dimensional @xmath53 ball , and fix an @xmath279-dimensional face @xmath280 of @xmath278 .",
    "let @xmath247 be a random projector from model  [ mod : p - mom - mtx ] .",
    "we say that @xmath126 preserves the face @xmath280 when @xmath281 is an @xmath279-dimensional face of the projection @xmath282",
    ". we can reinterpret proposition  [ prop : l1-univ ] as saying that @xmath283 the connection between @xmath53 minimization and the facial structure of the @xmath53 ball was identified in  @xcite ; see also  ( * ? ? ?",
    "10.1.1 ) .    here is another way of framing the same result .",
    "fix an index set @xmath284 with cardinality @xmath285 and a vector @xmath286 with @xmath287 entries .",
    "let @xmath288 be independent random vectors , drawn from model  [ mod : p - mom - mtx ] , and consider the absolute convex hull @xmath289 .",
    "the question is whether the set @xmath290 is an @xmath279-dimensional face of @xmath36 .",
    "we have the statements @xmath291 see the paper  @xcite for more discussion of the connection between the facial structure of polytopes and signal recovery .",
    "some universality results of this type also appear in bayati et al .",
    "@xcite .",
    "we often want to perform signal processing tasks on data after reducing its dimension . in this section ,",
    "we have focused on the problem of reconstructing a sparse signal from random measurements . here",
    "are some related problems :    * * detection .",
    "* does an observed signal consist of a template corrupted with noise ? or is it just noise ? * * classification .",
    "* does an observed signal belong to class or to class ?",
    "the literature contains many papers that propose methods for solving these problems after dimension reduction ; for example , see  @xcite .",
    "the existing analysis is either qualitative or it assumes that the dimension reduction map is gaussian .",
    "our universality laws can be used to study the precise behavior of compressed detection and classification with more general types of random projectors . for brevity ,",
    "we omit the details .",
    "one of the goals of coding theory is to design codes and decoding algorithms that can correct gross errors in transmission . in particular , it is common that some proportion of the received symbols are corrupted . in this section , we show that a large family of random codes can be decoded in the presence of structured errors .",
    "the number of errors that we can correct is universal over this family .",
    "this result is valuable because it applies to random codebooks that are closer to realistic coding schemes .",
    "the result on random decoding can also be interpreted as a statement about the behavior of the least - absolute - deviation ( lad ) method for regression .",
    "we also discuss how our universality results apply to a class of demixing problems .",
    "we work with a random linear code over the real field .",
    "consider a fixed message @xmath292 .",
    "let @xmath293 be a random matrix , which is called a codebook in this context . instead of transmitting the original message @xmath267",
    ", we transmit the coded message @xmath294 .",
    "suppose that we receive a version @xmath253 of the message where some number @xmath246 of the entries are corrupted .",
    "that is , @xmath295 where the error vector @xmath296 has at most @xmath246 nonzero entries . for simplicity , we assume that @xmath296 does not depend on the codebook @xmath126 .    in this",
    "setting , one can attempt to decode the message using an @xmath53 minimization method  @xcite .",
    "we solve the optimization problem @xmath297 in other words , we search for a message @xmath251 and a sparse corruption @xmath298 that match the received data .",
    "we say that the optimization   succeeds if it has a unique optimal point @xmath299 that coincides with @xmath300 ; otherwise it fails .",
    "decoding phase transition . _",
    "[ fig : l1-decode ] this plot shows the empirical probability that the @xmath53 method   decodes the message @xmath263 from the received message @xmath301 where the corruption @xmath302 has exactly @xmath246 nonzero entries .",
    "the codebook @xmath303 is a sparse rademacher matrix with an average of 20% nonzero entries .",
    "the * heatmap * gives the empirical probability of correct decoding , computed over 100 trials .",
    "the * white curve * is the exact phase transition @xmath304 promised by proposition  [ prop : univ - decode ] .",
    "see section  [ sec : repro - research ] for more details . ]",
    "the question is when the optimization problem   is effective at decoding the received transmission .",
    "that is , how many errors @xmath246 can we correct as a function of the message length @xmath196 and the code length @xmath255 ? the following result gives a solution to this problem for any codebook drawn from model  [ mod : p - mom - mtx ] .",
    "[ prop : univ - decode ] assume that    * the message length @xmath196 and the code length @xmath255 satisfy @xmath199 . * the message @xmath292 is arbitrary . *",
    "the error @xmath302 is an arbitrary vector with exactly @xmath246 nonzero entries . *",
    "the random codebook @xmath293 follows model  [ mod : p - mom - mtx ] with parameters @xmath110 and @xmath125 .",
    "* we observe the vector @xmath295 .",
    "then , as the message length @xmath305 , @xmath306 the function @xmath307 is defined in  .",
    "the little - o suppresses constants that depend only on @xmath110 and @xmath125 .",
    "this result is significant because it allows us to understand the behavior of this method for a sparse , discrete codebook .",
    "this type of code is somewhat closer to a practical coding mechanism than the ultra - random codebooks that have been studied in the past ; see remark  [ rem : decode - prior ] .",
    "figure  [ fig : l1-decode ] contains an illustration of how the theory compares with the actual performance of this coding scheme .    to analyze the decoding problem  , we change variables : @xmath308 and @xmath309 .",
    "we obtain the equivalent optimization problem @xmath310 the decoding procedure   succeeds if and only if the unique optimal point @xmath311 of the problem   is the pair @xmath312 .",
    "introduce the set @xmath1 of unit - norm descent directions of the @xmath53 norm at @xmath296 : @xmath313 the primal optimality condition for   shows that decoding succeeds if and only if @xmath314 .",
    "applying polarity  ( * ? ? ?",
    "( 2.7 ) ) , we obtain the equivalence that @xmath315 invoking theorem  [ thm : univ - embed ] , we discover that @xmath316 it remains to compute the statistical dimension @xmath317 to simplify the result .",
    "observe that @xmath318 the first relation is the polar identity   for the statistical dimension , and the value of the statistical dimension appears in  . combine the last two displays and revert the inequalities so that they are expressed in terms of @xmath246 .",
    "[ rem : lad ] proposition  [ prop : univ - decode ] can also be interpreted as a statement about the performance of the least - absolute deviation method for fitting models with outliers .",
    "suppose that we observe the data @xmath319 .",
    "each of the @xmath255 rows of @xmath320 is interpreted as a vector of @xmath196 measured variables for an independent subject in an experiment .",
    "the vector @xmath321 lists the coefficients in the true linear model , and the sparse vector @xmath298 contains a small number @xmath246 of arbitrary statistical errors .",
    "the least - absolute deviation method fits a model by solving @xmath322 proposition  [ prop : univ - decode ] shows that the procedure identifies the true model @xmath323 exactly , provided that the number @xmath246 of contaminated data points satisfies @xmath324 .",
    "[ rem : decode - prior ] the idea of using @xmath53 minimization for decoding in the presence of sparse errors dates at least as far back as the paper  @xcite .",
    "this scheme received further attention in the work  @xcite .",
    "later , donoho & tanner  @xcite applied phase transition calculations to assess the precise performance of this coding scheme for a standard normal codebook ; the least - absolute - deviation interpretation of this result appears in  ( * ? ? ?",
    "the paper  @xcite revisits the coding problem and develops a sharp analysis in the case where the codebook is a random orthogonal matrix .",
    "the current paper contains the first precise result that extends to codebooks with more general distributions .",
    "the decoding problem   is an example of a convex demixing problem  @xcite .",
    "our universality results can be used to study other questions of this species .",
    "let @xmath325 and @xmath326 be proper convex functions that measure the complexity of a signal .",
    "suppose that @xmath327 and @xmath328 are signals with `` low complexity . ''",
    "draw random matrices @xmath329 and @xmath330 from model  [ mod : p - mom - mtx ] .",
    "suppose that we observe the vector @xmath331 .",
    "we interpret the random matrices as known transformations of the unknown signals .",
    "for example , the matrices @xmath332 might denote dictionaries in which the two components of @xmath253 are sparse .",
    "we can attempt to reconstruct the original signal pair by solving @xmath333 in other words , we witness a superposition of two structured signals , and we attempt to find the lowest complexity pair @xmath334 that reproduces the observed data .",
    "the demixing problem succeeds if it has a unique optimal point that equals @xmath335 .    to analyze this problem",
    ", we introduce two descent sets : @xmath336 up to scaling , the descent directions of @xmath337 at the pair @xmath338 coincide with the direct product @xmath339 . the statistical dimension of a direct product of two spherical sets satisfies @xmath340 .",
    "therefore , theorem  [ thm : univ - embed ] demonstrates that @xmath341 in other words , the amount of information needed to extract a pair of signals from the superposition equals the total complexity of the two signals .",
    "this result holds true for a wide class of distributions on @xmath342 and @xmath343 .",
    "numerical linear algebra ( nla ) is the study of computational methods for problems in linear algebra , including the solution of linear systems , spectral calculations , and matrix approximations . over the last 15 years , researchers have developed many new algorithms for nla that exploit randomness to perform these computations more efficiently .",
    "see the surveys  @xcite for an overview of this field .",
    "in this section , we apply our universality techniques to obtain new results on dimension reduction in randomized nla .",
    "this discussion shows that a broad class of dimension reduction methods share the same quantitative behavior .",
    "therefore , within some limits , we can choose the random projector that is most computationally appealing when we design numerical algorithms based on dimension reduction .",
    "as an added bonus , the arguments here lead to a new proof of the bai ",
    "yin limit for the minimum singular value of a random matrix drawn from model  [ mod : p - mom - mtx ] .      in randomized nla ,",
    "one of the key primitives is a subspace embedding .",
    "a subspace embedding is nothing more than a randomized projector that does not annihilate any point in a fixed subspace .",
    "fix a natural number @xmath211 , and let @xmath210 be an arbitrary @xmath211-dimensional subspace .",
    "we say that a randomized projector @xmath5 is an oblivious subspace embedding of order @xmath211 if @xmath344 the term `` oblivious '' indicates that the projector @xmath7 is chosen without knowledge of the subspace @xmath210 .    in the definition of a subspace embedding , some authors include quantitative bounds on the stability of the embedding .",
    "these estimates are useful for analyzing certain algorithms , but we have left them out because they are not essential .",
    "a standard normal matrix provides an important theoretical and practical example of a subspace embedding .    for any natural number @xmath211 ,",
    "a standard normal matrix @xmath345 is a subspace embedding with probability one when the embedding dimension @xmath346 . in practice , it is preferable to select the embedding dimension @xmath347 to ensure that the restricted singular value @xmath348 is sufficiently positive , which makes the embedding more stable .",
    "see  @xcite for more details .",
    "a gaussian subspace embedding has superb dimension reduction properties .",
    "on the other hand , standard normal matrices are expensive to generate , to store , and to perform arithmetic with .",
    "therefore , in most randomized nla algorithms , it is better to use subspace embeddings that are discrete or sparse .",
    "our universality results demonstrate that , in a certain range of parameters , every matrix that follows model  [ mod : p - mom - mtx ] enjoys the same subspace embedding properties as a gaussian matrix .",
    "[ prop : subspace - embed ] suppose that    * the ambient dimension @xmath0 is sufficiently large .",
    "* the embedding dimension satisfies @xmath349 . *",
    "the random projector @xmath5 follows model  [ mod : p - mom - mtx ] with parameters @xmath110 and @xmath125 .",
    "then , for each @xmath211-dimensional subspace @xmath210 of @xmath2 , @xmath350 in particular , @xmath7 is a subspace embedding of order @xmath211 whenever @xmath351 . in these expressions ,",
    "the little - o suppresses constants that depend only on @xmath110 and @xmath125 .",
    "proposition  [ prop : subspace - embed ] is a consequence of theorem  [ thm : univ - rsv ] and   because @xmath352 the last identity holds because the @xmath211-dimensional subspace @xmath210 has statistical dimension @xmath211 .",
    "we introduce the error term @xmath353 to make sure that the stated result is only valid when the hypotheses of theorem  [ thm : univ - rsv ] are in force .",
    "note that proposition  [ prop : subspace - embed ] applies to a sparse rademacher projector with a fixed , but arbitrarily small , proportion of nonzero entries .",
    "this particular example has received extensive attention in recent years  @xcite , although these works typically focus on the regime where the subspace dimension @xmath211 is small and the sparsity level of the random projector is a vanishing proportion of the embedding dimension @xmath6 .    for the simple problem considered in proposition  [ prop : subspace - embed ] ,",
    "much sharper results are available in the random matrix literature .",
    "see the paper  @xcite for a recent analysis , as well as additional references .",
    "one of the most important problems in random matrix theory is to obtain bounds for the extreme singular values of a random matrix .",
    "the bai  yin law  @xcite gives a near - optimal result in case the entries of the random matrix are independent and standardized .",
    "we can reproduce a slightly weaker version of the bai ",
    "yin law for the minimum singular value by modifying the proof of proposition  [ prop : subspace - embed ] .",
    "fix an aspect ratio @xmath216 . for each natural number @xmath6 , define @xmath354 .",
    "draw a @xmath355 random matrix @xmath356 from model  [ mod : p - mom - mtx ] with fixed parameters @xmath110 and @xmath125 . for each @xmath357",
    ", we can apply theorem  [ thm : univ - rsv ] with @xmath358 to see that @xmath359 here , @xmath360 denotes the @xmath211th largest singular value of @xmath356 .    under these assumptions ,",
    "it is known  @xcite that the empirical distribution of the singular values of @xmath356 converges in probability to the marenko  pastur density , whose support is the interval @xmath361 .",
    "it follows that @xmath362 therefore , we may conclude that @xmath363 in comparison , the bai ",
    "yin law  ( * ? ? ?",
    "2 ) gives the same conclusion almost surely when the entries of @xmath356 have four finite moments .",
    "see the recent paper  @xcite for an optimal result .      in randomized nla , one of the core applications of dimension reduction is to solve over - determined least - squares problems , perhaps with additional constraints .",
    "this idea is attributed to sarls  @xcite , and it has been studied intensively over the last decade ; see the surveys  @xcite for more information . in this section , we develop sharp bounds for the simplest version of this approach .",
    "suppose that @xmath71 is a fixed @xmath364 matrix with full column rank .",
    "let @xmath365 be a vector , and consider the over - determined least - squares problem @xmath366 this problem can be expensive to solve when @xmath367 .",
    "one remedy is to apply dimension reduction .",
    "draw a random projector @xmath5 from model  [ mod : p - mom - mtx ] , and consider the compressed problem @xmath368 the question is how the quality of the solution of   depends on the embedding dimension @xmath6 .",
    "the following result provides an optimal estimate .",
    "[ prop : rand - ls ] instate the prevailing notation .",
    "fix parameters @xmath215 and @xmath160 and @xmath369 .",
    "assume that    * the number @xmath0 of constraints is sufficiently large as a function of the parameters .",
    "* the embedding dimension @xmath6 is comparable with the number @xmath0 of constraints : @xmath370 . *",
    "the embedding dimension @xmath6 is somewhat larger than the number @xmath255 of variables : @xmath371 .    with high probability , the solution @xmath254 to the reduced least - squares problem   satisfies @xmath372 where @xmath249 is the solution to the original least - squares problem  .",
    "in particular , @xmath373    in other words , the excess error @xmath374 incurred in solving the compressed least - squares problem   is negligible as compared with the optimal value of the least - squares problem   if we choose the embedding dimension @xmath6 sufficiently large .",
    "proposition  [ prop : rand - ls ] improves substantially on the most recent work  ( * ? ? ?",
    "2(a ) ) , both in terms of the error bound and in terms of the assumptions on the randomized projector .",
    "let us remark that there is nothing special about ordinary least squares .",
    "we can also solve least - squares problems with a convex constraint set by dimension reduction .",
    "for this class of problems , we can also obtain optimal bounds by adapting the argument below .",
    "for example , see the results in section  [ sec : lasso - err ] .",
    "let @xmath375 be the solution to the original least - squares problem  .",
    "define the optimal residual @xmath376 , and recall that @xmath377 is orthogonal to @xmath378 .",
    "moreover , @xmath379 without loss of generality , we may scale the problem so that @xmath380 .",
    "next , change variables .",
    "define @xmath381 , and note that @xmath382 is orthogonal to @xmath377 .",
    "we can write the reduced least - squares problem   as @xmath383 when dimension reduction is effective , we expect the solution @xmath384 to   to be close to zero .    since @xmath371",
    ", we can use the fact ( proposition  [ prop : subspace - embed ] ) that @xmath7 is a subspace embedding to obtain an a priori bound @xmath385 that holds with high probability .",
    "the number @xmath386 is a constant that depends on nothing but @xmath272 .",
    "we only need this observation to ensure that we are optimizing over a compact set with constant radius , so we omit the details .",
    "next , we invoke theorem  [ thm : univ - rsv ] to bound the optimal value of the reduced least - squares problem  .",
    "define the compact , convex set @xmath387 let @xmath357 be a parameter that will depend on the parameter @xmath388 . with high probability , @xmath389 by direct calculation , we can bound the excess width above .",
    "let @xmath390 be the orthogonal projector onto the range of @xmath71 , and observe that @xmath391 the first line is definition  [ def : excess - width ] , of the excess width .",
    "next , simplify via the orthogonality of @xmath382 and @xmath296 and the scaling @xmath392 .",
    "use translation invariance of the infimum to remove @xmath296 from the gaussian term .",
    "apply jensen s inequality to draw the expectation inside the infimum , and note that @xmath393 because @xmath394 . then make the change of variables @xmath395 , and solve the scalar convex optimization problem .",
    "the infimum occurs at the value @xmath396 since @xmath371 , we may be confident that @xmath397 .",
    "we have shown that , with high probability , @xmath398 furthermore , we have evidence that the norm of the minimizer @xmath399 .",
    "to prove the main result , we compute the value of the optimization problem   restricted to points with @xmath400 , where @xmath401 is a small positive number to be chosen later .",
    "then we verify that the optimal value of the restricted problem is usually larger than the bound   for the optimal value of  .",
    "this argument implies that @xmath402 with high probability .    to that end , define @xmath403 , and introduce the compact set @xmath404 theorem  [ thm : univ - rsv ] shows that , with high probability , @xmath405 calculating the excess width as before , @xmath406 add and subtract @xmath407 to reach the second line , and use the fact that @xmath393 .",
    "then apply the gaussian variance inequality , fact  [ fact : gauss - variance ] , to bound the expectation by one .",
    "the infimum occurs at @xmath408 because the objective is convex and @xmath408 exceeds the unconstrained minimizer @xmath409 .",
    "next , we simplify the expression involving @xmath408 .",
    "setting @xmath410 , we find that @xmath411 the inequality follows from the linear upper bound for the square root at one .",
    "combine  ,  , and   to arrive at @xmath412 comparing   with the last display , we discover that the choice @xmath413 is sufficient to ensure that , with high probability , @xmath414 it follows that the minimum of   usually occurs on the set @xmath415 .",
    "we determine that @xmath416 reinterpret this inequality to obtain the stated result  .",
    "we can obtain a matching lower bound for @xmath417 by considering the set of vectors @xmath418 where @xmath419 .",
    "we omit the details .    the idea of using random projectors to accelerate the solution of least - squares problems appears in the work of sarls  @xcite . this approach has been extended and refined in the literature on randomized nla ; see the surveys  @xcite for an overview .",
    "most of this research is concerned with randomized projectors that have favorable computational properties , but the results are much less precise than proposition  [ prop : rand - ls ] .",
    "recently , pilanci & wainright  @xcite have offered a more refined analysis of randomized dimension reduction for constrained least - squares problems , but it still falls short of describing the actual behavior of these methods .",
    "parts of the argument here is adapted from the work of oymak , thrampoulidis , and hassibi  @xcite .",
    "universality results have always played an important role in statistics .",
    "the most fundamental example is the law of large numbers , which justifies the use of the sample average to estimate the mean of a general distribution .",
    "similarly , the central limit theorem permits us to build a confidence interval for the mean of a distribution .",
    "high - dimensional statistics relies on more sophisticated methods , often based on optimization , to estimate population parameters .",
    "in particular , applied statisticians frequently employ the lasso estimator  @xcite to perform regression and variable selection in linear models .",
    "it is only recently that researchers have developed theory  @xcite that can predict the precise behavior of the lasso when the data are assumed to be gaussian .",
    "it is a critical methodological challenge to develop universality results that expand the range of models in which we can make confident assertions about the performance of the lasso .    in this section ,",
    "we prove the first precise universality result for the prediction error using a lasso model estimate .",
    "this theory offers a justification for using a lasso model to make predictions when the data derives from a sparse model .",
    "we expect that further developments in this direction will play an important role in applied statistics .",
    "the lasso is designed to perform simultaneous regression and variable selection in a linear model .",
    "let us present a simple statistical model in which to study the behavior of the lasso estimator .",
    "suppose that the random variable @xmath420 takes the form @xmath421 where    * the deterministic vector @xmath422 of model parameters has at most @xmath246 nonzero entries .",
    "* the random vector @xmath423 of predictor variables is drawn from model  [ mod : p - mom - mtx ] . *",
    "the noise variance @xmath424 is known . * the statistical error @xmath120 is drawn from model  [ mod : p - mom - mtx ] , independent of @xmath251 .",
    "we interpret @xmath425 as a family of predictor variables that we want to use to predict the value of the response variable @xmath420 . only the variables @xmath426",
    "where @xmath427 are relevant to the prediction , while the others are confounding .",
    "the observed value @xmath420 of the response is contaminated with a statistical error @xmath428 .",
    "these assumptions are idealized , but let us emphasize that our analysis holds even when the predictors and the noise are heavy - tailed .",
    "suppose that we observe independent pairs @xmath429 drawn from the model above , and let @xmath430 be the unknown vector of statistical errors .",
    "one of the goals of sparse regression is to use this data to construct an estimate @xmath431 of the model coefficients so that we can predict future responses .",
    "that is , given a fresh random vector @xmath432 of predictor variables , we can predict the ( unknown ) response @xmath433 using the linear estimate @xmath434 we want to control the mean squared error in prediction , which is defined as @xmath435.\\ ] ] using the statistical model  , it is easy to verify that @xmath436      = { { \\left\\vert { \\smash{\\widehat{{{\\bm{\\beta } } } } - { { \\bm{\\beta}}}_{\\star } } } \\right\\vert}^2 } + \\sigma^2.\\ ] ] in other words , the prediction error is controlled by the squared error in estimating the model coefficients",
    ".    the lasso uses convex optimization to produce an estimate @xmath437 of the model coefficients .",
    "the estimator is chosen arbitrarily from the set of solutions to the problem @xmath438 in this formula , the rows of the @xmath439 matrix @xmath320 are the observed predictor vectors @xmath440 .",
    "the entries of the vector @xmath441 are the measured responses . for simplicity",
    ", we also assume that we have the exact side information @xmath442 .",
    "we can prove the following result on the squared error in the lasso estimate of the sparse coefficient model .",
    "this is the first universal statement that offers a precise analysis for this class of statistical models .",
    "[ prop : lasso - err ] instate the prevailing notation .",
    "choose parameters @xmath215 and @xmath160 and @xmath443 .",
    "assume that    * the number @xmath255 of subjects is sufficiently large as a function of the parameters .",
    "* the number @xmath110 of predictors satisfies @xmath444 .",
    "* the number @xmath255 of subjects satisfies @xmath445 .    with high probability over the observed data , the mean squared error in prediction   satisfies @xmath446 the function @xmath307 is defined in  .",
    "furthermore , the bound   is sharp when @xmath447 or when @xmath448 .",
    "proposition  [ prop : lasso - err ] gives an upper bound for the @xmath449 , which matches the low - noise limit ( @xmath450 ) obtained in the gaussian case  @xcite .",
    "see figure  [ fig : lasso - msep ] for a numerical experiment that confirms our theoretical predictions .",
    "the proof of the result appears below in section  [ prop : lasso - err - pf ] .    the assumptions in proposition  [ prop : lasso - err ] are somewhat restrictive , in that the number @xmath255 of subjects must be roughly comparable with the number @xmath110 of predictors",
    ". this condition can probably be relaxed , but the error bound in theorem  [ thm : univ - rsv ] does not allow for a stronger conclusion .",
    "the argument can also be extended to give even more precise formulas for the @xmath449 under the same assumptions .",
    "we also note that there is nothing special about the @xmath53 constraint in  ; similar results are valid for many other convex constraints .",
    "this plot shows the @xmath449   obtained with the lasso estimator  , averaged over design matrices @xmath320 and statistical errors @xmath298 . in the linear model  , the number of predictors @xmath451 ; the number of active predictors @xmath452 ; each nonzero coefficient @xmath453 in the model has unit magnitude ; the statistical error @xmath120 is gaussian ; the variance @xmath454 ; and the number @xmath255 of subjects varies . the * dashed line * marks the location of the phase transition for the number of subjects required to identify the model exactly when the noise variance is zero .",
    "the * gray curve * delineates the asymptotic upper bound @xmath455 for the normalized @xmath449 from proposition  [ prop : lasso - err ] .",
    "the * markers * give an empirical estimate ( over 100 trials ) for the @xmath449 when the design matrix @xmath320 has the specified distribution . ]      without loss of generality , we may assume that the statistical model is scaled so the noise level @xmath456 .",
    "define the sublevel set @xmath36 of the @xmath53 norm at the true parameter vector @xmath321 : @xmath457 note that @xmath36 is a compact , convex set that contains the origin .",
    "define the @xmath458 random matrix @xmath459 , and note that @xmath126 also follows model  [ mod : p - mom - mtx ] .",
    "making the change of variables @xmath460 , we can rewrite the lasso problem   in the form @xmath461 this expression depends on the assumption that @xmath456 .",
    "let @xmath462 be an optimal point of the problem  . referring to",
    ", we see that a formula for @xmath463 leads to a formula for the @xmath449",
    ".    it will be helpful to introduce some additional sets . for a parameter @xmath4 ,",
    "define the compact ( but typically nonconvex ) set @xmath464 we also define the compact and convex set @xmath465 observe that @xmath466 .",
    "furthermore , @xmath467 the inclusion   holds because @xmath36 is convex and contains the origin .",
    "let @xmath386 be a constant that depends only on the parameters @xmath468 and @xmath388 .",
    "suppose that @xmath469 . to prove that @xmath470 , it suffices to establish the inequality @xmath471 indeed , recall that @xmath462 is the minimizer of the objective over @xmath36 , and let @xmath472 be the point in @xmath473 where the left - hand minimum in   is attained .",
    "the objective is a convex function of @xmath474 , so it does not decrease as @xmath474 traverses the line segment from @xmath462 to @xmath472 .",
    "if @xmath475 , this line segment muss pass through @xmath476 , which is impossible because the ordering   forces the objective to decrease on the way from @xmath476 to @xmath472 .",
    "fix the parameter @xmath477 in the range @xmath478 ; we will select a suitable value later .",
    "theorem  [ thm : univ - rsv ] demonstrates that , with high probability , @xmath479 the second relation holds because the excess width decreases with respect to set inclusion .",
    "observe that @xmath480 the last identity holds when we factor out @xmath481 and identify the gaussian width  .",
    "fix the second parameter @xmath408 , such that @xmath482 .",
    "theorem  [ thm : univ - rsv ] demonstrates that , with high probability , @xmath483 much as before , we calculate the excess width : @xmath484 the last inequality holds because of   and the fact that the gaussian width is increasing with respect to set inclusion .",
    "combine the last four displays to reach @xmath485 it follows that we can establish   by finding parameters for which @xmath486 and @xmath487      - \\left [ \\sqrt{n } ( r^2 + 1 ) ^{1/2 }      - r \\ , { \\mathscr{w}}\\big((1/r ) e_r \\big ) \\right ] \\geq o(\\sqrt{n}).\\ ] ] to that end , we replace the gaussian width by a number that does not depend on the parameter @xmath477 : @xmath488 the first relation is  ; the second follows from definition  [ def : stat - dim ] ; the last is the estimate  .",
    "moreover , these bounds are sharp when @xmath477 is sufficiently close to zero .",
    "therefore , we just need to verify that @xmath489      - \\left [ \\sqrt{n } ( r^2 + 1 ) ^{1/2 }      - r \\ , \\sqrt{d } \\right ]      \\geq o(\\sqrt{n}).\\ ] ]",
    "once we choose @xmath477 and @xmath408 appropriately , we can adapt the analysis in the proof of proposition  [ prop : rand - ls ] .    for @xmath490 ,",
    "introduce the function @xmath491 as in the proof of proposition  [ prop : rand - ls ] , by direct calculation , @xmath51 is minimized at the value @xmath492 note that @xmath477 is very close to zero when @xmath493 , in which case @xmath6 is an accurate bound for @xmath494 .",
    "furthermore , @xmath495 now , make the selection @xmath496 since @xmath497 , we see that @xmath408 is bounded by a constant @xmath386 that depends only on @xmath388 and @xmath468 . repeating the calculation in  , mutatis mutandis , we have @xmath498 combining   and  , we determine that @xmath499 the last inequality holds because @xmath388 is a fixed positive constant and we have assumed that @xmath500 .    in conclusion ,",
    "we have confirmed the claim   for @xmath477 and the value @xmath408 designated in  .",
    "it follows that   holds with high probability , and so the optimizer of   satisfies @xmath501 with high probability .",
    "therefore , the formula   for the mean squared error yields the bound @xmath502 once again , we have used the assumption that @xmath456 . by homogeneity",
    ", the @xmath449 must be proportional to @xmath503 , which leads to the stated result  . finally , if we allow @xmath450 with the other parameters fixed , the analysis here can be adapted to show that the error bound   is sharp .    in the special case of a standard normal design @xmath320 and a standard normal error @xmath298",
    ", the result of proposition  [ prop : lasso - err ] appeared in the paper  @xcite .",
    "our extension to more general random models is new .",
    "nevertheless , our proof has a lot in common with the analysis in  @xcite .",
    "[ part : rsv ]    in this part of the paper , we present a detailed proof of the universality law for the restricted singular value of a random matrix , theorem  [ thm : univ - rsv ] , and the first part of the universality law for the embedding dimension , theorem  [ thm : univ - embed ] .",
    "section  [ sec : rsv - bdd ] contains our main technical result , which establishes universality for the bounded random matrix model , model  [ mod : bdd - mtx ] .",
    "section  [ sec : rsv - four ] extends the result for bounded random matrices to the heavy - tailed random matrix model , model  [ mod : p - mom - mtx ] . in section  [ sec : rsv - four - to - rsv - univ ] , we obtain theorem  [ thm : univ - rsv ] as an immediate consequence of the result for heavy - tailed matrices . section  [ sec : rsv - four - to - embed - univ ] shows how to derive theorem  [ thm : univ - embed ] as an additional consequence .",
    "the remaining sections in this part lay out the lengthy calculations that undergird the result for bounded random matrices .",
    "the key challenges in establishing universality for the restricted minimum singular value are already present in the case where the random matrix is drawn from the bounded model , model  [ mod : bdd - mtx ] .",
    "this section presents a universality law for bounded random matrices , and it gives an overview of the calculations that are required to establish this result .",
    "the main technical result in this paper is a theorem on the behavior of the restricted minimum singular value of a bounded random matrix .",
    "[ thm : rsv - bdd ] place the following assumptions :    * let @xmath196 and @xmath255 be natural numbers with @xmath504 .",
    "* let @xmath138 be a closed subset of the unit ball @xmath84 in @xmath85 .",
    "* draw an @xmath112 random matrix @xmath126 from model  [ mod : bdd - mtx ] with bound @xmath505 .",
    "then the squared restricted singular value @xmath506 has the following properties :    1 .",
    "[ it : rsv - bdd - conc ] the squared restricted singular value concentrates about its mean on a scale of @xmath507 .",
    "for each @xmath508 , @xmath509 2 .",
    "[ it : rsv - bdd - expect - lower ] the expectation of the squared restricted singular value is bounded below in terms of the excess width : @xmath510 3 .",
    "[ it : rsv - bdd - expect - cvx ] if @xmath138 is a convex set , the squared restricted singular value is bounded above in terms of the excess width : @xmath511    furthermore , the entries of @xmath126 need not be symmetric for this result to hold .",
    "our proof of this result is very involved , and it will occupy us for the rest of this part of the paper .",
    "this section summarizes the required calculations , with cross - references to the detailed arguments .",
    "theorem  [ thm : rsv - bdd ] states that the squared restricted singular value @xmath506 concentrates around its mean .",
    "we prove this claim in proposition  [ prop : rsv - concentration ] , which appears below .",
    "the argument depends on some concentration inequalities that are derived using the entropy method .",
    "this approach is more or less standard , so we move on to the more interesting part of the proof .",
    "let us continue with the proof of theorem  [ thm : rsv - bdd ] , conclusions   and  .",
    "the overall approach is the same in both cases , but some of the details differ .",
    "the first step is to dissect the index set @xmath138 into appropriate subsets . for each set @xmath284",
    ", we introduce a closed subset @xmath512 of @xmath138 via the rule @xmath513 where @xmath514 . in other words",
    ", @xmath512 contains all the vectors in @xmath138 where the coordinates listed in @xmath233 are sufficiently small .",
    "note that convexity of the set @xmath138 implies convexity of each subset @xmath512 .",
    "fix a number @xmath515 . since @xmath138 is a subset of the unit ball @xmath84 , every",
    "vector @xmath516 satisfies @xmath517 therefore , @xmath91 belongs to some subset @xmath512 where the cardinality @xmath518 , and we have the decomposition @xmath519 it is clear that the number of subsets @xmath512 in this decomposition satisfies @xmath520 we must limit the cardinality @xmath211 of the subsets , so we can control the number of subsets we need to examine .",
    "let us proceed with the proof of theorem  [ thm : rsv - bdd ] , the lower bound for the rsv .",
    "for the time being , we fix the parameter @xmath211 that designates the cardinality of the sets @xmath75 .",
    "we require that @xmath521 .",
    "first , we pass from the restricted singular value @xmath522 over the whole set @xmath138 to a bound that depends on @xmath523 for the subsets @xmath512 . proposition  [ prop : rsv - dissection ] gives the comparison @xmath524 we have used the decomposition   and the bound   on the number of subsets in the decomposition to invoke the proposition .",
    "the main ingredient in the proof of this estimate is the concentration inequality for restricted singular values , proposition  [ prop : rsv - concentration ] .",
    "let us focus on a specific subset @xmath512 . to study the restricted singular value @xmath523 , we want to replace the entries of the random matrix @xmath126 with standard normal random variables .",
    "proposition  [ prop : partial - replacement ] allows us to make partial progress toward this goal .",
    "let @xmath35 be an @xmath112 standard normal matrix , independent from @xmath126 .",
    "define an @xmath112 random matrix @xmath525 where @xmath526 and @xmath238 .",
    "then @xmath527 this bound requires the assumption that @xmath521 .",
    "the argument is based on the lindeberg exchange principle , but we have used this method in an unusual way . for a vector @xmath528 ,",
    "the definition   gives us control on the magnitude of the entries listed in @xmath233 , which we can exploit to replace the column submatrix @xmath529 with @xmath530 .",
    "the coordinates of @xmath91 listed in @xmath75 may be large , so we can not replace the column submatrix @xmath240 without incurring a significant penalty . instead ,",
    "we just leave it in place .    next , we want to compare the expected minimum on the right - hand side of   with the excess width of the set @xmath512 .",
    "proposition  [ prop : excess - width ] provides the bound @xmath531 the second inequality is a consequence of the facts that @xmath532 and that the excess width is decreasing with respect to set inclusion .",
    "the calculation in proposition  [ prop : excess - width ] uses the gaussian minimax theorem ( see fact  [ fact : gauss - minmax ] ) to simplify the average with respect to the standard normal matrix @xmath35 .",
    "we also invoke standard results from nonasymptotic random matrix theory to control the expectation over @xmath240 .",
    "we can obtain a simple lower bound on the last term in   by linearizing the convex function @xmath533 at the point @xmath534 : @xmath535 the last estimate follows from the observation that @xmath536 in this calculation , @xmath229 is an arbitrary point in @xmath138 .",
    "finally , we sequence the four displays  , , , and   and combine error terms to obtain @xmath537 up to logarithms , an optimal choice of the cardinality parameter is @xmath538 .",
    "since @xmath504 , this choice ensures that @xmath521 .",
    "we conclude that @xmath539 combine the logarithm with the power , and adjust constants to complete the proof of theorem  [ thm : rsv - bdd ] .      at a high level ,",
    "the steps in the proof of theorem  [ thm : rsv - bdd ] are similar with the argument in the last section .",
    "many of the technical details , however , depend on convex duality arguments .    as before",
    ", we fix the cardinality parameter @xmath211 , with the requirement @xmath521 .",
    "the first step is to pass from @xmath522 to @xmath523 .",
    "we have @xmath540 this bound is a trivial consequence of the facts that @xmath532 for each subset @xmath75 of @xmath541 and that @xmath542 is decreasing with respect to set inclusion .    select any subset @xmath512 .",
    "we invoke proposition  [ prop : partial - replacement ] to exchange most of the entries of the random matrix @xmath126 for standard normal variables .",
    "using the same random matrix @xmath236 from the last section , we have @xmath543 the bound   requires the assumption @xmath521 , and the proof is identical with the proof of the lower bound  .",
    "next , we compare the expected minimum on the right - hand side of   with the excess width .",
    "proposition  [ prop : excess - width ] delivers @xmath544 this argument is somewhat more complicated than the analogous lower bound  , and it depends on the convexity of @xmath512 at several junctures .",
    "we also apply the assumption that @xmath521 here .    proposition  [ prop : excess - width - dissection ] allows us to replace the excess width of @xmath512 in   with the excess width of @xmath138 : @xmath545 we use the decomposition   and the bound   on the number of sets @xmath512 to invoke the proposition .",
    "this proof depends on the gaussian concentration inequality .",
    "sequence the bounds  , , , and  , and combine the error terms to arrive at @xmath546 expand the square using   to reach @xmath547 select @xmath538 to arrive at @xmath548 combine the power with the logarithm , and adjust constants to finish the proof of theorem  [ thm : rsv - bdd ] .",
    "in this section , we present an extension of theorem  [ thm : rsv - bdd ] to the heavy - tailed matrix model , model  [ mod : p - mom - mtx ] . in section  [ sec : rsv - four - to - rsv - univ ] , we explain how the universality result for the restricted singular value , theorem  [ thm : univ - rsv ] , is an immediate consequence . in section  [ sec : rsv - four - to - embed - univ ] , we show how to derive the first half of the universality result for the embedding dimension , theorem  [ thm : univ - embed ] .",
    "we can extend theorem  [ thm : rsv - bdd ] to the heavy - tailed random matrix model , model  [ mod : p - mom - mtx ] using a truncation argument .",
    "the following corollary contains a detailed statement of the result .",
    "[ cor : rsv - four ] fix parameters @xmath108 and @xmath109 .",
    "place the following assumptions :    * let @xmath196 and @xmath255 be natural numbers with @xmath504 .",
    "* let @xmath138 be a closed subset of the unit ball @xmath84 in @xmath85 .",
    "* draw an @xmath112 random matrix @xmath126 that satisfies model  [ mod : p - mom - mtx ] with given @xmath110 and @xmath125 .",
    "then the restricted singular value @xmath522 has the following properties :    1 .   with high probability",
    ", the restricted singular value is bounded below by the excess width : [ it : rsv - four - lower ] @xmath549 2 .",
    "if @xmath138 is a convex set , with high probability , the restricted singular value is bounded above by the excess width : [ it : rsv - four - upper ] @xmath550    the function @xmath551 is strictly positive for @xmath108 , and the constant @xmath552 depends only on @xmath110 .",
    "the proof of corollary  [ cor : rsv - four ] appears below in section  [ sec : rsv - truncation ] .",
    "the main idea is to truncate each entry of the heavy - tailed random matrix @xmath126 individually .",
    "we can treat the bounded part of the random matrix using theorem  [ thm : rsv - bdd ] .",
    "we show that the tails are negligible by means of a relatively simple norm bound for random matrices , fact  [ fact : heavy - tail - norm ] .",
    "theorem  [ thm : univ - rsv ] is an easy consequence of corollary  [ cor : rsv - four ] .",
    "recall the assumptions of the theorem :    * the embedding dimension satisfies @xmath219 .",
    "* @xmath36 is a closed subset of the unit ball in @xmath2 . *",
    "the @xmath6-excess width of @xmath36 satisfies @xmath220 . *",
    "the @xmath34 random projector @xmath7 follows model  [ mod : p - mom - mtx ] with parameters @xmath108 and @xmath109",
    ".    therefore , we may apply corollary  [ cor : rsv - four ] with @xmath553 and @xmath554 to see that @xmath555 for simplicity , we have dropped the embedding dimension @xmath6 from the right - hand side of the bounds in the last display .    to prove theorem  [ thm : univ - rsv ]",
    ", it suffices to check that we can make the error term @xmath556 smaller than @xmath557 if we select the ambient dimension @xmath0 large enough .",
    "indeed , the conditions @xmath558 and @xmath559 ensure that @xmath560 since @xmath551 is positive , there is a number @xmath561 for which @xmath163 implies @xmath562 this is what we needed to show .",
    "theorem  [ thm : univ - embed ] is also an easy consequence of corollary  [ cor : rsv - four ] .",
    "recall the assumptions of the theorem :    * @xmath36 is a compact subset of @xmath2 that does not contain the origin . *",
    "the statistical dimension of @xmath36 satisfies @xmath164 . *",
    "the @xmath34 random projector @xmath7 follows model  [ mod : p - mom - mtx ] with parameters @xmath108 and @xmath109 .    in this section",
    ", we consider the regime where the embedding dimension @xmath563 .",
    "we need to demonstrate that @xmath564    we begin with a reduction to a specific choice of the embedding dimension @xmath6 .",
    "let @xmath565 be the @xmath566 matrix formed from the first @xmath196 rows of the random projector @xmath7 .",
    "the function @xmath567 is weakly increasing because @xmath568 is a decreasing sequence of sets .",
    "therefore , it suffices to verify   in the case where @xmath569 .",
    "note that @xmath570 because the statistical dimension @xmath571 and @xmath572 .",
    "introduce the spherical retraction @xmath169 .",
    "proposition  [ prop : annihilate ] and   demonstrate that the condition @xmath573 therefore , to check  , it suffices to produce a high - probability lower bound on the restricted singular value @xmath574 . with the choices @xmath553 and @xmath575 , corollary  [ cor : rsv - four ] yields @xmath576 to complete the proof , we need to verify that our hypotheses imply @xmath577 this point follows from two relatively short calculations",
    ".    since @xmath1 is a subset of the unit sphere , we quickly compute ite excess width : @xmath578 the second identity holds because @xmath1 is a subset of the unit sphere , and we have used the relation  . recall that the statistical dimension of a general set @xmath36 is defined as @xmath579 , and then introduce the value @xmath580 of the embedding dimension .",
    "meanwhile , we can bound the dimensional term in   above : @xmath581 the first inequality holds because @xmath570 , and the second inequality uses the assumption @xmath582 .",
    "since @xmath551 is positive , we can find a number @xmath162 for which @xmath163 implies that @xmath583 combine the last display with   to determine that the claim   is valid .",
    "in this section , we demonstrate that the restricted minimum singular value of a bounded random matrix concentrates about its mean value . this result yields theorem  [ thm : rsv - bdd ] .",
    "[ prop : rsv - concentration ] let @xmath173 be a closed subset of @xmath84 .",
    "let @xmath126 be an @xmath112 random matrix drawn from model  [ mod : bdd - mtx ] with uniform bound @xmath505 . for all @xmath508 , @xmath584",
    "the proof relies on some modern concentration inequalities that are derived using the entropy method .",
    "we establish the bound   on the lower tail in section  [ sec : rsv - lower ] ; the bound   on the upper tail appears in section  [ sec : rsv - upper ] .",
    "in several other parts of the paper , we rely on variants of proposition  [ prop : rsv - concentration ] that follow from essentially the same arguments .",
    "we omit the details of these proofs to avoid repetition .",
    "first , we establish that the restricted singular value is unlikely to be much smaller than its mean .",
    "the proof depends on an exponential moment inequality derived using massart s modified logarithmic sobolev inequality  @xcite .",
    "for instance , see  ( * ? ? ? * thm .",
    "6.27 ) .",
    "[ fact : self - bdd - lower ] let @xmath585 be an independent sequence of real random variables , and let @xmath586 be an independent copy of this sequence . for a function @xmath587 , define @xmath588 suppose that @xmath589      \\leq a y + b      \\quad\\text{where $ a \\geq 0$.}\\ ] ] then @xmath590    with this fact at hand , we may derive the lower tail bound .",
    "introduce the random variable @xmath591 we need to verify that @xmath420 is not much smaller than its mean .    for each index pair @xmath82 , define a random matrix @xmath592 by replacing the @xmath82 entry @xmath593 of @xmath126 with an independent copy @xmath594 .",
    "now , introduce the random variable @xmath595 we must evaluate the lower variance proxy @xmath596.\\ ] ] to that end , select a point @xmath597 .",
    "for each index pair @xmath82 , @xmath598 the matrix @xmath592 differs from @xmath126 only in the @xmath599th row .",
    "therefore , when we expand the squared norms into components , all of the components cancel except for the @xmath599th one .",
    "we discover that @xmath600 the last inequality holds because @xmath601 and @xmath602 .",
    "taking the square and summing over pairs @xmath82 , we arrive at @xmath603 to reach the third line , we used the fact that @xmath604 .",
    "the last line depends on the definition of @xmath91 .    in summary",
    ", we have demonstrated that @xmath605 to apply fact  [ fact : self - bdd - lower ] , we need a bound for the expectation of @xmath420 .",
    "designate a point @xmath606 , and calculate that @xmath607 the identity holds because @xmath126 has independent , standardized entries .",
    "fact  [ fact : self - bdd - lower ] ensures that @xmath608 rewrite this formula to complete the proof of  .",
    "next , we establish that the restricted singular value of a bounded random matrix is unlikely to be much larger than its mean .",
    "the proof depends on another concentration inequality for self - bounded random variables .",
    "[ fact : self - bdd - upper ] let @xmath585 be an independent sequence of real random variables . for a function @xmath587 , define @xmath609 suppose that @xmath610      \\leq a y + b      \\quad\\text{where $ a \\geq 0$.}\\ ] ] then @xmath611 the symbol @xmath612 refers to the support of a random variable @xmath99 .",
    "to the best of our knowledge , fact  [ fact : self - bdd - upper ] does not appear explicitly in the literature , but it can be derived easily with the methods outlined in maurer s work on the entropy method . indeed , the proof just combines the arguments from ( * ? ? ?",
    "* thm .  14 , thm .  19 ) .",
    "as in the proof of  , we introduce the random variable @xmath613 for each index pair @xmath82 , we define a parameterized random matrix @xmath614 by replacing the @xmath82 entry of @xmath126 with the real number @xmath122",
    ". then we set @xmath615 we need to evaluate the lower variance proxy : @xmath616.\\ ] ] select a point @xmath597 . since @xmath617 for every fixed choice of @xmath126 , @xmath618 by convexity , the maximum occurs when @xmath619 . with this observation , the rest of the proof proceeds in the same fashion as the proof of  .",
    "just as before , we arrive at the bound @xmath620 we can invoke fact  [ fact : self - bdd - upper ] to reach @xmath621 the last inequality holds because @xmath622 .",
    "simplify this formula to finish the proof of  .",
    "in this section , we establish some results that compare the expectation of a minimum of random variables indexed by a set with the expectations of the minima indexed by subsets .",
    "these facts are easy consequences of concentration phenomenon .",
    "first , we show that the excess width of a set can be related to the excess width of a collection of subsets .",
    "[ prop : excess - width - dissection ] consider a closed subset @xmath138 of the unit ball @xmath84 that has been decomposed into a finite number of closed subsets @xmath512 : @xmath623 for each @xmath624 , it holds that @xmath625 in other words , @xmath626    since @xmath627 , we can stratify the minimum over @xmath138 : @xmath628 we will obtain a lower tail bound for the minimum over @xmath138 by combining lower tail bounds for each minimum over @xmath512 . then we integrate the tail probability to obtain the required expectation bound .",
    "each subset @xmath512 is contained in @xmath84 , so the map @xmath629 is 1-lipschitz .",
    "the gaussian concentration inequality , fact  [ fact : gauss - concentration ] , provides a tail bound . for each @xmath630 and for each @xmath631 , @xmath632 apply the union bound over @xmath630 to see that , for all @xmath508 , @xmath633    using the latter estimate , we quickly compute the excess width of @xmath138 .",
    "abbreviate @xmath634 if @xmath635 , the stated result is trivial , so we may assume that @xmath4 .",
    "the integration by parts representation of the expectation yields @xmath636 reintroduce the values of @xmath420 and @xmath122 , and combine constants to complete the argument .",
    "next , we show that the minimum singular value of a random matrix , restricted to a set , is controlled by the minimum singular value , restricted to subsets .",
    "[ prop : rsv - dissection ] consider a closed subset @xmath138 of the unit ball @xmath84 , and assume that it has been decomposed into a finite number of closed subsets @xmath512 : @xmath623 let @xmath126 be an @xmath112 random matrix that satisfies model  [ mod : bdd - mtx ] with uniform bound @xmath505 .",
    "then @xmath637    the argument is similar with the proof of proposition  [ prop : excess - width - dissection ] .",
    "since @xmath627 , @xmath638 the lower tail bound   for restricted singular values implies that , for all @xmath631 , @xmath639 next , we take a union bound . for all @xmath508 , @xmath640 define a random variable @xmath420 and a constant @xmath122 : @xmath641 if @xmath635 , the stated result is trivial , so we may assume that @xmath4 .",
    "calculating as in proposition  [ prop : excess - width - dissection ] , @xmath642 simplify this bound to complete the proof .",
    "in this section , we show that it is possible to replace most of the entries of a random matrix @xmath126 with standard normal random variables without changing the restricted singular value @xmath523 very much .",
    "[ prop : partial - replacement ] let @xmath126 be an @xmath112 random matrix that satisfies model  [ mod : bdd - mtx ] with magnitude bound @xmath505 .",
    "let @xmath75 be a subset of @xmath541 with cardinality @xmath211 , and let @xmath512 be a closed subset of @xmath84 for which @xmath643 define an @xmath112 random matrix @xmath236 where @xmath644 assuming that @xmath521 , we have @xmath645 equivalently , @xmath646 as usual , @xmath35 is an @xmath112 standard normal matrix .",
    "the hypothesis   is an essential ingredient in the proof of proposition  [ prop : partial - replacement ] .",
    "indeed , we can only exchange the elements of the random matrix @xmath126 in the columns indexed by @xmath233 because we depend on the uniform bound @xmath647 to control the replacement errors .",
    "the proof of proposition  [ prop : partial - replacement ] involves several long steps , so it is helpful to give an overview before we begin in earnest . throughout this discussion ,",
    "the index set @xmath75 is fixed .",
    "let @xmath161 be a discretization parameter .",
    "the first step in the argument is to replace the index set @xmath512 by a finite subset @xmath648 with cardinality @xmath649 .",
    "we obtain the bounds @xmath650 see lemma  [ lem : discretization ] for details , which are quite standard .",
    "next , we introduce a smoothing parameter @xmath651 , and we define the soft - min function : @xmath652 it is advantageous to work with the soft - min because it is differentiable .",
    "lemma  [ lem : smoothing ] demonstrates that we pay only a small cost for passing to the soft - min : @xmath653 this argument is also standard .    finally , we compare the expectation of the soft - min , evaluated at each of the random matrices : @xmath654 lemma  [ lem : exchange ] encapsulates the lengthy details of this argument , which is based on the lindeberg principle ( fact  [ fact : lindeberg ] , below ) .",
    "the idea is to replace one entry of @xmath529 at a time with the corresponding entry of @xmath655 , which is a gaussian random variable .",
    "we lose very little with each exchange because the function @xmath280 is smooth and the vectors in @xmath656 are bounded on the coordinates in @xmath233 .",
    "combine the relations  ,  , and   to obtain an estimate for the total error : @xmath657 we used the fact that @xmath658 in the smoothing term .",
    "it remains to identify appropriate values for the parameters .",
    "select @xmath659 so the discretization error is negligible .",
    "we choose the smoothing parameter so that @xmath660 . in summary , @xmath661 since @xmath102 and @xmath662 , the third term dominates .",
    "this is the bound stated in  .",
    "the first step in the proof of proposition  [ prop : partial - replacement ] is to replace the set @xmath512 with a finite subset @xmath648 without changing the restricted singular values of @xmath126 and @xmath236 substantially .",
    "[ lem : discretization ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement ] .",
    "fix a parameter @xmath663 $ ] .",
    "we can construct a subset @xmath648 of @xmath512 with cardinality @xmath649 that has the property @xmath664 furthermore , the bound   holds if we replace @xmath126 by @xmath236 .",
    "we choose the discretization @xmath648 to be an @xmath272-covering of @xmath512 with minimal cardinality .",
    "that is , @xmath665 since @xmath666 , we can use a classic volumetric argument to ensure that the covering satisfies @xmath667 .",
    "for example , see  ( * ? ? ? * lem .",
    "when we replace the set @xmath512 with the covering @xmath648 , we incur a discretization error .",
    "we establish the error bound for @xmath126 ; the argument for @xmath236 is identical .",
    "since @xmath668 , it is immediate that @xmath669 we claim that @xmath670 since @xmath126 has standardized entries , @xmath671 where @xmath672 denotes the frobenius norm . combine the last three displays to verify  .",
    "it is quite easy to establish the claim  . for all @xmath673 ,",
    "we have @xmath674 we have used standard norm inequalities and the fact that @xmath512 is a subset of the unit ball .",
    "now , let @xmath675 , and let @xmath676 be a point in @xmath648 for which @xmath677 .",
    "then @xmath678 taking expectations , we arrive at  .",
    "the second step in the proof of proposition  [ prop : partial - replacement ] is to pass from the restricted minimum singular value over the discrete set @xmath648 to a smooth function .",
    "we rely on an exponential smoothing technique that is common in the mathematical literature on statistical physics .",
    "[ lem : smoothing ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement ] , and let @xmath648 be the set introduced in lemma  [ lem : discretization ] . fix a parameter @xmath651 , and define the soft - min function @xmath280 via   then @xmath679 the bound   also holds if we replace @xmath126 by @xmath236",
    "this result follows from trivial bounds on the sum in the definition   of the soft - min function @xmath280 .",
    "the summands are nonnegative , so the sum exceeds its maximum term .",
    "thus , @xmath680 similarly , the sum does not exceed the number of summands times the maximum term , so @xmath681 combine the last two displays and multiply through by the negative number @xmath682 to reach @xmath683 replace @xmath71 by the random matrix @xmath126 and take the expectation to reach  .",
    "similarly , we can take @xmath684 to obtain the result for @xmath236 .",
    "the last step in the proof of proposition  [ prop : partial - replacement ] is the hardest .",
    "we must demonstrate that the expectation @xmath685 of the soft - min function does not change very much when we replace the submatrix @xmath529 with the submatrix @xmath655 .",
    "[ lem : exchange ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement ] ; let @xmath648 be the set described in lemma  [ lem : discretization ] ; and let @xmath280 be the soft - min function   with parameter @xmath651 . then @xmath686 the random matrix @xmath236",
    "is defined in  .",
    "we establish the lemma by replacing the rows of the random matrix @xmath126 by the rows of the random matrix @xmath236 one at a time .",
    "for each @xmath687 , let @xmath688 be the random matrix whose first @xmath689 rows are drawn from @xmath236 and whose remaining rows are drawn from @xmath126 . by construction , @xmath690 and @xmath691 .",
    "it follows that @xmath692 we will demonstrate that @xmath693 combining the last two displays , we arrive at the bound  .",
    "fix an index @xmath599 . by construction , the random matrices @xmath688 and @xmath694 are identical , except in the @xmath599th row . to perform the estimate",
    ", it will be convenient to suppress the dependence of the function @xmath280 on the remaining rows . to that end , define the functions @xmath695 where @xmath696.\\ ] ] we have written @xmath697 and @xmath698 for the @xmath78th rows of @xmath126 and @xmath236 . with these definitions , @xmath699 therefore",
    ", the inequality   is equivalent with @xmath700 since the matrix @xmath126 is drawn from model  [ mod : bdd - mtx ] , the random vector @xmath701 contains independent , standardized entries whose magnitudes are bounded by @xmath505 .",
    "meanwhile , the form   of the matrix @xmath236 shows that the random vector @xmath702 coincides with @xmath701 on the components indexed by @xmath75 , while the entries of @xmath702 indexed by @xmath233 are independent standard normal variables",
    ". sublemma  [ slem : compare - one ] , below , contains the proof of the inequality  .      to complete the argument in lemma  [ lem : exchange ] , we need to control how much a certain function changes when we replace some of the entries of its argument with standard normal variables .",
    "the following result contains the required calculation .",
    "[ slem : compare - one ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement ] , and let @xmath648 be the set defined in lemma  [ lem : smoothing ] . introduce the function @xmath703 where @xmath704 is an arbitrary function .",
    "suppose that @xmath705 is a random vector with independent , standardized entries that are bounded in magnitude by @xmath505 .",
    "suppose that @xmath706 is a random vector with @xmath707 where @xmath708 is a standard normal vector .",
    "then @xmath709    the proof of sublemma  [ slem : compare - one ] is based on a modern interpretation  @xcite of the lindeberg exchange principle  @xcite .",
    "it is similar in spirit with examples from the paper  @xcite .",
    "we apply the following version of the lindeberg principle , which is adapted from these works .",
    "[ fact : lindeberg ] let @xmath710 be a function with three continuous derivatives .",
    "let @xmath711 and @xmath712 be standardized random variables , not necessarily independent , with three finite moments .",
    "then @xmath713.\\ ] ]    the proof of fact  [ fact : lindeberg ] involves nothing more than a third - order taylor expansion of @xmath714 around the origin .",
    "the first- and second- order terms cancel because the random variables @xmath711 and @xmath712 have mean zero and variance one . with this inequality at hand",
    ", we may proceed with the proof of the sublemma .    without loss of generality ,",
    "assume that the index set @xmath715 .",
    "indeed , the entries of the random matrix @xmath126 are independent , standardized , and uniformly bounded so it does not matter which set @xmath75 of columns we distinguish .",
    "this choice allows more intuitive indexing .    for each fixed index @xmath716 ,",
    "define the interpolating vector @xmath717 introduce the function @xmath718 observe that @xmath719 the first identity holds because the sum telescopes .",
    "fact  [ fact : lindeberg ] shows that @xmath720.\\ ] ] we claim that , for each index @xmath716 , @xmath721      & \\leq { \\mathrm{c } } ( \\beta b^4 + \\beta^2 b^6 ) k^{-3/2 } , \\quad\\text{and } \\\\ { \\operatorname{\\mathbb{e}}}\\left [ { \\left\\vert { \\smash{\\psi_j } } \\right\\vert}^3 \\max_{{\\left\\vert { \\alpha } \\right\\vert } \\leq { \\left\\vert { \\smash{\\psi_j } } \\right\\vert } } { \\left\\vert { r_j'''(\\alpha ) } \\right\\vert } \\right ]      & \\leq { \\mathrm{c } } ( \\beta b^4 + \\beta^2 b^6 ) k^{-3/2}. \\end{aligned}\\ ] ] once we establish the bound  , we can combine the last three displays to reach @xmath722 the main result   follows .    to establish  , fix an index @xmath723 .",
    "the forthcoming sublemma  [ slem : derivatives ] will demonstrate that @xmath724 in this expression , @xmath725 is a random vector that is independent from @xmath726 ; the precise distribution of @xmath727 is immaterial .",
    "note that @xmath728 indeed , the derivative @xmath729 , where @xmath730 is the @xmath78th standard basis vector .",
    "the last inequality holds because the conditions @xmath668 and @xmath731 allow us to invoke the assumption  .",
    "we arrive at the following bound on the quantity of interest from  : @xmath732      \\leq { \\mathrm{c } } k^{-3/2 } \\left ( \\beta { \\operatorname{\\mathbb{e}}}\\left [ { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert}^3 \\max_{{\\left\\vert { \\alpha } \\right\\vert } \\leq { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert } } { \\left\\vert { { { \\bm{\\xi}}}_j(\\alpha ) \\cdot { { \\bm{v } } } } \\right\\vert } \\right ]      + \\beta^2 { \\operatorname{\\mathbb{e}}}\\left [ { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert}^3 \\max_{{\\left\\vert { \\alpha } \\right\\vert } \\leq { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert } } { \\left\\vert { { { \\bm{\\xi}}}_j(\\alpha ) \\cdot { { \\bm{v } } } } \\right\\vert}^3 \\right ] \\right).\\end{gathered}\\ ] ] we have merged the bounds   and   to control @xmath733 .",
    "next , we invoked jensen s inequality to draw the expectation over @xmath727 out of the maximum over @xmath122 .",
    "last , we combined the expectations to reach  .",
    "it remains to bound the expectations on the right - hand side .",
    "let us begin with the term in   that is linear in @xmath734 .",
    "in view of the identity @xmath735 , @xmath736      & \\leq { \\operatorname{\\mathbb{e}}}\\left [ { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert}^3 \\max_{{\\left\\vert { \\alpha } \\right\\vert } \\leq{\\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert } }      \\left ( { \\left\\vert { \\smash{\\alpha v_j } } \\right\\vert } + { \\left\\vert { { { \\bm{\\xi}}}_j(0 ) \\cdot { { \\bm{v } } } } \\right\\vert } \\right ) \\right ] \\\\      & = { \\operatorname{\\mathbb{e}}}\\left [ { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert}^4 { \\left\\vert { \\smash{v_j } } \\right\\vert } \\right ]      + \\left({\\operatorname{\\mathbb{e}}}{\\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert}^3 \\right)\\left ( { \\operatorname{\\mathbb{e}}}{\\left\\vert { { { \\bm{\\xi}}}_j(0)\\cdot { { \\bm{v } } } } \\right\\vert } \\right ) \\\\      & \\leq b^4 k^{-1/2 } + b^3 . \\end{aligned}\\ ] ] we used the assumption that @xmath737 is independent from @xmath738 to factor the expectation in the second term in the second line . the bounds in the third line",
    "exploit the fact that @xmath739 and , via the hypothesis  , the fact that @xmath740 .",
    "we also relied on the estimate @xmath741 indeed , jensen s inequality allows us to replace the expectation with the second moment , which simplifies to @xmath742 because @xmath743 is an independent family of standardized random variables . since @xmath740 , the norm @xmath742 does not exceed one .    continuing to the term in   that is quadratic in @xmath744 , we pursue the same approach to see that @xmath745      & \\leq { \\mathrm{c } } { \\operatorname{\\mathbb{e}}}\\left [ { \\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert}^3 \\max_{{\\left\\vert { \\alpha } \\right\\vert } \\leq{\\left\\vert { \\smash{{\\varphi}_j } } \\right\\vert } }      \\left ( { \\left\\vert { \\smash{\\alpha v_j } } \\right\\vert}^3 + { \\left\\vert { { { \\bm{\\xi}}}_j(0 ) \\cdot { { \\bm{v } } } } \\right\\vert}^3 \\right ) \\right ] \\\\      & \\leq { \\mathrm{c } } \\left ( b^6 k^{-3/2 } + b^6 \\right ) . \\end{aligned}\\ ] ] this bound relies on the khintchine - type inequality @xmath746 for example , see  ( * ? ? ?",
    "* cor .  5.12 ) .",
    "we remark that this estimate could be improved if we had additional information about the distribution of the entries of @xmath126 .    substituting the bounds   and   into",
    ", we obtain @xmath747      & \\leq { \\mathrm{c } } k^{-3/2 } \\left [ \\beta \\big(b^4 k^{-1/2 } + b^3 \\big )      + \\beta^2 \\big(b^6 k^{-3/2 } + b^6 \\big ) \\right ] \\\\      & \\leq { \\mathrm{c } } ( \\beta b^4 + \\beta^2 b^6 ) k^{-3/2}. \\end{aligned}\\ ] ] this establishes the first branch of the claim  .",
    "the second branch follows from a similar argument , where we use explicit values for the moments of a standard normal variable instead of the uniform upper bound @xmath505 .",
    "the final obstacle in the proof of proposition  [ prop : partial - replacement ] is to bound the derivatives that are required in sublemma  [ slem : compare - one ] .",
    "this argument uses some standard methods from statistical physics , and it is similar with the approach in the paper  @xcite .",
    "[ slem : derivatives ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement ] and sublemma  [ slem : compare - one ] . let @xmath748 be a linear function , so its derivative @xmath749 is a constant vector . define the function @xmath750 where @xmath704 is arbitrary .",
    "the third derivative of this function satisfies @xmath751 in this expression , @xmath725 is a random vector that is independent from @xmath752 .",
    "introduce a ( parameterized ) probability measure @xmath753 on the set @xmath648 : @xmath754 we treat the normalizing factor @xmath755 as a function of @xmath122 , and we write its derivatives as @xmath756 .",
    "it is convenient to use the statistical mechanics notation for expectation with respect to this measure . for any function @xmath757 , @xmath758 for brevity",
    ", we always suppress the dependence of @xmath759 on the parameter @xmath122 .",
    "we often suppress the dependence of @xmath760 on @xmath122 as well .",
    "the function @xmath714 is proportional to the logarithm of the normalizing factor @xmath755 : @xmath761 thus , it is straightforward to express the third derivative of @xmath714 in terms of the derivatives of @xmath762 : @xmath763 by direct calculation , the derivative @xmath764 satisfies @xmath765 the second derivative is @xmath766      { \\mathrm{e}}^{- \\beta ( { { \\bm{\\xi}}}(\\alpha ) \\cdot { { \\bm{t}}})^2 + q({{\\bm{t } } } ) } \\\\      & = - 2\\beta { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t}}})^2 } \\right\\rangle }      + 4\\beta^2 { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t}}})^2({{\\bm{\\xi}}}\\cdot { { \\bm{t}}})^2 } \\right\\rangle}. \\end{aligned}\\ ] ] the third derivative is @xmath767      { \\mathrm{e}}^{- \\beta ( { { \\bm{\\xi}}}(\\alpha ) \\cdot { { \\bm{t}}})^2 + q({{\\bm{t } } } ) } \\\\      & = 12 \\beta^2 { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t}}})^3 ( { { \\bm{\\xi } } } \\cdot { { \\bm{t } } } ) } \\right\\rangle }      - 8 \\beta^3 { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t}}})^3 ( { { \\bm{\\xi } } } \\cdot { { \\bm{t}}})^3 } \\right\\rangle }   \\end{aligned}\\ ] ] we ascertain that @xmath768 \\\\      & + \\beta^2 \\big [ 8 { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t}}})^3 ( { { \\bm{\\xi } } } \\cdot { { \\bm{t}}})^3 } \\right\\rangle }      - 24 { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t } } } ) ( { { \\bm{\\xi } } } \\cdot { { \\bm{t } } } ) } \\right\\rangle }      { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t}}})^2({{\\bm{\\xi } } } \\cdot { { \\bm{t}}})^2 } \\right\\rangle }      + 16 { \\left\\langle { ( { { \\bm{\\xi } } } ' \\cdot { { \\bm{t } } } ) ( { { \\bm{\\xi } } } \\cdot { { \\bm{t } } } ) } \\right\\rangle}^3       \\big ] . \\end{aligned}\\ ] ] this completes the calculation of the exact form of the third derivative of @xmath714 .    next , we simplify the formula   using basic probability inequalities for the expectation @xmath759 .",
    "first , consider the terms that are linear in @xmath734 .",
    "observe that @xmath769 indeed , jensen s inequality allows us to draw the absolute value inside the average , and we can invoke hlder s inequality to pull out the maximum of the first term . similarly , since @xmath770 , @xmath771 next , consider the terms that are quadratic in @xmath734 .",
    "the simplest is @xmath772 using jensen s inequality , we find that @xmath773 last , using lyapunov s inequality twice , @xmath774 introduce the last five displays into   to arrive at the bound @xmath775    last , we want to replace the averages with respect to @xmath753 by averages with respect to a simpler probability measure that does not depend on @xmath752 .",
    "this argument relies on a correlation inequality .",
    "define another probability measure @xmath776 on the set @xmath648 : @xmath777 write @xmath778 for averages with respect to this new measure .",
    "first , consider the average @xmath779 let @xmath780 be the random variable obtained by pushing forward the measure @xmath776 from @xmath648 to the nonnegative real line . since @xmath781 is increasing and @xmath782 is decreasing , chebyshev s association inequality  ( * ? ? ?",
    "* thm .  2.14 )",
    "provides that @xmath783 in summary , @xmath784 the same argument shows that @xmath785    introduce   and   into the bound   to reach the inequality @xmath786 the statement of the result follows when we reinterpret the averages with respect to @xmath776 as expectations with respect to a random vector @xmath725 with distribution @xmath787 .",
    "in section  [ sec : replacement ] , we showed that the restricted singular value @xmath523 of the random matrix @xmath126 does not change very much if we replace @xmath126 with a hybrid matrix @xmath236 , defined in  , that contains many standard normal random variables .",
    "our next goal is to relate the restricted singular value @xmath788 of the hybrid matrix to the excess width of the set @xmath512 .",
    "[ prop : excess - width ] let @xmath126 be an @xmath112 random matrix from model  [ mod : bdd - mtx ] with magnitude bound @xmath505 , and let @xmath35 be an @xmath112 random matrix with independent , standard normal entries .",
    "let @xmath75 be a subset of @xmath541 with cardinality @xmath211 , and let @xmath512 be a closed subset of @xmath84 . introduce the @xmath112 random matrix @xmath525 from  .",
    "then @xmath789 furthermore , if @xmath512 is convex and @xmath790 , @xmath791    to obtain this result , we will invoke the gaussian minimax theorem  @xcite to reduce the random matrix bounds to simpler bounds involving random vectors .",
    "this approach has been used to study the ordinary singular values of a gaussian matrix  ( * ? ? ?",
    ". it also plays a role in the analysis of restricted singular values of gaussian matrices  @xcite .",
    "the application here is complicated significantly by the presence of the non - gaussian matrix @xmath240 .",
    "the proof of proposition  [ prop : excess - width ] involves several long steps , so it is helpful once again to summarize the calculations that are required .",
    "first , let us explain the process by which we obtain the lower bound for a general closed subset @xmath512 of the euclidean unit ball @xmath792 .",
    "we will argue that @xmath793 in the first line , we pass from the expected square to the square of the expectation , which reduces the technical complexity of the rest of the argument .",
    "next , we replace the @xmath112 gaussian matrix @xmath35 with an expectation over two independent standard normal vectors @xmath794 and @xmath795 .",
    "third , we remove the remaining piece @xmath240 of the original random matrix to arrive at a formula involving only the random vector @xmath796 . finally , we replace the missing coordinates in the random vector @xmath796 to obtain a bound in terms of the excess width @xmath797 .",
    "we arrive at the inequality  .",
    "the upper bound follows from a calculation that , superficially , appears similar . assuming that @xmath512 is convex",
    ", we may calculate that @xmath798 the first step is a type of poincar inequality , which requires some specialized concentration results for the restricted singular value of the hybrid matrix .",
    "the second and fourth steps of this argument involve convex duality arguments that are not present in the proof of the lower bound  .",
    "we have used the assumption that @xmath790 to simplify the error term when we pass from the second line to the third .",
    "it is also quite tricky to reintroduce the missing coordinates in the last step .",
    "even so , we arrive at the bound  .",
    "we require a moment comparison inequality to pass from the expected square of the restricted singular value of the hybrid matrix to the expectation of the restricted singular value itself .",
    "[ lem : hybrid - rsv - poincare ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "then @xmath799    define the random variable @xmath800 we need a bound for @xmath97 .",
    "we obtain this result by applying a moment comparison inequality for @xmath126 , conditional on @xmath35 , and then we apply a moment comparison inequality for @xmath35 .",
    "sublemma  [ slem : hybrid - poincare ] , applied conditionally , with the choice @xmath801 , gives @xmath802 \\right ]      \\leq { \\operatorname{\\mathbb{e}}}\\left [ \\big ( { \\operatorname{\\mathbb{e } } } [ x \\,\\vert\\ , { { \\bm{\\gamma } } } ] + { \\mathrm{c}}b m^{1/4 } \\big)^2 \\right].\\ ] ] the function @xmath803 + { \\mathrm{c}}b m^{1/4}$ ] is 1-lipschitz , so the gaussian variance inequality , fact  [ fact : gauss - variance ] , implies that @xmath804 + { \\mathrm{c}}b m^{1/4 } \\big)^2 \\right ]      \\leq \\big ( { \\operatorname{\\mathbb{e}}}x + { \\mathrm{c } } b m^{1/4 } \\big)^2 + 1.\\ ] ] combine the last two displays , and adjust the constant to complete the proof .",
    "the proof of lemma  [ lem : hybrid - rsv - poincare ] requires a separate moment comparison result for a random variable that depends only on the original matrix @xmath240 .",
    "[ slem : hybrid - poincare ] adopt the notation and hypotheses of lemma  [ lem : hybrid - rsv - poincare ] . for a fixed @xmath112 matrix @xmath71 , @xmath805    define the random variable @xmath806 first ,",
    "observe that @xmath807 the last inequality is jensen s .",
    "we can use concentration to bound this quantity .",
    "mutatis mutandis , repeat the proof of equation   from proposition  [ prop : rsv - concentration ] to see that , for all @xmath508 , @xmath808 invoke the subadditivity of the square root and change variables : @xmath809 jensen s inequality and integration by parts deliver @xmath810 the third inequality follows from  . introduce the last display into   to reach @xmath811 rearrange this relation to complete to proof of  .",
    "to prove proposition  [ prop : excess - width ] , the first step is to replace the gaussian matrix in the quantity of interest with a pair of gaussian vectors .",
    "the key to this argument is the following technical result .",
    "[ lem : my - comparison ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "let @xmath71 be a fixed @xmath112 matrix , and let @xmath795 and @xmath794 be independent standard normal random vectors .",
    "then , for all @xmath812 , @xmath813 furthermore , if @xmath512 is convex , @xmath814    this result depends on the gaussian minimax theorem  @xcite ; see fact  [ fact : gauss - minmax ] for a statement .",
    "lemma  [ lem : my - comparison ] is similar with early results of gordon  ( * ? ? ?",
    "the detailed argument here is adapted from  ( * ? ? ?",
    "2.1 ) ; see also stojnic  @xcite .",
    "the basic idea is to express the quantity of interest as the value of a saddle - point problem : @xmath815 then we apply the gaussian minimax theorem to obtain probabilistic lower bounds .",
    "when @xmath512 is convex , we can also invoke convex duality to interchange the minimum and maximum , which leads to complementary bounds . to proceed with this approach",
    ", however , it is convenient to work with a slightly different minimax problem .",
    "define the deterministic function @xmath816 let @xmath100 be a standard normal random variable , independent from everything else . introduce two centered gaussian processes : @xmath817 indexed by @xmath528 and @xmath818 .",
    "let us verify that these processes satisfy the conditions required by the gaussian minimax theorem , fact  [ fact : gauss - minmax ] .",
    "first , for all parameters @xmath528 and @xmath818 , @xmath819 second , for all parameters @xmath820 and @xmath821 , @xmath822      - { \\operatorname{\\mathbb{e}}}\\big [ y({{\\bm{t } } } , { { \\bm{u } } } ) y({{\\bm{t } } } ' , { { \\bm{u } } } ' ) \\big ]      = \\big ( { \\left\\vert { { { \\bm{u } } } } \\right\\vert } { \\left\\vert { \\smash{{{\\bm{u } } } ' } } \\right\\vert } - { { \\bm{u } } } \\cdot { { \\bm{u } } } ' \\big )      \\big ( { \\left\\vert { \\smash{{{\\bm{t}}}_{j^c } } } \\right\\vert } { \\left\\vert { \\smash{{{\\bm{t}}}_{j^c } ' } } \\right\\vert }      - { { \\bm{t}}}_{j^c } \\cdot { { \\bm{t}}}'_{j^c } \\big).\\ ] ] by the cauchy ",
    "schwarz inequality , @xmath823      & = { \\operatorname{\\mathbb{e}}}\\big [ y({{\\bm{t } } } , { { \\bm{u } } } ) y({{\\bm{t } } } , { { \\bm{u } } } ' ) \\big ] ; \\label{eqn : minmax - cond2a } \\\\ { \\operatorname{\\mathbb{e}}}\\big [ x({{\\bm{t } } } , { { \\bm{u } } } ) x({{\\bm{t } } } ' , { { \\bm{u } } } ) \\big ]      & = { \\operatorname{\\mathbb{e}}}\\big [ y({{\\bm{t } } } , { { \\bm{u } } } ) y({{\\bm{t } } } ' , { { \\bm{u } } } ) \\big ] ; \\label{eqn : minmax - cond2b } \\\\ { \\operatorname{\\mathbb{e}}}\\big [ x({{\\bm{t } } } , { { \\bm{u } } } ) x({{\\bm{t } } } ' , { { \\bm{u } } } ' ) \\big ]      & \\geq { \\operatorname{\\mathbb{e}}}\\big [ y({{\\bm{t } } } , { { \\bm{u } } } ) y({{\\bm{t } } } ' , { { \\bm{u } } } ' ) \\big ] .",
    "\\label{eqn : minmax - cond3}\\end{aligned}\\ ] ] the formulas  ,  , and   verify the conditions of the gaussian minimax theorem .",
    "fact  [ fact : gauss - minmax ] delivers the bound @xmath824 this estimate does involve a small technicality .",
    "we can only apply the gaussian minimax theorem to a finite subset of @xmath825 , so we must make an approximation argument to pass to the entire set .",
    "we omit the details .",
    "now , let us determine the values of the saddle - point problems in  .",
    "first , @xmath826 similarly , @xmath827 introducing the last two identities into  , we arrive at @xmath828    our next task is to remove the term involving @xmath100 from the left - hand side of  . to do so",
    ", we simply calculate that @xmath829 combine the last two displays to reach @xmath830 take the complements of both probabilities and rearrange to conclude that   is correct .",
    "the second result   requires an additional duality argument .",
    "if we replace the function @xmath831 and the random processes @xmath99 and @xmath420 with their negations , all of the variance calculations above remain valid . in particular , the relations  ,  , and   permit us to apply fact  [ fact : gauss - minmax ] with the roles of @xmath512 and @xmath792 reversed .",
    "this step yields @xmath832 let us examine the saddle - point problems .",
    "since @xmath833 is bilinear and the sets @xmath792 and @xmath512 are compact and convex , the sion minimax theorem  @xcite allows us to interchange the minimum and maximum .",
    "thus @xmath834 similarly ,",
    "@xmath835 combining the last three displays , we reach @xmath836 proceeding as before , we may remove the dependence on @xmath100 from the left - hand side .",
    "we confirm that   is true .",
    "our next goal is to convert the probability bounds from lemma  [ lem : my - comparison ] into expectation bounds .",
    "lemma  [ lem : apply - minmax - lower ] gives a lower bound that is valid for every subset @xmath512 of the unit ball , and lemma  [ lem : apply - minmax - upper ] gives an upper bound that is valid when @xmath512 is also convex .",
    "[ lem : apply - minmax - lower ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "let @xmath71 be a fixed @xmath112 matrix , and let @xmath795 and @xmath794 be independent standard normal vectors .",
    "then @xmath837 in particular , @xmath838 the latter bound is also valid if we replace @xmath240 by @xmath839 .",
    "we make the abbreviations @xmath840 first , note that @xmath841 the last inequality is jensen s .",
    "to bound the right - hand side , we use integration by parts and formula   from lemma  [ lem : my - comparison ] : @xmath842 finally , we make the estimates @xmath843      \\leq \\operatorname{var } [ y ] ^{1/2 }      \\leq 1.\\ ] ] the last inequality follows from the gaussian variance inequality , fact  [ fact : gauss - variance ] .",
    "indeed , the function @xmath844 is 1-lipschitz because the set @xmath512 is contained in the unit ball . in summary , @xmath845 we arrive the advertised bound  .",
    "[ lem : apply - minmax - upper ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "let @xmath71 be a fixed @xmath112 matrix , and let @xmath795 and @xmath794 be independent standard normal vectors .",
    "then @xmath846 in particular , @xmath847    we follow the same pattern as in lemma  [ lem : apply - minmax - lower ] . using the same notation , we calculate that @xmath848 in this case , we invoke   from lemma  [ lem : my - comparison ] to obtain the penultimate inequality .",
    "the next step in the proof of proposition  [ prop : excess - width ] is to remove the remaining section of the random matrix @xmath126 from the bounds in lemmas  [ lem : apply - minmax - lower ] and  [ lem : apply - minmax - upper ] .",
    "[ lem : remove - phik ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "let @xmath320 be an @xmath112 random matrix with independent , standardized entries that satisfy assumption  [ eqn : subgauss - hyp ] .",
    "then @xmath849 in particular , this conclusion is valid when @xmath850 or when @xmath851 .",
    "abbreviate @xmath852 .",
    "observe that @xmath853 therefore , we have the deterministic bounds @xmath854 the @xmath855 random matrix @xmath236 has independent , standardized entries that satisfy  .",
    "for all @xmath508 , fact  [ fact : subgauss - mtx - tails ] shows that its extreme singular values satisfy the probability bounds @xmath856 these inequalities allow us to treat the singular values of @xmath236 as if they were equal to @xmath857 .",
    "we may now perform the following estimates : @xmath858 \\\\      & \\leq { \\operatorname{\\mathbb{e}}}\\min_{{{\\bm{t } } } \\in t_j } \\big ( \\sqrt{m } { \\left\\vert { { { \\bm{t } } } } \\right\\vert }      + { { \\bm{g}}}_{j^c } \\cdot { { \\bm{t}}}_{j^c } \\big ) + { \\operatorname{\\mathbb{e}}}\\big ( \\sigma_{\\max}({{\\bm{\\psi } } } ) - \\sqrt{m } \\big)_+ .",
    "\\end{aligned}\\ ] ] the first inequality is  . then we add and subtract @xmath857 from the maximum singular value .",
    "last , we recall that @xmath512 is a subset of the unit ball .",
    "set @xmath859 , and calculate that @xmath860 we split the integral at @xmath122 , and we change variables in the second integral . for the first",
    "integral , we use the trivial bound of one on the probability .",
    "then we invoke the probability inequality  .    combining the last two displays and collecting constants , we arrive at @xmath861 an entirely similar argument delivers a matching lower bound .",
    "together , these estimates complete the proof .",
    "the last step in the proof of proposition  [ prop : excess - width ] is to examine the excess - width - like functional from lemma  [ lem : remove - phik ] that involves the term @xmath862 .",
    "we must show that this term does not change very much if we reintroduce the coordinates listed in @xmath75 .",
    "lemma  [ lem : missing - coords - lower ] gives the easy proof of the lower bound .",
    "lemma  [ lem : missing - coords - upper ] contains the upper bound , which requires that @xmath512 be convex .    [ lem : missing - coords - lower ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "then @xmath863    this result is an immediate consequence of jensen s inequality : @xmath864 we rely on the fact that @xmath865 and @xmath866 are independent standard normal vectors .",
    "[ lem : missing - coords - upper ] adopt the notation and hypotheses of proposition  [ prop : excess - width ] .",
    "if the set @xmath512 is convex , then @xmath867    it takes some real work to reverse the inequality in lemma  [ lem : missing - coords - lower ] .",
    "first , a variant of the argument in lemma  [ lem : my - comparison ] shows that @xmath868 the latter relation implies , via the argument in lemma  [ lem : apply - minmax - upper ] , that @xmath869 adding and subtracting @xmath870 , which is at most @xmath857 , we see that @xmath871 we have used the assumption that @xmath512 is a subset of the euclidean unit ball . now , @xmath872^{1/2 }      \\leq 1 $ ] by the gaussian variance inequality , fact  [ fact : gauss - variance ] . in summary ,",
    "@xmath873 in other words , for a gaussian matrix , the excess width gives an _ upper _ bound for the restricted singular value @xmath874 .",
    "we can repeat the arguments from this section to show that the excess width with deleted coordinates gives a lower bound for the restricted singular value @xmath874 .",
    "indeed , @xmath875 combining   and  , we conclude that @xmath876 rearrange to complete the proof .",
    "in this section , we show how to establish corollary  [ cor : rsv - four ] as a consequence of theorem  [ thm : rsv - bdd ] .",
    "the proof depends on a truncation argument .",
    "fix parameters @xmath108 and @xmath109 .",
    "assume that @xmath126 is an @xmath112 matrix that follows model  [ mod : p - mom - mtx ] with parameters @xmath110 and @xmath125 .",
    "let @xmath138 be a compact subset of the unit ball @xmath84 .",
    "we will prove the lower bound for the minimum singular value of @xmath126 restricted to @xmath138 .",
    "when @xmath138 is convex , an entirely similar approach yields the corresponding upper bound .    fix a truncation parameter @xmath477 that satisfies @xmath877 .",
    "decompose the random matrix @xmath126 as @xmath878 by applying the truncation described below in lemma  [ lem : trunc - rv ] separately to each entry of @xmath126 .",
    "this procedure ensures that @xmath879 contains independent , symmetric , standardized entries , each bounded by @xmath880 . in other words , @xmath879 follows model  [ mod : bdd - mtx ] with @xmath881 .",
    "the tail @xmath882 contains independent , centered entries , each with variance bounded by @xmath883 and whose @xmath110th moment is bounded by @xmath884 .",
    "we can control the restricted singular values of @xmath126 using the triangle inequality : @xmath885 we bound the restricted singular value of the bounded matrix @xmath879 using theorem  [ thm : rsv - bdd ] . to bound @xmath886",
    ", we apply a simple norm estimate , fact  [ fact : heavy - tail - norm ] , based on the matrix rosenthal inequality  ( * ? ? ?",
    "i ) .    since @xmath879 follows model  [ mod : bdd - mtx ] with @xmath881 , theorem  [ thm : rsv - bdd ] and   give the probability bound @xmath887 select @xmath888 to make the tail probability negligible : @xmath889 taking square roots inside the event , we reach @xmath890 this step depends on the subadditivity of the square root .",
    "meanwhile , the entries of @xmath882 are centered , have variances at most @xmath883 , and have @xmath110th moments bounded by @xmath891 .",
    "therefore , we can apply the norm bound for heavy - tailed random matrices , fact  [ fact : heavy - tail - norm ] , to see that @xmath892 define the positive quantity @xmath272 via the relation @xmath893 .",
    "select @xmath894 to obtain @xmath895 the key point here is that we can arrange for @xmath886 to have order @xmath896 with high probability .",
    "combine  ,  , and   to reach @xmath897 set the truncation parameter @xmath898 to equate the exponents on @xmath899 in the two terms that depend on @xmath477 . then simplify using @xmath900 to obtain @xmath901",
    "note that both powers in the event are bounded away from @xmath902 , so we can absorb the logarithm by increasing the power slightly .",
    "furthermore , we can introduce a function @xmath551 that is strictly positive for @xmath108 to reach the inequality @xmath903 the constant @xmath166 depends only on the parameter @xmath110 .",
    "the exponential vanishes faster than any polynomial , so we can combine the terms on the right - hand side to complete the proof of  .    for the upper bound , we use theorem  [ thm : rsv - bdd ] to control the restricted singular value .",
    "this leads to a factor of @xmath904 instead of @xmath477 in the estimate corresponding with  . as a consequence , we select @xmath905 instead .",
    "we obtain an inequality of the form @xmath906 combine the terms on the right - hand side to complete the proof of  .      in this section",
    ", we describe a truncation procedure for scalar random variables .",
    "the arguments here are entirely standard , but we include details for completeness .",
    "[ lem : trunc - rv ] let @xmath711 be a random variable that satisfies the properties listed in model  [ mod : p - mom - mtx ] .",
    "let @xmath477 be a parameter that satisfies @xmath907 .",
    "then we have the decomposition @xmath908 where @xmath909 and @xmath910    define the random variable @xmath911.\\ ] ] since @xmath711 is standardized and symmetric , @xmath912 is also standardized and symmetric . to ensure that the decomposition   holds , we must set @xmath913 the random variable @xmath914",
    "is also centered because of the symmetry of @xmath711 .    to establish the other properties of @xmath912 and @xmath914",
    ", we need to calculate some expectations .",
    "first , using integration by parts and markov s inequality , @xmath915      & = \\int_0^r 2 \\zeta { { \\mathbb}{p}\\left\\ { { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } > r } \\right\\ } } { \\ , { \\mathrm{d}{\\zeta } } }      + \\int_r^\\infty 2 \\zeta { { \\mathbb}{p}\\left\\ { { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } > \\zeta } \\right\\ } } { \\ , { \\mathrm{d}{\\zeta } } } \\\\      & \\leq \\int_0^r 2 \\zeta \\frac{{\\operatorname{\\mathbb{e}}}{\\left\\vert { \\smash{{\\varphi } } } \\right\\vert}^p } { r^p } { \\ , { \\mathrm{d}{\\zeta } } }      + \\int_r^\\infty 2 \\zeta \\frac{{\\operatorname{\\mathbb{e}}}{\\left\\vert { \\smash{{\\varphi } } } \\right\\vert}^p } { \\zeta^p } { \\ , { \\mathrm{d}{\\zeta } } }      \\leq \\frac{2 \\nu^p}{r^{p-2}}. \\end{aligned}\\ ] ] in the last step , we used the assumption that @xmath916 .",
    "a similar calculation shows that @xmath917      & = \\int_0^r 2 \\zeta { { \\mathbb}{p}\\left\\ { { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } > \\zeta } \\right\\ } } { \\ , { \\mathrm{d}{\\zeta } } }   \\\\      & = \\int_0^\\infty 2 \\zeta { { \\mathbb}{p}\\left\\ { { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } > \\zeta } \\right\\ } } { \\ , { \\mathrm{d}{\\zeta } } }      - \\int_r^\\infty 2 \\zeta { { \\mathbb}{p}\\left\\ { { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } > \\zeta } \\right\\ } } { \\ , { \\mathrm{d}{\\zeta } } } \\\\      & \\geq { \\operatorname{\\mathbb{e}}}\\big [ { \\varphi}^2 \\big ] - \\frac{2\\nu^p}{(p-2 ) r^{p-2 } }      \\geq 1 - \\frac{\\nu^p}{r^{p-2}}.   \\end{aligned}\\ ] ] the last relation holds because @xmath711 is standardized .",
    "it follows that @xmath918 the last estimate holds because @xmath916 and of the assumption @xmath877 .",
    "we are now prepared to verify the uniform bound on @xmath912 : @xmath919 the last inequality follows from  .",
    "next , we need to bound the variance of @xmath914 .",
    "we have @xmath920      = { \\operatorname{\\mathbb{e}}}\\big[{\\varphi}^2 \\mathbb{1}\\ { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } > r \\ } \\big ]      + \\left(\\frac{1-\\alpha}{\\alpha}\\right)^2 { \\operatorname{\\mathbb{e}}}\\big [ { \\varphi}^2 \\mathbb{1}\\ { { \\left\\vert { \\smash{{\\varphi } } } \\right\\vert } \\leq r \\ } \\big ]      \\leq \\frac{2 \\nu^p}{r^{p-2 } } + \\left ( \\frac{\\nu^{p/2}/r^{p/2 - 1}}{1/2 } \\right)^2 { \\operatorname{\\mathbb{e}}}\\big [ \\psi^2 \\big ]      \\leq \\frac{6 \\nu^p}{r^{p-2}}. \\end{aligned}\\ ] ] the first identity holds because the two indicators are orthogonal random variables .",
    "the second relation uses the expectation calculation   and the estimate  ; we have dropped the indicator in the second expectation .",
    "the last estimate holds because @xmath711 is standardized .",
    "last , we need to check the moment inequality for @xmath914",
    ". this estimate follows by applying the triangle inequality to the definition  : @xmath921 we have dropped the indicators after invoking the triangle inequality .",
    "finally , we introduced the estimate  .",
    "[ part : rap ]    this part of the paper develops a condition under which the random projection of a set fails with high probability .",
    "this argument establishes the second part of the universality law for the embedding dimension , theorem  [ thm : univ - embed](b ) .",
    "section  [ sec : rap - bdd ] contains the main technical result , a condition under which a bounded random matrix maps a point in a set to the origin .",
    "section  [ sec : rap - four ] extends this argument to the heavy - tailed random matrix model , model  [ mod : p - mom - mtx ] . in section  [ sec : car - to - embed ] , we use the latter result to derive theorem  [ thm : univ - embed](b ) .",
    "the remaining parts of the section lay out the supporting argument .",
    "in this section , we introduce an analytic functional whose value determines whether a linear transformation maps a point in a set to the origin",
    ". then we present the main technical result , which gives an estimate for this functional evaluated on a random projector from model  [ mod : bdd - mtx ] .",
    "the rest of the section outlines the main steps in the proof of the result .      to study",
    "when a projector maps a point a set to the origin , we use an approach based on polarity .",
    "let us make the following definition .",
    "let @xmath922 be a closed , convex cone .",
    "let @xmath71 be an @xmath112 matrix .",
    "define the quantity @xmath923 note that the range of the inner minimum involves the polar @xmath924 of the cone .",
    "we refer to @xmath925 as the rap functional .    to see why the rap functional is important , consider a closed , spherically convex subset @xmath1 of @xmath926 . the second conclusion of proposition  [ prop : annihilate ] states that @xmath927 therefore , we can obtain a sufficient condition that @xmath71 maps a point in @xmath1 to zero by providing a lower bound for the rap functional .",
    "the main technical result in this part of the paper is a theorem on the behavior of the rap functional of a bounded random matrix .",
    "[ thm : car - bdd ] place the following assumptions :    * let @xmath196 and @xmath255 be natural numbers with @xmath928 . *",
    "let @xmath154 be a closed , convex cone in @xmath929 , and define @xmath930 .",
    "* draw an @xmath112 random matrix @xmath126 from model  [ mod : bdd - mtx ] with bound @xmath505 .",
    "then the squared rap functional @xmath931 has the following properties .    1 .",
    "[ it : car - bdd - concentration ] the squared rap functional concentrates about its mean on a scale of @xmath507 : @xmath932 2 .",
    "[ it : car - bdd - width ] the expected square of the rap functional is bounded below : @xmath933    this result _ does _ use the symmetry assumption in model  [ mod : bdd - mtx ] .",
    "the proof of theorem  [ thm : car - bdd ] is quite long , even though we can borrow a lot from the proof of theorem  [ thm : rsv - bdd ] .",
    "this section contains an overview of the calculations that are required with forward references to the detailed arguments .",
    "theorem  [ thm : car - bdd ] states that the quantity @xmath931 concentrates around its mean .",
    "the proof is essentially the same as the proof of proposition  [ prop : rsv - concentration ] , which shows that the squared restricted singular value @xmath934 concentrates .",
    "we omit the repetitive details .",
    "let us proceed with the proof of theorem  [ thm : car - bdd ] .",
    "define the sets @xmath935 where the radius @xmath936 for some universal constant @xmath74 .",
    "next , we construct a family of closed , convex subsets of @xmath173 and @xmath138 . for each @xmath937 and each @xmath938 , define @xmath939 fix the cardinality parameter @xmath940 . as in the proof of theorem  [ thm : rsv - bdd ] , we have the decompositions @xmath941 furthermore , @xmath942 we maintain the heuristic that the cardinality @xmath211 is much smaller than either ambient dimension @xmath196 or @xmath255 .      to bound the quantity @xmath943",
    ", we must perform several complicated estimates .",
    "we give an outline of the calculation here , with the details postponed to a series of propositions .",
    "first , we must account for the error we incur when we truncate the cone @xmath924 to the wedge @xmath173 .",
    "proposition  [ prop : car - cone - truncation ] demonstrates that @xmath944 this inequality is based on an estimate for the norm of the point @xmath945 where the inner minimum is achieved , as well as a probability bound for the norm of the random matrix @xmath126 .",
    "next , we pass from the minimum over the full sets @xmath173 and @xmath138 to minima over their subsets @xmath946 and @xmath512 : @xmath947 the proof of this inequality hews to the argument in proposition  [ prop : rsv - dissection ] .",
    "we just need to invoke the concentration inequality from theorem  [ thm : car - bdd ] in lieu of the concentration inequality from proposition  [ prop : rsv - concentration ] , and we take into account the bound   on the number of subsets in the decomposition .",
    "further details are omitted .    we are now prepared to perform the exchange argument to pass from the matrix @xmath126 to a matrix @xmath236 that contains many standard normal entries .",
    "fix subsets @xmath937 and @xmath284 , each with cardinality @xmath211 .",
    "introduce the random matrix @xmath948 where @xmath35 is an @xmath112 standard normal matrix .",
    "proposition  [ prop : partial - replacement - redux ] gives the bound @xmath949 the proof is similar with the proof of proposition  [ prop : partial - replacement ] .",
    "we discretize both sets ; we smooth the minima using the soft - min function ; and then we apply the lindeberg principle .",
    "the main distinction is that we can replace even less of the matrix @xmath126 than before .",
    "the second line in   is an immediate consequence of the facts @xmath950 and @xmath951 .    to continue",
    ", we must identify a geometric functional that is hiding within the expression  .",
    "write @xmath930 .",
    "proposition  [ prop : car - width ] demonstrates that @xmath952 as in proposition  [ prop : excess - width ] , the main tool is the gaussian minimax theorem , fact  [ fact : gauss - minmax ] , which allows us to break down the standard normal matrix @xmath35 into simpler quantities .",
    "the proof requires some convex duality arguments , as well as some delicate considerations that did not arise before .",
    "last , we linearize the function @xmath533 in  : @xmath953 we have employed the observation that @xmath954 here , @xmath955 is a standard normal vector .",
    "now , sequence the estimates  , , , , and   to arrive at @xmath956 we select @xmath957 , which results in the bound @xmath958 note that @xmath959 , as required , because we have assumed that @xmath960 .",
    "we obtain the result quoted in theorem  [ thm : car - bdd ] .",
    "in this section , we extend theorem  [ thm : car - bdd ] to the heavy - tailed matrix model , model  [ mod : p - mom - mtx ] . in section  [ sec : rsv - four - to - embed - univ ] , we show how to derive the second half of the universality result for the embedding dimension , theorem  [ thm : univ - embed ] , as a consequence .",
    "the following corollary extends theorem  [ thm : car - bdd ] to include random matrices drawn from model  [ mod : p - mom - mtx ] .",
    "[ cor : car - four ] fix parameters @xmath108 and @xmath109 .",
    "place the following assumptions :    * let @xmath196 and @xmath255 be natural numbers with @xmath928 .",
    "* let @xmath154 be a closed , convex cone in @xmath929 , and define @xmath930 .",
    "* draw an @xmath112 random matrix @xmath126 that satisfies model  [ mod : p - mom - mtx ] with given @xmath110 and @xmath125 .",
    "then the rap functional satisfies the probability bound @xmath961 the function @xmath551 is strictly positive for @xmath108 .",
    "the strictly positive constants @xmath962 and @xmath552 depend only on @xmath110 .",
    "the proof of corollary  [ cor : car - four ] follows from theorem  [ thm : car - bdd ] and the same kind of truncation argument that appears in section  [ sec : rsv - truncation ] .",
    "we omit further details .      theorem  [ thm : univ - embed ] is an easy consequence of corollary  [ cor : car - four ] .",
    "let us restate the assumptions of the theorem :    * @xmath36 is a compact subset of @xmath2 that does not contain the origin .",
    "* the statistical dimension of @xmath36 satisfies @xmath164 . *",
    "the @xmath34 random projector @xmath7 follows model  [ mod : p - mom - mtx ] with parameters @xmath108 and @xmath109 .",
    "we must now consider the regime where the embedding dimension @xmath963 .",
    "we need to demonstrate that @xmath964 as in section  [ sec : rsv - four - to - embed - univ ] , the probability is a decreasing function of the embedding dimension , so we may as well consider the case where @xmath965 .",
    "it is easy to see that @xmath966 because @xmath967 .",
    "introduce the spherical retraction @xmath169 .",
    "proposition  [ prop : annihilate ] and relation   show that @xmath968 therefore , to verify  , it is enough to produce a high - probability lower bound on the rap functional @xmath969 . with the choices @xmath970 and @xmath971 , corollary  [ cor : car - four ] yields @xmath972 we need to check that the lower bound on the rap functional is positive .",
    "that is , @xmath973 once again , this point follows from two relatively short calculations .",
    "since @xmath1 is a subset of the unit sphere , we quickly compute the excess width using  : @xmath974 the justifications are the same as in section  [ sec : rsv - four - to - embed - univ ] .",
    "we can easily bound the dimensional term in  : @xmath975 the first inequality holds because @xmath966 , and the last two relations both rely on the assumption @xmath976 . since @xmath977 is positive , we can find a number @xmath162 for which @xmath163 implies that @xmath978 combine the last result with   to see that the claim   holds true .",
    "in this section , we argue that functional @xmath943 does not change very much if we truncate the cone @xmath154 . replacing the unbounded set with a compact set allows us to develop discretization arguments .",
    "[ prop : car - cone - truncation ] adopt the notation and hypotheses of theorem  [ thm : car - bdd ] .",
    "let @xmath979 , where @xmath980 . then @xmath981    since @xmath982 , it is easy to see that @xmath983 meanwhile , the triangle inequality gives the bound @xmath984 it follows that @xmath985 since the norm of the random matrix @xmath126 concentrates , the bound   shows that the norm of the minimizer @xmath986 is unlikely to be large .    for any positive parameter @xmath477 ,",
    "the observation   allows us to calculate that @xmath987 \\\\      & = { \\operatorname{\\mathbb{e}}}\\left [ \\min_{{\\left\\vert { { { \\bm{t } } } } \\right\\vert } = 1 } \\min_{{{\\bm{s } } } \\in k^{\\circ}\\cap r \\mathsf{b}^m }      { { \\left\\vert { { { \\bm{s } } } - { { \\bm{\\phi}}}{{\\bm{t } } } } \\right\\vert}^2 } \\mathbb{1}\\ { { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert } \\leq r/2 \\}\\right ] \\\\      & = { \\operatorname{\\mathbb{e}}}\\min_{{\\left\\vert { { { \\bm{t } } } } \\right\\vert } = 1 } \\min_{{{\\bm{s } } } \\in k^{\\circ}\\cap r \\mathsf{b}^m }      { { \\left\\vert { { { \\bm{s } } } - { { \\bm{\\phi } } } { { \\bm{t } } } } \\right\\vert}^2 }      - { \\operatorname{\\mathbb{e}}}\\left [ \\max_{{\\left\\vert { { { \\bm{t } } } } \\right\\vert } = 1 } \\max_{{{\\bm{s } } } \\in k^{\\circ}\\cap r \\mathsf{b}^m }      { { \\left\\vert { { { \\bm{s } } } - { { \\bm{\\phi } } } { { \\bm{t } } } } \\right\\vert}^2 }      \\mathbb{1}\\ { { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert } > r/2 \\ } \\right ] .",
    "\\end{aligned}\\ ] ] to reach the last line , we write the indicator function in terms of its the complement .    to bound the second term on the right - hand side of  , crude estimates suffice .",
    "@xmath988      & \\leq { \\operatorname{\\mathbb{e}}}\\left [ ( r + { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert})^2 \\mathbb{1}\\ { { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert } > r/2 \\ } \\right ] \\\\       & \\leq 9 { \\operatorname{\\mathbb{e}}}\\left [ { { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert}^2 } \\mathbb{1}\\ { { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert } > r/2 \\ } \\right ] \\\\      & \\leq 9 \\left ( { \\operatorname{\\mathbb{e}}}{\\left\\vert { { { \\bm{\\phi } } } } \\right\\vert}^4 \\right)^{1/2 } \\big ( { { \\mathbb}{p}\\left\\ { { { \\left\\vert { { { \\bm{\\phi } } } } \\right\\vert } > r/2 } \\right\\ } } \\big)^{1/2}. \\end{aligned}\\ ] ] the last inequality is cauchy  schwarz . since @xmath126 satisfies the condition  , fact  [ fact : subgauss - mtx - tails ] implies that @xmath989 in particular , using integration by parts , @xmath990 furthermore , there is a constant @xmath74 for which @xmath991 if we set @xmath992 , then @xmath993      \\leq { \\mathrm{c } } b^4.\\ ] ] introduce the estimate   into   to complete the argument .",
    "in this section , we show that we can replace most of the entries of a random matrix @xmath126 with standard normal variables without changing the value of the functional @xmath994 substantially    [ prop : partial - replacement - redux ] let @xmath126 be an @xmath112 random matrix that satisfies model  [ mod : bdd - mtx ] with magnitude bound @xmath505 .",
    "fix the parameter @xmath995 .",
    "let @xmath79 be a subset of @xmath996 with cardinality @xmath211 , and let @xmath946 be a closed subset of @xmath997 for which @xmath998 let @xmath75 be a subset of @xmath541 with cardinality @xmath211 , and let @xmath512 be a closed subset of @xmath84 for which @xmath999 suppose that @xmath236 is an @xmath112 matrix with block form @xmath1000 then @xmath1001 as usual , @xmath35 is an @xmath112 standard normal matrix .",
    "fix the sets @xmath79 and @xmath75 .",
    "as in the proof of proposition  [ prop : partial - replacement ] , the error has three components : @xmath1002 the first error comes from discretizing the sets @xmath173 and @xmath138 at a level @xmath1003 $ ] .",
    "the second error appears when we replace the minima with a soft - min function with parameter @xmath651 .",
    "the last error emerges from the lindeberg exchange argument .    to complete the proof , we set @xmath659 to make the discretization error negligible .",
    "select the smoothing parameter so that @xmath1004 .",
    "we arrive at @xmath1005 since @xmath1006 and @xmath1007 , the second term dominates .",
    "we reach the stated result .",
    "the first step in the proof of proposition  [ prop : partial - replacement - redux ] is to replace the index sets by finite subsets .",
    "[ lem : discretization - redux ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement - redux ] . fix a parameter",
    "@xmath663 $ ] .",
    "then @xmath946 contains a finite subset @xmath1008 and @xmath512 contains a finite subset @xmath648 whose cardinalities satisfy @xmath1009 furthermore , these subsets have the property that @xmath1010 the bound   also holds if we replace @xmath126 by @xmath236 .",
    "we choose @xmath1008 to be an @xmath1011-covering of @xmath946 , and @xmath648 to be an @xmath272-covering of @xmath512 . since @xmath946 is a subset of @xmath997 and @xmath512",
    "is a subset of @xmath84 , we can be sure that the coverings have cardinality @xmath1012 and @xmath667 .",
    "see  ( * ? ? ?",
    "the rest of the proof is essentially the same as that of lemma  [ lem : discretization ] , so we omit the details .      the next step in the proof of proposition  [ prop : partial - replacement - redux ] is to pass from the minimum to the soft - min function .",
    "[ lem : smoothing - redux ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement - redux ] , and let @xmath1008 and @xmath648 be the sets introduced in lemma  [ lem : discretization - redux ] .",
    "fix a parameter @xmath651 , and introduce the function @xmath1013 then @xmath1014 the estimate   also holds if we replace @xmath126 by @xmath236 .",
    "the proof is almost identical with that of lemma  [ lem : smoothing ] .",
    "the main challenge in the proof of proposition  [ prop : partial - replacement - redux ] is to exchange most of the entries of the random matrix @xmath126 for the entries of @xmath236 .",
    "[ lem : exchange - redux ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement - redux ] , and let @xmath280 be the function defined in lemma  [ lem : smoothing - redux ] .",
    "then @xmath1015    the proof is similar with lemma  [ lem : exchange ] .",
    "this time , we replace only the rows of @xmath126 listed in @xmath1016 .",
    "we incur the same error for each of these @xmath1017 rows , so it suffices to control the error in exchanging a single row .",
    "the following sublemma achieves this goal .",
    "[ slem : compare - one - redux ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement - redux ] , and let @xmath1008 and @xmath648 be the sets defined in lemma  [ lem : smoothing - redux ] . for @xmath1018 , introduce the function @xmath1019 where @xmath1020 is an arbitrary function .",
    "suppose that @xmath705 is a random vector with independent , standardized entries that are bounded in magnitude by @xmath505 .",
    "suppose that @xmath706 is a random vector with @xmath707 where @xmath708 is a standard normal vector .",
    "then @xmath1021    the proof of this result is much the same as the proof of sublemma  [ slem : compare - one ] .",
    "there are only two points that require care .",
    "first , we use a slightly different result to compute the derivatives .",
    "[ slem : derivatives - redux ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement - redux ] and sublemma  [ slem : compare - one - redux ] .",
    "let @xmath748 be a linear function , so its derivative @xmath749 is a constant vector . for @xmath1018 , define the function @xmath1022 where @xmath1020 is arbitrary .",
    "the third derivative of this function satisfies @xmath1023 where @xmath725 is a random vector that does not depend on @xmath752 .",
    "second , when making further bounds on @xmath1024 , we need to exploit our control on the magnitude of @xmath90 on the coordinates in @xmath1016 .",
    "note that @xmath1025 the second inequality holds because @xmath1026 for each @xmath1018 .",
    "similarly , @xmath1027 repeating the arguments from sublemma  [ slem : compare - one ] , we obtain bounds of the form @xmath1028      \\leq \\frac{\\beta b^3 r}{k^{2 } } + \\frac{{\\mathrm{c } } \\beta^2 b^3 r^3}{k^{3 } }      + \\frac { { \\mathrm{c } } \\beta b^4 + { \\mathrm{c } } \\beta^2 b^6 } { k^{3/2 } } .\\ ] ] the first two terms dominate the third because our choice @xmath1007 implies that @xmath1029 .",
    "we arrive at the statement of sublemma  [ slem : compare - one - redux ] .",
    "the most difficult part of proving theorem  [ thm : car - bdd ] is to identify the excess width @xmath1030 after we replace the original matrix @xmath126 by the hybrid matrix @xmath236 defined in  .",
    "the following result does the job .",
    "[ prop : car - width ] adopt the notation and hypotheses of proposition  [ prop : partial - replacement - redux ] .",
    "let @xmath930 .",
    "then @xmath1031 the random matrix @xmath236 is defined in  .",
    "the proof of proposition  [ prop : car - width ] occupies the rest of this section . at the highest level ,",
    "the proof is similar with the argument underlying proposition  [ prop : excess - width ] .",
    "we write the quantity of interest as a minimax , and then we apply the gaussian minimax theorem to replace the gaussian matrix with a pair of gaussian vectors .",
    "afterward , we analyze the resulting expression to identify the gaussian width ; the new challenges appear in this step .      here is an overview of the calculations that we will perform ; the detailed justifications appear in the upcoming subsections .",
    "let us abbreviate @xmath1032 .",
    "we have the chain of inequalities @xmath1033 lemma  [ lem : car - duality ] is a standard convex duality argument , and the next line follows when we write out the quantity of interest more explicitly . to reach the fourth line , we apply the gaussian minimax theorem in the usual way to replace the random matrix @xmath35 with two standard normal vectors @xmath955 and @xmath1034 . in a rough sense ,",
    "the remaining part of the random matrix @xmath126 is negligible .",
    "the term @xmath1035 generates the gaussian width @xmath1036 , defined in  , while the term @xmath1037 contributes a dimensional factor @xmath1038 .",
    "apply the increasing convex function @xmath533 to the inequality in the last display , and invoke jensen s inequality to draw out the expectation .",
    "notice that @xmath1039 finally , @xmath1040 because of  .",
    "this point completes the proof .",
    "the first step in the argument is to apply the minimax inequality to pass to a saddle - point formulation that is amenable to analysis with the gaussian minimax theorem .",
    "[ lem : car - duality ] adopt the notation and hypotheses of proposition  [ prop : car - width ] .",
    "for any point @xmath1041 , @xmath1042 where @xmath1032 .    write the norm as maximum : @xmath1043 the minimax inequality allows us to interchange the maximum and minimum : @xmath1044 the value of @xmath1045 equals zero when @xmath1046 ; otherwise , it takes the value @xmath1047 .",
    "this step uses the assumption that @xmath154 is closed and convex .",
    "we conclude that @xmath1048 this is the stated result .      by an argument",
    "similar with the proof of lemma  [ lem : apply - minmax - lower ] , we can replace the gaussian block of @xmath236 with two gaussian vectors .",
    "[ lem : apply - minmax - lower - redux ] adopt the notation and hypotheses of proposition  [ prop : car - width ] . then @xmath1049 where @xmath955 and @xmath1034 be independent standard normal vectors",
    ".    there are no new ideas in this bound , so we refer the reader to lemmas  [ lem : my - comparison ] and  [ lem : apply - minmax - lower ] for the pattern of argument .",
    "to prove proposition  [ prop : car - width ] , most of the difficulty arises when we seek a good lower bound for the minimax problem that appears in lemma  [ lem : apply - minmax - lower - redux ] .",
    "we have the following result .",
    "[ lem : car - width - id ] adopt the notation and hypotheses of proposition  [ prop : car - width ] .",
    "define the set @xmath1050 . then @xmath1051    the proof of this bound is lengthy , so we break the argument into several steps .",
    "the overall result follows when we sequence the inequalities in sublemmas  [ slem : car - minmax - simple ] , [ slem : car - minmax - simpler ] , [ slem : prob - bounds ] , and  [ slem : car - width - minmax ] and consolidate the error terms .",
    "the first step in the proof of lemma  [ lem : car - width - id ] is to simplify the minimax so we can identify the key terms .",
    "[ slem : car - minmax - simple ] adopt the notation and hypotheses of lemma  [ lem : car - width - id ] .",
    "then @xmath1052    let us introduce notation for the quantity of interest : @xmath1053 we can introduce the positive - part operator because the fact that @xmath1054 ensures that the minimax is nonnegative .",
    "the first step in the argument is to reintroduce the missing piece of the random vector @xmath796 . adding and subtracting the quantity @xmath1055 inside the positive - part operator in  , we obtain the bound @xmath1056 the second inequality holds because @xmath1057 and @xmath1058 is a subset of the unit ball .",
    "this step is similar with the proof of lemma  [ lem : missing - coords - lower ] .",
    "next , we combine the terms in   involving @xmath1059 and the _ row _ vector @xmath1060 . since @xmath1061 , @xmath1062 the @xmath1063 random matrix on the right - hand side has independent , standardized entries that satisfy the subgaussian estimate   with bound @xmath505 . repeating the calculations in",
    ", we see that @xmath1064 apply the estimate   inside the minimax in   and then use   to arrive at the lower bound @xmath1065 in the second line , we have simply consolidated terms .      the next step in the proof of lemma  [ lem : car - width - id ]",
    "is to reduce the minimax problem in sublemma  [ slem : car - minmax - simple ] to a scalar optimization problem .",
    "[ slem : car - minmax - simpler ] adopt the notation and hypotheses of lemma  [ lem : car - width - id ] .",
    "then @xmath1066 } \\max \\big\\ { 0 ,       \\big ( \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot { { \\bm{g } } } - \\sqrt{n } \\big ) \\alpha ,      \\min_{{\\left\\vert { { { \\bm{s } } } } \\right\\vert } = 1 } \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot      \\begin{bmatrix } { { \\bm{\\phi}}}_{j } & { { \\bm{g } } } \\end{bmatrix } { { \\bm{s } } }      - \\sqrt{n } \\alpha \\big\\ }      - \\sqrt{k}.\\end{gathered}\\ ] ]    introduce the notation @xmath1067 we will develop two lower bounds on the maximum by coupling @xmath474 to the random matrix in different ways .",
    "afterward , we combine these results into a single bound .    in the first place , we can choose the _ row _ vector @xmath474 so that it depends only on the remaining gaussian vector : @xmath1068 since @xmath1069 , we obtain the bound @xmath1070 the second term on the right - hand side of   satisfies @xmath1071 indeed , @xmath1072 is a random vector with @xmath1073 that is stochastically independent from @xmath1074 , and the @xmath1075 random matrix @xmath1074 has independent , standardized entries .",
    "the second bound is even simpler . since @xmath1069 , @xmath1076 in this expression , the variable @xmath1077 .    introducing   and   into",
    ", we arrive at @xmath1078 } \\max \\big\\ { 0 ,       \\big ( \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot { { \\bm{g } } } - \\sqrt{n } \\big ) \\alpha ,      \\min_{{\\left\\vert { { { \\bm{s } } } } \\right\\vert } = 1 } \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot      \\begin{bmatrix } { { \\bm{\\phi}}}_{j } & { { \\bm{g } } } \\end{bmatrix } { { \\bm{s } } }      - \\sqrt{n } \\alpha \\big\\ }      - \\sqrt{k}. \\end{aligned}\\ ] ] the zero branch in the maximum accounts for the positive - part operator in  .",
    "the second line follows from  .",
    "we have also introduced a new parameter @xmath122 to stand in for @xmath1079 .      the last major step in lemma  [ lem : car - width - id ]",
    "is to develop probabilistic bounds for the terms that arise in sublemma  [ slem : car - minmax - simpler ] .",
    "[ slem : prob - bounds ] adopt the notation and hypotheses of lemma  [ lem : car - width - id ] .",
    "then @xmath1080 } \\max \\big\\ { 0 ,       \\big ( \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot { { \\bm{g } } } - \\sqrt{n } \\big ) \\alpha ,      \\min_{{\\left\\vert { { { \\bm{s } } } } \\right\\vert } = 1 } \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot      \\begin{bmatrix } { { \\bm{\\phi}}}_{j } & { { \\bm{g } } } \\end{bmatrix } { { \\bm{s } } }      - \\sqrt{n } \\alpha \\big\\ } \\\\",
    "\\geq \\frac{1}{2 } \\min_{\\alpha \\in [ 0,1 ] } \\max\\left\\ { ( { \\mathscr{w}}(u ) - \\sqrt{n } - 2 ) \\alpha ,      \\frac{{\\mathscr{w}}(u ) - 2}{2b\\sqrt{\\log m } }      - \\sqrt{n } \\alpha \\right\\ }      - { \\mathrm{c } } b^2 \\sqrt{k \\log m}.\\end{gathered}\\ ] ]    introduce the notation @xmath1081 } \\max \\big\\ { 0 ,       \\big ( \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot { { \\bm{g } } } - \\sqrt{n } \\big ) \\alpha ,      \\min_{{\\left\\vert { { { \\bm{s } } } } \\right\\vert } = 1 } \\max_{{{\\bm{u } } } \\in u } { { \\bm{u } } } \\cdot      \\begin{bmatrix } { { \\bm{\\phi}}}_{j } & { { \\bm{g } } } \\end{bmatrix } { { \\bm{s } } }      - \\sqrt{n } \\alpha \\big\\}.\\ ] ] we assume that @xmath1082 , which is permitted because the final result would otherwise become vacuous .",
    "the next stage in the argument is to introduce probabilistic bounds for the branches of the maximum and use these to control the expectation .",
    "it is convenient to abbreviate @xmath1083 note that @xmath1084 . since @xmath1085 is 1-lipschitz , the gaussian concentration inequality , fact  [ fact : gauss - concentration ] , implies that @xmath1086 on the other hand , sublemma  [ slem : empirical - width ] will demonstrate that @xmath1087 therefore , taking complements and a union bound , @xmath1088 for each nonnegative random variable @xmath120 and each number @xmath1089 , it holds that @xmath1090 . using the estimate   and",
    "the probability bound  , we find that @xmath1091 } \\max\\left\\ { 0 , ( { \\mathscr{w}}(u ) - \\sqrt{n } - 2 ) \\alpha ,      \\left ( \\frac{{\\mathscr{w}}(u)}{{\\mathrm{c } } b\\sqrt{\\log m } } - { \\mathrm{c}}b\\sqrt{k \\log m } \\right )      - \\sqrt{n } \\alpha \\right\\ }      - \\sqrt{k } \\\\      & \\geq \\frac{1}{2 } \\min_{\\alpha \\in [ 0,1 ] } \\max\\left\\ { ( { \\mathscr{w}}(u ) - \\sqrt{n } - 2 ) \\alpha ,      \\frac{{\\mathscr{w}}(u ) - 2}{{\\mathrm{c } } b\\sqrt{\\log m } }      - \\sqrt{n } \\alpha \\right\\ }      - { \\mathrm{c } } b^2 \\sqrt{k \\log m}. \\end{aligned}\\ ] ] once again , we have used shift - invariance of the maximum to combine the error terms . for convenience ,",
    "we have also dropped the zero branch of the maximum and introduced the number two into the numerator in the second branch .",
    "the final step in the proof of lemma  [ lem : car - width - id ] is to solve the scalar minimax problem that emerges in sublemma  [ slem : prob - bounds ] .",
    "[ slem : car - width - minmax ] adopt the notation and hypotheses of lemma  [ lem : car - width - id ] . then @xmath1092 } \\max\\left\\ { ( { \\mathscr{w}}(u ) - \\sqrt{n } - 2 ) \\alpha ,      \\frac{{\\mathscr{w}}(u ) - 2}{{\\mathrm{c } } b\\sqrt{\\log m } }      - \\sqrt{n } \\alpha \\right\\ }      \\geq \\frac{{\\mathscr{w}}(u ) - \\sqrt{n } - 2}{{\\mathrm{c } } b \\sqrt{\\log m}}.\\ ] ]    the first branch of the maximum is increasing in @xmath122 while the second branch is decreasing in @xmath122 , so the minimum occurs when the two branches are equal , provided that this situation occurs when @xmath1093 $ ] .",
    "setting the branches equal , we identify the point @xmath1094 where the saddle value is achieved .",
    "@xmath1095 we quickly verify that @xmath1096 $ ] , so the minimax takes the value @xmath1097 this is the required estimate .      in this section ,",
    "we explain how to obtain a lower bound for the minimax problem in   in terms of the gaussian width of the set @xmath1058 .",
    "[ slem : matrix - width ] assume that @xmath1098 , and let @xmath320 be an @xmath1075 random matrix that satisfies model  [ mod : bdd - mtx ] with bound @xmath505 .",
    "let @xmath955 be standard normal .",
    "let @xmath1058 be a subset of the unit ball in @xmath929 .",
    "then @xmath1099    fix @xmath1100 .",
    "let @xmath1101 be an @xmath272-net for the unit sphere in @xmath1102 .",
    "the cardinality of the net satisfies @xmath1103 by the standard volumetric argument  ( * ? ? ?",
    "5.2 )    we can estimate the quantity of interest below by discretizing the parameter @xmath90 .",
    "since @xmath1101 and @xmath1058 are subsets of the unit ball , we have the bound @xmath1104 we will establish a probabilistic lower bound for the right - hand side of  .",
    "first , we develop a probability bound for the second term on the right - hand side of  .",
    "a simple spectral norm estimate suffices .",
    "the @xmath1075 random matrix @xmath320 has standardized entries and @xmath955 is standard normal , so @xmath1105 as usual , @xmath672 denotes the frobenius norm .",
    "markov s inequality now implies that @xmath1106 it follows that @xmath1107 we have used the facts that @xmath1100 and @xmath1098 .",
    "let us turn to the second quantity on the right - hand side of  .",
    "we develop a strong probability bound for each fixed point @xmath1108 , and we extend it to the entire net using the union bound . for technical reasons ,",
    "it is easier to treat the random matrix @xmath320 and the random vector @xmath796 separately .",
    "fix a point @xmath1108 , and decompose it as @xmath1109 where @xmath1110 .",
    "construct a random vector @xmath1111 that satisfies @xmath1112 we may calculate that @xmath1113 the last estimate relies on the fact that @xmath1114 because @xmath1115 .",
    "the second term on the right - hand side of   is easy to handle using the gaussian concentration inequality , fact  [ fact : gauss - concentration ] : @xmath1116 indeed , @xmath1117 is a 1-lipschitz function with mean zero because the random vector @xmath1118 is stochastically independent from @xmath796 and has norm bounded by one .",
    "we can interpret the first term on the right - hand side of   as an `` empirical width . ''",
    "it takes a substantial amount of work to compare this quantity with the gaussian width .",
    "sublemma  [ slem : empirical - width ] contains a bound for the expectation , and sublemma  [ slem : empirical - width - conc ] contains a tail bound .",
    "together , they deliver the probability inequality @xmath1119 in other words , the empirical width of @xmath1058 is comparable with the gaussian width , modulo a logarithmic factor .",
    "introduce the two probability bounds   and   into the deterministic estimate  .",
    "we arrive at @xmath1120 finally , we take a union bound over @xmath1108 to obtain an estimate that is uniform over the net . recall that @xmath1121 and select @xmath1122 to reach @xmath1123 the numerical estimate holds because @xmath1124 .    the two probability bounds   and   hold simultaneous with probability at least @xmath1125 .",
    "therefore , we can substitute these results into   and adjust constants to obtain the stated bound .",
    "the next sublemma demonstrates that the gaussian width of a set is not more than a logarithmic factor larger than the empirical width of the set as computed with bounded random variables .",
    "this is the only step in the argument for bounded random matrices that requires the symmetry assumption .",
    "[ slem : empirical - width ] adopt the notation and hypotheses of sublemma  [ slem : matrix - width ] , and let @xmath90 be a fixed unit - norm vector .",
    "then @xmath1126    define the random vector @xmath1127 .",
    "our goal is to compare the empirical width of the set @xmath1058 computed using the vector @xmath727 with the gaussian width of the set .",
    "first , we develop a lower bound on the first moment of the entries of @xmath727 .",
    "fix an index @xmath599 . since the entries of @xmath320 are independent and symmetric @xmath1128 where @xmath1129 is an independent family of rademacher random variables , independent from @xmath320 .",
    "the khintchine inequality  @xcite allows us to compare the first moment with the second moment : @xmath1130 since @xmath90 has unit norm , we can regard the sum as a weighted average , and we can invoke jensen s inequality to draw the average out of the square root : @xmath1131 last , note that @xmath1132 because the entries of @xmath320 are standardized and bounded by @xmath505 . thus , @xmath1133    let us bound the width - like functional below .",
    "since @xmath727 has independent , symmetric coordinates , @xmath1134 where , again , @xmath1135 is an independent family of rademacher random variables , independent from @xmath474 . using a corollary of the contraction principle  (",
    "4.5 ) , @xmath1136 applying the contraction principle again  ( * ? ? ?",
    "( 4.9 ) ) , we can randomize the sum with independent gaussian variables : @xmath1137 here , @xmath1138 is a standard normal vector .",
    "identify the gaussian width to complete the proof .",
    "last , we present a concentration inequality for the bounded width .",
    "[ slem : empirical - width - conc ] adopt the notation and hypotheses of sublemma  [ slem : matrix - width ] .",
    "let @xmath90 be a fixed unit - norm vector . then @xmath1139    to prove this result , we need a lower tail inequality for functions that have a lipschitz - like property , which are sometimes called regular functions .",
    "see  ( * ? ? ?",
    "6.23 ) or  ( * ? ? ?",
    "[ fact : regular - fn - lower ] let @xmath585 be an independent sequence of real random variables . for a function @xmath587 , define @xmath1140 suppose that @xmath1141      \\leq a      \\quad\\text{where $ a \\geq 0$.}\\ ] ] then @xmath1142    introduce the random variable @xmath1143 for each index pair @xmath82 , introduce the parameterized random matrix @xmath1144 by replacing the @xmath82 entry of @xmath320 by the real number @xmath122 .",
    "define @xmath1145 select a point @xmath1146 . then @xmath1147 we have used the assumption that the entries of @xmath320 are bounded in magnitude by @xmath505 . therefore , @xmath1148 the last relation depends on the assumption that @xmath91 and @xmath90 have norms bounded by one .",
    "invoke fact  [ fact : regular - fn - lower ] to complete the proof .",
    "[ part : back - matter ]    two appendices contain statements of some results that we use throughout the paper .",
    "appendix  [ app : gauss ] presents some facts about gaussian analysis , while appendix  [ app : norm ] describes some spectral bounds for random matrices with independent entries .",
    "we conclude with acknowledgments and a list of works cited .",
    "we make extensive use of methods from gaussian analysis to provide precise information about the behavior of functions of gaussian random variables .",
    "these results come up in many places in the paper , so we have collected them here .",
    "we begin with two concentration results that apply to a lipschitz function of independent gaussian variables .",
    "recall that a function @xmath274 has lipschitz constant @xmath210 when @xmath1149 we also say , more briefly , that @xmath51 is @xmath210-lipschitz . the first result  ( * ? ? ?",
    "1.6.4 ) gives a bound on the variance of a lipschitz function .",
    "the second result  ( * ? ? ?",
    "1.7.6 ) provides a normal concentration inequality for lipschitz functions .",
    "[ fact : gauss - variance ] suppose that @xmath274 has lipschitz constant @xmath210 .",
    "let @xmath708 be a standard normal random vector .",
    "then @xmath1150 \\leq l.\\ ] ] equivalently , @xmath1151    [ fact : gauss - concentration ] suppose that @xmath274 has lipschitz constant @xmath210 .",
    "let @xmath708 be a standard normal random vector . then",
    ", for all @xmath508 , @xmath1152      to compute the expectations of certain functions of gaussian random variables , we depend on a comparison principle due to gordon  ( * ? ? ?",
    "let @xmath173 be an abstract set .",
    "a family @xmath1153 of real random variables is called a centered gaussian process when each element @xmath1154 has mean zero and each finite subcollection @xmath1155 has a jointly gaussian distribution .",
    "[ fact : gauss - minmax ] let @xmath138 and @xmath1058 be finite sets . consider two centered gaussian processes @xmath1156 and @xmath1157 , indexed over @xmath1158 . for all choices of indices ,",
    "suppose that @xmath1159 then , for all real numbers @xmath1160 and @xmath1161 , @xmath1162    fact  [ fact : gauss - minmax ] extends to infinite index sets @xmath1163 by approximation .",
    "our argument also depends heavily on some non - asymptotic bounds for the spectrum of a random matrix with independent entries .",
    "these results only give rough estimates , but they are adequate for our purposes .",
    "the first result gives tail bounds for the extreme singular values of a rectangular matrix with independent , subgaussian entries .",
    "[ fact : subgauss - mtx - tails ] let @xmath320 be an @xmath1164 random matrix with independent , standardized entries that are uniformly subgaussian : @xmath1165 then the largest singular value @xmath1166 and the @xmath1167-th largest singular value @xmath1168 satisfy the bounds @xmath1169    this result follows from  ( * ? ? ?",
    "* thm .  5.39 ) when we track the role of the subgaussian constant through the proof .",
    "the second result gives a tail bound for the norm of a matrix with independent entries that may only have two moments ; it is based on the matrix rosenthal inequality  ( * ? ? ?",
    "1.1 ) and a standard concentration inequality  ( * ? ? ?",
    "15.5 ) .",
    "[ fact : heavy - tail - norm ] fix a parameter @xmath1170 $ ] .",
    "let @xmath320 be a @xmath1164 random matrix with independent entries that have the following properties .",
    "* the entries are centered : @xmath1171 . * the variances of the entries",
    "are uniformly bounded : @xmath1172 . *",
    "the entries have uniformly bounded @xmath110th moments : @xmath1173 .",
    "then @xmath1174    write the random matrix as a sum of independent random matrices : @xmath1175 where @xmath1176 is the @xmath1164 matrix with a one in the @xmath82 position and zeros elsewhere",
    ". a straightforward application of the matrix rosenthal inequality  ( * ? ? ?",
    "i ) yields @xmath1177 the second line follows when we replace the maximum by a sum and exploit the uniform moment estimate .",
    "the third line is just the inequality between the geometric and arithmetic means .    a standard concentration inequality for moments  ( * ?",
    "15.5 ) gives @xmath1178^{1/p }      \\leq { \\mathrm{c } } \\sqrt{p } \\big ( { \\operatorname{\\mathbb{e}}}v_+^{p/2 } \\big)^{1/p}\\ ] ] in this expression , the variance parameter @xmath1179.\\ ] ] the @xmath82 entry of @xmath1180 is an independent copy of the corresponding entry of @xmath320 ; the remaining entries of the two matrices are the same . applying the usual method  (",
    "* ex .  3.14 )",
    ", we see that @xmath1181.\\ ] ] applying the same considerations as in the last paragraph , we obtain @xmath1182 combine these results and apply markov s inequality to obtain the tail bound @xmath1183 introduce the estimate for the expected norm to complete the argument .",
    "the authors would like to thank david donoho , surya ganguli , babak hassibi , michael mccoy , andrea montanari , ivan nourdin , giovanni peccati , adrian rllin , jared tanner , christos thrampoulidis , and madeleine udell for helpful conversations .",
    "so was generously supported by the simons institute for the theory of computing and nsf award ccf-1217058 .",
    "jat gratefully acknowledges support from onr award n00014 - 11 - 1002 and the gordon & betty moore foundation .",
    "e.  cands , m.  rudelson , t.  tao , and r.  vershynin .",
    "error correction via linear programming . in _ foundations of computer science , 2005 .",
    "focs 2005 .",
    "46th annual ieee symposium on _ , pages 668681 , oct 2005 .",
    "k.  l. clarkson and d.  p. woodruff .",
    "low rank approximation and regression in input sparsity time . in _",
    "stoc13proceedings of the 2013 acm symposium on theory of computing _ , pages 8190 .",
    "acm , new york , 2013 .",
    "m.  a. davenport , m.  f. duarte , m.  b. wakin , j.  n. laska , d.  takhar , k.  f. kelly , and r.  g. baraniuk .",
    "the smashed filter for compressive classification and target recognition . in _ proc .",
    "volume 6498 , pages 64980h64980h12 , 2007 .",
    "d.  donoho and j.  tanner . observed universality of phase transitions in high - dimensional geometry , with implications for modern data analysis and signal processing .",
    ", 367(1906):42734293 , 2009 . with electronic supplementary materials available",
    "online .                      y.  gordon . on milman s inequality and random subspaces which escape through a mesh in @xmath1186 . in _ geometric aspects of functional analysis ( 1986/87 )",
    "_ , volume 1317 of _ lecture notes in math .",
    "_ , pages 84106 .",
    "springer , berlin , 1988 .",
    "s.  mendelson .",
    "a remark on the diameter of random sections of convex bodies . in b.",
    "klartag and e.  milman , editors , _ geometric aspects of functional analysis _ , volume 2116 of _ lnm _ , pages 395404 .",
    "springer , 2014 .",
    "j.  nelson and h.  l. nguyen .",
    "osnap : faster numerical linear algebra algorithms via sparser subspace embeddings . in",
    "_ 2013 ieee 54th annual symposium on foundations of computer science ",
    "focs 2013 _ , pages 117126 .",
    "ieee computer soc . , los alamitos , ca , 2013 .",
    "s.  oymak , c.  thrampoulidis , and b.  hassibi .",
    "the squared - error of generalized lasso : a precise analysis . in _ communication , control , and computing ( allerton ) , 2013 51st annual allerton conference on _ , pages 10021009 , oct 2013 .",
    "available at http://arxiv.org/abs/1311.0830 .                              c.  thrampoulidis and b.  hassibi .",
    "isotropically random orthogonal matrices : performance of lasso and minimum conic singular values . in _ information theory ( isit ) ,",
    "2015 ieee international symposium on _ , pages 556560 , june 2015 .",
    "available at http://arxiv.org/abs/1503.07236 .        c.  thrampoulidis , s.  oymak , and b.  hassibi .",
    "the gaussian min ",
    "max theorem in the presence of convexity . in _ proceedings of the 28th conference on learning theory ( colt )",
    "available at http://arxiv.org/abs/1408.4837 .",
    "j.  a. tropp .",
    "code for reproducing figures from oymak & tropp , _ universality laws for randomized dimension reduction , with applications _",
    ", 2015.ropp , _ universality laws for randomized dimension reduction , with applications _ , 2015 .",
    "available at http://users.cms.caltech.edu/~jtropp , nov . 2015 .",
    "j.  a. tropp .",
    "convex recovery of a structured signal from independent random measurements . in g.",
    "pfander , editor , _ sampling theory : a rennaissance_. birkhuser verlag , 2015 .",
    "available at http://arxiv.org/abs/1405.1102 .",
    "j.  a. tropp .",
    "the expected norm of a sum of independent random matrices : an elementary approach . in _ high - dimensional probability vii _ , cargse , june 2015 . to appear .",
    "available at http://arxiv.org/abs/1506.04711 ."
  ],
  "abstract_text": [
    "<S> dimension reduction is the process of embedding high - dimensional data into a lower dimensional space to facilitate its analysis . in the euclidean setting , one fundamental technique for dimension reduction is to apply a random linear map to the data . </S>",
    "<S> this dimension reduction procedure succeeds when it preserves certain geometric features of the set . </S>",
    "<S> the question is how large the embedding dimension must be to ensure that randomized dimension reduction succeeds with high probability .    </S>",
    "<S> this paper studies a natural family of randomized dimension reduction maps and a large class of data sets . </S>",
    "<S> it proves that there is a phase transition in the success probability of the dimension reduction map as the embedding dimension increases . for a given data set , </S>",
    "<S> the location of the phase transition is the same for all maps in this family . </S>",
    "<S> furthermore , each map has the same stability properties , as quantified through the restricted minimum singular value . </S>",
    "<S> these results can be viewed as new universality laws in high - dimensional stochastic geometry .    </S>",
    "<S> universality laws for randomized dimension reduction have many applications in applied mathematics , signal processing , and statistics . </S>",
    "<S> they yield design principles for numerical linear algebra algorithms , for compressed sensing measurement ensembles , and for random linear codes . </S>",
    "<S> furthermore , these results have implications for the performance of statistical estimation methods under a large class of random experimental designs . </S>"
  ]
}