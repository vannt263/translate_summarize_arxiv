{
  "article_text": [
    "low - rank tensors span a wide variety of applications , like tensor compression @xcite , robust pca @xcite , completion @xcite and parameter approximation in cnns @xcite .",
    "however , the current literature lacks development in two main facets : 1 ) large scale processing and 2 ) generalization to non - euclidean domain ( graphs @xcite ) .",
    "for example , for a tensor @xmath0 , tensor robust pca via nuclear norm minimization @xcite costs @xmath1 per iteration which is unacceptable even for @xmath2 as small as 100 .",
    "many application specific alternatives , such as randomization , sketching and fast optimization methods have been proposed to speed up computations @xcite , but they can not be generalized for a broad range of applications .    in this work , we answer the following question : _ is it possible to 1 ) generalize the notion of tensors to graphs and 2 ) target above applications in a scalable manner ? _ to the best of our knowledge , little effort has been made to target the former @xcite , @xcite , @xcite at the price of higher cost , however , no work has been done to tackle the two problems simultaneously .",
    "therefore , we revisit tensors from a new perspective and develop an entirely novel , scalable and approximate framework which benefits from _ graphs_.    it has recently been shown for the case of 1d @xcite and time varying signals @xcite that the first few eigenvectors of the graph provide a smooth basis for data , the notion of graph stationarity .",
    "we generalize this concept for higher order tensors and develop a framework that encodes the tensors as a multilinear combination of few graph eigenvectors constructed from the rows of its different modes ( figure on the top of this page ) .",
    "this multilinear combination , which we call _ graph core tensor ( gct ) _ , is highly structured like the core tensor obtained by multilinear svd ( mlsvd ) @xcite and can be used to solve a plethora of tensor related inverse problems in a highly scalable manner .",
    "* contributions : * in this paper we propose multilinear low - rank tensors on graphs ( mlrtg ) as a novel signal model for low - rank tensors . using this signal model",
    ", we develop an entirely novel , scalable and approximate framework for a variety of inverse problems involving tensors , such as multilinear svd and robust tensor pca .",
    "most importantly , we theoretically link the concept to joint approximate graph stationarity and characterize the approximation error in terms of the eigen gaps of the graphs .",
    "various experiments on a wide variety of 4 artificial and 12 real benchmark datasets such as videos , face images and hyperspectral images using our algorithms demonstrate the power of our approach .",
    "the mlrtg framework is highly scalable , for example , for a tensor @xmath0 , robust tensor pca on graphs scales with @xmath3 , where @xmath4 , as opposed to @xmath1 .    *",
    "notation : * we represent tensors with bold calligraphic letters @xmath5 and matrices with capital letters @xmath6 . for a tensor @xmath7 , with a multilinear rank",
    "@xmath8 @xcite , its @xmath9 matricization / flattening @xmath10 is a re - arrangement such that @xmath11 . for simplicity we work with 3d tensors of same size @xmath2 and",
    "rank @xmath12 in each dimension .",
    "* graphs : * we specifically refer to a @xmath13-nearest neighbors graph between the rows of @xmath10 as @xmath14 with vertex set @xmath15 , edge set @xmath16 and weight matrix @xmath17 .",
    "@xmath17 , as defined in @xcite , is constructed via a gaussian kernel and the combinatorial laplacian is given as @xmath18 , where @xmath19 is the degree matrix .",
    "the eigenvalue of decomposition of @xmath20 and we refer to the 1st @xmath21 eigenvectors and eigenvalues as @xmath22 . throughout ,",
    "we use flann @xcite for the construction of @xmath23 which costs @xmath24 and is parallelizable .",
    "we also assume that a fast and parallelizable framework , such as the one proposed in @xcite or @xcite is available for the computation of @xmath25 which costs @xmath26 , where @xmath27 is the number of processors .",
    "a tensor @xmath28 is said to be multilinear low - rank on graphs ( mlrtg ) if it can be encoded in terms of the lowest @xmath4 laplacian eigenvectors as : @xmath29 where @xmath30 denotes the vectorization , @xmath31 denotes the kronecker product , @xmath32 and @xmath33 is the _ graph core tensor ( gct)_. we refer to a tensor from the set of all possible mlrtg as @xmath34 .",
    "the main idea is illustrated in the figure on the first page of this paper .",
    "we call the tuple @xmath35 , where @xmath36 , as the _ graph multilinear rank _ of @xmath37 . in the sequel , for the simplicity of notation : 1 ) we work with the matricized version ( along mode 1 ) of @xmath37 and 2 ) denote @xmath38 . then for @xmath39 , @xmath40 .    * in simple words : * one can encode a low - rank tensor in terms of the low - frequency laplacian eigenvectors .",
    "this multilinear combination is called gct .",
    "it is highly structured like the core tensor obtained by standard multilinear svd ( mlsvd ) and can be used for a broad range of tensor based applications .",
    "furthermore , the fact that gct encodes the interaction between the graph eigenvectors , renders its interpretation as a multi - dimensional graph fourier transform .    in real applications , due to noise",
    "the tensor @xmath5 is only approximately low - rank ( approximate mlrtg ) , so the following lemma holds :    [ lem : amlrtg ] for any @xmath41 , where @xmath34 and @xmath42 models the noise and errors , the @xmath9 matricization @xmath10 of @xmath5 satisfies @xmath43 where @xmath44 and @xmath45 denote the complement laplacian eigenvectors ( above @xmath21 ) and @xmath46 .",
    "furthermore , @xmath47",
    ".    please refer to the proof of theorem [ thm : props ] in appendix [ sec : proof_props ] .",
    "matrix.,scaledwidth=45.0% ]        let @xmath48 be the covariance of @xmath10 , then the _ graph spectral covariance ( gsc ) _",
    "@xmath49 is given as @xmath50 .",
    "for a signal that is approximately stationary on a graph , @xmath49 has most of its energy concentrated on the diagonal @xcite . for mlrtg , we additionally require the energy to be concentrated on the top corner of @xmath49 , which we call _ low - frequency energy concentration_.    * key properties of mlrtg : * thus , for any @xmath51 , the gsc @xmath49 of each of its @xmath9 matricization @xmath10 satisfies : 1 ) joint approximate graph stationarity , i.e , @xmath52 and 2 ) low frequency energy concentration , i.e , @xmath53 , @xmath54 .",
    "[ thm : props ] for any @xmath55 , @xmath34 if and only lemma [ lem : amlrtg ] and property 2 hold",
    ".    please refer to appendix [ sec : proof_props ] .",
    "[ fig : stationarity ] illustrates the properties in terms of an arbitrary gsc matrix .",
    "the leftmost plot corresponds to the case of approximate stationarity ( strong diagonal ) , the middle to the case of non - noisy mlrtg and the rightmost plot to the case of approximate mlrtg .",
    "note that the energy spilling out of the top left submatrix due to noise results in an approximate low - rank representation .",
    "* examples : * many real world datasets satisfy the approximate mlrtg assumption .",
    "[ fig : gsc ] presents the example of a hyperspectral face tensor .",
    "the singular values for each of the modes show the low - rankness property , whereas the graph spectral covariance and the energy concentration plot ( property 2 ) show that 99% of the energy of the mode 1 can be expressed in terms of the top 2% of the graph eigenvectors .",
    "examples of fmri and coil20 tensor are also presented in fig .",
    "[ fig : gsc_two ] of appendix [ sec : eg_amlrtg ] .",
    "any @xmath34 is the product of two important components 1 ) the laplacian eigenvectors and eigenvalues @xmath25 , @xmath56 for each of the modes of the tensor and 2 ) the gct @xmath57 . while the former can be pre - computed , the gct needs to be determined via an appropriate procedure .",
    "once determined , it can be used directly as a low - dimensional feature of the tensor @xmath5 or employed for other useful tasks .",
    "it is therefore possible to propose a general framework for solving a broad range of tensor / matrix inverse problems , which optimize the gct @xmath57 . for a general linear operator @xmath58 and its matricization @xmath59 , @xmath60 where @xmath61 is an @xmath62 norm depending on the application under consideration and @xmath63 , denote the kernelized laplacian eigenvalues as the weights for the nuclear norm minimization .",
    "assuming the eigenvalues are sorted in ascending order , this corresponds to a higher penalization of higher singular values of @xmath64 which correspond to noise .",
    "thus , the goal is to determine a graph core tensor @xmath57 whose rank is minimized in all the modes .",
    "such a nuclear norm minimization on the full tensor ( without weights ) has appeared in earlier works @xcite .",
    "however , note that in our case we lift the computational burden by minimizing only the core tensor @xmath57 .",
    "the first application corresponds to the case where one is only interested in the gct @xmath57 . for a clean matricized tensor @xmath65 ,",
    "it is straight - forward to determine the matricized @xmath57 as @xmath66 .    for the case of noisy @xmath5 corrupted with gaussian noise",
    ", one seeks a robust @xmath57 which is not possible without an appropriate regularization on @xmath57 .",
    "hence , we propose to solve problem [ eq : ginvt ] with frobenius norm : @xmath67 using @xmath68 in eq .",
    ", we get : @xmath69 which we call as _ graph core tensor pursuit ( gctp)_. to solve gctp , one just needs to apply the singular value soft - thresholding operation ( appendix [ sec : algo_trpca ] ) on each of the modes of the tensor @xmath57 . for @xmath70 , it scales with @xmath71 , where @xmath4 .",
    "notice that the decomposition defined by eq .",
    "is quite similar to the standard mulitlinear svd ( mlsvd ) @xcite .",
    "is it possible to define a graph based mlsvd using @xmath57 ?",
    "* mlsvd : * in standard mlsvd , one aims to decompose a tensor @xmath0 into factors @xmath72 which are linked by a core @xmath73 .",
    "this can be attained by solving the als problem @xcite which iteratively computes the svd of every mode of @xmath5 until the fit @xmath74 stops to improve .",
    "this costs @xmath75 per iteration for rank @xmath12 .    * from mlsvd to gmlsvd : * in our case the fit is given in terms of the pre - computed laplacian eigenvectors @xmath25 , i.e , @xmath76 and @xmath57 is determined by gctp eq . .",
    "this raises the question about how the factors @xmath77 relate to @xmath25 and core tensor @xmath78 to @xmath57 .",
    "we argue as following : let @xmath79 be the mlsvd of @xmath57 .",
    "then , if we set set @xmath80 , then @xmath81 .",
    "while we give an example below , a more thoretical study is presented in the theorem [ thm : recovery ] .",
    "* algorithm for gmlsvd : * thus , for a tensor @xmath5 , one can compute gmlsvd in the following steps : 1 ) compute the graph core tensor @xmath57 via gctp ( eq . ) , 2 ) perform the mlsvd of @xmath82 , 3 ) let the factors @xmath80 and the core tensor is @xmath83 .",
    "given the laplacian eigenvectors @xmath25 , gmlsvd scales with @xmath71 per iteration which is the same as the complexity of solving gctp .",
    "* example : * to understand this , imagine the case of 2d tensor ( a matrix ) of vectorized wedge images from coil20 dataset in the columns .",
    "@xmath84 for this matrix corresponds to the left singular vectors obtained by svd and @xmath85 correspond to the first @xmath21 eigenvectors of the laplacian @xmath86 between the rows ( pixels ) .",
    "[ fig : example_gmlsvd ] shows an example wedge image , 1st singular vector in @xmath84 obtained via svd and the 2nd laplacian eigenvector @xmath87 . clearly , the 1st singular vector in @xmath84 is not equal to the 2nd eigenvector @xmath87 ( and others ) .",
    "however , if we recover @xmath88 using gctp ( eq . ) and then perform the svd of @xmath89 and let @xmath90 , then @xmath91 ( bottom right plot in fig .",
    "[ fig : example_gmlsvd ] ) . for more examples ,",
    "please see fig .",
    "[ fig : eg_algo_gmlsvd ] in the appendices .     obtained via svd and the 2nd laplacian eigenvector @xmath87 . clearly , @xmath92 ( and other eigenvectors ) .",
    "however , @xmath91 , where @xmath93 , and @xmath94 are the left singular vectors of @xmath95 via gctp eq .",
    ", scaledwidth=40.0% ]      another important application of low - rank tensors is tensor robust pca ( trpca ) @xcite .",
    "unfortunately , this method scales as @xmath1 .",
    "we propose an alternate framework , tensor robust pca on graphs ( trpcag ) : @xmath96 the above algorithm requires nuclear norm on @xmath97 and scales with @xmath98 .",
    "this is a significant complexity reduction over trpca .",
    "we use parallel proximal splitting algorithm to solve eq .",
    "as shown in appendix [ sec : algo_trpca ] .",
    "although the inverse problems of the form ( eq . [ eq : ginvt ] ) are orders of magnitude faster than the standard tensor based inverse problems , they introduce some approximation .",
    "first , note that we do not present any procedure to determine the optimal @xmath21 .",
    "furthermore , as noted from the proof of theorem [ thm : props ] , for @xmath20 , the choice of @xmath21 depends on the eigen gap assumption ( @xmath99 ) , which might not exist for the @xmath13-laplacians .",
    "finally , noise in the data adds to the approximation as well .",
    "we perform our analysis for 2d tensors , i.e , matrices of the form @xmath100 .",
    "the results can be extended for high order tensors in a straight - forward manner .",
    "we assume further that 1 ) the eigen gaps exist , i.e , there exists a @xmath101 , such that @xmath102 and 2 ) we select a @xmath103 for our method . for the case of 2d tensor , the general inverse problem [ eq : ginvt ] , using @xmath104 can be written as : @xmath105    [ thm : recovery ] for any @xmath106 ,    1 .   let @xmath107 be the svd of @xmath6 and @xmath108 be the svd of the gct @xmath88 obtained via gctp ( eq . ) .",
    "now , let @xmath109 , where @xmath110 are the laplacian eigenvectors of @xmath10 , then , @xmath111 upto a sign permutation and @xmath112 .",
    "2 .   solving eq .",
    ", with a @xmath103 is equivalent to solving the following factorized graph regularized problem : @xmath113 where @xmath80 , @xmath114 and @xmath115 .",
    "any solution @xmath116 of , where @xmath117 and @xmath103 with @xmath118 and @xmath119 , where @xmath120 and @xmath121 satisfies @xmath122 where @xmath123 denote the @xmath124 eigenvalues of @xmath125 and @xmath126 , where @xmath127 denote the projection of the factors on the @xmath128 complement eigenvectors",
    ".    please see appendix [ sec : proof_thm_recovery ] .    * in simple words : * theorem [ thm : recovery ] states that 1 ) the singular vectors and values of a matrix / tensor obtained by gmlsvd are equivalent to those obtained by mlsvd , 2 ) in general , the inverse problem [ eq : ginv ] is equivalent to solving a graph regularized matrix / tensor factorization problem ( eq.[eq : greg ] ) where the factors @xmath129 belong to the span of the graph eigenvectors constructed from the modes of the tensor .",
    "the bound eq .",
    "[ eq : theory ] shows that to recover an mlrtg one should have large eigen gaps @xmath130 .",
    "this occurs when the rows of the matricized @xmath5 can be clustered into @xmath101 clusters .",
    "the smaller @xmath131 is , the closer @xmath132 is to @xmath133 . in case one selects a @xmath103 , the error is characterized by the projection of singular vectors @xmath134 on @xmath135 complement graph eigenvectors @xmath136 .",
    "our experiments show that selecting a @xmath137 always leads to a better recovery when the exact value of @xmath101 is not known .",
    "* datasets : * to study the performance of gmlsvd and trpcag , we perform extensive experimentation on 4 artificial and 12 real 2d-4d tensors .",
    "all types of low - rank artificial datasets are generated by filtering a randomly generated tensor with the @xmath13 combinatorial laplacians constructed from its flattened modes ( details in appendix [ sec : experiment_details ] ) .",
    "then , different levels of gaussian and sparse noise are added to the tensor .",
    "the real datasets include hyperspectral images , eeg , bci , fmri , 2d and 3d image and video tensors and 3 point cloud datasets .    * methods : * gmlsvd and trpcag are low - rank tensor factorization methods , which are programmed using gspbox @xcite , unlocbox @xcite and tensorlab @xcite toolboxes .",
    "note that gmlsvd is robust to gaussian and trpcag to sparse noise , therefore , these methods are tested under varying levels of these two types of noise . to avoid any confusion",
    ", we call the 2d tensor ( matrix ) version of gmlsvd as graph svd ( gsvd ) .    for the 3d tensors with gaussian noise",
    "we compare gmlsvd performance with mlsvd . for the 3d tensor with sparse noise",
    "we compare trpcag with tensor robust pca ( trpca ) @xcite and gmlsvd . for the 2d tensors ( matrices ) with gaussian noise",
    "we compare gsvd with simple svd .",
    "finally for the 2d matrix with sparse noise we compare trpcag with robust pca ( rpca ) @xcite , robust pca on graphs ( rpcag ) @xcite , fast robust pca on graphs ( frpcag ) @xcite and compressive pca ( cpca ) @xcite .",
    "not all the methods are tested on all the datasets due to computational reasons .",
    "* parameters : * for all the experiments involving trpcag and gmlsvd , the @xmath13 graphs are constructed from the rows of each of the flattened modes of the tensor , using @xmath138 and a gaussian kernel for weighting the edges . for all the other methods",
    "the graphs are constructed as required , using the same parameters as above .",
    "each method has several hyper - parameters which require tuning . for a fair comparison ,",
    "all the methods are properly tuned for their hyper - parameters and best results are reported . for details on all the datasets , methods and parameter tuning",
    "please refer to appendix [ sec : experiment_details ] .",
    "* evaluation metrics : * the metrics used for the evaluation can be divided into two types : 1 ) quantitative and 2 ) qualitative .",
    "three different types of quantitative measures are used : 1 ) normalized @xmath139 reconstruction error of the tensor @xmath140 , 2 ) the normalized @xmath139 reconstruction error of the first @xmath101 ( normally @xmath141 ) singular values along mode 1 @xmath142 3 ) the subspace angle ( in radian ) of mode 1 between the 1st five subspace vectors determined by the proposed method and those of the clean tensor : @xmath143 and 4 ) the alignment of the singular vectors @xmath144 , where @xmath145 and @xmath84 denote the mode 1 singular vectors determined by the proposed method and clean tensor .",
    "the qualitative measure involves the visual quality of the low - rank components of tensors .",
    "* performance study on artificial datasets : * the first two rows of fig .",
    "[ fig : gmlsvd_results ] show the performance of gsvd ( for 2d ) and gmlsvd ( for 3d ) on artificial tensors of the size 100 and rank 10 in each mode , for varying levels of gaussian noise ranging from 15db to 1db .",
    "the three plots show the @xmath139 reconstruction error of the recovered tensor , the first @xmath141 singular values and and the subspace angle of the 1st mode subspace ( top 5 vectors ) , w.r.t to those of the clean tensor .",
    "these results are compared with the standard svd for 2d tensor and standard mlsvd for the 3d tensor .",
    "it is interesting to note from the leftmost plot that the @xmath139 reconstruction error for gsvd tends to get lower as compared to svd at higher noise levels ( snr less than 5db ) .",
    "the middle plot explains this phenomena where one can see that the @xmath139 error for singular values is significantly lower for gsvd than svd at higher noise levels .",
    "this observation is logical , as for higher levels of noise the lower singular values are also affected .",
    "svd is a simple singular value thresholding method which does not eliminate the effect of noise on lower singular values , whereas gsvd is a smart weighted nuclear norm method which thresholds the lower singular values via a function of the graph eigenvalues .",
    "this effect is shown in detail in fig .",
    "[ fig : gsvd_example_small ] .",
    "on the contrary , the subspace angle ( for first 5 vectors ) for gsvd is higher than svd for all the levels of noise .",
    "this means that the subspaces of the gsvd are not well aligned with the ones of the clean tensor .",
    "however , as shown in the right plot in fig .",
    "[ fig : gsvd_example_small ] , strong first 7 elements of the diagonal @xmath146 show that the individual subspace vectors of gsvd are very well aligned with those of the clean tensor .",
    "this is the primary reason why the @xmath139 reconstruction error of gsvd is less as compared to svd at low snr . at higher snr ,",
    "the approximation error of gsvd dominates the error due to noise .",
    "thus , gsvd reveals its true power at low snr scenarios .",
    "similar observations can be made about gmlsvd from the 2nd row of fig .",
    "[ fig : gmlsvd_results ] .        * time , memory & performance on real datasets : * the 3rd row of fig .",
    "[ fig : gmlsvd_results ] shows the results of gmlsvd compared to mlsvd for a 4d real eeg dataset of size 3 gb and dimensions @xmath147 .",
    "a core tensor of size @xmath148 is used for this experiment .",
    "it is interesting to note that for low snr scenarios the @xmath139 reconstruction error of both methods is approximately same .",
    "gmlsvd and mlsvd both show a significantly lower error for the singular values ( note that the scale is small ) , whereas gmlsvd s subspaces are less aligned as compared to those of mlsvd .",
    "the rightmost plot of this figure compares the computation time of both methods .",
    "_ clearly , gmlsvd wins on the computation time ( 120 secs ) significantly as compared to mlsvd ( 400 secs)_. for this 3 gb dataset , gmlsvd requires 6 gb of memory whereas mlsvd requires 15 gb , as shown in the detailed results in fig .",
    "[ fig : real_data_gaussian_main ] of the appendices . fig .",
    "[ fig : real_data_gaussian_main ] also presents results for bci , hyperspectral and fmri tensors and reveals how gmlsvd performs better as compared to mlsvd while requiring less computation time and memory for big datasets in the low snr regime . for a visualization of the clean , noisy and gmlsvd recovered tensors , their singular values and the alignments of the subspace vectors for fmri and hyperspectral tensors",
    ", please refer to fig .",
    "[ fig : gmlsvd_main ] in appendices .    )",
    "3d hyperspectral tensors . using core of size @xmath149 we attained 150 times compression while maintaining an snr of 25db.,scaledwidth=45.0% ]         and",
    "size 1.5 gb via trpcag . using a core size of @xmath150 ,",
    "trpcag converged in less than 3 minutes .",
    ", scaledwidth=45.0% ]        * compression : * an obvious goal of mlsvd is the compression of low - rank tensors , therefore , gmlsvd can also be used for this purpose .",
    "[ fig : hyperspectral_compression ] shows results for the face ( @xmath151 ) 3d hyperspectral tensor . using core of size @xmath149 we attained 150 times compression while maintaining snr of 25db .",
    "[ fig : hyperspectral_compression_zoomed ] in the appendices shows such results for three other datasets .",
    "the rightmost plots of fig .",
    "[ fig : gmlsvd_main ] in appendices also show compression results for fmri , eeg and bci datasets .",
    "* performance study on artificial datasets : * the first two rows of fig . [",
    "fig : trpcag_results ] show experiments on the 2d and 3d artificial datasets with varying levels of sparse noise .",
    "the 2d version of trpcag is compared with state - of - the - art robust pca based methods , frpcag and rpca and also with svd based methods like mlsvd and gsvd .",
    "conclusions , similar to those for the gaussian noise experiments can be drawn for the 2d matrices ( 1st row ) .",
    "thus trpcag is better than state - of - the - art methods in the presence of large fraction of sparse noise . for the 3d tensor ( 2nd row ) , trpcag is compared with gmlsvd and trpca @xcite .",
    "interestingly , the performance of trpca is always better for this case , even in the presence of high levels of noise . while trpca produces the best results , its computation time is orders of magnitude more than trpcag ( discussed next ) .",
    "a detailed analysis of the singular values recovered by trpcag can be done via fig .",
    "[ fig : trpcag_example ] in the appendices .",
    "* time & performance on 2d real datasets : * the 3rd row of fig .",
    "[ fig : trpcag_results ] present experiments on the 2d real video dataset obtained from an airport lobby ( every frame vectorized and stacked as the columns of a matrix ) .",
    "the goal is to separate the static low - rank component from the sparse part ( moving people ) in the video .",
    "the results of trpcag are compared with rpca , rpcag , frpcag and cpca with a downsampling factor of 5 along the frames .",
    "clearly , trpcag recovers a low - rank which is qualitatively equivalent to the other methods in a time which is 100 times less than rpca and rpcag and an order of magnitude less as compared to frpcag .",
    "furthermore , trpcag requires the same time as sampling based cpca method but recovers a better quality low - rank structure as seen from the 3rd row .",
    "the performance quality of trpcag is also evident from the point cloud experiment in fig .",
    "[ fig : dancer ] where we recover the low - rank point cloud of a dancing person after adding sparse noise to it .",
    "experiments on two more videos ( shopping mall and escalator ) and two point cloud datasets ( walking dog and dancing girl ) are presented in figs .",
    "[ fig : video_trpcag_extra ] , [ fig : escalator ] & [ fig : point_cloud ] in appendices .",
    "* scalability of trpcag on 3d video : * to show the scalability of trpcag as compared to trpca , we made a video of snowfall at the campus and tried to separate the snow - fall from the low - rank background via both methods .",
    "for this 1.5 gb video of dimension @xmath152 , trpcag ( with core tensor size @xmath150 ) took less than 3 minutes , whereas trpca did not converge even in 4 hours . the result obtained via trpcag is visualized in fig .",
    "[ fig : snow ] . the complete videos of actual frames ,",
    "low - rank and sparse components , for all the above experiments are provided with the supplementary material of the paper .",
    "it is imperative to point out that the inverse problems of the form eq .",
    "[ eq : ginvt ] implicitly determine the subspace structures ( singular vectors ) from grossly corrupted tensors .",
    "examples of gmlsvd from section [ sec : gmlsvd ] correspond to the case of clean tensor .",
    "in this section we show that the singular vectors recovered by trpcag ( section [ sec : trpcag ] ) from the sparse and grossly corrupted tensors also align closely with those of the clean tensor .",
    "[ fig : sv_wedge ] shows the example of the same wedge image from 2d coil20 dataset that was used in section [ sec : gmlsvd ] .",
    "the leftmost plots show a clean and sparsely corrupted sample wedge image .",
    "other plots in the 1st row show the 1st singular vector recovered by various low - rank recovery methods , svd , frpcag , rpca , rpcag and trpcag and the 2nd row shows the alignment of the 1st 20 singular vecors recovered by these methods with those of the clean tensor .",
    "the rightmost plots correspond to the case of trpcag and clearly show that the recovered subspace is robust to sparse noise .",
    "examples of yale , coil20 and orl datasets are also shown in fig .",
    "[ fig : singular_vectors ] in the appendices .      due to space constraints we study the effect of parameters @xmath153 , the multilinear rank @xmath21 and the power @xmath154 of @xmath155 in eq .",
    "[ eq : ginvt ] in fig .",
    "[ fig : params ] of the appendices . to summarize , once the parameters @xmath153 and @xmath21 are tuned , eq .",
    "[ eq : ginvt ] becomes insensitive to parameter @xmath154 .",
    "inspired by the fact that the first few eigenvectors of the @xmath13-graph provide a smooth basis for data , we present a graph based low - rank tensor decomposition model .",
    "any low - rank tensor can be decomposed as a multilinear combination of the lowest @xmath21 eigenvectors of the graphs constructed from the rows of the flattened modes of the tensor ( mlrtg ) .",
    "we propose a general tensor based convex optimization framework which overcomes the computational and memory burden of standard tensor problems and enhances the performance in the low snr regime .",
    "more specifically we demonstrate two applications of mlrtg 1 ) graph based mlsvd and 2 ) tensor robust pca on graphs for 4 artificial and 12 real datasets under different noise levels . theoretically , we prove the link of mlrtg to the joint stationarity of signals on graphs .",
    "we also study the performance guarantee of the proposed general optimization framework by connecting it to a factorized graph regularized problem .",
    "in the main body of the paper , we assume ( for simplicity of notation ) that the graph multilinear rank of the tensor is equal in all the modes @xmath35 .",
    "however , for the proof of this theorem we adopt a more general notation and assume a different rank @xmath156 for every mode @xmath157 of the tensor .      1 .   * property 1 : joint approximate graph stationarity : * + [ def : jgwss ] a tensor @xmath34 satisfies joint approximate graph stationarity , i.e , its @xmath158 matricization / flattening @xmath159 satisfies approximate graph stationarity @xmath54 : @xmath160 2 .   *",
    "property 2 : low frequency energy concentration : * + [ def : pc ] a tensor @xmath34 satisfies the low - frequency energy concentration property , i.e , the energy is concentrated in the top entries of the graph spectral covariance matrices @xmath161 .",
    "@xmath162            * separable eigenvector decomposition of a graph laplacian : * the eigenvector decomposition of a combinatorial laplacian @xmath163 of a graph @xmath168 possessing an eigen gap ( satisfying definition [ def : egap ] ) can be written as : @xmath169 where @xmath170 denote the first @xmath21 low frequency eigenvectors and eigenvalues in @xmath171 .",
    "[ def : cp ] suppose we have two graphs @xmath175 and @xmath176 where the tupple represents ( vertices , edges , adjacency matrix , degree matrix ) .",
    "the cartesian product @xmath177 is a graph such that the vertex set is the cartesian product @xmath178 and the edges are set according to the following rules : any two vertices @xmath179 and @xmath180 are adjacent in @xmath168 if and only if either            with this definition of the adjacency matrix , it is possible to write the degree matrix of the cartesian product graph as cartesian product of the factor degree matrices : @xmath194 where we have used the following property @xmath195 this implies the following matrix equality : @xmath191 the * combinatorial * laplacian of the cartesian product is : @xmath196      [ def : sgap ] a cartesian product graph as defined in [ def : cp ] , is said to have an eigen gap if there exist @xmath197 , such that , @xmath198 , where @xmath199 denotes the @xmath156 eigenvalue of the @xmath158 graph laplacian @xmath200 .    [",
    "[ consequence - of - eigen - gap - assumption - separable - eigenvector - decomposition - of - a - cartesian - product - graph ] ] consequence of ` eigen gap assumption ' : separable eigenvector decomposition of a cartesian product graph ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    the eigen gap assumption ( definition [ def : sgap ] ) is important to define the notion of the ` separable eigenvector decomposition of a cartesian product graph ' which will be used in the final steps of this theorem . we define the eigenvector decomposition of a laplacian of cartesian product of two graphs .",
    "the definition can be extended in a straight - forward manner for more than two graphs as well .",
    "[ lem : ev ] for a cartesian product graph as defined in [ def : cp ] , the eigenvector decomposition can be written in a separable form as : @xmath201 where @xmath202 and @xmath21 denotes the first @xmath203 low frequency eigenvectors and eigenvalues in @xmath171 .    the eigenvector matrix of the cartesian product graph can be derived as : @xmath204 so the eigenvector matrix is given by the kronecker product between the eigenvector matrices of the factor graphs and the eigenvalues are the element - wise summation between all the possible pairs of factors eigenvalues , i.e. the cartesian product between the eigenvalue matrices .",
    "where we assume that @xmath206 with the first @xmath156 columns in @xmath207 and 0 appended for others and @xmath208 with the first @xmath156 columns equal to 0 and others copied from @xmath207 .",
    "the same holds for @xmath209 as well .",
    "now @xmath210 where we use @xmath211 . now removing the zero appended columns we get @xmath212 .",
    "now , let @xmath213 removing the zero appended entries we get @xmath214 .",
    "for a @xmath13-nearest neighbors graph constructed from a @xmath215-clusterable data ( along rows ) one can expect @xmath216 as @xmath217 and @xmath218 .",
    "furthermore @xmath219 ( definitions [ def : egap ] & [ def : sgap ] ) .",
    "thus eq . can be written as : @xmath220 [ \\lambda_{k_1 k_2 } |\\bar{\\lambda}_{k_1 k_2 } ] [ p_{k_1 k_2}| \\bar{p}_{k_1 k_2}]^\\top\\end{aligned}\\ ] ]      now we are ready to prove the theorem .",
    "we start by expanding the denominator of the expression from property 2 above .",
    "we start with the gsc @xmath49 for any @xmath10 and use @xmath221 . for a 3d tensor ,",
    "we index its modes with @xmath222 .",
    "while considering the eigenvectors of @xmath9 mode , i.e , @xmath223 we represent the kronecker product of the eigenvectors of other modes as @xmath224 for simplicity : @xmath225 [ p_{\\mu k_\\mu } x_\\mu p^\\top_{-\\mu k_{-\\mu}}+ \\bar{p}_{\\mu k_\\mu } \\bar{x}_\\mu \\bar{p}^\\top_{-\\mu k_{-\\mu}}]^\\top p_\\mu \\end{aligned}\\ ] ] the last step above follows from the eigenvalue decomposition of a cartesian product graph ( lemma [ lem : ev ] ) .",
    "note that this only holds if the eigen gap condition ( definition [ def : sgap ] ) holds true .",
    "@xmath226 [ p_{\\mu k_\\mu } x_\\mu p^\\top_{-\\mu k_{-\\mu}}+ \\bar{p}_{\\mu k_\\mu } \\bar{x}_\\mu \\bar{p}^\\top_{-\\mu k_{-\\mu}}]^\\top p_\\mu \\big)^\\top \\\\ & \\times",
    "p^\\top_\\mu [ p_{\\mu k_\\mu } x_\\mu p^\\top_{-\\mu k_{-\\mu}}+ \\bar{p}_{\\mu k_\\mu } \\bar{x}_\\mu \\bar{p}^\\top_{-\\mu k_{-\\mu } } ] [ p_{\\mu k_\\mu } x_\\mu p^\\top_{-\\mu k_{-\\mu}}+ \\bar{p}_{\\mu k_\\mu } \\bar{x}_\\mu \\bar{p}^\\top_{-\\mu k_{-\\mu}}]^\\top p_\\mu \\big ) \\\\ & = \\operatorname{tr}((x_\\mu x^\\top_\\mu)^\\top x_\\mu x^\\top_\\mu ) + \\operatorname{tr}((\\bar{x}_{\\mu } \\bar{x}^\\top_{\\mu})^\\top \\bar{x}_{\\mu } \\bar{x}^\\top_{\\mu } ) \\\\    & = \\|x_\\mu\\|^4_f    +   \\| \\bar{x}_\\mu\\|^4_f     \\end{aligned}\\ ] ] the third step follows from the fact that @xmath227 , @xmath228 , @xmath229 , @xmath230 , @xmath231 , @xmath232 and @xmath233 .    let @xmath234 be a matrix operator which represents the selection of the first @xmath156 rows and columns of a matrix .",
    "now , expanding the numerator : @xmath235p_\\mu\\}_{1:k_\\mu,1:k_\\mu}\\|^2_f   \\\\   & = \\|\\big\\{\\begin{bmatrix } i_{k_\\mu \\times k_\\mu } \\\\ 0_{(n - k_\\mu ) \\times k_\\mu}\\end{bmatrix } x_\\mu x_\\mu^\\top { \\begin{bmatrix } i_{k_\\mu \\times k_\\mu } \\\\ 0_{(n - k_\\mu ) \\times k_\\mu}\\end{bmatrix}}^\\top + \\begin{bmatrix } 0_{k_\\mu \\times ( n - k_\\mu ) } \\\\ i_{(n - k_\\mu ) \\times ( n - k_\\mu)}\\end{bmatrix}\\bar{x}_\\mu \\bar{x}^\\top_\\mu { \\begin{bmatrix } 0_{k_\\mu \\times ( n - k_\\mu ) } \\\\ i_{(n - k_\\mu ) \\times ( n - k_\\mu)}\\end{bmatrix}}^\\top   \\big\\}_{1:k_\\mu , 1:k_\\mu}\\|^2_f \\\\   & = \\|x_\\mu\\|^4_f   \\end{aligned}\\ ] ] finally ,          the reader might note that the whole framework relies on the existence of the eigen gap condition ( definition [ def : sgap ] ) .",
    "such a notion does not necessarily exist for the real data , however , we clarify a few important things right away .",
    "the existence of eigen gap is not strict for our framework , thus , practically , it performs reasonably well for a broad range of applications even when such a gap does not exist 2 ) we introduce this notion to study the theoretical side of our framework and characterize the approximation error ( section [ sec : theory ] ) .",
    "experimentally , we have shown ( section [ sec : results ] ) that choosing a specific number of eigenvectors of the graph laplacians , without knowing the spectral gap is good enough for our setup .",
    "thus , at no point throughout this paper we presented a way to compute this gap .",
    "let @xmath244 denote the element - wise soft - thresholding matrix operator : @xmath245 then we can define @xmath246 as the singular value thresholding operator for matrix @xmath64 , where @xmath247 is any singular value decomposition of @xmath64 .",
    "clearly ,            input : matricized tensor @xmath65 , weight matrices @xmath256 , parameter @xmath153 , @xmath257 .",
    "@xmath258 , all matricized along mode 1 .",
    "@xmath260 @xmath261 @xmath262 @xmath263 @xmath264",
    "@xmath265 @xmath266 , @xmath267 output : @xmath268          * * eigen gap assumption : * for a @xmath13-nearest neighbors graph constructed from a @xmath101-clusterable data one can expect @xmath269 as @xmath270 and @xmath271 ( definition [ def : egap ] ) .",
    "* * separable eigenvector decomposition of a graph laplacian : * @xmath272 , where @xmath273 is a diagonal matrix of lower eigenvalues and @xmath274 is also a diagonal matrix of higher graph eigenvalues .",
    "all values in @xmath275 are sorted in increasing order .        1 .",
    "the existence of an eigen gap ( 1st point above ) and the definition of mlrtg imply that one can obtain a loss - less compression of the tensor @xmath276 as : @xmath277 + note that this compression is just a projection of the rows and columns of @xmath276 onto the basis vectors which exactly encode the tensor , hence the compression is loss - less .",
    "now , as @xmath278 is loss - less , the singular values of @xmath276 should be an exact representation of the singular values of @xmath278 .",
    "thus , the svd of @xmath279 implies that @xmath280 , where @xmath281 are the singular values of @xmath276 .",
    "obviously , @xmath282 upto a sign permutation because the svd of @xmath278 is also unique upto a sign permutation ( a standard property of svd ) .",
    "the proof of second part follows directly from that of part 1 ( above ) and theorem 1 in @xcite .",
    "first , note that for any matrix ( 2d tensor ) @xmath100 eq .",
    "can be written as following : @xmath283 where @xmath284 and @xmath285 are diagonal matrices which indicate the weights for the nuclear norm minimization .",
    "+ let @xmath286 , @xmath287 and @xmath288 , then we can re - write eq .",
    "as following : @xmath289 + eq .",
    "is equivalent to the weighted nuclear norm ( eq .",
    "11 ) in @xcite . from the proof of the first part",
    "we know that @xmath93 and @xmath290 , thus we can write @xmath291 . from theorem 1 in @xcite @xmath292 + using @xmath293 this is equivalent to the following graph regularized problem : @xmath294 3 .   to prove the third point we directly work with eq . and follow the steps of the proof of theorem 1 in @xcite .",
    "we assume the following : 1 .",
    "we assume that the observed data matrix @xmath6 satisfies @xmath295 where @xmath106 and @xmath296 models noise / corruptions .",
    "furthermore , for any @xmath106 there exists a matrix @xmath297 such that @xmath298 and @xmath299 , so @xmath300 .",
    "2 .   for the proof of the theorem",
    ", we will use the fact that @xmath301 , where @xmath302 , @xmath127 and @xmath303 , @xmath304 .",
    "+ as @xmath305 is the solution of eq .",
    ", we have + @xmath306 + now using the facts 3b and the eigen gap condition we obtain the following two : + @xmath307 + and similarly , @xmath308 + now , using the fact 3a we get + @xmath309 + and @xmath310 + using all the above bounds in eq .",
    "yields + @xmath311 + for our choice of @xmath312 and @xmath313 this yields eq ..              1 .",
    "generate a random gaussian matrix , @xmath314 .",
    "2 .   construct a @xmath13 graph @xmath188 between the rows of @xmath6 and compute the combinatorial laplacian @xmath86 .",
    "3 .   construct a @xmath13 graph @xmath184 between the rows of @xmath315 and compute the combinatorial laplacian @xmath316 .",
    "4 .   compute the first @xmath21 ( where @xmath4 ) eigenvectors and eigenvalues of @xmath86 and @xmath316 , @xmath317 .",
    "generate a random matrix of the size @xmath318 .",
    "generate the low - rank artificial tensor by using @xmath319 , where @xmath320 denotes the kronecker product .",
    "* method 2 : * indirectly , by filtering a randomly generated tensor with the @xmath13 combinatorial laplacians constructed from the flattened modes of the tensor .",
    "we describe the procedure for a 2d tensor below :    1 .",
    "follow steps 1 to 3 of method 1 above . 2 .",
    "generate low - rank laplacians from the eigenvectors : @xmath321 and @xmath322 .",
    "3 .   filter the matrix @xmath6 with these laplacians to get the low - rank matrix @xmath323 .",
    "* 2d video datasets : * three video datasets ( 900 frames each ) collected from the following source : https://sites.google.com/site/backgroundsubtraction/test-sequences .",
    "each of the frames is vectorized and stacked in the columns of a matrix .        1 .   functional magnetic resonance imaging ( fmri ) : @xmath327 ( frequency bins , rois , time samples ) , size 1.5 gb .",
    "2 .   brain computer interface ( bci ) : @xmath328 ( time , frequency , channels ) , size 200 mb .",
    "hyperspectral face dataset : @xmath151 ( y - dimension , x - dimension , spectrum ) , size 250 mb , source : https://scien.stanford.edu / index.php / faces-1-meter - viewing - distance/. 4 .",
    "hyperspectral landscape dataset : @xmath329 ( y - dimension , x - dimension , spectrum ) , size 650 mb , source : https://scien.stanford.edu / index.php / landscapes/. 5 .",
    "snowfall video : @xmath152 ( y - dimension , x - dimension , number of frames ) , size 1.5 gb .",
    "this video is self made .",
    "* 3d point cloud datasets : * three 3d datasets ( @xmath330 ) collected from the following source : http://research.microsoft.com/en-us/um/redmond/events/geometrycompression/data/default.html . for the purpose of our experiments , we used one of the three coordinates of the tensors only .",
    "* compressive pca for low - rank matrices on graphs ( cpca ) @xcite * 1 ) sample the matrix @xmath6 by a factor of @xmath339 along the rows and @xmath340 along the columns as @xmath341 and solve frpcag to get @xmath342 , 2 ) do the svd of @xmath343 recovered from step 1 , 3 ) decode the low - rank @xmath88 for the full dataset @xmath6 by solving the subspace upsampling problems below :"
  ],
  "abstract_text": [
    "<S> we propose a new framework for the analysis of low - rank tensors which lies at the intersection of spectral graph theory and signal processing . as a first step </S>",
    "<S> , we present a new graph based low - rank decomposition which approximates the classical low - rank svd for matrices and multilinear svd for tensors . </S>",
    "<S> then , building on this novel decomposition we construct a general class of convex optimization problems for approximately solving low - rank tensor inverse problems , such as tensor robust pca . </S>",
    "<S> the whole framework is named as `` multilinear low - rank tensors on graphs ( mlrtg ) '' . </S>",
    "<S> our theoretical analysis shows : 1 ) mlrtg stands on the notion of approximate stationarity of multi - dimensional signals on graphs and 2 ) the approximation error depends on the eigen gaps of the graphs . </S>",
    "<S> we demonstrate applications for a wide variety of 4 artificial and 12 real tensor datasets , such as eeg , fmri , bci , surveillance videos and hyperspectral images . </S>",
    "<S> generalization of the tensor concepts to non - euclidean domain , orders of magnitude speed - up , low - memory requirement and significantly enhanced performance at low snr are the key aspects of our framework .    </S>"
  ]
}