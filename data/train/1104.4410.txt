{
  "article_text": [
    "structured nonparametric models such as additive models are known to circumvent the curse of dimensionality and allow reliable estimation when a full nonparametric model does not work . in the present paper",
    "we show that a similar assertion applies for semi - parametric models : structural modeling of the nonparametric part can lead to accurate estimation of the parametric part even in situations where otherwise only very poor , unreliable or unstable estimates would be available .",
    "we show this by comparing the partially linear and the partially linear additive model .",
    "in particular , we demonstrate that using an additive model for the nonparametric part in the partially linear model can lead to drastic gains of efficiency in the estimation of the parametric components .",
    "this holds if the dimension of the nonparametric covariates is high , or the parametric covariates can be approximated by non - additive transformations of the nonparametric covariates . in the extreme of the latter case ,",
    "if the approximation is exact , then estimation of the parametric part in the partially linear model breaks down .",
    "if the approximation is very crude , one sees large efficiency gains by using additive models for the nonparametric part.=-1    suppose we observe the i.i.d .",
    "copies @xmath0 of a random vector @xmath1 , where @xmath2 and @xmath3 .",
    "the partially linear model assumes @xmath4 where @xmath5 is an unknown @xmath6-vector and @xmath7 is an unknown @xmath8-variate function .",
    "the partially linear additive model puts an additive structure to the nonparametric function @xmath7 : @xmath9 these models exclude the interesting case where @xmath10 or @xmath11 includes some endogeneous variables of  @xmath12 , but they simplify our discussion on semi - parametric efficiency .",
    "we believe that our results can be extended to the corresponding semi - parametric models with time series data by following , for example , the arguments in @xcite .    for identifiability of the additive component functions",
    "@xmath13 , we put the constraints @xmath14 .",
    "we assume that @xmath15 has a joint density @xmath16 with respect to @xmath17 , where @xmath18 is a @xmath19-finite measure and @xmath20 is the lebesgue measure on each support of @xmath10 and @xmath11 , and that the marginal density of @xmath11 ( with respect to @xmath18 ) , denoted by @xmath21 , has compact support , say @xmath22^d$ ] .",
    "the model ( [ pladdmodel ] ) enjoys the advantages of both the partially linear model ( [ plmodel ] ) and the nonparametric additive model to the fully nonparametric model .",
    "it accommodates discrete covariates since we only require that @xmath18 is a @xmath19-finite measure , and also interaction effects between covariates by putting them into the parametric part . by the additive structure in the nonparametric part",
    "it avoids the curse of dimensionality , but retains the flexibility of the model .",
    "it also renders easy interpretation of the individual role of each covariate .",
    "we discuss semi - parametric efficient estimation of the parameter @xmath5 in the model ( [ pladdmodel ] ) .",
    "we present the semi - parametric fisher information bound and provide an estimator that achieves the efficiency bound .",
    "semi - parametric efficient estimation when @xmath23 has been studied by bhattacharya and zhao @xcite , cuzick @xcite and schick @xcite .",
    "their works can be easily extended to the model ( [ plmodel ] ) for @xmath24 . comparing the fisher information bounds for the models ( [ plmodel ] ) and ( [ pladdmodel ] )",
    ", we find that the information bound under the model ( [ pladdmodel ] ) is smaller than the bound under the model  ( [ plmodel ] ) . in our semi - parametric model ( [ pladdmodel ] )",
    ", we do not specify the distribution of the error term @xmath25 or the distribution @xmath16 of the covariates .",
    "we show that one can do as well without knowing those distributions .",
    "there have been a few works on the model ( [ pladdmodel ] ) .",
    "opsomer and ruppert @xcite obtained a @xmath26-consistent estimator of @xmath5 by a backfitting method with undersmoothing . recently",
    "liang _ et al .",
    "_ @xcite and carroll _ et al . _",
    "@xcite studied the model with measurement error and repeated measurements , respectively .",
    "but they did not discuss semiparametric efficiency .",
    "the model ( [ plmodel ] ) has been studied more often ; see @xcite , among others .",
    "most studies , however , are rather focused on the cases where there is only a single - dimensional ( or at most low - dimensional ) nonparametric function @xmath7 .",
    "this is because high - dimension costs higher - order smoothness in theory and poor small sample performances in practice .",
    "to avoid unnecessary complexity , we assume @xmath27 . we also assume that @xmath25 is independent with @xmath15 , and that @xmath28 , the density of @xmath25 , is symmetric and is absolutely continuous with respect to the lebesgue measure , having a derivative @xmath29 and finite fisher information @xmath30 .",
    "below , we give a heuristic argument for deriving the semi - parametric efficiency and present a rigorous statement in a theorem .",
    "suppose that @xmath28 is known and @xmath31 .",
    "we write @xmath32 and adopt the convention @xmath33 .",
    "the logarithm of the joint density of @xmath1 as a function of the parameters is given by @xmath34 , neglecting those terms that do not depend on @xmath35 , and the log - likelihood of @xmath36 by @xmath37 .",
    "let @xmath38 denote the space of all additive functions @xmath7 such that @xmath39 , @xmath40 and @xmath41 .",
    "calculation of the fisher information in a semi - parametric model is made locally : fix a value @xmath42 of the parameter @xmath43 and think of all ` regular ' parametric submodels @xmath44 passing through @xmath42 , where @xmath45 and the mapping @xmath46 is frchet differentiable as a function from @xmath47 to @xmath38 .",
    "define @xmath48 .",
    "then , each finite - dimensional submodel @xmath49 has the score function @xmath50 where @xmath51 is the tangent of the mapping @xmath46 at @xmath52 , and @xmath53 denotes the frchet derivative of @xmath54 with respect to @xmath7 .",
    "this gives the fisher information for estimating @xmath55 in each submodel as @xmath56 ^ 2 $ ] .",
    "the fisher information at @xmath57 in the _ full _ semi - parametric model typically equals to the fisher information at @xmath58 in the most difficult parametric submodel that gives minimal @xmath59 .",
    "theorem [ thm : info ] below demonstrates that this is the case with our problem .",
    "the least favorable direction @xmath60 that minimizes @xmath61 over @xmath62 is the solution of the following integral equation : for all @xmath63 @xmath64\\varphi(\\epsilon ) \\delta(\\mathbf{z})\\\\ & = & i_g \\cdot e \\bigl[\\bigl(e(x|\\mathbf{z } ) + \\delta^*(\\mathbf{z})\\bigr ) \\delta(\\mathbf{z})\\bigr],\\end{aligned}\\ ] ] where @xmath65 .",
    "this shows that @xmath66 , where @xmath67 denotes the projection operator onto @xmath38 , and that the ` curve ' @xmath68 corresponding to the least favorable submodel equals @xmath69 . the fisher information for the least favorable submodel is thus given by @xmath70 ^ 2 $ ] , where , with a slight abuse of notation , we write @xmath71 .",
    "the above arguments can be generalized to the case where @xmath72 .",
    "writing @xmath73 and @xmath74 , the least favorable direction equals @xmath75 so that the fisher information matrix for the least favorable submodel equals @xmath76[\\mathbf{x}-\\bolds{\\eta } ( \\mathbf{z})]^\\top$ ] . in the following theorem",
    "we show that the fisher information @xmath77 given above is indeed the semi - parametric information bound , as defined in @xcite , in our original semi - parametric model where the error density @xmath28 and the density @xmath16 of the covariate @xmath78 are not specified . to state the theorem ,",
    "let @xmath79 denote the set of all symmetric and absolutely continuous ( with respect to the lebesgue measure ) functions @xmath28 such that @xmath80 .",
    "let @xmath81 be an arbitrary class of density functions @xmath16 .",
    "for the spaces of @xmath7 , we consider hilbert spaces defined by @xmath82 where @xmath83 denotes the space of functions @xmath84 such that @xmath85 and @xmath86 means the expectation under the density @xmath16 . the semi - parametric model ( [ pladdmodel ] ) under study is then expressed as @xmath87 .",
    "let @xmath88 be a fixed point where we are calculating the semi - parametric fisher information .",
    "denote by @xmath89 the distribution corresponding to @xmath88 , and by @xmath90 the semi - parametric fisher information at @xmath89 for estimating @xmath5 under the model @xmath91 . in the theorem",
    "below , the ` efficient score ' @xmath92 for estimating @xmath5 is the score for @xmath5 at @xmath93 in the least favorable parametric submodel that is indexed only by @xmath5 and passes through @xmath89 .",
    "let @xmath94 denote the expectation under @xmath89 .",
    "[ thm : info ] the efficient score at @xmath89 for estimating @xmath5 is given by @xmath95 \\frac{{g_0}'}{g_0}\\bigl(y-\\mathbf{x}^\\top\\bolds{\\beta}^0 - m^0(\\mathbf{z})\\bigr),\\end{aligned}\\ ] ] where @xmath96)_{j=1}^p$ ] .",
    "the information bound at @xmath89 for estimating @xmath5 equals @xmath97 [ \\mathbf{x}-\\bolds{\\eta}(\\mathbf{z})]^\\top$ ] .",
    "a proof of theorem  [ thm : info ] can be found in an extended version of this paper that can be downloaded from http://stat.snu.ac.kr/theostat/papers/bej296_extendedversion.pdf .",
    "let @xmath98 denote the semi - parametric model ( [ plmodel ] ) .",
    "one can show @xmath99[\\mathbf { x}-e_0(\\mathbf{x}|\\mathbf{z})]^\\top$ ] using the arguments to derive @xmath90 .",
    "note that @xmath100 by the property of conditional expectation , and that the equality @xmath101 holds if @xmath102 are additive for all @xmath103 . according to the theory of semi - parametric efficiency",
    ", the minimal asymptotic variance that any regular estimator of @xmath104 can achieve equals the inverse of the fisher information matrix .",
    "the inequality @xmath105 implies @xmath106 , with equality holding if @xmath107 are all additive .",
    "[ thm : eff ] suppose @xmath108 is positive definite . then , @xmath109 unless @xmath110[\\bolds { \\eta}(\\mathbf{z})-e_0(\\mathbf{x}|\\mathbf{z})]^\\top = \\mathbf{o}$ ] , where @xmath111 is the @xmath112 matrix with all entries being zero , and @xmath113 means that @xmath114 is non - negative definite and @xmath115 .",
    "theorem  [ thm : eff ] tells that using an additive model for the nonparametric part can lead to drastic gains of efficiency in the estimation of the parametric components .",
    "the efficiency gains occur if the parametric covariates @xmath10 are approximated by non - additive transformations of the nonparametric covariates @xmath11 .",
    "if the approximation is exact , then estimation of the parametric part in the partially linear model ( [ plmodel ] ) breaks down since @xmath116 , while it does not with the partially linear additive model ( [ pladdmodel ] ) .",
    "if the approximation is very crude , one has large efficiency gains by using additive models for the nonparametric part .",
    "let @xmath93 and @xmath117 denote the true parameter values . in this section we present the semi - parametric efficient estimator of @xmath93 that achieves the minimal asymptotic variance @xmath118 .",
    "the construction is based on a smooth backfitting technique and a profiling method .",
    "the latter is basically for estimating the least favorable curve , and is applied to the gaussian error model to produce an initial estimator of @xmath93 to be used in the construction of the semi - parametric efficient estimator .",
    "the smooth backfitting method , introduced by mammen , linton and nielsen @xcite , is known to be a powerful technique for estimating additive regression functions . since our profiling method involves smooth backfitting for non - additive functions , we discuss some properties of the method when the target function is not additive .",
    "let @xmath119 be a random variable and @xmath120 be a random sample distributed as @xmath119 .",
    "the smooth backfitting estimator , @xmath121 , with responses @xmath122 and regressors @xmath123 , are defined as the solution of following integral equations : @xmath124 with the constraints @xmath125 for @xmath103 . here",
    ", @xmath126 and @xmath127 denotes the marginal regression kernel estimator obtained by regressing @xmath122 on @xmath128 only .",
    "the operator @xmath129 stands for a projection onto a hilbert space equipped with a scalar product @xmath130 ; see @xcite for details . for example , in the case where @xmath127 are the local constant marginal estimators , @xmath131 with @xmath132 being the kernel estimator of the design density @xmath21 . smoothing to the direction of @xmath133 is done by the boundary corrected kernel @xmath134 , where @xmath135 is a base kernel function , @xmath136 is the bandwidth , and @xmath137 is a factor that gives @xmath138 .    let @xmath139 .",
    "we do not assume that @xmath140 is an additive function .",
    "define @xmath141 to be the projection of @xmath140 onto the space of additive functions @xmath142 .",
    "then , @xmath143\\delta(\\mathbf{z})=0 $ ] for any @xmath144 .",
    "the additive function @xmath145 plays the role of the target function that the smooth backfitting estimator @xmath146 aims at .",
    "@xcite discussed the property of the smooth backfitting estimators under non - additive regression models in the context of spatial data analysis .",
    "however , they treated only the case where the bandwidth is asymptotic to @xmath147 .",
    "below , we give a uniform expansion of the smooth backfitting estimator for a wider range of the bandwidths , after tedious asymptotic calculation following the lines of the arguments in @xcite .",
    "to state the theorem , let @xmath148 and define @xmath149 accordingly .",
    "let @xmath150 and @xmath151 denote , respectively , the local constant and linear estimators with responses @xmath149 and the scalar regressors @xmath128 .",
    "let @xmath136 be the bandwidth associated with @xmath133 .",
    "the theorem relies on the following assumptions .    1 .   for @xmath152 , @xmath153",
    "are bounded away from zero and infinity on its support , @xmath22 ^ 2 $ ] , and have continuous partial derivatives .",
    "the base kernel function @xmath135 is symmetric , supported on a compact support and has bounded derivative .",
    "3 .   the functions @xmath154 s are twice continuously differentiable .",
    "@xmath155 for some @xmath156 .",
    "[ prop : unifsbf ] assume that the conditions 14 hold , and that @xmath136 are asymptotic to @xmath157 for @xmath158 .",
    "then , for @xmath159 it holds that @xmath160 } |\\hat m_{w , j}^{\\mathrm{add}}(z_j)-m_{w , j}^{\\mathrm{add}}(z_j)-h_j a_{1,j , n}(z_j)-h_j^2a_{2,j}(z_j)-\\tilde m_{\\varepsilon , j}(z_j)|=\\mathrm{o}_p((nh_j)^{-1/2})\\ ] ] in the local constant case , and that @xmath160 } |\\hat m_{w , j}^{\\mathrm{add}}(z_j)-m_{w , j}^{\\mathrm{add}}(z_j)-h_j^2a_{3,j}(z_j)-\\tilde m_{\\varepsilon , j}^{ll}(z_j)|=\\mathrm{o}_p((nh_j)^{-1/2})\\ ] ] in the local linear case , for some functions @xmath161 that are uniformly bounded and non - zero only for @xmath162 $ ] for some constant @xmath163 , and for some functions @xmath164 and @xmath165 that are continuous .",
    "a proof of theorem  [ prop : unifsbf ] can be found in an extended version of this paper that can be downloaded from http://stat.snu.ac.kr/theostat/papers/bej296_extendedversion.pdf .",
    "we apply a profiling technique to remove the infinite - dimensional parameter @xmath7 in the estimation of @xmath93 . for a general framework of profiling approaches to semi - parametric models",
    ", we refer to @xcite .",
    "see also @xcite for a more recent work on profile likelihood .",
    "define @xmath166 .",
    "we note that @xmath167 is an estimator of @xmath168 and @xmath169 is an estimator of @xmath170 . for each given @xmath104 ,",
    "let @xmath171 be the smooth backfitting estimator obtained by taking @xmath172 as responses and @xmath123 as covariates .",
    "recall that the least favorable curve is given by @xmath173 .",
    "thus , we may regard @xmath174 as an estimator of the least favorable curve @xmath175 .",
    "since @xmath176 by the fact that the smooth backfitting operation is linear in response vectors , the estimated profile likelihood based on the gaussian error model is given by @xmath177 ^ 2 = -\\sum_{i=1}^n \\bigl [ y^i-\\hat m_{y}^{\\mathrm{add}}(\\mathbf{z}^i)-\\bigl(\\mathbf { x}^i -\\hat m_{\\mathbf{x}}^{\\mathrm{add}}(\\mathbf{z}^i ) \\bigr)^\\top\\bolds{\\beta}\\bigr]^2.\\ ] ] the estimator that maximizes the above gaussian profile likelihood is then given by @xmath178 where @xmath179 and @xmath180 .",
    "[ thm : asymprofile ] suppose that the assumptions 14 hold with @xmath181 and @xmath182 , @xmath183 .",
    "also , assume that @xmath184<c$ ] a.s .",
    "for some @xmath185 , @xmath186 .",
    "if the bandwidths @xmath136 are asymptotic to @xmath157 for @xmath187 , then it holds that @xmath188^{-1}\\bigr).\\ ] ]    a proof of theorem  [ thm : asymprofile ] is given in the .",
    "we note that the asymptotic variance of the estimator @xmath189 is larger than @xmath190 .",
    "this can be seen directly from a projection property .",
    "in fact , @xmath191 and the equality hold if @xmath28 is gaussian .",
    "this means that the estimator @xmath189 achieves the semi - parametric efficiency in the reduced model where @xmath28 is specified as a gaussian density .",
    "it is also interesting to see what happens if @xmath192 does not belong to the partially linear additive model of the form ( [ pladdmodel ] ) . in this case",
    ", our estimator of @xmath193 converges to @xmath194 , which is the @xmath83-projection of @xmath193 onto the space @xmath195      in this subsection , we construct the semi - parametric efficient estimator that achieves the minimal asymptotic variance discussed in section [ sec : eff ] .",
    "we follow the approach adopted by bickel @xcite , schick @xcite , park @xcite , cuzick @xcite and bhattacharya and zhao @xcite .",
    "write @xmath196 and define @xmath197 \\varphi(\\epsilon)$ ] .",
    "then , the random sequence @xmath198 achieves the efficiency bound .",
    "we plug some estimators of the unknown quantities into @xmath198 .",
    "we estimate the error density @xmath28 by using the ` pseudo ' errors @xmath199 , where @xmath189 is the gaussian profile estimator constructed in section  [ subsec : profile ] .",
    "in particular , we take @xmath200 and @xmath201 , where @xmath202 and @xmath203 are positive constants that depend on the sample size @xmath204 , and @xmath205 is a symmetric differentiable density function",
    ". define @xmath206 where @xmath207 is the ` symmetrized ' estimator of @xmath208 defined by @xmath209/2 $ ] .",
    "our semi - parametric efficient estimator is then given by @xmath210    1 .",
    "the error @xmath25 has an absolutely continuous and symmetric density @xmath28 with respect to the lebesgue measure , @xmath211 , and @xmath212 .",
    "2 .   the kernel @xmath205 is a symmetric density function with three bounded and lipschitz continuous derivatives .",
    "the sequences @xmath202 and @xmath203 converge to zero , as @xmath213 , and satisfy @xmath214 and @xmath215 for all @xmath103 .",
    "[ thm : asymadap ] assume that the conditions of theorem [ thm : asymprofile ] and the assumptions 13 hold .",
    "then , @xmath216 .    a proof of theorem  [ thm : asymadap ] is given in the . for a choice of the bandwidth @xmath202 in @xmath217",
    ", one can devise a data - driven choice along the lines of park @xcite . for @xmath218",
    ", one can follow the approach of mammen and park @xcite .",
    "in this adaptation step , misspecification of the model may result in a meaningless estimator .",
    "this is in contrast to the estimation in the initial step where the procedure estimates the projection of the mean function onto the model space @xmath219 at ( [ modelspace ] ) .",
    "the reason is that the residuals from the initial step include not only the pure errors but also the deviation of the true regression function from its projection onto @xmath219 .",
    "these residuals mislead estimation of the score function .",
    "we generated 500 random samples of the size @xmath220 .",
    "we used epanechnikov kernel for the regression and the gaussian density kernel for the estimation of the score function .",
    "we applied a local constant version of smooth backfitting .",
    "we took @xmath221 and @xmath222 .",
    "we set @xmath223 , @xmath224 and @xmath225 .",
    "we drew @xmath226 from @xmath227 truncated to @xmath22 ^ 2 $ ] , where @xmath228 .",
    "we generated @xmath229 for some constant @xmath230 , where @xmath231 , and @xmath232 from @xmath233 , where @xmath234 and @xmath235 .",
    "note that @xmath236 is orthogonal to the space of additive functions .",
    "we compared the gaussian profile estimator ( sam ) , given in section [ subsec : profile ] , and the profile kernel estimator ( pl ) , given in @xcite , which is for the partial linear model without the additive structure .",
    "for this , we generated @xmath25 from @xmath237 and set @xmath238 . in the case",
    "where @xmath31 , that is , @xmath232 does not enter the model , the theoretical value of the ratio of the asymptotic variance of sam to that of pl equals @xmath239 .",
    "the empirical values from our simulation study for the bandwidth pair @xmath240 that gave the best mean square error ( mse ) were @xmath241 and @xmath242 for @xmath243 and @xmath244 , respectively , which nearly coincided with the theoretical values .",
    "we tried other values of @xmath245 , but the lesson was the same . in the case where @xmath246 and @xmath247 with @xmath248 from @xmath249 truncated to @xmath22 ^ 5 $ ] and @xmath250 for @xmath251 , we took @xmath252 and found that sam beat pl for all bandwidth choices that we tried .",
    "the gaussian profile estimator was stable while pl broke down for small bandwidths .",
    "the best mse of sam and that of pl , respectively , for various choices of the bandwidth pair @xmath240 were @xmath253 and @xmath254 for @xmath255 and @xmath256 and @xmath257 for @xmath258 .",
    "next , we compared sam with the semi - parametric efficient estimator ( asam ) .",
    "for this , we considered the case where @xmath259 and @xmath260 , and generated @xmath25 from @xmath237 , @xmath261-distribution with degree of freedom 3 , and @xmath262 .",
    "for asam , we took @xmath263 , and six different choices of @xmath202 : @xmath264 , for @xmath237 and @xmath265 errors and @xmath266 , for the gaussian mixture error .",
    "we used @xmath267 different choices for the bandwidth pair @xmath268 .",
    "figure  [ fig : eff1 ] is for the estimators of @xmath255 .",
    "each box - plot was obtained from the @xmath267 values of mse that corresponded to the @xmath267 bandwidth pairs @xmath269 . for asam",
    ", the value of @xmath202 is indicated on the horizontal scale .",
    "the figure suggests that the values of the mse of asam are far smaller than those of sam for the entire range of the bandwidth @xmath202 , under @xmath265 and the gaussian mixture error models .",
    "the box - plots for the gaussian error model are not given here since sam and asam gave similar performance .",
    "the results for @xmath258 are not reported either since they give a similar lesson .",
    "we applied the semi - parametric efficient estimators to boston housing data as an illustration .",
    "as in @xcite , we took the median price in 1,000 usd ( medv ) as the response @xmath12 .",
    "also , we chose as covariates @xmath270 , @xmath232 and @xmath271 , respectively , the eight variables lstat ( percentage values of lower status population ) , chas ( a dummy variable that takes the value @xmath272 if the tract borders charles river ; 0 otherwise ) , crim ( per capita crime rate ) , rm ( average numbers of rooms per dwelling ) , nox ( nitric oxides concentration ) , ptratio ( pupil  teacher ratios ) , dis ( weighted distances to five boston employment centers ) and tax ( full - value property tax rate per 10,000  usd ) . the logarithms of lstat , dis and tax were taken to reduce sparse areas , as in @xcite .",
    "we chose the model @xmath273 . in the data set ,",
    "there were 16 cases for which @xmath12 took the maximal value @xmath274 .",
    "these may be censored responses that one may remove from analysis .",
    "indeed , an initial analysis showed a strong asymmetry in the distribution of the residuals , which led us to exclude the 16 cases for further analysis . for additive regression , we applied local constant smooth backfitting with the epanechnikov kernel and bandwidths @xmath136 chosen by a rule of thumb .    with sam",
    ", we obtained @xmath275 and @xmath276 .",
    "their estimated standard errors were @xmath277 and @xmath278 , respectively .",
    "this suggests that @xmath279 is not strongly significant while @xmath280 is .",
    "the generalized @xmath281 was @xmath282 . for asam , in the estimation of the score function",
    ", we used a bandwidth @xmath202 that was obtained by r function ` bw.sj ( ) ` .",
    "with asam , we got @xmath283 and @xmath284 , and their estimated standard errors were @xmath285 and @xmath286 , respectively .",
    "thus , with asam , both the estimated coefficients are strongly significant .",
    "this may be an indication that a gaussian error model is not appropriate for the data set .",
    "the generalized @xmath281 was almost the same as in the analysis with sam .",
    "proof of theorem [ thm : asymprofile ] we only treat the case with local constant smooth backfitting . the case with local linear smooth backfitting can be dealt with similarly .",
    "we prove    @xmath287    write @xmath288 .",
    "the left - hand side of equation ( [ pfconv1 ] ) equals @xmath289 , where @xmath290 , @xmath291 , and @xmath292 .",
    "write @xmath293 . by theorem  [ prop : unifsbf ] ,",
    "standard techniques of kernel smoothing , integration by part and the representation of @xmath117 and @xmath294 as a solution of an integral equation with differentiable kernel ( see equation ( [ inteq ] ) ) , we have @xmath295^d}|\\delta(\\mathbf{z})|=\\mathrm{o}_p(\\delta_n ) , \\qquad \\sup_{z_j \\in[0,1 ] } \\biggl|\\frac { \\mathrm{d } } { \\mathrm{d}z_j } \\delta_j(z_j)- h_j b_{n , j}(z_j)\\biggr|=\\mathrm{o}_p(\\delta_n)\\ ] ] for some uniformly bounded non - random functions @xmath296 , where @xmath297 for some @xmath298 .",
    "these imply that @xmath299 with probability tending to one , where @xmath300 denotes a class of additive functions @xmath301 such that each @xmath302 is a real function defined on @xmath22 $ ] and satisfies @xmath303 } |g_j(t)- g_j(t')|\\leq|t - t'|$ ] .",
    "the covering number with bracketing of @xmath300 with respect to sup - norm , @xmath304}(\\eta ) \\equiv n_{[\\cdot]}(\\eta , b(\\mathbf{0},1),\\| \\cdot\\|_{\\infty})$ ] , is bounded by @xmath305 .",
    "define random functionals @xmath306 by @xmath307(g)=(x_j^i -\\eta_j(\\mathbf{z}^i))g(\\mathbf { z}^i)$ ] , and @xmath308 by @xmath309 .",
    "then , using corollary 8.8 of van de geer @xcite and the tail condition assumed in the theorem , one can show @xmath310 .",
    "let @xmath311 denote the @xmath312th element of @xmath313 .",
    "since @xmath314 , we obtain @xmath315 .",
    "one can prove @xmath316 using a truncation argument with theorem  [ prop : unifsbf ] and applying the chebyshev inequality conditioning on @xmath317 .",
    "the fact that @xmath318 follows from @xmath319 lies in @xmath320)=\\mathrm{o}(h_j)$ ] for some constant @xmath163 and theorem  [ prop : unifsbf ] .",
    "proof of theorem [ thm : asymadap ] we will show that @xmath321 . it suffices to show    @xmath322 \\varphi(\\epsilon^i)+\\mathrm{o}_p(n^{-1/2}).\\ ] ]    by theorem  [ prop : unifsbf ] and standard techniques of kernel smoothing along with assumption b3 , it holds that , uniformly over @xmath323 , @xmath324 also , using the proof of lemma  4.1 in @xcite and standard calculus , one can show @xmath325 and @xmath326 .",
    "thus , the proof of the theorem is completed if we verify @xmath327 proofs of ( [ effclaim3 ] ) and ( [ effclaim4 ] ) can be based on the following lemma , which follows from corollary 2.7.4 in @xcite and assumption b2 on @xmath205 .",
    "note that the moment condition on @xmath25 ensures the entropy bound . to state the lemma ,",
    "define @xmath328 for a set @xmath329 and a real number @xmath330 $ ] .",
    "let @xmath331 denote the @xmath332 norm with respect to the density @xmath28 .",
    "[ lemma : entropy ] assume the conditions of theorem [ thm : asymadap ]",
    ". then there exists a constant @xmath333 such that , with probability tending to one , @xmath334 , @xmath335^{1/2}(\\hat\\varphi-\\varphi_n ) \\in \\mathcal{c}_{m}^{1}(\\mathbb{r})$ ] and @xmath336 .",
    "moreover , there exist constants @xmath337 and @xmath338 such that @xmath339}(\\eta,\\mathcal{c}_{m}^{1}(\\mathbb{r}),\\| \\cdot\\|_{g } ) \\leq c_1 \\eta^{-(2-\\delta)}$ ] .",
    "research of kyusang yu was supported in part by basic science research program through the national research foundation of korea ( nrf ) funded by the ministry of education , science and technology ( 2010 - 0023488 ) .",
    "research of byeong u. park was supported by the mid - career researcher program through nrf grant funded by the mest ( no .",
    "2010 - 0017437 ) .",
    "carroll , r. , maity , a. , mammen , e. and yu , k. ( 2009 ) .",
    "efficient semiparametric marginal estimation for the partially linear additive model for longitudinal / clustered data . _ statist .",
    "biosci . _ * 1 * 1031 ."
  ],
  "abstract_text": [
    "<S> it is widely admitted that structured nonparametric modeling that circumvents the curse of dimensionality is important in nonparametric estimation . in this paper </S>",
    "<S> we show that the same holds for semi - parametric estimation . </S>",
    "<S> we argue that estimation of the parametric component of a semi - parametric model can be improved essentially when more structure is put into the nonparametric part of the model . </S>",
    "<S> we illustrate this for the partially linear model , and investigate efficiency gains when the nonparametric part of the model has an additive structure . </S>",
    "<S> we present the semi - parametric fisher information bound for estimating the parametric part of the partially linear additive model and provide semi - parametric efficient estimators for which we use a smooth backfitting technique to deal with the additive nonparametric part . </S>",
    "<S> we also present the finite sample performances of the proposed estimators and analyze boston housing data as an illustration .    </S>"
  ]
}