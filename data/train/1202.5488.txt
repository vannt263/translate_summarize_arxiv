{
  "article_text": [
    "we are interested in the following nonconvex semidefinite programming problem : @xmath1 where @xmath2 is convex , @xmath3 is a nonempty , closed convex set in @xmath4 and @xmath5 ( @xmath6 ) are nonconvex matrix - valued mappings and smooth .",
    "the notation @xmath7 means that @xmath8 is a symmetric negative semidefinite matrix .",
    "optimization problems involving matrix - valued mapping inequality constraints have large number of applications in static output feedback controller design and topology optimization , see , e.g. @xcite . especially , optimization problems with bilinear matrix inequality ( bmi ) constraints have been known to be nonconvex and np - hard  @xcite .",
    "many attempts have been done to solve these problems by employing convex semidefinite programming ( in particular , optimization with linear matrix inequality ( lmi ) constraints ) techniques  @xcite .",
    "the methods developed in those papers are based on augmented lagrangian functions , generalized sequential semidefinite programming and alternating directions .",
    "recently , we proposed a new method based on convex - concave decomposition of the bmi constraints and linearization technique @xcite .",
    "the method exploits the convex substructure of the problems .",
    "it was shown that this method can be applied to solve many problems arising in static output feedback control including spectral abscissa , @xmath9 , @xmath10 and mixed @xmath11 synthesis problems .    in this paper",
    ", we follow the same line of the work in @xcite to develop a new local optimization method for solving the nonconvex semidefinite programming problem . the main idea is to approximate the feasible set of the nonconvex problem by a sequence of inner positive semidefinite convex approximation sets .",
    "this method can be considered as a generalization of the ones in @xcite .",
    "0.1 cm _ contribution .",
    "_ the contribution of this paper can be summarized as follows :    * we generalize the inner convex approximation method in @xcite from scalar optimization to nonlinear semidefinite programming .",
    "moreover , the algorithm is modified by using a _ regularization technique _ to ensure strict descent .",
    "the advantages of this algorithm are that it is _ very simple to implement _ by employing available standard semidefinite programming software tools and _ no globalization strategy _ such as a line - search procedure is needed .",
    "* we prove the convergence of the algorithm to a stationary point under mild conditions .",
    "* we provide two particular ways to form an overestimate for bilinear matrix - valued mappings and then show many applications in static output feedback .    0.1 cm _ outline .",
    "_ the next section recalls some definitions , notation and properties of matrix operators and defines an inner convex approximation of a bmi constraint .",
    "section [ sec : alg_and_conv ] proposes the main algorithm and investigates its convergence properties .",
    "section [ sec : app ] shows the applications in static output feedback control and numerical tests .",
    "some concluding remarks are given in the last section .",
    "in this section , after given an overview on concepts and definitions related to matrix operators , we provide a definition of inner positive semidefinite convex approximation of a nonconvex set .",
    "let @xmath12 be the set of symmetric matrices of size @xmath13 , @xmath14 , and resp .",
    ", @xmath15 be the set of symmetric positive semidefinite , resp .",
    ", positive definite matrices . for given matrices @xmath16 and @xmath17 in @xmath12 , the relation @xmath18 ( resp . , @xmath19 )",
    "means that @xmath20 ( resp . , @xmath21 ) and @xmath22 ( resp . , @xmath23",
    ") is @xmath24 ( resp . , @xmath25 ) .",
    "the quantity @xmath26 is an inner product of two matrices @xmath16 and @xmath17 defined on @xmath12 , where @xmath27 is the trace of matrix @xmath28 . for a given symmetric matrix @xmath16",
    ", @xmath29 denotes the smallest eigenvalue of @xmath16 .",
    "[ de : psd_convex]@xcite a matrix - valued mapping @xmath30 is said to be positive semidefinite convex ( _ psd - convex _ ) on a convex subset @xmath31 if for all @xmath32 $ ] and @xmath33 , one has @xmath34 if holds for @xmath35 instead of @xmath36 for @xmath37 then @xmath38 is said to be _ strictly psd - convex _ on @xmath39 . in the opposite case , @xmath38 is said to be _ psd - nonconvex_. alternatively , if we replace @xmath36 in by @xmath40 then @xmath38 is said to be psd - concave on @xmath39 .",
    "it is obvious that any convex function @xmath2 is psd - convex with @xmath41 .",
    "a function @xmath42 is said to be _ strongly convex _ with parameter @xmath43 if @xmath44 is convex .",
    "the notation @xmath45 denotes the subdifferential of a convex function @xmath46 . for a given convex set @xmath39 , @xmath47 if @xmath48 and @xmath49 if @xmath50 denotes the normal cone of @xmath39 at @xmath51 .",
    "the derivative of a matrix - valued mapping @xmath38 at @xmath51 is a linear mapping @xmath52 from @xmath4 to @xmath53 which is defined by @xmath54 for a given convex set @xmath55 , the matrix - valued mapping @xmath56 is said to be differentiable on a subset @xmath16 if its derivative @xmath57 exists at every @xmath58 .",
    "the definitions of the second order derivatives of matrix - valued mappings can be found , e.g. , in @xcite .",
    "let @xmath59 be a linear mapping defined as @xmath60 , where @xmath61 for @xmath62 .",
    "the adjoint operator of @xmath8 , @xmath63 , is defined as @xmath64 for any @xmath65 . finally ,",
    "for simplicity of discussion , throughout this paper , we assume that all the functions and matrix - valued mappings are _ twice differentiable _ on their domain .",
    "let us first describe the idea of the inner convex approximation for the scalar case .",
    "let @xmath42 be a continuous nonconvex function .",
    "a convex function @xmath66 depending on a parameter @xmath67 is called a convex overestimate of @xmath68 w.r.t .",
    "the parameterization @xmath69 if @xmath70 and @xmath71 for all @xmath72 . let us consider two examples .",
    "0.1 cm _ example 1 .",
    "_ let @xmath46 be a continuously differentiable function and its gradient @xmath73 is lipschitz continuous with a lipschitz constant @xmath74 , i.e. @xmath75 for all @xmath76 .",
    "then , it is well - known that @xmath77 . therefore , for any @xmath78 we have @xmath79 with @xmath80 .",
    "moreover , @xmath81 for any @xmath51 . we conclude that @xmath82 is a convex overestimate of @xmath46 w.r.t the parameterization @xmath83 .",
    "now , if we fix @xmath84 and find a point @xmath85 such that @xmath86 then @xmath87 .",
    "consequently if the set @xmath88 is nonempty , we can find a point @xmath85 such that @xmath86 .",
    "the convex set @xmath89 is called an inner convex approximation of @xmath90 .    0.1 cm _ example 2 .",
    "_ @xcite we consider the function @xmath91 in @xmath92 .",
    "the function @xmath93 is a convex overestimate of @xmath46 w.r.t .",
    "the parameterization @xmath94 provided that @xmath95 .",
    "this example shows that the mapping @xmath96 is not always identity .",
    "let us generalize the convex overestimate concept to matrix - valued mappings .",
    "[ def : over_relaxation ] let us consider a psd - nonconvex matrix mapping @xmath97 .",
    "a psd - convex matrix mapping @xmath98 is said to be a psd - convex overestimate of @xmath38 w.r.t .",
    "the parameterization @xmath69 if @xmath99 and @xmath100 for all @xmath76 and @xmath101 in @xmath102 .    let us provide two important examples that satisfy definition [ def : over_relaxation ] .    _ example 3 .",
    "_ let @xmath103 be a bilinear form with @xmath104 , @xmath105 and @xmath106 arbitrarily , where @xmath16 and @xmath17 are two @xmath107 matrices .",
    "we consider the parametric quadratic form : @xmath108 one can show that @xmath109 is a psd - convex overestimate of @xmath110 w.r.t .",
    "the parameterization @xmath111 .    indeed , it is obvious that @xmath112 .",
    "we only prove the second condition in definition [ def : over_relaxation ] .",
    "we consider the expression @xmath113 . by rearranging this expression",
    ", we can easily show that @xmath114 .",
    "now , since @xmath104 , by @xcite , we can write : @xmath115 note that @xmath116 .",
    "therefore , we have @xmath117 for all @xmath118 and @xmath119 .",
    "_ example 4 .",
    "_ let us consider a psd - noncovex matrix - valued mapping @xmath120 , where @xmath121 and @xmath122 are two psd - convex matrix - valued mappings @xcite .",
    "now , let @xmath122 be differentiable and @xmath123 be the linearization of @xmath122 at @xmath124 .",
    "we define @xmath125 .",
    "it is not difficult to show that @xmath126 is a psd - convex overestimate of @xmath127 w.r.t .",
    "the parametrization @xmath128 .",
    "[ re : nonunique_of_bmi_app ] _ example 3 _ shows that the `` lipschitz coefficient '' of the approximating function is @xmath129 .",
    "moreover , as indicated by _ examples _ 3 and 4 , the psd - convex overestimate of a bilinear form is not unique . in practice , it is important to find appropriate psd - convex overestimates for bilinear forms to make the algorithm perform efficiently .",
    "note that the psd - convex overestimate @xmath130 of @xmath131 in _ example 3 _ may be less conservative than the convex - concave decomposition in @xcite since all the terms in @xmath130 are related to @xmath132 and @xmath133 rather than @xmath16 and @xmath17 .",
    "let us recall the nonconvex semidefinite programming problem .",
    "we denote by @xmath134 the feasible set of and @xmath135 the relative interior of @xmath136 , where @xmath137 is the relative interior of @xmath3 .",
    "first , we need the following fundamental assumption .    [ as : a1 ] the set of interior points @xmath138 of @xmath136 is nonempty .",
    "then , we can write the generalized kkt system of as follows : @xmath139 any point @xmath140 with @xmath141 is called a _ kkt point _ of , where @xmath142 is called a _ stationary point _ and @xmath143",
    "is called the corresponding lagrange multiplier .",
    "the main step of the algorithm is to solve a convex semidefinite programming problem formed at the iteration @xmath144 by using inner psd - convex approximations .",
    "this problem is defined as follows : @xmath145 here , @xmath146 is given and the second term in the objective function is referred to as a regularization term ; @xmath147 is the parameterization of the convex overestimate @xmath148 of @xmath149 .",
    "let us define by @xmath150 the solution mapping of [ eq : convx_subprob ] depending on the parameters @xmath151 .",
    "note that the problem [ eq : convx_subprob ] is convex , @xmath152 is multivalued and convex .",
    "the feasible set of [ eq : convx_subprob ] is written as : @xmath153      the algorithm for solving starts from an initial point @xmath154 and generates a sequence @xmath155 by solving a sequence of convex semidefinite programming subproblems [ eq : convx_subprob ] approximated at @xmath156 .",
    "more precisely , it is presented in detail as follows .",
    "[ alg : a1 ] * initialization . *",
    "determine an initial point @xmath157 .",
    "compute @xmath158 for @xmath6 . choose a regularization matrix @xmath159 .",
    "set @xmath160 .",
    "* iteration @xmath161 ( @xmath162 ) * perform the following steps :    * _ step 1 . _ for given @xmath156 , if a given criterion is satisfied then terminate . *",
    "_ solve the convex semidefinite program [ eq : convx_subprob ] to obtain a solution @xmath163 and the corresponding lagrange multiplier @xmath164 . *",
    "_ update @xmath165 , the regularization matrix @xmath166 ( if necessary ) .",
    "increase @xmath161 by @xmath167 and go back to step 1 .    *",
    "*    the core step of algorithm [ alg : a1 ] is step 2 where a general convex semidefinite program needs to be solved . in practice , this can be done by either implementing a particular method that exploits problem structures or relying on standard semidefinite programming software tools .",
    "note that the regularization matrix @xmath168 can be fixed at @xmath169 , where @xmath43 is sufficiently small and @xmath170 is the identity matrix .",
    "since algorithm [ alg : a1 ] generates a feasible sequence @xmath155 to the original problem and this sequence is strictly descent w.r.t .",
    "the objective function @xmath46 , _ no globalization strategy _ such as line - search or trust - region is needed .",
    "we first show some properties of the feasible set @xmath171 defined by . for notational simplicity , we use the notation @xmath172 .    [",
    "le : feasible_set ] let @xmath173 be a sequence generated by algorithm [ alg : a1 ] . then :    * @xmath174 the feasible set @xmath175 for all @xmath176 . *",
    "@xmath177 it is a feasible sequence , i.e. @xmath178 .",
    "* @xmath179 @xmath180 . *",
    "@xmath181 for any @xmath182 , it holds that : @xmath183 where @xmath184 is the strong convexity parameter of @xmath46 .    for a given @xmath156",
    ", we have @xmath185 and @xmath186 for @xmath6 . thus if @xmath187 then @xmath188 , the statement a ) holds .",
    "consequently , the sequence @xmath189 is feasible to which is indeed the statement b ) .",
    "since @xmath163 is a solution of [ eq : convx_subprob ] , it shows that @xmath190 .",
    "now , we have to show it belongs to @xmath191 . indeed , since @xmath192 by definition [ def",
    ": over_relaxation ] for all @xmath6 , we conclude @xmath193 .",
    "the statement c ) is proved .",
    "finally , we prove d ) . since @xmath163 is the optimal solution of [ eq : convx_subprob ]",
    ", we have @xmath194 for all @xmath187 .",
    "however , we have @xmath195 due to c ) . by substituting @xmath196 in the previous inequality",
    "we obtain the estimate d ) .",
    "now , we denote by @xmath197 the lower level set of the objective function .",
    "let us assume that @xmath198 is continuously differentiable in @xmath199 for any @xmath67 .",
    "we say that the _ robinson qualification _",
    "condition for [ eq : convx_subprob ] holds at @xmath124 if @xmath200 for @xmath6 . in order to prove the convergence of algorithm [ alg : a1",
    "] , we require the following assumption .    [ as : a2 ] the set of kkt points of is nonempty . for a given @xmath67 , the matrix - valued mappings @xmath198 are continuously differentiable on @xmath199 .",
    "the convex problem [ eq : convx_subprob ] is solvable and the robinson qualification condition holds at its solutions .",
    "we note that if algorithm 1 is terminated at the iteration @xmath161 such that @xmath201 then @xmath156 is a stationary point of .",
    "[ th : convergence ] suppose that assumptions a.[as : a1 ] and a.[as : a2 ] are satisfied .",
    "suppose further that the lower level set @xmath199 is bounded .",
    "let @xmath202 be an infinite sequence generated by algorithm [ alg : a1 ] starting from @xmath157 .",
    "assume that @xmath203 .",
    "then if either @xmath46 is strongly convex or @xmath204 for @xmath182 then every accumulation point @xmath205 of @xmath206 is a kkt point of . moreover ,",
    "if the set of the kkt points of is finite then the whole sequence @xmath207 converges to a kkt point of .",
    "first , we show that the solution mapping @xmath150 is _",
    "closed_. indeed , by assumption a.[as : a2 ] , [ eq : convx_subprob ] is feasible .",
    "moreover , it is strongly convex .",
    "hence , @xmath208 , which is obviously closed .",
    "the remaining conclusions of the theorem can be proved similarly as ( * ? ? ?",
    "* theorem 3.2 . ) by using zangwill s convergence theorem @xcite of which we omit the details here .",
    "[ rm : conclusions ] note that the assumptions used in the proof of the closedness of the solution mapping @xmath209 in theorem [ th : convergence ] are weaker than the ones used in ( * ? ? ?",
    "* theorem 3.2 . ) .",
    "in this section , we present some applications of algorithm [ alg : a1 ] for solving several classes of optimization problems arising in static output feedback controller design .",
    "typically , these problems are related to the following linear , time - invariant ( lti ) system of the form : @xmath210 where @xmath211 is the state vector , @xmath212 is the performance input , @xmath213 is the input vector , @xmath214 is the performance output , @xmath215 is the physical output vector , @xmath216 is state matrix , @xmath217 is input matrix and @xmath218 is the output matrix . by using a static feedback controller of the form @xmath219 with @xmath220 ,",
    "we can write the closed - loop system as follows : @xmath221 the stabilization , @xmath9 , @xmath222 optimization and other control problems of the lti system can be formulated as an optimization problem with bmi constraints .",
    "we only use the psd - convex overestimate of a bilinear form in _ example 3 _ to show that algorithm [ alg : a1 ] can be applied to solving many problems ins static state / output feedback controller design such as :    * sparse linear static output feedback controller design ; * spectral abscissa and pseudospectral abscissa optimization ; * @xmath223 optimization ; * @xmath224 optimization ; * and mixed @xmath225 synthesis .",
    "these problems possess at least one bmi constraint of the from @xmath226 , where @xmath227 , where @xmath118 and @xmath28 are matrix variables and @xmath228 is a affine operator of matrix variable @xmath28 . by means of _ example",
    "3 _ , we can approximate the bilinear term @xmath229 by its psd - convex overestimate . then using schur s complement to transform the constraint @xmath230 of the subproblem [ eq : convx_subprob ] into an lmi constraint @xcite .",
    "note that algorithm [ alg : a1 ] requires an interior starting point @xmath231 . in this work ,",
    "we apply the procedures proposed in @xcite to find such a point .",
    "now , we summary the whole procedure applying to solve the optimization problems with bmi constraints as follows :    [ scheme : a1 ]   + _ step 1 . _",
    "find a psd - convex overestimate @xmath232 of @xmath233 w.r.t .",
    "the parameterization @xmath234 for @xmath235 ( see _ example 1 _ ) .",
    "+ _ step 2 .",
    "_ find a starting point @xmath157 ( see @xcite ) .",
    "+ _ step 3 .",
    "_ for a given @xmath156 , form the convex semidefinite programming problem [ eq : convx_subprob ] and reformulate it as an optimization with lmi constraints .",
    "+ _ step 4 . _",
    "apply algorithm [ alg : a1 ] with an sdp solver to solve the given problem .",
    "now , we test algorithm [ alg : a1 ] for three problems via numerical examples by using the data from the comp@xmath236ib library @xcite .",
    "all the implementations are done in matlab 7.8.0 ( r2009a ) running on a laptop intel(r ) core(tm)i7 q740 1.73ghz and 4 gb ram .",
    "we use the yalmip package @xcite as a modeling language and sedumi 1.1 as a sdp solver @xcite to solve the lmi optimization problems arising in algorithm [ alg : a1 ] at the initial phase ( phase 1 ) and the subproblem [ eq : convx_subprob ] .",
    "the code is available at http://www.kuleuven.be/optec/software/bmisolver .",
    "we also compare the performance of algorithm [ alg : a1 ] and the convex - concave decomposition method ( ccdm ) proposed in @xcite in the first example , i.e. the spectral abscissa optimization problem . in the second example",
    ", we compare the @xmath10-norm computed by algorithm [ alg : a1 ] and the one provided by hifoo @xcite and penbmi @xcite .",
    "the last example is the mixed @xmath237 synthesis optimization problem which we compare between two values of the @xmath238-norm level .",
    "we consider an optimization problem with bmi constraint by optimizing the spectral abscissa of the closed - loop system @xmath239 as @xcite : @xmath240 here , matrices @xmath216 , @xmath217 and @xmath218 are given .",
    "matrices @xmath241 and @xmath220 and the scalar @xmath242 are considered as variables .",
    "if the optimal value of is strictly positive then the closed - loop feedback controller @xmath219 stabilizes the linear system @xmath243 .    by introducing an intermediate variable @xmath244 , the bmi constraint in the second line of",
    "can be written @xmath245 .",
    "now , by applying scheme [ scheme : a1 ] one can solve the problem by exploiting the sedumi sdp solver @xcite . in order to obtain a strictly descent direction ,",
    "we regularize the subproblem [ eq : convx_subprob ] by adding quadratic terms : @xmath246 , where @xmath247 .",
    "algorithm [ alg : a1 ] is terminated if one of the following conditions is satisfied :    * the subproblem [ eq : convx_subprob ] encounters a numerical problem ; * @xmath248 ; * the maximum number of iterations , @xmath249 , is reached ; * or the objective function of is not significantly improved after two successive iterations , i.e. @xmath250 for some @xmath251 and @xmath252 , where @xmath253 .",
    "we test algorithm [ alg : a1 ] for several problems in comp@xmath236ib and compare our results with the ones reported by the _ convex - concave decomposition method _",
    "( ccdm ) in @xcite .    -0.45",
    "cm    .computational results for in comp@xmath254ib [ cols= \" < , > , > , > , > , > , > , > , > , > \" , ]     here , @xmath225 are the @xmath223 and @xmath224 norms of the closed - loop systems for the static output feedback controller , respectively . with @xmath255 , the computational results show that algorithm [ alg : a1 ] satisfies the condition @xmath256 for all the test problems",
    ". the problems ac11 and ac12 encounter a numerical problems that algorithm [ alg : a1 ] can not solve . while , with @xmath257 , there are @xmath258 problems reported infeasible , which are denoted by `` - '' .",
    "the @xmath224-constraint of three problems ac11 and nn8 is active with respect to @xmath257 .",
    "we have proposed a new iterative procedure to solve a class of nonconvex semidefinite programming problems .",
    "the key idea is to locally approximate the nonconvex feasible set of the problem by an inner convex set .",
    "the convergence of the algorithm to a stationary point is investigated under standard assumptions .",
    "we limit our applications to optimization problems with bmi constraints and provide a particular way to compute the inner psd - convex approximation of a bmi constraint .",
    "many applications in static output feedback controller design have been shown and two numerical examples have been presented . note that this method can be extended to solve more general nonconvex sdp problems where we can manage to find an inner psd - convex approximation of the feasible set . this is also our future research direction ."
  ],
  "abstract_text": [
    "<S> in this work , we propose a new local optimization method to solve a class of nonconvex semidefinite programming ( sdp ) problems . the basic idea is to approximate the feasible set of the nonconvex sdp problem by inner positive semidefinite convex approximations via a parameterization technique . </S>",
    "<S> this leads to an iterative procedure to search a local optimum of the nonconvex problem . </S>",
    "<S> the convergence of the algorithm is analyzed under mild assumptions . </S>",
    "<S> applications in static output feedback control are benchmarked and numerical tests are implemented based on the data from the compl@xmath0ib library . </S>"
  ]
}