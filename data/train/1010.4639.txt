{
  "article_text": [
    "an electromagnetic field analysis is one of the most complex problems of physics . and ,",
    "the modeling and simulation of electrical systems use a very large computational algorithms to solve their problems .",
    "one of this algorithms is based on the conjugate gradient ( cg ) method @xcite .",
    "the cg method is an effective technique for symmetric positive definite systems .",
    "it is suitable for systems of the form _",
    "ax = b _ , where _ a _ is a known , square , symmetric , positive - definite matrix , _ x _ is a unknown solution vector and _ b _ is a known vector .",
    "iterative methods like cg are suited for use with sparse matrices .",
    "the solution of large , sparse linear systems of equations is the single most computationally expensive step .",
    "thus , any reduction in the linear system solution time will result in a significant saving in the total process time .",
    "this need demands for algorithms and software that can be used on parallel processors .",
    "this paper describes the use of the gpu as platform to implement a cg method to solve a linear system .",
    "the sample inputs are matrix stored in symmetric csr ( compressed sparse row)@xcite format obtained from a simulation of electric machines .",
    "the algorithms use standard blas library and two implemented kernels for matrix - vector product ( spmv ) .",
    "the next section describes the gpu which is the platform used and how to program it with cuda .",
    "section 3 depicts the matrix used and how to implement the solver .",
    "a execution performance is discussed in following section .",
    "finally , some concluding remarks are made about obtained results .",
    "gpu is a manycore processor attached to a graphics card dedicated to calculating floating point operations .",
    "even if gpus are a manycore processors , their parallelism continues to scale with moore s law .",
    "it is necessary to develop application software that transparently scales its parallelism .",
    "cuda ( compute unified device architecture)@xcite is a parallel programming model and software environment designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as c.      the gpu devotes more transistors to data processing rather data caching and flow control .",
    "this is the reason why the gpu is specialized for compute intensive .",
    "nvidia gpu , more precisely , is composed of array of sm(streaming multiprocessors ) , each one is equipped with 8 scalar cores ( the sp or streaming processors ) , 16834 32-bit registers , and 16 kb of high - bandwidth low - latency memory shared for up to 1024 co - resident threads .",
    "gpus such as the nvidia geforce gtx 280 contain 30 multiprocessors , so 30k threads can be created for a certain task .",
    "further , each multiprocessor executes groups , called _ warps _ , of 32 threads simultaneously .      in the nvidia gpu memory model ,",
    "there are per - thread local , per - block shared , and device memory which comprehends global , constant , and texture memories . shared memory can be only accessed by threads in the same block . because it is on chip , the shared memory space is much faster than the local and global memory spaces .",
    "but only 16 kb of shared memory are available on each sm .",
    "cuda is a c language extension developed by nvidia to facilitate writing programs on gpus .",
    "it allows the programmer to define c functions , called _",
    "kernels _ , that , when called , are executed n times in parallel by n different _ cuda threads _",
    ", as opposed to only once like regular c functions .",
    "one of the main features of cuda is the provision of a linear algebra library(cublas ) and an fast fourier transform library ( cufft ) @xcite .",
    "the next section describes the implementation of cg on the gpu and the use of cublas library .",
    "the conjugate gradient was applied to 30880x30880 symmetric sparse matrix @xmath0 with 449798 nonzero double precision elements .",
    "it is stored in full or symmetric csr format files which include the unknown elements vector _ x _ and the result vector _ b_. the cg algorithm used to implement the program is below .",
    "init variables @xmath1 @xmath2 _ break _ * if * _ convergence _",
    "@xmath3 @xmath4 @xmath5    all the internal loop operations are executed on gpu . the scalar product and _ axpy _ functions on lines 3,5,7 use cublas functions .",
    "since blas is only for dense matrix , naturally it is necessary to create a _ cuda kernel _ for sparse matrix .",
    "for that reason , a spmv algorithm for matrix - vector multiplication should be implemented to execute the @xmath6 product on line 2 .",
    "let @xmath7 , where @xmath8 is the main diagonal part of @xmath0 , @xmath9 is its strictly lower triangular part and @xmath10 is its strictly upper triangular part . since @xmath0 is symmetric , @xmath11 .",
    "thus , @xmath0 can be stored in csr format of @xmath12 .",
    "three algorithms were implemented in this work .",
    "the first one is a trivial solution of @xmath13 in which each thread executes @xmath14+= a[i-1]*x[ja[i-1]-1]$ ] , where _ tid _ is the thread identify , and [ _ i_,_ja _ ] are obtained from vectors of the stored matrix in compressed form .",
    "the two other solutions explore the symmetry characteristic .",
    "the aim is to cut down the time and memory allocation cost .",
    "the figure [ fig_algo ] shows the procedure of algorithm conception .",
    "the _ kernels _ are composed of blocks and each block has certain threads .",
    "only one kernel can execute in same time on same device and there are a few blocks in execution simultaneously in each sm .",
    "sequentially the _ kernel _ 1(scsr ) and _ kernel _",
    "2(scsc ) are executed and they take care of @xmath15 and @xmath10 respectively . to avoid writing conflicts on @xmath16 vector in global memory , the _ kernel _ 2 calls _ atomic functions _ and that allows just one thread write in a memory address at given moment . as the number of elements of a column or a row can be greater than the number of threads in a block ,",
    "it is necessary that the algorithm calculates the quantity of elements for each thread .",
    "a double precision version of cg was created on a contemporary conventional cpu and this one was used as a reference to calculate the speed - up .",
    "two other versions were developed on the gpu .",
    "the first one is a single precision version which was tested on a nvidia tesla c870 card .",
    "the second one is a double precision version which was executed on a nvidia geforce gtx 280 .",
    "the matrix reading from files takes time to be done properly . approximately 6ms in our tests . because this operation is basic for all experiments , it is not necessary to evaluate it .",
    "it is important to examine the _ scalar product , axpy , and spmv _ execution time because these functions are called repetitively in each loop interaction .",
    "yet , all the time that the interactive cg loop takes .",
    "the table [ table1 ] shows the operations and their respective spent time values .",
    "the application not use the _ sym spmv _",
    ", its times are there just for benchmarking .",
    "r|rrr + operation & cpu time & single prec .",
    "c870 & double prec .",
    "gtx280 +   + dotprod & 0.290ms & 0.529ms & 0.450ms + axpy & 0.109ms & 0.014ms & 0.008ms + spmv & 5.966ms & 0.013ms & 0.005ms + spmv(sym ) & - & 0.040ms & 0.035ms +   + cg/ # int & 2392ms/328 & 400ms(5.8x)/324 & 370ms(6.4x)/323 +    [ table1 ]",
    "this approach issues the viability of using gpu on sparse matrix solvers .",
    "the obtained results allow us to evaluate two important impacts in finite - element method of maxwell s equations using conjugate gradient algorithm . the first one is concerned to speed gain . on the gpu , the cg arrives faster than on the cpu due its parallel architecture .",
    "the second one is the impact of storing symmetric matrix in memory . although it can store in memory almost half of the matrix , the performance of algorithm decreases as shown in table of speed - up results",
    "this is due the repetitive way that the _ kernel _ computes and accumulates the @xmath13 result .",
    "however , both cases contribute significantly to accelerate the computation of basis functions ."
  ],
  "abstract_text": [
    "<S> nowadays , several industrial applications are being ported to parallel architectures . in fact , these platforms allow acquire more performance for system modelling and simulation . in the electric machines area , there are many problems which need speed - up on their solution . </S>",
    "<S> this paper examines the parallelism of sparse matrix solver on the graphics processors . </S>",
    "<S> more specifically , we implement the conjugate gradient technique with input matrix stored in csr , and symmetric csr and csc formats . </S>",
    "<S> this method is one of the most efficient iterative methods available for solving the finite - element basis functions of maxwell s equations . </S>",
    "<S> the gpu ( graphics processing unit ) , which is used for its implementation , provides mechanisms to parallel the algorithm . </S>",
    "<S> thus , it increases significantly the computation speed in relation to serial code on cpu based systems . </S>"
  ]
}