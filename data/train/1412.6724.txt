{
  "article_text": [
    "compressive sensing ( cs )  simultaneously acquires and compresses signals via random projections , and recovers the signals if there exists a basis or dictionary in which the signals can be expressed sparsely @xcite .",
    "recently , the application of cs  has been extended from signal recovery to parameter estimation through the design of parametric dictionaries ( pds ) that contain signal observations for a sampling of the parameter space @xcite .",
    "the resulting connection between parameter estimation and sparse signal recovery has made it possible for compressive parameter estimation to be implemented via standard cs recovery algorithms , where the dictionary coefficients obtained from signal recovery can be interpreted by matching the nonzero coefficient locations with the parameter estimates .",
    "this cs approach has been previously formulated for landmark compressive parameter estimation  problems , including localization and bearing estimation @xcite , time delay estimation @xcite , and frequency estimation ( also known as line spectral estimation ) @xcite .",
    "unfortunately , since only in the contrived case when the unknown parameters are all contained in the sampling set of the parameter space can the pd - based compressive parameter estimation  be perfect , dense sampling of the parameter space is needed to improve the parameter estimation resolution @xcite .",
    "this dense sampling introduces highly coherent elements in the pd , and previous approaches to address the resulting coherence issue need to carefully set the value of the maximum allowable coherence among the chosen pd  elements in the sparse approximations  @xcite . in practice",
    ", this approach restricts the minimum separation between any two parameters that can be observed simultaneously in a signal , and can potentially exclude a large class of observable signals from consideration .    additionally , the guarantee of bounded error between the true and the estimated coefficient vectors , measured via the euclidean distance in almost all existing methods , has a very limited impact on the performance of compressive parameter estimation .",
    "in contrast , the earth mover s distance ( emd ) @xcite is a very attractive option to measure the error of pd - based parameter estimation due to the fact that the emd  of two sparse pd  coefficient vectors is indicative of the parameter estimation error when the entries of the pd  coefficient vectors or the pd  elements are sorted by the value of the corresponding parameters .    in this paper , we propose a new method for compressive parameter estimation  that uses pds  and sparsity and replaces the hard thresholding operator with @xmath0-median clustering .",
    "this modification is motivated by the fact that @xmath0-median clustering solves the emd - sparse approximation problem @xcite .",
    "we also provide an analysis of the connection between the emd  between a pair of true and estimated coefficient vectors and the corresponding parameter estimation error .",
    "some of our contributions can be detailed as follows .",
    "first , we theoretically show that the emd  between the sparse pd  coefficient vectors provides an upper bound for the parameter estimation error , motivating the use of emd - based algorithms to minimize the parameter estimation error .",
    "second , we formulate theorems that provide performance guarantees for pd - based parameter estimation when clustering methods are used ; these guarantees refer to the correlation function of the parametric signal model , which measures the magnitude of the inner product between signal observations corresponding to different parameters .",
    "third , we analyze the effect of the decay of the parametric signal s correlation function on the performance of clustering parameter estimation methods , and relate these guarantees to the performance of the clustering methods under cs  compression and signal noise .",
    "finally , we introduce and analyze the joint use of thresholding and clustering methods to address performance loss resulting from compression and noise .",
    "although this paper focuses on the parameter estimation problem with @xmath1-dimensional parameters , our work can be easily extended to @xmath2-d and higher - dimensional parameter estimation .",
    "previous works have considered the use of the emd  to measure the sparse recovery error @xcite , but have not focused on its application with pds  and the resulting impact on parameter estimation performance .",
    "furthermore , these previous works require a specially tailored cs  measurement matrix and assume that sparsity is present in the canonical representation .",
    "finally , the recovery algorithm is implemented via optimization , in contrast to the greedy algorithm that is proposed in this paper , and we are not aware of prior work that leverages the previously known connection between emd  and @xmath0-median clustering in sparse recovery algorithms .",
    "this paper is organized as follows .",
    "we provide a summary of cs  and compressive parameter estimation  and the issues present in existing work in section  [ section : background ] . in section  [ section : cpe ] , we present and analyze the use of emd  and clustering methods for pd - based parameter estimation ; furthermore , we formulate and analyze an algorithm for pd - based sparse approximation in the emd  sense that employs @xmath0-median clustering and provides increased accuracy for parameter estimation . in section  [ section : experiments ] , we present numerical simulations that verify our results for the clustering method in the example applications of time delay estimation and frequency estimation .",
    "finally , we provide a discussion and conclusions in section  [ section : conclusion ] .",
    "compressive sensing ( cs ) has emerged as a technique integrating sensing and compression for signals that are known to be sparse or compressible in some basis .",
    "a discrete signal @xmath3 is @xmath0-sparse in a basis or a frame @xmath4 when the signal can be represented by the basis or frame as @xmath5 with @xmath6 , where the @xmath7 `` norm '' @xmath8 counts the number of nonzero entries .",
    "the dimension - reducing measurement matrix @xmath9 compresses the signal @xmath10 to obtain the measurements @xmath11 .",
    "though , in general , it is ill - posed to recover the signal @xmath10 from the measurements @xmath12 when @xmath13 since @xmath14 has a nontrivial null space , cs  theory shows that it is possible to recover the signal from a small number of measurements when the signal is sparse and the measurement matrix satisfies the restricted isometry property @xcite , a property that has been proven in the literature for matrices with random independent entries with @xmath15  @xcite .",
    "given these conditions , the cs  recovery problem usually can be solved via optimization methods , such as basis pursuit  @xcite , or greedy algorithms , such as iterative hard thresholding @xcite , compressive sampling matching pursuit @xcite , subspace pursuit @xcite , and orthogonal matching pursuit @xcite .",
    "many of the mentioned greedy algorithms rely on the hard thresholding operator , which sets all entries of an input vector to zero except those with largest magnitudes .",
    "the sparse vector resulting from the thresholding operator provides the optimal @xmath0-sparse approximation for the input vector in the sense that the output vector has minimum euclidean distance to the input vector among all possible @xmath0-sparse vectors .",
    "while classical cs  processes signals by exploiting the fact that they can be described as sparse in some basis or frame , the locations of the nonzero entries of the coefficient vectors often have additional underlying structure .",
    "to capture this additional structure , model - based cs  replaces the thresholding operator by a corresponding structured sparse approximation operator , which , similarly , finds the optimal structured sparse approximation for an input vector in the sense that the output vector exhibits the desired structure and is closest to the input vector among all possible structured sparse vectors @xcite .",
    "parameter estimation problems are usually defined in terms of a parametric signal class , which is defined via a mapping @xmath16 from the parameter space @xmath17 to the signal space @xmath18 .",
    "the signal observed in a parameter estimation problem contains @xmath0 unknown parametric components @xmath19 , and the goal is to obtain estimates of the parameters @xmath20 from the signal @xmath10 .",
    "examples include time delay estimation , where the signal @xmath21 corresponds to a known waveform with time delay @xmath22 , and line spectral estimation , where the parameter @xmath23 and the signal @xmath24 corresponds to a complex exponential of unit norm and frequency @xmath25 .",
    "one can introduce a parametric dictionary ( pd ) as a collection of samples from the signal space @xmath26 \\subseteq \\psi(\\theta),\\ ] ] which corresponds to a set of samples from the parameter space @xmath27 . in this way",
    ", the signal can be expressed as a linear combination of the pd  elements @xmath5 when all the unknown parameters are contained in the sampling set of the parameter space , i.e. , @xmath28 for each @xmath29 .",
    "therefore , finding the unknown parameters reduces to finding the pd  elements appearing in the signal representation or , equivalently , finding the nonzero entries or support of the sparse pd  coefficient vector @xmath30 for which we indeed have @xmath31 . the search for the vector @xmath30",
    "can be performed using cs  recovery .",
    "pd - based compressive parameter estimation  can be perfect only if the parameter sample set @xmath32 is dense and large enough to contain all of the unknown parameters @xmath33 .",
    "if this stringent case is not met for some unknown parameter @xmath34 , a denser sampling of the parameter space decreases the difference between the unknown parameter @xmath34 and the nearest parameter sample @xmath35 , so that we can approximate the parametric signal @xmath36 with the parametric signal @xmath37",
    ". however , highly dense sampling increases the similarity between adjacent pd  elements and by extension the pd  coherence  @xcite , corresponding to the maximum normalized inner product of pd  elements : @xmath38 additionally , dense sampling increases the difficulty of distinguishing pd  elements and severely hampers the performance of compressive parameter estimation  @xcite .",
    "prior work addressed such issues by using a coherence - inhibiting structured sparse approximation where the resulting @xmath0 nonzero entries of the coefficient vector correspond to pd  elements that have sufficiently low coherence , i.e. , @xmath39 in order to inhibit the highly coherent pd  elements from appearing in signal representation simultaneously ; this approach is known as band exclusion @xcite .",
    "the maximum allowed coherence @xmath40 that defines the restriction on the choice of pd  elements is essential to successful performance : setting its value too large results in the selection of coherent pd  elements , while setting its value too small tightens up requirements on the minimum separation of the parameters .",
    "another issue is that existing cs  recovery algorithms commonly used in this setting can only guarantee stable recovery of the sparse pd  coefficient vectors when the error is measured by the @xmath41 norm ; in other words , the estimated coefficient vector is close to the true coefficient vector in euclidean distance .",
    "such a guarantee is linked to the core hard thresholding operation , which returns the optimal sparse approximation to the input vector with respect to the @xmath41 norm .",
    "however , the guarantee provides control on the performance of parameter estimation only in the most demanding case of exact recovery , when parameter estimation would be perfect .",
    "otherwise , if exact coefficient vector recovery can not be met , such a recovery guarantee is meaningless for parameter estimation since the @xmath41 norm can not precisely measure the difference between the supports of the sparse pd  coefficient vectors . for an illustrative example , consider a simple frequency estimation problem where the pd collects complex sinusoids at frequencies @xmath42 and there is only one unknown frequency @xmath25 .",
    "in other words , the coefficient vector is @xmath43 , where @xmath44 denotes the canonical vector that is equal to @xmath1 at its @xmath45 entries and @xmath46 elsewhere .",
    "although two estimated coefficient vectors @xmath47 and @xmath48 have the same @xmath41 distance to the true coefficient vector @xmath30 , the frequency estimate @xmath49 from @xmath50 has smaller error than the frequency estimate @xmath51 from @xmath52 when the pd  elements are sorted so that @xmath53 .",
    "alternatively , the earth mover s distance ( emd ) has recently been used in cs  to measure the distance between coefficient vectors in terms of the similarity between their supports @xcite . in particular , the emd  between two vectors with the same @xmath54 norm optimizes the work of the flow ( i.e. , the amount of the flow and the distance of flow ) among one vector to make the two vectors equivalent . norm .",
    "nonetheless , one can add an additional cost due to norm mismatch that is equal to the mismatch times the length of the signal @xcite .",
    "] in our illustrative example , if the values of the frequency samples of the pd increase monotonically at a regular interval @xmath55 so that @xmath56 , the emds between the true coefficient vector @xmath30 and the estimated coefficient vectors @xmath50 and @xmath52 are proportional to the frequency errors @xmath57 and @xmath58 , respectively . based on the fact that the work of the flow between any two entries of a pd coefficient vector is isometric to the distance between the two corresponding parameters ,",
    "it is reasonable that the emd  between pairs of pd coefficient vectors efficiently measures the corresponding error of parameter estimation .",
    "we elaborate our study of this property in section  [ section : emd ] .",
    "cluster analysis partitions a set of data points based on the similarity information between each pair of points , which is usually expressed in terms of a distance  @xcite .",
    "clustering is the task of partitioning a set of points into different groups in such a way that the points in the same group , which is called a cluster , are more similar to each other than to those in other groups .",
    "the greater the similarity within a group is and the greater that the difference among groups is , the better or more distinct the clustering is .",
    "the goal of clustering @xmath59 points @xmath60 associated with weights @xmath61 and mutual similarities @xmath62 into @xmath0 clusters is to find the @xmath0 centroids @xmath63 of the clusters such that each cluster contains all points that are more similar ( i.e. , closer ) to their centroid than to other centroids : @xmath64 one can define a clustering quality measure as the total sum of weighted similarity between points and centroids : @xmath65 different choices of mutual similarity @xmath62 can result in different procedures to obtain the centroids  @xcite .",
    "if the square euclidean distance ( @xmath41 ) is used , i.e. , @xmath66 , then each cluster s centroid will be the mean of its elements , and so the clustering is called @xmath0-means clustering .",
    "if the manhattan distance ( @xmath54 ) is used , i.e. , @xmath67 , then each cluster s centroid will be the median of its elements , and so the clustering is called @xmath0-median clustering . in the special case where all points @xmath68 , i.e. , all the points are along a line , and the absolute value is used as a distance , i.e. , @xmath69 , the object function defined in ( [ equation : generalobject ] ) becomes @xmath70",
    "one can solve for the centroids by setting the derivative of the measure function with respect to each of the weights to zero ; the result is @xmath71 for @xmath29 , where @xmath72 returns the sign of @xmath10 . equation ( [ equation : medianproperty ] ) illustrates that the resulting centroids are the medians of the elements in each cluster and the points on the two sides of the centroids have maximally balanced weight : for each @xmath29 , @xmath73      the earth mover s distance ( emd ) between two vectors @xmath74 relies on the notion of mass assigned to each entry of the involved vectors , with the mass of each entry being equal to its magnitude .",
    "the goal of emd is to compute the lowest transfer of mass among the entries of the first vector needed in order to match the mass of the entries of the second vector .",
    "we assume that the vectors @xmath30 and @xmath75 are nonnegative and sparse , with nonzero entries @xmath76 and @xmath77 , and with supports @xmath78 and @xmath79 , respectively .",
    "@xmath80 represents the distance between the two vectors by finding the minimum sum of mass flows @xmath81 from entry @xmath82 to entry @xmath83 multiplied by the distance @xmath84 that can be applied to the first vector @xmath30 to yield the second vector @xmath75 .",
    "this is a typical linear programming problem that can be written as @xmath85      emd - optimal sparse approximation plays a crucial role in our proposed compressive parameter estimation  approach .",
    "this process makes it easy to integrate emd  into a cs  framework to formulate a new compressive parameter estimation  algorithm @xcite .",
    "assume that @xmath86 and @xmath87 is a @xmath0-element subset of @xmath88 .",
    "consider the problem of finding the @xmath0-sparse vector @xmath89 with support @xmath90 that has the smallest emd  to an arbitrary vector @xmath91 .",
    "the minimum flow work defined in the emd  is achieved if and only if the flow is active between each entry of the vector @xmath92 and its nearest nonzero entry @xmath93 of the vector @xmath75 . in other words ,",
    "the nonzero entries of @xmath75 partition the entries of @xmath92 into @xmath0 different groups .",
    "denote by @xmath94 the set of indices of the entries of @xmath92 that are matched to the nonzero entry @xmath93 of @xmath75 ; this set can be written as @xmath95 the emd  defined in ( [ equation : emd ] ) can be written as @xmath96 it is important to note that ( [ equation : emdsparse ] ) has the same formula as ( [ equation : generalobject][equation : medianobject ] ) , which is the objective function used in @xmath0-median clustering .",
    "thus , one can pose a @xmath0-median clustering problem to minimize the value of ( [ equation : emdsparse ] ) over all possible supports @xmath90 .",
    "to that end , define @xmath59 points in a one - dimensional space with weights @xmath97 and locations @xmath98 .",
    "it is easy to see that if we denote the set of centroid positions obtained by performing @xmath0-median clustering for this problem as @xmath90 , then the set @xmath90 corresponds to the support of the @xmath0-sparse signal that is closest to the vector @xmath92 when measured with the emd .",
    "one can then simply compute the sets in ( [ eq : sets ] ) and define the emd - optimal @xmath0-sparse approximation @xmath75 to the vector @xmath92 as @xmath99 for @xmath100 , with all other entries equal to zero .",
    "thus , it is computationally feasible to provide sparse approximations in the emd  sense @xcite .",
    "we wish to obtain parameter estimates such that the total errors of the unknown parameters and the estimated parameters is minimized .",
    "computing the estimation error between a set of @xmath0 one - dimensional true parameters @xmath101 and a set of @xmath0 one - dimensional estimates @xmath102 is an assignment problem that minimizes the cost of assigning each true parameter to a parameter estimate , when the cost of assigning the true parameter @xmath103 to the parameter estimate @xmath104 is the absolute distance between the two values @xmath105 .",
    "the resulting parameter estimation error  can be obtained by solving the following linear program , which is a relaxation of the formal integer program optimization ( * ? ? ?",
    "* corollary  19.2a ) : @xmath106    in words , the output @xmath107 for the optimization above encodes the matching between true parameter values and their estimates that minimizes the average parameter estimation error .    when the sampling interval of the parameter space that generates the pd is constant and equal to @xmath55 ( so that @xmath108 with @xmath109 denoting the index for the corresponding pd element @xmath110 ) , it is easy to see that for a pair of true and estimated coefficient vectors and the corresponding true and estimated parameters , the computations of the parameter estimation error  ( [ equation : pee ] ) and the emd  ( [ equation : emd ] ) are similar and obey @xmath111 .",
    "this straightforward comparison demonstrates the close relationship between the emd  and parameter estimation error , which is formally stated in the following theorem and proven in appendix  [ chapter : isometryproof ] .",
    "[ theorem : isometric ] assume that @xmath55 is the sampling interval of the parameter space that generates the pd  used for parameter estimation .",
    "if @xmath30 and @xmath75 are two @xmath0-sparse pd  coefficient vectors corresponding to two sets of parameters @xmath112 and @xmath113 , then the emd  between the two coefficient vectors provides an upper bound of the parameter estimation error  between the two sets of parameters : @xmath114 where @xmath115 is the smallest component magnitude among the nonzero entries of @xmath30 and @xmath75 .",
    "theorem  [ theorem : isometric ] clearly shows that the emd  between sparse pd  coefficient vectors provides an upper bound of the parameter estimation error  for the corresponding parameters , with a scaling factor proportional to the pd  parameter sampling rate @xmath55 . for a particular estimation problem where the pd  sampling interval is fixed",
    ", theorem  [ theorem : isometric ] gives the intuition that designing an algorithm that minimizes the emd - measured error of the coefficient vector estimation consequently will also minimize the parameter estimation error for the same case .",
    "it is worth noting from the proof of theorem  [ theorem : isometric ] that the relationship between emd  and parameter estimation error  becomes strictly linear when @xmath116 , i.e. , all component magnitudes have the same values .",
    "if the dynamic range of component magnitudes is defined as @xmath117 then @xmath118 reflects how close to equality ( [ eq : isometric ] ) can be .",
    "the dynamic range of component magnitudes is an important condition in parameter estimation , as we will show in the sequel .",
    "we follow the convention of greedy algorithms for cs , where a proxy of the coefficient vector is obtained via the correlation of the observations with the pd  elements , i.e. , @xmath119 , where @xmath120 denotes the hermitian ( conjugate transpose ) of @xmath4 .",
    "the resulting proxy vector @xmath92 can be expressed as a linear combination of shifted correlation functions .",
    "the magnitudes of these components will be proportional to the magnitude of the corresponding signal components .",
    "the correlation value between the pd  elements corresponding to parameters @xmath121 and @xmath122 is defined as @xmath123 where @xmath124 measures the difference between parameters , and we assume for simplicity that the correlation depends only on the parameter difference @xmath125 . in many parameter estimation problems , such as frequency estimation  and time delay estimation , the correlation function has bounded variation such that the cumulative correlation function , defined as @xmath126 is bounded .",
    "the cumulative correlation function is a nondecreasing function with infimum @xmath127 and supremum @xmath128 .",
    "as shown in figure  [ figure_corr ] , the correlation function @xmath129 achieves its maximum when the parameter difference @xmath130 and decreases as @xmath131 increases , finally vanishing when @xmath132 . in words ,",
    "the larger the parameter difference is , the smaller the similarity of corresponding pd  elements is . due to the even nature of the correlation function ,",
    "i.e. , @xmath133 , the cumulative correlation function is rotationally symmetric , i.e. , @xmath134 , as shown in figure  [ figure_corr ] .",
    "both figures also indicate that the correlation function for time delay estimation  decays much faster than that for frequency estimation , which is indicative of the increased difficulty for frequency estimation  with respect to time delay estimation  that will be shown in the sequel .    for convenience of analysis",
    ", we assume that the correlation function is real and nonnegative , while noting that the experimental results match our theory even when the assumption does not hold .",
    "when the signal is measured directly without cs  ( i.e. , the measurement matrix is the identity or @xmath135 ) in a noiseless setting , the observations exactly match the sparse signal and can be written as @xmath136 therefore , the proxy entries @xmath137 correspond to inner products between the observation vector @xmath12 and the pd  elements @xmath138 corresponding to the sampled parameters @xmath139 , and thus can be expressed as a linear combination of shifted correlation functions . for @xmath140 ,",
    "the @xmath141-th entry of proxy vector is @xmath142 it is easy to see that the proxy function will feature local maxima at the sampled parameter values from the pd  that are closest to the true parameter values .",
    "thus , the goal of the parameter estimation finally reduces to the search of local maxima in the proxy vector over all parameter samples represented in the pd , which often is addressed via optimal sparse approximation of the proxy .    when the correlation function has fast decay ( usually the case when the coherence of the pd  is very small ) , it is possible to find the local maxima of the proxy via the hard thresholding operator , as used in standard greedy algorithms for sparse signal recovery .",
    "however , when the correlation function decays slowly ( usually the case when the pd  elements are highly coherent ) , the thresholding operator will unavoidably focus its search around the peak of the proxy with the largest magnitude , unless additional approaches like band exclusion are implemented .",
    "in contrast , emd - optimal sparse approximation identifies the local maxima of the proxy directly by exploiting the fact that these local maxima correspond to the @xmath0-median clustering centroids , when certain conditions ( to be defined in theorem [ theorem : generalclustering ] in the next section ) are met .",
    "thus , we can propose an emd  and pd - based parameter estimation algorithm , shown as algorithm [ algorithm : clustering ] , which leverages a standard iterative , lloyd - style @xmath0-median clustering algorithm @xcite .",
    "this algorithm will repeatedly assign each entry of the input to the cluster whose median is closest to the entry and then update each median by finding the weighted median of the cluster using the balance property ( [ equation : balanceweight ] ) .",
    "pd proxy vector @xmath92 , set of pd  parameter values @xmath32 , target sparsity @xmath0 parameter estimates @xmath113 , sampled indices @xmath90 * initialize * : set @xmath143 , choose @xmath90 as a random @xmath0-element subset of @xmath144 @xmath145 for each @xmath146 @xmath147 for each @xmath148 @xmath149      there are some conditions that the signal @xmath10 should satisfy to minimize the estimation error when using algorithm [ algorithm : clustering ] .",
    "* _ minimum parameter separation : _ if two parameters @xmath22 and @xmath150 are too close to each other , the similarity of @xmath21 and @xmath151 makes it difficult to distinguish them . therefore",
    ", our first condition considers the minimum separation distance : @xmath152 * _ parameter range : _ any parameter observed should be sufficiently far away from the bounds of the parameter range .",
    "it is often convenient to restrict the feasible parameters and sampled parameters in a small range , i.e , @xmath32 is bounded . according to ( [ equation : balanceweight ] ) , which implies a balance of the proxy @xmath92 around the local maxima when using a clustering method",
    ", there will be a bias in the estimation due to the missed portion of the symmetric correlation function @xmath153 when the unknown parameter is too close to the bound of the parameter range .",
    "therefore , the condition should also consider the minimum off - bound distance @xmath154 , formally written as @xmath155 * _ dynamic range : _ if the magnitudes of some components in the signal are too small , they may be dwarfed by larger components and ignored by the greedy algorithms .",
    "thus , we need to pose an additional condition on the dynamic range of the component magnitudes as defined in ( [ equation : dynamic ] ) .    with these conditions , we can formulate the following theorem ( proven in appendix  [ chapter : clusteringproof ] ) that guarantees the performance of our clustering - based method for compressive parameter estimation .",
    "[ theorem : generalclustering ] assume that the sampling interval of the parameter space @xmath156 , and that the signal @xmath10 given in ( [ equation_signal ] ) involving @xmath0 parameters @xmath157 has a dynamic range of the component magnitudes as defined in ( [ equation : dynamic ] ) .",
    "for any allowed error @xmath158 , if the minimum separation distance ( [ equation : separation ] ) satisfies @xmath159 and the minimum off - bound distance ( [ equation : offbound ] ) satisfies @xmath160 then the estimates @xmath161 returned from algorithm [ algorithm : clustering ] have estimation error @xmath162    theorem [ theorem : generalclustering ] provides a guarantee on the parameter estimation error obtained from @xmath0-median clustering of the proxy @xmath92 for the pd  coefficients .",
    "theorem [ theorem : generalclustering ] also provides a connection between the conditions described earlier and the performance of pd - based parameter estimation , providing a guide for the design and evaluation of pds  that can achieve the required estimation error for practical problems .",
    "for example , in time delay estimation problems , instead of increasing the minimum separation distance required for recovery , one can try to design a transmitted waveform that improves the other conditions cited above ( such as one that increases the rate of decay of the cumulative correlation functions , as will be discussed in the sequel ) to achieve small estimation error .",
    "additionally , theorem [ theorem : generalclustering ] makes explicit the linear relationship between the normalized cumulative correlation for the minimum separation distance @xmath163 and the normalized cumulative correlation for the maximum observed error @xmath164 , cf .",
    "section  [ section : experiments ] .",
    "the required minimum separation distances will be dependent on the specific parameter estimation problem even when the maximum allowed error is kept constant .",
    "this illustrates the wide difference in performances between time delay estimation  and frequency estimation : the minimum separation distance required by time delay estimation  is much smaller than that of frequency estimation , due to the contrasting rates of decay of the function @xmath165 , cf .",
    "figure  [ figure_corr ] .",
    "although theorem [ theorem : generalclustering ] is derived for the case of a nonnegative real correlation function and is asymptotic on the parameter sampling interval @xmath55 , our numerical simulations in the sequel show that the predicted relationship between @xmath166 and @xmath167 are observed in practical problems of modest sizes .",
    "the addition of cs  and measurement noise make the estimation problem even harder , since in both of these cases there is a decrease in the rate of decay of the cumulative correlation function @xmath165 . when the measurement matrix @xmath14 is used to obtain the observed measurements @xmath12 from the signal @xmath10 such that @xmath168 , the proxy becomes @xmath169 only if @xmath170 can ( [ equation_phi_proxy ] ) be identical to ( [ equation_proxy ] ) . we define the compressed correlation function as @xmath171 the proxy",
    "can again be expressed as the linear combination of shifted copies of the redefined correlation function ( [ equation_induced_correlation ] ) : @xmath172    although in general we will have @xmath173 , we can use the preservation property of inner products through random projections  @xcite .",
    "that is , when @xmath14 has independent and identically distributed ( i.i.d . )",
    "random entries and sufficiently many rows , there exists a constant @xmath174 such that for all pairs @xmath175 of interest we have @xmath176 the parameter @xmath177 decays as the compression rate @xmath178 increases , and the manifolds @xmath179 we consider here are known to be amenable to large amounts of compression @xcite .",
    "such a relationship indicates that the compression can affect the correlation function and the performance of clustering methods for compressive parameter estimation .",
    "we choose to focus on simple bounds for the correlation function @xmath153 to analyze its role in the performance of emd  and pd - based parameter estimation .",
    "similarly to  @xcite , we use bounding functions to measure and control the decay rate of the correlation function @xmath153 .",
    "we approximate the correlation function @xmath180 with an exponential function @xmath181 that provides an upper bound of the actual correlation function , i.e. , @xmath182 .",
    "the performance obtained from the exponential function approximation @xmath183 provides an upper bound of the performance from the real correlation function @xmath179 . in the exponential function",
    ", @xmath184 is the parameter that controls the decay rate : the larger @xmath184 is , the faster that the correlation function decays .",
    "it is easy to see that the decay rate of the compressed correlation function ( [ equation_induced_correlation ] ) will be smaller than that of the original correlation function ( [ equation_correlation ] ) .",
    "we assume that @xmath185 and that a bound @xmath186 exists ; in this case , @xmath187 due to the fact that ( [ eq : ipp ] ) provides us with the following upper bound : @xmath188 where @xmath189 when @xmath174 .",
    "this shows that cs  reduces the decay speed of the correlation function and increases the necessary minimum separation distance and minimum off - bound distance to guarantee the preservation of parameter estimation performance .",
    "this dependence is also manifested in the experimental results of section [ section : experiments ] when the correlation function does not follow an exact exponential decay .",
    "we observe in practice that the issues with slow - decaying correlation functions arise whenever the sum of the copies of the correlation functions far from their peaks becomes comparable to the peak of any given copy .",
    "thus , one can use operators such as thresholding functions to remove this effect from appearing in algorithm [ algorithm : clustering ] .",
    "we can write a hard - thresholded version of the proxy from ( [ equation_induced_proxy ] ) as @xmath190 as demonstrated by the following theorem , proven in appendix  [ chapter : performanceproof ] , the thresholding operator reduces the required minimum separation distance for accurate estimation .    [",
    "theorem : performance ] under the setup of theorem [ theorem : generalclustering ] , assume that the correlation function defined in ( [ equation_induced_correlation ] ) is given by @xmath191 . for any allowed error @xmath158 ,",
    "if @xmath192 is the threshold given in ( [ equation_thresholding ] ) , the dynamic range given in ( [ equation : dynamic ] ) is equal to @xmath118 , and the minimum separation distance given in ( [ equation : separation ] ) satisfies @xmath193 where @xmath115 is the minimum component magnitude , then the estimates @xmath161 returned from performing algorithm [ algorithm : clustering ] on the thresholded proxy @xmath194 in  ( [ equation_thresholding ] ) have estimation error @xmath162    theorem [ theorem : performance ] extends theorem [ theorem : generalclustering ] by including the use of thresholding as a tool to combat the slow decay of the correlation function , due to an ill - posed estimation problem or the use of cs .",
    "one can also instinctively see that the presence of noise in the measurements will also slow the decay of the correlation function , which according to the theorem will require larger minimum separation or careful thresholding . in practice , the decay coefficient @xmath184 can usually be obtained by finding the minimum value such that the exponential function @xmath195 provides a tight upper bound for the correlation function @xmath129 .",
    "although theorem 3 is based on an approximation of the actual compressive parameter estimation  problem setup , our numerical results in the sequel show its validation in practical settings for time delay estimation  and frequency estimation .",
    "in order to test the performance of the clustering parameter estimation method on different problems , we present a number of numerical simulations involving time delay estimation  and frequency estimation . before detailing our experimental setups , we define the parametric signals and the parametric dictionaries ( pds ) involved in these two example applications .    for time delay estimation",
    ", the parametric signal model describes a sampled version of a chirp as shown in ( [ equation : chirp ] ) , where @xmath196 is the length of the chirp , @xmath197 is the chirp s starting frequency , @xmath198 is the chirp s frequency sweep , @xmath199 is the sampling frequency of the discrete version of the chirp , and @xmath200 samples are taken .",
    "the parameter space range goes from @xmath201 to @xmath202 .",
    "the pd  for time delay estimation  contains all chirp signals corresponding to the sampled parameters @xmath203 $ ] with sampling interval @xmath55 .",
    "@xmath204 =   \\left\\{\\begin{aligned } & \\sqrt{\\frac{2}{3 t f_s } } \\exp\\left(j 2 \\pi \\left(f_c + \\frac{nt_s-\\theta}{t }   f_a \\right)\\left(nt_s-\\theta\\right)\\right ) \\left(1 + \\cos \\left(2 \\pi \\frac{nt_s-\\theta}{t } \\right)\\right ) , & nt_s-\\theta \\in [ 0 , t],\\\\ & 0 , & \\text{otherwise . } \\end{aligned}\\right .",
    "\\label{equation : chirp}\\ ] ]    for frequency estimation , the parametric signals are the @xmath205-dimensional signals with entries @xmath204 = \\frac{\\exp\\left ( j 2 \\pi \\theta \\frac{n}{n } \\right)}{\\sqrt{n } } , \\quad n = 0 , 1 , \\dots , n-1.\\ ] ] the parameter space range goes from @xmath201 to @xmath206 . as before , the pd  for frequency estimation  contains all parametric signals corresponding to the sampled parameter @xmath207 $ ] with sampling interval @xmath55 . in both time delay estimation and frequency estimation ,",
    "the number of unknown parameters is set to @xmath208 .    in the first experiment",
    ", we illustrate the relationship between minimum separation distance and the maximum allowable error described in theorem [ theorem : generalclustering ] .",
    "we measure the performance for each minimum separation distance @xmath166 by the maximum estimation error @xmath167 over @xmath209 signals as shown in ( [ equation_signal ] ) with randomly chosen parameter values that are spaced by at least @xmath166 . for time delay estimation , the sampling step of the parameter space for all experiments is @xmath210 ( unless otherwise specified ) , so that the pd  contains observations for @xmath211 parameter samples , and we let the minimum separation @xmath212 $ ] . for frequency estimation ,",
    "the sampling step of the parameter space for all experiments is @xmath213 , so that the pd  contains observations for 10001 parameter samples , and we let the minimum separation @xmath214 $ ] .",
    "figure  [ figure_condition ] shows the normalized cumulative correlation for the maximum error @xmath164 as a function of the normalized cumulative correlation for the minimum separation distance @xmath163 for both time delay estimation  and frequency estimation ; recall that @xmath215 .",
    "the figure  also shows the relationship between the minimum separation @xmath166 and the maximum error @xmath167 without the use of the cumulative correlation function for both example cases .",
    "the approximately linear relationship between @xmath163 and @xmath164 for both the time delay estimation  and frequency estimation  cases numerically verifies the result of theorem [ theorem : generalclustering ] .",
    "the difference between the performance results as well as the relationship between @xmath216 and @xmath217 validates the conclusion that frequency estimation  requires a significantly larger minimum separation than time delay estimation . from figure  [ figure_condition ]",
    ", we know that it is impossible to get an arbitrarily small estimation error even if the minimum separation keeps increasing , as the estimation error can not be smaller than the parameter sampling step @xmath55 ( as observed in the figure ) . in fact , the figure shows that the relationship between @xmath218 and @xmath219 ends its linearity exactly when the value of the error becomes equal to the parameter sampling resolution @xmath55 for both application examples . to achieve more precise estimation ,",
    "the use of additional methods such as interpolation are needed @xcite .",
    "nonetheless , by employing the function @xmath220 ( which is dependent on the parameter estimation problem ) , we can consistently obtain a linear relationship between the minimum separation distance and the parameter estimation error across the parameter estimation problems considered ( cf .  figure  [ figure_condition ] ) .",
    "+    in the second experiment , we illustrate the application of theorem [ theorem : performance ] in the time delay estimation  problem .",
    "we vary the chirp s frequency sweep @xmath221 between @xmath222 and @xmath223 to generate different rates of decay of the correlation function and obtain the decay parameter @xmath184 as the smallest value that enables the exponential function @xmath224 to bound the correlation function @xmath225 .",
    "we then measure the performance of time delay estimation  in the same manner as before ( maximum error over 1000 randomly drawn signals ) by determining the minimum separation @xmath166 for which the maximum observed estimation error @xmath167 is equal to the pd  parameter sampling step @xmath226 .",
    "the results in figure [ figure_tde_condition ] show the reciprocal relationship between the normalized minimum separation @xmath227 and the decay parameter @xmath184 .",
    "additionally , figure [ figure_tde_condition ] shows the logarithmic relationship between the normalized minimum separation @xmath227 and the dynamic range of the component magnitudes @xmath118 when @xmath228 and @xmath229 .",
    "finally , figure [ figure_tde_condition ] shows the negative logarithm relationship between the normalized minimum separation @xmath227 and the threshold @xmath192 with @xmath230 and @xmath231 .",
    "all figures are indicative of agreement between theorem  [ theorem : performance ] and practical results .",
    "perhaps the most important application of theorem  [ theorem : performance ] focuses on the choice of threshold @xmath192 for the particular problem of interest , which can improve the performance of the clustering method in compressive parameter estimation . to deal with the problems of slow decay or large dynamic range",
    ", one can try to increase the threshold value on the proxy rather than increasing the minimum separation to improve the estimation performance .",
    "measurement vector @xmath12 , measurement matrix @xmath14 , sparsity @xmath0 , set of sampled parameters @xmath32 , threshold @xmath192 estimated signal @xmath232 , estimated parameter values @xmath113 * initialize : * @xmath233 , @xmath234 , generate pd @xmath4 from @xmath32 .",
    "@xmath235 @xmath236 @xmath237 @xmath238 @xmath239 @xmath240 @xmath241 @xmath149    our third and fourth experiments test the performance of clustering methods in compressive parameter estimation .",
    "@xmath0-median clustering is incorporated into subspace pursuit , a standard sparse recovery algorithm introduced in @xcite , by replacing each instance of hard thresholding in subspace pursuit with an instance of algorithm  [ algorithm : clustering ] . the resulting clustering subspace pursuit ( csp ) , as shown in algorithm [ algorithm : estimation ] , is compared with the band - exclusion subspace pursuit ( bsp ) used in  @xcite for time delay estimation  and frequency estimation .",
    ", @xmath242 denotes the pseudoinverse of @xmath243 . ] similarly , csp repeatedly computes the proxy for the coefficient vectors from the measurement residual and then obtains the indices of the potential parameter estimates from the thresholded proxy using algorithm [ algorithm : clustering ] . a second",
    "clustering refines the estimation after the potential estimates are merged the previous estimates in order to maintain a set of @xmath0 estimates .",
    "csp can also be armed with polar interpolation to significantly improve the estimation precision in a manner similar to band - excluded interpolating subspace pursuit ( bisp )  @xcite ; we call the resulting algorithm clustering interpolating subspace pursuit ( cisp ) .",
    "our third experiment tests the csp , bsp , cisp , and bisp algorithms on @xmath209 independent randomly generated time delay estimation  problems with minimum separation @xmath244 where cs measurements are taken under additive white gaussian noise ( awgn ) .",
    "we use a parametric dictionary with parameter space sampling step @xmath226 .",
    "the maximum allowed coherence for bsp and bisp is chosen as @xmath245 via grid search , and the threshold for csp and cisp is set as @xmath246 ( i.e. , no thresholding takes place ) .",
    "figure  [ figure_tde_performance ] shows the parameter estimation error as a function of the cs  compression rate @xmath178 when no noise is added .",
    "the results indicate that clustering - based algorithms match the performance of their band - exclusion counterparts for most compression rates , without the need to carefully tune a band exclusion parameter .",
    "additionally , figure  [ figure_tde_performance ] shows the parameter estimation error as a function of the measurement s signal - to - noise ratio ( snr ) when the compression rate @xmath247 , cf .",
    "figure  [ figure_tde_condition ] .",
    "csp and cisp are shown to achieve the same noise robustness as bsp and bisp , respectively , as their curves match almost exactly ; we emphasize that our algorithm did no need to perform careful parameter setting , since the threshold level @xmath246 .    in our fourth experiment",
    ", we repeat the third experiment on the frequency estimation  problem instead , with @xmath209 independent randomly drawn signals for each setup with minimum separation @xmath248 and parameter sampling step @xmath249 .",
    "the maximum allowed coherence for bsp and bisp is @xmath250 , and the threshold for csp and cisp is set to @xmath251 .",
    "similarly as in the third experiment , figure [ figure_fe_performance ] shows the estimation error as a function of the compression rate and the snr : csp can match the performance as bsp in this challenging problem with the proper threshold , even under the presence of compression and noise .    in the last experiment",
    ", we apply our proposed clustering - based algorithm for compressive parameter estimation  with a real - world signal .",
    "the lynx signal has @xmath252 samples and is used to test the performance of line spectral estimation algorithms in @xcite .",
    "it is well approximated by a sum of complex sinusoids with small minimum separation distance and large dynamic range among the component magnitudes .",
    "we increase the size of the signal by a factor of 10 using interpolation and obtain cs measurements of the resulting signal with a random matrix for various compression rates under several levels of measurement noise .",
    "the maximum allowed coherence in bisp is set to @xmath250 after a grid search to optimize the algorithm s performance , while the threshold level in cisp is set to @xmath246 .",
    "figure  [ figure_real ] shows the average relative estimation error between the estimates from cisp and bisp and those obtained from root music ( a line spectral estimation algorithm with high accuracy @xcite ) when applied to the full length signal .",
    "the results show that cisp without thresholding has closer performance to root music than the best configuration of bisp .",
    "in this paper , we have introduced and analyzed the relationship between the emd  applied to pd  coefficient vectors and the parameter estimation error  obtained from sparse approximation methods applied to the pd  representation .",
    "we also leveraged the relationship between emd - based sparse approximation and @xmath0-median clustering algorithms in the design of new compressive parameter estimation  algorithms .",
    "based on the relationship between the emd  and the parameter estimation error , we have analytically shown that the emd  between pd  coefficient vectors provides an upper bound of the parameter estimation error  obtained from methods that use pds  and emd . furthermore , we leveraged the known connection between emd - sparse approximation and @xmath0-median clustering , to formulate new algorithms that employ sparse approximation in terms of the emd ; we then derived three theoretical results that provide performance guarantees for emd - based parameter estimation algorithms under certain requirements for the signals observed , in contrast to existing work that does not provide similar guarantees .",
    "our experimental results show the validation of our analysis in several practical settings , and provides methods to control the effect of coherence , compression , and noise in the performance of compressive parameter estimation .",
    "in our new compressive parameter estimation  algorithms , we use @xmath0-median clustering rather than the more predominant @xmath0-means clustering to obtain the sparse approximation or the local maxima of the proxy .",
    "the main difference between @xmath0-median clustering and @xmath0-means clustering is that their criteria in ( [ equation : generalobject ] ) are different : the former uses the manhattan distance and the latter uses the squared euclidean distance .",
    "this difference prevents @xmath0-means clustering in general from being able to return the emd - optimal sparse approximation .",
    "our experiments have shown that our clustering - based algorithms for compressive parameter estimation  can achieve the same performance as those based on band exclusion .",
    "though both methods use additional parameters to improve the performance , the clustering method is preferable as it does not need to rely on parameter tuning under a noiseless setting and for simpler problems , while the band exclusion is highly dependent on the allowed coherence in all cases .",
    "the threshold level is needed only in cases where the estimation problem is particularly ill - posed , cs  has been heavily used , measurement noise is present , or in other cases where the correlation function decays slowly .",
    "as shown in figure [ figure_tde_performance ] , the clustering method without thresholding has the same performance as the band - exclusion method with optimally tuned maximum coherence .",
    "interested readers can refer to @xcite , where we have further studied the different sensitivities of these two methods on the additional parameters .",
    "atomic norm minimization has been recently proposed to apply sparsity concepts in line spectral estimation for fully sampled and subsampled signals @xcite as well as for the super - resolution problem @xcite .",
    "both our clustering method and atomic norm minimization exploit the concept of sparse representations for the signals of interest . in atomic norm minimization ,",
    "the sparsity is enforced by minimizing the atomic norm of the recovered signal , while in our method a parametric dictionary is used to obtain a sparse coefficient vector .",
    "additionally , it is not easy to extend the atomic norm minimization to cases where the observations can not be obtained via subsampling , since the equivalent semidefinite programming form of the atomic norm will not exist . on the contrary",
    ", our pd - based compressive parameter estimation  algorithm can be applied straightforwardly to these tasks by using a cs  measurement matrix @xmath14 containing only the rows of the identity matrix corresponding to the samples taken .",
    "we thank armin eftekhari and michael wakin for helpful comments and for pointing us to @xcite .",
    "we first consider the case where the two vectors @xmath30 and @xmath75 have the same @xmath54 norm , so that the standard definition of the emd applies .",
    "let @xmath253^t$ ] be the vector containing all @xmath254 that solves the optimization problem ( [ equation : emd ] ) , @xmath255 be the similarly - defined binary vector that solves the optimization problem ( [ equation : pee ] ) , and @xmath256 is a flow residual . similarly , let @xmath257 and @xmath192 be the similarly - defined vectors collecting all ground distances @xmath258 and @xmath259 . then from ( [ equation : pee ] ) and ( [ equation : emd ] ) , we have @xmath260 note that the first term in ( [ equation : errorrelationship ] ) is the value of the objective function in the optimization problem ( [ equation : emd ] ) when all entries of both @xmath30 and @xmath75 have magnitude @xmath115 .",
    "the second term corresponds to the contribution to the objective function due to magnitudes that are larger than @xmath115 .",
    "we show now that this second term is nonnegative .",
    "when the magnitude of the entry @xmath261 of @xmath30 increases from its baseline value of @xmath115 to @xmath82 , at least one of the outgoing flows @xmath81 will need to increase .",
    "this implies that the corresponding flow @xmath262 .",
    "thus we will have for such an increased flow , the residual @xmath263 .",
    "so , having shown that @xmath118 is nonnegative , we have that @xmath264 . then one can rewrite ( [ equation : errorrelationship ] ) as @xmath265 proving the theorem .",
    "the result is still valid when @xmath266 , as the added mismatch penalty further increases the emd value .",
    "asymptotically , when the sampling step of the parameter @xmath267 , the proxy defined as ( @xmath268 ) becomes a continuous function such that @xmath269 for all @xmath270 . in additional , the balanced weight properties around the cluster centroid @xmath20 , as defined in ( [ equation : balanceweight ] )",
    ", reduces to the equality @xmath271 where @xmath272 is the position function and @xmath273 is the weight function .",
    "additionally , the cumulative correlation function in ( [ equation : cumulative ] ) converges to the integral @xmath274      when the entries of the proxy @xmath92 is clustered into @xmath0 groups according to the centroids @xmath278 , as shown in algorithm [ algorithm_thresholding ] , the point @xmath279 is the upper bound for cluster @xmath109 and the lower bound for cluster @xmath280 , since it has the same distance to both centroids .",
    "we will show how large the minimum separation @xmath166 and minimum off - bound distance @xmath154 need to be such that the maximum estimation error is @xmath281 , i.e. @xmath282 .",
    "we first consider the cases @xmath283 : the @xmath284-th cluster with centroid @xmath285 includes the parameter range @xmath286 $ ] . according to the weight balance property ( [ equation : continuousbalance ] ) ,",
    "we need that the proxy function in ( [ equation : continuousproxy ] ) have the same sum over the range @xmath287 $ ] and @xmath288 $ ] , i.e. , @xmath289    since @xmath290 and @xmath291 , for @xmath292 , we obtain a lower bound of the left hand side of ( [ equation_middlecondition ] ) by repeatedly using the fact that @xmath293 is nondecreasing : @xmath294 plugging in this lower bound , we have that @xmath295 similarly , the upper bound of the left hand side of ( [ equation_middlecondition ] ) is @xmath296      next , we consider the first cluster with centroid @xmath301 , which includes the parameter range @xmath302 $ ] . from the weight balance property , we have @xmath303 if @xmath154 satisfies @xmath304 then we have the following result from ( [ equation : firstbalance ] ) : @xmath305 it can be similarly shown for the last cluster centroid that @xmath306 .    in summary , when all estimation errors are smaller than @xmath167 , it is straightforward for us to replace the @xmath281 in ( [ equation : minsep ] ) by @xmath167 to get the expected condition on @xmath166 : @xmath307      when the redefined correlation function is @xmath308 , the proxy function given in ( [ equation_induced_proxy ] ) is @xmath309 without loss of generality , assume parameter values are sorted so that @xmath310 and all component magnitudes no smaller than 1 .",
    "assume only the proxy in the parameter range @xmath311 $ ] around each @xmath150 will be preserved after thresholding with level @xmath192 .",
    "we have @xmath312 .",
    "so the proxy at @xmath313 has value equal to the threshold , i.e. , @xmath314 where @xmath315 , @xmath316 @xmath317 and @xmath318 .",
    "@xmath319 and @xmath320 satisfy @xmath321    one solution of the quadratic equation ( [ equation_upper ] ) is @xmath322 in this solution we can see that @xmath323 decreases as @xmath324 increases . the alternative solution is omitted , since @xmath323 will increase as @xmath324 increases .      in order that the solutions ( [ equation_uppersolution ] ) and ( [ equation_lowersolution ] ) are real value , @xmath324 or @xmath192 should be larger enough . only if @xmath328 can we have the relationship @xmath329 such that both ( [ equation_uppersolution ] ) and ( [ equation_lowersolution ] ) are real value .",
    "let @xmath330 $ ] be the estimated parameter for @xmath150 .",
    "asymptotically , when the sampling step of the parameter space @xmath55 goes to zero , the balance weight properties implies @xmath331 when @xmath332 , we have @xmath333 where @xmath334 , and @xmath335 after plugging ( [ equation : temp_left ] ) and ( [ equation : temp_right ] ) into ( [ equation_balance ] ) and moving all terms with @xmath336 or @xmath323 to the right side and moving other terms to the left side , we obtain @xmath337    one can show a similar result when @xmath338 and @xmath339 , so that ( [ equation_balance ] ) is reduced to @xmath340 where @xmath341 and @xmath342 with the relationship ( [ equation : max ] ) .",
    "we now show that if @xmath343 or @xmath344 as we expected , the estimation error is small .",
    "when @xmath345 , @xmath346 which requires @xmath347 due to the fact that @xmath348 .",
    "so @xmath349 since @xmath350 .",
    "this implies that @xmath351 and @xmath352 .",
    "similarly , when @xmath338 , @xmath353 and @xmath354 .",
    "v.  cevher , a.  c. gurbuz , j.  h. mcclellan , and r.  chellappa , `` compressive wireless arrays for bearing estimation , '' in _ ieee int .",
    "acoustics , speech and signal proc .",
    "( icassp ) _ , las vegas , nv , apr .",
    "2008 , pp . 24972500 .                  c.  d. austin , r.  l. moses , j.  n. ash , and e.  ertin , `` on the relation between sparse reconstruction and parameter estimation with model order selection , '' _ ieee j. sel .",
    "topics in signal proc .",
    "_ , vol .  4 , no .  3 , pp .",
    "560570 , jun . 2010 .",
    "s.  bourguignon , h.  carfantan , and j.  idier , `` a sparsity - based method for the estimation of spectral lines from irregularly sampled data , '' _ ieee j. sel .",
    "topics in signal proc .",
    "_ , vol .  1 , no .  4 , pp .",
    "575585 , dec . 2007 .",
    "k.  fyhn , h.  dadkhahi , and m.  f. duarte , `` spectral compressive sensing with polar interpolation , '' in _ ieee int .",
    "acoustics , speech and signal proc .",
    "( icassp ) _ ,",
    "vancouver , canada , may 2013 , pp . 62256229 .",
    "p.  indyk and e.  price , `` @xmath0-median clustering , model - based compressive sensing , and sparse recovery for earth mover distance , '' in _ acm symp .",
    "theory of computing ( stoc ) _ , san jose , ca , jun .",
    "2011 , pp . 627636 .",
    "d.  mo and m.  f. duarte , `` performance of compressive parameter estimation with earth mover s distance via @xmath0-median clustering , '' university of massachusetts , amherst , ma , tech . rep .",
    "2014 , available at http://arxiv.org/pdf/1412.6724 .",
    "r.  g. baraniuk , m.  davenport , r.  devore , and m.  b. wakin , `` a simple proof of the restricted isometry property for random matrices , '' _ constructive approximation _ , vol .",
    "28 , no .  3 , pp .",
    "253263 , dec .",
    "2008 .",
    "d.  l. donoho and m.  elad , `` optimally sparse representation in general ( non - orthogonal ) dictionaries via @xmath54 minimization , '' in _ proc .",
    "_ , vol . 100 , no .  5 , mar .",
    "2003 , pp . 21972202 .                      c.  ekanadham , d.  tranchina , and e.  p. simoncelli , `` recovery of sparse translation - invariant signals with continuous basis pursuit , '' _ ieee trans .",
    "inf . theory _ ,",
    "59 , no .",
    "10 , pp . 47354744 , oct ."
  ],
  "abstract_text": [
    "<S> in recent years , compressive sensing ( cs ) has attracted significant attention in parameter estimation tasks , including frequency estimation , time delay estimation , and localization . in order to use cs in parameter estimation , parametric dictionaries ( pds ) </S>",
    "<S> collect observations for a sampling of the parameter space and yield sparse representations for signals of interest when the sampling is sufficiently dense . </S>",
    "<S> while this dense sampling can lead to high coherence in the dictionary , it is possible to leverage structured sparsity models to prevent highly coherent dictionary elements from appearing simultaneously in the signal representations , alleviating these coherence issues . </S>",
    "<S> however , the resulting approaches depend heavily on a careful setting of the maximum allowable coherence ; furthermore , their guarantees applied on the coefficient recovery do not translate in general to the parameter estimation task . in this paper , we propose the use of the earth mover s distance ( emd ) , as applied to a pair of true and estimated coefficient vectors , to measure the error of the parameter estimation . </S>",
    "<S> we formally analyze the connection between the aforementioned emd  and the parameter estimation error . </S>",
    "<S> we theoretically show that the emd  provides a better - suited metric for the performance of pd - based parameter estimation than the commonly used euclidean distance . additionally , we leverage the previously described relationship between @xmath0-median clustering and emd - based sparse approximation to develop improved pd - based parameter estimation algorithms </S>",
    "<S> . finally , we present numerical experiments that verify our theoretical results and show the performance improvements obtained from the proposed compressive parameter estimation algorithms .    </S>",
    "<S> compressive sensing , parameter estimation , parametric dictionary , earth mover s distance , @xmath0-median clustering </S>"
  ]
}