{
  "article_text": [
    "for operating systems in weather forecasting , one of the challenges is to obtain the most appropriate initial conditions to ensure the best prediction from a physical - mathematical model that represents the evolution of a physical system . performing a smooth melding of data from observations and model predictions carries out a set of procedures to determine the best initial condition .",
    "atmospheric observed data are used to create meteorological fields over some spatial and/or temporal domain .",
    "data assimilation occurs when the observations and the dynamic model are combined .",
    "the analysis for the time evolution of the atmospheric flow is based on observational data and a model of the physical system , with some background information on initial condition .",
    "the analysis is useful in itself as a description of the physical system , but it can be used as an initial state for the further time evolution of the system @xcite .",
    "several techniques have been developed to identify the initial condition for numerical weather prediction ( nwp ) .",
    "these techniques are applied in models of atmospheric and oceanic dynamics , environmental and hydrological prediction , and ionosphere dynamics .",
    "the kalman filter ( kf ) @xcite is one strategy to estimate an appropriate initial condition .",
    "another strategy is to find a probability density function associated with the initial condition , characterizing the bayesian approaches @xcite .",
    "the ensemble kalman filter ( enkf ) @xcite and particle filter ( pf ) @xcite , are bayesian techniques .",
    "these modern techniques represent a computational challenge , even with the use of parallel computing with thousands of processors . indeed , the challenge is the computational cost because we are moving to higher resolution models and an exponential growth in the amount of observational data .",
    "the algorithms are constantly updated to improve their performance .",
    "one example is the version of the enkf restricted to small areas ( local ) : the local ensemble kalman filter ( lekf ) @xcite .",
    "the computational challenge to the techniques of data assimilation , lies in the size of matrices involved in operational nwp models , currently running under order of millions of equations ( equivalent to full matrix elements of the order of @xmath2 ) .",
    "the application of artificial neural networks ( nn ) was suggested as a possible technique for data assimilation by @xcite , @xcite , and @xcite .",
    "however , @xcite ( see also @xcite ) employed the first implementation of an nn as an approach for data assimilation , with independent development from the cited authors ; they used an nn over all spatial domain . later",
    ", this approach was improved by @xcite and @xcite , they analyzed the performance of two feed - forward nn ( multilayer perceptron and radial basis function ) , and two recurrent nn ( elman and jordan ) ( see @xcite ) .",
    "@xcite introduced a modification on the nn application , in which the analysis was obtained at each grid point , instead of at all points of the domain .",
    "the modification greatly reduced the dimension of the the computational processing . continuing the investigations , @xcite evaluated the performance of an nn to emulate the particle filter and the variational method for data assimilation applied to lorenz chaotic system .",
    "the nn technique was successful for all experiments .",
    "the use of the nn in data assimilation approach does not address error estimation . in these experiments ,",
    "the nn were applied to mimic other data assimilation methods . the main advantage to using nn",
    "is the speed - up of the data assimilation process .",
    "methods using nn have shown consistent results with implementation in the simple and low - order models .",
    "this paper presents the approach based on using nn to emulate an enkf version as a method of data assimilation .",
    "the nn are applied to a nonlinear dynamical system , an atmospheric general circulation model ( agcm ) .",
    "the agcm used , is the simplified parameterizations primitive - equation dynamics ( speedy ) model .",
    "the method is tested with synthetic conventional data , simulating measurements from surface stations ( data at each 6 hours on a day ) and upper - air soundings ( data at each 12 hours on a day ) .",
    "the application of nn produces a significant reduction for the computational effort compared to letkf .",
    "the goal to use the nn approach is to achieve a better computational performance with similar quality for the prediction , i.e. , an computational efficient process of atmospheric data assimilation ( the analysis ) .",
    "the technique uses nn to implement the function : @xmath3 where @xmath4 is the data assimilation process , @xmath5 are the observations , @xmath6 is a model forecast ( simulated ) , often called the first guess , and @xmath7 is the analysis field with innovation that represents the correction to the model .    generally , the observational data used in operational data assimilation are conventional data and satellite data .",
    "the conventional data include surface observations and upper - air observations , such as balloon soundings .",
    "global operational satellite data are taken and processed in real time to all earth surface . though small in number ,",
    "the conventional data are very important in the meteorological data assimilation .",
    "then , we apply that synthetic data type in our experiment .",
    "the experiment was conducted using the speedy model @xcite , which is a 3d global atmospheric model , with simplified physics parameterization by @xcite .",
    "the spatial resolution considered is t30l7 for the spectral method .",
    "the grid of synthetic observations seeks to reproduce the stations of world meteorological organization ( wmo ) of rawinsonde observations .",
    "this spatial grid points are employed in this numerical experiment . here , a set of nn _ multilayer perceptron _ ( mlp )",
    "[ see @xcite ] is employed to emulate the letkf .",
    "the letkf technique was used as the reference analysis .",
    "more information about letkf can be obtained from @xcite and @xcite .",
    "our paper shows that the analysis computed by the nn has the same quality as the analysis produced by letkf , which was analyzed by expert meteorologists , see @xcite and @xcite .",
    "an nn is a computational system with parallel and distributed processing that has the ability to learn and store experimental knowledge .",
    "an nn is composed of simple processing units that compute certain mathematical functions ( usually nonlinear ) .",
    "an nn consists of interconnected artificial neurons or nodes , which are inspired by biological neurons and their behaviour .",
    "the neurons are connected to others to form a nn , which is used to model relationships between artificial neurons .",
    "each artificial neuron is constituted by one or more inputs and one output .",
    "the neuron process is nonlinear , parallel , local , and adaptable .",
    "each neuron has a function to define outputs , associated with a learning rule .",
    "the neuron connection stores a nonlinear weighted sum , called a weight . in nn processing , the inputs are multiplied by weights ; these results summarized then go through the activation function .",
    "this function activates or inhibits the next neuron .",
    "mathematically , we can describe the @xmath8 input with the following form : @xmath9 where @xmath10 are the inputs ; @xmath11 are the synaptic weights ; @xmath12 is the output of linear combination ; @xmath13 is the activation function , and @xmath14 is the @xmath15 neuron output , @xmath16 is number of neurons ( fig .",
    "1 ) .    a feed - forward network , which processes in one direction from input to output",
    ", it has a layered structure .",
    "the first layer of an nn is called the input layer , the intermediary layers are called hidden layers , and the last layer is called the output layer .",
    "the number of layers and the quantity of neurons in each is determined by the nature of the problem . in most applications , a feed - forward",
    "nn with a single layer of hidden units is used with a sigmoid activation function , such as the hyperbolic tangent function for the units @xmath17    there is two distinct phases in using an nn : the training phase ( learning process ) and the run phase ( activation or generalization ) . the training phase of the nn consists of an iterative process for adjusting the weights for the best performance of the nn in establishing the mapping of input and target vector pairs .",
    "the learning algorithm is the set of procedures for adjusting the weights .",
    "the single pass through the entire training set in one process in called an epoch .",
    "testing of the verification set follows each epoch , the iterative process continues or stops after verification of defined criteria , which can be minimum error of mapping or number of epochs .",
    "once the nn is trained ( the process is stopped ) , the weights are fixed , and the nn is ready to receive new inputs ( different from training inputs ) for which it calculates the corresponding outputs .",
    "this phase is called the generalization . each connection ( after training ) has an associated weight value that stores the knowledge represented in the experimental problem and considers the input received by each neuron of that nn .    neural network designs or architectures are dependent upon the learning strategy adopted [ see haykin @xcite ] .",
    "the multilayer perceptron ( mlp ) is the nn architecture used in this study ; which the interconnections between the inputs and the output layer have at least one intermediate layer of neurons , a hidden layer [ @xcite , @xcite ] .",
    "neural networks can solve nonlinear problems if nonlinear activation functions are used for the hidden and/or the output layers .",
    "the use of units with nonlinear activation functions generalizes the delta rule .",
    "developed by @xcite , the delta rule is a version of the least mean square ( lms ) method . for a given input vector @xmath18 ,",
    "the output vector @xmath19 is compared to the target answer @xmath20 . if the difference is near zero , no learning takes place ; if the difference is not near zero , the weights are adjusted to reduce this difference .",
    "the purpose is to minimize the output errors by adjusting the nn weights , @xmath21 , using the delta rule algorithm , summarized as follows :    1 .   compute the error function @xmath22 , defining the distance between the target and the result : @xmath23 ^ 2 $ ] 2 .",
    "compute the gradient of the error function @xmath24 , defining which direction should move in weight space to reduce the error , and @xmath25 .",
    "3 .   select the learning rate @xmath26 which specifies the step size taken in the weight space of update equation ; 4 .   update the weight for the epoch @xmath27 :",
    "@xmath28 , where @xmath29 .",
    "one epoch or training step is a set of update weights for all training patterns .",
    "5 .   repeat step[4 ] until the nn error function , reach the required precision .",
    "this precision is a defined parameter that stops the iterative process .",
    "the overall idea is to treat the nn as a function of the weights @xmath21 ( eq .  [ rdelta ] ) , instead of the inputs .",
    "the goal is to minimize the error between the actual output @xmath14 ( or @xmath19 ) and the target output ( @xmath30 ) ( or @xmath20 ) of the training data . for each ( input / output )",
    "training pair , the delta rule determines the direction you need to be adjusted to reduce the error . by taking short steps",
    ", we can find the best direction for the entire training . to consider a set of ( input and target ) pairs of vectors @xmath31 , where @xmath32 is the number of patterns ( input elements ) , and one output vector @xmath33^t$ ] , a mlp performs a complex mapping @xmath34 parameterized by the synaptic weights @xmath35 , and the functions @xmath13 that provide the activation for the neuron .",
    "the set of procedures to adjust the weights is the learning algorithm .",
    "_ back - propagation _ , a well - known learning scheme , is generally used for the mlp training ( it performs the delta rule ( the algorithm above ) .",
    "back - propagation training is a supervised learning .",
    ", e.g. the adjustments to the weights are conducted by back propagating , or considering the difference between the nn calculated output and the target output ( considered the supervisor ) .",
    "@xcite , and @xcite reviewed applications of nn in environmental science including in atmospheric sciences .",
    "they reviewed some nn concepts and some applications , they presented others estimation methods and its applications .",
    "the nn applications , generally , are on function approximation of modeling of nonlinear transfer functions , and pattern classifications .",
    "@xcite included brief introductions of mlp and the back - propagation algorithm ; they showed a review for applications in the atmospheric sciences , looking at prediction of air - quality , surface ozone concentration , dioxide concentrations , severe weather , etc . , and pattern classifications applications in remote sensing data to obtain distinction between clouds and ice or snow",
    ". applications on classification of atmospheric circulation patterns , land cover and convergence lines from radar imagery , and classification of remote sensing data using nn , were also presented in hsieh ( 2009 ) and haupt et al .",
    "( 2009 ) .",
    "the speedy computer code is an agcm developed to study global - scale dynamics and to test new approaches for nwp .",
    "the dynamic variables for the primitive meteorological equations are integrated by the spectral method in the horizontal at each vertical level ( see @xcite ) .",
    "the model has a simplified set of physical parameterization schemes that are similar to realistic weather forecasting numerical models .",
    "the goal of this model is to obtain computational efficiency while maintaining characteristics similar to the state - of - the - art agcm with complex physics parameterization @xcite .    according to  @xcite",
    ", the speedy model simulates the general structure of global atmospheric circulation fairly well , and some aspects of the systematic errors are similar to many errors in the operational agcms .",
    "the package is based on the physical parameterizations adopted in more complex schemes of the agcm , such as convection ( simplified diagram of mass flow ) , large - scale condensation , clouds , short - wave radiation ( two spectral bands ) , long  wave radiation ( four spectral waves ) , surface fluxes of momentum , energy ( aerodynamic formula ) , and vertical diffusion .",
    "details of the simplified physical parameterization scheme can be found in  @xcite .",
    "the boundary conditions of the speedy model includes topographic height and land - sea mask , which are constant .",
    "sea surface temperature ( sst ) , sea ice fraction , surface temperature in the top soil layer , moisture in the top soil layer , the root - zone layer , snow depth , all of which are specified by monthly means .",
    "bare - surface albedo , and fraction of land - surface covered by vegetation , are specified by annual - mean fields .",
    "the lower boundary conditions such as sst are obtained from the ecmwf s reanalysis in the period 1981 - 90 . the incoming solar radiation flux and the boundary conditions ( sst , etc . ) , are updated daily .",
    "the speedy model is a hydrostatic model in sigma coordinates , @xcite also describes the vorticity - divergence transformation scheme .",
    "the speedy model is global with spectral resolution t30l7 ( horizontal truncation of 30 numbers of waves and seven levels ) , corresponding to a regular grid with 96 zonal points ( longitude ) , 48 meridian points ( latitude ) , and 7 vertical pressures levels ( 100 , 200 , 300 , 500 , 700 , 850 , and 925 hpa ) .",
    "the prognostic variables for the model input and output are the absolute temperature ( @xmath36 ) , surface pressure ( @xmath37 ) , zonal wind component ( @xmath38 ) , meridional wind component ( @xmath39 ) , and an additional variable and specific humidity ( @xmath40 ) .",
    "the analysis is the best estimate of the state of the system based on the optimizing criteria and error estimates . the probabilistic state space formulation and the requirement for updating information when",
    "new observations are encountered are ideally suited to the bayesian approach .",
    "the bayesian approach is a set of efficient and flexible monte carlo methods for solving the optimal filtering problem . here",
    ", one attempts to construct the posterior probability density function ( pdf ) of the state using all available information , including the set of received observations .",
    "since this pdf embodies all available statistical information , it may be considered as a complete solution to the estimation problem .    in the field of data assimilation , there are only few contributions in sequential estimation ( enkf or pf filters ) .",
    "the enkf was first proposed by @xcite and was developed by @xcite and @xcite .",
    "it is related to particle filters in the context that a particle is identified as an ensemble member .",
    "enkf is a sequential method , which means that the model is integrated forward in time and whenever observations are available ; these enkf results are used to reinitialise the model before the integration continues .",
    "the enkf originated as a version of the extended kalman filter ( ekf ) [ @xcite ] .",
    "the classical kf method by @xcite is optimal in the sense of minimizing the variance only for linear systems and gaussian statistics .",
    "analysis perturbations are added to run the ensemble forecasts .",
    "@xcite added gaussian white noise to run the same forecast for each member of the ensemble in letkf .",
    "the enkf is a monte carlo integration that governs the evolution of the pdf , which describes the _ a priori _ state , the forecast and error statistics . in the analysis step , each ensemble member is updated according to the kf scheme and replaces the covariance matrix by the sampled covariance computed from the ensemble forecasts .",
    "@xcite first applied the enkf to an atmospheric system .",
    "they applied an ensemble of model states to represent the statistical model error .",
    "the scheme of analysis acts directly on the ensemble of model states when observations are assimilated .",
    "the ensemble of analysis is obtained by assimilation for each member of the reference model .",
    "several methods have been developed to represent the modeling error covariance matrix for the analysis applying the enkf approach ; the local ensemble transform kalman filter ( letkf ) is one of them .",
    "@xcite proposed the letkf scheme as an efficient upgrade of the local ensemble kalman filter ( lekf ) .",
    "the lekf algorithm creates a close relationship between local dimensionality , error growth , and skill of the ensemble to capture the space of forecast uncertainties , formulated with the enkf scheme ( e.g. , @xcite ) . in addition",
    ", @xcite describes the theoretical foundation of the operational practice of using small ensembles , for predicting the evolution of uncertainties in high - dimension operational nwp models .",
    "the letkf scheme is a model - independent algorithm to estimate the state of a large spatial temporal chaotic system @xcite .",
    "the term  local  refers to an important feature of the scheme : it solves the kalman filter equations locally in model grid space .",
    "a kind of ensemble square root filtering @xcite,@xcite , in which the analysis ensemble members are constructed by a linear combination of the forecast ensemble members .",
    "the ensemble transform matrix , composed of the weights of the linear combination , is computed for each local subset of the state vector independently , which allows essentially parallel computations .",
    "the local subset depends on the error covariance localization @xcite .",
    "typically a local subset of the state vector contains all variables at a grid point .",
    "the letkf scheme first separates a global grid vector into local patch vectors with observations .",
    "the basic idea of letkf is to perform analysis at each grid point simultaneously using the state variables and all observations in the region centred at given grid point.the local strategy separates groups of neighbouring observations around a central point for a given region of the grid model .",
    "each grid point has a local patch ; the number of local vectors is the same as the number of global grid points @xcite .",
    "the algorithm of enkf follows the sequential assimilation steps of classical kalman filter , but it calculates the error covariance matrices as described bellow :    each member of the ensemble gets its forecast @xmath41 where @xmath27 is the total members at time @xmath42 , to estimate the state vector @xmath43 of the reference model . the ensemble is used to calculate the mean of forecasting @xmath44 @xmath45 therefore , the model error covariance matrix is : @xmath46 the analysis must determine a state estimate and the covariance error matrix @xmath47 , but also an ensemble with the appropriate sample analyses mean @xmath48 @xmath49    important properties of the letkf include the following :    * solve the estimation problem independently at each local region ; * update the estimate of the current state and also its uncertainty ; * assimilate all data at once ; * can deal with observations from the nonlinear functions of the state vector ; * interpolate observations in time as well as in space ( full 4d assimilation scheme ) ; * set local region size and ensemble size ( free parameters only ) ; * be model independent ( no adjoints ! ) ; * can estimate model parameters errors as well as initial conditions .",
    "the letkf code has been applied to a low - dimensional agcm speedy model @xcite , a realistic model according @xcite .",
    "the letkf scheme was also applied in : the agcm for the earth simulator by @xcite and the japan meteorological agency operational global and mesoscale models by@xcite ; the regional ocean modeling system by @xcite ; the global ocean model known as the geophysical fluid dynamics laboratory ( gfdl ) by @xcite ; and gfdl mars agcm by @xcite .",
    "the nn configuration for this experiment is a set of multilayer perceptrons , hereafter referred to as mlp - nn , with the following characteristics :    * two input nodes , one node for the meteorological observation vector and the other for the 6-hours forecast model vector .",
    "the vectors values represent individual grid points for a single variable with a correspondent observation value . * one output node for the analysis vector results . in the training algorithm ,",
    "the mlp - nn computes the output and compared it with the analysis vector of letkf results ( the target data , but not a node for the nn ) .",
    "the vectors represent individual analysis values for one grid point .",
    "* one hidden layer with eleven neurons . * the hyperbolic tangent ( eq . [ tgh ] ) as the activation function ( to guarantee the nonlinearity for results ) . *",
    "learning rate @xmath26 is @xmath50 * training stops when the error reaches @xmath51 or after 5000 epochs , which criterion first occurs .    on the present paper ,",
    "the nn configuration ( number of layers , nodes per layer , activation function , and learning rate parameter ) was defined by empirical tests .",
    "care must be taken in specifying the number of neurons .",
    "too many can lead the nn to memorize the training data ( over fitting ) , instead of extracting the general features that allow the generalization .",
    "too few neurons may force the nn to spend too much time trying to find an optimal representation and thus wasting valuable computation time .",
    "the automatic configuration of nn is a recent topic of a discussion by @xcite .",
    "one strategy used to collect data and to accelerate the processing of the mlp - nn training was to divide the entire globe into six regions : for the northern hemisphere , @xmath52 n and three longitudinal regions of @xmath53 each ; for the southern hemisphere , @xmath52 s and three longitudinal regions of @xmath53 each .",
    "this division is based on the size of the regions , but the number of observations is distinct , as illustrated by fig .",
    "this regional division is applied only for the mlp - nn ; the letkf procedures are not modified .",
    "the mlp - nn were developed with a set of thirty nn ( six regions with five prognostic variables ( @xmath54 _ u , v , t _ , and _ q _ ) .",
    "one mlp - nn with characteristics described above , was designed for each meteorological variable of the speedy model and each region .",
    "each mlp has two _ inputs _",
    "( model and observation vectors ) , one _ output _ neuron which is the _ analysis _ vector , and eleven neurons in a hidden layer .",
    "the activation function used to ensure nonlinearity of the problem is the hyperbolic tangent ( eq .  [ tanh ] ) , and learning rate parameter is defined to each variable set of networks , and the training scheme is the back - propagation algorithm .",
    "the mlp - nn is designed to emulate the letkf .",
    "fortran90 codes for speedy and letkf [ originally developed by @xcite ] were adapted to create the training dataset .",
    "the upper levels and the surface covariance error matrices to run the letkf system , as well as the speedy model boundary conditions data and physical parametrizations , are the same as those used for miyoshi s experiments .",
    "the initial process to run the implementation of the model assumes that it is perfect ( initialization=0 ) ; and the speedy model t30l7 integrated for one year of spin - up , i.e. the period required for a model to reach steady state and obtains the simulated atmosphere .",
    "the integration run for 01 january 1981 until 31 december 1981 and the result was the initial condition for speedy to 01 january 1982 , the date to initiate the experiment .",
    "the output model fields , so - called  true `` model , were generated without assimilation ( each 6-hours forecast of one execution is the initial condition for the next execution ) .",
    "true '' ( or control ) model forecasts collected for model executions without observations , considered four times per day ( 0000 , 0600 , 1200 , 1800 utc ) , from 01 january 1982 through 31 december 1984 .",
    "the synthetic observations were generated , reading the true speedy model fields , and adding a random noise of standard deviation error from meteorological variables : surface pressure ( @xmath37 ) , zonal wind component ( @xmath38 ) , vertical wind component ( @xmath39 ) , absolute temperature ( @xmath36 ) , and specific humidity ( @xmath40 ) at each grid point where is located an observations .",
    "the variables were located at all grid points of the model .",
    "an observation _ mask _ was designed , adding a positive flag at grid points where the observation should be considered , the locations chosen simulate the wmo data stations observations from radiosonde ( see fig .",
    "[ fig : fig3 ] ) . except for @xmath37 observations ,",
    "the other observations are upper level with seven levels .",
    "both assimilation schemes , letkf and mlp - nn , use the same number of observations at the same grid point localization .",
    "the training process for the experiment is conducted with forecasts obtained from the speedy model and letkf analyses for the input / target vectors .",
    "the letkf analyses are executed with synthetic observations : upper - levels wind , temperature and humidity , and surface pressure at 0000 and 1200 utc ( at 12,035 points ) , and only surface observations at 0600 and 1800 utc ( at 2,075 points ) .",
    "these letkf processes generated the observations and analysis vectors , and forecasts vectors for nn inputs .",
    "the analysis vector is the target for training the mpl - nn with back - propagation algorithm .",
    "the analyses and forecasts are the average of the ensemble fields .",
    "executions of the model with the letkf data assimilation were made for the same period mentioned for the  true  model : from 01 january 1982 through 31 december 1984 .",
    "the ensemble forecasts of letkf has 30 members .",
    "the ensemble average of the forecast and analyses fields , to this training process , are obtained by running speedy model with the letkf scheme .",
    "these data are collected , initially , by dividing the globe into two regions ( northern and southern hemispheres ) , but the computational cost was high because the training process took one day for the performance to converge .",
    "next , the two regions were divided each into three regions , for a total of six regions . when we divided the globe into six regions , the training , for a set of nn , took about minutes .",
    "the training time is important because if the nn lose knowledge of the system , the re - training is necessary .",
    "then , we use this division strategy to collect the thirty input vectors ( observations , mean forecasts , and mean analyses ) at chosen grid points by the observation mask ( see above ) , during letkf process . the nn training process begin after collecting the input vectors for whole period ( three years ) .",
    "the mlp - nn data assimilation scheme has no error covariance matrices to inform the spread of different observations .",
    "therefore , to capture the influence of observations from the neighbouring region around a grid point , it is necessary to consider these observations .",
    "this calculation was based on the distance from the grid point related to observations inside a determined neighbourhood ( initially:@xmath55 ) @xmath56 @xmath57 where @xmath58 is the weighted observation , @xmath59 is the number of discrete layers considered around observation , @xmath60 , where @xmath61 is coordinate of the grid point , and the @xmath62 is the coordinate of the observation , and @xmath63 is a counter of grid points with observations around that grid point @xmath62 . if @xmath64 there is no influence to be considered .",
    "each influence observation is a new grid observation location , hereafter referred to as pseudo - observation , which adds values to the three input vectors to nn training process .",
    "then , the grid points to be considered in mpl - nn analysis is greater than grid points considered to letkf analysis , although these calculations are made without interference on letkf system .",
    "the back - propagation algorithm stops the training process using the criteria cited at item 6 in this section , after obtaining the best set of weights ; it is a function of _ smallest error _ between the mpl - nn analysis and the target analysis , ( e. g. when the root mean square error between the calculated output and the input desired vector is less than @xmath51 ) for all nn . the training process was carried out for one or two epochs for each nn .",
    "the training for a set of 30 nn took about fifteen minutes to get the fixes weights before the mlp - nn data assimilation cycle or generalization process of mlp - nn .",
    "the training was performed with combined data from january , february , and march of 1982 , 1983 , and 1984 and mlp - nn is able to perform an analysis similar to the letkf analysis in generalization process .",
    "the generalization process is indeed , the data assimilation process .",
    "the mlp - nn results a analysis field .",
    "the mlp - nn activation was entering by input values at each grid point once , with no data used in the training process .",
    "the input vectors are done at grid point where is marked with observation or pseudo - observation ( eq .  [ eq : m_pobs ] ) .",
    "the procedure was the same for all nn , but one nn for each region and each prognostic variable , has different connection weights .",
    "the set of nn has one hidden layer , with the same number of neurons for all regions .",
    "the grid points are put in the global domain to make the analysis field after generalization process of mlp - nn , e.g. the activation of the set of 30 nn results a global analysis . the regional division is only for inputting each nn activation .",
    "the mlp - nn data assimilation was performed for one - month cycle .",
    "it started at 0000 utc 01 january 1985 , generating the initial condition to speedy model and running the model to get 6-hours forecast for the next execution , i.e. , the 0000 utc cycle runs mlp - nn with observations for 0000 utc of 01 january 1985 and the 6-hours forecast from 1800 utc of 31 december 1984l , the result of mlp - nn is the initial condition of 0000 utc to run the speedy model and its result is the forecast to 0006 utc , which is used to the 0006 utc cycle , and so on .    in this experiment",
    "the mlp - nn begins the activation in 01 january 1985 and generates analyses and 6 hours forecasts up through 31 january 1985 .",
    "the input and output values of prognostic variables ( @xmath37 , _ u _ , _ v _ , _ t _ , and _ q _ ) are processed on grid points for time integrations to an intermittent forecasting and analysis cycle .",
    "two discrete layers @xmath65 around a given observation are considered : in eq .",
    "[ eq : m_pobs ] .",
    "the results show the comparison of analyses fields , generated by the mlp - nn and the letkf data assimilation schemes , and the true model field .",
    "the global surface pressure fields ( at 11 january 1985 at 1800 utc ) and differences between the analyses are shown in fig .",
    "[ fig : pres ] . the analyses fields and the differences between both , for 11 january 1985 at 1800 utc at 950 hpa and 500 hpa ,",
    "are also shown , for _ t , u , v _ and _ q _ meteorological global fields , in figs .",
    "[ fig : res2 ] - [ fig : res9 ] .",
    "these results show that the application of mlp - nn , as an assimilation system , generates analyses similar to the analyses calculated by the letkf system .",
    "sub - figure ( d ) of figs .",
    "[ fig : res2 ] - [ fig : res9 ] shows that the differences between the mlp - nn and letkf analyses are very small , we can verify the difference field of absolute temperature at 500 hpa , the differences are about @xmath66 degrees ; or the differences field of humidity at 950 hpa , are about @xmath67 .",
    "[ fig : pres ]        [ fig : res2 ]        [ fig : res3 ]        [ fig : res4 ]        [ fig : res5 ]        [ fig : res6 ]        [ fig : res7 ]        [ fig : res8 ]        [ fig : res9 ]    monthly mean of absolute temperature analyses fields were obtained ; the differences field between the analyses ( letkf and mlp - nn ) for data assimilation cycles is shown in fig .",
    "[ fig : average ]",
    ". the differences are slightly larger in some regions , such as the northeast regions of north america and south america .",
    "[ fig : average ]    the root mean square error ( rmse ) of the absolute temperature analyses to true model , are calculated by fixing a point in longitude ( @xmath68w ) for all latitude points .",
    "[ fig : medt4 ] shows the temperature rmses for the entire period of the assimilation cycle ( january 1985 ) .",
    "subfigure ( a ) for fig .",
    "[ fig : medt4 ] shows the rmse of the letkf analysis by line , and the rmse of the mlp - nn analysis by circles ; and subfigure ( b ) for fig .",
    "[ fig : medt4 ] shows the differences between letkf and mlp - nn analyses rmse .",
    "the differences are less than @xmath69",
    ".          several aspects of modeling stress computational systems and push capability requirements .",
    "these aspects include increased resolution , the inclusion of improved physics processes and concurrent execution of earth - system components - that is , coupled models ( ocean circulation , and environmental prediction , for example ) .",
    "often , real - time necessities define capability requirements . in data assimilation ,",
    "the computational requirements become much more challenging .",
    "observations from earth - orbiting satellites in operational numerical prediction models are used to improve weather forecasts . however , using this amount of data increases the computational effort . as a result",
    ", there is a need for an assimilation method able to compute the initial field for the numerical model in the operational window - time to make a prediction . at present , most of the nwp centers find it difficult to assimilate all the available data because of computational costs and the cost of transferring huge amounts of data from the storage system to the main computer memory .",
    "the data assimilation cycle has a recent forecast and the observations as the inputs for assimilation system .",
    "the latter mlp - nn system produced a analysis to initiate the actual cycle , fig .",
    "[ fig : perfor ] shows 112 cycles of data assimilation runs .",
    "this time simulation experiment is for ( 28 days ) january 1985 .",
    "there were 2,075 observations inserted at runs of 0600 and 1800 utc for surface variables and 12,035 observations inserted at runs of 0000 and 1200 utc for all upper layer variables .",
    "the letkf data assimilation cycle initiates , running the ensemble forecasts with the speedy model and each analysis produced to each member at the latter letkf cycle to result thirty 6-hours forecasts ; the second step is to compute the average of those forecasts .",
    "after , with a set of observations and the mean forecast , the letkf system is performed .",
    "the letkf cycle results one analysis to each member for the ensemble , and one average field of the ensemble analyses .",
    "the mlp - nn data assimilation cycle is composed by the reading of 6-hours .",
    "forecast of speedy model from latter cycle and reading the set of observations to the cycle time , the division of input vectors , the activation of mlp - nn and the assembly of output vectors to a global analysis field .",
    "the mlp - nn runtime measurement is magenta point of fig .",
    "[ fig : perfor ] ; it initiates after reading the 6-hours .",
    "forecast of speedy model from latter cycle ; and the set of observations , it is the time of generalization mlp - nn with dividing forecasting / observations and with gathering global analysis .",
    "the letkf runtime measurement is the blue point of fig .",
    "[ fig : perfor ] ; it initiates after reading the mean 6-hours forecast of speedy model and the set of observations .",
    "the letkf time include the results of 30 analyses and one mean ensemble analysis .",
    "the comparison in table  [ tab : tempos1 ] is the data assimilation cycles for the same observations points and the same model resolution to the same time simulations . letkf and mlp - nn executions are performed independently .    considering the total execution time of those 112 cycles simulated , the computational performance of the mlp - nn data assimilation , is better than that obtained with the letkf approach .",
    "these results show that the computational efficiency of the nn for data assimilation to the speedy model , for the adopted resolution , is 90 times faster and produces analyses of the same quality ( see table  [ tab : tempos1 ] ) .",
    "considering only the analyses execution time of those 112 data assimilation processes simulated , the computational efficiency of mlp - nn is 421 times faster than letkf process .",
    "the table  [ tab : tempos2 ] shows the mean execution time of each element to one cycle of : the letkf data assimilation method ( ensemble forecast and analysis ) and the mlp - nn method ( model forecast and analysis ) .",
    "the computational efficiency of one mlp - nn execution , keeps the relationship about speed - up , comparing with one letkf execution .",
    "( 421 times faster ) .",
    "details for this experiment can be found in @xcite .",
    "[ cols=\"<,^ , > \" , ]     [ tab : tempos1 ]",
    "in this study , we evaluated the efficiency of the mlp - nn in an atmospheric data assimilation context .",
    "the mlp - nn is able to emulate systems for data assimilation . for the present investigation ,",
    "the mlp - nn approach is used to emulate the letkf approach , which is designed to improve the computational performance of the standard enkf .",
    "the another experiments with the same methodology can be found in @xcite .",
    "the nn learned the whole process of the mathematical scheme of data assimilation through training with the letkf .",
    "the results for the mlp - nn analysis are very close to the results obtained from the letkf data assimilation for initializing the speedy model forecast , i.e. , the analyses obtained with mlp - nn for the analysis field are similar to analyses computed by the letkf , the differences between mlp - nn and letkf analyses to surface pressure fields in hpa , for example , is about ( -5 to 5 ) hpa . however , the computational performance of the set of thirty nn is better .",
    "the mlp - nn assimilation speed - up the letkf scheme .",
    "the application of the present nn data assimilation methodology is proposed at the center for weather prediction and climate studies ( centro de previso de tempo e estudos climticos - cptec / inpe ) with operational numerical global model and observations using nowadays .",
    "the authors thank dr . takemasa miyoshi and prof .",
    "dr . eugnia kalnay for providing computer routines for the speedy model and the letkf system .",
    "this paper is a contribution of the brazilian national institute of science and technology ( inct ) for climate change funded by cnpq grant number 573797/2008 - 0 e fapesp grant number 2008/57719 - 9 .",
    "author hfcv also thanks to the cnpq ( grant number 311147/2010 - 0 ) .",
    "campos velho , h. f. , vijaykumar , n. l. , stephany , s. , preto , a. j. and nowosad , a. g. : a neural network implementation for data assimilation using mpi .",
    "application of high performance computing in engineering . c. a. brebia , p. melli , and a. zanasi , eds . , wit press , southampton , section 5 , 211 - 220 , 2002 .",
    "cintra , r. s. : assimilao de dados por redes neurais artificiais em um modelo global de circulao atmosfrica .",
    "dissertation on applied computing , national institute for space research , so jos dos campos - brazil , 2010 .",
    "cintra , r. s. and campos velho , h. f. : data assimilation for satellite temperature by artificial neural network in an atmospheric general circulation model .",
    "xvii congresso brasileiro de meteorologia , gramado , rgs , brasil , september 23rd-28th,2012 .    cintra , r. s. , campos velho , h. f. and furtado , h.c .",
    ": neural network for performance improvement in atmospheric prediction systems : data assimilation .",
    "1st brics countries & 11th cbic brazilian congress on computational intelligence .",
    "location : recife , brasil .",
    " porto de galinhas  beach , september 8th-11th,2013 .",
    "furtado , h. c. : redes neurais e diferentes m  etodos de assimilao de dados em dinmica no linear .",
    "brazilian national institute for space research - master thesis in applied computation program , so jos dos campos - brazil , 2008        hrter , f. and campos velho , h. f. : recurrent and feedforward neural networks trained with cross correlation applied to the data assimilation in chaotic dynamic .",
    "revista brasileira de meteorologia , 20 , 411 - 420 , 2005 .",
    "liaqat , a. , fukuhara , m. , and takeda , t. : applying a neural collocation method to an incompletely known dynamical system via weak constraint data assimilation . monthly weather review , 131 , n.8 , 1697 - 1714 , 2003          miyoshi , t. , sato , y. and kadowaki , t. : ensemble kalman filter and 4d - var inter- comparison with the japanese operational global analysis and prediction system . monthly weather review , 138 , 2846 - 2866 , 2010        nowosad , a. , neto , a. r .. and campos velho , h. : data assimilation in chaotic dynamics using neural networks . international conference on nonlinear dynamics , chaos , control and their applications in engineering sciences , 212 - 221 , 2000    ott , e. , hunt , b. r. , szyniogh , i. , zimin , a. v. , kostelich , e. j. , corazza , m. , kalnay , e. , patil , d. j. , york , j. : a local ensemble kalman filter for atmospheric data assimilation .",
    "tellus a , 56 , 415 - 428 , 2004 .",
    "penny , s. : data assimilation of the global ocean using the 4d local ensemble transform kalman filter ( 4d - letkf ) and themodular ocean model ( mom2 ) .",
    "thesis , university of maryland , college park , maryland , usa , 2011    sambatti , s. b. m. , furtado , h. c. m. , anochi , j. a. , da luz , e .",
    "f. p. and de campos velho , h. f. : automatic configuration of an artificial neural network with application o data assimilation .",
    "xii international conference on integral methods in science and engineering imse2012 , 2012      szunyogh , y. , kostelich , e .",
    ", gyarmart , g . ,",
    "kalnay , e . ,",
    "hunt , b. , ott , e. , satterfield , e. , and yorke , j. : a local ensemble transform kalman filter data assimilation system for the ncep global model .",
    "tellus , 60a , 113 - 130 , 2007"
  ],
  "abstract_text": [
    "<S> this paper presents an approach for employing an artificial neural network ( nn ) to emulate an ensemble kalman filter ( enkf ) as a method of data assimilation . </S>",
    "<S> the assimilation methods are tested in the simplified parameterizations primitive - equation dynamics ( speedy ) model , an atmospheric general circulation model ( agcm ) , using synthetic observational data simulating localization of balloon soundings . for the data assimilation scheme , a supervised nn , </S>",
    "<S> the multilayer perceptron ( mlp - nn ) , is applied . </S>",
    "<S> the mlp - nn is able to emulate the analysis from the local ensemble transform kalman filter ( letkf ) . after the training process , </S>",
    "<S> the method using the mlp - nn is seen as a function of data assimilation . </S>",
    "<S> the nn was trained with data from first three months of 1982 , 1983 , and 1984 . a hind - casting experiment for the 1985 data assimilation cycle using mlp - nn </S>",
    "<S> was performed with synthetic observations for january 1985 . </S>",
    "<S> the numerical results demonstrate the effectiveness of the nn technique for atmospheric data assimilation . </S>",
    "<S> the results of the nn analyses are very close to the results from the letkf analyses , the differences of the monthly average of absolute temperature analyses is of order @xmath0 . </S>",
    "<S> the simulations show that the major advantage of using the mlp - nn is better computational performance , since the analyses have similar quality . </S>",
    "<S> the cpu - time cycle assimilation with mlp - nn is @xmath1 times faster than cycle assimilation with letkf for the numerical experiment .    2 </S>"
  ]
}