{
  "article_text": [
    "we begin with the likelihood model @xmath1 where @xmath2 is a dictionary of unit @xmath3-norm basis vectors , @xmath4 is a vector of unknown coefficients we would like to estimate , @xmath5 is the observed signal , and @xmath6 is noise distributed as @xmath7 ( later we consider more general likelihood models ) . in many practical situations where large numbers of features are present relative to the signal dimension , the problem of estimating @xmath8 given @xmath9 becomes ill - posed .",
    "a bayesian framework is intuitively appealing for formulating these types of problems because prior assumptions must be incorporated , whether explicitly or implicitly , to regularize the solution space .",
    "recently , there has been a growing interest in models that employ sparse priors @xmath10 to encourage solutions @xmath8 with mostly small or zero - valued coefficients and a few large or unrestricted values , i.e. , we are assuming the generative @xmath8 is a sparse vector .",
    "such solutions can be favored by using @xmath11 = \\prod_i \\exp\\left[-\\frac{1}{2 } h\\left(x_i^2\\right ) \\right],\\ ] ] with @xmath12 concave and non - decreasing on @xmath13 @xcite .",
    "virtually all sparse priors of interest can be expressed in this manner , including the popular laplacian , jeffreys , student s @xmath14 , and generalized gaussian distributions .",
    "roughly speaking , the ` more concave ' @xmath12 , the more sparse we expect @xmath8 to be .",
    "for example , with @xmath15 , we recover a gaussian , which is not sparse at all , while @xmath16 gives a laplacian distribution , with characteristic heavy tails and a sharp peak at zero .",
    "all sparse priors of the form ( [ eq : strongly_sg_prior ] ) can be conveniently framed in terms of a collection of non - negative latent variables or hyperparameters @xmath17^t$ ] for purposes of optimization , approximation , and/or inference .",
    "the hyperparameters dictate the structure of the prior via @xmath18 where @xmath19 is some non - negative function that is sometimes treated as a hyperprior , although it will not generally integrate to one . for the purpose of obtaining sparse point estimates of @xmath8 , which will be our primary focus herein ,",
    "models with latent variable sparse priors are frequently handled in one of two ways .",
    "first , the latent structure afforded by ( [ eq : convex_prior ] ) offers a very convenient means of obtaining ( possibly local ) _ maximum a posteriori _",
    "( map ) estimates of @xmath8 by iteratively solving @xmath20,\\ ] ] where @xmath21 and @xmath22 is commonly referred to as a _ type i _ estimator .",
    "examples include minimum @xmath23-norm approaches @xcite , jeffreys prior - based methods sometimes called focuss @xcite , algorithms for computing the basis pursuit ( bp ) or lasso solution @xcite , and iterative reweighted @xmath0 methods @xcite .",
    "secondly , instead of maximizing over both @xmath8 and @xmath24 as in ( [ eq : type_i_map_problem ] ) , _ type ii _",
    "methods first integrate out ( marginalize ) the unknown @xmath8 and then solve the empirical bayesian problem @xcite @xmath25 where @xmath26 and @xmath27 $ ] .",
    "once @xmath28 is obtained , the conditional distribution @xmath29 is gaussian , and a point estimate for @xmath8 naturally emerges as the posterior mean @xmath30 = \\gamma_{(ii ) } \\phi^t \\left(\\lambda i + \\phi \\gamma_{(ii ) } \\phi^t \\right)^{-1 } { { \\bm y } } .\\ ] ] pertinent examples include sparse bayesian learning and the relevance vector machine ( rvm ) @xcite , automatic relevance determination ( ard ) @xcite , methods for learning overcomplete dictionaries @xcite , and large - scale experimental design @xcite .    while initially these two approaches may seem vastly different ,",
    "both can be directly compared using a dual - space view @xcite of the underlying cost functions . in brief , this involves expressing both the type i and type ii objective solely in terms of either @xmath8 or @xmath24 as reviewed in section [ sec : dual_space_view ] .",
    "the dual - space view is advantageous for several reasons , such as establishing connections between algorithms , developing efficient update rules , or handling more general ( non - gaussian ) likelihood functions . in section [ sec : learn_lambda ] , we utilize @xmath24-space cost functions to develop a principled method for choosing the trade - off parameter @xmath31 ( which accompanies the gaussian likelihood model and essentially balances sparsity and data fit ) and demonstrate its effectiveness via simulations .",
    "section [ sec : max_sparse_estimation ] then derives a new type ii - inspired algorithm in @xmath8-space that can compute maximally sparse ( minimal @xmath32 norm ) solutions even with highly coherent dictionaries , proving a result for clustered dictionaries that previously has only been shown empirically @xcite .",
    "finally , section [ sec : gen_likelihood_models ] leverages duality to address type ii methods with generalized likelihood functions that previously were rendered untenable because of intractable integrals . in general",
    ", some tasks and analyses are easier to undertake in @xmath24-space ( section [ sec : learn_lambda ] ) , while others are more transparent in @xmath8-space ( sections [ sec : max_sparse_estimation ] and [ sec : gen_likelihood_models ] ) . here",
    "we consider both with the goal of advancing the proper understanding and full utilization of the sparse linear model .",
    "type i is based on a natural cost function in @xmath8-space , @xmath33 , while type ii involves an analogous function in @xmath24-space , @xmath34 .",
    "the dual - space view defines a corresponding @xmath24-space cost function for type i and a @xmath8-space cost function for type ii to complete the symmetry .    * _ type ii in @xmath8-space _ * : using the relationship @xmath35 as in @xcite , it can be shown that the type ii coefficients from ( [ eq : posterior_mean ] ) satisfy @xmath36 , where @xmath37 and @xmath38 this reformulation of type ii in @xmath8-space is revealing for multiple reasons ( sections [ sec : max_sparse_estimation ] and [ sec : gen_likelihood_models ] will address additional reasons in detail ) . for many applications of the sparse linear model ,",
    "the primary goal is simply a point estimate that exhibits some degree of sparsity , meaning many elements of @xmath39 near zero and a few relatively large coefficients .",
    "this requires a penalty function @xmath40 that is concave and non - decreasing in @xmath41^t$ ] . in the context of type i ,",
    "any prior @xmath10 expressible via ( [ eq : strongly_sg_prior ] ) will satisfy this condition by definition ; such priors are said to be _ strongly super - gaussian _ and will always have positive kurtosis @xcite . regarding type ii , because the associated @xmath8-space penalty ( [ eq : g_definition ] ) is represented as a minimum of upper - bounding hyperplanes with respect to @xmath42 ( and the slopes are all non - negative given @xmath43 ) , it must therefore be concave and non - decreasing in @xmath42 @xcite",
    ".    for compression , interpretability , or other practical reasons , it is sometimes desirable to have _ exactly sparse _ point estimates , with many ( or most ) elements of @xmath8 equal to exactly zero .",
    "this then necessitates a penalty function @xmath40 that is concave and non - decreasing in @xmath44^t$ ] , a much stronger condition . in the case of type i ,",
    "if @xmath45 is concave and non - decreasing in @xmath46 , then @xmath47 satisfies this condition .",
    "the type ii analog , which emerges by further inspection of ( [ eq : g_definition ] ) stipulates that if @xmath48 is a concave and non - decreasing function of @xmath24 , then @xmath49 will be a concave , non - decreasing function of @xmath50 . for this purpose",
    "it is sufficient , but not necessary , that @xmath51 be a concave and non - decreasing function .",
    "note that this is a somewhat stronger criteria than type i since the first term on the righthand side of ( [ eq : concave_requirement ] ) ( which is absent from type i ) is actually convex in @xmath24 .",
    "regardless , it is now very transparent how type ii may promote sparsity akin to type i.    the dual - space view also leads to efficient , convergent algorithms such as iterative reweighted @xmath0 minimization and its variants as discussed in @xcite .",
    "however , building on these ideas , we can demonstrate here that it also elucidates the original , widely applied update procedures developed for implementing the relevance vector machine ( rvm ) , a popular type ii method for regression and classification that assumes @xmath52 @xcite .",
    "in fact these updates , which were inspired by a fixed - point heuristic from @xcite , have been widely used for a number of bayesian inference tasks without any formal analyses or justification .",
    "the dual - space formulation can be leveraged to show that these updates are in fact executing a coordinate - wise , iterative min - max procedure in search of a saddle point . specifically we have the following result ( all proofs are in the supplementary material ) :    [ lem : rvm_mackay_updates ] the original rvm update rule from ( * ? ? ?",
    "* equation ( 16 ) ) is equivalent to a closed - form , coordinate - wise optimization of @xmath53\\ ] ] over @xmath8 , @xmath24 , and @xmath54 , where @xmath55 is the convex conjugate function @xcite of @xmath56 \\phi^t \\right|$ ] with respect to @xmath57 .    *",
    "_ type i in @xmath24-space _ * : similar methodology and the expansion of @xmath58 can be used to express the type i optimization problem in @xmath24-space , which serves several useful purposes .",
    "let @xmath59 , with @xmath60 then the type i coefficients obtained from ( [ eq : type_i_map_problem ] ) satisfy @xmath61 section [ sec : learn_lambda ] will use @xmath24-space cost functions to derive well - motivated approaches for learning the trade - off parameter @xmath31 .",
    "the trade - off parameter is crucial for obtaining good estimates of @xmath8 . in general ,",
    "if @xmath31 is too large , @xmath62 ; too small and @xmath39 is overfitted to the noise . in practice , either expensive cross - validation or some heuristic procedure is often required . however , because @xmath31 can be interpreted as a variance , it is useful to address its estimation in @xmath24-space , in which existing unknowns ( i.e. , @xmath24 ) are also variances .    * _ learning @xmath31 with type i _ * : consider the type i cost function @xmath63 . the data - dependent term can be shown to be a convex , non - increasing function of @xmath24 , which encourages each element to be large .",
    "the second term is a penalty factor that regulates the size of @xmath24 .",
    "it is here that a convenient regularizer for @xmath31 can be incorporated .",
    "this can be accomplished as follows .",
    "first we expand @xmath64 via @xmath65 , where @xmath66 denotes the @xmath67-th column of @xmath68 and @xmath69 is a column vector of zeros with a ` @xmath70 ' in the @xmath71-th location .",
    "thus we observe that @xmath31 is embedded in the data - dependent term in the exact same fashion as each @xmath72 .",
    "this motivates a penalty on @xmath31 with similar correspondence , leading to the objective @xmath73 + \\sum_{j=1}^n \\left [ \\log \\lambda + f(\\lambda ) \\right ] \\nonumber \\\\ \\hspace*{-0.2 cm } & = & { { \\bm y } } ^t \\sigma_y^{-1 } { { \\bm y } } + \\sum_{i=1}^m \\left [ \\log \\gamma_i + f(\\gamma_i ) \\right ] + n\\log \\lambda + n f(\\lambda).\\end{aligned}\\ ] ] while admittedly simple , this construction is appealing because , regardless of how each @xmath72 is penalized , @xmath31 is penalized in a proportional manner , so both @xmath24 and @xmath31 have a properly balanced chance of explaining the observed data . this is important because the optimal @xmath31 will be highly dependent on both the true noise level , _ and crucially _ , the particular sparse prior assumed @xmath10 ( as reflected by @xmath51 ) .    for analysis or implementational purposes , we may convert @xmath74 back to @xmath8-space , with @xmath31-dependency now removed .",
    "it can then be shown that solving ( [ eq : type_i_map_problem ] ) , with @xmath31 fixed to the value that minimizes ( [ eq : type_i_cost_with_lambda ] ) , is equivalent to solving @xmath75 if @xmath76 and @xmath77 minimize ( [ eq : type_i_lambda_x_space ] ) , then we can demonstrate using @xcite that the corresponding @xmath31 estimate , which also minimizes ( [ eq : type_i_cost_with_lambda ] ) , is given by @xmath78 evaluated at @xmath79 .",
    "note that if we were just performing maximum likelihood estimation of @xmath31 given @xmath76 , the optimal value would reduce to simply @xmath80 , with no influence from the prior on @xmath8 .",
    "this is a fundamental weakness .    solving ( [ eq : type_i_lambda_x_space ] ) , or equivalently ( [ eq : type_i_cost_with_lambda ] ) , can be accomplished using simple iterative reweighted least squares , or if @xmath81 is concave in @xmath82 , an iterative reweighted second - order - cone ( soc ) minimization .    *",
    "_ learning @xmath31 with type ii _ * : the same procedure can be adopted for type ii yielding the cost function @xmath83 where we note that , unlike in the type i case above , the @xmath84-based term is already naturally balanced between @xmath31 and @xmath24 by virtue of the symmetric embedding in @xmath64 .",
    "it is important to stress that this type ii prescription for learning @xmath31 is not the same as originally proposed in the literature for type ii models of this genre . in this context , @xmath19 is interpreted a hyperprior on @xmath72 , and an equivalent distribution is assumed on the noise variance @xmath31 .",
    "importantly , these assumptions leave out the factor of @xmath85 in ( [ eq : type_ii_cost_with_lambda ] ) , and so an asymmetry is created .    *",
    "_ simulation examples _ * : empirical tests help to illustrate the efficacy of this procedure .",
    "as in many applications of sparse reconstruction , here we are only concerned with accurately estimating @xmath8 , whose nonzero entries may have physical significance ( e.g. , source localization @xcite , compressive sensing @xcite , etc . ) , as opposed to predicting new values of @xmath9 .",
    "therefore , automatically learning the value of @xmath31 is particularly relevant , since cross - validation is often not possible . and",
    "@xmath31 may be completely different for any new @xmath9 , which then necessitates that we estimate both jointly .",
    "] simulations are helpful for evaluation purposes since we then have access to the true sparse generating vector .",
    "figure [ fig : empirical_results ] compares the estimation performance obtained by minimizing ( [ eq : type_i_lambda_x_space ] ) with two different selections for @xmath81 : @xmath86 , with @xmath87 and @xmath88 .",
    "data generation proceeds as follows : we create a random @xmath89 dictionary @xmath68 , with @xmath3-normalized , iid gaussian columns .",
    "@xmath8 is randomly generated with 10 unit gaussian nonzero elements .",
    "we then compute @xmath90 , where @xmath6 is iid gaussian noise producing an snr of @xmath91db . to determine what @xmath31 values lead to optimal performance we solve ( [ eq : type_i_map_problem ] ) with the appropriate @xmath81 over a range of fixed @xmath31 values ( @xmath92 to @xmath93 ) and then compute the error between @xmath8 and @xmath39 .",
    "the minimum of this curve reflects the best performance we can hope to achieve when learning @xmath31 blindly . in figure",
    "[ fig : empirical_results ] ( _ top _ ) we plot these curves for both type i methods averaged over 1000 independent trials .",
    "next we solve ( [ eq : type_i_lambda_x_space ] ) , which produces an estimate of both @xmath8 and @xmath31 .",
    "we mark with an ` * * + * * ' the learned @xmath31 versus the corresponding error of @xmath39 . in both cases",
    "the learned @xmath31 s ( averaged across trials ) perform just as well as if we knew the optimal value a priori .",
    "results using other noise levels , problem dimensions @xmath85 and @xmath94 , sparsity levels @xmath95 , and sparsity penalties @xmath81 are similar .",
    "see the supplementary material for more examples .",
    "figure [ fig : empirical_results ] ( _ bottom _ ) shows the average sparsity of estimates @xmath39 , as quantified by the @xmath32 norm @xmath96 , across @xmath31 values ( @xmath97 returns a count of the number of nonzero elements in @xmath8 ) .",
    "the ` * * + * * ' indicates the average sparsity of each @xmath39 for the learned @xmath31 as before .",
    "in general , the @xmath98 penalty produces a much sparser estimate , very near the true value of @xmath99 at the optimal @xmath31 .",
    "the @xmath0 penalty , which is substantially less concave / sparsity - inducing , still sets some elements to exactly zero , but also substantially shrinks nonzero coefficients in achieving a similar overall reconstruction error .",
    "this highlights the importance of learning a @xmath31 via a penalty that is properly matched to the prior on @xmath8 : if we instead tried to force a particular sparsity value ( in this case 10 ) , then the @xmath0 solution would be very suboptimal .",
    "finally we note that maximum likelihood ( ml ) estimation of @xmath31 performs very poorly ( not shown ) , except in the special case where the ml estimate is equivalent to solving ( [ eq : type_i_cost_with_lambda ] ) as occurs when @xmath52 ( see @xcite )",
    ". the proposed method can be viewed as adding a principled hyperprior on @xmath31 , properly matched to @xmath10 , that compensates for this shortcoming of standard ml .",
    "type ii @xmath31 estimation has been explored elsewhere for the special case where @xmath52 @xcite , which renders the factor of @xmath85 in ( [ eq : type_ii_cost_with_lambda ] ) irrelevant ; however , for other selections we have found this factor to improve performance ( not shown ) . for space considerations",
    "we have focused our attention here on type i , which has frequently been noted for not lending itself well to @xmath31 estimation ( or related parameters ) @xcite .",
    "in fact , the symmetry afforded by the dual - space perspective reveals that type i is just as natural a candidate for this task as type ii , and may be preferred in high - dimensional settings where computational resources are at a premium .",
    "with the advent of compressive sensing and other related applications , there has been growing interest in finding _ maximally sparse _ signal representations from redundant dictionaries ( @xmath100 ) @xcite .",
    "the canonical form of this problem involves solving @xmath101 while ( [ eq : l0_prob ] ) is np - hard , whenever the dictionary @xmath68 satisfies a _ restricted isometry property _ ( rip ) @xcite or a related structural assumption , meaning that each @xmath102 columns of @xmath68 are sufficiently close to orthonormal ( i.e. , mutually uncorrelated ) , then replacing @xmath32 with @xmath0 in ( [ eq : l0_prob ] ) leads to a convex problem with an equivalent global solution .",
    "unfortunately however , in many situations ( e.g. , feature selection , source localization ) these rip equivalence conditions are grossly violated , implying that the @xmath0 solution may deviate substantially from @xmath103 .",
    "an alternative is to instead replace ( [ eq : l0_prob ] ) with minimization of ( [ eq : ard_map ] ) and then take the limit as @xmath104 .",
    "( note that the extension to the noisy case with @xmath105 is straightforward , but analysis is more difficult . ) in this regime the optimization problem reduces to @xmath106 if @xmath107 is concave , then ( [ eq : ard_map_nn ] ) can be minimized using reweighted @xmath0 minimization . with initial weight vector @xmath108 ,",
    "the @xmath109-th iteration involves computing @xmath110 with @xmath52 , iterating ( [ eq : reweighted_l1 ] ) will provably lead to an estimate of @xmath103 that is as good or better than the @xmath0 solution @xcite , in particular when @xmath68 has highly correlated columns .",
    "additionally , the assumption @xmath52 leads to a closed - form expression for the weights @xmath111 .",
    "let @xmath112^{q},\\ ] ] where @xmath113 denotes a diagonal matrix with @xmath67-th diagonal entry given by @xmath114 .",
    "then @xmath111 can be computed via @xmath115 .",
    "it remains unclear however in what circumstances this type of update can lead to guaranteed improvement nor if the functions @xmath116 are even the optimal choice .",
    "we will now demonstrate that for certain selections of @xmath117 and @xmath118 , we can guarantee that reweighted @xmath0 using @xmath119 is guaranteed to recover @xmath103 exactly if @xmath68 is drawn from what we call a _ clustered dictionary model_.    _ clustered dictionary model : _ let @xmath120 denote any dictionary such that @xmath0 minimization succeeds in solving ( [ eq : l0_prob ] ) for all @xmath121 .",
    "let @xmath122 denote any dictionary obtained by replacing each column of @xmath120 with a `` cluster '' of @xmath123 basis vectors such that the angle between any two vectors within a cluster is less than some @xmath124 .",
    "we also define the cluster support @xmath125 as the set of cluster indices whereby @xmath103 has at least one nonzero element .",
    "finally , we assume that the resulting @xmath122 is such that every @xmath126 submatrix is full rank .",
    "[ thm : irl1_recovery ] for any sparse vector @xmath103 and any dictionary @xmath122 obtained from the clustered dictionary model with @xmath127 sufficiently small , reweighted @xmath0 minimization using weights @xmath128 with some @xmath129 and @xmath117 sufficiently small will recover @xmath103 exactly provided that @xmath130 , @xmath131 @xmath132 , and within each cluster @xmath133 the coefficients do not sum to zero .",
    "theorem [ thm : irl1_recovery ] implies that even though @xmath0 may fail to find the maximally sparse @xmath103 because of severe rip violations ( high correlations between groups of dictionary columns as dictated by @xmath127 lead directly to a poor rip ) , a type ii - inspired method can still be successful .",
    "moreover , because whenever @xmath0 does succeed , type ii will always succeed as well ( assuming a reweighted @xmath0 implementation ) , the converse ( rip violation leading to type ii failure but not @xmath0 failure ) can never happen .",
    "recent work from @xcite has argued that type ii may be useful for addressing the sparse recovery problem with correlated dictionaries , and empirical evidence is provided showing vastly superior performance on clustered dictionaries .",
    "however , we stress that no results proving global convergence to the correct , maximally sparse solution have been shown before in the case of structured dictionaries ( except in special cases with strong , unverifiable constraints on coefficient magnitudes @xcite ) .",
    "moreover , the proposed weighting strategy @xmath128 accomplishes this without any particular tuning to the clustered dictionary model under consideration and thus likely holds in many other cases as well .",
    "type i methods naturally accommodate alternative likelihood functions .",
    "we simply must replace the quadratic data fit term from ( [ eq : type_i_map_problem ] ) with some preferred function and then coordinate - wise optimization may proceed provided we have an efficient means of computing a weighted @xmath3-norm penalized solution .",
    "in contrast , generalizing type ii is substantially more complicated because it is no longer possible to compute the marginalization ( [ eq : type_ii_map ] ) or the posterior distribution @xmath134 .",
    "therefore , to obtain a tractable estimate @xmath135 additional heuristics are required . for example , the rvm classifier from @xcite employs a laplace approximation for this purpose ; however , it is not clear what cost function is being minimized nor rigorous properties of the estimated solutions .",
    "fortunately , the dual @xmath8-space view provides a natural mechanism for generalizing the basic type ii methodology to address alternative likelihood functions in a more principled manner . in the case of classification problems",
    ", we might want to replace the gaussian likelihood @xmath136 implied by ( [ eq : basic_prob ] ) with a multivariate bernoulli distribution @xmath137 $ ] where @xmath138 is the function @xmath139 + ( 1 - y_j ) \\log \\left [ 1 - \\sigma_j ( { { \\bm x } } ) \\right ] \\right).\\ ] ] here @xmath140 and @xmath141 $ ] , with @xmath142 denoting the @xmath71-th row of @xmath68 .",
    "this function may be naturally substituted into the @xmath8-space type ii cost function ( [ eq : ard_map ] ) giving us the candidate penalized logistic regression function @xmath143 importantly , recasting type ii classification using @xmath8-space in this way , with its attendant well - specified cost function , facilitates more concrete analyses ( see below ) regarding properties of global and local minima that were previously rendered inaccessible because of intractable integrals and compensatory approximations .",
    "moreover , we retain a tight connection with the original type ii marginalization process as follows .",
    "consider the strict upper bound on the function @xmath138 ( obtained by a taylor series approximation and a hessian bound ) given by @xmath144 where @xmath145^t$ ] with @xmath146 .",
    "this bound holds for all @xmath147 with equality when @xmath148 . using this result",
    "we obtain the lower bound on the marginal likelihood given by @xmath149 p ( { { \\bm x } } ) d { { \\bm x } } \\geq \\int \\log[- \\pi ( { { \\bm y } } , { { \\bm x } } , { { \\bm v } } ) ] p ( { { \\bm x } } ) d { { \\bm x } } $ ] . the dual - space framework can then be used to derive the following result :    [ thm : rvm_class_bound ] minimization of ( [ eq : type_ii_classifier ] ) with @xmath150 is equivalent to solving @xmath151 \\prod_i \\mathcal{n } ( { { \\bm x } } ; 0,\\gamma_i ) \\varphi ( { { \\bm \\gamma } } _ i ) d x_i\\ ] ] and then computing @xmath135 by plugging the resulting @xmath24 into ( [ eq : posterior_mean ] ) .",
    "thus we may conclude that ( [ eq : type_ii_classifier ] ) provides a principled approximation to ( [ eq : type_ii_map ] ) when a bernoulli likelihood function is used for classification purposes . in empirical tests on benchmark data sets ( see supplementary material ) using @xmath52 , it performs nearly identically to the original rvm ( which also implicitly assumes @xmath52 ) , but nonetheless provides a more solid theoretical justification for type ii classifiers because of the underlying similarities and identical generative model . but",
    "while the rvm and its attendant approximations are difficult to analyze , ( [ eq : type_ii_classifier ] ) is relatively transparent .",
    "additionally , for other sparse priors , or equivalently other selections for @xmath51 , we can still perform optimization and analyze cost functions without any conjugacy requirements on the implicit @xmath10 .",
    "[ thm : generalized_likelihood ] if @xmath107 is a concave , non - decreasing function of @xmath24 ( as will be the case if @xmath51 is concave and non - decreasing ) , then every local optimum of ( [ eq : rvm_class_bound ] ) is achieved at a solution with at most @xmath85 nonzero elements in @xmath24 and therefore @xmath135 .",
    "in contrast , if @xmath152 is convex , then ( [ eq : rvm_class_bound ] ) can be globally solved via a convex program .    despite the practical success of the rvm and related bayesian techniques , and empirical evidence of sparse solutions ,",
    "there is currently no proof that the standard variants of these classification methods will always produce exactly sparse estimates .",
    "thus theorem [ thm : generalized_likelihood ] provides some analytical validation of these types of classifiers .    finally , if we take ( [ eq : type_ii_classifier ] ) as our starting point , we may naturally consider modifications tailored to specific sparse classification tasks ( that may or may not retain an explicit connection with the original type ii probabilistic model ) .",
    "for example , suppose we would like to obtain a maximally sparse classifier , where regularization is provided by a @xmath95 penalty .",
    "direct optimization is combinatorial because of what we call the _ global zero attraction property _ : whenever any individual coefficient @xmath153 goes to zero , we are necessarily at a local minimum with respect to this coefficient because of the infinite slope ( discontinuity ) of the @xmath32 norm at zero .",
    "however , ( [ eq : type_ii_classifier ] ) can be modified to approximate the @xmath32 without this property as follows .",
    "consider the type ii - inspired minimization problem @xmath154 which is equivalent to ( [ eq : type_ii_classifier ] ) with @xmath52 when @xmath155 .",
    "for some @xmath156 and @xmath157 sufficiently small ( but not necessarily equal ) , the support of @xmath39 will match the support of @xmath158 .",
    "moreover , ( [ eq : approx_l0_classifier ] ) does _ not _ satisfy the global zero attraction property .",
    "thus type ii affords the possibility of mimicking the @xmath32 norm in the presence of generalized likelihoods but with the advantageous potential for drastically fewer local minima .",
    "this is a direction for future research .",
    "additionally , while here we have focused our attention on classification via logistic regression , these ideas can presumably be extended to other likelihood functions provided certain conditions are met . to the best of our knowledge , while already demonstrably successful in an empirical setting , type ii classifiers and other related bayesian generalized likelihood models have never been analyzed in the context of sparse estimation as we have done in this section .",
    "the dual - space view of sparse linear or generalized linear models naturally allows us to transition @xmath8-space ideas originally developed for type i and apply them to type ii , and conversely , apply @xmath24-space techniques from type ii to type i. the resulting symmetry promotes a mutual understanding of both methodologies and helps ensure that they are not underutilized ."
  ],
  "abstract_text": [
    "<S> sparse linear ( or generalized linear ) models combine a standard likelihood function with a sparse prior on the unknown coefficients . </S>",
    "<S> these priors can conveniently be expressed as a maximization over zero - mean gaussians with different variance hyperparameters . </S>",
    "<S> standard map estimation ( type i ) involves maximizing over both the hyperparameters and coefficients , while an empirical bayesian alternative ( type ii ) first marginalizes the coefficients and then maximizes over the hyperparameters , leading to a tractable posterior approximation . </S>",
    "<S> the underlying cost functions can be related via a dual - space framework from @xcite , which allows both the type i or type ii objectives to be expressed in either coefficient or hyperparmeter space . </S>",
    "<S> this perspective is useful because some analyses or extensions are more conducive to development in one space or the other . </S>",
    "<S> herein we consider the estimation of a trade - off parameter balancing sparsity and data fit . as </S>",
    "<S> this parameter is effectively a variance , natural estimators exist by assessing the problem in hyperparameter ( variance ) space , transitioning natural ideas from type ii to solve what is much less intuitive for type i. in contrast , for analyses of update rules and sparsity properties of local and global solutions , as well as extensions to more general likelihood models , we can leverage coefficient - space techniques developed for type i and apply them to type ii . </S>",
    "<S> for example , this allows us to prove that type ii - inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties ( rip ) lead to failure of popular @xmath0 reconstructions . </S>",
    "<S> it also facilitates the analysis of type ii when non - gaussian likelihood models lead to intractable integrations . </S>"
  ]
}