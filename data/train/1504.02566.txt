{
  "article_text": [
    "searches for faint signals in counting experiments are often encountered in particle physics ( e.g.  in searches for new resonances ) and astrophysics ( e.g.  in searches for new sources of high - energy photons ) .",
    "many practical problems can be reduced to the case of a counting experiment where the events come from the sum of two independent poisson - distributed contributions , usually referred to as `` signal '' and `` background '' .",
    "although an accurate estimate of the background is desired in most applications , the sole parameter of interest is typically the signal intensity . in other words ,",
    "the background intensity can be viewed as a nuisance parameter of the statistical inference problem , whose goal it is to estimate the signal strength given the measured count rate .",
    "sometimes the background is the result of different sources . in such cases , one has a model in which the signal is superimposed to the total background .",
    "if we knew which event comes from which source , we could keep separate lists for the number of observed events for each source of signal or background .",
    "however , in many practical problems , only fewer quantities are directly observable , and sometimes only the total number of observed events can be measured .",
    "an example is the number of events recorded by a geiger - mller counter when measuring the intensity of a weak radioactive sample .",
    "the counts give only information about the total rate of radioactive decays , and not about the isotopic composition of the sample or the environment .",
    "often one has prior knowledge about the individual contributions which can be used to improve the result of the inference . in the example of the weak radioactive source",
    ", this could be any knowledge about the rate of natural radioactivity of the surrounding and the rate of cosmic muons , obtained from a measurement without the source .    in this paper",
    "we address the inference problem outlined above by means of objective bayesian methods .",
    "these are based on the likelihood function , on informative background priors , and on an objective signal prior .",
    "this work is based on refs .",
    "@xcite and @xcite , and extends their results .    choosing a bayesian approach",
    "forces the scientist to be explicit about all the assumptions made .",
    "some of them may be `` subjective '' , in the sense that they reflect the professional experience of the scientist .",
    "as this is validated by numerous cross checks and by the criticism of the colleagues , a better term might be `` inter - subjective '' but we will keep the standard terminology . on the other hand , `` objective '' means that the posterior depends only on the assumption of the model , although there is always some degree of ( scientific ) subjectivity in the choice of the model in the first place .",
    "this is exactly the meaning attributed to `` objective '' in frequentist approaches .",
    "hence we are concerned here with bayesian results that are by construction as objective ( or subjective ) as frequentist results .",
    "bayesian solutions have the advantage of a very simple interpretation : one obtains a range of admissible values for the parameter of interest ( the signal intensity ) together with the probability that the true value belongs to that range . on the other hand ,",
    "the true value of the parameter is not a random quantity in the frequentist approach and thus one can not speak about the probability that it is inside some interval .",
    "hence , a frequentist solution is a statement about the probability of the data given the true value , in a picture in which the experiment is imagined to be identically repeated a large number of times . in practical situations ,",
    "this is a very idealized picture , as identically repeating the experiment is either unfeasible or too expensive .",
    "furthermore , when feasible it might also appear undesirable .",
    "for example , in high - energy physics data are collected during several `` runs '' , ultimately related to the availability of particle interactions .",
    "colliders inject particles before the run starts and keep them circulating until the collision rate drops enough to justify a new fill .",
    "detectors enable data taking after each fill , and keep running until collisions are detected or some problem forces restarting the run . in practice , although to some extent all runs could be considered as identical repetitions of the same experiment , during data analysis they are joined to form a bigger sample , rather then treating them as repeated experiments .    from a computational perspective",
    ", bayesian methods require the calculation of multidimensional integrals while frequentist methods typically adopt maximization procedures .",
    "the latter are typically less cpu - intensive , if those calculations are performed using numerical methods .",
    "however , we are concerned here with a problem that can be treated analytically , hence the computing time is negligible .",
    "both approaches are perfectly legitimate .",
    "when addressing statistical inference , it is important to clearly formulate the question one wants to answer , because this implies the choice of the paradigm . for example , if one wants to speak in terms of the probability of a model ( in our case , a model including contributions from a signal source ) given the experimental result ( e.g. the total count rate ) , then the bayesian approach is the correct paradigm . on the other hand ,",
    "if the question is cast in terms of how ( un)likely the present outcome would be in an ideal infinite sequence of identical repetitions of the same experiment , then the frequentist approach is the choice .",
    "we perform the statistical inference from the bayesian point of view because we believe that this answers the most typical question asked by scientists once an experimental result is made public : what is the probability that this or that model is ( in)correct ?    regardless of using a bayesian or frequentist approach , the statistical model used in the inference problem has to be defined carefully .",
    "although the approach chosen in the following guarantees that in the asymptotic limit of a very large number of observed events the posterior will concentrate around the maximum of the likelihood , the most interesting class of problems is on the opposite side , i.e.  when collecting a very low ( possibly null ) number of events .",
    "it is hence important that the general solution to this problem does not rely on asymptotic approximations and assumptions , e.g. by using a gaussian model in the regime of small count rates .",
    "considering the contributions of signal and different background sources to the count rate allows not only to infer on the signal - plus - background rate , but also to investigate the individual contributions .",
    "it is possible to model such a situation by either a product of independent poisson distributions  one for each signal or background source  or by a product of a single poisson distribution and a multinomial distribution . in the latter form",
    "we can encode correlations between the different contributions with the help of an informative dirichlet prior .",
    "the inference problem may be then split into two parts : inference about the total intensity , and inference about the relative contributions of the different sources .",
    "the main result is that an analytic objective bayesian solution can be obtained , which is valid for any number of measured events ( even when it is very low or zero ) .",
    "an example for such a situation is the search for contributions of a new physics process at a hadron collider in a particular part of a spectrum , say , an invariant mass peak on top of a set of standard model background processes .",
    "the expected number of events due to background sources which are estimated based on theoretical cross - sections are partially correlated due to common uncertainties , for example the integrated luminosity of the data set or the scale of the invariant mass .",
    "these correlations can be modeled by suitably choosing a prior for the multinomial part .",
    "this paper is organized as follows .",
    "section   [ sec - likelihood ] gives details about the statistical models , while section   [ sec - bayes ] describes the bayesian inference .",
    "section   [ sec - summary ] concludes the paper .",
    "a statistical model is the specification of the probability to observe any outcome given the values of the parameters .",
    "when considered as a function of the unknown parameters given the observed data , the model is called the likelihood function .",
    "we first address the case of a counting experiment with @xmath0 contributions and for which the counts @xmath1 are independent poisson variables and are observed independently .",
    "we recall the well known result that the likelihood function can either be written as a product of poisson distributions , or as the product of a poisson distribution for the total number of counts , @xmath2 , and a multinomial distribution describing how the counts are partitioned across the different contributions .",
    "this is most often used when considering the entries in different histogram bins .",
    "the number of events in each bin , taken alone , follows a poisson distribution .",
    "the shape of the histogram , normalized to unit area , is described by a multinomial distribution . for this reason ,",
    "it is sometimes useful to imagine that our counts @xmath3 are represented by a histogram with @xmath0 bins .",
    "later , we consider the situation when one additional contribution has a special meaning , that is it represents the signal .",
    "if @xmath4 with @xmath5 are independent poisson random variables , the statistical model of the data is a product of poisson distributions @xmath6 with parameters @xmath7 , where @xmath8    the outcome of the experiment is the @xmath0-tuple @xmath9 of observed counts for each contribution , using a notation in which lowercase letters stand for actual values of the random variables denoted with uppercase letters . the total number of observed events is @xmath10 .",
    "one the other hand , the conditional distribution of the random vector @xmath11 , given the total number of counts @xmath12 , is a multinomial distribution @xmath13 with the conditions @xmath14 and @xmath15 the parameters @xmath16 represent the relative proportions of the poisson rates : @xmath17 the condition   implies that there are only @xmath18 independent shape variables in eqn .  , and could be used to write the multinomial distribution in a less symmetric form , for example by replacing @xmath19 with @xmath20 and @xmath21 with @xmath22 in eqn .  .",
    "the binomial distribution is a special case of the multinomial distribution with @xmath23 , @xmath24 with @xmath25    a well - known result is that the unconditional distribution of @xmath26 can be factored into the product @xmath27 of a single poisson term and a multinomial distribution .",
    "the poisson distribution gives the probability of observing a total number of events @xmath28 , given the sum of all expected contributions , @xmath29 .",
    "the multinomial term describes how the individual counts @xmath30 distribute among the @xmath0 bins .",
    "let us now assume that we have measured a total of @xmath28 counts , and that we want to infer about the model parameter using eqn .   as the likelihood function .",
    "an immediate consequence of eqn .",
    "is that @xmath28 carries no information about @xmath31 ( and vice versa ) , but only about @xmath32 .",
    "it is no surprise that the statistical inference about the total rate only requires the total number of counts .    on the other hand",
    ", @xmath31 encodes the shape of the distribution of counts across the @xmath0 contributions , but not the normalization : likelihood - based inferences about @xmath31 are the same whether one regards @xmath33 as sampled from @xmath0 independent poisson sources or from a single multinomial process . in other words ,",
    "estimates and tests of any function of @xmath31 will be the same whether we regard the number of counts as a random variable or as a fixed value .",
    "this means that we can separately address the statistical inference problems related to the total count rate and that related to the contributions from different background sources .",
    "the first step is to solve a problem with a single poisson model and @xmath28 total observed events .",
    "the second step is to solve a multinomial problem which focuses on the distribution of counts , given the total number @xmath28 of observed counts .",
    "if a poisson distributed signal source exists on top of @xmath0 background sources , and is independent of the background , one of the @xmath34 contributions to the observed number of counts assumes a special meaning ( the signal ) and the model can be written as @xmath35 where @xmath36    making use of the interpretation of a histogram described earlier , the different counts @xmath37 can be viewed as a histogram with counts from one signal region and @xmath0 auxiliary measurements .",
    "we are now interested in the case that the data show an evidence that , in addition to the known `` background '' source with non - negative bin yields @xmath38 and overall strength @xmath39 representing nuisance ( i.e.  not interesting ) parameters , there is a non - null contribution from a `` signal '' source whose non - negative bin yields @xmath40 and overall strength @xmath41 are our parameters of interest .",
    "this is not just the `` discovery '' problem , for which one would typically focus on the total number of observed counts , because here we are also interested in the shape of the signal contribution .",
    "this is likely to be the second step after a discovery phase of a new particle , for example when one wants to conduct a measurement of the mass or width of the new particle , or to distinguish between two competing models which predict different shapes for the distribution of some kinematical quantity .    even though we have @xmath42 independent sources , we do not know if any given event has come from a signal or background source .",
    "hence we only have @xmath0 observables , the counts in @xmath0 bins .",
    "formally , we start by considering the signal random variables @xmath43 , which are poisson distributed @xmath44 , and the background variables @xmath45 , which are also poisson distributed @xmath46 .",
    "next , we define the random variables @xmath47 which correspond to the observable counts in the @xmath0 bins . as",
    "the sum of poisson variables is again a poisson variable , we have @xmath48 , i.e.  we recover the notation used in section  [ sec - k - bins ] above with @xmath49 , @xmath50 and @xmath51 .    in terms of the variables @xmath52 and",
    "@xmath37 , if @xmath28 total events have been observed the likelihood function for @xmath42 bins is @xmath53 now we partition the counts into @xmath54 signal events and @xmath55 background events ( see appendix ) .",
    "conditional on @xmath56 ( and on @xmath28 ) , we can now write the previous multinomial as the product @xmath57 because we actually do not know @xmath56 , we need to sum over all possible ways of obtaining @xmath56 signal events out of the total @xmath28 counts , with the help of a binomial distribution @xmath58 whose parameter @xmath59 is the probability of obtaining a signal event when both signal and background are active sources .",
    "the likelihood for @xmath42 bins is then @xmath60 as a cross check , we can count the degrees of freedom : 1 poisson variable , plus 1 binomial variable , plus @xmath18 multinomial variables for the signal , plus @xmath18 multinomial variables for the background . in total , we have @xmath42 variables : @xmath61 together with @xmath18 signal yields ( e.g.  @xmath62 ) are the parameters of interest , whereas the @xmath0 background yields @xmath63 are nuisance parameters .",
    "we would have obtained the same result , equation  ( [ eq-2k - likelihood ] ) , by considering the likelihood  ( [ eq - model - k - bins ] ) as the merged version of separate signal and background sources ( see appendix ) , and defining @xmath64 , such that @xmath65 ( one must not forget the binomial weights due to the splitting of the @xmath28 events into two unknown partitions ) .    finally , because we can only observe the sum of signal and background counts in each bin , we have to merge the latter into a set of @xmath0 bins . given the observed @xmath30 counts in bin @xmath66 , there are several ways in which signal and background events can give this sum .",
    "again , we account for this with the help of binomial weights , @xmath67 with @xmath68 .",
    "the full likelihood becomes @xmath69 \\right\\ }    \\end{split}\\ ] ] that is the product of ( [ eq - model - k - bins ] )  which is the first line in ( [ eq - full - likelihood ] )  with terms accounting for all possible combinations of @xmath0-tuples of signal counts and @xmath0-tuples of background counts which give as a result the observed counts @xmath70 in each bin .    in general ,",
    "given the experimental result @xmath70 , the likelihood function  ( [ eq - full - likelihood ] ) is the starting point for performing statistical inference about the parameters of interest , which are the signal yields @xmath71 .",
    "however , when @xmath0 is not small this model is computationally very intensive , as the number of possible combinations becomes huge .",
    "in the bayesian approach , the statistical inference relies on the use of bayes theorem , which can be interpreted as the algorithm for updating our knowledge about a model and its parameters ( modeled as a probability distribution ) in view of the outcome of an experiment .",
    "the prior knowledge is encoded into the prior distribution for the parameters of the model , and their joint posterior distribution ( obtained with bayes theorem ) reflects our new state of knowledge about them . in many cases , we are only interested in a subset of the parameters .",
    "one can obtain their joint marginal posterior probability density by integrating the full posterior over all other ( nuisance ) parameters .",
    "we first address the problem of @xmath0 independent poisson sources and @xmath0 observable count rates .",
    "the likelihood function is given by eqn .",
    "with lowercase letters that remind us that the likelihood is a function of the parameters with fixed ( and known ) experimental outcome .",
    "bayes theorem gives the joint posterior probability density for @xmath29 and @xmath31 @xmath72 \\ ;        [ { \\ensuremath{\\text{mul}}}(\\vec{x } \\",
    ", | n , \\vec{\\eta } ) \\ ; p(\\vec{\\eta } ) ]     \\end{split}\\ ] ] ( where the normalization constant can be found by integrating over the r.h.s . )  in the form of a product of two posterior densities .",
    "the first corresponds to a poisson model with expectation value @xmath29 and prior density @xmath73 , from which @xmath28 events are generated .",
    "the second corresponds to a multinomial model with parameters @xmath31 and prior density @xmath74 , generating the observed vector @xmath75 of counts .",
    "they are two inference problems which can be solved independently , as is shown below .      [ [ conjugate - prior ] ] conjugate prior    the most convenient functional form for the prior density of the poisson parameter is a gamma function @xcite , i.e. , the conjugate prior for a poisson model , @xmath76 , @xmath77 with shape parameter @xmath78 and rate parameter @xmath79 ( or scale parameter @xmath80 ) .",
    "when the prior is @xmath81 , the posterior is @xmath82    [ [ informative - prior ] ] informative prior    in the simple but frequent case in which the prior knowledge about @xmath29 is summarized by its expectation @xmath83 $ ] and variance @xmath84 $ ] , the gamma parameters are determined with the method of moments by imposing @xmath83 = \\alpha/\\beta$ ] and @xmath84 =   \\alpha/\\beta^2 $ ] .",
    "this gives @xmath85/v[\\lambda]$ ] and @xmath86 $ ] .",
    "alternatively , one could start from the prior most probable value ( the gamma mode is at @xmath87 for @xmath88 ) and variance , or from the knowledge of intervals covering given prior probabilities ( e.g.  68.3% or 95% probability ; this requires a numerical treatment to find @xmath89 and @xmath90 ) , or from any set of conditions which is sufficient to determine the shape and rate parameters .    in principle , if there is quite detailed information about the prior for @xmath29 _ and _ it is known that the gamma density does not correctly represent its shape , one has two options .",
    "the first one is to adopt a different functional form and solve bayes theorem numerically , and the second is to find a linear combination of gamma densities which approximates the prior well enough . in the latter case , the posterior will be a linear combination of gamma functions .",
    "this simplifies the treatment without biasing the result , provided that enough terms are present in the linear combination of priors , and it is hence preferable to numerical integration . when @xmath28 is large enough , the solution becomes lesser and lesser dependent on the exact shape of the prior , is arbitrarily large the shape of the prior does not matter at all , provided that it is strictly non - null in the region where the true value is . ]",
    "hence one should consider using numerical methods or a linear combinations only if @xmath28 is very small and  most importantly  when one is very confident that a single gamma density is not good enough to model the prior .",
    "in practical problems , this happens very rarely ( although it is not impossible ) , and a vague knowledge of the prior shape is typically sufficient .    in cases where only vague prior information is available ,",
    "it is recommended to assess the dependence of the posterior on the choice of the prior by comparing the results obtained with different priors , all compatible with the available information ( e.g.  different functional forms giving the same first two moments ) .",
    "when the posterior does not change appreciably , it means that @xmath28 is large enough for the likelihood to dominate and that the result is robust .",
    "otherwise , one has to conclude that the experiment did not yield enough events for obtaining a solution that does not depend on our prior state of knowledge . in this case , it is best to report the differences of a few key figures of merit ( like the posterior mean and variance , or posterior credible intervals with given probability ) which correspond to the choice of quite different priors .",
    "how different should the priors then be ?",
    "of course , comparing our informative gamma density with a delta - function does not make any sense : there are requirements that admissible priors must satisfy to be considered acceptable .",
    "luckily enough , such requirements are really minimal and typically reasonable , like requiring that the prior is strictly positive over the entire domain and that the posterior be a proper density ( i.e.  it integrates to one ) .",
    "in addition , a formal procedure exists which gives us the `` most distant '' prior to our informative prior . we can call it the `` least informative '' prior or the `` objective '' prior , and it is provided by the reference analysis @xcite based on information theory .",
    "this reference prior is the function which maximizes the amount of missing information about the system , hence it is the `` most distant '' choice from our informative prior . in other words ,",
    "the reference prior encodes the minimal set of information about the problem .",
    "indeed , it only depends on the description of the model itself ( in the case under consideration here , on the poisson distribution ) and makes no use of any prior knowledge about the parameter other than the specification of its domain ( which for a poisson problem is the entire positive real line ) .",
    "for this reason , the reference prior shall be used when one wishes to adopt an `` objective prior '' .",
    "[ [ objective - prior ] ] objective prior    when assessing the dependence of the result from the prior information , it is recommended to compare against the reference posterior @xcite , i.e.  the solution obtained with the reference prior @xmath91 , which for the poisson model coincides with jeffreys prior : @xmath92 .",
    "the reference posterior is then @xmath93 and also represents the best choice when one is required to report an `` objective '' result , or when one claims to have the minimal prior information about the poisson parameter .",
    "compared to the informative prior , which is chosen to best encode all available prior information , the reference prior is the `` most distant '' choice from the information - theory point of view , as it encodes the minimal prior information .    [",
    "[ a - simple - example ] ] a simple example    in an attempt to measure the amount of background activity in a new lab , one performs two subsequent measurements for the same duration . for the first measurement , no prior knowledge about the expected rate @xmath29 is assumed , hence the non - informative prior is chosen .",
    "the measurement yields an observed number of events of @xmath94 , so that the posterior distribution is @xmath95 with expectation value @xmath83=9.5 $ ] and variance @xmath84=9.5 $ ] .",
    "the posterior of the first measurement is then used as an informative prior for the second measurement .",
    "the number of observed events is @xmath96 and so the posterior distribution is @xmath97 with expectation value @xmath83=10.75 $ ] and variance @xmath84=5.38 $ ] .",
    "one obtains the same result by merging the two data sets and applying the objective prior to this new `` experiment '' . in this case , the posterior is @xmath98 , where the new random variable @xmath99 because merging the two observations is equivalent to observing for a duration twice as long .",
    "it is very simple to show that , after changing variable , one obtains @xmath100 , which means that the posterior for the original parameter is @xmath101 , the same as above .",
    "[ [ conjugate - prior-1 ] ] conjugate prior    the conjugate prior of the multinomial model , @xmath102 , is the dirichlet distribution , which means that the posterior is also a dirichlet distribution and its parameters are simple functions of the prior and multinomial parameters .",
    "the dirichlet distribution with concentration parameters @xmath103 ( with @xmath104 and @xmath105 for @xmath5 ) is @xmath106 where the multidimensional beta function has been used to write the normalization constant @xmath107 if the prior for the multinomial problem is @xmath108 , the posterior is a dirichlet distribution with parameters @xmath109 : @xmath110    when @xmath23 , the multinomial becomes a binomial distribution .",
    "the corresponding conjugate prior is the beta density , @xmath111 which is indeed the special case of a dirichlet distribution with @xmath23 , @xmath112 , @xmath113 .",
    "one obtains a beta density also as the marginal distribution of any dirichlet parameter , i.e.  the beta density is the result of integrating a dirichlet over @xmath114 parameters .",
    "[ [ informative - prior-1 ] ] informative prior    the following properties of the dirichlet distribution can be used to find the values of the concentration parameters which best represent all the available prior information : @xmath115 & = & a_i / a     \\\\",
    "\\text{mode}[\\eta_i ] & = & \\frac{a_i - 1}{a - k }     \\\\",
    "v[\\eta_i ] & = & \\frac{a_i ( a - a_i)}{a^2 ( a+1 ) }     \\\\     \\text{cov}[\\eta_i,\\eta_j ] & = & \\frac{-a_i a_j}{a^2 ( a+1 ) }   \\end{aligned}\\ ] ]    often the parameters may be grouped into two classes : a set of parameters that are independent from any other parameter , and a set of parameters that have pairwise correlations . by grouping them correspondingly , one obtains a block - diagonal matrix in which the first block is actually diagonal .",
    "parameters in the first block are then fixed by imposing their prior variance and expectation .",
    "for the others , one has a system of requirements , for example in the form of a collection of variances and covariances .",
    "as there may be more constraints than free variables , the prior concentration parameters may be found e.g.  by solving a minimization problem .",
    "alternatively , one may choose to solve the problem multiple times , each with a different choice of concentration parameters which satisfies @xmath0 independent constraints .    on the other hand ,",
    "if there are less constraints than the number of parameters , one may impose the `` objective '' choice on the unconstrained variables , i.e.  equal prior concentration parameters .",
    "this might be done in a two - step algorithm starting with an objective prior with concentration parameters all set to @xmath116 ( see below ) , and by imagining an intermediate experiment in which only a subset of @xmath117 variables get updated to @xmath118 .",
    "the next step is to find real values @xmath119 such that the first @xmath120 concentration parameters satisfy all available constraints .",
    "the result can be used as the prior for the actual experiment .",
    "[ [ non - informative - prior ] ] non - informative prior    the problem of finding an objective prior for all multinomial parameters was addressed by @xcite and @xcite .",
    "being aware that for multidimensional models the reference prior depends on the ordering of the parameters , @xcite argued that , when no priority ranking is desired or allowed ( in other words , when one wants all @xmath121 parameters to be on the same footing ) , some sort of `` overall objective prior '' shall be used which treats all parameters fairly well .",
    "a very reasonable requirement is that the marginal posterior for any given parameter should be as close as possible to the solution obtained with the marginal model and the reference prior for that single parameter .",
    "for the multinomial model , the reference posterior when only @xmath121 is of interest is @xmath122 .",
    "is equivalent to a binomial distribution , this result can also be proved with the latter model .",
    "the reference posterior is the same beta density , as it is shown for example in ref .",
    "] on the other hand , if the prior @xmath123 is used  using the same concentration parameter @xmath124 for all multinomial @xmath121 parameters , as they are treated on the same footing  , the marginal posterior for @xmath121 is instead @xmath125 .",
    "the goal is to find @xmath124 such that , in an average sense , this result is closest to @xmath126 for each @xmath66 . among different possible approaches to this problem @xcite , @xcite addressed it with a hierarchical approach and showed that the overall objective prior is a dirichlet density with all identical concentration parameters , whose value is very well approximated by the simple choice @xmath127 ( just a bit lower than the intuitive choice @xmath128 ) , which gives an objective dirichlet posterior with parameters @xmath129 .",
    "[ [ a - simple - example - continued ] ] a simple example , continued    in the case of two subsequent measurements , one might be interested in the background stability with time .",
    "this is relevant during the preparation of low - background experiments , e.g.  if newly produced parts of an experiment are brought into underground laboratories . in our example , we could imagine to have installed new equipment between the two measurements and ask ourselves if the somewhat larger number of counts in the second measurement suggests that the equipment has increased the overall background rate .",
    "one way of checking this is to look at the 2-bins histogram with @xmath130 and @xmath131 counts and quantify the departure from equipartition .",
    "a more formal procedure would be to test against the null hypothesis that the background rate is constant , but it is not necessary in this particular example , which is quite straightforward .    to better illustrate the problem , we use first a binomial model with @xmath132 successes out of @xmath133 total trials and",
    "look at the deviation from @xmath134 , where @xmath135 is the ratio between the initial and the total rate .",
    "the reference posterior for @xmath135 is @xcite @xmath136 the posterior mean , mode and standard deviation are @xmath137 = 0.4318   \\qquad     m[\\varepsilon ] = 0.4250   \\qquad     \\sigma[\\varepsilon ] = 0.1033\\ ] ]    if we start with @xmath138 and use the overall objective prior @xmath139 recommended by @xcite , we obtain instead the marginal posterior @xmath140 whose mean , mode and standard deviation are @xmath141 = 0.4312   \\qquad     m[\\varepsilon ] = 0.4242   \\qquad     \\sigma[\\varepsilon ] = 0.1037\\ ] ] thus the two alternative solutions are hardly distinguishable in practice .",
    "the peaks are @xmath142 and @xmath143 lower than @xmath134 , not a significant difference to conclude that the background level has changed between the two measurements , even without performing a formal test .",
    "hence it is safe to use both measurements to estimate the background level in the cavern .",
    "[ [ an - example - from - high - energy - physics - lepton - universality ] ] an example from high - energy physics : lepton universality    assume an experiment built to test lepton universality ( see , e.g. refs .",
    "@xcite for the current experimental status ) in @xmath144-boson production which is equally sensitive to electrons , muons and taus , i.e. assuming equal reconstruction efficiencies , momentum resolutions and so on .",
    "events containing @xmath144-bosons are selected by measuring final states with two opposite - charge leptons and by requiring their invariant mass to be consistent with the mass of a @xmath144-boson .",
    "adding further stringent event - selection requirements , the resulting event sample can be made practically background - free .",
    "the number of observed events in this example are @xmath145 , @xmath146 and @xmath147 .",
    "lepton universality is tested by estimating the branching ratios , e.g. , @xmath148 , where @xmath149 , and comparing them . if they are found to not be equal , then lepton universality is disfavored .",
    "using eqn .   with a reference prior for the overall rate @xmath29 and an objective prior for the multinomial part yields @xmath150 \\ ; [ { \\ensuremath{\\text{mul}}}(\\vec{x } \\",
    ", | n , \\eta_{e } , \\eta_{\\mu},\\eta_{\\tau } ) \\ ; { \\ensuremath{\\text{dir}}}(\\eta_{e } , \\eta_{\\mu},\\eta_{\\tau } \\,|\\ , 0.8/3 , 0.8/3 , 0.8/3 ) ] \\ , .",
    "\\end{aligned}\\ ] ]    the resulting two - dimensional marginal posterior , depending on @xmath151 and @xmath152 , is shown in fig.[fig : marginal ] , together with the corresponding one - dimensional projections . the expectation from lepton universality , i.e. , @xmath153 ,",
    "is indicated by the asterisk and consistent with the observation .    [ cols=\"^,^,^ \" , ]",
    "in this paper , we have derived a statistical model for counting experiments in which the sources contributing to the resulting count rate are potentially correlated . using bayes theorem and assuming non - informative and informative priors on signal and background contributions respectively , we have shown that the marginal posterior probability for the signal contribution can be expressed in closed form .",
    "simple use cases in the field of experimental physics have been presented .    99    j.o .",
    "berger & j.m .",
    "bernardo , _ ordered group reference priors with application to the multinomial problem _ , biometrika 79 ( 1992 ) 25 - 37 .",
    "berger , j.m .",
    "bernardo , d.  sun , _ objective priors for discrete parameter spaces _ , j. amer .",
    "assoc . 107",
    "( 2012 ) 636 .",
    "berger , j.m .",
    "bernardo , d.  sun , _ overall objective priors _ , conference on `` recent advances in statistical inference : theory and case studies '' , university of padova , 2123 mar 2013 .",
    "bernardo , `` reference analysis '' , handbook of statistics 25 ( d.k .",
    "dey and c.r .",
    "rao eds . ) .",
    "amsterdam : elsevier ( 2005 ) 1790 .",
    "a.  caldwell , d.  kollar , k.  krninger , _ bat - the bayesian analysis toolkit _ ,",
    "computer physics communications 180 ( 2009 ) 2197 - 2209 , http://arxiv.org/abs/0808.2552[arxiv : 0808.2552 ] .",
    "d.  casadei , _ reference analysis of the signal + background model in counting experiments _ , jinst 7 ( 2012 ) p01012 , , http://arxiv.org/abs/1108.4270[arxiv : 1108.4270 ] .",
    "d.  casadei , _ estimating the selection efficiency _ , jinst 7 ( 2012 ) p08021 , , http://arxiv.org/abs/0908.0130[arxiv : 0908.0130 ] .",
    "d.  casadei , _ reference analysis of the signal + background model in counting experiments ii .",
    "approximate reference prior _ , jinst9 ( 2014 ) t10006 , , http://arxiv.org/abs/1407.5893[arxiv : 1407.5893 ] .    k.  a.  olive _ et al .",
    "_ [ particle data group collaboration ] , _ review of particle physics _ , chin .  phys .",
    "c * 38 * ( 2014 ) 090001 , .",
    "l.  ventura , s.  cabras , w.  racugno , _ prior distributions from pseudo - likelihoods in the presence of nuisance paramenters _ , j. amer .",
    "statist . assoc . 104 ( 2009 ) 768 .",
    "we recall here a few properties of the multinomial distribution that are important when considering practical problems , in particular its behavior under re - binning and partitioning .",
    "the first one is about re - binning : merging random variables distributed according to multinomial distributions again gives a multinomial distribution . if @xmath154 with @xmath16 , then @xmath155 where @xmath156 and @xmath157 for @xmath158 .",
    "one may obtain a binomial problem by sequentially merging @xmath114 bins .",
    "the second property is important if we know the total number of counts @xmath159 in a subset of the bins : the conditional distribution given that @xmath160 and @xmath161 is a product of two independent multinomial pieces : @xmath162 for example , this property tells us how to treat two histograms at the same time .",
    "conversely , the third property is that the joint distribution of two independent multinomial random vectors has a multinomial kernal . to see this ,",
    "one starts from @xmath163 and writes their joint distribution as @xmath164 defining @xmath165 such that @xmath166 for @xmath5 and @xmath167 for @xmath168 , one has @xmath169 furthermore , defining @xmath170 one also has @xmath171 such that eqn .",
    "may be rewritten as @xmath172 where we used @xmath173 .",
    "this looks like a multinomial with @xmath174 degrees of freedom .",
    "however , we started from two multinomials with @xmath18 and @xmath175 degrees of freedom , hence must obtain a distribution with @xmath176 free parameters .",
    "eqn .   gives the probability distribution conditional to @xmath28 and @xmath159 . if , however , only the total number of counts @xmath177 is known , then one must sum eqn .   over all possible pairs of @xmath28 and @xmath159 which result in a total count of @xmath178 .",
    "this reduce by one the number of independent variables ."
  ],
  "abstract_text": [
    "<S> searches for faint signals in counting experiments are often encountered in particle physics and astrophysics , as well as in other fields . </S>",
    "<S> many problems can be reduced to the case of a model with independent and poisson - distributed signal and background </S>",
    "<S> . often several background contributions are present at the same time , possibly correlated . </S>",
    "<S> we provide the analytic solution of the statistical inference problem of estimating the signal in the presence of multiple backgrounds , in the framework of objective bayes statistics . </S>",
    "<S> the model can be written in the form of a product of a single poisson distribution with a multinomial distribution . </S>",
    "<S> the first is related to the total number of events , whereas the latter describes the fraction of events coming from each individual source . </S>",
    "<S> correlations among different backgrounds can be included in the inference problem by a suitable choice of the priors .    counting experiments ; bayesian inference ; objective priors </S>"
  ]
}