{
  "article_text": [
    "recently , symmetric positive definite ( spd ) matrices of real numbers appear in many branches of computer vision .",
    "examples include region covariance matrices for pedestrian detection @xcite and texture categorization @xcite , joint covariance descriptor for action recognition @xcite , diffusion tensors for dt image segmentation @xcite and image set based covariance matrix for video face recognition @xcite . due to the effectiveness of measuring data variations ,",
    "such spd features have been shown to provide powerful representations for images and videos .",
    "however , such advantages of the spd matrices often accompany with the challenge of their non - euclidean data structure which underlies a specific riemannian manifold @xcite . applying the euclidean geometry directly to spd matrices",
    "often results in poor performances and undesirable effects , such as the swelling of diffusion tensors in the case of spd matrices @xcite . to overcome the drawbacks of the euclidean representation , recent works @xcite have introduced riemannian metrics , e.g. , affine - invariant metric @xcite , log - euclidean metric @xcite , to encode the riemannian geometry of spd manifold properly .    by applying these classical riemannian metrics ,",
    "a couple of works attempt to extend euclidean algorithms to work on manifolds of spd matrices for learning more discriminative spd matrices or their vector - forms . to this end ,",
    "several studies exploit effective methods on one spd manifold by either flattening it via tangent space approximation @xcite ( see fig.[fig1 ] ( a)@xmath0(b ) ) or mapping it into a high dimensional reproducing kernel hilbert space ( rkhs ) @xcite ( see fig.[fig1 ] ( a)@xmath0(c)@xmath0(b ) ) .",
    "obviously , both of the two families of methods inevitably distort the geometrical structure of the original spd manifold due to the procedure of mapping the manifold into a flat euclidean space or a high dimensional rkhs .",
    "therefore , the two learning schemes would lead to sub - optimal solutions for the problem of discriminative spd matrix learning .",
    "( b ) is to firstly flatten the original manifold @xmath1 by tangent space approximation and then learn a map @xmath2 to a discriminative euclidean space @xmath3 .",
    "the second one ( a)@xmath0(c)@xmath0(b ) is to firstly embed @xmath1 with an implicit map @xmath4 into an rkhs @xmath5 and then learn a mapping @xmath6 to a more discriminative euclidean space @xmath3 .",
    "the last one ( a)@xmath0(d ) aims to learn a map @xmath7 from the original spd manifold @xmath1 to a more discriminative spd manifold @xmath8 . here , @xmath9 and @xmath10 are the spd matrices , @xmath11 and @xmath12 are the tangent spaces . ]    [ fig1 ]    to more faithfully respect the original riemannian geometry",
    ", another kind of spd - based discriminant learning methods @xcite aims to pursue a column full - rank transformation matrix mapping the original spd manifold to a more discriminative spd manifold , as shown in fig.[fig1 ] ( a)@xmath0(d ) .",
    "however , as directly learning the manifold - manifold transformation matrix is hard , the work @xcite alternatively decomposes it to the product of an orthonormal matrix with a matrix in gl@xmath13 , and requires the employed riemannian metrics to be affine invariant . by doing so ,",
    "optimizing the manifold - manifold transformations is equivalent to optimizing over orthonormal projections .",
    "although the additional requirement simplifies the optimization of the transformation , this has not only reduced the original solution space but also inevitably excluded all non - affine invariant riemannian metrics such as the well - known log - euclidean metric , which has proved to be much more efficient than affine - invariant metric @xcite .",
    "while the work @xcite exploited the log - euclidean metric under the same scheme , it actually attempts to learn a tangent map , which implicitly approximate the tangent space and hence introduces some distortions of the true geometry of spd manifolds .    in this paper , also under the last scheme ( see fig.[fig1 ] ( a)@xmath0(d ) ) , we propose a new geometry - aware spd similarity learning ( spdsl ) framework to open a broader problem domain of learning discriminative spd features by exploiting either affine invariant or non - affine invariant riemannian metrics on spd manifolds . to realize the spdsl framework , there are three main contributions in this work :    * by exploiting the riemannian geometry of the manifold of fixed - rank positive semidefinite ( psd ) matrices , our spdsl framework provides a new solution to directly learn the manifold - manifold transformation matrix . as no additional constraint",
    "is required , the optimal transformation will be pursued in a favorable solution space , enabling a wide range of well - established riemannian metrics to work as well . * to fulfill the solution , a new supervised spd similarity learning technique is proposed to learn the transformation by regressing the similarities of selected spd pairs to the target similarities on the resulting spd manifold .",
    "* we derive an optimization approach which exploits the classical riemannian conjugate gradient ( rcg ) algorithm on the psd manifold to optimize the proposed objective function .",
    "let @xmath14 be a set of real , symmetric matrices of size @xmath15 and @xmath16 be a set of spd matrices .",
    "the mapping space @xmath17 is endowed with usual euclidean metric ( i.e. , inner product ) @xmath18 . as noted in @xcite , the set of spd matrices @xmath19 is an open convex subset of @xmath17 .",
    "thus , the tangent space to @xmath19 at any spd matrix in it can be identified with the set @xmath17 .",
    "a smoothly - varying family of inner products on each tangent space is known as riemannian metric , endowing which the space of spd matrices @xmath19 would yield a riemannian manifold . with such riemannian metric , the geodesic distance between two elements @xmath20 on the spd manifold is generally measured by @xmath21 . several riemannian metrics and divergences have been proposed to equip spd manifolds . for example",
    ", affine - invariant metric @xcite , stein divergence @xcite , jeffereys divergence @xcite are designed to be invariant to affine transformation .",
    "that is , for any @xmath22 ( i.e. , the group of real invertible @xmath23 matrices ) , the metric function @xmath24 has the property @xmath25 .",
    "in contrast , log - euclidean metric@xcite , cholesky distance @xcite and power - euclidean metric @xcite are not affine invariant . among these metrics , only affine - invariant metric @xcite and log - euclidean metric @xcite",
    "define a true geodesic distance on the spd manifold @xcite .",
    "in addition , the stein divergence are also widely used due to its favorable properties and high performances in visual recognition tasks @xcite .",
    "therefore , this paper focuses on studying such three representative riemannian metrics .",
    "* definition 1 . * _ by defining the inner product in the tangent space at the spd point @xmath26 on the spd manifold as @xmath27 and the logarithmic maps as @xmath28 , the geodesic distance between two spd matrices @xmath20 on the spd manifold is induced by affine - invariant metric ( aim ) as _",
    "* definition 2 . * _ the approximated geodesic distance between two spd matrices @xmath20 on the spd manifold is defined by using stein divergence as _",
    "* definition 3 . * _ by defining the inner product in the space at the spd point @xmath26 on the spd manifold as @xmath31 , \\text{d}\\log(\\bm{x}_1)[\\bm{h}_2 ] \\rangle$ ] ( @xmath32 $ ] denotes the directional derivative ) and the logarithmic maps as @xmath33 $ ] , the geodesic distance between two spd matrices @xmath20 is derived by log - euclidean metric ( lem ) as _",
    "in this section , we first propose a new solution of riemannian geometry - aware dimensionality reduction for spd matrices , and then present our supervised spd similarity learning method under the solution .",
    "finally , we give a detailed description of our developed optimization algorithm .",
    "given a set of spd matrices @xmath35 , where each matrix @xmath36 , and a transformation @xmath37 ( @xmath38 ) is pursued for mapping the original spd manifold @xmath19 to a lower - dimensional spd manifold @xmath39 .",
    "formally , this procedure attempts to learn the parameter @xmath40 , of a mapping in the form @xmath41 , which is defined as : @xmath42 to ensure the resulting mapping yields a valid spd manifold @xmath43 @xmath44 , the manifold - manifold transformation @xmath45 is basically required to be a full - rank matrix @xmath46 .",
    "since the solution space is a non - compact stiefel manifold @xmath47 where the distance function has no upper bound , directly optimizing on the manifold is infeasible .",
    "fortunately , the conjugates ( taking the form of @xmath48 ) of column full - rank matrices span a compact manifold @xmath49 of positive semidefinite ( psd ) matrices , which is a quotient space of @xmath47 and owns a well - established riemannian structure .",
    "in contrast , by additionally assuming the transformation @xmath40 to be orthogonal as done in @xcite , eqn.[eq3 ] could be optimized on compact stiefel manifold , which is a subset of the non - compact stiefel manifold @xmath47 .",
    "further , for the affine invariant metrics ( e.g. , aim ) , optimizing on stiefel manifold can be reduced to optimizing over grassmannian @xcite .",
    "however , such orthogonal solution space is smaller than the original solution space @xmath47 , making the optimization theoretically yield suboptimal solution of @xmath40 .",
    "thus , we choose to optimization on the psd manifold to search the optimal solution of @xmath40 in a more faithful way .",
    "now , we need to study the geometry of the psd manifold @xmath49 .    for all orthogonal matrices @xmath50 of size @xmath51 , the map @xmath52 leaves @xmath48 unchanged .",
    "this property of @xmath40 results in the equivalence class of the form @xmath53 = \\{\\bm{wo}| \\bm{o } \\in \\mathbb{r}^{m \\times m } , \\bm{o}^t\\bm{o}=\\bm{i}_m\\}$ ] , and a one - to - one correspondence with the rank-@xmath54 psd matrix @xmath55 . by quotienting this equivalence relation out , the set of rank-@xmath54 psd matrices @xmath49",
    "is reduced to the quotient of the manifold @xmath56 by the orthogonal group @xmath57 , i.e. , @xmath58 . with the studied relationship between @xmath49 and @xmath56 ,",
    "the function @xmath59 is able to derive the function @xmath60 defined as @xmath61 . here ,",
    "@xmath2 is defined in the total space @xmath56 and descends as a well - defined function in the quotient manifold @xmath49 .",
    "therefore , optimizing over the total space @xmath56 is reduced to optimizing on the psd manifold @xmath49 , which is well - studied in several works @xcite .",
    "note that , as each element @xmath62 on the psd manifold is simply parameterized by @xmath40 , optimizing on the manifold actually deals directly with @xmath40 .",
    "to more easily understand this point , one can take the well - known grassmann manifold as an analogy , where each element can be similarly represented by the equivalence class @xmath53 $ ] or the projection matrix @xmath48 ( here , @xmath63 ) , and the optimization on it directly seeks the solution of @xmath40 .",
    "it can be further proven that the quotient @xmath49 presents the structure of a riemannian manifold @xcite . as a result , endowing the total space @xmath56 with the usual riemannian structure of a euclidean space ( i.e. , the inner product @xmath18 ) , a riemannian structure for the quotient space @xmath49 follows .",
    "the inner product occurs on the tangent space @xmath64 of the manifold @xmath56 . in the case of the manifold @xmath49",
    ", the corresponding tangent space is decomposed into two orthogonal subspaces , the vertical space @xmath65 and the horizontal space @xmath66 , to achieve the inner product @xmath67 .",
    "this riemannian metric facilitates several classical optimization techniques such as riemannian conjugate gradient ( rcg ) algorithm @xcite working on the psd manifold @xmath49 . as for more detailed background on the riemannian geometry of the psd manifold",
    ", please refer to the works @xcite .    by exploiting the riemannian geometry of the fixed - rank psd manifold @xmath49 , we here open up the possibility of directly pursuing an optimal column full - rank manifold - manifold transformation matrix to solve the problem of dimensionality reduction on spd features .      as studied before , under the proposed framework of dimensionality reduction on spd features ,",
    "a target spd manifold @xmath39 of lower dimensionality can be derived . on the new spd manifold @xmath39 ,",
    "the geodesic distance between the two original spd points @xmath68 is achieved by : @xmath69 where @xmath70 is the manifold - manifold transformation computed by eqn.[eq3 ] , @xmath71 can be the geodesic distance induced by the commonly - used affine or non - affine invariant riemannian metrics eqn.[eq1 ] , eqn.[eq1.0 ] and eqn.[eq2 ] . in this paper",
    ", we are focusing on the problem of supervised spd similarity learning for more robust visual classification tasks where spd features have shown great power .",
    "formally , for each spd matrix @xmath36 , we define its class indicator vector : @xmath72 \\in \\mathbb{r}^c$ ] , where the @xmath73-th entry being 1 and other entries being 0 indicates that @xmath74 belongs to the @xmath73-th class of @xmath75 classes in total . as discriminant learning techniques developed in euclidean space",
    ", we assume that prior knowledge is known regarding the distances between pairs of spd points on the new spd manifold @xmath39 .",
    "let s take the similarity or dissimilarity between pairs of spd points into account : two spd points are similar if the similarity based on the geodesic distance between them on the new manifold is larger , while two spd points are dissimilar if their similarity is smaller .    given a set of the similarity constraints , our goal is to learn the manifold - manifold transformation matrix @xmath40 that parameterizes the similarities of spd points on the target spd manifold @xmath39 . to this end",
    ", we exploit the supervised criterion of centered kernel target alignment @xcite to learn discriminative features on the spd manifold by regressing the similarities of selected sample pairs to the target similarities .",
    "formally , our supervised spd similarity learning ( spdsl ) approach is to maximize the following objective function : @xmath76 where @xmath77 and @xmath78 are frobenius inner product and norm respectively .",
    "the elements of matrix @xmath79 encodes the similarities of spd data while the elements of @xmath80 presents the ground - truth similarities of the involved spd points .",
    "the matrix @xmath81 is used to select the pairs of spd points when the corresponding elements are 1 .",
    "the matrix @xmath82 is employed for centering the data similarity matrix @xmath79 and the similarity matrix @xmath80 on labels .",
    "@xmath83 is the number of samples , @xmath84 is the identity matrix of size @xmath85 , @xmath86 is the vector of size @xmath83 with all entries being ones , @xmath87^t$ ] is here supposed to be centered , i.e. , @xmath88 , for simplicity . in the following , we will give the formulations of the two matrices @xmath89 and @xmath81 in more details .",
    "more specifically , the employed matrix @xmath89 in eqn.[eq9 ] encodes the similarity between each pair of spd points @xmath90 on the spd manifold @xmath19 , which takes a form as : @xmath91 where @xmath92 is computed by eqn.[eq4 ] , @xmath93 is typically set as @xmath94 , @xmath95 is empirically set to mean of distances of the original training sample pairs .",
    "actually , the function eqn.[eq6 ] takes a form of gaussian kernel function .",
    "however , as the objective function eqn.[eq9 ] can be expressed as sum of the similarity regression results of selected sample pairs , the function eqn.[eq6 ] just serves as a tool to encode the similarities and is thus not necessarily positive definite ( pd ) .    in practical application , the computational burden of handling the full kernel matrix @xmath89 on the spd",
    "manifold scales quadratically with the size of training spd data . to address this problem",
    ", we exploit the idea of graph embedding technique @xcite to select a limited number of data pairs to construct a sparse kernel matrix ( non pd ) with a large number of elements being zero . with this idea in mind ,",
    "the matrix @xmath81 is defined to select the pairs of spd points for spd similarity learning . by employing it ,",
    "@xmath96 can be regarded as the sparse kernel matrix , where the operation @xmath97 denotes hadamard product and the matrix @xmath98 . here ,",
    "@xmath99 and @xmath100 are defined as : @xmath101 where @xmath102 is the set of @xmath103 nearest neighbors of @xmath74 that share the same class label as @xmath104 , and @xmath105 is the set of @xmath106 nearest neighbors of @xmath74 with different class labels from @xmath104 . according to the theory of graph embedding @xcite ,",
    "the within - class similarity graph @xmath99 and the between - class dissimilarity graph @xmath100 respectively defined in eqn.[eq7 ] and eqn.[eq8 ] can encode the local geometrical structure of the space of the processing data .",
    "thus , in addition to speeding up the discriminant learning on the spd features , exploiting the graph embedding technique can not only learn the discriminative information of spd data but also characterize the local riemannian geometry of the underlying spd manifold .",
    "the efficiency and effectiveness of the proposed discriminant learning approach working on spd manifolds will be further studied in the experimental part .      as discussed before , optimizing in the solution space @xmath56 of the column full - rank transformation matrices in our objective function",
    "can be reduced to optimizing on the riemannian manifold of rank-@xmath54 psd matrices , @xmath49 .",
    "therefore , in this section , we exploit the riemannian conjugate gradient ( rcg ) algorithm @xcite to optimize our objective function @xmath107 in eqn.[eq9 ] by deriving its corresponding gradient on the psd manifold @xmath49 .",
    "* input * : the initial matrix @xmath108 + 1 .",
    "repeat * + 3 .",
    "line search along the geodesic @xmath111 with the direction @xmath112 from @xmath113 to find @xmath114 .",
    "@xmath115 , @xmath116 .",
    "* until * convergence + * output * : the optimized matrix @xmath40 [ alg1 ]    as the conjugate gradient algorithm developed in euclidean space , the rcg algorithm on riemannian manifolds is an iterative procedure . as given in algorithm[alg1 ] , an outline for the iterative part of the algorithm goes as follows : at the @xmath73-th iteration , find @xmath117 by searching the minimum of @xmath118 along the geodesic in the direction @xmath119 from @xmath120 , compute the riemannian @xmath121 at this point , choose the new search direction by @xmath122 and iterate until convergence . in the procedure",
    ", the riemannian @xmath121 can be easily approximately from its corresponding euclidean gradient @xmath123 by the computation @xmath124 , and the operation @xmath125 is the parallel transport of tangent vector @xmath119 from @xmath120 to @xmath126 . for more details , we refer readers to @xcite .    as for now , we just need to compute the euclidean gradient for our objective function @xmath107 in eqn.[eq9 ] . as the euclidean gradient @xmath127 and its corresponding directional derivatives",
    "are related with the following equality : @xmath128=\\langle d_w\\mathcal{j}(\\bm{w } ) , \\dot{\\bm{w } } \\rangle .",
    "\\label{eq10}\\ ] ] by employing the basic rule and standard properties of the directional derivatives , @xmath129 $ ] can be derived by : @xmath130 \\\\ & = \\frac{\\langle \\bm{u}\\bm{g}\\circ d_w k(\\bm{w})[\\dot{\\bm{w}}]\\bm{u } , \\bm{g}\\circ(\\bm{y}\\bm{y}^{t } ) \\rangle_{\\mathcal{f } } \\|\\mathcal{l}\\|_{\\mathcal{f}}}{\\|\\mathcal{l}\\|_{\\mathcal{f}}^2}\\\\ - & \\frac{\\langle \\mathcal{l } , \\bm{g}\\circ(\\bm{y}\\bm{y}^{t } ) \\rangle_{\\mathcal{f } } \\langle \\frac{\\mathcal{l}}{\\|\\mathcal{l}\\|_{\\mathcal{f } } } , \\bm{u}\\bm{g}\\circ d_w k(\\bm{w})[\\dot{\\bm{w}}]\\bm{u } \\rangle_{\\mathcal{f}}}{\\|\\mathcal{l}\\|_{\\mathcal{f}}^2 } \\\\ & = \\langle d_w k(\\bm{w})[\\dot{\\bm{w } } ] , \\bm{u } \\left ( \\frac{\\bm{g}\\circ(\\bm{y}\\bm{y}^{t})}{\\|\\mathcal{l}\\|_{\\mathcal{f}}}- \\frac{\\mathcal{j}(\\bm{w})\\mathcal{l}}{\\|\\mathcal{l}\\|_{\\mathcal{f}}^2 } \\right ) \\bm{u } \\rangle_{\\mathcal{f } } , \\label{eq12 } \\end{aligned}\\ ] ] where @xmath131 , @xmath77 indicates frobenius inner product , @xmath78 denotes frobenius norm .",
    "accordingly , the key issue in eqn.[eq12 ] is to estimate @xmath132 , where @xmath133 is formulated by eqn.[eq6 ] .",
    "when @xmath71 in eqn.[eq4 ] is the geodesic distance of aim defined in eqn.[eq1 ] , the euclidean gradient of @xmath79 can be derived as : @xmath134 where @xmath135 , @xmath136 .    for other affine invariant metrics such as stein divergence @xcite , the corresponding euclidean gradient of @xmath79 with",
    "the geodesic distance function @xmath71 being defined in eqn.[eq1.0 ] can be computed by : @xmath137 where @xmath138 , and hence be able to work in our new proposed framework .    when endowing the spd manifold with the non - affine invariant metric lem , it not easy to calculate the euclidean gradient of @xmath132 due to the matrix logarithms in it .",
    "thus , we need to study the problem of the computation of the euclidean gradient for the lem case in the following .",
    "first , we decompose the derivative of lem w.r.t .",
    "@xmath40 into three derivatives with the trace form @xmath139 : @xmath140    * proposition 1 .",
    "* _ the derivatives of the three trace forms @xmath139 in eqn.[eq13 ] can be respectively computed by ( here , @xmath135 , @xmath141 ) _ :",
    "d_w(tr(^2(_i ) ) = 4_i ( _ i)[(_i ) ] .",
    "[ eq14 ] + d_w(tr(^2(_j ) ) = 4_j ( _ j)[(_j ) ] .",
    "[ eq15 ] +    & d_w(tr((_i)(_j ) ) + & = 2_i ( _ i)[(_j ) ] + 2_j ( _ j)[(_i ) ] .",
    "[ eq16 ]    _ proof .",
    "the three formulas for the gradients with the matrix logarithm correspond to the three ones with rotation matrices in @xcite ( section 5.3 ) , where a detailed proof is given . _    by using * proposition 1 . *",
    "( i.e. eqn.[eq14 ] , eqn.[eq15 ] , eqn.[eq16 ] ) and the sum rule of the directional derivatives , we derive @xmath132 with @xmath71 being the geodesic distance of lem in eqn.[eq4 ] as : @xmath142 \\\\   & + \\bm{b}_j \\text{d}\\log(\\bm{\\hat{x}}_j)[\\log(\\bm{\\hat{x}}_j)-\\log(\\bm{\\hat{x}}_i ) ] ) \\beta k_{ij}(\\bm{w } ) .",
    "\\label{eq17 } \\end{aligned}\\ ] ]    to calculate the formula eqn.[eq17 ] , we then apply a function of block triangular matrix developed in @xcite to compute the form of @xmath143 $ ] , which is the directional ( frchet ) derivative of @xmath144 at @xmath145 along @xmath146 .",
    "the following theorem shows that the directional derivative appears as the @xmath147 block of the resulting big matrix when @xmath148 is evaluated at a certain block triangular matrix",
    ".    * theorem 1 .",
    "* _ let @xmath148 be @xmath149 times continuously differentiable on @xmath150 and let the spectrum of @xmath151 lie in @xmath150 , where @xmath150 is an open subset of @xmath152 . then _",
    "@xmath153\\\\ 0 & f(\\bm{\\hat{x}})\\\\ \\end{bmatrix}. \\label{eq18 } \\end{aligned}\\ ] ]    _ proof .",
    "_ the result is proved by najfeld and havel @xcite ( theorem 4.11 ) under the assumption that @xmath7 is analytic . _    by using * theorem 1 * , the directional derivative of the matrix logarithm can be easily computed .",
    "the pseudo matlab code of computing @xmath143 $ ] is simply listed as : n = size(x , 1 ) ; z = zeros(n ) ; a = log([x , h ; z , x ] ) ; d = a(1:n , ( n+1):end ) , where @xmath154 $ ] .    with the derived gradient formulas in eqn.[eq11 ] , eqn.[eq19 ] and eqn.[eq17 ] , the euclidean gradient eqn.[eq12 ] of the objective function eqn.[eq9 ] for these cases can be computed to feed into the exploited rcg algorithm working on the psd manifold .",
    "since the global convergence of the rcg algorithm has been well - studied in the survey @xcite , we do not investigate it any further here .",
    "the main time complexity of the algorithm is computing the gradient eqn.[eq12 ] , being @xmath155 ( @xmath156 is the iteration number , @xmath157 is the number of selected samples / pairs , @xmath158 is the dimension of the original / target manifold ) in the lem case . in the experiment",
    ", we will also study the running time of each iteration of the algorithm by varying the number of selected between - class pairs for each spd sample .",
    "in this section , we study the effectiveness of the proposed geometry - aware spd similarity learning ( spdsl ) approach by conducting experimental evaluations for three visual classification tasks including face recognition , material categorization and action recognition .    in these three tasks , the spd features have been shown to provide powerful representations for images and videos via set - based covariances@xcite , region covariances @xcite and joint covariance descriptors @xcite .",
    "therefore , they are natural choices to evaluate the proposed spdsl exploiting affine - invariant metric ( aim ) , stein divergence and log - euclidean metric ( lem ) .    to evaluate the effectiveness of the proposed spdsl approach , we compare three categories of spd - based learning methods , including basic riemannian metric baseline methods , kernel learning based spd discriminant learning methods and dimensionality reduction based spd discriminant learning methods :    1 .",
    "basic riemannian metrics on spd manifold : + affine - invariant metric ( aim ) @xcite , stein divergence @xcite , log - euclidean metric ( lem ) @xcite 2 .",
    "kernel learning based spd matrix learning methods : + pls - based covariance discriminative learning ( cdl ) @xcite , riemannian sparse representation ( rsr ) @xcite and log - euclidean kernels ( lek ) @xcite 3",
    ".   dimensionality reduction based spd matrix learning methods : + log - euclidean metric learning ( leml ) @xcite and spd manifold learning ( spdml - aim and spdml - stein ) @xcite with aim and stein divergence    note that , the proposed spdsl belongs to the last category of spd discriminant learning methods . as",
    "this paper focuses on studying the problem of supervised spd discriminant learning , we here only report the performances of the original discriminant learning methods such as spdml rather than those of further coupling them with other classifiers as done in the work @xcite .",
    "in addition , in order to study the discriminant learning power of our proposed framework , we replace its supervised learning scheme with that of spdml but still perform optimization on the exploited solution space .",
    "the adaption of the proposed spdsl is denoted with spdsl - aim@xmath159 , spdsl - stein@xmath159 and spdsl - lem@xmath159 .    for rsr ,",
    "the parameter @xmath93 was densely sampled around the order of the mean distance and the parameter @xmath160 is sampled in the range of @xmath161 $ ] . for lek",
    ", there are three implements based on polynomial , exponential and radial basis kernels , which are respectively denoted as lek-@xmath162 , lek-@xmath163 and lek-@xmath164 . for lek-@xmath162 and lek-@xmath163 , we densely sampled the @xmath165 from 1 to 50 . the parameters @xmath93 in lek-@xmath164 and the @xmath160 in the three lek versions were all tuned in the same way as rsr . for leml",
    ", the parameter @xmath166 is tuned in the range of [ 0.1 , 1 , 10 ] , and @xmath167 is tuned from 0.1 to 0.5 .",
    "for spdml and our method spdsl , the maximum iteration number of the optimization algorithm is set to 50 , the parameters @xmath103 is fixed as the minimum number of samples in one class , the dimensionality of the lower - dimensional spd manifold and @xmath106 were tuned by cross - validation .",
    "the parameter @xmath93 in our method is set to @xmath94 , where @xmath95 is equal to the mean distance of all pairs of training data .",
    "[ fig3 ]    [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab4 ]      we then employ the hdm05 database @xcite to handle with the problem of human action recognition from motion capture sequences . as shown in fig.[fig3 ] , this dataset contains 2,337 sequences of 130 motion classes , e.g. , _ `",
    "clap above head',`lie down floor',`rotate arms ' , ` throw basket ball ' _ , in 10 to 50 realizations executed by various actors .",
    "the 3d locations of 31 joints of the subjects are provided over time acquired at the speed of 120 frames per second . following the previous works @xcite",
    ", we represent an action of a @xmath168 joints skeleton observed over m frames by its joint covariance descriptor .",
    "this descriptor is an form of spd matrix of size @xmath169 , which is computed by the second order statistics of 93-dimensional vectors concatenating the 3d coordinates of the 31 joints in each frame .    as the evaluation protocol on uiuc , on this dataset",
    ", we also conduct 10 times random evaluations , in which half of sequences ( around 1,100 sequences ) are randomly selected for training data , and the rest are used for testing . on the hdm05 database ,",
    "the work @xcite only used 14 motion classes for evaluation while we tested these methods for identifying 130 action classes .",
    "table.[tab4 ] summarizes the performances of the comparative algorithms on the uiuc dataset . in the evaluation ,",
    "the dimensions of resulting manifolds achieved by dimensionality reduction methods are all set as 30 .",
    "different from the last two evaluations , cdl and rsr performance worse than other competing methods .",
    "the proposed spdsl again improves the existing dimensionality reduction based methods leml and spdml with 1%-3% , and achieve state - of - the - art performance on the hdm05 database .      since our method spdsl and the two methods spdml , leml adopt the same spd matrix learning scheme , we here mainly make two pieces of discussions between them .",
    "first , compared with the related manifold learning method spdml , our spdsl framework proposes a more general solution and a more favorable objective function .",
    "this point has been validated by the three evaluations . as can be seen from table [ tab2 ] , table [ tab3 ] and table [ tab4 ] ,",
    "there are two key conclusions observed from the three visual recognition tasks :    \\a ) as for the new solution , its main benefits lie in enlarging the search domain and opening up the possibility of using non - affine invariant metrics ( e.g. lem ) . while spdml * for affine invariant metrics aim and stein improves spdml mildly ( this may depend on the data ) , the gains of spdml*-lem over the aim and stein cases are relatively obvious , i.e. 1.65% , 2.15% , 6.21% on average , respectively for the three datasets .",
    "\\b ) the new objective function ( for similarity regression ) is quite different from that ( for graph embedding ) used in [ 5 ] . while it s hard to theoretically prove the gains , we have empirically studied its priority . by comparing spdsl with spdml * , the improvements for the three datasets are 2.13% , 1.03% , 6.34% on average for the three used databases , respectively .",
    "second , in contrast to leml which focuses on metric learning , our spdsl learns discriminative similarities on spd manifolds . besides , while leml performs metric learning on the tangent space of spd manifolds , the proposed spdsl learns similarity directly on the spd manifolds .",
    "intuitively , our learning scheme would more faithfully respect the riemannian geometry of the data space , and thus could lead to more favorable spd features for classification tasks . from the above three evaluations , we can see some improvements of spdsl over leml .",
    "we have proposed a geometry - aware spd similarity learning ( spdsl ) framework for more robust visual classification tasks . under this framework , by exploiting the riemannian geometry of psd manifolds , we open the possibility of directly learning the manifold - manifold transformation matrix . to achieve the discriminant learning on the spd features , this work devises a new spdsl technique working on spd manifolds . with the objective of the proposed spdsl",
    ", we derive an optimization algorithm on psd manifolds to pursue the transformation matrix .",
    "extensive evaluations have studied both the effectiveness of efficiency of our spdsl on three challenging datasets .    for future work , the study on the relationship between the selected riemannian metrics of psd manifolds and spd manifolds would be interesting for the problem of supervised spd similarity learning . besides , if neglecting the designed discriminant function on spd features , learning the transformation on spd features for object sets is equal to learning the projection on single object features .",
    "thus , this work can be extended to learn hierarchical representations on object feature by leveraging the current powerful deep learning techniques .",
    "this work has been carried out mainly at the institute of computing technology ( ict ) , chinese academy of sciences ( cas ) .",
    "it is partially supported by 973 program under contract no .",
    "2015cb351802 , natural science foundation of china under contracts nos .",
    "61390511 , 61173065 , 61222211 , and 61379083 .",
    "m.  t. harandi , c.  sanderson , r.  hartley , and b.  c. lovell , `` sparse coding and dictionary learning for symmetric positive definite matrices : a kernel approach , '' in _ proc . euro",
    "comput . vision _ , 2012 .",
    "m.  e. hussein , m.  torki , m.  a. gowayyed , and m.  el - saban , `` human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations , '' in _ international joint conf . on artificial intelligence _",
    ", 2013 .",
    "v.  arsigny , p.  fillard , x.  pennec , and n.  ayache , `` geometric means in a novel vector space structure on symmetric positive - definite matrices , '' _ siam j. matrix analysis and applications _ , vol .  29 , no .  1 ,",
    "pp . 328347 , 2007 .",
    "m.  faraki , m.  t. harandi , and f.  porikli , `` approximate infinite - dimensional region covariance descriptors for image classification , '' in _ international conference on acoustics , speech and signal processing _ , 2015 , pp .",
    "13641368 .",
    "m.  t. harandi , r.  hartley , b.  lovell , and c.  sanderson , `` sparse coding on symmetric positive definite manifolds using bregman divergences , '' _ ieee transactions on neural networks and learning systems _ , vol .",
    "27 , no .  6 , pp .",
    "12941306 , 2016 .",
    "z.  huang , r.  wang , s.  shan , x.  li , and x.  chen .",
    ", `` log - euclidean metric learning on symmetric positive definite manifold with application to image set classification , '' in _ proc .",
    "_ , 2015 .",
    "s.  bonnabel and r.  sepulchre , `` riemannian metric and geometric mean for positive semidefinite matrices of fixed rank , '' _ siam journal on matrix analysis and applications _ ,",
    "31 , no .  3 , pp .",
    "10551070 , 2009 .",
    "s.  yan , d.  xu , b.  zhang , h.  zhang , q.  yang , and s.  lin , `` graph embedding and extensions : a general framework for dimensionality reduction , '' _ ieee trans . on pattern",
    "_ , vol .  29 , no .  1 ,",
    "pp . 4051 , 2007 .",
    "a.  al - mohy and n.  higham ,",
    "`` computing the frchet derivative of the matrix exponential , with an application to condition number estimation , '' _ siam journal on matrix analysis and applications _",
    "30 , no .  4 , pp .",
    "16391657 , 2009 ."
  ],
  "abstract_text": [
    "<S> symmetric positive definite ( spd ) matrices have been widely used for data representation in many visual recognition tasks . </S>",
    "<S> the success mainly attributes to learning discriminative spd matrices with encoding the riemannian geometry of the underlying spd manifold . in this paper </S>",
    "<S> , we propose a geometry - aware spd similarity learning ( spdsl ) framework to learn discriminative spd features by directly pursuing manifold - manifold transformation matrix of column full - rank . </S>",
    "<S> specifically , by exploiting the riemannian geometry of the manifold of fixed - rank positive semidefinite ( psd ) matrices , we present a new solution to reduce optimizing over the space of column full - rank transformation matrices to optimizing on the psd manifold which has a well - established riemannian structure . under this solution , we exploit a new supervised spd similarity learning technique to learn the transformation by regressing the similarities of selected spd data pairs to their ground - truth similarities on the target spd manifold . to optimize the proposed objective function </S>",
    "<S> , we further derive an algorithm on the psd manifold . </S>",
    "<S> evaluations on three visual classification tasks show the advantages of the proposed approach over the existing spd - based discriminant learning methods .    </S>",
    "<S> discriminative spd matrices , riemannian geometry , spd manifold , geometry - aware spd similarity learning , psd manifold . </S>"
  ]
}