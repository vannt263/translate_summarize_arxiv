{
  "article_text": [
    "in many medical studies , estimating the failure time distribution function , or quantities that depend on this distribution , as a function of patient demographic and prognostic variables , is of central importance for risk assessment and health planing . frequently , such data is subject to right censoring .",
    "the goal of this paper is to develop tools for analyzing such data using machine learning techniques .    traditional approaches to right censored failure time analysis",
    "include using parametric models , such as the weibull distribution , and semiparametric models such as proportional hazard models ( see * ? ? ?",
    "* for both ) .",
    "even when less stringent models  such as nonparametric estimation  are used , it is typically assumed that the distribution function is smooth in both time and covariates @xcite .",
    "these assumptions seem restrictive , especially when considering today s high - dimensional data settings . in this paper",
    ", we propose a support vector machine ( svm ) learning method for right censored data .",
    "the choice of svm is motivated by the fact that svm learning methods are easy - to - compute techniques that enable estimation under weak or no assumptions on the distribution @xcite .",
    "svm learning methods , which we review in detail in section  [ sec : notation ] , are a collection of algorithms that attempt to minimize the risk with respect to some loss function .",
    "an svm learning method typically minimizes a regularized version of the empirical risk over some reproducing kernel hilbert space ( rkhs ) .",
    "the resulting minimizer is referred to as the svm decision function .",
    "the svm learning method is the mapping that assigns to each data set its corresponding svm decision function .",
    "we adapt the svm framework to right censored data as follows .",
    "first , we represent the distribution s quantity of interest as a bayes decision function , i.e. , a function that minimizes the risk with respect to a loss function .",
    "we then construct a data - dependent version of this loss function using inverse - probability - of - censoring weighting @xcite .",
    "we then minimize a regularized empirical risk with respect to this data - dependent loss function to obtain an svm decision function for censored data .",
    "finally , we define the svm learning method for censored data as the mapping that assigns for every censored data set its corresponding svm decision function .",
    "note that unlike the standard svm decision function , the proposed censored svm decision function is obtained as the minimizer of a data - dependent loss function .",
    "in other words , for each data set , a different minimization loss function is defined . moreover , minimizing the empirical risk no longer consists of minimizing a sum of i.i.d . observations .",
    "consequently , different techniques are needed to study the theoretical properties of the censored svm learning method .",
    "we prove a number of theoretical results for the proposed censored svm learning method .",
    "we first prove that the censored svm decision function is measurable and unique .",
    "we then show that the censored svm learning method is a measurable learning method .",
    "we provide a probabilistic finite - sample bound on the difference in risk between the learned censored svm decision function and the bayes risk .",
    "we further show that the svm learning method is consistent for every probability measure for which the censoring is independent of the failure time given the covariates , and the probability that no censoring occurs is positive given the covariates . finally , we compute learning rates for the censored svm learning method .",
    "we also provide a simulation study that demonstrates the performance of the proposed censored svm learning method .",
    "our results are obtained under some conditions on the approximation rkhs and the loss function , which can be easily verified .",
    "we also assume that the estimation of censoring probability at the observed points is consistent .",
    "we note that a number of other learning algorithms have been suggested for survival data .",
    "@xcite and @xcite used neural networks .",
    "@xcite , @xcite , @xcite , and @xcite , among others , suggested versions of splitting trees and random forests for survival data .",
    "@xcite , @xcite , @xcite , and @xcite , among others , suggested versions of svm different from the proposed censored svm .",
    "the theoretical properties of most of these algorithms have never been studied .",
    "exceptions include the consistency proof of @xcite for random survival trees , which requires the assumption that the feature space is discrete and finite . in the context of multistage decision problems",
    ", @xcite proposed a q - learning algorithm for right censored data for which a theoretical justification is given , under the assumption that the censoring is independent of both failure time and covariates .",
    "however , both of these theoretically justified algorithms are not svm learning methods .",
    "therefore , we believe that the proposed censored svm and the accompanying theoretical evaluation given in this paper represent a significant innovation in developing methodology for learning in survival data .    although the proposed censored svm approach enables the application of the full svm framework to right censored data , one potential drawback is the need to estimate the censoring probability at observed failure times .",
    "this estimation is required in order to use inverse - probability - of - censoring weighting for constructing the data - dependent loss function .",
    "we remark that in many applications it is reasonable to assume that the censoring mechanism is simpler than the failure - time distribution ; in these cases , estimation of the censoring distribution is typically easier than estimation of the failure distribution .",
    "for example , the censoring may depend only on a subset of the covariates , or may be independent of the covariates ; in the latter case , an efficient estimator exists .",
    "moreover , when the only source of censoring is administrative , in other words , when the data is censored because the study ends at a prespecified time , the censoring distribution is often known to be independent of the covariates . fortunately , the results presented in this paper hold for any censoring estimation technique .",
    "we present results for both correctly specified and misspecified censoring models .",
    "we also discuss in detail the special cases of the kaplan - meier and the cox model estimators @xcite .    while the main contribution of this paper is the proposed censored svm learning method and the study of its properties , an additional contribution is the development of a general machine learning framework for right censored data .",
    "the principles and definitions that we discuss in the context of right censored data , such as learning methods , measurability , consistency , and learning rates , are independent of the proposed svm learning method . this framework can be adapted to other learning methods for right censored data , as well as for learning methods for other missing data mechanisms .",
    "the paper is organized as follows . in section  [ sec : notation ] we review right - censored data and svm learning methods . in section",
    "[ sec : no_censoring ] we briefly discuss the use of svm for right - censored data when no censoring is present .",
    "section  [ sec : censoring ] discusses the difficulties that arise when applying svm to right censored data and presents the proposed censored svm learning method .",
    "section  [ sec : main ] contains the main theoretical results , including finite sample bounds and consistency .",
    "simulations appear in section  [ sec : simulation ] .",
    "concluding remarks appear in section  [ sec : summary ] . the lengthier key proofs",
    "are provided in the appendix .",
    "finally , the matlab code for both the algorithm and the simulations can be found in [ sec : suppa ] .",
    "in this section , we establish the notation used throughout the paper .",
    "we begin by describing the data setup ( section  [ subsec : right_censored_data ] ) .",
    "we then discuss loss functions ( section  [ subsec : loss ] ) .",
    "finally we discuss svm learning methods ( section  [ subsec : svm ] ) .",
    "the notation for right censored data generally follows @xcite . for the loss function and the svm definitions , we follow @xcite .",
    "we assume the data consist of @xmath0 independent and identically - distributed random triplets @xmath1 .",
    "the random vector @xmath2 is a covariate vector that takes its values in a set @xmath3 .",
    "the random variable @xmath4 is the observed time defined by @xmath5 , where @xmath6 is the failure time , @xmath7 is the censoring time , and where @xmath8 .",
    "the indicator @xmath9 is the failure indicator , where @xmath10 is @xmath11 if @xmath12 is true and @xmath13 otherwise , i.e. , @xmath14 whenever a failure time is observed .",
    "let @xmath15 be the survival functions of @xmath16 , and let @xmath17 be the survival function of @xmath7 .",
    "we make the following assumptions :    1 .",
    "@xmath7 takes its values in the segment @xmath18 $ ] for some finite @xmath19 , and @xmath20.[as : positiverisk ] 2 .",
    "@xmath7 is independent of @xmath16 , given @xmath2.[as : t_independent_c ]    the first assumption assures that there is a positive probability of censoring over the observation time range ( @xmath18 $ ] ) .",
    "note that the existence of such a @xmath21 is typical since most studies have a finite time period of observation . in the above",
    ", we also define @xmath22 to be the left - hand limit of a right continuous function @xmath23 with left - hand limits .",
    "the second assumption is standard in survival analysis and ensures that the joint nonparametric distribution of the survival and censoring times , given the covariates , is identifiable .",
    "we assume that the censoring mechanism can be described by some simple model .",
    "below , we consider two possible examples , although the main results do not require any specific model .",
    "first , we need some notation . for every @xmath24 $ ] , define @xmath25 and @xmath26 .",
    "note that since we are interested in the survival function of the censoring variable , @xmath27 is the counting process for the censoring , and not for the failure events , and @xmath28 is the at - risk process for observing a censoring time . for a cadlag function @xmath12 on @xmath29 $ ] , define the product integral @xmath30 @xcite .",
    "define @xmath31 to be the empirical measure , i.e. , @xmath32 .",
    "define @xmath33 to be the expectation of @xmath34 with respect to @xmath35 .",
    "[ ex : km ] * independent censoring : * assume that @xmath7 is independent of both @xmath16 and @xmath2 .",
    "define @xmath36 then @xmath37 is the kaplan - meier estimator for @xmath38 .",
    "@xmath39 is a consistent and efficient estimator for the survival function @xmath38 .",
    "[ ex : ph ] * the proportional hazards model : * consider the case that the hazard of @xmath7 give @xmath2 is of the form @xmath40 for some unknown vector @xmath41 and some continuous unknown nondecreasing function @xmath42 with @xmath43 and @xmath44 .",
    "let @xmath45 be the zero of the estimating equation @xmath46 define @xmath47 then @xmath48 is a consistent and efficient estimator for survival function @xmath38 .",
    "even when no simple form for the censoring mechanism is assumed , the censoring distribution can be estimated using a generalization of the kaplan - meier estimator of example  [ ex : km ] .",
    "[ ex : generalized_km ] * generalized kaplan - meier : * let @xmath49 be a kernel function of width @xmath50 . define @xmath51 and @xmath52 .",
    "define @xmath53 then the generalized kaplan - meier estimator is given by @xmath54 , where the product integral @xmath55 is defined for every fixed @xmath56 . under some conditions , @xcite proved consistency of the estimator and discussed its convergence rates .",
    "usually we denote the estimator of the survival function of the censoring variable @xmath57 by @xmath58 without referring to a specific estimation method . when needed , the specific estimation method will be discussed .",
    "when independent censoring is assumed , as in example  [ ex : km ] , we denote the estimator by @xmath59 .",
    "[ rem : survival_greater than_zero ] by assumption ( a[as : positiverisk ] ) , @xmath60 , and thus if the estimator @xmath39 is consistent for @xmath38 , then , for all @xmath0 large enough , @xmath61 . in the following , for simplicity , we assume that the estimator @xmath39 is such that @xmath62 . in general",
    ", one can always replace @xmath39 by @xmath63 , where @xmath64 . in this case , for all @xmath0 large enough , @xmath62 and for all @xmath0 , @xmath65 .",
    "let the input space @xmath66 be a measurable space .",
    "let the response space @xmath67 be a closed subset of @xmath68 .",
    "let @xmath35 be a measure on @xmath69 .",
    "a function @xmath70 is a _",
    "loss function _ if it is measurable .",
    "we say that a loss function @xmath71 is _ convex _ if @xmath72 is convex for every @xmath73 and @xmath74 .",
    "we say that a loss function @xmath71 is _ locally lipschitz continuous _ with lipschitz local constant function @xmath75 if for every @xmath76 @xmath77\\,.\\end{aligned}\\ ] ] we say that @xmath71 is _ lipschitz continuous _ if there is a constant @xmath78 such that the above holds for any @xmath79 with @xmath80 .",
    "for any measurable function @xmath81 we define the _ @xmath71-risk _ of @xmath34 with respect to the measure @xmath35 as @xmath82 $ ] .",
    "we define the _ bayes risk _ @xmath83 of @xmath34 with respect to loss function @xmath71 and measure @xmath35 as @xmath84 , where the infimum is taken over all measurable functions @xmath81 .",
    "a function @xmath85 that achieves this infimum is called a bayes decision function .",
    "we now present a few examples of loss functions and their respective bayes decision functions . in the next section",
    "we discuss the use of these loss functions for right censored data .",
    "[ ex : classification ] * binary classification : * assume that @xmath86 .",
    "we would like to find a function @xmath87 such that for almost every @xmath56 , @xmath88 .",
    "one can think of @xmath34 as a function that predicts the label @xmath89 of a pair @xmath90 when only @xmath56 is observed . in this case , the desired function is the bayes decision function @xmath85 with respect to the loss function @xmath91 . in practice , since the loss function @xmath92 is not convex , it is usually replaced by the hinge loss function @xmath93 .",
    "[ ex : expectation ] * expectation : * assume that @xmath94 .",
    "we would like to estimate the expectation of the response @xmath95 given the covariates @xmath2 .",
    "the conditional expectation is the bayes decision function @xmath85 with respect to the squared error loss function @xmath96 .",
    "[ ex : median ] * median and quantiles : * assume that @xmath94 .",
    "we would like to estimate the median of @xmath97 .",
    "the conditional median is the bayes decision function @xmath85 for the absolute deviation loss function latexmath:[$l_{\\mathrm{ad}}(z , y , s)=    given @xmath2 is obtained as the bayes decision function for the loss function @xmath99    note that the functions @xmath100 , @xmath101 , @xmath102 , and @xmath103 for @xmath104 are all convex .",
    "moreover , all these functions except @xmath101 are lipschitz continuous , and @xmath101 is locally lipschitz continuous when @xmath67 is compact .",
    "let @xmath71 be a convex locally lipschitz continuous loss function .",
    "let @xmath105 be a separable reproducing kernel hilbert space ( rkhs ) of a bounded measurable kernel on @xmath106 .",
    "let @xmath107 be a set of @xmath0 i.i.d .",
    "observations drawn according to the probability measure @xmath35 .",
    "fix @xmath108 and let @xmath105 be as above .",
    "define the _ empirical svm decision function _",
    "@xmath109 where @xmath110 is the empirical risk .",
    "for some sequence @xmath111 , define the _ svm learning method @xmath112 _ , as the map @xmath113 for all @xmath114 .",
    "we say that @xmath112 is _ measurable _ if it is measurable for all @xmath0 with respect to the minimal completion of the product @xmath50-field on @xmath115 .",
    "we say that that @xmath112 is ( @xmath71-risk ) @xmath35-consistent if for all @xmath116 @xmath117 we say that @xmath112 is _ universally consistent _ if for all distributions @xmath35 on @xmath69 , @xmath112 is @xmath35-consistent .",
    "we now briefly summarize some known results regarding svm learning methods needed for our exposition .",
    "more advanced results can be obtained using conditions on the functional spaces and clipping .",
    "we will discuss these ideas in the context of censoring in section  [ sec : main ] .",
    "[ thm : regular_svm ] let @xmath70 be a convex lipschitz continuous loss function such that @xmath118 is uniformly bounded .",
    "let @xmath105 be a separable rkhs of a bounded measurable kernel on the set @xmath3 .",
    "choose @xmath119 such that @xmath120 , and @xmath121 .",
    "then    a.   the empirical svm decision function @xmath122 exists and is unique .",
    "b.   the svm learning method @xmath112 defined in   is measurable . c.   the @xmath71-risk @xmath123 .",
    "d.   if the rkhs @xmath105 is dense in the set of integrable functions on @xmath106 , then the svm learning method @xmath112 is universally consistent .",
    "the proof of  ( a ) follows from , lemma  5.1 and theorem  5.2 . for the proof of",
    "( b ) , see , lemma  6.23 .",
    "the proof of  ( c ) follows from theorem  6.24 .",
    "the proof of  ( d ) follows from , theorem  5.31 , together with theorem  6.24 .",
    "in this section we present a few examples of the use of svm for survival data but without censoring . we show how different quantities obtained from the conditional distribution of @xmath16 given @xmath2 can be represented as bayes decision functions .",
    "we then show how svm learning methods can be applied to these estimation problems and briefly review theoretical properties of such svm learning methods . in the next section",
    "we will explain why these standard svm techniques can not be employed directly when censoring is present .",
    "let @xmath124 be a random vector where @xmath2 is a covariate vector that takes its values in a set @xmath3 , @xmath16 is survival time that takes it values in @xmath125 $ ] for some positive constant @xmath21 , and where @xmath124 is distributed according to a probability measure @xmath35 on @xmath126 .",
    "note that the conditional expectation @xmath127 $ ] is the bayes decision function for the least squares loss function @xmath101 .",
    "in other words @xmath128={\\operatornamewithlimits{argmin}}_f p[l_{\\mathrm{ls}}(z , t , f(z))]\\,,\\end{aligned}\\ ] ] where the minimization is taken over all measurable real functions on @xmath106 ( see example  [ ex : expectation ] ) .",
    "similarly , the conditional median and the @xmath129-quantile of @xmath130 can be shown to be the bayes decision functions for the absolute deviation function @xmath102 and @xmath103 , respectively ( see example  [ ex : median ] ) . in the same manner",
    ", one can represent other quantities of the conditional distribution @xmath130 using bayes decision functions .    defining quantities computed from the survival function as bayes decision functions",
    "is not limited to regression ( i.e. , to a continuous response ) .",
    "classification problems can also arise in the analysis of survival data ( see , for example , * ? ? ?",
    "* ; * ? ? ?",
    "for example , let @xmath131 , @xmath132 , be a cutoff constant .",
    "assume that survival to a time greater than @xmath131 is considered as death unrelated to the disease ( i.e. , remission ) and a survival time less than or equal to @xmath131 is considered as death resulting from the disease .",
    "denote @xmath133 in this case , the decision function that predicts remission when the probability of @xmath134 given the covariates is greater than @xmath135 and failure otherwise is a bayes decision function for the binary classification loss @xmath92 of example  [ ex : classification ] .",
    "let @xmath136 be a data set of @xmath0 i.i.d .",
    "observations distributed according to @xmath35 .",
    "let @xmath137 where @xmath138 is some deterministic measurable function . for regression problems ,",
    "@xmath95 is typically the identity function and for classification @xmath95 can be defined , for example , as in  .",
    "let @xmath71 be a convex locally lipschitz continuous loss function , @xmath70 .",
    "note that this includes the loss functions @xmath139 , @xmath103 , and @xmath100 .",
    "define the empirical decision function as in   and the svm learning method @xmath112 as in  .",
    "then it follows from theorem  [ thm : regular_svm ] that for an appropriate rkhs @xmath105 and regularization sequence @xmath111 , @xmath112 is measurable and universally consistent .",
    "in the previous section , we presented a few examples of the use of svm for survival data without censoring . in this section",
    "we explain why standard svm techniques can not be applied directly when censoring is present .",
    "we then explain how to use inverse probability of censoring weighting @xcite to obtain a censored svm learning method .",
    "finally , we show that the obtained censored svm learning method is well defined .",
    "let @xmath1 be a set of @xmath0 i.i.d .",
    "random triplets of right censored data ( as described in section  [ subsec : right_censored_data ] ) .",
    "let @xmath140 be a convex locally lipschitz loss function .",
    "let @xmath105 be a separable rkhs of a bounded measurable kernel on @xmath106 .",
    "we would like to find an empirical svm decision function .",
    "in other words , we would like to find the minimizer of @xmath141 where @xmath142 is a fixed constant , and @xmath143 is a known function .",
    "the problem is that the failure times @xmath144 may be censored , and thus unknown .",
    "while a simple solution is to ignore the censored observations , it is well known that this can lead to severe bias @xcite .    in order to avoid this bias",
    ", one can reweight the uncensored observations .",
    "note that at time @xmath144 , the @xmath145-th observation has probability @xmath146 not to be censored , and thus , one can use the inverse of the censoring probability for reweighting in @xcite .",
    "more specifically , define the random loss function @xmath147 by @xmath148 where @xmath39 is the estimator of the survival function of the censoring variable based on the set of @xmath0 random triplets @xmath149 ( see section  [ subsec : right_censored_data ] ) .",
    "when @xmath149 is given , we denote @xmath150 .",
    "note that in this case the function @xmath151 is no longer random . in order to show that @xmath151 is a loss function , we need to show that @xmath151 is a measurable function .    [",
    "lem : lnd_is_measurable ] let @xmath71 be a convex locally lipschitz loss function .",
    "assume that the estimation procedure @xmath152 is measurable .",
    "then for every @xmath153 the function @xmath154 is measurable .    by remark  [ rem : survival_greater than_zero ] ,",
    "the function @xmath155 is well defined .",
    "since by definition , both @xmath95 and @xmath71 are measurable , we obtain that @xmath156 is measurable .",
    "we define the _ empirical censored svm decision function _ to be @xmath157 the existence and uniqueness of the empirical censored svm decision function is ensured by the following lemma :    [ lem : unique_csvm ] let @xmath71 be a convex locally lipschitz loss function .",
    "let @xmath105 be a separable rkhs of a bounded measurable kernel on @xmath106 .",
    "then there exists a unique empirical censored svm decision function .    note that given @xmath149 , the loss function @xmath158 is convex for every fixed @xmath56 , @xmath159 , and @xmath160 .",
    "hence , the result follows from lemma  5.1 together with theorem  5.2 of .",
    "note that the empirical censored svm decision function is just the empirical svm decision function of  , after replacing the loss function @xmath71 with the loss function @xmath151",
    ". however , there are two important implications to this replacement .",
    "firstly , empirical censored svm decision functions are obtained by minimizing a different loss function for each given data set .",
    "secondly , the second expression in the minimization problem , namely , @xmath161 is no longer constructed from a sum of i.i.d .",
    "random variables .",
    "we would like to show that the learning method defined by the empirical censored svm decision functions is indeed a learning method .",
    "we first define the term learning method for right censored data or _ censored learning method _ for short .",
    "a censored learning method @xmath162 on @xmath126 maps every data set @xmath163 , @xmath114 , to a function @xmath164 .",
    "choose @xmath119 such that @xmath120 .",
    "define the _ censored svm learning method _ @xmath162 , as @xmath165 for all @xmath114 .",
    "the measurability of the censored svm learning method @xmath162 is ensured by the following lemma , which is an adaptation of lemma 6.23 of to the censored case .",
    "[ lem : lc_is_measurable ] let @xmath71 be a convex locally lipschitz loss function .",
    "let @xmath105 be a separable rkhs of a bounded measurable kernel on @xmath106 .",
    "assume that the estimation procedure @xmath166 is measurable .",
    "then the censored svm learning method @xmath162 is measurable , and the map @xmath167 is measurable .    first , by lemma  2.11 of , for any @xmath168 , the map @xmath169 is measurable .",
    "the survival function @xmath39 is measurable on @xmath170 and by remark  [ rem : survival_greater than_zero ] , the function @xmath171 is well defined and measurable .",
    "hence @xmath172 is measurable .",
    "note that the map @xmath173 where @xmath168 is also measurable .",
    "hence we obtain that the map @xmath174 , defined by @xmath175 is measurable . by lemma  [ lem : unique_csvm ] , @xmath176 is the only element of @xmath105 satisfying @xmath177 by aumann s measurable selection principle , the map @xmath178 is measurable with respect to the minimal completion of the product @xmath50-field on @xmath179 .",
    "since the evaluation map @xmath180 is measurable , the map @xmath181 is also measurable .",
    "in the following , we discuss some theoretical results regarding the censored svm learning method proposed in section  [ sec : censoring ] . in section  [ subsec : clipping ] we discuss function clipping which will serve as a tool in our analysis . in section  [ subsec : finite_sample ]",
    "we discuss finite sample bounds . in section  [ subsec : consistency ]",
    "we discuss consistency .",
    "learning rates are discussed in section  [ subsec : rates ] . finally , censoring model misspecification",
    "is discussed in section  [ subsec : misspecification ] .      in order to establish the theoretical results of this section we first need to introduce the concept of clipping .",
    "we say that a loss function @xmath71 can be clipped at @xmath182 , if , for all @xmath183 , @xmath184 where @xmath185 denotes the clipped value of @xmath186 at @xmath187 , that is , @xmath188 .",
    "the loss functions @xmath100 , @xmath101 , @xmath102 , and @xmath103 can be clipped at some @xmath189 when @xmath190 or @xmath86 .    in our context",
    "the response variable @xmath95 usually takes it values in a bounded set ( see section  [ sec : no_censoring ] ) .",
    "when the response space is bounded , we have the following criterion for clipping .",
    "let @xmath71 be a distance - based loss function , i.e. , @xmath191 for some function @xmath55 .",
    "assume that @xmath192 .",
    "then @xmath71 can be clipped at some @xmath189 .    moreover , when the sets @xmath106 and @xmath67 are compact , we have the following criterion for clipping which is usually easy to check .",
    "[ lem : clipping ] let @xmath106 and @xmath67 be compact .",
    "let @xmath70 be continuous and strictly convex , with a bounded minimizer for every @xmath193 .",
    "then @xmath71 can be clipped at some @xmath189 .",
    "see proof in appendix  [ sec : additional_proofs ] .    for a function @xmath34",
    ", we define @xmath194 to be the clipped version of @xmath34 , i.e. , @xmath195 .",
    "finally , we note that the clipped censored svm learning method , that maps every data set @xmath163 , @xmath114 , to the function @xmath196 is measurable , where @xmath196 is the clipped version of @xmath197 defined in  .",
    "this follows from lemma  [ lem : lc_is_measurable ] , together with the measurability of the clipping operator .",
    "we would like to establish a finite - sample bound for the generalization of clipped censored svm learning methods .",
    "we first need some notation .",
    "define the censoring estimation error @xmath198 to be the difference between the estimated and true survival functions of the censoring variable .",
    "let @xmath105 be an rkhs over the covariates space @xmath3 .",
    "define the @xmath0-th dyadic entropy number as the infimum over @xmath199 , such that @xmath105 can be covered with no more than @xmath200 balls of radius @xmath199 with respect to the metric induced by the norm . for a bounded linear transformation where @xmath23 is a normed space",
    ", we define the dyadic entropy number @xmath201 as . for details ,",
    "the reader is referred to appendix  5.6 of .",
    "define the bayes risk @xmath202 , where the infimum is taken over all measurable functions @xmath81 .",
    "note that bayes risk is defined with respect to both the loss @xmath71 and the distribution @xmath35 .",
    "when a function @xmath203 exists such that @xmath204 we say that @xmath203 is a bayes decision function .",
    "we need the following assumptions :    1 .",
    "the loss function @xmath205 is a locally lipschitz continuous loss function that can be clipped at @xmath182 such that the supremum bound @xmath206 holds for all @xmath207 $ ] and for some @xmath208 .",
    "moreover , there is a constant @xmath209 such that @xmath210 for all @xmath211 and for some @xmath212 .",
    "[ as : locallylipchitzclippable ] 2 .",
    "[ as : var_bound]@xmath105 is a separable rkhs of a measurable kernel over @xmath106 and @xmath35 is a distribution over @xmath126 for which there exist constants @xmath213 $ ] and @xmath214 such that @xmath215 for all @xmath207 $ ] and @xmath168 ; and where @xmath216 is shorthand for the function @xmath217 .",
    "3 .   there are constants @xmath218 and @xmath219 , such that for for all @xmath220 the following entropy bound holds:[as : entropy_bound1 ] @xmath221\\leq ai^{-\\frac{1}{2p}}\\,,\\end{aligned}\\ ] ] where @xmath222 is the embedding of @xmath105 into the space of square integrable functions with respect to the empirical measure @xmath31 .",
    "before we state the main result of this section , we present some examples for which the assumptions above hold :    when @xmath67 is contained in a compact set , assumption  ( b[as : locallylipchitzclippable ] ) holds with @xmath223 for @xmath100 , @xmath102 and @xmath103 and with @xmath224 for @xmath101 ( recall the definitions of the loss functions from section  [ subsec : loss ] ) .",
    "assumption  ( b[as : var_bound ] ) holds trivially for @xmath225 with @xmath226 .",
    "it holds for @xmath101 with @xmath227 for compact @xmath67 . under some conditions on the distribution",
    ", it also holds for @xmath102 and @xmath103 .",
    "when @xmath3 is compact , the entropy bound of assumption  ( b[as : entropy_bound1 ] ) is satisfied for smooth kernels such as the polynomial and gaussian kernels for all @xmath228 .",
    "the assumption also holds for gaussian kernels over @xmath229 for distributions @xmath230 with positive tail exponent .",
    "we are now ready to establish a finite sample bound for the clipped censored svm learning methods :    [ thm : main ] let @xmath71 be a loss function and @xmath105 be an rkhs such that assumptions ( b[as : locallylipchitzclippable])(b[as : entropy_bound1 ] ) hold .",
    "let @xmath231 satisfy @xmath232 for some @xmath233 .",
    "let @xmath58 be an estimator of the survival function of the censoring variable and assume ( a[as : positiverisk])(a[as : t_independent_c ] ) .",
    "then , for any fixed regularization constant @xmath142 , @xmath114 , and @xmath234 , with probability not less than @xmath235 , @xmath236 where @xmath237 is a constant that depends only @xmath238 , @xmath189 , @xmath239 , @xmath240 , @xmath241 and @xmath242 .",
    "the proof appears in appendix  [ sec : thm_main ] .    for the kaplan - meier estimator ( see example  [ ex : km ] ) bounds of the random error @xmath243 were established @xcite . in this case",
    "we can replace the bound of theorem  [ thm : main ] with a more explicit one .",
    "specifically , let @xmath39 be the kaplan - meier estimator .",
    "let @xmath244 be a lower bound on the survival function at @xmath21 .",
    "then , for every @xmath114 and @xmath116 the following dvoretzky - kiefer - wolfowitz - type inequality holds ( * ? ? ?",
    "* theorem  2 ) : @xmath245 where @xmath246 is some universal constant ( see * ? ? ?",
    "* for a bound on @xmath246 ) .",
    "some algebraic manipulations then yield @xcite that for every @xmath234 and @xmath114 @xmath247    as a result , we obtain the following corollary :    consider the setup of theorem  [ thm : main ] .",
    "assume that the censoring variable @xmath7 is independent of both @xmath16 and @xmath2 .",
    "let @xmath39 be the kaplan - meier estimator of @xmath38 .",
    "then for any fixed regularization constant @xmath108 , @xmath114 , and @xmath234 , with probability not less than @xmath248 , @xmath249 where @xmath237 is a constant that depends only on @xmath238 , @xmath189 , @xmath239 , @xmath240 , @xmath241 and @xmath242 .      in this section",
    "we discuss consistency of the clipped version of the censored svm learning method @xmath162 proposed in section  [ sec : censoring ] .",
    "in general , @xmath35-consistency means that   holds for all @xmath116 .",
    "universal consistency means that the learning method is @xmath35-consistent for every probability measure @xmath35 on @xmath251 . in the following we discuss a more restrictive notion than universal consistency , namely @xmath250-universal consistency . here , @xmath250 is the set of all probability distributions for which there is a constant @xmath242 such that conditions  ( a[as : positiverisk])(a[as : t_independent_c ] ) hold .",
    "we say that a censored learning method is @xmath250-universally consistent if   holds for all @xmath252 .",
    "we note that when the first assumption is violated for a set of covariates @xmath253 with positive probability , there is no hope of learning the optimal function for all @xmath254 , unless some strong assumptions on the model are enforced .",
    "the second assumption is required for proving consistency of the learning method @xmath255 proposed in section  [ sec : censoring ] .",
    "however , it is possible that other censored learning techniques will be able to achieve consistency for a larger set of probability measures .    in order to show @xmath250-universal consistency ,",
    "we utilize the bound given in theorem  [ thm : main ] .",
    "we need the following additional assumptions :    1 .   for all distributions @xmath35 on @xmath106 , @xmath256.[as : dense ] 2 .",
    "@xmath39 is consistent for @xmath38 and there is a finite constant @xmath257 such that @xmath258 for any @xmath259.[as : consistency_of_g ]    before we state the main result of this section , we present some examples for which the assumptions above hold :    assumption  ( b[as : dense ] ) holds when the loss function @xmath71 is lipschitz continuous and the rkhs @xmath105 is dense in @xmath260 for all distribution @xmath261 on @xmath106 , where @xmath260 is the space of equivalence classes of integrable functions . .",
    "[ rem : universal ] assume that @xmath106 is compact .",
    "a continuous kernel @xmath262 whose corresponding rkhs @xmath105 is dense in the class of continuous functions over the compact set @xmath106 is called universal",
    ". examples of universal kernels include the gaussian kernels , and other taylor kernels . for more details ,",
    "the reader is referred to . for universal kernels , assumption  ( b[as : dense ] ) holds for @xmath101 , @xmath100 , @xmath102 , and @xmath103 . .",
    "assume that @xmath39 is consistent for @xmath38 .",
    "when @xmath39 is the kaplan - meier estimator , assumption ( b[as : consistency_of_g ] ) holds for all @xmath263 ( * ? ?",
    "* theorem  3 ) .",
    "similarly , when @xmath39 is the proportional hazards estimator ( see example  [ ex : ph ] ) , under some conditions , assumption ( b[as : consistency_of_g ] ) holds for all @xmath263 ( see * ? ? ?",
    "* theorem  3.2 and its conditions ) .",
    "when @xmath39 is the generalized kaplan - meier estimator ( see example  [ ex : generalized_km ] ) , under strong conditions on the failure time distribution , @xcite showed that assumption ( b[as : consistency_of_g ] ) holds for all @xmath264 where @xmath265 is the dimension of the covariate space ( see * ? ? ?",
    "* corollary  2.2 and its conditions there ) .",
    "recently , @xcite relaxed these assumptions and showed that assumption ( b[as : consistency_of_g ] ) holds for all @xmath266 where @xmath104 satisfies @xmath267}|s(t|z_1)-s(t|z_2)|+\\sup_{t\\in[0,\\tau]}|g(t|z_1)-g(t|z_2)|\\right)=o(h^{\\alpha})\\ , ,    \\end{aligned}\\ ] ] where @xmath268 is the survival function of @xmath16 given @xmath269 ( see * ? ? ?",
    "* for the conditions ) .",
    "now we are ready for the main result .",
    "[ thm : puniversal_consistency ] let @xmath71 be a loss function and @xmath105 be an rkhs of a bounded kernel over @xmath106",
    ". assume ( a[as : positiverisk])(a[as : t_independent_c ] ) and ( b[as : locallylipchitzclippable])(b[as : consistency_of_g ] ) .",
    "let @xmath120 , where @xmath119 , and @xmath270 , where @xmath271 is defined in assumption  ( b[as : locallylipchitzclippable ] ) .",
    "then the clipped censored learning method @xmath162 is @xmath250-universally consistent .",
    "define the approximation error @xmath272 by theorem  [ thm : main ] , for @xmath273 we obtain @xmath274 for any fixed regularization constant @xmath142 , @xmath114 , and @xmath234 , with probability not less than @xmath235 .",
    "define @xmath275 where @xmath276 and where @xmath277 and @xmath271 are defined in assumption  ( b[as : locallylipchitzclippable ] ) .",
    "we now show that @xmath278 .",
    "since the kernel @xmath262 is bounded , it follows from that @xmath279 . by the definition of @xmath280 ,",
    "note that for all @xmath193 @xmath282 thus @xmath283    assumption  ( b[as : dense ] ) , together with lemma  5.15 of , shows that @xmath284 converges to zero as @xmath0 converges to infinity .",
    "clearly @xmath285 converges to zero .",
    "@xmath286 converges to zero since @xmath287 . by assumption  ( b[as : consistency_of_g ] ) , @xmath288 converges to zero .",
    "finally , @xmath289 converges to zero since @xmath290 .",
    "hence , for every fixed @xmath291 , the right hand side of   converges to zero , which implies  . since   holds for every @xmath252",
    ", we obtain @xmath250-universal consistency .      in the previous section we discussed @xmath250-universal consistency which ensures that for every probability @xmath252 , the clipped learning method @xmath255 asymptotically learns the optimal function . in this section we would like to study learning rates .",
    "we define learning rates for censored learning methods similarly to the definition for regular learning methods :    let @xmath205 be a loss function .",
    "let @xmath252 be a distribution .",
    "we say that a censored learning method @xmath255 learns with a rate @xmath292 , where @xmath293 $ ] is a sequence decreasing to @xmath13 , if for some constant @xmath294 , all @xmath114 , and all @xmath295 , there exists a constant @xmath296 that depends on @xmath291 and @xmath297 but not on @xmath35 , such that @xmath298    in order to study the learning rates , we need an additional assumption :    1 .   there exist constants @xmath299 and @xmath300 $ ] such that @xmath301 for all @xmath302 , where @xmath303 is the approximation error function defined in  .[as : approx_error ]    [ lem : learning_rates ] let @xmath71 be a loss function and @xmath105 be an rkhs of a bounded kernel over @xmath106 .",
    "assume ( a[as : positiverisk])(a[as : t_independent_c ] ) and ( b[as : locallylipchitzclippable])(b[as : approx_error ] ) .",
    "then the learning rate of the clipped @xmath162 is given by @xmath304 where @xmath271 , @xmath240 , @xmath238 , @xmath186 , and  @xmath305 , are as defined in assumptions  ( b[as : locallylipchitzclippable ] ) ,  ( b[as : var_bound ] ) ,  ( b[as : entropy_bound1 ] ) ,  ( b[as : consistency_of_g ] ) , and  ( b[as : approx_error ] ) , respectively .",
    "before we provide the proof , we derive learning rates for two specific examples",
    ".    * fast rate : * assume that the censoring mechanism is known , the loss function is the square loss , the kernel is gaussian , @xmath106 is compact , @xmath67 is bounded , and let @xmath306 .",
    "it follows that ( b[as : locallylipchitzclippable ] )  holds for @xmath224 , ( b[as : var_bound ] )  holds for @xmath227 , ( b[as : entropy_bound1 ] )  holds for all @xmath219 , and ( b[as : consistency_of_g ] )  holds for all @xmath257 .",
    "thus the obtained rate is @xmath307 , where @xmath116 is an arbitrarily small number .",
    "* standard rate : * assume that the censoring mechanism follows the proportional hazards assumption , the loss function is either @xmath100 , @xmath102 or @xmath103 , the kernel is gaussian , @xmath106 is compact , and let @xmath308 .",
    "it follows that ( b[as : locallylipchitzclippable ] ) holds for @xmath223 , ( b[as : var_bound ] )  holds trivially for @xmath225 , ( b[as : entropy_bound1 ] )  holds for all @xmath219 , and ( b[as : consistency_of_g ] )  holds for all @xmath263 .",
    "thus the obtained rate is @xmath309 , where @xmath116 is an arbitrarily small number .    using assumption  ( b[as : approx_error ] ) and substituting   in   we obtain @xmath310 with probability not less than @xmath235 , for some constant @xmath311 that depends on @xmath238 , @xmath189 , @xmath240 , @xmath299 , @xmath241 , and @xmath242 but not on @xmath35 .",
    "denote @xmath312 it can be shown that for @xmath313 , we obtain @xmath314 to see this , denote @xmath315 , @xmath316 , @xmath317 , @xmath318 and @xmath319 and note that @xmath320 $ ] and that @xmath321 .",
    "then apply lemma a.1.7 of to bound the lhs of  , while noting that the proof of this lemma holds for all @xmath321 .    by assumption  ( b[as : consistency_of_g ] ) and the fact that @xmath322 , there exists a constant @xmath323 that depends only on @xmath291 , such that for all @xmath114 , @xmath324    it then follows that @xmath325 for some constants @xmath326 that depends on @xmath238 , @xmath189 , @xmath240 , @xmath277 , @xmath239 , @xmath241 , and @xmath242 but is independent of @xmath291 , and @xmath327 that depends only on @xmath291 .",
    "in section  [ subsec : consistency ] we showed that under conditions ( b[as : locallylipchitzclippable])(b[as : consistency_of_g ] ) the clipped censored svm learning method @xmath162 is @xmath250-universally consistent . while one can choose the hilbert space @xmath105 and the loss function @xmath71 in advance such that conditions ( b[as : locallylipchitzclippable])(b[as : dense ] ) hold , condition  ( b[as : consistency_of_g ] ) need not hold when the censoring mechanism is misspecified . in the following",
    ", we consider this case .",
    "let @xmath328 be the estimator of the survival function for the censoring variable .",
    "the deviation of @xmath328 from the true survival function @xmath329 can be divided into two terms .",
    "the first term is the deviation of the estimator @xmath328 from its limit , while the second term is the difference between the estimator limit and the true survival function .",
    "more formally , let @xmath330 be the limit of the estimator under the probability measure @xmath35 , and assume it exists .",
    "define the errors @xmath331 note that @xmath332 is a random function that depends on the data , the estimation procedure , and the probability measure @xmath35 , while @xmath333 is a fixed function that depends only on the estimation procedure and the probability measure @xmath35 . when the model is correctly specified , and the estimator is consistent , the second term vanishes .",
    "[ thm : misspecification ] let @xmath71 be a loss function and @xmath105 be an rkhs of a bounded kernel over @xmath106",
    ". assume ( a[as : positiverisk])(a[as : t_independent_c ] ) and ( b[as : locallylipchitzclippable])(b[as : dense ] ) .",
    "let @xmath120 , where @xmath119 and @xmath270 . then",
    ", for every fixed @xmath116 , @xmath334    by  , for every fixed @xmath234 and @xmath114 , @xmath335 for any fixed regularization constant @xmath142 , @xmath114 , and @xmath234 , with probability not less than @xmath235 . since @xmath336 , it follows from the same arguments as in the proof of theorem  [ thm : puniversal_consistency ] , that the first expression on the rhs of   converges in probability to zero . by the law of large numbers , @xmath337 , and the result follows .",
    "theorem  [ thm : misspecification ] proves that even under misspecification of the censored data model , the clipped censored learning method @xmath162 achieves the optimal risk up to a constant that depends on @xmath338 , which is the expected distance of the limit of the estimator from the true distribution .",
    "if the estimator estimates reasonably well , one can hope that this term is small , even under misspecification .",
    "we now show that the additional condition @xmath339 of theorem  [ thm : misspecification ] holds for both the kaplan - meier estimator and the cox model estimator .",
    "* kaplan - meier estimator : * let @xmath39 be the kaplan - meier estimator of @xmath38 .",
    "let @xmath340 be the limit of @xmath39 .",
    "note that @xmath340 is the marginal distribution of the censoring variable .",
    "it follows from   that condition   holds for all @xmath263 .",
    "* cox model estimator:*[ex : cox_convergence ] let @xmath39 be the estimator of @xmath38 when the cox model is assumed ( see example  [ ex : ph ] ) .",
    "let @xmath340 be the limit of @xmath39 .",
    "it has been shown that the limit @xmath340 exists , regardless of the correctness of the proportional hazards model @xcite . moreover , for all @xmath116 , and all @xmath0 large enough , @xmath341 where @xmath342 , @xmath343 are universal constants that depend on the set @xmath106 , the variance of @xmath2 , the constants @xmath242 and @xmath344 , but otherwise do not depend on the distribution @xmath35 ( see * ? ? ?",
    "* theorem 3.2 , and conditions therein ) .",
    "fix @xmath234 and write @xmath345 some algebraic manipulations then yield @xmath346 hence , condition   holds for all @xmath263 .",
    "and @xmath347 . the censoring percentage is given for each sample size .",
    "an observed failure times is represented by an @xmath348 , and an observed censoring time is represented by an @xmath349.,scaledwidth=80.0% ]         and @xmath347 .",
    "the censoring percentage is given for each sample size .",
    "an observed failure times is represented by an @xmath348 , and an observed censoring time is represented by an @xmath349.,scaledwidth=80.0% ]                 and @xmath347 .",
    "the censoring percentage is given for each sample size .",
    "an observed failure times is represented by an @xmath348 , and an observed censoring time is represented by an @xmath349.,scaledwidth=80.0% ]        in this section we illustrate the use of the censored svm learning method proposed in section  [ sec : censoring ] via a simulation study .",
    "we consider five different data - generating mechanisms , including one - dimensional and multidimensional settings , and different types of censoring mechanisms .",
    "we compute the censored svm decision function with respect to the absolute deviation loss function @xmath102 . for this loss function",
    ", the bayes risk is given by the conditional median ( see example  [ ex : median ] ) .",
    "we choose to compute the conditional median and not the conditional mean , since censoring prevents reliable estimation of the unrestricted mean survival time when no further assumptions on the tail of the distribution are made ( see discussions in @xcite ) .",
    "we compare the results of the svm approach to the results obtained by the cox model and to the bayes risk .",
    "we test the effects of ignoring the censored observations .",
    "finally , for multidimensional examples , we also check the benefit of variable selection .    the algorithm presented in section  [ sec : censoring ]",
    "was implemented in the matlab environment . for the implementation we used the spider library for matlab .",
    "the matlab code for both the algorithm and the simulations can be found in [ sec : suppa ] .",
    "the distribution of the censoring variable was estimated using the kaplan - meier estimator ( see example  [ ex : km ] ) .",
    "we used the gaussian rbf kernel @xmath350 , where the width of the kernel @xmath50 was chosen using cross - validation . instead of minimizing the regularized problem",
    ", we solve the equivalent problem : @xmath351 where @xmath105 is the rkhs with respect to the kernel @xmath352 , and @xmath108 is some constant chosen using cross - validation .",
    "note that there is no need to compute the norm of the function @xmath34 in the rkhs space @xmath105 explicitly .",
    "the norm can be obtained using the kernel matrix @xmath242 with coefficients @xmath353 .",
    "the risk of the estimated functions was computed numerically , using a randomly generated data set of size @xmath354 .    in some simulations the failure time is distributed according to the weibull distribution @xcite .",
    "the density of the weibull distribution is given by @xmath355 where @xmath356 is the shape parameter and @xmath357 is the scale parameter .",
    "assume that @xmath358 is fixed and that @xmath359 , where @xmath360 is a constant , @xmath305 is the coefficient vector , and @xmath2 is the covariate vector . in this case , the failure time distribution follows the proportional hazards assumption , i.e. , the hazard rate is given by @xmath361 , where @xmath362 .",
    "when the proportional hazards assumption holds , estimation based on cox regression is consistent and efficient ( see example  [ ex : ph ] ; note that the distribution discussed there is of the censoring variable and not of the failure time , nevertheless , the estimation procedure is similar ) .",
    "thus , when the failure time distribution follows the proportional hazards assumption , we use the cox regression as a benchmark .    in the first",
    "setting , the covariates @xmath2 are generated uniformly on the segment @xmath363 $ ] .",
    "the failure time follows the weibull distribution with shape parameter @xmath364 and scale parameter @xmath365 .",
    "note that the proportional hazards assumption holds .",
    "the censoring variable @xmath7 is distributed uniformly on the segment @xmath366 $ ] where the constant @xmath367 is chosen such that the mean censoring percentage is @xmath368 .",
    "we used @xmath369-fold - cross - validation to choose the kernel width and the regularization constant among the set of pairs @xmath370 we repeated the simulation @xmath371 times for each of the sample sizes @xmath372 , and @xmath347 .    in figure  [",
    "fig : weibull_fig ] , the conditional median obtained by the censored svm learning method and by cox regression are plotted .",
    "the true median is plotted as a reference . in figure",
    "[ fig : weibull_boxplot ] , we compare the risk of the svm method to the median of the survival function obtained by cox regression ( to which we refer as the cox regression median ) .",
    "we also examined the effect of ignoring the censored observations by computing the standard svm decision function for the data set in which all the censored observations were deleted .",
    "both figures show that even though the svm does not use the proportional hazards assumption for estimation , the results are comparable to those of cox regression , especially for larger sample sizes . figure  [ fig : weibull_boxplot ] also shows that there is a non - negligible price for ignoring the censored observations .",
    "the second setting differs from the first setting only in the failure time distribution . in the second setting the failure time distribution follows the weibull distribution with scale parameter @xmath373 .",
    "note that the proportional hazards assumption holds for @xmath374 , but not for the original covariate @xmath2 . in figure",
    "[ fig : weibull2_fig ] , the true , the svm median , and the cox regression median are plotted . in figure",
    "[ fig : weibull2_boxplot ] , we compare the risk of svm to that of cox regression .",
    "both figures show that in this case svm does better than cox regression .",
    "figure  [ fig : weibull2_boxplot ] also shows the price of ignoring censored observations .",
    "the third and forth settings are generalizations of the first two , respectively , to 10-dimensional covariates .",
    "the covariates @xmath2 are generated uniformly on @xmath363^{10}$ ] .",
    "the failure time follows the weibull distribution with shape parameter @xmath364 .",
    "the scale parameter of the third and forth settings are @xmath375 and @xmath376 , respectively .",
    "note that these models are sparse , namely , they depend only on the first three variables . the censoring variable @xmath7 is distributed uniformly on the segment @xmath366 $ ] , where the constant @xmath367 is chosen such that the mean censoring percentage is @xmath377 .",
    "we used @xmath369-fold - cross - validation to choose the kernel width and the regularization constant among the set of pairs @xmath378    the results for the third and the forth settings appears in figure  [ fig : weibull_10d_boxplot ] and figure  [ fig : weibull2_10d_boxplot ] , respectively .",
    "we compare the risk of standard svm that ignores censored observations , censored svm , censored svm with variable selection , and cox regression .",
    "we performed variable selection for censored svm based on recursive feature elimination as in ( * ? ? ?",
    "* section  2.6 ) .",
    "when the proportional hazards assumption holds ( setting  3 ) , svm performs reasonably well , although the cox model performs better as expected .",
    "when the proportional hazard assumption fails to hold ( setting  4 ) , svm performs better and it seems that the risk of cox regression converges , but not to the bayes risk ( see example  [ ex : cox_convergence ] for discussion ) .",
    "both figures show that variable selection achieves a slightly smaller median risk with the price of higher variance and that ignoring the censored observations leads to higher risk .    in the fifth setting",
    ", we consider a non - smooth conditional median .",
    "we also investigate the influence of using a misspecified model for the censoring mechanism .",
    "the covariates @xmath2 are generated uniformly on the segment @xmath363 $ ] .",
    "the failure time is normally distributed with expectation @xmath379 and variance @xmath11 .",
    "note that the proportional hazards assumption does not hold for the failure time .",
    "the censoring variable @xmath7 follows the weibull distribution with shape parameter @xmath364 , and scale parameter @xmath380 which results in mean censoring percentage of @xmath377 .",
    "note that for this model , the censoring is independent of the failure time only given the covariate @xmath2 ( see assumption  ( a[as : t_independent_c ] ) ) .",
    "estimation of the censoring distribution using the kaplan - meier corresponds to estimation under a misspecified model .",
    "since the censoring follows the proportional hazards assumption , estimation using the cox estimator corresponds to estimation under the true model .",
    "we use @xmath369-fold - cross - validation to choose the regularization constant and the width of the kernel , as in setting  1 .    in figure",
    "[ fig : jump_fig ] , the conditional median obtained by the censored svm learning method using both the misspecified and true model for the censoring , and by cox regression , are plotted .",
    "the true median is plotted as a reference . in figure",
    "[ fig : jump_boxplot ] , we compare the risk of the svm method using both the misspecified and true model for the censoring .",
    "we also examined the effect of ignoring the censored observations .",
    "both figures show that in general svm does better than the cox model , regardless of the censoring estimation .",
    "the difference between the misspecified and true model for the censoring is small and the corresponding curves in figure  [ fig : jump_fig ] almost coincide .",
    "figure  [ fig : jump_boxplot ] shows again that there is a non - negligible price for ignoring the censored observations .",
    "we studied an svm framework for right censored data .",
    "we proposed a general censored svm learning method and showed that it is well defined and measurable .",
    "we derived finite sample bounds on the deviation from the optimal risk .",
    "we proved risk consistency and computed learning rates .",
    "we discussed misspecification of the censoring model .",
    "finally , we performed a simulation study to demonstrate the censored svm method .",
    "we believe that this work illustrates an important approach for applying support vector machines to right censored data , and to missing data in general",
    ". however , many open questions remain and many possible generalizations exist .",
    "first , we assumed that censoring is independent of failure time given the covariates , and the probability that no censoring occurs is positive given the covariates .",
    "it should be interesting to study the consequences of violation of one or both assumptions .",
    "second , we have used the inverse - probability - of - censoring weighting to correct the bias induced by censoring . in general , this is not always the most efficient way of handling missing data ( see , for example , * ? ? ?",
    "* chapter  25.5 )",
    ". it would be worthwhile to investigate whether more efficient methods could be developed .",
    "third , we discussed only right - censored data and not general missing mechanisms .",
    "we believe that further development of svm techniques that are able to better utilize the data and to perform under weaker assumptions and in more general settings is of great interest .",
    "[ thm : svr7 ] let @xmath71 be a loss function and @xmath105 be an rkhs that satisfies assumptions ( b[as : locallylipchitzclippable])(b[as : entropy_bound1 ] ) .",
    "let @xmath381 be such that @xmath382 for some @xmath233 .",
    "fix @xmath142 and @xmath234 , and let @xmath168 . then for all @xmath383 , with probability not less than @xmath384 , @xmath385 where @xmath386 is a constant that depends only on @xmath238 , @xmath189 , @xmath240 , and @xmath241 , but not on @xmath34 .",
    "let @xmath390 for every @xmath391 , write @xmath392 define @xmath393 note that for every @xmath168 , @xmath394 .",
    "it can be shown that @xmath395 .",
    "using talagrand s inequality we obtain @xmath396+\\sqrt{\\frac{2\\eta vr^{\\vartheta-2}}{n}}+\\left(\\frac23+\\frac1\\gamma\\right)\\frac{2\\eta b}{nr } \\right)\\geq 1-e^{-\\eta}\\end{aligned}\\ ] ] for every fixed @xmath397 . using assumption  ( a[as : entropy_bound1 ] )",
    ", it can be shown that there is a constant @xmath398 that depends only on @xmath238 , @xmath189 , @xmath240 , and @xmath241 , such that for every @xmath399 @xmath400\\leq \\frac{8}{30}\\end{aligned}\\ ] ] .",
    "substituting @xmath401 in  , and using the bound  , we obtain that with probability of not less than @xmath384 , @xmath402 for all @xmath399 .    using the fact that @xmath383 , some algebraic manipulations yield that for all @xmath403 @xmath404 fix @xmath168 . using the definition of @xmath405 , together with the estimates in   for the probability bound",
    ", we obtain that for @xmath406 the inequality @xmath407 holds with probability not less than @xmath384 , and the desired result follows .        using conditional expectation , we obtain that for every @xmath168 , @xmath413= p\\left[p\\left[\\left.\\frac{\\delta}{g(t|z)}l(z , y , f(z))\\right|z , t\\right]\\right ] \\\\ & = p[l_g(z , u,\\delta , f(z)]= { \\mathcal{r}_{l_g , p}}(f)\\ , .",
    "\\end{split}\\end{aligned}\\ ] ] therefore , we can rewrite the term @xmath414 as @xmath415 where @xmath203 is the bayes decision function .    for every function @xmath168 ,",
    "define the functions @xmath416 as @xmath417 for all @xmath418 . using this notation ,",
    "we can rewrite   as @xmath419 in order to bound @xmath420 we follow the same arguments that lead to  , adapted to our setting .",
    "write @xmath421 since @xmath422 , we obtain from the definition of @xmath423 , and the bound on @xmath381 that @xmath424 $ ] .",
    "it thus follows that @xmath425 using bernstein s inequality for the function @xmath426 , we obtain that with probability not less than @xmath427 , @xmath428 using @xmath429 , we obtain @xmath430 which leads to the bound @xmath431 which holds with probability not less than @xmath427 .",
    "note that by the definition of @xmath423 , we have @xmath432 where we used   in the equalities and   in the inequality .",
    "let @xmath433 .",
    "it follows from the proof of  , eq .",
    "7.8 , together with  , that with probability not less than @xmath427 @xmath434 summarizing , we obtain from   and   that @xmath435    we are now ready to bound the second term in  . by theorem  [ thm : svr7 ] , with probability not less than @xmath384 , for all @xmath383 , @xmath436 where @xmath386 is a constant that depends only on @xmath238 , @xmath189 , @xmath240 , and @xmath437 .      summarizing , we obtain that with probability not less than @xmath235 @xmath441 note that by conditional expectation  , @xmath442 . since @xmath443 and @xmath444 , @xmath445 hence , using the fact that @xmath446 , and some algebraic transformations , we obtain @xmath447    until now we assumed that @xmath383 .",
    "assume now that @xmath448 . by substituting the bounds  ,   and   in  ,",
    "we obtain the following bound , that holds with probability not less than @xmath449 , and where we did not use any assumption on the relation between @xmath0 and @xmath291 : @xmath450 by the definition of @xmath451 , we obtain that @xmath452 . using the fact that @xmath453 , we obtain that @xmath454 and thus the result follows also for the case @xmath448 .",
    "we now show that @xmath459 is continuous at a general point @xmath460 .",
    "let @xmath461 be any sequence that converges to @xmath460 .",
    "let @xmath462 , and assume by contradiction that @xmath463 does not converge to @xmath464 .",
    "since @xmath459 is bounded from above by @xmath465 and @xmath69 is compact , there is a subsequence @xmath466 that converges to some @xmath467 . by the continuity of @xmath71",
    ", there is a further subsequence @xmath468 such that @xmath469 and @xmath470 converges to @xmath471 $ ] . if @xmath472 , then by definition @xmath473 , and hence from the continuity of @xmath71 for all @xmath0 large enough @xmath474 , and we arrive at a contradiction .",
    "assume now that @xmath475 , and without loss of generality , let @xmath476 .",
    "note that @xmath477 is bounded from above by @xmath478 .",
    "chose @xmath479 such that for all @xmath480 , @xmath481 . by the continuity of @xmath71",
    ", there is an @xmath116 such that for all @xmath482 , @xmath483 , and note that @xmath484 . recall that @xmath71 is strictly convex in the last variable , and",
    "hence it must be increasing at @xmath485 for all points @xmath482 ( see for example * ? ? ? * proposition  1.3.5 ) .",
    "consequently , for all @xmath0 big enough , @xmath486 , and we again arrive at a contradiction , since @xmath487 .",
    "we now show that @xmath488 is continuous at a general point @xmath460 .",
    "let @xmath461 be a sequence that converges to @xmath460 .",
    "let @xmath489 .",
    "assume , by contradiction , that @xmath490 does not converge to @xmath458 .",
    "hence , there is a subsequence @xmath491 that converges to some @xmath492 ( @xmath493 can not happen , see above ) .",
    "hence , @xmath494 , and @xmath495 , which contradicts the fact that @xmath496 is strictly convex and therefore has a unique minimizer .",
    "e.  biganzoli , p.  boracchi , l.  mariani , and e.  marubini .",
    "feed forward neural networks for the analysis of censored survival data : a partial logistic regression approach .",
    "_ statist .",
    "_ , 170 ( 10):0 11691186 , 1998",
    ".                              b.  a. johnson , d.  y. lin , j.  s. marron , j.  ahn , j.  parker , and c.  m. perou .",
    "threshhold analyses for inference in high dimension low sample size datasets with censored outcomes . unpublished manuscript , 2004 .",
    "t.  g. karrison .",
    "use of irwin s restricted mean as an index for comparing survival in different treatment groups  interpretation and power considerations . _ controlled clinical trials _ , 180 ( 2):0 151167 , 1997 .        b.  d. ripley and r.  m. ripley .",
    "neural networks as statistical methods in survival analysis . in ri .",
    "dybowski and v.  gant , editors , _ clinical applications of artificial neural networks _ , pages 237255 .",
    "cambridge university press , 2001 .    j.  m. robins , a.  rotnitzky , and l.  p. zhao .",
    "estimation of regression coefficients when some regressors are not always observed .",
    "_ journal of the american statistical association _ , 890 ( 427):0 846866 , 1994 .        p.  k. shivaswamy , w.  chu , and m.  jansche .",
    "a support vector approach to censored targets . in _ proceedings of the 7th ieee international conference on data mining ( icdm 2007 ) , omaha , nebraska , usa _ , pages 655660 .",
    "ieee computer society , 2007 ."
  ],
  "abstract_text": [
    "<S> we develop a unified approach for classification and regression support vector machines for data subject to right censoring . </S>",
    "<S> we provide finite sample bounds on the generalization error of the algorithm , prove risk consistency for a wide class of probability measures , and study the associated learning rates . </S>",
    "<S> we apply the general methodology to estimation of the ( truncated ) mean , median , quantiles , and for classification problems . </S>",
    "<S> we present a simulation study that demonstrates the performance of the proposed approach .    and </S>"
  ]
}