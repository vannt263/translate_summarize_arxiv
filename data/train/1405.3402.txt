{
  "article_text": [
    "these notes are based on a series of three lectures on probability and statistics , given at the aepshep 2012 physics school .",
    "while most of the attendance was composed of phd students , it also included master students and young post - docs ; in consequence , a variable level of familiarity with the topics discussed was implicit . for consistency , the scope of the lectures spanned from very general concepts up to more advanced , recent developments .",
    "the first lecture reviewed basic concepts in probability and statistics ; the second lecture focussed on maximum likelihood and multivariate techniques for statistical analysis of experimental data ; the third and last lecture covered topics on hypothesis - testing and interval estimation .",
    "whenever possible , the notation aligns with common usage in experimental high - energy physics ( hep ) , and the discussion is illustrated with examples related to recent physics results , mostly from the @xmath0-factories and the lhc experiments .",
    "mathematical probability is an abstract axiomatic concept , and probability theory is the conceptual framework to assess the knowledge of random processes .",
    "a detailed discussion of its development and formalism lies outside the scope of these notes .",
    "other than standard classic books , like  @xcite , there are excellent references available , often written by ( high - energy ) physicists , and well - suited for the needs of physicists .",
    "a non - comprehensive list includes  @xcite , and can guide the reader into more advanced topics .",
    "the sections on statistics and probability in the pdg  @xcite are also a useful reference ; often also , the large experimental collaborations have ( internal ) forums and working groups , with many useful links and references .      for a process to be called random , two main conditions are required : its outcome can not be predicted with complete certainty , and if the process is repeated under the very same conditions , the new resulting outcomes can be different each time . in the context of experimental particle physics , such an outcome could be `` a collision '' , or `` a decay '' . in practice",
    ", the sources of uncertainty leading to random processes can be    * due to reducible measurement errors , i.e. practical limitations that can in principle be overcome by means of higher - performance instruments or improved control of experimental conditions ; * due to quasi - irreducible random measurement errors , i.e. thermal effects ; * fundamental , if the underlying physics is intrinsically uncertain , i.e. quantum mechanics is not a deterministic theory .    obviously in particle physics ,",
    "all three kinds of uncertainties are at play .",
    "a key feature of collider physics is that events resulting from particle collisions are independent of each other , and provide a quasi - perfect laboratory of quantum - mechanical probability processes .",
    "similarly , unstable particles produced in hep experiments obey quantum - mechanical decay probabilities .",
    "let @xmath1 be the total universe of possible outcomes of a random process , and let @xmath2 be elements ( or realizations ) of @xmath1 ; a set of such realizations is called a sample . a probability function @xmath3",
    "is defined as a map onto the real numbers : @xmath4 \\ ,",
    "\\nonumber \\\\ x & \\rightarrow & { \\cal p}(x ) \\ . \\end{aligned}\\ ] ] this mapping must satisfy the following axioms : @xmath5 from which various useful properties can be easily derived , i.e. @xmath6 ( where @xmath7 is the complement of @xmath8 ) .",
    "two elements @xmath8 and @xmath9 are said to the independent ( that is , their realizations are not linked in any way ) if @xmath10      conditional probability @xmath11 is defined as the probability of @xmath8 , given @xmath9 .",
    "the simplest example of conditional probability is for independent outcomes : from the definition of independence in eq .",
    "( [ eq : independence ] ) , it follows that if @xmath8 and @xmath9 are actually independent , the condition @xmath12 is satisfied .",
    "the general case is given by bayes theorem : in view of the relation @xmath13 , it follows that @xmath14 an useful corollary follows as consequence of bayes theorem : if @xmath1 can be divided into a number of disjoint subsets @xmath15 ( this division process is called a partition ) , then @xmath16      in the context of these lectures , the relevant scenario is when the outcome of a random process can be stated in numerical form ( i.e. it corresponds to a measurement ) : then to each element @xmath8 ( which for hep - oriented notation purposes , it is preferable to design as an event ) corresponds a variable @xmath17 ( that can be real or integer ) . for continuous @xmath17 , its probability density function ( pdf ) @xmath18",
    "is defined as @xmath19 ) \\ = \\ p(x)dx \\ , \\end{aligned}\\ ] ] where @xmath18 is positive - defined for all values of @xmath17 , and satisfies the normalization condition @xmath20 for a discrete @xmath21 , the above definition can be adapted in a straightworward way : @xmath22 finite probabilities are obtained by integration over a non - infinitesimal range .",
    "it is sometimes convenient to refer to the cumulative density function ( cdf ) : @xmath23 so that finite probabilities can be obtained by evaluating the cdf on the boundaries of the range of interest : @xmath24 other than the conditions of normalization eq .",
    "( [ eq : normalization ] ) and positive - defined ( or more precisely , to have a compact support , which implies that the pdf must become vanishingly small outside of some finite boundary ) the pdfs can be arbitrary otherwise , and exhibit one or several local maxima or local minima . in contrast , the cdf is a monotonically increasing function of @xmath17 , as shown on figure  [ fig : pdfcanvas ] , where a generic pdf and its corresponding cdf are represented .    ; the pdf is assumed to have negligible values outside of the plotted range .",
    "right : the corresponding cumulative density function ( cdf ) , plotted in the same range.,title=\"fig:\",width=283 ] ; the pdf is assumed to have negligible values outside of the plotted range .",
    "right : the corresponding cumulative density function ( cdf ) , plotted in the same range.,title=\"fig:\",width=283 ]      when more than on random number is produced as outcome in a same event , it is convenient to introduce a @xmath25-dimensional set of random elements @xmath26 , together with its corresponding set of random variables @xmath27 and their multidimensional pdf : @xmath28 lower - dimensional pdfs can be derived from eq .",
    "( [ eq : multidimensionalpdf ] ) ; for instance , when one specific variable @xmath29 ( for fixed @xmath30 , with @xmath31 ) is of particlar relevance , its one - dimensional marginal probability density @xmath32 is extracted by integrating @xmath33 over the remaining @xmath34 dimensions ( excluding the @xmath30-th ) : @xmath35 without loss of generality , the discussion can be restricted to the two - dimensional case , with random elements @xmath8 and @xmath9 and random variables @xmath36 .",
    "the finite probability in a rectangular two - dimensional range is @xmath37 for a fixed value of @xmath9 , the conditional density function for @xmath8 is given by @xmath38 as already mentioned , the relation @xmath39 holds only if @xmath8 and @xmath9 are independent ; for instance , the two - dimensional density function in figure  [ fig : pdf2dcanvas ] is an example of non - independent variables , for which @xmath40 .     and",
    "@xmath41 : the pattern implies that in general , the probability densities are larger when @xmath17 and @xmath41 are both large or both small ( i.e. they are positively correlated ) , and thus the variables are not independent .",
    "the pdf is assumed to have negligible values outside of the plotted two - dimensional range.,width=359 ]",
    "the description of a random process via density functions is called a model . loosely speaking",
    ", a parametric model assumes that its pdfs can be completely described using a finite number of parameters  .",
    "a straightforward implementation of a parametric pdf is when its parameters are analytical arguments of the density function ; the notation @xmath42 indicates the functional dependence of the pdf ( also called its shape ) in terms of variables @xmath43 and parameters @xmath44 .      consider a random variable @xmath8 with pdf @xmath18 . for a generic function @xmath45 ,",
    "its expectation value @xmath46 $ ] is the pdf - weighted average over the @xmath17 range : @xmath47 \\ = \\ \\int dx p(x ) f(x ) \\ .\\end{aligned}\\ ] ] being often used , some common expectation values have their own names . for one - dimensional pdfs , the mean",
    "value and variance are defined as @xmath48 \\ = \\",
    "\\int dx p(x ) x \\ , \\\\ { \\rm variance } & : \\ \\sigma^2 \\ = & \\ v[x ] \\ = \\",
    "e[x^2 ] - \\mu^2 \\ = \\",
    "e[(x-\\mu)^2 ] \\ ; \\end{aligned}\\ ] ] for multidimensional pdfs , the covariance @xmath49 and the dimensionless linear correlation coefficient @xmath50 are defined as : @xmath51 -\\mu_i\\mu_j \\ = \\",
    "e[(x_i-\\mu_i)(x_j-\\mu_j ) ] \\ , \\\\ { \\rm linear } \\ { \\rm   correlation } & : \\ \\rho_{ij } \\",
    "= & \\ \\frac{c_{ij}}{\\sigma_i\\sigma_j } \\ .\\end{aligned}\\ ] ] by construction , linear correlation coefficients have values in the @xmath52 interval .",
    "the sign of the @xmath53 coefficient indicates the dominant trend in the @xmath54 pattern : for positive correlation , the probability density is larger when @xmath21 and @xmath55 are both small or large , while a negative correlation indicates that large values of @xmath21 are preferred when small values of @xmath55 are realized ( and viceversa ) . when the random variables @xmath15 and @xmath56 are independent , that is @xmath57 , one has @xmath58 \\ = \\",
    "\\int\\int dx_idx_jp(x_i , x_j ) x_ix_j \\ = \\ \\mu_i\\mu_j \\ , \\end{aligned}\\ ] ] and thus @xmath59 : independent variables have a zero linear correlation coefficient .",
    "note that the converse needs not be true : non - linear correlation patterns among non - independent variables may `` conspire '' and yield null values of the linear correlation coefficient , for instance if negative and positive correlation patterns in different regions of the @xmath60 plane cancel out .",
    "figure  [ fig : corrcanvas ] shows examples of two - dimensional samples , illustrating a few representative correlation patterns among their variables .    ) ; no correlation , with a zero linear correlation coefficient ; a slightly milder , negative correlation ( with @xmath61 ) ; and a more complex correlation pattern , with variables very strongly correlated , but in such a way that the linear correlation coefficient is zero.,title=\"fig:\",width=141 ] ) ; no correlation , with a zero linear correlation coefficient ; a slightly milder , negative correlation ( with @xmath61 ) ; and a more complex correlation pattern , with variables very strongly correlated , but in such a way that the linear correlation coefficient is zero.,title=\"fig:\",width=141 ] ) ; no correlation , with a zero linear correlation coefficient ; a slightly milder , negative correlation ( with @xmath61 ) ; and a more complex correlation pattern , with variables very strongly correlated , but in such a way that the linear correlation coefficient is zero.,title=\"fig:\",width=141 ] ) ; no correlation , with a zero linear correlation coefficient ; a slightly milder , negative correlation ( with @xmath61 ) ; and a more complex correlation pattern , with variables very strongly correlated , but in such a way that the linear correlation coefficient is zero.,title=\"fig:\",width=141 ]      in practice , the true probability density function may not be known , and the accessible information can only be extracted from a finite - size sample ( say consisting of @xmath62 events ) , which is assumed to have been originated from an unknown pdf . assuming that this underlying pdf is parametric , a procedure to estimate its functional dependence and the values of its parameters",
    "is called a characterisation of its shape .",
    "now , only a finite number of expectation values can be estimated from a finite - size sample .",
    "therefore , when choosing the set of parameters to be estimated , each should provide information as useful and complementary as possible ; such procedure , despite being intrinsically incomplete , can nevertheless prove quite powerful .",
    "the procedure of shape characterisation is first illustrated with the one - dimensional case of a single random variable @xmath17 .",
    "consider the empirical average @xmath63 ( also called sample mean ) @xmath64 as shown later in sec .",
    "[ sec : musigma ] , @xmath63 is a good estimator of the mean value @xmath65 of the underlying distribution @xmath18 . in the same spirit ,",
    "the quadratic sample mean ( or root - mean - square rms ) , @xmath66 is a reasonable estimator of its variance @xmath67 ( an improved estimator of variance can be easily derived from the rms , as discussed later in sec .",
    "[ sec : musigma ] ) . intuitively speaking , these two estimators together provide complementary information on the `` location '' and `` spread '' of the region with highest event density in @xmath17 .",
    "the previous approach can be discussed in a more systematic way , by means of the characteristic function , which is a transformation from the @xmath17-dependence of the pdf @xmath18 onto a @xmath68-dependence of @xmath69 $ ] , defined as @xmath70 \\ = \\",
    "e\\left[e^{ik\\frac{x-\\mu}{\\sigma}}\\right ] \\ = \\",
    "\\sum_j \\frac{(ik)^j}{j!}\\mu_j \\ .\\end{aligned}\\ ] ] the coefficients @xmath65 of the expansion are called reduced moments ; by construction , the first moments are @xmath71 and @xmath72 ; these values indicate that in terms of the rescaled variable @xmath73 , the pdf has been shifted to have zero mean and scaled to have unity variance .    in principle , the larger the number of momenta @xmath74 that are estimated , the more detailed is the characterisation of the pdf shape . among higher - order moments ,",
    "the third and fourth have specific names , and their values can be interpreted , in terms of shape , in a relatively straightforward manner .",
    "the third moment is called skewness : a symmetric distribution has zero skewness , and a negative ( positive ) value indicates a larger spread to the left ( right ) of its median .",
    "the fourth moment is called kurtosis ; it is a positive - defined quantity , that ( roughly speaking ) can be related to the `` peakedness '' of a distribution : a large value indicates a sharp peak and long - range tails ( such a distribution is sometimes called leptokurtic ) , while a smaller value reflects a broad central peak with short - range tails ( a so - called platykurtic distribution ) . figure  [ fig : kurtosisetal ] shows a few distributions , chosen to illustrate the relation of momenta and shapes .",
    "the characterization of the shape of a pdf through a sequential estimation of shape parameters discussed in sec .",
    "[ sec : shape ] , aimed at a qualitative introduction to the concept of parameter estimation ( also called point estimation in the litterature ) .",
    "a more general approach is now discussed in this paragraph .",
    "consider a @xmath25-dimensional , @xmath68-parametric pdf , @xmath75 for which the values @xmath76 are to be estimated on a finite - sized sample , by means of a set of estimators denoted @xmath77 .",
    "the estimators themselves are random variables , with their own mean values and variances : their values differ when estimated on different samples .",
    "the estimators should satisfy two key properties : to be consistent and unbaised .",
    "consistency ensures that , in the infinite - sized sample limit , the estimator converges to the true parameter value ; absence of bias ensures that the expectation value of the estimator is , for all sample sizes , the true parameter value .",
    "a biased , but consistent estimator ( also called asymptotically unbiased ) is such that the bias decreases with increasing sample size .",
    "additional criteria can be used to characterize the quality and performance of estimators ; two often mentioned are    * efficiency : an estimator with small variance is said to be more efficient than one with larger variance ; * robustness : this criterion characterizes the sensitivity of an estimator to uncertainties in the underlying pdf .",
    "for example , the mean value is robust against uncertainties on even - order moments , but is less robust with respect to changes on odd - order ones .",
    "note that these criteria may sometimes be mutually conflicting ; for practical reasons , it may be preferable to use an efficient , biased estimator to an unbiased , but poorly convergent one .",
    "the convergence and bias requirements can be suitably illustrated with two classical , useful examples , often encountered an many practical situations : the mean value and the variance .",
    "the empirical average @xmath63 is a convergent , unbiased estimation of the mean value @xmath65 of its underlying distribution : @xmath78 .",
    "this statement can easily be demostrated , by evaluating the expectation value and variance of @xmath63 : @xmath79 & = & \\frac{1}{n } \\sum_{i=1}^n e\\left [ x \\right ] \\ = \\ \\mu \\ ,",
    "\\label{eq : samplemean } \\\\",
    "v\\left [ \\overline{x } \\right ] & = & e\\left [ ( \\overline{x}-\\mu)^2 \\right ] \\ = \\",
    "\\frac{\\sigma^2}{n } \\ .\\end{aligned}\\ ] ] in contrast , the empirical rms of a sample is a biased , asymptotically unbiased estimator of the variance @xmath67 : this can be demonstrated by first rewriting its square ( also called sample variance ) in terms of the mean value : @xmath80 and so its expectation value is @xmath81 \\ = \\ \\sigma^2 - v\\left[\\overline{x}\\right ] \\ = \\",
    "\\frac{n-1}{n}\\sigma^2 \\ , \\end{aligned}\\ ] ] which , while properly converging to the true variance @xmath67 in the @xmath82 limit , systematically underestimates its value for a finite - sized sample .",
    "one can instead define a modified estimator @xmath83 that ensures , for finite - sized samples , an unbiased estimation of the variance .    in summary ,",
    "consistent and unbiased estimators for the mean value @xmath65 and variance @xmath67 of an unknown underlying pdf can be extracted from a finite - sized sample realized out of this pdf : @xmath84 as a side note , the @xmath85 and @xmath86 factors for @xmath87 in eq .",
    "( [ eq : muhat ] ) and for @xmath88 in eq .",
    "( [ eq : sigmahat ] ) can be intuitively understood as follows : while the empirical average be estimated even on the smallest sample consisting of a single event , at least two events are needed to estimate their empirical dispersion .",
    "these two classical examples discussed in  [ sec : musigma ] refer to a single random variable . in presence of several random variables , expressed as a random @xmath25-dimensional vector @xmath89 , the discussion leads to the definition of the empirical covariance matrix , whose elements @xmath90 can be estimated on a sample of @xmath62 events as @xmath91 ( the @xmath92 indices run over random variables , @xmath93 ) . assuming the covariance is known ( i.e. by means of the estimator above , or from first principles ) , the variance of an arbitrary function of these random variables @xmath94 can be evaluated from a taylor - expansion around the mean values @xmath95 as @xmath96 which leads to @xmath97 \\ \\simeq \\",
    "f(\\hat{\\vec{\\mu}})$ ] ; similarly , @xmath98 \\",
    "f^2(\\hat{\\vec{\\mu } } ) + \\sum_{a , b=1}^n\\left.\\frac{df}{dx_a}\\frac{df}{dx_b}\\right|_{\\vec{x}=\\hat{\\vec{\\mu } } } \\hat{c}_{ab } \\ , \\end{aligned}\\ ] ] and thus the variance of @xmath99 can be estimated as @xmath100 this expression in eq .",
    "( [ eq : errorpropagation ] ) , called the error propagation formula , allows to estimate the variance of a generic function @xmath94 from the estimators of mean values and covariances .",
    "a few particular examples of error propagation deserve being mentioning explicitly :    * if all random variables @xmath101 are uncorrelated , the covariance matrix is diagonal , @xmath102 and the covariance of @xmath94 reduces to @xmath103 * for the sum of two random variables @xmath104 , the variance is @xmath105 and the corresponding generalization to more than two variables is straightforward : @xmath106 in absence of correlations , one says that absolute errors are added in quadrature : hence the expression in eq .",
    "( [ eq : sumquadrature ] ) is often written as @xmath107 .",
    "* for the product of two random variables @xmath108 , the variance is @xmath109 with its generalization to more than two variables : @xmath110 in absence of correlations , one says that relative errors are added in quadrature , and eq .",
    "( [ eq : prodquadrature ] ) @xmath111 . * for a generic power law function , @xmath112 , if all variables are uncorrelated , the variance is @xmath113",
    "in this section , a brief description of distributions , often encountered in practical applications , is presented .",
    "the rationale leading to this choice of pdfs is driven either by their specific mathematical properties , and/or in view of their common usage in the modellingi of important physical processes ; such features are correspondingly emphasized in the discussion .        consider a scenario with two possible outcomes : `` success '' or `` failure '' , with a fixed probability @xmath114 of `` success '' being realized ( this is also called a bernouilli trial ) .",
    "if @xmath25 trials are performed , @xmath115 may actually result in `` success '' ; it is assumed that the sequence of trials is irrelevant , and only the number of `` success '' @xmath68 is considered of interest .",
    "the integer number @xmath68 follows the so - called binomial distribution @xmath116 : @xmath117 where @xmath68 is the random variable , while @xmath25 and @xmath114 are parameters .",
    "the mean value and variance are @xmath118 & = & \\sum_{k=1}^n kp\\left(k;n , p\\right ) \\ = \\ np \\ , \\nonumber \\\\",
    "v\\left[k\\right ] & = & np(1-p ) \\ .\\end{aligned}\\ ] ]    , @xmath119 and @xmath120 , respectively.,title=\"fig:\",width=283 ] , @xmath119 and @xmath120 , respectively.,title=\"fig:\",width=283 ]      in the @xmath121 , @xmath122 limit ( with @xmath123 finite and non - zero ) for the binomial distribution , the random variable @xmath68 follows the poisson distribution @xmath124 , @xmath125 for which @xmath126 is the unique parameter . for poisson , the mean value and variance are the same : @xmath118 \\ = \\",
    "v\\left[k\\right ] \\ = \\ \\lambda \\ .\\end{aligned}\\ ] ] the poisson distribution , sometimes called law of rare events ( in view of the @xmath122 limit ) , is a useful model for describing event - counting rates .",
    "examples of a binomial and a poisson distribution , for @xmath127 , @xmath119 and for @xmath120 , respectively , are shown on figure  [ fig : binomialpoisson ] .",
    "the first two continuous random variables discussed in this paragraph are the uniform and the exponential distributions , illustrated in figure  [ fig : uniformexponential ] .    ,",
    "@xmath128 ; for the exponential , two values @xmath129 ( solid line ) and @xmath130 ( dashed line ) are used.,title=\"fig:\",width=283 ] , @xmath128 ; for the exponential , two values @xmath129 ( solid line ) and @xmath130 ( dashed line ) are used.,title=\"fig:\",width=283 ]      consider a continuous random variable @xmath17 , with a probability density @xmath131 that is non - zero only inside a finite interval @xmath132 $ ] : @xmath133 for this uniform distribution , the mean value and variance are @xmath134 & = & \\frac{a+b}{2 } \\ , \\\\ v\\left[x\\right ] & = & \\frac{(b - a)^2}{12 } \\ .\\end{aligned}\\ ] ] while not being the most efficient one , a straightforward monte - carlo generation approach would be based on a uniform distribution in the @xmath135 $ ] range , and use randomly generated values of @xmath17 in the @xmath136 $ ] range as implementation of an accept - reject algorithm with success probability @xmath114 .",
    "consider a continuous variable @xmath17 , with a probability density @xmath137 given by @xmath138 whose mean value and variance are @xmath134 & = & \\xi \\ , \\\\",
    "v\\left[x\\right ] & = & \\xi^2 \\ .\\end{aligned}\\ ] ] a common application of this exponential distribution is the description of phenomena occuring independently at a constant rate , such as decay lengths and lifetimes . in view of the self - similar feature of the exponential function : @xmath139 the exponential distribution is sometimes said to be memoryless .       and",
    "@xmath140.,width=359 ]    now turn to the normal ( or gaussian ) distribution .",
    "consider a random variable @xmath17 , with probability density @xmath141 and with mean value and variance given by @xmath134 \\ & = \\",
    "\\mu \\ , \\\\ v\\left[x\\right ] \\ & = \\ \\sigma \\ .\\end{aligned}\\ ] ] the pdf corresponding to the special @xmath142 , @xmath143 case is usually called `` reduced normal '' .    on purpose ,",
    "the same symbols @xmath65 and @xmath144 have been used both for the parameters of the gaussian pdf and for the mean value and variance .",
    "this is an important feature : the gaussian distribution is uniquely characterized by its first and second moments . for all gaussians , the @xmath145 $ ] covers @xmath146 of pdf integral , and is customary called a `` one - sigma interval '' ; similarly for the two - sigma interval and its @xmath147 .",
    "the dispersion of a peaked distribution is sometimes characterised in terms of its fwhm ( full width at half - maximum ) ; for gaussian distributions , this quantity is uniquely related to its variance , as @xmath148 ; figure  [ fig : gaussian ] provides a graphical illustration of the gaussian pdf and its parameters .    in terms of conceptual relevance and practical applications",
    ", the gaussian certainly outnumbers all other common distributions ; this feature is largely due to the central limit theorem , which asserts that gaussian distributions are the limit of processes arising from multiple random fluctuations .",
    "consider @xmath25 independent random variables @xmath149 , each with mean and variances @xmath150 and @xmath151 ; the variable @xmath152 , built as the sum of reduced variables @xmath153 can be shown to have a distribution that , in the large-@xmath25 limit , converges to a reduced normal distribution , as illustrated in figure  [ fig : theoremcanvas ] for the sum of up to five uniform distributions .",
    "not surprisingly , any measurement subject to multiple sources of fluctuations is likely to follow a distribution that can be approximated with a gaussian distribution to a good approximation , regardless of the specific details of the processes at play .    , with a normal pdf ( solid line ) of same mean and variance ; similarly , the plots on the top right , bottom left and bottom right plot the corresponding histograms for the sums @xmath154 , @xmath155 and @xmath156 of two , three and five reduced uniform variables @xmath157 , respectively .",
    "the sequence of variables follow distributions that quickly converge to a reduced gaussian.,width=548 ]    the gaussian is also encountered as the limiting distribution for the binomial and and poisson ones , in the large @xmath25 and large @xmath126 limits , respectively : @xmath158 note that , when using a gaussian as approximation , an appropriate continuity correction needs to be taken into account : the range of the gaussian extends to negative values , while binomial and poisson are only defined in the positive range .      now the @xmath159 ( or chi - squared ) distributions are considered . the following pdf @xmath160 with a single parameter @xmath25 , and where @xmath161 denotes the gamma function , has mean value and variance given by @xmath134 \\",
    "n \\ , \\nonumber \\\\",
    "v\\left[x\\right ] \\ & = \\ 2n \\ .\\end{aligned}\\ ] ] the shape of the @xmath159 distribution depends thus on @xmath25 , as shown in figure  [ fig : chi2canvas ] where the @xmath159 pdf , for the first six integer values of the @xmath25 parameter , are shown .",
    "probability density functions , for integer numbers of degrees of freedom @xmath25 .",
    "the width of solid lines increases monotonically with @xmath25 in the @xmath162 range.,width=359 ]    it can be shown that the @xmath159 distribution can be written as the sum of squares of @xmath25 normal - reduced variables @xmath21 , each with mean @xmath150 and variance @xmath151 : @xmath163 in view of this important feature of the @xmath159 distribution , the quantity @xmath25 is called `` number of degrees of freedom '' ; this name refers to the expected behaviour of a least - square fit , where @xmath164 data points are used to estimate @xmath165 parameters ; the corresponding number of degrees of freedom is @xmath166 . for a well - behaved fit ,",
    "the @xmath159 value should follow a @xmath159 distribution . as discussed in  [ sec : chi2test ] , the comparison of an observed @xmath159 value with its expectation , is an example of goodness - of - fit test .",
    "the survey closes with the discussion of two physics - motivated pdfs .",
    "the first is the breit - wigner function , which is defined as @xmath167 whose parameters are the most probable value @xmath168 ( which specifies the peak of the distribution ) , and the fwhm @xmath169 .",
    "the breit - wigner is also called lorentzian by physicists , and in matematics it is often referred to as the cauchy distribution .",
    "it has a peculiar feature , as a consequence of its long - range tails : the empirical average and empirical rms are ill - defined ( their variance increase with the size of the samples ) , and can not be used as estimators of the breit - wigner parameters . the truncated mean and interquartile range , which are obtained by removing events in the low and high ends of the sample , are safer estimators of the breit - wigner parameters .    in the context of relativistic kinematics , the breit - wigner function provides a good description of a resonant process ( for example the invariant mass of decay products from a resonant intermediate state ) ; for a resonance , the parameters @xmath168 and @xmath169 are referred to as its mass and its natural width , respectively .",
    "finally , the voigtian function is the convolution of a breit - wigner with a gaussian , @xmath170 and is thus a three - parameter distribution : mass @xmath168 , natural width @xmath169 and resolution @xmath144 . while there is no straightforward analytical form for the voigtian , efficient numerical implementations are available , i.e. the tmath::voigt member function in the root  @xcite data analysis framework , and the roovoigtian class in the roofit  @xcite toolkit for data modeling . for values of @xmath169 and",
    "@xmath144 sufficiently similar , the fwhm of a voigtian can be approximated as a combination of direct sums and sums in quadrature of the @xmath169 and @xmath144 parameters .",
    "a simple , crude approximation yields : @xmath171 + \\gamma/2   \\ .\\end{aligned}\\ ] ] when the instrumental resolutions are sufficiently well described by a gaussian , a voigtian distribution is a good model for observed experimental distributions of a resonant process .",
    "figure  [ fig : bwvoigt ] represents an analytical breit - wigner pdf ( evaluated using the @xmath172 boson mass and width as parameters ) , and a dimuon invariant mass spectrum around the @xmath172 boson mass peak , as measured by the atlas experiment using 8 tev data  @xcite .",
    "the width of the observed peak is interpreted in terms of experimental resolution effects , as indicated by the good data / simulation agreement .",
    "of course , the atlas dimuon mass resolution is more complicated than a simple gaussian ( as hinted by the slightly asymmetric shape of the distribution ) , therefore a simple voigtian function would not have reached the level of accuracy provided by the complete simulation of the detector resolution .",
    "boson mass and width as parameters .",
    "right : the invariant dimuon mass spectrum around the @xmath172 boson mass peak ; the figure is from an atlas measurement using 8 tev data  @xcite.,title=\"fig:\",width=257 ]   boson mass and width as parameters .",
    "right : the invariant dimuon mass spectrum around the @xmath172 boson mass peak ; the figure is from an atlas measurement using 8 tev data  @xcite.,title=\"fig:\",width=309 ]",
    "the discussion in section  [ sec : parameterestimation ] was aimed at describing a few intuitive examples of parameter estimation and their properties .",
    "obviously , such case - by - case approaches are not general enough .",
    "the maximum likelihood estimation ( mle ) is an important general method for parameter estimation , and is based on properties following the maximum likelihood ( ml ) theorem .    consider a sample made of @xmath62 independent outcomes of random variables @xmath173 , arising from a @xmath25-parametric pdf @xmath174 , and whose analytical dependence on variables and parameters is known , but for which the value(s ) of at least one of its parameters @xmath175 is unknown . with the mle , these values are extracted from an analytical expression , called the likelihood function , that has a functional dependence derived from the pdf , and is designed to maximise the probability of realizing the observed outcomes . the likelihood function @xmath176 can be written as @xmath177 this notation shows implictly the functional dependence of the likelihood on the parameters @xmath175 , and on the @xmath62 realizations @xmath173 of the sample .",
    "the ml theorem states that the @xmath178 values that maximize @xmath176 , @xmath179 are estimators of the unknown parameters @xmath175 , with variances @xmath180 that are extracted from the covariance of @xmath176 around its maximum .    in a few cases ,",
    "the mle can be solved analytically .",
    "a classical example is the estimation of the mean value and variance of an arbitrary sample , that can be analytically derived under the assumption that the underlying pdf is a gaussian .",
    "most often though , a numerical study of the likelihood around the @xmath175 parameter space is needed to localize the @xmath181 point that minimizes @xmath182 ( the `` negative log - likelihood '' , or nll ) ; this procedure is called a ml fit .      formally speaking ,",
    "several conditions are required for the ml theorem to hold .",
    "for instance , @xmath176 has to be at least twice derivable with respect to all its @xmath175 parameters ; constraints on ( asymptotical ) unbiased behaviour and efficiency must be satisfied ; and the shape of @xmath176 around its maximum must be normally distributed .",
    "this last condition is particularly relevant , as it ensures the accurate extraction of errors .",
    "when it holds , the likelihood is said ( in a slightly abusive manner ) to have a `` parabolic '' shape ( more in reference to @xmath183 than to the likelihood itself ) , and its expansion around @xmath181 can be written as @xmath184 in eq .",
    "( [ eq : parabolic ] ) the covariance matrix @xmath185 has been introduced , and its elements are given by : @xmath186 \\ .\\end{aligned}\\ ] ] when moving away from its maximum , the likelihood value decreases by amounts that depend on the covariance matrix elements : @xmath187   \\ = \\",
    "\\sum_{i , j}^ { } \\left ( \\theta_i - \\hat{\\theta}_i \\right ) \\mathbf{\\sigma}_{ij}^{-1 } \\left ( \\theta_i - \\hat{\\theta}_j \\right ) \\ .\\end{aligned}\\ ] ] this result , together with the result on error propagation in eq .  ( [ eq : errorpropagation ] ) , indicates that the covariance matrix defines contour maps around the ml point , corresponding to confidence intervals . in the case of a single - parameter likelihood @xmath188 , the interval contained in a @xmath189 contour around @xmath181",
    "defines a @xmath190 confidence interval , corresponding to a @xmath191 range around the ml point ; in consequence the result of this mle can be quoted as @xmath192 .        in a typical scenario ,",
    "the outcome of a random process may arise from multiple sources . to be specific , consider that events in the data sample are a composition of two `` species '' , called generically `` signal '' and `` background '' ( the generalization to scenarios with more than two species is straighforward ) .",
    "each species is supposed to be realized from its own probability densities , yielding similar ( but not identical ) signatures in terms of the random variables in the data sample ; it is precisely these residual differences in pdf shapes that are used by the ml fit for a statistical separation of the sample into species . in the two - species example , the underlying total pdf is a combination of both signal and background pdfs , and the corresponding likelihood function is given by @xmath193 \\ , \\end{aligned}\\ ] ] where @xmath194 and @xmath195 are the pdfs for the signal and background species , respectively , and the signal fraction @xmath196 is the parameter quantifying the signal purity in the sample : @xmath197 .",
    "note that , since both @xmath194 and @xmath195 satisfy the pdf normalization condition from eq .",
    "( [ eq : normalization ] ) , the total pdf used in eq .",
    "( [ eq : mlfit1 ] ) is also normalized .",
    "it is worth mentioning that some of the parameters @xmath175 can be common to both signal and background pdfs , and others may be specific to a single species .",
    "then , depending on the process and the study under consideration , the signal fraction can either have a known value , or belong to the set of unknown parameters @xmath175 to be estimated in a ml fit .      in event - counting experiments ,",
    "the actual number of observed events of a given species is a quantity of interest ; it is then convenient to treat the number of events as an additional parameter @xmath126 of the likelihood function . in the case of a single species",
    ", this amounts to `` extending '' the likelihod , @xmath198 where an additional multiplicative term , corresponding to the poisson distribution ( c.f .",
    "section  [ sec : binomialpoisson ] ) , has been introduced .",
    "( the @xmath199 term in the denominator can be safely dropped ; a global factor has no impact on the shape of the likelihood function nor on the ml fit results ) .",
    "it is straightforward to verify that the poisson likelihod in eq .",
    "( [ eq : emlfit1 ] ) is maximal when @xmath200 , as intended ; now , if some of the pdfs also depend on @xmath126 , the value of @xmath201 that maximises @xmath176 may differ from @xmath62 .",
    "the generalization to more than one species is straightforward as well ; for each species , a multiplicative poisson term is included in the extended likelihood , and the pdfs of each species are weighted by their corresponding relative event fractions ; in presence of two species , the extended version of the likelihood in eq .",
    "[ eq : mlfit1 ] becomes : @xmath202 \\ .\\end{aligned}\\ ] ]      as discussed in section  [ sec : contours ] , the condition in eq .",
    "( [ eq : parabolic ] ) about the asymptotically normal distribution of the likelihood around its maximum is crucial to ensure a proper interpretation of @xmath203 contours in terms of confidence intervals . in this paragraph ,",
    "two scenarios in which this condition can break down are discussed .",
    "plane , obtained from a global electroweak fit ( ew ) performed by the gfitter collaboration  @xcite .",
    "center : the change in the ew likelihood as a function of @xmath204 , expressed in terms of @xmath205 . both plots illustrate the non - parabolic shape of the ew likelihood . , title=\"fig:\",width=283 ]   plane , obtained from a global electroweak fit ( ew ) performed by the gfitter collaboration  @xcite .",
    "center : the change in the ew likelihood as a function of @xmath204 , expressed in terms of @xmath205 .",
    "both plots illustrate the non - parabolic shape of the ew likelihood .",
    ", title=\"fig:\",width=283 ]    ; the definition of cl is such that @xmath206 at the solutions of the ml fit .",
    "an eight - fold constraint on @xmath207 is extracted from measurements in @xmath208 decays by the experiment  @xcite .",
    "the two pairs of solutions around @xmath209 are very close in values , and barely distinguishable in the figure .",
    ", width=359 ]    a first scenario concerns a likelihood that is not totally symmetric around its maximum .",
    "such a feature may occur , when studying low - statistics data samples , in view of the binomial or poissonian behaviour of event - counting related quantities ( c.f . figure  [ fig : binomialpoisson ] ) .",
    "but it can also occur on larger - sized data samples , indicating that the model has a limited sensitivity to the parameter @xmath175 being estimated , or as a consequence of strong non - linear relations in the likelihood function . as illustration",
    ", examples of two- and one - dimensional likelihood contours , with clear non - parabolic shapes , are shown in figure  [ fig : gfitter ] .",
    "also , the likelihood function may possess one or more local maxima , or even completely degenerate maxima .",
    "there are various possible sources for such degeneracies .",
    "for example in models with various species , if some pdf shapes are not different enough to provide a robust statistical discrimination among their corresponding species .",
    "for example , if swapping the pdfs for a pair of species yields a sufficiently good ml fit , a local maximum may emerge ; on different sample realizations , the roles of local and global maxima may alternate among the correct and swapped combinations .",
    "the degeneracies could also arise as a reflexion of explicit , physical symmetries in the model : for example , time - dependent asymmetries in @xmath210 decays are sensitive to the ckm angle @xmath207 , but the physical observable is a function of @xmath211 , with an addtional phase @xmath212 at play ; in consequence , the model brings up to eight indistinguishible solutions for the ckm angle @xmath207 , as illustrated in figure  [ fig : alphababar ] .    in all such cases , the ( possibly disjoint )",
    "@xmath189 interval(s ) around the @xmath181 central value(s ) can not be simply reported with a symmetric uncertainty @xmath180 only . in presence of a single , asymmetric solution",
    ", the measurement can be quoted with asymmetric errors , i.e. @xmath213 , or better yet , by providing the detailed shape of @xmath203 as a function of the estimated parameter @xmath175 . for multiple solutions ,",
    "more information needs to be provided : for example , amplitude analyses ( `` dalitz - plot '' analyses ) produced by the b - factories and belle , often reported the complete covariance matrices around each local solution ( see e.g.  @xcite as examples ) .",
    "consider a process with two possible outcomes : `` yes '' and `` no '' .",
    "the intuitive estimator of the efficiency @xmath214 is a simple ratio , expressed in terms of the number of outcomes @xmath215 and @xmath216 of each kind : @xmath217 for which the variance is given by @xmath218 \\",
    "=   \\ \\frac{\\hat{\\varepsilon}(1-\\hat{\\varepsilon})}{n } \\ , \\end{aligned}\\ ] ] where @xmath219 is the total number of outcomes realized .",
    "this estimator @xmath220 clearly breaks down for low @xmath25 , and in the very low or very high efficiency regimes .",
    "the mle technique offers a robust approach to estimate efficiencies : consider a pdf @xmath221 to model the sample , and include in it an additional discrete , bivariate random variable @xmath222 , so that the pdf becomes @xmath223 \\",
    ".\\end{aligned}\\ ] ] in this way , the efficiency is no longer a single number , but a function of @xmath17 ( plus some parameters @xmath175 that may be needed to characterize its shape ) . with this function , the efficiency can be extracted in different @xmath17 domains , as illustated in figure  [ fig : efficiencycanvas ] , or can be used to produce a multidimensional efficiency map .    .",
    "the sample contains two categories of events : `` accepted '' and `` rejected '' .",
    "the red dots indicate the bins of the histogram for those `` accepted '' only , and the black dots for the two categories cumulated .",
    "right : the efficiency as a parametric function of @xmath17 , with parameter values extracted from a ml fit to the data sample .",
    "figure extracted from the roofit user s guide  @xcite.,width=434 ]      as discussed in section  [ sec : contours ] , in mle the covariance of @xmath176 is the estimator of statistical uncertainties .",
    "other potential sources of ( systematical ) uncertainties are usually at play , and need to be quantified . for this discussion , a slight change in the notation with respect to eq.([eq : likelihood ] ) is useful ; in this notation , the likelihood function is now written as @xmath224 where the parameters are explicitly partitioned into a set @xmath225 , called parameters of interest ( poi ) , that correspond to the actual quantities that are to be estimated ; and a set @xmath76 , called nuisance parameters ( np ) , that represent potential sources of systematic biases : if inaccurate or wrong values are assigned to some nps , the shapes of the pdfs can be distorted , the estimators of pois can become biased . the systematic uncertainties due to nps are usually classified in two categories .",
    "the first category refers to `` type - i '' errors , for which the sample ( or other control data samples ) can ( in principle ) provide information on the nps under consideration , and are ( in principle ) supposed to decrease with sample size .",
    "the second category refers to `` type - ii '' errors , which arise from incorrect assumptions in the model ( i.e. a choice of inadequate functional dependences in the pdfs ) , or uncontrolled features in data that can not be described by the model , like the presence of unaccounted species . clearly , for type - ii errors",
    "the task of assigning systematic uncertainties to them may not be well - defined , and may spoil the statistical interpretation of errors in termos of confidence levels .      to deal with type - i nuisance parameters ,",
    "a popular approach is to use the so - called profile - likelihood method .",
    "this approach consists of assigning a specific likelihood to the nuisance parameters , so that the original likelihood is modified to have two different components : @xmath226 then , for a fixed value of @xmath65 , the likelihood is maximized with respect to the nuisance @xmath175 ; the sequencial outcome of this procedure , called profile likelihood , is a function that depends only on @xmath65 : it is then said that the nuisance parameter has been profiled out of the likelihood .",
    "as an example , consider the measurement of the cross - section of a generic process , @xmath227 .",
    "if only a fraction of the processes is actually detected , the efficiency @xmath214 of reconstructing the final state is needed to convert the observed event rate @xmath228 into a measurement @xmath229 .",
    "this efficiency is clearly a nuisance : a wrong value of @xmath214 directly affects the value of @xmath229 , regardless of how accurately @xmath228 may have been measured . by estimating @xmath220 on a quality control sample ( for example , high - statistics simulation , or a high - purity control data sample ) , the impact of this nuisance can be attenuated .",
    "for example , an elegant analysis would produce a simultaneous fit to the data and control samples , so that the values and uncertainties of nps are estimated in the ml fit , and are correctly propagated to the values and variances of the pois .    as another example",
    ", consider the search for a resonance ( `` a bump '' ) over a uniform background .",
    "if the signal fraction is very small , the width @xmath169 of the bump can not be directly estimated on data , and the value used in the signal pdf has to be inferred from external sources .",
    "this width is clearly a nuisance : using an overestimated value would translate into an underestimation of the signal - to - background ratio , and thus an increase in the variance of the signal pois , and possibly biases in their central values as well , i.e. the signal rate would tend to be overestimated .",
    "similar considerations can be applied in case of underestimation of the width .",
    "if an estimation @xmath230 of the width is available , this information can be implemented as in eq .",
    "( [ eq : profilelikelihood ] ) , by using a gaussian pdf , with mean value @xmath231 and width @xmath231 , in the @xmath232 component of the likelihood .",
    "this term acts as a penalty in the ml fit , and thus constraints the impact of the nuisance @xmath169 on the pois .",
    "often , there are large regions in sample space where backgrounds are overwhelming , and/or signals are absent . by restricting the data sample to `` signal - enriched '' subsets of the complete space ,",
    "the loss of information may be minimal , and other advantages may compensate the potential losses : in particular for multi - dimensional samples , it can be difficult to characterize the shapes in regions away from the core , where the event densities are low ; also reducing the sample size can relieve speed and memory consumption in numerical computations .",
    "the simplest method of sample reduction is by requiring a set of variables to be restricted into finite intervals . in practice ,",
    "such `` cut - based '' selections appear at many levels in the definition of sample space : thresholds on online trigger decisions , filters at various levels of data acquisition , removal of data failing quality criteria ... but at more advanced stages of a data analysis , such `` accept - reject '' sharp selections may have to be replaced by more sophisticated procedures , generically called multivariate techniques .",
    "a multi - dimensional ml fit is an example of a multivariate technique . for a mle to be considered ,",
    "a key requirement is to ensure a robust knowledge of all pdfs over the space of random variables .",
    "consider a set of @xmath25 random variables @xmath233 .",
    "if all variables are shown to be uncorrelated , their @xmath25-dimensional pdf is completely determined by the product of their @xmath25 one - dimensional pdfs ; now , if variables are correlated , but their correlation patterns are completely linear , one can instead use variables @xmath234 , linear combinations of @xmath173 obtained by diagonalizing the inverse covariance . for some non - linear correlation patterns , it may be possible to find analytical descriptions ; for instance , the ( mildly ) non - linear correlation pattern represented in figure  [ fig : pdf2dcanvas ] , was produced with the roofit package , by applying the conditional option in rooprodpdf to build a product of pdfs . in practice , this elegant approach can not be easily extended to more than two dimensions , and is not guaranteed to reproduce complex , non - linear patterns . in such scenarios , the approach of dimensional reduction can potentially bring more effective results .",
    "a typical scenario for dimensional reduction is when several variables carry common information ( and thus exhibit strong correlations ) , together with some diluted ( but relevant ) pieces of independent information .",
    "an example is the characterization of showers in calorimeters ; for detectors with good transverse and/or longitudinal segmentation , the signals deposited in nearby calorimetric channels can be used to reconstruct details from the shower development ; for example , a function that combines informations from longitudinal and transverse shower shapes , can be used to discriminate among electromagnetic and hadronic showers .",
    "the simplest algorithm for dimensional reduction is the fisher discriminant : it is a linear function of variables , with coefficients adjusted to match an optimal criterion , called separation among two species , which is is the ratio of the variance between the species to the variance within the species , and can be expressed in a close , simple analytical form .    in presence of more complex , non - linear correlation patterns , a large variety of techniques and tools",
    "are available .",
    "the tmva  @xcite package is a popular implementation of dimensional - reduction algorithms ; other than linear and likelihood - based discriminants , it provides easy training and testing methods for artificial neural networks and ( boosted ) decision trees , which are among those most often encountered in hep analyses . as a general rule , a multivariate analyzer uses a collection of variables , realized on two different samples ( corresponding to `` signal '' and `` background '' species ) , to perform a method - dependent training , guided by some optimization criteria ; then performances of the trained analyzer are evaluated on independent realizations of the species ( this distinction between the training and testing stages is crucial to avoid `` over - training '' effects ) .",
    "figure  [ fig : tmva ] shows a graphical representation of a figure - of - merit comparison of various analyzers implemented in tmva .",
    "sections  [ sec : paramestimation ] and  [ sec : mltheorem ] mainly discussed procedures for extracting numerical information from data samples ; that is , to perform measurements and report them in terms of values and uncertanties .",
    "the next step in data analysis is to extract qualitative information from data : this is the domain of statistical hypothesis testing . the analytical tool to assess the agreement of an hypothesis with observations",
    "is called a test statistic , ( or a statistic in short ) ; the outcome of a test is given in terms of a @xmath114-value , or probability of a `` worse '' agreement than the one actually observed .",
    "for a set of @xmath25 independent measurements @xmath21 , their deviation with respect to predicted values @xmath150 , expressed in units of their variances @xmath235 is called the @xmath159 test , and is defined as @xmath236 an ensemble of @xmath159 tests is a random variable that , as mentioned in section  [ sec : examples ] , follows the @xmath237 distribution ( c.f .",
    ".  [ eq : chi2 ] ) for @xmath25 degrees of freedom .",
    "its expectation value is @xmath25 and its variance @xmath238 ; therefore one does expect the @xmath159 value observed in a given test not to deviate much from the number of degrees of freedom , and so this value can be used to probe the agreement of prediction and observation .",
    "more precisely , one expects @xmath190 of tests to be contained within a @xmath239 interval , and the @xmath114-value , or probability of having a test with values larger than a given @xmath159 value is @xmath240 roughly speaking , one would tend to be suspicious of small observed @xmath114-values , as they may indicate a trouble , either with the prediction or the data quality .",
    "the interpretation of the observed @xmath114-value ( i.e. to decide whether it is too small or large enough ) is an important topic , and is discussed below in a more general approach .      consider two mutually excluding hypotheses @xmath241 and @xmath242 , that may describe some data sample ; the hypothesis testing procedure states how robust is @xmath241 to describe the observed data , and how incompatible is @xmath242 with the observed data .",
    "the hypothesis @xmath241 being tested is called the `` null '' hypothesis , while @xmath242 is the `` alternative '' hypothesis ,    note that in the context of the search for a ( yet unknown ) signal , the null @xmath241 corresponds to a `` background - only '' scenario , and the alternative @xmath242 to `` signal - plus - background '' ; while in the context of excluding a ( supposedly inexistent ) signal , the roles of the null and alternative hypotheses are reversed : the null @xmath241 is `` signal - plus - background '' and the alternative @xmath242 is `` background - only '' .    the sequence of a generic test , aiming at accepting ( or rejecting ) the null hypothesis @xmath241 by confronting it to a data sample , can be sketched as follows :    * build a test statistic @xmath243 , that is , a function that reduces a data sample to a single numerical value ; * define a confidence interval @xmath244 $ ] ; * measure @xmath245 ; * if @xmath245 is contained in @xmath246 , declare the null hypothesis accepted ; otherwise , declare it rejected .    to characterize the outcome of this sequence ,",
    "two criteria are defined : a `` type - i error '' is incurred in , if @xmath241 is rejected despite being true ; while a `` type - ii error '' is incurred in , if @xmath241 is accepted despite being false ( and thus @xmath242 being true ) .",
    "the rates of type - i and type - ii errors are called @xmath207 and @xmath247 respectively , and are determined by integrating the @xmath241 and @xmath242 probability densities over the confidence interal @xmath246 , @xmath248 the rate @xmath207 is also called `` size of the test '' ( or size , in short ) , as fixing @xmath207 determines the size of the confidence interval @xmath246 .",
    "similarly , @xmath249 is also called `` power '' . together , size and power characterize the performance of a test statistic ; the neyman - pearson lemma states that the optimal statistic is the likelihood ratio @xmath250 , @xmath251 the significance of the test is given by the @xmath114-value , @xmath252 which is often quoted in terms of `` sigmas '' , @xmath253 so that for example a @xmath254 outcome can be reported as a `` two - sigma '' effect .",
    "alternatively , it is common practice to quote the complement of the @xmath114-value as a confidence level ( c.l . ) .    the definition of a @xmath114-value as in eq .",
    "( [ eq : pvalue ] ) ( or similarly in eq .",
    "( [ eq : chi2test ] ) for the example for a @xmath159 test ) is clear and unambiguous . but",
    "interpretation of @xmath114-values is partly subjective : the convenience of a numerical `` threshold of tolerance '' may depend on the kind of hypothesis being tested , or on common practice .",
    "in hep usage , three different traditional benchmarks are conventionally employed :    * in exclusion logic , a @xmath255 c.l .",
    "threshold on a signal - plus - background test to claim exclusion ; * in discovery logic , a three - sigma threshold ( @xmath256 ) on a background - only test to claim `` evidence '' ; * and a five - sigma threshold ( @xmath257 ) on the background - only test is required to reach the `` observation '' benchmark .      in experimental hep , there is a tradition of reaching consensual agreement on the choices of test statistics .",
    "the goal is to ensure that , in the combination of results from different samples and instruments , the detector - related components ( specific to each experiment ) factor out from the physics - related observables ( which are supposed to be universal ) .",
    "for example in the context of searches for the standard model ( sm ) higgs boson , the four lep experiments agreed on analyzing their data using the following template for their likelihoods : @xmath258 where @xmath259 is the number of higgs decay channels studied , @xmath260 is the observed number of event candidates in channel @xmath261 , @xmath262 and @xmath263 ( @xmath264 and @xmath265 ) are the pdf and event yield for the signal ( background ) species in that channel .",
    "also , the test statistic @xmath126 , derived from a likelihood ratio , is @xmath266 so that roughly speaking , positive values of @xmath126 favour a `` background - like '' scenario , and negative ones are more in tune with a `` signal - plus - background '' scenario ; values close to zero indicate poor sensitivity to distinguish among the two scenarios .",
    "the values use to test these two hypotheses are :    * under the background - only hypothesis , @xmath267 is the probability of having a @xmath268 value larger than the observed one ; * under the signal+plus+background hypothesis , @xmath269 is the probability of having a @xmath268 value larger than the observed one",
    ".    figure  [ fig : lep1d ] shows , for three different higgs mass hypotheses , the @xmath268 values from the combination of the four lep experiments in their searches for the sm higgs boson , overlaid with the expected @xmath269 and @xmath270 distributions .",
    "figure  [ fig : lep2d ] shows the evolution of @xmath268 values as a function of the hypothetized higgs boson mass ; ( as stated in the captions , the color conventions in the one- and two - dimensional plots are different ) note that for masses below @xmath271 gev , a positive value of the @xmath268 test statistic would have provided evidence for a signal ; and sensitivity is quickly lost above that mass .",
    "gev hypotheses for the higgs mass are used .",
    "the dashed blue and dashed red lines correspond to the pdfs for the background - only and signal - plus - background hypotheses , respectively .",
    "the observed values of the test statistic @xmath268 are marked by black vertical lines .",
    "the yellow areas indicate the @xmath270 values for the background - only hypothesis , and the green areas the @xmath269 value for signal - plus - background .",
    "the three figures are taken from  @xcite . ,",
    "title=\"fig:\",width=185 ]   gev hypotheses for the higgs mass are used .",
    "the dashed blue and dashed red lines correspond to the pdfs for the background - only and signal - plus - background hypotheses , respectively .",
    "the observed values of the test statistic @xmath268 are marked by black vertical lines .",
    "the yellow areas indicate the @xmath270 values for the background - only hypothesis , and the green areas the @xmath269 value for signal - plus - background .",
    "the three figures are taken from  @xcite .",
    ", title=\"fig:\",width=185 ]   gev hypotheses for the higgs mass are used .",
    "the dashed blue and dashed red lines correspond to the pdfs for the background - only and signal - plus - background hypotheses , respectively .",
    "the observed values of the test statistic @xmath268 are marked by black vertical lines .",
    "the yellow areas indicate the @xmath270 values for the background - only hypothesis , and the green areas the @xmath269 value for signal - plus - background .",
    "the three figures are taken from  @xcite .",
    ", title=\"fig:\",width=185 ]     as a function of the higgs boson mass @xmath272 , obtained by combining the data of the four lep experiments .",
    "the dashed line is the mean value of the background - only distribution at each mass @xmath272 , and the green and yellow areas represent @xmath190 and @xmath255 contours for the background - only distribution at each mass @xmath272 .",
    "the black line follows the @xmath268 value observed in data as a function of @xmath272 .",
    "figure taken from  @xcite .",
    ", width=359 ]      the choice of @xmath269 to test the signal - plus - background hypothesis , while suitably defined as a @xmath114-value .",
    "may drag some subjective concern : in case of a fortuitous simultaneous downward fluctuation in both signal and background , the standard @xmath255 benchmark may lead to an exclusion of the signal , even if the sensitivity is poor .",
    "a modification of the exclusion benchmark called @xmath273 , has been introduced in this spirit@xcite , and is defined as @xmath274 this test , while not corresponding to a @xmath114-value ( a ratio of probabilities is not a probability ) , has the desired property of protecting against downwards fluctuations of the background , and is commonly used in exclusion results , including the searches for the sm higgs boson from the tevatron and lhc experiments .      following the recommendations from the lhc higgs combination group  @xcite , the atlas and cms experiments",
    "have agreed on using a common test statistic , called profiled likelihood ratio and defined as @xmath275 where the pio @xmath276 is the `` signal strength modifier '' , or higgs signal rate expressed in units of the sm predicted rate , @xmath277 are the fitted values of the nps at fixed values of the signal strength , and @xmath87 and @xmath181 are the fitted values when both @xmath65 and nps are all free to vary in the ml fit  .",
    "the lower constraint on @xmath278 ensures that the signal rate is positive , and the upper constraint imposes that an upward fluctuation would not disfavor the sm signal hypothesis .    for an observed statistic value @xmath279 , the @xmath114-values for testing the signal - plus - background and background - only hypotheses , @xmath280 and @xmath281 , are @xmath282 and the results on searches",
    "are reported both in terms of the exclusion significance using the @xmath273 observed and expected values , and the observation significance expressed in terms of the `` local ''   @xmath281 expected and observed values .    to conclude the discussion on hypothesis testing",
    ", there is certainly no better illustration than figure  [ fig : fourthjuly ] , taken from the results announced by the atlas and cms experiments on july 4th , 2012 : in the context of the search for the sm higgs boson , both collaborations established the observation of a new particle with a mass around 125 gev .",
    "these lectures aimed at providing a pedagogical overview of probability and statistics .",
    "the choice of topics was driven by practical considerations , based on the tools and concepts actually encountered in experimental high - energy physics .",
    "a bias in the choices , induced by the author s perspective and personal experience is not to be excluded .",
    "the discussion was completed with examples from recent results , mostly ( although not exclusively ) stemming from the @xmath0-factories and the lhc experiments .",
    "i would like to express my gratitude to all who helped make the aepshep 2012 school a success : the organizers , the students , my fellow lecturers and discussion leaders .",
    "the highly stimulating atmosphere brought fruitful interactions during lectures , discussion sessions and conviviality activities .",
    "a.  stuart , j.k .",
    "ord , and s.  arnold , _",
    "kendall s advanced theory of statistics _",
    "2a : _ classical inference and the linear model _",
    "6th ed . , oxford univ . press ( 1999 ) , and earlier editions by kendall and stuart ."
  ],
  "abstract_text": [
    "<S> a pedagogical selection of topics in probability and statistics is presented . choice and emphasis </S>",
    "<S> are driven by the author s personal experience , predominantly in the context of physics analyses using experimental data from high - energy physics detectors . </S>"
  ]
}