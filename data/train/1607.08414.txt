{
  "article_text": [
    "an egocentric camera captures rich and varied information of how the wearer interacts with their environment .",
    "the challenge for the visual understanding of this information is currently significant and not only incited by the enormous variety of such interactions but also by limitations in the available visual descriptors , e.g.  those rooted in motion or appearance . supervised learning from labelled examples is used to alleviate some of these ambiguities .",
    "egocentric datasets  @xcite and interaction recognition methods  @xcite differ in the features used and classification techniques adopted , yet they all assume a semantically distinct set of _ pre - selected _ verbs or verb - noun combinations for supervision . when free annotations are available - unbounded choice of verbs or verb - nouns - from audio scripts  @xcite or textual annotations  @xcite , a single label is selected to represent each interaction using a majority vote .",
    "less frequent annotations are treated as outliers , though they typically represent a meaningful and correct annotation .",
    "for example , lifting an object from a workspace could be described as _ pick - up _ , _ lift _ , _ take _ or _ grab _ ; all valid labels .",
    "note that assuming multiple _ valid _ labels is different from the problem of ambiguous label learning , @xcite , where the aim is to find a single valid label from a mixed set of related and unrelated labels .",
    "egocentric video offers a unique insight into object interactions in particular .",
    "the camera is ideally positioned to capture objects being used and , equally interesting , the different ways in which the same object is used .",
    "one interaction ( e.g.  _ open _ ) applies to a wide variety of objects , and each video can be labelled by multiple valid labels ( e.g. _ open door _ vs _ push door _ ) . in this",
    "context , recognition can not be simplified as a one - vs - all classification task .",
    "capturing the semantic relationships between annotations and the visual ambiguities between accompanying video segments can better represent the space of possible interactions .",
    "figure  [ fig : motivational ] shows a graphical abstract of our work .    given a dataset of egocentric object interactions with free annotations ,",
    "we contribute four diversions from previous attempts : ( i )  we treat all free annotations as valid , correct labellings , ( ii )  a graph that combines semantic relationships with visual similarities is built , inspired by previous work on object class categories in single images  @xcite  ( sec .",
    "[ sec : methodembed ] ) , ( iii )  a  test video is embedded into the previously learnt semantic - visual graph and the probability distribution over its possible annotations is estimated ( sec .",
    "[ sec : methodclass ] ) and ( iv )  when verb meanings are available , we discover semantic relationships between annotations using wordnet  ( sec .",
    "[ sec : methodah ] ) .",
    "we test semantic embedding ( sembed ) on three public egocentric datasets @xcite .",
    "we show that as the number of verb annotations and their semantic ambiguities increase , sembed outperforms classification approaches .",
    "we also show that incorporating higher level semantic relationships , such as the hyponymy relationship , improves the results .",
    "note that while we focus on _ egocentric object interaction recognition _ as a rich domain of semantic and visual ambiguities , some of the arguments can apply to action recognition in general .",
    "to the best of our knowledge , embedding for egocentric action recognition has not been attempted previously .",
    "we first review works on recognising egocentric object interactions , then review works which incorporate semantic knowledge for recognition tasks .    *",
    "egocentric object interaction recognition : * egocentric action recognition works range from self - motion  @xcite ( e.g. walk , cycle ) to high - level activities ( e.g.  @xcite ) . on the task of object interaction recognition , approaches vary in whether they use hand - centred features  @xcite , object - specific features  @xcite or a combination  @xcite .",
    "ishihara  @xcite use dense trajectories in addition to global hand shape features and apply a linear svm to determine the action class .",
    "kumar  @xcite sample and describe superpixel regions around the hand .",
    "their method allows for hand detectors to be trained spontaneously with the user performing the action .",
    "object - specific features are better suited for recognising verb - noun actions ( e.g. _ pick - cup vs pick - plate _ ) rather than a general _ picking _ action . in damen",
    "@xcite , spatio - temporal interest points have been used to discover object interactions in an unsupervised manner .",
    "the works of fathi  @xcite have tested features including gaze , colour , texture and shape for verb - noun action classification .",
    "of these , @xcite specifically discusses the change in the object state as a useful feature to recognise object interactions .",
    "though attempting video summarisation primarily , ghosh  @xcite introduces a collection of features that could be used to classify object - interactions such as distance from the hand , saliency , objectness represented using a spatio - temporal pyramid to detect change .",
    "these features were proven useful for segmenting object - interactions from a lengthy video , but have not been tested for action classification _ per se_. on several publicly available datasets ,",
    "li  @xcite compare motion , object , head motion and gaze information along with a linear svm for object interaction classification .",
    "their results prove that improved dense trajectories ( idt ) proposed by  @xcite outperform other motion features .    with the emergence of highly - discriminative appearance - based features , pre - trained convolutional neural networks ( cnn ) on imagenet",
    "have also been tested . in @xcite ,",
    "cnn is evaluated for distinguishing manipulation from non - manipulation actions on an rgb - d egocentric dataset .",
    "ryoo  @xcite combine cnn with idt along with novel time series pooling for dog - centric manipulation and non - manipulation actions .",
    "more recently , fine - tuned multi - stream cnn approaches have achieved state of the art results on egocentric datasets  @xcite , though are tuned on each dataset independently .    based on  @xcite conclusions , in this work we report results on idt as a state - of - the - art motion feature and",
    "pre - trained cnn features a state - of - the - art appearance feature .",
    "testing tuned cnns is left for future work .    *",
    "semantic embedding for object and action recognition : * using linguistic semantic knowledge for computer vision tasks , including action recognition , has been fuelled by the accessibility of text or audio descriptions from online sources .",
    "one such dataset which made this possible was gathered from youtube videos @xcite with free annotations .",
    "the dataset includes a variety of real - world scenarios , though not limited to egocentric or object - interactions . for each video ,",
    "multiple annotators were asked to describe the video . both @xcite use this dataset for action recognition . in motwani and mooney",
    "@xcite , the most frequently annotated verb for each video is used , and verbs are grouped into classes using semantic similarity measures , extracted from the wordnet hierarchy as well as information corpuses .",
    "videos are described by hog and hof features around spatio - temporal interest points .",
    "guadarrama  @xcite find subject , object and verb triplets in an attempt to automatically annotate the action .",
    "they create a separate semantic hierarchy for each , formulated by co - occurrences of words within the free annotations and use spearman s rank to find the distances between clusters .",
    "semantic links are used to generate specific , rather than general , annotations and a classifier is trained for each leaf node within the hierarchies .",
    "their method allows zero - shot action annotation by trading - off specificity and semantic similarity . while combining semantics , both works use majority voting to limit the description per class to a single verb .",
    "another recent youtube dataset was collected of users performing tasks while narrating their actions @xcite .",
    "labels are extracted from audio descriptions using automatic speech recognition .",
    "verb labels are then used to align videos using a wordnet similarity measure as well as visual similarity ( hof and cnn ) to find the sequence of actions in a task .",
    "semantics have also been used for object recognition with images .",
    "jin @xcite use wordnet to remove noisy labels from images which have multiple labels . similarly , ordonez  @xcite use wordnet to find the most frequently - used object labels amongst multiple annotations .",
    "we build our work on fang and torresani @xcite , where images are embedded in a semantic - visual graph . in  @xcite , images are clustered depending on the semantic relationships between the labels and edges of the graph are weighted with the visual similarity .",
    "they use imagenet as the database for training , and benefit from the fact that images within imagenet are organised according to the wordnet hierarchy .",
    "we differ from  @xcite in how we add visual links to the semantic graphs as will be explained next .",
    "we next , in sec .  [",
    "sec : methodembed ] , explain how we build a semantic - visual graph ( svg ) that encodes label and visual ambiguities in the training set . in sec .",
    "[ sec : methodclass ] , we detail how videos with an unknown class are embedded in svg , and how the probability distribution over their annotations is estimated . finally , in sec .",
    "[ sec : methodah ] we explore further semantic relationships when verb meanings are annotated",
    ".      the semantic - visual graph ( svg ) is a representation of the training videos , with three sources of information encoded .",
    "first , videos that are semantically linked , e.g. have the same label , are linked in svg .",
    "second , nodes that are visually similar , yet semantically distinct , should also be linked as these indicate visual ambiguities .",
    "third , edge weights correspond to the normalised visual similarity , over neighbouring nodes , using a visual descriptor and a defined distance measure . in this section",
    "we explain how svg@xmath0 , an undirected graph , is constructed , then normalised to achieve the directed graph svg .",
    "svg@xmath0 is an undirected graph , where one node @xmath1 corresponds to one training video .",
    "assume ax(@xmath2 , @xmath3 ) is a binary function that checks whether two video labels are semantically related .",
    "initially , ax(@xmath2 , @xmath3 ) is _ true _ when both videos are annotated by the exact same verb .",
    "this assumption is revisited in sec .",
    "[ sec : methodah ] .",
    "edges in svg@xmath0 are created between nodes with a semantic relationship :    @xmath4    the undirected edge @xmath5 is assigned a weight @xmath6 where @xmath7 is a distance measure defined over the visual descriptor chosen .",
    "assume @xmath8 is a function that returns the relative position of the distance measure amongst all the remaining pairs of videos such that , @xmath9    and @xmath10 is the @xmath11 minimum element in the list .",
    "in addition , assume + @xmath12 is a function that returns the relative position of @xmath13 amongst all nodes not connected to @xmath2 such that , @xmath14    further links are added to svg@xmath0 to encode visual ambiguities such that , @xmath15    where @xmath16 is the number of visual connections in svg@xmath0 that correspond to the top @xmath16 visually similar and semantically dissimilar nodes in svg@xmath0 .",
    "we differ from  @xcite in that we ensure each node is connected to its top visually similar but semantically distinct node .    the undirected graph svg@xmath0 is then converted to a directed graph by replacing each edge with two directed edges . @xmath17",
    "the weights of directed edges are initially the same as the weights for their undirected counterparts however they are normalised to define the probability of traversing from video @xmath2 to @xmath3 , @xmath18    the reciprocal of the weights is taken so that the most visually similar path will have the highest probability .    , two",
    "nearest neighbours are found ( yellow ) and a markov walk of 2 steps ( step1-red and step2-orange ) finds the probability distribution over potential labellings .",
    "supplementary material for animation.,title=\"fig:\",scaledwidth=100.0% ] [ eq : prob ]      given a test video , @xmath19 , we first embed the video into svg then use the markov walk ( mw ) method from @xcite to determine @xmath20 . to embed @xmath19 , we begin by finding the set @xmath21 which contains the @xmath22 closest neighbours to @xmath19 based on visual distance , such that @xmath23    we embed @xmath19 into svg by adding directed edges connecting @xmath19 to nodes in + with normalised weights @xmath24 . following the embedding ,",
    "mw attempts to traverse the nodes in the directed graph to estimate the probability of @xmath25 . given the markovian assumption and a predefined number of steps @xmath26",
    ", we calculate the probability distribution of reaching a node @xmath2 as follows @xmath27    to perform mw efficiently , we construct the vector q such that @xmath28    we also construct a matrix @xmath29 such that @xmath30 ( eq .  [ eq : travprob ] ) , note that this matrix is asymmetrical as nodes have a different set of neighbours in svg .",
    "accordingly , @xmath31 where @xmath32 is the transpose of @xmath33 and @xmath26 is the number of steps in mw . we can then accumulate @xmath34 for every unique annotation @xmath35 as follows    @xmath36    we then select @xmath37 as the semantic label of @xmath19 .",
    "figure  [ fig : graphex ] shows an example of svg and video embedding . in the figure , given two nearest neighbours @xmath38 and two steps in mw @xmath39 , the probability distribution over possible labellings is calculated .",
    "[ sec : methodembed ] , videos are considered semantically linked only when the annotated verbs are the same .",
    "svg then enables handling ambiguities via incorporating visual similarity links in the graph .",
    "however , further semantic relationships , such as synonymy and hyponymy relationships , can be exploited between annotations . in linguistics , two words",
    "are _ synonyms _ if they have the same meaning , and the set of all synonyms is a _ synset_. moreover , two words are described as a _",
    "hyponym _ and a _ hypernym _ respectively if the first is a more specific instance of the second .",
    "the terms originate from the greek word @xmath40 and @xmath41 - _ under _ and _",
    "over_.    synonymy and hyponymy relationships are encoded in lexical databases .",
    "wordnet ( v3.1 , 2012 ) is a commonly - used lexical database that is based on six semantic relations  @xcite . in the wordnet verb hierarchy",
    ", verbs are first separated into their various meanings by the notation @xmath42 where @xmath43 is the number of disjoint meanings .",
    "the meanings are then arranged in hierarchies that encapsulate semantic relationships . to benefit from such hierarchies",
    ", verbs should be annotated with their meanings .",
    "we annotate  @xcite using verb meanings , and fig .",
    "[ fig : annotationex ] shows how such annotations of the same action can be synonyms and hyponyms , as annotators chose different or more specific action descriptions .",
    "[ sec : methodah ]     and @xmath44 were chosen by annotators .",
    "@xmath45 , a hyponym of @xmath46 was also used . for washing a cup ( right ) ,",
    "the verbs @xmath47 , @xmath48 and @xmath49 were chosen .",
    "@xmath49 is a hyponym of @xmath47.,scaledwidth=100.0% ]    given annotated meanings , we define the term action synsets ( as ) to indicate that annotations are linked by a synonymy relationship solely , and the term action hyponym ( ah ) to indicate that annotations are linked by both the synonymy or the hyponymy relationships . for comparison , we define the term action meaning ( am ) where annotations are linked only when the annotation matches exactly .",
    "we use the general term ax where @xmath50am , as , ah@xmath51 is one of the the possible types of semantic relationships tested .",
    "we selected three publicly available datasets that primarily focus on object interactions from egocentric videos  @xcite ( figure [ fig : datasetinfo ] ) .",
    "* verb annotations : * we exploited the annotations provided by the authors to split the cmu and gtea+ sequences into object - interaction segments . for cmu ,",
    "object - interaction annotations are only provided for the activity of _ making brownies_. annotators chose from 12 disjoint verbs to ground - truth segments . in gtea+ annotators chose from verb - noun pairing to ground - truth , e.g. _",
    "cut_cucumber _ versus _ divide_bun _ and similarly _",
    "squeeze_ketchup _ versus _",
    "compress_bun_. when removing the nouns , verbs could be used interchangeably but free annotations were not available to annotators .",
    "40s ) are detailed .",
    "we report the number of annotated verbs along with @xmath52 and @xmath53 for the number of segments per verb .",
    "@xmath54 : due to the size of gtea+ we sampled 1000 videos randomly . ref .",
    "supplementary material for frequencies of verb annotations per dataset.,scaledwidth=70.0% ]    while beoid contains a variety of activities and locations , ranging from a desktop to operating a gym machine , it does not provide action - level annotations so we annotated beoid using free annotations , allowing annotators to split video sequences into object - interaction segments in addition to choosing the verb .",
    "we recruited 20 native english speakers .",
    "these annotators were given a free textbox to label each segment with the verb that best described the seen interaction _ in their opinion_. once a verb has been chosen , the annotators were given the set of potential meanings extracted from wordnet for the chosen verb .",
    "again , they were asked to select the meaning that , _ in their opinion _ , best suited the segment .",
    "multiple annotators ( 8 - 10 ) were asked to label each task to intentionally introduce variability in the choice of verbs and start - end times of object interaction segments .",
    "* motion and appearance features : * we test two state - of - the - art feature descriptors to represent both the motion and the appearance of the videos .",
    "these are the improved dense trajectories  ( idt )  @xcite and overfeat convolutional neural networks pre - trained for imagenet classes  ( cnn )  @xcite . for cnn features ,",
    "we take every 5th frame from 30fps video , starting always from the first frame , and rescale to 320x240 pixels .    * encodings : * we test two encodings , using bag of words ( bow ) @xcite and fisher vectors ( fv )  @xcite with euclidean distance . for idt , when creating the bow and fv representations , we use a 25% random sample from every video to model the gaussians for efficiency .",
    "we vary the number of gaussians ( @xmath55 ) and the size of the codebook ( @xmath56 ) in reported results .",
    "* classification : * in all results , leave - one - person - out cross validation has been used .",
    "namely , when testing a video containing one person performing an action , all other videos captured from the same person are excluded from the training set . for svm results , as the tested datasets contain an imbalance in the distribution of instances per class , we weight the classes by the term @xmath57 where @xmath58 $ ] is the exponent that best fits the distribution of segments per verb for a given dataset ( ref supplementary material ) .",
    "[ table : resultstableav ]     and @xmath59 vary for cmu , gtea+ , beoid .",
    "results were shown with @xmath60 , @xmath61 , @xmath62 , @xmath63 .",
    "similar performance is seen for other parameters.,scaledwidth=100.0% ]    * results on annotated verbs : * [ sec : resultsav ] table  [ table : resultstableav ] compares the three datasets for every @xmath64features , encoding , classifier@xmath65 combination .",
    "the following conclusions can be made : ( i ) for all datasets , motion features  ( idt ) outperform appearance features  ( cnn ) when classifying verbs without considering the object used .",
    "( ii ) for cmu and gtea+ , we produce comparable results to published results using motion information on the same datasets .",
    "these are reported under ` other works ' but are not directly comparable as published works tend to report on verb - noun classes .",
    "( iii ) for the three datasets with varying number of verbs , as the number of verbs increases ( @xmath66 ) with an increase in semantic ambiguity , sembed outperforms standard classifiers ( svm and k - nn ) . while the table shows the best results for encoding , fig .",
    "[ fig : gammafigure ] reports comparative results as @xmath67 is changed - @xmath68 generally led to higher accuracies on all datasets , compared to @xmath69 .",
    "we test the sensitivity of sembed to its key parameters @xmath22 and @xmath26 and report results in fig .",
    "[ fig : sembedzandt ] showing the accuracy over various features for beoid and across the three datasets for idt - bow ( ref .",
    "supplementary material for all combinations ) .",
    "as noted , @xmath22 and @xmath26 behave differently for the various appearance and motion descriptors as well as for different encodings .",
    "generally , sembed is more sensitive to the choice of @xmath22 than @xmath26 .",
    "this is because the markovian walk ( mw ) is unable to represent the probability distribution over labels unless the starting positions are representative of the visual ambiguity .",
    "figure  [ fig : sembedzandt ] also shows that mw is nt too helpful for cmu ( as @xmath26 increases , accuracy decreases ) because it has visually distinctive verb classes . on all datasets , sembed is resilient to changing @xmath16 values ; the results are comparable on @xmath70 .",
    "and @xmath26 parameters with @xmath61.,scaledwidth=100.0% ]    * results on annotated verbs and meanings : * as mentioned earlier , we also annotate beoid with verb - meaning ground - truth .",
    "this resulted in 108 @xmath42 annotations for the 1225 segments in the dataset .",
    "note the increase in the number of classes from 75 when using verbs only to 108 when using verb - meaning ground - truth .",
    "this increase is due to two reasons - one _ helpful _ , another _",
    "problematic_. for example , it is _ helpful _ when annotators choose between @xmath71 : _ `` keep in a certain state , position '' _ and @xmath72 : _ `` hold in one s hand''_. annotators would then use the first for when a button is pressed and the second for when an object is grasped . however , frequently , wordnet meanings can appear ambiguous resulting in _ problematic _ cases , especially in the context of egocentric actions .",
    "an example of this is the action of turning a tap on so water would flow .",
    "annotators used @xmath73 : _ `` change orientation or direction '' _ and @xmath74 : _ `` cause to move around or rotate '' _ interchangeably . in wordnet",
    "though , @xmath73 and @xmath74 are not semantically related , introducing unwanted ambiguity affecting the ground - truth labels . while we accept that wordnet may not be the best method to incorporate meaning , we report results as semantic links are incorporated .",
    "[ table : resultstableash ]    [ fig : sembedexample ]      we test the three types of semantic relationships @xmath75am , as , ah@xmath51 .",
    "histograms of all classes for the various semantic relationships are included in the supplementary material .",
    "table  [ table : resultstableash ] shows that embedding consistently improved performance as synsets and then hypernyms are grouped .",
    "results also demonstrate the advantages of introducing semantic links between videos .",
    "additionally , idt continues to outperform cnn .",
    "figure  [ fig : ex_sembed ] shows one example of sembed in action when using meanings and ah semantic links .",
    "it should be noted that the best performance of sembed on meanings is inferior to using verbs only .",
    "this is due to the difficulty in assigning meanings to verbs as previously noted .",
    "approaches to address meaning ambiguities are left for future work .",
    "the paper proposes embedding an egocentric action video in a semantic - visual graph to estimate the probability distribution over potentially ambiguous labels .",
    "sembed profits from semantic knowledge to capture interchangeable labels for the same action , along with similarities in visual descriptors .",
    "while showing clear potential , outperforming classification approaches on a challenging dataset , results merely evaluate the @xmath76 label when compared to ground - truth .",
    "further analysis of the probability distribution will be targeted next . other approaches to identify semantically related object - interaction labels from , for example , other lexical sources , overlapping annotations or object labels",
    "will also be attempted .",
    "sembed s ability to scale to other object interactions and more discriminative visual descriptors will also be tested .",
    "bleser , g. , damen , d. , behera , a. , hendeby , g. , mura , k. , miezal , m. , gee , a. , petersen , n. , macaes , g. , domingues , h. , gorecky , d. , almeida , l. , mayol - cuevas , w. , calways , a. , cohen , a. , hogg , d. , stricker , d. : cognitive learning , monitoring and assistance of industrial workflows using egocentric sensor networks .",
    "plos one ( 2015 )          damen , d. , leelasawassuk , t. , haines , o. , calway , a. , mayol - cuevas , w. : you - do , i - learn : discovering task relevant objects and their modes of interaction from multi - user egocentric video . in : bmvc ( 2014 )",
    "guadarrama , s. , krishnamoorthy , n. , malkarnenkar , g. , venugopalan , s. , mooney , r. , darrell , t. , saenko , k. : youtube2text : recognizing and describing arbitrary activities using semantic hierarchies and zero - shot recognition . in : iccv ( 2013 )"
  ],
  "abstract_text": [
    "<S> we present sembed , an approach for embedding an egocentric object interaction video in a semantic - visual graph to estimate the probability distribution over its potential semantic labels . </S>",
    "<S> when object interactions are annotated using unbounded choice of verbs , we embrace the wealth and ambiguity of these labels by capturing the semantic relationships as well as the visual similarities over motion and appearance features . </S>",
    "<S> we show how can interpret a challenging dataset of 1225 freely annotated egocentric videos , outperforming svm classification by more than 5% . </S>"
  ]
}