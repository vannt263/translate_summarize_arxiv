{
  "article_text": [
    "in recent years , statisticians , scientists and engineers are increasingly analyzing enormous data sets .",
    "when data sets grow sufficiently large , computational costs may play a major role in the analysis , potentially constraining our choice of methodology or the number of data points we can afford to process .",
    "computational savings can translate directly to statistical gains if they :    enable us to experiment with and prototype a variety of models , instead of trying only one or two ,    allow us to refit our models more often to adapt to changing conditions ,    allow for cross - validation , bagging , boosting , bootstrapping or other computationally intensive statistical procedures or    open the door to using more sophisticated statistical techniques on a compressed data set .",
    "@xcite discuss the tradeoffs arising when we adopt this point of view .",
    "one simple manifestation of these tradeoffs is that we may run out of computing resources before we run out of data , in effect making the sample size @xmath4 a function of the efficiency of our fitting method .",
    "class imbalance is pervasive in modern classification problems and has received a great deal of attention in the machine learning literature [ @xcite ] .",
    "it can come in two forms :    : :    _ marginal imbalance_. one of the classes is quite rare ; for instance ,    @xmath5 .",
    "such imbalance    typically occurs in data sets for predicting click - through rates in    online advertising , detecting fraud or diagnosing rare diseases .",
    ": :    _ conditional imbalance_. for most values of the features    @xmath6 , the response @xmath7 is very easy to predict ;    for instance , @xmath8 but    @xmath9 .",
    "for example , such    imbalance might arise in the context of email spam filtering , where    well - trained classifiers typically make very few mistakes .",
    "both or neither of the above may occur in any given data set .",
    "the machine learning literature on class imbalance usually focuses on the first type , but the second type is also common .",
    "if , for example , our data set contains one thousand or one million negative examples for each positive example , then many of the negative data points are in some sense redundant . typically in such problems , the statistical noise is primarily driven by the number of representatives of the rare class ,",
    "whereas the total size of the sample determines the computational cost .",
    "if so , we might hope to finesse our computational constraints by subsampling the original data set in a way that enriches for the rare class .",
    "such a strategy must be implemented with care if our ultimate inferences are to be valid for the full data set .",
    "this article proposes one such data reduction scheme , local case - control sampling , for use in fitting logistic regression models .",
    "the method requires one parallelizable scan over the full data set and yields a potentially much smaller subsample containing roughly half of the information found in the original data set .      the simplest way to reduce",
    "the computational cost of a procedure is to subsample the data before doing anything else .",
    "however , uniform subsampling from an imbalanced data set is inefficient since it fails to exploit the unequal importance of the data points .",
    "case - control sampling  sampling uniformly from each class but adjusting the mixture of the classes  is a more promising approach .",
    "this procedure originated in epidemiology , where the positive examples ( cases ) are typically diseased patients and negative examples ( controls ) are disease - free [ @xcite ] .",
    "often , an equal number of cases and controls are sampled , resulting in a subsample with no marginal imbalance , and costly measurements of predictor variables are only made for selected patients [ @xcite ] .",
    "this method is useful in our context as well , since a logistic regression model fitted on the subsample can be converted to a valid model for the original population via a simple adjustment to the intercept [ @xcite ] .",
    "however , standard case - control sampling still may not make most efficient use of the data .",
    "for instance , it does nothing to exploit conditional imbalance in a data set that is marginally balanced .",
    "even with some marginal imbalance , a control that looks similar to the cases is often more useful for discrimination purposes than one that is obviously not a case .",
    "we propose a method , local case - control sampling , which attempts to remedy imbalance _ locally _ throughout the feature space .",
    "given a pilot estimate @xmath10 of the logistic regression parameters , local case - control sampling preferentially keeps data points for which @xmath7 is surprising given @xmath6 .",
    "specifically , if @xmath11 , we accept @xmath12 with probability @xmath13 , the @xmath14 residual of the pilot model . in the presence of extreme marginal or conditional imbalance , these errors will generally be quite small and the subsample can be many orders of magnitude smaller than the full data set .    just as with case - control sampling , we can fit our model to the subsample and make an equally simple correction to obtain an estimate for the original data set .",
    "when the logistic regression model is correctly specified and the pilot is consistent and independent of the data , the asymptotic variance of the local case - control estimate is exactly twice the variance of a logistic regression fit on the ( potentially much larger ) full data set .",
    "this factor of two improves to @xmath1 if we accept with probability @xmath15 and weight accepted points by a factor of @xmath16 .",
    "for example , if @xmath17 then the variance of the subsampled estimate is only 20% greater than the variance of the full - sample mle .",
    "the subsample we take with @xmath2 is no more than @xmath18 times larger than the subsample for @xmath19 , and for data sets with large imbalance is roughly @xmath3 times as large .",
    "local case - control sampling also improves on the bias of standard case - control sampling .",
    "when the logistic regression model is misspecified , case - control sampling is in general inconsistent for the risk minimizer in the original population .",
    "by contrast , local case - control sampling is always consistent given a consistent pilot , and is also asymptotically unbiased when the pilot is .",
    "sections  [ secsimulations ] and  [ secspam ] present empirical results demonstrating the advantages of our approach in simulations and on the yahoo ! webspam data set .",
    "our setting is that of predictive classification : we are given @xmath4 independent and identically distributed observations , each consisting of predictors @xmath20 and a binary response @xmath21 , with joint probability measure @xmath22 . for our purposes",
    ", we assume the predictors are mapped into some real covariate vector space , so that @xmath23 our aim is to learn the function @xmath24 or equivalently to learn @xmath25 which could be infinite for some @xmath26 .",
    "a linear logistic regression model assumes @xmath27 is linear in @xmath26 ; that is , @xmath28 where @xmath29 . this is less of a restriction than it might seem , since @xmath26 may represent a very large basis expansion of some smaller set of `` raw '' features .",
    "nevertheless , in the real world , @xmath27 is unlikely to satisfy our parametric model for any given basis @xmath26 .",
    "when the model is misspecified , we can still view logistic regression as an m - estimator with convex loss equal to the negative log - likelihood for a single data set : @xmath30    as an m - estimator , under general conditions logistic regression in large samples will converge to the minimizer of the population risk @xmath31 [ @xcite ] .",
    "that is , @xmath32 converges to the population maximizer of the expected log - likelihood @xmath33.\\end{aligned}\\ ] ]    if @xmath34 for some @xmath35 , then @xmath36 ; otherwise @xmath37 is the best linear approximation to @xmath27 in the sense of ( [ eqnpoprisk ] ) . for a misspecified model",
    ", @xmath38 can not possibly converge to @xmath27 no matter what sampling scheme or estimation procedure we use , or how much data we obtain .",
    "consistency , then , will mean that @xmath39 .",
    "model misspecification is ubiquitous in real - world applications of regression methods .",
    "for reasons of exposition , the misspecification always takes a simple form in our simulations , for example , in example  [ ex1 ] there are two binary predictors , and we would have correct specification if only we added one interaction  but in the real world it usually is neither possible nor even desirable to expand the feature basis until the model is correctly specified .",
    "for instance , if @xmath40 , then there are @xmath41500,500 quadratic terms .",
    "even if we included all those terms as features , we would still be missing cubic terms , quartic terms , and so on .",
    "some kinds of misspecification are milder than others , and some are easier to find and fix than others . seeking better - specified models ( without adding too much model complexity )",
    "is a worthy goal , but realistically perfect specification is unattainable .    our goal ,",
    "then , is to speed up computation while still obtaining a good estimate of @xmath0 , the population logistic regression parameters .",
    "as we will see , standard case - control sampling achieves the first goal , but may fail at the second .",
    "recent years have seen substantial work on classification in imbalanced data sets .",
    "see @xcite and @xcite for surveys of machine learning efforts on this problem .",
    "many of the methods proposed involve some form of undersampling the majority class , oversampling the minority class , or both .",
    "@xcite examined the limit of marginally imbalanced logistic regression and proved it is equivalent to fitting an exponential family model to the minority class .",
    "one recurring theme is to preferentially sample negative examples that lie near positive examples in feature space .",
    "for example , @xcite propose selecting majority - class examples whose average distance to its three nearest minority examples is smallest .",
    "our method has a similar flavor since the probability of sampling a negative example @xmath42 is @xmath43 , which is large when the features @xmath26 are similar to those characteristic of positive examples .",
    "our proposal lies more in the tradition of the epidemiological case - control sampling literature .",
    "in particular , case - control sampling within several categorical strata has been studied by @xcite .",
    "typically , the strata are based on easy - to - measure screening variables available for a wide population , with more laborious - to - collect variables being measured on the sampled subjects .",
    "@xcite discuss survey calibration methods for efficient regression in two - stage sampling schemes , which are interesting but too computationally intensive for our purposes here .",
    "case - control sampling is commonly carried out by taking all the cases and exactly @xmath18 times as many controls for some fixed @xmath18 ( e.g. , @xmath44 ) . however , for our purposes it will be simpler to consider a nearly equivalent procedure based on accept  reject sampling .",
    "define some acceptance probability function @xmath45 and let @xmath46 , the log - selection bias .",
    "consider the following algorithm :    generate independent @xmath47 .",
    "fit a logistic regression to the subsample @xmath48 , obtaining unadjusted estimates @xmath49 .",
    "assign @xmath50 and @xmath51 .",
    "specifically , we could generate the @xmath52 by first generating @xmath53 mutually independent of the pilot , the data , and each other , then taking @xmath54 .",
    "note that steps ( 2)(3 ) are equivalent to logistic regression with offset @xmath55 for each data point .",
    "this variant is convenient to analyze because the subsample thus obtained is an i.i.d .",
    "sample from a new population : @xmath56 with @xmath57 , the marginal probability of @xmath58 .    the estimate @xmath59 is motivated by a simple application of bayes rule relating the odds of @xmath60 in @xmath22 and @xmath61 .",
    "if @xmath62 is the true conditional log - odds function for  @xmath63 , we have @xmath64 that is , the log - odds @xmath62 in our biased population is simply a vertical shift by @xmath55 of the log - odds @xmath65 in the original population , so given an estimate of @xmath66 we can subtract @xmath55 to estimate @xmath27 . if the model is correctly specified , logistic regression on the subsample yields a consistent estimate for the function @xmath62 , so the estimate for @xmath65 is also consistent .",
    "note that the derivation ( [ eqnbayesccbegin])([eqnbayesccend ] ) is equally valid if the sampling bias @xmath55 depends on @xmath26 , in which case we have @xmath67 .",
    "local case - control sampling exploits this more general identity .",
    "if @xmath6 is integrable , then upon differentiating the population risk ( [ eqnpoprisk ] ) with respect to @xmath32 we obtain the population score criterion : @xmath68 = \\int { \\bigl(p(x ) - p_\\theta(x ) \\bigr ) \\pmatrix{1 \\cr x } \\,d\\mathbb{p}(x)}.\\end{aligned}\\ ] ] informally , the best linear predictor is the one that gets the conditional probabilities right on average .",
    "note this is not the same as a predictor that gets the conditional log - odds right on average .    to illustrate the difference between approximating probabilities and approximating logits",
    ", suppose that @xmath69 and @xmath70 .",
    "the left panel of figure  [ figjumpplots ] shows @xmath65 as a solid line and its best linear approximation as a dashed line . on the logit scale ,",
    "the dashed line appears to be a very poor fit to the black curve .",
    "it fits reasonably well for large @xmath26 , but it appears more or less to ignore the smaller values of @xmath26 .",
    "the right panel of figure  [ figjumpplots ] shows why . when we transform both curves to the probability scale ,",
    "the fit looks much more reasonable .",
    "@xmath71 need not approximate @xmath65 particularly well for small @xmath26 , because in that range even a large change in the log - odds produces a negligible change in the conditional probability @xmath72 .",
    "by contrast , @xmath71 needs to approximate @xmath65 well for larger @xmath26 , where @xmath72 changes more rapidly .",
    "approximates the true log - odds @xmath65 in the sense of matching its implied conditional probabilities , not logits . ]    in general , logistic regression places highest priority on fitting @xmath27 where @xmath73 is largest : where @xmath74 and @xmath75 . in this example , with its strong marginal imbalance , the regions that matter most are those where @xmath72 is largest .",
    "this often makes sense in applications such as medical screening or advertising click - through rate prediction , where accuracy is most important when the probability of disease or click - through is nonnegligible . in section  [ secdiscussion ] ,",
    "we consider how to modify the method to obtain classifiers that prioritize correctness near some other , user - defined level curve of @xmath72 .",
    "finally , note that figure  [ figjumpplots ] suggests the case - control sampling estimate is unlikely to be consistent for @xmath0 in general .",
    "the nature of our linear approximation in the left panel is intimately related to the fact that @xmath76 everywhere in the sample space . if @xmath65 were shifted upward by some constant , the response of the dashed curve would be more complicated than a simple constant shift by @xmath55 , since the relative importance of the two segments would change .",
    "therefore , estimating @xmath77 and then subtracting @xmath55 may not be a successful strategy .      if the linear model is misspecified , the case - control estimate is generically not consistent for the best linear predictor @xmath0 as @xmath78 [ @xcite ] .",
    "the unadjusted estimate will instead converge to the best linear predictor of @xmath66 for the distribution @xmath63 , which solves the score criterion @xmath79 let @xmath80 be the large - sample limit of the _ adjusted _ case - control sampling estimate with bias @xmath55 . then @xmath80 solves the population score criterion @xmath81 which differs from ( [ eqnpopscore ] ) in two ways .",
    "first , the integral is taken over a different distribution for @xmath6 .",
    "second , and more importantly , the integrand is different .",
    "we are now approximating @xmath65 in a different sense than we were .    in general under misspecification",
    ", @xmath80 is different for every @xmath55 .",
    "if we sample cases and controls equally , in the limit we will get a different answer than if we sample twice as many controls ; and in either case we will get a different answer than if we use the entire data set or subsample uniformly",
    ".    these differences can be quite consequential for our inferences about @xmath82 or the predictive performance of our model , as we see next .",
    "[ ex1 ] in this fictitious example , we consider estimating the effect of exposure to oatmeal on a person s risk of developing some rare disease .",
    "suppose that @xmath83 of the population has a family history of the disease , half the population eats oatmeal ( independently of family history ) , and that both exposure and family history are binary predictors .",
    "suppose further that the true conditional log - odds function @xmath65 is given by the top - left panel of table  [ taboatmeal ] .",
    "[ taboatmeal ]    @lcc@ +   + & * history * @xmath84 & * history * @xmath85 + oatmeal @xmath86 & @xmath865 & @xmath864 + oatmeal @xmath87 & @xmath8610 & @xmath861 +    + 6pt    @lcc@ + & * history * @xmath84 & * history * @xmath85 + oatmeal @xmath86 & 0.007 & 0.02 + oatmeal @xmath87 & 5e@xmath865 & 0.37 +    @lcc@ +   + & * history * @xmath84 & * history * @xmath85 + oatmeal @xmath86 & @xmath861.2 & @xmath860.2 + oatmeal @xmath87 & @xmath866.2 & 2.8 +    + 6pt    @lcc@ + & * history * @xmath84 & * history * @xmath85 + oatmeal @xmath86 & 0.24 & 0.46 + oatmeal @xmath87 & 0.002 & 0.94 +    the corresponding conditional probabilities @xmath72 are given in the lower - left panel of table  [ taboatmeal ] .",
    "notice that oatmeal increases the risk for people who are already at risk by virtue of their family history , but has a protective effect for everyone else .",
    "this interaction means that the additive logistic regression model is misspecified .",
    "because only the probabilities in the `` history @xmath87 '' column are large enough to matter , the fitted model for @xmath65 pays more attention to the at - risk population , for whom oatmeal elevates the risk of disease .",
    "a logistic regression on a large sample from this population estimates the coefficient for oatmeal as @xmath88 , implying an odds ratio of about 4.0 .",
    "this is close to the marginal odds ratio of roughly 4.3 that we would obtain if we did not control for family history .",
    "suppose , however , that we sampled an equal number of cases and controls .",
    "then the conditional log - odds of disease in our sample would reflect the top - right panel of table  [ taboatmeal ] , with all cells increased by the same amount .    for large samples ,",
    "the case - control estimate is @xmath89 , implying an odds ratio of about 0.44 . using case - control sampling",
    "has reversed our inference about the effect of oatmeal exposure , because after shifting the log - odds the left column becomes much more important .",
    "[ ex2 ] suppose that @xmath90 , and that @xmath91 .",
    "let @xmath92 data simulated from this model are shown in the left panel of figure  [ fig2dmix ] . in this example , the true log - odds @xmath65 is an additive quadratic function of the two coordinates @xmath93 and @xmath94",
    ".    in this example as in the previous one , the population - optimal case - control parameters differ substantially from the optimal parameters in the original population , with dramatic effects for the predictive performance of the model .",
    "the boundaries for the two estimates are overlayed on the left panel of figure  [ fig2dmix ] . in the right panel",
    ", we plot the precision  recall curves resulting from each set of parameters on a large test set .",
    "a simple alternative to standard case - control sampling is to weight the subsampled data points by the inverse of their probability of being sampled .",
    "we include weighted case - control sampling as a competitor in our simulation studies in section  [ secsimulations ] . because it is a horvitz  thompson estimator with positive sampling probabilities for any @xmath95 pair , this method is @xmath96-consistent , and asymptotically normal and unbiased under general conditions [ @xcite ] .",
    "although weighting succeeds in removing the bias induced by the case - control sampling , this consistency comes at a cost of increasing the variance , since the effective sample size is reduced [ @xcite ( @xcite ) ] .     and @xmath97 .",
    "]    despite its inefficiency , the weighted case - control method can be an attractive means of obtaining a consistent pilot if another good pilot is not immediately available , and we later will use it to that end in our experiments .",
    "in this section , we describe local case - control subsampling , a generalization of standard case - control sampling that both improves on its efficiency and resolves its problem of inconsistency . to achieve these benefits ,",
    "we require a pilot estimate , that is , a good guess @xmath98 for the population - optimal @xmath0 .",
    "local case - control sampling differs from case - control sampling only in that the acceptance probability @xmath99 is allowed to depend on @xmath26 as well as @xmath100",
    ". our criterion for selection will be the degree of `` surprise '' we experience upon observing @xmath101 given @xmath102 : @xmath103 where @xmath104 is the pilot estimate of @xmath105 .",
    "the algorithm is :    generate independent @xmath106 .",
    "fit a logistic regression to the sample @xmath107 to obtain unadjusted estimates @xmath108 .",
    "assign @xmath109 and @xmath110 .",
    "as before , steps ( 2)(3 ) are equivalent to fitting a logistic regression in the subsample with offsets @xmath111 .",
    "the @xmath52 are generated as in section  [ seccc ] , and the adjustment is again justified by ( [ eqnbayesccbegin])([eqnbayesccend ] ) , only now with the constant @xmath55 replaced by @xmath112 in other words , the subsample is drawn from a measure with @xmath113 if @xmath65 is well approximated by the pilot estimate , then @xmath114 throughout feature space .",
    "that is , conditional on selection into @xmath115 , @xmath101 given @xmath102 is nearly a fair coin toss .    to motivate this choice heuristically ,",
    "recall that the fisher information for the log - odds of a bernoulli random variable is maximized when the probability is  @xmath116 : fair coin tosses are more informative than heavily biased ones . in effect",
    ", local case - control sampling tilts the conditional distribution of @xmath7 given @xmath117 to make each @xmath101 in the subsample more informative .",
    "we then fit a logistic regression in the more favorable sampling measure , and `` tilt back '' to obtain a valid estimate for the original population .    in marginally imbalanced data sets where @xmath105 is small everywhere in the predictor space",
    ", a good pilot has @xmath118 for all @xmath26 , and the number of cases discarded by this algorithm will be quite small .",
    "if we wish to avoid discarding any cases , we can always modify the algorithm so that instead of keeping @xmath119 with probability @xmath120 , we keep it with probability 1 and assign weight @xmath120 .      in many applications , there may be a natural choice of pilot fit @xmath121 ; for instance , if we are refitting a classification model every day to adapt to a changing world , then yesterday s fit is a natural choice for today s pilot .",
    "if no pilot fit is available from such a source , we recommend an initial pass of weighted case - control sampling ( described in section  [ subsecwcc ] ) to obtain the pilot .",
    "weighted case - control sampling using a fixed fraction of the original sample is itself @xmath96-consistent and asymptotically unbiased for the true parameters .",
    "consequently , if the pilot were fit using an independent data set the second - stage estimate would enjoy consistency and asymptotic unbiasedness per the results in section  [ sectheory ] .",
    "our experiments suggest that mild inaccuracy in the pilot estimate , and using a data - dependent pilot , do not unduly degrade the performance of the local case - control algorithm .",
    "for example , is simulation  2 of section  [ subsecsim2 ] , the pilot is fifty times less efficient than the final local case - control estimate .",
    "the main role of the pilot fit is to guide us in discarding most of the data points for which @xmath101 is obvious given @xmath102 while keeping those for which @xmath101 is conditionally surprising .",
    "in our example and simulations , we use a pilot sample about the same size as the local case - control subsample , on the principle that we can afford to spend about as much time computing the pilot as computing the second - stage estimate .",
    "when @xmath122 is small throughout @xmath123 , this rule amounts roughly to weighted case - control sampling using all the cases and one control per case .",
    "although the above rule has worked reasonably well for us , at this time we can offer no finite - sample guarantees that any given pilot sample size is large enough .",
    "because standard case - control sampling amounts to local case - control sampling with a constant - only pilot fit , we might expect that the pilot fit need not be perfect to improve upon case - control sampling .",
    "our experiments in sections  [ secsimulations ] and  [ secspam ] support this intuition .",
    "as we will see in section  [ subsecasymptotics ] , under correct model specification , and with an independent and consistent pilot , the baseline procedure described above has exactly twice the asymptotic variance as a logistic regression estimated with the full sample , despite using a potentially very small subset of the data .",
    "we can improve upon this factor of two by increasing the size of the subsample .",
    "one simple way to achieve this is to multiply all acceptance probabilities by some constant @xmath18 , for example , @xmath17 .",
    "when deciding whether to sample the point @xmath12 , we would then generate @xmath124 and assign weight @xmath125 to each sampled point .",
    "this amounts to a larger , weighted subsample from @xmath63 , and we can make the same correction to the estimates from the subsample .",
    "we see in section  [ subsecasylargersample ] that for @xmath2 the factor of two is replaced by a factor of @xmath1 .    in the case of large imbalance , most of the @xmath126 are near 0 or 1 . for @xmath2 ,",
    "the marginal acceptance probability at @xmath102 becomes @xmath127 where the approximation holds for @xmath128 or 1 . for @xmath19 ,",
    "the marginal acceptance probability is @xmath129 , so for @xmath2 we take roughly @xmath3 times as many data points as for @xmath19 . for example , if @xmath17 , the subsample accepted is roughly 3 times as large , and the relative efficiency improves from @xmath130 to @xmath131 .    alternatively ,",
    "if @xmath4 is extremely large , even a small fraction of the full data set may still be more than we want . in that case",
    ", we can proceed as above with @xmath132 , or simply sample any desired number @xmath133 of data points uniformly from the local case - control subsample .",
    "we now turn to examining the asymptotic behavior of the local case - control estimate .",
    "we first establish consistency , assuming a consistent pilot estimate @xmath121 .",
    "we expressly do not assume that the pilot estimate is independent of the data , since in some cases we may recycle into the subsample some of the data we used to calculate the pilot .    by assuming independence of @xmath121 and the data , we can obtain finer results about the asymptotic distribution of @xmath134 .",
    "we show it is asymptotically unbiased when @xmath121 is , and derive the asymptotic variance of the estimate .",
    "when the logistic regression model is correctly specified , the local case - control estimate has exactly twice the asymptotic variance of the mle for the full data set .      for better clarity of notation in this section , we will use the letter @xmath135 in place of @xmath121 to denote pilot estimates .",
    "additionally , we drop the notation @xmath136 and absorb the constant term into @xmath26 , so that @xmath137 . to avoid trivialities , assume without loss of generality that there is no @xmath138 for which @xmath139 ( if not , we can discard redundant features ) .",
    "for @xmath140 $ ] define the `` soft hinge '' function @xmath141 and note that @xmath142 = h\\bigl(\\theta'x ; p(x)\\bigr).\\ ] ] as a function of @xmath143 , @xmath144 is positive and strictly convex , its magnitude is bounded by @xmath145 , and it has lipschitz constant @xmath146 . if @xmath147 , @xmath144 diverges as @xmath148 , and if @xmath149 @xmath144 diverges as @xmath150 .    as a function of @xmath135 ,",
    "@xmath151 has lipschitz constant @xmath152 .",
    "hence , @xmath153 with lipschitz constant @xmath154 .",
    "the marginal acceptance probability given @xmath26 is @xmath155    given pilot @xmath135 , the local case - control subsampling scheme effectively samples from the probability measure @xmath156 , where @xmath157 and @xmath158 is the marginal probability of acceptance . under this measure , @xmath159 because @xmath160 , if @xmath161 is integrable under @xmath22 it is also integrable under any @xmath162",
    ".    conditioning on @xmath6 , we can write the population risk of the logistic regression parameters @xmath32 with respect to sampling measure @xmath162 as @xmath163    by cauchy  schwarz , the integrand in ( [ eqnpoprisklam2 ] ) is bounded by @xmath164 .",
    "if @xmath165 , then , we may appeal to dominated convergence and take limits with respect to @xmath32 and @xmath135 inside the integral .",
    "@xmath166 is strictly convex because the integrand is , and always has a unique population minimizer if there is no separating hyperplane in the population .",
    "[ lemma1 ] assume there is no @xmath167 for which @xmath168 henceforth , we refer to this assumption as _",
    "nonseparability_. then @xmath166 attains a unique minimum for every @xmath169 .",
    "denote by @xmath170 the empirical risk on a local case - control subsample taken using the pilot estimate @xmath135",
    ". then @xmath171.\\ ] ] it will be somewhat simpler to replace the random subsample size @xmath172 with its expectation @xmath173 .",
    "define @xmath174.\\ ] ] since minimizing ( [ eqnemprisk0 ] ) with respect to @xmath32 is equivalent to minimizing ( [ eqnemprisk1 ] ) , the two are equivalent for our purposes .",
    "if the unadjusted parameters @xmath175 minimize @xmath176 , the local case - control estimate @xmath177 is an @xmath178-estimator minimizing @xmath179 .",
    "we use analogous notation for the population version : @xmath180 for any given pilot estimate @xmath135 and large @xmath4 , we expect @xmath181 define the right - hand side of ( [ eqnargminqlam ] ) to be @xmath182 , the large - sample limit of local case - control sampling with pilot estimate fixed at @xmath135 .",
    "the best linear predictor for the original population corresponds to the case @xmath183 ( uniform subsampling ) , that is , @xmath184 .",
    "consistency means that for large @xmath4 , @xmath185 .    recall that if the model is correctly specified with true parameters @xmath35 , then @xmath186 for _ any _ fixed pilot estimate @xmath135 . minimizing @xmath187",
    "therefore yields a estimate .",
    "unfortunately , in the misspecified case @xmath188 . in this sense ,",
    "local case - control sampling with the pilot @xmath135 held fixed is in general _ not _",
    "consistent for @xmath0 .",
    "however , we see below that it is consistent if @xmath189 .",
    "[ propthetastarthetastar ] assume @xmath165 , that the classes are nonseparable , and that @xmath184 is the best linear predictor for the original measure @xmath22 .",
    "then @xmath190    in other words , if we could only choose our pilot perfectly , then the local case - control estimate would converge to @xmath0 as @xmath78 .",
    "proof of proposition [ propthetastarthetastar ] write @xmath191 .",
    "the population optimality criterion for lcc with pilot @xmath135 is @xmath192.\\end{aligned}\\ ] ] noting that @xmath193 , if we evaluate the above at @xmath194 , we obtain @xmath195 & = & \\tfrac{1}{2}\\mathbb{e } \\bigl [ x \\bigl(p(x ) \\bigl(1-p^*(x)\\bigr ) - \\bigl(1-p(x)\\bigr)p^*(x ) \\bigr ) \\bigr]\\hspace*{-25pt } \\\\ &",
    "= & \\tfrac{1}{2}\\mathbb{e } \\bigl [ x \\bigl(y - p^*(x ) \\bigr ) \\bigr]\\end{aligned}\\ ] ] which is exactly half the population score ( [ eqnpopscore ] ) for the original population .",
    "since @xmath0 optimizes the risk for the original population , this value is 0 .",
    "there is an intuitive explanation of this result : in @xmath196 , the _ acceptance probabilities _ are @xmath197 if @xmath198 and @xmath199 if @xmath60 ; hence they play the same role as the _ pseudoresiduals _ @xmath200 did in the original measure @xmath22 . for example , the point @xmath42 would contribute @xmath201 to the gradient if we evaluated the full - sample score at @xmath0 .",
    "evaluating the subsample score at 0 , the same point now contributes @xmath202 to the score  but only if it is accepted , which occurs with probability exactly @xmath203 .",
    "so , in essence , the subsampling stands in for the reweighting that we otherwise would have done when fitting our logistic regression to the full sample .",
    "of course , in practice we never have a perfect pilot ",
    "if we did we would not need to estimate @xmath0but proposition  [ propthetastarthetastar ] suggests that if @xmath135 is near @xmath0 , minimizing @xmath187 yields a good estimate .",
    "in fact , we will see that if @xmath204 then @xmath39 as well .      for our asymptotic results ,",
    "assume we have an infinite reservoir @xmath205 of i.i.d .",
    "pairs , a sequence of i.i.d .",
    "@xmath206 variables @xmath207 for making accept  reject decisions , and a sequence of pilot estimates @xmath208 the @xmath209 are possibly dependent upon the data , but the @xmath210 are assumed to be independent of everything else .",
    "@xmath211 is the local case - control estimate , computed using pilot @xmath209 , data@xmath212 , and accept  reject decisions @xmath213 .",
    "the main result of this section is that if the pilot estimate @xmath209 is consistent for @xmath0 , then so is @xmath211 .",
    "the details are somewhat technical , especially the proof of proposition  [ prop_pointwise ] , but the main idea is that if @xmath214 , then for large @xmath4 @xmath215 in the appropriate sense .",
    "@xmath216 is what the local case - control estimate actually minimizes , whereas the last function is minimized by @xmath0 , our ultimate target .",
    "first , we establish pointwise convergence .",
    "[ prop_pointwise ] if @xmath217 and @xmath218 , then for each @xmath219 , @xmath220    because we avoid assuming independence between the pilot @xmath209 and the data @xmath12 , the proof is technical and is deferred to the .",
    "the proof relies on the coupling of the acceptance decisions @xmath52 for different pilot estimates through @xmath210 . with this coupling",
    ", two nearby pilot estimates will differ on very few accept  reject decisions .    because neither @xmath221 nor @xmath222 changes very fast , pointwise convergence also implies uniform convergence on compacts .    [ propuniform ] if @xmath217 and @xmath223 , then for compact @xmath224 , @xmath225    define @xmath226 by proposition  [ prop_pointwise ] , @xmath227 pointwise .",
    "next , we show it is lipschitz .",
    "the integrand in ( [ eqnpopoptqlam ] ) is @xmath26 times two factors each bounded by @xmath228 , hence @xmath229 similarly for @xmath216 , we have @xmath230 so that @xmath231 it follows that , with probability tending to 1 , @xmath232 has lipschitz constant less than @xmath233 .",
    "now , for any @xmath234 , we can cover @xmath235 with finitely many euclidean balls of radius @xmath236 , centered at @xmath237 .",
    "let @xmath238 be the event that @xmath239 has lipschitz constant less than @xmath18 and @xmath240 on @xmath238 , we have @xmath241 , and @xmath242 as @xmath243 .    finally , we come to the main result of the section , in which we prove that the local case - control estimate is consistent when the pilot is .",
    "because the functions are strictly convex , we can ignore everything but a neighborhood of @xmath0 .",
    "[ thmconsistency ] assume @xmath244 and the classes are nonseparable .",
    "if @xmath214 then the local case - control estimate @xmath245 as well .",
    "let @xmath224 be any compact set with @xmath246 in its interior , and let @xmath247 where the strict inequality follows from strict convexity .",
    "uniform convergence implies that with probability tending to 1 , @xmath248 which implies in turn that @xmath249 whenever this is the case , the strictly convex function @xmath216 has a unique minimizer in the interior of @xmath235 . since @xmath235 was arbitrary , we can take its diameter to be less than any @xmath250",
    "hence , @xmath251 .      in this section ,",
    "we derive the asymptotic distribution of the local case - control logistic regression estimate , in the same asymptotic regime as the previous section . to prove our results here",
    ", we assume the pilot estimate @xmath209 is independent of our data set .",
    "this would not be the case if our pilot were based on a subsample of the data ( the procedure we use for all our simulations ) , but it could hold if the pilot came from a model fitted to data from an earlier time period .",
    "the main result of this section is that if the logistic regression model is correctly specified and the pilot is consistent , the asymptotic covariance matrix of the local case - control estimate for @xmath32 is exactly twice the asymptotic covariance matrix of a logistic regression performed on the entire data set . for the results in this section , we will need @xmath252 .",
    "it will be convenient to give names to some recurring quantities .",
    "first , we have seen that if @xmath165 we can differentiate @xmath253 inside the integral to obtain the gradient of the population risk : @xmath254 whereas @xmath255 is the expectation of the logistic regression score with respect to @xmath162 , we can also define its covariance matrix : @xmath256.\\ ] ] when @xmath257 , @xmath258 , and is continuous in @xmath32 and @xmath135 by dominated convergence .",
    "since the derivatives of the integrand in ( [ eqnintegralg ] ) are uniformly bounded by @xmath259 , dominated convergence implies we can again differentiate inside the integral . differentiating with respect to @xmath32 we",
    "obtain @xmath260 here , the integrand is dominated by @xmath261 , so dominated convergence again applies and thus we see that @xmath262 is continuous in @xmath32 and @xmath135 .",
    "@xmath263 for any @xmath264 since we have assumed there is no nonzero @xmath167 for which @xmath139 .",
    "finally , define the matrix of crossed partials : @xmath265 to be concrete , @xmath266 .",
    "continuity of @xmath267 again follows from noting the derivative of the integrand in ( [ eqnintegralg ] ) with respect to @xmath135 is dominated by @xmath268 .    to begin",
    ", we consider the behavior of @xmath182 for @xmath135 near @xmath0 . by proposition  [ propthetastarthetastar ]",
    ", we have @xmath269 . since @xmath263",
    ", we can apply the implicit function theorem to the relation @xmath270 to obtain @xmath271    by standard m - estimator theory , if we fix @xmath135 and send @xmath78 the coefficients of a logistic regression performed on a sample of size @xmath272 from @xmath162 would be asymptotically normal with covariance matrix @xmath273    in light of this and the fact that @xmath274 , we might predict the following .",
    "[ thm_condl_var ] assume @xmath257 . if @xmath275 independently of the data , then @xmath276 with @xmath277 .",
    "again , we defer the proof to the . we can combine ( [ eqncondlvar ] ) with ( [ eqncondlbias ] ) to immediately obtain the following reassuring facts .",
    "[ corbiasvar ] assume @xmath257 and @xmath209 is a sequence of pilot estimators given independently of the data .",
    "then :    if @xmath209 is @xmath96-consistent , so is @xmath211 .    if @xmath209 is asymptotically unbiased , so is @xmath211 .",
    "if @xmath278 then @xmath279 with @xmath280 in ( [ eqnfullvar ] ) , we have suppressed the arguments of @xmath0 in @xmath281 and @xmath282 .",
    "the first term in ( [ eqnfullvar ] ) characterizes the contribution of conditional bias ( given  @xmath209 ) to the overall variance , and the second is the contribution of conditional variance .    in the special case where logistic regression model is correctly specified",
    ", we have the following .    [ thmasyvar ] assume the logistic regression model is correct and let @xmath283 be the asymptotic variance of the mle for the full sample . then if @xmath257 and @xmath284 independently of the data , we have @xmath285    hence , although the size of a local case - control subsample is roughly @xmath286 , the variance of @xmath134 is the same as if we took a simple random sample of size @xmath287 from the full data set . in other words ,",
    "each point sampled is worth about @xmath288 points sampled uniformly .",
    "proof of theorem [ thmasyvar ] if logistic regression is correctly specified for @xmath22 , it is also for @xmath162 , regardless of @xmath135 , so @xmath289 .",
    "furthermore , by standard maximum likelihood theory @xmath290 for each @xmath135 . therefore , ( [ eqncondlvar ] ) specializes to @xmath291 but @xmath292 \\biggl[\\frac{e^{\\lambda'x}+e^{f(x)}}{(1+e^{\\lambda ' x})(1+e^{f(x ) } ) } \\biggr ] xx ' \\,d\\mathbb{p}(x)}.\\ ] ] if @xmath293 and @xmath294 , then ( [ eqngeneralhessian ] ) simplifies to @xmath295    this result is surprisingly simple .",
    "no characterization like theorem  [ thmasyvar ] is available for the case - control and weighted case - control estimates , whose variances are not simple scalar multiples of @xmath296 .",
    "we can offer a simple heuristic argument for theorem  [ thmasyvar ] , similar to that of proposition  [ propthetastarthetastar ] . in @xmath297 , the acceptance probability @xmath298 for an observation at @xmath26",
    "is @xmath299 , and given that it is accepted it contributes @xmath300 to the observed information .",
    "in the full sample , a point at @xmath26 is always accepted but contributes less , @xmath301 , to the observed information .",
    "again , the sampling probability stands in for the reweighting we would have done in the full sample .",
    "if @xmath302 is very small , we are discarding most of the data instead of keeping all of it and assigning it a tiny weight in the fit .",
    "the practical meaning of theorem  [ thmasyvar ] is that local case - control sampling is most advantageous when @xmath303 is small , that is , when @xmath7 is easy to predict throughout much of the covariate space .",
    "this can happen as a result of marginal or conditional imbalance , or both .",
    "standard case - control sampling can also improve our efficiency in the presence of marginal imbalance , but unlike local case - control sampling , it does not exploit conditional imbalance . hence , we would expect local case - control to outperform standard case - control most dramatically when the marginal imbalance is very high , as in the simulation of section  [ subsecsim2 ] .    for data - dependent pilots ,",
    "the efficiency picture is somewhat more complicated .",
    "for example , @xmath182 is approximately a linear function of @xmath304 .",
    "thus , if @xmath135 is unbiased but correlated with the noise in the data , we might get more or less variance relative to  ( [ eqnasyvar2 ] ) , depending on how this correlation interacts with @xmath267 .",
    "if the model is correctly specified , it less clear whether an adversarially chosen pilot can affect the efficiency .    either way , we do not anticipate serious problems from nonindependence .",
    "to stress - test our results against violations of independence , we expressly use a data - dependent pilot for all of our experiments : namely , a weighted case - control sample with sample points allowed to be recycled for the second - stage fit .      in section  [ subseclargersample ]",
    ", we proposed increasing the size of the local case - control subsample by multiplying all the acceptance probabilities @xmath305 by a constant @xmath2 and assigning weight @xmath306 when @xmath307 .",
    "we analyze the asymptotic variance here as a function of @xmath18 . to simplify matters , suppose the model is correctly specified and @xmath135 is fixed at @xmath35 .    the weighted log - likelihood for the subsample and its derivatives are then @xmath308 conditionally on @xmath26 ,",
    "there is a @xmath309 chance @xmath310 and @xmath311 , where @xmath312 .",
    "similarly , there is a @xmath313 chance @xmath314 , and @xmath315 .",
    "we immediately obtain @xmath316    the expectation and variance of the score evaluated at 0 are @xmath317 and the expected hessian is @xmath318 we have derived @xmath319 for @xmath19 , we recover the factor of two from ( [ eqnasyvar2 ] ) , but , for example , @xmath17 we only pay 20% increased variance relative to the full sample .",
    "here , we compare our method to standard weighted and unweighted case - control sampling for two - class gaussian models like the one considered in section  [ subsecccbias ] . the standard case - control estimates use a 5050 split between the two classes .",
    "we begin with a five - dimensional two - class gaussian simulation where the classes have different covariance matrices . if @xmath320 , then @xmath321\\\\[-8pt ] & & { } + \\frac{1}{2}(x-\\mu_0)'\\sigma_0^{-1}(x-\\mu_0 ) + \\operatorname{const.}\\nonumber\\end{aligned}\\ ] ]    equation  ( [ eqnqda ] ) is linear if @xmath322 , and quadratic otherwise , so if the two covariance matrices were the same the linear logistic model would be correctly specified . in this case",
    "the model is incorrectly specified , letting us compare the behavior of the different methods under model misspecification .",
    "take @xmath90 , @xmath323 , and @xmath324 .",
    "the covariance matrices are @xmath325 and @xmath326 .",
    "hence @xmath65 is additive , but with a nonzero quadratic term in @xmath327 .    for our simulation ,",
    "we first generate a large ( @xmath328 ) sample from the population described above .",
    "second , we obtain a pilot model using the weighted case - control method on @xmath329 data points .",
    "next , we take a local case - control of size 1000 using that pilot model .",
    "[ tabmse ]    @@l c d1.4d2.6 c d1.3d2.6@@xmath330 + & & & & & & + lcc & & 0.0049 & ( 0.00031 ) & & 0.025 & ( 0.00059 ) + wcc & & 0.023 & ( 0.0022 ) & & 0.16 & ( 0.0038 ) + cc & & 0.15 & ( 0.0016 ) & & 0.043 & ( 0.00096 ) +   + & & & & & & + lcc & & 0.0037 & ( 0.0083 ) & & 0.039 & ( 0.00045 ) + wcc & & 0.59 & ( 0.064 ) & & 1.7 & ( 0.017 ) + cc & & 0.06 & ( 0.042 ) & & 0.87 & ( 0.0086 ) +    for comparison , we obtain standard case - control ( cc ) and weighted case - control ( wcc ) estimates . for the comparison estimators we do not use a sample of size 1000 again but rather use the total number of observations seen by the lcc model or the pilot model , roughly 2000 , so",
    "the lcc estimate must pay for its pilot sample .",
    "we repeat this entire procedure 1000 times .",
    "table  [ tabmse ] shows the squared bias and variance of @xmath331 over the 1000 realizations for each of the three methods . as expected , we face a bias - variance tradeoff in choosing between the wcc and cc methods , whereas the lcc method improves substantially on the bias of cc and the variance of wcc .",
    "standard errors for both bias and variance are computed via bootstrapping the 1000 realizations .",
    "more surprising is the fact that lcc enjoys smaller bias than wcc and smaller variance than cc , dominating the other two methods on both measures .",
    "the improvement in variance over the cc estimate is likely due to the conditional imbalance present in the sample , while the improvement in bias over the wcc estimate may come from the fact that the methods are only unbiased asymptotically and the lcc estimate is closer to its asymptotic limiting behavior .",
    "next , we simulate a two - class gaussian model with each class having the same variance , so that the true log - odds function @xmath27 is linear .",
    "we also increase the dimension to 50 for this simulation .",
    "since the model is now correctly specified , all three methods are asymptotically unbiased . however , in this case we introduce more substantial conditional imbalance , to demonstrate the variance - reduction advantages of local case - control sampling in that setting .    for this example , @xmath332 , @xmath333 , @xmath334 , and @xmath335 .",
    "we repeat the procedure from section  [ subsecsim1 ] , now with @xmath336 . instead of generating a full sample ,",
    "the full data set is implicit and we sample directly from @xmath63 .    in this example , the difference between the methods is more dramatic .",
    "table  [ tabmse ] shows the squared bias and variance of the three methods . here ,",
    "local case - control enjoys substantially better bias than the other two methods , improving on cc more than twenty - fold . for the correct pilot model ,",
    "@xmath337 is roughly 0.005 , so the local case - control subsample size is around @xmath338 . since the model is correctly specified , the variance is roughly twice that of logistic regression on the full sample of size @xmath4 . in other words ,",
    "local case - control subsampling is roughly 100 times more efficient than uniform subsampling .",
    "asymptotically , all three methods are unbiased but it appears that lcc again enjoys a smaller bias in finite samples .",
    "relative to standard case - control sampling , local case - control sampling is especially well - suited for data sets with significant conditional imbalance , that is , data sets in which @xmath101 is easy to predict for most @xmath102 .",
    "one such application is spam filtering . to demonstrate the advantages of local case - control sampling and compare asymptotic predictions to actual performance , we test our method on the web spam data available on the libsvm website . ] and originally from @xcite .",
    "the data set contains 350,000 web pages , of which about 60% are labeled as `` web spam , '' that is , web pages designed to manipulate search engines rather than display legitimate content .",
    "this data set is marginally balanced , though as we will see the conditional imbalance is considerable .    as features , we use frequency of the 99 unigrams that appeared in at least 200 documents , log - transformed with an offset so as to reduce skew in the features . in this data",
    "set , the downsampling ratio @xmath339 is around 10% , that is , when using a good pilot we will retain about 10% of the observations .    since we only have a single data set , we use subsampling as a method to assess the sampling distribution of our estimators . in each of 100 replications , we begin by taking a uniform subsample of size @xmath340100,000 from the population of 350,000 documents . after obtaining 100 data sets of size @xmath340100,000 , we use the same procedure as we used in our two simulations with @xmath34110,000 .",
    "our asymptotic theory predicts that the variance of the local case - control sampling estimate of @xmath32 should be a little more than twice the variance using the full sample ( more because the model is misspecified and our pilot has some variance ) .",
    "because the full sample is close to marginally balanced , the standard case - control sampling methods should do about as well as a uniform subsample of size 20,000that is , they should have variance roughly 5 times that of the full sample .    note",
    "that 20,000 is roughly twice the size of the local case - control sample , since we are counting the pilot sample against the local case - control method .",
    "if we had a readily available pilot model , as we would in many applications , it would be more relevant to give the cc and wcc methods access to only 10,000 data points , doubling their variance relative to the observed variance in this experiment .",
    "the theoretical predictions come reasonably close in this experiment , as shown in figure  [ figwebspamcoefs ] .",
    "the horizontal axis indexes each of the 100 coefficients to be fit ( there are 99 covariates and an intercept ) , and the vertical axis gives the variance of each estimated coefficient , relative to the variance of the same coefficient in a model fitted to the full sample .",
    "variance for local case - control , @xmath342 variance for standard ) are reasonably close to the mark , though a bit optimistic . ]",
    "the magnitude of our improvement over standard case - control sampling is substantial here , but could be much larger in a data set with an even stronger signal .",
    "the key point is that standard case - control methods have no way to exploit conditional imbalance , so the more there is , the more local case - control dominates the other methods .",
    "we have shown that in imbalanced logistic regression , we can speed up computation by subsampling the data in a biased fashion and making a post - hoc correction to the coefficients estimated in the subsample .",
    "standard case - control sampling is one such scheme , but it has two main flaws : it has no way to exploit conditional imbalance , and when the model is misspecified it is inconsistent for the population risk minimizer .",
    "local case - control sampling generalizes standard case - control sampling to address both flaws , subsampling with a bias that is allowed to depend on both @xmath26 and @xmath100 .",
    "when the pilot is consistent , our estimate is consistent even under misspecification , and if the model is correct then local case - control sampling has exactly twice the asymptotic variance of logistic regression on the full data set .",
    "our simulations suggest that local case - control performs favorably in practice .      in the",
    ", we motivated our inquiry by identifying four ways that computational gains can translate to statistical ones .",
    "specifically , we suggested that computational savings can :    enable us to experiment with and prototype a variety of models , instead of trying only one or two ,    allow us to refit our models more often to adapt to changing conditions ,    allow for cross - validation , bagging , boosting , bootstrapping , or other computationally intensive statistical procedures or    open the door to using more sophisticated statistical techniques on a compressed data set .",
    "it is relatively clear how our proposed method can help with points ( 1 ) and ( 2 ) .",
    "as for point ( 3 ) , faster fitting procedures can directly speed up straightforward resampling techniques like bootstrapping or cross - validation , possibly making them feasible at scales where they previously were not .",
    "we discuss in section  [ subsecextens ] how it can also help with boosting .    the basic method as we have described it above does not deliver on point ( 4 ) , because the pilot model and second - stage model are the same .",
    "however , an extension of our method can help , which we discuss below .",
    "there is no reason in principle why the pilot model must be linear , or belong to the same model class as the model we fit to the local case - control sample .",
    "we can use any pilot predictions @xmath343 in the sampling algorithm , and then model the log - odds in the subsample quite flexibly  by a gam , kernel logistic regression , random forests or any other method  so long as we can use offsets @xmath344 in the second - stage procedure .",
    "for example , we could use as our pilot fit a simple model with a few important variables explaining most of the response , and in the second stage estimate more complex models refining the first .    formally , our theoretical results may not cover this use .",
    "suppose the second - stage model can be written as a logistic regression after some basis expansion .",
    "then consistency of the second - stage estimate requires either that the pilot be consistent ( the new variables contribute nothing to the population fit ) or that the second - stage model be correctly specified . if neither of these assumptions holds approximately , then our estimate could be biased  though perhaps not as biased as case - control sampling , which is a special case of local case - control with an intercept - only pilot .",
    "if we are prototyping , guarantees of consistency may not be a high priority .",
    "if they are , then as with case - control sampling , we can repair the inconsistency of the local case - control estimate by using a horvitz  thompson estimator with weights @xmath345 .",
    "this may come at a cost of some added variance .",
    "it would be interesting to examine the bias of local case - control and the variance of weighted local case - control in this more general problem setting .",
    "this work suggests extensions in several directions , described below .      in some applications ( e.g. , diagnostic medical screening )",
    ", a false negative may be more costly than a false positive , or .",
    "one of the implications of the discussion in section  [ subsecccbias ] is that the bernoulli log - likelihood implicitly places most emphasis on approximating the log - odds well near the 0 ( 50% probability ) level curve , which may not be appropriate if the decision boundary relevant to our application is at 10% . in general",
    ", we would expect to obtain a better model in the large-@xmath4 limit if we target the decision boundary we care most about .    in a sense , the reason that standard case - control sampling performed so badly in example  [ ex2 ] of section  [ subsecccbias ] is that it targeted a level curve of @xmath346 other than 50% . specifically , it targeted the level curve corresponding to 50% in the subsampling population for equal - sampled case - control sampling , which corresponds to the marginal @xmath347 level curve in the original population .",
    "what happened by accident in example  [ ex2 ] need not always be one , and it would be interesting to generalize our procedure so as to target any chosen decision threshold . more generally still , our indifference point could depend on our features @xmath26in online advertising , for instance , some advertisers may be willing to pay more per click than others .",
    "in section  [ subsecadvantages ] we suggested using offsets to obtain a complex second - stage fit .",
    "alternatively , we can obtain any fitted log - odds function @xmath348 for the sample and simply add it to the pilot @xmath349 to obtain an estimate for @xmath65 .",
    "this observation suggests the possibility of iteratively fitting a `` base model '' to the subsample , then adding it to @xmath349 to obtain a new pilot for the next iteration .",
    "indeed , that iterative algorithm is closely related to the adaboost algorithm of @xcite . even more similarly to adaboost , we could weight each point by @xmath13 instead of sampling it with that probability .",
    "@xcite show that the adaboost algorithm can be thought of as fitting a logistic regression model additive in base learners . in adaboost",
    ", the function @xmath350 simply records the number of classifiers @xmath351 classifying @xmath26 as belonging to class @xmath352 minus the number classifying it as class @xmath353 , and friedman et al .",
    "show that @xmath354 can be thought of as approximating the log - odds of @xmath355 given @xmath117 .",
    "the difference is that while adaboost weights the point @xmath12 by@xmath356 , the local case - control version would use weights @xmath357    operationally , this alternative weighting scheme limits the influence of `` outliers , '' that is , hard - to - classify points that can unduly drive the adaboost fit .      in high - dimensional settings , lasso- or ridge - penalized logistic regressions",
    "are often preferable to standard logistic regression , the model considered here .",
    "one could use local case - control sampling with a regularized version of logistic regression , but our asymptotic results might need revisiting in such a case  especially in a high - dimensional asymptotic regime [ @xmath358 or @xmath359 . since the high - dimensional setting is important in modern statistics and machine learning , this bears further investigation .",
    "one way of viewing the method is as a way of `` tilting '' the conditional distribution of @xmath7 by a linear function of @xmath6 in the natural parameter space so as to enrich our subsample for more informative observations .",
    "we could use similar tricks on other glms .",
    "for instance , suppose we are given a poisson variable with natural parameter @xmath360 . by sampling with acceptance probability proportional to @xmath361 ,",
    "we obtain ( conditional on acceptance ) a poisson with natural parameter @xmath362 .",
    "since poisson variables with larger means carry more information , this could yield a substantial improvement over uniform subsampling.=1    if our data arise from a poisson glm with @xmath363 , we could generalize the local case - control scheme by sampling @xmath12 with probability proportional to @xmath364 , where the extra parameter @xmath365 guarantees that we always tilt the conditional mean of @xmath101 upward .",
    "similar generalizations may apply for multinomial logit and survival models .",
    "because @xmath166 is strictly convex , it is sufficient to show that @xmath366 as @xmath367 in any direction",
    ".    assume w.l.o.g .",
    "there is some neighborhood @xmath368 for which @xmath369 @xmath370 is linear in its second argument , and is increasing for sufficiently large @xmath143 .",
    "thus , for large enough @xmath371 , the population risk for @xmath22 is @xmath372    @xmath373 for any @xmath135 , so ( [ eqncond ] ) holds for @xmath162 with the same @xmath374 ( but a different @xmath375 ) .",
    "thus , we can repeat the same argument with @xmath22 replaced by @xmath376 .",
    "fix @xmath32 and begin by writing @xmath377    let @xmath378 be the bernoulli selection decisions , generated by comparing mutually independent @xmath53 to the threshold @xmath379 .",
    "the @xmath378 are independent conditional on @xmath135 and the data . also , write @xmath380 , so that @xmath381 .    by the cauchy",
    "schwarz inequality , we have @xmath382    now , for @xmath250 define @xmath383 . for @xmath384 ,",
    "we have @xmath385 which is integrable by assumption . finally let @xmath386 denote an average taken over indices @xmath387 , that is , @xmath388 . then @xmath389    by continuity , @xmath390 .",
    "therefore , it suffices to show @xmath391 . because @xmath392 by the law of large numbers , it suffices equally well to show that @xmath393 .",
    "now fix @xmath234 and take @xmath394 large enough that @xmath395 . for @xmath396 we have @xmath397    with probability one the second term is eventually less than @xmath398 .",
    "further , for @xmath399 , we have @xmath400    now , write @xmath401    @xmath402 iff @xmath210 lies between @xmath403 and @xmath404",
    ". hence , conditionally on @xmath209 and the data , the @xmath405 are mutually independent nonnegative random variables bounded by @xmath394 with means @xmath406 since @xmath407 .",
    "continuing , we have @xmath408    conditioning on @xmath135 and @xmath409 , the first term is a sum of independent zero - mean random variables that are bounded in absolute value by @xmath394 . by hoeffding s inequality , @xmath410.\\ ] ]    since this bound is deterministic , the same applies to the unconditional probability that @xmath411 is large .",
    "take @xmath412 .",
    "with probability tending to  1 , @xmath399 and the event in ( [ eqnhoeffding ] ) holds , in which case @xmath413    since @xmath414 was arbitrary , the proof is complete .",
    "by the mean value theorem , we have for each @xmath4 @xmath415 where @xmath416 is some convex combination of @xmath211 and @xmath417 . noting that the lhs is by definition 0 and rearranging , we obtain @xmath418",
    "if we can show the first factor tends in probability to @xmath419 and the second tends in distribution to @xmath420 , then by slutsky s theorem we have the desired result .    using the skorokhod construction define a joint probability space for @xmath209 such that @xmath421 .",
    "we will condition on the sequence @xmath209 and use a triangular array central limit theorem for the random variables @xmath422 because @xmath209 is independent of the data , @xmath423 for any @xmath27 .",
    "the triangular array clt applies since @xmath424 \\\\ &",
    "= & \\mathbb{p}(z_{ni}=1{|}\\lambda_n)\\bar a ( \\lambda_n)^{-2 } { \\operatorname{var}}_{\\lambda_n}\\bigl ( \\nabla_\\theta\\ell\\bigl(\\bar\\theta(\\lambda_n)- \\lambda_n;x_{ni},y_{ni}\\bigr)\\bigr ) \\\\ & = & \\bar a(\\lambda_n)^{-1 } j\\bigl(\\bar\\theta ( \\lambda_n),\\lambda_n\\bigr ) \\\\ & \\stackrel{\\mathrm{a.s .",
    "} } { \\rightarrow } & \\bar a\\bigl(\\theta^ * \\bigr)^{-1}j\\bigl(\\theta^*,\\theta^*\\bigr).\\end{aligned}\\ ] ] therefore , defining @xmath425 and @xmath426 , the clt tells us @xmath427 whenever @xmath428 , which we assumed occurs with probability 1 . by dominated convergence",
    ", we also have @xmath429 .",
    "next we turn to the hessian .",
    "we have @xmath245 by theorem  [ thmconsistency ] , so @xmath430 as well .",
    "writing @xmath431 we need to show that @xmath432 note that @xmath433 , which is integrable ; hence @xmath434 .",
    "since @xmath339 is continuous and strictly positive , and @xmath435 , it suffices to show that @xmath436 note that @xmath437 , and define @xmath438 .",
    "following the structure of the proof of proposition  [ prop_pointwise ] , take @xmath394 large enough that @xmath439 and truncate the @xmath440 : @xmath441 the second term is eventually less than @xmath398 .",
    "now , @xmath442 is small , because @xmath443 hence , by cauchy ",
    "schwarz @xmath444 so on the event @xmath445 , we have @xmath446 finally , we can bound the first term exactly as we did in the proof of proposition  [ prop_pointwise ] , defining @xmath447 . the same argument implies @xmath448 $ ] , so as @xmath78 we have with probability approaching 1 , @xmath449 so taking @xmath450 , the right - hand side is less than @xmath451 .",
    "the authors are grateful to jerome friedman for suggesting that we investigate the bias of case - control sampling , and to nike sun for helpful comments and suggestions ."
  ],
  "abstract_text": [
    "<S> for classification problems with significant class imbalance , subsampling can reduce computational costs at the price of inflated variance in estimating model parameters . </S>",
    "<S> we propose a method for subsampling efficiently for logistic regression by adjusting the class balance locally in feature space via an accept  reject scheme . </S>",
    "<S> our method generalizes standard case - control sampling , using a pilot estimate to preferentially select examples whose responses are conditionally rare given their features . </S>",
    "<S> the biased subsampling is corrected by a post - hoc analytic adjustment to the parameters . </S>",
    "<S> the method is simple and requires one parallelizable scan over the full data set .    </S>",
    "<S> standard case - control sampling is inconsistent under model misspecification for the population risk - minimizing coefficients @xmath0 . </S>",
    "<S> by contrast , our estimator is consistent for @xmath0 provided that the pilot estimate is . </S>",
    "<S> moreover , under correct specification and with a consistent , independent pilot estimate , our estimator has exactly twice the asymptotic variance of the full - sample even if the selected subsample comprises a miniscule fraction of the full data set , as happens when the original data are severely imbalanced . </S>",
    "<S> the factor of two improves to @xmath1 if we multiply the baseline acceptance probabilities by @xmath2 ( and weight points with acceptance probability greater than  1 ) , taking roughly @xmath3 times as many data points into the subsample . </S>",
    "<S> experiments on simulated and real data show that our method can substantially outperform standard case - control subsampling . </S>"
  ]
}