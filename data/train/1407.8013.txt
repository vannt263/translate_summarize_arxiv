{
  "article_text": [
    "the question if given a time series measurements we can identify an underlying dynamical model and predict its future values begin certainly with yule @xcite and is posed today in several disciplines as well as economy , geophysics or fluids dynamics . in economy research",
    ", the short time prediction plays an important role in financial risk decision ; notwithstanding dynamical models for the different observable are not known . then the huge quantities of data we dispose do possible statistical approaches .",
    "antagonist examples are seismic inversion , and oil and water research where the model is in general well known , i.e. the wave equation or darcy like models , but experimental data are only accessible at the frontier of the studied region . as a consequence geophysics research has developed powerful tools of collecting the bulk information .",
    "an other interesting and different example is fluid dynamics because it has a good model , the navier stokes equation , and the possibility of taking data everywhere .",
    "unfortunately the harvests of the initial conditions are difficult since requires the measurements of these functions over a three - dimensional domain .",
    "then typical experiences in hydrodynamics produce time series and so , the most practical situations deal with time series .",
    "all these examples are different outlooks of the same universal question : how can we obtain dynamical systems from measurements ?",
    "it is awful question because we have an infinite number of models m belonging to a specific class of functions ( radial functions , polynomials , etc . )",
    "which must be specified in view of the a priori knowledge about the problem , and even when the model is known it will be parameterized by a set of unknown numbers @xmath0 .    in this paper",
    "we consider a classical problem in nonlinear dynamical systems : given a noisy time series , we want to capture the underlying dynamics and , to do that we suppose that it can be modeled by a coupled system of ordinary differential equations ( ode ) parameterized by a set of numbers @xmath0 .",
    "we propose a constraint minimization to reduce the model complexity , that is , to find out parameters with scanty influence on the dynamic ( @xmath1 ) , and in addition to reduce the overfitting risk .",
    "the order of the model is given by the number of non zero components of the vector @xmath0 .",
    "we apply a variational approach to compute the derivatives of @xmath2 for a defined measure @xmath3 and a step descent method to find the optimal set of parameters",
    ". we will show on chaotic time series that this technique is robust and able to reconstruct orbits from noisy data .",
    "the baseline time series are generated by the following model of @xmath4 coupled ode : @xmath5 where the parameter vector @xmath6 .",
    "the integration method is an euler schema with time step @xmath7 to assure the stability for long time integrations . a gaussian noise with zero average and standard deviation @xmath8 , @xmath9 ,",
    "is then added to the noiseless signal @xmath10 to build the `` observed '' data @xmath11 .",
    "different noisy time series are produced by modifying the standard deviation @xmath8 .",
    "the amount of noise over the signal is quantified by the signal - to - noise ratio ( snr ) @xmath12 where @xmath13 stands by the root mean square , in particular in our case @xmath14 .",
    "the logarithm relation @xmath15 gives the ratio in @xmath16 .",
    "the `` observed '' data used for computations are @xmath17 where we assumed the measurements performed at fixed sampling time @xmath18 .    to asses",
    "the quality of the reconstruction we define a functional @xmath19 which consists on the addition of the euclidean distances between the observed @xmath20 and the reconstructed data @xmath21 on @xmath22 time windows over a time integration @xmath23 : latexmath:[\\[\\begin{aligned } { \\cal f}_i= \\sum_{w}\\int^l_0 || \\",
    "d_i(t ) - m_i(t ) \\",
    "the reconstructed data @xmath21 are generated by a model @xmath25 at fixed parameters @xmath0 , and we note that for a free noise series , when @xmath26 and @xmath11 coincide both with @xmath10 we have @xmath27 . according to the fact that the measurement are performed in a discrete way we define a delta function @xmath28 related to the sampling time @xmath29 ; for instance @xmath30 for multiple of the sampling time of observed data and zero elsewhere .",
    "we are in presence of an inverse problem , that is to seek for an optimal set of parameters of a model with respect to a measure @xmath31 .",
    "the classical approach for a @xmath32 dimensional problem is the unconstrained optimization : minimize @xmath33 with @xmath6 .",
    "this becomes most of the time a classical least squares approach or one of its several variations . for a linear model in the parameters",
    "@xmath0 , the cost function is quadratic and there is only one global minimum .",
    "we can estimate directly the derivatives of the model @xmath34 from observed data @xmath35 but noise prevent an appropriate evaluation .",
    "we recall that inverse problems are generally ill - conditioned which is reflected in the lack of robustness face to noise showed in numerical simulations  @xcite .    this work presents a constrained optimization and we solve it using a variational approach .",
    "the formal definition of constrained optimization is the following : minimize @xmath33 with @xmath6 subject to a constraints @xmath36 .",
    "we define specifically the @xmath37 as the proposed model @xmath34 for the `` observed '' data .",
    "we therefore write @xmath36 as @xmath38 where @xmath4 is the dimension of the data series and the fonction @xmath39 belonging to some specific class of functions .",
    "explicit dependency in time is removed for sake of readability . on each windows",
    "@xmath22 the model is integrated between @xmath40 and @xmath23 and the initial conditions are the `` observed '' values at the beginning , @xmath41 .",
    "within this situation we are close to an initial value problem in the framework of the multiple shooting approach .",
    "we define the following lagrangian function @xmath42 where @xmath43 is the dual variable corresponding to the constraint or lagrange multiplier . as the constraint",
    "is always verified we have @xmath44 for any choice of @xmath45 .",
    "the total variation is then @xmath46 we observe that the term @xmath47 is equivalent to the imposed constraints and therefore zero . provided that @xmath48 it is clear that we can computed the gradient explicitly as a matter of fact @xmath49 implies @xmath50 imposing @xmath51 results in a system for @xmath52 that must be integrated backward in time over the length window @xmath23 . for each window @xmath22",
    "this leads to the following expression @xmath53 with the boundary condition set at the final time @xmath23 , @xmath54 and where @xmath55 is the local error .",
    "the gradient of the components @xmath56 of the cost function , @xmath57 , can be write explicitly as @xmath58 and finally the gradient for a given parameter @xmath2 is @xmath59    once the gradient established we perform a descent in the direction of the gradient of @xmath3 .",
    "we apply a _ quasi - newton _ method which uses the function gradient at each iteration @xcite .",
    "the optimisation algorithm find an optimal set which depends on the noise level @xmath8 and on the window length @xmath23 , @xmath60 .",
    "the model is evaluated on each window for fixed parameters @xmath0 for different @xmath23 starting from @xmath61 .",
    "the @xmath22 windows of temporal length @xmath23 are thus equivalent to the fitting intervals of the multiple shooting method but the difference being that we do not impose the matching between solutions at the interval frontier ( @xmath62 ) .",
    "this exemple has been well examined in reference @xcite in two cases : when the model is available and when we specify only its class .",
    "( from the top to the bottom ) .",
    ", scaledwidth=50.0% ]    the figure  [ fig:0 ] shows a schematic illustration of the process for @xmath63 .",
    "we paid attention to a specific data ( the third one beginning from the left ) , we observe that it is used on the window : ( i ) for @xmath61 as initial condition , ( ii ) for @xmath64 as the final condition , and ( iii ) for @xmath65 as internal data .",
    "then , it is straightforward of concluding that data are used more that once in the process in contrast to the multiple shooting method , and that `` window overlap '' enhances the statistics . to improve yet the statistic",
    "we repeat the procedure over a large number of probes @xmath66 of length @xmath67 . using the optimal values from each probe we compute the average value @xmath68 and the standard deviation @xmath69 .",
    "we can apply this procedure to experimental data by splitting experimental data series in probes of size @xmath70 .",
    "the figure [ fig:1 ] shows the variation of three parameters @xmath71 as function of the window length @xmath23 for the lorenz equation with noise equal to 14.53 @xmath16 @xmath72 . in this case",
    "we have @xmath73 and the probe size @xmath70 is set to @xmath74 .",
    "we will discuss the lorenz model and the figure in the next section .",
    "the standard deviation @xmath75 is represented by horizontal ticks .",
    "note that the average values converge from @xmath76 , in term of optimisation we can infer that no more information is available .",
    "finally parameters @xmath0 with small mean values and weak ratio @xmath77 are discarded and this determines the end of the first stage of the optimisation process .",
    "the ratio @xmath78 is called reliability which is an estimation of the statistical reliability ( the difference is that we known the average value @xmath68 instead of the actual value ) . a central point is how do we quantify it .",
    "we decide that @xmath79 are discarded and we set @xmath80 which is quite arbitrary .",
    "we argue that if the parameters @xmath0 are of the order of 1 this @xmath81 implies that the discarded parameters are at least around 2% of the keeping ones . on the contrary in a case with parameters @xmath0 of the order of less than 1 we have to re - define this cutoff .",
    "next , for the reliability the criterion is less arbitrary as long as parameters with @xmath82 are considered weak , the cutoff is then @xmath83 .",
    "we keep parameters which in general , have high reliability , small @xmath75 , and are different from zero .",
    "we reduce by the procedure the number of parameters to avoid overfitting . in the following stages we use the original model with a smaller number of parameters and the recursive procedure stops when the number of parameters does not change anymore . in practice three or four stages",
    "are suffisant to finish the optimisation procedure .",
    "the strategy of model construction and parameter selection presented above is applied on a particular class of functions , a full 2d - order polynomial taking into account the squares and the cross products for @xmath84 :    @xmath85    @xmath86    @xmath87    the vector of parameters @xmath0 is composed of 27 values . the integration method is an euler schema with time step @xmath7 and the sampling time @xmath88 . for statistics we use @xmath73 and the probe size @xmath70 is set to @xmath74 .",
    "we use the same model ( @xmath89 ) to generate the noisy series .",
    "we note that the chaotic lorenz model @xcite and the roessler model @xcite belong to this class .",
    "we apply therefore the optimisation process to both models .",
    "chaotic data series from the lorenz model are built using equation  ( [ eq : model ] ) with the following seven non - zero @xmath0 components : @xmath90 . in order of obtaining",
    "the `` observed '' data we add some amount of noise ( equation  ( [ eq : noise ] ) ) and we pick data at sampling times .",
    "the set of optimal parameters will indeed depend on the noise level @xmath8 and on the window length @xmath23 and , in addition , the functional @xmath3 will become more and more non linear as long as @xmath23 growths @xcite . with this in mind",
    "we monitor the @xmath91 as a function of the window length @xmath23 starting from @xmath61 .",
    "the case @xmath61 is particular , because in a strict sense the numerical evaluation of the derivatives of the model ( [ eq : model ] ) , @xmath92 by the way , are almost the same using either @xmath93 or @xmath29 .",
    "it easily follows that for @xmath94 and @xmath61 the optimisation problem becomes a classical least square , then the functional function @xmath3 is quadratic and the solution unique and moreover we can compute the derivatives explicitly from @xmath3 without doing a variational approach .",
    "the optimal parameters of window length @xmath61 are used as initial guess for the window length @xmath64 and we continue until convergence with the window size @xmath23 .",
    "the key features of the process are showed in figure  [ fig:1 ] for three parameters @xmath95 where the standard deviation @xmath75 is represented by horizontal ticks .",
    "the level of noise is high , @xmath96 in equation  ( [ eq : noise ] ) , which is around of 14.53 @xmath16 .",
    "note that the average values converge from @xmath76 .",
    "pay attention to the @xmath97 axis scale , the parameter @xmath98 is not but fluctues around zero , its reliability is low and it is discarded at the first stage as is shown in figure  [ fig:2 ] .",
    "the figure  [ fig:2 ] presents then the process of discarding parameters : the @xmath97 axis say if given parameter is dropped off or not a that stage . in some detail we can see that parameter 1 ( @xmath99 ) is stilll present at the end of the process ( stage 3 ) , and conversely parameter 3 ( @xmath98 ) disappear in the first run ( stage 1 ) . as noted earlier parameters @xmath0 with small mean values and weak reliability",
    "are discarded . in the figure  [ fig:3 ]",
    "we show the equivalent of figure  [ fig:1 ] but at the end of the process ( stage 3 ) .",
    "we note that the convergence is done before @xmath76 and both the optimal parameters and dispersions are less affected by noise ( in particular compare optimal values for small window length @xmath23 in figure  [ fig:1 ] ) .",
    "the table  [ tab : probes ] shows optimal values and standard deviation for several noisy series ( @xmath100 ) .",
    "each column shows the average value @xmath91 and its standard deviation .",
    "columns two and three illustrate their behavior for the same noise level @xmath101 and for two probes number @xmath102 and @xmath73 , we can see that statistic does not change too much , we conserve for that reason @xmath73 for the subsequent computations . columns four to seven present the evolution of the numerical findings for noise levels @xmath103 which is in @xmath16 @xmath104 using the noiseless signal @xmath105 . at @xmath106 the parameter 26 ( @xmath107 does not disappear and is still present at the end of the optimization process .",
    "we note that the other parameter are not affected by its presence and the numerical reconstruction ( not shown ) using the ode system  ( equation ( [ eq : model ] ) ) does not differs from the original time series .    on the contrary",
    ", the next result ( @xmath108 ) shows that parameter 11 is seriously affected varying of one order of magnitude , from @xmath109 to @xmath110 .",
    "parameters coming from the linearized system as @xmath111 among others are really difficult to obtain .",
    "these terms give , in a first approximation exponential solutions behaving like @xmath112 .",
    "when noise is added we shadow the orbits and a lot of them are equivalent , we are in the conditions of the `` shadowing '' lemma ( @xcite and references therein ) but for an inverse problem : under these conditions there are not enough information to provide accurate estimates of the parameter values and then the optimization algorithm is not able to separate contributions coming from linear or nonlinear terms @xcite .",
    "even though the parameters are affected by noise the reconstruction using equation  ( [ eq : model ] ) shows the typical `` strange '' lorenz attractor and we can see in figure [ fig:4 ] that the time series for the variable @xmath113 is very similar to the original @xmath114 .",
    "nevertheless a close examination of the `` burst '' regions show us that the reconstructed one is less sharp .",
    "this characteristic is governed by the coefficient @xmath115 which , as mentioned above , is not well evaluated .     and ( bottom ) reconstructed one @xmath113 for @xmath116.,scaledwidth=80.0% ]     and ( bottom ) reconstructed one @xmath113 for @xmath116.,scaledwidth=80.0% ]    coming back to column 5 ( @xmath106 ) and examining the parameter 26 ( @xmath117 ) we remark that even if it is larger than the cutoff value `` eps '' its standard deviation is also quite large , then its reliability is poor . figure  [ fig:5 ] presents the logarithm of the reliability of all parameters , this result sugests that satisfying both criteria ( small mean values and weak ratio @xmath118 ) conjointly is too restrictive .",
    "we restart the process using either small mean values or weak ratio @xmath118 , the `` or '' condition for the rest of the paper .",
    ", column 6.,scaledwidth=80.0% ]    the table  [ tab:2 ] shows numerical results for @xmath119 and @xmath116 for the `` or '' condition , in both cases the optimal values are closed to the actual ones and their resist quite well to the noise , except for the parameter 11 which is becoming not reliable , @xmath120 .",
    ".optimal coefficients for the lorenz model using the `` or '' condition ( see text ) .",
    "noise values are @xmath119 and @xmath116 . [ cols=\"^,^,^\",options=\"header \" , ]     [ tab:3 ]    we use the model  ( [ eq : model ] ) to identify the unknown parameters and we look the behavior of the optimal parameters @xmath0 .",
    "table [ tab:3 ] shows the numerical findings , the first point is that the procedure is able to determine the important model s parameters even in presence of high amount of noise .",
    "second , that the standard deviations increase linearly with the noise level .",
    "third , that the optimal values resist quite well to noise excepted parameter 19 @xmath121 which has around @xmath122 of error .",
    "this parameter corresponds to the term @xmath123 ( see the model ( [ eq : model ] ) ) and is directly responsible for the aperiodic and rough burst in coordinate 3 on the rossler model .",
    "this burst dynamics is not easy of capturing because very sharp and short ( typical half time peak is around of 1 sec , which is @xmath124 ) .    numerical optimisations show that the optimal values are in excellent accord and the reconstructed orbits ( not shown ) are in agreement with the original ones .",
    "we have presented an optimization procedure able to retain the important parameters for a mathematical model given a noisy time series . we have applied the procedure to two chaotic series from the lorenz and the rossler models .",
    "we have demonstrated using an ode system that the procedure ( i ) is appropriate to reduce the complexity of the model , ( ii ) is powerful to make a parameters estimation , and ( iii ) is very robust against noise .",
    "we also observe that we can reduce the time integration , and in the limit @xmath125 we could compute the continuos parameters of a system .    in our numerical examples we know the actual number and value of the model parameters , this is an important help to decide what criterium we have to use .",
    "we have shown that using both criteria , small mean values and weak ratio @xmath118 , conjointly is too constrain .",
    "when at least one criterium is fulfilled we have proved that the optimization procedure is improved . in a case of unknown data series",
    "we have to ameliorate the procedure , an interesting way could be to compute the covariance matrix @xmath126 and analyse the eigenvalues to display linear combinations .",
    "we can yet improve the procedure working on the initial conditions for each window : the choice @xmath127 is may be not the optimal , numerical tests show that using @xmath128 ( which is in fact impossible on real data ) the optimal results are closest to the actual values",
    ". then an issue could be either to make a some kind of average process in the neighbor of the initial conditions or add some constraints as well as in the multiple shooting approach .",
    "we could define the initial conditions @xmath129 as parameters to optimize as in reference @xcite but the number of unknowns will become too important , adding by the way @xmath130 parameters ( @xmath22 being the window number ) to the optimization problem .",
    "however many questions are still open : the application to real data or the tractability for applying this procedure to systems including unobserved data series ( when data dimension is greater that model dimension )",
    ". the present procedure should be effective for different problems from classical or less classical parameter identification @xcite to control / synchronization applications @xcite .",
    "in particular an extension to high degree polynomial class is straightforward and could be effortless applied to neuronal or electrical circuits @xcite ."
  ],
  "abstract_text": [
    "<S> we present an optimization process to estimate parameters in systems of ordinary differential equations from chaotic time series . </S>",
    "<S> the optimization technique is based on a variational approach , and numerical studies on noisy time series demonstrate that it is very robust and appropriate to reduce the complexity of the model . </S>",
    "<S> the proposed process also allows to discard the parameters with scanty influence on the dynamic . </S>"
  ]
}