{
  "article_text": [
    "let @xmath7 be an @xmath8 matrix of rank ( at most ) @xmath1 and assume that @xmath9 uniformly random entries of @xmath7 are revealed .",
    "does this knowledge allow to approximately reconstruct @xmath7 ?    the answer is negative unless the matrix has some specific structure . in this paper",
    "we assume that @xmath7 is a _ random rank-@xmath1 matrix _ , i.e. @xmath10 where @xmath11 is a @xmath12 matrix with iid entries and @xmath13 an independent @xmath14 matrix with iid entries .",
    "the distributions of the entries of @xmath11 and @xmath13 are denoted , respectively as @xmath15 and @xmath16 .",
    "the metric we shall consider is the root mean square error ( rmse ) . if @xmath17 are the entries of @xmath7 , and @xmath18 is its estimate based on the observed entries , we have @xmath19 notice that this coincides , up to a factor , with the distance induced by the frobenius norm @xmath20 .    in the following",
    "we shall denote by @xmath21 the set of rows of @xmath7 and by @xmath22 its set of columns .",
    "the subset of revealed entries will be denoted by @xmath23 .",
    "low rank matrices have been proposed as statistical models to describe a number of complex data sources .",
    "for instance , the matrix of empirical correlations among stock prices in a market is approximately low rank if price fluctuations are driven by a few underlying mechanisms @xcite .",
    "a completely different application is provided by the matrix of square distances among @xmath5 sensors in @xmath24 dimension , which has rank @xmath25 @xcite .",
    "low rank matrices have been proposed as a model for collaborative filtering data . as a concrete example we shall focus here on the netflix challenge dataset @xcite .",
    "this dataset concerns a set @xmath26 of approximately @xmath27 customers and @xmath28 of @xmath29 movies . for about @xmath30 customer - movie pairs",
    "@xmath31 , the corresponding rating ( an integer between @xmath32 and @xmath33 ) is provided .",
    "the challenge consists in predicting the ratings of @xmath34 non - revealed customer - movie pairs within a root mean square error smaller than @xmath35 .",
    "one possible approach consists in considering the customer - movie matrix @xmath7 ( or a rescaled version of it ) and assuming that it has low rank to predict the requested entries .",
    "indeed , a simple coordinate descent algorithm that minimizes the energy function @xmath36 provides good predictions ( within the netflix competition , it was used by simonfunk ) .",
    "in general , the matrix completion problem is not convex , and the descent algorithm is not guaranteed to converge to the original matrix @xmath7 even if this is the unique rank @xmath1 matrix consistent with the observations .",
    "a possible alternative consists in relaxing the rank constraint , by looking instead for a matrix @xmath18 of minimal nuclear norm ( recall that the nuclear norm of @xmath18 is the sum of the absolute values of its singular values ) .",
    "the problem then becomes convex and indeed reducible to semidefinite programming .",
    "in @xcite it was shown that this relaxation indeed recovers the original low rank matrix @xmath7 , given that a sufficient number of random linear combinations of its entries are revealed .",
    "the case in which a random subset of the entries is revealed ( which is relevant for collaborative filtering ) was treated in @xcite .",
    "this paper proves that the convex relaxation is tight with high probability . ]",
    "if @xmath37 . in particular this implies two statements : @xmath38 for @xmath37 , @xmath9 random entries uniquely determine a random rank-@xmath1 matrix .",
    "@xmath39 this matrix is the unique minimum of a semidefinite program .      the results briefly reviewed above leave open several key issues :    1 .",
    "why is it necessary to observe @xmath40 entries to reconstruct a rank-@xmath1 matrix , that has @xmath41 degrees of freedom ?",
    "2 .   as the netflix challenge shows , it is not realistic nor necessary to reconstruct @xmath7 exactly .",
    "what is the trade - off between rmse distortion and number of observations ? 3 .   in general",
    ", semidefinite programming has @xmath42 complexity @xcite .",
    "this is affordable up to @xmath43 , but way beyond current capabilities when @xmath44 as in modern datasets .    in this paper",
    "we address the first two points and show that @xmath45 observations are sufficient to reconstruct a low rank matrix within any positive distortion .",
    "[ thm : mainub ] let @xmath46 be a random rank-@xmath1 matrix with @xmath5 rows and @xmath47 columns and assume the distributions of @xmath48 and @xmath49 to have support in @xmath50 $ ] .",
    "let @xmath51 be a random subset of @xmath9 entries in @xmath52 .",
    "then , with high probability , any rank-@xmath1 matrix @xmath18 such that @xmath53 for all @xmath31 , and with factors @xmath54 $ ] , also satisfies @xmath55 where @xmath56 .",
    "notice that the term @xmath57 in the above inequality is unavoidable .",
    "since we are looking for matrices that match the observed entries only within precision @xmath57 , we can not hope for a rmse smaller than @xmath57 . in the second term",
    ", the factor @xmath58 corresponds to the maximal distance between matrix entries in the present model , while the @xmath59-dependent factor tends to @xmath60 as @xmath61 .",
    "notice that @xmath62 is exactly the number of observations per degree of freedom .",
    "the proof of this statement is given in section [ sec : ub ] , which also provides a much more accurate upper bound .",
    "the latter is however not straightforward to evaluate . while it is clear that small rmse can not be achieved with less than @xmath41 observed matrix elements , section [ sec : lb ] proves a quantitative lower bound of this form .    in section [",
    "sec : numerical ] we address the question of efficient reconstruction and demonstrate that @xmath63 operations are sufficient to reconstruct random low rank matrices with rank @xmath64 , from @xmath45 entries .",
    "indeed such performances are achieved by a straightforward stochastic local search algorithm that we refer to as walkrank or by a coordinate descent algorithm .",
    "a formal analysis of these algorithms will be presented in a future publication .",
    "finally , in section [ sec : netflix ] we use these results to compare random low rank matrices and the netflix dataset .    before dwelling on the intricacies of the full problem ,",
    "the next section discusses a particularly simple but perhaps instructive case : rank @xmath65 .",
    "if @xmath7 has rank @xmath32 , most of the questions listed above have a simple answer with a suggestive graph - theoretical interpretation .",
    "assume that you know @xmath24 entries of the matrix @xmath7 that belong to the same @xmath66 minor .",
    "explicitly , for two row indices @xmath67 and two column indices @xmath68 , the entries @xmath69 , @xmath70 , @xmath71 are known . unless @xmath72 , the fourth entry of the same minor is then uniquely determined @xmath73 .",
    "the case @xmath72 can be treated separately but , for the sake of simplicity we shall assume that the distributions @xmath15 , @xmath16 do not have mass on @xmath60 .",
    "this observation suggests a simple matrix completion algorithm : recursively look for a @xmath66 minor with a unique unknown entry and complete it according to the rule @xmath73 .",
    "as anticipated above , this algorithm has a nice graph - theoretic interpretation .",
    "consider the bipartite graph @xmath74 with vertices corresponding to the row and columns of @xmath7 and edges for the observed entries .",
    "if a @xmath66 minor has a unique unknown entry , it means that the corresponding vertices @xmath75 , @xmath76 are connected by a length-@xmath24 path in @xmath77 .",
    "hence the algorithm recursively adds edges to @xmath77 connecting distance-@xmath24 vertices .",
    "( -105,-5)@xmath59 ( -225,80)@xmath78    after at most @xmath79 operations the process described halts on a graph that is a disjoint union of cliques , corresponding to the connected components in @xmath77 .",
    "each edge corresponds to a correctly predicted matrix entry .",
    "clearly , in the large @xmath5-limit only the components with @xmath41 matter ( as they have @xmath80 edges ) .",
    "it is a fundamental result in random graph theory that there is no such component for @xmath81 .",
    "for @xmath82 there is one such component involving approximately @xmath83 in @xmath28 and @xmath84 vertices in @xmath26 , where @xmath85 is the unique positive solution of @xmath86 this analysis implies the following result .",
    "[ propo : rank1 ] let @xmath46 be a random rank @xmath32 matrix , and denote by @xmath87 , @xmath88 the largest solution of eq .",
    "( [ eq : giantcomponent ] ) .",
    "then there exists an algorithm with @xmath79 complexity achieving , with high probability , rmse @xmath89 where @xmath90 .",
    "further , if the entries @xmath91 , @xmath92 have symmetric distribution , then no algorithm achieves smaller distortion .",
    "the mentioned distortion is achieved by the recursive completion algorithm , whereby matrix element corresponding to vertex pairs in distinct components are predicted to vanish .",
    "this is optimal if the matrix element distribution is symmetric .",
    "indeed the conditional matrix element distribution remains symmetric even given the observations .    for massive datasets",
    "even @xmath79 complexity is unaffordable .",
    "figure [ fig : rank1walk ] compares the minimal distortion guaranteed by proposition [ propo : rank1 ] with the performances of the walkrank algorithm described in section [ sec : numerical ] . here",
    "the factors @xmath91 , @xmath92 where chosen uniformly in @xmath93 .",
    "in this section we prove the upper bound on distortion stated in theorem [ thm : mainub ] .",
    "the proof proceeds in three steps .",
    "first we will consider the case in which the factor entries @xmath48 , @xmath49 are supported on a finite set , and prove a ( tighter ) upper bound via a counting argument .",
    "then we ll use a quantization argument to generalize this bound to the continuous case .",
    "finally , we simplify our bound to get the pleasing expression in theorem [ thm : mainub ] .",
    "unfortunately this simplification entails a worsening of the bound .",
    "we start by introducing a couple of new notations . given a row index @xmath94 , we let @xmath95 be the @xmath96-th row of @xmath11 .",
    "analogously , for @xmath97 , let @xmath98 be the @xmath99-th column of @xmath13 .",
    "we then have @xmath100 we also write @xmath101 and @xmath102 for the components of these vectors . these are assumed to be iid s with distributions @xmath15 ( for @xmath103 ) and @xmath16 ( for @xmath104 ) supported on a finite set @xmath105 with @xmath106 points .",
    "typical examples are @xmath107 or @xmath108 ) . our basic counting estimate is stated below .",
    "[ propo : discrete ] let @xmath109 and @xmath7 be a random rank-@xmath1 matrix with factors supported in @xmath110 .",
    "then , with high probability any rank-@xmath1 matrix @xmath18 with factors supported in @xmath110 that satisfies @xmath111 for all @xmath31 also satisfies @xmath112 , where @xmath113 here the @xmath114 over @xmath115 ( over @xmath116 ) is taken over the space of distributions @xmath117 ( respectively @xmath118 ) over @xmath119 such that @xmath120 ( respectively @xmath121 ) .",
    "the functionals appearing in eq .",
    "( [ eq : upperbound ] ) are defined by @xmath122 and @xmath123+\\\\ & + \\eps\\ , \\e_{p_0,q_0}\\log \\prob_{p , q}\\big\\{|\\vu\\cdot\\vv-\\vu^0\\cdot\\vv^0| \\le \\delta\\;\\big|\\;\\vu^0,\\vv^0\\big\\}\\ , , \\nonumber \\ ] ]    define @xmath124 ( @xmath77 is the bipartite graph with edge set @xmath51 ) as the number of matrices @xmath18 of the form ( [ eq : matrixform ] ) such that :    1 .",
    "@xmath53 for all @xmath31 ; 2 .",
    "@xmath125 .",
    "this can be written as @xmath126 where @xmath127 is the set of vectors that satisfy condition @xmath128 above .",
    "we further define the set of _ typical instances _ @xmath129 , @xmath130 through the following conditions :    1 .",
    "let @xmath131 be the type of factor @xmath11 , namely @xmath132 is the number of row indices @xmath94 such that @xmath133 .",
    "then for @xmath134 , we have @xmath135 .",
    "analogously , for the type of factor @xmath13 we require @xmath136 .",
    "3 .   finally , let @xmath137 be the edge type , i.e. @xmath138 is the number of edges @xmath31 such that @xmath133 and @xmath139 .",
    "we then require @xmath140 ( where @xmath141 is the product distribution on @xmath103 , @xmath104 ) .    by standard arguments @xcite we have @xmath142 for any positive @xmath143 as @xmath144 .",
    "we then define @xmath145 according to lemma [ numberofsolutions ] , the expectation of @xmath146 vanishes as @xmath5 tends to infinity for @xmath147 . since @xmath142 and using markov inequality , this implies that @xmath148 . in conclusion , any matrix @xmath18 that satisfies @xmath111 for all @xmath31 results in a distance metric smaller than @xmath149 with high probability , as @xmath5 tends to infinity .",
    "[ numberofsolutions ] for any @xmath147 there exists @xmath150 such that @xmath151 .",
    "@xmath124 is a random variable where the randomness comes from the matrix elements @xmath69 and the choice of the sampling set @xmath51 . since @xmath51 is uniformly random , we can take any realization of @xmath152 from the typical set according to iid @xmath15 and iid @xmath16 . given one such realization of @xmath153 and @xmath154 , go through all the estimations @xmath155 , where @xmath156 and @xmath157 . now group the set of assignments @xmath158 and @xmath159 that have the same empirical distribution , and let @xmath160 and @xmath161 denote the joint distribution .",
    "then , the number of different assignments with same empirical distribution @xmath162 is @xmath163 . for each distribution pair @xmath162 that satisfy condition ( 2 ) above",
    ", we fix the factors @xmath158 and @xmath159 and compute the probability that they satisfies condition ( 1 ) . denoting by @xmath164 the expectation restricted to @xmath165 , we have @xmath166 to compute the expectation in the last inequality , we look at a typical realization of @xmath51 and partition it into subsets @xmath167 for @xmath168 , defined as follows .",
    "@xmath169 is in @xmath170 if @xmath171 and @xmath172 . by definition @xmath173 .",
    "further @xmath170 is uniformly random given its size . within the typical set @xmath174 ,",
    "@xmath175 is close to @xmath176 .",
    "we thus get @xmath177 finally , we get , @xmath178 where @xmath179 as @xmath180 . for @xmath162 that satisfies @xmath181 , we know that @xmath182 by definition . hence , for @xmath143 small enough , @xmath183 is a sufficient condition for @xmath151 .      above tighter",
    "upper bound can be generalized to matrices in theorem [ thm : mainub ] via quantization argument . in this section ,",
    "we re interested in recovering a continuous real valued matrix @xmath7 from samples of its entries .",
    "first , we estimate it using factors @xmath184 , @xmath185 supported in the continuous alphabet .",
    "then , the distortion is bounded using the upper bound from section [ subsec : discrete ] via quantization .",
    "let @xmath109 and @xmath7 be a random rank-@xmath1 matrix with factors supported in continuous bounded alphabet @xmath186 .",
    "let @xmath187 be discrete quantized alphabet of @xmath186 , with maximum quantization error less than @xmath188 .",
    "@xmath18 is the rank-@xmath1 estimation with factors supported in @xmath186 .",
    "then , with high probability , any matrix @xmath18 that satisfies @xmath111 for all @xmath31 also satisfies @xmath189 , where @xmath149 is defined as in eq .",
    "( [ eq : upperbound ] ) and @xmath190 is the quantization error which only depends on @xmath2 .    let @xmath191 be the quantized version of the original matrix @xmath7 , which is defined as follows . define @xmath192 and @xmath193 to be the quantized version of @xmath194 and @xmath195 respectively , where @xmath194 is the @xmath96-th row of @xmath11 and @xmath195 is the @xmath99-th column @xmath13 .",
    "then , @xmath191 is defined as , @xmath196 note that @xmath197 satisfies @xmath198 .",
    "analogously , define @xmath199 to be the quantized version of the estimated matrix @xmath18 .",
    "then , the @xmath191 and @xmath199 satisfy @xmath200 for all @xmath31 .",
    "let @xmath201 be the upper bound in proposition [ propo : discrete ] .",
    "then , the distortion is bounded with high probability by @xmath202 note that twice the quantization error is added to @xmath57 since now we only have @xmath200 for all @xmath31 .",
    "the ( tighter ) upper bound in proposition [ propo : discrete ] is not easily computed . to get a bound that can be analyzed ,",
    "we relax the constraint @xmath203 and get a relaxed or simplified upper bound on @xmath149 .",
    "furthermore , this simplified upper bound is used to prove theorem [ thm : mainub ] .",
    "[ propo : simplebound ] for all @xmath204 , @xmath205 and @xmath206 , we have @xmath207 where @xmath149 is defined as in proposition [ propo : discrete ] , @xmath208 , @xmath209 , and @xmath210 .",
    "define the upper bound @xmath211 as @xmath212 where @xmath117 , @xmath117 and @xmath213 are defined in eq .",
    "( [ eq : upperbound ] ) .",
    "the only difference is the relaxed constraint function @xmath214 , defined as @xmath215 by jensen s and markov inequality , @xmath216 is larger than @xmath217 .",
    "this implies that the supremum in the simplified upper bound is taken over a larger set of distributions than the tighter upper bound , hence we have @xmath218 . and after some computation ,",
    "it s easy to show that @xmath219\\right)\\right\\}^{1/2}$ ]  , which concludes the proof .",
    "this simplified upper bound can be generalized , in the same manner , to the continuous support case .",
    "the following example illustrates this generalization and introduces bounds necessary in the proof of theorem [ thm : mainub ] .    for the original matrix @xmath46 , assume the distributions of @xmath48 and @xmath49 to have support in @xmath220 .",
    "also , the factors of the rank-@xmath1 solution @xmath18 are supported on the same discrete set .",
    "then , the simplified upper bound is given by @xmath221 where @xmath222 and @xmath56 .",
    "note that @xmath223 = @xmath57 , which means that we can not get rmse smaller than @xmath57 .",
    "the maximum quantization error associated with @xmath224 is @xmath225 , which happens when all the entries of @xmath226 and @xmath227 are @xmath228 and quantized to @xmath32 .",
    "for simplicity , @xmath229 is used .",
    "combined with eq .",
    "( [ continuousupperbound ] ) , we have a simple analytical upper bound on the distortion when the original matrix and the estimation have continuous support @xmath50 $ ] .    from the example",
    "above , we can compute the simplified upper bound directly to bound the distortion .",
    "@xmath230 remember n is defined as the alphabet size @xmath231 , where the discrete alphabet @xmath232 is used .",
    "fixing @xmath233 , we can minimize the right hand side of the last inequality with respect to the alphabet size @xmath234 .",
    "since the exact minimizer can not be represented in a closed form , we use instead an approximate minimizer @xmath235 , which results in @xmath236 where the last inequality in ( [ proof_theorem ] ) is true for @xmath237 .",
    "this is practical since we are typically interested in the region where @xmath238 .",
    "when the number of observed elements is smaller than @xmath41 , high distortion is inevitable . in this section",
    "we derive a quantitative lower bound which supports this observation .",
    "[ propo : lowerbound ] let @xmath46 be a random rank-@xmath1 matrix with @xmath5 rows and @xmath47 columns and assume the distributions of @xmath48 and @xmath49 to have support in @xmath50 $ ] , and @xmath51 a random subset of @xmath9 row - column pairs .",
    "then , with high probability , any rank-@xmath1 matrix @xmath18 such that @xmath239 for all @xmath31 , also satisfies @xmath240 where @xmath241 is a strictly positive constant that only depends on the rank @xmath1 and the initial distributions @xmath15 and @xmath16 .",
    "think of the following algorithm which has clearly better performance than any other that satisfies the assumptions .",
    "consider the bipartite graph @xmath74 with vertices corresponding to the row and columns of @xmath7 and edges for the observed entries .",
    "for every pair of row and column indices @xmath242 , @xmath243 and @xmath244 , that is not connected by an edge , we do the following .",
    "if degree of @xmath96 ( @xmath99 ) is less than @xmath1 , we assume that all the neighbors of node @xmath96 ( @xmath99 ) are known and make mmse estimation of @xmath226 ( @xmath227 ) .",
    "if degree of @xmath96 ( @xmath99 ) is greater than @xmath245 , we assign the correct value of @xmath226 ( @xmath227 ) . with high probability",
    "the resulting rmse is greater than @xmath246 as defined below .",
    "@xmath247 where @xmath248  , @xmath249 and @xmath250 .",
    "here , @xmath251 and @xmath252 represent the mmse estimate of @xmath226 and @xmath227 respectively , assuming that @xmath245 neighbors and corresponding edges are known .    without loss of generality ,",
    "assume @xmath253 .",
    "then , we can simplify above bound to get , eq .",
    "( [ lowerbound1 ] )",
    "in the previous sections we proved that @xmath45 random entries determine a random low rank matrix within an arbitrarily small rmse .",
    "how hard is it to find such a matrix ? in this section we present a numerical investigation using a low complexity stochastic local search algorithm that we call walkrank .",
    "walkrank is inspired by successful local search algorithms for constraint satisfaction problem , such as walksat @xcite .",
    "it is particularly suited to low - rank matrices whose factors @xmath48 , @xmath49 take values in a finite set @xmath110 .",
    "the algorithm tries to find assignments of the vectors @xmath254 , and @xmath255 that minimize the cost function @xmath256 which counts the number of observations @xmath257 that are not described by the current assignment .",
    "the algorithm initializes the vectors @xmath258 , @xmath259 to random iid values and then alternates between two type of moves .",
    "the first are greedy moves , described here in the case of @xmath11 factors .",
    "ll + 1 : & sample a column index @xmath260 uniformly ; + 2 : & find @xmath261 that minimizes @xmath262 over @xmath194 ; + 3 : & set @xmath263 +    greedy moves for @xmath13 factors are defined analogously .",
    "the second type of move potentially increases the cost function .",
    "ll + 1 : & sample @xmath31 s.t .",
    "@xmath264 ; + 2 : & find @xmath265 such that @xmath266 + 3 : & set @xmath263 , and @xmath267 +    walkrank recursively executes one of these moves , choosing a walk move with probability @xmath268 , and a greedy one with probability @xmath269 .",
    "the parameter @xmath268 can be optimized over , and we found @xmath270 to be a reasonable choice .    in figures [ fig : rank2walk ] to [ fig",
    ": rank4walk ] we present the distortion achieved by the walkrank algorithm , averaged over @xmath271 instances .",
    "we used factors with entries @xmath48 , @xmath49 uniformly distributed in @xmath93 .",
    "it is clear that the resulting distortion is essentially independent of @xmath5 over two orders of magnitude and decreases rapidly with @xmath59 .",
    "we compare these numerical results with an analytical lower bound on the distortion achieved by a maximum likelihood algorithm .",
    "the latter fills each unknown position in @xmath7 with its most likely value .",
    "while there exists no practical implementation of the maximum likelihood rule , we can provide a sharp lower bound on its performances using techniques explained in @xcite .",
    "it appears that , for low values of the rank , walkrank achieves the same distortion as maximum likelihood , provided it is given one or two more entries per column / row .",
    "the complexity of one walkrank step is independent of the matrix size ( but grows with the rank ) .",
    "the results in figures [ fig : rank2walk ] to [ fig : rank4walk ] were obtained with a number of steps slightly superlinear in @xmath5 . in fig .",
    "[ fig : costtime ] we show the evolution of the cost function for averaged over @xmath271 instances for @xmath272 to @xmath273 , @xmath274 and @xmath275 .",
    "the number of steps per variable required to reach the asymptotic value increases mildly with @xmath5 .",
    "a reasonable conjecture is that the number of steps scales like @xmath276poly@xmath6 .",
    "as shown in the last section , local search algorithms efficiently fit low rank matrices of very large dimensions , using few observations .",
    "they therefore provide an efficient tool for checking whether a dataset is well described by the random low rank model .    in figures [ fig : netflix3 ] and [ fig : netflix5 ]",
    "we compare the evolution of fit and prediction error for three matrices with @xmath277 :    1 .",
    "a submatrix of the netflix dataset given by the first @xmath278 movies and customers .",
    "2 .   a matrix with the same subset @xmath51 of revealed entries , each of them chosen uniformly at random in @xmath279 $ ] .",
    "3 .   a random rank-@xmath24 matrix ( for fig .",
    "[ fig : netflix3 ] ) or rank-@xmath33 matrix ( for fig .  [ fig : netflix5 ] ) , with set of revealed entries as above .",
    "the fit error is defined by restricting the average in eq .",
    "( [ eq : rmsedef ] ) to @xmath31 .",
    "the prediction error is instead obtained by averaging over @xmath280 . in the case of the netflix matrix the latter was estimated by hiding @xmath281 entries from the dataset , and averaging over those .",
    "we used a coordinate descent algorithm in the factors @xmath258 , @xmath259 , with regularized cost function given by eq .",
    "( [ eq : regularized ] ) . in agreement with the results of previous sections , random low rank matrices",
    "are efficiently fitted with small fitting _ and _ prediction error .",
    "the difference with iid entries is striking .",
    "the fit error decreases only slowly over time , while the prediction error actually increases .",
    "as expected , revealed entries do not provide any information on the hidden ones .",
    "netflix data lie somewhat in between : both fit and prediction error decrease over time , albeit not as sharply as for genuine low rank matrices .",
    "andrea montanari is partially supported by a terman fellowship and by the nsf career grant 0743978 ."
  ],
  "abstract_text": [
    "<S> how many random entries of an @xmath0 , rank @xmath1 matrix are necessary to reconstruct the matrix within an accuracy @xmath2 ? </S>",
    "<S> we address this question in the case of a random matrix with bounded rank , whereby the observed entries are chosen uniformly at random . </S>",
    "<S> we prove that , for any @xmath3 , @xmath4 observations are sufficient .    </S>",
    "<S> finally we discuss the question of reconstructing the matrix _ efficiently _ , and demonstrate through extensive simulations that this task can be accomplished in @xmath5poly@xmath6 operations , for small rank . </S>"
  ]
}