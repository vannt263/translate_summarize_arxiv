{
  "article_text": [
    "in biology , medicine , and many other domains of statistical application , researchers are increasingly faced with problems involving numerous variables and a natural problem is that of studying their relationships .",
    "standard examples are the construction of social or communication networks and systems biology",
    ". when the underlying variables are binary ( which is the focus of this paper ) , a classical way for studying their relationships is to use _ poisson log - linear models _ for multiway _ contingency tables _",
    "@xcite@xcite . how to perform selection in log - linear models , or equivalently in binary graphical models , depends upon the number of variables @xmath1",
    "indeed , the total number of cell entries for a @xmath1-way contingency table is @xmath2 , and the total number of free parameters in the associated saturated model is @xmath3 .",
    "when @xmath1 is low , a standard approach for model selection is greedy stepwise forward - selection or backward - deletion : in each step , selection or deletion is based on hypothesis testing at some level @xmath4 .",
    "however , the computational complexity even for modestly dimensioned contingency tables plus the multiple hypothesis testing issues related to such a procedure has made it unpopular in this context .",
    "consequently , @xcite recently performed selection in log - linear models by using an @xmath0-penalized version of the log - likelihood , extending the lasso ideas @xcite originally designed for selection in linear models .",
    "however , computing the ( penalized ) log - likelihood for log - linear models generally requires the enumeration of each of the @xmath2 profiles , which is not plausible for large @xmath1 ( e.g. , larger than about 30 ) . for such moderate - to - large values of @xmath1 ,",
    "alternative methods are required .",
    "roughly speaking , two approaches have been proposed in the literature .",
    "first , exact inference can be performed in the case of highly sparse models .",
    "for instance , exact computation via the junction tree algorithm is manageable for highly sparse graphs but becomes unwieldy for dense graphs @xcite .",
    "the second approach is to use approximate inference .",
    "notably , much attention has recently been paid to methods relying on proxies for the exact likelihood . @xcite and @xcite proposed two distinct algorithms to maximize an @xmath0-penalized version of the so - called _ pseudo - likelihood _ @xcite .",
    "these methods are closely related to the one formerly proposed by @xcite who used @xmath0-penalized logistic regressions on each single node to construct the whole graphical model . besides these three methods",
    ", @xcite used a convex relaxation technique to derive a gaussian approximate log - likelihood as well as its sparse maximum solution .",
    "interestingly , @xcite showed through an extensive simulation study that approximate solutions ( either solutions maximizing the pseudo - likelihood or those derived from the method proposed by @xcite ) are much faster and only slightly less accurate than exact methods .",
    "however , no empirical evaluation of the gaussian approximate solution proposed by @xcite has ever been conducted and filling this gap is the primary objective of this paper .",
    "we thereby propose to conduct such an evaluation by comparing this method with the two other approximate methods of @xcite and @xcite on simulated data .    here",
    "is a brief outline of the paper . in section [ section_mm ]",
    "we first summarize the principles of the aforementioned approximate methods .",
    "we then present results from an extensive empirical comparison study . a slight modification of the method proposed by @xcite is especially shown to achieve very good accuracy and to be extremely fast .",
    "finally , we present an application from a real example where we looked for associations among causes of death in the database of french death certificates of the year 2005 ( section [ sec_results ] ) .",
    "let @xmath5 be a @xmath1-dimensional vector of binary random variables . given a random sample @xmath6 of i.i.d .",
    "replicae of @xmath7 , we wish to study the associations between the coordinates of @xmath7 . one way to do so is to construct the binary graphical model for the random vector @xmath7 , that is an undirected graph @xmath8 , where @xmath9 contains @xmath1 vertices corresponding to the @xmath1 coordinates and the edges @xmath10 describe the conditional independence relationships among @xmath11 .",
    "the edge @xmath12 between @xmath13 and @xmath14 is absent if and only if @xmath13 and @xmath14 are independent conditional on the other variables . for binary graphical models , it is common to focus only on the family of probability distributions given by the quadratic exponential binary model @xcite , also known as the ising model .",
    "namely , for all @xmath15 , we assume that the probability of @xmath16 is given by @xmath17 where the so - called @xmath18 _ partition function _",
    "@xmath19 is defined as follows @xmath20 note that @xmath19 is strictly convex and ensures that @xmath21 ( note also that the strict convexity of this function ensures the identifiability of the parameter matrix @xmath22 ) . from ( [ ising ] ) , we have @xmath23 therefore , the parameters @xmath24 are the conditional log - odds ratios .",
    "the conditional independence between @xmath13 and @xmath14 is then equivalent to @xmath25 , that is , the edge @xmath12 is absent if and only if @xmath25 .",
    "consequently , selection in binary graphical models is equivalent to identifying the @xmath26 pairs for which @xmath25 .    from ( [ ising ] )",
    ", using the fact that @xmath27 is binary , and denoting by @xmath28 the matrix representing the whole dataset , the @xmath0-penalized log - likelihood writes @xmath29 where @xmath22 is a symmetric matrix with @xmath30 for @xmath31 . however , because of the complexity of the log - partition function , methods based on approximate inference are needed in most cases and we recall the principle of three of them in the following paragraphs .",
    "let us begin by recalling that the approximation established by @xcite is only valid for the first - order - interaction log - linear model described above .",
    "on the other hand , @xcite , @xcite and @xcite also only consider this simple model but higher - order interaction models can be ( at least theoretically ) handled with these methods , at a cost of a dramatically increased computational time .      from ( [ ising ] ) , it is easy to see that , for all @xmath31 , setting @xmath32 @xmath33 , we have @xmath34 @xcite then extensively study a method ( which will be referred hereafter as ` seplogit ` following the terminology adopted in @xcite ) in which @xmath0-penalized logistic regression is used to estimate the neighborhood of each of the @xmath1 nodes in the graph separately .",
    "@xcite gives rigorous consistency results in a high - dimensional setting , where the number of nodes is allowed to grow as a function of the number of samples .",
    "the authors give sufficient conditions under which the method will consistently estimate the neighborhood for every node in the graph simultaneously . in a sense ,",
    "the paper can be seen as a discrete version of @xcite .    for a finite number of samples ,",
    "the @xmath1 logistic regression problems are solved separately and , since the results may be asymmetric , they can be combined in one of two ways to draw a graph .",
    "one possibility is to draw an edge between two nodes in the graph only if each node is estimated to belong to the neighborhood of the other ( method ` seplogit and ` ) .",
    "alternatively , we can decide to draw an edge between two nodes so long as at least one of them is estimated to belong to the neighborhood of the other ( method ` seplogit or ` ) .    in our empirical comparison study",
    ", this method will be implemented using the coordinate descent procedure developed by @xcite ( and implemented in the ` glmnet ` r package ) .",
    "one of the shortcomings of the method proposed by @xcite is the aforementioned asymmetry . to overcome this limitation , @xcite and @xcite recently proposed to use the pseudo-(log-)likelihood , first suggested by @xcite .",
    "the pseudo - likelihood is formally defined as @xmath35 accordingly , the approach based on the maximization of the @xmath0-penalized pseudo - likelihood solves all @xmath1 logistic regression problems simultaneously , while enforcing symmetry .",
    "apart from symmetry enforcement , this methods differs from the one studied in @xcite in that the @xmath0-norm penalty is applied to the entire network , while in ` seplogit ` it is applied to each neighborhood .    for future use , and still denoting by @xmath28 the matrix representing the whole dataset , observe that the pseudo - likelihood writes @xmath36\\theta[,k])\\big\\},\\ ] ] where @xmath37 is the same as @xmath38 with @xmath39th column set to 1 and @xmath40 ( i.e. , @xmath41 is the spin version of @xmath42 ) . here and elsewhere , @xmath43 $ ] ( resp .",
    "$ ] ) denotes the @xmath45-th row ( resp . @xmath39-th",
    "column ) of a matrix @xmath46 .",
    "@xcite first develop and implement an algorithm for maximizing the @xmath0-penalized pseudo - likelihood function , using a local quadratic approximation to the pseudo - likelihood .",
    "they then use this algorithm as a building block for a new algorithm that maximizes the true log - likelihood .",
    "however , as we already said , they observed that the approximate pseudo - likelihood is much faster than the exact procedure , and only slightly less accurate .",
    "therefore , to save computational time , we only considered the approximate pseudo - likelihood in this paper . in the forthcoming empirical comparison study",
    ", this method will be implemented using the ` bmn ` r package and will be referred to as ` bmnpseudo ` .    interestingly , and as pointed out by @xcite , the derivative of the pseudo - likelihood on the off - diagonal is roughly twice as large as the derivative of the exact likelihood . moreover , in the case @xmath47 , it is easy to see that the deviance of the model with no association ( i.e. minus twice the difference between the log - likelihood of this model and the log - likelihood of the saturated model ) when computing with the pseudo - likelihood is twice as large as the one computed with the exact likelihood ( while , obviously , the pseudo - likelihood coincides with the exact likelihood for the model with no interaction ) .",
    "the generalization of this striking result for higher @xmath1 is not straightforward , but our empirical examples suggest it may hold at least approximately ( see section [ sec_comp_approx_deviances ] ) .",
    "therefore , we will consider methods relying on both @xmath48 ( method ` bmnpseudo ` ) and @xmath49 ( method ` bmnpseudo 1/2 ` ) in the sequel .",
    "for the sake of completeness , we shall add that @xcite develop a gradient - descent algorithm to maximize the @xmath0-penalized pseudo - likelihood .",
    "they further propose an extension to account for spatial correlation among the variables ( which was relevant in their example dealing with genomic data ) .",
    "however , the corresponding ` logitnet ` r package was not available at the time we wrote this paper , so we were not able to include it in our empirical comparison study .",
    "the basic idea of the method described by @xcite is to replace the log - partition function in the ising model with an upper bound suggested by @xcite .",
    "the resulting approximation can then , with some manipulation , be put in a form that can be solved efficiently using block coordinate descent . in order to add some specific details",
    ", we shall define some notation .",
    "denote by @xmath50 the spin version of @xmath51 , and let @xmath52 denote the sample mean of variable @xmath53 , for @xmath31 .",
    "now , define the empirical covariance matrix @xmath54 as @xmath55 where @xmath56 is the vector of sample means @xmath52 . making use of a _ convex relaxation _ and a useful upper bound on the @xmath18-partition function obtained by @xcite , @xcite established that an approximate sparse maximum likelihood solution for a given @xmath57 has the following form @xmath58 where the matrix @xmath59 is the solution of the following optimization problem @xmath60 more precisely , @xcite proposed a block - coordinate descent algorithm to solve a dual formulation of ( [ optim_binary_0 ] ) , which can be written as @xmath61 in the gaussian case , @xcite showed that the @xmath0-penalized covariance selection problem could be written @xmath62 where @xmath63 is the empirical covariance matrix attached to a given sample of gaussian vectors . an algorithm for handling binary graphical models can be derived by comparing ( [ optim_binary ] ) and ( [ dual_1 ] ) .",
    "the original @xmath64 data has first to be transformed into @xmath65 data .",
    "then , adding the constant @xmath66 to the diagonal elements of the resulting empirical covariance matrix , the algorithms developed in the gaussian case ( in particular the ` glasso ` r package developed by @xcite ) can be reused .",
    "a common question when working with gaussian variables is whether to standardize them , or equivalently , whether to use the covariance or the correlation matrix .",
    "moreover , in the binary case , the correlation coefficient between two variables ( also known as the @xmath67-coefficient ) is closely related to the @xmath68 statistic used to test for ( marginal ) independence .",
    "putting these two observations together , we decided to evaluate a simple modification of the method proposed by @xcite where the quantity @xmath54+diag(1/3 ) is replaced by the correlation matrix .",
    "lastly , we also decided to evaluate the modification in which @xmath54+diag(1/3 ) is simply replaced by @xmath54 .    to recap",
    ", we will consider the three following optimization problems @xmath69 where @xmath70 , @xmath71 and @xmath72 .",
    "for any @xmath57 , and every @xmath73 an estimation of @xmath24 is then given by @xmath74 .",
    "in our empirical comparison study , the three methods will be implemented using the ` glasso ` r package @xcite and will be referred to as ` gausscov 1/3 ` , ` gausscov ` and ` gausscor ` for the choices @xmath75 , @xmath76 and @xmath77 respectively ( we may as well use the generic expression ` gaussapprox ` when dealing with either methods ) .",
    "two procedures for selecting tuning parameters are generally considered , namely cross - validation ( cv ) and bayesian information criterion ( bic ) , the latter being computationally more efficient as suggested by @xcite for instance . in the case of gaussian graphical models , @xcite",
    "further demonstrate the advantageous performance of bic for sparsity parameter selection through simulation studies . in this paper",
    ", we therefore decided to only consider bic .",
    "when trying to select the optimal sparsity parameter @xmath57 using either cv or bic , however , one has to pay attention to the following fact . since , for each @xmath78 , estimates of the parameters of interest are shrunk , using them for choosing @xmath57 from cv or bic often results in severe over - fitting @xcite .",
    "therefore , un - shrunk estimates have to be derived before computing the bic .",
    "taking the example of methods ` gaussapprox ` , for any @xmath57 , we have to compute the un - shrunk matrix @xmath79 here , @xmath80 ( @xmath81 being the set of positive definite matrices of order @xmath1 , and @xmath82 being as in ( [ sol_opt_binary ] ) ) . to solve the optimization problem ( [ pr_unshrunk_gauss ] ) ,",
    "one approach is to reuse the algorithm used to solve ( [ gauss_approx_sol ] ) after replacing the scalar parameter @xmath57 by the penalty matrix @xmath83 such that @xmath84 if @xmath85 , and @xmath86 otherwise , where @xmath87 is the shrunk estimation of the coefficient @xmath24 obtained with the value @xmath57 and is used as an initial value for the optimization .",
    "alternative approaches might be considered , such as the algorithm developed by @xcite for instance .",
    "defining @xmath88 the bic procedure now simply corresponds to selecting the sparsity parameter @xmath89 such that @xmath90 where @xmath91 is the degree of freedom of the model selected with the sparsity parameter @xmath57 @xcite .      in the binary case ,",
    "a standard measure of the strength of association between two variables is the ( conditional ) odds - ratio , which is related to coefficient @xmath24 ( see ( [ or_01 ] ) ) .",
    "therefore , consistent estimates of the parameters @xmath24 would yield consistent estimates of the conditional odds - ratios . here again un - shrunk estimates are preferable , and the methods described in the previous section have to be used .",
    "in this section , we compare the model selection performances as well as the computational time for the methods described in the previous section .",
    "results are presented for @xmath92 and @xmath93 .",
    "the choice @xmath92 has been made for several reasons .",
    "first , for such low values of @xmath1 the true log - likelihood can be quickly computed , and we can then compare it with the approximate log - likelihoods ( see section [ sec_comp_approx_deviances ] below ) .",
    "second , approximate methods are still faster to compute when @xmath1 is small , and conclusions drawn in the case of low @xmath1 are likely to hold for high @xmath1 as well ( as will be confirmed from our results ) .      for each method , every value of @xmath57 corresponds to a sparsity structure for the matrix @xmath22 that can be compared with the true sparsity structure .",
    "namely , for all @xmath57 and for each method , we can compute the rate of true positives ( correctly identified associations ) , the rate of false positives ( incorrectly identified associations ) as well as the overall accuracy .",
    "precision and recall ( the latter being identical to the true positive rate ) can also be computed , as well as their harmonic mean , often referred to as the f1-score .    in a first evaluation study",
    ", we present for each method the performances achieved by the `` oracle '' model , that is the model constructed with optimal sparsity parameter regarding accuracy .",
    "such an evaluation was not conducted for ` seplogit ` because under this method the @xmath0 penalty is applied to each neighborhood and the `` oracle '' model would invariably coincide with the true model . the alternative would be to force the algorithm to choose the same sparsity parameter value for every regression model .",
    "however , using this alternative approach , we sometimes obtained `` oracle '' models that achieved performances worse than the models selected by the bic procedure .",
    "therefore , we do not recommend to force the algorithm to choose the same sparsity parameter value for every regression model .    in a second evaluation study , we present for each method the performances achieved by the model selected according the bic procedure described above . for methods ` gaussapprox `",
    ", un - shrunk estimates were derived along the lines described in section [ sec_sel_sparsity_param ] .",
    "a similar approach was used for method ` bmnpseudo ` .",
    "the ` bmn ` r package also allows the use of a matrix of penalty coefficients . for ` seplogit ` , we had to slightly adapt this approach because the ` glmnet ` package does not allow for building models with only un - penalized coefficients .",
    "so , whenever needed , a standard logistic regression model was used to get un - shrunk estimates .",
    "this may make the method a little slower , but not much since for each variable , this situation can only arise for the smallest tested @xmath57 value , and only if this smallest tested @xmath57 value corresponds to the saturated model .",
    "in addition , the computational time is reported .",
    "more precisely , we used a grid @xmath94 $ ] of 50 equally - spaced values ( on a log - scale ) for the parameter @xmath57 and we report the time needed to compute the 50 corresponding models for each method ( @xmath95 is the data derived smallest value for which all coefficients are zero ) .",
    "each method was run on a windows vista machine with intel core 2duo 2.26ghz with 4 gb ram in the case @xmath92 and on a mac pro machine with intel xeon 2@xmath962.26ghz quad core with 6 gb ram in the case @xmath93 ( the mac pro machine was approximately 3.5 times as fast as the windows machine ) .        in model ( [ ising ] ) ,",
    "given that @xmath97 individuals are observed , the distribution of the corresponding cell counts @xmath98 is multinomial with probability @xmath99 .",
    "accordingly , given a symmetric matrix @xmath22 , data were drawn from the multinomial distribution with probability vector @xmath100 .",
    "four matrices @xmath101 , @xmath102 , @xmath103 and @xmath104 were considered , leading to four different simulation designs .",
    "for @xmath101 , `` primary '' coefficients @xmath24 were simulated independently using a normal distribution with mean zero and variance @xmath105 for some @xmath106 .",
    "subsequently , only coefficients @xmath24 with an absolute value greater than 0.06 ( corresponding to a conditional odds - ratio of @xmath107 , since for @xmath65 variables , the conditional odds - ratio is @xmath108 ) were retained , all others being set to 0 .",
    "the function @xmath109 was then computed according to equation ( [ logpart ] ) .",
    "selecting @xmath110 led to a true model with 10 associations ( among the @xmath111 potential associations ) .",
    "matrices @xmath102 and @xmath103 were constructed so that they share the same sparsity pattern as @xmath101 , i.e. , @xmath112 but they have different non - zero coefficients . in either cases , we selected @xmath113 . for matrix @xmath102 ,",
    "the non - zero @xmath24 coefficients were set to @xmath114 , while they were set to @xmath115 for matrix @xmath103 .    for matrix @xmath104",
    ", we proceeded as for matrix @xmath101 but we selected @xmath116 and only the @xmath24 coefficients with an absolute value greater than 0.2 were retained ( the others being set to 0 ) .",
    "moreover , we selected @xmath117 .",
    "this led to a true model with 19 associations .    a graphical representation of matrices @xmath101 , @xmath102 , @xmath103 and @xmath104 as well as the corresponding marginal probabilities @xmath118 , for @xmath119 , estimated on a sample of size @xmath120 are presented on figure [ fig_descr_data_p10 ] .    , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121",
    "are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels .",
    ", title=\"fig:\",scaledwidth=35.0% ] , for @xmath119 , estimated on a sample of size @xmath120 generated using the matrix @xmath121 are presented in the left panels .",
    "a graphical representation of matrix @xmath122 is given in the right panels . , title=\"fig:\",scaledwidth=35.0% ]      for @xmath93 , we first considered the case of block - diagonal matrices @xmath22 . for @xmath123",
    ", we then used matrices @xmath22 of the form @xmath124 .    in a fifth example ,",
    "matrix @xmath125 was build as follows . for every @xmath126 ,",
    "we first draw one observation @xmath127 from a ( 0,1)-uniform distribution , and @xmath128 was then set to 0 if @xmath129 , @xmath130 if @xmath131 and @xmath132 otherwise . the resulting true model consisted of 125 edges .",
    "coefficients @xmath133 were set to ( logit(0.1), ... ,logit(0.2 ) ) .",
    "gibbs sampling was further used to generate the data ( consisting of @xmath64 variables this time ) .",
    ", for @xmath134 , estimated on a sample of size @xmath120 generated using the matrix @xmath135 are presented in the left panel .",
    "a graphical representation of matrix @xmath125 is given in the right panel.,title=\"fig:\",scaledwidth=35.0% ] , for @xmath134 , estimated on a sample of size @xmath120 generated using the matrix @xmath135 are presented in the left panel .",
    "a graphical representation of matrix @xmath125 is given in the right panel.,title=\"fig:\",scaledwidth=35.0% ]      in this section , our goal is to empirically evaluate the approximate likelihoods on which the methods under study rely .",
    "to do so , we will focus on the case where @xmath92 since the exact log - likelihood of the poisson log - linear model can be computed for such a value of @xmath1 . for each of the four @xmath22 matrices described above , we proceed as follows .",
    "we generate a random sample of size @xmath136 , and for each value of the tuning parameter @xmath57 on an appropriate grid , we apply method ` bmnpseudo ` .",
    "this leads to some sparsity structure in the corresponding ising model and we can then compute the gaussian approximate log - likelihoods @xmath137 , @xmath73 ( see ( [ pr_dev_gauss ] ) below ) as well as both the exact poisson log - linear log - likelihood and the pseudo - likelihood for the ising model corresponding to this particular sparsity structure .",
    "more precisely , the following quantities were considered , @xmath138\\theta_\\lambda[,k])\\big\\ } \\label{pr_dev_pseudo}\\\\ \\mathcal{l}^{{\\footnotesize\\mbox{po}}}_\\lambda&=&\\sum_{i=1}^n \\log\\big\\{p({{\\bf x}}_i,\\theta_\\lambda^{\\footnotesize\\mbox{po}})\\big\\}\\label{pr_dev_poisson}\\\\ \\mathcal{l}^{{\\footnotesize\\mbox{g$_\\nu$}}}_\\lambda&= & \\log| c^\\nu_{\\lambda}|- \\mbox{tr}(c^\\nu_{\\lambda}s_\\nu)\\mbox { for } \\nu=1,2,3.\\label{pr_dev_gauss}\\end{aligned}\\ ] ] in ( [ pr_dev_pseudo ] ) , @xmath139 stands for the un - shrunk matrix derived under the sparsity structure inferred from method ` bmnpseudo ` with the sparsity parameter value @xmath57 , and @xmath140 and @xmath37 are as in ( [ pseudo_likelihood_uncond ] ) . in ( [ pr_dev_poisson ] ) ,",
    "@xmath141 is the matrix of coefficients obtained using a poisson log - linear model under the constrained induced by this sparsity structure , and @xmath142 is as in ( [ ising ] ) .",
    "lastly , in ( [ pr_dev_gauss ] ) , @xmath143 , @xmath144 , @xmath145 and @xmath146 is defined as @xmath147 where @xmath148 ( with @xmath139 as in ( [ pr_dev_pseudo ] ) ) .",
    "figure [ fig_comp_gauss_app_log_lik ] shows the corresponding deviances .",
    "it can be seen that using the gaussian approximate log - likelihood based on the covariance matrix with the additional 1/3 term on the diagonal results in a deviance which is quite far from the exact one .",
    "furthermore , the deviance obtained with the covariance matrix ( without adding the 1/3 term on the diagonal ) equals that obtained with the correlation matrix , and both are closer to the exact deviance .",
    "finally , the deviance of the pseudo-(log)-likelihood is always greater than the exact deviance .",
    "using half the pseudo - likelihood corrects this undesirable effect in most cases .",
    "these results should obviously be considered with caution .",
    "even if we tried to use various @xmath22 matrices to generate the data ( and the conclusions were consistently the same ) , a theoretical study would be needed to confirm these empirical findings .",
    "generated with matrices @xmath101 ( upper left corner ) , @xmath102 ( upper right corner ) , @xmath103 ( lower left corner ) , and @xmath104 ( lower right corner ) .",
    "deviances were computed using the exact log - linear log - likelihood ( solid black line , solid circles ) , the pseudo - likelihood ( dashed blue line , circles ) , half the pseudo - likelihood ( solid blue line , solid circles ) , and the gaussian approximate log likelihoods based on the covariance matrix with an additional 1/3 term on the diagonal ( dotted green line , circles ) , the covariance matrix ( dashed green line , squares ) and the correlation matrix ( solid green line , solid circles)(see ( [ pr_dev_pseudo])-([pr_dev_gauss ] ) for the corresponding formula).,title=\"fig:\",scaledwidth=47.0% ]    generated with matrices @xmath101 ( upper left corner ) , @xmath102 ( upper right corner ) , @xmath103 ( lower left corner ) , and @xmath104 ( lower right corner ) .",
    "deviances were computed using the exact log - linear log - likelihood ( solid black line , solid circles ) , the pseudo - likelihood ( dashed blue line , circles ) , half the pseudo - likelihood ( solid blue line , solid circles ) , and the gaussian approximate log likelihoods based on the covariance matrix with an additional 1/3 term on the diagonal ( dotted green line , circles ) , the covariance matrix ( dashed green line , squares ) and the correlation matrix ( solid green line , solid circles)(see ( [ pr_dev_pseudo])-([pr_dev_gauss ] ) for the corresponding formula).,title=\"fig:\",scaledwidth=47.0% ]    generated with matrices @xmath101 ( upper left corner ) , @xmath102 ( upper right corner ) , @xmath103 ( lower left corner ) , and @xmath104 ( lower right corner ) .",
    "deviances were computed using the exact log - linear log - likelihood ( solid black line , solid circles ) , the pseudo - likelihood ( dashed blue line , circles ) , half the pseudo - likelihood ( solid blue line , solid circles ) , and the gaussian approximate log likelihoods based on the covariance matrix with an additional 1/3 term on the diagonal ( dotted green line , circles ) , the covariance matrix ( dashed green line , squares ) and the correlation matrix ( solid green line , solid circles)(see ( [ pr_dev_pseudo])-([pr_dev_gauss ] ) for the corresponding formula).,title=\"fig:\",scaledwidth=47.0% ]    generated with matrices @xmath101 ( upper left corner ) , @xmath102 ( upper right corner ) , @xmath103 ( lower left corner ) , and @xmath104 ( lower right corner ) .",
    "deviances were computed using the exact log - linear log - likelihood ( solid black line , solid circles ) , the pseudo - likelihood ( dashed blue line , circles ) , half the pseudo - likelihood ( solid blue line , solid circles ) , and the gaussian approximate log likelihoods based on the covariance matrix with an additional 1/3 term on the diagonal ( dotted green line , circles ) , the covariance matrix ( dashed green line , squares ) and the correlation matrix ( solid green line , solid circles)(see ( [ pr_dev_pseudo])-([pr_dev_gauss ] ) for the corresponding formula).,title=\"fig:\",scaledwidth=47.0% ]      let us first consider the performances achieved by the oracle models ( tables [ table_simul1_acc_1 ] and [ table_simul1_acc_2 ] ) .",
    "overall , methods ` bmnpseudo ` and ` gausscor ` achieve good performances in terms of accuracy and f1-score .",
    "it is also noteworthy that the computational time is much higher for ` bmnpseudo ` , while overall performances of ` gausscor ` appear to be slightly higher ( especially under the fourth simulation design ) .    when focusing on the three ` gaussapprox ` methods , we observed the following ranking @xmath149 consequently , and to save space , methods ` gausscov ` and ` gausscov 1/3 ` will not be considered in the evaluation of the models selected via the bic procedure .",
    "it is still interesting to note that ` gausscor`@xmath150`gausscov ` although we observed that the approximate deviances under these two methods were equal and close to the exact ones , which turn out to be an insufficient condition for achieving good performances .",
    "l r@.l r@.l c c c c c method & & & fpr & tpr@xmath151 & pre & acc.&f1 score +   +   + bmnpseudo & 6&47 & 3&22 & 0.034 & 0.202 & 0.724 & 0.796 & 0.291 + gausscov 1/3 & 0&48 & 1&84 & 0.010 & 0.150 & 0.935 & 0.804 & 0.214 + gausscov & 0&48&2&02 & 0.012 & 0.160 & 0.933 & 0.804 & 0.220 + gausscor & 0&48 & 1&98 & 0.011 & 0.158 & 0.937 & 0.804 & 0.219 +   + bmnpseudo & 20&20 & 6&96 & 0.031 & 0.588 & 0.883 & 0.884 & 0.676 + gausscov 1/3 & 0&72 & 7&02 & 0.033 & 0.588 & 0.877 & 0.883 & 0.674 + gausscov & 0&72 & 7&00 & 0.031 & 0.590 & 0.881 & 0.884 & 0.677 + gausscor & 0&72 & 7&02 & 0.031 & 0.592 & 0.881 & 0.885 & 0.678 +   + bmnpseudo & 82&65 & 9&48 & 0.003 & 0.938 & 0.991 & 0.984 & 0.961 + gausscov 1/3 & 0&96 & 9&36 & 0.002 & 0.928 & 0.992 & 0.982 & 0.957 + gausscov & 0&96 & 9&40 & 0.002 & 0.932 & 0.992 & 0.983 & 0.959 + gausscor & 0&96 & 9&40 & 0.002 & 0.932 & 0.992 & 0.983 & 0.959 +   +   +   + bmnpseudo & 20&17 & 3&62 & 0.024 & 0.278 & 0.864 & 0.821 & 0.376 + gausscov 1/3 & 0&54 & 3&72 & 0.025 & 0.284 & 0.867 & 0.821 & 0.376 + gausscov & 0&54 & 3&92 & 0.028 & 0.294 & 0.857 & 0.821 & 0.386 + gausscor & 0&55 & 3&60 & 0.025 & 0.274 & 0.870 & 0.820 & 0.366 +   + bmnpseudo & 23&16 & 7&02 & 0.014 & 0.654 & 0.950 & 0.912 & 0.755 + gausscov 1/3 & 0&79 & 7&44 & 0.02 & 0.674 & 0.927 & 0.912 & 0.761 + gausscov & 0&79 & 7&46 & 0.019 & 0.678 & 0.93 & 0.913 & 0.765 + gausscor & 0&86 & 8&24 & 0.017 & 0.766 & 0.938 & 0.935 & 0.832 +   + bmnpseudo & 83&76 & 9&74 & 0.004 & 0.960 & 0.987 & 0.988 & 0.972 + gausscov 1/3 & 0&96 & 9&86 & 0.008 & 0.958 & 0.975 & 0.984 & 0.964 + gausscov & 0&97 & 9&70 & 0.005 & 0.954 & 0.985 & 0.986 & 0.967 + gausscor & 0&99 & 10&04 & 0.002 & 0.998 & 0.995 & 0.998 & 0.996 + @xmath151 tpr@xmath152rec .",
    "l r@.l r@.l c c c c c method & & & fpr & tpr@xmath151 & pre & acc.&f1 score +   +   + bmnpseudo & 35&08 & 6&26 & 0.018 & 0.564 & 0.929 & 0.889 & 0.678 + gausscov 1/3 & 0&72 & 6&54 & 0.021 & 0.582 & 0.921 & 0.891 & 0.687 + gausscov & 0&72 & 6&60 & 0.022 & 0.584 & 0.917 & 0.891 & 0.689 + gausscor & 0&78 & 7&66 & 0.028 & 0.668 & 0.893 & 0.904 & 0.746 +   + bmnpseudo & 21&02 & 8&76 & 0.011 & 0.838 & 0.966 & 0.956 & 0.889 + gausscov 1/3 & 0&84 & 8&92 & 0.027 & 0.796 & 0.913 & 0.933 & 0.836 + gausscov & 0&87 & 8&88 & 0.017 & 0.828 & 0.946 & 0.948 & 0.874 + gausscor & 0&96 & 9&68 & 0.006 & 0.948 & 0.981 & 0.984 & 0.963 +   + bmnpseudo & 80&60 & 10&02 & 0.002 & 0.994 & 0.993 & 0.997 & 0.993 + gausscov 1/3 & 0&92 & 10&84 & 0.035 & 0.962 & 0.894 & 0.964 & 0.923 + gausscov & 0&97 & 10&10 & 0.007 & 0.984 & 0.976 & 0.991 & 0.979 + gausscor & 0&99 & 10&00 & 0.001 & 0.998 & 0.998 & 0.999 & 0.998 +   +   +   + bmnpseudo & 39&89 & 5&46 & 0.055 & 0.212 & 0.816 & 0.635 & 0.312 + gausscov 1/3 & 0&60 & 6&84 & 0.084 & 0.245 & 0.775 & 0.633 & 0.333 + gausscov & 0&60 & 5&16 & 0.053 & 0.199 & 0.800 & 0.631 & 0.297 + gausscor & 0&60 & 6&86 & 0.079 & 0.253 & 0.761 & 0.639 & 0.357 +   + bmnpseudo & 183&22 & 12&94 & 0.084 & 0.566 & 0.859 & 0.768 & 0.668 + gausscov 1/3 & 0&68 & 12&36 & 0.08 & 0.541 & 0.853 & 0.760 & 0.651 + gausscov & 0&69 & 13&02 & 0.089 & 0.563 & 0.840 & 0.764 & 0.665 + gausscor & 0&75 & 13&64 & 0.055 & 0.643 & 0.905 & 0.818 & 0.745 +   + bmnpseudo & 255&87 & 15&06 & 0.058 & 0.714 & 0.906 & 0.846 & 0.796 + gausscov 1/3 & 0&76 & 14&68 & 0.072 & 0.674 & 0.880 & 0.820 & 0.759 + gausscov & 0&78 & 15&16 & 0.072 & 0.700 & 0.884 & 0.832 & 0.778 + gausscor & 0&85 & 15&70 & 0.037 & 0.776 & 0.944 & 0.884 & 0.848 + @xmath151 tpr@xmath152rec .",
    "turning our attention to the evaluation of models selected with the bic procedure ( tables [ table_simul1_bic_1 ] and [ table_simul1_bic_2 ] ) , a first observation is that , as suggested by the results of section [ sec_comp_approx_deviances ] , computing the bic with half the pseudo - likelihood ( rather than the pseudo - likelihood itself ) results in better models in most cases .",
    "moreover , from the comparisons of the results of tables [ table_simul1_acc_1 ] and [ table_simul1_acc_2 ] and tables [ table_simul1_bic_1 ] and [ table_simul1_bic_2 ] , as @xmath97 grows , the bic procedure appears to enable to select models achieving performances similar to those achieved by the `` oracle '' models .",
    "moreover , the computation of the un - shrunk estimates with method ` bmnpseudo ` appears to be very slow ( the oracle models were much faster to compute than the models selected by the bic approach for this particular method , especially when the sample size is small and in the fourth simulation design ) .",
    "overall , ` seplogit or ` , ` seplogit and ` and ` gausscor ` are the best methods , closely followed by ` bmnpseudo 1/2 ` .",
    "lastly , among these candidate methods , ` gausscor ` is the fastest .    we should lastly mention that method ` seplogit `",
    "was further tested using standardized covariates in each @xmath0-penalized logistic regression models ( results not shown ) . to motivate this choice , we may mention that this is the default option in package ` glmnet ` as this approach is often adopted in applications when using @xmath0-penalization ( see @xcite for instance ) ; its suitability in our context of binary variables was yet questionable .",
    "interestingly , this approach yielded results very similar to those obtained via the `` standard '' one on data generated using matrices @xmath101 , @xmath102 and @xmath103 and slightly better when using matrix @xmath104 ( which however corresponds to the situation where we observed the greatest variability in the performances of every method ) .",
    "l r@.l r@.l c c c c c method & & & fpr & tpr@xmath151 & pre & acc.&f1 score +   +   + seplogit or & 21&94 & 2&86 & 0.043 & 0.136 & 0.540 & 0.775 & 0.234 + seplogit and & 21&94 & 2&12 & 0.029 & 0.112 & 0.585 & 0.780 & 0.216 + bmnpseudo & 14&20 & 8&16 & 0.141 & 0.322 & 0.424 & 0.740 & 0.355 + bmnpseudo 1/2 & 14&20 & 2&60 & 0.039 & 0.122 & 0.492 & 0.774 & 0.233 + gausscor & 1&12 & 2&56 & 0.035 & 0.132 & 0.572 & 0.780 & 0.235 +   + seplogit or & 22&91 & 4&80 & 0.018 & 0.416 & 0.885 & 0.856 & 0.549 + seplogit and & 22&91 & 4&12 & 0.014 & 0.364 & 0.892 & 0.848 & 0.504 + bmnpseudo & 42&43 & 9&48 & 0.079 & 0.670 & 0.725 & 0.865 & 0.687 + bmnpseudo 1/2 & 42&43 & 4&60 & 0.015 & 0.408 & 0.895 & 0.857 & 0.550 + gausscor & 1&16 & 4&48 & 0.014 & 0.400 & 0.895 & 0.856 & 0.539 +   + seplogit or & 27&89 & 9&52 & 0.006 & 0.93 & 0.979 & 0.980 & 0.952 + seplogit and & 27&89 & 9&32 & 0.004 & 0.918 & 0.986 & 0.979 & 0.949 + bmnpseudo & 176&09 & 11&66 & 0.055 & 0.972 & 0.846 & 0.951 & 0.901 + bmnpseudo 1/2 & 176&09 & 9&36 & 0.005 & 0.920 & 0.985 & 0.979 & 0.948 + gausscor & 1&19 & 9&30 & 0.005 & 0.912 & 0.983 & 0.976 & 0.943 +   +   +   + seplogit or & 23&45 & 4&68 & 0.058 & 0.266 & 0.618 & 0.792 & 0.361 + seplogit and & 23&45 & 2&76 & 0.027 & 0.182 & 0.687 & 0.797 & 0.302 + bmnpseudo & 215&65 & 9&82 & 0.154 & 0.444 & 0.482 & 0.757 & 0.443 + bmnpseudo 1/2 & 215&65 & 3&22 & 0.033 & 0.208 & 0.688 & 0.799 & 0.325 + gausscor & 1&11 & 3&80 & 0.041 & 0.236 & 0.686 & 0.798 & 0.341 +   + seplogit or & 33&18 & 7&20 & 0.018 & 0.658 & 0.922 & 0.910 & 0.758 + seplogit and & 33&18 & 6&18 & 0.007 & 0.592 & 0.960 & 0.904 & 0.720 + bmnpseudo & 97&70 & 10&82 & 0.087 & 0.776 & 0.735 & 0.882 & 0.747 + bmnpseudo 1/2 & 97&70 & 6&56 & 0.014 & 0.606 & 0.934 & 0.901 & 0.720 + gausscor & 1&16 & 7&02 & 0.014 & 0.652 & 0.931 & 0.912 & 0.758 +   + seplogit or & 34&52 & 10&28 & 0.011 & 0.990 & 0.966 & 0.989 & 0.977 + seplogit and & 34&52 & 9&92 & 0.005 & 0.976 & 0.985 & 0.991 & 0.980 + bmnpseudo & 175&80 & 11&70 & 0.049 & 0.998 & 0.872 & 0.961 & 0.926 + bmnpseudo 1/2 & 175&80 & 10&16 & 0.013 & 0.972 & 0.960 & 0.984 & 0.965 + gausscor & 1&30 & 10&06 & 0.005 & 0.988 & 0.984 & 0.993 & 0.985 + @xmath151 tpr@xmath152rec .",
    "l r@.l r@.l c c c c c method & & & fpr & tpr@xmath151 & pre & acc.&f1 score +   +   + seplogit or & 30&86 & 7&86 & 0.055 & 0.594 & 0.775 & 0.867 & 0.660 + seplogit and & 30&86 & 5&44 & 0.021 & 0.470 & 0.868 & 0.866 & 0.599 + bmnpseudo & 276&48 & 12&18 & 0.147 & 0.702 & 0.613 & 0.819 & 0.640 + bmnpseudo 1/2 & 276&48 & 6&44 & 0.031 & 0.534 & 0.856 & 0.872 & 0.638 + gausscor & 1&26 & 7&42 & 0.037 & 0.612 & 0.834 & 0.885 & 0.697 +   + seplogit or & 33&10 & 10&14 & 0.027 & 0.918 & 0.912 & 0.960 & 0.912 + seplogit and & 33&10 & 8&90 & 0.009 & 0.858 & 0.967 & 0.961 & 0.906 + bmnpseudo & 163&20 & 12&78 & 0.098 & 0.936 & 0.760 & 0.910 & 0.830 + bmnpseudo 1/2 & 163&20 & 9&80 & 0.030 & 0.876 & 0.906 & 0.949 & 0.885 + gausscor & 1&26 & 9&78 & 0.014 & 0.930 & 0.957 & 0.974 & 0.940 +   + seplogit or & 40&76 & 10&30 & 0.009 & 1.000 & 0.973 & 0.993 & 0.986 + seplogit and & 40&76 & 10&02 & 0.001 & 1.000 & 0.998 & 0.999 & 0.999 + bmnpseudo & 176&22 & 11&14 & 0.033 & 1.000 & 0.914 & 0.975 & 0.951 + bmnpseudo 1/2 & 176&22 & 10&18 & 0.005 & 1.000 & 0.984 & 0.996 & 0.992 + gausscor & 1&26 & 10&12 & 0.003 & 1.000 & 0.989 & 0.997 & 0.994 +   +   +   + seplogit or & 30&63 & 20&96 & 0.443 & 0.497 & 0.452 & 0.532 & 0.470 + seplogit and & 30&63 & 14&88 & 0.322 & 0.343 & 0.440 & 0.537 & 0.383 + bmnpseudo & 193&43 & 27&16 & 0.577 & 0.640 & 0.446 & 0.515 & 0.523 + bmnpseudo 1/2 & 193&43 & 18&70 & 0.405 & 0.431 & 0.440 & 0.526 & 0.431 + gausscor & 1&87 & 20&72 & 0.452 & 0.473 & 0.432 & 0.516 & 0.445 +   + seplogit or & 36&10 & 10&48 & 0.038 & 0.500 & 0.916 & 0.767 & 0.642 + seplogit and & 36&10 & 8&30 & 0.012 & 0.421 & 0.967 & 0.749 & 0.583 + bmnpseudo & 1090&87 & 13&52 & 0.107 & 0.565 & 0.815 & 0.755 & 0.657 + bmnpseudo 1/2 & 1090&87 & 9&84 & 0.044 & 0.458 & 0.897 & 0.746 & 0.600 + gausscor & 1&80 & 10&70 & 0.025 & 0.529 & 0.945 & 0.787 & 0.675 +   + seplogit or & 52&32 & 15&26 & 0.048 & 0.737 & 0.922 & 0.861 & 0.817 + seplogit and & 52&32 & 13&44 & 0.016 & 0.685 & 0.971 & 0.858 & 0.802 + bmnpseudo & 2299&82 & 17&40 & 0.122 & 0.748 & 0.829 & 0.823 & 0.783 + bmnpseudo 1/2 & 2299&82 & 14&72 & 0.056 & 0.698 & 0.905 & 0.840 & 0.786 + gausscor & 1&86 & 14&76 & 0.026 & 0.741 & 0.957 & 0.876 & 0.834 + @xmath151 tpr@xmath152rec .",
    "to save space , we only present here the performances achieved by models selected via the bic procedure , on samples of size @xmath136 and @xmath120 generated using either matrix diag@xmath153 , for @xmath154 or matrix @xmath125 .",
    "moreover , the results obtained in the case @xmath92 especially show that method ` bmnpseudo ` can be quite slow , and that it does not outperform method ` seplogit ` .",
    "lastly , among the methods relying on a gaussian approximate of the ising likelihood , method ` gausscor ` was observed to be the best .",
    "therefore , in order to save computational time , only methods ` gausscor ` and ` seplogit ` were considered in the case @xmath93 .",
    "results are presented in table [ table_simul1_bic50 ] .",
    "they are consistent with what was observed in the case @xmath92 .",
    "more precisely , methods ` seplogit ` and ` gausscor ` achieve comparable performances .",
    "regarding computational time , ` gausscor ` is still significantly faster than ` seplogit ` .",
    "l r@.l r@.l c c c c c method & & & fpr & tpr@xmath151 & pre & acc.&f1 score +   +   + seplogit or & 236&21 & 39&00 & 0.016 & 0.399 & 0.515 & 0.960&0.447 + seplogit and & 236&21 & 30&10 & 0.011 & 0.352 & 0.588 & 0.963 & 0.438 + gausscor & 31&85 & 34&06 & 0.013 & 0.376 & 0.562 & 0.962 & 0.446 +   + seplogit or & 248&74 & 54&48 & 0.006 & 0.938 & 0.864 & 0.991 & 0.898 + seplogit and & 248&74 & 50&64 & 0.004 & 0.921 & 0.910 & 0.993 & 0.915 + gausscor & 28&86 & 52&34 & 0.005 & 0.918 & 0.881 & 0.991 & 0.898 +   +   +   + seplogit or & 285&91 & 53&36 & 0.017 & 0.661 & 0.626 & 0.970 & 0.640 + seplogit and & 285&91 & 34&14 & 0.007 & 0.527 & 0.778 & 0.974 & 0.626 + gausscor & 32&75 & 45&42 & 0.011 & 0.642 & 0.717 & 0.975 & 0.673 +   + seplogit or & 304&09 & 59&26 & 0.008 & 0.988 & 0.837 & 0.991 & 0.905 + seplogit and & 304&09 & 50&80 & 0.003 & 0.950 & 0.937 & 0.995 & 0.943 + gausscor & 29&00 & 54&40 & 0.004 & 0.995 & 0.917 & 0.996 & 0.954 +   +   +   + seplogit or & 305&44 & 68&66 & 0.019 & 0.936 & 0.685 & 0.980 & 0.790 + seplogit and & 305&44 & 47&04 & 0.005 & 0.834 & 0.888 & 0.989 & 0.859 + gausscor & 30&34 & 60&34 & 0.010 & 0.968 & 0.811 & 0.989 & 0.880 +   + seplogit or & 380&86 & 57&72 & 0.007 & 1.000 & 0.868 & 0.994 & 0.929 + seplogit and & 380&86 & 51&50 & 0.001 & 1.000 & 0.971 & 0.999 & 0.985 + gausscor & 25&48 & 50&48 & 0.000 & 1.000 & 0.991 & 1.000 & 0.995 +   +   +   + seplogit or & 271&84 & 95&20 & 0.017 & 0.610 & 0.802 & 0.945 & 0.692 + seplogit and & 271&84 & 71&00 & 0.007 & 0.508 & 0.894 & 0.944 & 0.647 + gausscor & 74&71 & 86&96 & 0.013 & 0.582 & 0.839 & 0.946 & 0.685 +   + seplogit or & 307&38 & 129&00 & 0.008 & 0.960 & 0.931 & 0.989 & 0.945 + seplogit and & 307&38 & 117&48 & 0.002 & 0.922 & 0.981 & 0.990 & 0.950 + gausscor & 65&41 & 123&68 & 0.007 & 0.927 & 0.938 & 0.986 & 0.932 + @xmath151 tpr@xmath152rec .",
    "one way to measure the agreement between two selected models is to compare them with their intersection .",
    "let @xmath155 and @xmath156 be the two graphs to be compared . further let @xmath157 and denote by @xmath158 the cardinality of a set @xmath159 of edges . to compare graphs @xmath160 and @xmath161",
    ", we will consider the quantities @xmath162 as measures of agreement and disagreement respectively ( @xmath163 is the cardinality of the symmetric difference between @xmath164 and @xmath165 ) .",
    "observe that according to these measures , the saturated graph would both agree and disagree `` a lot '' with any sub - graph .",
    "results are presented in table [ table_agree ] for @xmath92 and @xmath166 which correspond to the two extreme situations in terms of signal - to - noise ratio .",
    "overall , @xmath167 is closer to @xmath168 than @xmath169 , what was expected given the respective principles of the methods .",
    "moreover , agreement [ resp .",
    "disagreement ] between the various models is higher [ resp .",
    "lower ] when the signal - to - noise ratio is high , that is when @xmath120 and/or @xmath170 . when the signal - to - noise ratio is low and models are quite different , a natural question is how to get the best model .",
    "intersecting two models is one of the candidate approaches . for the sake of brevity",
    ", we do not present the complete results here , but intersecting ` gausscor ` and ` seplogit or ` for instance resulted in quite conservative models that generally achieved better performances than either ` gausscor ` or ` seplogit or ` ( in terms of accuracy and f1-score ) .",
    "models derived under method ` seplogit ` with standardized covariates were also compared to the other models ( results not shown ) .",
    "overall , we observed very good agreement between the standard approach and the standardized approach ( especially in the case of high signal - to - noise ratio ) .",
    "we also observed slightly better agreements between these models and those derived under method ` gausscor ` , especially on datasets generated using matrix @xmath104 .",
    "l c c c c comparisons & & +   + seplogit or seplogit and & 1.000 & ( 0.000 ) & 2.420 & ( 1.605 ) + seplogit or bmnpseudo & 0.970 & ( 0.071 ) & 2.140 & ( 1.807 ) + seplogit and bmnpseudo & 0.965 & ( 0.099 ) & 1.720 & ( 1.578 ) + bmnpseudo gausscor & 0.941 & ( 0.087 ) & 2.940 & ( 1.463 ) + seplogit or gausscor & 0.941 & ( 0.077 ) & 2.480 & ( 1.632 ) + seplogit and gausscor & 0.940 & ( 0.086 ) & 3.060 & ( 1.476 ) +   + seplogit or seplogit and & 1.000 & ( 0.000 ) & 0.280 & ( 0.497 ) + seplogit or bmnpseudo & 1.000 & ( 0.000 ) & 0.160 & ( 0.370 ) + seplogit and bmnpseudo & 0.998 & ( 0.013 ) & 0.200 & ( 0.495 ) + bmnpseudo gausscor & 1.000 & ( 0.000 ) & 0.100 & ( 0.303 ) + seplogit or gausscor & 1.000 & ( 0.000 ) & 0.220 & ( 0.418 ) + seplogit and gausscor & 0.998 & ( 0.013 ) & 0.140 & ( 0.405 ) +   + seplogit or seplogit and & 1.000 & ( 0.000 ) & 6.080 & ( 2.146 ) + seplogit or bmnpseudo & 0.952 & ( 0.048 ) & 4.800 & ( 2.711 ) + seplogit and bmnpseudo & 0.957 & ( 0.053 ) & 5.400 & ( 2.148 ) + bmnpseudo gausscor & 0.967 & ( 0.054 ) & 6.740 & ( 2.863 ) + seplogit or gausscor & 0.896 & ( 0.081 ) & 8.380 & ( 2.900 ) + seplogit and gausscor & 0.931 & ( 0.081 ) & 9.900 & ( 3.112 ) +   + seplogit or seplogit and & 1.000 & ( 0.000 ) & 1.820 & ( 1.424 ) + seplogit or bmnpseudo & 0.981 & ( 0.031 ) & 1.880 & ( 1.335 )",
    "+ seplogit and bmnpseudo & 0.983 & ( 0.030 ) & 1.900 & ( 1.359 ) + bmnpseudo gausscor & 0.950 & ( 0.050 ) & 2.900 & ( 1.474 ) + seplogit or gausscor & 0.972 & ( 0.035 ) & 2.220 & ( 1.183 ) + seplogit and gausscor & 0.980 & ( 0.035 ) & 2.200 & ( 1.666 ) +      both methods ` seplogit ` and ` bmnpseudo ` have been empirically shown to yield appropriate estimates for the conditional odds - ratios . on the other hand ,",
    "it is rather unclear whether estimates derived from methods relying on the gaussian approximation would be consistent .",
    "we therefore conclude this simulation study with a simple study to check it .    to avoid interpretation difficulties related to the model selection accuracy ,",
    "coefficient estimates were computed under the sparsity structure of the true models and compared with the true coefficients ( this corresponds to the situation where the true sparsity structure is known ) .",
    "to do so , we used an approach similar to the one used to get un - shrunk estimates for the bic procedure .",
    "the mean squared errors for the conditional log - odds ratios , which we defined here as @xmath171 are reported in table [ table_sce ] for methods ` seplogit ` , ` gausscov 1/3 ` and ` gausscor ` in the case @xmath92 and for samples of sizes @xmath136 and @xmath120 ( for ` seplogit ` each coefficient @xmath172 was set as the mean of the two coefficients returned by the two constrained logistic regressions involved in this method ) .",
    "it can be observed that overall neither ` gausscov 1/3 ` nor ` gausscor ` led to consistent estimates for the @xmath24 coefficients .",
    "these methods ( especially ` gausscor ` ) should therefore be combined with other methods ( for instance ` seplogit ` ) when estimates of the conditional odds - ratios are needed .",
    "inconsistency of the estimates derived under method ` gausscor ` can be explained as follows .",
    "since this method relies on the correlation matrix , it is closely related to the method consisting in performing linear regressions at each node ( as shown by @xcite in the gaussian case ) .",
    "therefore , the coefficients returned by this method are related to the coefficients @xmath173 involved in the linear model @xmath174 clearly , coefficients @xmath173 are quite different from the conditional log odds - ratios @xmath24 involved in the ising model ( see ( [ ising ] ) ) .",
    "l r@.l r@.l r@.l sample size & & & +   + @xmath136 & 2&613 & 2&073 & 1&957 + @xmath120 & 2&140 & 0&451 & 0&435 +   + @xmath136 & 14&568 & 7&743 & 6&175 + @xmath120 & 14&342 & 6&573 & 1&201 +   + @xmath136 & 58&502 & 29&197 & 10&704 + @xmath120 & 57&451 & 26&779 & 1&739 +   + @xmath136 & 98&304 & 68&034 & 50&007 + @xmath120 & 97&402 & 65&973 & 12&794 +",
    "the general objective of the application in this example is to detect associations between causes of death and identify possibly relevant groupings of causes contributing to the death .",
    "the dataset we used consists in causes of death recorded in all death certificates ( @xmath175 ) in france for the year 2005 . in france ,",
    "death certification ( compulsory with 100% coverage ) conforms to the who guideline : the death process is described starting from the underlying causes of death and ending with the immediate cause of death ; other contributing causes of death are also recorded .",
    "all these causes were considered in the analysis .",
    "causes are further coded according to the international classification of diseases ( icd)(in 2005 , the tenth revision @xcite ) .",
    "the total number of code categories is large ( about 4000 codes used ) but for this analysis we applied a simplified classification of 59 entities according to the eurostat shortlist @xcite ( see appendix a for the classification used ) .",
    "therefore , @xmath176 and each death certificate can be regarded as a vector @xmath177 in @xmath178 , where , for all @xmath179 , @xmath180 if and only if the @xmath39-th class is recorded on the death certificate .",
    "about 11% of the certificates had more than five causes of death , 14% had four causes , 26% had three , 30% had two and 18% had a sole cause of death .",
    "the frequencies of each cause are reported in appendix a ; the most frequent causes of death were , in decreasing order , heart failure , ischaemic heart diseases , cerebrovascular diseases , hypertensive diseases , pneumonia , diabetes , lung cancer and senility .",
    "most common causes of death clearly depend upon age and gender , and a natural question is whether associations among causes of death also vary with age and gender .",
    "we then decided to split the whole population into strata defined by gender and age the complete analysis of every stratum is out of the scope of the present paper , and we only present here the results from the analysis of two sub - groups , namely those of males aged between 15 and 24 ( 2918 observations ) and males aged between 45 and 64 ( 57045 observations ) .",
    "first considering the group of age 15 - 24 , we applied ` gausscor ` , ` bmnpseudo ` , ` seplogit and ` , and ` seplogit or ` after selecting the sparsity parameter according to the bic procedure described above .",
    "this yielded models with 113 , 107 , 61 and 129 associations respectively .",
    "good agreement was observed between models derived under methods ` seplogit ` and ` bmnpseudo ` ( we had @xmath181 for the comparison between ` bmn ` and ` seplogit or ` for instance ) .",
    "the model derived under method ` gausscor ` was slightly different from the three other models ( we had @xmath182 for the comparison between ` gausscor ` and ` seplogit or ` and @xmath183 for the comparison between ` gausscor ` and ` seplogit and ` ) .",
    "more precisely , the model obtained with method ` gausscor ` entailed many more positive associations , a few of which corresponding to variables co - occurring only once in the sub - group .",
    "this suggests that method ` gausscor ` might be a little too sensitive to positive associations , especially when the signal - to - noise ratio is low ( in this particular study , the signal - to - noise ratio is low due to highly unbalanced variables ) .",
    "regarding computational time , 1.6 second was needed to compute method ` gausscor ` while it took 19628 and 876 seconds for computing methods ` bmnpseudo ` and ` seplogit ` respectively ( analyses were performed on the windows machine ) . for these latter two methods ,",
    "we were not able to conduct the analysis with the choice @xmath184 , and we had to select @xmath185 and @xmath186 for methods ` bmnpseudo ` and ` seplogit ` respectively .",
    "it is also noteworthy that the computational time needed for methods ` bmnpseudo ` and ` seplogit ` is mostly due to the computation of the un - shrunk estimates ( necessary for the derivation of the bic ) ; omitting this step , the computational time using method ` bmnpseudo ` [ resp . `",
    "seplogit ` ] is reduced to 4598 seconds [ resp .",
    "198 seconds ] .",
    "figure [ fig_graph_complet ] shows the final retained model for the group 15 - 24 , which is the intersection of the models derived under methods ` seplogit or ` and ` gausscor ` .",
    "apart from the obvious association between depression and suicide , the strongest ( positive ) associations identified were between diabetes and other endocrinal diseases , colorectal cancer and metastasis , septicemia and pneumonia , and between diseases of arteries , arterioles and capillaries and cerebrovascular diseases .",
    "the strong negative associations between transport accidents and all other conditions , and between suicide and most other conditions ( except depression and other mental disorders ) is also worth noting .",
    "these latter associations correspond to well - known sequences of causes leading to death and most of those present in the figure have strong biological plausibility . in the analysis of the older group , we only applied methods ` seplogit ` and ` gausscor ` ( to save computational time ) , which took 15241 and 4.8 seconds respectively . in this case , we had to use @xmath185 for method ` seplogit ` .",
    "moreover , when omitting the computation of the un - shrunk estimates , the computational time using method ` seplogit ` reduced to 1943 seconds .",
    "600 , 778 and 708 associations were detected by method ` seplogit and ` , ` seplogit or ` and ` gausscor ` respectively .",
    "good agreement was observed between the models ( we had @xmath187 for the comparison between ` gausscor ` and ` seplogit or ` and @xmath188 for the comparison between ` gausscor ` and ` seplogit and ` ) , which tends to confirm that agreement between the models returned by methods ` seplogit ` and ` gausscor ` increases with the signal - to - noise ratio .",
    "in this paper we empirically compared several approximate methods designed to search for associations among binary variables .",
    "we observed that methods ` seplogit ` and ` bmnpseudo ` achieved similar performances in terms of accuracy and f1-score , with a slight advantage to method ` seplogit ` .",
    "moreover , the models selected by both methods are very similar in most cases , as could be expected given the similarity between them . in terms of computational time , ` seplogit ` appeared to be overall faster than ` bmnpseudo ` , but the two methods share the disadvantage of being quite slow to compute , especially for low values of the sparsity parameter .",
    "for the method ` bmnpseudo ` , we observed that using half the pseudo - likelihood rather than the pseudo - likelihood itself when computing the bic enables us to select better models in most cases .",
    "the multiplicative coefficient 1/2 might not be optimal in all situation and some adaptive coefficient might be derived from a theoretical study of the pseudo - likelihood .",
    "alternatively , cross - validation could be considered at a cost of an increased computational time , which seems undesirable given the aforementioned lack of speed of this method . moreover",
    ", the suitability of cross - validation for model selection remains questionable @xcite .    in terms of accuracy",
    ", the method proposed by @xcite was shown to be generally too conservative .",
    "we then proposed a slight modification , referred to as ` gausscor ` , in which we remove the additive @xmath66 term on the diagonal , and use the sample correlation matrix as a starting point for the algorithm . with these modifications , ` gausscor ` combines good overall performances ( comparable to the performances achieved by ` seplogit ` and ` bmnpseudo ` ) and exceptional computational speed .",
    "in particular , ` gausscor ` was observed to be between 3 and 200 times faster than the other methods on simulated data .",
    "this speed is particularly desirable for handling truly high - dimensional datasets since the concurrent methods ( ` seplogit ` or ` bmnpseudo ` ) might be dramatically slow in such cases . to be complete , we should mention that method ` seplogit ` could be implemented using other sparse logistic regression algorithms that might be faster than the ` glmnet ` r package ( see @xcite for instance ) .",
    "however , we think that the comparison conducted here was fair since both r packages ` glmnet ` and ` glasso ` rely on a coordinate descent algorithm @xcite .",
    "interestingly , we also observed that the models selected by methods ` gausscor ` and ` seplogit ` can be significantly different , especially in the situation of low signal - to - noise ratio . on our real example",
    ", we decided to retain the intersection of the two selected models as the final model , which led to conservative but competitive models on simulated examples .",
    "however , other approaches can be considered and it would be interesting to further study how these methods can be optimally combined .",
    "approximate methods that use either multiple logistic regressions or the pseudo - likelihood have been shown to attain performances similar to those reached using exact inference at a lower computational cost @xcite .",
    "our results suggest here that using gaussian approximates of the ising likelihood can ensure similar statistical performance at a greatly improved speed . in the absence of a theoretical justification for the good performances achieved by this method however",
    ", we can only claim here that ` gausscor ` is a candidate method that can be recommended in some cases ; a theoretical study might enable to have a better idea of what these cases are .",
    "p5 cm c p5 cm c    septicaemia & septicaemia & a40-a41 & 25713 + tuberculosis & tuberc & a15-a19 , b90 & 1720 + aids and hiv infection & aids & b20-b24 & 1094 + other infectious disease & other_infect & a00-a14 , a20-a39 , a42-b19 , b25-b89 , b91-b99 & 14188 + oral cancer & c_oral & c00-c14 & 5076 + oesophageal cancer & c_oesoph & c15 & 4654 + stomach cancer & c_estomac & c16 & 5642 + colorectal cancer & c_colorect & c18-c21 & 19587 + liver cancer & c_liver & c22 & 8528 + pancreas cancer & c_pancreas & c25 & 8615 + larynx cancer & c_larynx & c32 & 2062 + lung cancer & c_lung & c33-c34 & 30642 + breast cancer & c_breast & c50 & 14439 + uterus cancer & c_uterus & c53-c55 & 3708 + prostate cancer & c_prostate & c61 & 13361 + bladder cancer & c_bladder & c67 & 5946 + hodgkin s disease and leukemia & c_hodgkin & c81-c96 & 15574 + secondary malignant neoplasm & c_metasta & c77-c79 & 50314 + other cancers & c_other & c17 , c23-c24 , c26-c31 , c35-c49 , c51-c52 , c56-c60 , c62-c66 , c68-c76 , c80 , c97-d49 & 72943 + diseases of the blood & blood_dis & d50-d89 & 12955 + diabetes & diabetes & e10-e14 & 32704 + malnutrition & malnut & e40-e46 & 12713 + other endocrinal disease & other_endo & e00-e09 , e15-e39 , e47-e90 & 27490 + dementia & dementia & f01-f03 & 22966 + mental disorders due to use of alcohol & alcohol_ment & f10 & 12634 + mood disorders & depression & f30-f39 & 8628 + other mental disorders & other_ment & f00 , f04-f09 , f11-f29 , f40-f99 & 15629 + parkinson s disease & parkinson & g20 & 8598 + alzheimer s disease & alzheimer & g30 & 22568 + other diseases of the nervous system , the eye and adnexa & other_nervous & g00-g19 , g21-g29 , g31-h95 & 27028 + hypertensive diseases & hypertens & i10-i15 & 44117 + ischaemic heart diseases & isc_heart_dis & i20-i25 & 62071 + pulmonary embolism , phlebitis and thrombophlebitis & embolism & i26 , i80-i82 & 16697 + cardiac arrhythmias & card_arrhytm & i47-i49 & 38020 + heart failure & heart_fail & i50 & 73268 + cerebrovascular diseases & cere_vasc & i60-i69 & 58161 + diseases of arteries , arterioles and capillaries & arteries & i70-i79 & 25567 + other diseases of the circulatory system & other_circ & i00-i09 , i16-i19 , i27-i46 , i51-i59 , i83-i99 & 80060 + influenza ( other than avian influenza ) & influenza & j10-j11 & 1192 + pneumonia & pneumo & j12-j18 & 38677 + asthma and status asthmaticus & asthma & j45-j46 & 2886 + other chronic lower respiratory diseases & other_low_resp & j40-j44 , j47 & 17739 + lung diseases due to external agents & lung_extern & j60-j70 & 10137 + other diseases of the respiratory system & other_resp & j00-j09 , j19-j39 , j48-j59 , j71-j99 & 59123 + peptic ulcer & pept_ulcer & k25-k28 & 1965 + diseases of liver & liver_dis & k70-k77 & 20661 + other diseases of the digestive system & other_digest & k00-k24 , k29-k69 , k78-k99 & 34507 + diseases of the skin and subcutaneous tissue & skin_dis & l00-l99 & 11056 + diseases of the musculoskeletal system and connective tissue & musculoskeletal & m00-m99 & 9644 + diseases of the genitourinary system & genito_urinary & n00-n99 & 37683 + pregnancy , childbirth and the puerperium & pregnancy & o00-o99 & 67 + certain conditions originating in the perinatal period & perinat_cond & p00-p96 & 2100 + congenital malformations , deformations and chromosomal abnormalities & congenit_malf & q00-q99 & 2411 + senility & senility & r54 & 23646 + other symptoms and abnormal clinical findings , not elsewhere classified & other_undef & r00-r53 , r55-r59 & 252979 + transport accidents & transp_acc & v01-v99 & 5686 + falls & falls & w00-w19 & 6217 + intentional self - harm & suicide & x60-x84 & 10900 + other external causes of morbidity and mortality & other_extern & w20-x59 , x85-y89 & 21813 +"
  ],
  "abstract_text": [
    "<S> looking for associations among multiple variables is a topical issue in statistics due to the increasing amount of data encountered in biology , medicine and many other domains involving statistical applications . </S>",
    "<S> graphical models have recently gained popularity for this purpose in the statistical literature . following the ideas of the lasso procedure designed for the linear regression framework , </S>",
    "<S> recent developments dealing with graphical model selection have been based on @xmath0-penalization . in the binary case , however , exact inference is generally very slow or even intractable because of the form of the so - called log - partition function . </S>",
    "<S> various approximate methods have recently been proposed in the literature and the main objective of this paper is to compare them . through an extensive simulation study </S>",
    "<S> , we show that a simple modification of a method relying on a gaussian approximation achieves good performance and is very fast . </S>",
    "<S> we present a real application in which we search for associations among causes of death recorded on french death certificates . </S>"
  ]
}