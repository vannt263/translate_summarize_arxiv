{
  "article_text": [
    "in this section we present the gradient of a certain cost function w.r.t . a certain parameter for a system described by the helmholtz equation .",
    "suppose we have a field @xmath8 which adheres to the following equation : @xmath9 where @xmath7 is the local wave number , and @xmath10 is a certain source term which can for instance encode an input vector .",
    "we assume there exists a certain cost functional @xmath11 that we want to minimise by adapting @xmath7 .",
    "it is possible to show ( see the supplementary material ) that the gradient of this cost functional w.r.t .",
    "@xmath12 is proportional to : @xmath13 here , @xmath14 is a second complex field ( which can be interpreted as the ` error ' signal ) , which adheres to the following equation : @xmath15 with @xmath16 this means that the information for the local gradient can be obtained by combining information from two complex fields , both of which are solutions of the wave equation , and therefore possible to generate physically .",
    "+ we can now associate these equations with a trainable neural network block as follows .",
    "first of all , we assume that the source term @xmath10 emerges from an array of emitters , and can be related to an input vector @xmath3 as follows : @xmath17 with @xmath18 the @xmath19-th element of @xmath3 of size @xmath20 , and @xmath21 a source field associated with the @xmath19-th emitter .",
    "next we assume that there exists an array of @xmath22 receivers which make measurements the complex field @xmath8 to generate an output vector @xmath23 of dimensionality @xmath22 , where each element @xmath24 is described by : @xmath25 with @xmath26 a function describing the properties of the @xmath19-th receiver . here",
    "it is important to note that  due to the fully linear nature of the entire system  the output vector @xmath23 can be written as @xmath27 , i.e. , there exists a matrix @xmath0 , determined by the combination of the wave number function @xmath7 , the boundary conditions , and the emitter - receiver functions @xmath21 and @xmath26 . in this sense , the whole system acts as an implicit mvm .",
    "+ in the common neural network training scheme , there will be a certain cost vector @xmath1 associated with the output @xmath23 , which represents the gradient of an external cost function w.r.t .",
    "the individual elements of @xmath23 . we can relate this with the cost functional @xmath11 by explicitly writing the dependencies : @xmath28 ( note that @xmath23 by definition does not depend on @xmath29 ) .",
    "therefore , the functional derivative @xmath30 can be written as @xmath31 where @xmath32 are the individual elements of @xmath1 .",
    "note that this equation takes on a very similar form as equation .",
    "indeed , when we can use the emitters and receivers in two directions i.e. , when they act as transducers ) , it is possible to simply _ emit _ the error signal into the system .       and",
    "@xmath33 , emitted from the left and right , respectively . , scaledwidth=95.0% ]    in this section we will apply the previously derived concepts to a numerical examples . we will train a slab of material to perform a matrix - vector product of the input vector @xmath3 with a pre - specified matrix @xmath0 . as we wish to use the classic neural network training mechanism",
    ", we will do this by drawing examples of input and desired output , and as a cost function we will use the squared error between the actual and desired output .",
    "suppose we draw an input vector @xmath3 , with complex elements where the real and imaginary parts have been drawn from a standard normal distribution .",
    "using the previously defined ideas , this is then sent into a medium encoded as waves , and converted back into an output vector @xmath23 .",
    "if we define @xmath34 as the `` target '' output , the cost function is given by @xmath35 we find that the derivative @xmath36 is given by @xmath37 note that the gradient found for a single input / output example pair will not suffice to make the system emulate @xmath0 .",
    "rather , we will use an iterative optimisation scheme such that at the @xmath38-th iteration we update @xmath7 as follows : @xmath39 with @xmath40 the learning rate , and @xmath41 the gradient obtained using input - output pair sample @xmath42 and @xmath43 .",
    "such an optimisation scheme is the equivalent of stochastic gradient descent used in neural network training , where each training iteration only a small fraction of the data is presented to the network . in our case",
    "this is the extreme case of a single data point per iteration .",
    "+ figure [ fig : planar_results](a ) shows a schematic depiction of the simulated setup .",
    "further details of the experiment are explained in the supplementary material .",
    "the results are displayed in figure [ fig : planar_results ] .",
    "the medium adapts itself successfully to perform the required linear transformation between input and output fields , as can be seen from figure [ fig : planar_results](b ) , which shows the evolution of the nrmse as a function of the number of training iterations .",
    "the sudden peak at around the @xmath44-th iteration likely has to do with poor convergence of the method we used to compute the complex fields .",
    "we also found that the lowest nrmse the system can reach during training depends on the precision tolerance used in this computation .",
    "the more precise the complex fields are computed , the lower the nrmse gets .",
    "+ figure [ fig : planar_results](a ) depicts the structure of the simulated device , where we plotted the varying refractory index as a function of space .",
    "we have included the scaling of the colour coding , which shows the relative magnitude of the variations w.r.t . the average ( equal to one ) .",
    "as can be seen , these variations remain relatively small , staying within 10 % of the average value .",
    "figure [ fig : planar_results](c ) shows examples of the complex fields of @xmath45 and @xmath33 .      here",
    "we will detail the requirements to implement a trainable mvm using optics .",
    "we will assume the trainable medium consists of a planar material of which the refractory index can be modulated spatially , for example by using the pockels effect in indium phosphide or lithium niobate , by using free carrier dispersion in semiconductors , or by the thermo - optic effect , present in many materials including silicon .",
    "+ in many cases , the maxwell equations in a two - dimensional structure can be reduced to a scalar wave equation , which makes optics a viable implementation platform for a trainable mvm . + looking at the expression for the gradient ( equation ) , one would need to make a measurement of the real part of the product of two complex fields , over the entire region where one wishes to modulate the refractory index .",
    "this requires a system - wide interferometric measurement of the complex fields @xmath45 and @xmath33 , such that we can directly measure their respective real and imaginary parts . in a planar material",
    ", this is  at least in principle  possible , as we could access the evanescent field at any location . in reality , however , this would pose a significant challenge , as for interferometry we would need to have access to a reference signal at every location too . even when we succeed in separately measuring @xmath46 , @xmath47 , @xmath48 , and @xmath49 , we would still need to compute the quantity @xmath50 for each location , greatly increasing the device s complexity . +",
    "an alternative way of obtaining the gradient is to generate the complex conjugate field of either @xmath45 or @xmath33 .",
    "if we are capable to do so , we could make three system - wide field intensity measurements ( which are far simpler than interferometric measurements ) as follows : @xmath51 with @xmath52 indicating the intensity of the field @xmath53 .",
    "immediately obvious from this expression is that the necessary computations are far simpler ( additions instead of multiplications , which can be performed in an analog fashion easily ) . indeed , using this approach seems more promising than direct measurements of @xmath45 and @xmath33 .",
    "the issue then shifts to the following question : how well are we able to generate the field @xmath29 ( or @xmath54 ) ?",
    "+ generating the complex conjugate of an optical field is a well - studied problem , and has many potential applications in microscopy , lasers , sensors , holography , etc .",
    "especially the concept of complex conjugate mirrors to correct phase distortions has received lots of attention . in our case",
    ", equation shows that we do nt require a complex conjugate mirror , but rather , we d need to generate the complex conjugate field separately ( at a different instant than which the original field is present ) and add this to another optical field .",
    "one possible approach to achieve this would be to construct an optical phased array .",
    "if we could measure the phase front @xmath55 , with @xmath56 consisting of the set of points where light exits the system , _ and _ we are able to regenerate the complex conjugate of this phase front as a source , we can  in principle ",
    "generate @xmath29 and @xmath54 .",
    "there is one important caveat though : there should be no significant internal losses anywhere within the medium ; phase conjugation can only invert phase distortions but not optical losses .",
    "this means that we would need to capture and detect _ all _ the light that exits the system .",
    "internal losses are unfortunately ubiquitous in integrated photonics . in our case",
    "they are even a necessity if we want to measure the optical intensity throughout the trainable medium .          clearly",
    ", bulk material slabs in which light can propagate freely may pose too many challenges .",
    "we need an optical system that offers more control over the complex field that is generated within . for this ,",
    "let us consider optical circuits that consist of single - mode waveguides , which greatly simplifies the conceptual description of the optical field ; the optical signal present in each waveguide can be described by a single complex number ( when considering one direction of travel ) .",
    "note that optical circuits simply are defined by the refractory index of the system varying in space .",
    "this means that the theory that has been presented in the first part of this paper can be applied to such systems too .",
    "+ let us consider the aforementioned theory on an ideal waveguide .",
    "equation tells us how we can express the gradient w.r.t . the local refractory index as a function of intensities . in the case of a waveguide",
    ", @xmath8 can represent a guided mode with light traveling in one directional mode which we could consider the ` forward ' direction .",
    "the field @xmath14 will represent a guided mode traveling in the opposite direction , i.e. , ` backwards ' .",
    "its complex conjugate , @xmath57 is reversed in direction , and therefore corresponds to a guided mode traveling also traveling forward , in the same direction as @xmath8 .",
    "if we consider the intensity @xmath58 , throughout the waveguide , this is the intensity of a guided mode , unchanging in the direction of travel .",
    "therefore , the gradient one obtains within a single waveguide is uniform throughout its length .",
    "it follows that the primary effect of adapting the refractory index will be to change the phase shift induced by this waveguide .",
    "it also follows that it suffices to measure light intensities at a single location within the waveguide .",
    "tuneable phase shifters are an integral part of integrated photonics , and can be implemented in several ways ( mechanically , electro - optically , or thermo - optically ) .      we start from the idea that we have an optical chip with @xmath20 input waveguides and @xmath20 output waveguides . using trainable waveguides as phase modulators , we then consider a structure as represented in figure [ fig : chip](a ) . here ,",
    "the signal alternates between being passed through a set of tuneable waveguides ( phase shifters ) and being transformed with fixed , unitary transformations @xmath59 .",
    "+ the presented structure would need to have coherent detection and generation of signals exclusively at the in- and output sources . generating the conjugate field @xmath29 or @xmath54 would only require the coherent measurement of the field at the receivers ( which is a requirement in any case to produce the output vector @xmath23 ) , and generating the complex conjugate to be sent back into the system .",
    "after this , we need to perform the three measurements mentioned before and measure the light intensities from equation within each waveguide in order to know how to change its refractory index . + to shed some light on the functionality of such a device and foreseeable issues",
    ", we will simulate its training process under successively less ideal circumstances .",
    "note that the device represented in figure [ fig : chip ] will ( if there are no losses ) always perform a unitary transformation from input to output . therefore , we will use a unitary matrix as a target for training .",
    "+ in the supplementary material we explain in detail how the chip is modelled , and how the training is performed . during a single training update , ` signal '",
    "light enters the chip from the left , is transformed by the chip , and is coherently detected at the output ( right side ) . in order to perform the training , we generate the complex conjugate of this light at the right side , add it up to the ` error ' input , and send it backwards into the chip such that we can detect @xmath60 , where @xmath61 stands for the approximation of the complex conjugate field @xmath29 ( the difference is due to losses in the system ) .",
    "next we send in the error signal and the conjugated output signal separately to determine @xmath62 and @xmath63 . in total",
    "this means that three measurements are necessary to obtain the gradient , all based on light propagating through the chip in the backwards direction .",
    "alternatively , we can work from the left side , where we use an approximation for the error field @xmath64 , combined with the normal input field @xmath45 . in case of losses",
    "it is beneficial to do both ( as we show later ) .",
    "practically this can also be performed in three measurements , as we can simply send in light from two directions at once and measure the sum of their intensities ( if we average out the intensity fluctuations caused by standing waves ) .",
    "before we move to numerical results , it is worth considering what form the unitary transforms would take on a chip .",
    "we found that  as long as @xmath59 sufficiently ` mixes ' the states ",
    "it s exact form matters very little for final performance .",
    "with sufficient mixing we mean that each exiting waveguide should contain light of a sufficiently large number of entering waveguides , such that information of each input channel is effectively spread over the chip . in the numerical experiments that follow we constructed the @xmath59 by combining multiple 50/50 directional couplers ( see figure [ fig : chip](b ) for more details ) .          we will simulate the training process of a chip with 50 inputs and 50 outputs .",
    "the details of the experiments are explained in the supplementary material .",
    "we test the following scenarios :    * first of all we consider the ideal , lossless scenario . *",
    "next we assume a 6% power loss in each waveguide / phase shifter , based on assuming we couple out one percent of power for local measurements , and a 5% intrinsic loss ( which was the lowest number we could find in the integrated photonics literature @xcite ) . * we assume the same losses as before , but now we compute two gradients resulting from the two directions light can travel through the chip , and we use the average gradient of that . *",
    "finally we assume uneven losses , where each waveguide loses an additional fraction of power , uniformly and randomly picked between zero and one percent . here",
    "we will use the average gradient again , resulting from two directional modes .",
    "figure [ fig : chip_results ] shows the evolution of the nrmse for each of the four experiments . under ideal circumstances , the chip performs very well and the error goes down to negligible levels . when the chip is lossy , the training process is hampered .",
    "this is mostly due to the fact that power levels decrease by almost a factor 80 ( -19 db ) from entrance to exit , which means that the gradients obtained for phase shifters close to the light source are far larger than those further away . indeed , when we use light coming from both directions this problem is partially eliminated .",
    "note that there are probably more elegant ways to avoid this problem , for instance by having different ` sensitivities ' for different phase shifters at different locations , such that internal losses are compensated completely .",
    "+ finally , uneven losses seem to pose the biggest challenge .",
    "one needs to be careful to interpret this result , however : due to uneven losses , the chip will be intrinsically unable to produce a unitary transformation from input to output , as the intermediate stages no longer are unitary .",
    "this means that the nrmse will always have a certain lower bound given by the uneven losses . to check to",
    "what degree the result is due to this bound or to impairment in the training process , we simply re - measure the nrmse after the training phase on the same ( trained ) chip , but without the uneven losses .",
    "it turns out this reduces the resulting nrmse by a factor about 2 ( @xmath65 ) .",
    "if we do not keep the learning rate fixed throughout training , but let it drop linearly to zero , this even becomes almost a factor 10 ( @xmath66 ) , while there is no difference in final performance for the chip _ with _ the uneven losses .",
    "this shows that uneven losses do not seem to critically endanger the training process .",
    "+ one final observation is that the trained phase shifts have a relatively small standard deviation .",
    "we initialised them at zero , and after training , we observed that nearly all the phase shifts are still within @xmath67 $ ] , i.e. , only covering one fifth of the full @xmath68 range .",
    "this is important as it implies that we do not necessarily need phase shifters which can cover this entire range . indeed , truncating the phase shifts within the aforementioned range does not visibly affect performance in any of the presented experiments .",
    "many more factors need to be taken into account before a trainable mvm using waves may be constructed in practice . for the nanophotonic implementation ,",
    "two important remaining factors we havent yet studied are measurement noise and scalability .",
    "the first is concerned with how a noisy measurement will affect the obtained gradient .",
    "indeed , we do not wish to lose a lot of power in the local power measurements within each waveguide / phase shifter .",
    "this means that the adaptations of the chip will need to be based on measurements of very low optical power , perhaps close to the noise floor .",
    "one factor that works in our advantage is the fact that these power measurements do not need to happen extremely fast .",
    "whereas in typical telecommunication applications photodiodes need to measure optical power at rates well over the gigahertz range , we can work with far lower measurement rates .",
    "suppose for example we allow one microsecond for each of the three required power measurements , this would still allow several hundreds of thousand updates per second , while allowing a relatively long measurement time ( i.e. , time to accumulate energy ) for each phase shifter .",
    "+ scalability in terms of allowable losses pose a more significant challenge .",
    "right now we simulated a chip where the light needs to travel through 70 arrays of phase shifters , losing almost 99 % of power in the process .",
    "scaling the chip up to larger proportions will certainly exacerbate this problem , and one would very quickly reach a situation in which the chip would be unusable .",
    "this means that scalability of nanophotonic trainable chips would hinge completely on the ability of using low - loss components , or potentially on the use of optical amplification that compensates the losses . here",
    ", again , one factor that plays in our advantage is the fact that the mechanism used for phase shifting within the chip does nt need to have a fast response time .",
    "note that in typical ( e.g. , telecom ) applications , speed is more important than losses .",
    "this means that one of the most typical design constraints is lifted , and when developing suitable phase shifters for trainable optics , one can focus on compactness and low losses .",
    "+ the proposed physical optimisation method of this paper may also find applications outside of neural networks .",
    "it could be used more generally to automatically tune optical linear systems if a specified target transformation is not available , but when an error ( a difference between a desired and actual output field ) can be defined .",
    "this may have applications in decoding multimode communication channels ( similar to @xcite ) , when the channel is not reliable ( changing over time ) .",
    "a fast adaptation method based on physically implemented gradient descent may be useful here .",
    "10 url # 1`#1`urlprefix[2]#2 [ 2][]#2    _ et  al .",
    "_ _ * * , ( ) .    , & . in _",
    "_ , ( ) .    , &",
    "_ _ * * , ( ) .    , & .",
    "in _ _ , ( ) .",
    "_ et  al . _ . in _ _",
    "( ) .    , , , & .",
    "_ _ * * ( ) .",
    "_ et  al . _ . _",
    "_ ( ) .    , & _ _ ( , ) .    .",
    "_ _ * * , ( ) .    , , & .",
    "_ _ * * , ( ) .    & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    , &",
    "http://dx.doi.org/10.1117/2.1201306.004932 .",
    "_ et  al . _ . _",
    "_ * * ( ) .",
    "_ et  al . _ . _",
    "_ * * , ( ) .",
    "_ _ * * ( ) .    &",
    "_ _ * * , ( ) .",
    "_ et  al . _ . _",
    "_ * * ( ) .    .",
    "_ _ * * , ( ) .    .",
    "_ _ * * , ( ) .    , , , & .",
    "_ _ ( ) .    , , , & .",
    "in _ _ , ( , ) .    & . _ _ * * , ( ) .    , , & . _ _ * * , ( ) .",
    "_ _ * * , ( ) .    & .",
    "_ * * , ( ) .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    & .",
    "_ _ * * , ( ) .",
    "_ et  al . _ . _",
    "_ * * , ( ) .",
    "_ et  al . _ . _",
    "_ * * , ( ) .",
    "_ _ * * , ( ) .",
    "_ et  al . _ . in _ _ , ( , ) .",
    "et  al . _ . _",
    "_ * * , ( ) .",
    "mh has been supported by the interuniversity attraction poles ( iap ) program of the belgian science policy office ( belspo ) ( iap p7 - 35 `` photonics@be '' ) .",
    "tvv wants to thank the special research fund of ghent university ( bof ) for financial support through a postdoctoral grant .",
    "in this section we explain how we can find the gradient of a certain cost function w.r.t . a certain parameter for a system described by the helmholtz equation .",
    "suppose we have a field @xmath8 which adheres to the following equation : @xmath69 where @xmath70 is a closed domain of space , and @xmath71 is its edge , such that @xmath8 vanishes at the boundary .",
    "the function @xmath10 acts as the source , which in our case will encode the input vector of the mvm we wish to implement .",
    "we now introduce a certain cost functional @xmath72 that we want to minimize .",
    "in particular we wish to adapt the function @xmath7 .",
    "therefore , we are interested in finding the gradient of @xmath11 w.r.t .",
    "normally , we can write : @xmath73 at this point however , care needs to be taken on how to interpret these derivatives . while @xmath11 and @xmath7 are both strictly real variables , @xmath8 is generally complex .",
    "the fact that @xmath11 is strictly real means that it is not an analytic function , and we need to write : @xmath74 , the asterisk @xmath75 indicating the complex conjugate . we need to apply the chain rule separately for @xmath8 and @xmath76 ( using the so - called wirtinger derivatives ) .",
    "we find that @xmath77 since @xmath78 is strictly real , it follows that @xmath79^*.\\ ] ] similarly , we find that @xmath80^*,\\ ] ] such that we can rewrite the equation as : @xmath81 we will use a more compact notation for the first factor : @xmath82 the second factor can be found when we consider equation and as a linear operator @xmath83 , such that we can rewrite them as : @xmath84 where @xmath85 and @xmath86 for @xmath87 . for points on the boundary @xmath88",
    "we define @xmath83 as the identity : @xmath89 , and @xmath90 , such that the condition of equation is absorbed into the definition of @xmath83 , and equation describes both the helmholtz equation and the boundary conditions .",
    "taking the derivative of equation w.r.t .",
    "@xmath7 we find : @xmath91 the right hand side is a functional derivative , and is equal to @xmath92 , with @xmath93 the dirac delta function .",
    "the left hand side is the operator @xmath83 operating on @xmath94 .",
    "therefore , @xmath94 is the solution of the same helmholtz equation with the same boundary conditions , but a different source function .",
    "+ we will use the green s function @xmath95 that acts as the inverse of operator @xmath83 , which allows us to write : @xmath96 as a solution to @xmath97 . when using @xmath98 as source function , this leads to : @xmath99 inserting this in equation yields : @xmath100 note that the arguments of the green s function in the integral have switched places . it can be proven that the green s function associated with equation is self - adjoint , which means that @xmath101 .",
    "therefore we can write : @xmath102 in other words , if we define a second field which adheres to the equation @xmath103 this becomes : @xmath104 both @xmath8 and @xmath14 can be obtained in a physical manner as they are solutions to the same wave equation with different source terms .",
    "+ we can simplify the gradient even further when we assume that @xmath7 only varies slightly around a fixed average value which only has a global scaling effect on the gradient : @xmath105",
    "we use a planar medium with ten sources / receivers , which are simulated as having @xmath106 @xmath107 with @xmath108 and @xmath109 are the locations of the @xmath19-th source and receiver , respectively . the parameter @xmath110 we chose to be approximately equal to half a wavelength in the medium surrounding the sources and receivers .",
    "the spacing between the sources and receivers was slightly over two wavelengths .",
    "+ in between the sources and receivers is the medium of which we spatially modulate the wave number .",
    "each iteration we draw a vector @xmath42 and use this to generate the source @xmath111 .",
    "we compute the complex field @xmath8 , and the output @xmath112 .",
    "next , we redo the simulation with the error source term @xmath113 , and compute the resulting complex field @xmath14 . after both simulations , we compute the gradient and update @xmath7 in the region which was designated as the trainable medium .",
    "+ we numerically solve the respective helmholtz equations using the stabilised biconjugate gradient method . the simulation area",
    "is represented by a rectangular grid of elements , which have a size of roughly one tenth of the wavelength ( for the average wave number of the system ) . to speed up the convergence of the computation of the complex fields we use an absorbing boundary layer , which we implement by adding a gradually increasing imaginary part to the wave number .",
    "note that this is not a necessary element for the physical setup .",
    "we could for instance put the trainable medium between two mirrors , such that less energy is lost between the source and receivers .",
    "+ we used 1000 training iterations .",
    "the matrix @xmath0 was picked to be a random complex matrix , and was next scaled down with a factor 25 .",
    "this scaling was necessary as only a fraction of the total energy of the sources is radiated onto the receivers .",
    "interestingly , when we used much larger scaling factors for @xmath0 , the training algorithm adapts @xmath7 to extreme values in an attempt to reflect as much energy as possible back to the receivers ( eventually leading to the inability to compute the complex fields ) .",
    "the learning rate @xmath40 we picked at a suitable starting value @xmath114 , and we let it drop linearly to zero over the course of the training phase : @xmath115 , with @xmath116 the total number of iterations .",
    "as a qualitative measure of performance of the system at the @xmath117-th iteration we use the normalised root mean square error ( nrmse ) , equal to @xmath118 , where @xmath119 means the average over @xmath38 .",
    "we will obtain gradients using the following expression : @xmath120 which requires the measurement of three intensities , and the generation of the complex conjugate field of either @xmath45 or @xmath33 .",
    "note that there is an important difference between the way light enters a photonic chip , and the theory presented in section [ section : helmholtz ] . here",
    ", we assumed that there exists a source term , which for em radiation would imply an optical antenna ( represented in the maxwell equations by an alternating current with the same frequency as the light ) . in reality , however , light from an external laser source entering a chip is modulated in phase and amplitude , which means there is no internal ` source ' within the chip .",
    "this means that , when we generate the field @xmath33 , we need to modulate the light at the point of measurement _ as if _ it was produced by a source term .",
    "it turns out that this means that we need to multiply the complex field of the ` error ' light entering the chip with a factor @xmath121 .",
    "this can be proven when we apply the theory of section [ section : helmholtz ] to a waveguide with guided modes .",
    "we start by writing the ` output ' field exiting a chip in the form of a guided mode in a waveguide as : @xmath122 where @xmath123 is the normalised transversal field profile of the guided mode , @xmath124 is the effective wave number of that particular guided mode , and @xmath125 is a complex variable that will be the effective output .",
    "note that this implies that @xmath126 is the transversal direction , the waveguide lies according to the @xmath127-axis , and light is traveling in the positive @xmath127-direction .",
    "we assume that the output is generated at @xmath128 by @xmath129 , such that @xmath130 conversely , the source term for the error becomes @xmath131 we assume @xmath33 can be split up into different coordinate parts as well , with the transversal part having the same field profile i.e. : @xmath132 when we insert this into the helmholtz equation with the source term stated above , we obtain : @xmath133 note that @xmath134 , as we assume a waveguide structure which does nt change in the @xmath127-direction . we can eliminate @xmath123 by inserting equation into the homogeneous helmholtz equation .",
    "this leads to : @xmath135 if we insert equation in equation , and divide by @xmath123 , we obtain : @xmath136 the solution to this equation can be found by using the one - dimensional green s function of the helmholtz equation , which yields : @xmath137 we are only concerned with the light going back into the chip , i.e. , for @xmath138 , which indeed represents light propagating into the negative @xmath127-direction and therefore into the chip . in order to ` simulate ' a source with these properties , we need to modulate the incoming light with the error signal @xmath139 , times a factor @xmath121 ( ignoring the scaling factor @xmath140 ) .",
    "when we describe the optical fields at the input waveguides with @xmath3 and those at the output with @xmath23 , the transformation performed by the entire chip can be written as @xmath141{\\mathbf{a}},\\ ] ] where @xmath142 , a diagonal matrix with phase shifts written in vector form as @xmath143 , describing the phase shifts performed by the waveguides after the @xmath19-th unitary transformation of the signal .",
    "concretely , after abstracting the operation of the chip we find that the complex field of the light before entering the @xmath144-th unitary transformation is given by : @xmath145{\\mathbf{a}}\\ ] ] for the error light , entering from the opposite direction , the complex field of the incoming light at the same locations is given by : @xmath146{\\mathbf{e}},\\ ] ] with @xmath147 , the output error .",
    "we wish to apply equation , so we will need to measure the complex state @xmath148 at the point where the original input @xmath3 was inserted , and re - emit its complex conjugate back into the network , added to the original @xmath3 . after all , when the system is completely unitary we can write : @xmath149{\\mathbf{e}}_0^*.\\ ] ] when @xmath59 and @xmath150 are lossy , however , this is no longer true , and we can only generate an approximation of the complex fields : @xmath151{\\mathbf{e}}_0^*.\\ ] ] when we define @xmath152 , this field at the same locations throughout the chip becomes : @xmath153{\\mathbf{h}}\\ ] ] we will denote the gradient for the phase shift @xmath143 with @xmath154 . following equation we can write this gradient as : @xmath155 which correspond to the intensities at the locations of the waveguides . intensity measurements will cause a small loss everywhere , and typically variable phase shifters are lossy too",
    ". let s incorporate this effect ; we assume that @xmath156 , with @xmath157 a loss factor slightly smaller than one . in this case",
    "@xmath158 decays throughout the chip at the same rate as @xmath42 . in the end",
    "this means that , even though the signals lose power throughout the chip , the phase relations between @xmath42 and @xmath158 remain correct .",
    "indeed , in the numerical simulation we show that the resulting gradient is effective in training the conceptual optical chip .",
    "+ note that we can also obtain the gradient using light going in the opposite direction by approximating the complex conjugate field @xmath45 as @xmath159{\\mathbf{o}}^*,\\ ] ] which would give an alternative gradient : @xmath160 @xmath161{\\mathbf{h}}',\\ ] ] and @xmath162 .",
    "indeed , if the chip is lossy it is desirable to take the average of both approximate gradients .",
    "this will even out the effects of losses and perhaps reduce approximation errors .",
    "we will also simulate what happens when the transformations @xmath59 are nt perfectly unitary .",
    "suppose for instance that a certain amount of energy is lost , but a different amount for each output channel .",
    "effectively , the transformation could be written as @xmath163 , with @xmath164 a true unitary matrix and @xmath165 a diagonal matrix with elements smaller than one on the diagonal , which all differ from each other .",
    "this adds another problem to training the chips , as unevenly distributed losses will affect phase relations when attempting to create the field @xmath166 . of course , there are gradations to this issue",
    ". if the differences in losses are only slight , it might only affect the operation to a limited degree , and one would still obtain useful gradients throughout the chip . + we will simulate the training process of a chip with 50 inputs and 50 outputs . to have a sufficient number of trainable parameters we assume @xmath167 phase shifter arrays .",
    "it can be proven that a unitary matrix of size @xmath168 is defined by @xmath169 parameters ( as opposed to a random square complex matrix which has @xmath170 parameters ) .",
    "it is unclear if the chip structure as we presented is theoretically able to emulate all possible @xmath168 unitary matrices if it has only @xmath171 layers of phase shifters ( which would provide the right number of parameters ) . in practice",
    "we find that @xmath172 does not converge to a satisfactory level .",
    "more layers , lead to a much better performance .",
    "we settled for @xmath173 as a tradeoff between performance and additional losses .",
    "+ for each experiment we performed 20,000 training iterations , where as before , each iteration a single input / desired output pair was presented to the simulated chip .",
    "the learning rate @xmath174 we kept fixed during the training , but it was optimised in each of the four experiments to give the best final result .",
    "note that the assumed losses will severely reduce the optical power that exits the chip .",
    "we want to train the chip to perform a unitary transform , however , which would preserve the total power .",
    "therefore we ` compensate ' all losses by using only normalised input vectors and renormalizing the measurements at the output stages ( both the output signal and the transformed error signal @xmath148 ) .",
    "these renormalised signals are then further used , both for training and for determining the nrmse ."
  ],
  "abstract_text": [
    "<S> in this paper we study the concept of using the interaction between waves and a trainable medium in order to construct a matrix - vector multiplier . </S>",
    "<S> in particular we study such a device in the context of the backpropagation algorithm , which is commonly used for training neural networks . here , the weights of the connections between neurons are trained by multiplying a ` forward ' signal with a backwards propagating ` error ' signal . </S>",
    "<S> we show that this concept can be extended to trainable media , where the gradient for the local wave number is given by multiplying signal waves and error waves . </S>",
    "<S> we provide a numerical example of such a system with waves traveling freely in a trainable medium , and we discuss a potential way to build such a device in an integrated photonics chip .     with a matrix @xmath0 , multiply an error @xmath1 with @xmath2 , and is able to adapt its weights based on @xmath3 and @xmath1.,scaledwidth=70.0% ]    the last decade has been marked by a conspicuous renaissance of neural network research and development . a field of study which was once confined largely to theory , has bloomed into a highly applied research domain , which succeeds in tackling intricate problems such as speech recognition @xcite and computer vision @xcite with unrivalled success . </S>",
    "<S> the main driving forces behind this development are the more widespread availability of cheap , high - powered digital processors , the advent of broad - purpose massively parallel computing on gpus , and the availability of very large datasets . </S>",
    "<S> the combination of these allowed researchers to drastically scale up and accelerate their models , in turn allowing an exploration of the limits of their performance . </S>",
    "<S> currently , neural networks ( nns ) outperform all other approaches in a variety of applications @xcite . </S>",
    "<S> trends include the exploration of very challenging linguistic problems such as machine translation @xcite , and the integration of several types of tasks into single systems such as the automatic captioning of pictures @xcite and training agents to play arcade games @xcite . </S>",
    "<S> nns are remarkably successful in these tasks , and the limits of their capabilities seem yet far from being reached . </S>",
    "<S> + nns are still trained using a straightforward , half - century old algorithm called backpropagation @xcite , which computes gradients for the parameters of the networks . a generic neural network structure and </S>",
    "<S> its associated training process is visualised in figure [ fig : nn ] , as well as a representation of a basic , trainable element . </S>",
    "<S> + by far the most computationally intensive part of nn implementations  both in training and during the actual operation  are the required matrix - vector multiplications , i.e. , large - scale linear transformations . increasing the speed and scale of such computations for digital processors always comes with a significant cost in terms of power consumption . therefore </S>",
    "<S> a sizeable research effort has been made in the past to build matrix - vector - multipliers ( mvm ) using analog processors , where a linear transformation is performed through a physical process rather than by explicit digital computation . in this paper </S>",
    "<S> we aim not just at an mvm , but rather at constructing a basic trainable neural network connection layer , as represented in figure [ fig : nn ] . </S>",
    "<S> this means it needs to have the following properties    * bidirectionality : the ability to perform a multiplication with a matrix in one direction , and the transposed matrix in the other . * </S>",
    "<S> trainability : the ability to adapt itself by combining information of the forward signal and the error signal . </S>",
    "<S> ideally this should be a local operation , where no external processing is required .    </S>",
    "<S> note that in this paper we are not concerned with adding a nonlinear function , ( or a multiplication with the local derivative in the backpropagation phase ) , but rather purely focus on have a trainable linear transformation . </S>",
    "<S> if such a device can be constructed , a full neural network can be constructed by adding intermediate nonlinearities .        </S>",
    "<S> optically implemented mvms have been studied extensively @xcite , where very fast matrix - vector multiplications are performed by passing light through a spatial light modulator ( e.g. , a transparent lcd screen , see figure [ fig : slm ] ) . here </S>",
    "<S> , the input vector elements are encoded in an array of light sources ( either as complex amplitudes or as light intensities ) and the matrix elements are encoded as pixel values on a spatial light modulator . such a device ( and its many variants ) works </S>",
    "<S> very well to perform vector matrix multiplications and it is in principle bidirectional , meaning that we simply can pass light through the system in the opposite direction in order to implement a transpose matrix multiplication . </S>",
    "<S> it is bulky , however , relying on free - space optics , and can not easily be integrated into a compact device . </S>",
    "<S> + one integrated - optics mvm has been proposed @xcite where the input vector is encoded in light intensities at different wavelengths , and matrix elements are implemented by modulating the refractory indices of an array of microring resonators . </S>",
    "<S> while such a device is certainly compact , it is not sure how scalable it would be ( the number of wavelengths that can be applied is typically limited by the properties of the waveguides and the ring resonators ) . </S>",
    "<S> furthermore , as signal summation happens in the intensity domain , there is no straightforward way in which the transpose matrix can be implemented , though perhaps such a functionality could be added by changing the design . + a promising and more recent development is the use of arrays of memristor devices @xcite . here , weights are stored as the individual conductances of a large array of memristors in a crosshair circuit . </S>",
    "<S> such a device can be miniaturised easily and has properties that are very desirable for nn applications . </S>",
    "<S> while conductance is a strictly positive quantity , four - quadrant multiplication can be achieved using differential pairs of currents , such that a weight is encoded as a difference in conductance between two memristors . </S>",
    "<S> no existing implementations can yet perform the transpose matrix multiplication , but perhaps future models could incorporate such a functionality . + </S>",
    "<S> all the devices that are discussed above explicitly encode the matrix elements as a two - dimensional array of physical variables , closely adhering to the underlying structure of the problem . in this paper </S>",
    "<S> we will consider an alternative approach , where we drop the one - to - one correspondence to system parameters and matrix elements . </S>",
    "<S> take for example the following situation . </S>",
    "<S> we have a room in which there are a set of acoustic speakers emitting sine waves , all with the same frequency , but with different amplitudes and phases , which can be expressed by a vector of complex amplitudes @xmath3 . on the other side of the room </S>",
    "<S> are a set of microphones . </S>",
    "<S> they will receive a set of signal which we can express as a vector of complex amplitudes @xmath4 . </S>",
    "<S> when we assume no nonlinear effects , the signals are simply linear combinations of the complex amplitudes @xmath5 , i.e. , @xmath6 . </S>",
    "<S> the transition matrix @xmath0 is a function of the shape of the room , the reflection of the walls , the positions of the speakers and microphones , etc . </S>",
    "<S> + such a system is reciprocal ; this means that we can multiply with the transpose of @xmath0 by emitting a signal at the locations of the receivers , and measuring at the location of the original microphones ( see @xcite for more details ) . </S>",
    "<S> generally , however , the relationship between the parameters that describe the properties of the room and the elements of @xmath0 is difficult to model ; it would be a hard task to find the shape of the room which would implement a given @xmath0 . </S>",
    "<S> the problem we will discuss in this paper is a similar one . rather than finding the shape of a room </S>",
    "<S> , we consider a planar structure in which the wave number @xmath7 can be varied locally : for example , the local refractory index of an optical planar wave - guide . the task then becomes : find a function @xmath7 that implements a given linear transformation between the set of emitters and receivers . </S>",
    "<S> + what we will argue in this paper , is that even though finding a set of parameters that directly implements a certain linear transformation @xmath0 is too hard to be practical ( especially in an iterative nn training setup ) , finding the _ gradient _ of these parameters w.r.t . </S>",
    "<S> a certain cost function can be straightforward . </S>",
    "<S> this means that , if we would incorporate such a device in a physical analog implementation of a nn , we are able to _ train _ it in a way which is very similar to how weight matrices are commonly trained in nns . for this we will rely on properties of the wave equation . </S>",
    "<S> we will show that the gradient for a space - dependent parameter ( particularly the wave number @xmath7 ) can be determined by correlating the waves propagating in the forward direction ( which encode a signal ) , and waves propagating in the backwards direction ( encoding an error signal ) . concretely , we show that all the necessary information for determining a local parameter is present at the location where it is physically manifested . </S>",
    "<S> this means that we can adapt the parameters of such a device without needing to process information non - locally , potentially leading to very fast neural hardware . </S>",
    "<S> + a number of devices based on neural networks and implemented in integrated photonics have been presented @xcite . in @xcite </S>",
    "<S> it is argued that one can construct a self - tuning optical device to perform a conversion from one set of orthogonal modes to another with only a minimal need for external computations . </S>",
    "<S> similarly , in @xcite the concept of on - chip , fully tuneable optical transformations is considered , using an array of integrated mach - zehnder interferometers . </S>",
    "<S> both these works use constructive methods to determine the tuneable parameters in the network , which is not applicable to the typical neural network setting that works with pairs of input - output examples . </S>",
    "<S> + the work presented in this paper is essentially a physically implemented version of adjoint optimisation , which is a standard optimisation strategy for complex systems in many industries . </S>",
    "<S> indeed , similar methods as the ones presented in this paper can be used for numerical optimisation of optical components . </S>",
    "<S> interestingly , there seems to be only little attention for such techniques in the photonics literature ( some examples include @xcite ) . </S>",
    "<S> it should be stated that they are in many ways superior to blind optimisation such as , e.g. , the particle swarm algorithm @xcite , which is popular in nanophotonics design @xcite . </S>",
    "<S> + using light itself for error backpropagation for neural network applications has been suggested and analysed before in a specific setup , where the kerr nonlinearity is used to spatially modulate the refractory index of a thin layer of material @xcite . </S>",
    "<S> later the same group provided an experimental demonstration of this concept @xcite , which unfortunately received little attention . </S>",
    "<S> the advent of integrated photonics , and the concepts provided in this paper , will hopefully revive this line of research . </S>"
  ]
}