{
  "article_text": [
    "luttrell s p , 1997 , _ mathematics of neural networks : models , algorithms and applications _ , kluwer , ellacott s w , mason j c and anderson i j ( eds . ) , a theory of self - organising neural networks , 240 - 244 .",
    "* encode then decode : @xmath1 . * @xmath2 = input vector ; @xmath3 = code ; @xmath4 = reconstructed vector .",
    "* code vector @xmath5 for @xmath6 . * @xmath7 = input pdf ; @xmath8 = stochastic encoder ; @xmath9 = stochastic decoder . *",
    "@xmath10 = euclidean reconstruction error .",
    "@xmath11    * do the @xmath12 integration .",
    "* @xmath13 = reconstruction vector .",
    "* @xmath13 is the solution of @xmath14 , so it can be deduced by optimisation .",
    "@xmath15    * @xmath8 implies the components @xmath16 of @xmath3 are conditionally independent given @xmath2 .",
    "* @xmath13 implies the reconstruction is a superposition of contributions @xmath17 for @xmath18 . * the stochastic encoder samples @xmath19 times from the same @xmath20 .",
    "@xmath21    * @xmath22 is a stochastic vector quantiser with the vector code @xmath3 replaced by a scalar code @xmath23 .",
    "* @xmath24 is a non - linear ( note @xmath20 ) encoder with a superposition term @xmath25 .",
    "* @xmath26 : the stochastic encoder measures @xmath20 accurately and @xmath24 dominates .",
    "* @xmath27 : the stochastic encoder samples @xmath20 poorly and @xmath22 dominates .",
    "luttrell s p , 1999 , _ combining artificial neural nets : ensemble and modular multi - net systems _ , 235 - 263 , springer - verlag , sharkey a j c ( ed . ) , self - organised modular neural networks for encoding data .",
    "+ luttrell s p , 1999 , an adaptive network for encoding data using piecewise linear functions , _ proceedings of the 9th international conference on artificial neural networks ( icann99 ) _ , edinburgh , 7 - 10",
    "september 1999 , 198 - 203 .",
    "@xmath29    * stationarity condition is @xmath30 .",
    "@xmath32    * stationarity condition is @xmath33 , subject to @xmath34 . * 3 types of solution : @xmath35 ( trivial ) , @xmath36 ( ensures @xmath37 ) , and @xmath38 .",
    "@xmath39      @xmath40      @xmath41      @xmath42          @xmath43            @xmath44      @xmath45    * @xmath20 depends jointly on @xmath46 and @xmath47 .",
    "* requires @xmath48 to encode @xmath2 . * for a given resolution the size of the codebook increases exponentially with input dimension .",
    "@xmath49    * @xmath50 and @xmath51 are non - intersecting subsets of the allowed values of @xmath23 .",
    "* @xmath20 depends either on @xmath46 or on @xmath47 , but not on both at the same time .",
    "* requires @xmath52 to encode @xmath2 . * for a given resolution the size of the codebook increases linearly with input dimension .          *",
    "fixed @xmath19 , increasing @xmath53 : joint encoding is eventually favoured because the size of the codebook is eventually large enough . * fixed @xmath53 , increase @xmath19 : factorial encoding",
    "is eventually favoured because the number of samples is eventually large enough .",
    "* factorial encoding is encouraged by using a small codebook and sampling a large number of times .",
    "luttrell s p , 1997 , to appear in _ proceedings of the conference on information theory and the brain _ ,",
    "newquay , 20 - 21 september 1996 , the emergence of dominance stripes and orientation maps in a network of firing neurons .",
    "+ luttrell s p , 1997 , _ mathematics of neural networks : models , algorithms and applications _ , kluwer , ellacott s w , mason j c and anderson i j ( eds . ) , a theory of self - organising neural networks , 240 - 244 .",
    "+ luttrell s p , 1999 , submitted to a special issue of _ ieee trans .",
    "information theory on information - theoretic imaging _ ,",
    "stochastic vector quantisers .",
    "@xmath54    * @xmath55 is needed to ensure a valid @xmath20 .",
    "* this does not restrict @xmath20 in any way .",
    "@xmath56    @xmath57    * @xmath58 is the set of neurons that lie in a predefined `` neighbourhood '' of @xmath59 .",
    "* @xmath60 is the `` inverse neighbourhood '' of @xmath23 defined as @xmath61 .",
    "* neighbourhood is used to introduce `` lateral inhibition '' between the firing neurons . *",
    "this restricts @xmath20 , but allows limited range lateral interactions to be used .",
    "@xmath62    * @xmath63 is the amount of probability that leaks from location @xmath59 to location @xmath23 .",
    "* @xmath64 is the `` leakage neighbourhood '' of @xmath59 .",
    "* @xmath65 is the `` inverse leakage neighbourhood '' of @xmath23 defined as @xmath66 . *",
    "leakage is to allow the network output to be `` damaged '' in a controlled way .",
    "* when the network is optimised it automatically becomes robust with respect to such damage .",
    "* leakage leads to topographic ordering according to the defined neighbourhood .",
    "* this restricts @xmath20 , but allows topographic ordering to be obtained , and is faster to train .",
    "@xmath67    * this shorthand notation simplifies the appearance of the gradients of @xmath22 and @xmath24 .",
    "* for instance , @xmath68 .",
    "@xmath69    * the extra factor @xmath70 in @xmath71 arises because there is a @xmath72 hidden inside the @xmath73 .",
    "@xmath75    * differentiate w.r.t .",
    "@xmath74 because @xmath55 .",
    "@xmath76    * this is a standard `` sigmoid '' function . * this restricts @xmath20 , but it is easy to implement , and leads to results similar to the ideal analytic results .",
    "@xmath79            * @xmath80 and @xmath81 were used .",
    "* the reference vectors @xmath28 ( for @xmath82 ) are initialised close to the origin . *",
    "the training history leads to stationary @xmath28 just outside the unit circle .          *",
    "each of the posterior probabilities @xmath20 ( for @xmath82 ) is large mainly in a @xmath83 radian arc of the circle .",
    "* there is some overlap between the @xmath20 .",
    "* @xmath84 and @xmath85 were used , which lies inside the joint encoding region of the stability diagram . *",
    "each of the posterior probabilities @xmath20 is large mainly in a localised region of the torus .",
    "* there is some overlap between the @xmath20 .",
    "* @xmath84 and @xmath86 were used , which lies inside the factorial encoding region of the stability diagram . *",
    "each of the posterior probabilities @xmath20 is large mainly in a collar - shaped region of the torus ; half circle one way round the torus , and half the other way .",
    "* there is some overlap between the @xmath20 that circle the same way round the torus .",
    "* there is a localised region of overlap between a pair of @xmath20 that circle the opposite way round the torus . *",
    "these localised overlap regions are the mechanism by which factorial encoding has a small reconstruction distortion .            *",
    "the targets were unit height gaussian bumps with @xmath87 .",
    "* the additive noise was uniformly distributed variables in @xmath88 $ ] .          * @xmath89 and @xmath81 were used . *",
    "each of the reference vectors @xmath28 becomes large in a localised region .",
    "* each input vector causes a subset of the neurons to fire corresponding to locations of the targets .",
    "* this is a factorial encoder because each neuron responds to only a subspace of the input .            *",
    "the targets were unit height gaussian bumps with @xmath90 .",
    "* @xmath91 and @xmath92 were used . *",
    "each of the reference vectors @xmath28 becomes large in a pair of localised regions .",
    "* each neuron responds to a small range of positions and separations of the pair of targets . *",
    "the neurons respond _ jointly _ to the position and separation of the targets .",
    "* @xmath93 and @xmath94 were used ; this is a 2-stage encoder . *",
    "the second encoder uses as input the posterior probability output by the first encoder . *",
    "the objective function is the sum of the separate encoder objective functions ( with equal weighting given to each ) . *",
    "the presence of the second encoder affects the optimisation of the first encoder via `` self - supervision '' . *",
    "each of the reference vectors @xmath28 becomes large in a single localised region .",
    "* @xmath93 and @xmath95 were used ; this is a 2-stage encoder .",
    "* during training the ratio of the weighting assigned to the first and second encoders is increased from 1:5 to 1:40 . *",
    "each of the reference vectors @xmath28 becomes large in a single broad region .",
    "* each neuron responds only the position ( and not the separation ) of the pair of targets .",
    "* the response of the neurons is _ invariant _ w.r.t .",
    "the separation of the targets .",
    "* this data is the superposition of a pair of waveforms plus noise . * in each training vector the relative phase of the two waveforms is randomly selected .",
    "* @xmath89 and @xmath86 were used . *",
    "each of the reference vectors @xmath28 becomes one or other of the two waveforms , and has a definite phase .",
    "* each neuron responds to only one of the waveforms , and then only when its phase is in a localised range .",
    "* this data is an 8-channel ecg recording taken from a pregnant woman . *",
    "the large spikes are the woman s heart beat .",
    "* the noise masks the foetus heartbeat .",
    "* this data was whitened before training the neural network .",
    "* @xmath91 and @xmath86 were used .",
    "* the results shown are @xmath96 computed for all neurons ( @xmath97 ) for each 8-dimensional input vector @xmath2 .",
    "* after limited training some , but not all , of the neurons have converged . *",
    "the broadly separated spikes indicate a neuron that responds to the mother s heartbeat .",
    "* the closely separated spikes indicate a neuron that responds to the foetus heartbeat .            *",
    "this is a brodatz texture image , whose spatial correlation length is 5 - 10 pixels .",
    "* @xmath98 and @xmath48 were used .",
    "* input window size = @xmath99 , neighbourhood size = @xmath100 , leakage neighbourhood size = @xmath101 were used . *",
    "leakage probability was sampled from a 2-dimensional gaussian pdf , with @xmath102 in each direction . *",
    "each of the reference vectors @xmath28 typically looks like a small patch of image .",
    "* leakage induces topographic ordering across the array of neurons * this makes the array of reference vectors look like an `` orientation map '' .          *",
    "the trained network is used to encode and decode a typical input image . *",
    "left image = input .",
    "* middle image = posterior probability .",
    "this shows `` sparse coding '' with a small number of `` activity bubbles '' . * right image = reconstruction .",
    "apart from edge effects , this is a low resolution version of the input .",
    "* interdigitate a pair of training images , so that one occupies on the black squares , and the other the white squares , of a `` chess board '' .",
    "* preprocess this interdigitated image to locally normalise it using a finite range neighbourhood . *",
    "@xmath103 and @xmath48 were used .",
    "* input window size = @xmath101 , neighbourhood size = @xmath104 , leakage neighbourhood size = @xmath101 were used .",
    "* leakage probability was sampled from a 2-dimensional gaussian pdf , with @xmath102 in each direction . * the dominance stripe map records for each neuron which of the 2 interdigitated images causes it to respond more strongly . *",
    "the dominance stripes tend to run perpendicularly into the boundaries , because the neighbourhood window is truncated at the edge of the array .",
    "luttrell s p , 1997 , to appear in _ proceedings of the conference on information theory and the brain _ ,",
    "newquay , 20 - 21 september 1996 , the emergence of dominance stripes and orientation maps in a network of firing neurons .",
    "+ luttrell s p , 1997 , _ mathematics of neural networks : models , algorithms and applications _ , kluwer , ellacott s w , mason j c and anderson i j ( eds . ) , a theory of self - organising neural networks , 240 - 244 .",
    "+ luttrell s p , 1999 , _ combining artificial neural nets : ensemble and modular multi - net systems _ , 235 - 263 , springer - verlag , sharkey a j c ( ed . ) , self - organised modular neural networks for encoding data .",
    "+ luttrell s p , 1999 , an adaptive network for encoding data using piecewise linear functions , _ proceedings of the 9th international conference on artificial neural networks ( icann99 ) _ , edinburgh , 7 - 10",
    "september 1999 , 198 - 203 .",
    "+ luttrell s p , 1999 , submitted to a special issue of _ ieee trans .",
    "information theory on information - theoretic imaging _ ,",
    "stochastic vector quantisers ."
  ],
  "abstract_text": [
    "<S> the processing of mega - dimensional data , such as images , scales linearly with image size only if fixed size processing windows are used </S>",
    "<S> . it would be very useful to be able to automate the process of sizing and interconnecting the processing windows . </S>",
    "<S> a stochastic encoder that is an extension of the standard linde - buzo - gray vector quantiser , called a stochastic vector quantiser ( svq ) , includes this required behaviour amongst its emergent properties , because it automatically splits the input space into statistically independent subspaces , which it then separately encodes .    </S>",
    "<S> various optimal svqs have been obtained , both analytically and numerically . analytic solutions which demonstrate how the input space is split into independent subspaces may be obtained when an svq is used to encode data that lives on a 2-torus ( e.g. the superposition of a pair of uncorrelated sinusoids ) . </S>",
    "<S> many numerical solutions have also been obtained , using both svqs and chains of linked svqs : ( 1 ) images of multiple independent targets ( encoders for single targets emerge ) , ( 2 ) images of multiple correlated targets ( various types of encoder for single and multiple targets emerge ) , ( 3 ) superpositions of various waveforms ( encoders for the separate waveforms emerge - this is a type of independent component analysis ( ica ) ) , ( 4 ) maternal and foetal ecgs ( another example of ica ) , ( 5 ) images of textures ( orientation maps and dominance stripes emerge ) .    </S>",
    "<S> overall , svqs exhibit a rich variety of self - organising behaviour , which effectively discovers the internal structure of the training data . </S>",
    "<S> this should have an immediate impact on `` intelligent '' computation , because it reduces the need for expert human intervention in the design of data processing algorithms . </S>"
  ]
}