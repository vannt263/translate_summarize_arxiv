{
  "article_text": [
    "humans are adept at a wide array of complicated sensory inference tasks , from recognizing objects in an image to understanding phonemes in a speech signal , despite significant variations such as the position , orientation , and scale of objects and the pronunciation , pitch , and volume of speech .",
    "indeed , the main challenge in many sensory perception tasks in vision , speech , and natural language processing is a high amount of such _ nuisance variation_. nuisance variations complicate perception by turning otherwise simple statistical inference problems with a small number of variables ( e.g. , class label ) into much higher - dimensional problems .",
    "the key challenge in developing an inference algorithm is then _ how to factor out all of the nuisance variation in the input_. over the past few decades , a vast literature that approaches this problem from myriad different perspectives has developed , but the most difficult inference problems have remained out of reach .",
    "recently , a new breed of machine learning algorithms have emerged for high - nuisance inference tasks , achieving super - human performance in many cases .",
    "a prime example of such an architecture is the _ deep convolutional neural network _ ( dcn ) , which has seen great success in tasks like visual object recognition and localization , speech recognition and part - of - speech recognition .",
    "the success of deep learning systems is impressive , but a fundamental question remains : _ why do they work ?",
    "_ intuitions abound to explain their success .",
    "some explanations focus on properties of feature invariance and selectivity developed over multiple layers , while others credit raw computational power and the amount of available training data .",
    "however , beyond these intuitions , a coherent theoretical framework for understanding , analyzing , and synthesizing deep learning architectures has remained elusive .    in this paper , we develop a new theoretical framework that provides insights into both the successes and shortcomings of deep learning systems , as well as a principled route to their design and improvement .",
    "our framework is based on a _ generative probabilistic model that explicitly captures variation due to latent nuisance variables_. the _ rendering mixture model _ ( rmm ) explicitly models nuisance variation through a _ rendering function _ that combines task target variables ( e.g. , object class in an object recognition ) with a collection of task nuisance variables ( e.g. , pose ) . the _ deep rendering mixture model _ ( drmm )",
    "extends the rmm in a hierarchical fashion by rendering via a product of affine nuisance transformations across multiple levels of abstraction .",
    "the graphical structures of the rmm and drmm enable efficient inference via message passing ( e.g. , using the max - sum / product algorithm ) and training via the expectation - maximization ( em ) algorithm",
    ". a key element of our framework is the relaxation of the rmm / drmm generative model to a discriminative one in order to optimize the bias - variance tradeoff .",
    "below , we demonstrate that the computations involved in joint map inference in the relaxed drmm coincide exactly with those in a dcn .",
    "the intimate connection between the drmm and dcns provides a range of new insights into how and why they work and do not work . while our theory and methods apply to a wide range of different inference tasks ( including , for example , classification , estimation , regression , etc . )",
    "that feature a number of task - irrelevant nuisance variables ( including , for example , object and speech recognition ) , for concreteness of exposition , we will focus below on the classification problem underlying visual object recognition .",
    "the proofs of several results appear in the appendix .",
    "* theories of deep learning . * our theoretical work shares similar goals with several others such as the _ i_-theory @xcite ( one of the early inspirations for this work ) , nuisance management @xcite , the scattering transform @xcite , and the simple sparse network proposed by arora et al .",
    "@xcite .",
    "* hierarchical generative models . *",
    "the drmm is closely related to several hierarchical models , including the deep mixture of factor analyzers @xcite and the deep gaussian mixture model @xcite .    like the above models , the drmm attempts to employ parameter sharing , capture the notion of nuisance transformations explicitly , learn selectivity / invariance , and promote sparsity",
    "however , the key features that distinguish the drmm approach from others are : ( i ) the drmm explicitly models nuisance variation across multiple levels of abstraction via a product of affine transformations .",
    "this factorized linear structure serves dual purposes : it enables ( ii ) tractable inference ( via the max - sum / product algorithm ) , and ( iii ) it serves as a regularizer to prevent overfitting by an exponential reduction in the number of parameters .",
    "critically , ( iv ) inference is not performed for a single variable of interest but instead for the full global configuration of nuisance variables .",
    "this is justified in low - noise settings . and",
    "most importantly , ( v ) we can derive the structure of dcns _ precisely _ , endowing dcn operations such as the convolution , rectified linear unit , and spatial max - pooling with principled probabilistic interpretations . independently from our work , soatto et al .",
    "@xcite also focus strongly on nuisance management as the key challenge in defining good scene representations .",
    "however , their work considers max - pooling and relu as _ approximations _ to a marginalized likelihood , whereas our work interprets those operations differently , in terms of max - sum inference in a specific probabilistic generative model .",
    "the work on the number of linear regions in dcns @xcite is complementary to our own , in that it sheds light on the complexity of functions that a dcn can compute .",
    "both approaches could be combined to answer questions such as : how many templates are required for accurate discrimination ?",
    "how many samples are needed for learning ?",
    "we plan to pursue these questions in future work",
    ".    * semi - supervised neural networks . *",
    "recent work in neural networks designed for semi - supervised learning ( few labeled data , lots of unlabeled data ) has seen the resurgence of generative - like approaches , such as ladder networks @xcite , stacked what - where autoencoders ( swwae ) @xcite and many others .",
    "these network architectures augment the usual task loss with one or more regularization term , typically including an image reconstruction error , and train jointly . a key difference with our drmm - based approach",
    "is that these networks do not arise from a proper probabilistic density and as such they must resort to learning the bottom - up recognition and top - down reconstruction weights separately , and they can not keep track of uncertainty .",
    "although we focus on the drmm in this paper , we define and explore several other interesting variants , including the deep rendering factor model ( drfm ) and the evolutionary drmm ( e - drmm ) , both of which are discussed in more detail in @xcite and the appendix .",
    "the e - drmm is particularly important , since its max - sum inference algorithm yields a decision tree of the type employed in a random decision forest classifier@xcite .",
    "the rmm is a _ generative probabilistic model _ for images that explicitly models the relationship between images @xmath1 of the same object @xmath2 subject to nuisance @xmath3 , where @xmath4 is the set of all nuisances ( see fig .",
    "1a for the graphical model depiction ) .",
    "@xmath5 here , @xmath6 is a template that is a function of the class @xmath2 and the nuisance @xmath7 .",
    "the switching variable @xmath8 determines whether or not to render the template at a particular patch ; a sparsity prior on @xmath9 thus encourages each patch to have a few causes .",
    "the noise distribution is from the exponential family , but without loss of generality we illustrate below using gaussian noise @xmath10 .",
    "we assume that the noise is i.i.d . as a function of pixel location @xmath11 and that the class and nuisance variables are independently distributed according to categorical distributions .",
    "( independence is merely a convenience for the development ; in practice , @xmath7 can depend on @xmath2 . ) finally , since the world is spatially varying and an image can contain a number of different objects , it is natural to break the image up into a number of _ patches _ , that are centered on a single pixel @xmath11 .",
    "the rmm described in ( [ eqn : rm ] ) then applies at the patch level , where @xmath2 , @xmath7 , and @xmath9 depend on pixel / patch location @xmath11 .",
    "we will omit the dependence on @xmath11 when it is clear from context",
    ".     has been suppressed for clarity .",
    "( c ) the sparse sum - over - paths formulation of the drmm .",
    "a rendering path contributes only if it is active ( green arrows).,scaledwidth=80.0% ]    [ fig : bigfig ]    * inference in the shallow rmm yields one layer of a dcn . *",
    "[ sec : infrm ] we now connect the rmm with the computations in one layer of a deep convolutional network ( dcn ) . to perform object recognition with the rmm , we must marginalize out the nuisance variables @xmath7 and @xmath9 . maximizing the log - posterior over @xmath3 and @xmath12 and then choosing the most likely class yields the _ max - sum classifier _ @xmath13 that computes the most likely global configuration of target and nuisance variables for the image . assuming that gaussian noise is added to the template , the image is normalized so that @xmath14 , and @xmath15 are uniformly distributed , ( [ eqn : msc ] ) becomes @xmath16 where @xmath17 is the soft - thresholding operation performed by the rectified linear units in modern dcns . here",
    "we have reparameterized the rmm model from the _ moment parameters _",
    "@xmath18 to the _ natural parameters _ @xmath19 .",
    "the relationships @xmath20 are referred to as the _ generative parameter constraints_.    we now demonstrate that _ the sequence of operations in the max - sum classifier in ( [ eqn : cnn1 ] ) coincides exactly with the operations involved in one layer of a dcn _ : image normalization , linear template matching , thresholding , and max pooling .",
    "first , the image is _ normalized _ ( by assumption ) .",
    "second , the image is filtered with a set of noise - scaled rendered templates @xmath21 .",
    "if we assume _ translational invariance _ in the rmm , then the rendered templates @xmath21 yield a _",
    "convolutional _ layer in a dcn @xcite ( see appendix lemma  [ lem : trans - to - dcn - conv ] ) .",
    "third , the resulting activations ( log - probabilities of the hypotheses ) are passed through a pooling layer ; if @xmath7 is a translational nuisance , then taking the maximum over @xmath7 corresponds to _ max pooling _ in a dcn .",
    "fourth , since the switching variables are latent ( unobserved ) , we max - marginalize over them during classification .",
    "this leads to the relu operation ( see appendix proposition  [ prop : detailedreluproof ] ) .",
    "[ subsubsec : drm ]    marginalizing over the nuisance @xmath3 in the rmm is intractable for modern datasets , since @xmath4 will contain all configurations of the high - dimensional nuisance variables @xmath7 . in response",
    ", we extend the rmm into a hierarchical _ deep rendering mixture model _",
    "( drmm ) by factorizing @xmath7 into a number of different nuisance variables @xmath22 at different levels of abstraction .",
    "the drmm image generation process starts at the highest level of abstraction ( @xmath23 ) , with the random choice of the object class @xmath24 and overall nuisance @xmath25 .",
    "it is then followed by random choices of the lower - level details @xmath26 ( we absorb the switching variable @xmath9 into @xmath7 for brevity ) , progressively rendering more concrete information level - by - level ( @xmath27 ) , until the process finally culminates in a fully rendered @xmath28-dimensional image @xmath1 ( @xmath29 ) .",
    "generation in the drmm takes the form : @xmath30 \\\\      \\mu_{c^{(l)}g } & \\equiv \\lambda_{g}\\mu_{c^{(l ) } } \\equiv \\lambda^{(1)}_{g^{(1)}}\\lambda^{(2)}_{g^{(2 ) } } \\cdots \\lambda^{(l-1)}_{g^{(l-1)}}\\lambda^{(l)}_{g^{(l ) } } \\mu_{c^{(l ) } } \\\\",
    "i & \\sim \\mathcal{n}(\\mu_{c^{(l)}g},\\psi \\equiv \\sigma^2 \\mathbf{1}_d ) ,      \\label{eqn : drm}\\end{aligned}\\ ] ] where the latent variables , parameters , and helper variables are defined in full detail in appendix  [ supp : drmm - to - dcn ] .",
    "the drmm is a deep gaussian mixture model ( gmm ) with special constraints on the latent variables . here , @xmath31 and @xmath32 , where @xmath33 is the set of target - relevant nuisance variables , and @xmath34 is the set of all target - irrelevant nuisance variables at level @xmath35 .",
    "the _ rendering path _ is defined as the sequence @xmath36 from the root ( overall class ) down to the individual pixels at @xmath29 .",
    "@xmath37 is the template used to render the image , and @xmath38 represents the sequence of local nuisance transformations that partially render finer - scale details as we move from abstract to concrete .",
    "note that each @xmath39 is an _ affine _ transformation with a bias term @xmath40 that we have suppressed for clarity .",
    "[ fig : bigfig]b illustrates the corresponding graphical model . as before ,",
    "we have suppressed the dependence of @xmath26 on the pixel location @xmath41 at level @xmath35 of the hierarchy .",
    "* sum - over - paths formulation of the drmm .",
    "* we can rewrite the drmm generation process by expanding out the matrix multiplications into scalar products .",
    "this yields an interesting new perspective on the drmm , as each pixel intensity @xmath42 is the sum over all _ active paths _ to that pixel , of the product of weights along that path .",
    "a rendering path @xmath43 is active iff every switch on the path is active i.e. @xmath44 .",
    "while exponentially many possible rendering paths exist , only a very small fraction , controlled by the sparsity of @xmath9 , are active .",
    "[ fig : bigfig]c depicts the sum - over - paths formulation graphically .",
    "* recursive and nonnegative forms .",
    "* we can rewrite the drmm into a recursive form as @xmath45 , where @xmath46 and @xmath47 .",
    "we refer to the helper latent variables @xmath48 as _ intermediate rendered templates_. we also define the _ nonnegative drmm _",
    "( nn - drmm ) as a drmm with an extra nonnegativity constraint on the intermediate rendered templates , @xmath49 $ ] .",
    "the latter is enforced in training via the use of a relu operation in the top - down reconstruction phase of inference . throughout the rest of the paper , we will focus on the nn - drmm , leaving the unconstrained drmm for future work . for brevity , we will drop the nn prefix .",
    "* factor model .",
    "* we also define and explore a variant of the drmm that where the top - level latent variable is gaussian : @xmath50 and the recursive generation process is otherwise identical to the drmm : @xmath51 where @xmath52 .",
    "we call this the _ deep rendering factor model _ ( drfm ) . the drfm is closely related to the spike - and - slab sparse coding model @xcite . below we explore some training results , but we leave most of the exploration for future work .",
    "( see fig .",
    "[ fig : rfm_nn ] in appendix  [ sec : rfm - nn ] for architecture of the rfm , the shallow version of the drfm )    * number of free parameters . * compared to the shallow rmm , which has @xmath53 parameters , the drmm has only @xmath54 parameters , an _ exponential reduction in the number of free parameters _",
    "( here @xmath55 and @xmath56 is the number of units in the @xmath35-th layer with @xmath57 ) .",
    "this enables efficient inference , learning , and better generalization .",
    "note that we have assumed dense ( fully connected ) @xmath58 s here ; if we impose more structure ( e.g. translation invariance ) , the number of parameters will be further reduced . *",
    "bottom - up inference . *",
    "[ sec : infdrm ] as in the shallow rmm , given an input image @xmath1 the drmm classifier infers the most likely global configuration @xmath59 , @xmath60 , @xmath61 by executing the max - sum / product message passing algorithm in two stages : ( i ) bottom - up ( from fine - to - coarse ) to infer the overall class label @xmath62 and ( ii )  top - down ( from coarse - to - fine ) to infer the latent variables @xmath63 at all intermediate levels @xmath35 .",
    "first , we will focus on the fine - to - coarse pass since it leads directly to dcns .    using ( [ eqn : cnn1 ] ) , the _ fine - to - coarse _ nn - drmm inference algorithm for inferring the most likely cateogry @xmath64",
    "is given by @xmath65 here , we have assumed the bias terms @xmath66 . in the second line",
    ", we used the max - product algorithm ( distributivity of max over products i.e. for @xmath67 , @xmath68 ) . see appendix  [ supp : drmm - to - dcn ] for full details .",
    "this enables us to rewrite ( [ eqn : f2c ] ) recursively : @xmath69 where @xmath70 is the output _",
    "feature maps _ of layer @xmath35 , @xmath71 and @xmath72 are the filters / weights for layer @xmath35 . comparing to ( [ eqn : cnn1 ] )",
    ", we see that the @xmath35-th iteration of ( [ eqn : f2c ] ) and ( [ eqn : req ] ) corresponds to feedforward propagation in the @xmath35-th layer of a dcn . _",
    "thus a dcn s operation has a probabilistic interpretation as fine - to - coarse inference of the most probable configuration in the drmm . _    * top - down inference . *",
    "a unique contribution of our generative model - based approach is that we have a principled derivation of a top - down inference algorithm for the nn - drmm ( appendix  [ supp : drmm - to - dcn ] ) .",
    "the resulting algorithm amounts to a simple top - down reconstruction term @xmath73 .",
    "* discriminative relaxations : from generative to discriminative classifiers . *",
    "[ sec : gen - to - discr ] we have constructed a correspondence between the drmm and dcns , but the mapping is not yet complete . in particular , recall the generative constraints on the weights and biases .",
    "dcns do not have such constraints  their weights and biases are free parameters . as a result , when faced with training data that violates the drmm s underlying assumptions , the dcn will have more freedom to compensate . in order to complete our mapping from the drmm to dcns ,",
    "we _ relax _ these parameter constraints , allowing the weights and biases to be free and independent parameters .",
    "we refer to this process as a _ discriminative relaxation of a generative classifier _ ( @xcite , see the appendix  [ sec : discr - relax ] for details ) .",
    "we describe how to learn the drmm parameters from training data via the hard em algorithm in algorithm  [ hardem - rm ] .",
    "@xmath74    the drmm e - step consists of bottom - up and top - down ( reconstruction ) e - steps at each layer @xmath35 in the model .",
    "the @xmath75 are the responsibilities , where for brevity we have absorbed @xmath9 into @xmath7 .",
    "the drmm m - step consists of m - steps for each layer @xmath35 in the model .",
    "the per - layer m - step in turn consists of a responsibility - weighted regression , where @xmath76 denotes the solution to a generalized least squares regression problem that predict targets @xmath77 from predictors @xmath78 and is closely related to the svd .",
    "the iversen bracket is defined as @xmath79 if expression @xmath80 is true and is @xmath81 otherwise .",
    "there are several interesting and useful features of the em algorithm .",
    "first , we note that it is a _ derivative - free alternative to the back propagation algorithm _ for training that is both intuitive and potentially much faster ( provided a good implementation for the gls problem ) .",
    "second , it is easily parallelized over layers , since the m - step updates each layer separately ( model parallelism ) .",
    "moreover , it can be extended to a batch version so that at each iteration the model is simultaneously updated using separate subsets of the data ( data parallelism ) .",
    "this will enable training to be distributed easily across multiple machines .",
    "in this vein , our em algorithm shares several features with the admm - based bregman iteration algorithm in @xcite .",
    "however , the motivation there is from an optimization perspective and so the resulting training algorithm is not derived from a proper probabilistic density .",
    "third , it is far more interpretable via its connections to ( deep ) sparse coding and to the hard em algorithm for gmms .",
    "the sum - over - paths formulation makes it particularly clear that the mixture components are paths ( from root to pixels ) in the drmm .    [ [ g - step . ] ] g - step .",
    "+ + + + + + +    for the training results in this paper , we use the generalized em algorithm wherein we replace the m - step with a gradient descent based g - step ( see algorithm  [ hardem - rm ] ) .",
    "this is useful for comparison with backpropagation - based training and for ease of implementation .",
    "but before we use the g - step , we would like to make a few remarks about the proper m - step of the algorithm , saving the implementation for future work .",
    "* flexibility and extensibility . *",
    "since we can choose different priors / types for the nuisances @xmath7 , the larger drmm family could be useful for modeling a wider range of inputs , including scenes , speech and text .",
    "the em algorithm can then be used to train the whole system end - to - end on different sources / modalities of labeled and unlabeled data . moreover , the capability to sample from the model allows us to probe what is captured by the drmm , providing us with principled ways to improve the model . and finally , in order to properly account for noise / uncertainty , it is possible in principle to extend this algorithm into a _ soft _ em algorithm .",
    "we leave these interesting extensions for future work .",
    "* dcns are message passing networks . *",
    "the drmm inference algorithm is equivalent to performing _ max - sum - product message passing of the drmm _ note that by `` max - sum - product '' we mean a novel combination of max - sum and max - product as described in more detail in the proofs in the appendix .",
    "the factor graph encodes the same information as the generative model but organizes it in a manner that simplifies the definition and execution of inference algorithms @xcite .",
    "such inference algorithms are called _ message passing _ algorithms , because they work by passing real - valued functions called messages along the edges between nodes . in the drmm , the messages sent from finer to coarser levels are in fact the feature maps @xmath70 .",
    "the factor graph formulation provides a powerful interpretation : the _ convolution , max - pooling and relu operations in a dcn correspond to max - sum / product inference in a drmm_. thus , we see that architectures and layer types commonly used in today s dcns can be derived from precise probabilistic assumptions that entirely determine their structure .",
    "the drmm therefore unifies two perspectives  neural network and probabilistic inference ( see table  [ tab : twopovs ] in the appendix for details ) .",
    "* shortcomings of dcns .",
    "* dcns perform poorly in categorizing transparent objects @xcite .",
    "this might be explained by the fact that transparent objects generate pixels that have multiple sources , conflicting with the drmm sparsity prior on @xmath9 , which encourages few sources .",
    "dcns also fail to classify slender and man - made objects @xcite .",
    "this is because of the locality imposed by the locally - connected / convolutional layers , or equivalently , the small size of the template @xmath37 in the drmm . as a result ,",
    "dcns fail to model long - range correlations . * class appearance models and activity maximization .",
    "* the drmm enables us to understand how trained dcns distill and store knowledge from past experiences in their parameters . specifically , the drmm generates rendered templates @xmath37 via a mixture of products of affine transformations , thus implying that _ class appearance models in dcns are stored in a similar factorized - mixture form over multiple levels of abstraction .",
    "_ as a result , it is the product of all the filters / weights over all layers that yield meaningful images of objects ( eq .  [ eqn : drm ] ) .",
    "we can also shed new light on another approach to understanding dcn memories that proceeds by searching for input images that maximize the activity of a particular class unit ( say , class of cats ) @xcite , a technique we call _ activity maximization_. results from activity maximization on a high performance dcn trained on 15 million images is shown in fig .  1 of @xcite .",
    "the resulting images reveal much about how dcns store memories . using the drmm ,",
    "the solution @xmath82 of the activity maximization for class @xmath24 can be derived as the sum of individual activity - maximizing patches @xmath83 , each of which is a function of the learned drmm parameters ( see appendix  [ supp : actmax - proof ] ) : @xmath84 this implies that @xmath82 contains multiple appearances of the same object but in various poses .",
    "each activity - maximizing patch has its own pose @xmath85 , consistent with fig .  1 of @xcite and our own extensive experiments with alexnet , vggnet , and googlenet ( data not shown ) .",
    "such images provide strong confirmational evidence that the underlying model is a mixture over nuisance parameters , as predcted by the drmm .",
    "* unsupervised learning of latent task nuisances . * a key goal of representation learning is to disentangle the factors of variation that contribute to an image s appearance . given our formulation of the drmm , it is clear that dcns are discriminative classifiers that capture these factors of variation with latent nuisance variables @xmath7 . as such ,",
    "the theory presented here makes a clear prediction that _ for a dcn , supervised learning of task targets will lead to unsupervised learning of latent task nuisance variables_. from the perspective of manifold learning , this means that the architecture of dcns is designed to learn and disentangle the intrinsic dimensions of the data manifolds .    in order to test this prediction , we trained a dcn to classify synthetically rendered images of naturalistic objects , such as cars and cats , with variation in factors such as location , pose , and lighting .",
    "after training , we probed the layers of the trained dcn to quantify how much linearly decodable information exists about the task target @xmath24 and latent nuisance variables @xmath7 .",
    "2 ( left ) shows that the trained dcn possesses significant information about latent factors of variation and , furthermore , the more nuisance variables , the more layers are required to disentangle the factors .",
    "this is strong evidence that depth is necessary and that the amount of depth required increases with the complexity of the class models and the nuisance variations .",
    "we evaluate the drmm and drfm s performance on the mnist dataset , a standard digit classification benchmark with a training set of 60,000 @xmath86 labeled images and a test set of 10,000 labeled images .",
    "we also evaluate the drmm s performance on cifar10 , a dataset of natural objects which include a training set of 50,000 @xmath87 labeled images and a test set of 10,000 labeled images . in all experiments , we use a full e - step that has a bottom - up phase and a principled top - down reconstruction phase . in order to approximate the class posterior in the drmm",
    ", we include a kullback - leibler divergence term between the inferred posterior @xmath88 and the true prior @xmath89 as a regularizer @xcite .",
    "we also replace the m - step in the em algorithm of algorithm [ hardem - rm ] by a g - step where we update the model parameters via gradient descent .",
    "this variant of em is known as the generalized em algorithm @xcite , and here we refer to it as eg .",
    "all drmm experiments were done with the nn - drmm .",
    "configurations of our models and the corresponding dcns are provided in the appendix  [ supp : config ] .",
    "* supervised training .",
    "* supervised training results are shown in table [ tab : test - error - sup - unsup ] in the appendix .",
    "_ shallow rfm : _ the 1-layer rfm ( rfm sup ) yields similar performance to a convnet of the same configuration ( 1.21% vs. 1.30% test error ) . also ,",
    "as predicted by the theory of generative vs discriminative classifiers , eg training converges 2 - 3x faster than a dcn ( 18 vs.  40 epochs to reach 1.5% test error , fig .",
    "[ fig : test - error - drm - sup ] , middle ) . _",
    "deep rfm : _ training results from an initial implementation of the 2-layer drfm eg algorithm converges @xmath90 faster than a dcn of the same configuration , while achieving a _ similar _ asymptotic test error ( fig .",
    "[ fig : test - error - drm - sup ] , right ) . also , for completeness , we compare supervised training for a 5-layer drmm with a corresponding dcn , and they show comparable accuracy ( 0.89% vs 0.81% , table [ tab : test - error - sup - unsup ] ) .     [",
    "fig : entanglement ]     [ fig : test - error - srm - sup ]     [ fig : semisupervised ]    * unsupervised training . * we train the rfm and the 5-layer drmm unsupervised with @xmath91 images , followed by an end - to - end re - training of the whole model ( unsup - pretr ) using @xmath92 labeled images .",
    "the results and comparison to the swwae model are shown in table [ tab : test - error - semi - sup ] .",
    "the drmm model outperforms the swwae model in both scenarios ( filters and reconstructed images from the rfm are available in the appendix  [ tab : test - error - semi - sup - cifar10 ] . )",
    ".comparison of test error rates ( % ) between best drmm variants and other best published results on mnist dataset for the semi - supervised setting ( taken from @xcite ) with @xmath93 unlabeled images , of which @xmath94 are labeled . [ cols=\"<,^,^,^,^ \" , ]     [ tab : test - error - reg ]",
    "in our experiments , configurations of the rfm and 2-layer drfm are similar to lenet5 @xcite and its variants . also , configurations of the 5-layer drmm ( for mnist ) and the 9-layer drmm ( for cifar10 ) are similar to conv - small and conv - large architectures in @xcite , respectively ."
  ],
  "abstract_text": [
    "<S> we develop a probabilistic framework for deep learning based on the _ deep rendering mixture model _ ( drmm ) , a new _ generative probabilistic model _ that explicitly capture variations in data due to latent task nuisance variables . </S>",
    "<S> we demonstrate that max - sum inference in the drmm yields an algorithm that exactly reproduces the operations in deep convolutional neural networks ( dcns ) , providing a first principles derivation . </S>",
    "<S> our framework provides new insights into the successes and shortcomings of dcns as well as a principled route to their improvement . </S>",
    "<S> drmm training via the expectation - maximization ( em ) algorithm is a powerful alternative to dcn back - propagation , and initial training results are promising . classification based on the drmm and other variants </S>",
    "<S> outperforms dcns in supervised digit classification , training 2 - 3@xmath0 faster while achieving similar accuracy . </S>",
    "<S> moreover , the drmm is applicable to semi - supervised and unsupervised learning tasks , achieving results that are state - of - the - art in several categories on the mnist benchmark and comparable to state of the art on the cifar10 benchmark .    </S>",
    "<S> = 1 </S>"
  ]
}