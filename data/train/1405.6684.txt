{
  "article_text": [
    "nowadays , there is a need for efficient data mining techniques .",
    "the human readability of the model is an important factor of a good data mining algorithm . among various data mining methods very popular are decision trees @xcite , @xcite .",
    "although , they have an easy interpretable model , a single tree does not always obtain the highest accuracy . to overcome this problem , various ensemble methods were proposed . among them , the popular is random forest ( rf ) proposed by leo breiman @xcite .",
    "the rf builds a set of trees using bagging and random subspace methods .",
    "the final output is a mode of responses from all individual trees .",
    "the rf can be used for classification and regression tasks . despite the high accuracy of the rf ,",
    "the human readability of the model is lost .",
    "there exist some methods to look inside rf black - box , like : examining variable importance @xcite , parallel cooridinate plots by variable @xcite or visualizing the rf proximity distance matrix with multidimensional scaling @xcite .",
    "herein , we propose a novel method for visualizing the rf proximity matrix based on self - organising maps ( som ) @xcite .",
    "the som is an artificial neural network model that maps high - dimensional input data space onto usually two - dimensional lattice of neurons in an unsupervised way .",
    "although , the som is an originally unsupervised algorithm there exist supervised extensions @xcite ,",
    "@xcite , @xcite , @xcite .",
    "the som has been proved as an efficient data mining tool in many real life applications @xcite , @xcite , @xcite , @xcite . in this paper , we focus on using the labeled som model for mapping the rf used in classification tasks . the rf proximity matrix will be used for the som learning .",
    "it was shown that using more sophisticated distance metric than euclidean can improve the accuracy of the som @xcite .",
    "the rf proximity matrix was used earlier for improving clustering accuracy .",
    "horvath et al .",
    "@xcite presented method for building clusters from the rf learned with unlabeled data and successfully used it for tumor detection @xcite .",
    "moosmann et al .",
    "@xcite used the rf for efficient segmentation of images , where leaves were assigned to distinct image regions rather than to specific class .",
    "gray et al .",
    "@xcite used the rf proximity matrix and the mds for classification of medical images of different types of dementia .",
    "the paper is organized as follow : firstly , we describe the som and the rf algorithms ; secondly , proposed approach for learning the som with the rf proximity matrix is presented ; then , the mds vs the som visualization and the accuracy of the som learned with euclidean metric and the rf proximity matrix are compared .",
    "let s denote data set as @xmath0 , where @xmath1 is an attribute vector , @xmath2 , @xmath3 is attribute vector length and @xmath4 is a discrete class number of _ i_-th sample , @xmath5 $ ] and @xmath6 $ ] .      in this paper",
    ", we used the som as a two - dimensional grid of neurons .",
    "each neuron is represented by a weights vector @xmath7 , where @xmath8 are indices of the neuron in the grid .",
    "it is important to notice , that neuron s weights vector has the same length as sample s attribute vector in the data set .",
    "the neuron s weights directly corresponds to attributes in the data set . in the training phase , for each sample we search for a neuron which is the closest to the _ i_-th sample . in the original som algorithm",
    "the distance is computed with squared euclidean distance by following equation : @xmath9 the neuron @xmath8 with the smallest distance to _ i_-th sample is so - called the best matching unit ( bmu ) , and we note its indicies as @xmath10 . once the bmu is found , the weights update step is executed .",
    "the weights of each neuron are updated with formula : @xmath11 where @xmath12 is an iteration number and @xmath13 is a learning coefficient and @xmath14 is a neighbourhood function .",
    "we assume that one iteration is a presentation of one training sample , whereas a presentation of all training samples is one learning epoch .",
    "the learning coefficient @xmath13 is decreased between consecutive epochs to improve network s ability to remember patterns .",
    "it is described by : @xmath15 where @xmath16 is the initial step size , @xmath17 is the current epoch number and @xmath18 is responsible for regulating the speed of the decrease .",
    "the neighbourhood function controls changing of weights with respect to the distance to the bmu in the grid .",
    "it is noted as : @xmath19 where @xmath20 describes the neighbourhood function width .",
    "this parameter is increasing during learning @xmath21 - it assures that neighbourhood becomes narrower during the training .",
    "the network is trained till chosen number of learning procedure epochs @xmath22 is exceeded .    in the described algorithm",
    "the class label information is not used .",
    "the simplest approach of using the som as a classifier is to label the neurons after the unsupervised training . for each neuron",
    "we remember the overal sum of neighbourhood values @xmath14 from each class over all samples .",
    "the label of major class is assigned to the neuron . in the testing phase ,",
    "the input sample s class is designated based on the class of the found bmu .      in the rf algorithm a set of single trees is built .",
    "the process of constructing one tree can be described in the following steps :    1 .",
    "draw a bootstrap data set @xmath23 by choosing @xmath24 times with replacement from all @xmath25 training samples .",
    "determine a decision at node using only @xmath26 attributes , where @xmath26 is smaller than @xmath3 .",
    "the split is selected based on maximal information gain .",
    "[ dec ] 3 .",
    "move the data through the node with respect to decision from the step [ dec ] .",
    "[ mov ] 4 .",
    "repeat steps [ dec ] , [ mov ] till full tree is grown .    at the end of tree constructing ,",
    "the class label is assigned to each leaf based on a class of samples in it . in the testing phase ,",
    "a new sample is pushed down through all trees . from each tree",
    "a class label is remembered based on class of the reached leaf .",
    "the final response is the mode of votes from all trees .",
    "the proximity matrix @xmath27 , with size @xmath25x@xmath25 , can be easily obtained by putting all samples down the all trees .",
    "if two samples @xmath28 and @xmath29 are in the same terminal node in the tree , their proximity is increased by one @xmath30 .",
    "after the presentation of all samples the proximities are divided by the number of trees in the rf .",
    "the greater proximity value is , the more similar samples are .",
    "the dissimilarity measure can be formulated as @xmath31 .      in the proposed approach",
    "we assume that the rf is already learned .",
    "the learning of the network in one epoch can be summarized in the following steps :    1 .   build a data set @xmath32 as a union of all network s weights @xmath33 and attribute vector @xmath34 of @xmath29-th sample , @xmath35 .",
    "the matrix @xmath33 size is @xmath36x@xmath3 , where the @xmath36 is a total number of neurons in the network . in this matrix",
    "each row contains weights from one neuron , the mapping from neurons s 2d grid to matrix @xmath33 is assumed .",
    "[ fst ] 2 .   for set @xmath32 compute dissimilarity matrix @xmath37 using the rf .",
    "the @xmath37 size is @xmath38x@xmath38 .",
    "3 .   find the smallest distance to the neurons in dissimilarity matrix @xmath37 , in distances corresponding to @xmath29-th sample : @xmath39 where @xmath40 is an index of bmu in the matrix @xmath33 , which can be mapped into @xmath41 indices in the network 2d grid .",
    "[ bmu ] 4 .",
    "update the network weigths with formula [ learning ] .",
    "[ lst ] 5 .",
    "repeat steps [ fst]-[lst ] for all samples in the training set .    after the end of the som learning",
    ", it is labeled as described in section [ som ] . in the testing phase , for input sample",
    "the bmu search is performed by taking the steps [ fst]-[bmu ] .",
    "the output class label is the same as the bmu class .",
    "we will denote the proposed method as rf - som .",
    "the computational complexity of using the euclidean distance in the som is @xmath42 , because we need to compute for each sample , from @xmath25 samples , a distance between sample and @xmath36 neurons . whereas , the using of the rf proximity matrix in the proposed rf - som has complexity @xmath43 , where @xmath44 is a number of nodes in the tree . in the rf - som for each sample",
    "we propagate @xmath45 input vectors through @xmath46 trees in the rf , and passage through a tree has complexity @xmath47 .",
    "the complexity of distance computation using the rf proximity matrix is worse than using euclidean distance .",
    "although , it is beneficial when compared to the memory complexity of the mds used for rf proximity visualization , which uses @xmath48 memory .",
    "the rf - som requires only @xmath49 .",
    "this discards the mds as a method of the rf proximity matrix visualization for large data sets .",
    "we used 6 real data sets to examine properties of the rf - som .",
    "there were used data sets : glass , wine , iris , sonar , ionosphere , pima from the uci machine learning repository @xcite . in the table",
    "[ sets ] are presented data sets properties . in all experiments we used following parameters values for each som type : @xmath50 , @xmath51 , @xmath52 , @xmath53 , @xmath54 .",
    "we will denote a network learned with eculidean distance as som .",
    "the network sizes for the som and the rf - som for each data set are equal , they are presented in the table [ sets ] .",
    "the network sizes were chosen arbitrarily because selecting optimal network size is not in the scope of this paper . in all cases the som and the rf - som starts learning from the same initial weights values .",
    "the rf was constructed with 100 trees and @xmath55 for all data sets .",
    "cm | > m1.7 cm | > m1.7 cm | c | & samples & attributes & classes & network size + & 214 & 9 & 6 & 7x7 + & 178 & 13 & 3 & 4x4 + & 150 & 4 & 3 & 5x5 + & 208 & 60 & 2 & 8x8 + & 351 & 34 & 2 & 8x8 + & 768 & 8 & 2 & 7x7 +    to present visulization properties of the proposed method we used pima data set . in the fig.[mds_som ] there are presented : the som ( fig.[som_pi ] ) and the mds ( fig.[mds_pi ] ) both learned with euclidean distance ; and rf - som ( fig.[rfsom_pi ] ) and rf - mds ( fig.[rfmds_pi ] ) constructed using the rf proximity matrix .",
    "the som networks were presented as a 2d grid of neurons , where for each neuron , its weigths are presented by a polar area diagram ( sometimes called coxcomb plot ) . in the mds and",
    "the rf - mds plots information about points distribution in reduced 2d space is available",
    ". however , information about how point s position is affected by combination of attributes values is missing .",
    "what is more , there is hard to find crisp border to distinguish two classes on the mds neither on the rf - mds .",
    "in contrary to mds technique , the som and rf - som plots do not provide information about explicit point distribution but rather a mapping of attributes combination onto 2d grid of neurons .",
    "after network labeling the class labels are assigned to neurons , therefore distinguishing specific combination of attributes in each class is possible . as expected , although , the som and the rf - som started learning from the same initial weights values they have different final distribution of attributes combinations across the network .",
    "0.49    0.49    0.47        0.47        to compare the accuracy of the som learned with euclidean distance and rf - som learned with rf proximity matrix we use information about samples class label .",
    "we measure the accuracy of classification .",
    "the results of comparison are presented in the table [ results ] .",
    "all of the results are mean over 10-fold cross validation . in the table",
    "[ results ] we also include the accuracy of the alone rf as the reference .",
    "the rf - som on 4 data sets ( glass , sonar , ionosphere , pima ) obtained better results than the som .",
    "the greatest improvement over the som was achived on sonar set , it was @xmath56 .",
    "the same performance of the som and the rf - som was on wine and iris , when on the latter the rf - som has higher standard deviation value .",
    "it is worth noting , that the improvement of the rf - som over the som depends on the accuracy of the rf . on data sets ( like sonar or glass ) where the accuracy difference between the rf and the som is high , the rf - som noted large improvement over the som . whereas , on data sets ( wine and iris ) with small accuracy difference between the rf and the som , the rf - som obtained the same mean accuracy as the som .",
    ".the classification accuracy for rf , som and rf - som .",
    "the results are mean and std . over 10-fold",
    "cross validation . [ cols=\"^,^,^,^,^,^,^,^ \" , ]     we measure classfication accuracy for different number of trees in the rf to examine the influence of a number of trees in the rf to performance of the rf - som . the accuracy for the rf and the rf - som for a number of trees in the rf , @xmath57 , is presented in fig.[diff_rf_som ] .",
    "it can be observed that for all data sets except sonar and glass the accuracy of the rf does not depend on @xmath46 .",
    "the good results of classification are obtained even for 10 trees in the rf . for sonar and",
    "glass sets the accuracy of the rf increases with increasing a number of trees .",
    "the very similar behaviour can be observed for the rf - som . the accuracy for wine",
    ", iris , ionosphere and pima slightly varies with @xmath46 growing .",
    "however , for sonar and glass sets the rf - som obtains better results if more trees are used in the rf .",
    "the observed behaviour can be explained by more complex data relationships in sonar and glass sets which are better modeled with greater number of trees in the rf .",
    "0.49     0.49",
    "the novel method for visualizing the rf proximity matrix by the som was proposed .",
    "the rf - som method uses the rf to compute distances between input sample and neurons .",
    "the proposed method of visualization provide better understanding of relationship between data in the rf structure than the mds .",
    "the rf - som contrary to the mds provides a mapping of data onto 2d neurons grid . in case of new coming samples",
    "there is no need to recompute the whole rf proximity matrix like in the mds method .",
    "additionally , the proposed method has lower memory complexity than the mds , which for large data sets is not applicable .",
    "what is more , the experimental results show that the rf - som learned with the rf dissimilarity gained better or the same accuracy than the som learned with euclidean distance .",
    "as pointed in @xcite the rf dissimilarity has attractive features : it can handle mixed variable types well , is invariant to monotonic transformations of the input variables and is robust to outliers .",
    "the attractiveness of rf dissimilarity and obtained results with rf - som encourage to focus our future work on using the rf proximity matrix in other clustering algorithms .",
    "pp has been supported by the european union in the framework of european social fund through the warsaw university of technology development programme .",
    "gray , k.r . ,",
    "aljabar , p. , heckemann , r.a . , hammers , a. , rueckert , d. : random forest - based manifold learning for classification of imaging data in dementia , lecture notes in computer science , vol.7009 , pp 159 - 166 ( 2011 )                          shi , t. , seligson , d. , belldegrun , a.s . ,",
    "palotie , a. , horvath , s. : tumor classification by tissue microarray profiling : random forest clustering applied to renal cell carcinoma , modern pathology , vol.18 , pp 547557 ( 2005 )"
  ],
  "abstract_text": [
    "<S> random forest ( rf ) is a powerful ensemble method for classification and regression tasks . </S>",
    "<S> it consists of decision trees set . </S>",
    "<S> although , a single tree is well interpretable for human , the ensemble of trees is a black - box model . the popular technique to look inside the rf model is to visualize a rf proximity matrix obtained on data samples with multidimensional scaling ( mds ) method . </S>",
    "<S> herein , we present a novel method based on self - organising maps ( som ) for revealing intrinsic relationships in data that lay inside the rf used for classification tasks . </S>",
    "<S> we propose an algorithm to learn the som with the proximity matrix obtained from the rf . </S>",
    "<S> the visualization of rf proximity matrix with mds and som is compared . </S>",
    "<S> what is more , the som learned with the rf proximity matrix has better classification accuracy in comparison to som learned with euclidean distance . </S>",
    "<S> presented approach enables better understanding of the rf and additionally improves accuracy of the som .    </S>",
    "<S> forest , self - organising maps , visualization , classification , proximity matrix </S>"
  ]
}