{
  "article_text": [
    "we show the validity of equation ( [ eq:19 ] ) . using the same notation as in the text ,",
    "let @xmath121 be the pdf for synapses at time @xmath28 .",
    "then @xmath122 it is useful to observe that , due to the symmetry of our problem , the distribution @xmath121 will be symmetric under permutations of synapses ( provided initial conditions respect the symmetry ) .",
    "it follows that @xmath123 is a function of the only non - trivial invariant for permutations one can build out of @xmath2 binary variables , i.e. @xmath124 .",
    "after standard calculations @xcite , the probability distribution for @xmath27 , @xmath26 , is found to evolve according to @xmath125 where the time - independent kernel @xmath126 is given by @xmath127 the structure of this kernel is , in the limit @xmath15 : @xmath128 where @xmath129 @xmath130 @xmath131 the time evolution for the synaptic distribution is then given by the following equation : @xmath132 as a consequence , in the large @xmath2 limit the integral in ( [ eq:18 ] ) is dominated by the physical saddle point , this means that the evolution operator @xmath133 becomes , in the large @xmath2 limit , a liouville operator , describing a deterministic evolution .",
    "the saddle point is determined by the equations : @xmath134 . after a little algebra",
    ", it turns out that at the saddle point the relation @xmath135 holds . since @xmath126 is ( by construction ) normalized for every @xmath2 ,",
    "also the limiting kernel , as @xmath2 goes to infinity , will be normalized : we can then conclude that the limiting kernel is given by @xmath136 , where @xmath30 .",
    "figure 1 : the dashed lines represent the @xmath27-distributions from numerical simulations for @xmath137 ( 1 ) , @xmath138 ( 2 ) , @xmath139 ( 3 ) , @xmath88 ( 4 ) , to be compared with the invariant distribution of ( 4 ) , here represented by the solid line .",
    "the case @xmath82 , @xmath83 and @xmath84 is here considered .",
    "figure 2 : in the plane @xmath90 of parameters ( see the text ) , the transition lines between the ferro and paramagnetic phases are depicted , for @xmath140 ( continuous line ) , @xmath141 ( dashed line ) and @xmath142 ( dotted line ) ."
  ],
  "abstract_text": [
    "<S> we consider the dynamics of diluted neural networks with clipped and adapting synapses . unlike previous studies , the learning rate is kept constant as the connectivity tends to infinity : the synapses evolve on a time scale intermediate between the quenched and annealing limits and all orders of synaptic correlations must be taken into account . </S>",
    "<S> the dynamics is solved by mean - field theory , the order parameter for synapses being a function . </S>",
    "<S> we describe the effects , in the double dynamics , due to synaptic correlations .    </S>",
    "<S> 6.5 in 8.8 in -0.3 in    diluted neural networks with adapting and correlated synapses     +   + massimo mannarelli , giuseppe nardulli , and sebastiano stramaglia +   + _ dipartimento di fisica , i.n.f.n . </S>",
    "<S> sezione di bari + via amendola 173 , 70126 bari , italy _ </S>",
    "<S> +   +   +    pacs numbers : 87.10.+e , 05.20.-y    .5 cm    in the past years , many models with a coupled dynamics of fast ising spins and slow interactions have been studied to understand the simultaneous learning and retrieval in recurrent neural networks @xcite . a major approach to this problem </S>",
    "<S> is replica mean - field theory with the replica number being the ratio of two temperatures characterizing the stocasticity in the spin dynamics and the interaction dynamics , respectively @xcite . recently </S>",
    "<S> this approach has been used to study coupled dynamics in the xy spin glass @xcite ; the generalization of these ideas @xcite to the case of a hierarchy of subsystems with different characteristic time - scales , in the sherrington - kirckpatrick model , interestingly leads to parisi s solution @xcite . </S>",
    "<S> other approaches to coupled dynamics in neural networks are described in @xcite , using a discrete time master equation approach , and in @xcite , exploring temporal learning rules . </S>",
    "<S> stochastic learning rules in diluted neural networks were considered in @xcite : it was shown that in order to preserve the associative memory capability of the network the learning rate @xmath0 must be kept very small ( e.g. , @xmath1 , where @xmath2 is the connectivity ) . moreover , in @xcite the choice of a very small learning rate implied that the correlation between synaptic variables could be neglected so that the dynamics was solved by flow equations for a few number of macroscopic order parameters . </S>",
    "<S> it is the purpose of this work to reconsider coupled dynamics in diluted neural networks and keep the learning rate fixed as the connectivity @xmath2 tends to @xmath3 . </S>",
    "<S> the dynamics of the network , in this limit , can be exactly solved by taking into account all the orders of correlations between synapses , the order parameter for synapses being a function on the interval @xmath4 $ ] . according to the argument in @xcite , the functioning of this model as an associative memory is questionable ; we regard it as a simple model to analyze the effects due to synaptic correlations in the double dynamics .    as in @xcite </S>",
    "<S> we consider a diluted neural network with uni - directional synapses obeying a stochastic learning mechanism @xcite . </S>",
    "<S> the model is made of @xmath5 three states neurons @xmath6 , each connected ( by binary synapses @xmath7 ) to @xmath2 input sites , chosen at random among the @xmath5 sites . </S>",
    "<S> the parallel rule for updating synapses is the following : with probability @xmath0 each synapse @xmath8 assumes the value @xmath9 if this product is not zero ; otherwise the synapse remains unchanged . a parallel stochastic dynamics with inverse temperature @xmath10 is assumed for neurons , where the local field acting on neuron @xmath11 is given by @xmath12 , the sum being over the input neurons . </S>",
    "<S> the coupled dynamics consists in alternate updating of neurons and synapses . </S>",
    "<S> we will consider the limit @xmath13 with @xmath14 : it is well known @xcite that neurons can then be treated as i.i.d . </S>",
    "<S> stochastic variables . </S>",
    "<S> moreover we choose @xmath0 constant as @xmath15 : @xmath0 controls the ratio between the time scales over which neurons and synapses evolve and the _ adiabatic _ approximation is recovered by sending @xmath0 to zero @xcite . as a consequence , in the present case one can not neglect the correlations among synapses .    </S>",
    "<S> let us denote @xmath16 , @xmath17 , ..... , @xmath18 the input neurons and @xmath19 , @xmath20 , .... , @xmath21 the set of @xmath2 input synapses for a given neuron @xmath22 ( due to the translational symmetry the following reasoning holds for an arbitrary @xmath22 ) .    </S>",
    "<S> we start considering the following simple situation : the synapses being independently updated by the transition matrix : @xmath23 , where the transition matrix for the single synapse is the following :    @xmath24 a good order parameter for synapses is @xmath25 $ ] . indeed , denoting with @xmath26 the pdf for @xmath27 at time @xmath28 </S>",
    "<S> , one can demonstrate ( see the appendix ) that in the large @xmath2 limit the evolution of @xmath27 is ruled by a deterministic liouville operator : @xmath29 with @xmath30 . </S>",
    "<S> the moments of @xmath31 provide the synaptic correlations : @xmath32 where the synapses @xmath33 , @xmath34 , @xmath35 are all different . </S>",
    "<S> the probability distribution , at time @xmath28 , for the local field acting on neuron @xmath22 is @xmath36 , \\label{eq:21}\\ ] ] where @xmath37 is @xmath38 , the average magnetization of the neuronic configuration . </S>",
    "<S> we will denote @xmath39 the activity of neurons , satisfying @xmath40 for every time @xmath28 .    </S>",
    "<S> let us now come back to our problem . </S>",
    "<S> due to the synaptic learning rules , the values of @xmath41 and @xmath42 now depend on the value of @xmath22 . if @xmath43 then @xmath44 and @xmath45 . </S>",
    "<S> if @xmath46 then @xmath47 , @xmath48 and @xmath49 . </S>",
    "<S> if @xmath50 then @xmath51 , @xmath52 and @xmath53 . </S>",
    "<S> this implies that even if at time @xmath28 we know @xmath27 exactly ( i.e. , @xmath31 is a @xmath54-function ) , at time @xmath55 @xmath27 is not determined ( @xmath56 will generically be a convex sum of three @xmath54 s ) . the full distribution @xmath57 now plays the role of order parameter for the synaptic variables , the time evolution law being given by a mixture of three liouville operators : @xmath58 @xmath59 is heaviside s function .    </S>",
    "<S> let us now consider the dynamics of neurons . </S>",
    "<S> we assume the following form for the conditional probability for neurons : @xmath60 where @xmath61 is the vector of neurons at time @xmath28 , and @xmath62 controls the mean activity of the network . </S>",
    "<S> the time evolution law for neuronic order parameters is then given by @xmath63 these two equations , together with ( 4 ) and the initial conditions , @xmath64 , @xmath65 and @xmath66 , solve the double dynamics for the present model .    </S>",
    "<S> now we turn to analyze the flow equations . </S>",
    "<S> firstly we consider the case of @xmath67 and @xmath68 being kept constant : @xmath31 tends asymptotically to the invariant distribution @xmath69 of ( 4 ) . </S>",
    "<S> one can easily derive a recurrence formula for the moments of the stationary distribution :    @xmath70    where @xmath71 ( @xmath72 ) is over even ( odd ) positive integers less than or equal to @xmath73 . </S>",
    "<S> the invariant distribution is a @xmath54 -function in the following cases . </S>",
    "<S> if @xmath74 then @xmath75 . if @xmath76 then @xmath77 , and in the adiabatic limit @xmath78 we have @xmath79 . in the general case </S>",
    "<S> the first two cumulants are given by : @xmath80 which is independent of @xmath0 , and @xmath81    the last formula clearly shows how the synaptic correlations are controlled by the learning rate @xmath0 . </S>",
    "<S> for example , in figure 1 the invariant distribution of ( 4 ) , we numerically find , is depicted ( for @xmath82 , @xmath83 , and @xmath84 ) . </S>",
    "<S> we compare it with the @xmath27-distribution , over time , we find simulating a system of @xmath2 synapses , evolving by the stochastic learning mechanism , where neurons @xmath22 and @xmath85 are independently sampled with @xmath86 and @xmath87 at each time step . </S>",
    "<S> the agreement with the theoretical curve increases as @xmath2 grows and it is fairly good already for @xmath88 ( see fig . </S>",
    "<S> 1 ) .    the stationary regime of the coupled dynamics shows a paramagnetic phase with @xmath74 and a ferromagnetic phase with @xmath89 @xcite . by numerical analysis </S>",
    "<S> we find the transition line between the two phases in the @xmath90 plane : in figure 2 our results are shown for some values of @xmath0 . at fixed @xmath62 , the critical temperature decreases as @xmath0 is increased : the synaptic correlations seem to amplify the disordering capability of thermal noise . </S>",
    "<S> the two phases are separated by a first order transition , in agreement with @xcite where the para - ferro transition changes from second to first order as the influence of spins on the couplings dynamics becomes dominant .    </S>",
    "<S> let us now study the role of adapting synapses in the damage spreading phenomenon ( see , e.g. , @xcite ) . for simplicity </S>",
    "<S> we assume two state neurons @xmath91 , and we work in the disordered phase @xmath74 . </S>",
    "<S> we assume the local fields to be : @xmath92    where @xmath93 are random magnetic fields whose gaussian distribution has variance @xmath42 , and the normalization has been chosen differently from the previous case so as to have a non trivial @xmath15 limit in this case . we assume to be at zero temperature and consider two replicas of the system , subject to the same random fields and the same noise in the stochastic learning mechanism . </S>",
    "<S> we introduce the order parameters @xmath94 and @xmath95 defined as follows : @xmath96 is the probability that two corresponding synapses , in the two replicas , are equal , while @xmath97 is the probability that two corresponding neurons , in the two replicas , are equal . as in the previous section , </S>",
    "<S> one easily finds that even if @xmath94 is exactly known at a certain time , it is not determined al later times : it must be described by a probability distribution @xmath98 , whose evolution is given by eq.(4 ) with @xmath99 and @xmath37 replaced by @xmath100 . while keeping fixed @xmath94 , the variables @xmath101 are equal , in the two replicas , with probability @xmath102 . therefore the local fields in the two replicas can be written @xmath103 and @xmath104 , where @xmath105 and @xmath106 are random gaussian variables with variance , respectively @xmath107 and @xmath108 . </S>",
    "<S> one can then easily obtain the time evolution law for @xmath95 : @xmath109 studying damage spreading is equivalent to check the stability of the trivial fixed point @xmath110 and @xmath111 , corresponding to two identical replicas . </S>",
    "<S> we find that , for every finite @xmath42 , damage spreading occurs and a nontrivial fixed point @xmath112 is stable . for low values of @xmath0 the stationary distribution @xmath113 is peaked around its average @xmath114 : approximating the @xmath115 by taylor expansion at the second order around @xmath116 , the equation for the fixed point reads : @xmath117 where @xmath118 at equilibrium .    </S>",
    "<S> the solution @xmath119 of the equation above is the asymptotic correlation between neurons in the two replicas as a function of @xmath0 . in figure 3 </S>",
    "<S> we depict @xmath120 versus @xmath42 . </S>",
    "<S> since we find this quantity to be always positive , it follows that the synaptic correlations act against the damage spreading phenomenon and tend to increase the correlation between the configurations of neurons in the two replicas , as one might intuitively expect .    </S>",
    "<S> we have described an exactly solvable model of double dynamics where synaptic correlations , arising from a stochastic learning mechanism , are important at all orders . </S>",
    "<S> the order parameter for synapses in the mean - field dynamical theory is a function whose evolution is given by a mixture of liouville operators . </S>",
    "<S> the critical temperature for the ferromagnetic transition is found to decrease as the learning rate increases : there is a wide range of temperatures such that the system may order or not depending on the speed at which it adapts , and ordering is asymptotically achieved only if the adaptation is sufficiently slow . </S>",
    "<S> we also outlined the role played by synaptic correlations in the damage spreading phenomenon . </S>"
  ]
}