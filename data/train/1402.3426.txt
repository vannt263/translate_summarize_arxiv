{
  "article_text": [
    "data obfuscation is a mechanism for hiding private data by using misleading , false , or ambiguous information with the intention of confusing an adversary @xcite .",
    "a data obfuscation mechanism acts as a noisy information channel between a user s private data ( secret ) and an untrusted observer @xcite .",
    "the noisier this channel is , the higher the privacy of the user will be .",
    "we focus on _ user - centric _ mechanisms , in which each user independently perturbs her secret before releasing it .",
    "note that we are not concerned with database privacy , but with the privacy issues of releasing a single sensitive data sample ( which however could be continuously shared over time ) .",
    "for example , consider a mobile user who is concerned about the information leakage through her location - based queries .",
    "in this case , obfuscation is the process of randomizing true locations so that the location - based server only receives the user s perturbed locations .    by using obfuscation mechanisms ,",
    "the _ privacy _ of a user and her _ utility _ experience are at odds with each other , as the service that the user receives is a function of what she shares with the service provider .",
    "there are problems to be addressed here .",
    "one is how to design an obfuscation mechanism that protects privacy of the user and imposes a _ minimum _ utility cost .",
    "another problem is how to _ guarantee _ the user s privacy , despite the lack of a single best metric for privacy .    regarding utility optimization , we define utility loss of obfuscation as the degradation of the user s service - quality expectation due to sharing the noisy data instead of its true value .",
    "regarding privacy protection , there are two major metrics proposed in the literature .",
    "_ differential _ privacy limits the information leakage through observation .",
    "but , it does not reflect the absolute privacy level of the user , i.e. , what actually is learned about the user s secret .",
    "so , user would not know how close the adversary s estimate will get to her secret if she releases the noisy data , despite being sure that the relative gain of observation for adversary is bounded .",
    "_ distortion _ privacy ( inference error ) metric overcomes this issue and measures the error of inferring user s secret from the observation .",
    "this requires assumption of a prior knowledge which enables us to quantify absolute privacy , but is not robust to adversaries with arbitrary knowledge .",
    "thus , either of these metrics alone is incapable of capturing privacy as a whole .",
    "the problem of optimizing the tradeoff between privacy and utility has already been discussed in the literature , but notably for differential privacy in the context of statistical databases @xcite . regarding user - centric obfuscation mechanisms ,",
    "@xcite solves the problem of maximizing distortion privacy under a constraint on utility loss .",
    "the authors construct the optimal adaptive obfuscation mechanism as the user s best response to the adversary s optimal inference in a bayesian zero - sum game . in the same context",
    ", @xcite solves the opposite problem , i.e. , optimizing utility but for differential privacy . in both papers ,",
    "the authors construct the optimal solutions using linear programming .",
    "differential and distortion metrics for privacy complement each other .",
    "the former is sensitive to the likelihood of observation given data .",
    "the latter is sensitive to the joint probability of observation and data .",
    "thus , by guaranteeing both , we encompass all the defense that is theoretically possible . in this paper",
    ", we model and solve the optimal obfuscation mechanism that : ( i ) minimizes utility loss , ( ii ) satisfies differential privacy , and ( iii ) guarantees distortion privacy , given a public knowledge on prior leakage about the secrets .",
    "we measure the involved metrics based on separate distance functions defined on the set of secrets .",
    "we model prior leakage as a probability distribution over secrets , that can be estimated from the user s previously released data .",
    "ignoring such information leads to overestimating the user s privacy and thus designing a weak obfuscation mechanism ( against adversaries who include such exposed information in their inference attack ) .",
    "a protection mechanism for distortion privacy metric can be designed such that it is optimal against a particular inference algorithm ( e.g. , bayesian inference @xcite as privacy attacks @xcite ) .",
    "but , by doing so , it is not guaranteed that the promised privacy level can be achieved in practice : an adversarial observer can run inference attacks that are optimally tailored against the very obfuscation mechanism used by the user ( regardless of the algorithm that the user assumes a priori ) .",
    "in fact , the adversary has the upper hand as he infers the user s secret ( private information ) _",
    "after _ observing the output of the obfuscation mechanism .",
    "thus , the obfuscation mechanisms must _ anticipate _ the adaptive inference attack that will follow the observation .",
    "this enables us to design an obfuscation mechanism that is independent of the adversary s inference algorithm .    to address this concern",
    ", we adapt a game - theoretic notion of privacy for designing optimal obfuscation mechanisms against adaptive inference .",
    "we formulate this game as a stackelberg game and solve it using linear programming .",
    "we then add the differential privacy guarantee as a constraint in the linear program and solve it to construct the optimal mechanism .",
    "the result of using such obfuscation mechanism is that , not only the perturbed data samples are indistinguishable from the true secret ( due to differential privacy bound ) , but also they can not be used to accurately infer the secret using the prior leakage ( due to distortion privacy measure ) . to the best of our knowledge",
    ", this work is the first to construct utility maximizing obfuscation mechanisms with such formal privacy guarantees .",
    "we illustrate the application of optimal protection mechanisms on a real data set of users locations , where users want to protect their location privacy against location - based services .",
    "we evaluate the effects of privacy guarantees on utility cost .",
    "we also analyze the robustness of our optimal obfuscation mechanism against inference attacks with different algorithms and background knowledge .",
    "we show that our joint differential - distortion mechanisms are robust against adversaries with optimal attack and background knowledge .",
    "moreover , the utility loss is at most equal to the utility loss of differential or distortion privacy , separately .",
    "the novelty of this paper in the context of user - centric obfuscation is twofold :    * we construct optimal obfuscation mechanisms that provably limit the user s privacy risk ( i.e. , by guaranteeing the user s distortion privacy ) against _ any _ inference attack , with minimum utility cost .",
    "* we design obfuscation mechanisms that optimally balance the tradeoff between utility and joint distortion - differential privacy .",
    "the solution is robust against adversary with arbitrary knowledge , yet it guarantees a required privacy given the user s estimation of the prior information leakage .",
    "this paper contributes to the broad area of research that concerns designing obfuscation mechanisms , e.g. , in the context of quantitative information flow @xcite , quantitative privacy in data sharing systems @xcite , as well as differential privacy @xcite .",
    "the conflict between privacy and utility has been discussed in the literature @xcite .",
    "we build upon prevalent notions of privacy and protect it with respect to information leakage through both observation ( differential privacy ) and posterior inference ( distortion privacy ) while optimizing the tradeoff between utility and privacy .",
    "we also formalize this problem and solve it for user - centric obfuscation mechanisms , where it s each individual user who perturbs her secret data before sharing it with external observers ( e.g. , service providers ) .",
    "the problem of perturbing data for differential and distortion privacy , separately , and optimizing their effect on utility has already been discussed in the literature .",
    "original metric for differential privacy measures privacy of output perturbation methods in statistical databases @xcite . assuming two statistical databases to be neighbor",
    "if they differ only in one entry , @xcite and @xcite design utility maximizing perturbation mechanisms for the case of counting queries . in @xcite , authors propose different approaches to designing perturbation mechanisms for counting queries under differential privacy .",
    "however , @xcite presents some impossibility results of extending these approaches to other types of database queries . under some assumptions about the utility metric",
    ", @xcite shows that the optimal perturbation probability distribution has a symmetric staircase - shaped probability density function . @xcite",
    "extend differential privacy metric using generic distance functions on the set of secrets .",
    "some extensions of differential privacy also consider the problem of incorporating the prior knowledge into its privacy definition @xcite .",
    "the most related paper to our framework , in this domain , is @xcite where the authors construct utility - maximizing differentially private obfuscation mechanisms using linear programming .",
    "the authors prove an interesting relation between utility - maximizing differential privacy and distortion - privacy - maximizing mechanisms that bound utility , when distance functions used in utility and privacy metrics are the same .",
    "this , however , can not guarantee distortion privacy for general metrics . the optimal differentially private mechanisms , in general , do not incorporate the available knowledge about the secret while achieving differential privacy",
    "distortion privacy , which evaluates privacy as the inference error @xcite , is a follow - up of information - theoretic metrics for anonymity and information leakage @xcite .",
    "this class of metrics is concerned with what can be inferred about the true secret of the user by combining the observation ( of obfuscated information ) and prior knowledge .",
    "the problem of maximizing privacy under utility constraint , assuming a prior , is proven to be equivalent to the user s best strategy in a zero - sum game against adaptive adversaries @xcite . with this approach",
    ", one can find the optimal strategies using linear programming .",
    "in fact , linear programming is the most efficient solution for this problem @xcite .",
    "however , if we want to guarantee a certain level of privacy for the user and maximize her utility , the problem can not be modeled as a zero - sum game anymore and there has been no solution for it so far .",
    "we formalize this game , and construct a linear programming solution for these privacy games too .",
    "regarding the utility metric , we consider the expected distance between the observation and the secret as the utility metric @xcite .",
    "the distance function can depend on the user and also the application .",
    "in the case of applying obfuscation over time , we need to update the user s estimation of the prior leakage according to what has been shared by the user  @xcite .",
    "we might also need to update the differential privacy budget over time @xcite . in this paper",
    ", we model one time sharing of a secret , assuming that the prior leakage and the differential privacy budget are properly computed and adjusted based on the previous observations .    our problem",
    "is also related to the problem of adversarial machine learning @xcite and the design of security mechanisms , such as intelligent spam detection algorithms @xcite , against adaptive attackers .",
    "it is also similar to the problem of placing security patrols in an area to minimize the threat of attackers @xcite , and faking location - based queries to protect against localization attack @xcite .",
    "the survey @xcite explores more examples of the relation between security and game theory .",
    "in this section , we define different parts of our model .",
    "we assume a user shares her data through an information sharing system in order to obtain some service ( utility ) .",
    "we also assume that users want to protect their sensitive information , while they share their data with untrusted entities .",
    "for example , in the case of sharing location - tagged data with a service provider , a user might want to hide the exact visited locations , their semantics , or her activities that can be inferred from the visited locations .",
    "we refer to the user s sensitive information as her _",
    "secret_. to protect her privacy , we assume that user obfuscates her data before sharing or publishing it .",
    "figure  [ fig : framework ] illustrates the information flow that we assume in this paper .",
    "the input to the protection mechanism is a secret @xmath0 , where @xmath1 is the set of all possible values that @xmath2 can take ( for example , the locations that the user can visit , or the individuals that she is acquainted with ) .",
    "let prior leakage @xmath3 be the probability distribution over values of @xmath2 to reflect the data model and the a priori exposed information about the secret .",
    "@xmath4    the probability distribution @xmath3 is estimated by the suer to be the predictability of the user s secret given her exposed information in the past .",
    "thus , anytime that user shares some ( obfuscated ) information , she needs to update this probability distribution @xcite .",
    "this is how we incorporate the correlation between users data shared over time .",
    "we assume that a user wants to preserve her privacy with respect to @xmath2 . to protect her privacy",
    ", a user obfuscates her secret @xmath2 and shares an inaccurate version of it through the system .",
    "we assume that this obfuscated data @xmath5 is observable through the system .",
    "we consider a generic class of obfuscation mechanisms , in which the observable @xmath6 is sampled according to the following probability distribution .",
    "@xmath7    thus , we model the privacy preserving mechanism as a noisy channel between the user and the untrusted observer .",
    "this is similar to the model used in quantitative information flow and quantitative side - channel analysis @xcite .",
    "the output , i.e. , the set of observables @xmath8 , can in general be a member of the powerset of @xmath1 . as an example , in the most basic case , @xmath9 , i.e. , the protection mechanism can only perturb the secret by replacing it with another possible secret s value",
    ". this can happen through adding noise to @xmath2 . in a more generic case",
    ", the members of @xmath8 can contain a subset of secrets .",
    "for example , the protection mechanism can generalize a location coordinate , by reducing its granularity .",
    "[ auto = left , scale=0.45 ] ( p ) at ( 1 , 1 ) @xmath10 ; ( ar ) at ( 4 , 1 ) @xmath2 ; ( f ) at ( 7 , 1 ) @xmath11 ; ( or ) at ( 10 , 1 ) @xmath6 ; ( h ) at ( 13 , 1 ) @xmath12 ; ( er ) at ( 16 , 1 ) @xmath13 ; ( dq ) at ( 7 , -2 ) @xmath14 ; ( dp ) at ( 13 , 5 ) @xmath15 ;    \\(p )  ( ar ) ; ( ar ) ",
    "( f ) ; ( f )  ( or ) ; ( or )  ( h ) ; ( h )  ( er ) ;    ( dp ) edge [ < -,bend right ] node ( ar ) edge [ < -,bend left ] node ( er ) ( dq ) edge [ < -,bend left ] node ( ar ) edge [ < -,bend right ] node ( or ) ;      users incur a utility loss due to obfuscation .",
    "let the distance function @xmath14 determine the utility cost ( information usefulness degradation ) due to replacing a secret @xmath2 with an observable @xmath6 .",
    "the cost function is dependent on the application of the shared information , on the specific service that is provided to the user , and also on the user s expectations .",
    "we compute the expected utility cost of a protection mechanism @xmath16 as @xmath17    we can also compute the worst ( maximum ) utility cost over all possible secrets as @xmath18    in this work , we do not plan to determine which metrics are the best representative utility loss metrics for different types of services or users .",
    "we only assume that the designer of optimal obfuscation mechanism is provided with such a utility function , for example , by constructing it according to the application @xcite , or by learning it automatically @xcite from the users preferences and application profile .",
    "we stated that the user wants to protect her privacy with respect to secret @xmath2 against untrusted observers . to be consistent with this",
    ", we define the adversary as an entity who aims at finding the user s secret by observing the outcome of the protection mechanism and minimizing the user s privacy with respect to her privacy sensitivities . for any observation @xmath6",
    ", then we determine the probability distribution over the possible secrets @xmath19 as to be the true secret of the user",
    ". @xmath20    the goal of the inference algorithm @xmath21 is to invert a given protection mechanism @xmath16 to estimate @xmath13 .",
    "the error of adversary , in this estimation process , determines the effectiveness of the inference algorithm , which is captured by the distortion privacy metric .      as stated above ,",
    "the user s privacy and the adversary s inference error are two sides of the same coin .",
    "we define the privacy gain of the user with secret @xmath2 as a distance between the two data points : @xmath22 , where @xmath13 is the a posteriori estimation of the secret @xcite . the distance function @xmath23 is determined by the sensitivity of the user towards each secret @xmath2 when estimated as @xmath13 .",
    "a user would be less worried about revealing @xmath24 , if the portrait of her secret @xmath2 in the eyes of adversary is an estimate @xmath13 with a large distance @xmath15 .",
    "this distance function is defined by the user .",
    "it could be a semantic distance between different values of secrets to reflect the privacy risk of @xmath13 on user when her secret is @xmath2 .",
    "usually , the highest risk is associated with the case where the estimate @xmath13 is equal to the secret @xmath2 .",
    "however , sometimes even wrong estimates can impose a high risk on the user , for example when they leak information about the semantic of the secret .",
    "we compute the user privacy obtained through a protection mechanism @xmath16 , with respect to a given inference algorithm @xmath21 , for a specific secret @xmath2 as @xmath25    by averaging this value over all possible secrets , we compute the expected distortion privacy of the user as @xmath26    this metric shows the average estimation error , or how distorted the reconstructed user s secret is .",
    "thus , we refer to it as the _ distortion _ privacy metric .    what associates a semantic meaning to this metric is the distance function @xmath23 .",
    "many distance functions can be defined to reflect distortion privacy .",
    "this depends on the type of the secret and to the sensitivity of the user .",
    "for example , if the user s secret is her age , function @xmath23 could be the absolute distance between two numbers .",
    "if the secret is the user s location , function @xmath23 could be a euclidean distance between locations , or their semantic dissimilarity .",
    "if the secret is the movies that she has watched , function @xmath23 could be the jaccard distance between two sets of movies .",
    "the privacy that is achieved by an obfuscation mechanism can be computed with respect to the information leakage through the mechanism , regardless of the secret s inference .",
    "for example , the differential privacy metric , originally proposed for protecting privacy in statistical databases @xcite , is sensitive only to the difference between the probabilities of obfuscating multiple secrets to the same observation ( which is input to the attack ) .",
    "according to the original definition of differential privacy , a randomized function @xmath27 ( that acts as the privacy protection mechanism ) provides @xmath28-differential privacy if for all data sets @xmath29 and @xmath30 , that differ on at most one element , and all @xmath31 , the following inequality holds .",
    "differential privacy is not limited to statistical databases .",
    "it has been used in many different contexts where various types of adjacency relations capture the context dependent privacy .",
    "a typical example is edge privacy in graphs @xcite .",
    "it has also been proposed for arbitrary distance function between secrets @xcite .",
    "this notion can simply be used for measuring information leakage @xcite .",
    "it has been shown that differential privacy imposes a bound on information leakage @xcite . and , this is exactly why we are interested in this metric .",
    "let @xmath33 be a distinguishability metric between @xmath34 .",
    "a protection mechanism is defined to be differentially private if for all secrets @xmath34 , where @xmath35 , and all observables @xmath5 , the following inequality holds .",
    "@xmath36    in this paper , we use a generic definition of differential privacy , assuming arbitrary distance function @xmath37 on the secrets @xcite . in this form ,",
    "a protection mechanism is differentially private if for all secrets @xmath34 , with distinguishability @xmath33 , and for all observables @xmath5 , the following holds .",
    "@xmath38    in fact , the differential privacy metric guarantees that , given the observation , there is not enough convincing evidence to prefer one secret to other similar ones ( given @xmath39 ) .",
    "in other words , it makes multiple secret values indistinguishable from each other .",
    "the problem that we address in this paper is to find an optimal balance between privacy and utility , and to construct the protection mechanisms that achieve such optimal points .",
    "more precisely , we want to construct utility - maximizing obfuscation mechanisms with joint differential - distortion privacy guarantees .    the problem is to find a probability distribution function @xmath40 such that it minimizes utility cost of the user , on average , @xmath41 or , alternatively , over all the secrets @xmath42 under the user s privacy constraints .",
    "let @xmath43 be the minimum desired distortion privacy level .",
    "the user s average distortion privacy is guaranteed if the obfuscation mechanism @xmath40 satisfies the following inequality .",
    "@xmath44 where @xmath45 is the optimal inference attack against @xmath40 .",
    "let @xmath46 be the differential privacy budget associated with the minimum desired privacy of the user , and @xmath47 be the distinguishability threshold .",
    "the user s privacy is guaranteed if @xmath40 satisfies the following inequality . @xmath48    or , alternatively ( following @xcite s definition of differential privacy ) : @xmath49    in this paper , we mainly use the latter definition , but make use of the former one as the basis to reduce the computation cost of optimizing differential privacy ( see appendix  [ sec : approx ] ) .",
    "the flow of information starts from the user where the secret is generated .",
    "the user then selects a protection mechanism , and obfuscates her secret according to its probabilistic function .",
    "after the adversary observes the output , he can design an optimal inference attack against the obfuscation mechanism to invert it and estimate the secret .",
    "we assume the obfuscation mechanism is not oblivious and is known to the adversary .",
    "this gives the adversary the upper hand against the user in their conflict .",
    "so , designing an obfuscation mechanism against a fixed attack is always suboptimal .",
    "the best obfuscation mechanism is the one that _ anticipates _ the adversary s attack .",
    "thus , the obfuscation mechanism should be primarily designed against an _ adaptive _ attack which is tailored to each specific obfuscation mechanism .",
    "so , by assuming that the adversary designs the best inference attack against each protection mechanism , the user s goal ( as the defender ) must be to design the obfuscation mechanism that maximizes her ( privacy or utility ) objective against an adversary that optimizes the conflicting objective of guessing the user s secret .",
    "the adversary is an entity assumed by the user as the entity whose objective s exactly the opposite of the user s .",
    "so , we do not model any particular attacker but the one that minimizes user s privacy according to distance functions @xmath23 and @xmath50 .    for each obfuscation mechanism",
    "there is an inference attack that optimizes the adversary s objective and leads to a certain privacy and utility payoff for the user .",
    "the optimal obfuscation mechanism for the user is the one that brings the maximum payoff for her , against the mechanism s corresponding optimal inference attack .",
    "enumerating all pairs of user - attacker mechanisms to find the optimal obfuscation function is infeasible .",
    "we model the joint user - adversary optimization problem as a leader - follower ( stackelberg ) game between the user and the adversary .",
    "the user leads the game by choosing the protection mechanism @xmath16 , and the adversary follows by designing the inference attack @xmath21 .",
    "the solution to this game is the pair of user - adversary best response strategies @xmath40 and @xmath45 which are mutually optimal against each other .",
    "if the user implements @xmath40 , we have already considered the strongest attack @xmath45 against it .",
    "thus , @xmath40 is robust against _ any _ algorithm used as inference attack .    for any secret @xmath0 ,",
    "the strategy space of the user is the set of observables @xmath8 . for any observable @xmath5 ,",
    "the strategy space of the adversary is the set of secrets @xmath1 ( all possible adversary s estimates @xmath19 ) .",
    "for a given secret @xmath0 , we represent a mixed strategy for the user by a vector @xmath51 , where @xmath52 .",
    "similarly , a mixed strategy for the adversary , for a given observable @xmath5 is a vector @xmath53 , where @xmath54 .",
    "note that the vectors @xmath55 and @xmath56 are respectively the conditional distribution functions associated with an obfuscated function for a secret @xmath2 and an inference algorithm for an observable @xmath6 .",
    "let @xmath57 and @xmath58 be the sets of all mixed strategies of the user and the adversary , respectively .",
    "@xmath59    a member vector of sets @xmath57 or @xmath58 with a @xmath60 for the @xmath61th component and zeros elsewhere is the pure strategy of choosing action @xmath61 .",
    "for example , an obfuscation function @xmath55 for which @xmath62 and @xmath63 is the pure strategy of exclusively and deterministically outputting observable @xmath64 for secret @xmath2 .",
    "thus , the set of pure strategies of a player is a subset of mixed strategies of the player .    in the case of the distortion privacy metric",
    ", the game needs to be formulated as a _",
    "bayesian stackelberg game_. in this game , we assume the probability distribution @xmath3 on the secrets and we find @xmath65 and @xmath66 that create the equilibrium point .",
    "if user deviates from this strategy and chooses @xmath67 , there would be an inference attack @xmath68 against it such that @xmath69 leads to a lower privacy for the user , i.e. , @xmath40 is optimal .    in the case of a differential privacy",
    "metric , as the metric is not dependent to the adversary s inference attack , the dependency loop between finding optimal @xmath40 and @xmath45 is broken .",
    "nevertheless , it is still the user who plays first by choosing the optimal protection mechanism . in the following sections , we solve these games and provide solutions on how to design the optimal user - adversary strategies .",
    "assume that the nature draws secret @xmath2 according to the probability distribution @xmath10 .",
    "given @xmath2 , the user draws @xmath6 according to her obfuscation mechanism @xmath70 , and makes it observable to the adversary .",
    "given observation @xmath6 , the adversary draws @xmath13 according to his inference attack @xmath71 .",
    "we assume that @xmath10 is known to both players .",
    "we want to find the mutually optimal @xmath72 : the solution of the bayesian stackelberg privacy game .",
    "to this end , we first design the optimal inference attack against any given protection mechanism @xmath16",
    ". this will be the _",
    "best response _ of the adversary to the user s strategy .",
    "then , we design the optimal protection mechanism for the user according to her objective and constraints , as stated in section  [ sec : problem ] .",
    "this will be the user s best utility - maximizing strategy that anticipates the adversary s best response .",
    "the adversary s objective is to minimize ( the user s privacy and thus ) the inference error in estimating the user s secret . given a secret @xmath2 , the distance function @xmath15 determines the error of an adversary in estimating the secret as @xmath13 .",
    "in fact , this distance is exactly what a user wants to maximize ( or put a lower bound on ) according to the distortion privacy metric .",
    "we compute the expected error of the adversary as @xmath73    therefore , we design the following linear program , through which we can compute the adversary s inference strategy that , given the probability distribution @xmath3 and obfuscation @xmath16 , minimizes his expected error with respect to a distance function @xmath23 .",
    "[ eq : lp : adversary : bayesian ] @xmath74    under the constraint that the solution is a proper conditional probability distribution function .    in the next subsection",
    ", we will show that the optimal deterministic inference ( that associates one single estimate with probability one to each observation ) results in the same privacy for the user .",
    "alternative ways to formulate this problem is given in appendix  [ sec : optimalattack ] .",
    "in this case , we assume the user would like to minimize her utility cost under a ( lower bound ) constraint on her privacy .",
    "therefore , we can formulate the problem as    [ eq : lp : user : utility - privacy : nested ] @xmath75    however , solving this optimization problem requires us to know the optimal @xmath45 against @xmath40 , for which we need to know @xmath40 as formulated in .",
    "so , we have two linear programs ( one for the user and one for the adversary ) to solve .",
    "but , the solution of each one is required in solving the other .",
    "this optimization dependency loop reflects the game - theoretic concept of _ mutual best response _ of the two players .",
    "this game is a _ nonzero - sum stackelberg game _ as the user ( leader player ) and adversary ( follower player ) have different optimization objectives ( one maximizes utility , and the other minimizes privacy ) .",
    "we break the dependency loop between the optimization problems using the game - theoretic modeling , and we prove that the user s best strategy can be constructed using linear programming .    given a probability distribution @xmath3 , the distance functions @xmath23 and @xmath76 , and the threshold @xmath43 , the solution to the following linear program is the optimal protection strategy @xmath40 for the user , which is the solution to with respect to adversary s best response .",
    "[ eq : lp : user : utility - privacy : game ] @xmath77    see appendix  [ sec : proof ] .",
    "in this section , we design optimal differentially private protection mechanisms .",
    "we solve the optimization problems for maximizing utility under privacy constraint .",
    "we design the following linear program to find the user strategy @xmath40 that guarantees user differential privacy , for a maximum privacy budget @xmath46 , and minimizes the utility cost of the obfuscation mechanism .",
    "[ eq : lp : user : utility - privacy : diff : mult ] @xmath78    or , alternatively , for a distinguishability bound @xmath47 , we can solve the following .",
    "[ eq : lp : user : utility - privacy : diff : mult2 ] @xmath79",
    "obfuscation mechanisms designed based on distortion and differential privacy protect the user s privacy from two different angles . in general , for arbitrary @xmath23 and @xmath50 , there is no guarantee that a mechanism with a bound on one metric holds a bound on the other .",
    "distortion privacy metric reflects the _ absolute _ privacy of the user , based on the posterior estimation on the obfuscated information .",
    "differential privacy metric reflects the _ relative _ information leakage of each observation about the secret .",
    "however , it is not a measure on the extent to which the observer , who already has some knowledge about the secret from the previously shared data , can guess the secret correctly .",
    "so , the inference might be very accurate ( because of the background knowledge ) despite the fact that the obfuscation in place is a differentially - private mechanism .    as distortion and differential",
    "metrics guarantee different dimensions of the user s privacy requirements , we respect both in a protection mechanism .",
    "this assures that not only the information leakage is limited , but also the absolute privacy level is at the minimum required level .",
    "thanks to our unified formulation of privacy optimization problems as linear programs , the problem of jointly optimizing and guaranteeing privacy with both metrics can also be formulated as a linear program .",
    "the solution to the following linear program is a protection mechanism @xmath40 that maximizes the user s utility and guarantees a minimum distortion privacy @xmath43 and a minimum differential privacy @xmath46 , given probability distribution @xmath3 and distance functions @xmath76 and @xmath23 and distinguishability metric @xmath39 .",
    "the value of the optimal solution is the utility cost of the optimal mechanism .",
    "[ eq : lp : user : utility - jointprivacy ] @xmath80",
    "we have implemented all our linear program solutions in a software tool that can be used to process data for different applications , in different settings . in this section",
    ", we use our tool to design privacy protection mechanisms , and also to make a comparison between different optimal mechanisms , i.e. , distortion , differential , and joint distortion - differential privacy preserving mechanisms .",
    "we study the properties of these mechanisms and we show how robust they are with respect to inference attack algorithms as well as to the adversary s knowledge on secrets .",
    "we also investigate their utility cost for protecting privacy .",
    "furthermore , we show that the optimal joint distortion - differential mechanisms are more robust than the two mechanisms separately . in appendix",
    "[ sec : approx ] , we discuss and evaluate approximations of the optimal solution for large number of constraints .",
    "we run experiments on location data , as today they are included in most of data sharing applications .",
    "we use a real data - set of location traces collected through the nokia lausanne data collection campaign @xcite .",
    "the location information belong to a @xmath81 km area .",
    "we split the area into @xmath82 cells .",
    "we consider location of a mobile user in a cell as her secret .",
    "hence , the set of secrets is equivalent to the set of location cells .",
    "we assume the set of observables to be the set of cells , so the users obfuscate their location by perturbation ( i.e. , replacing their true location with any location in the map ) .",
    "we run our experiments on @xmath83 randomly selected users , to see the difference in the results due to difference in user s location distribution @xmath3 based on users different location access profiles .",
    "we build @xmath3 for each user separately given their individual location traces , using maximum likelihood estimation ( normalizing the user s number of visits to each cell in the tarce ) .",
    "we assume a euclidean distance function for @xmath23 and @xmath39 .",
    "this reflects the sensitivity of user towards her location . by using this distance function for distortion privacy",
    ", we guarantee that the adversary can not guess the user s true location with error lower than the required privacy threshold ( @xmath43 ) . choosing euclidean distance function as the metric for distinguishability ensures that the indistinguishability between locations is larger for locations that are located closer to each other .",
    "we assume a hamming distortion function for @xmath76 ( i.e. , the utility cost is @xmath84 only if the user s location and the observed location are the same , otherwise the cost is @xmath60 ) .",
    "the utility metric can vary depending on the location - based sharing application and also the purpose for which the user shares her location @xcite . choosing the hamming function reflects the utility requirement of users who want to inform others about their current location in location check - in applications .",
    "we evaluate utility - maximizing optimal protection mechanisms with three different privacy constraints :    * _ distortion privacy protection _ , . * _ differential privacy protection _ , . * _ joint distortion - differential privacy protection _ , .",
    "we compare the effectiveness of these protection mechanisms against inference attacks by using the distortion privacy metric .",
    "we consider two inference attacks :    * _ optimal attack _ , . *",
    "_ bayesian inference attack _ , using the bayes rule : @xmath85      [ [ scenario-1 . ] ] _ scenario 1 . _",
    "+ + + + + + + + + + + + +    our first goal is to have a fair comparison between optimal distortion privacy mechanism and optimal differential mechanism . to this end",
    ", we set the privacy parameter @xmath46 to @xmath86 .    for each user and each value of @xmath46 ,    1 .",
    "we compute the optimal differential privacy mechanism using .",
    "let @xmath87 be the optimal mechanism .",
    "we run optimal attack on @xmath87 , and compute the user s absolute distortion privacy as @xmath88 .",
    "we compute the optimal distortion privacy mechanism @xmath89 using . for this",
    ", we set the privacy lower - bound @xmath43 to @xmath88 .",
    "this enforces the distortion privacy mechanism to guarantee what the differential privacy mechanism provides .",
    "4 .   we compute the optimal joint distortion - differential privacy mechanism @xmath90 using .",
    "we set the privacy lower - bounds to @xmath46 and @xmath43 for the differential and distortion constraints , respectively .",
    "we run optimal attack on both @xmath89 and @xmath90 , and compute the user s absolute distortion privacy as @xmath91 and @xmath92 , respectively .",
    "6 .   as a baseline for comparison",
    ", we run bayesian inference attack on the three optimal mechanisms @xmath87 , @xmath89 , and @xmath90 .",
    "figure  [ fig : privacy_loop_optimalattack ] shows the results of our analysis , explained above .",
    "distortion privacy is measured in km and is equivalent to the expected error of adversary in correctly estimating location of users .",
    "figure  [ fig : privacy_loop_epsilon_vs_privacy_optimal ] shows how expected privacy of users @xmath88 decreases as we increase the value of the lower - bound on differential privacy @xmath46 .",
    "users have different secret probability distribution , with different randomness .",
    "however , as @xmath46 increases , expected error of adversary ( the location privacy of users ) converges down to below @xmath60 km .",
    "figure  [ fig : privacy_loop_privacy_vs_cost_optimalattack ] plots the utility cost versus distortion privacy of each optimal protection mechanism .",
    "as we have set the privacy bound of the optimal distortion mechanism ( and of course the optimal joint mechanism ) to the privacy achieved by the optimal differential mechanism , we can make a fair comparison between their utility costs .",
    "we observe that the utility cost for achieving some level of distortion privacy is much higher for optimal differential and joint mechanisms compared with the optimal distortion mechanism .",
    "note that the utility cost of differential and joint mechanisms are the same .",
    "so , distortion privacy bound does not impose more cost than what is already imposed by the differential privacy mechanism .",
    "as we set @xmath43 to @xmath88 , the user s distortion privacy in using optimal distortion and optimal differential mechanism is the same , when we confront them with the optimal attack . in figure",
    "[ fig : privacy_loop_diff_vs_bayes_inferenceattack ] , however , we compare the effectiveness of these two mechanisms against bayesian inference attack . it is interesting to observe that the optimal differential mechanism is more robust to such attacks compared to the optimal distortion mechanisms .",
    "this explains the extra utility cost due to optimal differential mechanisms .    .",
    "]    . ]        in figure  [ fig : privacy_loop_inference_vs_optimal ] , we compare the effectiveness of bayesian inference attack and optimal attack . we show the results for all three optimal protection mechanisms .",
    "it is clear that optimal attack outperforms the bayesian attack , as users have a relatively higher privacy level under the bayesian inference .",
    "however , the difference is more obvious for the case of differential protection and joint protection mechanisms . the bayesian attack overestimates users privacy , as it ignores the distance function @xmath23 , whereas the optimal attack minimizes the expected value of @xmath23 over all secrets and estimates .",
    "[ [ scenario-2 . ] ] _ scenario 2 . _",
    "+ + + + + + + + + + + + +    in this paper , we introduce the optimal joint distortion - differential protection mechanisms to provide us with the benefits of both mechanisms .",
    "figure  [ fig : privacy_loop_privacy_vs_cost_optimalattack ] shows that the optimal joint mechanism is not more costly than the two optimal distortion and differential mechanisms .",
    "it also shows that it guarantees the highest privacy for a certain utility cost . to further study the effectiveness of optimal joint mechanisms , we run the following evaluation scenario .",
    "we design optimal differential mechanisms for some values of @xmath46 . and , we design optimal distortion mechanisms for some values of @xmath43 that are higher than the distortion privacy resulted from those differential privacy mechanisms .",
    "we also construct their joint mechanisms given the @xmath46 and @xmath43 parameters .",
    "figure  [ fig : privacy_alljoint_joint_vs_diffbayes_optimalattack ] shows how the optimal joint mechanism adapts itself to guarantee the maximum of the privacy levels guaranteed by optimal bayesian and optimal differential mechanisms individually .",
    "this is clear from the fact that users privacy for the optimal joint mechanism is equal to their privacy for distortion mechanism ( that as we set in our scenario , they are higher than that of differential mechanisms ) .    thus , by adding the distortion privacy constraints in the design of optimal mechanisms , we can further increase the privacy of users ( with the same utility cost ) that can not be otherwise achieved by only using differential mechanisms .",
    "[ [ scenario-3 . ] ] _ scenario 3 . _ + + + + + + + + + + + + +    in order to further investigate the relation between the privacy ( and utility ) outcome of the optimal joint mechanism and that of individual differential or distortion privacy mechanisms , we run the following set of experiments on all the available user profiles .    1 .   for any value of @xmath46 in @xmath93 , we compute the utility of optimal differential privacy mechanism as well as its privacy against optimal attack .",
    "2 .   for any value of @xmath43 in @xmath94",
    ", we compute the utility of optimal distortion privacy mechanism as well as its privacy against optimal attack .",
    "@xmath95 is dependent on @xmath3 and is the maximum value that the threshold can take ( beyond which there is no solution to the optimization problem ) .",
    "3 .   for any value of @xmath46 in @xmath93 , and for any value of @xmath43 in @xmath94 ,",
    "we compute the utility and privacy of the optimal joint mechanism .",
    "figure  [ fig : joint_vs_distdiff ] shows the results . by an experiment we refer to the comparison of privacy ( or utility ) of a joint mechanism ( with bounds @xmath46 , @xmath43 ) with the corresponding differential privacy mechanism ( with bound @xmath46 ) and the corresponding distortion privacy mechanism ( with bound @xmath43 ) .",
    "note that here the thresholds @xmath46 and @xmath43 are chosen independently as opposed to scenarios 1 ( and also 2 ) .",
    "we put the results of all the experiments next to each other in the x - axis .",
    "therefore , any vertical cut on the figure  [ fig : joint_vs_distdiff ] s plots contain three points for privacy / utility of @xmath90 , @xmath87 , and @xmath89 .",
    "to better visualize the results , we have sorted all the experiments based on the privacy / utility of the joint mechanism .",
    "as the results show , the privacy achieved by the optimal joint mechanism is equal to the maximum privacy that each of the individual differential / distortion mechanisms provides separately .",
    "this means that the user would indeed benefit from including a distortion privacy constraint based on her prior leakage into the design criteria of the optimal obfuscation mechanism .",
    "this comes at no extra utility cost for the user , as the utility graph shows .",
    "in fact , the utility cost of an optimal joint mechanism is not additive and instead is the maximum of the two components , which is the differential privacy mechanism in all tested experiments .",
    "the reason behind this is that the differential privacy component makes the joint obfuscation mechanism robust to the case where the background knowledge of the adversary includes not only the prior leakage but also other auxiliary information available to him .     and for a different prior assumed in the attack .",
    "the red dots correspond to the cases where the probability @xmath3 assumed in designing the protection mechanism is the same as the attacker s knowledge . ]",
    "[ fig : prior ]         when using distortion metric in protecting privacy , we achieve optimal privacy given the user s estimated prior leakage modeled by probability distribution @xmath3 over the secrets . in the optimal attack against various protection mechanisms ,",
    "a real adversary makes use of a prior distribution over the secrets . in this subsection , we evaluate to what extent a more informed adversary can harm privacy of users further than what is promised by the optimal protection mechanisms . note that no matter what protection mechanism is used by the user , a more knowledgable adversary will learn more about the secret . in this section ,",
    "our goal is not to show this obvious fact , but to evaluate how robust our mechanisms are with respect to adversaries with different knowledge accuracy levels .    to perform this analysis",
    ", we consider a scenario in which the adversary s assumption on @xmath3 , for each user , has a lower level of uncertainty compared to @xmath3 .",
    "this can happen in the real world when an adversary obtains new evidence about a user s secret that is not used by user for computing @xmath3 .",
    "let @xmath96 be the other version of @xmath3 assumed by adversary , for a given user .",
    "for the sake of our analysis , we generate @xmath96 by providing the adversary with more evidence about most frequently visited locations , e.g. , home and work .",
    "this is equivalent to the scenario in which the adversary knows the user s significant locations , e.g. , where the user lives and works .",
    "the entropy of @xmath96 is less than that of @xmath3 , hence it contains more information about the user s mobility .",
    "we construct the protection mechanisms assuming @xmath3 , and we attack them by optimal inference attacks , but assuming the lower entropy @xmath96 priors .",
    "figure  [ fig : prior ] illustrates privacy of users for different assumptions of @xmath96 , using optimal differential protection versus optimal distortion protection ( assuming @xmath3 ) . we observe that a more informed adversary has a lower expected error",
    "however , it further shows that an optimal differential protection mechanism compared to an optimal distortion mechanism is more robust to knowledgable adversaries .",
    "note that we set @xmath43 to @xmath88 , according to scenario 1 in section  [ sec : analysis : firsteval ] .",
    "so , when @xmath97 , both optimal protection mechanisms guarantee the same level of privacy .",
    "however , as there is more information in @xmath96 than in @xmath3 , more information can be inferred from the optimal distortion mechanism compared to the differential mechanism .",
    "we have solved the problem of designing _ optimal _ user - centric obfuscation mechanisms for data sharing systems .",
    "we have proposed a novel methodology for designing such mechanisms against any _ adaptive _ inference attack , while maximizing users utility .",
    "we have proposed a generic framework for quantitative privacy and utility , using which we formalize the problems of maximizing users utility under a lower - bound constraint on their privacy .",
    "the major novelty of the paper is to solve these optimization problems for both state - of - the - art distortion and differential privacy metrics , for the generic case of any distance function between the secrets .",
    "being generic with respect to the distance functions , enables us to formalize any sensitivity function on any type of secrets .",
    "we have also proposed a new privacy notion , joint distortion - differential privacy , and constructed its optimal mechanism that has the strengths of both metrics .",
    "we have provided linear program solutions for our optimization problems that provably achieve minimum utility loss under those privacy bounds .",
    "we would like to thank the pc reviewers for their constructive feedback , and kostas chatzikokolakis for very useful discussions on this work .",
    "10    m.  s. alvim , m.  e. andrs , k.  chatzikokolakis , p.  degano , and c.  palamidessi .",
    "differential privacy : on the trade - off between utility and information leakage . in _ formal aspects of security and trust _ , pages 3954 .",
    "springer , 2012 .",
    "m.  s. alvim , m.  e. andrs , k.  chatzikokolakis , and c.  palamidessi . on the relation between differential privacy and quantitative information flow . in _",
    "automata , languages and programming _ , pages 6076 .",
    "springer , 2011 .",
    "m.  s. alvim , m.  e. andrs , k.  chatzikokolakis , and c.  palamidessi . quantitative information flow and applications to differential privacy . in _ foundations of security analysis and design",
    "vi_. 2011 .",
    "m.  e. andrs , n.  e. bordenabe , k.  chatzikokolakis , and c.  palamidessi .",
    "geo - indistinguishability : differential privacy for location - based systems . in _ proceedings of the 2013 acm sigsac conference on computer & communications security _ ,",
    "pages 901914 .",
    "acm , 2013 .",
    "m.  barreno , b.  nelson , r.  sears , a.  d. joseph , and j.  tygar",
    ". can machine learning be secure ?",
    "in _ proceedings of the acm symposium on information , computer and communications security _ , 2006 .",
    "g.  barthe , b.  kpf , f.  olmedo , and s.  zanella  bguelin .",
    "probabilistic relational reasoning for differential privacy . , 2012 .",
    "j.  o. berger . .",
    "springer , 1985 .",
    "i.  bilogrevic , k.  huguenin , s.  mihaila , r.  shokri , and j .-",
    "hubaux . predicting users motivations behind location check - ins and utility implications of privacy protection mechanisms .",
    "in _ in network and distributed system security ( ndss ) symposium _ , 2015 .",
    "n.  e. bordenabe , k.  chatzikokolakis , and c.  palamidessi .",
    "optimal geo - indistinguishable mechanisms for location privacy . in",
    "_ proceedings of the 16th acm conference on computer and communications security _ , 2014 .",
    "s.  p. boyd and l.  vandenberghe . .",
    "cambridge university press , 2004 .",
    "h.  brenner and k.  nissim .",
    "impossibility of differentially private universally optimal mechanisms . in _",
    "foundations of computer science ( focs ) , 2010 51st annual ieee symposium on _ , pages 7180 .",
    "ieee , 2010 .",
    "j.  brickell and v.  shmatikov .",
    "the cost of privacy : destruction of data - mining utility in anonymized data publishing . in _ proceedings of the 14th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 08 , pages 7078 , new york , ny , usa , 2008 .",
    "m.  brckner and t.  scheffer .",
    "stackelberg games for adversarial prediction problems . in _",
    "17th acm sigkdd international conference on knowledge discovery and data mining ( kdd 2011 ) _ , 2011 .",
    "f.  brunton and h.  nissenbaum .",
    "vernacular resistance to data collection and analysis : a political theory of obfuscation .",
    ", 16(5 ) , 2011 .",
    "k.  chatzikokolakis , m.  e. andrs , n.  e. bordenabe , and c.  palamidessi . broadening the scope of differential privacy using metrics . in _ privacy enhancing technologies _ ,",
    "pages 82102 .",
    "springer , 2013 .",
    "k.  chatzikokolakis , c.  palamidessi , and p.  panangaden .",
    "anonymity protocols as noisy channels . , 206(2 - 4):378401 , 2008 .",
    "k.  chatzikokolakis , c.  palamidessi , and m.  stronati . a predictive differentially - private mechanism for mobility traces . in _ privacy enhancing technologies",
    "_ , pages 2141 .",
    "springer international publishing , 2014 .",
    "v.  conitzer and t.  sandholm . computing the optimal strategy to commit to . in _ proceedings of the 7th acm conference on electronic commerce _ , 2006 .",
    "g.  danezis and c.  troncoso .",
    "you can not hide for long : de - anonymization of real - world dynamic behaviour . in _ proceedings of the 12th acm workshop on workshop on privacy in the electronic society _ , pages 4960 .",
    "acm , 2013 .",
    "c.  diaz , s.  seys , j.  claessens , and b.  preneel . towards measuring anonymity . in _ privacy enhancing technologies _",
    ", pages 5468 .",
    "springer berlin heidelberg , 2003 .",
    "c.  dwork .",
    "differential privacy . in _",
    "automata , languages and programming _ , pages 112 .",
    "springer , 2006 .    c.  dwork , f.  mcsherry , k.  nissim , and a.  smith . calibrating noise to sensitivity in private data analysis . in _ theory of cryptography _",
    ", pages 265284 .",
    "springer , 2006 .",
    "v.  f. farias and b.  van  roy .",
    "tetris : a study of randomized constraint sampling . in _",
    "probabilistic and randomized methods for design under uncertainty_. 2006 .",
    "q.  geng and p.  viswanath .",
    "the optimal mechanism in differential privacy . , 2012 .",
    "a.  ghosh , t.  roughgarden , and m.  sundararajan .",
    "universally utility - maximizing privacy mechanisms . in _ proceedings of the 41st annual acm symposium on theory of computing _ , pages 351360 .",
    "acm , 2009 .",
    "a.  ghosh , t.  roughgarden , and m.  sundararajan .",
    "universally utility - maximizing privacy mechanisms .",
    ", 41(6):16731693 , 2012 .",
    "m.  grtschel , l.  lovsz , and a.  schrijver .",
    "the ellipsoid method and its consequences in combinatorial optimization . , 1981 .",
    "m.  gupte and m.  sundararajan .",
    "universally optimal privacy mechanisms for minimax agents . in _ proceedings of the twenty - ninth acm sigmod - sigact - sigart symposium on principles of database systems _ , 2010 .",
    "x.  he , a.  machanavajjhala , and b.  ding .",
    "blowfish privacy : tuning privacy - utility trade - offs using policies . in _ proceedings of the 2014 acm",
    "sigmod international conference on management of data _ , pages 14471458 .",
    "acm , 2014 .",
    "l.  huang , a.  d. joseph , b.  nelson , b.  i. rubinstein , and j.  tygar .",
    "adversarial machine learning . in _ proceedings of the 4th acm workshop on security and artificial intelligence _ , 2011 .",
    "s.  ioannidis , a.  montanari , u.  weinsberg , s.  bhagat , n.  fawaz , and n.  taft .",
    "privacy tradeoffs in predictive analytics .",
    ", 2014 .",
    "d.  kifer and a.  machanavajjhala . no free lunch in data privacy . in _ proceedings of the 2011 acm sigmod international conference on management of data _ , pages 193204 .",
    "acm , 2011 .",
    "n.  kiukkonen , j.  blom , o.  dousse , d.  gatica - perez , and j.  laurila . towards rich mobile phone datasets : lausanne data collection campaign . , 2010 .",
    "b.  kpf and d.  basin . an information - theoretic model for adaptive side - channel attacks . in _ proceedings of the 14th acm conference on computer and communications security _ , 2007 .",
    "d.  korzhyk , z.  yin , c.  kiekintveld , v.  conitzer , and m.  tambe .",
    "stackelberg vs. nash in security games : an extended investigation of interchangeability , equivalence , and uniqueness . , 41:297327 , may ",
    "august 2011 .    c.  li , m.  hay , v.  rastogi , g.  miklau , and a.  mcgregor . optimizing linear counting queries under differential privacy . in _ proceedings of the twenty - ninth acm sigmod - sigact - sigart symposium on principles of database systems _ , pages 123134 .",
    "acm , 2010 .",
    "w.  liu and s.  chawla .",
    "a game theoretical model for adversarial learning . in _",
    "ieee international conference on data mining workshops ( icdm 2009 ) _",
    ", 2009 .",
    "d.  j. mackay . .",
    "cambridge university press , 2003 .",
    "m.  manshaei , q.  zhu , t.  alpcan , t.  basar , and j .-",
    "game theory meets network security and privacy . , 45(3 ) , 2012 .",
    "p.  mardziel , m.  s. alvim , m.  hicks , and m.  r. clarkson .",
    "quantifying information flow for dynamic secrets . in _",
    "ieee symposium on security and privacy _ , 2014 .",
    "s.  a. mario , k.  chatzikokolakis , c.  palamidessi , and g.  smith .",
    "measuring information leakage using generalized gain functions . ,",
    "r.  t. marler and j.  s. arora .",
    "survey of multi - objective optimization methods for engineering .",
    ", 26(6):369395 , 2004 .",
    "k.  micinski , p.  phelps , and j.  s. foster .",
    "an empirical study of location truncation on android .",
    ", 2:21 , 2013 .",
    "k.  miettinen .",
    ", volume  12 .",
    "springer , 1999 .",
    "y.  e. nesterov and a.  nemirovskii . .",
    "siam publications .",
    "siam , philadelphia , usa , 1993 .",
    "k.  nissim , s.  raskhodnikova , and a.  smith .",
    "smooth sensitivity and sampling in private data analysis . in _ proceedings of the thirty - ninth annual acm symposium on theory of computing _ , pages 7584 .",
    "acm , 2007 .",
    "v.  pareto . , volume  13 .",
    "societa editrice , 1906 .",
    "p.  paruchuri , j.  p. pearce , j.  marecki , m.  tambe , f.  ordez , and s.  kraus .",
    "efficient algorithms to solve bayesian stackelberg games for security applications . in _ conference on artificial intelligence _",
    ", 2008 .",
    "j.  reed and b.  c. pierce .",
    "distance makes the types grow stronger : a calculus for differential privacy . , 2010 .",
    "a.  serjantov and g.  danezis . towards an information theoretic",
    "metric for anonymity . in",
    "_ privacy enhancing technologies _ ,",
    "pages 4153 .",
    "springer berlin heidelberg , 2003 .",
    "r.  shokri , g.  theodorakopoulos , j .- y .",
    "le  boudec , and j .-",
    "hubaux . quantifying location privacy . in _ proceedings of the ieee symposium on security and privacy _ , 2011 .",
    "r.  shokri , g.  theodorakopoulos , c.  troncoso , j .-",
    "hubaux , and j .- y .",
    "le  boudec . protecting location privacy :",
    "optimal strategy against localization attacks . in _ proceedings of the acm conference on computer and communications security _ , 2012 .",
    "g.  theodorakopoulos , r.  shokri , c.  troncoso , j .-",
    "hubaux , and j .- y .",
    "l. boudec . prolonging the hide - and - seek game : optimal trajectory privacy for location - based services . in _",
    "acm workshop on privacy in the electronic society ( wpes 2014 ) _ , 2014 .",
    "c.  troncoso and g.  danezis . the bayesian traffic analysis of mix networks . in _ proceedings of the 16th acm conference on computer and communications security _ , 2009 .",
    "l.  zadeh .",
    "optimality and non - scalar - valued performance criteria . ,",
    "given the user s protection mechanism @xmath40 , the inference attack is a valid strategy for the adversary , as there is no dependency between the defender and attacker strategies in the case of differential privacy metric .",
    "however , as the differential privacy metric ( used in the protection mechanism ) does not include any probability distribution on secrets , we can design an inference attack whose objective is to minimize the conditional expected error @xmath98 : @xmath99 for all secrets @xmath2 .",
    "this is a multi - objective optimization problem @xcite that does not prefer any of the @xmath98 ( for any secret ) to another . under no such preferences , the objective is to minimize @xmath100 , using weighted sum method with equal weight for each secret .",
    "thus , the following linear program constitutes the optimal inference attack , under the mentioned assumptions .",
    "@xmath101    as all the wights of @xmath98 are positive ( @xmath102 ) , the minimum of is pareto optimal @xcite .",
    "thus , minimizing is sufficient for pareto optimality . the optimal point in a multi - objective",
    "optimization ( as in our case ) is pareto optimal `` if there is no other point that improves at least one objective function without detriment to another function '' @xcite .",
    "an alternative approach is to use the min - max formulation , and minimize the maximum conditional expected error @xmath98 over all secrets @xmath2 . for this",
    ", we introduce a new unknown parameter @xmath103 ( that will be the maximum @xmath98 ) .",
    "the following linear program solves the optimal inference attack using the min - max formulation .",
    "this also provides a necessary condition for the pareto optimality @xcite .",
    "[ eq : lp : adversary : diff : minmax ] @xmath104    we can also consider the expected error conditioned on both secret @xmath2 and estimate @xmath13 as the adversary s objective to minimize .",
    "so , we can use @xmath105 instead of @xmath106 in , and use the same approach as in .",
    "the following linear program finds the optimal inference attack that minimizes the conditional expected estimation error over all @xmath2 and @xmath13 , using the min - max formulation .",
    "[ eq : lp : adversary : diff : minmax2 ] @xmath107    overall , we prefer the linear program as it has the least number of constraints among the above three .",
    "we can also use for comparison of optimal protection mechanisms based on distortion and differential metrics .",
    "we construct from . in , we condition the optimal obfuscation @xmath40 on its corresponding optimal inference ( best response ) attack @xmath45 .",
    "so , for any observable @xmath6 , the inference strategy @xmath108 is the one that , by definition of the best response , minimizes the expected error @xmath109 thus , the privacy value to be guaranteed is @xmath110    note that is an average of @xmath111 over @xmath13 , and thus it must be larger or equal to the smallest value of it for a particular @xmath13 .",
    "@xmath112    let @xmath113 be a conditional probability distribution function such that for any given observable @xmath6 , @xmath114 note that @xmath115 is a pure strategy that represents one particular inference attack",
    ". moreover , constructs @xmath45 such that it optimizes over the set of all mixed strategies @xmath58 that include all the pure strategies .",
    "the minimum value for the optimization over the set of all mixed strategies is clearly less than or equal to the minimum value for the optimization over its subset ( the pure strategies ) .",
    "thus , the following inequality holds .",
    "@xmath116    therefore , from inequalities and we have @xmath117 where @xmath118 , or equivalently @xmath119 .    thus , the constraint in the linear program is equivalent to ( and can be replaced by ) the constraints and in the linear program .",
    "here , we briefly discuss the computational aspects of the design of optimal protection mechanisms .",
    "although the solution to linear programs provides us with the optimal protection mechanism , their computation cost is quadratic ( for distortion mechanisms ) and cubic ( for differential mechanisms ) in the cardinality of the set of secrets and observables .",
    "providing privacy for a large set of secrets needs a high computation budget . to establish a balance between the computation budget and privacy requirements ,",
    "we can make use of approximation techniques to design optimal protection mechanisms .",
    "we explore some possible approaches .",
    "linear programming @xcite is one of the fundamental areas of mathematics and computer science , and there is a variety of algorithms to solve a linear program . surveying those algorithms and evaluating their efficiencies is out of the scope of this paper .",
    "these algorithms search the set of feasible solutions of a problem for finding the optimal solution that meets the constraints .",
    "many of these algorithms are iterative and they converge to the optimal solution as the number of iterations increases @xcite .",
    "thus , a simple approximation method is to stop the iterative algorithm when our computation budget is over .",
    "other approximation methods exist .",
    "for example , @xcite suggests a sampling algorithm to select a subset of constraints in an optimization problem to speed up the computation .",
    "moreover , we can rely on the particular structure of secrets to reduce the set of constraints @xcite .",
    "we can implement those approximation techniques to solve approximately optimal protection mechanisms in an affordable time .",
    "furthermore , we can rely on the definition of privacy to find the constraints that have a minor contribution to the design of the protection mechanism . in this section ,",
    "we study one approximation method , following the intuition behind the differential privacy bound : we remove the constraints for which the distance @xmath120 is larger than a threshold .",
    "we can justify this by observing that , in the definition of differential privacy metric , the privacy is more protected when for secrets @xmath121 , the distance @xmath120 is small . to put this in perspective ,",
    "note that if we use the original definition of differential privacy , there would not be any constraint if @xmath122 .",
    "we also apply this approximation to the distance between observables and secrets .    in figure",
    "[ fig : privacy_appx ] , we show the privacy loss of users as well as the speed - up of their computation due to approximation .",
    "we performed the computation on a machine with 4 core cpu model intel(r ) xeon(r ) 2.40ghz .",
    "as we increase the approximation threshold ( which is the distance beyond it we ignore the constraints ) , the approximation error goes to zero .",
    "this suggests that , for a large set of secrets , if we choose a relatively small threshold the approximated protection mechanism provides almost the same privacy level as in the optimal solution .",
    "the computation time , however , increases as the approximation error decreases ( due to increasing the approximation threshold ) . figure  [ fig : privacy_appx ] captures such a tradeoff of our approximation method ."
  ],
  "abstract_text": [
    "<S> consider users who share their data ( e.g. , location ) with an untrusted service provider to obtain a personalized ( e.g. , location - based ) service . </S>",
    "<S> data obfuscation is a prevalent user - centric approach to protecting users privacy in such systems : the untrusted entity only receives a noisy version of user s data . </S>",
    "<S> perturbing data before sharing it , however , comes at the price of the users utility ( service quality ) experience which is an inseparable design factor of obfuscation mechanisms . </S>",
    "<S> the entanglement of the utility loss and the privacy guarantee , in addition to the lack of a comprehensive notion of privacy , have led to the design of obfuscation mechanisms that are either suboptimal in terms of their utility loss , or ignore the user s information leakage in the past , or are limited to very specific notions of privacy which e.g. , do not protect against adaptive inference attacks or the adversary with arbitrary background knowledge . </S>",
    "<S> + in this paper , we design user - centric obfuscation mechanisms that impose the minimum utility loss for guaranteeing user s privacy . </S>",
    "<S> we optimize utility subject to a joint guarantee of differential privacy ( indistinguishability ) and distortion privacy ( inference error ) . this double shield of protection limits the information leakage through obfuscation mechanism as well as the posterior inference . </S>",
    "<S> we show that the privacy achieved through joint differential - distortion mechanisms against optimal attacks is as large as the maximum privacy that can be achieved by either of these mechanisms separately . </S>",
    "<S> their utility cost is also not larger than what either of the differential or distortion mechanisms imposes . </S>",
    "<S> we model the optimization problem as a leader - follower game between the designer of obfuscation mechanism and the potential adversary , and design adaptive mechanisms that anticipate and protect against optimal inference algorithms . </S>",
    "<S> thus , the obfuscation mechanism is optimal against any inference algorithm . </S>"
  ]
}