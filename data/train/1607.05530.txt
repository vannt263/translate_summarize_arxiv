{
  "article_text": [
    "in 1894 , with their pioneering work on free - recall experiments , binet and henry introduced a key tool for the controlled investigation of short - term memory ( binet and henry , 1894 ) . in its traditional form",
    ", a free - recall experiment is performed by presenting the subject with a list of words and then requesting him or her to recall it in any order ( murdock , 1960 , 1962 ; roberts , 1972 ; standing , 1973 ) .",
    "several types of effects have been reported :    \\1 ) effects depending on the lexical properties of individual words . in particular ,",
    "lists of short words are recalled better than lists of long ones , a fact known in the literature as the word - length effect ( baddeley et al . , 1975 ;",
    "russo and grammatopoulou , 2003 ; tehan and tolan , 2007 ; bhatarah et al . , 2009 ) .",
    "\\2 ) effects in which the recall probability depends on the absolute position of words in the list .",
    "it has been observed that the first and last words in the list are recalled more easily ( `` primacy '' and `` recency '' effects ) .",
    "\\3 ) effects depending on the relative position of words with respect to each other .",
    "most notably , the recall probabilities of contiguous words correlate positively , a fact known as the contiguity effect ( murdock , 1960 , 1962 ) .",
    "the need to understand serial - position effects led to the devising of retrieved - context models , such as the temporal context model ( howard and kanaha , 2002 ) . in these models",
    "the recall process , rather than retrieving a word directly , retrieves the context associated to the word first .    within this scenario ,",
    "recency effects appear because the context at the time of the `` memory test '' is most similar to the context associated with recent items .",
    "when an item is retrieved , it reinstates the context active when that item was presented . because this context overlaps with the encoding context of the items neighbors , a contiguity effect results .    through these models ,",
    "serial - position effects have been substantially understood over the past fifteen years ( howard and kahana , 2002 , 2002b ; sederberg , howard , and kahana , 2008 ; polyn , norman , and kahana , 2009 , 2009b ; lohnas , polyn , and kahana , 2015 ; kahana , 2012 ) . the same can not be said , however , about the word - length effect .",
    "the word - length effect ( wle ) has been a traditional testing ground for models of short - term memory ( campoy , 2011 ; jalbert et al . ,",
    "2011 ) , and it has played a key role in establishing the working - memory paradigm and the phonological loop hypothesis ( baddeley and hitch . , 1974 ) .",
    "the standard account of the effect ( baddeley , 2007 ) relies on a trade - off between memory decay ( in the phonological store ) and subvocal rehearsal via an articulatory control process .",
    "because shorter words take less time to rehearse , more decaying traces of them can be refreshed than decaying traces of long items , and , therefore , more short items can be recalled .",
    "this picture , however , is not able to account for all experimental observations concerning this effect , and has been repeatedly called into question .    in ( neath et al . , 2003",
    ") , it was shown that with words having the same number of syllables but different pronunciation times , no unambiguous wle arises .",
    "this result ( extended in jalbert et al . 2011 ) suggests that the effect depends on the number of syllables , and not on the time it takes to pronounce them .",
    "experiments have also been performed in conditions where there was a delay between lists , making subvocal rehearsal possible in the interval .",
    "no appreciable difference in recall probabilities was found ( campoy , 2008 ) .    in the same study , experiments were performed in which subvocal rehearsal was prevented by a high presentation rate .",
    "no delay was allowed between the presentation of word lists and the memory test . yet , the wle occurred unperturbed .    in the 2011 paper i just cited , jalbert et al",
    ". concluded : `` the wle may be better explained by the differences in linguistic and lexical properties of short and long words rather than by length per se '' .      in the meanwhile , within the fields of experimental and computational linguistics , progress has been made in understanding the role of word length in verbal processing . over the years , it has emerged that words with different lengths tend to have different semantic properties .",
    "the idea was first put forth in pedagogical studies ( klare , 1988 ) . in ( elts , 1995 ) , a correlation coefficient of 0.96 was found between a noun s length and its average tendency to be used as a technical term ( `` terminologicality '' ) .",
    "mikk et al .",
    "( 2000 ) , using data on the human - assessed complexity of a large sample of words , found a correlation coefficient 0.86 between words length and their semantic complexity .",
    "pinning down the precise semantic property that correlates to word length has proven difficult . already in ( greenberg , 1966 )",
    "it was argued that a word s length correlates positively to its conceptual `` markedness '' of meaning .",
    "various notions of markedness have subsequently been discussed in the literature ( haspelmath , 2006 ) .",
    "piantadosi et al .",
    "( piantadosi et al . , 2011 , 2011b ) and later mahowald et al . , ( mahowald et al . , 2012 ) reported that the length of words correlates positively with their contextual information rate .",
    "more recently , lewis and frank ( 2016 ) have carried out a comprehensive experimental study across 80 languages .",
    "they found that , in all the languages considered , judgments of conceptual complexity for a sample of real words correlate highly with their length , and they even control for frequency , familiarity , imageability , and concreteness .",
    "their conclusion is : `` while word lengths are systematically related to usage @xmath0 both frequency and contextual predictability @xmath0 our results reveal a systematic relationship with meaning as well '' .    in the light of these findings",
    ", it would be a natural step to attempt an explanation of the wle in terms of the semantic differences among words .",
    "however , no such approach seems to have been attempted in the literature .",
    "recently , new aspects of the wle have emerged through the analysis of a large set of data from experiments by miller and al ( miller et al . , 2012 ) .",
    "the data analysis was performed by katkov et al .",
    "( katkov et al . , 2014 ) , who found no negative correlation between total length of presented items and number of recalled words , thus disproving both rehearsal - time theories and hypotheses based on the increasing complexity of longer items .    moreover , they reported an inversion of the effect in mixed lists , that is , lists where words are selected irrespectively of their length .",
    "they observed that , in this type of lists , the mean values of recall probabilities allow to establish an increasing trend .",
    "long words are recalled better than short ones .",
    "an `` inverse '' wle had been previously reported by at least two groups , but in somewhat less general circumstances : one of them ( hulme et al . , 2006 )",
    "embedded strictly pure lists with a single word of a different type , while the results of xu et al .",
    "( xu et al . , 2009 ) , may not bear direct comparison with data in languages other than chinese .",
    "if the inversion of the wle for mixed lists will be confirmed by further experiments , it will have to be taken into account by every general theory of the standard wle .",
    "let us consider , therefore , what requirements a model should fulfill to explain both phenomena simultaneously .",
    "call @xmath1 the fraction of long words in the list ; call @xmath2 the probability of recalling successfully a given long word from a list in which a fraction @xmath1 of words are long ; and let @xmath3 be the probability of recalling successfully a short word , from a list with a fraction @xmath1 of long words .",
    "obviously , the function @xmath2 is only defined for @xmath4 , and the function @xmath3 only for @xmath5 . for @xmath60 , 1[$ ] , both functions are defined .",
    "theorists would have to reconcile two observations on the curves",
    "@xmath7 :    \\1 . @xmath8    \\2 .",
    "@xmath9 for @xmath60 , 1[$ ] .",
    "the only way these two inequalities can be simultaneously satisfied is if _ both _ @xmath10 and @xmath11 are , on the whole , decreasing functions of @xmath1 .",
    "the simplest choice of these curves compatible with experiments is one where both are monotonously decreasing , that is :    @xmath12    [ fig1 ]   ( -225,5)@xmath13 ( -50,5 ) @xmath14 ( -200,146 ) @xmath10 ( -200,97 )",
    "@xmath11    this means that , whenever we replace a short word of the list with a long word , we are lowering the recall probability of all the words in the list , both long and short . a higher number of long words makes every single word in the list harder to recall .",
    "it is difficult to imagine how this could ensue from the different @xmath15 of long and short words .",
    "the question is then : can equation ( [ derivatives ] ) result directly from the different semantic properties of long and short words ?    in this paper , i will show that the answer is positive as long as one models carefully the process of verbal perception .",
    "a suitable way of doing so is demonstrated in the next section . in section iii ,",
    "i employ a retrieved - context description of verbal recall to derive both wles ( standard and inverse ) . in section iv ,",
    "i test two key predictions of the theory against data from the peers experiment of kahana et al . ( lohnas and kahana , 2013 ; healey and kahana , 2016 ) . in section",
    "v , i list five experiments designed to test further predictions of the theory . in the conclusions ,",
    "i sketch a possible interpretation of the results .",
    "verbal perception is the conversion of words into meaning , and any theory of the phenomenon must begin by defining the space in which the mental trajectory takes place .",
    "i will refer to this as `` semantic space '' and will represent `` semantic states '' as integers belonging to a segment @xmath16 , with @xmath17 .",
    "measurable quantities are to be computed in the limit @xmath18 , and a generalization to higher dimensions will be introduced below .    since @xmath19",
    ", it follows that , between two discrete times @xmath20 and @xmath21 , the psychological trajectory can be represented by a sequence of integers @xmath22 . as memory plays a crucial role in thought process , the laws of motion governing the trajectory will be in general highly non - markovian .    [ fig2 ] .,title=\"fig:\",scaledwidth=100.0% ]    call @xmath23 the vocabulary available to the system .",
    "naturally , a given word @xmath24 may be appropriate to describing more than one state , though with a varying degree of appropriateness .",
    "for each word @xmath25 , thus , one can define its semantic range @xmath26 as the set of all states described by that word .",
    "meanings should be seen as distributed in an universal way over @xmath27 , as in figure 2 .",
    "the function @xmath28 matches each word with locations associated to meanings suitable for that word .",
    "not all states are equally fit to be verbalizable .",
    "hence , there will be a varying probability @xmath29 that the state @xmath30 will match the word describing it .",
    "the value of @xmath29 is the degree of verbalizability of state @xmath30 ; or conversely , given @xmath31 , @xmath29 is the fitness of word @xmath25 as a descriptor of state @xmath30 .    if the sets @xmath32 do not overlap , we can define a verbalization function @xmath33 such that @xmath34 whenever @xmath31 .",
    "when the system tries to verbalize state @xmath35 , it produces word @xmath36 with probability @xmath29 , and the silent word @xmath37 with probability @xmath38 .",
    "we will define @xmath29 so that it is only null for nonverbalizable states .",
    "finally , we can take the trajectory to become markovian whenever it is driven by a specific verbal input : at the instant in which a new word is presented , the system hops into the nearest state described by that word .",
    "accordingly , let the function @xmath39 be defined by the condition :    [ fig3 ]     @xmath40 satisfy this condition , @xmath41 is chosen by flipping a coin",
    ".    given a verbal input @xmath42 , with the @xmath43 s @xmath44 , and a starting point @xmath45 , the trajectory of the system while perceiving the input will be given by :    @xmath46    @xmath47 .    obviously , the ordering of words in the input is essential .",
    "suppose for instance that the vocabulary contains two words , @xmath48 , with @xmath49 and @xmath50 , and the starting point is @xmath51 .",
    "the verbal input @xmath52 yields the trajectory @xmath53 , while the input @xmath54 yields the trajectory @xmath55 .",
    "word @xmath56 corresponds to two different states in the two inputs .",
    "@xmath57    [ fig4 ]       the verbalizable states associated to word @xmath25 can be seen as seeds of a voronoi partition of @xmath27 into cells @xmath58 , where states located on the boundaries are understood to belong to each of two cells with probability @xmath59 .    [ fig5 ]     during the perception of verbal inputs , the distance travelled by the system is controlled by the function latexmath:[$d(x , w):=    state @xmath30 .",
    "this is just the distance of @xmath30 from the closest state belonging to @xmath61 .",
    "the level of nonlocality of a word @xmath25 may be measured by its reaching distance averaged over all starting points :    @xmath62    obviously @xmath63 does not necessarily imply @xmath64 .",
    "consider two words @xmath65 , @xmath66 such that @xmath67 and @xmath68 , with @xmath56 even and @xmath69 .",
    "we have @xmath70 , but @xmath71 .",
    "a direct relation can be shown to exist between a word s average reaching distance and the size of its voronoi cells .",
    "call @xmath72 the size of the @xmath73-th voronoi cell of word @xmath25 .",
    "assuming @xmath74 , we may neglect boundary effects and write    @xmath75    thus , the average reaching distance of a word depends solely on the first two moments of its voronoi length distribution .    if @xmath76 for a word @xmath25 , its word structure contains strong fluctuations , so one may separate @xmath27 into regions were states described by @xmath25 are denser , and regions were they are sparser .",
    "word @xmath25 is effectively `` localized '' inside the regions were such states are dense , that is , it expresses those semantic areas better than those where its states are sparse .",
    "the degree of word localization has arguably a strong effect on the dynamics , as shown in figure 6 .",
    "[ fig6 ]       in most experiments , the relevant number of word - lengths is four , since words with more than four syllables are rare in english . for simplicity , here we will consider the existence of only two word - lengths , short and long .",
    "call @xmath77 ( with @xmath78 ) the reaching distance averaged over space and over all words of the same length ( short or long ) . from eq .",
    "( [ voronoi ] ) , we see that the space - averaged reaching distance is the product of two factors , involving respectively the first and second moment of the voronoi length distribution .",
    "let us consider how these two moments depend on the word s length .",
    "the average voronoi length @xmath79 may be related to the frequency of a given word in a corpus of the language .",
    "indeed , if the system , in its speaking mode , explores semantic space ergodically and uniformly , the frequency of a word is @xmath80 .    in a typical corpus of the english language ,",
    "the frequency @xmath81 of words with @xmath82 syllable is monotonously decreasing . as a consequence",
    ", we expect the average of the voronoi length @xmath83 to be larger for short words than for long words . while this is correct for most languages , notice that there are exceptions , such as turkish and arabic , where the function @xmath81 is peaked at @xmath84 ( fucks , 1956 ; grzybek , 2007 ) .",
    "let us now look at the relative fluctuations of the voronoi length , described by @xmath85 .",
    "we will surmise their magnitude through a qualitative argument .",
    "as mentioned in the introduction , various approaches have been taken to prove that long words are on average more technical , specialized , distinctive or marked than short words .",
    "several claims made in ( elts , 1995 ) , ( mikk et al . , 2000 ) , and ( lewis and frank , 2016 ) may be rephrased as the statement that long words are , on average , conceptually more specific .",
    "a word is conceptually specific if it is localized in certain areas of semantic space .",
    "a correlation exists , therefore , between word - length and semantic localization .",
    "localization , in turn , will occur if the scale over which the voronoi length fluctuates is comparable or greater than its average value .",
    "this leads to the conclusion that the relative fluctuations of the voronoi length will be larger for longer words .",
    "thus , both factors in eq .",
    "( [ voronoi ] ) take a greater value if the word is long .",
    "it follows that @xmath86 .",
    "a one - dimensional modeling of semantic space is of course unrealistic , and may not suffice for every application of the theory . if @xmath27 is taken to be a connected subset of @xmath87 , the definition we have given for the word structure @xmath88 applies all the same .",
    "the points in space are now vectors , and equations ( [ hopping ] ) and ( [ reachingdistance ] ) are still valid , the distance in eq .",
    "( [ reachingdistance ] ) being the euclidean distance in @xmath89 dimensions .",
    "the voronoi cells , however , become less simple to treat as they can be arbitrary polyhedra ( for a complete treatement , see aurenhammer et al . ,",
    "formula ( [ voronoi ] ) for the reaching distance must be modified , and it takes a geometry - dependent form .",
    "the voronoi structure , yet , affects directly only the process of verbal perception , not the process of memory retrieval , which will be the subject of the next section .",
    "thus , while in the figures i will refer to the one - dimensional case , the mathematical results will apply to any number of dimensions .",
    "we have seen that the rules of motion of the system become markovian during the perception of verbal input . in retrieved - context models of short - term memory ,",
    "the rules of motion are also markovian during the search for memories , which is conducted through the principle of free association . in these models ,",
    "the retrieval of memories per se is not a measurable phenomenon .",
    "what can be observed is the retrieval of words describing those memories .",
    "therefore , a recall experiment must be seen as the composition of two processes : a processes of memory retrieval , and a process of memory verbalization .",
    "the markovian process of memory retrieval must be described here , in keeping with the spirit of context - driven models , as a random walk on @xmath27 that effects a retrieval whenever it meets a state corresponding to the experience to be recalled .",
    "the verbalization process , on the other hand , depends on the verbalizability of memories : a memory @xmath30 , once retrieved , has a probability @xmath29 of leading the system to produce the word describing it .    [ fig7 ]   ( -135,420)@xmath90 ( -216,253)@xmath45    the following mathematical problem arises . supposing one is given    \\1 ) the structure @xmath91 of the vocabulary ;    \\2 ) a word list @xmath92 presented to the system ;    \\3 ) the state of the system when the word list begins to be presented , that is , a probability distribution @xmath93 $ ] on its position @xmath90 ;    \\4 ) the state of the system when the retrieval process begins , that is , a probability distribution @xmath94 $ ] on the new position @xmath45 ;    one wants to predict the probability that the @xmath73-th word will be among those recalled by the system .    in the next section",
    ", this program will be carried out for the particular case of a vocabulary containing words of two different lengths .",
    "we will begin by defining the probability @xmath95 that a memory placed at distance @xmath96 from @xmath45 is met by the retrieving random walk for the first time after a time @xmath97 . in one dimension ,",
    "this is given by    @xmath98    where @xmath99 and @xmath100 .",
    "the probability that a memory will be retrieved is therefore    @xmath101    where the cutoff @xmath102 is needed to obtain meaningful results in one or two dimensions , and can otherwise be let to infinity .",
    "suppose a list @xmath103 has been presented .",
    "call @xmath104 the probability of retrieving the memory created by the @xmath73-th word in the list , given a certain initial position @xmath90 for the trajectory during presentation , and a certain initial position @xmath45 for the trajectory during retrieval .",
    "we have :    @xmath105    where @xmath106 .",
    "our goal is to compute the average recall probability for an arbitrary list composed by @xmath82 short words and @xmath107 long words arranged into a given order .",
    "this amounts to averaging eq .",
    "( [ piv ] ) over all lists of the same type @xmath108 where @xmath109 for @xmath110 :    @xmath111    where @xmath112 is the type of word @xmath25 and @xmath113 ( @xmath114 ) is the number of short ( long ) words in the vocabulary",
    ".    we will perform the averaging over lists through a mean - field approach .",
    "mean - field approaches , widely employed in physics , consist in inverting the order of the two steps : the computation of observables and the averaging . instead of averaging the final probabilities ,",
    "one averages an intermediate , non - observable quantity usually called the `` field '' . in this case",
    ", we may average the functions @xmath115 themselves .    from section iic , we know that the average displacement induced by the function @xmath115 is equal to @xmath116 with @xmath117 , while no constraints emerged concerning the direction of this displacement as a function of word type .",
    "therefore , to average the functions @xmath118 over all word lists of one type , we replace @xmath115 with the function @xmath119 defined by @xmath120 , where @xmath121 is a unity vector randomly chosen from a suitable distribution @xmath122 $ ] .    we can thus write @xmath123 , and eq .",
    "( [ piv2 ] ) becomes    @xmath124    where @xmath125 $ ] .    the initial positions @xmath90 and @xmath45 are not measurable quantities . in eq . ( [ meanfield ] ) ,",
    "therefore , they must be averaged over through two suitable distributions @xmath126 and @xmath127 , yielding    @xmath128    where @xmath129 and @xmath130 .",
    "the function we need to average may be rewritten as    @xmath131 + \\sum_{j \\neq i } \\mathbb{p}\\big[i_1=j\\big ] \\",
    "p(y_i | y_j)\\ ] ]    where @xmath132 $ ] is the probability that the @xmath73-th memory , @xmath133 , will be the first one to be found , and @xmath134 < 1 $ ] . substituting eq .",
    "( [ compound ] ) into eq .",
    "( [ meanfield2 ] ) , we find    @xmath135 \\big\\rangle_{\\chi , \\psi } + \\sum_{j",
    "\\neq i } \\big \\langle \\mathbb{p}\\big[i_1=j\\big ] \\big\\rangle_{\\chi , \\psi } \\",
    "p(y_i | y_j ) \\bigg \\rangle_{\\omega}\\ ] ]    the distribution @xmath136 refers to the state @xmath45 of the system after the so - called retention interval , during which the subject freely elaborates the information gathered during presentation .",
    "a full understanding of such elaboration would require modeling the free motion of this system , but we have only been able to markovianize the equations of motion during a progressive searching task or in the presence of a driving input .",
    "thus , a reasonable ansatz is necessary .",
    "neglecting recency effects , we can suppose @xmath136 to contain @xmath137 similar peaks at the locations @xmath138 explored during presentation .",
    "in this picture , the dependence of @xmath139 \\big\\rangle_{\\chi , \\psi}$ ] on the index @xmath73 will be negligeable , so we can approximate @xmath140 \\big\\rangle_{\\chi , \\psi}$ ] with a constant value @xmath141 .    substituting this into eq .",
    "( [ meanfield4 ] ) , and rewriting the argument of @xmath142 , we find    @xmath143\\ ] ]    where @xmath144 is now the product of @xmath145 copies of the distribution @xmath146 , and the value of the summand depends solely on the number of long and short words located between word @xmath73 and word @xmath147 .    using @xmath148 to include the @xmath149 term into the sum , and noticing that the labels of the @xmath150 s are interchangeable , we can finally rewrite eq .",
    "( [ meanfield5 ] ) as    @xmath151    where we have introduced the quantities    @xmath152    and the unit vectors @xmath153 , @xmath154 are independently distributed according to @xmath155 copies of the distribution @xmath146 .    for pure lists , eq .",
    "( [ meanfield6 ] ) becomes @xmath156\\ ] ]      as mentioned above , the probability of recalling the @xmath73-th word of the list is equal to the probability of retrieving the @xmath73-th _ memory _ , multiplied by the factor @xmath157 . in the previous section , we averaged the retrieval probability over all words of the same type ; similarly , we must now average the verbalizability @xmath29 , which we take to be distributed independently of the word structure @xmath158 . defining @xmath159 as the average verbalizability of words of type @xmath160",
    ", we obtain the full recall probability    @xmath161    as mentioned in the introduction , the classical wle is the experimental fact that pure lists made of shorter words are easier to remember .",
    "of course such behavior may always be prevented , in principle , by making the verbalizability ratio @xmath162 sufficiently low . yet ,",
    "if the reported inversion of the wle exists within this model , it must rely on the opposite requirement ",
    "namely , that the verbalizability ratio is sufficiently high .",
    "the questions we are therefore supposed to answer are : first , whether there exists for this model a range of values of @xmath162 where both the classical and the inverse wle occur ; second , under what conditions on the parameters this may happen , and whether such conditions are relevant to current experiments .",
    "let us describe the typical experimental situation . in the experiments ,",
    "word lists are generated by drawing words at random from a vocabulary @xmath23 . in a double - word length scenario ,",
    "this vocabulary will contain @xmath113 short words and @xmath113 long ones .",
    "we may consider , therefore , an ensemble of lists of length @xmath137 , where each word has a probability @xmath163 of being long , and a probability @xmath164 of being short .",
    "the recall probability for the @xmath73-th word of the list can be averaged over all lists whose @xmath73-th word is of type @xmath160 , yielding @xmath165 .",
    "substituting eq .",
    "( [ meanfield6 ] ) , this becomes :    @xmath166\\ ] ]    in the first sum , which is the contribution from @xmath167 in eq .",
    "( [ final ] ) , @xmath82 and @xmath107 stand for the number of short or long words in positions @xmath168 such that @xmath169 ; in the second sum ( the contribution from @xmath170 ) @xmath82 and @xmath107 stand for the number of short or long words in positions @xmath168 such that @xmath171 .    in eq .",
    "( [ random ] ) , the notation @xmath172 has come to denote an averaging over @xmath82 , @xmath107 performed for each separate value of @xmath173 and summed together .",
    "this is done through the binomial distribution    @xmath174    given that the total recall probability is @xmath175 , the standard wle effect ( @xmath176 ) will occur if @xmath177 , where @xmath178 , and the inverse effect will occur if @xmath179 , that is , if @xmath180 , where @xmath181 .",
    "the two effects coexist if @xmath182 , which can only happen if @xmath183 .",
    "this condition can be rewritten as    @xmath184    now , it can be seen that @xmath185 a decreasing function of @xmath1 , because by increasing @xmath1 one transfers weight from the first to the second argument of @xmath186 inside both terms of eq .",
    "( [ random ] ) , which reduces the value of the function @xmath186 .",
    "hence , we have @xmath187 and @xmath188 , from which it follows that the inequality ( [ inequality ] ) is identically satisfied .",
    "we conclude that , in this model , the wle can undergo an inversion for any @xmath60 , 1[$ ] , as sketched in figure 1 .",
    "consider a list of the type @xmath189 , where @xmath190 , containing @xmath107 words and @xmath82 short ones .",
    "the trajectory during presentation is illustrated in figure 8 .",
    "the system begins from a random position @xmath90 , and at each new word @xmath43 of type @xmath191 , its position is shifted forth by the operator @xmath192 .",
    "the distance travelled at step @xmath73 is , in the mean field approach , equal to @xmath193 .",
    "so a memory produced by the presentation of a short word will be formed in the vicinity of the latest memory ( that is , within a distance of order @xmath194 ) whereas a memory produced by the presentation of a long word will be formed at a longer distance ( of order @xmath195 ) from the memory preceding it .",
    "consequently , memories are divided into clusters separated by a distance of order @xmath195 from each other .",
    "each cluster spreads over a width of order @xmath194 .",
    "[ fig8 ] , where @xmath196 stands for long words and @xmath197 for short ones .",
    "the longer jumps correspond to the presentation of long ( localized ) words.,title=\"fig:\",scaledwidth=100.0% ]    these memory clusters correspond to different `` segments '' of the list .",
    "call @xmath198 the index values corresponding to long words within a given list .",
    "if @xmath199 , the segments are @xmath200 for @xmath201 , and @xmath202 .",
    "if @xmath65 is short , there is an additional segment @xmath203 , and the number of segments is @xmath204 .",
    "call @xmath205 the length of segment @xmath206 .",
    "all the @xmath205 s must be positive except @xmath207 , which may be null , and @xmath208 .",
    "the direction in which the trajectory moves at each step is defined by the unknown distribution @xmath146 , which depends on the details of the word structure @xmath158 .",
    "for a generic choice of @xmath146 , clusters formed at longer time intervals from each other will lie further apart in semantic space .",
    "hence , the trajectory during presentation is the composition of two processes : a clustering process and a diffusion process .",
    "notice that in eq .",
    "( [ meanfield5 ] ) the argument of @xmath142 is of the order of @xmath209 .",
    "if the list is short enough ( that is , if @xmath210 , @xmath211 and @xmath212 ) , the summand has two orders of magnitude : one of the order of @xmath213 and one of the order of @xmath214 .",
    "this corresponds to the fact that the diffusion process is slow on the scale of the trajectory during presentation . in this regime ,",
    "formula ( [ meanfield5 ] ) for @xmath215 may be estimated by replacing the summand with @xmath216 whenever there is at least one long word between @xmath217 and @xmath218 , and with @xmath219 otherwise .",
    "this is equivalent to approximating the matrix elements @xmath220 of eq .",
    "( [ pimatrix ] ) with @xmath221 if @xmath222 and @xmath223 , and with @xmath224 if @xmath225 , thus ignoring the dependence of the retrieval process on the distance between segments of the list .",
    "call @xmath226 the first memory to be retrieved .",
    "the conditional retrieval probability for memory @xmath227 is @xmath228 . in the slow - diffusion limit , this is of the order of @xmath219 if memories @xmath226 and @xmath229 belong to the same cluster , and is of the order of @xmath216 otherwise .",
    "averaging over the first retrieval , we find    @xmath230\\ ] ]    where @xmath231 is the length of the segment of the list to which word @xmath73 belongs .",
    "the average recall probabilty @xmath232 for words of type @xmath160 will thus be equal to :    @xmath233\\\\ p_l = \\frac{q_l p_0}{l } \\sum_{i=1}^{l } \\big [ 1 + ( a_i -1 ) p_s + ( n - a_i ) p_l \\big]\\end{aligned}\\ ] ]    defining @xmath234 and @xmath235 , we can rewrite this as    @xmath236   \\\\",
    "\\label{pl } p_l = q_l p_0 \\big [ n p_l - p_s + 1 + \\mu ( p_s - p_l ) \\big]\\end{aligned}\\ ] ]    all the dependence on the ordering of words in the list , therefore , enters the recall probabilities through the parameteres @xmath237 and @xmath238 .",
    "the values of these parameters are shown in table i for simple lists .",
    "[ table3 ]    c c c c list structure   &   @xmath237   & @xmath238 & @xmath239 +   + @xmath240 & @xmath241 & @xmath241 & + @xmath242 & @xmath241 & @xmath243 & @xmath244 + @xmath245 & @xmath246 & @xmath247 & @xmath248 + @xmath249 & @xmath250 &  @xmath251   & @xmath252 } $ ] + @xmath253 &   @xmath254   & @xmath255 & @xmath256 +   +    for a pure list , entirely composed of words of type @xmath160 , eqs .",
    "( [ ps ] ) and ( [ pl ] ) become :    @xmath257 \\end{aligned}\\ ] ]    data for mixed lists may be interpolated with formulas ( [ ps ] ) and ( [ pl ] ) to test the theory and fix the values of the internal parameters .    finally , let us look at the range of occurrence of the wles in the slow - diffusion regime . the classical wle ( @xmath258 )",
    "emerges for @xmath259 where    @xmath260    the ratio @xmath261 for mixed lists can be estimated as follows . given a fixed value of @xmath262 , @xmath237 ranges between @xmath263 ( for @xmath264 ) and @xmath265 ( for @xmath266 ) .",
    "the minimum value of @xmath238 is obtained by starting the list with a short word and having @xmath267 segments of @xmath268 words and @xmath269 segments of @xmath270 words .",
    "the maximal @xmath238 is obtained by setting @xmath271 and lumping all the short words into one segment : @xmath272 .",
    "substituting @xmath273 and @xmath274 in ( [ ps ] ) , ( [ pl ] ) , one obtains a strict upper bound on the ratio @xmath275 : @xmath276 , where    @xmath277}\\ ] ]    once again , the inverse wle ( @xmath278 ) will emerge under the sufficient condition @xmath180 ; hence , the possibility that the classical and inverse effects may coexist requires @xmath279 .    if we rewrite this inequality by means of eq .",
    "( [ thetaclslow ] ) and ( [ thetainvslow ] ) , the `` microscopic '' probability parameters @xmath219 and @xmath216 cancel out and we obtain the general condition @xmath280 , which is identically satisfied for any @xmath281 .",
    "it follows that , in the slow - diffusion regime , the system displays an inversion of the wle for _ all _ mixed lists containing more than two long words .",
    "this statement , unlike the conclusions of the previous section , is not only true on average , but holds true regardless of the order in which short and long words are arranged within the list .      in the previous section we have considered the case where the diffusion process was much slower than the clustering process  which translates into upper bounds on the order of magnitude of @xmath82 and @xmath107 .",
    "notice that we defined these bounds in terms of the parameters @xmath194 and @xmath195 , whose value may vary from subject to subject .",
    "therefore , the very same list may be experienced in a fairly stationary regime by one subject , and in a regime of fast diffusion by a more easily distracted one .    here",
    ", we will consider the opposite case of very fast diffusion , defined as the regime where @xmath282 goes down fast on the length scale of @xmath194 , so the matrix elements of @xmath283 decay quickly as the value of the indices grows , and the recall dynamics is dominated by the contiguity effect . if a long word causes a lesser diffusion than two short words , we may neglect all but the top - left elements of the @xmath283-matrix : @xmath284 , @xmath285 , and @xmath286 .",
    "otherwise , we can work in the lowest approximation by neglecting also @xmath216 .",
    "( [ random ] ) becomes    @xmath287\\ ] ]    where i dropped the pedix @xmath73 because all dependence on @xmath73 vanishes as long as @xmath73 is neither @xmath241 nor @xmath137 .",
    "the thresholds for the verbalizability ratio are    @xmath288    and for @xmath289 , eq . ( [ fast ] ) is nothing but a linear version of figure 1 .",
    "when experiments yield a near - linear curve @xmath290 , thus , it may be taken as a sign that the system is operating in a very fast diffusion mode . when the observed curve is nonlinear",
    ", one must infer that terms of the type @xmath291 for @xmath292 are relevant , and a polynomial interpolation can be performed , by truncating the sums in eq .",
    "( [ random ] ) .",
    "the crossover toward linearity of the curves @xmath290 may be explored by tuning experimentally the amount of diffusion in the system , as we will see in section v.",
    "the foregoing analysis has proven that , within this model of verbal perception , the recall process can display consistently both the classical and the inverse wle , as observed in the experiments .",
    "there may be , however , other mechanisms leading to a similar prediction .",
    "katkov et al , for instance , propose a lexical explanation for both the direct and inverse wle , based on possible differences in the long - term neural representation of long and short words ( katkov et al . , 2014 ) .",
    "in order to tease out which mechanism is really responsible for the effects , one needs to extract further information from the data , going beyond the computation of mere recall probabilities .",
    "i will do so by using data from peers ( penn electrophysiology of encoding and retrieval study ) , a large study conducted at the university of pennsylania and devoted to assembling a database on the electrophysiological correlates of memory ( lohnas and kahana , 2013 ) .",
    "the sample i have considered corresponds to experiment i of peers .",
    "it includes data from trials on @xmath293 college students ( age range : 18@xmath29430 ) and on @xmath295 older adults ( age range : 61@xmath29485 years ) . in each trial , @xmath296 words were presented one at a time on a computer screen .",
    "each word was drawn from a pool of @xmath297 words with length varying between @xmath298 and @xmath299 syllables .",
    "the word - length distribution was peaked at @xmath300 , reflecting that of a typical english lexicon ( but not of a typical english corpus ) .",
    "each item was kept on the screen for @xmath301 ms , followed by an interstimulus interval of 800@xmath2941200 ms .",
    "after the last item in the list , there was a delay of 1200@xmath294 1400 ms , after which the participant was given @xmath302 s to attempt to recall aloud any of the just - presented items .",
    "multiple trials were performed on each subject , summing up to @xmath303 trials for the students sample and to @xmath304 for older adults . for more details on the experimental procedure , see healey and kahana , 2016 .    because the word lengths involved are more than two , the formulas we derived in the previous section may not be applied verbatim .",
    "nonetheless , two key consequences of the theory afford a direct comparison with the data .",
    "we say that a word is recalled `` by contiguity '' when its recall occurs immediately after the recall of a word contiguous to it within the list . in the model we have developed ,",
    "no matter how high the amount of diffusion , recall by contiguity will be more frequent for short words than for long words .",
    "this follows from the fact that two consecutive short words belong necessarily to the same segment , whereas a short word and a long word , though contiguous , may belong to different segments , and two long words are sure to belong to different segments .    while this prediction has been derived in a two - length model",
    ", it generalizes immediately to models with multiple lengths .",
    "we have proven in section ii that shorter words are characterized by a shorter reaching distance ; hence , memories formed by shorter words have a higher chance of being located in the proximity of memories formed by contiguous words .",
    "contiguity will play therefore a stronger role in the recall of a shorter word : the shorter the words involved , the higher the chances of recall by contiguity .    [ contiguity ]   syllables ( blue dots ) ; probability of recall by contiguity ( black ) ; and mean probability of recall by contiguity ( red ) .",
    "all values have been computed from peers data.,title=\"fig : \" ] ( -370,255)@xmath305 ( -47,5 ) @xmath82    let us call @xmath306 the probability that a given word @xmath25 , if recalled , will be recalled by contiguity .",
    "the theory predicts that @xmath305 should be larger for shorter words .",
    "if so , computing @xmath305 from the data and averaging it over all words of the same length would yield a decreasing curve @xmath307 .    in figure 9",
    ", @xmath305 is shown for all words having the same number of syllables ( blue dots ) .",
    "the data have been aggregated from all trials .",
    "the distribution of @xmath305 ( black line ) is wide for all word lengths ; nevertheless , the mean probability of recall by contiguity ( shown in red ) decreases monotonically with the number of syllables , in agreement with the theory .",
    "the @xmath297-element wordpool used in peers contains only four @xmath308 five - syllable words , and a single @xmath309-syllable word ( `` encyclopedia '' ) .",
    "hence , the statistics for these two lengths may not be considered reliable .",
    "while figure 9 refers to experiment 1 of peers , the robustness of the effect has been checked for by repeating the analysis on the databases from experiments 2 and 3 ( see lohnas et al . , 2015 ) .",
    "the decreasing trend has proven invariant across databases .",
    "let @xmath310 be the serial positions of the words recalled by the subject during a certain trial .",
    "at each step @xmath89 in the recall process , the system performs a serial - position jump of a magnitude @xmath311 . in our terminology ,",
    "longer jumps will require the exploration of a larger portion of semantic space .",
    "therefore , the distribution of jumps should be monotonously decreasing as a function of their size ( as appears indeed from the peers data , figure 10 ) .    [",
    "distribution ] , computed from peers data . in colors , distribution of jumps for a fixed size @xmath312 of the word - length barrier , shown in the legend . as",
    "the lists contain @xmath296 words , @xmath313 ranges between @xmath241 and @xmath314.,title=\"fig : \" ] ( -320,225)@xmath315 ( -33,5 ) @xmath313    from our analysis of the two - length scenario , we know that these jumps cover greater distances in semantic space if the memory created by the word of departure and the memory created by the word of arrival belong to different clusters . since it is long words that have the ability to break clusters , a long word located in a position @xmath168 such that @xmath316 plays effectively the role of a recall barrier , hindering direct transitions between @xmath43 and @xmath317 .",
    "[ clusters ] -word list .",
    "word lenght is encoded in colors as shown in the vertical sidebar . at each level of segmentation",
    ", clusters are displayed as horizontal bars covering the given portions of the list . to the right , a plot of the mean cluster length ( blue ) and of the number of clusters per list ( red ) , as a function of the hierarchical level @xmath82 of clusters , computed from peers data by averaging over all trials .",
    ", title=\"fig : \" ] ( -10,-10 ) @xmath82    similarly , in experiments with multiple word lengths , longer words create memories at longer distances . as clustering in semantic space",
    "will occur then over multiple length scales , clusters assume a hyerarchical structure .",
    "so does the segmentation of the list , with moderately long words marking off smaller segments within the larger segments delimited by longer words .",
    "this is shown in figure 11 , together with the average length of clusters as computed from the data , and the average number of clusters per list .",
    "the probability for the transition @xmath318 ( a jump of size @xmath319 ) will be affected mainly by the hyerarchical level of the clustering involved .",
    "since longer words break clusters at a higher level , we conclude that the effect of clustering on such a jump will be controlled by the length of the longest word located between the positions @xmath73 and @xmath147 :    @xmath320    where @xmath321 is the length of word @xmath25 and , once again , the lowest index has been excluded for the same reasons why it was not counted in eq .",
    "( [ meanfield5 ] ) of section iii .",
    "[ distribution ]   to word @xmath322 during the recall of a nine - word list ( with words from the peers wordpool ) .",
    "the red bars describe word length : the `` barrier word '' for this transition is marked in yellow.,title=\"fig : \" ]    we will call @xmath323 the `` barrier size '' for the @xmath324 transition .",
    "the role played by this quantity is depicted in figure 12 .",
    "if the clustering mechanism does exist , it will enhance the transition probability for small values of @xmath323 , and suppress it for higher values .    to test this prediction",
    ", we must first study the problem in the absence of clustering , calculating the transition probability @xmath325 for jumps of size @xmath326 .",
    "the resulting formula must be evaluated using the experimental values of parameters , and can then be compared with the values of @xmath325 extracted from the data .",
    "if the clustering occurs , we will observe an enhancement of @xmath315 for small values of the barrier size @xmath312 and a suppression at high values .    let us suppose that the clustering does not occur .",
    "the system then will not `` feel '' the word - length barriers , and the probability @xmath315 of a jump will be independent on the size of the barrier to be overcome .",
    "conversely , the probability distribution of barriers over the jumps performed by the system will depend only on the jumps size , and on the probability distribution of word length with the list .    the probability that a jump of size @xmath313 involves a word - length barrier @xmath312 is equal to the probability that the longest word out of a random sequence of @xmath313 words has length @xmath312 .",
    "this is equal to    @xmath327    where @xmath328 is the word - length distribution with the lists .",
    "formula ( [ no_clustering ] ) describes the no - clustering barrier - size distribution , that is , the distribution of barrier sizes for jumps of a given length if the jumps are not influenced by the barriers .",
    "i have evaluated formula ( [ no_clustering ] ) using values of @xmath328 computed directly from the lists recorded in the peers database . in figure 13",
    "( see the end - page ) , the results are compared with the values of @xmath329 extracted directly from data .",
    "the no - clustering curves are dashed , the experimental curves solid .",
    "the effect of clustering is apparent .",
    "all experimental curves reveal an enhancement of the transition probability for small values of @xmath312 , and a suppression at large values .",
    "morevoer , the effect persists across the whole range of possible jump sizes .",
    "we would expect that the influence of clustering should be small for jumps shorter than the typical size of the clusters , and grow as the jumps get longer .",
    "this is indeed what we notice in figure 13 .",
    "the influence of clustering can be observed already for jumps of size one , with an enhancement of probability for small barriers and suppression for larger barriers . yet",
    ", the clustering becomes more prominent as the jumps get longer .",
    "a noticeable increase in the importance of clustering is observed after @xmath330 .",
    "this may be due to the fact that the @xmath331 clusters are becoming important , as the jumps have gotten longer than the average length of the clusters of the third level ( see figure 11 ) .",
    "since the peers lists have length @xmath296 , there is no available statistics for values of @xmath313 larger than @xmath314 .",
    "nonetheless , we can predict that this growth will saturate when the probability of meeting a maximal bareer approaches near - certainty . for jumps",
    "longer than that , @xmath312 is no longer a controlling parameter ; from that point on , the continued suppression of recall will derive from the need to find the next memory multiple clusters away .",
    "[ jumps ]   of serial - position jumps in the recall process , plotted as a function of the size @xmath312 of word - length barriers , for all possible values of jump size @xmath313 .",
    "the red curves have been calculated in the absence of clustering , as per formula ( [ no_clustering ] ) .",
    "the blue curves are extracted from peers data .",
    "clustering manifests as an enhancement of transition probability for low barriers ( small @xmath312 ) and as a suppression where larger barriers are encountered .",
    ", title=\"fig:\",scaledwidth=100.0% ] ( -340,490)@xmath332 ( -340,425)@xmath333 ( -340,360)@xmath334 ( -340,295)@xmath335 ( -340,235)@xmath336 ( -340,170)@xmath337 ( -340,110)@xmath338 ( -340,43)@xmath339 ( -55,490)@xmath340 ( -55,425)@xmath341 ( -55,360)@xmath330 ( -55,295)@xmath342 ( -55,235)@xmath343 ( -55,170)@xmath344 ( -55,110)@xmath345",
    "most free - recall experiments performed so far have employed either a wordpool with an arbitrary number of syllables , as in peers , or a pool characterized by a fixed length that is usually @xmath346 syllables ( as in the toronto wordpool , see friendly et al . , 1982 ) .",
    "further tests on this theory , however , would be easiest to perform through experiments in which the words presented for recall are drawn from either of two sets : a set of very long words ( e.g. number of syllables @xmath347 ) and a set of very short ones ( @xmath298 ) .",
    "when the lists are sufficiently short , the system operates in a slow - diffusion ( or `` clustering '' ) regime . in this case , three parameters should be kept track of in the experiment : @xmath238 , @xmath237 , and the number @xmath107 of long words in each list ( see section iii ) . while the recall probability of individual words depends on the details of the list , the average recall probability for short or long words should be affected by the list structure only through those three parameters , which completely characterize the list for the purposes of this type of experiment .",
    "it is then feasible to test qualitatively the following predictions :    \\1 .",
    "@xmath348 correlates positively with @xmath238 and negatively with @xmath237 .",
    "@xmath349 correlates positively with @xmath237 .",
    "these are straightforward consequences of ( [ ps ] ) , ( [ pl ] ) .    in experiments with words of two lengths",
    ", data may be interpolated with formulas ( [ ps ] ) and ( [ pl ] ) to test the theory and to fix the values of the internal parameters .",
    "serial position effects related to word - length can be isolated experimentally by using word lists constructed on two or more basic templates of segment - structure @xmath350 , with @xmath351 .",
    "the experimenter would present equivalent lists to a number of participants and would correlate the recall probability of words with their positions within each template structure .",
    "large data sets from such experiments should be able to corroborate or to rule out the mechanisms i have described .",
    "competing effects of familiar types , such as primacy and recency , may be easily subtracted .",
    "two simple predictions can be made , easier to check in a slow - diffusion regime but equally valid with faster diffusion :    \\1 .",
    "words from longer segments have a higher recall probability than words of the same type from shorter segments ;    \\2 .",
    "successful recall of words from one segment hinders the recall of words from others .",
    "the first prediction follows from eq .",
    "( [ pi ] ) , and should be easy to test .    the second prediction must be compared with data by computing the correlation functions @xmath352 , that is , the joint probability for the recall of the @xmath73-th and @xmath147-th words in the list .",
    "from there , it is straightforward to obtain the in - segment and cross - segment correlation functions :    @xmath353    and to verify whether @xmath354 , as the theory suggests .",
    "we may add to these predictions a third one  namely , the fact that the long word of each segment is the easiest one to recall .",
    "this results directly from the bounds we derived in the previous section on the ratio @xmath162 .",
    "a further prediction of the theory regards inter - response intervals  that , is the time elapsing between one recalled item and the next  whose measurement in free - recall experiments dates back to ( murdock and okada , 1970 ) .    during retrieval from long - term memory ,",
    "it was shown ( gruenewald and lockhead , 1980 ) that clusters occur due to stable semantic associations between objects : a subject who is asked to list some animals , for instance , may recall first a set of farm animals , then a number of house pets , then several birds .",
    "the inter - response intervals are shorter within clusters than between clusters .",
    "the retrieval of examples in the experiment of gruenewald and lockhead depends entirely on the long - term representation of items .",
    "in the situation we have described , on the contrary , short - term memory is at play , and the retrieval process has to locate the vanishing traces of a recent experience .",
    "nonetheless , it is easy to see that the time interval elapsing between the retrieval of two memories will be longer between two memories belonging to different clusters , and shorter for memories belonging to the same cluster .    hence , the same is true among items of the list that are successfully verbalized .",
    "the inter - response intervals will be longer for the consecutive recall of two words belonging to different segments of the list , and shorter for the consecutive retrieval of two words belonging to the same segment .",
    "as mentioned in the introduction , the wle has been reported in experiments where the time interval between the presentation of consecutive items was a controlled parameter .",
    "experiments on the wle with rapid presentation of the stimuli were first performed by coltheart and langdon ( 1998 ) , who found the wle by presenting an item every 114 ms , every 157 ms , and every 243 ms . in ( campoy , 2008 ) , somewhat lower presentation rates were used ( between 300 and 400 ms ) , and again the persistence of the effect was proven over different rehearsal times .    here , i will argue that such experiments may offer an ideal tool to study the crossover between the `` diffusive '' and the `` clustering '' regime .    indeed ,",
    "if we modify the foregoing computation to allow the system to random - walk on its own for a time @xmath355 between the presentation of the @xmath73-th and @xmath356-th items , this will be equivalent to increasing the average distance @xmath357 travelled between the memory @xmath133 generated by word @xmath43 and the memory @xmath358 generated by word @xmath358 .",
    "this amounts to rescaling time while replacing @xmath195 and @xmath194 with larger effective distances .",
    "the matrix elements of @xmath283 will decay faster as their indices grow .",
    "therefore , the system will move closer to the diffusive regime .    on the other hand ,",
    "the theory predicts ( section iiie ) that the curve @xmath359 will become linear in the fast - diffusion regime .",
    "hence , by reducing the presentation rate , one should see the two curves in fig .",
    "1 becoming progressively linearized , at least up to values of @xmath355 so large that not only the clustering , but also the contiguity effect breaks down . in this limit , moreover , eq . ( [ fast ] ) predicts that the curves @xmath3 and @xmath2 will become parallel .",
    "other ways of controlling the crossover between clustering and diffusing regime ( e.g. by pharmacological means , or through distractor tasks such as those of bjork and whitten , 1974 ) can be similarly applied .",
    "we have just examined some ways of testing the theory that are going to require ad - hoc experiments .",
    "let us conclude by mentioning one type of test that may be performed on already available databases  namely , data from the experiments performed so far on the classical wle .",
    "it is known that different people employ different strategies in order to exploit the structure of semantic space during memorization and recall ( healey et al . , 2014 ) . when it comes to the clustering described above",
    ", we expect its strength to be just as dependent on the subject considered .",
    "on the other hand , the strength of the classical wle is also a function of the particular subject .",
    "the difference between the recall probabilities of all - short lists and all - long lists , while being positive on average , will vary in magnitude from subject to subject .",
    "if the mechanism at the root of the of the wle is indeed a clustering phenomenon , we expect that it will be stronger for subjects for whom the clustering is stronger .",
    "let @xmath310 be again the serial positions of the words that have been recalled by the subject during a certain trial .",
    "the average serial - position jump between consecutively recalled words for this trial is @xmath360 . for each subject",
    ", we can define @xmath361 as the average of @xmath362 over all trials performed on him or her .",
    "the parameter @xmath361 may serve as a simple measure of the subject s inclination toward clustering .",
    "indeed , @xmath361 is close to unity for subjects strongly inclined toward clustering , and @xmath363 if the subject s tendency toward clustering is low .",
    "one way of gaining insight into the likelihood of the theory consists in correlating @xmath361 with the strength of the classical wle effects , controlled by @xmath364 , where @xmath219 and @xmath365 are the recall probabilities calculated for pure lists of short and long words .",
    "a negative correlation between these two variables would be a strong confirmation that the wle is indeed due to a clustering phenomenon .",
    "i have proposed a theory of verbal perception , extracted some of its properties , and validated it through a comparison with free - recall data .",
    "the theory is based on the notion that a word does not have , in general , a single meaning .",
    "a human subject exposed to a stream of verbal input will decide on the meaning of each new word on the basis of both the structure of its vocabulary and the meaning he / she has given to the words preceding it .",
    "this also applies to a list of random words , because our mind strives to interpret them as parts of a meaningful discourse .",
    "it may be instructive to think of such discourses as `` narratives '' .",
    "common experience tells us that a two - word list is already capable of creating a strong narrative sense ( e.g. : picnic , lightning ) .",
    "when a word in the list has no semantic connection to the context created by the words preceding it , the mind perceives a `` change of scenery '' and assumes that a _ new _ narrative is beginning .    a list of words is thus perceived as a collection of distinct `` stories '' .",
    "when prompted to recall the list , the subject remembers each story as a separate experience , and needs to re - experiences a given story before retrieving the words responsible for creating it .",
    "words that have specific meanings have obviously less probability of fitting into a randomly generated story .",
    "otherwise said , the words most likely to break the narrative are those with the highest level of localization in semantic space .",
    "we have argued that this correlates positively with word length .",
    "hence , a list of @xmath137 long words is likely to break into as many one - word stories , whereas a list of short words is more likely to be perceived as a single continuous narrative . since a single narrative is easier to recall than many unrelated ones , the standard word length effect ensues .",
    "the clustering property of short words is at play in mixed lists as well .",
    "but its effect is hindered by the presence of long words breaking the narrative . as our analysis has shown",
    ", this can lead to an inversion of the wle , encountered in experimental observations .    in this scenario ,",
    "the behavior depicted by figure 1 becomes quite logical . by replacing a short word with a long word ,",
    "one splits the list into a larger number of narratives , which makes every single word in the list ( whether short or long ) harder to reach during the retrieval process .",
    "the interplay between the trajectory of the system during the presentation of lists and the trajectory during the memory test produces a nontrivial spectrum of behaviors , highly dependent both on the structure of lists and on the amount of `` diffusion '' that interferes with clustering .",
    "a telltale symptom of these mechanisms is that a short word is more likely than a longer one to be recalled right after a word contiguous to it within the list .",
    "an analysis of data from the peers experiments ( healey and kahana , 2016 ) confirms this prediction .",
    "another prediction of the theory is that the serial - position jumps performed during recall will be enhanced by clustering for shorter words , and suppressed for longer ones .",
    "this is also confirmed by an analysis of peers data , across the whole available spectrum of jump lengths .",
    "several directions stand open for experimental and theoretical work on this model .",
    "experimentally , further tests may be obtained by performing the five types of measurements listed in section v. open directions for theoretical work include : 1 . generalizing the recall probability formulas to the case",
    "where the word lengths available are more than two ; 2 .",
    "including possible competition between words for the verbalization of a given state ; 3 . singling out extra effects from the fluctuations around the mean field behavior ; 4 .",
    "accounting for primacy and recency effects ; 5 .",
    "applying the same technique to predicting the genesis of false memories .    finally , by positing a suitable mechanism for spontaneous language production , it would be useful to derive equations linking the underlying word structure to emerging verbal patterns , thus providing a direct link between the hidden variables and the observables of the model .",
    "i am grateful to michael j. kahana and his university of pennsylvania research group for making their raw data publicly available .",
    "i would like to thank heartily people at the neurotheory center of columbia university ( ken miller , misha tsodyks ) for hosting me there in the fall of 2015 and for responding with enthusiasm to this theory .",
    "thanks also to yashar ahmadian of the university of oregon , for reading an early draft of the paper and suggesting several improvements .",
    "aurenhammer f. , klein r. , and lee d.t . ,",
    "voronoi diagrams and delaunay triangulations .",
    "singapore : world scientific publishing company .",
    "bhatarad p. , ward g. , smith j. hayes l. , 2009 .",
    "examining the relationship between free recall and immediate serial recall : similar patterns of rehearsal and similar effects of word length , presentation rate and articulatory suppression .",
    "memory and cognition 37 : 689 - 713 .",
    "friendly m. , franklin p.e . , hoffman d. , rubin d.c . , 1982 .",
    "the toronto word pool : norms for imagery , concreteness , orthographic variables , and grammatical usage for 1,080 words . behavior research methods and instrumentation , vol .",
    "14(4 ) , 375 - 399 .",
    "lohnas , l. j. and kahana , m. j. ( 2013 ) .",
    "parametric effects of word frequency effect in memory for mixed frequency lists .",
    "journal of experimental psychology : learning , memory , and cognition , 39 , 19431946 .",
    "j. , heli u. , and elts j. , 2001 .",
    "word length as an indicator of semantic complexity , in _ text as a linguistic paradigm : levels , constituents , constructs _ , festschrift in honour of ludek hrebcek .",
    "trier , 187 - 195 .",
    "neath i. , brown g. d. a. , 2006 .",
    "simple : further applications of a local distinctiveness model of memory , in the psychology of learning and motivation , ed .",
    "ross b. h. , editor .",
    "( san diego , ca : academic press ) , 201 - 243 ."
  ],
  "abstract_text": [
    "<S> a theoretical framework is proposed for the understanding of verbal perception  the conversion of words into meaning , modeled as a compromise between lexical demands and contextual constraints  and the theory is tested against experiments on short - term memory . </S>",
    "<S> the observation that lists of short words are recalled better than lists of long ones has been a long - standing subject of controversy , further complicated by the apparent inversion of the effect for mixed lists . in the framework here proposed , these behaviors emerge as an effect of the different level of localization of short and long words in semantic space . </S>",
    "<S> events corresponding to the recognition of a nonlocal word have a clustering property in phase space , which facilitates associative retrieval . </S>",
    "<S> the standard word - length effect arises directly from this property , and the inverse effect from its breakdown . </S>",
    "<S> an analysis of data from the peers experiments ( healey and kahana , 2016 ) confirms the main predictions of the theory . </S>",
    "<S> further predictions are listed and new experiments are proposed . </S>",
    "<S> finally , an interpretation of the above results is presented . </S>"
  ]
}