{
  "article_text": [
    "recently , we have proposed using class - adapted gaussian mixture priors to address several inverse imaging problems , such as deblurring and compressive imaging @xcite , by exploiting the so - called _ plug - and - play _ ( pnp ) approach @xcite .",
    "we proposed training a _",
    "gaussian mixture model _ ( gmm ) from external datasets containing images of particular classes ( such as text , faces , fingerprints ) , and then _ plug _ a gmm - based denoiser @xcite into the iterations of an _ alternating direction method of multipliers _ ( admm ) @xcite , obtaining state - of - the - art results on images of the specific classes considered .",
    "we have recently shown that the same machinery can be used to address a different task : hyperspectral image sharpening ( also known as fusion ) @xcite . in this problem ,",
    "the goal is to fuse two types of data describing the same scene , thus we can use one of them to train a model that is targeted to that scene . under the hypothesis that both data have the same spatial structure ( since they are imaging the same scene ) , we can leverage this model to reconstruct the other type of data . hence the designation _ scene - adapted _ prior .",
    "this paper differs from @xcite in two major aspects .",
    "first , following the same rationale as in @xcite , we propose a modification of the gmm denoiser used in @xcite , which can be interpreted as keeping the dictionary support fixed throughout the iterations of admm .",
    "second , although we observed convergence in practice in @xcite , this modification enables us to theoretically guarantee convergence .",
    "the paper is organized as follows .",
    "section  [ sec : formulation ] formulates the addressed problem while section  [ sec : pnp ] shows how to tackle it using a pnp - admm scheme .",
    "sections  [ sec : method ] and [ sec : conv ] describe the adopted denoiser and show that the proposed pnp - admm is guaranteed to converge to a minimum of an implicitly defined objective function . in section  [ sec : results ] , we present experimental results showing that the proposed method performs competitively with , in some cases better than , another state - of - the - art method .",
    "section [ sec : conclusion ] concludes the paper .",
    "as mentioned above , hyperspectral image sharpening is a data fusion problem where the goal is to combine two types of observed data : a _ hyperspectral _ ( hs ) data cube , with very high spectral resolution , but low spatial resolution ; a _ multispectral _ ( ms ) data cube , with high spatial resolution , but low spectral resolution .",
    "the observation model underlying this data fusion problem assumes that the hs bands are blurred and down - sampled versions of a target cube with high spectral and spatial resolution , whereas the ms bands are spectrally degraded versions of the same target cube , due to the spectral responses of the sensor(s ) .",
    "the forward model is thus @xmath0 where @xmath1 is the target cube to be estimated ( with rows indexing bands , and columns indexing spatial locations ) , @xmath2 is the observed hs data , @xmath3 models a spatial convolution , @xmath4 is a sub - sampling operator , @xmath5 is the observed ms data , @xmath6 models the spectral responses of the ms sensor , and @xmath7 and @xmath8 are gaussian noises of known variance . for simplicity , we assume that the sensor has an invariant point spread function across the @xmath9 spectral bands and the matrices @xmath10 and @xmath11 are known .",
    "estimation of those matrices is addressed in @xcite .",
    "another common feature of hs data is that it lives , with very good approximation , in a subspace of lower dimension , say @xmath12 , than the number of bands @xmath9 @xcite , and this subspace can be accurately inferred from the hs data cube @xcite .",
    "therefore , we may write @xmath13 , where @xmath14 . instead of estimating @xmath15 directly , we infer @xmath16 , denoted latent or coefficient image , and then recover the target image . with this simplification ,",
    "the forward model becomes @xmath17    to the best of our knowledge , the current best methods for hyperspectral fusion seek a _",
    "maximum a posteriori _ ( or regularized ) estimate of @xmath16 as @xmath18 where @xmath19 is the negative log - prior ( or regularizer ) , while @xmath20 and @xmath21 control the relative weight of each term .",
    "the regularizer can be chosen to induce spatial coherence ( _ e.g. _ , via tv regularization @xcite ) or some form of sparse representation on a dictionary @xcite .    in",
    ", we implicitly assumed that the components of the noise matrices @xmath7 and @xmath8 are independent and gaussian , with zero - mean and constant variance .",
    "other forms of noise correlations , decoupled with respect to the bands and to the pixels , are easily accommodated in our formulation ( see , for example , @xcite ) .",
    "the tool of choice to solve has been admm @xcite . due to space limitations",
    ", we do not describe the derivations leading to this instance of admm ( for details , see @xcite and references therein ) . in summary",
    ", admm solves a sequence of sub - problems w.r.t .",
    "each of the primal variables , while keeping the others fixed , followed by updating the dual variables .",
    "this process is repeated until some criterion is satisfied .",
    "the admm algorithm that solves has the form @xmath22 @xmath23 where @xmath24 and @xmath25 are the primal variables , @xmath26 are the scaled dual variables , and @xmath27 is a penalty parameter .",
    "the first three problems are quadratic , with closed - form solutions , @xmath28\\left [ { { \\bf b}}{{\\bf b}}^t   \\!+ \\ ! 2\\ , { { \\bf i}}\\right]^{-1 } , \\\\ { { \\bf v}}_1^{k+1 } & = \\left[{{\\bf e}}{{\\bf e}}^t + \\rho{{\\bf i}}\\right]^{-1}\\left[{{\\bf e}}^t{{\\bf y}}_h + \\rho\\left({{\\bf x}}^{k+1}{{\\bf b}}- { { \\bf d}}_1^{k}\\right)\\right]\\odot{{\\bf m}}\\notag \\\\ & \\quad + \\left({{\\bf x}}^{k+1}{{\\bf b}}- { { \\bf d}}_1^{k}\\right)\\odot \\left(1-{{\\bf m}}\\right),\\\\ { { \\bf v}}_2^{k+1 } & = \\left[\\lambda{{\\bf e}}^t{{\\bf r}}^t{{\\bf r}}{{\\bf e}}+ \\rho{{\\bf i}}\\right]^{-1}\\left[\\lambda{{\\bf e}}^t{{\\bf r}}^t{{\\bf y}}_m + \\rho\\left({{\\bf x}}^{k+1 } - { { \\bf d}}_2^{k}\\right)\\right],\\end{aligned}\\ ] ] where the symbol @xmath29 denotes entry - wise ( hadamard ) product .",
    "the optimization with respect to @xmath30 corresponds to the _ proximity operator _ of the regularizer @xmath19 @xcite ( which can be seen as a denoising operator ) acting on @xmath31 .",
    "the usual choice for @xmath19 is a convex regularizer , for two main reasons : computational tractability and convergence guarantees for admm @xcite .",
    "the fact that state - of - the - art denoisers ( such as the famous bm3d @xcite ) can not , in general , be formulated as the proximity operators of convex regularizers has motivated the pnp approach @xcite , where the proximity operator is replaced with a black - box denoising algorithm .    in this paper",
    ", we adopt a pnp approach using a patch - based _ minimum mean squared error _ ( mmse ) denoiser with a gmm prior @xcite .",
    "a gmm prior can either be learned directly from noisy image patches @xcite or from an external set of clean patches @xcite .",
    "in particular , this latter possibility motivated our class - adapted gmm priors @xcite , where the external set contains images from of a specific class ( or classes ) . here",
    ", we take this gmm - based prior adaptation one step further , by considering a scene - adapted prior .",
    "specifically , we use a gmm prior learned from the ms images ( the bands in @xmath32 ) , under the assumption that they have the same spatial statistical properties as the underlying unknown image , since they are acquired from the same scene .",
    "a key fact motivating the use of gmm priors is that the corresponding mmse estimate , from observations with gaussian noise , can be computed in closed - form @xcite . letting @xmath33 and @xmath34",
    "be an arbitrary patch of the unknown clean image and a noisy version thereof ( zero - mean gaussian noise , with variance @xmath35 ) , the mmse estimate of @xmath36 is @xmath37 = \\sum_{m=1}^k \\beta_m({\\bf y}_i ) \\ ; { \\bf v}_m({\\bf y}_i),\\label{eq : mmse}\\ ] ] where @xmath38 and @xmath39 in eqs .",
    "- , @xmath40 is the number of mixture components , @xmath41 denotes a gaussian probability density function of mean @xmath42 and covariance @xmath43 , and @xmath44 is the weight of the @xmath45-th mixture component .    for simplicity",
    ", we use only zero - mean gaussian components ( thus a zero - mean gmm ) , _ i.e. _ , @xmath46 for @xmath47 .",
    "this is reasonable since the variance of the noise affecting the mean of each patch is only @xmath48 , where @xmath49 is the number of pixels in each patch .",
    "hence , we can remove the mean of each noisy patch , obtain the mmse estimate , and add back the mean .",
    "the final task of the denoiser is to combine the overlapping patch estimates @xmath50 to obtain a final image @xmath51 , which can be formulated as an optimization problem , @xmath52 where @xmath53 is a binary matrix that extracts the @xmath54-th patch from the image ( thus @xmath55 puts the patch back into its place ) and @xmath56 is the number of patches .",
    "the solution to is @xmath57 assuming that the patches are extracted with unit step and periodic boundary conditions , every pixel belongs to @xmath49 patches and @xmath58 . in this case , corresponds to combining the overlapping estimates of each pixel using a straight average .",
    "extension to other patch extraction schemes is straightforward ; in any case , as long as all pixels belong at least to one patch , the matrix inversion in is well defined as yields a diagonal matrix .",
    "having chosen a denoiser to plug into admm , we turn our attentions to the convergence of the resulting algorithm . in @xcite ,",
    "the authors provide two sufficient conditions on the denoiser that ensure convergence to a global minimum of some implicitly defined objective function : it should be non - expansive is _ non - expansive _ if , for any @xmath59 , @xmath60 ] and the sub - gradient of a convex function . in practice , these conditions are restrictive and hard to verify by a black - box denoiser .",
    "the gmm denoiser described in section  [ sec : method ] is not non - expansive , as shown in a simple one - dimensional counter - example in fig .",
    "[ fig : nonlinear ] , which plots the mmse estimate @xmath61 in , as a function of the noisy observed @xmath62 , under a gmm prior with 2 zero mean components .",
    "clearly , the presence of regions of the function with derivative larger than 1 shows that it is not non - expansive .",
    "note that this does not invalidate the convergence of the pnp - admm with the gmm - denoiser ; in fact , we observe convergence in practice and , under some mild conditions , such as those proposed in @xcite , we can guarantee that the iterates converge to a fixed point .",
    "the failure to be non - expansive of the mmse denoiser can be traced to the non - linearity of the weights @xmath63 .",
    "in fact , each function @xmath64 defined in is itself non - expansive , because the eigenvalues of @xmath65 are no larger than 1 ; thus , if the weights are fixed ( @xmath66 ) , the corresponding convex combination of non - expansive operators is non - expansive @xcite , as illustrated with a one - dimensional example in fig .",
    "[ fig : fixed ] .",
    "we claim that , if the hyperspectral fusion problem in eqs .  ",
    "is addressed with a gmm prior learned from the ms images , it makes sense to keep the weights @xmath67 of each patch equal to its value for the corresponding patch of @xmath32 . in other words ,",
    "we use the weights as if we were denoising the high resolution ms image , with a gmm trained from its noisy patches .",
    "in fact , another instance of this idea was previously used in @xcite . there , the authors propose an approach to the same problem , based on sparse representations of the patches on a dictionary learned from the ms images , but instead of recomputing the sparse support of the dictionary and the corresponding weights , the support is kept fixed and equal to the support of the representation of the corresponding patch in the ms images .",
    "we stress that this approach is only reasonable in the case of image denoising or image reconstruction using scene - adapted priors , where the training and target images are of the same scene , and thus have a similar spatial structure .    recalling that @xmath68",
    ", for @xmath69 , the patch estimates are @xmath70 with @xmath71 and @xmath72 , for @xmath73 , computed at the training stage , and kept constant thereafter .",
    "note that although the weights @xmath71 no longer dependent on the values in patch @xmath74 , we introduce a superscript to denote that each patch has a different set of fixed weights @xmath75 , such that @xmath76 . the second equality uses matrix @xmath77 , which extracts patch @xmath74 from @xmath78 .",
    "plugging eq . into yields @xmath79 showing that the denoised image is a linear function of the noisy one . clearly , @xmath80 is symmetric , as is obvious from its definition and the fact that each @xmath81 is symmetric because it is a convex combination of symmetric matrices .    in @xcite ,",
    "a very similar formula was presented for approximate map estimation under a gmm prior , where the authors include a parameter to avoid numerical instability when the noise level is very small .",
    "moreover , theorem 1 in @xcite remains valid here .",
    "recall that the weights @xmath82 , although fixed , are non - negative and sum to one , and that the covariance matrices of the gmm components are positive - semidefinite ( p.s.d . ) .",
    "in fact , since we train the model using a low - noise panchromatic image ( as will be seen in the next section ) , most , if not all , singular values of each @xmath72 are stricly positive ; besides , adding a small constant to the diagonal to ensure this holds does not change the obtained results whatsoever .",
    "hence , every matrix @xmath81 ( defined in eq . ) is a convex combination of positive - definite ( p.d . )",
    "matrices , thus also a p.d . matrix .    using the eigendecomposition @xmath83 of each covariance matrix , where @xmath84 contains the eigenvalues of @xmath72 , sorted in non - increasing order , we may write @xmath85 where @xmath86 is a diagonal matrix . consequently , all the eigenvalues of @xmath87 are in @xmath88 \\in ( 0,1),\\ ] ] since @xmath89 and @xmath90 . from , each @xmath81 is a convex combination @xmath91 weyl s inequality @xcite implies that the eigenvalues of a convex combination of symmetric matrices is bounded below ( above ) by the same convex combination of the smallest ( largest ) eigenvalues of those matrices .",
    "consequently , the eigenvalues of @xmath81 are all in @xmath92 , that is , @xmath81 is a p.d . matrix and @xmath93 .",
    "we now show that the same is true for @xmath80 .",
    "consider the following decomposition : @xmath94 where @xmath95 denotes a set of non - overlapping patches , and the sum over @xmath96 accounts for all possible shifts of the image , as in @xcite .",
    "when considering non - overlapping patches , there is a permutation of the image pixels that allows writing @xmath97 as a block - diagonal matrix , where the blocks are the @xmath98 matrices , with @xmath99 .",
    "since the set of eigenvalues of a block - diagonal matrix is the union of the sets of eigenvalues of its blocks , the eigenvalues of each @xmath97 are within the same bounds as the eigenvalues of the @xmath98 matrices , i.e. , they are all in @xmath92 .",
    "finally , invoking weyl s inequality once again , we can conclude the eigenvalues of @xmath80 are bounded above ( below ) by the average of the largest ( smallest ) eigenvalues of each @xmath97 , thus also in @xmath92 .    summarizing",
    ", we have shown that the gmm denoiser , with the proposed modification ( fixed weights ) , is a non - expansive operator .",
    "non - expansiveness is one of the sufficient conditions stated in @xcite for the convergence of pnp - admm .",
    "following the same reasoning as in @xcite , we can also prove that the denoiser is the sub - gradient of some convex function , and thus it is a proximity operator @xcite , ensuring that admm converges to a global minimum , if one exists .",
    "in fact , we can guarantee that a minimum does exist because the denoiser described above is contractive : @xmath100 . recall that the proximity operator of @xmath101 is defined as @xmath102 from",
    ", we see that the denoiser is linear w.r.t . to @xmath78 , so a possible regularizer is @xmath103 .",
    "in fact , if @xmath103 , the proximity operator becomes @xmath104 comparing to , we obtain @xmath105 , where the inverse is well defined since @xmath106 . moreover , since @xmath100 , matrix @xmath107 is also p.d .",
    ", thus @xmath19 is a coercive function , ensuring that a solution to exists @xcite .",
    "finally , for the convergence of admm to be guaranteed , we must consider that the denoiser is always the same , thus parameter @xmath35 must be kept constant . in practice",
    ", this is not restrictive as long as we consider that , after a number of iterations , @xmath35 converges to some value .",
    "the first few iterations can then be seen as a transient stage , which in fact only changes the initial estimate of the algorithm .",
    "in this section , we compare the hyperspectral sharpening results using the gmm - based denoiser from @xcite , the gmm denoiser herein proposed , and the dictionary - based method from @xcite ( which , to the best of our knowledge , holds the state - of - the - art ) .",
    "we measured the performance of all of the above using three different metrics , namely ergas ( _ erreur relative globale adimensionnelle de synthse _ ) , sam ( _ spectral angle mapper _ ) , and sre ( _ signal to reconstruction - error _ ) @xcite , in four different settings .",
    "in the first experiment , we considered sharpening the hyperspectral images using the panchromatic ( pan ) image , at a signal - to - noise ratio ( snr ) of 50db , on both the hs and pan images .",
    "the second experiment refers to pan - sharpening as well , this time with 35db snr on the first 43 hyperspectral bands and 30db on the 50 remaining hs bands and pan image , as in @xcite .",
    "experiments 3 and 4 consider the same noise levels as in experiments 1 and 2 , respectively , but the hs bands are sharpened based on 4 ms bands , namely the r , g , b , and near - infrared channels . in every experiment , a gmm with 20 components was learned from the pan image , where the pan and ms images were generated from the original hs bands , using ikonos and landsat spectral responses .",
    "table  [ tab : sharp2 ] describes the results on a cropped area of the _ rosis pavia university _ and _ moffett field",
    "_ datasets .",
    "the three methods have comparable performances , with the proposed method being slightly better .",
    "furthermore , the results strongly support the hypothesis that the target image has the same spatial structure as the panchromatic image used to train the model , otherwise keeping the posterior mixture weights would not lead to state - of - the - art results .",
    "figures  [ fig : orig ] to [ fig : rec ] illustrate the results from experiment 4 on the moffett field dataset , in terms of visual quality , yet the differences are not noticeable to the naked eye .",
    "another way to visualize the difference in performance is to plot the ( sorted ) errors for each pixel and to notice that the proposed method has a larger number below a given threshold , _",
    "i.e. _ , in figure  [ fig : plotnorms2 ] the dashed curve is below the solid curve , and furthermore , in figures  [ fig : err1 ] and [ fig : err2 ] the dashed curve is closer ( smaller error ) to the dotted one , representing the true pixel value across all bands , than the solid line .",
    "this paper leveraged from the flexibility provided by the plug - and - play framework to address a data fusion problem known as hyperspectral image sharpening .",
    "for this specific problem , and motivated by recent results using class - adapted gmm priors on several image restoration / reconstruction problems @xcite , we introduce what we called scene - adapted priors . as the name suggests",
    ", the model is learned from an image over the same scene we aim to reconstruct .    in addition , we propose some modifications for the gmm - based denoiser that allow us to show that the pnp - admm algorithm is guaranteed to converge to a global minimum of a cost function , defined by the choice of the denoiser itself .",
    "experimental results show that the proposed method outperforms another state - of - the - art algorithm based on sparse representations on learned dictionaries @xcite , for most of the test settings .",
    "moreover , the modifications also improve the results over the gmm - based denoiser used in @xcite , giving supporting evidence to the hypothesis that the panchromatic image constitutes a very useful training example , as the hyperspectral bands ( and corresponding latent images ) share a similar spatial structure .",
    "m.  afonso , j.  bioucas - dias , m.  figueiredo ,  an augmented lagrangian approach to the constrained optimization formulation of imaging inverse problems \" , _ ieee trans .",
    "image processing , _ vol .",
    "3 , pp . 681 - 695 , 2011 .        j.  bioucas - dias , a.  plaza , n.  dobigeon , m.  parente , q.  du , p.  gader , j.  chanussot , `` hyperspectral unmixing overview : geometrical , statistical , and sparse regression - based approaches , '' , vol .  5 , pp .  354379 , 2012 .",
    "l.  loncan , l.  almeida , j.  bioucas - dias , x.  briottet , j.  chanussot , n.  dobigeon , s.  fabre , w.  liao , g.  licciardi , m.  simes , j .- y .",
    "tourneret , m.  veganzones , g.  vivone , q.  wei , n.  yokoya , ",
    "hyperspectral pansharpening : a review \" , , vol .  3 , pp .",
    "2746 , 2015 .",
    "s.  sreehari , s.  venkatakrishnan , b.  wohlberg , g.  buzzard , l.  drummy , j.  simmons , c.  bouman  plug - and - play priors for bright field electron tomography and sparse interpolation \" , , vol .  2 , no .  4 , pp .",
    "408423 , 2016 ."
  ],
  "abstract_text": [
    "<S> this paper tackles a hyperspectral data fusion problem , using the so - called plug - and - play approach , which combines an admm algorithm with a denoiser based on a gaussian mixture prior . </S>",
    "<S> we build upon the concept of scene - adapted prior where , as the name suggests , we learn a model that is targeted to the specific scene being imaged , and show state - of - the - art results on several hyperspectral sharpening experiments . additionally , we prove that the algorithm is guaranteed to converge .    </S>",
    "<S> = 1    data fusion , hyperspectral sharpening , plug - and - play , gaussian mixture , scene - adapted prior . </S>"
  ]
}