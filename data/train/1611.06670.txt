{
  "article_text": [
    "inspired from fredholm integral equations , fredholm learning algorithms are designed recently for density ratio estimation @xcite and semi - supervised learning @xcite .",
    "fredholm learning can be considered as a kernel method with data - dependent kernel .",
    "this kernel usually is called as fredholm kernel , and can naturally incorporate the data information . although its empirical performance has been well demonstrated in the previous works , there is no learning theory analysis on generalization bound and learning rate .",
    "it is well known that generalization ability and learning rate are important measures to evaluate the learning algorithm @xcite . in this paper , we focus on this theoretical theme for regularized least square regression with fredholm kernel .    in learning theory literature , extensive studies have been established for least square regression with regularized kernel methods , e.g. , @xcite .",
    "although the fredholm learning in @xcite also can be considered as a regularized kernel method , there are two key features : one is that fredholm kernel is associated with the  inner \" kernel and the  outer \" kernel simultaneously , the other is that for the prediction function is double data - dependent .",
    "these characteristics induce the additional difficulty on learning theory analysis . to overcome the difficulty of generalization analysis",
    ", we introduce novel stepping - stone functions and establish the decomposition on excess generalization error .",
    "the generalization bound is estimated in terms of the capacity conditions on the hypothesis spaces associated with the  inner \" kernel and the  outer \" kernel , respectively .",
    "in particular , the derived result implies that fast learning rate with @xmath1 can be reached with proper parameter selection , where @xmath2 is the number of labeled data .",
    "to best of our knowledge , this is the first discussion on generalization error analysis for learning with fredholm kernel .",
    "the rest of this paper is organized as follows .",
    "regression algorithm with fredholm kernel is introduced in section [ section2 ] and its generalization analysis is presented in section [ section3 ] .",
    "the proofs of main results are listed in section [ section4 ] .",
    "simulated examples are provided in section [ section5 ] and a brief conclusion is summarized in section [ section6 ] .",
    "let @xmath3 be a compact input space and @xmath4 $ ] for some constant @xmath5 .",
    "the labeled data @xmath6 are drawn independently from a distribution @xmath7 on @xmath8 and the unlabeled data @xmath9 are derived random independently according to the marginal distribution @xmath10 on @xmath11 .",
    "given @xmath12 , the main purpose of semi - supervised regression is to find a good approximation of the regression function @xmath13    in learning theory , @xmath14 and its discrete version @xmath15 are called as the expected risk and the empirical risk of function @xmath16 , respectively .",
    "let @xmath17 be a continuous bounded function on @xmath18 with @xmath19 .",
    "define the integral operator @xmath20 as @xmath21 where @xmath22 is the space of square - integrable functions .",
    "let @xmath23 be a reproducing kernel hilbert space ( rkhs ) associated with mercer kernel @xmath24 .",
    "denote @xmath25 as the corresponding norm of @xmath23 and assume the upper bound @xmath26 .",
    "if choose @xmath27 as the hypothesis space , the learning problem can be considered as to solve the fredhom integral equation @xmath28 .",
    "sine the distribution @xmath7 is unknown , we consider the empirical version of @xmath29 associated with @xmath30 , which is defined as @xmath31    in the fredholm learning framework , the prediction function is constructed from the data dependent hypothesis space @xmath32    given @xmath33 , least - square regression with fredholm kernel ( lfk ) can be formulated as the following optimization @xmath34 where @xmath35 is a regularization parameter .",
    "equation ( [ algorithm1 ] ) can be considered as a discrete and regularized version of the fredholm integral equation @xmath36 .",
    "when @xmath37 is the @xmath38-function , ( [ algorithm1 ] ) becomes the regularized least square regression in rkhs @xmath39 when @xmath40 and replacing @xmath41 with @xmath42 , ( [ algorithm1 ] ) is equivalent to the data - dependent coefficient regularization @xmath43 where @xmath44 it is well known that ( [ algorithm2 ] ) and ( [ algorithm3 ] ) have been studied extensively in learning literatures , see , e.g. @xcite .",
    "these results relied on error analysis techniques for data independent hypothesis space @xcite and data dependent hypothesis space @xcite , respectively .",
    "therefore , the fredholm learning provides a novel framework for regression related with the data independent space @xmath23 and the data dependent hypothesis space @xmath45 simultaneously .",
    "equation ( [ algorithm1 ] ) involves the  inner \" kernel @xmath46 and the  outer \" kernel @xmath37 .",
    "denote @xmath47 @xmath48 , and @xmath49 .",
    "it has been demonstrated in @xcite that @xmath50 where @xmath51 .",
    "therefore , fredholm regression in ( [ algorithm1 ] ) can be implemented efficiently and the data - dependent kernel @xmath52 is called fredholm kernel in @xcite .",
    "to provide the estimation on the excess risk , we introduce some conditions on the hypothesis space capacity and the approximation ability of fredholm learning framework .    for @xmath53 , denote @xmath54 and @xmath55 for any @xmath56 and function space @xmath57 , denote @xmath58 as the covering number with @xmath59-metric .",
    "( capacity condition ) for the  inner \" kernel @xmath46 and the  outer \" kernel @xmath37 , there exists positive constants @xmath60 and @xmath61 such that for any @xmath56 , @xmath62 and @xmath63 , where @xmath64 are constants independent of @xmath65 .",
    "[ condition1 ]    it is worthy notice that the capacity condition has been well studied in @xcite . in particular",
    ", this condition holds true when setting the  inner \" and  outer \" kernels as gaussian kernel .    for a function @xmath66 and @xmath67 ,",
    "denote the @xmath68-norm on @xmath69 as @xmath70    define the data independent regularized function @xmath71    the predictor associated with @xmath72 is @xmath73 and the approximation ability of fredholm scheme in @xmath23 is characterized by @xmath74    ( approximation condition ) there exists a constant @xmath75 $ ] such that @xmath76 where @xmath77 is a positive constant independent of @xmath78.[condition2 ]    this approximation condition relies on the regularity of @xmath79 , and has been investigated extensively in @xcite . to get tight estimation",
    ", we introduce the projection operator @xmath80    it is a position to present the generalization bound .    under assumptions [ condition1 ] and [ condition2 ] , there exists @xmath81 where @xmath82 is a positive constant independent of @xmath83 [ theorem1 ]    the generalization bound in theorem [ theorem1 ] depends on the capacity condition , the approximation condition , the regularization parameter @xmath78 , and the number of labeled data . in particular , the labeled data is the key factor on the excess risk without the additional assumption on the marginal distribution .",
    "this observation is consistent with the previous analysis for semi - supervised learning @xcite .    to understand the learning rate of fredholm regression , we present the following result where @xmath78 is chosen properly .",
    "under assumptions [ condition1 ] and [ condition2 ] , for any @xmath84 , with confidence @xmath85 , there exists some positive constant @xmath86 such that @xmath87 where @xmath88 [ theorem2 ]    theorem [ theorem2 ] tells us that fredholm regression has the learning rate with polynomial decay .",
    "when @xmath89 , there exists some constant @xmath90 such that @xmath91 with confidence @xmath85 , where @xmath92 ; \\\\                         \\frac{2\\beta}{s+2\\beta+s\\beta } , & \\beta\\in(\\frac{2}{2+s},+\\infty ] .",
    "\\\\                         \\end{array }                       \\right.\\end{aligned}\\ ] ] and the rate is derived by setting @xmath93 ; \\\\",
    "l^{-\\frac{2}{s+2\\beta+s\\beta } } , & \\beta\\in(\\frac{2}{2+s},+\\infty ] .",
    "\\\\                         \\end{array }                       \\right.\\end{aligned}\\ ] ] this learning rate can be arbitrarily close to @xmath94 as @xmath60 tends to zero , which is regarded as the fastest learning rate for regularized regression in the learning theory literature .",
    "this result verifies the lfk in ( [ algorithm1 ] ) inherits the theoretical characteristics of least square regularized regression in rkhs @xcite and in data dependent hypothesis spaces @xcite .",
    "we first present the decomposition on the excess risk @xmath95 , and then establish the upper bounds of different error terms .      according to the definitions of @xmath96",
    ", we can get the following error decomposition .",
    "[ proposition1 ] for @xmath97 defined in ( [ algorithm1 ] ) , there holds @xmath98 where @xmath99 and @xmath100    * proof * : by introducing the middle function @xmath101 , we get @xmath102\\\\ & & + \\mathcal e_{\\mathbf z}(l_{w,\\mathbf{x}}f_{\\lambda})-\\mathcal e(l_{w,\\mathbf{x}}f_{\\lambda})+\\mathcal e(l_{w,\\mathbf{x}}f_{\\lambda})-\\mathcal e(l_{w}f_{\\lambda } ) + \\mathcal e(l_{w}f_{\\lambda})-\\mathcal e(f_\\rho)+\\lambda\\|f_{\\lambda}\\|_k^2 \\\\ & \\leq & e_1+e_2+e_3+d(\\lambda)\\end{aligned}\\ ] ] where the last inequality follows from the definition @xmath103 . this completes the proof.@xmath104    in learning theory , @xmath105 are called the sample error , which describe the difference between the empirical risk and the expected risk .",
    "@xmath106 is called the hypothesis error which reflects the divergence of expected risks between the data independent function @xmath107 and data dependent function @xmath108 .",
    "we introduce the concentration inequality in @xcite to measure the divergence between the empirical risk and the expected risk .",
    "let @xmath109 be a measurable function set on @xmath110 .",
    "assume that , for any @xmath111 , @xmath112 and @xmath113 for some positive constants @xmath114 .",
    "if for some @xmath115 and @xmath116 , @xmath117 for any @xmath56 , then there exists a constant @xmath118 such that for any @xmath119 , @xmath120 with confidence at least @xmath121",
    ". [ lemma1 ]    to estimate @xmath122 , we consider the function set containing @xmath103 for any @xmath123 , @xmath124 .",
    "the definition @xmath103 in ( [ algorithm1 ] ) tells us that @xmath125 .",
    "hence , @xmath126 with @xmath127 and @xmath128 .",
    "[ proposition2 ] under assumption [ condition1 ] , for any @xmath84 , @xmath129 with confidence @xmath85 .",
    "* proof * : for @xmath130 , denote @xmath131 for any @xmath132 , @xmath133 moreover , @xmath134 for any @xmath135 , there exists @xmath136 this relation implies that @xmath137 where the last inequality from assumption [ condition1 ] .    applying the above estimates to lemma [ lemma1 ]",
    ", we derive that @xmath138 with confidence @xmath85 .    considering @xmath139 with @xmath127 , we obtain the desired result .",
    "@xmath104    [ proposition3 ] under assumption 1 , with confidence @xmath140 , there holds @xmath141 where @xmath142 is a positive constant independent of @xmath143 .    * proof * : denote @xmath144    from the definition @xmath145",
    ", we can deduce that @xmath146 with @xmath147 .",
    "for @xmath148 , define @xmath149 it is easy to check that for any @xmath132 @xmath150    then , @xmath151    for any @xmath152 , there exists @xmath153    then from assumption [ condition1 ] , @xmath154    combining ( [ p11])-([p33 ] ) with lemma [ lemma1 ] , we get with confidence @xmath85 @xmath155    considering @xmath156 , we get the desired result.@xmath104      the following concentration inequality with values in hilbert space can be found in @xcite , which is used in our analysis .",
    "let @xmath157 be a hilbert space and @xmath158 be independent random variable on @xmath159 with values in @xmath157 .",
    "assume that @xmath160 almost surely .",
    "let @xmath161 be independent random samples from @xmath7 .",
    "then , for any @xmath162 , @xmath163 holds true with confidence @xmath85 .",
    "now we turn to estimate @xmath106 , which reflects the affect of inputs @xmath30 to the regularization function @xmath145 .",
    "[ proposition ] for any @xmath84 , with confidence @xmath85 , there holds @xmath164    * proof * : note that @xmath165    denote @xmath166 , which is continuous and bounded function on @xmath69 .",
    "then @xmath167 and @xmath168    we can deduce that @xmath169 and @xmath170 . from lemma [ lemma2 ] , for any @xmath84 , there holds with confidence @xmath85 @xmath171    combining ( [ p111 ] ) and ( [ p222 ] ) , we get with confidence @xmath85 , @xmath172    then , the desired result follows from @xmath173 .",
    "@xmath104      * proof of theorem 1 : * combining the estimations in propositions 1 - 4 , we get with confidence @xmath174 , @xmath175    considering @xmath176 , for @xmath84 , we have with confidence @xmath174 @xmath177,\\end{aligned}\\ ] ] where @xmath82 is a constant independent of @xmath83 .",
    "* proof of theorem 2 : * when setting @xmath178 , we obtain @xmath179 .",
    "then , theorem [ theorem1 ] implies that @xmath180    when setting @xmath181 , we get @xmath182 .",
    "then , with confidence @xmath174 @xmath183 this complete the proof of theorem 2 .",
    "to verify the effectiveness of lfk in ( [ algorithm1 ] ) , we present some simulated examples for the regression problem . the competing method is support vector machine regression ( svm ) , which has been used extensively used in machine learning community ( https://www.csie.ntu.edu.tw/  cjlin / libsvm/ ) .",
    "the gaussian kernel @xmath184 is used for svm .",
    "for lfk in ( [ algorithm1 ] ) , we consider the following `` inner '' and `` outer '' kernels :    * lfk1 : @xmath185 and @xmath186 .",
    "* lfk2 : @xmath187 and @xmath188 . * lfk3 : @xmath187 and @xmath186 .",
    "here the scale parameter @xmath189 belongs to @xmath190 $ ] and the regularization parameter belongs to @xmath191 $ ] for lfk and svm .",
    "these parameters are selected by 4-fold cross validation in this section .",
    "the following functions are used to generate the simulated data : @xmath192\\\\ f_2(x)&=&xcos(x),~~x\\in[0,10]\\\\ f_3(x)&=&\\min(2|x|-1,1),~~x\\in[-2,2]\\\\ f_4(x)&=&sign(x),~~x\\in[-3,3].\\\\\\end{aligned}\\ ] ] note that @xmath193 is highly oscillatory , @xmath194 is smooth , @xmath195 is continuous not smooth , and @xmath196 is not even continuous .",
    "these functions have been used to evaluate regression algorithms in @xcite .",
    "c|ccccc function & number & svm & lfk1 & lfk2 & lfk3 + @xmath193 & 50 & @xmath197 & @xmath198 & @xmath199 & @xmath200 + & 300 & @xmath201 & @xmath202 & @xmath203 & @xmath204 + @xmath194 & 50 & @xmath205 & @xmath206 & @xmath207 & @xmath208 + & 300 & @xmath209 & @xmath210 & @xmath211 & @xmath212 + @xmath195 & 50 & @xmath213 & @xmath214 & @xmath215 & @xmath216 + & 300 & @xmath217 & @xmath218 & @xmath219 & @xmath220 + @xmath196 & 50 & @xmath221 & @xmath222 & @xmath223 & @xmath224 + & 300 & @xmath225 & @xmath226 & @xmath227 & @xmath228 +    [ tab1 ]    in our experiment , gaussian noise @xmath229 is added to the data respectively . in each test , we first draw randomly 1000 samples according to the function and noise distribution , and then obtain a training set randomly with sizes @xmath230 respectively .",
    "three hundred samples are selected randomly as the test set .",
    "mean squared error _ ( mse ) is used to evaluate the regression results on synthetic data . to make the results more convincing",
    ", each test is repeated 10 times .",
    "table [ tab1 ] reports the average mse and _ standard deviation _ ( std ) with 50 training samples and 300 training samples respectively .",
    "furthermore , we study the impact of the number of training samples on the final regression performance .",
    "figure 1 shows the mse for learning @xmath231 with numbers of training samples .",
    "these results illustrate that lfk has competitive performance compared with svm .",
    "this paper investigated the generalization performance of regularized least square regression with fredholm kernel .",
    "generalization bound is presented for the fredholm learning model , which shows that the fast learning rate with @xmath0 can be reached . in the future , it is interesting to investigate the leaning performance of ranking @xcite with fredholm kernel .      the authors would like to thank prof.dr.l.q .",
    "li for his valuable suggestions .",
    "this work was supported by the national natural science foundation of china(grant nos .",
    "11671161 ) and the fundamental research funds for the central universities ( program nos .",
    "2662015py046 , 2014py025 ) .",
    "99                          l. shi , y. feng , and d.x .",
    "zhou , `` concentration estimates for learning with @xmath232-regularizer and data dependent hypothesis spaces , '' _ appl .",
    "2 , pp . 286302 , 2011 .",
    "b. zou , r. chen , and z.b .",
    "xu , `` learning performance of tikhonov regularization algorithm with geometrically beta - mixing observations , '' _ journal of statistical planning and inference _ , vol .",
    "10771087 , 2011 ."
  ],
  "abstract_text": [
    "<S> learning with fredholm kernel has attracted increasing attention recently since it can effectively utilize the data information to improve the prediction performance . despite rapid progress on theoretical and experimental evaluations , </S>",
    "<S> its generalization analysis has not been explored in learning theory literature . in this paper </S>",
    "<S> , we establish the generalization bound of least square regularized regression with fredholm kernel , which implies that the fast learning rate @xmath0 can be reached under mild capacity conditions . </S>",
    "<S> simulated examples show that this fredholm regression algorithm can achieve the satisfactory prediction performance .    </S>",
    "<S> fredholm learning , generalization bound , learning rate , data dependent hypothesis spaces </S>"
  ]
}