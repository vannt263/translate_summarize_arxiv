{
  "article_text": [
    "bayesian inference in mixtures of distributions has been studied quite extensively in the literature .",
    "see , e.g. , @xcite and @xcite for book - long references and @xcite for one among many surveys . from a bayesian perspective , one of the several difficulties with this type of distribution , @xmath0 is that its ill - defined nature ( non - identifiability , multimodality , unbounded likelihood , etc . ) leads to restrictive prior modelling since most improper priors are not acceptable .",
    "this is due in particular to the feature that a sample from may contain no subset from one of the @xmath1 components @xmath2 ( see .",
    "e.g. , @xcite ) .",
    "albeit the probability of such an event is decreasing quickly to zero as the sample size grows , it nonetheless prevents the use of independent improper priors , unless such events are prohibited @xcite .",
    "similarly , the exchangeable nature of the components often induces both multimodality in the posterior distribution and convergence difficulties as exemplified by the _ label switching _ phenomenon that is now quite well - documented @xcite .",
    "this feature is characterized by a lack of symmetry in the outcome of a monte carlo markov chain ( mcmc ) algorithm , in that the posterior density is exchangeable in the components of the mixture but the mcmc sample does not exhibit this symmetry .",
    "in addition , most mcmc samplers do not concentrate around a single mode of the posterior density , partly exploring several modes , which makes the construction of bayes estimators of the components much harder .",
    "when specifying a prior over the parameters of , it is therefore quite delicate to produce a manageable and sensible non - informative version and some have argued against using non - informative priors in this setting ( for example , @xcite argue that it is impossible to obtain proper posterior distribution from fully noninformative priors ) , on the basis that mixture models were ill - defined objects that required informative priors to give a meaning to the notion of a component of .",
    "for instance , the distance between two components needs to be bounded from below to avoid repeating the same component over and over again .",
    "alternatively , the components all need to be informed by the data , as exemplified in @xcite who imposed a completion scheme ( i.e. , a joint model on both parameters and latent variables ) such that all components were allocated at least two observations , thereby ensuring that the ( truncated ) posterior was well - defined .",
    "@xcite proved ten years later that this truncation led to consistent estimators and moreover that only this type of priors could produce consistency .",
    "while the constraint on the allocations is not fully compatible with the i.i.d .",
    "representation of a mixture model , it naturally expresses a modelling requirement that all components have a meaning in terms of the data , namely that all components genuinely contributed to generating a part of the data .",
    "this translates as a form of weak prior information on how much one trusts the model and how meaningful each component is on its own ( by opposition with the possibility of adding meaningless artificial extra - components with almost zero weights or almost identical parameters ) .",
    "while we do not seek jeffreys priors as the ultimate prior modelling for non - informative settings , being altogether convinced of the lack of unique reference priors @xcite , we think it is nonetheless worthwile to study the performances of those priors in the setting of mixtures in order to determine if indeed they can provide a form of reference priors and if they are at least well - defined in such settings .",
    "we will show that only in very specific situations the jeffreys prior provides reasonable inference .    in section [ sec :",
    "jeffreys ] we provide a formal characterisation of properness of the posterior distribution for the parameters of a mixture model , in particular with gaussian components , when a jeffreys prior is used for them . in section [ sec : prosper ] we will analyze the properness of the jeffreys prior and of the related posterior distribution : only when the weights of the components ( which are defined in a compact space ) are the only unknown parameters it turns out that the jeffreys prior ( and so the relative posterior ) is proper ; on the other hand , when the other parameters are unknown , the jeffreys prior will be proved to be improper and in only one situation it provides a proper posterior distribution . in section [ sec : alternative ] we propose a way to realize a noninformative analysis of mixture models and introduce improper priors for at least some parameters .",
    "section [ sec : concl ] concludes the paper .",
    "we recall that the jeffreys prior was introduced by @xcite as a default prior based on the fisher information matrix @xmath3 whenever the later is well - defined ; @xmath4 stand for the expected fisher information matrix and the symbol @xmath5 denotes the determinant . although the prior is endowed with some frequentist properties like matching and asymptotic minimal information ( * ? ? ?",
    "* chapter 3 ) , it does not constitute the ultimate answer to the selection of prior distributions in non - informative settings and there exist many alternative such as reference priors @xcite , maximum entropy priors @xcite , matching priors @xcite , and other proposals @xcite . in most settings jeffreys priors are improper , which may explain for their conspicuous absence in the domain of mixture estimation , since the latter prohibits the use of most improper priors by allowing any subset of components to go ",
    "empty \" with positive probability .",
    "that is , the likelihood of a mixture model can always be decomposed as a sum over all possible partitions of the data into @xmath1 groups at most , where @xmath1 is the number of components of the mixture .",
    "this means that there are terms in this sum where no observation from the sample brings any amount of information about the parameters of a specific component .",
    "approximations of the jeffreys prior in the setting of mixtures can be found , e.g. , in @xcite , where the authors revert to independent jeffreys priors on the components of the mixture .",
    "this induces the same negative side - effect as with other independent priors , namely an impossibility to handle improper priors .",
    "@xcite provide a closed - form expression for the jeffreys prior for a location - scale mixture with two components .",
    "the family of distributions they consider is @xmath6 ( which thus hardly qualifies as a mixture , due to the orthogonality in the supports of both components that allows to identify which component each observation is issued from ) .",
    "the factor @xmath7 in the fraction is due to the assumption of symmetry around zero for the density @xmath8 .",
    "for this specific model , if we impose that the weight @xmath9 is a function of the variance parameters , @xmath10 the jeffreys prior is given by @xmath11 however , in this setting , @xcite demonstrate that the posterior associated with the ( regular ) jeffreys prior is improper , hence not relevant for conducting inference .",
    "( one may wonder at the pertinence of a fisher information in this model , given that the likelihood is not differentiable in @xmath12 . )",
    "@xcite also consider alternatives to the genuine jeffreys prior , either by reducing the range or even the number of parameters , or by building a product of conditional priors .",
    "they further consider so - called non - objective priors that are only relevant to the specific case of the above mixture .",
    "another obvious explanation for the absence of jeffreys priors is computational , namely the closed - form derivation of the fisher information matrix is almost inevitably impossible .",
    "the reason is that integrals of the form    @xmath13}{\\partial \\theta_i \\partial \\theta_j}\\left[\\sum_{h=1}^k p_h\\,f(x|\\theta_h)\\right]^{-1 } d x\\ ] ]    ( in the special case of component densities with a single parameter ) can not be computed analytically .",
    "we derive an approximation of the elements of the fisher information matrix based on riemann sums .",
    "the resulting computational expense is of order @xmath14 if @xmath15 is the total number of ( independent ) parameters .",
    "since the elements of the information matrix usually are ratios between the component densities and the mixture density , there may be difficulties with non - probabilistic methods of integration . here",
    ", we use riemann sums ( with @xmath16 points ) when the component standard deviations are sufficiently large , as they produce stable results , and monte carlo integration ( with sample sizes of @xmath17 ) when they are small . in the latter case",
    ", the variability of mcmc results seems to decrease as @xmath18 approaches @xmath19 .",
    "unsurprisingly , most jeffreys priors associated with mixture models are improper , the exception being when only the weights of the mixture are unknown , as already demonstrated in @xcite .",
    "we will characterize properness and improperness of jeffreys priors and derived posteriors , when some or all of the parameters of distributions from location - scale families are unknown .",
    "these results are established both analytically and via simulations , with sufficiently large monte carlo experiments checking the behavior of the approximated posterior distribution .",
    "a representation of the jeffreys prior and the derived posterior distribution for the weights of a 3-component mixture model is given in figure [ weights - priorpost ] : the prior distribution is much more concentrated around extreme values in the support , i.e. , it is a prior distribution conservative in the number of important components .",
    "[ lem : weights ] when the weights @xmath20 are the only unknown parameters in , the corresponding jeffreys prior is proper .",
    "figure [ weights - boxplots ] shows the boxplots for the means of the approximated posterior distribution for the weights of a three - component gaussian mixture model .     for 50 simulated samples of size @xmath21 , obtained via mcmc with @xmath22 simulations .",
    "the red crosses represent the true values of the weights.,width=245,height=283 ]    the generic element of the fisher information matrix is ( for @xmath23 )    @xmath24    when we consider the parametrization in @xmath25 , with @xmath26    we remind that , since the fisher information matrix is a positive semi - definite , its determinant is bounded by the product of the terms in the diagonal , thanks to the hadamard s inequality .",
    "therefore , we may consider the diagonal term , @xmath27 since both integrals are equal .",
    "therefore , the jeffreys prior will be bounded by the square root of the product of the terms in the diagonal of the fisher information matrix    @xmath28    which is a generalization to @xmath1 components of the prior provided in @xcite for @xmath29 ( however , @xcite find the reference prior for the limiting case when all the components have pairwise disjoint supports , while for the opposite limiting case where all the components converge to the same distribution , the jeffrey s prior is the uniform distribution on the @xmath1-dimensional simplex ) .",
    "this reasoning leads @xcite to conclude that the usual @xmath30 dirichlet prior with @xmath31 $ ] for @xmath32 seems to be a reasonable approximation .",
    "they also prove that the jeffreys prior for the weights @xmath20 is convex , with a argument based on the sign of the second derivative .    as a remark ,",
    "the configuration shown in proof of lemma [ lem : weights ] is compatible with the dirichlet configuration of the prior proposed by @xcite .",
    "the shape of the jeffreys prior for the weights of a mixture model depends on the type of the components .",
    "figure [ weights - gmm ] , [ weights - gtmm ] and [ weights - gtmm - df ] show the form of the jeffreys prior for a 2-component mixture model for different choices of components .",
    "it is always concentrated around the extreme values of the support , however the amount of concentration around @xmath19 or @xmath33 depends on the information brought by each component .",
    "in particular , figure [ weights - gmm ] shows that the prior is much more symmetric as there is symmetry between the variances of the distribution components , while figure [ weights - gtmm ] shows that the prior is much more concentrated around 1 for the weight relative to the normal component if the second component is a student t distribution .",
    "finally figure [ weights - gtmm - df ] shows the behavior of the jeffreys prior when the first component is gaussian and the second is a student t and the number of degrees of freedom is increasing . as expected , as the student t is approaching a normal distribution , the jeffreys prior becomes more and more symmetric .",
    "( black ) , @xmath34 ( red ) and @xmath35 ( blue).,width=245,height=283 ]     ( black ) , @xmath36 ( red ) and @xmath37 ( blue).,width=245,height=283 ]          if the components of the mixture model are distributions from a location - scale family and the location or scale parameters of the mixture components are unknown , this turns the mixture itself into a location - scale model . as a result",
    ", model may be reparametrized by following @xcite , in the case of gaussian components    @xmath38    namely using a reference location @xmath12 and a reference scale @xmath39 ( which may be , for instance , the location and scale of a specific component ) .",
    "equation may be generalized to the case of @xmath1 components as    @xmath40    in this way , the mixture model is more cleary a location - scale model , which implies that the jeffreys prior is flat in the location and powered as @xmath41 if @xmath15 is the total number of parameters of the components , respectively ( * ? ? ?",
    "* chapter 3 ) , as we will see in the following .",
    "[ lem : meansd - prior ] if the parameters of the components of a mixture model are either location or scale parameters , the corresponding jeffreys prior is improper .    in the proof of lemma [ lem : meansd - prior ] , we will consider a gaussian mixture model and then extend the results to the general situation of components from a location - scale family .",
    "we first consider the case where the means are the only unknown parameters of a gaussian mixture model    @xmath42    the generic elements of the expected fisher information matrix are , in the case of diagonal and off - diagonal terms respectively :    @xmath43=\\frac{p_i^2}{\\sigma_i^4 } \\bigintsss_{-\\infty}^\\infty \\frac{\\left [ ( x-\\mu_i ) \\mathfrak{n}(x|\\mu_i,\\sigma_i^2)\\right]^2}{\\sum_{l=1}^k",
    "p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x\\ ] ] @xmath44=\\frac{p_i p_j}{\\sigma_i^2 \\sigma_j^2 } \\bigintsss_{-\\infty}^\\infty \\frac{(x-\\mu_i ) \\mathfrak{n}(x|\\mu_i,\\sigma_i^2)(x-\\mu_j ) \\mathfrak{n}(x|\\mu_j,\\sigma_j^2 ) } { \\sum_{l=1}^k p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) }   d x\\ ] ] now , consider the change of variable @xmath45 in the above integrals , where @xmath46 is thus the mean of the @xmath47-th gaussian component ( @xmath48 ) .",
    "the above integrals are then equal to @xmath49 & = \\frac{p_j^2}{\\sigma_j^4 } \\bigintsss_{-\\infty}^\\infty \\frac{\\left [ ( t-\\mu_j+\\mu_i ) \\mathfrak{n}(t|\\mu_j-\\mu_i,\\sigma_i^2)\\right]^2}{\\sum_{l=1}^k p_l",
    "\\mathfrak{n}(t|\\mu_l-\\mu_i,\\sigma_l^2 ) } d x\\\\ \\mathbb{e}\\left[- \\frac{\\partial^2 \\log g_x(x)}{\\partial \\mu_j \\partial \\mu_m}\\right ] & = \\frac{p_j p_m}{\\sigma_j^2 \\sigma_m^2 } \\bigintsss_{-\\infty}^\\infty \\frac{(t-\\mu_j+\\mu_i ) \\mathfrak{n}(x|\\mu_j,\\sigma_j^2)(t-\\mu_m+\\mu_i ) \\mathfrak{n}(t|\\mu_m-\\mu_i,\\sigma_m^2 ) } { \\sum_{l=1}^k p_l \\mathfrak{n}(t|\\mu_l-\\mu_i,\\sigma_l^2 ) }   d x\\\\ \\label{eq : means - prior}\\end{aligned}\\ ] ] therefore , the terms in the fisher information only depend on the differences @xmath50 for @xmath51 .",
    "this implies that the jeffreys prior is improper since a reparametrization in ( @xmath52 ) shows the prior does not depend on @xmath46 .",
    "this feature will reappear whenever the location parameters are unknown .",
    "when considering the general case of components from a location - scale family , this feature of improperness of the jeffreys prior distribution is still valid , because , once reference location - scale parameters are chosen , the mixture model may be rewritten as    @xmath53    then the second derivatives of the logarithm of model behave as the ones we have derived for the gaussian case , i.e. they will depend on the differences between each location parameter and the reference one , but not on the reference location itself .",
    "then the jeffreys prior will be constant with respect to the global location parameter .",
    "when considering the reparametrization , the jeffreys prior for @xmath54 for a fix @xmath12 has the form :    @xmath55 ^ 2}{{p\\sigma\\exp\\{-\\frac{\\sigma^2(x+\\frac{\\delta}{\\sigma\\tau})^2}{2}\\}}+{(1-p)\\exp\\{-\\frac{x^2}{2}\\ } } } d x \\right]^{\\frac{1}{2}}\\ ] ]    and the following result may be demonstrated .",
    "the jeffreys prior of @xmath54 conditional on @xmath12 when only the location parameters are unknown is improper .",
    "the improperness of the conditional jeffreys prior on @xmath54 depends ( up to a constant ) on the double integral    @xmath56 ^ 2}{p\\sigma\\exp\\{-\\frac{\\sigma^2(x+\\frac{\\delta}{\\sigma\\tau})^2}{2}\\}+(1-p)\\exp\\{-\\frac{x^2}{2}\\ } } d x d\\delta.\\end{aligned}\\ ] ]    the order of the integrals is allowed to be changed , then    @xmath57 ^ 2}{p\\sigma\\exp\\{-\\frac{\\sigma^2(x+\\frac{\\delta}{\\sigma\\tau})^2}{2}\\}+(1-p)\\exp\\{-\\frac{x^2}{2}\\ } } d\\delta d x \\end{aligned}\\ ] ]    define @xmath58 .",
    "then    @xmath59    since the behavior of @xmath60 $ ] depends on @xmath61 as @xmath54 goes to @xmath62 , we have that    @xmath63    because the integrand function is positive",
    ". then    @xmath64    therefore the conditional jeffreys prior on @xmath54 is improper .",
    "figure [ fig : priorpost - diff ] compares the behavior of the prior and the resulting posterior distribution for the difference between the means of a two - component gaussian mixture model : the prior distribution is symmetric and it has different behaviors depending on the value of the other parameters , but it always stabilizes for large enough values ; the posterior distribution appears to always concentrate around the true value .",
    "( black lines ) , @xmath65 ( green and blue lines ) and known standard deviations equal to @xmath66 ( black lines ) , @xmath67 ( green lines ) and @xmath68 ( blue lines).,width=245,height=283 ]      consider now the second case of the scale parameters being the only unknown parameters .    first , consider a gaussian mixture model and suppose the mixture model is composed by only two components ; the jeffreys prior for the scale parameters is defined as    @xmath69 ^ 2}{\\sum_{l=1}^2 p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x \\right . \\nonumber \\\\                      & \\cdot \\left .",
    "{ } \\frac{p_2 ^ 2}{\\sigma_2 ^ 2 } \\bigintsss_{-\\infty}^\\infty \\frac{\\left [ \\left(\\frac{(x-\\mu_2)^2}{\\sigma_2 ^ 2}-1\\right ) \\mathfrak{n}(x|\\mu_2,\\sigma_2 ^ 2)\\right]^2}{\\sum_{l=1}^2 p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x \\right .",
    "\\nonumber \\\\",
    "& - \\left . { } \\left[\\frac{p_1 p_2}{\\sigma_1 \\sigma_2 } \\bigintsss_{-\\infty}^\\infty \\frac { \\left(\\frac{(x-\\mu_1)^2}{\\sigma_1 ^ 2}-1\\right ) \\left(\\frac{(x-\\mu_2)^2}{\\sigma_2 ^ 2}-1\\right ) \\mathfrak{n}(x|\\mu_1,\\sigma_1 ^ 2)\\mathfrak{n}(x|\\mu_2,\\sigma_2 ^ 2)}{\\sum_{l=1}^2 p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x \\right]^2\\right\\}^\\frac{1}{2 } \\end{aligned}\\ ] ]    since the fisher information matrix is positive definite , it is bounded by the product on the diagonal , then we can write :    @xmath70    in particular , if we reparametrize the model by introducing @xmath71 and @xmath72 and study the behavior of the following integral    @xmath73    where the internal integrals with respect to @xmath74 and @xmath75 converge with respect to @xmath76 and @xmath39 , then the behavior of the external integrals only depends on @xmath77",
    ". therefore they do not converge .",
    "this proof can be easily extended to the case of @xmath1 components : the behavior of the prior depends on the inverse of the product of the scale parameters , which implies that the prior is improper .",
    "moreover this proof may be easily extended to the general case of mixtures of location - scale distributions , because the second derivatives of the logarithm of the model will depend on factors @xmath78 for @xmath79 .",
    "when the square root is considered , it is evident that the integral will not converge .",
    "figures [ fig : sd - priorpost - clm ] and [ fig : sd - priorpost - asym ] show the prior and the posterior distributions of the scale parameters of a two - component mixture model for some situations with different weights and different means .",
    "but with known weights equal to @xmath65 and known means equal to @xmath80.,width=245,height=283 ]     but with known weights equal to @xmath65 and known means equal to @xmath81.,width=245,height=283 ]    summarized results of the posterior approximation obtained via a random - walk metropolis - hastings algorithm by exploring the posterior distribution associated with the jeffreys prior on the standard deviations are shown in figures [ fig : sd2-bxp ] and [ fig : sd3-bxp ] , which display boxplots of the posterior means : provided a sufficiently high sample size , simulations exhibit a convergent behavior .     for 50 replications of the experiment and a sample size equal to @xmath82 , obtained via mcmc with @xmath22 simulations .",
    "the red cross represents the true values.,width=245,height=283 ]     for 50 replications of the experiment and a sample size equal to @xmath83 , obtained via mcmc with @xmath22 simulations .",
    "the red cross represents the true values.,width=245,height=283 ]      consider now the case where both location and scale parameters are unknown . once again",
    ", each element of the fisher information matrix is an integral in which a change of variable @xmath84 can be used , for some choice of @xmath85 so that each term only depends on the difference @xmath50 ; the elements are @xmath86=\\frac{p_i^2}{\\sigma_i^2 } \\int_{-\\infty}^\\infty \\frac{\\left [ \\left(\\frac{(x-\\mu_i)^2}{\\sigma_i^2}-1\\right ) \\mathfrak{n}(x|\\mu_i,\\sigma_i^2)\\right]^2}{\\sum_{l=1}^k p_l",
    "\\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x\\ ] ]    @xmath87=\\frac{p_i p_j}{\\sigma_i \\sigma_j } \\int_{-\\infty}^\\infty \\frac { \\left(\\frac{(x-\\mu_i)^2}{\\sigma_i^2}-1\\right ) \\left(\\frac{(x-\\mu_j)^2}{\\sigma_j^2}-1\\right ) \\mathfrak{n}(x|\\mu_i,\\sigma_i^2)\\mathfrak{n}(x|\\mu_j,\\sigma_j^2)}{\\sum_{l=1}^k p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x\\ ] ]    @xmath88=\\frac{p_i^2}{\\sigma_i^3 } \\int_{-\\infty}^\\infty \\frac { \\left(x-\\mu_i\\right)\\left(\\frac{(x-\\mu_i)^2}{\\sigma_i^2}-1\\right ) \\left[\\mathfrak{n}(x|\\mu_i,\\sigma_i^2)\\right]^2}{\\sum_{l=1}^k p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x\\ ] ]    @xmath89=\\frac{p_i p_j}{\\sigma_i \\sigma_j } \\int_{-\\infty}^\\infty \\frac { \\frac{(x-\\mu_i)}{\\sigma_i^2 \\sigma_j }",
    "\\left(\\frac{(x-\\mu_j)^2}{\\sigma_j^2}-1\\right ) \\mathfrak{n}(x|\\mu_i,\\sigma_i^2)\\mathfrak{n}(x|\\mu_j,\\sigma_j^2)}{\\sum_{l=1}^k p_l \\mathfrak{n}(x|\\mu_l,\\sigma_l^2 ) } d x\\ ] ]      when considering all the parameters unknown , the form of the jeffreys prior may be partly defined by considering the mixture model as a location - scale model , for which a general solution exists ; see @xcite .",
    "when all the parameters of a gaussian mixture model are unknown , the jeffreys prior is constant in @xmath12 and powered as @xmath41 , where @xmath15 is the total number of components parameters .",
    "we have already proved the jeffreys prior is constant on the global mean ( first proof of lemma [ lem : meansd - prior ] ) .",
    "consider a two - component mixture model and the reparametrization . with some computations ,",
    "it is straightforward to derive the fisher information matrix for this model , partly shown in table [ tab : fishinfo_repar ] , where each term is multiplied for a term which does not depend on @xmath39 .    .",
    "[ tab : fishinfo_repar ]    .factors depending on @xmath39 of the fisher information matrix for the reparametrized model [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      + 3 & 0.1947 & 0.00 & 0.00 & 1.1034 & 0.9825 & 0.0838 & 0.5778 + 4 & 0.2295 & 0.00 & 0.00 & 1.0318 & 1.0300 & 0.4678 & 0.5685 + 5 & 0.2230 & 0.00 & 0.00 & 0.9572 & 0.9924 & 0.8464 & 0.7456 + 6 & 0.2275 & 0.00 & 0.00 & 0.9870 & 0.9641 & 0.6614 & 0.6696 + 7 & 0.2112 & 0.00 & 0.00 & 1.0658 & 1.0043 & 0.8406 & 0.7848 + 8 & 0.2833 & 0.00 & 0.00 & 1.0077 & 1.0284 & 0.8268 & 0.8495 + 9 & 0.2696 & 0.00 & 0.00 & 1.0741 & 1.0179 & 0.8854 & 0.8613 + 10 & 0.2266 & 0.00 & 0.00 & 1.1446 & 0.9968 & 0.9589 & 0.8508 + 15 & 0.1982 & 0.00 & 0.00 & 1.0201 & 0.9959 & 0.9409 & 0.9280 + 20 & 0.2258 & 0.00 & 0.00 & 1.2023 & 1.0145 & 0.9172 & 0.9400 + 30 & 0.2073 & 0.00 & 0.00 & 0.9888 & 1.0022 & 1.0424 & 0.9656 + 50 & 0.2724 & 0.00 & 0.00 & 1.0493 & 1.0043 & 1.0281 & 0.9859 + 100 & 0.2739 & 0.00 & 0.00 & 1.0932 & 1.0025 & 1.0805 & 0.9932 + 200 & 0.3031 & 0.00 & 0.00 & 1.1610 & 1.0036 & 1.1519 & 0.9964 + 500 & 0.2753 & 0.00 & 0.00 & 1.1729 & 1.0023 & 1.1694 & 0.9989 + 1000 & 0.2317 & 0.00 & 0.00 & 1.1800 & 1.0021 & 1.1772 & 0.9994 + & & & & & & & + 3 & 0.2840 & 0.00 & 0.00 & 1.1316 & 1.0503 & 0.3432 & 0.2950 + 4 & 0.2217 & 0.00 & 0.00 & 1.0326 & 0.9452 & 0.6699 & 0.6624 + 5 & 0.2144 & 0.00 & 0.00 & 1.0610 & 1.0421 & 0.6858 & 0.6838 + 6 & 0.2258 & 0.00 & 0.00 & 1.0908 & 0.9683 & 0.6472 & 0.6355 + 7 & 0.1843 & 0.00 & 0.00 & 1.0436 & 0.9915 & 0.7878 & 0.8008 + 8 & 0.2760 & 0.00 & 0.00 & 1.0276 & 1.0077 & 0.7996 & 0.7958 + 9 & 0.2028 & 0.00 & 0.00 & 1.0025 & 1.0145 & 0.7830 & 0.8016 + 10 & 0.2116 & 0.00 & 0.00 & 1.0426 & 1.0015 & 0.8752 & 0.8591 + 15 & 0.2023 & 0.00 & 0.00 & 1.0247 & 1.0063 & 0.8810 & 0.8871 + 20 & 0.2211 & 0.00 & 0.00 & 1.0281 & 1.0104 & 0.9290 & 0.9268 + 30 & 0.2242 & 0.00 & 0.00 & 1.1978 & 1.0123 & 1.0841 & 0.9508 + 50 & 0.2513 & 0.00 & 0.00 & 1.0543 & 1.0142 & 1.0148 & 0.9775 + 100 & 0.2768 & 0.00 & 0.00 & 1.0563 & 1.0206 & 1.0324 & 0.9955 + 200 & 0.2910 & 0.00 & 0.00 & 1.0325 & 1.0118 & 1.0200 & 0.9993 + 500 & 0.2329 & 0.00 & 0.00 & 1.0943 & 1.0079 & 1.0882 & 1.0002 + 1000 & 0.2189 & 0.00 & 0.00 & 1.1068 & 1.0105 & 1.1212 & 1.0110 +    figures [ fig : hierc_densmean2_3_8][fig : hierc_densmean3_30_1000 ] show the results how a simulations study to approximate the posterior distribution of the means of a two or three - component mixture model , compared to the true values ( red vertical lines ) and for different sample sizes , from @xmath90 to @xmath91 .     and global variance @xmath92 , based on @xmath83 replications of the experiment with different sample sizes , black and blue lines for the marginal posterior distribution of @xmath93 and @xmath94 respectively . ]    .",
    "]    . ]    . ]     and global variance @xmath92 , based on @xmath83 replications of the experiment with different sample sizes ( the red lines stands for the true values , black , green and blue lines for the marginal posterior distributions of @xmath93 , @xmath94 and @xmath95 respectively ) . ]    . ]    . ]    . ]",
    "the computing expense due to derive the jeffreys prior for a set of parameter values is in @xmath14 if @xmath15 is the total number of ( independent ) parameters .",
    "each element of the fisher information matrix is an integral of the form    @xmath13}{\\partial \\theta_i \\partial \\theta_j}\\left[\\sum_{h=1}^k p_h\\,f(x|\\theta_h)\\right]^{-1 } d x\\ ] ]    which has to be approximated .",
    "we have applied both numerical integration and monte carlo integration and simulations show that , in general , numerical integration obtained via gauss - kronrod quadrature ( see @xcite for details ) , has more stable results .",
    "neverthless , when one or more proposed values for the standard deviations or the weights is too small , the approximations tend to be very dependent on the bounds used for numerical integration ( usually chosen to omit a negligible part of the density ) or the numerical approximation may not be even applicable . in this case , monte carlo integration seems to have more stable , where the stability of the results depends on the monte carlo sample size .",
    "figure [ fig : mcvsnum_incrn ] shows the value of the jeffreys prior obtained via monte carlo integration of the elements of the fisher information matrix for an increasing number of monte carlo simulations both in the case where the jeffreys prior is concentrated ( where the standard deviations are small ) and where it assumes low values .",
    "the value obtained via monte carlo integration is then compared with the value obtained via numerical integration . the sample size relative to the point where the graph stabilizes",
    "may be chosen to perform the approximation .",
    "a similar analysis is shown in figures [ fig : mcvsnum_bpl1 ] and [ fig : mcvsnum_bpl2 ] which provide the boxplots of @xmath21 replications of the monte carlo approximations for different numbers of simulations ( on the _",
    "x_-axis ) ; one can choose to use the number of simulations which lead to a reasonable or acceptable variability of the results .",
    "( above ) and for the model @xmath96 ( below ) . ]     for sample sizes from @xmath97 to @xmath98 .",
    "the value obtained via numerical integration is represented by the red line . ]     for the model @xmath96 . ]",
    "since the approximation problem is one - dimensional , another numerical solution could be based on the sums of riemann ; figure [ fig : mcvsnumsrvsintr ] shows the comparison between the results of the gauss - kronrod quadrature procedure and a procedure based on sums of riemann for an increasing number of points considered in a region which contain the @xmath99 of the data density .",
    "moreover , figure [ fig : mcvsriembxp ] shows the comparison between the approximation to the jeffreys prior obtained via monte carlo integration and via the sums of riemann : it is clear that the sums of riemann lead to more stable results in comparison with monte carlo integration . on the other hand ,",
    "they can be applied in more situations than the gauss - kromrod quadrature , in particular , in cases where the standard deviations are very small ( of order @xmath100 ) . nevertheless , when the standard deviations are smaller than this , one has to pay attention on the features of the function to integrate .",
    "in fact , the mixture density tends to concentrate around the modes , with regions of density close to 0 between them .",
    "the elements of the fisher informtation matrix are , in general , ratios between the components densities and the mixture density , then in those regions an indeterminate form of type @xmath101 is obtained ; figure [ fig : fishinfoelem ] represents the behavior of one of these elements when @xmath102 for @xmath103 .",
    "( above ) and @xmath96 ( below ) . ]     for sample sizes from @xmath97 to @xmath104 . the value obtained via numerical integration",
    "is represented by the red line ( in the graph below , all the approximations obtained with more than @xmath16 knots give the same result , exactly equal to the one obtained via gauss - kronrod quadrature ) . ]    . ]",
    "thus , we have decided to use the sums of riemann ( with a number of points equal to @xmath16 ) to approximate the jeffreys prior when the standard deviations are sufficiently big and monte carlo integration ( with sample sizes of @xmath17 ) when they are too small . in this case",
    ", the variability of the results seems to decrease as @xmath18 approaches @xmath19 , as shown in figure [ fig : mcsmallsd ] .    , where @xmath76 is taken equal for both components and decreasing .",
    "]    we have chosen to consider monte carlo samples of size equal to @xmath17 because both the value of the approximation and its standard deviations are stabilizing .",
    "an adaptive mcmc algorithm has been used to define the variability of the kernel density functions used to propose the moves . during the burnin ,",
    "the variability of the kernel distributions has been reduced or increased depending on the acceptance rate , in a way such that the acceptance rate stay between @xmath105 and @xmath106 .",
    "the transitional kernel used have been truncated normals for the weights , normals for the means and log - normals for the standard deviations ( all centered on the values accepted in the previous iteration ) .",
    "this thorough analysis of the jeffreys priors in the setting of gaussian mixtures shows that mixture distributions can also be considered as an ill - posed problem with regards to the production of non - informative priors .",
    "indeed , we have shown that most configurations for bayesian inference in this framework do not allow for the standard jeffreys prior to be taken as a reference .",
    "while this is not the first occurrence where jeffreys priors can not be used as reference priors , the wide range of applications of mixture distributions weights upon this discovery and calls for a new paradigm in the construction of non - informative bayesian procedures for mixture inference .",
    "our proposal in section [ sec : alternative ] could constitute such a reference , as it simplifies the representation of @xcite .",
    "bernardo , j. and girn , f. ( 1988 ) . a bayesian analysis of simple mixture problems . in",
    "bayesian statistics 3 _ ( j.  bernardo , m.  degroot , d.  lindley and a.  smith , eds . ) .",
    "oxford university press , oxford , 6778 .",
    "lee , k. , marin , j .-",
    ", mengersen , k. and robert , c. ( 2009 ) .",
    "ayesian inference on mixtures of distributions . in _",
    "perspectives in mathematical sciences i : probability and statistics _ ( n.  n. sastry , m.  delampady and b.  rajeev , eds . ) .",
    "world scientific , singapore , 165202 .",
    "mengersen , k. and robert , c. ( 1996 ) .",
    "testing for mixtures : a bayesian entropic approach ( with discussion ) . in _",
    "bayesian statistics 5 _ ( j.  berger , j.  bernardo , a.  dawid , d.  lindley and a.  smith , eds . ) .",
    "oxford university press , oxford , 255276 .",
    "puolamki , k. and kaski , s. ( 2009 ) .",
    "bayesian solutions to the label switching problem . in _ advances in intelligent data analysis viii _",
    "( n.  adams , c.  robardet , a.  siebes and j .- f .",
    "boulicaut , eds . ) , vol .",
    "lecture notes in computer science_. springer berlin heidelberg , 381392 ."
  ],
  "abstract_text": [
    "<S> while jeffreys priors usually are well - defined for the parameters of mixtures of distributions , they are not available in closed form . furthermore , they often are improper priors . </S>",
    "<S> hence , they have never been used to draw inference on the mixture parameters . </S>",
    "<S> we study in this paper the implementation and the properties of jeffreys priors in several mixture settings , show that the associated posterior distributions most often are improper , and then propose a noninformative alternative for the analysis of mixtures . </S>"
  ]
}