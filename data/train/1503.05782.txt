{
  "article_text": [
    "attribute learning aims at achieving an intermediate representation on top of the low - level visual feature space , which encodes semantic properties shared across different categories of objects or scenes .",
    "such an intermediate representation plays a role as the vehicles of semantics in human - machine communication .",
    "farhadi @xcite and lampert  @xcite showed that supervised attributes can be transferred across object categories , allowing description and naming of objects from categories not seen during training .",
    "therefore , attributes provide a way to encode and share knowledge to achieve challenging tasks such as the zero - shot learning ( zst ) problem , where the goal is to categorize classes that are unseen during training  @xcite .",
    "a lot of approaches have been proposed for attribute learning , @xcite .",
    "two fundamental issues remain unsolved , although recent researches have started to pay attention to them @xcite .",
    "first , traditional approaches learn attributes independently from each other ( one - vs - all classifiers )  @xcite , without explicitly exploiting the correlation between attributes .",
    "second , learning attribute classifiers are typically done independent of the subsequent tasks , such as categorization or zero shot learning , and typically category labels are ignored in the learning process .",
    "optimizing the attribute prediction independent of the succeeding task does not guarantee to yield the best attribute predictor for that task .    correlations naturally exist among attributes .",
    "is it better to exploit the correlation or to discourage the correlation during attribute learning , decorrelation ?",
    "several papers have argued that exploiting correlation between attributes improves their discriminative powers , @xcite .",
    "recent works attempted to address the issue of joint attribute learning with a focus on decorrelating attributes  @xcite .",
    "jayaraman @xcite argued that attribute learning approaches are prone to learn visual features that correlates with attributes , not attributes themselves .",
    "therefore they argued for decorrelation of attributes , by exploiting feature competition during learning through a multitask learning framework .",
    "decorrelation of attributes might be suitable in tasks such as describing images with text , key - word based retrieval , or generating image annotations .",
    "however , preserving and exploiting correlation between attributes should preserve the natural clustering in the data and should be better for classification and zero - shot learning tasks .",
    "correlation is a nature of attributes and compulsively decorrelating the attributes may break the original relations of attributes in the visual space as well as in the semantic space .    to illustrate our point we use the example in figure  [ hypercut ] where we used attributes from the awa dataset  @xcite .",
    "consider learning the attribute `` water '' . in the figure , there are two clusters denoted as two superclasses ( terrestrial and aquatic animals ) .",
    "it is expected that the attributes in each cluster will be highly correlated and coexist in images .",
    "the conventional classifiers directly learn an optimal separation for the attribute `` water '' which clearly ignores the correlations as well as the natural clusters in the data .",
    "although it achieves the optimal attribute prediction , it clearly reduces the utility of attribute predictors in subsequent tasks such as categorization , and is much easier to get mired in overfitting",
    ". instead we aim at a cut that , besides minimizing the attribute prediction loss , tries to preserve the clustering in the data .",
    "preserving the correlation can be even more beneficial for attributes that are not visual .",
    "for example the attribute `` swim '' describes an action and it is very hard to predict it from visual features if it is forced to be decorrelated from other attributes such as `` water '' .",
    "our goal is to design a new attribute learning framework that addresses the two aforementioned issues , jointly learning attributes while exploiting the correlation between attributes , and exploiting class information as well as any available side information .",
    "we propose to model the attribute learning as a supervised hypergraph cut problem . as a generalization of graphs ,",
    "hypergraphs are typically used to depict the high - order and multiple relations of data  @xcite .",
    "one merit of hypergraph is that it can capture the correlations of multiple relations , since the partition of a vertex set who has many common hyperedges will lead to a heavy penalty  @xcite . in our formulation , we define a hypergraph where each vertex corresponds to a sample and a hyperedge is a vertex set sharing the same attribute label",
    ". then , we can consider the attribute prediction problem as a hypergraph cut problem .",
    "more specifically , a collection of hypergraph cuts ( one cut per attribute ) that minimizes the loss of attribute relations ( defined by the hyperedges ) is jointly learned .",
    "moreover , such cuts also minimize the attribute prediction errors of training data .",
    "since hypergraph cuts can be deemed as the hypergraph embedding from the perspective of graph embedding  @xcite , this step actually tries to align the embedding space , which encodes the attribute relations , with the semantic attribute space .",
    "we also propose attribute predictors ( or classifiers ) that can be obtained by introducing a mapping from the feature space to this aligned hypergraph embedding space .",
    "we name this approach hypergraph - based attribute predictor ( hap ) , which can be combined with different hypergraph models to obtain classifiers from the cuts .",
    "we illustrate our model in figure  [ aps ] .    in order to incorporate class information and any additional side information within the hap formulation ,",
    "we consider it as a multi - graph cut problem .",
    "one or several additional graphs ( or hypergraphs ) , which encode side information , will be introduced as the penalties to the hap . in that case",
    ", the new cuts should not only minimize the loss of attribute relations , but also the losses of the side information . in case of class labels ,",
    "we formulate a new hap framework , denoted as class - specific hap ( cshap ) , which enhances the discriminating ability of the attribute predictors . a hypergraph and a graph , which encode the class information in two different ways ,",
    "are respectively leveraged to produce two different versions of the cshap approaches .",
    "we denote them hypergraph - based cshap ( cshap@xmath1 ) and graph - based cshap ( cshap@xmath2 ) respectively . finally , all the proposed approaches are kernelized to incorporate the nonlinearity .",
    "we summarize the contributions of this paper as follows :    1 .   as far as we know , our approach is the first to formulate attribute learning as a supervised hypergraph cut problem .",
    "we propose an approach to construct predictors ( linear and nonlinear classifier ) jointly while solving for the cuts .",
    "this idea is applicable in general ( not limited to the context of attribute learning ) to any hypergraph cut algorithm , as a general path to derive the classifiers from graph model .",
    "we provide a flexible solution to incorporate the side information in attribute learning .",
    "the proposed approach provides efficient attribute predictions , since the computational complexity of the attribute prediction is linear with respect to the dimension of feature .",
    "we experimented with three datasets : animal with attributes ( awa )  @xcite , caltech - ucsd birds ( cub )  @xcite and unstructured social activity attribute ( usaa )  @xcite .",
    "the results on attribute prediction , zero - shot , n - shot learning , and categorization consistently validate the effectiveness of the proposed framework .",
    "the rest of paper is organized as follows : section [ s2 ] presents the related works ; section [ s3 ] describes the proposed approach .",
    "section [ s4 ] shows the experimental evaluation of our works ; the conclusion is summarized in section [ s6 ] .",
    "traditional attribute learning approaches follow a supervised discriminative learning pipeline ( one - vs - all classifiers ) where attribute classifiers are learned independently given attribute labels for each image or each class  @xcite .",
    "recently several papers suggested approaches for joint learning of attributes  @xcite .",
    "wang  @xcite and song  @xcite construct a graph of attributes in attribute domain from the training data and consider it as the latent variables in the latent svm for categorization , and they showed that exploiting attribute relations helps improve class prediction .",
    "in contrast our hypergraph is constructed on the samples , which facilitates aligning the feature space with the attribute semantic space .",
    "mahajan @xcite proposed a joint learning framework that removes the correlations as the redundancies during learning the mapping between attributes and classes which actually ignores the contributions of the co - occurrence attributes .",
    "akata @xcite proposed an approach that simultaneously target three problems : optimizing attribute prediction using class labels , using side information , and incremental learning .",
    "they achieved decorrelation by using dimensionality reduction on the class - attribute matrix , in the label space , and showed that the attribute dimensions can be reduced significantly without affecting the accuracy .",
    "in contrast our hypergraph construction achieve correlation / decorrelation by employing the sample - attribute relations and embeding the data samples in a space that is aligned with the attribute labels .",
    "recently , jayaraman  @xcite attempted to decorrelate the attributes via solving a structure sparsity model in which semantic attribute groups are manually provided as auxiliary data .",
    "attribute learning is just the preliminary step for some other visual tasks .",
    "few approaches have been proposed to incorporate the additional information , in particular class labels into attribute learning for benefiting the subsequent tasks  @xcite .",
    "however , the attribute learning and the exploitation of the additional information are highly coupled in these approaches .",
    "it is very hard to add novel additional information into the model , or unplug the additional information exploitation part from the original models when the additional information of the given data is not available .",
    "in contrast , our proposed method can flexibly to address this issue by considering the attribute prediction task as multi - graph ( hypergraph ) cut problem , enabling adding any side information as an extra graph or hypergraph .",
    "zero - shot learning ( zsl ) is the task of object recognition for categories with no training examples  @xcite .",
    "several intermediate representations has been used for zsl , including attributes  @xcite , linguistic knowledge  @xcite , textual description  @xcite and visual abstraction  @xcite .",
    "the core attribute - based zsl approaches are the attribute learning .",
    "therefore our attribute prediction can be integrated to several zsl frameworks .",
    "although our work is an attribute - based zsl , other intermediate representation can also be readily plugged into the hap framework to replace the attributes , since it generally provides a mapping from the low - level representation to intermediate representation .",
    "several zsl approaches can be extended to n - shot learning  @xcite .",
    "hypergraph is a generalization of the regular graph which has been widely applied to depict the high - order relations between data points  @xcite .",
    "since the attribute prediction task can be regarded as a multi - label classification problem , we will introduce not only the relevant hypergraph model , but also some hypergraph - based multi - label classification algorithms , which motivated our work .",
    "more specifically , zhou  @xcite proposed a normalized hypergraph model for embedding and transduction .",
    "our formulation is based on zhou s model , however it is inductive .",
    "moreover , we show how the hypergraph cut can be reformulated to provide direct linear or nonlinear class predictors .",
    "chen  @xcite leveraged the hypergraph to capture the correlation of categories and introduced it as a regularization to svm model for multi - label classification .",
    "similar to  @xcite , sun  @xcite used the hypergraph to capture the correlation of classes and performed a hypergraph embedding as the new representation for multi - class classification . in  @xcite the hypergraph is utilized to measure the loss of multi - labels during the multi - kernel learning .",
    "in contrast to all the existing hypergraph learning algorithms , as far as we know , our model is the first approach that directly derives the multi - label classifier from the hypergraph embedding .",
    "we start by reviewing some basic definitions of hypergraphs and introducing the notations .",
    "hypergraphs are a generalization of graphs in which a hyperedge ( the analogy of an edge ) is an arbitrary non - empty subsets of the vertex set  ( fig  [ hypercut ] shows a hypergraph with five hyperedges ) . given a hypergraph @xmath3 in an arbitrary feature space , @xmath4 and @xmath5 are the vertex set and hyperedge set , where each vertex and hyperedge are respectively defined as @xmath6 and @xmath7 .",
    "moreover , a hyperedge @xmath8 is a subset of @xmath4 .",
    "the vertex - edge incidence matrix @xmath9 is defined as follows @xmath10 the degree of a hyperedge @xmath8 , which is denoted as @xmath11 , is the number of vertices in @xmath8 @xmath12 and the degree of a vertex @xmath6 is defined as follows @xmath13 where @xmath14 is the weight of the hyperedge @xmath8 .",
    "we denote the diagonal matrix forms of @xmath11 , @xmath15 and @xmath14 as @xmath16 , @xmath17 and @xmath18 respectively .",
    "note , here we defined @xmath19 as a binary matrix for simplicity . for the continuous value case , a probabilistic hypergraph model  @xcite can be adopted in which each element of @xmath19 denotes the probability of a vertex in a hyperedge .      in our model",
    ", we define a hypergraph to depict the attribute relations of samples ( corresponding to images in the training set ) . in this hypergraph ,",
    "the vertex @xmath20 is corresponding to the sample @xmath21 , which is the @xmath22-th column of the @xmath23-dimensional sample matrix @xmath24 . here , @xmath25 is the number of samples and @xmath26 is the dimension of the feature space .",
    "each hyperedge is defined as a vertex set that shares the same attribute label . in such case , the number of hyperedges is equal to the number of attributes @xmath27 and the @xmath28-dimensional matrix incident matrix @xmath19 is exactly the attribute label matrix . the more common attributes among a set of images , the more hyperedges will exist between their corresponding vertices , and the stronger the link will be between these vertices .",
    "therefore , break such a link will lead to a heavy penalty during the learning process . in this way",
    ", the hypergraph actually provides a natural way to capture the correlation / decorrelation of attributes .",
    "we regard the hyperedge @xmath8 as a clique and consider the mean of the heat kernel weights of the pairwise edges in this clique as the hyperedge weight @xmath29 certainly , some other hyperedge weighing schemes can be also applied .",
    "normalized hypergraph cut is often utilized to learn the high - order relation and correlation information .",
    "the main idea of our model stems from the hypergraph - based transduction which is regarded as a regularized normalized hypergraph cut model  @xcite .",
    "in contrast , our method is a supervised inductive model . since our proposed attribute predictor ( or classifier )",
    "is based on the hypergraph model , we call it hypergraph - based attribute predictor ( hap ) .    in hap , a collection of hypergraph cuts @xmath30 $ ] is defined as the predictors of attributes in the feature space where @xmath27 is the number of attributes and the cut @xmath31 is a column vector whose elements are the predictions of @xmath22-th attribute for each sample .",
    "an optimal cut should not disrupt the hyperedges during hypergraph partition as much as possible .",
    "in other words , the optimal cut should keep the attribute relations of samples as much as possible since each hyperedge is given by an attribute label .",
    "similar to zhou s normalized hypergraph  @xcite , we can define an attribute relation loss function with respect to the given hypergraph @xmath32 and a collection of hyperedge cuts @xmath33 can be denoted as follows @xmath34 where @xmath35 returns a row vector corresponding to the predictions of attributes for the vertex @xmath36 .",
    "clearly , the loss will be reduced when the signs of @xmath35 and @xmath37 are identical . following some deductions , equation  [ infloss ]",
    "can be reformulated as follows @xmath38 where @xmath39 is the normalized hypergraph laplacian matrix which is derived from the hypergraph of attributes , and @xmath40 is an identity matrix .",
    "@xmath41 is the trace of the matrix .",
    "the detail deductions of equation  [ infloss ] can be found in the supplementary material .    besides measuring the loss of attribute relation information",
    ", we also need to consider the attribute prediction errors of the train data , which can be obtained via calculating the euclidean distance between attribute predictions @xmath33 and the attribute label matrix . in order to make zero as the classification boundary",
    ", we define a shifted attribute label matrix @xmath42 via shifting the attribute labels as @xmath43 where @xmath44 is a matrix of the same size as @xmath19 whose elements are all equal to 1 . in @xmath42 , if an attribute exists in a sample , its corresponding attribute label is 1 , otherwise it is -1 . given this definition ,",
    "the attribute prediction loss is defined as @xmath45 simultaneously minimizing the the previous two losses leads to our model @xmath46 where @xmath47 is a positive parameter to reconcile these two losses .",
    "now , the optimal hypergraph cut @xmath31 introduces a binary partition to hypergraph that can preserve the information of @xmath22-th attribute relation and reduce the prediction error of @xmath22-th attribute as much as possible .    from the perspective of graph embedding ,",
    "the hypergraph cuts are the embedding of the given hypergraph , where the embedding coordinate of sample @xmath36 is the @xmath36-th row of @xmath33 .",
    "equation  [ predmodel ] actually aligns the hypergraph embedding space ( defined by @xmath33 ) with the shifted attribute space .",
    "consequently , we now transform the problem of seeking attribute predictors / cuts to the problem of finding a mapping from the feature space to this aligned embedding space , @xmath48 where the projection matrix @xmath49 $ ] is such collection of mappings whose @xmath22-th column is a predictor of the @xmath22-th attribute , corresponding to the @xmath22-th hypergraph cut @xmath50 .",
    "we then substitute equation  [ proj ] into equation  [ predmodel ] , and introduce @xmath51-norm constraint to @xmath52 to avoid the overfitting .",
    "thus , the equation  [ predmodel ] is reformulated as the following optimization problem with respect to @xmath52 @xmath53 where @xmath54 is a positive regularization parameter . since @xmath39 is a positive semi - definite matrix ,",
    "this problem is a typical regularized least square ( rls ) problem that can be efficiently solved .",
    "we obtain the partial derivative of equation  [ modelb ] with respect to @xmath52 , and equate it to zero , which leads to a closed - form solution for @xmath52 as follows @xmath55 at test time , given a unlabeled sample @xmath56 , its attribute predictions can be achieved by projecting the sample into the subspace spanned by @xmath52 , @xmath57 where @xmath58 returns the sign of each element of a vector and @xmath59 $ ] is a row vector encoded the predicted attributes .",
    "its @xmath60-th element @xmath61 is the confidence of the existence of the @xmath60-th attribute with respect to the sample @xmath56 .",
    "we call the subspace spanned by @xmath52 attribute prediction space ( aps ) , since each basis of this space actually is a predictor of a specific attribute .      as a regularized graph learning approach ,",
    "it is flexible to introduce other meaningful constraints to further enhance attribute learning . in this section",
    ", we take the class label as an example to show how to leverage any additional information to enhance our model .",
    "the exploitation of class labels can enhance the classification abilities of hap algorithms , since homogenous samples always share more similarities in attributes .",
    "we adopt two approaches to incorporate the class information .",
    "the first approach uses a hypergraph @xmath62 to depict the class relation of samples , similar to the way we used a hypergraph to depict the attribute relations in the previous subsection .",
    "it is not hard to derive the hypergraph laplacian @xmath63 from this hypergraph via following the same way as equation  [ infloss ] .",
    "the second approach , following @xcite , constructs a pairwise graph @xmath64 in a supervised way for encoding the class information .",
    "we can encode class information using a graph , since unlike the attributes , the classes are disjoint .",
    "two samples are connected with an edge if they belong to the same class ( homogenous samples ) .",
    "similar to the hypergraph model , the heat kernel weighting is adopted as the edge weighting scheme .",
    "finally , the well known laplacian eigenmapping model can easily derive the graph laplacian @xmath65 .",
    "introducing such class - label graph @xmath66or hypergraph @xmath67 to the loss function in equation  [ infloss ] leads to the new loss function @xmath68 where @xmath69 denotes either @xmath66 or @xmath67 and @xmath70 is the corresponding laplacian matrix .",
    "@xmath71 is the combination of the original and new laplacian matrices .",
    "according to equation  [ modelb ] , the new objective function can be reformulated as follows @xmath72 and the solution of @xmath52 achieved by just replacing @xmath39 with @xmath73 in equation  [ sols ] @xmath74 we call these models class specific hypergraph - based attribute predictor ( cshap ) . to distinguish the hypergraph - based cshap and graph - based ( laplacian eigenmapping - based ) cshap , we respectively denote them by cshap@xmath1 and cshap@xmath75 for short .",
    "we hypothesize that cshap@xmath75 is expected to capture the intra - manifold structure between the samples better than cshap@xmath1 , since cshap@xmath1 just group the homogenous samples using hyperedges , while cshap@xmath75 preserves the pair - wise structure .",
    "if more additional information are available , the graph laplacians that encode these information , can be also added , @xmath76 .",
    "such positive regularization parameters @xmath77 can be deduced by multiple kernel learning , since each laplacian matrix is associated with an affinity matrix ( similarity matrix ) which can be considered as a kernel matrix .",
    "the mapping from the feature space to the shifted attribute space ( aligned embedding space ) may be not linear .",
    "this motivated us to present the kernelization for our method . according to the generalized representer theorem @xcite",
    ", a minimizer of a regularized empirical risk function over a rkhs can be represented as a linear combination of kernels , evaluated on the training set .",
    "inspired by the representer theorem on the attribute classification risk function , we embed kernel representation of the samples ( @xmath78 ) .",
    "this transformation could be interpreted that each dimension is the embedding is linear combination of the kernel - evaluations on the training set , which matches the representer theorem .",
    "@xmath79 where @xmath80 is an @xmath81 kernel matrix associated with a kernel function @xmath82 , and @xmath25 is the number of points in the training set .",
    "therefore , the objective functions of the kernelized hap ( khap ) and kernerlized cshap ( kcshap ) can be denoted as follows @xmath83 where @xmath84 is equal to @xmath39 in the khap case and @xmath73 in the kcshap case .",
    "the solution of equation  [ kmodelbc ] is @xmath85 then , the attribute predictions can be obtained as follows @xmath86 where @xmath87 $ ] .",
    "@xmath88 is the test sample .",
    "typically there are two ways to annotate the samples using attributes , either to assign the attributes for each sample , or to assign the attributes for each class .",
    "our proposed approach supports both of these two scenarios . at zero - shot or n - shot time , before we classify samples based on the predicted attributes , we use the sigmoid function to normalize the obtained attribute confidences @xmath89 into the range @xmath90 $ ] .",
    "@xmath91 where @xmath92 is a positive scaling parameter and @xmath93 $ ] is the normalized attribute confidence vector which can be deemed as the probabilities of the existences of attributes .    in the case",
    "where only the classes are labeled with attributes , we follow the approach of direct attribute prediction ( dap )  @xcite where the bayes rule is adopted to calculate the posterior of a test class of a given sample based on its attribute probabilities @xmath94 .",
    "the sample is labeled with the class with the maximum posterior .",
    "with regard to the case where each sample is annotated with attributes , we define the mean of the attribute prototypes in the same class as the attribute template for this class .",
    "we denote the template of class @xmath60 as @xmath95 .",
    "the elements of this template indicate the prior probabilities of the attributes with respect to this class .",
    "we classify the samples by directly measuring the euclidean distance between the attribute existence probabilities of the sample and the attribute template of a class    @xmath96    where @xmath97 returns the class label of a sample .",
    "* datasets : * we use three datasets to validate the proposed approach : animal with attributes ( awa )  @xcite , caltech - ucsd birds ( cub )  @xcite and unstructured social activity attribute ( usaa )  @xcite .",
    "awa contains 30,475 images of 50 animal classes .",
    "each class is annotated with 85 attributes . following  @xcite",
    ", we divide the dataset into 40 classes ( 24,295 images ) to be used for training and 10 classes ( 6180 images ) for testing .",
    "cub ( 2011 version )  @xcite contains roughly 11,800 images of 200 bird classes .",
    "each class is annotated with 312 binary attributes .",
    "we split the dataset following  @xcite to facilitate direct comparison ( 150 classes for training and the rest 50 classes for testing ) .",
    "usaa is a video dataset  @xcite with 69 instance - level attributes for 8 classes of complex social group activity videos from youtube .",
    "each class has around 100 training and testing videos respectively .",
    "we follow  @xcite for splitting the dataset by randomly dividing the 8 classes into two disjoint sets of four classes each for training and testing ( the mean accuracies will be reported ) .",
    "* features : * we adopt the 4096-dimensional deep learning features named decaf  @xcite as the baseline feature for the awa dataset since these features are already been available online for comparison .",
    "we extract 4096-dimensional deep learning features called caffe  @xcite for representing the images in cub database .",
    "the usaa databases already provided the 14,000-dimensional baseline featureswhich are constructed from six histogram features , namely rgb color histograms , sift , rgsift , phog , surf and local self - similarity histograms  @xcite .",
    "* metrics : * we report the classification accuracy ( in % ) averaged over the classes as the n - shot learning and zsl accuracy in the awa and cub databases . in the usaa database ,",
    "we follow  @xcite and report the absolute classification accuracy of data . for attribute prediction accuracies",
    ", we report the average area under curve ( auc ) for the roc .",
    ".average attribute prediction accuracies ( in auc ) .",
    "[ attpred ] [ cols=\"^,^,^,^ \" , ]      we also conduct several experiments to test the potentials of the kernel hap algorithms in zero - shot learning . the gaussian kernel and cauchy kernel",
    "are applied .",
    "figure  [ kernelfig ] shows the zsl performances of these kernel hap algorithms in usaa and cub datasets . in usaa dataset",
    ", we can find that gaussian kernel improves the zsl accuracies of hap , cshap@xmath1 and cshap@xmath2 from 44.1% , 45.3% and 44.6% to 46.3% , 48.2% and 46.7% .",
    "the zsl accuracies of three cauchy kernel - based hap algorithms are 46.1% , 48.3% and 47.0% . in cub",
    "database , these two kernels actually reduced the zsl accuracies of hap algorithms .",
    "the accuracies of the kernel hap algorithms is around 15.5% to 16.5% .",
    "we attribute this to the fact that the deep features used in cub dataset are originally designed to be linear .",
    "the experimental results of the choices of the parameters are reported in the supplementary material .",
    "since hap is graph - based algorithm and involves the matrix inversion , its computational complexity for training is the minimum of @xmath98 and @xmath99 and its computational complexity of testing is @xmath100 .",
    "so , it is more time consuming for training but quite efficient for testing . taking the cub dataset as an example ( 5994 samples for training and 5974 samples for testing ) ,",
    "the time for training 312 attribute predictors is 23.33 seconds .",
    "the time for predicting the attributes of all test samples is 0.14 seconds .",
    "the code is written in matlab and the experimental hardware configuration is quad - core cpu : 2.5 ghz , ram : 8 g .",
    "we presented a novel attribute prediction approach called hypergraph - based attribute predictor ( hap ) via deriving a collection of attribute classifiers from the hypergraph embedding , in which the attribute relations are considered as hyperedges and the hypergraph cuts are the attribute predictions .",
    "the hypergraph formulation facilitates exploiting the correlations of the attributes as well as jointly learning the attribute predictors .",
    "moreover , the additional information can be flexibly incorporated into hap via encoding the information in a penalty graph or hypergraph . to generalize the mappings between the feature space and attribute space which are known as the attribute predictors",
    ", we also kernelized the model .",
    "extensive experiments on three well known attribute datasets demonstrated the effectiveness of our model for attribute prediction , zero - shot learning , n - shot learning and categorization . from the results",
    "we can conclude that the cshap@xmath2 variant is the best to integrate class labels for n - shot learning , however the three proposed variants performs similarly in zero - shot learning task .",
    "the work described in this paper was partially supported by the national natural science foundation of china ( grant no . 61173131,91118005 , 11202249 ) , program for changjiang scholars and innovative research team in university ( grant no .",
    "irt1196 ) and the fundamental research funds for the central universities ( grant nos . cdjzr12098801 and cdjzr11095501 ) .",
    "dan yang is the corresponding author of this paper ."
  ],
  "abstract_text": [
    "<S> we present a novel attribute learning framework named hypergraph - based attribute predictor ( hap ) . in hap , </S>",
    "<S> a hypergraph is leveraged to depict the attribute relations in the data . </S>",
    "<S> then the attribute prediction problem is casted as a regularized hypergraph cut problem in which hap jointly learns a collection of attribute projections from the feature space to a hypergraph embedding space aligned with the attribute space . </S>",
    "<S> the learned projections directly act as attribute classifiers ( linear and kernelized ) . </S>",
    "<S> this formulation leads to a very efficient approach . by considering our model as a multi - graph cut task </S>",
    "<S> , our framework can flexibly incorporate other available information , in particular class label . </S>",
    "<S> we apply our approach to attribute prediction , zero - shot and @xmath0-shot learning tasks . </S>",
    "<S> the results on awa , usaa and cub databases demonstrate the value of our methods in comparison with the state - of - the - art approaches . </S>"
  ]
}