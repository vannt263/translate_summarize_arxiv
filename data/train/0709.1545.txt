{
  "article_text": [
    "emerging trends in data acquisition and imaging technologies have resulted in a rapid increase in the volume of data .",
    "hence source coding ( or data compression ) continues to occupy an important area of research in the design of communication and storage systems .",
    "shannon , the father of information theory , provided the definition of entropy as the ultimate limit of lossless data compression .",
    "ever since , there have been a number of compression algorithms which tries to achieve this limit .",
    "the source coding problem is stated as follows : given an independent and identically distributed ( i.i.d ) binary source @xmath0 emitting bits of information in the absence of noise , how do we obtain the shortest lossless representation of this information ?",
    "source coding is also known as entropy coding or data compression and is an important part of most communication systems  @xcite .",
    "shannon in his 1948 masterpiece  @xcite defined the most important concept of information theory , namely ` entropy ' .",
    "shannon s entropy of a source @xmath1 is defined as the amount of information content or the amount of uncertainty associated with the source , or equivalently the least number of bits required to represent the information content of a source without any loss .",
    "shannon proposed a method ( shannon - fano coding  @xcite ) that achieves this limit as the block - length ( number of symbols taken together ) for coding increases asymptotically to infinity .",
    "huffman  @xcite proposed what are called minimum - redundancy codes with integer code - word lengths that achieve shannon s entropy in the limit of the block - length tending to infinity",
    ". however , there are problems associated with both shannon - fano coding and huffman coding ( and other similar techniques ) . as the block - length increases , the number of alphabets exponentially increase , thereby increasing the memory needed for storing and handling . also ,",
    "the complexity of the encoding algorithm increases since these methods build code - words for all possible messages for a given length instead of designing the codes for a particular message at hand .",
    "another disadvantage of all such methods is that they do not lend themselves easily to an adaptive coding technique  @xcite .",
    "the idea of adaptive coding is to update the probability model of the source during the coding process .",
    "unfortunately , for both huffman and shannon - fano coding , the updating of the probability model would result in re - computation of the code - words for all the symbols which is an expensive process .",
    "recently , we have proposed a new approach to address the source coding problem using a dynamical systems perspective  @xcite .",
    "we modeled the information bits of the source @xmath0 as measurements of a non - linear dynamical system .",
    "since measurement is rarely accurate , we treat these measured bits of information as a symbolic sequence  @xcite of the tent map  @xcite and their skewed cousins .",
    "source coding is seen as determination of the initial condition that has generated the given symbolic sequence .",
    "subsequently , we established that such an approach leads us to a well known entropy coding technique ( arithmetic coding ) which is optimal for compression .",
    "furthermore , this new approach enabled a robust framework for joint source coding and encryption .    in this paper",
    ", we focus on constrained sources where certain words are forbidden from its message space .",
    "such sources are longer i.i.d because they violate the independence assumption ( most natural sources are not independent , for e.g. , english text is clearly not independent ) .",
    "we consider only ergodic markov sources in this paper .",
    "we propose a non - linear dynamical systems approach to compress messages from these constrained sources .",
    "we shall first start with an i.i.d source @xmath0 which can be thought of as emitting a sequence of random variables @xmath2 .",
    "each random variable takes values independently from the common alphabet @xmath3 with probabilities @xmath4 respectively .",
    "a message from this source is a particular sequence of values @xmath5 where @xmath6 is drawn from @xmath7 , @xmath8 from @xmath9 and so on .",
    "since these are i.i.d , we can think of them as being drawn from the common random variable @xmath0 ( we use the notation @xmath0 for both the source and the common random variable ) .",
    "we always deal with finite sized alphabets ( @xmath10 ) and finite length messages ( @xmath11 ) since real - world messages are always finite in length .",
    "our aim is to embed this i.i.d source into a non - linear discrete dynamical system .",
    "the reason for doing this will be clear soon .",
    "to this end , we model the i.i.d source as a 0-order markov source ( memoryless source ) .",
    "each alphabet can be seen as a markov state .",
    "since these are independent , the transition probability from state @xmath12 to state @xmath13 is equal to the probability of being in state @xmath13 . in other words , @xmath14 .",
    "we wish to embed this 0-order markov source into a non - linear dynamical system @xmath15 where @xmath16 is the set @xmath17 , @xmath18 is the borel @xmath19-algebra on @xmath17 , @xmath20 is the measure preserving transformation ( yet to be defined ) and @xmath21 is the invariant measure .",
    "here we consider the probability measure ( or lebesgue measure ) as the invariant measure .",
    "we now need to define @xmath20 which preserves the lebesgue measure and which can _ simulate _",
    "the 0-order markov source faithfully .",
    "the non - linear discrete dynamical system known as generalized lurth series ( gls )  @xcite ( figure  [ fig : figgls1 ] ) embeds the 0-order markov source .",
    "we list the important properties of gls which enable this embedding :    1 .",
    "the number of partitions ( disjoint intervals which cover the space , also known as cylinders ) of the gls is equal to the size of the alphabet .",
    "each alphabet is used to ` label ' a partition .",
    "the size of each partition is equal to the probability of the corresponding alphabet .",
    "4 .   the map is linear and surjective on each of the partitions .",
    "gls preserves the lebesgue measure  @xcite .",
    "6 .   successive digits ( or symbols ) of the gls are i.i.d .",
    ".   every unique sequence of digits ( or symbols ) maps to a unique point @xmath22 in [ 0,1 ) under @xmath20 .",
    "in other words , every point @xmath22 has an unique representation in terms of the alphabets of gls .",
    "we call @xmath22 as the initial condition corresponding to the symbolic sequence ( any sequence of digits composed from the alphabets associated with the partitions )",
    "gls is chaotic ( positive lyapunov exponent and positive topological entorpy ) .",
    "the gls transformation @xmath20 on [ 0,1 ) is isomorphic to the bernoulli shift  @xcite .",
    "hence gls is ergodic with lebesgue as the invariant measure .",
    "are the alphabets from @xmath23 .",
    "the lengths of the intervals are precisely the respective probabilities @xmath24 . ]",
    "gls preserves the lebesgue measure .",
    "a probability density @xmath25 on [ 0,1 ) is invariant , if for each interval @xmath26 \\subset [ 0,1)$ ] , we have :    @xmath27 ) } \\pi(x)dx.\\ ] ]    where @xmath28 ) = \\ { x | c \\leq t(x ) \\leq d \\}$ ] .    for the gls , the above condition has constant probability density on @xmath17 as the only solution .",
    "it then follows from birkhoff s ergodic theorem  @xcite that the asymptotic probability distribution of the points of almost every trajectory is uniform .",
    "we can hence calculate lyapunov exponent as follows :    @xmath29    here , we measure @xmath30 in bits / iteration .",
    "@xmath25 is uniform with value 1 on [ 0,1 ) and @xmath31 since @xmath32 is linear in each of the partitions , the above expression simplifies to :    @xmath33    this is nothing but shannon s entropy of the source @xmath0 .",
    "thus lyapunov exponent of the gls that embeds the i.i.d source @xmath0 is equal to the shannon s entropy of the source .",
    "lyapunov exponent can be understood as the amount of information in bits revealed by the dynamical system in every iteration ( ? ) .",
    "the number of partitions together with the lyapunov exponent completely characterizes gls ( up to a permutation of the partitions and flip of the graph in each parition - this changes the sign of the slope , but not its magnitude ) .",
    "in the previous section , we have seen how we can embed a stochastic i.i.d source @xmath0 in to a dynamical system ( generalized lurth series ) .",
    "the motivation for this is that modeling the stochastic source by embedding in to a non - linear dynamical system is way to achieve compression .",
    "we know that huffman coding is not shannon optimal  @xcite .",
    "this can be easily seen if the original source @xmath0 took only 2 values ` a ' and ` b ' with probabilities @xmath34 . for @xmath35 ,",
    "shannon s entropy of source @xmath0 is @xmath36 bit whereas in huffman coding , we would allocate one bit to encode ` a ' and b. that is the best huffman coding can do .",
    "thus , by using huffman coding , we would be up to 1 bit away from shannon s entropy per symbol .",
    "this can be very expensive for skewed sources ( where @xmath37 is close to 0 or 1 ) which have a very low shannon entropy .",
    "this means , we can do better for such sources .    we have already said that every sequence of measurements ( the message ) is a symbolic sequence on an appropriate gls .",
    "it is well known fact about dynamical systems that the symbolic sequence contains as much information as the initial condition .",
    "hence , we could as well find out the initial condition for every symbolic sequence and use that as our compressed stream .",
    "hence , the task of capturing the essential information of the source @xmath0 now translates to determining the initial condition on the gls ( the source model ) and storing the initial condition in whatever base we wish ( typically the initial condition is binary encoded ) .",
    "thus , the task of source compression is now one of finding the initial condition",
    ". we shall henceforth refer to this method as gls - coding .",
    "how good is gls - coding when compared to huffman coding ?",
    "it turns out that the method just described is the popular arithmetic coding algorithm which is used in international compression standards such as jpeg2000 and h.264 .",
    "it is already known that arithmetic coding always achieves shannon s optimality without having to compute codewords for all possible messages and this make it better than huffman coding .",
    "we have thus re - discovered arithmetic coding using a dynamical systems approach to source coding . for full details of the proof of equivalence with arithmetic coding ,",
    "the reader is referred to @xcite .",
    "so far , we have dealt with an i.i.d source @xmath0 . in the real world , most sources are not independent even if they are identically distributed . as an example , for a particle traveling in space , the measurements of its position and velocity is clearly not independent across successive time units .",
    "thus , the assumption of independence needs to be relaxed .    in communications ,",
    "independence assumption is not generally true .",
    "for example , assume that the source @xmath0 is an excerpt from an english text .",
    "the probability that the letter ` @xmath38 ' appears after ` @xmath39 ' is very high .",
    "thus , given that a particular symbol has occurred , there is probably a very small set of alphabets that can occur with a very high probability .    in this paper",
    ", we consider another kind of dependence known as a constrained source .",
    "a constrained source is one where certain ` words ' are forbidden from the message space . as an example , if the source is an english text , certain words are forbidden ( words which are profane or even words which are just gibberish , like for e.g. , ` qwzty ' ) . for the rest of the paper",
    ", we shall consider a binary constrained source @xmath0 .",
    "the constraint is given in terms of a list of forbidden words ( for e.g. , the word ` 101 ' and ` 011 ' may be known never to occur in any of the messages emitted by the source , then both these words are defined as forbidden words ) .",
    "this is clearly not an i.i.d source any more . because given that a ` 10 ' as occurred , the next symbol can only be ` 0 ' .",
    "thus the present symbol depends on what occurred in the last two instances .",
    "we are interested in compressing sequences of such a source in the most optimal fashion .",
    "yields an i.i.d source and the case @xmath40 yields the source with forbidden word ` 101 ' .",
    "thus the general 4-state ergodic markov source captures both cases.,title=\"fig : \" ]   yields an i.i.d source and the case @xmath40 yields the source with forbidden word ` 101 ' .",
    "thus the general 4-state ergodic markov source captures both cases.,title=\"fig : \" ]      we shall model constrained sources as a markov source . in this paper",
    ", we shall consider only those markov sources that are ergodic .",
    "a markov source is ergodic if either the markov chain itself is ergodic  @xcite or equivalently , the dynamical system in which the markov chain is embedded is ergodic .",
    "we know by the theory of markov chains  @xcite that all finite discrete time markov chains that are ergodic ( i.e. they are irreducible and aperiodic ) have an unique stationary distribution , also known as invariant distribution .",
    "this means that given any arbitrary initial distribution on the markov states , it eventually settles to an unique probability distribution .",
    "once we have an unique stationary distribution with finite states , shannon s entropy rate can be calculated . in general , it is hard to determine the invariant measure of the underlying dynamical system in which the markov chain is embedded , but it is relatively easy to determine whether the markov chain is ergodic ( all we need to do is test for irreducibility and aperiodicity ) .",
    "we already saw in section 2 how an i.i.d source @xmath0 can be seen as a 0-order markov chain with finite number of states .",
    "we could do similarly for ergodic markov sources . as an example",
    ", a binary ergodic markov source with ` 101 ' as a forbidden word is shown in figure  [ fig : figforbidden12 ] .      in figure",
    "[ fig : figforbidden12 ] , a general four state markov source is also shown .",
    "the condition @xmath41 implies that the source is i.i.d .",
    "the condition @xmath42 and @xmath43 corresponds to the ergodic markov source with the forbidden word ` 101 ' .",
    "we shall deal with the general four state markov source ( assuming that it is ergodic ) .",
    "the transition probability matrix for the general four state ergodic markov source is given by : @xmath44    the unique stationary probability distribution @xmath45 ( a row vector of dimension @xmath46 ) can be determined by solving the equation : @xmath47    since @xmath48 is always a stochastic matrix ( every row adds to 1 ) of dimension @xmath49 , there exists a unique eigenvector @xmath45 for @xmath48 corresponding to the eigenvalue 1 .",
    "we normalize @xmath45 as follows : @xmath50    once we have @xmath51 we can compute shannon s entropy of the source @xmath0 as follows :    @xmath52    the units of @xmath53 is bits / symbol and @xmath54 represents the number of markov states . for the general 4-state ergodic markov source , this will be 1/2 since each states accounts for 2 bits of the message .      in this section , we shall embed the general 4-state ergodic markov source into a non - linear dynamical system .",
    "we shall determine the initial condition for any given symbolic sequence ( message ) on the resulting dynamical system and use the initial condition to compress the message ( similar to gls - coding ) .",
    "we shall show that such a method achieves shannon s entropy rate @xmath53 .    :  [ 0,1 ) @xmath55 [ 0,1 ) . ]",
    "we shall embed the general 4-state ergodic markov source into a non - linear dynamical system similar to gls for the i.i.d source . to each markov state , we associate a markov partition of the dynamical system ( refer to figure  [ fig : figmodgls1 ] ) .",
    "we want the messages of this source to be symbolic sequences of the dynamical system .",
    "draw straight lines with slopes yet to be determined connecting those states that communicate .",
    "for example , 00 communicates only with 01 and 10 .",
    "let the lengths of the markov paritions be @xmath6 , @xmath8 , @xmath56 and @xmath57 .",
    "we then write the `` measure - preserving '' constraints as follows :    @xmath58    these constraints automatically satisfy @xmath59 . solving the first two of the above equation yields @xmath60 .",
    "the above linear set of equations can be solved for the given set of @xmath61 ( if the markov chain is ergodic , a unique solution always exists ) .",
    "the solution gives the length of the markov partitions @xmath62 which is unique .",
    "the slopes of the line segments are determined from the probabilities .",
    "we thus have a dynamical system ( modified gls ) and we shall show that this embeds the ergodic markov source .",
    "how do we understand the `` measure - preserving '' constraints ? as an example , consider the markov partition of length @xmath6 corresponding to the state 00 . since 00 communicates only with itself and 01",
    ", we have the range of the function limited to intervals 00 and 01 .",
    "the slope of the line segment ( in blue ) which maps fraction of @xmath6 to 00 is @xmath63 .",
    "this is because whenever we are in state 00 , the probability that we end up in the same state on receiving the next symbol is @xmath64 .",
    "thus @xmath65 is the fraction of initial conditions which end up in the same state 00 .",
    "the remaining @xmath66 fraction of initial conditions end up in 01 .",
    "similarly , we do this for all the states .",
    "the `` measure - preserving '' constraint for say the state 00 is indicating what fraction of initial conditions end up in state 00 in one iteration .",
    "this is formed precisely as the sum of the fraction @xmath65 which come from 00 and @xmath67 that comes from 10 ( because these are the only two states that communicate with 00 ) .",
    "this is how we get all the constraint equations .",
    "the linear set of `` measure - preserving '' constraints can be written in matrix form as follows : + @xmath69    notice that the above equation is the same as @xmath68 where @xmath48 is the transition probability matrix of the markov source .",
    "thus @xmath70 is nothing but the unique stationary probability distribution obtained from the equation @xmath68 .",
    "we have thus embedded the ergodic markov source in to a non - linear dynamical system .          in order to prove that encoding the initial condition on the modified gls will achieve shannon s entropy rate ,",
    "we compute the lyapunov exponent of the modified gls and show that this is the same as shannon s entropy rate .",
    "in fact , the non - linear dynamical system is a faithful modelling of the general 4-state ergodic markov source .",
    "it is important to observe that the modified gls shown in figure  [ fig : figmodgls1 ] does not preserve the lebesgue measure .",
    "however , if we restrict all the intervals on the y - axis to strictly lie in one of the four intervals labelled 00 , 01 , 10 and 11 only , then we see that the lebesgue measure ( = probability measure ) is actually preserved ( sum of the measures of all the inverse images of a particular interval on the y - axis is equal to the measure of that interval we started with ) .",
    "in fact , this is what we ensured by the `` measure preserving '' constraints in the first place .",
    "we could hence compute the lyapunov exponent as if the map preserved the lebesgue measure everywhere .",
    "computation of lyapunov exponents is then straightforward and yields the following expression ( the base of the logarithm in all our lyapunov exponent computation is always 2 , though the standard practice is to use @xmath71 . )",
    "@xmath72    this is nothing but twice the shannon s entropy rate @xmath53 of the source .",
    "the factor 2 is because in every iteration , the symbolic sequence emitted by the source consists of two symbols .",
    "thus , we have faithfully modelled the general 4-state ergodic markov source as a non - linear dynamical system .",
    "hence modified - gls coding would achieve the shannon s entropy rate and is optimal for compression .",
    "in this paper , we have shown how one can embed an i.i.d source in to a non - linear dynamical system , namely generalized lurth series or gls .",
    "we then considered markov sources which are not independent anymore .",
    "constrained sources were defined as ergodic markov sources with certain forbidden words .",
    "we showed how to compute the shannon s entropy rate and also modified gls to compresses messages from these sources .",
    "the modified - gls is a faithful embedding of constrained sources and hence achieves shannon s entropy rate .",
    "it is possible to generalize our method for a list of arbitrary forbidden words .",
    "implementation issues were not discussed in this paper .",
    "it may be worthwhile to investigate joint compression and encryption for constrained sources .",
    "nithin nagaraj would like to express his sincere gratitude to the department of science and technology ( dst ) for funding the ph.d .",
    "fellowship program at national institute of advanced studies ( nias ) .",
    "we gratefully acknowledge dst , govt . of india and council of scientific and industrial research ( csir ) ,",
    "govt . of india for providing with travel grant to present this work at the `` international conference on non - linear dynamics and chaos : advances and perspectives '' , held at university of aberdeen , scotland , september 17 - 21 , 2007 .",
    "vaidya , n. nagaraj , foundational issues of chaos and randomness : `` god or devil , do we have a choice ? '' , proc . of foundations of sciences , project of history of indian science , philosophy and culture new delhi ( 2006 ) ."
  ],
  "abstract_text": [
    "<S> we have recently established a strong connection between the tent map ( also known as generalized luroth series or gls which is a chaotic , ergodic and lebesgue measure preserving non - linear dynamical system ) and arithmetic coding which is a popular source compression algorithm used in international compression standards such as jpeg2000 and h.264 . </S>",
    "<S> this was for independent and identically distributed binary sources . in this paper , we address the problem of compression of ergodic markov binary sources with certain words forbidden from the message space . </S>",
    "<S> we shall show that gls can be modified suitably to achieve shannon s entropy rate for these sources . </S>"
  ]
}