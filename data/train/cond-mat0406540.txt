{
  "article_text": [
    "kinetic monte carlo ( kmc ) is an extremely efficient method @xcite to carry out dynamical simulations when the relevant activated atomic - scale processes are known , and kmc simulations have been used to model a variety of dynamical processes ranging from catalysis to thin - film growth .",
    "the basic principle of kinetic monte carlo is that the probability that a given event will be the next event to occur is proportional to the rate for that event . since all processes are assumed to be independent poisson processes , the time of the next event is determined by the total overall rate for all processes , and after each event the rates for all processes are updated as necessary .",
    "in contrast to metropolis monte carlo , @xcite in which each monte carlo step corresponds to a configuration - independent time interval and each event is selected randomly but only accepted with a configuration - dependent probability , in kinetic monte carlo both the selected event and the time interval between events are configuration - dependent while all attempts are accepted . in the context of traditional monte carlo simulations",
    "this is sometimes referred to as the n - fold way .",
    "@xcite while kmc requires additional book - keeping to keep track of the rates of all possible events , the kmc algorithm is typically significantly more efficient than the metropolis algorithm since no selected moves are rejected . in particular , for problems such as thin - film growth in which the possible rates or probabilities for events can vary by several orders of magnitude , the kinetic monte carlo algorithm can be orders of magnitude more efficient than metropolis monte carlo .    because the attempt time in metropolis monte carlo is independent of system configuration ,",
    "parallel metropolis monte carlo simulations may be carried out by using an asynchronous  conservative \" algorithm.@xcite in such an algorithm all processors whose next attempt time is less than their neighbor s next attempt times are allowed to proceed .",
    "unfortunately such a  conservative \" algorithm does not work for kinetic monte carlo since in kmc the event - time depends on the system configuration . in particular , since fast events may  propagate \" across processors , the time for an event already executed by a processor may change due to earlier events in nearby processors , thus leading to an incorrect evolution . as a result ,",
    "the development of efficient parallel algorithms for kinetic monte carlo simulations remains a challenging problem .",
    "lubachevsky has developed@xcite and korniss et al have implemented@xcite a more efficient version of the conservative asynchronous algorithm for parallel dynamical monte carlo simulations of the spin - flip ising model .",
    "the basic idea is to apply metropolis dynamics to events on the boundary of a processor , but to accelerate interior moves by using the @xmath4-fold way .",
    "the choice of a boundary or interior move is determined by the ratio of the number of boundary sites to the sum of the acceptance probabilities for all interior moves .",
    "while all  n - fold way \" interior moves are immediately accepted , all metropolis attempts must wait until the neighboring processor s next attempt time is later before being either accepted or rejected . since such an algorithm is equivalent to the conservative metropolis monte carlo algorithm described above , it is generally scalable,@xcite and has been found to be relatively efficient in the context of kinetic ising model simulations in the metastable regime .",
    "@xcite    recently we have shown@xcite that such an approach can be generalized in order to carry out parallel kmc simulations.in this approach , all possible kmc moves are first mapped to metropolis moves with an acceptance probability for each event given by the rate for that event divided by the fastest possible rate in the kmc simulation . at each stage , the choice of a boundary move versus an interior move is determined by the ratio of a fixed number corresponding to the sum of the rates for all _ possible _ events which might occur in the boundary region to the sum of the rates for all existing interior moves .",
    "however , because of the possibility of significant rejection of boundary events , the parallel efficiency of such an algorithm can be very low for problems with a wide range of rates for different processes . for example , we have recently@xcite used such a mapping to carry out parallel kmc simulations of a simple 2d solid - on - solid  fractal \" model of submonolayer growth with a moderate ratio @xmath5 of monomer hopping rate @xmath2 to ( per site ) deposition rate @xmath3 . however , due to the existence of significant rejection of boundary events , very low parallel efficiencies were obtained.@xcite furthermore , in order to use such an approach , in general one needs to know in advance _ all _ the possible events and their rates and then to map them to metropolis dynamics so that all events may be selected with the appropriate probabilities .",
    "while such a mapping may be carried out for the simplest models , for more complicated models it is likely to be prohibitive .    in order to overcome these problems ,",
    "we have recently proposed a semi - rigorous synchronous sublattice ( sl ) parallel algorithm@xcite in which each processor s domain is further divided into sublattices in order to avoid a possible conflict between processors . at the beginning of a cycle , one sublattice",
    "is randomly selected so that all processors operate on the same sublattice .",
    "each processor then carries out kmc events for the selected sublattice over a time interval which is typically smaller than the inverse of the fastest single - event rate . at the end of each cycle",
    ", each processor communicates with its neighboring processors in order to update its boundary region . by carrying out extensive simulations of simple models of thin - film growth",
    "we have found that this algorithm leads to a relatively high parallel efficiency and is scalable , i.e. the parallel efficiency is constant as a function of the number of processors @xmath6 .",
    "however , for extremely small processor sizes ( smaller than a typical diffusion length in epitaxial growth ) weak finite - size effects are observed .",
    "thus for problems in which the diffusion length is large or very small processor sizes are required , it may be preferable to use a more rigorous algorithm .    in this paper",
    "we discuss the application of a second rigorous algorithm , the synchronous relaxation ( sr ) algorithm,@xcite to kinetic monte carlo simulations .",
    "this algorithm was originally used by eick et al @xcite to simulate large circuit - switched communication networks .",
    "more recently an estimate of its efficiency has been carried out by lubachevsky and weiss@xcite in the context of ising model simulations , in the sr algorithm , all processors remain globally synchronized at the beginning and end of a time interval , while an iterative relaxation method is used to correct errors due to neighboring processors boundary events .",
    "since this algorithm is rigorous , the cycle length can be tuned to optimize the parallel efficiency and several optimization methods are discussed .",
    "however , we find that the requirement of global synchronization leads to a logarithmic increase with increasing processor number in both the relevant fluctuations in the number of events per processor as well as the global communication time .",
    "accordingly , the sr algorithm does not scale since the parallel efficiency decreases logarithmically as the number of processors increases . as a result , the parallel efficiency is generally significantly smaller than for the sl algorithm .",
    "the organization of this paper is as follows . in section",
    "ii we describe the algorithm and discuss several different methods of optimization . in section",
    "iii we present results obtained using this algorithm for three different models of thin - film growth , along with a brief comparison with serial results .",
    "we then discuss the three key factors@xmath0number of additional iterations , fluctuations , and communications time@xmath0which determine the parallel efficiency of the sr algorithm .",
    "the dependence of the parallel efficiency on such parameters as the number of processors as well as the cycle length , processor size , and ratio @xmath1 of monomer hopping rate @xmath2 to ( per site ) deposition rate @xmath3 is also discussed .",
    "finally , in section iv we summarize our results .",
    "schematic diagram of ( a ) square and ( b ) strip decompositions .",
    "solid lines correspond to processor domains while dashed lines indicate  ghost - region \" surrounding central processor .",
    ", width=170 ]    as in previous work on the  conservative \" asynchronous algorithm,@xcite in the synchronous relaxation ( sr ) algorithm , different parts of the system are assigned to different processors via spatial decomposition .",
    "for the thin - film growth simulations considered here , one may consider two possible methods of spatial decomposition , a square decomposition and a strip decomposition as shown in fig .",
    "[ fig : fig1 ] .",
    "since the square decomposition requires communications with 4 neighbors while the strip decomposition only requires communications with 2 neighbors , we expect that the strip decomposition will have reduced communication overhead .",
    "accordingly , all the results presented here are for the case of strip decomposition . however , as discussed in more detail later , there may be some cases where the square decomposition is preferable .    in order to avoid communicating with processors beyond the nearest - neighbors , the processor size must be larger than the range of interaction ( typically only a few lattice units in simulations of thin - film growth ) .",
    "in addition , in order for each processor to calculate its event rates , the configuration in neighboring processors must be known as far as the range of interaction . as a result",
    ", in addition to containing the configuration information for its own domain , each processor s array also contains a  ghost - region \" which includes the relevant information about the neighboring processor s configuration beyond the processor s boundary .",
    "we now describe the sr algorithm in detail . at the beginning of each cycle corresponding to a time interval @xmath7 ,",
    "each processor initializes its time to zero .",
    "a first iteration is then performed in which each processor carries out kmc events until the time of the next event exceeds the time interval @xmath7 as shown in fig .",
    "[ fig : fig2 ] . as in the usual serial kmc",
    ", each event is carried out with time increment @xmath8 where @xmath9 is a uniform random number between @xmath10 and @xmath11 and @xmath12 is the total kmc event rate .",
    "diagram showing time evolution in the synchronous relaxation algorithm .",
    "dashed lines correspond to events , while dashed line with an @xmath13 corresponds to an event which is rejected since it exceeds the time interval @xmath7 .",
    "arrows indicate boundary events carried out by processors @xmath14 and @xmath15.,height=151 ]    at the end of each iteration , each processor communicates any boundary events with its neighboring processors , i.e. any events which are in the range of interaction of a neighboring processor and which could thus potentially affect the neighboring processor s event rates and times . here",
    "we define an event as consisting of the lattice sites which have changed due to the event along with the unique time @xmath16 of the event .",
    "if at the end of a given iteration , a processor has received any new or missing boundary events ( i.e. any boundary events different from those received in the previous iteration ) then that processor must  undo \" all of the kmc events which occurred after the time of the earliest new or missing boundary event , and then perform another iteration starting at that point using the new boundary information received .",
    "however , if no processors have received new or missing boundary events , then the iterative relaxation is complete , and all processors move on to the next cycle . in order to check for this , a global communications between all processors",
    "is performed at the end of each iteration .    in order for the iteration process to converge , one must ensure that within the same cycle the same starting configuration always leads to the same event or transition . since pseudorandom numbers are used in the kmc simulations considered here",
    ", this requires keeping a list of all the random numbers used during that cycle , and backtracking appropriately along the random number list as events are  undone \" so that the same random numbers are used for the same configurations . in the ideal implementation described above",
    ", events are only redone starting from the earliest new boundary event .",
    "however , in the kmc simulations carried out here , lists were used to efficiently select and keep track of all possible events . since properly undoing such lists is somewhat complex , here we have used a slightly less efficient but simpler method in which every iteration was restarted at the beginning of the cycle . in this case , the necessary changes in the configuration and random numbers were  undone \" back to the beginning of the cycle , while the state of the lists at the beginning of the cycle was restored . since there is significant overhead associated with",
    "undoing \" each move , and since in every iteration except the first , one needs to  undo \" on average only half of the events in the previous iteration we estimate such a simplification leads to at most a 25% reduction in the parallel efficiency .",
    "we now consider the general dependence of the parallel efficiency on the cycle time @xmath7 .",
    "if the cycle time is too short then there will be a small number of events in each cycle and as a result there will be large fluctuations in the number of events in different processors .",
    "this leads to poor utilization , i.e. some processors may process events during a given cycle while others may have very few or no events .",
    "in addition , for a short cycle time the communication latency may become comparable to the calculation time which also leads to a reduction in the parallel efficiency . on the other hand",
    ", a very long cycle time will lead to a large number of boundary events in each cycle , and as a result the number of relaxation iterations will be large .",
    "thus , in general the cycle length @xmath7 must be optimized in order to balance out the competing effects of communication latency , fluctuations , and iterations in order to obtain the maximum possible efficiency .",
    "we have used three different methods to control the time interval @xmath7 in order to optimize the parallel efficiency . in the first method ,",
    "we have used a fixed cycle length ( e.g. @xmath17 where @xmath2 is the monomer hopping rate ) and then carried out simulations with different values of @xmath18 in order to determine the optimal cycle length and maximize the parallel efficiency . in the second method ,",
    "we have used feedback to dynamically control the cycle length @xmath7 during a simulation . in particular , every @xmath19 cycles corresponding to a feedback interval , the elapsed execution time was either calculated or measured , and then used to calculate the ratio @xmath20 ( proportional to the parallel efficiency ) of the average number of events per cycle @xmath21 to the execution time . based on the values of @xmath20 obtained during the previous two feedback intervals , the cycle length @xmath7 was adjusted in order to maximize the parallel efficiency . in the third optimization method",
    "the cycle length was dynamically controlled in order to attain a pre - determined value for a target quantity such as the number of events per cycle ( @xmath22 ) or the number of iterations per cycle ( @xmath23 ) whose optimal value was determined in advance .",
    "this method turned out be the most effective since the parallel performance depends strongly on the number of iterations and/or the number of events per cycle .",
    "in contrast , while the parallel efficiency obtained using direct feedback was significantly better than that obtained using the first method , it was not quite as good as that obtained using the third method described above . as a result , here we focus mainly on the first and last methods .",
    "unfortunately , both of these methods require the use of additional simulations in order to determine the optimal parameters .",
    "in order to test the performance and accuracy of the synchronous relaxation algorithm we have used it to simulate three specific models of thin - film growth . in particular , we have studied three solid - on - solid ( sos ) growth models on a square lattice : a  fractal \" growth model , an edge - and - corner diffusion ( ec ) model , and a reversible model with one - bond detachment (  reversible model \" ) . in each of these three models the lattice configuration is represented by a two - dimensional array of heights and periodic boundary conditions are assumed . in the  fractal \" model,@xcite atoms ( monomers ) are deposited onto a square lattice with ( per site ) deposition rate @xmath3 , diffuse ( hop ) to nearest - neighbor sites with hopping rate @xmath2 and attach irreversibly to other monomers or clusters via a nearest - neighbor bond ( critical island size of @xmath11 ) .",
    "the key parameter is the ratio @xmath1 which is typically much larger than one in epitaxial growth . in this",
    "model fractal islands are formed in the submonolayer regime due to the absence of island relaxation .",
    "the ec model is the same as the fractal model except that island relaxation is allowed , i.e. atoms which have formed a single nearest - neighbor bond with an island may diffuse along the edge of the island with diffusion rate @xmath24 and around island - corners with rate @xmath25 ( see fig .",
    "[ fig : fig3 ] ) .",
    "finally , the reversible model is also similar to the fractal model except that atoms with one - bond ( edge - atoms ) may hop along the step - edge or away from the step with rate @xmath26 , thus allowing both edge - diffusion and single - bond detachment . for atoms hopping up or down a step , an extra ehrlich - schwoebel barrier to interlayer diffusion @xcite",
    "may also be included . in this model",
    ", the critical island size @xmath27@xcite can vary from @xmath28 for small values of @xmath29 , to @xmath30 for sufficiently large values of @xmath1 and @xmath29.@xcite     schematic diagram of island - relaxation mechanisms for ( a ) edge - and - corner and ( b ) reversible models.,height=132 ]    for the fractal and reversible models , the range of interaction corresponds to one nearest - neighbor ( lattice ) spacing , while for the ec model it corresponds to the next - nearest - neighbor distance .",
    "thus , for these models the width of the  ghost - region \" corresponds to one lattice - spacing .",
    "we note that at each step of the simulation , either a particle is deposited or a particle diffuses to a nearest - neighbor or next - nearest - neighbor lattice site . in more general models , for which concerted moves involving several atoms may occur , @xcite the ghost region needs to be at least as large as the range of interaction and/or the largest possible concerted move .",
    "in such a case , the processor to which a concerted event belongs can be determined by considering the location of the center - of - mass of the atoms involved in the concerted move .    in order to maximize both the serial and parallel efficiency in our kmc simulations",
    ", we have used lists to keep track of all possible events of each type and rate .",
    "each processor maintains a set of lists which contains all possible moves of each type .",
    "a binary tree is used to select which type of move will be carried out , while the particular move is then randomly chosen from the list of the selected type . after each move , the lists are updated .      in order to test our algorithm",
    "we have carried out both  serial emulations \" as well as parallel simulations . however , since our main goal is to test the performance and scaling behavior on parallel machines we have primarily focused on direct parallel simulations using the itanium and amd clusters at the ohio supercomputer center ( osc ) as well as on the alpha cluster at the pittsburgh supercomputer center ( psc ) .",
    "all of these clusters have fast communications@xmath0the itanium and amd clusters have myrinet and the alphaserver cluster has quadrics . in our simulations ,",
    "the interprocessor communications were carried out using mpi ( message - passing interface ) .",
    "comparison between serial and parallel results for the fractal model with @xmath31 and @xmath5.,width=264 ]    we first present a brief comparison between serial and parallel results in order to verify the correctness of our implementation of the sr algorithm .",
    "figure [ fig : fig4 ] shows the monomer and island densities as a function of coverage @xmath32 for the fractal model with @xmath33 and system size @xmath31 with parallel processor sizes @xmath34 and @xmath35 and @xmath36 corresponding to @xmath37 and @xmath38 respectively ( where @xmath6 is the number of processors ) . as can be seen , there is excellent agreement between the serial and parallel calculations even for @xmath39 , the smallest processor size we have tested . using the sr algorithm , we have also obtained excellent agreement between serial and parallel results for the monomer and island densities for the ec model ( not shown ) .",
    "the parallel efficiency is determined by the competing effects of communication time , fluctuations , and number of relaxation iterations .",
    "in particular , the parallel execution time @xmath40 for @xmath6 processors in cycle @xmath41 can be written as @xmath42 where @xmath43 and @xmath44 denote the calculation and communication time respectively , while the last term @xmath45 includes all other timing costs not included in @xmath46 such as sorting boundary events received from neighbors and comparing new boundary events with old ones to see if a new iteration is needed .",
    "if there are few boundary events ( as is often the case ) , then @xmath45 may be ignored .",
    "the calculation time @xmath47 in eq .",
    "[ eqn1 ] may be written as , @xmath48 where @xmath49 denotes the average serial calculation time per kmc event , @xmath50 is the average number of actual events ( averaged over all processors ) per processor in cycle @xmath41 , and @xmath51 corresponds to the additional number of events which must be processed due to fluctuations and relaxation iterations .    since all processors are synchronized after each iteration , in each iteration the total calculation time is determined by the processor which has the maximum number of events to process .",
    "therefore one may write , @xmath52 ) - n_{av}(\\tau ) \\label{eqn3}\\ ] ] where @xmath53 is the maximum ( over all processors ) number of new events in the @xmath54th iteration , @xmath55 is the total number of relaxation iterations , and @xmath56 is a factor which reflects the reduced work to  undo \" a kmc event as compared to executing a kmc event .",
    "thus , the parallel efficiency ( pe ) can be approximated as @xmath57^{-1 }   \\label{eqn4}\\ ] ] where @xmath58 is the time for a serial simulation of a single processor s domain , @xmath59 , and the brackets denote an average over all cycles . if the communication time is negligible compared to @xmath60 ( i.e. , @xmath61 ) , the maximum possible parallel efficiency can be approximated as @xmath62^{-1}\\label{eqn5}\\ ] ] we note that @xmath63 depends primarily on two quantities , the ( extreme ) fluctuations over all processors in the number of actual events in each cycle @xmath64 , and the average number of iterations @xmath55 per cycle .",
    "in particular , one may approximate the additional overhead due to iterations and fluctuations as , @xmath65 where a factor of @xmath66 with @xmath67 has been included to take into account the fact that after the first iteration , the number of new events @xmath68 is typically less than @xmath69 .",
    "this result indicates that in the limit of negligible communications overhead , both the average number of iterations per cycle @xmath55 and the relative fluctuations @xmath70 should be small in order to maximize the parallel efficiency .",
    "we now consider the dependence of each of these quantities on the the cycle length @xmath7 and the number of processors @xmath6 .",
    "number of additional iterations as a function of average number of events per cycle @xmath21 with @xmath71 . for the ec model @xmath72 and @xmath73",
    "are used with @xmath74 and @xmath75 . here",
    "@xmath76 and @xmath77 for all cases and @xmath78 . , width=264 ]    figure [ fig : fig5 ] shows the number of additional iterations beyond the first iteration @xmath79 as a function of the average number of events per cycle @xmath21 for the fractal and ec models with @xmath80 using strip decomposition and two different processor sizes for different values of @xmath1 .",
    "also shown in fig .",
    "[ fig : fig5 ] are results for a larger than required ghost - region @xmath81 in order to test the dependence of the number of iterations on the range of interaction .",
    "as can be seen , the number of additional iterations is roughly linearly proportional to the average number of events per cycle .",
    "interestingly , for the same average number of events per cycle @xmath21 , the number of additional iterations depends relatively weakly on the model , the processor height @xmath82 , and the value of @xmath1 .",
    "however , doubling the width of the  ghost \" region from @xmath83 to @xmath81 leads to an increase by a factor of approximately @xmath84 in the number of iterations .",
    "figure [ fig : fig6 ] shows the number of additional iterations @xmath85 as a function of the cycle length @xmath7 for the fractal model with @xmath33 , @xmath76 , and @xmath86 . as can be seen , the number of additional iterations is roughly but not quite proportional to the cycle length @xmath7 . also shown in fig .",
    "[ fig : fig6 ] is the parallel efficiency , which has been directly measured from the execution time using the definition given in eq .",
    "the maximum parallel efficiency occurs when @xmath87 and corresponds to @xmath88 kmc events per cycle .",
    "we have also calculated ( not shown ) the optimal cycle length @xmath89 for the same processor size for other values of @xmath1 ranging from @xmath90 to @xmath91 .",
    "while the optimal cycle length varies by approximately two orders of magnitude over this range of @xmath1 , the average optimal number of events per cycle does not change much@xmath0@xmath92 and @xmath93 for @xmath94 and @xmath91 respectively .",
    "number of additional iterations and parallel efficiency for the fractal model as a function of the multiplication factor @xmath18 with @xmath76 , @xmath77 and @xmath17.,width=264 ]     number of additional iterations as a function of the number of processors @xmath6 with @xmath95 for the fractal and edge diffusion models with @xmath33 , @xmath71 and @xmath77 . in the ec model , @xmath72 and @xmath96 and the width of the ghost region @xmath74 unless specified .",
    ", width=264 ]    since the probability of an  extreme \" number of boundary events in one of the processors increases with the number of processors for fixed processor size , the number of iterations increases with @xmath6 . as shown in fig .",
    "[ fig : fig7 ] , such an increase is well described by the logarithmic form , @xmath97 where the exponent @xmath98 ranges from @xmath99 to @xmath100 depending on the model and processor size .",
    "[ fig : fig7 ] also indicates that an increase of the interaction length from @xmath74 to @xmath101 also yields an approximate doubling in the number of iterations when a fixed time interval @xmath71 is used .",
    "thus , in order to keep the number of iterations constant , the time interval must decrease as the range of interaction increases .",
    "relative fluctuation in number of events for the fractal model as a function of @xmath1 with @xmath71 , @xmath76 and @xmath77.,width=264 ]    a second important factor which determines the parallel efficiency is the existence of fluctuations in the number of events in different processors . in particular , since all processors are globally synchronized , the processor having the maximum number of events @xmath69 can determine the execution time of each iteration .",
    "thus the extreme fluctuations @xmath70 , as opposed to the usual r.m.s .",
    "fluctuations , determine the parallel efficiency .     relative fluctuation in number of events for fractal and edge diffusion models as a function of number of processors @xmath6 with @xmath33 , @xmath77 and @xmath71 , where @xmath74 in all cases . in the ec model , @xmath72 and @xmath73 .",
    "dotted lines are linear fits to data.,width=264 ]    figure [ fig : fig8 ] shows the measured fluctuations @xmath70 for the simple fractal model as a function of @xmath1 for fixed processor size @xmath102 , @xmath103 and @xmath80 averaged over many cycles . as expected the relative fluctuations in a smaller system are larger than those in a bigger system . for the simple fractal model , one expects that @xmath104 which implies @xmath105 .",
    "as can be seen , there is very good agreement with this form for the @xmath1-dependence .",
    "figure [ fig : fig9 ] shows the relative ( extreme ) fluctuations as a function of the number of processors @xmath6 for the fractal and ec models with two different processor sizes . as for the dependence of the number of iterations on @xmath6",
    ", we find a logarithmic dependence .",
    "in particular , we find that @xmath106 with @xmath107 regardless of model and processor size . again , for a fixed @xmath6 , a bigger system shows smaller relative fluctuations than a smaller system .",
    "in addition , for the same processor size , the ec model shows smaller relative fluctuations than the fractal model due to the additional number of edge - diffusion events in the model .",
    "power - law decay in the relative fluctuations for the fractal model as a function of multiplication factor @xmath18 and @xmath17 with @xmath76 and @xmath77 .",
    ", width=264 ]    we now consider the dependence of the fluctuations on the time interval @xmath7 .",
    "for the fractal model , the average number of events per cycle in each processor may be written , @xmath108 where @xmath3 is the deposition rate , @xmath2 is the monomer hopping rate , and @xmath109 is the monomer density . the fluctuation in the number of events may be written as the sum of the fluctuation ( proportional to @xmath110 ) assuming all processors have the same average event rate , and an additional term due to fluctuations in the number of monomers in different processors , i.e. @xmath111 .",
    "we note that the fluctuation @xmath112 also depends on the number of processors @xmath6 and the processor size @xmath113 . dividing to obtain the ( extreme ) relative fluctuation we obtain , @xmath114^{-1/2}\\ ] ]",
    "where @xmath115 .",
    "the first term is independent of the cycle length @xmath116 while for @xmath117 , @xmath118 and so the second term is simply proportional to @xmath119 . as can be seen in fig .",
    "[ fig : fig10 ] , we find good agreement with this form for the fractal model with @xmath80 , @xmath5 , @xmath86 and the time interval @xmath7 ranging over more than two decades .",
    "the third factor which determines the parallel efficiency is the communications overhead . for the case of strip decomposition , in every iteration each processor must carry out two local send",
    "/ receive communications with its neighbors .",
    "typically , a send / receive communication with a small message size ( @xmath120 bytes ) between two processors in the same node takes less than 10 @xmath121 but it can take longer if they are in different nodes . for a larger message size",
    "the communication overhead increases linearly with message size . since the processor size in all of our simulations is moderate , the message size is only about 2000 bytes which takes roughly @xmath122 .    a global communication ( global sum or  and \" and broadcast )",
    "must also be carried out at the end of every iteration to check if a new iteration is necessary .",
    "the time for the global sum and broadcast is larger than for a local send / receive and increases logarithmically with the number of processors @xmath6 .",
    "overall , we find that the estimated minimum total communication overhead per cycle is roughly @xmath123 for a small number of processors . in comparison ,",
    "the serial calculation time @xmath124 for one kmc event is about @xmath125 on the itanium cluster .",
    "thus , even for a small number of processors the overhead due to communications ( @xmath126 ) is significant unless @xmath127 .",
    "( a ) parallel efficiency and ( b ) measured additional number of events and estimated communication time per the average number of events as a function of target number of events with @xmath128 and @xmath76 . , width=264 ]    one way to maximize the parallel efficiency is to use the event optimization method . in this method ,",
    "the cycle length @xmath7 is dynamically adjusted during the course of the simulation in order to achieve a fixed target number of events per cycle . by varying the target number of events and measuring the simulation time",
    "one may determine the optimal target number @xmath22 .",
    "figure [ fig : fig11 ] ( a ) shows the measured parallel efficiency for the fractal model with @xmath129 , and @xmath5 as a function of the target number of events . as can be seen , for a target number given by @xmath130 there is an optimal efficiency of approximately 41% . also shown ( dashed line ) is the parallel efficiency calculated using the measured additional number of events @xmath63 due to fluctuations and relaxation iterations along with the estimated communication time which may be approximated by the fit @xmath131 .",
    "the resulting calculated parallel efficiency curve is close to the measured curve but has a slightly lower peak .",
    "[ fig : fig11](b ) shows separately the two contributions to the calculated parallel efficiency @xmath132 and @xmath133 as a function of the target number of events . as can be seen",
    ", the peak of the parallel efficiency is close to the point where these two contributions have the same magnitude .",
    "parallel efficiency for fractal model as function of @xmath1 with @xmath71 ( open symbols ) and event optimization ( @xmath134 ) method ( filled symbols ) .",
    "same symbol shape is used for the same processor size . here",
    "@xmath76 and @xmath77 in all cases . in the @xmath134 method ,",
    "@xmath135 for @xmath136 and all @xmath1 , and @xmath137 for @xmath138 with @xmath139 ( @xmath140).,width=264 ]    we now consider the parallel efficiency of the sr algorithm as a function of @xmath1 for the fractal model for a fixed number of processors @xmath80 . as shown in fig .",
    "[ fig : fig12 ] , when a fixed time interval @xmath71 is used , the parallel efficiency ( open symbols ) shows a distinct peak as a function of @xmath1 , with a maximum parallel efficiency @xmath141 for both processor sizes .",
    "the existence of such a peak may be explained as follows . for small @xmath1",
    "the pe is low due to the large number of events in each processor which leads to a large number of boundary events and relaxation iterations in each cycle . for large @xmath1",
    "the number of events per cycle is reduced but the communications overhead and fluctuations become significant due to the small number of events . at an intermediate value of @xmath1",
    "which increases with increasing processor size , neither of these effects dominate and the parallel efficiency is maximum .    in contrast , when the event optimization method is used ( filled symbols ) the parallel efficiency is significantly higher and is almost independent of @xmath1 for @xmath142 . although the value of the optimum target number of events @xmath22 increases with processor size there is only a weak dependence on @xmath1 for fixed processor size . also shown in fig . [",
    "fig : fig12 ] ( dashed line ) is the estimated ideal parallel efficiency assuming negligible communications overhead . in this case ,",
    "a small target number of events , @xmath143 was found to yield the maximum ideal parallel efficiency over the range of @xmath1 studied here .",
    "parallel efficiency for edge diffusion model as function of @xmath1 with @xmath71 ( open symbols ) and with event optimization method ( filled symbols ) .",
    "same symbol shape is used for the same processor size . here , @xmath76 , @xmath77 , @xmath72 and @xmath73 .",
    "in the @xmath134 method , @xmath144 for @xmath136 ( @xmath145 and @xmath146 ) for all @xmath1 .",
    "dashed line represents ideal parallel efficiency @xmath147 $ ] using @xmath143 with @xmath145 and @xmath146.,width=264 ]    similar results are shown in fig .",
    "[ fig : fig13 ] for the edge diffusion model . since for the edge - diffusion model",
    "the  event density \" is significantly higher than for the fractal model , the communications overhead and fluctuations are significantly reduced . as a result , for the case of a fixed time interval @xmath71 , the maximum parallel efficiency is about @xmath148 for the edge - diffusion model which is significantly higher than the peak value of @xmath149 for the fractal case .",
    "when the event optimization method is used , the pe is also higher than for the fractal case and is again roughly independent of @xmath1 . also shown in fig .",
    "[ fig : fig13 ] for the larger processor size is the calculated ideal parallel efficiency assuming negligible communications overhead and a target number of events given by @xmath150 ( dashed line ) . due to the reduced communications overhead for this model ,",
    "the ideal pe is only slightly higher than the corresponding optimal pe with communications included ( filled squares ) .",
    "parallel efficiency for fractal model with @xmath151 as a function of number of processors with @xmath71 ( open symbols ) and with event optimization method ( @xmath152 ) .",
    "dashed line represents ideal parallel efficiency using a target number of events @xmath143 .",
    "solid line is a fit to the event - optimization data with a form , @xmath153$].,width=264 ]    we first consider the dependence of the parallel efficiency on the number of processors @xmath6 with fixed processor size . as before , the parallel efficiency is defined as the ratio of the execution time for an ordinary serial simulation of one processor s domain to the parallel execution time of @xmath6 domains using @xmath6 processors ( eq .  [ eqn4 ] ) . fig .",
    "[ fig : fig14 ] shows the parallel efficiency for the fractal model with @xmath5 as a function of the number of processors @xmath6 for fixed processor size .",
    "results ( open symbols ) are shown for two different processor sizes for the case of fixed cycle length @xmath154 .",
    "also shown ( filled symbols ) is the parallel efficiency obtained using event optimization for the larger processor size . while the parallel efficiencies obtained using event optimization are significantly higher than the corresponding results obtained using a fixed time interval , the percentage difference decreases slightly as the number of processors increases .    the solid line in fig .",
    "[ fig : fig14 ] shows a fit of the form @xmath155",
    "\\label{fit}\\ ] ] ( see eq .",
    "[ eqn4 ] ) to the parallel efficiency obtained for the larger processor size using event optimization .",
    "as can be seen , there is excellent agreement with the simulation results .",
    "the value of the exponent ( @xmath156 ) is in reasonable agreement with the dependence of the number of additional iterations on @xmath6 shown in fig .",
    "[ fig : fig7 ] . also shown in fig .",
    "[ fig : fig14 ] is the ideal parallel efficiency in the absence of communication overhead calculated using a target number of events given by @xmath150 .",
    "as expected , the ideal pe is significantly larger than the actual pe even for large @xmath6 . in this case",
    "a similar fit of the form of eq .",
    "[ fit ] may be made but with @xmath157 .",
    "[ fig : fig15 ] shows similar results for the edge - diffusion model with @xmath5 , @xmath158 and @xmath159 . for the larger processor size ( @xmath86 ) both the results obtained using a fixed cycle size and those using event optimization are very similar to the corresponding results already obtained for the fractal model .",
    "however , for a fixed cycle length the parallel efficiencies for the smaller processor size ( @xmath160 ) are somewhat higher than the corresponding results for the fractal model .",
    "again , the ideal parallel efficiency is well described by a fit of the form of eq .",
    "[ fit ] with @xmath157 . in general , all the parallel efficiencies shown in figs .",
    "[ fig : fig14 ] and [ fig : fig15 ] are reasonably well described by fits of the form of eq .",
    "[ fit ] with @xmath161 .",
    "parallel efficiency for edge diffusion model with @xmath5 as a function of @xmath6 with @xmath71 ( open symbols ) and with event optimization method ( @xmath162 ) .",
    "dashed line represents ideal parallel efficiency obtained using a target number of events given by @xmath143 . solid line is a fit to the data for @xmath134 with a form , @xmath163 $ ] . , width=264 ]    we now consider the dependence of the parallel efficiency on the number of processors for a fixed total system size @xmath164 in the case of strip geometry ( i.e. , @xmath165 and @xmath166 . using @xmath167 ev , and a step - edge barrier @xmath168 ev at @xmath169 k , we have carried out multilayer simulations of growth using the reversible model up to a coverage of @xmath170 ml . in this case the parallel efficiency may be written as , @xmath171 where @xmath60 is the calculation time for a serial simulation of the @xmath172 system .",
    "we expect that in this case the parallel efficiency will decrease more rapidly with increasing @xmath6 than for the case of fixed processor size , since the decreased processor size leads to increased fluctuations and communications overhead . using the event optimization method ,",
    "the parallel efficiencies obtained were 28% ( @xmath76 ) , 18% ( @xmath173 ) and 9% ( @xmath174 ) , respectively , which corresponds to an approximate @xmath175 dependence for the parallel efficiency .",
    "we have carried out parallel kinetic monte carlo ( kmc ) simulations of three different simple models of thin - film growth using the synchronous relaxation ( sr ) algorithm for parallel discrete - event simulation . in particular , we have studied the dependence of the parallel efficiency on the processor size , number of processors , and cycle length @xmath7 , as well as the ratio @xmath1 of the monomer hopping rate @xmath2 to the ( per site ) deposition rate @xmath3 .",
    "a variety of techniques for optimizing the parallel efficiency were also considered .",
    "as expected since the sr algorithm is rigorous , excellent agreement was found with serial simulations .",
    "our results indicate that while reasonable parallel efficiencies may be obtained for a small number of processors , due to the requirement of global communications and the existence of fluctuations , the sr algorithm does not scale , i.e. the parallel efficiency decreases logarithmically as the number of processors increases . in particular , for the fractal and edge - diffusion models with event optimization",
    ", we have found that the dependence of the parallel efficiency as a function of @xmath6 may be fit to the form @xmath176^{-1}$ ] where @xmath177 .",
    "if the communication time is negligible compared to the calculation time , then the parallel efficiency is higher but a similar fit is obtained with an exponent close to @xmath11 , i.e. @xmath157 .",
    "these results suggest that while the sr algorithm may be reasonably efficient for a moderate number of processors , for a very large number of processors the parallel efficiency may be unacceptably low .",
    "these results are also in qualitative agreement with the analysis presented in ref .",
    "that for parallel ising spin simulations using the sr algorithm with a fixed cycle length , the parallel efficiency should decay as @xmath178 for large @xmath6 .    we have also studied in detail the three main factors which determine the parallel efficiency in the sr algorithm .",
    "the first is the extra calculation overhead due to relaxation iterations which are needed to correct boundary events in neighboring processors .",
    "as expected , the number of relaxation iterations @xmath85 is proportional to the number of boundary events and is also roughly proportional to both the cycle length @xmath7 and the range of interaction . as a result ,",
    "decreasing the cycle length will decrease the overhead due to relaxation iterations .    the second main factor determining the parallel efficiency is the relative ( extreme ) fluctuation @xmath70 in the number of events over all processors in each iteration .",
    "for a fixed number of processors the relative fluctuation decreases as one over the square - root of the number of events per cycle and is thus inversely proportional to the square - root of the product of the processor size and the cycle length . as a result ,",
    "decreasing the cycle length @xmath7 will increase the overhead due to fluctuations .",
    "however , for a fixed processor size and cycle length the relative fluctuation also increases logarithmically with the number of processors .",
    "this increase in the relative fluctuation also leads to an increase in the number of relaxation iterations with increasing @xmath6 as well as decreased processor utilization in each iteration . as a result the parallel efficiency decreases as the number of processors increases .",
    "the third factor determining the parallel efficiency is the overhead due to local and global communications . for the kmc models we have studied",
    "the calculation time per event is smaller than the latency time due to local communications . as a result , in our simulations",
    "the optimal parallel efficiency was obtained by using a cycle length such that @xmath179 where @xmath21 is the average number of events per processor per cycle . in general",
    ", the optimal value of @xmath21 may be determined by balancing the overhead due to relaxation iterations and fluctuations with the overhead due to communications .",
    "since the global communications time increases logarithmically with the number of processors , the communications overhead also leads to a decrease in the parallel efficiency with increasing @xmath6 .    in order to optimize the parallel efficiency , we have considered and applied several techniques . these include ( i ) carrying out several simulations with a different fixed time interval @xmath116 in order to determine the optimum value of @xmath18 , ( ii ) using direct feedback to dynamically control the cycle length during a simulation in order to maximize the ratio of the average number of events per cycle @xmath21 to the measured or estimated execution time , and ( iii ) using feedback to dynamically control the cycle length in order to obtain a pre - determined  target number \" for an auxiliary quantity such as the number of events per cycle or the number of iterations per cycle .",
    "while the first two methods are the most direct , we have found that in most cases , the third method results in the highest parallel efficiency . however , since there is no a priori way of knowing the optimal target number in advance , this optimization method must be accompanied by additional simulations .    for the case of negligible communication time , corresponding to simulations in which the calculation time is much longer than the communication time , the cycle time should be small in order to minimize the number of additional iterations but not too small since a very small cycle time will lead to large relative fluctuations . for a processor size @xmath86",
    ", we found that @xmath180 leads to ideal parallel efficiencies which were significantly larger than obtained using event optimization with the communications time taken into account .",
    "we note that in our simulations we have focused primarily on the case of strip decomposition in order to minimize the communications overhead .",
    "however , if the calculation time is significantly larger than the communications time , then for a square system the parallel efficiency may be somewhat larger if a square decomposition is used instead . to illustrate this we consider the decomposition of an @xmath164 by @xmath164 system into @xmath6 domains .",
    "if the width of the boundary region or range of interaction is given by @xmath181 , then for the case of strip decomposition the area of the boundary region in each processor is given by @xmath182 .",
    "however ignoring corner effects , the area of the boundary region for the square decomposition case is given by @xmath183 .",
    "for @xmath184 , the area of the boundary region is smaller for square decomposition than for strip decomposition .",
    "since the number of iterations is roughly proportional to the area of the boundary region , the calculation overhead due to relaxation iterations will be larger for @xmath184 for the case of strip decomposition . as a result , we expect that for a fixed ( square ) system size and a large number of processors , and for systems ( unlike those studied here ) with a high ratio of ( per event ) calculation time to communications time , square decomposition may be significantly more efficient than strip decomposition .",
    "we also note that in our simulations we have used two slightly different definitions for the parallel efficiency . in the first definition",
    "[ eqn4 ] ) , the parallel execution time was compared with the serial execution time of a system whose size is the same as a single processor .",
    "in contrast , in the second definition ( eq . [ eqn7 ] )",
    "the parallel execution time was directly compared with @xmath185 times the serial execution time of a system whose total system size is the same as in the parallel simulation .",
    "if the serial kmc calculation time per event is independent of system size , then there should be no difference between the two definitions .",
    "since in the models studied here we have used lists for each type of event , we would expect the serial calculation time per event to be independent of system size , and thus the two definitions of parallel efficiency should be equivalent . to test",
    "if this is the case , we have calculated the serial simulation time per event for the fractal model for @xmath186 and @xmath5 for a variety of system sizes ranging from @xmath187 to @xmath188 .",
    "somewhat surprisingly , we found that the serial calculation time per event increases slowly with increasing processor size .",
    "in particular , an increase of approximately 50% in the calculation time per event was obtained when going from a system of size @xmath187 to @xmath188 .",
    "we believe that this is most likely due to memory or ",
    "cache \" effects in our simulations .",
    "this increase in the serial calculation time per event with increasing system size indicates that the calculated parallel efficiencies shown in fig .",
    "[ fig : fig14 ] and fig .",
    "[ fig : fig15 ] would actually be somewhat larger if the more direct definition of parallel efficiency ( eq . [ eqn7 ] ) were used .",
    "we now discuss some possible improvements of the method described here .",
    "as already noted , in our parallel kmc simulations , lists were used in order to maximize the serial efficiency . however , for simplicity each additional iteration after the first iteration was restarted at the beginning of the cycle rather than starting with the first new boundary event in each processor . by using the more efficient method of only redoing events starting with the first new boundary event",
    ", we expect a possible maximum increase in the parallel efficiency of approximately 25% over the results presented here .",
    "in addition , it is also possible that by using improved feedback methods , the parallel efficiency may be somewhat further increased .",
    "for example , by modifying the feedback algorithm it may be possible to further improve the direct optimization method . it may also be possible to combine all three optimization methods to obtain an improved parallel efficiency for a given simulation .",
    "finally , we note that the main reason for the low parallel efficiency for a large number of processors is the global requirement that all processors must be perfectly synchronized .",
    "however , for systems with short - range interactions it should be possible to at least temporarily relax this synchronization requirement for processors which are sufficiently far away from one another .",
    "thus , in large systems with a large number of processors it may be possible to increase the parallel efficiency by slightly modifying the sr algorithm by making it somewhat less restrictive . in this connection",
    ", we have recently developed @xcite a semi - rigorous synchronous sublattice algorithm which yields excellent agreement with serial simulations for all but the smallest processor sizes and in which the asymptotic parallel efficiency is constant with increasing processor number . by combining the synchronous sublattice algorithm with the sr algorithm it may be possible to obtain a hybrid algorithm which contains the best features of both e.g. accuracy and efficiency .",
    "this research was supported by the nsf through grant no .",
    "we would also like to acknowledge grants of computer time from the ohio supercomputer center ( grant no .",
    "pjs0245 ) and the pittsburgh supercomputer center ( grant no . dmr030007jp ) .",
    "g. korniss , m. a. novotny , z. toroczkai , and p. a. rikvold , in _ computer simulated studies in condensed matter physics xiii _",
    ", d. p. landau , s. p. lewis and h .- b .",
    "schuttler eds .",
    "springer proceedings in physics , vol .",
    "86 ( springer - verlag , berlin heidelberg , 2001 ) ."
  ],
  "abstract_text": [
    "<S> we investigate the applicability of the synchronous relaxation ( sr ) algorithm to parallel kinetic monte carlo simulations of simple models of thin - film growth . </S>",
    "<S> a variety of techniques for optimizing the parallel efficiency are also presented . </S>",
    "<S> we find that the parallel efficiency is determined by three main factors @xmath0 the calculation overhead due to relaxation iterations to correct boundary events in neighboring processors , the ( extreme ) fluctuations in the number of events per cycle in each processor , and the overhead due to interprocessor communications . </S>",
    "<S> due to the existence of fluctuations and the requirement of global synchronization , the sr algorithm does not scale , i.e. the parallel efficiency decreases logarithmically as the number of processors increases . </S>",
    "<S> the dependence of the parallel efficiency on simulation parameters such as the processor size , domain decomposition geometry , and the ratio @xmath1 of the monomer hopping rate @xmath2 to the deposition rate @xmath3 is also discussed . </S>"
  ]
}