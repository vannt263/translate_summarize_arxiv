{
  "article_text": [
    "in this article we consider markov processes that are stopped when reaching the boundary of a given set @xmath1 .",
    "these processes appear in a wide range of applications , such as population genetics @xcite , finance @xcite , neuroscience @xcite , physics @xcite and engineering @xcite .",
    "the vast majority of the papers in the literature deal with fully observed stopped processes and assume the parameters of the model are known . in this paper",
    "we address problems when this is not the case .",
    "in particular , bayesian inference for the model parameters is considered , when the stopped process is observed indirectly via data .",
    "we will propose a generic simulation method that can cope with many types of partial observations . to the best of our knowledge",
    ", there is no previous work in this direction .",
    "an exception is @xcite , where maximum likelihood inference for the model parameters is investigated for the fully observed case .    in the fully observed case ,",
    "stopped processes have been studied predominantly in the area of rare event simulation . in order to estimate the probability of rare events related to stopped processes , one needs to efficiently sample realisations of a process that starts in a set @xmath0 and terminates in the given rare target set @xmath1 before returning to @xmath0 or getting trapped in some absorbing set .",
    "this is usually achieved using importance sampling ( is ) or multi - level splitting ; see@xcite and the references in those articles for an overview .",
    "recently , sequential monte carlo ( smc ) methods based on both these techniques have been used in @xcite . in @xcite",
    "the authors also prove under mild conditions that smc can achieve same performance as popular competing methods based on traditional splitting .",
    "sequential monte carlo methods can be described as a collection of techniques used to approximate a sequence of distributions whose densities are known point - wise up to a normalizing constant and are of increasing dimension .",
    "smc methods combine importance sampling and resampling to approximate distributions .",
    "the idea is to introduce a sequence of proposal densities and to sequentially simulate a collection of @xmath4 samples , termed particles , in parallel from these proposals .",
    "the success of smc lies in incorporating a resampling operation to control the variance of the importance weights , whose value would otherwise increase exponentially as the target sequence progresses e.g.  @xcite .    applying smc in the context of",
    "fully observed stopped processes requires using resampling while taking into account how close a sample is to the target set .",
    "that is , it is possible that particles close to @xmath1 are likely to have very small weights , whereas particles closer to the starting set @xmath0 can have very high weights . as a result",
    ", the diversity of particles approximating longer paths before reaching @xmath1 would be depleted by successive resampling steps . in population genetics , for the coalescent model @xcite ,",
    "this has been noted as early as in the discussion of @xcite by the authors of @xcite .",
    "later , in @xcite the authors used ideas from splitting and proposed to perform the resampling step only when each sample of the process reaches intermediate level sets , which define a sequence of nested sets from @xmath0 to @xmath1 .",
    "the same idea appeared in parallel in ( * ? ? ?",
    "* section 12.2 ) , where it was formally interpreted as an interacting particle approximation of appropriate multi - level feynman - kac formulae . naturally , the choice of the intermediate level sets is critical to the performance of such a scheme .",
    "that is , the levels should be set in a `` direction '' towards the set @xmath1 and so that each level can be reached from the previous one with some reasonable probability @xcite .",
    "this is usually achieved heuristically using trial simulation runs .",
    "also more systematic techniques exist : for cases where large deviations can be applied in @xcite the authors use optimal control and in @xcite the level sets are computed adaptively on the fly using the simulated paths of the process .",
    "the contribution of this paper is to address the issue of inferring the parameters of the law of the stopped markov process , when the process itself is a latent process and is only partially observed via some given dataset . in the context of bayesian inference one often needs to sample from the posterior density of the model parameters , which can be very complex .",
    "employing standard markov chain monte carlo ( mcmc ) methods is not feasible , given the difficulty one faces to sample trajectories of the stopped process .",
    "in addition , using smc for sequential parameter inference has been notoriously difficult ; see @xcite . in particular , due to",
    "the successive resampling steps the simulated past of the path of each particle will be very similar to each other .",
    "this has been a long standing bottleneck when static parameters @xmath5 are estimated online using smc methods by augmenting them with the latent state .",
    "these issues have motivated the recently introduced particle markov chain monte carlo ( pmcmc ) @xcite .",
    "essentially , the method constructs a markov chain on an extended state - space in the spirit of @xcite , such that one may apply smc updates for a latent process , i.e.  use smc approximations within mcmc . in the context of parameter inference",
    "for stopped process this brings up the possibility of using the multi - level smc methodology as a proposal in mcmc . this idea to the best of our knowledge has not appeared previously in the literature .",
    "the main contributions made in this article are as follows :    * when the sequence of level sets is fixed _ a priori _ , the validity of using multi - level smc within pmcmc is verified .",
    "* to enhance performance we propose a flexible scheme where the level sets are adapted to the current parameter sample .",
    "the method is shown to produce unbiased samples from the target posterior density .",
    "in addition , we show both theoretically and via numerical examples how the mixing of the pmcmc algorithm is improved when this adaptive strategy is adopted .",
    "this article is structured as follows : in section [ sec : problem - formulation ] we formulate the problem and present the coalescent as a motivating example . in section [ sec : stopping_time ] we present multi - level smc for stopped processes . in section",
    "[ sec : pmcmc ] we detail a pmcmc algorithm which uses multi - level smc approximations within mcmc .",
    "in addition , specific adaptive strategies for the levels are proposed , which are motivated by some theoretical results that link the convergence rate of the pmcmc algorithm to the properties of multi - level smc approximations . in section [ sec : numerical ] some numerical experiments for the the coalescent are given .",
    "the paper is concluded in section [ sec : summary ] .",
    "the proofs of our theoretical results can be found in the appendix .",
    "the following notations will be used . a measurable space",
    "is written as @xmath6 , with the class of probability measures on @xmath7 written @xmath8 . for @xmath9 , @xmath10",
    "the borel sets are @xmath11 . for a probability measure",
    "@xmath12 we will denote the density with respect to an appropriate @xmath13-finite measure @xmath14 as @xmath15 . the total variation distance between two probability measures @xmath16",
    "is written as @xmath17 . for a vector @xmath18",
    ", the compact notation @xmath19 is used ; if @xmath20 @xmath19 is a null vector . for a vector @xmath21",
    ", @xmath22 is the @xmath23norm .",
    "the convention @xmath24 is adopted .",
    "also , @xmath25 is denoted as @xmath26 and @xmath27 is the indicator of a set @xmath1 .",
    "let @xmath7 be a countable ( possibly infinite dimensional ) state - space , then @xmath28 denotes the class of stochastic matrices which possess a stationary distribution .",
    "in addition , we will denote as @xmath29 the @xmath30-dimensional vector whose @xmath31 element is @xmath32 and is @xmath33 everywhere else . finally , for the discrete collection of integers we will use the notation @xmath34 .",
    "let @xmath5 be a parameter vector on @xmath35 , @xmath36 with an associated prior @xmath37 .",
    "the stopped process @xmath38 is a @xmath39valued discrete - time markov process defined on a probability space @xmath40 , where @xmath41 is a probability measure defined for every @xmath42 such that for every @xmath43 , @xmath44 is @xmath45measurable . for simplicity",
    "will we will assume throughout the paper that the markov process is homogeneous .",
    "the state of the process @xmath38 begins its evolution in a non empty set @xmath0 obeying an initial distribution @xmath46 and a markov transition kernel @xmath47 .",
    "the process is killed once it reaches a non - empty target set @xmath48 such that @xmath49 .",
    "the associated stopping time is defined as @xmath50 where it is assumed that @xmath51 and @xmath52 , where @xmath53 is a collection of positive integer values related to possible stopping times .",
    "in this paper we assume that we have no direct access to the state of the process . instead the evolution of the state of the process generates a random observations vector , which we will denote as @xmath54 .",
    "the realisation of this observations vector is denoted as @xmath55 and assume that it takes value in some non empty set @xmath56 .",
    "we will also assume that there is no restriction on @xmath1 depending on the observed data @xmath55 , but to simplify exposition this will be omitted from the notation .    in the context of bayesian inference we are interested in the posterior distribution",
    ": @xmath57 where @xmath58 is the stopping time , @xmath59 is the prior distribution and @xmath60 is the un - normalised complete - data likelihood with the normalising constant of this quantity being : @xmath61 the subscript on @xmath5 will be used throughout to explicitly denote the conditional dependance on the parameter @xmath5 .",
    "given the specific structure of the stopped processes one may write @xmath60 as @xmath62 where @xmath63 is the likelihood of the data given the trajectory of the process . throughout",
    ", it will be assumed that for any @xmath42 , @xmath64 , @xmath60 admits a density @xmath65 with respect to  a @xmath66finite measure @xmath67 on @xmath68 and the posterior and prior distributions @xmath69 admit densities @xmath70",
    "respectively both defined with respect to appropriate @xmath66finite dominating measures .",
    "note that ( [ eq : target ] ) is expressed as an inference problem for @xmath71 and not only @xmath5 .",
    "the overall motivation originates from being able to design a mcmc that can sample from @xmath72 , which requires one to write the target ( or an unbiased estimate of it ) up - to a normalizing constant @xcite .",
    "still , our primary interest lies in bayesian inference for the parameter and this can be recovered by the marginals of @xmath72 with respect to @xmath5 . as it will become clear in section [",
    "sec : pmcmc ] the numerical overhead when augmenting the argument of the posterior is necessary and we believe that the marginals with respect to @xmath73 might also be useful by - products .",
    "the framework presented so far is rather abstract , so we introduce the coalescent model as a motivating example . in figure",
    "[ fig : coalgraph ] we present a particular realisation of the coalescent for two genetic types @xmath74 .",
    "the process starts at epoch @xmath75 when the most recent common ancestor ( mrca ) splits into two versions of itself . in this example @xmath1",
    "is chosen to be the mcra and the process continues to evolve by split and mutation moves . at the stopping point ( here @xmath76 )",
    "we observe some data @xmath55 , which corresponds to the number of genes for each genetic type .",
    "( 1,1 ) ( .5,.75)(0,1)0.25 ( 0.25,.75)(1,0).5 ( 0.75,.75)(0,-1)0.75 ( 0.25,.75)(0,-1)0.25 ( 0.125,.5)(1,0)0.25 ( 0.125,.5)(0,-1)0.5 ( 0.375,.5)(0,-1)0.5 ( -0.1,0.0)(0.1,0)12(1,0)0.02 ( -0.1,0.2)(0.1,0)12(1,0)0.02 ( -0.1,0.5)(0.1,0)12(1,0)0.02 ( -0.1,0.5)(0.1,0)12(1,0)0.02 ( -0.1,0.6)(0.1,0)12(1,0)0.02 ( -0.1,0.75)(0.1,0)12(1,0)0.02 ( 0.56,.90)@xmath1 ( 0.76,0.62)@xmath77 ( -0.05,0.21)@xmath77 ( 0.1,-0.05)@xmath78 ( 0.35,-0.05)@xmath1 ( 0.73,-0.05)@xmath78    ( 1.1,0.0)@xmath79 stop @xmath80 ( 1.1,0.2)@xmath81 mutation ( 1.1,0.5)@xmath82 split ( 1.1,0.6)@xmath83 mutation ( 1.1,0.75)@xmath75 split    in general we will assume there are @xmath30 different genetic types .",
    "the latent state of the process @xmath84 is composed of the number of genes of each type @xmath85 at epoch @xmath86 of the process and let also @xmath87 .",
    "the process begins by default when the first split occurs , so the markov chain @xmath88 is initialised by the density @xmath89 and is propagated using the following transition density : @xmath90 where @xmath91 is defined in section [ sub : notations ] . here",
    "the first transition type corresponds to individuals changing type and is called mutation , e.g. @xmath77 at @xmath92 in figure [ fig : coalgraph ] .",
    "the second transition is called a split event , e.g. @xmath93 in the example of figure [ fig : coalgraph ] . to avoid any confusion",
    "we stress that in figure [ fig : coalgraph ] we present a particular realisation of the process that is composed by a sequence of alternate split and mutations , but this is not the only possible sequence . for example , the bottom of the tree could have be obtained with @xmath78 being the possible mcra and a sequence of two consecutive splits and a mutation .",
    "the process is stopped at epoch @xmath94 when the number of individuals in the population reaches @xmath95 .",
    "so for the state space we define : @xmath96 and for the initial and terminal sets we have : @xmath97 the data is generated by setting @xmath98 , which corresponds to the counts of genes that have been observed . in the example of figure [ fig : coalgraph ]",
    "this corresponds to @xmath99 .",
    "hence for the complete likelihood we have : @xmath100\\label{eq : gamma_y}\\ ] ] as expected , the density is only non - zero if at time @xmath94 @xmath101 matches the data @xmath55 exactly .",
    "our objective is to infer the genetic parameters @xmath102 , where @xmath103 and @xmath104 and hence the parameter space can be written as @xmath105 . to facilitate monte carlo inference",
    ", one can reverse the time parameter and simulate backward from the data .",
    "this is now detailed in the context of importance sampling following the approach in @xcite .      to sample realisations of the process for a given @xmath42 , importance sampling is adopted but with time reversed .",
    "first we introduce a time reversed markov kernel @xmath106 with density @xmath107 .",
    "this is used as an importance sampling proposal where sampling is performed backwards in time and the weighting forward in time .",
    "we initialise using the data and simulate the coalescent tree backward in time until two individuals remain of the same type .",
    "this procedure ensures that the data is hit when the tree is considered forward in time .",
    "the process defined backward in time can be interpreted as a stopped markov process with the definitions of the initial and terminal sets appropriately modified . for convenience",
    "we will consider the reverse event sequence of the previous section , i.e we posed the problem backwards in time with the reverse index being @xmath108 .",
    "the proposal density for the full path starting from the bottom of the tree and stopping at its root can be written as @xmath109 with reference to we have @xmath110 then the marginal likelihood can be obtained @xmath111 in @xcite the authors derive an optimal proposal @xmath112 with respect to the variance of the marginal likelihood estimator .",
    "for the sake of brevity we omit any further details . in the current setup",
    "where there is only mutation and coalescences , the stopped - process can be integrated out @xcite , but this is not typically possible in more complex scenarios . a more complicated problem , including migration , is presented in section [ sec : migration ] .",
    "finally , we remark that the relevance of the marginal likelihood above will become clear later in section [ sec : pmcmc ] as a crucial element in numerical algorithms for inferring @xmath5 .",
    "in this section we shall briefly introduce generic smc without extensive details .",
    "we refer the reader for a more detailed description to @xcite . to ease exposition ,",
    "when presenting generic smc , we shall drop the dependence upon parameter @xmath5 .",
    "smc algorithms are designed to simulate from a sequence of probability distributions @xmath113 defined on state space of increasing dimension , namely @xmath114 .",
    "each distribution in the sequence is assumed to possess densities with respect to a common dominating measure : @xmath115 with each un - normalised density being @xmath116 and the normalizing constant being @xmath117 .",
    "we will assume throughout the article that there are natural choices for @xmath118 and that we can evaluate each @xmath119 point - wise .",
    "in addition , we do not require knowledge of @xmath120      smc algorithms approximate @xmath121 recursively by propagating a collection of properly weighted samples , called particles , using a combination of importance sampling and resampling steps . for the importance sampling part of the algorithm , at each step @xmath122 of the algorithm , we will use general proposal kernels @xmath123 with densities @xmath124 , which possess normalizing constants that do not depend on the simulated paths .",
    "a typical smc algorithm is given in algorithm [ alg : generic - smc - algorithm ] and we obtain the following smc approximations for @xmath125 @xmath126 and for the normalizing constant @xmath117 : @xmath127    initialisation , @xmath128 :    for @xmath129    1 .",
    "sample @xmath130 .",
    "2 .   compute weights @xmath131    for @xmath132 ,    for @xmath129 ,    1 .   resampling : sample index @xmath133 , where @xmath134 .",
    "2 .   sample @xmath135 and set @xmath136 .",
    "3 .   compute weights @xmath137    in this paper we will use @xmath138 to be the multinomial distribution .",
    "then the resampled index of the ancestor of particle @xmath85 at time @xmath122 , namely @xmath139 , is also a random variable with value chosen with probability @xmath140 . for each time @xmath122",
    ", we will denote the complete collection of ancestors obtained from the resampling step as @xmath141 and the randomly simulated values of the state obtained during sampling ( step 2 for @xmath142 ) as @xmath143 .",
    "we will also denote @xmath144 the concatenated vector of all these variables obtained during the simulations from time @xmath145 .",
    "note @xmath146 the is a vector containing all @xmath147 simulated states and should not be confused with the particle sample of the path @xmath148 .",
    "furthermore , the joint density of all the sampled particles and the resampled indices is @xmath149 the complete ancestral genealogy at each time can always traced back by defining an ancestry sequence @xmath150 for every @xmath151 and @xmath142 . in particular , we set the elements of @xmath150 using the backward recursion @xmath152 where @xmath153 . in this context",
    "one can view smc approximations as random probability measures induced by the imputed random genealogy @xmath154 and all the possible simulated state sequences that can be obtained using @xmath155 .",
    "this interpretation of smc approximations was introduced in @xcite and will be later used together with @xmath156 for establishing the complex extended target distribution of pmcmc .      for different classes of problems one can find a variety of enhanced smc algorithms",
    "; see e.g.  @xcite . in the context of stopped processes ,",
    "a multi - level smc implementation was proposed in @xcite and the approach was illustrated for the coalescent model of section [ sec : coal_model ] .",
    "we consider a modified approach along the lines of section 12.2 of @xcite which seems better suited for general stopped processes and can provably yield estimators of much lower variance relative to vanilla smc .",
    "introduce an arbitrary sequence of @xmath157nested sets @xmath158 with the corresponding stopping times denoted as @xmath159 note that the markov property of @xmath160 implies  @xmath161 .",
    "the implementation of multi - level smc differs from the generic algorithm of section [ sec : smc_algo ] in that between successive resampling steps one proceeds by propagating in parallel trajectories of @xmath162 until the set @xmath163 is reached for each @xmath164 . for a given @xmath164",
    "the path @xmath162 is `` frozen '' once @xmath165 , until the remaining particles reach @xmath163 and then a resampling step is performed .",
    "more formally denote for @xmath128 @xmath166 where @xmath167 is a realisation for the stopping time @xmath168 and similarly for @xmath169 we have @xmath170    multi - level smc is a smc algorithm which ultimately targets a sequence of distributions @xmath171 each defined on a space @xmath172 where @xmath173 , @xmath174 and @xmath175 are finite collections of positive integer values related to the stopping times @xmath176 respectively . in the spirit of generic smc",
    "define intermediate target densities @xmath177 w.r.t to an appropriate @xmath13-finite dominating measure @xmath178 .",
    "we will assume there exists a natural sequence of densities @xmath179 obeying the restriction @xmath180 so that the last target density @xmath181 coincides with @xmath182 in ( [ eq : unnormalised_likeli ] ) .",
    "note that we define a sequence of @xmath183 target densities , but this time the dimension of @xmath119 compared to @xmath184 grows with a random increment of @xmath185 .",
    "in addition , @xmath181 should clearly depend on the value of @xmath5 , but this suppressed in the notation .",
    "@xmath186the following proposition is a direct consequence of the markov property :    [ prop : markov_level]assume @xmath51 . then the stochastic sequence defined @xmath187 forms a markov chain taking values in @xmath188 defined . in addition , for any bounded measurable function @xmath189 , then @xmath190",
    ".    the proof can be found in ( * ? ? ?",
    "* proposition 12.2.2 , page 438 ) , ( * ? ? ? * proposition 12.2.4 , page 444 ) and the second part is due to @xmath191 .",
    "we will present multi - level smc based as a particular implementation of the generic smc algorithm .",
    "firstly we replace @xmath192 with @xmath193 respectively .",
    "contrary to the presentation of algorithm [ alg : generic - smc - algorithm ] for multi - level smc we will use a homogeneous markov importance sampling kernel @xmath194 , where @xmath195 , @xmath196 by convention and @xmath112 is the corresponding density w.r.t .",
    "@xmath14 . to compute the importance sampling weights of step 3 for @xmath142 in algorithm [ alg : generic - smc - algorithm ] we use instead :",
    "@xmath197 and for step 2 at @xmath128 : @xmath198 to simplify notation from",
    "herein we write @xmath199 and given @xmath183 , for any @xmath169 we have @xmath200 where again we have suppressed the @xmath5-dependance of @xmath201 in the notation .",
    "we present the multi - level smc algorithm in algorithm [ alg : multilevel - smc - algorithm ] .",
    "note here we include a procedure whereby at each stage @xmath122 , particles that do not reach @xmath163 before time @xmath202 are rejected by assigning them a zero weight , whereas before it was hinted that resampling is performed when all particles reach @xmath163 .",
    "similar to , it is clear that the joint probability density of all the random variables used to implement a multi - level smc algorithm with multinomial resampling is given by : @xmath203 where @xmath204 is defined similarly to @xmath146 . finally , recall by construction @xmath205 so the approximation of the normalizing constant of @xmath60 for a fixed @xmath5 is @xmath206    initialisation , @xmath128 :    for @xmath129    1 .   for @xmath207 :",
    "1 .   sample @xmath208 .",
    "2 .   if @xmath209 set @xmath210 , @xmath211 and go to step 2 .",
    "compute weights @xmath212    for @xmath132 ,    for @xmath129 ,    1 .   resampling : sample index @xmath133 , where @xmath134 .",
    "2 .   for @xmath213",
    ": 1 .   sample @xmath208 .",
    "2 .   if @xmath214 set @xmath215 , @xmath216 and go to step 3 . 3 .   set @xmath217 .",
    "4 .   compute weights @xmath218      we will begin by showing how the levels can be set for the coalescent example of section [ sec : coal_model ]",
    ". we will proceed in the spirit of section [ sec : like_comp ] and consider the backward process so that the `` time '' indexing is set to start from the bottom of the tree towards the root .",
    "we introduce a a collection of integers @xmath219 and define @xmath220 clearly we have @xmath221 , @xmath222 and @xmath223 .",
    "one can also write the sequence of target densities for the multi - level setting as : @xmath224    the major design problem that remains in general is that given _ any _ candidates for @xmath225 , how to set the spacing ( in some sense ) of the @xmath226 and how many levels are needed so that good smc algorithms can be constructed .",
    "that is , if the @xmath226 are far apart , then one can expect that weights will degenerate very quickly and if the @xmath226 are too close that the algorithm will resample too often and hence lead to poor estimates .",
    "for instance , in the context of the coalescent example of section [ sec : coal_model ] , if one uses the above construction for @xmath226 the importance weight at the @xmath122-th resampling time is @xmath227 now , in general for any @xmath228 and @xmath183 it is hard to know beforehand how much better ( or not ) the resulting multi - level algorithm will perform relative to a vanilla smc algorithm .",
    "whilst @xcite show empirically that in most cases one should expect a considerable improvement , there @xmath5 is considered to be fixed . in this case",
    "one could design the levels sensibly using offline heuristics or more advanced systematic methods using optimal control @xcite or adaptive simulation @xcite , e.g. by setting the next level using the median of a pre - specified rank of the particle sample .",
    "what we aim to establish in the next section is that when @xmath5 is varies as in the context of mcmc algorithms , one can both construct pmcmc algorithms based on multi - level smc and more importantly easily design for each @xmath5 different sequences for @xmath226 based on similar ideas .",
    "particle markov chain monte carlo ( pmcmc ) methods are mcmc algorithms , which use all the random variables generated by smc approximations as proposals . as in standard",
    "mcmc the idea is to run an ergodic markov chain to obtain samples from the distribution of interest .",
    "the difference lies that in order use the simulated variables from smc , one defines a complex invariant distribution for the mcmc on an extended state space .",
    "this extended target is such that a marginal of this invariant distribution is the one of interest .",
    "this section aims on providing insight to the following questions :    1 .",
    "is it valid in general to use multi - level smc within pmcmc ? 2 .",
    "given that it is , how can we use the levels to improve the mixing of pmcmc ?",
    "the answer to the first question seems rather obvious , so we will provide some standard but rather strong conditions for which multi - level pmcmc is valid . for the second question we will propose an extension to pmmh that adapts the level sets used to @xmath5 at every iteration of pmcmc .",
    "@xcite introduces three different and generic pmcmc algorithms : particle independent metropolis hastings algorithm ( pimh ) , particle marginal metropolis hastings ( pmmh ) and particle gibbs samplers . in the remainder of the paper we will only focus on the first two of these .      1 .",
    "sample @xmath229 from using the multi - level implementation of algorithm [ alg : generic - smc - algorithm ] detailed in section and compute @xmath230 .",
    "sample @xmath231 .",
    "2 .   set @xmath232 and @xmath233 3 .   for @xmath234",
    ": 1 .   propose a new @xmath235 and @xmath236 as in step 1 and compute @xmath237 2 .",
    "accept this as the new state of the chain with probability @xmath238 if we accept , set @xmath239 and @xmath240 .",
    "otherwise reject , @xmath241 and @xmath242 .",
    "we will begin by presenting the simplest generic algorithm found in @xcite , namely the particle independent metropolis hastings algorithm ( pimh ) . in this case @xmath5 and @xmath183 are fixed and pimh is designed to sample from the pre - specified target distribution @xmath243 also considered in section [ sec : smc_algo ] . although pimh is not useful for parameter inference it is included for pedagogic purposes .",
    "one must bear in mind that pimh is the most basic of all pmcmc algorithms .",
    "as such it is easier to analyse but still can provide useful intuition that can be used later in the context of pmmh and varying @xmath5 .",
    "pimh is presented in algorithm [ alg : pimh ] .",
    "it can be shown , using similar arguments to @xcite , that the invariant density of the markov kernel above is exactly ( see the proof of proposition [ prop : stop_within_mcmc ] ) @xmath244 where @xmath245 is as in and as before we have @xmath246 and @xmath247 for every @xmath248 .",
    "note that @xmath249 admits the target density of interest , @xmath250 as the marginal , when @xmath251 and @xmath252 are integrated out .",
    "we commence by briefly investigating some convergence properties of pimh with multi - level smc . even though the scope of pimh is not parameter inference",
    ", one can use insight on what properties are desired by multi - level smc for pimh when designing other pmcmc algorithms used for parameter inference .",
    "we begin with posing the following mixing and regularity assumption :    [ assump : a1 ] for every @xmath42 and @xmath253 there exist a @xmath254 such that for every @xmath255 : @xmath256 there exist a @xmath257 such that for @xmath222 and every @xmath258 : @xmath259 the stopping times are finite , that is for @xmath222 there exist a @xmath260 such that @xmath261    assumption ( a[assump : a1 ] ) is rather strong , but are often used in the analysis of these kind of algorithms @xcite because they simplify the proofs to a large extent .",
    "recall that @xmath42 and @xmath253 are fixed .",
    "we proceed by stating the following proposition :    [ prop : conv_rate ] assume ( a[assump : a1 ] ) . then for @xmath262 algorithm [ alg : pimh ] generates a sequence @xmath263 that for any @xmath264 , @xmath265 , @xmath42 satisfies : @xmath266    the proof can be found in the appendix .",
    "the following remarks are generalised and do not always hold , but provide some intuition for the ideas that follow .",
    "the result shows intrinsically that as the supremum of the sum of the stopping times with respect to @xmath267 gets smaller , so does the convergence rate increase .",
    "this can be also linked to the variance of the estimator of @xmath230 , which is well known to increase linearly with @xmath183 ( * ? ? ?",
    "* theorem 12.2.2 , pages 451 - 453 ) .",
    "shorter stopping times will typically yield lower variance and hence better mcmc convergence properties . on the other hand",
    "often @xmath268 will be larger for longer @xmath183 and longer stopping times ( proposition [ prop : conv_rate ] is derived for a fixed @xmath183 ) .",
    "in addition , sampling a stopped process is easier using a higher number of levels . in summary ,",
    "the tradeoff is that although it is more convenient to use more auxiliary variables for simulating the process , these will slow down the mixing of pmcmc . in practice one balances this by trying to use a moderate number of levels for which most the particles to reach @xmath1 .",
    "this tradeoff serves as a motivation for developing flexible schemes to vary @xmath269 with @xmath5 in the pmcmc algorithm presented later in section [ sec : adaptive_strat ] .      in the remainder of this section",
    "we will focus on using a multi - level smc implementation within a pmmh algorithm .",
    "given the commentary in section [ sec : stopping_time_smc ] and our interest in drawing inference on @xmath42 , it seems that using multi - level smc within pmcmc should be highly beneficial .",
    "recall can be expressed in terms of densities as :    @xmath270    and let the marginal density given by @xmath271 for the time being we will consider the case when @xmath183 is fixed . in the context of our stopped markov process , we propose a pmmh algorithm targeting @xmath272 in algorithm [ fig : stop_within_mcmc ] .",
    "1 .   sample @xmath273 .",
    "given @xmath274 sample @xmath275 using multi - level smc and compute @xmath276 .",
    "sample @xmath231 .",
    "2 .   set @xmath277 and @xmath278 .",
    "3 .   for @xmath234 : 1 .   sample @xmath279 ; given @xmath280 propose a new @xmath281 and @xmath236 as in step 1 and compute @xmath282 .",
    "2 .   accept this as the new state of the chain with probability @xmath283 if we accept , set @xmath284 and @xmath285 .",
    "otherwise reject , @xmath241 and @xmath286 .",
    "we will establish the invariant density and convergence of this algorithm , under the following assumption :    [ assump : a5 - 6 ] for any @xmath42 and @xmath253 we define the following sets for @xmath145 : @xmath287 and @xmath288 . for any @xmath42 we have that @xmath289 .",
    "in addition the ideal metropolis hastings targeting @xmath290 using proposal density @xmath291 is irreducible and aperiodic .",
    "this assumption contains assumptions 5 and 6 of @xcite modified to our problem with a simple change of notations .",
    "we proceed with the following result :    [ prop : stop_within_mcmc ] assume ( a[assump : a5 - 6 ] ) ; then for any @xmath262 :    1 .",
    "the invariant density of the procedure described in algorithm [ fig : stop_within_mcmc ] , is on the space @xmath292 and has the representation @xmath293 where @xmath294 is as in and @xmath295 as in .",
    "in addition , admits @xmath290 as a marginal .",
    "algorithm [ fig : stop_within_mcmc ] generates a sequence @xmath296 such that @xmath297 where @xmath72 is as in .",
    "the proof of the result is in the appendix .",
    "the result is based on theorem 4 of @xcite .",
    "note that algorithm [ fig : stop_within_mcmc ] presented in a generic form of a `` vanilla '' pmmh algorithm , so it can be enhanced using various strategies .",
    "for example , it is possible to add block updating of the latent variables or backward simulation in the context of a particle gibbs version @xcite . in the next section ,",
    "we propose a flexible scheme that allows to set a different number of levels after a new @xmath298 @xmath186is proposed .",
    "the remaining design issue for pmmh is how to tune multi - level smc by choosing @xmath183 and @xmath267 .",
    "whilst , for a fixed @xmath42 , one could solve the problem with preliminary runs , when @xmath5 varies this is not an option .",
    "in general the value of @xmath5 should dictate how small or large @xmath183 should be to facilitate an efficient smc algorithm .",
    "hence , to obtain a more accurate estimate of the marginal likelihood and thus an efficient mcmc algorithm , we need to consider adaptive strategies to propose randomly a different number of levels @xmath183 and levels sequence @xmath267 for each @xmath299 sampled at every pmmh iteration @xmath85 . to ease exposition",
    "we will assume that @xmath269 can be expressed as functions of an arbitrary auxiliary parameter @xmath300 .",
    "given @xmath299 is a random variable , the main questions we wish to address is how to perform such an adaptive strategy consistently .",
    "an important point , is the fact that since we are interested in parameter inference , it is required that the marginal of the pmmh invariant density is @xmath290",
    ". this can be ensured ( see proposition [ prop : adap_stop ] ) by introducing at each pmmh iteration , the parameters that form the level sets @xmath301 as an auxiliary process , which given @xmath299 is conditionally independent of @xmath302 . this way we define an extended target for the mcmc algorithm , which includes @xmath183 and @xmath267 in the target variables .",
    "it should be noted that this scheme is explicitly different from proposition 1 of @xcite , where the mcmc transition kernel at iteration @xmath85 is dependent upon an auxiliary process .",
    "here one just augments the target space with more auxiliary variables .",
    "1 .   sample @xmath273 . given @xmath274 : sample @xmath303 from @xmath304 , then @xmath305 using multi - level smc and compute @xmath276 . sample @xmath231 .",
    "2 .   set @xmath306 and @xmath278 .",
    "3 .   for @xmath234 : 1 .",
    "sample @xmath279 ; sample @xmath307 from @xmath308 and @xmath309 @xmath236 as in step 1 and compute @xmath282 .",
    "2 .   accept this as the new state of the chain with probability @xmath283 if we accept , set @xmath310 and @xmath285 .",
    "otherwise reject , @xmath241 and @xmath286 .",
    "consider now that it is possible at every pmmh iteration @xmath85 to simulate the auxiliary process @xmath300 defined upon an abstract state - space @xmath311 .",
    "let this with associated random variable @xmath300 , be distributed according to @xmath312 , which is assumed to possess a density with respect to a.  @xmath66finite measure @xmath313 written as @xmath314 .",
    "as hinted by the notation @xmath312 should depend on @xmath5 and @xmath300 is meant be used to determine the sequence of levels @xmath267 for each @xmath299 in pmmh .",
    "this auxiliary variable will induce for every @xmath42 :    * a random number of level sets @xmath315 . *",
    "a sequence of level sets @xmath316 with @xmath317 .",
    "we will assume that for any @xmath42 , proposition [ prop : markov_level ] and will hold @xmath318almost everywhere , where this time @xmath183 should be replaced by @xmath319 .",
    "this implies that for every @xmath42 we have : @xmath320 where the expression holds @xmath318 almost everywhere . in algorithm",
    "[ fig : stop_within_mcmc1 ] we propose a pmmh algorithm , which at each step @xmath85 uses @xmath299 to adapt the levels @xmath321 . for algorithm",
    "[ fig : stop_within_mcmc1 ] we present the following proposition that verifies varying the level sets in this way is theoretically valid :    [ prop : adap_stop ] assume ( a[assump : a5 - 6 ] ) and hold .",
    "then , for any @xmath262 :    1 .",
    "the invariant density of the procedure in algorithm [ fig : stop_within_mcmc1 ] is defined on the space @xmath322 and has the representation @xmath323 where @xmath294 is as in and @xmath295 is as in .",
    "in addition , admits @xmath290 as a marginal .",
    "the generated sequence @xmath324 satisfies @xmath325 where @xmath72 is as in .",
    "the proof is contained in the appendix .",
    "we are essentially using an auxiliary framework similar to @xcite . as in",
    "we included @xmath73 in the target posterior , when we were primarily interested in @xmath5 , this time we augment the target posterior with @xmath300 and the smc variables @xmath326 , which is a consequence of using pmcmc .",
    "the disadvantage is that as the space of the posterior increases it is expected that the mixing of the algorithm will be slower .",
    "this could be improved if we have opted @xmath73 and @xmath300 to be dependent on each other given @xmath5 , but this would need additional assumptions for the structure of @xmath60 .",
    "in addition , in many applications the parameters @xmath300 that determine @xmath267 appear naturally and @xmath300 often is low dimensional .",
    "also , in most applications it might seem easier to find intuition on how to construct and tune @xmath312 than computing the level sets directly from @xmath5 .",
    "for example , for the coalescent model of section [ sec : coal_model ] with the mutation matrix @xmath327 is fixed , one can envisage for a larger value of @xmath328 coalescent events are less likely and more level sets closer together are needed compared to smaller values of @xmath328 .",
    "we will illustrate the performance of pmmh using numerical examples on two models from population genetics . the first one deals with the coalescent model of section [ sec : coal_model ] when a low dimensional dataset is observed .",
    "this is meant as an academic / toy example suitable for comparing different pmmh implementations .",
    "the second example is a more realistic application and deals with a coalescent model that allows migration of individual genetic types from one sub - group to another @xcite . in both cases",
    "we will illustrate the performance of pmmh implemented with a simple intuitive strategy for adapting the level sets .",
    "we will use a known stochastic matrix @xmath327 with all entries equal to @xmath329 . in this example",
    "@xmath330 with and the dataset is @xmath331 .",
    "the parameter - space is set as @xmath332 $ ] and a uniform prior will be used .",
    "for @xmath106 we will use the optimal proposal distributions provided by @xcite .",
    "the pmmh proposal @xmath333 in algorithm [ fig : stop_within_mcmc1 ] is a log normal random walk , i.e. we use @xmath334 with @xmath335 .",
    "we will compare pmmh when implemented with a simple adaptive scheme for @xmath183 and when @xmath183 is fixed . in the latter case we set @xmath336 .",
    "when an adaptive strategy is employed we will sample each time @xmath183 directly using a multinomial distribution defined on @xmath337 with weights proportional to @xmath338 . in both cases given",
    "@xmath183 we place the levels almost equally spaced apart .",
    "the adaptive and normal versions were run with @xmath339 for @xmath340 iterations . in each case",
    "the algorithm took approximately @xmath341 , @xmath342 , @xmath343 hours to complete when implemented in matlab and run on a linux workstation using a intel core 2 quad q9550 cpu at 2.83 ghz .",
    "the results are shown in figure [ fig : plots_noadap ] and [ fig : plots_adap ] .",
    "we observed that when we varied the number of levels , this allowed the sampler to traverse through a bigger part of the state space compared to when a fixed number of levels is used . as a result",
    "the estimated pdf of the adaptive case manages to include a second mode that is not seen in the non adaptive case . in the fixed levels case",
    "we see a clear improvement with increasing @xmath344 , although the difference in the mixing between @xmath345 and @xmath346 is marginal . in the adaptive case",
    "the sampler performed well even with lower values of @xmath344 .",
    "level sets is used .",
    "left : estimated pdf of @xmath328 for @xmath339 .",
    "centre : the trace plot for @xmath345 .",
    "right : autocorrelation plots for @xmath339 .",
    "the average acceptance ratio was @xmath347 , @xmath348 and @xmath349 respectively.,title=\"fig:\",scaledwidth=25.0% ] level sets is used .",
    "left : estimated pdf of @xmath328 for @xmath339 .",
    "centre : the trace plot for @xmath345 .",
    "right : autocorrelation plots for @xmath339 .",
    "the average acceptance ratio was @xmath347 , @xmath348 and @xmath349 respectively.,title=\"fig:\",scaledwidth=25.0% ] level sets is used .",
    "left : estimated pdf of @xmath328 for @xmath339 .",
    "centre : the trace plot for @xmath345 .",
    "right : autocorrelation plots for @xmath339 .",
    "the average acceptance ratio was @xmath347 , @xmath348 and @xmath349 respectively.,title=\"fig:\",scaledwidth=25.0% ]    . far left : estimated pdf of @xmath328 for @xmath339 .",
    "central left : the trace plot for @xmath345 .",
    "central right : histogram of number of levels in the posterior for @xmath345 .",
    "far right : autocorrelation function plots for @xmath339 .",
    "the average acceptance ratio was @xmath349 , @xmath350 and @xmath351 respectively.,title=\"fig:\",scaledwidth=25.0% ] . far left : estimated pdf of @xmath328 for @xmath339 .",
    "central left : the trace plot for @xmath345 .",
    "central right : histogram of number of levels in the posterior for @xmath345 .",
    "far right : autocorrelation function plots for @xmath339 .",
    "the average acceptance ratio was @xmath349 , @xmath350 and @xmath351 respectively.,title=\"fig:\",scaledwidth=25.0% ] . far left : estimated pdf of @xmath328 for @xmath339 .",
    "central left : the trace plot for @xmath345 .",
    "central right : histogram of number of levels in the posterior for @xmath345 .",
    "far right : autocorrelation function plots for @xmath339 .",
    "the average acceptance ratio was @xmath349 , @xmath350 and @xmath351 respectively.,title=\"fig:\",scaledwidth=25.0% ] . far left : estimated pdf of @xmath328 for @xmath339 .",
    "central left : the trace plot for @xmath345 .",
    "central right : histogram of number of levels in the posterior for @xmath345 .",
    "far right : autocorrelation function plots for @xmath339 .",
    "the average acceptance ratio was @xmath349 , @xmath350 and @xmath351 respectively.,title=\"fig:\",scaledwidth=25.0% ]      the model is similar to the one as described in section [ sec : coal_model ] .",
    "the major difference is that this time genetic types are of classified into sub - groups within which most activity happens .",
    "in addition , individuals are allowed to migrate from one group to another .",
    "we commence with a brief description of the model and refer the interested reader to @xcite for more details . as in section",
    "[ sec : coal_model ] we will consider the process forward in time .",
    "let @xmath352 be the number of groups and the state at time @xmath86 be composed as the concatenation of @xmath352 groups of different genetic types as : @xmath353 the process under - goes split , mutation and migration transitions as follows : @xmath354 where @xmath355 with @xmath356 and @xmath357 is a vector with a zero in every element except the @xmath358 -th one . similarly to the simpler model of section [ sec : coal_model ]",
    "the transition probabilities are parameterised by the mutation parameter @xmath328 , mutation matrix @xmath327 and a migration matrix @xmath359 .",
    "the latter is a symmetric matrix with zero values on the diagonal and positive values on the off - diagonals .",
    "finally the data is generated when at time @xmath94 the number of individuals in the population reaches @xmath95 , and @xmath360 .",
    "as for the model described in section [ sec : coal_model ] one can reverse time and employ an backward sampling forward weighting importance sampling method ; see @xcite for the particular implementation details .",
    "in our example we generated data with @xmath361 , @xmath362 and @xmath363 .",
    "this is quite a challenging set - up . as in the previous example we set the mutation matrix @xmath327 to be known and uniform and we will concentrate on inferring the @xmath364 .",
    "independent gamma priors with shape and scale parameters equal to @xmath32 were adopted for each of the parameters .",
    "we implemented pmmh using @xmath339 and a simple adaptive scheme for @xmath183 .",
    "we allow @xmath365 and use approximately equal spacing between the levels .",
    "we choose each @xmath183 with probability proportional to @xmath366 the proposals for the parameters were gaussian random - walks on the log - scale .",
    "the algorithm was implemented in c / c++ was run for @xmath340 iterations , which took approximately 3 , 6 and 12 hours to complete . whilst the run - time is quite long it can be improved by at least one order of magnitude if the smc is implemented on graphical processing units ( gpu ) as in @xcite .    for the dataset plotted in figure [ fig : plots2 ] ( left )",
    "the results are plotted in figures [ fig : plots2 ] ( right ) and [ fig : plots1 ] .",
    "the auto - correlation and trace plots indicate that the sampler mixes reasonably well for every @xmath344 .",
    "these results in this example are encouraging as to the best of our knowledge bayesian inference has not been attempted for this class of problems .",
    "we expect that practitioners with insight in the field of population genetics can come with more sophisticated mcmc proposals or adaptive schemes for the level sets , so that the methodology can be extended to realistic applications .     in the resulting posterior for @xmath345.,title=\"fig:\",scaledwidth=25.0% ] in the resulting posterior for @xmath345.,title=\"fig:\",scaledwidth=25.0% ]    .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] + .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] + .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ] .",
    "top row : estimated pdfs for @xmath328 , @xmath367 , @xmath368 , @xmath369 ( from left to right ) .",
    "middle : trace plots for @xmath345 .",
    "bottom : autocorrelation function plots .",
    "the acceptance rate was @xmath370 respectively.,title=\"fig:\",scaledwidth=25.0% ]",
    "in this article we have presented a multi - level pmcmc algorithm which allows one to perform bayesian inference for the parameters of a latent stopped processes . in terms of methodology the main novelty of the approach",
    "is that uses auxiliary variables to adaptively compute the level sets with @xmath5 .",
    "the general structure of this auxiliary variable allows it to incorporate the use of independent smc runs with less particles to set the levels . in the numerical examples we demonstrated that the addition auxiliary variables slow down the convergence of pmcmc , but this seemed a reasonable compromise in terms of performance compared when",
    "fixed number of level sets were used .",
    "the proposed algorithm requires considerable amount of computation , but to the authors best knowledge for such problems there seems to be a lack of alternative approaches .",
    "also , recent developments gpu hardware can be adopted to speed up the computations even by orders of magnitude as in @xcite .",
    "there are several extensions to the work here , which may be considered .",
    "firstly , the scheme that is used to adapt the level sets relies mainly on intuition .",
    "we found simple adaptive implementations to work well in practice . in the rare events",
    "literature one may find more systematic techniques to design the level sets , based upon optimal control @xcite or simulation @xcite .",
    "although these methods are not examined here , they can be characterised using alternative auxiliary variables similar to the ones in proposition [ prop : adap_stop ] , so the auxiliary variable framework we use is quite generic .",
    "in addition , we emphasise that within a pmcmc framework one may also include multi - level splitting algorithms instead of smc , which might appeal practitioners familiar with multi - level splitting .",
    "secondly , one could seek to use these ideas within a smc sampler framework of @xcite as done in @xcite .",
    "as noted in the latter article , a sequential formulation can improve the sampling scheme , sometimes at a computational complexity that is the same as the original pmcmc algorithm .",
    "in addition , this article focuses on the pmmh algorithm , so clearly extensions using particle gibbs and block updates might prove valuable for many applications .    finally , from a modelling perspective",
    ", it may be of interest to apply our methodology in the context of hidden markov models . in this context",
    ", one has @xmath371 with @xmath372 being the conditional likelihood of the observations",
    ". it would be important to understand , given a range of real applications , the feasibility of statistical inference , combined with the development of our methodology .",
    "an investigation of the effectiveness of such a scheme when applied to queuing networks is currently underway .",
    "we thank arnaud doucet and maria de iorio for many conversations on this work .",
    "the first author was supported by a ministry of education grant .",
    "the second author was kindly supported by the epsrc programme grant on control for energy and sustainability ep / g066477/1 .",
    "much of this work was completed when the first author was employed by the department of mathematics , imperial college london .",
    "[ proof of proposition [ prop : conv_rate ] ] the result is a straight forward application of theorem 6 of @xcite which adapted to our notation states :    @xmath373\\wedge\\mathbb{e}_{\\psi_{\\theta}}\\left[1\\wedge\\frac{\\hat{z}_{p}(\\xi)}{\\hat{z}_{p}(\\xi)}\\bigg|\\xi\\right]\\right)\\bigg)^{i}\\bigg],\\ ] ] where the conditional expectation is the expectation w.r.t .",
    "the smc algorithm ( i.e.  @xmath374 ) and the outer expectation is w.r.t .",
    "the pimh target ( i.e.  @xmath375 ) .",
    "we also denote the estimate of the normalizing constant as @xmath376 with @xmath377 denoting which random variables generate the estimate .",
    "now , clearly via ( a[assump : a1 ] ) @xmath378^{\\tau_{n}+\\tau_{n-1}}\\ ] ] with the convention that @xmath379 .",
    "thus , it follows that @xmath380^{\\bar{\\tau}_{n}+\\bar{\\tau}_{n-1}}\\leq\\bigg[\\frac{1}{\\rho\\varphi}\\bigg]^{2\\sum_{n=1}^{p}\\bar{\\tau}_{n}}\\ ] ] and we obtain : @xmath381 note that by assumption @xmath382 and thus we have @xmath383\\bigg)^{i}\\ ] ] given ( * ? ? ?",
    "* theorem 7.4.2 , equation ( 7.17 ) , page 239 ) and the fact that @xmath60 is defined to be strictly positive in ( a[assump : a1 ] ) we have that the smc approximation @xmath376 is an unbiased estimate of the normalizing constant @xmath384 @xmath385=z_{p},\\label{eq : smc_unbiased}\\ ] ] and we can easily conclude .",
    "[ proof of proposition [ prop : stop_within_mcmc ] ] the proof of parts 1 . and 2 .",
    "follows the line of arguments used in theorem 4 of @xcite , which we will adapt to our set - up .",
    "the main difference lies in the multi - level construction and second statement regarding the marginal of @xmath386 .",
    "for the validity of the multi - level set - up we will rely on proposition [ prop : markov_level ] .",
    "suppose we design a metropolis hastings kernel with invariant density @xmath386 and use a proposal @xmath387 .",
    "then @xmath388 where we denote the normalising constant of the posterior in as : @xmath389 therefore the metropolis - hastings procedure to sample from @xmath390 will be as in algorithm [ fig : stop_within_mcmc ] .    alternatively using similar arguments one we may write @xmath391 summing over @xmath251 and using the unbiased property of the smc algorithm in equation it follows that @xmath392 admits @xmath393 as a marginal , so the proof of part 1 .",
    "is complete .      [",
    "proof of proposition [ prop : adap_stop ] ] the proof is the similar as that of proposition [ prop : stop_within_mcmc ] . for the proof of the first statement of part 1 .",
    "one repeats the same arguments as for proposition [ prop : stop_within_mcmc ] with difference being in the inclusion of @xmath394 for @xmath395 and @xmath396 . for the second statement , to get the marginal of @xmath386 , re - write the target as : @xmath397 let @xmath398 denote the marginal of @xmath392 obtained in proposition [ prop : stop_within_mcmc ] .",
    "using and the conditional independence of @xmath300 and @xmath326 , then for the marginal of @xmath399 w.r.t @xmath300 , @xmath326 , @xmath251 we have that @xmath400 where the summing over  @xmath251 and integrating w.r.t .",
    "@xmath326 is as in proposition [ prop : stop_within_mcmc ] .    for part 2 .",
    "note that the conditional density given @xmath251 and @xmath300 and @xmath5 of @xmath401 is @xmath402 hence the sequence @xmath403 satisfies the required property as direct consequence theorem 1 in @xcite and assumption ( a[assump : a5 - 6 ] ) .",
    "lee , a. , yau , c. , giles , m. , doucet , a. & holmes c.c .",
    "( 2010 ) on the utility of graphics cards to perform massively parallel implementation of advanced monte carlo methods , _ j. comp .",
    ". statist .",
    "_ , * 19 * , 769789 ."
  ],
  "abstract_text": [
    "<S> in this article we consider bayesian parameter inference associated to partially - observed stochastic processes that start from a set @xmath0 and are stopped or killed at the first hitting time of a known set @xmath1 . </S>",
    "<S> such processes occur naturally within the context of a wide variety of applications . </S>",
    "<S> the associated posterior distributions are highly complex and posterior parameter inference requires the use of advanced markov chain monte carlo ( mcmc ) techniques . </S>",
    "<S> our approach uses a recently introduced simulation methodology , particle markov chain monte carlo ( pmcmc ) @xcite , where sequential monte carlo ( smc ) @xcite approximations are embedded within mcmc . </S>",
    "<S> however , when the parameter of interest is fixed , standard smc algorithms are not always appropriate for many stopped processes . in @xcite the authors introduce smc approximations of multi - level feynman - kac formulae , which can lead to more efficient algorithms . </S>",
    "<S> this is achieved by devising a sequence of nested sets from @xmath0 to @xmath1 and then perform the resampling step only when the samples of the process reach intermediate level sets in the sequence . </S>",
    "<S> naturally , the choice of the intermediate level sets is critical to the performance of such a scheme . in this paper </S>",
    "<S> , we demonstrate that multi - level smc algorithms can be used as a proposal in pmcmc . </S>",
    "<S> in addition , we propose a flexible strategy that adapts the level sets for different parameter proposals . </S>",
    "<S> our methodology is illustrated on the coalescent model with migration . </S>",
    "<S> + * key - words * : stopped processes , sequential monte carlo , markov chain monte carlo    @xmath2department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg ` + @xmath3department of electrical engineering , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`n.kantas@imperial.ac.uk ` </S>"
  ]
}