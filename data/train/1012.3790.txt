{
  "article_text": [
    "prediction by partial matching ( ppm ) @xcite has set a benchmark for text compression algorithms due to its high compression efficiency . in ppm ,",
    "texts are modeled as markov processes in which the occurrence of a character only depends on its context , i.e. , @xmath0 preceding characters , where @xmath0 is called the context order and is a parameter of ppm . in each context , probabilities of next character is maintained .",
    "when a new character comes , its probability is estimated using context models and is encoded by an arithmetic coder . during the encoding / decoding process , the context models ( probability tables ) are updated after a character is encoded / decoded . by using such adaptive context models , ppm is able to predict text as well as human do @xcite , and achieves higher compression ratio than other compression algorithms @xcite .",
    "however , one limitation of ppm is that the prediction is character - based , i.e. , characters are encoded one by one sequentially , which is not quite efficient . in @xcite ,",
    "word - based ppm is proposed in which every word is predicted by its preceding words , but its performance is even worse than character - based ppm @xcite , mainly because the alphabet size of english words is so large that very long texts are required to collect sufficient statistical information for word - based models .",
    "similarly , horspool @xcite introduced an algorithm that alternatively switches between word - based and character - based ppm , but it needs to explicitly encode the length of characters when character - based ppm is used , resulting in unnecessary overhead . recently , skibiski @xcite extended the alphabet of ppm to long repeated strings by pre - processing the whole texts before encoding , which showed performance improvement over traditional ppm .    in this paper",
    ", we propose an enhanced algorithm that combines traditional character - based context models and dictionary models .",
    "the basic idea is that , for most english words , given the first a few characters ( prefix ) the rest of the word ( suffix ) can be well predicted .",
    "specifically , in addition to context models for character prediction used in traditional ppm @xcite we introduce dictionary models that contain words with a common prefix . by doing so ,",
    "a word can be predicted and encoded given its prefix , i.e. , the first a few characters .",
    "therefore , different from traditional ppm @xcite and its variations @xcite , proposed algorithm can achieve variable - length prediction by partial matching ( vlppm ) .",
    "the remainder of the paper is organized as follows .",
    "section [ sec_vlp ] describes the dictionary model used for variable - length prediction and the framework of proposed scheme .",
    "section [ sec_details ] details the encoding and decoding algorithms .",
    "test results and conclusions are presented in section [ sec_results ] and [ sec_con ] , respectively .",
    "first of all , a simple example is given to show how variable - length prediction can be achieved by a dictionary model .",
    "suppose we are encoding a sequence of texts like this : `` ... information ... '' .",
    "the first 3 characters `` inf '' have been encoded and characters to be encoded next are `` ormation ... '' .",
    "if we use order-@xmath1 ppm , characters are encoded one after another given its context , i.e. 3 preceding characters , as shown in fig .",
    "[ fig_compare ] ( a ) . on the other hand ,",
    "if we divide every word into two parts : a fixed - length prefix and a variable - length suffix , and suppose we have a dictionary that contains words with prefix `` inf '' as shown in fig .",
    "[ fig_compare ] ( b ) . instead of predicting characters one by one",
    ", we can predict suffixes in this dictionary at one time , provided that we know the prefix is `` inf '' .",
    "since the dictionary contains suffixes with different length , the prediction is variable - length",
    ".    next , let s take a look at the advantage of using dictionary model .",
    "we still focus on the above example , and estimate how many bits are required to encode `` ormation '' . for traditional order-@xmath1 ppm ,",
    "assume 1.5 bits are needed on average for encoding one character , then we need @xmath2 bits to encode `` ormation '' .",
    "notice that 1.5 bpc is not an unreasonable assumption because order-@xmath1 ppm usually achieves more than 2 bpc for text compression @xcite . on the other hand , as we can see from fig .",
    "[ fig_compare ] ( b ) , there are 8 possible words that start with `` inf '' in the dictionary and `` information '' is one of them . therefore ,",
    "if we assign different indexes to these 8 words , we can easily encode `` ormation '' with as few as @xmath3 bits , achieving 9 bits saving over order-@xmath1 ppm .        in practice ,",
    "the dictionary model contains not only words but also their counts so that an arithmetic coder can be used to encode any word in the dictionary .",
    "specifically , every dictionary @xmath4 includes three parts : a common prefix @xmath5 , a list of strings @xmath6 that are suffixes of words with prefix @xmath5 , and corresponding counts @xmath7 .",
    "given a prefix , we can find the associated dictionary model and then perform encoding / decoding .",
    "moreover , different from context models in character - based ppm in which contexts from order-@xmath0 to order-@xmath8 are used , we only maintain dictionary models with fixed - length prefix ( fixed context order ) .",
    "if the prefix is too short , each dictionary will contain a lot of words which is not good for efficient compression . on the other hand ,",
    "if the prefix is too long , number of characters that can be predicted by dictionary model will be very small . in order to take full advantage of dictionary model",
    ", we choose the length of prefix as 3 .",
    "as we can see , the dictionary model works on word basis . by word we mean a sequence of consecutive english letters .",
    "we can always parse a text file into a sequence of alternating words and non - words . for non - words , they can not be encoded by dictionary model , and even for the prefix of a word we need to encode character by character .",
    "therefore , dictionary model is combined with context model used in traditional character - based ppm . at the beginning of encoding / decoding ,",
    "both context model and dictionary model are empty , and texts are encoded on character basis . after a word is encoded , corresponding dictionary model is updated .",
    "detailed encoding and decoding algorithms will be introduced in section [ sec_details ] .    compared with the methods in @xcite and @xcite , the above framework has two advantages .",
    "first , no extra bits are required to indicate the switch from context model to dictionary model , because every time after the prefix of a word ( 3 consecutive english letters ) is encoded or decoded by context model , encoder or decoder will automatically switch to dictionary model . moreover , since dictionary models are constructed and updated during encoding / decoding , no pre - processing is required to build initial dictionaries .",
    "any compression algorithms using more than one model face the problem of model switching @xcite . for example , in traditional ppm in which up to @xmath9 context models ( order-@xmath0 to order-@xmath10 ) might be used when encoding a character , @xmath11 code is sent as a signal to let decoder know switch from current order to lower order . in our case , we need a mechanism to control the switch between dictionary model and context model .",
    "we use finite state machine ( fsm ) , and for both encoder and decoder there are 3 states : @xmath12 , @xmath13 and @xmath14 .",
    "the transition rules between states for encoding and decoding are different , which will be described next .",
    "the encoding process starts at @xmath12 , and the state transition rules are as follows :    * at @xmath12 : encode the next character using context model . if it is an english letter , assign it to an empty string @xmath5 and move to @xmath13 ; otherwise stay at @xmath12 . * at @xmath13 : encode the next character using context model .",
    "if it is an english letter , append it to string @xmath5 ; otherwise , go back to @xmath12 .",
    "if the length of @xmath5 reaches 3 , move to @xmath14 .",
    "* at @xmath14 : read consecutive english letters , i.e. suffix string of current word , denoted as @xmath15 .",
    "if a dictionary @xmath4 associated with prefix @xmath5 if found and @xmath15 exists in @xmath4 , encode @xmath15 using dictionary model @xmath4 . otherwise , encode characters in @xmath15 one by one using context models ( an @xmath11 code should be encoded using dictionary model @xmath4 if @xmath15 can not be found in @xmath4 ) .",
    "move to @xmath12 .",
    "the encoding state transition diagram is depicted in fig .",
    "[ fig_encoder ] .",
    "pseudo code of the algorithm is provided below . on line 6 and 10 , @xmath16 is a function that checks character @xmath17 is an english letter or not . due to limited space , `` * * break * * '' at the end of each `` * * case * * '' clause and the `` * * default * * '' clause",
    "are omitted .",
    "[ alg_encoding ]    @xmath18    at state @xmath14 , the probability of @xmath11 code is calculated by @xmath19 and the probability for suffix @xmath6 is @xmath20      similar to the encoding algorithm , the decoding algorithm also uses fsm with 3 states and starts at @xmath12 , but with different state transition rules :    * at @xmath12 : decode the next character using context model .",
    "if it is an english letter , assign it to an empty string @xmath5 and move to @xmath13 ; otherwise stay at @xmath12 .",
    "* at @xmath13 : decode the next character using context model .",
    "if it is an english letter , append it to string @xmath5 ; otherwise , go back to @xmath12 .",
    "if the length of @xmath5 reaches 3 , decode using dictionary model .",
    "if a string of characters are decoded , move to @xmath12 ; if an @xmath11 code is decoded , move to @xmath14 .",
    "* at @xmath14 : decode characters one by one using context model until a non - english letter is decoded .",
    "move to @xmath12 .",
    "[ alg_decoding ]    @xmath18              as we can see from the decoding algorithm , decoder automatically switches from context model to dictionary model once 3 consecutive english letters are decoded . by doing so , we do nt need to waste any bits to indicate switches between dictionary model and context model .",
    "however , this leads to another problem : if a prefix in the dictionary is a word of length 3 ( e.g. , `` let '' can be either a word or a prefix of `` lettuce '' ) , then every time this word occurs extra bits will be used by the dictionary model to encode an @xmath11 code , resulting in performance degradation . to resolve this problem , we introduce an exclusion mechanism : after a word is encoded / decoded , if it is a prefix of a dictionary , this dictionary is discarded and this prefix is put in a `` blacklist '' for future reference . during encoding / decoding ,",
    "if a prefix is in `` blacklist '' , encoder / decoder skip dictionary model and use context model directly .",
    "in order to show the compression efficiency of the proposed algorithm , vlppm encoder / decoder are implemented and tested on a large set of texts .",
    "specifically , we implemented ppmc encoder / decoder as described in @xcite , and further developed vlppm based on ppmc .",
    "text files from two popular data compression corpora , calgary corpus @xcite and canterbury corpus @xcite , are chosen as the data to be compressed .",
    "compression ratio of vlppm is presented in table [ tab_compression ] in terms of bits per character ( bpc ) , and is compared with traditional ppm ( ppmc ) . as we can see , using proposed vlppm algorithm leads to considerable performance improvements : 16.3% and 6.3% gains are achieved over traditional ppm for order-@xmath21 and order-@xmath1 , respectively . moreover , although vlppm implemented here is based on ppmc , it is applicable to any other character - based predictive compression schemes , such as all the variations of ppm @xcite .",
    ".compression ratio comparison of ppm and vlppm [ cols=\"^,^,^,^,^,^,^ \" , ]",
    "we have presented a text compression algorithm using variable - length prediction by partial matching ( vlppm ) . by introducing dictionary model which contains words with common prefix and combing it with context model used in traditional character - based ppm ,",
    "the proposed method can predict one or more characters at once , further improving the compression efficiency without increasing computational complexity a lot .",
    "moreover , the proposed method does not require any text preprocessing and can be applied to any other character - based predictive compression algorithms without increasing much computational complexity ."
  ],
  "abstract_text": [
    "<S> we propose a method to improve traditional character - based ppm text compression algorithms . consider a text file as a sequence of alternating words and non - words , the basic idea of our algorithm </S>",
    "<S> is to encode non - words and prefixes of words using character - based context models and encode suffixes of words using dictionary models . by using dictionary models , </S>",
    "<S> the algorithm can encode multiple characters as a whole , and thus enhance the compression efficiency . </S>",
    "<S> the advantages of the proposed algorithm are : 1 ) it does not require any text preprocessing ; 2 ) it does not need any explicit codeword to identify switch between context and dictionary models ; 3 ) it can be applied to any character - based ppm algorithms without incurring much additional computational cost . </S>",
    "<S> test results show that significant improvements can be obtained over character - based ppm , especially in low order cases .    </S>",
    "<S> text compression ; markov model ; ppm ; dictionary model . </S>"
  ]
}