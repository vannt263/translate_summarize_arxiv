{
  "article_text": [
    "we develop a theory of data for contingency table data analysis , a priority area of application of correspondence analysis .",
    "much of the foundations of data theory that we discuss are quite general to data analysis , and independent of the correspondence analysis .",
    "motivation includes the following .",
    "correspondence analysis is carried out on a cloud of points ( rows , columns ) through finding of principal directions of elongation , etc .",
    "what legitimizes our assumption of a compact cloud of points ? more generally , what legitimizes our data analysis of a given data set , when we assume that the data set is a sampling of facets or events ( which are to be explained and interpreted through the data analysis ) ?",
    "should we instead allow for singularities or other pathologies or irregularities in such a cloud of points ?",
    "the data analyst , in a somewhat slipshod approach to analyzing data , ignores such issues , and instead cavalierly takes data as sometimes discrete and sometimes continuous . as an example of such singularities ,",
    "consider the preprocessing of data using normalization through taking the logarithm ( common in dealing with astronomical stellar magnitudes , or financial ratios ) .",
    "such normalization can potentially give rise to undefined data values .",
    "why do we consider that our input data sets do not also contain undefined data values ?",
    "in all generality , what justifies the ruling out of such pathologies in our input data ?",
    "the number of attributes used to characterize our observations is possibly infinite .",
    "can our general foundations cope with this ? a priori the answer is clearly no . in this article , we describe a foundation for data analysis , based on henstock s approach to integration , which allows us to bypass such pitfalls in a rigorous manner .",
    "we need a theory which begins with empirical distribution functions deduced from empirical data ( i ) for which there is no analytical description , and ( ii ) that are amenable to empirical computation .",
    "we propose in this article a foundation for data analysis which is at the level of the data , rather than at higher levels of model fitting , so that we are fully compatible thereafter with all statistical modeling approaches . in passing we",
    "will note how quantitative and qualititive data coding are encompassed within our approach ( in section [ sectgenrie ] ) .",
    "neither can be considered as the more legitimate .",
    "there is no one necessary a priori statistical model to be used because there is no one necessary a priori morphology for a data cloud .",
    "( see section [ cylint ] . ) nor is there any one necessary level of resolution in data encoding ( section [ infint ] ) .",
    "empirical distribution functions can be deduced from empirical data for which there is no analytical description ; and then the riemann sums , with their finite number of terms , are amenable to empirical computation .    in multivariate",
    "data analysis , the input data set is assumed to be representative and comprehensive .",
    "however the former can not do justice to an unknown ( and perhaps unknowable ) underlying ( physical , social , etc . )",
    "the latter is approximated very crudely in practice .",
    "can these goals of representativity and comprehensiveness even hypothetically be well approximated in practice ? only with the framework that we present in this article can pathologies be excluded ( in regard to representativity ) , and ( in regard to comprehensiveness ) can we be at ease with infinite dimensional spaces .",
    "as is clear from this list of motivations , we are concerned with the well - foundedness of numerical data , which will subsequently be subject to a statistical data analysis .",
    "the supposition that ( multivariate , time series , etc . ) data can be addressed as such has only been examined in terms of measurement theory ( ordinal , interval , qualitative , quantitative , etc . ) or levels of measurement by s.s .",
    "stevens in the 1940s ( see velleman and wilkinson , 1984 ) .",
    "however suppositions regarding input data have not been examined before in terms of the data set giving rise a well - behaved and exploitable processing input .",
    "we will do so in this article by showing how the henstock or generalized riemann theory of integration also provides a basis for asserting : _ a numerical data set can be analyzed_. the focus on integration , and the perspective introduced , is easily extended to expectation , scalar product , distance , correlation , data aggregation , and so on .    a word on terminology used here : all statistical analysis of data starts with ( qualitative or quantitative ) data in numeric form , presupposing a valuation function mapping facets ( or events ) of the domain studied onto numerical values .",
    "we speak of this as data valuation , or more usually in this context as data encoding . the bigger picture of data encoding together with data normalization or other preprocessing , or indeed processing in the data analysis pipeline , is referred to in this article as data coding .",
    "probability theory , with foundations provided by kolmogorov , is based on probability measures on algebras of events and based ultimately on the lebesgue integral .",
    "lebesgue s just happened to be the first of a number of such investigations into the nature of mathematical integration during the twentieth century .",
    "subsequent developments in integration , by perron , denjoy , henstock and kurzweil , have similar properties and were devised to overcome shortcomings in the lebesgue theory .",
    "see gordon ( 1994 ) for detailed comparison of modern theories of integration .",
    "however , theorists of probability and random variation have not yet really `` noticed '' , or taken account of , these developments in the underlying concepts .",
    "there are many benefits to be reaped by bringing these fundamental new insights in integration or averaging to the study of random variation , and this article aims to demonstrate some of them in the context of data coding .",
    "it is possible to formulate a theory of random variation and probability , linked to data coding , on the basis of a conceptually simpler riemann - type approach , and without reference to the more difficult theories of measure and lebesgue integration . in particular",
    "it is possible to present a riemann - type model of data encoding in which a valuation ( potentially a data value ) is a limit of riemann sums formed by suitably partitioning the sample space @xmath0 in which the process @xmath1 takes its values .",
    "see muldowney ( 1999 , 2000/2001 ) .    to contrast ( traditional )",
    "legesgue and ( more recent ) riemann integration , consider determining a mean value .",
    "suppose the sample space is the set of real numbers , or a subset of them .",
    "if successive instances of the random variable are obtained , we might partition the resulting data into an appropriate number of classes ; then select a representative value of the random variable from each class ; multiply each of the representatives by the relative frequency of the class in which it occurs ; and add up the products .",
    "the result is an estimate of the mean value of the random variable .",
    "table [ riem1 ] illustrates this procedure .",
    "the sample space is partitioned into intervals @xmath2 of the sample variable @xmath1 , the random variable is @xmath3 , and the relative frequency of the class @xmath2 is @xmath4 .",
    "@xmath5    the approach to random variation that we are concerned with in this article consists of a formalization of this relatively simple riemann sum technique which puts at our disposal powerful results in analysis such as the dominated convergence theorem .    in contrast the kolmogorov approach requires , as a preliminary , an excursion into abstract measurable subsets @xmath6 of the sample space , @xmath0 ( table [ kol1 ] ) .",
    "@xmath7    in practice , @xmath0 is often identified with the real numbers or some proper subset of them ; or with a cartesian product , finite or infinite , of such sets . in table",
    "[ kol1 ] , numbers @xmath8 are chosen in the range of values of the random variable @xmath3 , and @xmath9 is @xmath10 .",
    "the resulting @xmath11 is an estimate of the expected value of the random variable @xmath3 .",
    "but the @xmath12-measurable sets @xmath9 are mathematically abstruse , and they can place heavy demands on the understanding and intuition of anyone who is not well - versed in mathematical analysis .",
    "for instance , it can be difficult for a non - specialist to visualize a measurable set @xmath13 in terms of laboratory , industrial or financial measurements of some real - world quantity .",
    "in contrast , the data classes @xmath2 of elementary statistics in table [ riem1 ] are easily understood as real intervals , of one or more dimensions ; and these are the basis of the riemann approach to random variation .    to illustrate the lebesgue - kolmogorov approach , suppose @xmath14 is a normally distributed random variable in a sample space @xmath0 .",
    "then we can represent @xmath0 as @xmath15 , the set of real numbers ; with @xmath14 represented as the identity mapping @xmath16 , @xmath17 ; and with distribution function @xmath18 defined on the family @xmath19 of intervals @xmath20 of @xmath15 , @xmath21 $ ] : @xmath22 then , in the lebesgue - kolmogorov approach , we generate , from the distribution function @xmath18 , a probability measure @xmath23 $ ] on the family @xmath24 of lebesgue measurable subsets of @xmath25 .",
    "so the expectation @xmath26 of any @xmath27-measurable function @xmath28 of @xmath1 is the lebesgue integral @xmath29 . with @xmath0",
    "identified as @xmath15 , this is just the lebesgue - stieltjes integral @xmath30 , and , since @xmath1 is just the standard normal variable of ( [ normal ] ) , the latter integral reduces to the riemann - stieltjes integral  with cauchy or improper extensions , since the domain of integration is the unbounded @xmath31-\\infty,\\infty[$ ] .",
    "in presenting this outline we have skipped over many steps , the principal ones being the probability calculus and the construction of the probability measure @xmath12 .",
    "it is precisely these steps which cease to be necessary preliminaries if we take a generalized riemann approach , instead of the lebesgue - kolmogorov one , in the study of random variation .",
    "because the generalized riemann approach does not make use of an abstract measurable space @xmath0 as the sample space , from here onwards we will take as given the identification of the sample space with @xmath15 or some subset of @xmath15 , or with a cartesian product of such sets , and take the symbol @xmath0 as denoting such a space . accordingly we will drop the traditional notations @xmath14 and @xmath32 for denoting random variables . instead",
    "a random variable will be denoted by the variable ( though unpredictable ) element @xmath1 of the ( now cartesian ) sample space , or by some function @xmath3 of @xmath1 .",
    "the associated likelihoods or probabilities will be given by a distribution function @xmath33 defined on intervals ( which may be cartesian products of one - dimensional intervals ) of @xmath0 . whenever it is necessary to relate the distribution function @xmath34 to its underlying random variable @xmath1",
    ", we may write @xmath34 as @xmath35 .",
    "the standard approach starts with a probability measure @xmath12 defined on a sigma - algebra of measurable sets in an abstract sample space @xmath0 ; it then deduces probability density functions @xmath34 .",
    "these distribution functions ( and not some abstract probability measure ) are the practical starting point for the analysis of many actual random variables  normal ( as described above in ( [ normal ] ) ) , exponential , brownian , geometric brownian , and so on , i.e.  practical data analysis .",
    "in contrast , the generalized riemann approach posits the probability distribution function @xmath34 as the starting point of the theory , and proceeds along the lines of the simpler and more familiar ( table [ riem1 ] ) instead of the more complicated and less intuitive ( table [ kol1 ] ) .    to formalize the concepts , a random variable ( or _ observable _ ) is now taken to be a function @xmath3 defined on a domain @xmath36 where @xmath37 is @xmath15 or some subset of @xmath15 and @xmath38 is an indexing set which may be finite or infinite ; the elements of @xmath0 being denoted by @xmath1 ; along with a likelihood function @xmath34 defined on the intervals of @xmath39 .    in some basic examples such as throwing dice ,",
    "@xmath37 may be a set such as @xmath40 , or , where there is repeated sampling , a cartesian product of such sets .",
    "alternatively , @xmath37 will be the set of positive numbers @xmath41 .",
    "so quantitative and qualitative data encoding are easily supported .",
    "the lebesgue - kolmogorov approach develops probability density functions @xmath34 from probability measures @xmath42 of measurable sets @xmath13 . even though distribution functions are often the starting point in practice ( as in ( [ normal ] ) above ) , kolmogorov gives primacy to the probability measures @xmath12 , and they are the basis of the calculus of probabilities , including the crucial relation @xmath43 viewed as an axiom , relation ( [ eqn2 ] ) is a somewhat mysterious statement about rather mysterious objects .",
    "but it is the lynch - pin of the lebesgue - kolmogorov theory , and without it the twentieth century understanding of random variation would have been impossible .",
    "the generalized riemann approach starts with probability density functions @xmath35 defined only on intervals @xmath20 of the sample space @xmath44 .",
    "we can , as shown below ( [ prob ] ) , deduce from this approach probability functions @xmath45 defined on a broader class of `` integrable '' sets @xmath13 , and a calculus of probabilities which includes the relation ( [ measure])but as a theorem rather than an axiom .",
    "what , if any , is the relationship between these two approaches to random variation ? there is a theorem ( muldowney and skvortsov , 2001/2002 ) which states that every lebesgue integrable function ( in @xmath46 ) is also generalized riemann integrable . in effect",
    ", this guarantees that every result in the lebesgue - kolmogorov theory also holds in the generalized riemann approach .",
    "so , in this sense , the former is a special case of the latter .    the key point in developing a rigorous theory of random variation ( which supports data valuation and",
    "hence data analysis ) by means of generalized riemann integration is , following the scheme of table [ riem1 ] , to partition the domain or sample space @xmath44 , in an appropriate way , as we shall proceed to show .",
    "( whereas in the lebesgue - kolmogorov - it approach we step back from table [ riem1 ] , and instead use table [ kol1 ] supported by ( [ measure ] ) . the two approaches part company at the tables [ riem1 ] and [ kol1 ] stage . )    in the generalized riemann approach we focus on the classification of the sample data into mutually exclusive classes or intervals @xmath20 .",
    "i.e. , through data encoding we undertake partitioning of the sample space @xmath47 into mutually exclusive intervals @xmath20 .    in pursuing a rigorous theory of random variation along these lines this basic idea of partitioning the sample space is the key . instead of retreating to the abstract ( kolmogorov measures on subsets ) machinery of table [ kol1 ]",
    ", we find a different way ahead by carefully selecting the intervals @xmath2 which partition the sample space @xmath48 .",
    "an idea of what is involved in this can be obtained by recalling the role of riemann sums in basic integration theory .",
    "suppose for simplicity that the sample space @xmath0 is the interval @xmath49 and the random variable @xmath3 is given by @xmath50 ; and suppose @xmath51 $ ] where @xmath52 is the family of subintervals @xmath53 .",
    "we can interpret @xmath34 as the probability distribution function of the underlying random variable @xmath1 , so @xmath33 is the likelihood that @xmath54 . as a distribution function",
    ", @xmath34 is finitely additive on @xmath52 .    the simplest intuition of likelihood  as something intermediate between certainty of non - occurrence and certainty of occurrence  implies that likelihoods must be representable as numbers between 0 and 1 .",
    "it follows that distribution functions are finitely additive on @xmath52 .",
    "this immediately lifts the burden of credulity that ( [ measure ] ) imposes on our naive or `` natural '' sense of what probability or likelihood is .    with @xmath28 a deterministic function of the random variable @xmath1 ,",
    "the random variation of @xmath3 is our object of investigation . in the first instance",
    "we wish to establish @xmath55 , the expected value of @xmath3 , as , in some sense , the integral of @xmath28 with respect to @xmath34 , which is often estimated as in table [ riem1 ] .",
    "following broadly the scheme of table [ riem1 ] , we first select an arbitrary number @xmath56 . then we choose a finite number of disjoint intervals @xmath57 ; @xmath58 , @xmath59 , with each interval @xmath60 satisfying @xmath61 we then select a representative @xmath62 , @xmath63 , @xmath64 .",
    "( for simplicity we are using superscript @xmath65 instead of @xmath66  for labelling , not exponentiation .",
    "the reason for not using subscript @xmath67 is to keep such subscripts available to denote dimensions in multi - dimensional variables . )",
    "then the riemann ( or riemann - stieltjes ) integral of @xmath28 with respect to @xmath34 exists , with @xmath68 , if , given any @xmath69 , there exists a number @xmath56 so that @xmath70 for every such choice of @xmath62 , @xmath60 satisfying ( [ riem ] ) , @xmath71 .",
    "if we could succeed in creating a theory of random variation along these lines then we could reasonably declare that the expectation @xmath72 of the random variable @xmath3 , relative to the distribution function @xmath33 , is @xmath73 whenever the latter exists in the sense of ( [ int1 ] ) .",
    "( in fact this statement is true , but a justification of it takes us deep into the kolmogorov theory of probability and random variation .",
    "a different justification is given in this article . )    but ( [ riem ] ) and ( [ int1 ] ) on their own do not yield an adequate theory of random variation . for one thing ,",
    "it is well known that not every lebesgue integrable function is riemann integrable .",
    "so in this sense at least , table [ kol1 ] goes further than table [ riem1 ] and relation ( [ int1 ] ) .",
    "more importantly , any theory of random variation must contain results such as central limit theorems and laws of large numbers , which are the core of our understanding of random variation , and the proofs of such results require theorems like the dominated convergence theorem , which are available for table [ kol1 ] and lebesgue integrals , but which are not available for the ordinary riemann integrals of table [ riem1 ] and ( [ int1 ] ) .",
    "however , before we take further steps towards the generalization of the riemann integral ( [ int1 ] ) which will give us what we need , let us pause to give further consideration to data encoding .",
    "though the classes @xmath60 used in ( [ int1 ] ) above are not required to be of equal length , it is certainly consistent with ( [ int1 ] ) to partition the sample data into equal classes . to see this ,",
    "choose @xmath74 so that @xmath75 , and then choose each @xmath76 so that @xmath77",
    ". then @xmath78 ( @xmath71 ) gives us a partition of @xmath79 in which each @xmath60 has the same length @xmath80 .",
    "we could also , in principle , obtain quantile classification of the data by this method of @xmath81-partitioning .",
    "suppose we want decile classification ; that is , @xmath82 with @xmath83 , @xmath64 .",
    "this is possible , since the function @xmath84 is monotone increasing and continuous for almost all @xmath85a , b[$ ] , and hence there exist @xmath76 such that @xmath86 for @xmath87 .",
    "so if @xmath81 happens to be greater than @xmath88 , then the decile classification satisfies @xmath89 for @xmath87 .",
    "( this argument merely establishes the existence of such a classification . actually determining quantile points for a particular distribution function requires _ ad hoc _ consideration of the distribution function in question . )    in fact , this focus on the system of data encoding is the avenue to a rigorous theory of random variation within a riemann framework , as we shall now see .",
    "in the previous section we took the sample space @xmath0 to be @xmath90 . as our attention from here on is going to be ( below in the application study ) increasingly focussed on counts or frequencies , which are non - negative , we will take the sample space to be @xmath910 , \\infty[$ ] , or a multiple cartesian product of @xmath41 by itself .",
    "figure [ pic1 ] shows a partition of an unbounded finite - dimensional domain such as @xmath92 . in this illustration , @xmath930,u_2 ^ 1 [ \\\\ i^5 & = & [ u_1 ^ 5,\\infty [ \\times [ u_2 ^ 1,u_2 ^ 3 [ \\\\ i^6 & = & [ u_1 ^ 4,\\infty [ \\times [ u_2 ^ 3,\\infty [ \\\\ i^7 & = & [ u_1 ^ 2,u_1 ^ 4 [ \\times [ u_2 ^ 4,\\infty [ \\\\ i^8 & = & ] 0,u_1 ^ 2 [ \\times [ u_2 ^ 3 , \\infty [ \\\\ i^9 & = & ] 0,u_1 ^ 1 [ \\times [ u_2 ^ 2,u_2 ^ 3 [ \\\\ i^{10 } & = & ] 0,u_1 ^ 3 [ \\times ] 0,u_2 ^ 2[. \\end{array}\\ ] ]    ( 400,420 )    ( 10,10)(1,0)350 ( 10,10)(0,1)350    ( 60,120)(1,0)150 ( 60,210)(1,0)270 ( 120,330)(1,0)150 ( 210,90)(1,0)120 ( 90,120)(0,1)90 ( 120,210)(0,1)150 ( 210,60)(0,1)150 ( 270,210)(0,1)150 ( 300,90)(0,1)120    ( 90,0)(0,0)@xmath94 ( 120,0)(0,0)@xmath95 ( 210,0)(0,0)@xmath96 ( 270,0)(0,0)@xmath97 ( 300,0)(0,0)@xmath98    ( 0,90)(0,0)@xmath99 ( 0,120)(0,0)@xmath100 ( 0,210)(0,0)@xmath101 ( 0,330)(0,0)@xmath102    ( 120,60)(0,0)@xmath103 ( 270,60)(0,0)@xmath104 ( 45,165)(0,0)@xmath105 ( 150,165)(0,0)@xmath106 ( 255,150)(0,0)@xmath107 ( 45,270)(0,0)@xmath108 ( 195,280)(0,0)@xmath109 ( 360,270)(0,0)@xmath110 ( 195,360)(0,0)@xmath111 ( 360,150)(0,0)@xmath112    for each elementary occurrence @xmath113 ( @xmath74 a positive integer ) , let @xmath114 be a positive number",
    ". then an admissible classification of the sample space , called a @xmath81__-fine division _ _ of @xmath0 , is a finite collection @xmath115 so that @xmath62 is in @xmath60 .",
    "the @xmath60 are disjoint with union @xmath0 , and the lengths of the edges ( or sides ) of each @xmath116 are bounded by @xmath117 .",
    "so , referring back to table [ riem1 ] of elementary statistics , what we are doing here is selecting the data classification intervals @xmath60 along with a representative value @xmath62 from @xmath60 .",
    "it is convenient ( though not a requirement of the theory ) that the representative value @xmath62 should be a vertex of @xmath60 , and that is how we shall proceed .    in the case of the ordinary riemann integral in a compact domain ( cf .",
    "( [ int1 ] ) ) , the positive function @xmath81 is simply a positive constant , and the bound in question is simply the condition that each edge of each interval has length less than @xmath81 .",
    "ordinary riemann integration over unbounded domains , or domains which contain singularity points of the integrand , is obtained by means of the _ improper riemann integral _ ( for details of which , see rudin ( 1970 ) for instance ) .",
    "in contrast , the generalized riemann integral handles all of these situations in essentially the same way , removing the need for improper extension . in the illustration in figure [ pic1 ] above",
    ", some of the edges are infinitely long .",
    "the precise sense in which each edge ( finite or infinite ) of @xmath60 is bounded by @xmath118 is explained at the end of this section .",
    "the riemann sum corresponding to ( [ div ] ) is @xmath119 i.e.  it is simply the sum over the terms in equation ( [ div ] ) .",
    "we say that @xmath28 is _ generalized riemann integrable _ with respect to @xmath34 , with @xmath120 , if , for each @xmath121 , there exists a function @xmath122 so that , for every @xmath123 , @xmath124 with this step we overcome the two previously mentioned objections to the use of riemann - type integration in a theory of random variation .",
    "firstly , every function @xmath28 which is lebesgue - stieltjes integrable in @xmath0 with respect to @xmath34 is also generalized riemann integrable , in the sense of ( [ int ] ) .",
    "see gordon ( 1994 ) for a proof of this .",
    "secondly , we have theorems such as the dominated convergence theorem ( see , for example , gordon , 1994 ) which enable us to prove laws of large numbers , central limit theorems and other results which are needed for a theory of random variation .",
    "so we can legitimately use the usual language and notation of probability theory .",
    "thus , the expectation of the random variable @xmath3 with respect to the probability distribution function @xmath33 is @xmath125 to recapitulate , elementary statistics involves calculations of the form ( [ riem1 ] ) , often with classes @xmath20 of equal size or equal likelihood .",
    "we refine this method by carefully selecting the data classification intervals @xmath20 .",
    "in fact our riemann sum estimates involve choosing a finite number of occurrences @xmath126 from @xmath0 ( actually , from the closure of @xmath0 ) , and then selecting associated classes @xmath127 , disjoint with union @xmath0 , with @xmath128 in @xmath129 ( or with each @xmath128 a vertex of @xmath2 , in the version of the theory that we are presenting here ) , such that for each @xmath64 , @xmath2 is @xmath81-_fine_. the meaning of this is as follows .",
    "let @xmath130 be @xmath41 with the points @xmath131 and @xmath132 adjoined .",
    "( in the following paragraph , @xmath133 and @xmath134 are given special treatment .",
    "many functions are undefined for @xmath134 ; and @xmath133 is a singularity for the function @xmath135 which may be of use in data normalization  for instance when dealing with astronomy stellar magnitudes or financial ratios . )    let @xmath20 be an interval in @xmath41 , of the form @xmath1360 , v [ , \\,\\,\\,\\ , [ u , v [ , \\,\\,\\,\\ , \\mbox{or   } [ u , \\infty[,\\ ] ] and let @xmath1370,\\infty[$ ] be a positive function defined for @xmath138 .",
    "the function @xmath81 is called a _ gauge _ in @xmath41 .",
    "we say that @xmath20 is _ attached to _ @xmath1 ( or _ associated with _",
    "@xmath1 ) if @xmath139 respectively .",
    "if @xmath20 is attached to @xmath1 we say that @xmath140 is @xmath81-_fine _ ( or simply that @xmath20 is @xmath81-fine ) if @xmath141 respectively .",
    "that is what we mean by @xmath81-fineness in one dimension .",
    "what about higher dimensions ?",
    "suppose @xmath142 is an interval of @xmath143 , each @xmath144 being a one - dimensional interval of form ( [ interval ] ) .",
    "a point @xmath145 of @xmath146 is attached to @xmath20 in @xmath147 if each @xmath148 is attached to @xmath144 in @xmath41 , @xmath149 .",
    "given a function @xmath1500 , \\infty[$ ] , an associated pair @xmath140 is @xmath81-fine in @xmath147 if each @xmath144 satisfies the relevant condition in ( [ fine ] ) with the new @xmath114 .",
    "a finite collection of associated @xmath140 is a @xmath81-fine division of @xmath147 if the intervals @xmath20 are disjoint with union @xmath147 , and if each of the @xmath140 is @xmath81-fine .",
    "a proof of the existence of such a @xmath81-fine division is given in henstock ( 1988 ) , theorem 4.1 .    a glance at diagram ( [ pic1 ] ) above",
    "will show that many of points @xmath1 involved in a division of @xmath147 ( vertices of the partitioning intervals ) , which correspond to the representative occurrences @xmath128 of the data encoding in table [ riem1 ] , will belong to @xmath151 ; in other words @xmath1 may have some components @xmath148 equal to @xmath131 or @xmath132 .",
    "the special arrangements we have made for such points , in ( [ fine ] ) above , are in anticipation of the singularities that are present at such points in the expressions that arise in our data encoding problem . these arrangements , which are characteristic of generalized riemann integration , forestall any need for the kind of improper extensions which are needed in other integration theories .",
    "there are certain familiar landmarks in the study of probability theory and its offshoots such as the calculus of probabilities , which has not entered into the discussion thus far .",
    "the key point in this calculus is the relationship @xmath152 in fact the set - functions @xmath12 and their calculus are not used as the basis of the generalized riemann approach to the study of random variation . instead , the basis is the simpler set - functions @xmath34 , defined only on intervals , and finitely additive on them .",
    "but , as mentioned earlier , an outcome of the generalized riemann approach is that we can recover set - functions defined on sets ( including the measurable sets of the kolmogorov theory ) which are more general than intervals , and we can recover the probability calculus which is associated with them .    to see this ,",
    "suppose @xmath153 is such that @xmath154 exists in the sense of ( [ int ] ) .",
    "then define @xmath155 and we can easily deduce from the dominated convergence theorem for generalized riemann integrals , that for disjoint @xmath6 for which @xmath156 exists , @xmath157 other familiar properties of the calculus of probabilities are easily deduced from ( [ prob ] ) .    since every lebesgue integrable function is also generalized riemann integrable ( gordon , 1994 ) , every result obtained by lebesgue integration is also valid for generalized riemann integration .",
    "so in this sense , the generalized riemann theory of random variation is an extension or generalization of the theory developed by kolmogorov , levy , it and others .",
    "however the kind of argument which is natural for lebesgue integration is different from that which would naturally be used in generalized riemann integration , so it is more productive in the latter case to develop the theory of random variation from first principles on riemann lines .",
    "some pointers to such a development are given in ( muldowney , 1999 ) .",
    "many of the standard distributions ( normal , exponential and others ) are mathematically elementary , and the expected or average values of random variables , with respect to these distributions  whether computed by means of the generalized riemann or lebesgue methods  often reduce to riemann or riemann - stieltjes integrals .",
    "many aspects of these distributions can be discovered with ordinary riemann integration .",
    "but it is their existence as generalized riemann integrals , possessing properties such as the dominated convergence theorem and fubini s theorem , that gives us access to a full - blown theory of random variation .",
    "when random variables @xmath158 are being considered jointly , their _ marginal _ behavior is a primary consideration .",
    "this means examining the joint behavior of any finite subset of the variables , the remaining ones ( whether finitely or infinitely many ) being arbitrary or left out of consideration .",
    "thus we are led to families @xmath159 where the sets @xmath160 belong to the family @xmath161 of finite subsets of @xmath38 , the set @xmath38 being itself finite or infinite .",
    "( when @xmath38 is infinite the family @xmath162 is often called a _ process _ or _ stochastic process _ , especially when the variable @xmath163 represents time",
    ". we will write the random variable @xmath164 as @xmath165 depending on the context ; likewise @xmath166 . ) in the following discussion we will suppose , for illustrative purposes , that for each @xmath163 the domain of values of @xmath164 is the set @xmath41 of positive numbers",
    ". this would apply if , for instance , @xmath167 is price history , @xmath168 .",
    "the marginal behavior of a process is specified by marginal distribution functions .",
    "the marginal distribution function of the random variable or process @xmath169 , for any finite subset @xmath170 , is the function @xmath171 defined on the intervals @xmath172 of @xmath147 , which we interpret as the likelihood that the random variable @xmath173 takes a value in the one - dimensional interval @xmath144 for each @xmath174 , @xmath64 ; with the remaining random variables @xmath164 arbitrary for @xmath175 .",
    "one of the uses to which the marginal behavior is put is to determine the presence or absence of _ independence_. the family of random variables @xmath162 is _ independent _ if the marginal distribution functions satisfy @xmath176 for every finite subset @xmath177 .",
    "that is , the likelihood that the random variables @xmath178 , @xmath179 , @xmath180 , @xmath181 jointly take values in @xmath182 , @xmath183 @xmath184 , @xmath185 ( with @xmath164 arbitrary for @xmath186 ) is the product over @xmath187 of the likelihoods of @xmath188 belonging to @xmath144 ( with @xmath164 arbitrary for @xmath189 , @xmath190 ) for every choice of such intervals , and for every choice of finite subset @xmath160 of @xmath38 .    of course , if @xmath38 is itself finite , it is sufficient to consider only @xmath191 in order to establish whether or not the random variables are independent .",
    "when @xmath38 is infinite ( so the random variable @xmath192 is a stochastic process ) , it is usual to define the distribution of @xmath1 as the family of distribution functions @xmath193 this is somewhat awkward , since up to this point the distribution of a random variable has been given as a single function defined on intervals of the sample space , and not as a family of functions .",
    "however we can tidy up this awkwardness as follows .",
    "firstly , the sample space @xmath0 is now the cartesian product @xmath194 .",
    "let @xmath161 denote the family of finite subsets @xmath195 of @xmath38 .",
    "then for any @xmath196 , the set @xmath197 : = i_{t_1 } \\times i_{t_2}\\times \\cdots \\times i_{t_n } \\times \\prod\\{{{\\rm{i\\!\\!\\!r}}}_+ : b \\setminus n \\}\\ ] ] is called a _",
    "cylindrical interval_. taking all choices of @xmath198 and all choices of one - dimensional intervals @xmath144 ( @xmath199 ) , denote the resulting class of cylindrical intervals by @xmath52 .",
    "these cylindrical intervals are the subsets of the sample space that we need to define the distribution function @xmath34 of @xmath1 in @xmath200 : @xmath201 ) : = f_{(x(t_1 ) , x(t_2 ) , \\ldots , x(t_n))}(i_{t_1 } \\times i_{t_2 } \\times \\cdots \\times i_{t_n})\\ ] ] for every @xmath196 and every @xmath202 \\in \\mathcal{i}$ ] .    by thus defining the distribution function @xmath34 ( of the underlying random variable @xmath203 ) on the family of subsets @xmath52 ( the cylindrical intervals ) of @xmath200 , we are in conformity with the system used for describing distribution functions in finite - dimensional sample spaces .    as in the elementary situation of table",
    "[ riem1 ] , it naturally follows , if we want to estimate the expected value of some deterministic function of the random variable ( or process ) @xmath204 , that the joint sample space @xmath205 of the individual random variables @xmath165 should be partitioned by means of cylindrical intervals @xmath202 $ ] .    to demonstrate such a partition ,",
    "we suppose @xmath38 is the time interval @xmath206\\tau , t]$ ] , so the sample space @xmath0 is @xmath207\\tau , t ] } { { \\rm{i\\!\\!\\!r}}}_+ = { { \\rm{i\\!\\!\\!r}}}_+^{]\\tau , t]}$ ] .",
    "suppose @xmath208 and , with @xmath160 denoting @xmath209 , suppose @xmath197 = i_1 \\times",
    "i_2 \\times \\cdots \\times i_n \\times { { \\rm{i\\!\\!\\!r}}}_+^{b \\setminus n}\\ ] ] is one of the cylindrical intervals forming a partition of @xmath200 .    in figure [ pic2 ]",
    ", we can show only three dimensions . as in figure",
    "[ pic1 ] , the fact that the sample space is unbounded in each of its separate dimensions means that many of the partitioning intervals have associated points with one or more components equal to @xmath131 or @xmath132 .",
    "we have terms @xmath210 in the integrand which are undefined for @xmath211 , just as @xmath212 is undefined . in generalized riemann integration ,",
    "any intervals involving a singularity must have the point of singularity as the attached or associated point . by arranging things in this way , generalized riemann integration avoids having to resort to the improper or cauchy extensions when the integrand involves a point of singularity .",
    "in contrast to figure [ pic1 ] , the partitioning intervals may have different restricted dimensions .",
    "for instance , in figure [ pic2 ] , the cylindrical interval @xmath213 is restricted only in the vertical direction @xmath214 ; and is unrestricted in the horizontal direction @xmath215 and in each of the infinitely many other directions @xmath216 ( of which only one of the directions perpendicular to both @xmath215 and @xmath214 is shown in the diagram ) .",
    "this is a particular feature of partitioning infinite - dimensional domains by means of infinite - dimensional cylindrical intervals , which we must take account of when we construct riemann sums of integrands over such partitions .    in this illustration ( figure [ pic2 ] )",
    "the cylindrical intervals mostly correspond to the finite - dimensional intervals of ( [ intervals1 ] ) , but an extra one , @xmath213 , has been included to demonstrate that the restricted dimensions of the cylindrical intervals do not all have to be the same in a partition of an infinite - dimensional space .",
    "( of course this is also true for finite dimensional spaces .",
    "we could have included an interval corresponding to @xmath213 in ( [ intervals1 ] ) , but in partitioning for riemann sum estimates in the finite - dimensional case , these kind of intervals can be avoided and nothing is gained by admitting them .",
    "but in partitioning infinite - dimensional spaces they can not be avoided . )",
    "the intervals in figure [ pic2 ] are : @xmath2170,u_2 ^ 1 [   \\times \\prod \\{{{\\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ } \\},\\\\",
    "i^5 & = & [ u_1 ^ 5,\\infty [ \\times [ u_2 ^ 1,u_2 ^ 3 [   \\times \\{\\prod { { \\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ } \\},\\\\",
    "i^6 & = & [ u_1 ^ 4,\\infty [ \\times [ u_2 ^ 4,\\infty [   \\times \\{\\prod { { \\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ } \\},\\\\",
    "i^7 & = & [ u_1 ^ 2,u_1 ^ 4 [ \\times [ u_2 ^ 5,\\infty [   \\times \\{\\prod { { \\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ } \\},\\\\",
    "i^8 & = & ] 0,u_1 ^ 2 [ \\times [ u_2 ^ 4 , \\infty [   \\times \\{\\prod",
    "{ { \\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ } \\},\\\\",
    "i^9 & = & ] 0,u_1 ^ 1 [ \\times [ u_2 ^ 2,u_2 ^ 3 [   \\times \\{\\prod { { \\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ }",
    "\\},\\\\ i^{10 } & = & ] 0,u_1 ^ 3 [ \\times ] 0,u_2 ^ 2 [ \\times \\{\\prod { { \\rm{i\\!\\!\\!r}}}_+ : t \\in b\\setminus \\{t_1,t_2\\ }",
    "\\},\\\\ i^{11 } & = & ] u_2 ^ 3,u_2 ^ 4 [ \\times",
    "\\prod \\{{{\\rm{i\\!\\!\\!r}}}_+ : t \\in b , t",
    "\\neq t_2\\}. \\end{array}\\ ] ]    criteria ( [ int ] ) , ( [ genriemint2 ] ) place no a priori conditions on the functions @xmath28 and @xmath34 in the integrand when we test it for integrability . there are no required or preferred kinds of function .",
    "it is true that we have required @xmath34 to be finitely additive , but this is related to our secondary purpose of constructing an alternative to the kolmogorov theory of probability and random variation . of course",
    ", in meeting the criteria ( [ int ] ) , ( [ genriemint2 ] ) , any good properties possessed by @xmath28 and @xmath34 may come into play in order to give us a good encoding .",
    "the foregoing remarks may be translated into language that is more appropriate for statistical data analysis : there is no necessary a priori morphology for the data cloud to be analyzed ; or there is no necessary a priori model or distribution for the data .",
    "( 400,420 )    ( 60,120)(1,0)150 ( 60,210)(1,0)270 ( 120,330)(1,0)150 ( 210,90)(1,0)120 ( 60,240)(1,0)270    ( 90,120)(0,1)90 ( 120,240)(0,1)120 ( 210,60)(0,1)150 ( 270,240)(0,1)120 ( 300,90)(0,1)120    ( 90,0)(0,0)@xmath94 ( 120,0)(0,0)@xmath95 ( 210,0)(0,0)@xmath96 ( 270,0)(0,0)@xmath97 ( 300,0)(0,0)@xmath98    ( 0,90)(0,0)@xmath99 ( 0,120)(0,0)@xmath100 ( 0,210)(0,0)@xmath101 ( 0,240)(0,0)@xmath102 ( 0,330)(0,0)@xmath218    ( 150,60)(0,0)@xmath103 ( 240,60)(0,0)@xmath104 ( 45,165)(0,0)@xmath105 ( 150,165)(0,0)@xmath106 ( 255,150)(0,0)@xmath107 ( 45,270)(0,0)@xmath108 ( 195,280)(0,0)@xmath109 ( 195,225)(0,0)@xmath213 ( 360,270)(0,0)@xmath110 ( 195,360)(0,0)@xmath111 ( 360,150)(0,0)@xmath112    ( 60,105)(2,1)40 ( 60,195)(2,1)40 ( 90,225)(2,1)40 ( 240,315)(2,1)40 ( 90,315)(2,1)40 ( 240,225)(2,1)40 ( 180,195)(2,1)40 ( 270,195)(2,1)40 ( 180,105)(2,1)40 ( 180,75)(2,1)40 ( 270,75)(2,1)40",
    "as discussed earlier , the riemann sum approach can be adapted so that it yields a theory of random variation which meets the theoretical and practical needs of analysis .",
    "the adaptation that is needed when only a finite number of random variables is involved has been explained already .    but how can it be adapted to the situation when there are infinitely many random variables to be considered jointly ?",
    "what kind of riemann sums are appropriate in a rigorous theory of joint variation of infinitely many variables ?",
    "in other words , what kind of partitions are permitted in forming the riemann sum approximation to the expected value of a random variable which depends on infinitely many underlying random variables ?    in ordinary riemann integration",
    "we form riemann sums by choosing partitions whose finite - dimensional intervals have edges ( sides ) which are bounded by a positive constant @xmath81 .",
    "then we make @xmath81 successively smaller .",
    "likewise for generalized riemann integration , where the constant @xmath81 is replaced by a positive function @xmath219 . in any case",
    ", we are choosing successive partitions in which the component intervals successively `` shrink '' in some sense .",
    "for the infinite - dimensional situation , we seek likewise to `` shrink '' the cylindrical intervals @xmath202 $ ] of which successive partitions are composed . in figure",
    "[ pic4 ] we show different ways in which a cylindrical interval can be a subset of a larger cylindrical interval , and hence seek to establish effective rules by which intervals of successive partitions can be made successively smaller .    ( 400,420 )    ( 30,45)(330,0 ) ( 30,330)(330,0 )    ( 45,30)(1,1)20 ( 45,315)(1,1)20 ( 315,30)(1,1)20 ( 315,315)(1,1)20    ( 0,45)(0,0)@xmath220 ( 0,330)(0,0)@xmath221    ( 30,120)(1,0)330 ( 30,250)(1,0)330    ( 0,120)(0,0)@xmath222 ( 0,250)(0,0)@xmath223 ( 45,105)(1,1)20 ( 45,235)(1,1)20 ( 315,105)(1,1)20 ( 315,235)(1,1)20    ( 120,120)(1,0)150 ( 120,250)(1,0)150 ( 120,120)(0,1)130 ( 270,120)(0,1)130    ( 120,0)(0,0)@xmath224 ( 270,0)(0,0)@xmath225    ( 105,105)(1,1)20 ( 105,235)(1,1)20 ( 255,105)(1,1)20 ( 255,235)(1,1)20    ( 150,165)(1,0)60 ( 150,195)(1,0)60 ( 150,165)(0,1)30 ( 210,165)(0,1)30    ( 210,195)(1,1)10 ( 210,165)(1,1)10    ( 160,205)(1,0)60 ( 220,175)(0,1)30 ( 150,195)(1,1)10    ( 30,340)(0,0)[bl]@xmath106 ( 30,260)(0,0)[bl]@xmath109 ( 130,260)(0,0)[bl]@xmath107 ( 170,210)(0,0)[bl]@xmath104    let the horizontal direction in figure [ pic4 ] be denoted @xmath215 , denote the vertical direction by @xmath214 , and denote the direction perpendicular to both by @xmath226 .",
    "let @xmath38 denote the set of all the dimensions , or mutually perpendicular directions , of the domain @xmath200",
    ". then @xmath106 is @xmath227 .",
    "the interval @xmath228 is a subinterval of @xmath106 , in which the side corresponding to restricted dimension @xmath214 is shorter than the corresponding side of @xmath106 .",
    "this kind of `` shrinking '' is familiar from finite - dimensional riemann integration .",
    "we get it by imposing a condition that the sides of the intervals be less than some positive function @xmath81 , and then taking @xmath81 successively smaller .",
    "now consider @xmath229 , which is a subset of @xmath109 , in which the length of the restricted sides is the same as the length of the restricted side of @xmath109 ; but in which there is an additional restricted dimension @xmath215 . here",
    "we obtain shrinking , without changing @xmath81 , but by requiring the interval to have additional restricted dimensions .",
    "we can do this by specifying some minimal finite set of dimensions in which the interval must be restricted .",
    "( we may allow the interval to be restricted in additional dimensions outside of this minimal set ; just as the sides can be as small as we like provided their length is bounded by @xmath81 . )",
    "then we can obtain shrinking of the intervals by increasing without limit the number of elements in this minimal finite set , just as we can obtain shrinking by decreasing towards zero the size of the @xmath81 which bounds the lengths of the restricted sides .",
    "if we compare @xmath104 with @xmath109 we see both factors at work simultaneously  increased restricted dimensions and reduced length of sides .",
    "this provides us with the intuition we need to construct appropriate rules for forming partitions for riemann sums in infinite - dimensional spaces .",
    "as before , suppose @xmath38 is a set with a possibly infinite number of elements .",
    "let @xmath161 denote the family of finite subsets @xmath160 of @xmath38 .",
    "let a typical @xmath196 be denoted @xmath230 .",
    "suppose the sample space is @xmath231 .",
    "for @xmath196 , let @xmath232 denote the projection of @xmath0 into the finite set @xmath160 .",
    "suppose @xmath144 is an interval of type ( [ interval ] ) in @xmath233 .",
    "then @xmath234 is a cylindrical interval , denoted @xmath202 $ ] .",
    "as before , let @xmath52 denote the class of cylindrical intervals obtained through all choices of @xmath196 , and all choices of intervals @xmath144 of type ( [ interval ] ) , for each @xmath199 .",
    "a point @xmath235 is associated with a cylindrical interval @xmath202 $ ] if , for each @xmath199 , the component @xmath236 is associated with @xmath144 in the sense of ( [ tag ] ) .",
    "a finite collection @xmath237 of associated pairs @xmath238)$ ] is a _ division _ of @xmath200 if the finite number of the cylindrical intervals @xmath202 $ ] form a partition of @xmath200 ; that is , if they are disjoint with union @xmath200 .",
    "now define functions @xmath239 and @xmath240 as follows .",
    "let @xmath241 , and for each @xmath196 let @xmath2420 , \\infty[$ ] .",
    "the mapping @xmath240 is defined on the set of associated points of the cylindrical intervals @xmath202 \\in \\mathcal{i}$ ] ; and , for each @xmath196 , the mapping @xmath239 is a function defined on the set of associated points of intervals @xmath172 in @xmath232 .    the sets @xmath243 and the numbers @xmath244 determine the kinds of cylindrical intervals , partitioning the sample space , which we permit in forming riemann sums .",
    "a set @xmath245 determines a minimal set of restricted dimensions which must be possessed by any cylindrical interval @xmath202 $ ] associated with @xmath1 . in other words , we require that @xmath246 . the numbers @xmath247 form the bounds on the lengths of the restricted faces of the cylindrical intervals @xmath202 $ ] associated with @xmath1 .",
    "formally , the role of @xmath240 and @xmath239 is as follows .    for",
    "any choice of @xmath240 and any choice of the family @xmath248 , let @xmath249 denote @xmath250 .",
    "we call @xmath249 a _ gauge _ in @xmath200 .",
    "the class of all gauges is obtained by varying the choices of the mappings @xmath240 and @xmath239 .    given a gauge @xmath249 , an associated pair @xmath238)$ ] is @xmath249-_fine _ provided @xmath251 , and provided , for each @xmath199 , @xmath252 is @xmath239-fine , satisfying the relevant condition in ( [ fine ] ) with @xmath253 in place of @xmath219 .    given a random variable , or function @xmath28 of @xmath1 , with a probability distribution function @xmath34 defined on the cylindrical intervals @xmath202 $ ] of @xmath52 , the integrand @xmath254)$ ] is integrable in @xmath200 , with @xmath255 ) = \\alpha$ ] , if , given @xmath256 , there exists a gauge @xmath249 so that , for every @xmath249-fine division @xmath257 of @xmath200 , the corresponding riemann sum satisfies @xmath258 ) - \\alpha \\right| < \\varepsilon.\\ ] ] if @xmath38 is finite , this definition reduces to definition ( [ int ] ) , because , as each @xmath243 increases , in this case it is not `` without limit '' ; as eventually @xmath259 for all @xmath1 , and then ( [ genriemint2 ] ) is equivalent to ( [ int ] ) . also ( [ genriemint2 ] ) yields results such as fubini s theorem and the dominated convergence theorem ( see muldowney , 1988 ) which are needed for the theory of joint variation of infinitely many random variables .",
    "in a number of papers , muldowney ( 2000/2001 , 2002 , 2005 ) has explored expectation and , more generally , integral properties of the black - scholes model of derivative asset pricing . in the application studied in this article , we will consider the finding of structure in empirical financial data .",
    "for this we will use correspondence analysis , because it provides an integrated tool set for assessing departure from standard behavior in the data .",
    "correspondence analysis is a data analysis approach based on low - dimensional spatial projection . unlike other such approaches , it particularly well caters for qualitative or categorical input data , including counts .",
    "hence it is an ideal example of our view that generalized riemann integration offers a solid theoretical framework on which to base such an analysis .",
    "our objectives in this analysis are to take data recoding as proposed in ross ( 2003 ) and study it as a type of coding commonly used in correspondence analysis .",
    "ross ( 2003 ) uses input data recoding to find faint patterns in otherwise apparently structureless data .",
    "the implications of doing this are important : we wish to know if such data recoding can be applied in general to apparently structureless financial or other data streams .",
    "more particularly our objectives are to assess the following :    1 .",
    "using categorical or qualitative coding may allow structure , imperceptible with quantitative data , to be discovered .",
    "quantile - based categorical coding ( i.e. , the uniform prior case ) has beneficial properties , as will be demonstrated . but the issue of appropriate coding granularity , or scale of problem representation , remains , and we will address this issue below .",
    "2 .   in the case of a time - varying data signal ( which also holds for spatial data , _ mutatis mutandis _ ) non - respect of stationarity should be checked for : the consistency of our results will inform us about stationarity present in our data . more generally , structures ( or models or associations or relationships ) found in our data are validated through consistency of results obtained using subsets of the population studied .",
    "3 .   departure from average behavior is made easy in the analysis framework adopted .",
    "this amounts to fingerprinting the data , i.e.   determining patterns in the data that are characteristic of it .",
    "using crude oil data , ross ( 2003 ) shows how structure can be found in apparently geometric brownian motion , through data recoding . considering monthly oil price values , @xmath260 , and then @xmath261 , and finally @xmath262 , a histogram of @xmath263 for all @xmath264 should approximate a gaussian .",
    "the following recoding , though , gives rise to a somewhat different picture : response categories or states 1 , 2 , 3 , 4 are used for values of @xmath263 less than or equal to @xmath265 , between the latter and 0 , from 0 to @xmath266 , and greater than the latter .",
    "then a cross - tabulation of states 1 through 4 for @xmath267 , against states 1 through 4 for @xmath268 , is determined .",
    "the cross - tabulation can be expressed as a percentage . under geometric brownian",
    "motion , one would expect constant percentages .",
    "this is not what is found .",
    "instead there is appreciable structure in the contingency table .",
    "ross ( 2003 ) pursues exploration of a geometric brownian motion justification for black - scholes option cost .",
    "states - based pricing leads to greater precision compared to a one - state alternative .",
    "the number of states is left open with both a 4-state and a 6-state analysis discussed ( ross , 2003 , chap .",
    "a @xmath269 test of independence of the contingency table from a product of marginals is used with degrees of freedom associated with contingency table row and column dimensions : this provides a measure of how much structure we have , but not between alternative contingency tables .",
    "the latter is very fittingly addressed with the @xmath269 metric ( see murtagh , 2005 ) used in correspondence analysis : we can say that correspondence analysis is the transformation of pairwise @xmath269 distances into euclidean distances , and that the latter greatly facilitates visualization ( e.g. , low - dimensional projection ) and interpretation .",
    "the total inertia or trace of the data table grows with contingency table dimensionality , so that is of no direct help to us . for the futures data used below , and contingency tables of size @xmath270 , @xmath271 , @xmath272 , @xmath273 , and @xmath274",
    ", we find traces of value : 0.0118 , 0.0268 , 0.0275 , 0.0493 , and 0.0681 , respectively .",
    "barring the presence of low - dimensional patterns arising in such a sequence of contingency tables , we will _",
    "always _ find that greater dimensionality implies greater complexity ( quantified , e.g. , by trace ) and therefore structure .",
    "to address the issue of number of coding states to use , in order to search for latent structure in such data , one approach that seems very reasonable is to explore the dependencies and associations based on fine - grained structure ; and include in this exploration the possible aggregation of the fine - grained states .",
    "( aggregation of states in correspondence analysis is catered for through the property of distributional equivalence : see murtagh , 2005 , for discussion . )",
    "we take sets of 2500 values from the time series .",
    "tables [ table1 ] shows data to be analyzed derived from time series values 1 to 2500 ( identifier @xmath264 ) .",
    "further , we use similar cross - tabulations for values 3001 to 5500 ( identifier @xmath275 ) , 2001 to 4500 ( identifier @xmath276 ) , and values 3600 to 6100 ( identifier @xmath74 ) .    ....",
    "j1     j2     j3     j4     j5     j6     j7     j8     j9    j10   i1   23.29   7.23   8.84   6.02 14.86   1.20 10.44   8.84   8.43 10.84    i2   11.60 11.60 11.20   8.80 13.20   5.20 11.60   8.80   8.80   9.20   i3   10.00 13.20 10.80 12.80 14.40   2.00 12.80   5.60 10.80   7.60    i4    8.00   9.20   9.20 12.00 15.60   4.80 12.00 10.40   9.60   9.20   i5    7.50   9.50   9.75 11.00 22.25   5.25   7.50 10.25   9.00   8.00    i6    5.05   8.08   9.09 10.10 20.20   6.06   9.09 16.16   4.04 12.12    i7    4.80   9.60 12.40 11.60 21.60   2.40 10.40   9.20 10.40   7.60    i8    8.40   7.20   8.40 12.40 13.20   7.20   8.40 10.80 11.60 12.40    i9    8.40 12.00   8.40   6.80 15.60   2.00 10.00 13.60   9.60 13.60   i10   11.20 11.60 11.60   8.00   8.00   4.00   8.80 10.00 14.80 12.00 ....    figure [ plot1 ] shows the projections of the profiles in the plane of factors 1 and 2 , using all four data tables  one of which is shown in table [ table1 ] .",
    "the result is very consistent : cf .  how @xmath277 are tightly grouped , as are @xmath278 , reasonably so @xmath279 , and so on .",
    "the full space of all factors has to be used to verify the clustering seen in this planar ( least squares optimal ) projection .",
    "an analysis of clusters found is listed in table [ table5 ] .",
    "( contributions to , and correlations with , the principal factors are used : see murtagh , 2005 , for a discussion of where these may differ from projections onto the factors .",
    "projections , e.g.  as shown in figure [ plot1 ] , are descriptive : `` what is ? '' , but correlations and contributions point to influence : `` what causes ? '' . correlations and contributions",
    "are used therefore , in preference to projections . )    in cluster 65 , coding category 9 is predominant . in cluster 68 ,",
    "coding categories 2 and 3 are predominant .",
    "cluster 69 is mixed .",
    "cluster 70 is dominated by coding category 10 . in cluster 71 , coding category 8 is predominant .",
    "cluster 72 is defined by coding category 1 .",
    "finally , cluster 73 is dominated by coding category 5 .    ....",
    "clusters                        quantile coding category           cluster 65 : k9 n9 k7 n7 i4 m9               predominant : 9   cluster 68 : i3 k3 m3 m4 i2 m2 k2 n2         predominant : 2 , 3 cluster 69 : n6 i8 m7                        predominant : none cluster 70 : i10 m10 i9 k10 n10              predominant : 10 cluster 71 : i6 k4 n4 m8 k8 n8               predominant : 8 cluster 72 : i1 m1 k1 n1                     predominant : 1 cluster 73 : i5 m5 n3 k5 n5 k6 i7 m6         predominant : 5 ....    from the clustering , we provisionally retain coding categories 1 ; 2 and 3 together ; 5 ; 8 ; 9 ; and 10 .",
    "we flag response categories 4 , 6 , and 7 as being unclear and best avoided when our aim is prediction of the futures data .",
    "to check the coding relative to stationarity , we check that the global code boundaries are close to the time series sub - interval code boundaries .",
    "( see murtagh , 2005 , for more discussion on this , including confirmation of stationarity . ) in broad terms , what we are checking here is the consistency of the representative elements , found in different subsets of the data , as illustrated above , right at the start of our presentation in this article , in table [ pic1 ] .",
    "typical movements can be read off in percentage terms in a table such as table [ table1 ] .",
    "more atypical movements serve to define the strong patterns in our data .",
    "we consider the clusters of current time - step code categories numbered 65 , 68 , 69 , 70 , 71 , 72 , 73 from table [ table5 ] , and we ask what are the likely movements , for one time step . alternatively expressed the current code categories are defined at time step @xmath163 , and the one - step - ahead code categories are defined at time step @xmath280 .",
    "we find the following predominant movements in table [ table5 ] ( using a thresholded contribution value  not shown here ; we recall that `` contribution '' is used in the correspondence analysis sense , meaning mass times projection squared ) :    cluster 65 , i.e.  code category 9 : @xmath281 weakly 8 and more weakly 9 .",
    "cluster 68 , i.e.  code categories 2 and 3 : @xmath281 7 .",
    "cluster 69 , i.e.  mixed code categories : @xmath281 6 .",
    "cluster 70 , i.e.  code category 10 : @xmath281 10 .",
    "cluster 71 , i.e.  code category 8 : @xmath281 weakly 8 .",
    "cluster 72 , i.e.  code category 1 : @xmath281 1 .",
    "cluster 73 , i.e.  code category 5 : @xmath281 5    consider the situation of using these results in an operational setting . from informative structure , we have found that code category 1 ( values less than the 10th percentile , i.e.  very low ) has a tendency , departing from typical tendencies , to be prior to code category 1 ( again very low ) . from any or all of tables such as table [ table1 ]",
    "we can see how often we are likely to have this situation in practice : 19.04% (= average of 23.29% from table [ table1 ] , and 17.67% , 16.4% , and 18.8% , from the other analogous tables not shown here ) , given that we have code category 1 .    applying a similar fingerprinting analysis to ross s ( 2003 ) oil data , 749 values , we found that clustering the initial code categories did not make much sense : we retained therefore the trivial partition with all 10 code categories . for the output or one - step - ahead future code categories",
    ", we agglomerated 6 and 7 , and denoted this cluster as 11 .",
    "we find the following , generally weak , associations derived from the contributions .",
    "input code category 6 @xmath281 output code categories 1 , 10 ( weak ) .",
    "input code category 3 @xmath281 output code category 2 .",
    "input code category 4 @xmath281 output code category 4 .",
    "input code categories 9 , 2 @xmath281 output code category 5 ( weak ) .",
    "input code category 10 @xmath281 output code category 8 .",
    "not surprisingly , we find very different patterns in the two data sets of different natures used , the futures and the oil price signals .",
    "we have shown that structure can be discovered in data where such structure is not otherwise apparent .",
    "furthermore we have used correspondence analysis , availing of its spatial projection and clustering aspects , as a convenient analysis environment .",
    "validating the conclusions drawn is always most important , and this is facilitated by ( i ) semi - interactive data analysis , and ( ii ) consistency of results across subsets of the domain under investigation , @xmath0 .",
    "our new framework for data , and the handling of data ( including our defining of a normed vector space ) , could be considered in a sense as `` only '' formalizing standard data analysis practice .",
    "but in the exploration and analysis of complex phenomena ( cf .  the search for local structure and patterns in price movements ) we need to be sure of our belief in how our data express the underlying phenomena .",
    "the traditional kolmogorov approach based on lebesgue integration and sigma algebras of probability - measurable sets is unnecessarily abstract and therefore largely ignored by the `` engineering '' or pragmatic common sense of the data analyst .    in this article",
    "we have shown how the generalized riemann integral lends itself to a more transparent definition of probability , in line with empirical data analysis practice . as a foundation for our data analysis tasks",
    ", it achieves a far better cohesiveness between data , and data analyses , vis  vis the underlying phenomena .",
    "some of this work was carried out in the project `` integration methods in financial analysis '' , supported by the british council , uk , and the polish state committee for scientific research , kbn , poland .",
    "1 .   j.p . benzcri , lanalyse des donnes .",
    "lanalyse des correspondances , 2nd ed . , dunod , 1976 .",
    "2 .   j.p .",
    "benzcri , correspondence analysis handbook , marcel dekker , 1992 . 3 .",
    "r. gordon , the lebesgue , denjoy , perron and henstock integrals , american mathematical society , 1994 . 4 .",
    "r. henstock , lectures on the theory of integration , world scientific , singapore , 1988 .",
    "r.n . mantegna and h.e .",
    "stanley , an introduction to econophysics , cambridge university press , 2000 . 6 .",
    "p. muldowney , a general theory of integration in function spaces , pitman research notes in mathematics no .",
    "153 , harlow , 1988 . 7 .",
    "p. muldowney , topics in probability using generalised riemann integration , mathematical proceedings of the royal irish academy , 99(b)1 ( 1999 ) 3950 . 8 .",
    "p. muldowney , the henstock integral and the black - scholes theory of derivative asset pricing , real analysis exchange , 25(1 ) ( 2000/2001 ) 117132 9 .",
    "p. muldowney and v.a .",
    "skvortsov , lebesgue integrability implies generalized riemann integrability in @xmath282}$ ] , real analysis exchange , 27(1 ) ( 2001/2002 ) 223234 .",
    "f. murtagh , correspondence analysis and data coding with r and java , chapman and hall / crc press , 2005 .",
    "ross , an elementary introduction to mathematical finance , 2nd ed . , cambridge university press , 2003 .",
    "w. rudin , real and complex analysis , mcgraw - hill , new york , 1970 . 13 .",
    "velleman and l. wilkinson , nominal , ordinal , interval and ratio typologies are misleading , the american statistician , 47 ( 1993 ) 6572 ."
  ],
  "abstract_text": [
    "<S> practical data analysis involves many implicit or explicit assumptions about the good behavior of the data , and excludes consideration of various potentially pathological or limit cases . in this work </S>",
    "<S> , we present a new general theory of data , and of data processing , to bypass some of these assumptions . </S>",
    "<S> the new framework presented is focused on integration , and has direct applicability to expectation , distance , correlation , and aggregation . in a case study , we seek to reveal faint structure in financial data . our new foundation for data encoding and handling offers increased justification for our conclusions .    </S>",
    "<S> * keywords : * data coding , data encoding , data valuation , correspondence analysis , hierarchical clustering , geometric brownian motion , financial modeling , time series prediction , data aggregation </S>"
  ]
}