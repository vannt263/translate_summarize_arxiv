{
  "article_text": [
    "the context of the predictive maintenance of the french railway switches ( or points ) which enable trains to be guided from one track to another at a railway junction , we have been brought to extract features from switch operations signals representing the electrical power consumed during a point operation ( see fig .",
    "[ signal_intro ] ) .",
    "the final objective is to exploit these parameters for the identification of incipient faults .",
    "the switch operations signals can be seen as time series presenting non - linearities and various changes in regime .",
    "basic linear regression can not be adopted for this type of signals because a constant linear relationship is not adapted . as alternative to linear regression , some authors use approaches based on a piecewise regression model @xcite@xcite@xcite .",
    "piecewise regression is a segmentation method providing a partition of the data into @xmath0 segments , each segment being characterized by its mean curve ( constant , polynomial , ... ) and its variance in the gaussian case . under this type of modeling , the parameters estimation is generally based on a global optimization using dynamic programming @xcite like fisher s algorithm @xcite .",
    "this algorithm optimizes an additive criterion representing a cost function over all the segments of the signal @xcite@xcite . however , the dynamic programming procedure is known to be computationally expensive .",
    "an iterative algorithm can be derived to improve the running time of fisher s algorithm as in @xcite .",
    "this iterative approach is a local optimization approach estimating simultaneously the regression model parameters and the transition points .",
    "these two approaches will be recalled in our work , where the second one will be extended to supposing different variances for the various segments instead of using a constant variance for all the segments .",
    "other alternative approaches are based on hidden markov models @xcite in a context of regression @xcite where the model parameters are estimated by the baum - welch algorithm @xcite .",
    "the method we propose for feature extraction is based on a specific regression model incorporating a discrete hidden process allowing for abrupt or smooth transitions between various regression models .",
    "this approach has a connection with the switching regression model introduced by quandt and ramsey @xcite and is very linked to the mixture of experts ( me ) model introduced by jordan and jacobs @xcite by the using of a time - dependent logistic transition function .",
    "the me model , as discussed in @xcite , uses a conditional mixture modeling where the model parameters are estimated by the expectation maximization ( em ) algorithm @xcite@xcite .",
    "this paper is organized as follows .",
    "section 2 recalls the piecewise regression model and two techniques of parameter estimation using a dynamic programming procedure : the method of global optimization of fisher and its iterative variant .",
    "section 3 introduces the proposed model and section 4 describes the parameters estimation via the em algorithm .",
    "the fifth section is devoted to the experimental study using simulated and real data .",
    "let @xmath1 be @xmath2 real observations of a signal or a time serie where @xmath3 is observed at time @xmath4 .",
    "the piecewise regression model supposes that the signal presents unknown transition points whose indexes can be denoted by @xmath5 with @xmath6 and @xmath7 .",
    "this defines a partition @xmath8 of the time serie into @xmath0 segments of lengths @xmath9 such that : @xmath10 with @xmath11 and @xmath12\\gamma_{k},\\gamma_{k+1}]$ ] .",
    "thus , the piecewise regression model generating the signal @xmath13 is defined as follows : @xmath14 where @xmath15 , is the @xmath16-dimensional coefficients vector of a @xmath17 degree polynomial associated to the @xmath18 segment , @xmath19 , @xmath20 is the time dependent @xmath16-dimensional covariate vector associated to the parameter @xmath21 and the @xmath22 are independent random variables distributed according to a gaussian distribution with zero mean and unit variance representing an additive noise on each segment @xmath23 .      under this model ,",
    "the parameters estimation is performed by maximum likelihood .",
    "we assume a conditional independence of the data between the segments , and the data within a segment are also supposed to be conditionally independent .",
    "thus , according to the model ( [ eq.piecewise regression model ] ) , the log - likelihood of the parameter vector @xmath24 and the transition points @xmath25 characterizing the piecewise regression model is a sum of local log - likelihoods over all the segments and can be written as follows : @xmath26 where is the log - likelihood within the segment @xmath23 and @xmath27 is a constant .",
    "thus , the log - likelihood is finally written as : @xmath28 + \\mbox{c},\\end{aligned}\\ ] ] where @xmath29 is a constant .    maximizing this",
    "log - likelihood is equivalent to minimizing with respect to @xmath30 and @xmath31 the criterion @xmath32 \\nonumber\\\\ & = & \\sum_{k=1}^k j_k({\\boldsymbol{\\psi}},\\gamma_k,{\\boldsymbol{\\gamma}}_{k+1 } ) , \\label{eq.picewise_reg criterion}\\end{aligned}\\ ] ] where @xmath33 $ ] .",
    "the optimization algorithm of fisher is an algorithm based on dynamic programming , providing the optimal partition of the data by minimizing an additive criterion @xcite@xcite@xcite .",
    "this algorithm minimizes the criterion @xmath34 or equivalently minimizes , with respect to @xmath31 , the criterion @xmath35,\\nonumber \\\\ & = & \\sum_{k=1}^k c(\\gamma_k,\\gamma_{k+1 } ) , \\label{dynaic programming criterion}\\end{aligned}\\ ] ] with @xmath36 $ ] , where @xmath37 @xmath38^t$ ] being the regression matrix associated to @xmath39 , and @xmath40 @xmath41 being the number of points of the segment @xmath23 .",
    "it can be observed that the criterion @xmath42 is a sum of cost @xmath43 over the @xmath0 segments .",
    "therefore , due to the additivity of this criterion , its optimization can be performed using a dynamic programming procedure @xcite@xcite .",
    "dynamic programming considers that an optimal partition of the data into @xmath0 segments is the union of an optimal partition into @xmath44 segments and a set of one segment . by introducing the cost @xmath45,\\ ] ] with @xmath46 and @xmath47 ,",
    "the dynamic programming optimization algorithm runs as follows :      this step consists of computing the cost matrix @xmath48 corresponding to one segment @xmath49a , b]$ ] for @xmath46 .",
    "this cost matrix is computed as follows : @xmath50 \\nonumber\\\\ & = & \\sum_{i = a+1}^{b } \\big[\\log{\\hat{\\sigma}^{2 } } + \\frac{(x_i-\\hat{{\\boldsymbol{\\beta}}}^{t}{\\boldsymbol{r}}_i)^2}{\\hat{\\sigma}^2}\\big ] , \\label{dynamic programming criterion}\\end{aligned}\\ ] ] where @xmath51 and @xmath52 are computed respectively according to the equations ( [ estimation beta ] ) and ( [ estimation sigma ] ) by replacing @xmath49\\gamma_k,\\gamma_{k+1}]$ ] by @xmath49a , b]\\cdot$ ]      this step consists of computing the optimal cost @xmath53 for @xmath54 and @xmath46 using the following formula : @xmath55.\\ ] ]      from the optimal costs @xmath53 , the optimal partition can be deduced ( for more details see appendix a in @xcite ) .",
    "while the fisher algorithm provides the global optimum , it is known to be computationally expensive . to accelerate the convergence of this algorithm",
    ", one can derive an iterative variant as in @xcite .      in the iterative procedure ,",
    "the criterion @xmath56 given by equation ( [ eq.picewise_reg criterion ] ) is iteratively minimized by starting from an initial value of the transition points @xmath57 and alternating the two following steps until convergence :      compute the regression model parameters @xmath59 for the current values of the transition points @xmath60 by minimizing the criterion @xmath61 given by equation ( [ eq.picewise_reg criterion ] ) with respect to @xmath30 .",
    "this minimization consists of performing @xmath0 separated polynomial regressions and provides the following estimates : @xmath62 where @xmath63^t$ ] is the regression matrix associated to the elements of the @xmath18 segment @xmath64\\gamma_{k}^{(m)},\\gamma_{k+1}^{(m)}]\\}$ ] at the iteration @xmath58 , @xmath65      compute the transition points @xmath66 by minimizing the criterion @xmath56 for the current value of @xmath67 , with respect to @xmath31 .",
    "this minimization can be performed using a dynamic programming procedure since the criterion @xmath68 is additive .",
    "however , in contrast with the previous method , where the computation of the cost matrix @xmath48 requires the computation of the regression model parameter @xmath69 for @xmath46 , this iterative procedure simply uses the cost matrix computed with the current values of the parameters @xmath70 which improves the running time of the algorithm .",
    "the next section presents the proposed regression model with a hidden logistic process .",
    "we represent a signal by the random sequence @xmath71 of @xmath2 real observations , where @xmath3 is observed at time @xmath4 .",
    "this sample is assumed to be generated by the following regression model with a discrete hidden logistic process @xmath72 , where @xmath73 : @xmath74 in this model , @xmath75 is the @xmath16-dimensional coefficients vector of a @xmath17 degree polynomial , @xmath20 is the time dependent @xmath16-dimensional covariate vector associated to the parameter @xmath75 and the @xmath22 are independent random variables distributed according to a gaussian distribution with zero mean and unit variance .",
    "this model can be reformulated in a matrix form by @xmath76 where @xmath77 is a diagonal matrix whose diagonal elements are @xmath78 with @xmath79 if @xmath3 is generated by the @xmath18 regression model and @xmath80 otherwise , @xmath81^t$ ] is the @xmath82 $ ] matrix of covariates , and @xmath83 is the noise vector distributed according to a zero mean multidimensional gaussian density with identity covariance matrix .",
    "this section defines the probability distribution of the process @xmath72 that allows the switching from one regression model to another .",
    "the proposed hidden logistic process supposes that the variables @xmath84 , given the vector @xmath85 , are generated independently according to the multinomial distribution @xmath86 , where @xmath87 is the logistic transformation of a linear function of the time - dependent covariate @xmath88 , @xmath89 is the @xmath90-dimensional coefficients vector associated to the covariate @xmath91 and @xmath92 .",
    "thus , given the vector @xmath85 , the distribution of @xmath93 can be written as : @xmath94 where @xmath95 if @xmath96 i.e when @xmath3 is generated by the @xmath18 regression model , and @xmath80 otherwise .",
    "the pertinence of the logistic transformation in terms of flexibility of transition can be illustrated through simple examples with @xmath97 components .",
    "the first example is designed to show the effect of the dimension @xmath98 of @xmath99 on the temporal variation of the probabilities @xmath100 .",
    "we consider different values of the dimension @xmath98 ( @xmath101 ) of @xmath99 . in that case",
    ", only the probability @xmath102 should be described , since @xmath103 . as shown in fig .",
    "[ logistic_function_k=2_q=012 ] , the dimension @xmath98 controls the number of changes in the temporal variations of @xmath104 .",
    "in fact , the larger the dimension of @xmath99 , the more complex the temporal variation of @xmath100 .",
    "more particularly , if the goal is to segment the signals into convex segments , the dimension @xmath98 of @xmath99 must be set to @xmath105 .    [ cols=\"^,^ \" , ]     to illustrate the signal generation model , we generate two signals according to the proposed model using the parameters estimated by the em algorithm .",
    "it can be seen that the generated signals are very similar to the original signals ( see fig .",
    "[ genereted signals ] ) .",
    "in this paper a new approach for feature extraction from time series signals , in the context of the railway switch mechanism monitoring , has been proposed .",
    "this approach is based on a regression model incorporating a discrete hidden logistic process .",
    "the logistic probability function , used for the hidden variables , allows for smooth or abrupt transitions between polynomial regressive components over time .",
    "in addition to signals parametrization , an accurate denoising and segmentation of signals can be derived from the proposed model .",
    "the experiments applied to real and simulated data have shown good performances of the proposed approach compared to two algorithms devoted to the piecewise regression .",
    "w. d. fisher ,  on grouping for maximum homogeneity , \" _ journal of american statistics . society _ 53 , 789 - 798 , 1958 .",
    "b. krishnapuram , l. carin , m.a.t .",
    "figueiredo and a.j .",
    "hartemink ,  sparse multinomial logistic regression : fast algorithms and generalization bounds , \" _ ieee transactions on pattern analysis and machine intelligence , _",
    "27(6 ) : 957 - 968 , june 2005 .",
    "baum , t. petrie , g. soules and n. weiss ,  a maximization technique occurring in the statistical analysis of probabilistic functions of markov chains , \" _ annals of mathematical statistics _",
    ", 41 : 164 - 171 , 1970 .",
    "s. r. waterhouse , _ classification and regression using mixtures of experts , _ phd thesis , department of engineering , cambridge university , 1997 .",
    "y. lechevalier , optimal clustering on ordered set , technical report , the french national institute for research in computer science and control ( inria ) , 1990 .",
    "v. l. brailovsky and y. kempner ,  application of piece - wise regression to detecting internal structure of signal , \" _ pattern recognition _",
    ", 25(11 ) , 1361 - 1370 , november 1992 ."
  ],
  "abstract_text": [
    "<S> a new approach for feature extraction from time series is proposed in this paper . </S>",
    "<S> this approach consists of a specific regression model incorporating a discrete hidden logistic process . </S>",
    "<S> the model parameters are estimated by the maximum likelihood method performed by a dedicated expectation maximization ( em ) algorithm . </S>",
    "<S> the parameters of the hidden logistic process , in the inner loop of the em algorithm , are estimated using a multi - class iterative reweighted least - squares ( irls ) algorithm . a piecewise regression algorithm and its iterative variant </S>",
    "<S> have also been considered for comparisons . </S>",
    "<S> an experimental study using simulated and real data reveals good performances of the proposed approach . </S>"
  ]
}