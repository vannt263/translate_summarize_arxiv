{
  "article_text": [
    "as a branch of convex optimization theory , geometric programming is next in line to linear and quadratic programming in importance @xcite .",
    "it has applications in chemical equilibrium problems @xcite , structural mechanics @xcite , integrated circuit design @xcite , maximum likelihood estimation @xcite , stochastic processes @xcite , and a host of other subjects @xcite . geometric programming deals with posynomials , which are functions of the form @xmath0 here the index set @xmath1 is finite , and all coefficients @xmath2 and all components @xmath3 of the argument @xmath4 of @xmath5 are positive .",
    "the possibly fractional powers @xmath6 corresponding to a particular @xmath7 may be positive , negative , or zero . for instance , @xmath8 is a posynomial on @xmath9 . in geometric programming",
    "we minimize a posynomial @xmath5 subject to posynomial inequality constraints of the form @xmath10 for @xmath11 , where the @xmath12 are again posynomials . in some versions of geometric programming , equality constraints of posynomial type are permitted @xcite .",
    "a signomial function has the same form as the posynomial ( [ general_posynomial ] ) , but the coefficients @xmath2 are allowed to be negative .",
    "a signomial program is a generalization of a geometric program , where the objective and constraint functions can be signomials . from a computational point of view , signomial programming problems are significantly harder to solve than geometric programming problems . after suitable change of variables ,",
    "a geometric program can be transformed into a convex optimization problem and globally solved by standard methods .",
    "in contrast , signomials may have many local minima .",
    "wang et al .",
    "@xcite recently derived a path algorithm for solving unconstrained signomial programs .",
    "the theory and practice of geometric programming has been stable for a generation , so it is hard to imagine saying anything novel about either .",
    "the attractions of geometric programming include its beautiful duality theory and its connections with the arithmetic - geometric mean inequality .",
    "the present paper derives new algorithms for both geometric and signomial programming based on a generic device for iterative optimization called the mm algorithm @xcite .",
    "the mm perspective possesses several advantages .",
    "first it provides a unified framework for solving both geometric and signomial programs .",
    "the algorithms derived here operate by separating parameters and reducing minimization of the objective function to a sequence of one - dimensional minimization problems .",
    "separation of parameters is apt to be an advantage in high - dimensional problems .",
    "another advantage is ease of implementation compared to competing methods of unconstrained geometric and signomial programming @xcite . finally , straightforward generalizations of our mm algorithms extend beyond signomial programming .",
    "we conclude this introduction by sketching a roadmap to the rest of the paper .",
    "section [ sec : mm ] reviews the mm algorithm .",
    "section [ sec : mm - unconst ] derives mm algorithm for unconstrained signomial program from two simple inequalities .",
    "the behavior of the mm algorithm is illustrated on a few numerical examples in section [ sec : unconst - examples ] .",
    "section [ sec : mm - constr ] extends the mm algorithm for unconstrained problems to the constrained cases using the penalty method .",
    "section [ sec : constr - examples ] specializes to linearly constrained quadratic programming on the positive orthant .",
    "convergence results are discussed in section [ sec : convergence ] .",
    "the mm principle involves majorizing the objective function @xmath5 by a surrogate function @xmath13 around the current iterate @xmath14 ( with @xmath15th component @xmath16 ) of a search .",
    "majorization is defined by the two conditions @xmath17 in other words , the surface @xmath18 lies above the surface @xmath19 and is tangent to it at the point @xmath20 .",
    "construction of the majorizing function @xmath13 constitutes the first m of the mm algorithm .",
    "the second m of the algorithm minimizes the surrogate @xmath13 rather than @xmath5 . if @xmath21 denotes the minimizer of @xmath13 , then this action forces the descent property @xmath22 .",
    "this fact follows from the inequalities @xmath23 reflecting the definition of @xmath21 and the tangency conditions ( [ majorization_definition ] ) .",
    "the descent property lends the mm algorithm remarkable numerical stability . strictly speaking",
    ", it depends only on decreasing @xmath13 , not on minimizing @xmath13 .",
    "the art in devising an mm algorithm revolves around intelligent choice of the majorizing function . for signomial programming problems ,",
    "fortunately one can invoke two simple inequalities . for terms with positive coefficients",
    "@xmath2 , we use the arithmetic - geometric mean inequality @xmath24 for nonnegative numbers @xmath25 and @xmath6 and @xmath26 norm @xmath27 @xcite .",
    "if we make the choice @xmath28 in inequality ( [ original - arithmetic - geometric ] ) , then the majorization @xmath29 emerges , with equality when @xmath30 .",
    "we can broaden the scope of the majorization ( [ majorizing - arithmetic - geometric ] ) to cases with @xmath31 by replacing @xmath25 by the reciprocal ratio @xmath32 whenever @xmath31 .",
    "thus , for terms @xmath33 with @xmath34 , we have the majorization @xmath35 where @xmath36 is the sign function .",
    "the terms @xmath33 with @xmath37 are handled by a different majorization .",
    "our point of departure is the supporting hyperplane minorization @xmath38 at the point @xmath39 .",
    "if we let @xmath40 , then it follows that @xmath41 is a valid minorization in @xmath4 around the point @xmath14 .",
    "multiplication by the negative coefficient @xmath2 now gives the desired majorization .",
    "the surrogate function separates parameters and is convex when all of the @xmath6 are positive .    in summary ,",
    "the objective function ( [ general_posynomial ] ) is majorized up to an irrelevant additive constant by the sum @xmath42 where @xmath43 , and @xmath44 . to guarantee that the next iterate is well defined and occurs on the interior of the parameter domain , it is helpful to assume for each @xmath15 that at least one @xmath45 has @xmath6 positive and at least one @xmath45 has @xmath6 negative . under these conditions",
    "each @xmath46 is coercive and attains its minimum on the open interval @xmath47 .",
    "minimization of the majorizing function is straightforward because the surrogate functions @xmath46 are univariate functions .",
    "the derivative of @xmath46 with respect to its left argument equals @xmath48 assuming that the exponents @xmath6 are integers , this is a rational function of @xmath49 , and once we equate it to 0 , we are faced with solving a polynomial equation .",
    "this task can be accomplished by bisection or by newton s method .    in a geometric program , the function @xmath50 has a single root on the interval @xmath47 . for a proof of this",
    "fact , note that making the standard change of variables @xmath51 eliminates the positivity constraint @xmath52 and renders the transformed function @xmath53 strictly convex .",
    "because @xmath54 , the second derivative @xmath55 is positive . hence , @xmath56 is strictly convex and possesses a unique minimum point .",
    "these arguments yield the even sweeter dividend that the mm iteration map is continuously differentiable . from the vantage point of the implicit function theorem @xcite , the stationary condition @xmath57 determines @xmath58 , and consequently @xmath59 , in terms of @xmath14 .",
    "observe here that @xmath60 as required by the implicit function .",
    "it is also worth pointing out that even more functions can be brought under the umbrella of signomial programming .",
    "for instance , majorization of the functions @xmath61 and @xmath62 is possible for any posynomial @xmath63 . in the first case , @xmath64 \\label{log_complication1}\\end{aligned}\\ ] ] holds for @xmath65 and @xmath66 because jensen s inequality applies to the convex function @xmath67 . in the second case , the supporting hyperplane inequality applied to the convex function @xmath67 implies @xmath68.\\end{aligned}\\ ] ] this puts us back in the position of needing to majorize a posynomial , a problem we have already discussed in detail . by our previous remarks , the coefficients @xmath2 can be negative as well as positive in this case .",
    "similar majorizations apply to any composition @xmath69 of a posynomial @xmath5 with an arbitrary concave function @xmath70 .",
    "our first examples demonstrate the robustness of the mm algorithms in minimization and illustrate some of the complications that occur . in each case",
    "we can explicitly calculate the mm updates .",
    "to start , consider the posynomial @xmath71 with the implied constraints @xmath72 and @xmath73 .",
    "the majorization ( [ majorizing - arithmetic - geometric ] ) applied to the third term of @xmath74 yields @xmath75 \\\\ & = & \\frac{x_{m2}}{2 x_{m1 } } x_1 ^ 2+\\frac{x_{m1}}{2 x_{m2}}x_2 ^ 2.\\end{aligned}\\ ] ] applied to the second term of @xmath74 using the reciprocal ratios , it gives @xmath76 \\\\ & = &   { x_{m1}^2 \\over x_{m2}^2 } { 1 \\over x_1 ^ 3}+{2 x_{m2 } \\over x_{m1}}{1 \\over x_2 ^ 3}.\\end{aligned}\\ ] ] the sum @xmath13 of the two surrogate functions @xmath77 majorizes @xmath74 .",
    "if we set the derivatives @xmath78 of each of these equal to 0 , then the updates @xmath79{3\\left({x_{m1}^2 \\over x_{m2}^2}+1\\right){x_{m1 } \\over x_{m2 } } } , \\quad \\quad x_{m+1,2 } \\;\\;\\ , = \\;\\;\\",
    ", \\sqrt[5]{6 { x_{m2}^2 \\over x_{m1}^2}}\\end{aligned}\\ ] ] solve the minimization step of the mm algorithm .",
    "it is also obvious that the point @xmath80{6},\\sqrt[5]{6})^t$ ] is a fixed point of the updates , and the reader can check that it minimizes @xmath74 .",
    "it is instructive to consider the slight variations @xmath81 of this objective function . in the first case",
    ", the reader can check that the mm algorithm iterates according to @xmath82{{x_{m1}^2 \\over x_{m2}^2 } } , \\quad \\quad x_{m+1,2 } \\;\\;\\",
    ", = \\;\\;\\ , \\sqrt[3]{{x_{m2 } \\over x_{m1 } } } .\\end{aligned}\\ ] ] in the second case , it iterates according to @xmath83{{x_{m1}^3 \\over x_{m2}^3 } } , \\quad \\quad x_{m+1,2 } \\;\\;\\ , = \\;\\;\\ , \\sqrt[5]{2 { x_{m2}^2 \\over x_{m1}^2 } } .\\end{aligned}\\ ] ] the objective function @xmath84 attains its minimum value whenever @xmath85 .",
    "the mm algorithm for @xmath84 converges after a single iteration to the value 2 , but the converged point depends on the initial point @xmath86 .",
    "the infimum of @xmath87 is 0 .",
    "this value is attained asymptotically by the mm algorithm , which satisfies the identities @xmath88 and @xmath89 for all @xmath90 .",
    "these results imply that @xmath91 tends to 0 and @xmath92 to @xmath93 in such a manner that @xmath94 tends to 0 .",
    "one could not hope for much better behavior of the mm algorithm in these two examples .",
    "the function @xmath95 is a signomial but not a posynomial .",
    "the surrogate function ( [ signomial_majorizer ] ) reduces to @xmath96 with all variables separated .",
    "the mm updates @xmath97{\\frac{x_{m1}^3x_{m3}x_{m4}}{x_{m2 } } } , \\quad \\quad x_{m+1,2 } \\;\\ ; = \\;\\ ; \\sqrt[4]{\\frac{x_{m2}^3x_{m3}x_{m4}}{x_{m1 } } }   \\\\ x_{m+1,3 } & = & \\sqrt[4]{\\frac{x_{m3}^3x_{m1}x_{m2}}{x_{m4 } } } , \\quad \\quad x_{m+1,4 } \\;\\ ; = \\;\\ ; \\sqrt[4]{\\frac{x_{m4}^3x_{m1}x_{m2}}{x_{m3 } } } \\end{aligned}\\ ] ] converge in a single iteration to a solution of @xmath98 . again",
    "the limit depends on the initial point .",
    "the function @xmath99 is more complicated than a signomial .",
    "it also is unbounded because the point @xmath4 with components @xmath100 and @xmath101 satisfies @xmath102 . according to the majorization ( [ log_complication1 ] )",
    ", an appropriate surrogate is @xmath103 up to an irrelevant constant .",
    "the mm updates are @xmath104 if the components of the initial point coincide , then the iterates converge in a single iteration to the saddle point with all components equal to @xmath105 .",
    "otherwise , it appears that @xmath106 tends to @xmath107 .",
    "the following objective functions @xmath108 from the reference @xcite are intended for numerical illustration .",
    "table [ table : unconstr - examples ] lists initial conditions , minimum points , minimum values , and number of iterations until convergence under the mm algorithm .",
    "convergence is declared when the relative change in the objective function is less than a pre - specified value @xmath109 , in other words , when @xmath110 optimization of the univariate surrogate functions easily succumbs to newton s method .",
    "the mm algorithm takes fewer iterations to converge than the path algorithm for all of the test functions mentioned in @xcite except @xmath111 .",
    "furthermore , the mm algorithm avoids calculation of the gradient and hessian and requires no matrix decompositions or selection of tuning constants .    as section [ sec : convergence ] observes ,",
    "mm algorithms typically converge at a linear rate .",
    "although slow convergence can occur for functions such as the test function @xmath111 , there are several ways to accelerate an mm algorithm .",
    "for example , our published quasi - newton acceleration @xcite often reduces the necessary number of iterations by one or two orders of magnitude .",
    "figure [ fig : test6-qn ] shows the progress of the mm iterates for the test function @xmath111 with and without quasi - newton acceleration . under a convergence criterion of @xmath112 and @xmath113 secant condition , the required number of iterations falls to 30 ; under the same convergence criterion and @xmath114 secant conditions , the required number of iterations falls to 12 .",
    "it is also worth emphasizing that separation of parameters enables parallel processing in high - dimensional problems .",
    "we have recently argued @xcite that the best approach to parallel processing is through graphics processing units ( gpus ) .",
    "these cheap hardware devices offer one to two orders of magnitude acceleration in many mm algorithms with parameters separated .",
    "@xmath115{test01-surface.eps } & \\includegraphics[width=2.3in]{test01-contour - q0.eps }     \\\\ \\includegraphics[width=2.3in]{test01-contour - q1.eps } & \\includegraphics[width=2.3in]{test01-contour - q2.eps } \\end{array}\\ ] ]",
    "extending the mm algorithm to constrained geometric and signomial programming is challenging .",
    "box constraints @xmath116 are consistent with parameter separation as just developed , but more complicated posynomial constraints that couple parameters are not .",
    "posynomial inequality constraints take the form @xmath117 the corresponding equality constraint sets @xmath118 .",
    "we propose handling both constraints by penalty methods . before we treat these matters in more depth ,",
    "let us relax the positivity restrictions on the @xmath119 but enforce the restriction @xmath120 .",
    "the latter objective can be achieved by multiplying @xmath121 by @xmath122 for all @xmath15 .",
    "if we subtract the two sides of the resulting equality , then the equality constraint @xmath118 can be rephrased as @xmath123 , with no restriction on the signs of the @xmath124 but with the requirement @xmath125 in effect .",
    "for example , the equality constraint @xmath126 becomes @xmath127    in the quadratic penalty method @xcite with objective function @xmath5 and a single equality constraint @xmath128 and a single inequality constraint @xmath129 , one minimizes the sum @xmath130 , where @xmath131 .",
    "as the penalty constant @xmath132 tends to @xmath93 , the solution vector @xmath133 typically converges to the constrained minimum . in the revised objective function ,",
    "the term @xmath134 is a signomial whenever @xmath135 is a signomial .",
    "for example , in our toy problem the choice @xmath136 has square @xmath137 of course , the powers in @xmath135 can be fractional here as well as integer .",
    "the term @xmath138 is not a signomial and must be subjected to the majorization @xmath139 ^ 2    & s({\\boldsymbol{x}}_m ) < 0 \\\\",
    "s({\\boldsymbol{x}})^2 & s({\\boldsymbol{x}}_m ) \\ge 0      \\end{cases}\\end{aligned}\\ ] ] to achieve this status . in practice",
    ", one does not need to fully minimize @xmath140 for any fixed @xmath132 .",
    "if one increases @xmath132 slowly enough , then it usually suffices to merely decrease @xmath140 at each iteration .",
    "the mm algorithm is designed to achieve precisely this goal .",
    "our exposition so far suggests that we majorize @xmath134 , @xmath141 , and @xmath142 ^ 2 $ ] in exactly the same manner that we majorize @xmath5 .",
    "separation of parameters generalizes , and the resulting mm algorithm keeps all parameters positive while permitting pertinent parameters to converge to 0 .",
    "section [ sec : convergence ] summarizes some of the convergence properties of this hybrid procedure .",
    "the quadratic penalty method traditionally relies on newton s method to minimize the unconstrained functions @xmath140 .",
    "unfortunately , this tactic suffers from roundoff errors and numerical instability .",
    "some of these problems disappear with the mm algorithm .",
    "no matrix inversions are involved , and iterates enjoy the descent property .",
    "ill - conditioning does cause harm in the form of slow convergence , but the previously mentioned quasi - newton acceleration largely remedies the situation @xcite . as an alternative to quadratic penalties , exact penalties",
    "take the form @xmath143 .",
    "remarkably , the exact penalty method produces the constrained minimum , not just in the limit , but for all finite @xmath132 beyond a certain point .",
    "although this desirable property avoids the numerical instability encountered in the quadratic penalty method , the kinks in the objective functions @xmath144 are a nuisance .",
    "we will demonstrate in a future paper how to harness the mm algorithm to exact penalization .",
    "as an illustration of constrained signomial programming , consider quadratic programming over the positive orthant .",
    "let @xmath145 be the objective function , @xmath146 the linear equality constraints , and @xmath147 the linear inequality constraints .",
    "the symmetric matrix @xmath148 can be negative definite , indefinite , or positive definite .",
    "the quadratic penalty method involves minimizing the sequence of penalized objective functions @xmath149 as @xmath132 tends to @xmath93 .",
    "based on the obvious majorization @xmath150 the term @xmath151 is majorized by @xmath152 , where @xmath153 a brief calculation shows that @xmath140 is majorized by the surrogate function @xmath154 up to an irrelevant constant , where @xmath155 and @xmath156 are defined by @xmath157 it is convenient to assume that the diagonal coefficients @xmath158 appearing in the quadratic form @xmath159 are positive .",
    "this is generally the case for large @xmath132 .",
    "one can handle the off - diagonal term @xmath160 by either the majorization ( [ majorizing - arithmetic - geometric ] ) or the majorization ( [ eqn : supp - hp - maj ] ) according to the sign of @xmath161 . the reader can check that the mm updates reduce to @xmath162 , \\label{eqn : qp - update2}\\end{aligned}\\ ] ] where @xmath163 when @xmath164 , the update ( [ eqn : qp - update2 ] ) collapses to @xmath165 to avoid sticky boundaries , we replace 0 in equation ( [ eqn : qp - update3 ] ) by a small positive constant @xmath109 such as @xmath166 .",
    "sha et al .",
    "@xcite derived the update ( [ eqn : qp - update2 ] ) for @xmath167 ignoring the constraints @xmath146 and @xmath147 .    for a numerical example without equality constraints take @xmath168 the minimum occurs at the point @xmath169 .",
    "table [ table : dummy2-quadpen ] lists the number of iterations until convergence and the converged point @xmath170 for the sequence of penalty constants @xmath171 .",
    "the quadratic program @xmath172 converges much more slowly .",
    "its minimum occurs at the point @xmath173 .",
    "table [ table : dummy - quadpen ] lists the numbers of iterations until convergence with @xmath174 ) and without ( @xmath175 ) acceleration and the converged point @xmath170 for the same sequence of penalty constants @xmath171 .",
    "fortunately , quasi - newton acceleration compensates for ill conditioning in this test problem .",
    ".iterates from the quadratic penalty method for the test function @xmath176 .",
    "the convergence criterion for the inner loops is @xmath166 . [",
    "cols=\">,>,^\",options=\"header \" , ]",
    "as we have seen , the behavior of the mm algorithm is intimately tied to the behavior of the objective function @xmath5 . for the sake of simplicity",
    ", we now restrict attention to unconstrained minimization of posynomials and investigate conditions guaranteeing that @xmath5 possesses a unique minimum on its domain .",
    "uniqueness is related to the strict convexity of the reparameterization @xmath177 of @xmath5 , where @xmath178 is the inner product of @xmath7 and @xmath179 and @xmath180 for each @xmath15 .",
    "the hessian matrix @xmath181 of @xmath182 is positive semidefinite , so @xmath182 is convex .",
    "if we let @xmath183 be the subspace of @xmath184 spanned by @xmath185 , then @xmath182 is strictly convex if and only if @xmath186 . indeed , suppose the condition holds .",
    "for any @xmath187 , we then must have @xmath188 for some @xmath189 .",
    "it follows that @xmath190 and @xmath191 is positive definite .",
    "conversely , suppose @xmath192 , and take @xmath187 with @xmath193 for every @xmath189 .",
    "then @xmath194 for every scalar @xmath195 , which is incompatible with @xmath182 being strictly convex .",
    "strict convexity guarantees uniqueness , not existence , of a minimum point .",
    "coerciveness ensures existence .",
    "the objective function @xmath5 is coercive if @xmath5 tends to @xmath93 whenever any component of @xmath4 tends to 0 or @xmath93 . under the reparameterization @xmath180 ,",
    "this is equivalent to @xmath196 tending to @xmath93 as @xmath197 tends to @xmath93 .",
    "a necessary and sufficient condition for this to occur is that @xmath198 for every @xmath187 . for a proof , suppose the contrary condition holds for some @xmath187 .",
    "then it is clear that @xmath199 remains bounded above by @xmath200 as the scalar @xmath195 tends to @xmath93 .",
    "conversely , if the stated condition is true , then the function @xmath201 is continuous and achieves its minimum of @xmath202 on the sphere @xmath203 .",
    "it follows that @xmath204 and that @xmath205 this lower bound shows that @xmath182 is coercive .",
    "the coerciveness condition is hard to apply in practice .",
    "an equivalent condition is that the origin @xmath206 belongs to the interior of the convex hull of the set @xmath185 .",
    "it is straightforward to show that the negations of these two conditions are logically equivalent .",
    "thus , suppose @xmath207 for some @xmath187 .",
    "every convex combination @xmath208 then satisfies @xmath209 .",
    "if the origin is in the interior of the convex hull , then @xmath210 is also for every sufficiently small @xmath211 . but this leads to the contradiction @xmath212 .",
    "conversely , suppose @xmath206 is not in the interior of the convex hull . according to the separating hyperplane theorem for convex sets , there exists a unit vector @xmath213 with @xmath214 for every @xmath189 .",
    "in other words , @xmath215 . the convex hull criterion is easier to check , but it is not constructive . in simple cases such as the objective function @xmath74 where the power vectors are @xmath216 , @xmath217 , and @xmath218 , it is visually obvious that the origin is in the interior of their convex hull .",
    "one can also check the criterion @xmath219 for all @xmath187 by solving a related geometric programming problem .",
    "this problem consists in minimizing the scalar @xmath195 subject to the inequality constraints @xmath220 for all @xmath189 and the nonlinear equality constraint @xmath221 . if @xmath222 , then the original criterion fails .    in some cases , the objective function @xmath5 does not attain its minimum on the open domain @xmath223",
    "this condition is equivalent to the corresponding function @xmath224 being unbounded below on @xmath184 . according to gordon s theorem @xcite",
    ", this can happen if and only if @xmath206 is not in the convex hull of the set @xmath185 .",
    "alternatively , both conditions are equivalent to the existence of a vector @xmath213 with @xmath225 for all @xmath189 . for the objective function @xmath87 ,",
    "the power vectors are @xmath217 and @xmath218 .",
    "the origin @xmath226 does not lie on the line segment between them , and the vector @xmath227 forms a strictly oblique angle with each . as predicted , @xmath87 does not attain its infimum on @xmath228 .",
    "the theoretical development in reference @xcite demonstrates that the mm algorithm converges at a linear rate to the unique minimum point of the objective function @xmath5 when @xmath5 is coercive and its convex reparameterization @xmath182 is strictly convex",
    ". the theory does not cover other cases , and it would be interesting to investigate them . the general convergence theory of mm algorithms @xcite states that five properties of the objective function @xmath5 and mm algorithmic map @xmath229 guarantee convergence to a stationary point of @xmath5 :",
    "( a ) @xmath5 is coercive on its open domain ; ( b ) @xmath5 has only isolated stationary points ; ( c ) @xmath230 is continuous ; ( d ) @xmath231 is a fixed point of @xmath230 if and only if @xmath231 is a stationary point of @xmath5 ; and ( e ) @xmath232 \\ge f({\\boldsymbol{x}}^*)$ ] , with equality if and only if @xmath231 is a fixed point of @xmath230 . for a general signomial program ,",
    "items ( a ) and ( b ) are the hardest to check .",
    "our examples provide some clues .",
    "the standard convergence results for the quadratic penalty method are covered in the references @xcite . to summarize the principal finding , suppose that the objective function @xmath5 and the constraint functions @xmath233 and @xmath234 are continuous and that @xmath5 is coercive on @xmath228 . if @xmath170 minimizes the penalized objective function @xmath235 and @xmath236 is a cluster point of @xmath170",
    "as @xmath132 tends to @xmath93 , then @xmath236 minimizes @xmath5 subject to the constraints . in this regard",
    "observe that the coerciveness assumption on @xmath5 implies that the solution set @xmath237 is bounded and possesses at least one cluster point .",
    "of course , if the solution set consists of a single point , then @xmath133 tends to that point .",
    "the current paper presents novel algorithms for both geometric and signomial programming .",
    "although our examples are low dimensional , the previous experience of sha et al .",
    "@xcite offers convincing evidence that the mm algorithm works well for high - dimensional quadratic programming with nonnegativity constraints .",
    "the ideas pursued here  the mm principle , separation of variables , quasi - newton acceleration , and penalized optimization  are surprisingly potent in large - scale optimization .",
    "the mm algorithm deals with the objective function directly and reduces multivariate minimization to a sequence of one - dimensional minimizations .",
    "the mm updates are simple to code and enjoy the crucial descent property .",
    "treating constrained signomial programming by the penalty method extends the mm algorithm even further .",
    "quadratic programming with linear equality and inequality constraints is the most important special case of constrained signomial programming .",
    "our new mm algorithm for constrained quadratic programming deserves consideration in high - dimensional problems .",
    "even though mm algorithms can be notoriously slow to converge , quasi - newton acceleration can dramatically improve matters .",
    "acceleration involves no matrix inversion , only matrix times vector multiplication .",
    "finally , it is worth keeping in mind that parameter separated algorithms are ideal candidates for parallel processing .",
    "because geometric programs are ultimately convex , it is relatively easy to pose and check sufficient conditions for global convergence of the mm algorithm .",
    "in contrast it is far more difficult to analyze the behavior of the mm algorithm for signomial programs .",
    "theoretical progress will probably be piecemeal and require problem - specific information .",
    "a major difficulty is understanding the asymptotic nature of the objective function as parameters approach 0 or @xmath93 . even in the absence of theoretical guarantees ,",
    "the descent property of the mm algorithm makes it an attractive solution technique and a diagnostic tool for finding counterexamples .",
    "some of our test problems expose the behavior of the mm algorithm in non - standard situations .",
    "we welcome the help of the optimization community in unraveling the mysteries of the mm algorithm in signomial programming .",
    "f.  sha , l.k .",
    "saul , and d.d .",
    "multiplicative updates for nonnegative quadratic programming in support vector machines . in s. becker , s. thrun , and k. obermayer ( eds . ) , _ advances in neural information processing systems",
    "_ 15 , pages 1065 - 1073 .",
    "mit press : cambridge , ma ."
  ],
  "abstract_text": [
    "<S> this paper derives new algorithms for signomial programming , a generalization of geometric programming . </S>",
    "<S> the algorithms are based on a generic principle for optimization called the mm algorithm . in this </S>",
    "<S> setting , one can apply the geometric - arithmetic mean inequality and a supporting hyperplane inequality to create a surrogate function with parameters separated . </S>",
    "<S> thus , unconstrained signomial programming reduces to a sequence of one - dimensional minimization problems . </S>",
    "<S> simple examples demonstrate that the mm algorithm derived can converge to a boundary point or to one point of a continuum of minimum points . </S>",
    "<S> conditions under which the minimum point is unique or occurs in the interior of parameter space are proved for geometric programming . </S>",
    "<S> convergence to an interior point occurs at a linear rate . </S>",
    "<S> finally , the mm framework easily accommodates equality and inequality constraints of signomial type . for the most important special case , constrained quadratic programming , </S>",
    "<S> the mm algorithm involves very simple updates .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}