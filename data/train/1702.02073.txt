{
  "article_text": [
    "[ [ data ] ] * data * + + + + + +    the state of oklahoma ( usa ) has recently experienced a dramatic surge in seismic activity  @xcite that has been correlated with the intensification of waste water injection  @xcite . here",
    ", we focus on the particularly active area near guthrie ( oklahoma ) . in this region , the oklahoma state geological survey ( ogs ) cataloged 2021 seismic events from 15 february 2014 to 16 november 2016 ( see figure  [ fig : mapevents ] ) .",
    "their seismic moment magnitudes range from @xmath0 -0.2 to @xmath0 5.8 .",
    "we use the continuous ground velocity records from two local stations gs.ok027 and gs.ok029 ( see figure  [ fig : mapevents ] ) .",
    "gs.ok027 was active from 14 february 2014 to 3 march 2015 .",
    "gs.ok029 was deployed on 15 february 2014 and has remained active since .",
    "signals from both stations are recorded at 100 hz on 3 channels corresponding to the three spatial dimensions : hhz oriented vertically , hhn oriented north - south and hhe oriented west - east .",
    "[ [ generating - location - labels ] ] * generating location labels * + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we partition the 2021 earthquakes into 6 geographic _",
    "clusters_. for this we use the k - means algorithm @xcite , with the euclidean distance between epicenters as the metric .",
    "the centrods of the clusters we obtain define 6 areas on the map ( figure  [ fig : mapevents ] ) .",
    "any point on the map is assigned to the cluster whose centrod is the closest ( i.e. , each point is assigned to its vorono cell ) .",
    "we find that 6 clusters allow for a reasonable partition of the major earthquake sequences .",
    "our classification thus contains 7 labels , or _ classes _ in the machine learning terminology : class 0 corresponds to seismic noise without any earthquake , classes 1 to 6 correspond to earthquakes originating from the corresponding geographic area .    [ [ extracting - windows - for - classification ] ] * extracting windows for classification * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we divide the continuous waveform data into monthly _",
    "streams_. we normalize each stream individually by subtracting the mean over the month and dividing by the absolute peak amplitude ( independently for each of the 3 channels ) .",
    "we extract two types of 10 second long _ windows _ from these streams : windows containing events and windows free of events ( i.e.  containing only seismic noise ) .    to select the event windows and attribute their geographic cluster , we use the catalogs from the ogs .",
    "together , gs.ok027 and gs.ok029 yield 2918 windows of labeled earthquakes for the period between 15 february 2014 and 16 november 2016 .",
    "we look for windows of seismic noise in between the cataloged events .",
    "because some of the low magnitudes earthquakes we wish to detect are most likely buried in seismic noise , it is important that we reduce the chance of mislabeling these events as noise .",
    "this is why we use a more exhaustive catalog created by @xcite to select our noise examples .",
    "this catalog covers the same geographic area but for the period between 15 february and 31 august 2014 only and does not locate events .",
    "this yields 831,111 windows of seismic noise .    [",
    "[ trainingtesting - split ] ] * training / testing split * + + + + + + + + + + + + + + + + + + + + + + + +    we split the windows dataset into two independent sets : a test set and a training set .",
    "the test set contains all the windows for july 2014 ( 209 events and 131,072 windows of noise ) while the training set contains the remaining windows .",
    "[ [ dataset - augmentation ] ] * dataset augmentation * + + + + + + + + + + + + + + + + + + + + + +    deep classifiers like ours have many trainable parameters .",
    "they require a large amount of examples of each class to ovoid overfitting and generalize correctly to unseen examples . to build a large enough dataset of events , we use streams recorded at two stations ( gsok029 and gsok27 , see figure  s3 ) .",
    "the input of our network is a single waveform recorded at either of these stations .",
    "furthermore , we generate additional event windows by perturbing existing ones with zero - mean gaussian noise .",
    "this balances the number of event and noise windows during training , a strategy to regularize the network and prevent overfitting  @xcite .",
    "[ [ convnetquake ] ] * convnetquake * + + + + + + + + + + + + + +    our model is a deep convolutional network ( figure  [ fig : network ] ) .",
    "it takes as input a window of 3-channel waveform data and predicts its label ( noise or event , with its geographic cluster ) .",
    "the parameters of the network are optimized to minimize the discrepancy between the predicted labels and the true labels on the training set ( see the methods section for details ) .    [",
    "[ detection - accuracy ] ] * detection accuracy * + + + + + + + + + + + + + + + + + + + +    in a first experiment to assess the _ detection _ performance of our algorithm , we ignore the geographic label ( i.e. , labels 16 are considered as a single `` earthquake '' class ) . the detection accuracy is the percentage of windows correctly classified as earthquake or noise .",
    "our algorithm successfully detects all the events cataloged by the ogs , reaching 100 @xmath1 accuracy on event detection ( see table  [ table : results ] ) . among the 131,972 noise windows of our test set ,",
    "convnetquake correctly classifies 129,954 noise windows .",
    "it classifies 2018 of the noise windows as events . among them ,",
    "1902 windows were confirmed as events by the autocorrelation method ( detailed in the supplementary materials ) .",
    "that is , our algorithm made 116 false detections , for an accuracy of 99.9 @xmath1 on noise windows .",
    "[ [ location - accuracy ] ] * location accuracy * + + + + + + + + + + + + + + + + + + +    we then evaluate the _ location _ performance . for each of the detected events , we compare the predicted class ( 16 ) with the true geographic label . we obtain 74.5 @xmath1 location accuracy on the test set ( see table  [ table : results ] ) . for comparison with a `` chance '' baseline , selecting a class at random would give @xmath2 accuracy",
    ".    we also experimented with a larger number of clusters ( 50 , see figure s4 ) and obtained 22.5 @xmath1 in location accuracy , still 10 times better than chance at @xmath3 .",
    "this performance drop is not surprising since , on average , each class now only provides 40 training samples , which is insufficient for proper training .",
    "[ [ probabilistic - location - map ] ] * probabilistic location map * + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our network computes a probability distribution over the classes .",
    "this allows us to create a probabilistic map of earthquake location .",
    "we show in figure  [ fig : mapproba ] the maps for a correctly located event and an erroneous classification . for the correctly classified event ,",
    "most of the probability mass is on the correct class .",
    "this event is classified with approximately 99 @xmath1 confidence . for the misclassified event ,",
    "the probability distribution is more diffuse and the location confidence drops to 40 @xmath1 .    [ [ generalization - to - non - repeating - events ] ] * generalization to non - repeating events * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our algorithm generalizes well to waveforms very dissimilar from those in the training set .",
    "we quantify this using synthetic seismograms , comparing our method to template matching .",
    "we generate day - long synthetic waveforms by inserting multiple copies of a given template over a gaussian noise floor , varying the signal - to - noise - ratio ( snr ) from -1 to 8 db .",
    "an example of synthetic seismogram is shown in figure s2 .",
    "we choose two templates waveforms @xmath4 and @xmath5 ( shown in figure s1 ) . using the procedure described above",
    ", we generate a training set using @xmath4 and two testing sets using @xmath4 and @xmath5 respectively .",
    "we train both convnetquake and the template matching method ( see supplementary materials ) on the training set ( generated with @xmath4 ) .    on the @xmath4 testing",
    "set , both methods successfully detect all the events . on the other testing set ( containing only copies of @xmath5 )",
    ", the template matching method fails to detect inserted events even at high snr .",
    "convnetquake however recognizes the new ( unknown ) events .",
    "the accuracy of our model remarkably increases with snr ( see figure  [ fig : perf_synth ] ) . for snrs",
    "higher than 7 db , convnetquake detects all the inserted seismic events .",
    "many events in our dataset from oklahoma are non - repeating events ( we highlighted two in figure  [ fig : mapevents ] ) .",
    "our experiment on synthetic data suggests that methods relying on template matching can not detect them while convnetquake can .",
    "[ [ earthquake - detection - on - continuous - records ] ] * earthquake detection on continuous records * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we run convnetquake on one month of continuous waveform data recorded with gs.ok029 in july 2014 .",
    "the 3-channel waveforms are cut into 10 second long , non overlapping windows , with a 1 second offset between consecutive windows to avoid possibly redundant detections .",
    "our algorithm detects 4225 events never cataloged before by the ogs .",
    "this is about 5 events per hour .",
    "autocorrelation confirms 3949 of these detections ( see supplementary for details ) .",
    "figure  [ fig : july_waveforms ] shows the most repeated waveform ( 479 times ) among the 3949 detections .",
    "[ [ comparison - with - other - detection - methods ] ] * comparison with other detection methods * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we compare our _ detection _ performances to autocorrelation and fingerprint and similarity thresholding ( fast , reported from @xcite ) .",
    "both techniques can only find repeating events , and do not provide event location .",
    "@xcite used autocorrelation and fast to detect new events during one week of continuous waveform data recorded at a single station with the a single channel from 8 january 2011 to 15 january 2011 .",
    "the bank of templates used for fast consists in 21 earthquakes : a @xmath0 4.1 that occurred on 8 january 2011 on the calaveras fault ( north california ) and 20 of its aftershocks ( @xmath0 0.84 to @xmath0 4.10 , a range similar to our dataset ) .",
    "table  [ table : results ] reports the classification accuracy of all three methods .",
    "convnetquake has an acccuracy comparable to autocorrelation and outperforms fast .",
    "[ [ scalability - to - large - datasets ] ] * scalability to large datasets * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the runtimes of the autocorrelation method , fast , and convnetquake necessary to analyze one week of continuous waveform data are reported in table  [ table : results ] .",
    "our runtime excludes the training phase which is performed once .",
    "similarly , fast s runtime excludes the time required to build the database of templates .",
    "we ran our algorithm on a dual core intel i5 2.9 ghz cpu .",
    "it is approximately 13,500 times faster than autocorrelation and 48 times faster than fast ( table  [ table : results ] ) .",
    "convnetquake is highly scalable and can easily handle large datasets .",
    "it can process one month of continuous data in 4 minutes 51 seconds while fast is 120 times slower ( 4 hours 20 minutes , see figure  [ fig : scaling_prop]a ) .",
    "like other template matching techniques , fast s database grows as it creates and store new templates during detection . for 2 days of continuous recording , fast s database is approximately 1 gb ( see figure  [ fig : scaling_prop]b ) .",
    "processing years of continuous waveform data would increase dramatically the size of this database and adversely affect performance .",
    "our network only needs to store a compact set of parameters , which entails a constant memory usage ( 500 kb , see figure  [ fig : scaling_prop]b ) .",
    "convnetquake achieves state - of - the - art performances in probabilistic event detection and location using a single signal . for this",
    ", it requires a pre - existing history of cataloged earthquakes at training time .",
    "this makes it ill - suited to areas of low seismicity or areas where instrumentation is recent . in this study we focused on local earthquakes , leaving larger scale for future work",
    "finally , we partitioned events into discrete categories that were fixed beforehand .",
    "one might extend our algorithm to produce continuous probabilistic location maps .",
    "our approach is ideal to monitor geothermal systems , natural resource reservoirs , volcanoes , and seismically active and well instrumented plate boundaries such as the subduction zones in japan or the san andreas fault system in california .",
    "convnetquake takes as input a 3-channel window of waveform data and predicts a discrete probability over @xmath6 categories , or _",
    "classes _ in the machine learning terminology . classes",
    "@xmath7 to @xmath8 correspond to predefined geographic `` clusters '' and class 0 corresponds to event - free `` seismic noise '' .",
    "the clusters for our dataset are illustrated in figure  [ fig : mapevents ] .",
    "our algorithm outputs a @xmath6-d vector of probabilities that the input window belongs to each of the @xmath6 classes .",
    "figure  [ fig : network ] illustrates our architecture .    [ [ network - architecture ] ] * network architecture * + + + + + + + + + + + + + + + + + + + + + +    the network s input is a 2-d tensor @xmath9 representing the waveform data of a fixed - length window .",
    "the rows of @xmath10 for @xmath11 correspond to the channels of the waveform and since we use 10 second - long windows sampled at 100 hz , the time index is @xmath12 .",
    "the core of our processing is carried out by a feed - forward stack of 8 convolutional layers ( @xmath13 to @xmath14 ) followed by 1 fully connected layer @xmath15 that outputs class scores .",
    "all the layers contain multiple channels and are thus represented by 2-d tensors .",
    "each channel of the 8 convolutional layers is obtained by convolving the channels of the previous layer with a bank of linear 1-d filters , summing , adding a bias term , and applying a point - wise non - linearity as follows : @xmath16 where @xmath17 is the non - linear relu activation function . the output and input channels",
    "are indexed with @xmath18 and @xmath19 respectively and the time dimension with @xmath20 , @xmath21 .",
    "@xmath22 is the number of channels in layer @xmath23 .",
    "we use 32 channels for layers @xmath7 to @xmath24 while the input waveform ( layer @xmath25 ) has 3 channels .",
    "we store the filter weights for layer @xmath23 in a 3-d tensor @xmath26 with dimensions @xmath27 .",
    "that is , we use 3-tap filters .",
    "the biases are stored in a 1-d tensor @xmath28 .",
    "all convolutions use zero - padding as the boundary condition .",
    "equation   shows that our formulation slightly differs from a standard convolution : we use _ strided _ convolutions with stride @xmath29 , i.e.  the kernel slides horizontally in increments of 2 samples ( instead of 1 ) .",
    "this allows us to downsample the data by a factor 2 along the time axis after each layer .",
    "this is equivalent to performing a regular convolution followed by subsampling with a factor 2 , albeit more efficiently .",
    "because we use small filters ( the kernels have size 3 ) , the first few layers only have a local view of the input signal and can only extract high - frequency features . through progressive downsampling ,",
    "the deeper layers have an exponentially increasing receptive field over the input signal ( by indirect connections ) .",
    "this allow them to extract low - frequency features ( cf .",
    "figure  [ fig : network ] ) .",
    "after the 8th layer , we vectorize the tensor @xmath14 with shape @xmath30 into a 1-d tensor with @xmath31 features @xmath32 . this feature vector is processed by a linear , fully connected layer to compute class scores @xmath33 with @xmath34 given by : @xmath35 thanks to this fully connected layer , the network learns to combine multiple parts of the signal ( e.g. , p - waves , s - waves , seismic coda ) to generate a class score and can detect events anywhere within the window .",
    "finally , we apply the softmax function to the class scores to obtain a properly normalized probability distribution which can be interpreted as a posterior distribution over the classes conditioned on the input @xmath10 and the network parameters @xmath36 and @xmath37 : @xmath38 @xmath39 is the set of all the weights , and @xmath40 is the set of all the biases .",
    "compared to a fully - connected architecture like in @xcite ( where each layer would be fully connected as in equation  ) , convolutional architectures like ours are computationally more efficient .",
    "this efficiency gain is achieved by sharing a small set of weights across time indices .",
    "for instance , a connection between layers @xmath13 and @xmath41 , which have dimensions @xmath42 and @xmath43 respectively , requires @xmath44 parameters in the convolutional case with a kernel of size 3 .",
    "a fully - connected connection between the same layers would entail @xmath45 parameters , a 4 orders of magnitude increase .",
    "furthermore , models with many parameters require large datasets to avoid overfitting . since labeled datasets for our problem are scarce and costly to assemble , a parsimonious model such as ours is desirable .    [",
    "[ training - the - network ] ] * training the network * + + + + + + + + + + + + + + + + + + + + + +    we optimize the network parameters by minimizing a @xmath46-regularized cross - entropy loss function on a dataset of @xmath47 windows indexed with @xmath48 : @xmath49 the cross - entropy loss measures the average discrepancy between our predicted distribution @xmath50 and the true class probability distribution @xmath51 for all the windows @xmath48 in the training set . for each window , the true probability distribution @xmath51 has all of its mass on the window s true class : @xmath52    to regularize the neural network , we add an @xmath46 penalty on the weights @xmath36 , balanced with the cross - entropy loss via the parameter @xmath53 .",
    "regularization favors network configurations with small weight magnitude .",
    "this reduces the potential for overfitting  @xcite .    since both the parameter set and the training data set are too large to fit in memory , we minimize equation   using a batched stochastic gradient descent algorithm .",
    "we first randomly shuffle the @xmath54 windows from the dataset .",
    "we then form a sequence of batches containing 128 windows each . at each training step",
    "we feed a batch to the network , evaluate the expected loss on the batch , and update the network parameters accordingly using backpropagation  @xcite .",
    "we repeatedly cycle through the sequence until the expected loss stops improving . since our dataset is unbalanced ( we have many more noise windows than events ) , each batch",
    "is composed of 64 windows of noise and 64 event windows .",
    "for optimization we use the adam  @xcite algorithm , which k.pdf track of first and second order moments of the gradients , and is invariant to any diagonal rescaling of the gradients .",
    "we use a learning rate of @xmath55 and keep all other parameters to the default value recommended by the authors .",
    "we implemented convnetquake in tensorflow  @xcite and performed all our trainings on a nvidia tesla k20xm graphics processing unit .",
    "we train for 32,000 iterations which takes approximately @xmath56h .",
    "[ [ evaluation - on - an - independent - testing - set ] ] * evaluation on an independent testing set * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    after training , we test the accuracy of our network on windows from july 2014 ( 209 windows of events and 131,072 windows of noise ) .",
    "the class predicted by our algorithm is the one whose posterior probability @xmath57 is the highest .",
    "we evaluate our predictions using two metrics .",
    "the _ detection accuracy _ is the percentage of windows correctly classified as events or noise .",
    "the _ location accuracy _ is the percentage of windows already classified as events that have the correct cluster number .",
    "the convnetquake software is open - source .",
    "the waveform data used in this paper can be obtained from the incorporated research institutions for seismology ( iris ) data management center and the network gs is available at doi:10.7914/sn / gs . the earthquake catalog used is provided by the oklahoma geological survey .",
    "the computations in this paper were run on the odyssey cluster supported by the fas division of science , research computing group at harvard university .",
    "t. p.s research was supported by the national science foundation grant division for materials research 14 - 20570 to harvard university with supplemental support by the southern california earthquake center ( scec ) , funded by nsf cooperative agreement ear-1033462 and usgs cooperative agreement g12ac20038 .",
    "t.p . thanks jim rice for his continuous support during his phd and loc viens for insightful discussions about seismology .    28 natexlab#1#1[2]#2 , , ( ) .",
    ", , , , , , , , ( ) . , , , ( ) .",
    ", , , , , ( ) . , , , internal report , ucrl - tr-222758 , .",
    ", , , ( ) . , , , ( ) .",
    ", , , , , , ( ) . , , , , , ( ) . , , , , in : , pp . .",
    ", , , , ( ) . , , , , , , , , , , ( ) . , , , , , , , , , ( ) . , , ( ) .",
    ", , , , , ( ) . , , , ( ) .",
    ", , , , , , ( ) .",
    ", , , , , , ( ) .",
    ", et  al . , , in : , volume  , , pp . .",
    ", , , ( ) . , , , in : . , , , , ( ) .",
    ", , , ( ) . , , , , , ( ) .",
    ", , in : , , p.  .",
    ", , , ( ) . , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . .",
    "c c c c & autocorrelation & fast & convnetquake ( ours ) +   + noise detection accuracy & 100 @xmath1 & @xmath58 100 @xmath1 & 99.9 @xmath1 +   + event detection accuracy & 100 @xmath1 & 87.5 @xmath1 & 100 @xmath1 +   + event location accuracy & n / a & n / a & 74.6 @xmath1 +   + runtime & 9 days 13 hours & 48 min & 1 min 1 sec +",
    "in this section we show that convnetquake generalizes well to unseen examples of earthquake waveforms .",
    "we do this by comparing the number of detections missed by convnetquake and by the template matching method on synthetic data , which provides ground truth .",
    "our synthetic dataset is made of day - long seismic records constructed by inserting at random times , a scaled version of a waveform template ( extracted from true data ) over a gaussian noise floor .",
    "an example of synthetic time series is shown in figure  [ fig : synthetics ] .",
    "we generate day - long seismic records with signal to noise ratio ( snr ) ranging from -1 to 8 db .",
    "the snr of a time series is defined as the ratio of the power of the inserted event waveforms over the power of the computer - generated seismic noise .",
    "that is , @xmath59,\\ ] ] where @xmath60 and @xmath61 are the signal amplitude . for a 3-second long template of 3-channel waveform data @xmath62 sampled at 100 hz",
    ", we define the amplitude of a signal as the @xmath46 norm of the waveform , @xmath63 where the time index is @xmath20 and the channel index is @xmath18 .",
    "similarly @xmath61 is evaluated using the generated gaussian noise for the 3 second duration .",
    "we choose two template waveforms @xmath4 ( figure  [ fig : templates]a ) and @xmath5 ( figure  [ fig : templates]b ) . using the procedure described above ,",
    "we generate a training set of day - long records using @xmath4 and two testing sets of day - long records using @xmath4 and @xmath5 respectively .",
    "we partition the continuous synthetic waveform data used for training into windows labeled as either seismic noise or earthquake .",
    "we train convnetquake on these two categories using the procedure detailed in the section methods of the paper .",
    "this allows to test the detection ability of convnetquake .",
    "the template matching method consists in cross - correlating the 3-channel day - long seismic records with a 3-channel template of earthquake waveform to detect seismic events .",
    "we tag a time window as an event when the cross - correlation coefficient is above a threshold @xmath64 , where @xmath65 is the median absolute deviation ( mad ) of all the cross - correlation coefficients and @xmath66 is specified by the user . using the training synthetic records we find that @xmath67 provides the best detection accuracy .",
    "both convnetquake and the template matching method detect all the events inserted using template @xmath4 ( figure  [ fig : templates]a ) seen during training ; the number of missed detection is 0 for all the records with snr between -1 db and 8 db . for the time series created by inserting template @xmath5 ( figure  [ fig : templates]b ) not seen during the training phase , the template matching method misses almost all of the inserted new templates while convnetquake detects most of them ( see figure 4 in main manuscript ) .",
    "this demonstrates convnetquake s ability to generalize to new , unseen events .",
    "the detection accuracy on windows of ( unknown ) events increases with snr for convnetquake whereas template matching s remains low ( see figure 4 ) .",
    "in this section , we expand on our autocorrelation analysis to discriminate true from false detections in the set of detections made by convnetquake",
    ". there are @xmath68 windows labeled as events by convnetquake .",
    "we cross - correlate all pairs of windows ( there are @xmath69 cross - correlations ) and take the peak absolute value of the correlation coefficient ( cc ) per pair .",
    "we do not distinguish between correlated and anti - correlated events because our goal is to detect new events regardless of their polarization , and therefore of their source mechanism .",
    "figure  [ fig : ccs ] shows that the distribution of those correlation coefficients is skewed and does not peak at cc=0 .",
    "this is because most of the event windows detected by convnetquake possess some level of correlation : our algorithm has discarded the uncorrelated seismic noise .",
    "we choose a threshold based on visual inspection of waveform coherence among the three components .",
    "we show in figures  [ fig : cluster3_1 ] , [ fig : cluster3_2 ] , and [ fig : cluster3_3 ] the waveforms of detected events that belong to cluster 3 for three different threshold , cc @xmath70 0.1 ( 2271 event waveforms ) , cc @xmath70 0.2 ( 2129 event waveforms ) , and cc @xmath70 0.3 ( 845 event waveforms ) .",
    "we decide on a threshold that retains most event signals while allowing for a diversified set of waveform shapes .",
    "a threshold of 0.2 retains coherent waveforms visible on at least two out of the three components .",
    "because of the geometry between focal mechanisms , source depth , and receiver location , most of the events detected by @xcite and @xcite are strike slip , with a dominant p - wave on the vertical and s - wave on the horizontal components . instead , we find that dominant s - waves also appear on the vertical components , suggesting a variety of focal mechanisms for that area",
    ". waveform similarity selection would restrict the search to strike - slip events only and miss all other events if those were not considered in the bank of templates .",
    "harley  m benz , nicole  d mcmahon , richard  c aster , daniel  e mcnamara , and david  b harris .",
    "hundreds of earthquakes per day : the 2014 guthrie , oklahoma , earthquake sequence . _ seismological research letters _ , 860 ( 5):0 13181325 , 2015",
    ".    daniel  e mcnamara , harley  m benz , robert  b herrmann , eric  a bergman , paul earle , austin holland , randy baldwin , and a  gassner .",
    "earthquake hypocenters and focal mechanisms in central oklahoma reveal a complex system of reactivated subsurface strike - slip faulting .",
    "_ geophysical research letters _ , 420 ( 8):0 27422749 , 2015 .     used to build both the synthetic training set and the first synthetic test set .",
    "this waveform corresponds to an earthquake from 3 august 2014 at 4:57:59 recorded by gs.ok029 .",
    "( b ) template @xmath5 used to create the second synthetic test set .",
    "this is the waveform of an earthquake from 1 april 2014 at 17:07:19 recorded by gs.ok029.,width=35 ]                     300 events that have the highest absolute correlation coefficient , suggesting a strike - slip mechanism .",
    "however , s - waves dominate the vertical component for most of the events , suggesting a different focal mechanism.,width=28 ]"
  ],
  "abstract_text": [
    "<S> the recent evolution of induced seismicity in central united states calls for exhaustive catalogs to improve seismic hazard assessment . over the last decades </S>",
    "<S> , the volume of seismic data has increased exponentially , creating a need for efficient algorithms to reliably detect and locate earthquakes . today </S>",
    "<S> s most elaborate methods scan through the plethora of continuous seismic records , searching for repeating seismic signals . in this work , </S>",
    "<S> we leverage the recent advances in artificial intelligence and present _ convnetquake _ , a highly scalable convolutional neural network for earthquake detection and location from a single waveform . </S>",
    "<S> we apply our technique to study the induced seismicity in oklahoma ( usa ) . </S>",
    "<S> we detect 20 times more earthquakes than previously cataloged by the oklahoma geological survey . </S>",
    "<S> our algorithm is orders of magnitude faster than established methods .    </S>",
    "<S> the recent exploitation of natural resources and associated waste water injection in the subsurface have induced many small and moderate earthquakes in the tectonically quiet central united states @xcite . induced </S>",
    "<S> earthquakes contribute to seismic hazard . during the past 5 years </S>",
    "<S> only , six earthquakes of magnitude higher than 5.0 might have been triggered by nearby disposal wells . </S>",
    "<S> most earthquake detection methods are designed for large earthquakes . as a consequence </S>",
    "<S> , they tend to miss many of the low - magnitude induced earthquakes that are masked by seismic noise . </S>",
    "<S> detecting and cataloging these earthquakes is key to understanding their causes ( natural or human - induced ) ; and ultimately , to mitigate the seismic risk .    </S>",
    "<S> traditional approaches to earthquake detection  @xcite fail to detect events buried in even modest levels of seismic noise . </S>",
    "<S> waveform similarity can be used to detect earthquakes that originate from a single region , with the same source mechanism . </S>",
    "<S> waveform _ </S>",
    "<S> autocorrelation _ is the most effective method to identify these repeating earthquakes from seismograms  @xcite . while particularly robust and reliable , the method is computationally intensive and does not scale to long time series . </S>",
    "<S> one approach to reduce computation is to select a small set of short representative waveforms as _ templates _ and correlate only these with the full - length continuous time series  @xcite . </S>",
    "<S> the detection capability of _ template matching _ </S>",
    "<S> techniques strongly depends on the number of templates used . today </S>",
    "<S> s most elaborate methods seek to reduce the number of templates by principal component analysis  @xcite , or locality sensitive hashing  @xcite . </S>",
    "<S> these techniques still become computationally expensive as the database of templates grows . more fundamentally </S>",
    "<S> , they do not address the issue of representation power . </S>",
    "<S> these methods are restricted to the sole detection of _ repeating _ signals . </S>",
    "<S> finally , most of these methods do not locate earthquakes .    </S>",
    "<S> we cast earthquake detection as a supervised classification problem and propose the first convolutional network for earthquake detection and location ( _ convnetquake _ ) . </S>",
    "<S> our algorithm builds on recent advances in deep learning  @xcite . </S>",
    "<S> it is trained on a large dataset of labeled waveforms and learns a compact representation that can discriminate seismic noise from earthquake signals . </S>",
    "<S> the waveforms are no longer classified by their similarity to other waveforms , as in previous work . </S>",
    "<S> instead , we analyze the waveforms with a collection of nonlinear local filters . during the training phase , </S>",
    "<S> the filters are optimized to select features in the waveforms that are most relevant to classify them as either noise or an earthquake . </S>",
    "<S> this bypasses the need to store a perpetually growing library of template waveforms . </S>",
    "<S> thanks to this representation , our algorithm generalizes well to earthquake signals never seen during training . </S>",
    "<S> it is more accurate than state - of - the - art algorithms and runs orders of magnitude faster . </S>",
    "<S> additionally , convnetquake outputs a probabilistic location of an earthquake s source from a _ single _ waveform . </S>",
    "<S> we evaluate our algorithm and apply it to induced earthquakes in central oklahoma ( usa ) . </S>",
    "<S> we show that it uncovers earthquakes absent from standard catalogs . </S>"
  ]
}