{
  "article_text": [
    "topic modeling is a popular method that learns thematic structure from large document collections without human supervision .",
    "the model is simple : documents are mixtures of topics , which are modeled as distributions over a vocabulary @xcite .",
    "each word token is generated by selecting a topic from a document - specific distribution , and then selecting a specific word from that topic - specific distribution .",
    "posterior inference over document - topic and topic - word distributions is intractable  in the worst case it is np - hard even for just two topics  @xcite . as a result , researchers have used approximate inference techniques such as singular value decomposition @xcite , variational inference @xcite , and mcmc @xcite .",
    "recent work in theoretical computer science focuses on designing _ provably _ efficient algorithms for topic modeling .",
    "these treat the topic modeling problem as one of _ statistical recovery _ : assuming the data was generated _",
    "perfectly _ from the hypothesized model using an unknown set of parameter values , the goal is to recover the model parameters in polynomial time given a reasonable number of samples .",
    "@xcite present an algorithm that provably recovers the parameters of topic models provided that the topics meet a certain _ separability _ assumption @xcite .",
    "separability requires that every topic contains at least one _ anchor word _ that has non - zero probability only in that topic .",
    "if a document contains this anchor word , then it is guaranteed that the corresponding topic is among the set of topics used to generate the document .",
    "the algorithm proceeds in two steps : first it selects anchor words for each topic ; and second , in the recovery step , it reconstructs topic distributions given those anchor words .",
    "the input for the algorithm is the second - order moment matrix of word - word co - occurrences .",
    "@xcite present a provable algorithm based on third - order moments that does not require separability , but , unlike the algorithm of arora et al .",
    ", assumes that topics are not correlated .",
    "although standard topic models like lda @xcite assume that the choice of topics used to generate the document are uncorrelated , there is strong evidence that topics are dependent  @xcite : economics and politics are more likely to co - occur than economics and cooking .",
    "both algorithms run in polynomial time , but the bounds that have been proven on their sample complexity are weak and their empirical runtime performance is slow .",
    "the algorithm presented by @xcite solves numerous linear programs to find anchor words . @xcite and @xcite reduce the number of linear programs needed .",
    "all of these algorithms infer topics given anchor words using matrix inversion , which is notoriously unstable and noisy : matrix inversion frequently generates negative values for topic - word probabilities .",
    "in this paper we present three contributions .",
    "first , we replace linear programming with a combinatorial anchor selection algorithm .",
    "so long as the separability assumption holds , we prove that this algorithm is stable in the presence of noise and thus has polynomial sample complexity for learning topic models .",
    "second , we present a simple probabilistic interpretation of topic recovery given anchor words that replaces matrix inversion with a new gradient - based inference method .",
    "third , we present an empirical comparison between recovery - based algorithms and existing likelihood - based topic inference .",
    "we study both the empirical sample complexity of the algorithms on synthetic distributions and the performance of the algorithms on real - world document corpora .",
    "we find that our algorithm performs as well as collapsed gibbs sampling on a variety of metrics , and runs at least an order of magnitude faster .",
    "our algorithm both inherits the provable guarantees from @xcite and also results in simple , practical implementations .",
    "we view our work as a step toward bridging the gap between statistical recovery approaches to machine learning and maximum likelihood estimation , allowing us to circumvent the computational intractability of maximum likelihood estimation yet still be robust to model error .",
    "we consider the learning problem for a class of admixture distributions that are frequently used for probabilistic topic models .",
    "examples of such distributions include latent dirichlet allocation @xcite , correlated topic models @xcite , and pachinko allocation @xcite .",
    "we denote the number of words in the vocabulary by @xmath0 and the number of topics by @xmath1 .",
    "associated with each topic @xmath2 is a multinomial distribution over the words in the vocabulary , which we will denote as the column vector @xmath3 of length @xmath0 .",
    "each of these topic models postulates a particular prior distribution @xmath4 over the topic distribution of a document .",
    "for example , in latent dirichlet allocation ( lda ) @xmath4 is a dirichlet distribution , and for the correlated topic model @xmath4 is a logistic normal distribution .",
    "the generative process for a document @xmath5 begins by drawing the document s topic distribution @xmath6 .",
    "then , for each position @xmath7 we sample a topic assignment @xmath8 , and finally a word @xmath9",
    ".    we can combine the column vectors @xmath3 for each of the @xmath1 topics to obtain the word - topic matrix @xmath10 of dimension @xmath11 .",
    "we can similarly combine the column vectors @xmath12 for @xmath13 documents to obtain the topic - document matrix @xmath14 of dimension @xmath15 .",
    "we emphasize that @xmath14 is unknown and stochastically generated : we can never expect to be able to recover it .",
    "the learning task that we consider is to find the word - topic matrix @xmath10 .",
    "for the case when @xmath4 is dirichlet ( lda ) , we also show how to learn hyperparameters of @xmath4 .",
    "maximum likelihood estimation of the word - topic distributions is np - hard even for two topics @xcite , and as a result researchers typically use approximate inference .",
    "the most popular approaches are variational inference @xcite , which optimizes an approximate objective , and markov chain monte carlo @xcite , which asymptotically samples from the posterior distribution but has no guarantees of convergence .",
    "@xcite present an algorithm that provably learns the parameters of a topic model given samples from the model , provided that the word - topic distributions satisfy a condition called _ separability _ :    the word - topic matrix @xmath10 is @xmath16-separable for @xmath17 if for each topic @xmath2 , there is some word @xmath7 such that @xmath18 and @xmath19 for @xmath20 .",
    "such a word is called an _ anchor word _ because when it occurs in a document , it is a perfect indicator that the document is at least partially about the corresponding topic , since there is no other topic that could have generated the word .",
    "suppose that each document is of length @xmath21 , and let @xmath22 $ ] be the @xmath23 topic - topic covariance matrix .",
    "let @xmath24 be the expected proportion of topic @xmath2 in a document generated according to @xmath4 .",
    "the main result of @xcite is :    there is a polynomial time algorithm that learns the parameters of a topic model if the number of documents is at least @xmath25 where @xmath16 is defined above , @xmath26 is the condition number of @xmath27 , and @xmath28 .",
    "the algorithm learns the word - topic matrix @xmath10 and the topic - topic covariance matrix @xmath27 up to additive error @xmath29 .    unfortunately , this algorithm is not practical .",
    "its running time is prohibitively large because it solves @xmath0 linear programs , and its use of matrix inversion makes it unstable and sensitive to noise . in this paper",
    ", we will give various reformulations and modifications of this algorithm that alleviate these problems altogether .",
    "textual corpus @xmath30 , number of anchors @xmath1 , tolerance parameters @xmath31 .",
    "word - topic matrix @xmath10 , topic - topic matrix @xmath27 @xmath32 form @xmath33 , the normalized rows of @xmath34 .",
    "@xmath35 @xmath36 fastanchorwords(@xmath33 , @xmath1 , @xmath37 ) ( algorithm  [ alg : anchorword ] ) @xmath38 recoverkl(@xmath39 ) ( algorithm  [ alg : nnr ] ) @xmath40    the @xcite algorithm has two steps : _ anchor selection _ , which identifies anchor words , and _ recovery _ , which recovers the parameters of @xmath10 and of @xmath4 .",
    "both anchor selection and recovery take as input the matrix @xmath34 ( of size @xmath41 ) of word - word co - occurrence counts , whose construction is described in the supplementary material .",
    "@xmath34 is normalized so that the sum of all entries is @xmath42 .",
    "the high - level flow of our complete learning algorithm is described in algorithm  [ alg : high_level ] , and follows the same two steps . in this section",
    "we will introduce a new recovery method based on a probabilistic framework .",
    "we defer the discussion of anchor selection to the next section , where we provide a purely combinatorial algorithm for finding the anchor words .",
    "the original recovery procedure ( which we call `` recover '' ) from @xcite is as follows .",
    "first , it permutes the @xmath34 matrix so that the first @xmath1 rows and columns correspond to the anchor words .",
    "we will use the notation @xmath43 to refer to the first @xmath1 rows , and @xmath44 for the first @xmath1 rows and just the first @xmath1 columns .",
    "if constructed from infinitely many documents , @xmath34 would be the second - order moment matrix @xmath45 = a\\mathbb{e}[ww^t]a^t = ara^t$ ] , with the following block structure : @xmath46 where @xmath47 is a diagonal matrix of size @xmath48 .",
    "next , it solves for @xmath10 and @xmath27 using the algebraic manipulations outlined in algorithm  [ alg : original_recovery ] .",
    "matrix @xmath34 , set of anchor words @xmath35 matrices @xmath10,@xmath27 permute rows and columns of @xmath34 compute @xmath49 ( equals @xmath50 ) solve for @xmath51 : @xmath52 ( @xmath53 equals @xmath54 ) solve for @xmath55 = @xmath56 solve for @xmath57 @xmath40    the use of matrix inversion in algorithm  [ alg : original_recovery ] results in substantial imprecision in the estimates when we have small sample sizes .",
    "the returned @xmath10 and @xmath27 matrices can even contain small negative values , requiring a subsequent projection onto the simplex . as we will show in section  [ sec : results ] , the original recovery method performs poorly relative to a likelihood - based algorithm .",
    "part of the problem is that the original recover algorithm uses only @xmath1 rows of the matrix @xmath34 ( the rows for the anchor words ) , whereas @xmath34 is of dimension @xmath41 . besides ignoring most of the data ,",
    "this has the additional complication that it relies completely on co - occurrences between a word and the anchors , and this estimate may be inaccurate if both words occur infrequently .",
    "here we adopt a new _ probabilistic _ approach , which we describe below after introducing some notation .",
    "consider any two words in a document and call them @xmath58 and @xmath59 , and let @xmath60 and @xmath61 refer to their topic assignments .",
    "we will use @xmath62 to index the matrix of word - topic distributions , i.e. @xmath63 . given infinite data ,",
    "the elements of the @xmath34 matrix can be interpreted as @xmath64 .",
    "the row - normalized @xmath34 matrix , denoted @xmath65 , which plays a role in both finding the anchor words and the recovery step , can be interpreted as a conditional probability @xmath66 .",
    "matrix @xmath34 , set of anchor words @xmath35 , tolerance parameter @xmath29 .",
    "matrices @xmath10,@xmath27 normalize the rows of @xmath34 to form @xmath65 .",
    "store the normalization constants @xmath67 .",
    "@xmath68 is the row of @xmath65 for the @xmath69 anchor word .",
    "solve @xmath70 subject to : @xmath71 and @xmath72 with tolerance : @xmath29 @xmath73 normalize the columns of @xmath74 to form @xmath10 .",
    "@xmath75 @xmath40    denoting the indices of the anchor words as @xmath76 , the rows indexed by elements of @xmath35 are special in that every other row of @xmath65 lies in the convex hull of the rows indexed by the anchor words . to see this",
    ", first note that for an anchor word @xmath77 , @xmath78 where uses the fact that in an admixture model @xmath79 , and is because @xmath80 . for any other word @xmath7",
    ", we have @xmath81 denoting the probability @xmath82 as @xmath83 , we have @xmath84 .",
    "since @xmath85 is non - negative and @xmath71 , we have that any row of @xmath65 lies in the convex hull of the rows corresponding to the anchor words .",
    "the mixing weights give us @xmath86 ! using this together with @xmath87 , we can recover the @xmath10 matrix simply by using bayes rule : @xmath88 finally , we observe that @xmath87 is easy to solve for since @xmath89 .",
    "our new algorithm finds , for each row of the empirical row normalized co - occurrence matrix , @xmath90 , the coefficients @xmath86 that best reconstruct it as a convex combination of the rows that correspond to anchor words .",
    "this step can be solved quickly and in parallel ( independently ) for each word using the exponentiated gradient algorithm .",
    "once we have @xmath91 , we recover the @xmath10 matrix using bayes rule .",
    "the full algorithm using kl divergence as an objective is found in algorithm  [ alg : nnr ] .",
    "further details of the exponentiated gradient algorithm are given in the supplementary material .",
    "one reason to use kl divergence as the measure of reconstruction error is that the recovery procedure can then be understood as maximum likelihood estimation .",
    "in particular , we seek the parameters @xmath92 , @xmath91 , @xmath93 that maximize the likelihood of observing the _ word co - occurence counts _ , @xmath94 .",
    "however , the optimization problem does not explicitly constrain the parameters to correspond an admixture model .",
    "we can also define a similar algorithm using quadratic loss , which we call recoverl2 .",
    "this formulation has the extremely useful property that both the objective and gradient can be kernelized so that the optimization problem is independent of the vocabulary size . to see this , notice",
    "that the objective can be re - written as @xmath95 where @xmath96 is @xmath48 and can be computed once and used for all words , and @xmath97 is @xmath98 and can be computed once prior to running the exponentiated gradient algorithm for word @xmath7 .    to recover the @xmath27 matrix for an admixture model , recall that @xmath99 .",
    "this may be an over - constrained system of equations with no solution for @xmath27 , but we can find a least - squares approximation to @xmath27 by pre- and post - multiplying @xmath34 by the pseudo - inverse @xmath100 .",
    "for the special case of lda we can learn the dirichlet hyperparameters . recall that in applying bayes rule we calculated @xmath101 .",
    "these values for @xmath102 specify the dirichlet hyperparameters up to a constant scaling .",
    "this constant could be recovered from the @xmath27 matrix @xcite , but in practice we find it is better to choose it using a grid search to maximize the likelihood of the training data .",
    "we will see in section  [ sec : results ] that our nonnegative recovery algorithm performs much better on a wide range of performance metrics than the recovery algorithm in @xcite . in the supplementary material",
    "we show that it also inherits the theoretical guarantees of @xcite : given polynomially many documents , our algorithm returns an estimate @xmath103 at most @xmath29 from the true word - topic matrix @xmath10 .",
    "here we consider the _ anchor selection _ step of the algorithm where our goal is to find the anchor words . in the _ infinite data _ case where we have infinitely many documents , the convex hull of the rows in @xmath104 will be a simplex where the vertices of this simplex correspond to the anchor words .",
    "since we only have a finite number of documents , the rows of @xmath104 are only an approximation to their expectation .",
    "we are therefore given a set of @xmath0 points @xmath105 that are each a perturbation of @xmath106 whose convex hull @xmath107 defines a simplex .",
    "we would like to find an approximation to the vertices of @xmath107 .",
    "see @xcite and @xcite for more details about this problem .",
    "* input : * @xmath0 points @xmath108 in @xmath0 dimensions , almost in a simplex with @xmath1 vertices and @xmath109    * output : * @xmath1 points that are close to the vertices of the simplex .",
    "project the points @xmath110 to a randomly chosen @xmath111 dimensional subspace@xmath112 s.t .",
    "@xmath110 is the farthest point from the origin .",
    "let @xmath113 be the point in @xmath114 that has the largest distance to @xmath115 .",
    "@xmath117 . let @xmath113 be the point that has the largest distance to @xmath118 update @xmath119 to @xmath113 return @xmath120 .",
    "notation : @xmath115 denotes the subspace spanned by the points in the set @xmath121 .",
    "we compute the distance from a point @xmath122 to the subspace @xmath115 by computing the norm of the projection of @xmath122 onto the orthogonal complement of @xmath115 .    @xcite give a polynomial time algorithm that finds the anchor words . however , their algorithm is based on solving @xmath0 linear programs , one for each word , to test whether or not a point is a vertex of the convex hull . in this section",
    "we describe a purely combinatorial algorithm for this task that avoids linear programming altogether .",
    "the new `` fastanchorwords '' algorithm is given in algorithm  [ algorithm : fasteranchors ] . to find all of the anchor words , our algorithm iteratively finds the furthest point from the subspace spanned by the anchor words found so far .",
    "since the points we are given are perturbations of the true points , we can not hope to find the anchor words exactly . nevertheless , the intuition is that even if one has only found @xmath123 points @xmath121 that are close to @xmath123 ( distinct ) anchor words , the point that is furthest from @xmath115 will itself be close to a ( new ) anchor word .",
    "the additional advantage of this procedure is that when faced with many choices for a next anchor word to find , our algorithm tends to find the one that is most different than the ones we have found so far .",
    "the main contribution of this section is a proof that the fastanchorwords algorithm succeeds in finding @xmath1 points that are close to anchor words . to precisely state the guarantees , we recall the following definition from @xcite :    a simplex @xmath107 is @xmath26-robust if for every vertex @xmath124 of @xmath107 , the @xmath125 distance between @xmath124 and the convex hull of the rest of the vertices is at least @xmath26 .    in most reasonable settings",
    "the parameters of the topic model define lower bounds on the robustness of the polytope @xmath107 .",
    "for example , in lda , this lower bound is based on the largest ratio of any pair of hyper - parameters in the model @xcite .",
    "our goal is to find a set of points that are close to the vertices of the simplex , and to make this precise we introduce the following definition :    let @xmath106 be a set of points whose convex hull @xmath107 is a simplex with vertices @xmath126 .",
    "then we say @xmath127 @xmath29-covers @xmath128 if when @xmath129 is written as a convex combination of the vertices as @xmath130 , then @xmath131 .",
    "furthermore we will say that a set of @xmath1 points @xmath29-covers the vertices if each vertex is @xmath29 covered by some point in the set .",
    "we will prove the following theorem : suppose there is a set of points @xmath132 whose convex hull @xmath107 is @xmath26-robust and has vertices @xmath126 ( which appear in @xmath133 ) and that we are given a perturbation @xmath105 of the points so that for each @xmath7 , @xmath134 , then :    there is a combinatorial algorithm that runs in time @xmath135 . ] and outputs a subset of @xmath136 of size @xmath1 that @xmath137-covers the vertices provided that @xmath138 .",
    "this new algorithm not only helps us avoid linear programming altogether in inferring the parameters of a topic model , but also can be used to solve the nonnegative matrix factorization problem under the separability assumption , again without resorting to linear programming .",
    "our analysis rests on the following lemmas , whose proof we defer to the supplementary material .",
    "suppose the algorithm has found a set @xmath121 of @xmath2 points that are each @xmath139-close to distinct vertices in @xmath140 and that @xmath141 .",
    "[ lem : perturbation ] there is a vertex @xmath142 whose distance from @xmath115 is at least @xmath143 .",
    "the proof of this lemma is based on a volume argument , and the connection between the volume of a simplex and the determinant of the matrix of distances between its vertices .",
    "[ lem : inductionforfastalg ] the point @xmath113 found by the algorithm must be @xmath144 close to some vertex @xmath142 .    this lemma is used to show that the error does not accumulate too badly in our algorithm , since @xmath139 only depends on @xmath29 , @xmath26 ( not on the @xmath139 used in the previous step of the algorithm ) .",
    "this prevents the error from accumulating exponentially in the dimension of the problem , which would be catastrophic for our proof .    after running the first phase of our algorithm",
    ", we run a cleanup phase ( the second loop in alg .",
    "[ alg : anchorword ] ) that can reduce the error in our algorithm .",
    "when we have @xmath145 points close to @xmath145 vertices , only one of the vertices can be far from their span .",
    "the farthest point must be close to this missing vertex .",
    "the following lemma shows that this cleanup phase can improve the guarantees of lemma  [ lem : inductionforfastalg ] :    [ lem : cleanup ] suppose @xmath146 and each point in @xmath121 is @xmath147 close to some vertex @xmath142 , then the farthest point @xmath148 found by the algorithm is @xmath149 close to the remaining vertex .",
    "this algorithm is a greedy approach to maximizing the volume of the simplex .",
    "the larger the volume is , the more words per document the resulting model can explain .",
    "better anchor word selection is an open question for future work .",
    "we have experimented with a variety of other heuristics for maximizing simplex volume , with varying degrees of success",
    ".    * related work . *",
    "the separability assumption has also been studied under the name  pure pixel assumption \" in the context of hyperspectral unmixing .",
    "a number of algorithms have been proposed that overlap with ours  such as the vca @xcite algorithm ( which differs in that there is no clean - up phase ) and the n - findr @xcite algorithm which attempts to greedily maximize the volume of a simplex whose vertices are data points .",
    "however these algorithms have only been proven to work in the infinite data case , and for our algorithm we are able to give provable guarantees even when the data points are perturbed ( e.g. , as the result of sampling noise ) .",
    "recent work of @xcite and @xcite follow the same pattern as our paper , but use non - negative matrix factorization under the separability assumption .",
    "while both give applications to topic modeling , in realistic applications the term - by - document matrix is too sparse to be considered a good approximation to its expectation ( because documents are short ) . in contrast , our algorithm works with the gram matrix @xmath34 so that we can give provable guarantees even when each document is short .",
    "we compare three parameter recovery methods , recover @xcite , recoverkl and recoverl2 to a fast implementation of gibbs sampling @xcite .",
    "linear programming - based anchor word finding is too slow to be comparable , so we use fastanchorwords for all three recovery algorithms . using gibbs sampling",
    "we obtain the word - topic distributions by averaging over 10 saved states , each separated by 100 iterations , after 1000 burn - in iterations .",
    "we train models on two synthetic data sets to evaluate performance when model assumptions are correct , and real documents to evaluate real - world performance . to ensure that synthetic documents resemble the dimensionality and sparsity characteristics of real data , we generate _ semi - synthetic _ corpora . for each real corpus , we train a model using mcmc and then generate new documents using the parameters of that model ( these parameters are _ not _ guaranteed to be separable ) .",
    "we use two real - world data sets , a large corpus of * new york times * articles ( 295k documents , vocabulary size 15k , mean document length 298 ) and a small corpus of * nips * abstracts ( 1100 documents , vocabulary size 2500 , mean length 68 )",
    ". vocabularies were pruned with document frequency cutoffs .",
    "we generate semi - synthetic corpora of various sizes from models trained with @xmath150 from ny times and nips , with document lengths set to 300 and 70 , respectively , and with document - topic distributions drawn from a dirichlet with symmetric hyperparameters @xmath151 .",
    "we use a variety of metrics to evaluate models : for the semi - synthetic corpora , we can compute * reconstruction error * between the true word - topic matrix @xmath10 and learned topic distributions .",
    "given a learned matrix @xmath103 and the true matrix @xmath10 , we use an lp to find the best matching between topics .",
    "once topics are aligned , we evaluate @xmath152 distance between each pair of topics .",
    "when true parameters are not available , a standard evaluation for topic models is to compute * held - out probability * , the probability of previously unseen documents under the learned model .",
    "this computation is intractable but there are reliable approximation methods @xcite .",
    "topic models are useful because they provide interpretable latent dimensions",
    ". we can evaluate the * semantic quality * of individual topics using a metric called _",
    "coherence_. coherence is based on two functions , @xmath153 and @xmath154 , which are number of documents with at least one instance of @xmath155 , and of @xmath58 and @xmath59 , respectively @xcite .",
    "given a set of words @xmath156 , coherence is @xmath157 the parameter @xmath158 is used to avoid taking the @xmath159 of zero for words that never co - occur @xcite .",
    "this metric has been shown to correlate well with human judgments of topic quality .",
    "if we perfectly reconstruct topics , all the high - probability words in a topic should co - occur frequently , otherwise , the model may be mixing unrelated concepts .",
    "coherence measures the quality of individual topics , but does not measure redundancy , so we measure * inter - topic similarity*. for each topic , we gather the set of the @xmath160 most probable words .",
    "we then count how many of those words do not appear in any other topic s set of @xmath160 most probable words .",
    "some overlap is expected due to semantic ambiguity , but lower numbers of unique words indicate less useful models .",
    "the recover algorithms , in python , are faster than a heavily optimized java gibbs sampling implementation @xcite .",
    "[ fig : timing ] shows the time to train models on synthetic corpora on a single machine .",
    "gibbs sampling is linear in the corpus size .",
    "recoverl2 is also linear ( @xmath161 ) , but only varies from 33 to 50 seconds .",
    "estimating @xmath34 is linear , but takes only 7 seconds for the largest corpus .",
    "fastanchorwords takes less than 6 seconds for all corpora .",
    "the new algorithms have good @xmath152 reconstruction error on semi - synthetic documents , especially for larger corpora .",
    "results for semi - synthetic corpora drawn from topics trained on ny times articles are shown in fig .",
    "[ fig : nyt - ss - l1 ] for corpus sizes ranging from 50k to 2 m synthetic documents .",
    "in addition , we report results for the three recover algorithms on `` infinite data , '' that is , the true @xmath34 matrix from the model used to generate the documents .",
    "error bars show variation between topics .",
    "recover performs poorly in all but the noiseless , infinite data setting .",
    "gibbs sampling has lower @xmath152 with smaller corpora , while the new algorithms get better recovery and lower variance with more data ( although more sampling might reduce mcmc error further ) .",
    "error for a semi - synthetic model generated from a model trained on ny times articles with @xmath150 .",
    "the horizontal line indicates the @xmath152 error of @xmath1 uniform distributions . ]",
    "results for semi - synthetic corpora drawn from nips topics are shown in fig .",
    "[ fig : nips - ss - l1 ] .",
    "recover does poorly for the smallest corpora ( topic matching fails for @xmath162 , so @xmath152 is not meaningful ) , but achieves moderate error for @xmath47 comparable to the ny times corpus .",
    "recoverkl and recoverl2 also do poorly for the smallest corpora , but are comparable to or better than gibbs sampling , with much lower variance , after 40,000 documents .     error for a semi - synthetic model generated from a model trained on nips papers with @xmath150 .",
    "recover fails for @xmath162 . ]",
    "the non - negative algorithms are more robust to violations of the separability assumption than the original recover algorithm . in fig .",
    "[ fig : nips - ss - l1 ] , recover does not achieve zero @xmath152 error even with noiseless `` infinite '' data . here",
    "we show that this is due to lack of separability . in our semi - synthetic corpora ,",
    "documents are generated from the lda model , but the topic - word distributions are learned from data and may not satisfy the anchor words assumption .",
    "we test the sensitivity of algorithms to violations of the separability condition by adding a synthetic anchor word to each topic that is by construction unique to the topic .",
    "we assign the synthetic anchor word a probability equal to the most probable word in the original topic .",
    "this causes the distribution to sum to greater than 1.0 , so we renormalize .",
    "results are shown in fig .",
    "[ fig : nyt - ss - anchor - l1 ] .",
    "the @xmath152 error goes to zero for recover , and close to zero for recoverkl and recoverl2 .",
    "the reason recoverkl and recoverl2 do not reach exactly zero is because we do not solve the optimization problems to perfect optimality .",
    "error goes to zero for recover and close to zero for recoverkl and recoverl2 . ]",
    "the theoretical guarantees of the new algorithms apply even if topics are correlated . to test how algorithms respond to correlation , we generated new synthetic corpora from the same @xmath150 model trained on ny times articles .",
    "instead of a symmetric dirichlet distribution , we use a logistic normal distribution with a block - structured covariance matrix .",
    "we partition topics into 10 groups . for each pair of topics in a group , we add a non - zero off - diagonal element to the covariance matrix .",
    "this block structure is not necessarily realistic , but shows the effect of correlation .",
    "results for two levels of covariance ( @xmath163 ) are shown in fig .",
    "[ fig : nyt - ss - corr - l1 ] .",
    "error increases as we increase topic correlation .",
    "we use the same @xmath150 topic model from ny times articles , but add correlation : top @xmath164 , bottom @xmath165.,title=\"fig : \" ]   error increases as we increase topic correlation .",
    "we use the same @xmath150 topic model from ny times articles , but add correlation : top @xmath164 , bottom @xmath165.,title=\"fig : \" ]    results for recover are much worse in both cases than the dirichlet - generated corpora in fig .",
    "[ fig : nyt - ss - l1 ] . the other three algorithms , especially gibbs sampling , are more robust to correlation , but performance consistently degrades as correlation increases , and improves with larger corpora . with infinite data",
    "@xmath152 error is equal to @xmath152 error in the uncorrelated synthetic corpus ( non - zero because of violations of the separability assumption ) .",
    "the new algorithms produce comparable quantitative and qualitative results on real data .",
    "[ fig : real - data ] shows three metrics for both corpora .",
    "error bars show the distribution of log probabilities across held - out _ documents _ ( top panel ) and coherence and unique words across _ topics _ ( center and bottom panels ) .",
    "held - out sets are 230 documents for nips and 59k for ny times . for the small nips",
    "corpus we average over 5 non - overlapping train / test splits .",
    "the matrix - inversion in recover failed for the smaller corpus ( nips ) . in the larger corpus ( ny times ) , recover produces noticeably worse held - out log probability per token than the other algorithms .",
    "gibbs sampling produces the best average held - out probability ( @xmath166 under a paired @xmath167-test ) , but the difference is within the range of variability between documents .",
    "we tried several methods for estimating hyperparameters , but the observed differences did not change the relative performance of algorithms .",
    "gibbs sampling has worse coherence than the recover algorithms , but produces more unique words per topic .",
    "these patterns are consistent with semi - synthetic results for similarly sized corpora ( details are in supplementary material ) .    for each ny times topic learned by recoverl2",
    "we find the closest gibbs topic by @xmath152 distance .",
    "the closest , median , and farthest topic pairs are shown in table [ tbl : samples ] .",
    "we observe that when there is a difference , recover - based topics tend to have more specific words ( _ anaheim angels _ vs. _ pitch _ ) .",
    "[ tbl : samples ]",
    "we present new algorithms for topic modeling , inspired by @xcite , which are efficient and simple to implement yet maintain provable guarantees .",
    "the running time of these algorithms is effectively independent of the size of the corpus .",
    "empirical results suggest that the sample complexity of these algorithms is somewhat greater than mcmc , but , particularly for the @xmath125 variant , they provide comparable results in a fraction of the time .",
    "we have tried to use the output of our algorithms as initialization for further optimization ( e.g. using mcmc ) but have not yet found a hybrid that out - performs either method by itself . finally ,",
    "although we defer parallel implementations to future work , these algorithms are parallelizable , potentially supporting web - scale topic inference .",
    "25 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    anandkumar , a. , foster , d. , hsu , d. , kakade , s. , and liu , y. two svds suffice : spectral decompositions for probabilistic topic modeling and latent dirichlet allocation . in _",
    "nips _ , 2012 .",
    "arora , s. , ge , r. , kannan , r. , and moitra , a. computing a nonnegative matrix factorization  provably . in",
    "_ stoc _ , pp .   145162 , 2012 .",
    "arora , s. , ge , r. , and moitra , a. learning topic models  going beyond svd . in _",
    "focs _ , 2012 .",
    "bittorf , v. , recht , b. , re , c. , and tropp , j. factoring nonnegative matrices with linear programs . in _ nips _ , 2012 .",
    "blei , d. introduction to probabilistic topic models . _ communications of the acm _ , pp .   7784 , 2012 .",
    "blei , d. and lafferty , j. a correlated topic model of science . _",
    "annals of applied statistics _ , pp .   1735 , 2007 .",
    "blei , d. , ng , a. , and jordan , m. latent dirichlet allocation .",
    "_ journal of machine learning research _ , pp .   9931022 , 2003 .",
    "preliminary version in _ nips _ 2001 .",
    "buntine , wray  l. estimating likelihoods for topic models . in _",
    "asian conference on machine learning _ , 2009 .",
    "deerwester , s. , dumais , s. , landauer , t. , furnas , g. , and harshman , r. indexing by latent semantic analysis .",
    "_ jasis _ , pp .   391407 , 1990 .",
    "donoho , d. and stodden , v. when does non - negative matrix factorization give the correct decomposition into parts ? in _ nips _ , 2003 .",
    "gillis , n. robustness analysis of hotttopixx , a linear programming model for factoring nonnegative matrices , 2012 .",
    "http://arxiv.org/abs/1211.6687 .",
    "gomez , c. , borgne , h.  le , allemand , p. , delacourt , c. , and ledru , p. n - findr method versus independent component analysis for lithological identification in hyperspectral imagery .",
    "j. remote sens .",
    "_ , 280 ( 23 ) , january 2007 .",
    "griffiths , t.  l. and steyvers , m. finding scientific topics . _",
    "proceedings of the national academy of sciences _ , 101:0 52285235 , 2004 .",
    "kivinen , jyrki and warmuth , manfred  k. exponentiated gradient versus gradient descent for linear predictors .",
    "_ inform . and comput .",
    "_ , 132 , 1995 .",
    "kumar , a. , sindhwani , v. , and kambadur , p. fast conical hull algorithms for near - separable non - negative matrix factorization .",
    "http://arxiv.org/abs/1210.1190v1 .",
    "li , w. and mccallum , a. pachinko allocation : dag - structured mixture models of topic correlations . in _ icml _ , pp .   633640 , 2007 .",
    "mccallum , a.k .",
    "mallet : a machine learning for language toolkit , 2002 .",
    "http://mallet.cs.umass.edu .",
    "mimno , david , wallach , hanna , talley , edmund , leenders , miriam , and mccallum , andrew . optimizing semantic coherence in topic models . in _ emnlp _ , 2011 .",
    "nascimento , j.m .",
    "p. and dias , j. m.  b. vertex component analysis : a fast algorithm to unmix hyperspectral data .",
    "_ ieee trans .",
    "sens _ , 43:0 898910 , 2004 .",
    "nocedal , j. and wright , s.  j. _ numerical optimization_. springer , new york , 2nd edition , 2006 .",
    "stevens , keith , kegelmeyer , philip , andrzejewski , david , and buttler , david . exploring topic coherence over many models and many topics . in _",
    "emnlp _ , 2012 .",
    "thurau , c. , kersting , k. , and bauckhage , c. yes we can  simplex volume maximization for descriptive web  scale matrix factorization . in _",
    "cikm10 _ , 2010 .",
    "wallach , hanna , murray , iain , salakhutdinov , ruslan , and mimno , david .",
    "evaluation methods for topic models . in _ icml _ , 2009 .",
    "wedin , p. perturbation bounds in connection with singular value decomposition .",
    "_ bit numerical mathematics _ , 120 ( 1):0 99111 , 1972",
    ".    yao , limin , mimno , david , and mccallum , andrew .",
    "efficient methods for topic model inference on streaming document collections . in _",
    "kdd _ , 2009 .",
    "recall that the correctness of the algorithm depends on the following lemmas :    [ lem : perturbation ] there is a vertex @xmath142 whose distance from @xmath115 is at least @xmath143 .",
    "[ lem : inductionforfastalg ] the point @xmath168 found by the algorithm must be @xmath144 close to some vertex @xmath142 .    in order to prove lemma  [ lem : perturbation ] , we use a volume argument .",
    "first we show that the volume of a robust simplex can not change by too much when the vertices are perturbed .",
    "[ lem : volperturbation ] suppose @xmath140 are the vertices of a @xmath26-robust simplex @xmath121 .",
    "let @xmath169 be a simplex with vertices @xmath120 , each of the vertices @xmath170 is a perturbation of @xmath142 and @xmath171 . when @xmath172 the volume of the two simplices satisfy    @xmath173    as the volume of a simplex is proportional to the determinant of a matrix whose columns are the edges of the simplex , we first show the following perturbation bound for determinant .",
    "let @xmath10 , @xmath174 be @xmath23 matrices , the smallest eigenvalue of @xmath10 is at least @xmath26 , the frobenius norm @xmath175 , when @xmath176 we have    @xmath177    since @xmath178 , we can multiply both @xmath10 and @xmath179 by @xmath180 .",
    "hence @xmath181 .",
    "the frobenius norm of @xmath182 is bounded by    @xmath183    let the eigenvalues of @xmath182 be @xmath184 , then by definition of frobenius norm @xmath185 .",
    "the eigenvalues of @xmath186 are just @xmath187 , and the determinant @xmath188 .",
    "hence it suffices to show    @xmath189    to do this we apply lagrangian method and show the minimum is only obtained when all @xmath190 s are equal .",
    "the optimal value must be obtained at a local optimum of    @xmath191    taking partial derivatives with respect to @xmath190 s , we get the equations @xmath192 ( here using @xmath193 is small so @xmath194 ) . the right hand side is a constant , so each @xmath190 must be one of the two solutions of this equation .",
    "however , only one of the solution is larger than @xmath195 , therefore all the @xmath190 s are equal .    for the lower bound",
    ", we can project the perturbed subspace to the @xmath145 dimensional space .",
    "such a projection can not increase the volume and the perturbation distances only get smaller .",
    "therefore we can apply the claim directly , the columns of @xmath10 are just @xmath196 for @xmath197 ; columns of @xmath174 are just @xmath198 .",
    "the smallest eigenvalue of @xmath10 is at least @xmath26 because the polytope is @xmath26 robust , which is equivalent to saying after orthogonalization each column still has length at least @xmath26 .",
    "the frobenius norm of @xmath174 is at most @xmath199 .",
    "we get the lower bound directly by applying the claim .    for the upper bound ,",
    "swap the two sets @xmath121 and @xmath169 and use the argument for the lower bound .",
    "the only thing we need to show is that the smallest eigenvalue of the matrix generated by points in @xmath169 is still at least @xmath143 .",
    "this follows from wedin s theorem @xcite and the fact that @xmath200 .",
    "now we are ready to prove lemma  [ lem : perturbation ] .",
    "the first case is for the first step of the algorithm , when we try to find the farthest point to the origin .",
    "here essentially @xmath201 .",
    "for any two vertices @xmath202 , since the simplex is @xmath26 robust , the distance between @xmath203 and @xmath204 is at least @xmath26 . which means @xmath205 , one of them must be at least @xmath143 .",
    "for the later steps , recall that @xmath121 contains vertices of a perturbed simplex .",
    "let @xmath169 be the set of original vertices corresponding to the perturbed vertices in @xmath121 .",
    "let @xmath124 be any vertex in @xmath140 which is not in @xmath121 .",
    "now we know the distance between @xmath124 and @xmath121 is equal to @xmath206 . on the other hand , we know @xmath207 . using lemma  [ lem : volperturbation ] to bound the ratio between the two pairs @xmath208 and @xmath209 , we get : @xmath210 when @xmath211 .",
    "lemma  [ lem : inductionforfastalg ] is based on the following observation : in a simplex the point with largest @xmath125 is always a vertex . even if two vertices have the same norm if they are not close to each other the vertices on the edge connecting them will have significantly lower norm .",
    "( lemma  [ lem : inductionforfastalg ] )    since @xmath113 is the point found by the algorithm , let us consider the point @xmath129 before perturbation .",
    "the point @xmath129 is inside the simplex , therefore we can write @xmath129 as a convex combination of the vertices :    @xmath212    let @xmath213 be the vertex with largest coefficient @xmath214 .",
    "let @xmath215 be the largest distance from some vertex to the space spanned by points in @xmath121 ( @xmath216 . by lemma  [ lem : perturbation ]",
    "we know @xmath217 . also notice that we are not assuming @xmath218 .",
    "now we rewrite @xmath129 as @xmath219 , where @xmath155 is a vector in the convex hull of vertices other than @xmath213 .",
    "observe that @xmath129 must be far from @xmath115 , because @xmath113 is the farthest point found by the algorithm .",
    "indeed : @xmath220    the second inequality is because there must be some point @xmath221 that correspond to the farthest vertex @xmath222 and have @xmath223 .",
    "thus as @xmath113 is the farthest point @xmath224 .",
    "the point @xmath129 is on the segment connecting @xmath213 and @xmath155 , the distance between @xmath129 and @xmath115 is not much smaller than that of @xmath213 and @xmath155 .",
    "following the intuition in @xmath125 norm when @xmath213 and @xmath155 are far we would expect @xmath129 to be very close to either @xmath213 or @xmath155 . since @xmath225 it can not be really close to @xmath155 ,",
    "so it must be really close to @xmath213 .",
    "we formalize this intuition by the following calculation ( see figure  [ fig : twodimensional ] ) :    , after projecting to the orthogonal subspace of @xmath115.,width=336 ]    project everything to the orthogonal subspace of @xmath115 ( points in @xmath115 are now at the origin ) .",
    "after projection distance to @xmath115 is just the @xmath125 norm of a vector . without loss of generality",
    "we assume @xmath226 because these two have length at most @xmath215 , and extending these two vectors to have length @xmath215 can only increase the length of @xmath113 .",
    "the point @xmath213 must be far from @xmath155 by applying lemma  [ lem : perturbation ] : consider the set of vertices @xmath227 .",
    "the set @xmath228 satisfy the assumptions in lemma  [ lem : perturbation ] so there must be one vertex that is far from @xmath229 , and it can only be @xmath213 . therefore even after projecting to orthogonal subspace of @xmath115 , @xmath213 is still far from any convex combination of @xmath230 .",
    "the vertices that are not in @xmath230 all have very small norm after projecting to orthogonal subspace ( at most @xmath231 ) so we know the distance of @xmath213 and @xmath155 is at least @xmath232 .",
    "now the problem becomes a two dimensional calculation .",
    "when @xmath214 is fixed the length of @xmath129 is strictly increasing when the distance of @xmath213 and @xmath155 decrease , so we assume the distance is @xmath233 . simple calculation ( using essentially just pythagorean theorem ) shows    @xmath234    the right hand side is largest when @xmath235 ( since the vectors are in unit ball ) and the maximum value is @xmath236 .",
    "when this value is smaller than @xmath237 , we must have @xmath238 .",
    "thus @xmath239 and @xmath240 .",
    "the cleanup phase tries to find the farthest point to a subset of @xmath145 vertices , and use that point as the @xmath1-th vertex .",
    "this will improve the result because when we have @xmath145 points close to @xmath145 vertices , only one of the vertices can be far from their span . therefore the farthest point must be close to the only remaining vertex .",
    "another way of viewing this is that the algorithm is trying to greedily maximize the volume of the simplex , which makes sense because the larger the volume is , the more words / documents the final lda model can explain .",
    "the following lemma makes the intuitions rigorous and shows how cleanup improves the guarantee of lemma  [ lem : inductionforfastalg ] .",
    "[ lem : cleanup ] suppose @xmath146 and each point in @xmath121 is @xmath147 close to some vertex @xmath142 , then the farthest point @xmath148 found by the algorithm is @xmath149 close to the remaining vertex .",
    "we still look at the original point @xmath129 and express it as @xmath241 . without loss of generality let @xmath203 be the vertex that does not correspond to anything in @xmath121 . by lemma  [ lem : perturbation ]",
    "@xmath203 is @xmath143 far from @xmath115 . on the other hand",
    "all other vertices are at least @xmath242 close to @xmath115 .",
    "we know the distance @xmath243 , this can not be true unless @xmath244 .",
    "these lemmas directly lead to the following theorem :    [ thm : fastanchorwords ] fastanchorwords algorithm runs in time @xmath135 and outputs a subset of @xmath245 of size @xmath1 that @xmath137-covers the vertices provided that @xmath138 .    in the first phase of the algorithm , do induction using lemma  [ lem : inductionforfastalg ] .",
    "when @xmath138 lemma  [ lem : inductionforfastalg ] shows that we find a set of points that @xmath236-covers the vertices .",
    "now lemma  [ lem : cleanup ] shows after cleanup phase the points are refined to @xmath137-cover the vertices .",
    "in order to show recoverl2 learns the parameters even when the rows of @xmath65 are perturbed , we need the following lemma that shows when columns of @xmath65 are close to the expectation , the posteriors @xmath246 computed by the algorithm is also close to the true value .",
    "[ lem : recoverposterior ] for a @xmath26 robust simplex @xmath121 with vertices @xmath140 , let @xmath124 be a point in the simplex that can be represented as a convex combination @xmath247 . if the vertices of @xmath121 are perturbed to @xmath248 where @xmath249 and @xmath124 is perturbed to @xmath250 where @xmath251",
    "let @xmath252 be the point in @xmath169 that is closest to @xmath250 , and @xmath253 , when @xmath254 for all @xmath255 $ ] @xmath256 .",
    "consider the point @xmath257 , by triangle inequality : @xmath258 .",
    "hence @xmath259 , and @xmath260 is in @xmath169 .",
    "the point @xmath252 is the point in @xmath169 that is closest to @xmath250 , so @xmath261 and @xmath262 .",
    "then we need to show when a point ( @xmath260 ) moves a small distance , its representation also changes by a small amount .",
    "intuitively this is true because @xmath121 is @xmath26 robust . by lemma  [ lem : perturbation ]",
    "when @xmath263 , the simplex @xmath169 is also @xmath143 robust . for any @xmath7 ,",
    "let @xmath264 and @xmath265 be the projections of @xmath252 and @xmath260 in the orthogonal subspace of @xmath266 , then @xmath267 and this completes the proof .    with this lemma",
    "it is not hard to show that recoverl2 has polynomial sample complexity .",
    "when the number of documents @xmath13 is at least @xmath268 our algorithm using the conjunction of fastanchorwords and recoverl2 learns the @xmath10 matrix with entry - wise error at most @xmath29 .",
    "( sketch ) we can assume without loss of generality that each word occurs with probability at least @xmath269 and furthermore that if @xmath13 is at least @xmath270 then the empirical matrix @xmath271 is entry - wise within an additive @xmath272 to the true @xmath273 see @xcite for the details . also , the @xmath1 anchor rows of @xmath65 form a simplex that is @xmath274 robust .",
    "the error in each column of @xmath65 can be at most @xmath275 . by theorem  [ thm : fastanchorwords ]",
    "when @xmath276 ( which is satisfied when @xmath277 ) , the anchor words found are @xmath278 close to the true anchor words .",
    "hence by lemma  [ lem : recoverposterior ] every entry of @xmath85 has error at most @xmath279 .    with such number of documents ,",
    "all the word probabilities @xmath280 are estimated more accurately than the entries of @xmath281 , so we omit their perturbations here for simplicity .",
    "when we apply the bayes rule , we know @xmath282 , where @xmath283 is @xmath24 which is lower bounded by @xmath284 . the numerator and denominator",
    "are all related to entries of @xmath85 with positive coefficients sum up to at most 1 .",
    "therefore the errors @xmath285 and @xmath286 are at most the error of a single entry of @xmath85 , which is bounded by @xmath279 . applying taylor s expansion to @xmath287 ,",
    "the error on entries of @xmath10 is at most @xmath288 .",
    "when @xmath289 , we have @xmath290 , and get the desired accuracy of @xmath10 .",
    "the number of document required is @xmath291 .    the sample complexity for @xmath27",
    "can then be bounded using matrix perturbation theory .",
    "this section contains plots for @xmath152 , held - out probability , coherence , and uniqueness for all semi - synthetic data sets . up is better for all metrics except @xmath152 error .",
    ".,title=\"fig : \" ] .,title=\"fig : \" ] .,title=\"fig : \" ] .,title=\"fig : \" ]    , with a synthetic anchor word added to each topic.,title=\"fig : \" ] , with a synthetic anchor word added to each topic.,title=\"fig : \" ] , with a synthetic anchor word added to each topic.,title=\"fig : \" ] , with a synthetic anchor word added to each topic.,title=\"fig : \" ]    , with moderate correlation between topics.,title=\"fig : \" ] , with moderate correlation between topics.,title=\"fig : \" ] , with moderate correlation between topics.,title=\"fig : \" ] , with moderate correlation between topics.,title=\"fig : \" ]    , with stronger correlation between topics.,title=\"fig : \" ] , with stronger correlation between topics.,title=\"fig : \" ] , with stronger correlation between topics.,title=\"fig : \" ] , with stronger correlation between topics.,title=\"fig : \" ]    . for @xmath292 , recover produces log probabilities of @xmath293 for some held - out documents.,title=\"fig : \" ] . for @xmath292 , recover produces log probabilities of @xmath293 for some held - out documents.,title=\"fig : \" ] . for @xmath292 , recover produces log probabilities of @xmath293 for some held - out documents.,title=\"fig : \" ] . for @xmath292 , recover produces log probabilities of @xmath293 for some held - out documents.,title=\"fig : \" ]      tables [ tbl : samples1 ] , [ tbl : samples2 ] , and [ tbl : samples3 ] show 100 topics trained on real ny times articles using the recoverl2 algorithm . each topic is followed by the most similar topic ( measured by @xmath152 distance ) from a model trained on the same documents with gibbs sampling . when the anchor word is among the top six words by probability it is highlighted in bold .",
    "note that the anchor word is frequently not the most prominent word .",
    "[ tbl : samples1 ]    [ tbl : samples2 ]    [ tbl : samples3 ]",
    "for each document , let @xmath294 be the vector in @xmath295 such that the @xmath7-th entry is the number of times word @xmath7 appears in document @xmath5 , @xmath296 be the length of the document and @xmath12 be the topic vector chosen according to dirichlet distribution when the documents are generated .",
    "conditioned on @xmath12 s , our algorithms require the expectation of @xmath34 to be @xmath297 .    in order to achieve this , similar to @xcite , let the normalized vector @xmath298 and diagonal matrix @xmath299 . compute the matrix @xmath300 } e_{z_{d , i}}e_{z_{d , j}}^t.\\ ] ] here @xmath301 is the @xmath7-th word of document @xmath5 , and @xmath302 is the basis vector . from the generative model , the expectation of all terms @xmath303 are equal to @xmath304 , hence by linearity of expectation we know @xmath305 = aw_dw_d^t a^t.$ ]    if we collect all the column vectors @xmath306 to form a large sparse matrix @xmath307 , and compute the sum of all @xmath308 to get the diagonal matrix @xmath309 , we know @xmath310 has the desired expectation .",
    "the running time of this step is @xmath311 where @xmath312 is the expectation of the length of the document squared .",
    "the optimization problem that arises in recoverkl and recoverl2 has the following form , @xmath313 where @xmath314 is a bregman divergence , @xmath122 is a vector of length @xmath1 , and @xmath315 is a matrix of size @xmath11 .",
    "we solve this optimization problem using the exponentiated gradient algorithm @xcite , described in algorithm  [ alg : gradientdescent ] . in our experiments",
    "we show results using both squared euclidean distance and kl divergence for the divergence measure .",
    "stepsizes are chosen with a line search to find an @xmath316 that satisfies the wolfe and armijo conditions ( for details , see @xcite ) .",
    "we test for convergence using the kkt conditions .",
    "writing the kkt conditions for our constrained minimization problem :      for every iterate of @xmath122 generated by exponentiated gradient , we set @xmath322 to satisfy conditions 1 - 3 .",
    "this gives the following equations : @xmath323 by construction conditions 1 - 3 are satisfied ( note that the multiplicative update and the projection step ensure that @xmath122 is always primal feasible ) .",
    "convergence is tested by checking whether the final kkt condition holds within some tolerance . since @xmath324 and @xmath122 are nonnegative , we check complimentary slackness by testing whether @xmath325",
    ". this convergence test can also be thought of as testing the value of the primal - dual gap , since the lagrangian function has the form : @xmath326 , and @xmath327 is zero at every iteration .",
    "matrix @xmath315 , vector @xmath328 , divergence measure @xmath314 , tolerance parameter @xmath29 non - negative normalized vector @xmath122 close to @xmath329 , the minimizer of @xmath330 initialize @xmath331 initialize converged @xmath36 false @xmath332 choose a step size @xmath333 @xmath334 ( gradient step ) @xmath335 ( projection onto the simplex ) @xmath336 @xmath337 converged @xmath338    the running time of recoverl2 is the time of solving @xmath0 small ( @xmath23 ) quadratic programs .",
    "especially when using exponentiated gradient to solve the quadratic program , each word requires @xmath339 time for preprocessing and @xmath340 per iteration .",
    "the total running time is @xmath341 where @xmath315 is the average number of iterations .",
    "the value of @xmath315 is about @xmath342 depending on data sets ."
  ],
  "abstract_text": [
    "<S> topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora . </S>",
    "<S> most approaches to topic model inference have been based on a maximum likelihood objective . </S>",
    "<S> efficient algorithms exist that approximate this objective , but they have no provable guarantees . </S>",
    "<S> recently , algorithms have been introduced that provide provable bounds , but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions . in this paper </S>",
    "<S> we present an algorithm for topic model inference that is both provable and practical . </S>",
    "<S> the algorithm produces results comparable to the best mcmc implementations while running orders of magnitude faster . </S>"
  ]
}