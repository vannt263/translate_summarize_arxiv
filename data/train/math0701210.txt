{
  "article_text": [
    "independent component analysis ( ica ) @xcite aims to recover linearly or non - linearly mixed independent and hidden sources .",
    "there is a broad range of applications for ica , such as blind source separation , feature extraction and denoising .",
    "particular applications include the analysis of financial and neurobiological data , fmri , eeg , and meg . for recent review concerning ica the reader",
    "is referred to the literature @xcite .",
    "traditional ica algorithms are in the sense that all sources are assumed to be independent real valued random variables .",
    "nonetheless , applications in which only certain groups of sources are independent may be highly relevant in practice . in this case",
    ", the independent sources can be multidimensional .",
    "for instance , consider the generalization of the cocktail - party problem , where _ independent groups of people _ are talking about independent topics or more than one _ group of musicians _ is playing at the party .",
    "the separation task requires an extension of ica , which can be called independent subspace analysis ( isa ) @xcite , multidimensional independent component analysis ( mica ) @xcite , and group ica @xcite .",
    "we will use the first of these abbreviations throughout this paper .",
    "strenuous efforts have been made to develop isa algorithms @xcite .",
    "for the most part , isa - related theoretical problems concern the estimation of entropy or of mutual information . for this ,",
    "the @xmath0-nearest neighbors @xcite and the geodesic spanning tree methods @xcite can be applied .",
    "other recent approaches seek independent subspaces via kernel methods @xcite and joint block diagonalization @xcite .",
    "another extension of the original ica task is the blind source deconvolution ( bsd ) problem .",
    "such a problem emerges , for example , at a cocktail - party being held in an _",
    "echoic room_. several bsd algorithms were developed in the past .",
    "see , for example , the review of @xcite . like ica",
    ", bsd has several applications : ( i ) remote sensing applications ; passive radar / sonar processing @xcite , ( ii ) image - deblurring , image restoration @xcite , ( iii ) speech enhancement using microphone arrays , acoustics @xcite , ( iv ) multi - antenna wireless communications , sensor networks @xcite , ( v ) biomedical signal ",
    "eeg , ecg , meg , fmri ",
    "analysis @xcite , ( vi ) optics @xcite , ( vii ) seismic exploration @xcite .",
    "the simultaneous assumption of the two extensions , that is , isa combined with bsd , seems to be a more realistic model than either of the two models alone .",
    "for example , at the cocktail - party , groups of people or groups of musicians may form independent source groups and echoes could be present .",
    "this task will be called blind subspace deconvolution ( bssd ) .",
    "we treat the undercomplete case ( ubssd ) here . in terms of the cocktail - party problem",
    ", it is assumed that there are more microphones than acoustic sources .",
    "here we note that the complete , and in particular the overcomplete , bssd task is challenging and as of yet no general solution is known .",
    "we can show that temporal concatenation turns the ubssd task into an isa problem .",
    "one of the most stringent applications of bssd could be the analysis of eeg or fmri signals .",
    "the ica assumptions could be highly problematic here , because some sources may depend one another , so an isa model seems better .",
    "furthermore , the passing of information from one area to another and the related delayed and transformed activities may be modeled as echoes .",
    "thus , one can argue that bssd may fit this important problem domain better than ica or even isa .    in principle , the isa problem can be treated with the methods listed above .",
    "however , the dimension of the isa problem derived from an ubssd task is not amenable to state - of - the - art isa methods .",
    "according to a recent decomposition principle , the isa separation theorem @xcite , the isa task can be divided into two consecutive steps under certain conditions : after the application of the ica algorithm , the ica elements need to be grouped .",
    "the importance of this direction stems from the fact that ica methods can deal with problems in high dimensions .",
    "the derived isa task will be solved with the use of the decomposition principle augmented by the joint ( jfd ) technique @xcite .",
    "we show other isa approaches beyond the jfd method : we adapted the kernel canonical correlation analysis ( kcca ) and the kernel generalized variance ( kgv ) methods @xcite to measure the mutual dependency of multidimensional variables .",
    "one can show that similarly to the jfd and the kcca methods , the kgv technique deals with nonlinear decorrelation in function spaces .",
    "we found that they can be more precise but are limited to smaller problems .",
    "the paper is structured as follows : section  [ sec : bssd+isa - model ] formulates the problem domain .",
    "section  [ sec : reduction - steps ] shows how to transform the ubssd task into an isa task .",
    "the jfd method , which we use to solve the derived isa task , is the subject of section  [ sec : isa - methods ] .",
    "this section also addresses how to tailor the kcca and kgv kernel - ica methods to solve the isa problem .",
    "section  [ sec : illustrations ] contains the numerical illustrations and conclusions are drawn in section  [ sec : conclusions ] .",
    "the bssd task and its special case , the isa model , are defined in section  [ sec : bssd - model ] .",
    "section  [ sec : isa - ambiguities ] details the ambiguities of the isa task .",
    "section  [ sec : isa - cost ] introduces some possible isa cost functions .      here",
    ", we define the bssd task .",
    "assume that we have @xmath1 hidden , independent , multidimensional _ components _ ( random variables ) .",
    "suppose also that only their casual fir filtered mixture is available for observation : in @xmath2 .",
    "fir : the number of terms in the sum is finite . ]",
    "@xmath3 where @xmath4\\in{\\mathbb{r}}^{md}$ ] is a vector concatenated of components @xmath5 . for a given @xmath6",
    ", @xmath7 is i.i.d .",
    "( independent and identically distributed ) in time @xmath8 , @xmath9s are non - gaussian , and @xmath10 , where @xmath11 stands for the mutual information of the arguments .",
    "the total dimension of the components is , the dimension of the observation @xmath12 is @xmath13 .",
    "matrices @xmath14 @xmath15 describe the mixing , these are the _ mixing matrices_. without any loss of generality it may be assumed that @xmath16={\\mathbf}{0}$ ] , where @xmath17 denotes the expectation value .",
    "then @xmath18={\\mathbf}{0}$ ] holds , as well .",
    "the goal of the bssd problem is to estimate the original source @xmath19 by using observations @xmath20 only .",
    "the case @xmath21 corresponds to the isa task , and if @xmath22 also holds then the ica task is recovered . in the bsd task @xmath22 and @xmath23 is a non - negative integer .",
    "@xmath24 is the _ undercomplete _ , @xmath25 is the _ complete _ , and @xmath26 is the _ overcomplete _ task . here , we treat the undercomplete bssd ( ubssd ) problem",
    ". we will transform the ubssd task to undercomplete isa ( uisa ) or to complete isa . from now on they both will be called isa .    mixing matrices @xmath27 ( @xmath28 ) have a one - to - one mapping to polynomial matrix$ ] is also known as _ channel matrix _ or _ transfer function _ in the literature . ] , where @xmath29 is the time - shift operation , that is .",
    "@xmath30 $ ] may be regarded as an operation that maps @xmath31-dimensional series to @xmath13-dimensional series .",
    "equation   can be written as @xmath32{\\mathbf}{s}$ ] .",
    "[ note : fir - inverse - exists ] it can be shown @xcite that in the ubssd task @xmath30 $ ] has a polynomial matrix left inverse @xmath33\\in{\\mathbb{r}}[z]^{d_x\\times d_s}$ ] with probability 1 , under mild conditions . in other words , for these polynomial matrices @xmath33 $ ] and @xmath30 $ ] , @xmath33{\\mathbf}{h}[z]$ ] is the identity mapping .",
    "the mild condition is as follows : coefficients of polynomial matrix @xmath30 $ ] , that is , random matrix @xmath34 $ ] is drawn from a continuous distribution . under this condition ,",
    "hidden source @xmath19 can be estimated by a _",
    "suitable _ causal fir filtered form of observation @xmath20 .    for the ubssd task",
    "it is assumed that @xmath30 $ ] has a polynomial matrix left inverse .",
    "for the uisa and isa tasks it is supposed that _ mixing matrix _ @xmath35 has full column rank , that is its rank is @xmath31 .      because the ubssd task will be reduced to isa",
    ", it is important to see the ambiguities of the isa task .",
    "first , the complete isa problem ( @xmath36 ) is presented , the undercomplete isa will be treated later .",
    "the identification of the isa model is ambiguous .",
    "however , the ambiguities are simple @xcite : hidden multidimensional components can be determined up to permutation and up to invertible transformation within the subspaces .",
    "ambiguities within the subspaces can be weakened .",
    "namely , because of the invertibility of mixing matrix , it can be assumed without any loss of generality that both the sources and the observation are _ white _ , that is , @xmath37&={\\mathbf}{0},cov\\left[{\\mathbf}{s}\\right]={\\mathbf}{i}_{d_s},\\\\ e[{\\mathbf}{x}]&={\\mathbf}{0},cov\\left[{\\mathbf}{x}\\right]={\\mathbf}{i}_{d_x},\\end{aligned}\\ ] ] where @xmath38 is the identity matrix and @xmath39 is the covariance matrix .",
    "it then follows that the mixing matrix @xmath40 and thus the _ demixing matrix _",
    "@xmath41 are orthogonal : @xmath42=e\\left[{\\mathbf}{x}{\\mathbf}{x}^*\\right]={\\mathbf}{h}_0e\\left[{\\mathbf}{s}{\\mathbf}{s}^*\\right]{\\mathbf}{h}_0^*={\\mathbf}{h}_0{\\mathbf}{i}_{d_s}{\\mathbf}{h}_0^*={\\mathbf}{h}_0{\\mathbf}{h}_0^*,\\ ] ] where @xmath43 denotes transposition . in sum , @xmath44 , where @xmath45 denotes the set of orthogonal matrices .",
    "now , @xmath9 sources are determined up to permutation and orthogonal transformation .    in order to transform the undercomplete isa task into a complete isa task with white observations let denote the covariance matrix of the observation .",
    "rank of @xmath46 is @xmath31 , since the rank of matrix @xmath40 is @xmath31 according to our assumptions .",
    "matrix @xmath46 is symmetric ( @xmath47 ) , thus it can be decomposed as follows : @xmath48 , where @xmath49 , and the columns of matrix @xmath50 are orthogonal , that is , @xmath51 .",
    "furthermore , the rank of diagonal matrix @xmath52 is @xmath31 .",
    "the principal component analysis can provide a decomposition in the desired form .",
    "let @xmath53 .",
    "then the original observation @xmath12 can be modified to @xmath54 .",
    "the resulting @xmath55 is white and can be regarded as the observation of a _ complete _ isa task having mixing matrix @xmath56 .",
    "after the whitening procedure ( section  [ sec : isa - ambiguities ] ) , the isa task can be viewed as the minimization of the mutual information between the estimated components on the orthogonal group : @xmath57 where , , @xmath58 , and @xmath59 .",
    "this formulation of the isa task serves us in section  [ sec : studied - isa - costs ] , where we estimate the dependencies of the multidimensional variables .",
    "the isa task can be rewritten into the minimization of the sum of shannon s multidimensional differential entropies @xcite : @xmath60 where , , @xmath58 , @xmath59 .    until now , we formulated the isa task by means of the entropy or the mutual information of multidimensional random variables , see equations   and",
    ". however , any algorithm that treats mutual information between _ 1-dimensional _ random variables can also be sufficient .",
    "this statement is based on the considerations below .",
    "well - known identities of mutual information and entropy expressions @xcite show that the minimization of cost function @xmath61 or that of @xmath62 can also solve the isa task . here ,",
    "is the estimated isa source , where @xmath63 is the whitened observation in the isa model .",
    "@xmath64 is the estimated isa demixing matrix , and in the @xmath65 , @xmath66 , represent the estimated components with coordinates @xmath67 . the first term of both cost functions @xmath68 and @xmath69 is an ica cost function .",
    "thus , these first terms can be fixed by means of ica preprocessing .",
    "suits the ica preprocessing step . ] in this case , if the separation theorem holds ( for details see section  [ sec : isa2ica ] ) , then term @xmath70 implies that the maximization of the sum of mutual information between _ _ random variables _ within _ the subspaces is sufficient for solving the isa task .",
    "here we show that the direct search for inverse fir filter can be circumvented ( note  [ note : fir - inverse - exists ] ) .",
    "namely , temporal concatenation reduces the ubssd task to an ( u)isa problem ( section  [ sec : ubssd2(u)isa ] ) .",
    "our earlier results will allow further simplifications .",
    "we will reduce the isa task to an ica task plus a search for optimal permutation of the ica coordinates .",
    "this decomposition principle will be elaborated in section  [ sec : isa2ica ] by means of the separation theorem .",
    "we reduce the ubssd task to an isa problem .",
    "the bsd literature provides the basis for our reduction ; @xcite use temporal concatenation in their work .",
    "this method can be extended to multidimensional @xmath9 components in a natural fashion :    let @xmath71 be such that @xmath72 is fulfilled .",
    "such @xmath71 exists due to the undercomplete assumption @xmath24 : @xmath73 this choice of @xmath71 guarantees that the reduction gives rise to an ( under)complete isa task : let @xmath74 denote the @xmath75 coordinate of observation @xmath20 and let the matrix be decomposed into @xmath76 sized blocks .",
    "that is , @xmath77}_{i=1 .. d_x , j=1 .. m}$ ] @xmath78 , where @xmath79 and @xmath80 denote row and column indices , respectively . using notations @xmath81\\in{\\mathbb{r}}^{d(l+l')},\\\\ { \\mathbf}{x}^m(t)&:=[x_m(t ) ; x_m(t-1);\\ldots ; x_m(t - l'+1)]\\in{\\mathbb{r}}^{l'},\\\\ { \\mathbf}{s}(t)&:=[{\\mathbf}{s}^1(t ) ; \\ldots ; { \\mathbf}{s}^m(t)]\\in{\\mathbb{r}}^{md(l+l')=d_s(l+l')},\\\\ { \\mathbf}{x}(t)&:=[{\\mathbf}{x}^1(t ) ; \\ldots ; { \\mathbf}{x}^{d_x}(t)]\\in{\\mathbb{r}}^{d_xl'},\\\\ { \\mathbf}{a}^{ij}&:= \\left[\\begin{array}{cccccc}{\\mathbf}{h}^{ij}_0&\\ldots&{\\mathbf}{h}^{ij}_l & { \\mathbf}{0 } & \\ldots&{\\mathbf}{0}\\\\&\\ddots&&\\ddots&&\\\\&&\\ddots&&\\ddots\\\\{\\mathbf}{0}&\\ldots&{\\mathbf}{0}&{\\mathbf}{h}^{ij}_0&\\ldots&{\\mathbf}{h}^{ij}_l\\end{array}\\right]\\in{\\mathbb{r}}^{l'\\times d(l+l')},\\\\ { \\mathbf}{a}&:=[{\\mathbf}{a}^{ij}]_{i=1 .. d_x , j=1 .. m}\\in{\\mathbb{r}}^{d_xl'\\times md(l+l')=d_xl'\\times d_s(l+l')},\\end{aligned}\\ ] ] model @xmath82 can be obtained . here ,",
    "@xmath7s are i.i.d .  in time @xmath8",
    ", they are independent for different @xmath6 values , and equation   holds for @xmath71 .",
    "thus , is either an undercomplete or a complete isa task , depending on the relation of the l.h.s and the r.h.s of : the task is complete if the two sides are equal .",
    "the number of the components and the dimension of the components in task are and @xmath83 , respectively .",
    "if we end up with an undercomplete isa problem in then it can be reduced to a complete one , as was shown in section  [ sec : isa - ambiguities ] .",
    "thus , choosing the minimal value for @xmath71 in , the dimension of the obtained isa task is @xmath84 taking into account the ambiguities of the isa task ( section  [ sec : isa - ambiguities ] ) , the original @xmath9 components will occur @xmath85 times and up to orthogonal transformations . as a result , in the ideal case ,",
    "our estimations are as follows @xmath86 where @xmath87 , @xmath88 .",
    "the separation theorem @xcite conjectured by @xcite allows one to decompose the solution of the isa problem , under certain conditions , into 2 steps : in the first step , ica estimation is executed by minimizing @xmath89 . in the second step ,",
    "the ica elements are grouped by finding an optimal permutation .",
    "this principle will be formalized in section  [ sec : isa - sep - t ] .",
    "section  [ sec : suff - conds - of - sep - t ] provides sufficient conditions for the theorem .",
    "we state the isa separation theorem for components having possibly different @xmath90 dimensions :    let @xmath91={\\mathbf}{w}{\\mathbf}{x}\\in{\\mathbb{r}}^d$ ] , where , @xmath63 is the whitened observation of the isa model , and .",
    "let @xmath92 denote the surface of the unit sphere , that is",
    ".    presume that the @xmath93 sources @xmath94 of the isa model satisfy condition @xmath95 and that the ica cost function @xmath96 has minimum over the orthogonal matrices in @xmath97 .",
    "then it is sufficient to search for the solution to the isa task as a permutation of the solution of the ica task . using the concept of demixing matrices , it is sufficient to explore forms @xmath98 where @xmath99 is a permutation matrix to be determined and @xmath100 is the isa demixing matrix .",
    "the proof of the theorem is presented in appendix  [ sec : risa - sep - theorem ] .",
    "it is intriguing that if is satisfied then the simple decomposition principle provides the _ global _ minimum of . in the literature on joint block diagonalization ( jbd )",
    "@xcite have put forth a similar _ conjecture _ recently . according to this conjecture , for quadratic cost function ,",
    "if jacobi optimization is applied , the block - diagonalization of the matrices can be found by the optimization of permutations following the joint diagonalization of the matrices .",
    "isa solutions formulated within the jbd framework @xcite make efficient use of this idea in practice .",
    "@xcite could justify this approach for _ local _ minimum points .",
    "the question of which types of sources satisfy the separation theorem is open .",
    "equation   provides only a sufficient condition .",
    "below , we list sources @xmath9 that satisfy . details and the extension of the separation theorem for complex variables can be found in a technical report of @xcite .    1 .",
    "assume that variables @xmath101 satisfy the so - called w - epi condition ( epi is shorthand for the _ entropy power inequality _",
    "@xcite ) , that is , @xmath102 then inequality holds for these variables too .",
    "the proof can be found in lemma  [ lem : suff ] of appendix  [ sec : risa - sep - theorem ] .",
    "the w - epi condition is valid 1 .   for spherically symmetric or shortly spherical variables @xcite .",
    "the distribution of such variables is invariant for orthogonal transformations .",
    "sketch of the proof ( @xmath101 ) : the w - epi condition concerns projections to unit vectors . for spherical variables ,",
    "the distribution and thus the entropy of these projections are independent of @xmath103 . because @xmath104 and @xmath105 , the is satisfied with equality @xmath106 .",
    "@xmath107 2 .   for variables invariant to @xmath108 rotation . under this condition ,",
    "density function @xmath109 of component @xmath9 is subject to the following invariance @xmath110 + sketch of the proof ( @xmath101 ) : assume that function @xmath111 has global minimum on set @xmath112 .",
    "concerns each coordinate . ]",
    "let this minimum be at @xmath113 .",
    "then , the @xmath108 invariance warrants that function @xmath114 take its global minimum also on @xmath115 , which is perpendicular to @xmath116 .",
    "let @xmath117\\in\\mathcal{o}^2 $ ] .",
    "now , we can estimate variables @xmath118 .",
    "this is sufficient because the isa solution is ambiguous up to orthogonal transformations within each subspace .",
    "@xmath107 + a special case of this requirement is invariance to permutation and sign changes @xmath119 in other words , there exists a function @xmath120 , which is symmetric in its variables and @xmath121 special cases within this family are distributions @xmath122 which are constant over the spheres of @xmath123-space .",
    "they are called  @xmath123 spherical variables which , for @xmath124 , corresponds to spherical variables .",
    "certain weakly dependent variables : @xcite has determined sufficient conditions when epi holds . may be generalized to higher dimensions .",
    "we are not aware of such generalizations . ]",
    "if the epi property is satisfied on unit sphere @xmath125 , then the isa separation theorem holds ( lemma  [ lem : suff ] ) .",
    "these results are summarized schematically in table  [ tab : suffcond - summary ] .",
    "@xmath126\\ar[dr]|-{\\text{specially}}&\\\\    & & \\text{invariance to sign and permutation}\\ar[d]|-{\\text{specially}}\\\\    & & \\text{$l^p$ spherical ( $ p>0$)}\\\\    \\txt{takano 's dependency\\\\($d=2 $ ) } \\ar@{=>}[r ] & \\text{w - epi}\\ar@{=>}[d ] & \\txt{spherical symmetry}\\ar@{=>}[l]\\ar[u]|-{\\text{generalization for $ d=2$}}\\\\    & \\txt{equation~\\eqref{eq : suff } : sufficient\\\\ for the isa separation theorem } & } \\ ] ]",
    "we showed how to convert the ubssd task to an isa task in section  [ sec : ubssd2(u)isa ] . in the following",
    "we will present methods that can solve the isa task . in section  [ sec : studied - isa - costs ] we treat estimations of the mutual information of the isa cost functions in section  [ sec : isa - cost ] .",
    "methods that can optimize these cost functions are elaborated in section  [ sec : opt - of - isa - costs ] .",
    "we also present here the pseudocode of the procedures studied . in section  [ sec :",
    "diff - or - unknown - dims ] we review methods that can treat non - equal or unknown component dimensions . in what follows , and in accordance with ,",
    "let @xmath63 denote the whitened observation , while ( @xmath64 ) and @xmath65 stand for the estimated source and its components in the isa task , respectively .",
    "here we introduce two dependency estimators .",
    "first , in section  [ sec : jfd ] we describe a decorrelation method that uses a set of functions jointly .",
    "this method is called joint ( jfd ) method @xcite .",
    "our second technique ( section  [ sec : kernelisa - methods ] ) generalizes earlier kernel - ica methods for the isa task .",
    "the motivation for this latter method is the efficiency and precision of kernel - ica methods in finding independent components @xcite .",
    "our experiences are similar with kernel - isa methods , see section  [ sec : simulations ] .",
    "we found that kernel - isa methods need more computations , but can provide more precise solutions than the jfd technique .",
    "the jfd method estimates the hidden @xmath9 components through the decorrelation over a function set @xmath127 @xcite .",
    "formally , let the empirical @xmath128-covariance matrix of @xmath129 and @xmath130 for function @xmath131\\in{\\ensuremath{\\mathscr{f}}}$ ] over @xmath132 be denoted by @xmath133=\\\\       & =       \\frac{1}{t}\\sum_{t=1}^t\\left\\{{\\mathbf}{f}[{\\mathbf}{y}(t)]-\\frac{1}{t}\\sum_{k=1}^t{\\mathbf}{f}[{\\mathbf}{y}(k)]\\right\\}\\left\\{{\\mathbf}{f}[{\\mathbf}{y}(t)]-\\frac{1}{t}\\sum_{k=1}^t{\\mathbf}{f}[{\\mathbf}{y}(k)]\\right\\}^*,\\nonumber\\\\       { \\sigma}^{i , j}({\\mathbf}{f},t,{\\mathbf}{w})&=\\widehat{cov}\\left[{\\mathbf}{f}^i\\left({\\mathbf}{y}^i\\right),{\\mathbf}{f}^j\\left({\\mathbf}{y}^j\\right)\\right]=\\\\       & = \\frac{1}{t}\\sum_{t=1}^t\\left\\{{\\mathbf}{f}^i[{\\mathbf}{y}^i(t)]-\\frac{1}{t}\\sum_{k=1}^t{\\mathbf}{f}^i[{\\mathbf}{y}^i(k)]\\right\\}\\left\\{{\\mathbf}{f}^j[{\\mathbf}{y}^j(t)]-\\frac{1}{t}\\sum_{k=1}^t{\\mathbf}{f}^j[{\\mathbf}{y}^j(k)]\\right\\}^*.\\nonumber\\end{aligned}\\ ] ] then , the joint decorrelation on @xmath134 can be formulated as the minimization of cost function @xmath135 here : ( i ) @xmath64 , ( ii ) @xmath134 denotes a set of @xmath136 functions , and each function acts on each coordinate separately , ( iii ) @xmath137 denotes the point - wise multiplication , called the hadamard - product , ( iv ) @xmath138 masks according to the subspaces , @xmath139 , where all elements of matrix and are equal to 1 , @xmath140 is the kronecker - product , ( v ) @xmath141 denotes the square of the frobenius norm , that is , the sum of the squares of the elements .",
    "cost function can be interpreted as follows : for _ any _ function @xmath142 that acts on independent variables @xmath143 @xmath94 the variables @xmath144 remain independent .",
    "thus , covariance matrix @xmath145 of variable @xmath146 $ ] is block - diagonal .",
    "independence of estimated sources @xmath143 is gauged by the uncorrelatedness on the function set @xmath134 .",
    "thus , the non - block - diagonal portions ( @xmath147 , @xmath148 ) of covariance matrices @xmath145 are punished .",
    "this principle is expressed by the term @xmath149 .",
    "two alternatives for the isa cost function of are presented .",
    "they estimate the mutual information based isa cost defined in via kernels : the kcca and kgv kernel - ica methods of @xcite are extended to the isa task .",
    "the original methods estimate pair - wise independence between _",
    "random variables.pairwise independence is _ not _ equivalent to mutual independence @xcite . nonetheless , according to our numerical experiences it is an efficient approximation in many situations . ]",
    "the extension to the multidimensional case is straightforward , the arguments of the kernels can be modified to multidimensional variables and the derivation of @xcite can be followed .",
    "the main steps are provided below for the sake of completeness .",
    "the resulting expressions can be used for the estimation of dependence between multidimensional random variables .",
    "the performance of these simple extensions on the related isa applications is shown in section  [ sec : kernelisa - vs - jfd ] .",
    "[ [ sec : kcca ] ] the kcca method + + + + + + + + + + + + + + +    first , the 2-variable - case is treated and then it will be generalized to many variables .    [",
    "[ variable - case ] ] 2-variable - case    assume that the mutual dependence of two random variables @xmath150 and @xmath151 has to be measured .",
    "let positive semi - definite kernels , and @xmath152 be chosen in the respective spaces .",
    "let @xmath153 and @xmath154 denote the reproducing kernel hilbert spaces ( rkhs ) @xcite associated with the kernels . here ,",
    "@xmath153 and @xmath154 are function spaces having elements that perform mappings @xmath155 and @xmath156 , respectively .",
    "then the mutual dependence between @xmath157 and @xmath158 can be measured , for instance , by the following expression :    @xmath159,\\end{aligned}\\ ] ]    where @xmath160 denotes correlation .",
    "the value of @xmath161 can be estimated empirically : assume that we have @xmath162 samples both from @xmath157 and from @xmath158 .",
    "these samples are @xmath163 and @xmath164 .",
    "then , using notations , @xmath165 , the empirical estimation of @xmath161 could be the following : @xmath166[h({\\mathbf}{v}_t)-\\bar{h } ] } { \\sqrt{\\frac{1}{t}\\sum_{t=1}^t[g({\\mathbf}{u}_t)-\\bar{g}]^2 } \\sqrt{\\frac{1}{t}\\sum_{t=1}^t [ h({\\mathbf}{v}_t)-\\bar{h}]^2 } } .\\ ] ] however , it is worth including some regularization for @xmath167 @xcite , therefore @xmath167 is modified to @xmath168}{\\sqrt{\\textrm{var}\\left[g({\\mathbf}{u})\\right]+\\kappa \\left\\|g\\right\\|^2_{{\\ensuremath{\\mathscr{f}}}^{{\\mathbf}{u } } } } \\sqrt{\\textrm{var}\\left[h({\\mathbf}{v})\\right]+\\kappa \\left\\|h\\right\\|^2_{{\\ensuremath{\\mathscr{f}}}^{{\\mathbf}{v}}}}},\\label{eq : j - kcca - reg}\\ ] ] where expression ` @xmath169 ' stands for variance , @xmath170 is the regularization parameter , @xmath171 and @xmath172 denote the rkhs norm of their arguments in @xmath153 and @xmath154 , respectively .",
    "now , expanding the denominator up to second order in @xmath173 , setting the expectation value of the samples to zero in the respective rkhss , and using the notation @xmath174 @xcite , the empirical estimation of is    @xmath175    where @xmath176 , @xmath177 are the so - called centered kernel matrices : these matrices are derived from kernel matrices @xmath178_{i , j=1,\\ldots , t},{\\mathbf}{k}^{{\\mathbf}{v}}=[k({\\mathbf}{v}_i,{\\mathbf}{v}_j)]_{i , j=1,\\ldots , t}\\in{\\mathbb{r}}^{t\\times t}$ ] , as is described below .",
    "let @xmath179 denote a vector whose all elements are equal to @xmath180 and let denote the so - called @xmath162-dimensional centering matrix .",
    "then @xmath181 , @xmath182 .",
    "computing the stationary points of @xmath183 in , that is , setting @xmath184 , the resulting task is to solve a _ generalized eigenvalue problem _ of the form @xmath185 : @xmath186 where the objective is to maximize @xmath187 .",
    "our task is to estimate @xmath183 , the maximum of @xmath188 .",
    "[ [ generalization - for - many - variables ] ] generalization for many variables    the kcca method can be generalized for more than two random variables and can be used to measure pair - wise dependence : let us introduce the following notations : let @xmath189 be random variables .",
    "we want to measure the dependence between these variables .",
    "let positive semi - definite kernels @xmath190 @xmath94 be chosen in the respective spaces .",
    "let @xmath191 denote the rkhs associated with kernel @xmath192 .",
    "having @xmath162 samples @xmath193 for all random variables @xmath143 @xmath94 , matrices @xmath194_{i , j=1,\\ldots , t}\\in { \\mathbb{r}}^{t \\times t}$ ] and @xmath195 can be created .",
    "let the regularization parameter be chosen as @xmath170 and let @xmath196 denote the auxiliary variable @xmath197 .",
    "it can be proven that the computation of @xmath183 involves the solution of the following generalized eigenvalue problem :    @xmath198    analogously to the two - variable - case , the largest eigenvalue of this task is a measure of the value of the pair - wise dependence of the random variables .    [ [ sec : kgv ] ] the kgv method + + + + + + + + + + + + + +    equation   in section  [ sec : isa - cost ] indicates that the isa task can be seen as the minimization of the mutual information .",
    "the basic idea of the kgv technique is that  even for non - gaussian variables  it estimates the mutual information by the gaussian approximation @xcite .",
    "namely , let @xmath199 $ ] be multidimensional normal random variable with covariance matrix @xmath46 .",
    "let @xmath200 denote the cross - covariance between components of @xmath201 .",
    "the mutual information between components @xmath202 is @xcite : @xmath203 the quotient @xmath204 is called the _",
    "generalized variance_. if @xmath205 is _ not normal_this is the typical situation in the isa task  then let us transform the individual components @xmath143 using feature mapping @xmath206 associated with the reproducing kernel and assume that the image is a normal variable .",
    "thus , the cost function @xmath207\\ ] ] is associated with the isa task . in equation",
    "@xmath208 $ ] , @xmath209 $ ] , and the sub - matrices are @xmath210 $ ] .",
    "expression @xmath211 is called the _ kernel generalized variance _ ( kgv ) .",
    "the next theorem shows that the kgv technique can be interpreted as a decorrelation based method :    let @xmath212 be a positive semi - definite matrix , let @xmath213 denote the @xmath75 block in the diagonal of matrix @xmath214 , and let @xmath215 .",
    "then the function @xmath216\\ ] ] is 0 iff @xmath217 .",
    "this theorem can be proven for @xmath218 , as in the case of @xmath219 @xcite , see the work of @xcite .",
    "the theorem implies the following :    [ conseq : kgv - jfd : equiv ] setting @xmath220 , the kgv technique is a decorrelation technique according to feature mapping @xmath206 .",
    "the kgv technique aims at minimizing of cross - covariances to @xmath221 .",
    "we note that the kernel covariance ( kc ) ica method @xcite  similarly to the kcca method  can be extended to measure the mutual dependence of multidimensional random variables and thus to solve the isa task .",
    "again , the computation of the cost function can be converted to the solution of a generalized eigenvalue problem .",
    "this eigenvalue problem is provided in appendix  [ sec : kc - deduction ] for the sake of completeness .",
    "the kcca , kgv and kc methods can estimate only _ pair - wise _ dependence .",
    "nonetheless , the joint mutual information can be estimated by recursive methods computing pair - wise mutual information : for the mutual information of random variables @xmath201 @xmath94 it can be shown that the recursive relation @xmath222\\right)\\label{eq : isa - cost - i - rec}\\ ] ] holds @xcite .",
    "thus , for example , the kcca eigenvalue problem of can be replaced by pair - wise estimation of mutual information .",
    "we note that the tree - dependent component analysis model @xcite estimates the joint mutual information from the pair - wise mutual information .",
    "there are several possibilities to optimize isa cost functions :    1 .   without ica preprocessing ,",
    "optimization problems concern either the _ stiefel manifold _",
    "@xcite or the _ flag manifold _ @xcite . according to our experiences , these gradient based optimization methods may be stuck in poor local minima .",
    "2 .   according to the isa separation theorem , it may be sufficient to search for optimal permutation of the ica components provided by ica preprocessing .",
    "we applied greedy permutation search : two coordinates of different subspaces are exchanged provided that this change decreases cost function @xmath223 . here , @xmath223 denotes , for example , @xmath224 , @xmath225 , or @xmath226 depending on the isa technique applied .",
    "the variable of @xmath223 is the permutation matrix @xmath227 using the parametrization @xmath228 .",
    "pseudocode is provided in table  [ tab : isa - pseudocode ] .",
    "our experiences show that greedy permutation search is often sufficient for the estimation of the isa subspaces .",
    "however , it is easy to generate examples in which this is not true @xcite . in such cases , global permutation search method of higher computational burden may become necessary @xcite .",
    ".pseudocode of the isa algorithm .",
    "cost @xmath223 stands for the isa cost function of jfd , kcca , or kgv methods .",
    "the permutation matrix of the isa separation theorem is the variable of @xmath223 .",
    "[ cols= \" < \" , ]",
    "we have introduced a new model , the blind subspace deconvolution ( bssd ) for data analysis . this model deals with the casual convolutive mixture of multidimensional independent sources .",
    "the undercomplete version ( ubssd ) of the task has been presented , and it has been shown how to derive an independent subspace analysis ( isa ) task from the ubssd problem .",
    "recent developments of the isa techniques enabled us to handle the emerging high dimensional problems . our earlier results ,",
    "namely the isa separation theorem @xcite motivated us to reduce the isa task to the search for the optimal permutation of the ica components .",
    "the components were grouped with a novel joint decorrelation technique , the joint ( jfd ) method @xcite .    also , we adapted other ica techniques , such as the kcca and kgv methods to the isa task and studied their efficiency .",
    "simulations indicated that although the kcca and kgv methods give rise to serious computational burden relative to the jfd method , they can be advantageous for smaller isa tasks and for isa tasks when the number of samples is small .",
    "finally , we note that we achieved small errors in these high dimensional computations .",
    "these small errors indicate that the separation theorem is robust and might be extended to a larger class of noise sources .",
    "we shall rely on entropy inequalities ( section  [ sec : repiineq - s ] ) . in section  [ sec : connection2rica ] connection to the ica cost function is derived ( lemma  [ r - prop ] ) .",
    "the isa separation theorem then follows .",
    "first , consider the so - called entropy power inequality ( epi ) @xmath229 where @xmath230 denote continuous random variables .",
    "this inequality holds , for example , for independent continuous variables @xcite .",
    "if epi is satisfied on the surface of the unit sphere @xmath231 , then a further inequality holds :    [ lem : suff ] suppose that continuous random variables @xmath232 satisfy the following inequality @xmath233 this inequality will be called the _ w - epi _ condition .",
    "then equation   holds , too .",
    "assume that @xmath234 .",
    "applying @xmath235 on condition , and using the monotonicity of the @xmath235 function , we can see that the first inequality is valid in the following inequality chain @xmath236 here ,    1 .",
    "we used the relation @xmath237 for the entropy of the transformed variable @xcite . hence @xmath238 2 .   in the second inequality ,",
    "the concavity of @xmath235 was exploited .",
    "w - epi holds , for example , for independent variables @xmath239 , because independence is not affected by multiplication with a constant .",
    "the isa separation theorem will be a corollary of the following claim :    [ r - prop ] let @xmath240={\\mathbf}{y}({\\mathbf}{w})={\\mathbf}{w}{\\mathbf}{s}\\in { \\mathbb{r}}^d$ ] , where @xmath241 ( @xmath215 ) , @xmath201 is the estimation of the @xmath75 component of the isa task .",
    "let @xmath67 be the @xmath242 coordinate of the @xmath75 component ( @xmath243 ) .",
    "similarly , let @xmath244 stand for the @xmath242 coordinate of the @xmath75 source .",
    "let us assume that the @xmath93 sources satisfy the condition  .",
    "then @xmath245    let us denote the @xmath246 element of matrix @xmath247 by @xmath248 .",
    "coordinates of @xmath205 and @xmath249 will be denoted by @xmath250 and @xmath251 , respectively .",
    "further , let @xmath252 denote the indices of the @xmath75 subspace ( @xmath253 ) , that is , @xmath254 @xmath255 .",
    "now , writing the elements of the @xmath242 row of matrix multiplication @xmath256 , we have @xmath257 and thus , @xmath258 \\right)\\label{eq : w^2-in}\\\\ & \\ge\\sum_{m=1}^m\\left[\\left(\\sum_{l\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , l}^2\\right)h\\left(\\frac{\\sum_{j\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , j}s_j}{\\left(\\sum_{l\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , l}^2\\right)^{\\frac{1}{2}}}\\right)\\right ] \\label{eq : lem2-applied}\\\\ & = \\sum_{m=1}^m\\left[\\left(\\sum_{l\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , l}^2\\right ) h\\left(\\sum_{j\\in{\\ensuremath{\\mathscr{g}}}^m}\\frac{w_{i , j}}{\\left(\\sum_{l\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , l}^2\\right)^{\\frac{1}{2}}}s_j\\right)\\right ] \\label{eq : lem2-applied - again - pre}\\\\ & \\ge\\sum_{m=1}^m\\left[\\left(\\sum_{l\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , l}^2\\right ) \\sum_{j\\in{\\ensuremath{\\mathscr{g}}}^m}\\left(\\frac{w_{i , j}}{\\left(\\sum_{l\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , l}^2\\right)^{\\frac{1}{2}}}\\right)^2h\\left(s_j\\right)\\right]\\label{eq : lem2-applied - again}\\\\ & = \\sum_{j\\in{\\ensuremath{\\mathscr{g}}}^1}w_{i , j}^2h\\left(s_j\\right)+\\ldots+\\sum_{j\\in{\\ensuremath{\\mathscr{g}}}^m}w_{i , j}^2h\\left(s_j\\right).\\label{eq : h(yi - last)}\\end{aligned}\\ ] ] the above steps can be justified as follows :    1 .",
    ": equation   was inserted into the argument of @xmath259 .",
    "new terms were added for lemma  [ lem : suff ] .",
    "sources @xmath9 are independent of one another and this independence is preserved upon mixing _ within _ the subspaces , and we could also use lemma  [ lem : suff ] , because @xmath247 is an orthogonal matrix .",
    "nominators were transferred into the @xmath260 terms .",
    "5 .   : variables @xmath9 satisfy condition   according to our assumptions .",
    ": we simplified the expression after squaring .    using this inequality , summing it for @xmath79 , exchanging the order of the sums , and making use of the orthogonality of matrix @xmath247",
    ", we have @xmath261    ica minimizes the l.h.s . of equation  ,",
    "that is , it minimizes .",
    "the set of minima is invariant to permutations and to changes of the signs .",
    "also , according to proposition  [ r - prop ] , @xmath262 , that is , the coordinates of the @xmath9 components of the isa task belong to the set of the minima . @xmath107",
    "for the sake of completeness , the extension of the kc method @xcite for the isa task is detailed below .",
    "the extension is similar to the extensions presented in section  [ sec : kcca ] ( see the kcca method ) , and we use the notations of that section .    first , we would like to measure the dependence of two 2 random variables @xmath150 and @xmath151 .",
    "the kc technique defines their dependence as their maximal covariance on the unit spheres @xmath263 , @xmath264 of function spaces @xmath153 , @xmath154 :      @xmath267[h({\\mathbf}{v}_t)-\\bar{h}]\\right|.\\ ] ] the estimation can be reduced to the following conditional maximization problem : @xmath268 after the adaptation of the lagrange multiplier technique and the computation of the stationary points of it can be realized that the values of @xmath269;@xmath270 $ ] and @xmath271 can be computed as the solutions of the generalized eigenvalue problem @xmath272                    cardoso , j. : multidimensional independent component analysis . in : proceedings of international conference on acoustics , speech , and signal processing ( icassp 98 ) .",
    "volume  4 . , seattle , wa , usa ( 1998 ) 19411944    theis , f.j . :",
    "blind signal separation into groups of dependent signals using joint block diagonalization . in : proceedings of international society for computer aided surgery ( iscas 2005 ) , kobe , japan ( 2005 ) 58785881      hyvrinen , a. , kster , u. : fastisa : a fast fixed - point algorithm for independent subspace analysis . in : proceedings of european symposium on artificial neural networks ( esann 2006 ) , bruges , belgium ( 2006 )        stgbauer , h. , kraskov , a. , astakhov , s.a . , grassberger , p. : least dependent component analysis based on mutual information .",
    "physical review e - statistical , nonlinear , and soft matter physics * 70 * ( 2004 )    pczos , b. , lrincz , a. : independent subspace analysis using k - nearest neighborhood distances .",
    "artificial neural networks : formal models and their applications - icann 2005 , pt 2 , proceedings * 3697 * ( 2005 ) 163168          szab , z. , lrincz , a. : real and complex independent subspace analysis by generalized variance . in : proceedings of ica research network international workshop ( icarn 2006 ) , liverpool , u.k . ( 2006 ) 8588 http://arxiv.org/abs/math.st/0610438 .",
    "hedgepeth , j.b . ,",
    "gallucci , v.f . , osullivan , f. , thorne , r.e .",
    ": an expectation maximization and smoothing approach for indirect acoustic estimation of fish size and density .",
    "journal of marine science * 56 * ( 1999 ) 3650      douglas , s.c . , sawada , h. , makino , s. : natural gradient multichannel blind deconvolution and speech separation using causal fir filters .",
    "ieee transactions on speech and audio processing * 13 * ( 2005 ) 92104      roan , m.j . , gramann , m.r . ,",
    "erling , j.g . ,",
    "sibul , l.h . :",
    "blind deconvolution applied to acoustical systems identification with supporting experimental results .",
    "the journal of the acoustical society of america * 114 * ( 2003 ) 19881996    araki , s. , makino , s. , mukai , r. , nishikawa , t. , saruwatari , h. : fundamental limitation of frequency domain blind source separation for convolved mixture of speech",
    ". transactions on speech and audio processing * 11 * ( 2003 ) 109116        jung , t. , makeig , s. , lee , t. , mckeown , m.j . ,",
    "brown , g. , bell , a.j . ,",
    "sejnowski , t.j . : independent component analysis of biomedical signals . in : proceedings of international workshop on independent component analysis and signal separation ( ica 2000 ) , helsinki ( 2000 ) 633644            szab , z. , pczos , b. , lrincz , a. : separation theorem for @xmath276-independent subspace analysis with sufficient conditions .",
    "technical report , etvs lornd university , budapest ( 2006 ) http://arxiv.org/abs/math.st/0608100 .",
    "szab , z. , pczos , b. , lrincz , a. : cross - entropy optimization for independent process analysis . in : proceedings of independent component analysis and blind signal separation ( ica 2006 ) .",
    "volume 3889 of lncs . , springer ( 2006 ) 909916            fvotte , c. , doncarli , c. : a unified presentation of blind source separation for convolutive mixtures using block - diagonalization . in : proceedings of independent component analysis and blind signal separation ( ica 2003 ) , nara , japan ( 2003 ) 349354                        plumbley , m.d . : lie group methods for optimization with orthogonality constraints . in : proceedings of independent component analysis and blind signal separation ( ica 2004 ) .",
    "volume 3195 of lncs . , springer ( 2004 ) 12451252    quinquis , n. , yamada , i. , sakaniwa , k. : efficient dual cayley parametrization technique for ica with orthogonality constraints . in : proceedings of ica research network international workshop ( icarn 2006 ) , liverpool , u.k .",
    "( 2006 ) 123126    nishimori , y. , akaho , s. , plumbley , m.d . :",
    "riemannian optimization method on the flag manifold for independent subspace analysis . in : proceedings of independent component analysis and blind signal separation ( ica 2006 ) .",
    "volume 3889 of lncs . , springer ( 2006 ) 295302"
  ],
  "abstract_text": [
    "<S> we introduce the blind subspace deconvolution ( bssd ) problem , which is the extension of both the blind source deconvolution ( bsd ) and the independent subspace analysis ( isa ) tasks . </S>",
    "<S> we examine the case of the undercomplete bssd ( ubssd ) . applying temporal concatenation </S>",
    "<S> we reduce this problem to isa . </S>",
    "<S> the associated ` high dimensional ' isa problem can be handled by a recent technique called joint ( jfd ) . </S>",
    "<S> similar decorrelation methods have been used previously for kernel independent component analysis ( kernel - ica ) . </S>",
    "<S> more precisely , the kernel canonical correlation ( kcca ) technique is a member of this family , and , as is shown in this paper , the kernel generalized variance ( kgv ) method can also be seen as a decorrelation method in the feature space . </S>",
    "<S> these kernel based algorithms will be adapted to the isa task . in the numerical examples , we ( i ) </S>",
    "<S> examine how efficiently the emerging higher dimensional isa tasks can be tackled , and ( ii ) explore the working and advantages of the derived kernel - isa methods . </S>"
  ]
}