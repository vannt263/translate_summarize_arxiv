{
  "article_text": [
    "[ [ keywords ] ] keywords    bandwidth , inverse problems , kernel estimators , local linear methods , local polynomial methods , minimum contrast methods , nonparametric curve estimation , nonparametric density estimation , nonparametric regression , penalised contrast methods , rate of convergence , sinc kernel , statistical smoothing            our aim in this paper is to give a brief survey of kernel methods for solving problems involving measurement error , for example problems involving density deconvolution or regression with errors in variables , and to relate these ` classical ' methods ( they are now about twenty years old ) to new approaches based on minimum contrast methods .",
    "section  1.1 motivates the treatment of problems involving errors in variables , and section  1.2 describes conventional kernel methods for problems where the extent of measurement error is so small as to be ignorable .",
    "section  2.1 shows how those standard techniques can be modified to take account of measurement errors , and section  2.2 outlines theoretical properties of the resulting estimators .    in section  3",
    "we show how kernel methods for dealing with measurement error are related to new techniques based on minimum contrast ideas . for this purpose , in section  3.1 we specialise the work in section  2 to the case of the sinc kernel .",
    "that kernel choice is not widely used for density deconvolution , although it has previously been studied in that context by stefanski and carroll ( 1990 ) , diggle and hall ( 1993 ) , barry and diggle ( 1995 ) , butucea ( 2004 ) , meister ( 2004 ) and butucea and tsybakov ( 2007a , b ) . section  3.2 outlines some of the properties that are known of sinc kernel estimators , and section  3 points to the very close connection between that approach and minimum contrast , or penalised contrast , methods .",
    "measurement errors arise commonly in practice , although only in a minority of statistical analyses is a special effort made to accommodate them . often they are minor , and ignoring them makes little difference , but in some problems they are important and significant , and we neglect them at our peril .    areas of application of deconvolution , and regression with measurement error , include the analysis of seismological data ( e.g.  kragh and laws , 2006 ) , financial analysis ( e.g.  bonhomme and robin , 2008 ) , disease epidemiology ( e.g.  brookmeyer and gail , 1994 , chapter  8) , and nutrition .",
    "the latter topic is of particular interest today , for example in connection with errors - in - variables problems for data gathered in food frequency questionnaires ( ffqs ) , or dietary questionnaires for epidemiological studies ( dqess ) .",
    "formally , an ffq is ` a method of dietary assessment in which subjects are asked to recall how frequently certain foods were consumed during a specified period of time , ' according to the nutrition glossary of the european food information council .",
    "an ffq seeks detailed information about the nature and quantity of food eaten by the person filling in the form , and often includes a query such as , `` how many of the above servings are from fast food outlets ( mcdonalds , taco bell , etc . ) ? ''",
    "( stanford university , 1994 ) .",
    "this may seem a simple question to answer , but nutritionists interested in our consumption of fat generally find that the quantity of fast food that people admit to eating is biased downwards from its true value .",
    "the significant concerns in western society about fat intake , and about where we purchase our oleaginous food , apparently influences our truthfulness when we are asked probing questions about our eating habits .",
    "examples of the use of statistical deconvolution in this area include the work of stefanski and carroll ( 1990 ) and delaigle and gijbels ( 2004b ) , who address nonparametric density deconvolution from measurement - error data , obtained from ffqs during the second national health and nutrition examination survey ( 19761980 ) ; carroll ( 1997 ) , who discuss design and analysis aspects of linear measurement - error models when data come from ffqs ; carroll ( 2006 ) , who use measurement - error models , and deconvolution methods , to develop marginal mixed measurement - error models for each nutrient in a nutrition study , again when ffqs are used to supply the data ; and staudenmayer ( 2008 ) , who employ a dataset from nutritional epidemiology to illustrate the use of techniques for nonparametric density deconvolution .",
    "see carroll ( 2006 , p.  7 ) for further discussion of applications to data on nutrition .",
    "how might we correct for errors in variables ?",
    "one approach is to use methods based on deconvolution , as follows .",
    "let us write @xmath0 for the quantity of fast food that a person admits to eating , in a food frequency questionnaire ; let @xmath1 denote the actual amount of fast food ; and put @xmath2 .",
    "we expect that the distribution of @xmath3 will be skewed towards values greater than  1 , and we might even have an idea of the shape of the distribution responsible for this effect , i.e.  the distribution of @xmath4 . indeed , we typically work with the logarithm of the formula @xmath5 , and in that context , writing @xmath6 , @xmath7 and @xmath8 , the equation defining the variables of interest is : @xmath9 we have data on @xmath10 , and from that we wish to estimate the distribution of @xmath11 , i.e.  the distribution of the logarithm of fast - food consumption .",
    "it can readily be seen that this problem is generally not solvable unless the distribution of @xmath12 , and the joint distribution of @xmath11 and @xmath12 , are known . in practice",
    "we usually take @xmath11 and @xmath12 to be independent , and undertake empirical deconvolution ( i.e.  estimation of the distribution , or density , of @xmath11 from data on @xmath10 ) for several candidates for the distribution of  @xmath12 .",
    "if we are able to make repeated measurements of @xmath11 , in particular to gather data on @xmath13 for @xmath14 , say , then we have an opportunity to estimate the distribution of @xmath12 as well .",
    "it is generally reasonable to assume that @xmath11 , @xmath15 ,  , @xmath16 are independent random variables .",
    "the distribution of @xmath12 can be estimated whenever @xmath17 and the distribution is uniquely determined by @xmath18 , where @xmath19 denotes the characteristic function of  @xmath12 .",
    "the simplest example of this type is arguably that where @xmath12 has a symmetric distribution for which the characteristic function does not vanish on the real line .",
    "one example of repeated measurements in the case @xmath20 is that where a food frequency questionnaire asks at one point how many times we visited a fast food outlet , and on a distant page , how many hamburgers or servings of fried chicken we have purchased .",
    "the model at ( [ wxu ] ) is simple and interesting , but in examples from nutrition science , and in many other problems , we generally wish to estimate the response to an explanatory variable , rather than the distribution of the explanatory variable .",
    "therefore the proper context for our food frequency questionnaire example is really regression , not distribution or density estimation . in regression with errors in variables",
    "we observe data pairs @xmath21 , where @xmath22 @xmath23 , and the random variable @xmath24 , denoting an experimental error , has zero mean . in this case",
    "the standard regression problem is altered on account of errors that are incurred when measuring the value of the explanatory variable . in ( [ wyfromxu ] ) the variables @xmath12 , @xmath24 and @xmath11 are assumed to be independent .",
    "the measurement error @xmath12 , appearing in ( [ wxu ] ) and ( [ wyfromxu ] ) , can be interpreted as the result of a ` laboratory error ' in determining the ` dose ' @xmath11 which is applied to the subject .",
    "for example , a laboratory technician might use the dose @xmath11 in an experiment , but in attempting to determine the dose after the experiment they might commit an error @xmath12 , with the result that the actual dose is recorded as @xmath25 instead of  @xmath11 .",
    "another way of modelling the effect of measurement error is to reverse the roles of @xmath11 and @xmath10 , so that we observe @xmath21 generated as @xmath26 here a precise dose @xmath10 is specified , but when measuring it prior to the experiment our technician commits an error @xmath12 , with the result that the actual dose is @xmath27 . in ( [ xyfromwu ] )",
    "it assumed that @xmath12 , @xmath24 and @xmath10 are independent .",
    "the measurement error model ( [ wyfromxu ] ) is standard .",
    "the alternative model ( [ xyfromwu ] ) is believed to be much less common , although in some circumstances it is difficult to determine which of ( [ wyfromxu ] ) and ( [ xyfromwu ] ) is the more appropriate . the model at ( [ xyfromwu ] )",
    "was first suggested by berkson ( 1950 ) , for whom it is named .",
    "if the measurement error @xmath12 were very small then we could estimate the density @xmath28 of @xmath11 , and the function @xmath29 in the model ( [ wyfromxu ] ) , using standard kernel methods .",
    "for example , given data @xmath30 ,  , @xmath31 on @xmath11 we could take @xmath32 to be our estimator of @xmath33 . here",
    "@xmath34 is a kernel function and @xmath35 , a positive quantity , is a bandwidth .",
    "likewise , given data @xmath36 , ",
    ", @xmath37 on @xmath38 we could take @xmath39 to be our estimator of @xmath40 , where @xmath29 is as in the model at  ( [ wyfromxu ] ) .    the estimator at ( [ fhat ] ) is a standard kernel density estimator , and is itself a probability density if we take @xmath34 to be a density .",
    "it is consistent under particularly weak conditions , for example if @xmath28 is continuous and @xmath41 and @xmath42 as @xmath43 increases .",
    "density estimation is discussed at length by silverman ( 1986 ) and scott  ( 1992 ) .",
    "the estimator @xmath44 , which we generally also compute by taking @xmath34 to be a probability density , is often referred to as the ` local constant ' or nadaraya  watson estimator of  @xmath29 .",
    "the first of these names follows from the fact that @xmath45 is the result of fitting a constant to the data by local least squares : @xmath46 the estimator @xmath44 is also consistent under mild conditions , for example if the variance of the error , @xmath24 , in ( [ wyfromxu ] ) is finite , if @xmath28 and @xmath29 are continuous , if @xmath47 at the point @xmath48 where we wish to estimate  @xmath29 , and if @xmath49 and @xmath50 as @xmath43 increases .",
    "general kernel methods are discussed by wand and jones ( 1995 ) , and statistical smoothing is addressed by simonoff  ( 1996 ) .",
    "local constant estimators have the advantage of being relatively robust against uneven spacings in the sequence @xmath30 ,  , @xmath31 .",
    "for example , the ratio at ( [ ghat ] ) never equals a nonzero number divided by zero . however , local constant estimators are particularly susceptible to boundary bias .",
    "in particular , if the density of @xmath11 is supported and bounded away from zero on a compact interval , then @xmath44 , defined by ( [ ghat ] ) or ( [ ghatargmin ] ) , is generally inconsistent at the endpoints of that interval .",
    "issues of this type have motivated the use of local polynomial estimators , which are defined by @xmath51 where , in a generalisation of  ( [ ghatargmin ] ) , @xmath52 see , for example , fan and gijbels  ( 1996 ) . in ( [ chat ] ) , @xmath53 denotes the degree of the locally fitted polynomial .",
    "the estimator @xmath51 , defined by ( [ chat ] ) , is also consistent under the conditions given earlier for the estimator defined by ( [ ghat ] ) and  ( [ ghatargmin ] ) .",
    "estimators of all these types can be quickly extended to cases where errors in variables are present , for example as in the models at ( [ wxu ] ) and ( [ wyfromxu ] ) , simply by altering the kernel function @xmath34 so that it acts to cancel out the influence of the errors .",
    "we shall give details in section  2 .",
    "section  3 will discuss recently introduced methodology which , from some viewpoints looks quite different from , but is actually almost identical to , kernel methods .",
    "we first discuss a generalisation of the estimator at ( [ fhat ] ) to the case where there are errors in the observations of @xmath57 , as per the model at  ( [ wxu ] ) .",
    "in particular , we assume that we observe data @xmath58 ,  ,",
    "@xmath59 which are independent and identically distributed as @xmath60 , where @xmath11 and @xmath12 are independent and the distribution of @xmath12 has known characteristic function @xmath19 which does not vanish anywhere on the real line .",
    "let @xmath34 be a kernel function , write @xmath61 for the associated fourier transform , and define @xmath62 then , to construct an estimator @xmath63 of the density @xmath64 of @xmath11 , when all we observe are the contaminated data @xmath58 ,  ,",
    "@xmath59 , we simply replace @xmath34 by @xmath65 , and @xmath57 by @xmath66 , in the definition of @xmath63 at ( [ fhat ] ) , obtaining the estimator @xmath67 here the subscript ` decon ' signifies that @xmath68 involves empirical deconvolution .",
    "the adjustment to the kernel takes care of the measurement error , and results in consistency in a wide variety of settings .",
    "likewise , if data pairs @xmath69 ,  , @xmath70 are generated under the model at ( [ wyfromxu ] ) then , to construct the local constant estimator at ( [ ghat ] ) , or the local linear estimator defined by ( [ loclin ] ) and ( [ sxtx ] ) , all we do is replace each @xmath57 by @xmath66 , and @xmath34 by  @xmath65 .",
    "other local polynomial estimators can be calculated using a similar rule , replacing @xmath71 in @xmath72 and @xmath73 by @xmath74 , where @xmath75    the estimator at ( [ fhatdecon ] ) dates from work of carroll and hall ( 1988 ) and stefanski and carroll ( 1990 ) .",
    "deconvolution - kernel regression estimators in the local - constant case were developed by fan and truong ( 1993 ) , and extended to the general local polynomial setting by delaigle ( 2009 ) .",
    "the kernel @xmath65 is deliberately constructed to be the function whose fourier transform is @xmath76 .",
    "this adjustment permits cancellation of the influence of errors in variables , as discussed at the end of section  1.3 . to simplify calculations , for example computation of the integral in ( [ wyfromxu ] ) , we generally choose @xmath34 not to be a density function but to be a smooth , symmetric function for which @xmath77 vanishes outside a compact interval .",
    "the commonly - used candidates for @xmath77 are proportional to functions that are used for @xmath34 , rather than @xmath77 , in the case of regular kernel estimation discussed in section  1.3 .",
    "for example , kernels @xmath34 for which @xmath78 for @xmath79 , and @xmath80 otherwise , are common ; here @xmath81 and @xmath82 are integers . taking @xmath83 , @xmath84 and @xmath85 corresponds to the fourier inverses of the biweight , quartic and triweight kernels , respectively .",
    "taking @xmath86 gives the inverse of the uniform kernel , i.e.  the sinc kernel , which we shall meet again in section  3 .",
    "further information about kernel choice is given by delaigle and hall ( 2006 ) .",
    "these kernels , and others , have the property that @xmath87 when @xmath88 , thereby guaranteeing that @xmath89 .",
    "the latter condition ensures that the density estimator , defined at ( [ fhatdecon ] ) and constructed using this kernel , integrates to  1 .",
    "( however , the estimator defined by ( [ fhatdecon ] ) will generally take negative values at some points  @xmath48 . )",
    "the normalisation property is not so important when the kernel is used to construct regression estimators , where the effects of multiplying @xmath34 by a constant factor cancel from the ` deconvolution ' versions of formulae ( [ ghat ] ) and  ( [ loclin ] ) , and likewise vanish for all deconvolution - kernel estimators based on local polynomial methods .",
    "note that , as long as @xmath77 and @xmath19 are supported either on the whole real line or on a symmetric compact domain , the kernel @xmath65 , defined by ( [ ku ] ) , and its generalised form @xmath90 , are real - valued . indeed , using properties of the complex conjugate of fourier transforms of real - valued functions , and the change of variable @xmath91 , we have , using the notation @xmath92 for the complex conjugate of a complex - valued function @xmath93 of a real variable @xmath94 , @xmath95 in practice it is almost always the case that the distribution of @xmath12 is symmetric , and in the discussion of variance in section  2.2 , below , we shall make this assumption .",
    "we shall also suppose that @xmath34 is symmetric , again a condition which holds almost invariably in practice .",
    "the estimators discussed above were based on the assumption that the characteristic function @xmath19 of the errors in variables is known .",
    "this enabled us to compute the deconvolution kernel @xmath65 at  ( [ ku ] ) . in cases where the distribution of @xmath12 is not known , but can be estimated from replicated data ( see section  1.2 ) , we can replace @xmath19 by an estimator of it and , perhaps after a little regularisation , compute an empirical version of  @xmath65 .",
    "this can give good results , in both theory and practice .",
    "in particular , in many cases the resulting estimator of the density of @xmath11 , or the regression mean @xmath29 , can be shown to have the same first - order properties as estimators computed under the assumption that the distribution of @xmath12 is known .",
    "details are given by delaigle ( 2008 ) .",
    "the expected value of the estimator at ( [ fhatdecon ] ) equals @xmath96\\;\\frac{\\phi_k(t)}{\\phi_u(t / h)}\\;dt\\notag \\\\ & = \\frac1{2\\pi}\\int    e^{-itx}\\frac{\\phi_k(ht)}{\\phi_u(t)}\\;\\phi_x(t)\\,\\phi_u(t)\\,dt\\notag \\\\ & = \\frac1{2\\pi}\\int e^{-itx}\\phi_k(ht)\\,\\phi_x(t)\\,dt = \\frac1h\\int k(u / h)\\,f(x - u)\\,du\\notag \\\\ & = e\\{\\hf(x)\\}\\,,\\label{efhatdecon}\\end{aligned}\\ ] ] where the first equality uses the definition of @xmath65 , and the fourth equality uses plancherel s identity .",
    "therefore the deconvolution estimator @xmath97 , calculated from data contaminated by measurement errors , has exactly the same mean , and therefore the same bias , as @xmath98 , which would be computed using values of @xmath57 observed without measurement error .",
    "this confirms that using the deconvolution kernel estimator does indeed allow for cancellation of measurement errors , at least in terms of their presence in the mean .",
    "of course , variance is a different matter .",
    "since @xmath97 equals a sum of independent random variables then @xmath99 ( here the relation @xmath100 means that the ratio of the left- and right - hand sides converges to  1 as @xmath41 . ) thus it can be seen that the variance of @xmath97 depends intimately on tail behaviour of the characteristic function @xmath19 of the measurement - error distribution .    if @xmath77 vanishes outside a compact set , which , as we noted in section  2.1 , is generally the case , and if @xmath101 is asymptotic to a positive regularly varying function @xmath102 ( see bingham 1989 ) , in the sense that @xmath103 ( meaning that the ratio of both sides is bounded away from zero and infinity as @xmath104 ) , then the integral on the right - hand side of ( [ efhatdecon ] ) is bounded between two constant multiples of @xmath105 as @xmath41",
    ". therefore by ( [ varfhat ] ) , provided that @xmath106 , @xmath107 as @xmath43 increases and @xmath35 decreases . recall that we are assuming that @xmath108 and @xmath34 are both symmetric functions .",
    "if the density @xmath28 of @xmath11 has two bounded and continuous derivatives , and if @xmath34 is bounded and symmetric and satisfies @xmath109 , then the bias of @xmath68 can be found from ( [ efhatdecon ] ) , using elementary calculus and arguments familiar in the case of standard kernel estimators : @xmath110 as @xmath41 , where @xmath111 . therefore , provided that @xmath112 , the bias of the conventional kernel estimator @xmath98 is exactly of size @xmath113 as @xmath41 . combining this property , ( [ efhatdecon ] ) and ( [ varasymp ] )",
    "we deduce a relatively concise asymptotic formula for the mean squared error of  @xmath97 : @xmath114 for a given error distribution we can work out the behaviour of @xmath115 as @xmath41 , and then from ( [ mse ] ) we can calculate the optimal bandwidth and determine the exact rate of convergence of @xmath97 to @xmath33 , in mean square .",
    "in many instances this rate is optimal , in a minimax sense ; see , for example , fan  ( 1991 ) .",
    "it is also generally optimal in the case of the errors - in - variables regression estimators discussed in section  2.1 , based on deconvolution - kernel versions of local polynomial estimators .",
    "see fan and truong  ( 1993 ) .",
    "therefore , despite their almost naive simplicity , deconvolution - kernel estimators of densities and regression functions have features that can hardly be bettered by more complex , alternative approaches .",
    "the results derived in the previous paragraph , and their counterparts in the regression case , imply that the estimators are limited by the extent to which they can recover from the data .",
    "( this is reflected in the fact that the rate of decay of the tails of @xmath19 drives the results on convergence rates . )",
    "however , the fact that the estimators are nevertheless optimal , in terms of their rates of convergence , implies that this restriction is inherent to the problem , not just to the estimators ; no other estimators would have a better convergence rate , at least not uniformly in a class of problems .",
    "the sinc , or fourier integral , kernel is given by @xmath116 its fourier transform , defined as a riemann integral , is the ` boxcar function ' , @xmath117 if @xmath79 and @xmath118 otherwise . in particular , @xmath119 vanishes outside a compact set , which property , as we noted in section  2.1 , aids computation .",
    "the version of @xmath65 , at ( [ ku ] ) , for the sinc kernel is @xmath120 where the second identity holds if the distribution of @xmath12 is symmetric and has no zeros on the real line .",
    "the kernel @xmath121 is sometimes said to be of ` infinite order ' , in the sense that if @xmath93 is any function with an infinite number of bounded , integrable derivatives then @xmath122 ^ 2\\,dx = o\\big(h^r\\big)\\label{intintaa}\\ ] ] as @xmath123 , for all @xmath124 .",
    "if @xmath34 were of finite order then ( [ intintaa ] ) would hold only for a finite range of values of @xmath81 , no matter how many derivatives the function @xmath93 enjoyed .",
    "for example , if @xmath34 were a symmetric function for which @xmath125 , and if we were to replace @xmath121 in ( [ intintaa ] ) by @xmath34 , then ( [ intintaa ] ) would hold only for @xmath126 , not for all  @xmath81 . in this case",
    "we would say that @xmath34 was of second order , because @xmath127    if we take @xmath93 to be the density , @xmath28 , of the random variable @xmath11 , and take @xmath34 in the definition of @xmath63 at ( [ fhat ] ) to be the sinc kernel , @xmath121 , then ( [ intintaa ] ) equals the integral of the squared bias of  @xmath63 .",
    "therefore , in the case of a very smooth density , the ` infinite order ' property of the sinc kernel ensures particularly small bias , in an average sense .",
    "properties of conventional kernel density estimators , but founded on the sinc kernel , for data without measurement errors , have been studied by , for example , davis ( 1975 , 1977 ) .",
    "glad ( 1999)have provided a good survey of properties of sinc kernel methods for density estimation , and have argued that those estimators have received an unfairly bad press . despite criticism of sinc kernel estimators ( see e.g.  politis and romano ,  1999 ) , the approach is `` more accurate for quite moderate values of the sample size , has better asymptotics in non - smooth cases ( the density to be estimated has only first derivative ) , [ and ] is more convenient for bandwidth selection etc '' than its conventional competitors , suggest glad ( 1999 ) .",
    "the property of greater accuracy is borne out in both theoretical and numerical studies , and derives from the infinite - order property noted above .",
    "indeed , if @xmath28 is very smooth then the low level of average squared bias can be exploited to produce an estimator @xmath63 with particularly low mean squared error , in fact of order @xmath128 in some cases .",
    "the most easily seen disadvantage of sinc - kernel density estimators is their tendency to suffer from spurious oscillations , inherited from the infinite number of oscillations of the kernel itself .",
    "these properties can be expected to carry over to density and regression estimators based on contaminated data , when we use the sinc kernel . to give a little detail in the case of density estimation from data contaminated by measurement errors",
    ", we note that if the density @xmath28 of @xmath11 is infinitely differentiable , but we observe only the contaminated data @xmath58 ,  , @xmath59 distributed as @xmath10 , generated as at ( [ wxu ] ) ; if we use the density estimator at ( [ fhat ] ) , but computed using @xmath129 , the sinc kernel ; and if @xmath130 for constants @xmath131 , @xmath132 ; then , in view of ( [ efhatdecon ] ) , ( [ varfhat ] ) and ( [ intintaa ] ) , we have for all @xmath124 , @xmath133 ^ 2\\,dx + ( nh)\\mo\\,\\int l_u^2\\notag \\\\ & { } \\qquad = o\\bigg\\{h^r+(nh)\\mo\\,\\int_{-1}^1|\\phi_u(t / h)|\\mt\\,dt\\bigg\\}\\notag \\\\ & { } \\qquad = o\\big\\{h^r+\\big(nh^{2\\a+1}\\big)\\mo\\big\\}\\,.\\label{intfhat}\\end{aligned}\\ ] ] it follows that , if @xmath28 has infinitely many integrable derivatives and if the tails of @xmath134 decrease at no faster than a polynomial rate as @xmath135 , then the bandwidth @xmath35 can be chosen so that the mean integrated squared error of a deconvolution kernel estimator of @xmath28 , using the sinc kernel , converges at rate @xmath136 for any given @xmath137 .    this very fast rate of convergence contrasts with that which occurs if the kernel @xmath34 is of only finite order .",
    "for example , if @xmath34 is a second - order kernel , in which case ( [ intintaa ] ) holds only for @xmath138 when @xmath121 is replaced by @xmath34 , the argument at ( [ intfhat ] ) gives : @xmath139 the fastest rate of convergence of the right - hand side to zero is attained with @xmath140 , giving @xmath141 in fact , this is generally the best rate of convergence of mean integrated squared error that can be obtained using a second - order kernel when the characteristic function @xmath19 decreases like @xmath142 in the tails , even if the density @xmath28 is exceptionally smooth .",
    "nevertheless , second - order kernels are often preferred to the sinc kernel in practice , since they do not suffer from the unwanted oscillations that afflict estimators based on the sinc kernel .",
    "[ [ minimum - contrast - estimators - and - their - relationship - to - deconvolution - kernel - estimators ] ] minimum contrast estimators , and their relationship to deconvolution kernel estimators ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    in the context of the measurement error model at ( [ wxu ] ) , comte _ et al . _",
    "( 2007 ) suggested an interesting minimum contrast estimator of the density @xmath28 of @xmath11 .",
    "their approach has applications in a variety of other settings ( see comte 2006 , 2008 ; comte and taupin , 2007 ) , including to the regression model at ( [ wyfromxu ] ) , and the conclusions we shall draw below apply in these cases too .",
    "therefore , for the sake of brevity we shall treat only the density deconvolution problem .    to describe the minimum contrast estimator in that setting ,",
    "define @xmath143 where @xmath144 denotes the fourier transform of the function @xmath145 defined by @xmath146 , @xmath147 is an integer and @xmath148 . in this notation the minimum contrast nonparametric density estimator",
    "is @xmath149 there are two tuning parameters , @xmath150 and  @xmath151 .",
    "comte ( 2007 ) suggest choosing @xmath151 to minimise a penalisation criterion .",
    "the resulting minimum contrast estimator is called a penalised contrast density estimator . the penalisation criterion suggested by comte ( 2007 ) for choosing @xmath151",
    "is related to cross - validation , although its exact form , which involves the choice of additional terms and multiplicative constants , is based on simulation experiments .",
    "it is clear on inspecting the definition of @xmath152 that @xmath151 plays a role similar to that of the inverse of bandwidth in a conventional deconvolution kernel estimator . in particular",
    ", @xmath151 should diverge to infinity with  @xmath43 .",
    "comte ( 2007 ) suggest taking @xmath153 , where @xmath154 is an integer . in numerical experiments they use @xmath155 , which gives good performance in the cases they consider . more generally , @xmath156 should diverge to infinity as sample size increases .",
    "the minimum contrast density estimator of comte ( 2007 ) is actually very close to the standard deconvolution kernel density estimator at ( [ fhat ] ) , where in the latter we use the sinc kernel at  ( [ lx ] ) . indeed , as the theorem below shows , the two estimators are exactly equal on a grid , which becomes finer as the bandwidth , @xmath35 , for the sinc kernel density estimator decreases .",
    "however , this relationship holds only for values of @xmath48 for which @xmath157 ; for larger values of @xmath158 on the grid , @xmath159 vanishes .",
    "( this property is one of the manifestations of the fact that , as noted earlier , @xmath147 and @xmath151 generally should be chosen to depend on sample size in such a manner that @xmath160 as @xmath161 . )",
    "let @xmath68 denote the deconvolution kernel density estimator at @xmath162 , constructed using the sinc kernel and employing the bandwidth @xmath163 .",
    "then , for any point @xmath164 with @xmath147 an integer , we have @xmath165    a proof of the theorem will be given in section  3.3 . between grid points",
    "the estimator @xmath152 is a nonstandard interpolation of values of the kernel estimator @xmath68 .",
    "note that , if we take @xmath163 , the weights @xmath166 used in the interpolation decrease quickly as @xmath147 moves further from @xmath167 , and , except for small @xmath147 , neighbour weights are close in magnitude but differ in sign .",
    "( here @xmath121 is the sinc kernel defined at  ( [ lx ] ) . ) in effect , the interpolation is based on rather few values @xmath168 corresponding to those @xmath147 for which @xmath147 is close to @xmath167 .    in practice",
    "the two estimators are almost indistinguishable . for example , figure  3.1 compares them using the bandwidth that minimises the integrated squared difference between the true density and the estimator , for one generated sample in the case where @xmath11 is normal n@xmath169 , @xmath12 is laplace with @xmath170 , and @xmath171 or @xmath172 . in the left graphs the two estimators",
    "can hardly be distinguished .",
    "the right graphs show magnifications of these estimators for @xmath173 $ ] . here",
    "it can be seen more clearly that the minimum contrast estimator is an approximation of the deconvolution kernel estimator , and is exactly equal to the latter at @xmath174 .",
    "these results highlight the fact that the differences in performance between the two estimators derive more from different tuning parameter choices than from anything else . in their comparison ,",
    "comte ( 2007 ) used a minimum contrast estimator with the sinc kernel @xmath121 and a bandwidth chosen by penalisation , whereas for the deconvolution kernel estimator they employed a conventional second - order kernel @xmath34 and a different bandwidth - choice procedure . against the background of the theoretical analysis in section  3.1 ,",
    "the different kernel choices ( and different ways of choosing smoothing parameters ) explain the differences observed between the penalised contrast density estimator and the deconvolution kernel density estimator based on a second - order kernel .",
    "note that @xmath175 and @xmath176 therefore , @xmath177 if @xmath81 is a nonzero integer then @xmath178 .",
    "therefore , if @xmath179 for an integer @xmath82 then @xmath180 whenever @xmath181 , and @xmath182 if @xmath183 .",
    "hence , ( [ tildef ] ) implies that @xmath184 if @xmath185 , and @xmath186 otherwise .",
    "bonhomme , s. , and robin , j .-",
    "_ generalized nonparametric deconvolution with an application to earnings dynamics_. university college london , centre for microdata methods & practice working paper 3/08 ; http://www.cemmap.ac.uk/wps/cwp308.pdf .",
    "glad , i. k. , hjort , n. l. , and ushakov , n. 1999 . _ density estimation using the sinc kernel_. department of mathematical sciences , norwegian university of science & technology , trondheim , statistics preprint no .",
    "2/2007 ; http://www.math.ntnu.no/preprint/statistics/2007/s2-2007.pdf ."
  ],
  "abstract_text": [
    "<S> we survey classical kernel methods for providing nonparametric solutions to problems involving measurement error . </S>",
    "<S> in particular we outline kernel - based methodology in this setting , and discuss its basic properties </S>",
    "<S> . then we point to close connections that exist between kernel methods and much newer approaches based on minimum contrast techniques . </S>",
    "<S> the connections are through use of the sinc kernel for kernel - based inference . </S>",
    "<S> this ` infinite order ' kernel is not often used explicitly for kernel - based deconvolution , although it has received attention in more conventional problems where measurement error is not an issue . </S>",
    "<S> we show that in a comparison between kernel methods for density deconvolution , and their counterparts based on minimum contrast , the two approaches give identical results on a grid which becomes increasingly fine as the bandwidth decreases . in consequence , </S>",
    "<S> the main numerical differences between these two techniques are arguably the result of different approaches to choosing smoothing parameters . </S>"
  ]
}