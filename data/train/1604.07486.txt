{
  "article_text": [
    "expansions of polynomials as finite series in orthogonal polynomial bases have applications throughout scientific computing , engineering , and physics  @xcite .",
    "the most popular expansions are in the chebyshev and legendre basis , @xmath2 , \\label{eq : pn}\\ ] ] where @xmath3 is a degree @xmath1 polynomial and @xmath4 and @xmath5 are the degree @xmath6 chebyshev and legendre polynomials , respectively .",
    "chebyshev expansions are used because of their near - optimal approximation properties and associated fast transforms  @xcite and legendre expansions for their @xmath7 orthogonality  ( * ? ? ?",
    "* table  18.3.1 ) as well as other recurrence relations that they satisfy  @xcite .",
    "a useful working paradigm is to represent a smooth function on a finite interval using the chebyshev basis and to convert to a different polynomial basis , such as legendre , whenever it is algorithmically convenient to do so  @xcite .",
    "it is therefore important to have fast transforms for converting a vector of coefficients in one polynomial basis to another",
    ".    given two sequences of orthogonal polynomials @xmath8 and @xmath9 , there is an @xmath10 upper - triangular _ conversion matrix _",
    "( sometimes called the connection coefficients matrix ) , @xmath11 , such that @xmath12 where @xmath13 and @xmath14 . in this paper",
    "we describe how to compute the matrix - vector product @xmath15 in @xmath0 operations when @xmath8 and @xmath9 are sets of standard orthogonal polynomials such as chebyshev , legendre ( see sections  [ sec : hankelpart ] and  [ sec : cheb2leg ] ) , ultraspherical ( see section  [ sec : ultraphericalconversions ] ) , jacobi ( see section  [ sec : jacobiconversions ] ) , and laguerre ( see section  [ sec : additionalconversions ] ) .",
    "there are many existing fast algorithms for computing @xmath15 that exploit a variety of structures , including : ( 1 ) a hierarchical off - diagonal low rank structure  @xcite , ( 2 ) an eigenvalue decomposition involving a semiseparable matrix  @xcite , and ( 3 ) trigonometric - like behavior of orthogonal polynomials via asymptotic expansions  @xcite . in this paper , we exploit a new observation that all standard conversion matrices can be decomposed using diagonally - scaled hadamard products involving toeplitz and hankel matrices . , is a matrix that is constant along each diagonal , i.e. , @xmath16 .",
    "a hankel matrix , @xmath17 , is a matrix that is constant along each anti - diagonal , i.e. , @xmath18 . ]",
    "this structure allows us to derive fast fft - based algorithms for computing @xmath15 that can be implemented in any programming language in just a handful of lines of code .",
    "we believe this algorithm is conceptually simpler than previous approaches , while being competitive in terms of computational speed ( see sections  [ sec : numericalresults ] and  [ sec : otherconversions ] ) .",
    "it is easy to see the structure that we exploit by considering an example .",
    "consider the @xmath10 legendre - to - chebyshev conversion matrix , which converts a vector of legendre coefficients to chebyshev coefficients for a degree @xmath1 polynomial .",
    "it is given by  ( * ? ? ?",
    "( 2.18 ) ) @xmath19 where @xmath20 , @xmath21 is the gamma function , and @xmath22 and @xmath23 are the chebyshev and legendre coefficients of @xmath24 in  , respectively .",
    "the explicit formula in   reveals that after a trivial diagonal scaling , the matrix @xmath25 can be written as a hadamard product between an upper - triangular toeplitz matrix , from the @xmath26 term , and a hankel matrix , from the term @xmath27 .",
    "thus , for the matrix @xmath25 in   we can write @xmath28 where @xmath29 and @xmath30 are diagonal matrices , @xmath31 is an upper - triangular toeplitz matrix , @xmath17 is a hankel matrix , and ` @xmath32 ' is the hadamard matrix product , i.e. , entrywise multiplication between two matrices ( see   for explicit formulas for @xmath29 , @xmath30 , @xmath31 and @xmath17 ) .",
    "we find that the structure in   holds for many of the standard conversion matrices ( see section  [ sec : otherconversions ] ) .    at first",
    "it is not obvious why the decomposition in   is useful for deriving a fast matrix - vector product because for general toeplitz and hankel matrices we are not aware of a fast algorithm for computing @xmath33 .",
    "however , for conversion matrices the hankel matrix in   is often real and positive semidefinite and hence , is severely ill - conditioned  @xcite .",
    "theorem  [ thm : lownumericalrank ] shows that the matrix @xmath17 in   can be approximated , up to an accuracy of @xmath34 , by a rank @xmath35 matrix . in practice , we take @xmath36 as a small multiple of machine epsilon .    for an integer @xmath37 , we construct a rank @xmath37 approximation of @xmath17 in @xmath38 operations using the pivoted cholesky algorithm ( see section  [ sec : pivotedcholesky ] ) to obtain the approximation , @xmath39 since @xmath37 needs to be no larger than @xmath40 we can compute   in a total of @xmath41 operations .",
    "moreover , using @xmath42 , where @xmath43 , we can write @xmath44 where each term is a diagonally - scaled toeplitz matrix whose matrix - vector product can be computed in @xmath45 operations via the fast fourier transform ( fft )  ( * ? ? ?",
    "there are @xmath37 terms in  , so the matrix - vector product @xmath46 can be computed , up to an error of @xmath47 , in @xmath48 operations . for the majority of this paper we write algorithmic complexities without the explicit dependency on @xmath36 .",
    "the approach for fast polynomial basis conversion , as outlined above , requires no hierarchical data structures , no precomputional setup cost , and no matrix partitioning .",
    "the fundamental step is a fast toeplitz matrix - vector product and the cost of the matrix - vector product is precisely @xmath49 ffts of size @xmath50  ( * ? ? ?",
    "the fact that our algorithm relies on the fft means that the implementation is automatically tuned to personal computer architectures , thanks to fftw  @xcite .",
    "our algorithm is now the default algorithm for polynomial basis conversion in the matlab software system called chebfun  @xcite ( see the commands leg2cheb ,",
    "cheb2leg , ultra2ultra , and jac2jac ) and the julia package called approxfun.jl  @xcite , via the package fasttransforms.jl  @xcite ( see commands leg2cheb and cheb2leg ) .",
    "the paper is structured as follows . in section  [ sec : fastmatvec ] we describe a pivoted cholesky algorithm for constructing low rank approximations of real , symmetric , and positive semidefinite matrices and use it to compute @xmath33 in @xmath51 operations , where @xmath31 is an @xmath10 toeplitz matrix and @xmath17 is a real , symmetric , and positive semidefinite hankel matrix . in section  [ sec : hankelpart ] we write @xmath52 as in   and show that the hankel part , @xmath17 , can be approximated , up to an error of @xmath34 , by a rank @xmath53 matrix . in section",
    "[ sec : numericalresults ] we compare various algorithms for converting legendre - to - chebyshev basis conversion . in section",
    "[ sec : otherconversions ] we show that our algorithm allows for fast matrix - vector products involving many of the standard conversion matrices and we give numerical results throughout that section .",
    "we say that a matrix @xmath11 is a _ toeplitz - dot - hankel _ matrix if @xmath11 can be written as a hadamard product of a toeplitz and hankel matrix , i.e. , @xmath54 , where @xmath31 is a toeplitz matrix , @xmath17 is a hankel matrix , and ` @xmath32 ' denotes the hadamard product . in this section ,",
    "we suppose that @xmath17 is a real , symmetric , and positive semidefinite hankel matrix and that it is approximated , up to an error of @xmath34 , by a rank @xmath37 matrix .",
    "later , in section  [ sec : hankelpart ] we show that this holds for the hankel part of the legendre - to - chebyshev conversion matrix in   when @xmath55 .",
    "once we have constructed the low rank approximation in  , costing @xmath38 operations , a fast matrix - vector product is immediate as   shows that @xmath33 can be computed as a sum of matrix - vector products involving a toeplitz matrix .",
    "suppose that @xmath17 is approximated up to an error of @xmath36 by a rank @xmath37 matrix .",
    "we would like to compute a rank @xmath37 approximation to @xmath17 , i.e. , @xmath56 and @xmath57 in  .    in principle , one could construct a best rank @xmath37 approximation of @xmath17 by computing the singular value decomposition ( svd ) of @xmath17 and taking the first @xmath37 left and right singular vectors as well as the first @xmath37 singular values .",
    "naively , this costs a prohibitive @xmath58 operations , which can be reduced to @xmath59 operations if the hankel structure is exploited  @xcite .",
    "instead , we describe an algorithm that costs only @xmath60 operations based on a pivoted cholesky algorithm .",
    "it can be applied to any real , symmetric , and positive semidefinite matrix and does not exploit the hankel structure of @xmath17 . in practice ,",
    "if the singular values of @xmath17 decay rapidly , then the rank @xmath37 approximation constructed by our pivoted cholesky algorithm is observed to be near - best ( see figure  [ fig : comparecholeskyandsvd ] ) .",
    "our algorithm is very similar to the pivoted cholesky algorithm described in  @xcite , except we avoid square roots and have a different stopping criterion .",
    "one can trace back the origin of our algorithm to the rank revealing literature  @xcite .",
    "set @xmath61 and assume that @xmath17 is a nonzero , real , symmetric , and positive semidefinite matrix .",
    "first , the maximum on the diagonal of @xmath17 is selected , say @xmath62 , which is also the absolute global maximum entry of @xmath17  ( * ? ? ?",
    "then , one step of the cholesky algorithm is performed with the entry @xmath63 as the pivot , i.e. , @xmath64 where @xmath65 and @xmath66 denotes the @xmath67th column and @xmath67th row of @xmath68 , respectively .",
    "the matrix @xmath69 has a zero @xmath67th column and @xmath67th row .",
    "the cholesky step in   is closely related to the schur complement of the @xmath63 entry in @xmath68 and the matrix @xmath69 is guaranteed to be real , symmetric , and positive semidefinite .",
    "furthermore , the rank of @xmath69 is exactly one less than @xmath68  ( * ? ? ?",
    "19.2 ) .",
    "next , the maximum on the diagonal of @xmath69 is found , say @xmath70 .",
    "if @xmath71 or is sufficiently small , then the process is terminated ; otherwise , another cholesky step is taken with the entry @xmath72 as the pivot , i.e. , @xmath73 again , the matrix @xmath74 is guaranteed to be real , symmetric , and positive semidefinite and has a rank that is exactly one less than @xmath69 .",
    "the pivoted cholesky algorithm continues until the maximum on diagonal is either zero or sufficiently small .",
    "since the rank of @xmath17 is at most @xmath1 and the rank decreases by precisely one after each cholesky step , the algorithm terminates in at most @xmath1 steps . for the algorithm to be computationally more efficient than the svd one hopes to need just @xmath75 steps .",
    "suppose that the pivoted cholesky algorithm takes @xmath37 steps before terminating .",
    "since @xmath76 are symmetric matrices , we can write the @xmath77th cholesky step as follows : @xmath78 therefore , we use the @xmath77th pivoted cholesky step to construct the @xmath77th term in  , where after @xmath37 steps the rank @xmath37 approximation to @xmath17 is constructed .",
    "the pivoted cholesky algorithm described so far requires a total of @xmath79 operations because at each step an @xmath10 matrix is updated .",
    "now we will describe how to construct the same rank @xmath37 approximant to @xmath17 in @xmath38 operations , which is a significant saving when @xmath75 .",
    "the main idea to reduce the computational cost is to note that it is not necessary to update the whole matrix at each cholesky step .",
    "for example , consider the hankel part , @xmath17 , of the matrix @xmath25 in   for @xmath80 .",
    "figure  [ fig : pivotedcholeskylocations ] shows the pivot locations selected by the pivoted cholesky algorithm , where a total of @xmath81 steps was required to construct a low rank approximant of @xmath17 that is accurate up to double precision .",
    "this means that only @xmath81 columns from @xmath17 are required to compute   ( see black vertical lines on figure  [ fig : pivotedcholeskylocations ] ) .",
    "therefore , we rewrite the pivoted cholesky algorithm so that it only updates the diagonal entries ( required to determine the pivot locations ) and those @xmath81 columns of @xmath17 .",
    "this allows for a significant computational saving when @xmath75 .",
    "pivotcholeskylocations    let @xmath82 be the diagonal entries of @xmath17 . in the first step , instead of",
    ", we only update the diagonal as follows : @xmath83 where we have used the fact that @xmath84 . the diagonal can then be used to determine the location of the second pivot . for the second step ,",
    "we again only update the diagonal , @xmath85 where @xmath86 means that the vector @xmath87 is squared entry - by - entry .",
    "since the pivot locations at each step are determined by the diagonal entries , one can select the pivoting entries by only updating the diagonal vector @xmath88 . at the @xmath77th cholesky step the column @xmath89",
    "is required , which is not directly available from the matrix @xmath17 .",
    "we calculate this by first constructing @xmath90 and by applying each of the previous @xmath91 cholesky steps to @xmath90 ( see also  @xcite ) .",
    "figure  [ fig : cholekydiagram ] presents a summary of the algorithm .",
    "a simple operation count reveals that the algorithm costs @xmath38 operations",
    ".    one may be legitimately concerned that the low rank approximants constructed by the pivoting cholesky algorithm in figure  [ fig : cholekydiagram ] are of poor quality , as they are not strictly best low rank approximants .",
    "more precisely , suppose that @xmath92 is the best rank @xmath37 approximant of @xmath17 in the matrix @xmath93-norm computed via the svd and @xmath94 is constructed via the pivoted cholesky algorithm .",
    "the best mathematical statement we know of is  ( * ? ? ?",
    "* thm .  3.2 ) , which states that , provided that the singular values @xmath95 of @xmath17 decay at a geometric rate that is faster than @xmath96 , then the constructed rank @xmath37 approximant converges geometrically to @xmath17 .",
    "however , the assumptions of their theorem are considered to be much stronger than necessary ( see  ( * ? ? ?",
    "* chap .  4 ) ) .",
    "is it possible that @xmath97 otherwise ?",
    "when @xmath17 has moderately decaying singular values , we believe not .",
    "one representative numerical experiment is shown in figure  [ fig : comparecholeskyandsvd ] ( left ) , where the low rank approximants constructed by the pivoted cholesky algorithm are compared against those from the svd for the hankel part of the matrix @xmath25 in   when @xmath98 .",
    "all other investigations have revealed similar results  @xcite .",
    "a precise theorem that adequately describes the power of the pivoted cholesky algorithm for constructing low rank approximants is a remaining mathematical challenge and may require a much deeper understanding on the numerical stability of gaussian elimination . in principle",
    ", we could have presented a variant on the pivoted cholesky algorithm to ensure that it is a so - called strong rank - revealing algorithm  @xcite .",
    "however , this makes the pivoted cholesky algorithm have a cost of @xmath99 operations and such a modification seems unnecessary in practice here .",
    "choleskyversussvd ( 43,55 ) ( 45,35 ) ( 0,25 ) ( 47,0 ) rank ( 10,73 ) cholesky s near - best approximants    choleskyversussvd_timings ( 55,40 ) ( 32,45 ) ( 57,23 ) ( 45,45 ) ( 20,73 ) cholesky s execution time ( 50,0 ) @xmath1 ( 0,10 )    our pivoted cholesky algorithm costs @xmath38 operations . in figure",
    "[ fig : comparecholeskyandsvd ] ( right ) we compare the execution time for computing the svd and the pivoted cholesky algorithm on the hankel part of the matrix @xmath25 in   for obtaining an accuracy of essentially double precision .",
    "one can see that even when @xmath100 , the pivoted cholesky algorithm can be employed to construct  , whereas the svd is limited to @xmath101 . in some applications",
    "the size of the matrix @xmath25 is fixed and one wishes to convert between two polynomial bases for many different polynomials of the same degree .",
    "in such a situation , the pivoted cholesky algorithm can construct a low rank approximation for the hankel part just once and it can be reused for each matrix - vector product .",
    "let @xmath1 be an integer .",
    "the @xmath10 legendre - to - chebyshev matrix , denoted by @xmath25 , in   can be written as a diagonal - scaled toeplitz - dot - hankel matrix .",
    "that is , @xmath52 , where @xmath102 , @xmath103 the @xmath104 identity matrix , and @xmath105 here @xmath20 , where @xmath21 is the gamma function . in this section",
    "we show that @xmath17 is : ( 1 ) real , symmetric , and positive semidefinite ( see section  [ sec : hankelposdef ] ) and ( 2 ) @xmath17 can be approximated , up to an accuracy of @xmath34 , by a rank @xmath35 matrix ( see section  [ sec : hankellowrank ] ) .",
    "the hankel matrix @xmath17 in   is immediately seen to be real and symmetric . to show that it is positive semidefinite , we recall that the hamburger moment problem states that a real hankel matrix is positive semidefinite if and only if it is associated to a nonnegative borel measure supported on the real line .",
    "[ lem : hamburger ] a real @xmath106 hankel matrix , @xmath17 , is positive semidefinite if and only if there exists a nonnegative borel measure @xmath107 supported on the real line such that @xmath108    for a proof , see  ( * ? ? ?",
    "we show that the hankel matrix in   is positive semidefinite by expressing its entries in the form of  .",
    "the hankel matrix , @xmath17 , in   is positive semidefinite .",
    "[ thm : posdefhankelmatrix ]    it can be verified that ( see  ( * ? ? ?",
    "* sec .  7 ) for integral representations of ratios of gamma functions ) @xmath109 by setting @xmath110 in lemma  [ lem : hamburger ] , where @xmath111 is the characteristic function for the interval @xmath112 , we conclude that @xmath17 is a positive semidefinite matrix .",
    "theorem  [ thm : posdefhankelmatrix ] shows that @xmath17 is positive semidefinite and therefore , the pivoted cholesky algorithm described in section  [ sec : pivotedcholesky ] is applicable .",
    "we now show that @xmath17 can be well - approximated by a rank @xmath37 matrix where @xmath75 .",
    "in section  [ sec : hankelposdef ] we showed that @xmath17 in   is real , symmetric , and positive semidefinite .",
    "such hankel matrices are severely ill - conditioned , and beckermann has proved the remarkably tight bound of @xmath113  @xcite , where @xmath114 denotes the condition number of @xmath17 in the matrix @xmath93-norm .",
    "this shows that @xmath17 is in fact exponentially ill - conditioned , so one might expect that the singularly values @xmath115 of @xmath17 decay geometrically to zero .",
    "indeed they do decay exponentially , and in this section we show that @xmath17 can be approximated to an accuracy of @xmath34 by a rank @xmath116 matrix .    first , we extract out the first row of @xmath17 . that is , @xmath117 & \\hat{h } &   \\end{bmatrix } ,   \\label{eq : splitfirstrow}\\ ] ] where @xmath118 is an @xmath119 submatrix of @xmath17 .",
    "then , we view @xmath118 as being obtained by sampling the function @xmath120 at the tensor grid @xmath121 . a similar strategy is employed by grasedyck to show that the singular values of the cauchy function decay geometrically to zero  @xcite .",
    "we build a function @xmath122 defined on @xmath123\\times [ 1,n]$ ] that is of rank @xmath124 such that @xmath125\\times [ 1,n ] } \\left| \\lambda\\left(\\tfrac{x+y}{2}\\right ) - h(x , y ) \\right| \\leq \\epsilon.\\ ] ] by sampling @xmath122 at the grid @xmath121 we construct a matrix of rank @xmath124 that approximates @xmath118 to an accuracy of @xmath36 .",
    "we construct @xmath122 by geometrically dividing the domain @xmath123\\times [ 1,n]$ ] at @xmath126 into domains @xmath123\\times [ 1,\\beta^{k } n ] ,   [ 0,n]\\times [ \\beta^k n,\\beta^{k-1 } n ] , \\ldots , [ 0,n]\\times [ \\beta n , n]$ ] , where @xmath127 is a parameter to be selected ( see figure  [ fig : geometricsubdivision ] ) .",
    "\\times [ 1,n]$ ] that is used to show that the hankel part , @xmath17 , of @xmath25 in   can be approximated , up to an error of @xmath36 , by a rank @xmath35 matrix .",
    "first , we extract off the first row of @xmath17 and view the matrix @xmath118 in   as derived from sampling @xmath120 at equally - spaced points on @xmath123\\times [ 1,n]$ ] . motivated by  @xcite , we geometrically subdivide the domain @xmath123\\times [ 1,n]$ ] at @xmath126 and bound the numerical rank of @xmath120 on each domain by using taylor series expansions ( see lemma  [ lem : lownumericalranksections ] ) . ]    on each domain of the form @xmath123\\times [ \\beta^j n,\\beta^{j-1 } n]$ ] we can use taylor expansions to relatively tightly bound the numerical rank of the function @xmath120 .",
    "later , we will sum these ranks together to obtain a bound on the numerical rank of @xmath120 on @xmath123\\times[1,n]$ ] .",
    "our first task is to control the error from a taylor expansion and to do this we bound the @xmath128-derivative of @xmath120 .",
    "let @xmath129 and @xmath130 be integers .",
    "then , @xmath131\\times[1,n].\\ ] ] [ lem : boundedderivative ]    note that @xmath20 is an analytic function in @xmath132 except at the poles @xmath133 .",
    "therefore , for each fixed @xmath134 $ ] the function @xmath135 is analytic except at @xmath136 .",
    "hence , @xmath135 is analytic in the disk in @xmath132 of radius @xmath128 centered at @xmath137 .",
    "( note that it is also analytic in the disk of radius @xmath138 centered at @xmath137 . )    directly from the cauchy integral formula ( see , for example ,  ( * ? ? ?",
    "* lemma  2.2 ) ) , we know that @xmath139 since @xmath140 for all @xmath141 such that @xmath142  ( * ? ? ? * lemma  2.4 ) , we have @xmath143 as required .",
    "now that we have a bound on the @xmath128-derivatives of @xmath144 , we can derive error bounds on a taylor expansion of @xmath144 in the @xmath128-variable on the domain @xmath123\\times [ \\beta^kn,\\beta^{k-1}n]$ ] .",
    "let @xmath34 , @xmath6 be an integer , and @xmath145 such that @xmath146 .",
    "then , there is a rank @xmath147 function @xmath148 on @xmath123\\times [ \\beta^kn,\\beta^{k-1}n]$ ] such that @xmath149\\times [ \\beta^kn,\\beta^{k-1}n].\\ ] ] [ lem : lownumericalranksections ]    by taylor s theorem expanding about @xmath150 , we have @xmath151 .",
    "\\label{eq : taylor}\\ ] ] moreover , for @xmath152\\times [ \\beta^kn,\\beta^{k-1}n]$ ] we have @xmath153 where we used the fact that @xmath154 $ ] and lemma  [ lem : boundedderivative ] .",
    "since @xmath155 , the taylor error satisfies @xmath156 .",
    "the result follows since @xmath148 in   is a function of rank at most @xmath37 , i.e. , it can be written as a sum of @xmath37 terms of the form @xmath157 .",
    "lemma  [ lem : lownumericalranksections ] shows that on each domain of the form @xmath123\\times [ \\beta^kn,\\beta^{k-1}n]$ ] the function @xmath144 can be approximated , up to an error of @xmath36 , by a rank @xmath158 function .",
    "importantly , the rank of the function @xmath148 on @xmath123\\times [ \\beta^kn,\\beta^{k-1}n]$ ] in lemma  [ lem : lownumericalranksections ] does not depend on @xmath6 .",
    "this means that @xmath144 has approximately the same numerical rank on each subdomain in figure  [ fig : geometricsubdivision ] .",
    "since @xmath123\\times [ 1,n]$ ] was partitioned into a total of @xmath159 domains , the function @xmath144 on @xmath123\\times [ 1,n]$ ] can be approximated up to an error of @xmath36 , by a rank @xmath40 function .",
    "there is a rank @xmath37 function @xmath160 defined on @xmath123\\times [ 1,n]$ ] such that @xmath161\\times [ 1,n],\\ ] ] where @xmath162 .",
    "[ thm : lownumericalrank ]    let @xmath163 .",
    "concatenate together the @xmath164 functions @xmath165 on the domains @xmath123\\times [ 1,\\beta^{k } n ] ,   [ 0,n]\\times [ \\beta^k n,\\beta^{k-1 }",
    "n ] , \\ldots , [ 0,n]\\times [ \\beta n , n]$ ] , respectively , from lemma  [ lem : lownumericalranksections ] .",
    "the resulting function , say @xmath160 , has a rank at most the sum of the ranks of @xmath166 .",
    "theorem  [ thm : lownumericalrank ] is sufficient for the purposes of this paper .",
    "one can sample the constructed function in @xmath122 in theorem  [ thm : lownumericalrank ] at the tensor grid @xmath121 to obtain a rank @xmath167 matrix @xmath118 that approximates @xmath168 in  , up to an accuracy of @xmath36 .",
    "therefore , @xmath17 can also be approximated by a rank @xmath169 matrix . in practice",
    ", we use the pivoted cholesky algorithm ( see section  [ sec : pivotedcholesky ] ) to construct a low rank approximant for @xmath17 in @xmath38 operations .    using the formula in   we",
    "can then calculate @xmath170 via a sum of @xmath49 diagonally - scaled toeplitz matrix - vector products .",
    "hence , we have described an @xmath51 algorithm for computing @xmath170 ( see figure  [ fig : summary ] ) .",
    "all the numerical results were performed on a 3.1 ghz intel core i7 macbook pro 2015 with matlab 2015b or julia v0.4.5  @xcite . in these numerical experiments",
    "we employ three different algorithms for computing the matrix - vector product @xmath171 :    * * direct : * the direct algorithm computes @xmath172 by first constructing the @xmath10 matrix @xmath25 one row at a time and then calculating the dot product with @xmath173 . therefore ,",
    "the vector @xmath172 is computed entry - by - entry , costing a total of @xmath99 operations and requiring @xmath174 storage . * * asy : * the algorithm that for shorthand we call asy here is described in  @xcite .",
    "it computes the matrix - vector product @xmath171 in @xmath175 operations by using a trigonometric asymptotic formula for legendre polynomials . before this paper , it was the algorithm employed in the leg2cheb command in chebfun  @xcite .",
    "* * new : * the algorithm described in this paper .",
    "it is summarized in figure  [ fig : summary ] , costing @xmath51 operations .",
    "other algorithms for computing the matrix - vector product @xmath170 in fewer than @xmath99 operations are given in the pioneering paper by alpert and rokhlin  @xcite as well as  @xcite .",
    "as a first test we take arbitrarily distributed vectors @xmath173 with various rates of decay and consider the accuracy of our algorithm described in this paper against an extended precision computation ( performed using the bigfloat type in julia ) . with @xmath176 decay can be reproduced exactly by the julia code srand(0 ) ; c = randn(101)./(1:101 ) .",
    "the fixed random seed is employed for the sake of reproducibility . ]",
    "figure  [ fig : leg2chebresults ] ( left ) shows the absolute maximum errors in the computed vectors @xmath172 for @xmath177 . in  (",
    "5.1 ) analogous errors were calculated for the direct and asy algorithms . in table  [ tab : errorgrowth ] we summarise the observed error growth in the absolute maximum for the three different algorithms . in many applications the legendre expansion in   represents a polynomial interpolant of a smooth function . in this setting , if the function is hlder continuous with parameter greater than @xmath178 , then we observe that our new algorithm has essentially no error growth with @xmath1 .",
    "leg2cheb_errors - eps - converted - to.pdf ( 50,0 ) @xmath1 ( 0,22 ) ( 35,52 )    newversusold_timings2 ( 45,45 ) ( 60,33 ) ( 0,10 ) ( 50,0 ) @xmath1    ccccc + & @xmath179 & @xmath180 & @xmath176 & @xmath181 +    ' '' ''    direct & @xmath182 & @xmath183 & @xmath184 & @xmath159 + asy & @xmath174 & @xmath185 & @xmath159 & @xmath179 + new & @xmath186 & @xmath179 & @xmath179 & @xmath179 +    for a second test , in figure  [ fig : leg2chebresults ] ( right ) we compare the execution times for the three algorithms . despite the direct algorithm requiring @xmath99 operations , it is computationally more efficient when @xmath187 . the new algorithm presented here is @xmath93 or @xmath188 times faster than the asy algorithm for large @xmath1 , while being conceptually simpler and more accurate for nondecaying vectors @xmath173 .",
    "based on these numerical experiments , the leg2cheb command in chebfun  @xcite and the leg2cheb in fasttransforms.jl  @xcite use the direct algorithm when @xmath187 and the new algorithm otherwise .",
    "so far the paper has focused on the task of converting legendre coefficients for @xmath3 in   to chebyshev coefficients . in this section",
    "we consider other standard polynomial basis conversions , showing how our @xmath51 algorithm summarized in figure  [ fig : summary ] remains applicable .      to compute the legendre coefficients of a given polynomial @xmath3 in fewer than @xmath99 operations",
    ", one can first compute the chebyshev coefficients using the discrete cosine transform ( dct ) of its values at chebyshev points in @xmath45 operations  @xcite , then use a fast chebyshev - to - legendre conversion .",
    "alternatively , a direct transform taking values of the polynomial in the complex plane to legendre coefficients is given in  @xcite and a fast transform for converting values of the polynomial at legendre points to legendre coefficients is given in  @xcite .",
    "the inverse of the legendre - to - chebyshev matrix @xmath189 , denoted by @xmath190 in  @xcite , converts chebyshev coefficients to legendre coefficients , i.e. , @xmath191 in  .",
    "explicit formulas for the entries of @xmath192 are given as follows  ( * ? ? ?",
    "* ( 2.19 ) ) : @xmath193 where @xmath20 and @xmath21 is the gamma function .",
    "the fact that @xmath190 can be written as @xmath194 is almost immediate from  . in particular , we have @xmath195 , @xmath196 , and @xmath197",
    "unfortunately , the matrix @xmath17 is not positive semidefinite .",
    "this turns out not to matter , because the submatrix @xmath198 is positive semi - definite by the following identity ( see lemma  [ lem : hamburger ] ) , @xmath199 moreover , a similar analysis to that in section  [ sec : hankellowrank ] can be used to show that @xmath168 can be approximated , up to an error of @xmath34 , by a rank @xmath200 matrix .",
    "hence , when computing @xmath191 , we compute the first entry of @xmath173 directly , and use the algorithm described in figure [ fig : summary ] on @xmath201 to compute the remaining entries . the resulting algorithm is implemented in the cheb2leg commands in chebfun  @xcite and fasttransforms.jl  @xcite .    in figure",
    "[ fig : cheb2legresults ] we repeat the same experiments as for the legendre - to - chebyshev conversion in section  [ sec : numericalresults ] . in figure  [ fig : cheb2legresults ] ( left ) we compute the maximum error of the resulting vector @xmath173 for different decay rates in @xmath172 . due to the @xmath174 growth in the entries of @xmath29",
    ", we find that the conversion requires a decay faster than @xmath176 in @xmath172 to have essentially no error growth .",
    "this holds when @xmath3 in   is a chebyshev interpolant of a hlder continuous function with hlder parameter @xmath202 .",
    "the observed error growth is less than that observed for the chebyshev - to - legendre algorithm in  ( * ? ?",
    "5.2 ) .    in figure",
    "[ fig : cheb2legresults ] ( right ) we show the execution times of the three algorithms : ( 1 ) direct , an algorithm that costs @xmath99 operations and requires @xmath174 memory based on generating the whole matrix @xmath190 one row at a time , ( 2 ) asy , an @xmath175 complexity algorithm described in  ( * ? ? ?",
    "4 ) , and ( 3 ) new , the algorithm described in this paper ( see figure  [ fig : summary ] ) . our new algorithm is faster than direct when @xmath203 and is about @xmath93 or @xmath188 times faster than the asy algorithm for large @xmath1 .    cheb2leg_errors - eps - converted - to ( 50,0 ) @xmath1 ( 0,22 ) ( 45,57 ) ( 45,43 ) ( 45,31 )    newversusold_timings - eps - converted - to ( 45,45 ) ( 60,30 ) ( 0,10 ) ( 50,0 ) @xmath1      the ultraspherical polynomial of degree @xmath6 and parameter @xmath204 is denoted by @xmath205  ( * ? ? ?",
    "if @xmath204 , then @xmath206 is a family of orthogonal polynomials that are orthogonal with respect to the weight function @xmath207 on @xmath208 $ ] .",
    "ultraspherical polynomials can be seen as a generalization of legendre polynomials since @xmath209  ( * ? ? ?",
    "* ( 18.7.9 ) ) . for papers on computing and converting ultraspherical expansions",
    "see , for example ,  @xcite .",
    "let @xmath210 and @xmath211 .",
    "the degree @xmath1 polynomial @xmath3 in   can be expanded in the two ultraspherical polynomial bases associated to @xmath212 and @xmath213 , i.e. , @xmath214 there is an upper - triangular conversion matrix , @xmath11 , such that @xmath215 .",
    "we desire a fast algorithm for computing the matrix - vector product @xmath215 .",
    "there are several cases to consider : ( 1 ) @xmath216 is an integer , ( 2 ) @xmath217 , and ( 3 ) @xmath218 , but the difference is a noninteger .",
    "if @xmath216 is an integer , then an @xmath174 matrix - vector product is immediate from the recurrence relation  ( * ? ? ?",
    "* ( 18.9.7 ) ) .",
    "for example , if @xmath220 is a positive integer , then we can factor @xmath11 into the product of sparse matrices as follows : @xmath221 where @xmath222 is the @xmath223 project matrix .",
    "the matrix - vector product @xmath215 can be computed in @xmath174 operations by applying each truncated sparse factor in turn .",
    "since @xmath224 is banded and upper - triangular for all @xmath204 , the matrix - vector product @xmath215 can also be computed in @xmath174 operations when @xmath225 by using backward substitution .",
    "the factorization in   is one key decomposition for the ultraspherical spectral method  @xcite .",
    "now assume that @xmath227 , then the conversion matrix , @xmath11 , in @xmath215 has the following explicit formula  ( * ? ? ?",
    "* ( 3.6 ) ) : @xmath228 the formula in   reveals that the matrix @xmath11 can be written as a diagonally - scaled toeplitz - dot - hankel matrix .",
    "more precisely , let @xmath229 , @xmath230 be the @xmath10 identity matrix , and @xmath231 since the entries of the hankel part can be expressed as @xmath232 we know from lemma  [ lem : hamburger ] that @xmath17 is real , symmetric , and positive semidefinite ( note that @xmath233 and @xmath234 so that the measure @xmath107 is locally finite ) .",
    "moreover , a similar analysis to section  [ sec : hankellowrank ] shows that @xmath17 can be approximated , up to an error of @xmath36 , by a rank @xmath35 matrix .",
    "therefore , the algorithm that is summarized in figure  [ fig : summary ] is applicable in this case and can be used to compute @xmath215 in @xmath51 operations .",
    "if @xmath218 , then we reduce the quantity @xmath236 by converting to either increase or reduce @xmath212 by one ( see case @xmath237 ) . this is repeated if necessary until @xmath217 and the criterion for case @xmath93 is satisfied .",
    "conversions such as ultraspherical - to - chebyshev and chebyshev - to - ultraspherical are associated to upper - triangular matrices that can also be written as diagonally - scaled toeplitz - dot - hankel matrices .",
    "fast @xmath51 algorithms based on figure  [ fig : summary ] for these conversions are also possible using the formulas in  ( * ? ? ?",
    "3.1 ) .",
    "the jacobi polynomial of degree @xmath6 and parameter @xmath238 is denoted by @xmath239 , where @xmath240  ( * ? ? ?",
    "18.3.1 ) . the family of orthogonal polynomials @xmath241 is orthogonal with respect to the weight function @xmath242 on @xmath208 $ ] .",
    "the jacobi polynomials can be seen as a generalization of chebyshev , legendre , and ultraspherical polynomials , see  ( * ? ? ?",
    "18.7 ) .    for @xmath243 , the degree @xmath1 polynomial @xmath24 in   can be expanded in jacobi bases as follows : @xmath244 where there is an upper - triangular matrix , @xmath11 , such that @xmath245 . by the reflection formula @xmath246  (",
    "18.6.1 ) , it is sufficient to assume that @xmath247 in  .",
    ", then a jacobi @xmath238 expansion can first be converted to a jacobi @xmath248 expansion and then a jacobi @xmath249 expansion . ] a different fast algorithm using off - diagonal low rank structure of the conversion matrix , which has a fast quasilinear complexity online cost and an @xmath99 precomputation is given in  @xcite .",
    "another fast algorithm for computing jacobi expansions coefficients of analytic functions is described in  @xcite .    as in section",
    "[ sec : ultraphericalconversions ] there are several cases to consider when computing @xmath245 , where @xmath247 , in fewer than @xmath99 operations : ( 1 ) @xmath250 is an integer , ( 2 ) @xmath251 and @xmath252 , ( 3 ) @xmath251 and @xmath253 , and ( 4 ) @xmath254 , but the difference is a noninteger .      first , suppose that @xmath256 is an integer .",
    "a fast matrix - vector product for @xmath257 is almost immediate via the recurrence relation  @xcite and  ( * ? ? ?",
    "that is , assuming that @xmath258 we can factor @xmath11 as follows : @xmath259 & \\frac{\\alpha+\\beta+2}{\\alpha+\\beta+3}&-\\frac{\\alpha+2}{\\alpha+\\beta+5}\\\\[3pt ] & & \\frac{\\alpha+\\beta+3}{\\alpha+\\beta+5}&-\\frac{\\alpha+3}{\\alpha+\\beta+7}\\\\[3pt ] & & & \\ddots & \\ddots \\end{pmatrix},\\ ] ] where @xmath260 is the @xmath223 project matrix .",
    "the matrix - vector product @xmath257 can be computed in @xmath174 operations by applying each truncated sparse factor in turn . if @xmath261 , then since each @xmath262 is bidiagonal and upper - triangular @xmath257 can still be computed in @xmath174 operations by using backward substitution .      when @xmath251 there is no known _ sparse _",
    "factorization for the conversion matrix like in case 1 . however , the following explicit formula for its entries is known  @xcite : @xmath265 where @xmath266 .",
    "the entries of @xmath11 are zero otherwise .",
    "a careful inspection of this formula reveals that it can also be expressed as a diagonally - scaled toeplitz - dot - hankel matrix .",
    "moreover , the entries of the hankel matrix can be expressed as follows : @xmath267 proving that @xmath17 is real , symmetric , and positive semidefinite ( see lemma  [ lem : hamburger ] ) since @xmath252 and @xmath268 . a similar analysis to that in section  [ sec : hankellowrank ]",
    "shows that @xmath17 can be approximated , up to an error of @xmath36 , by a rank @xmath40 matrix .",
    "therefore , the @xmath51 complexity algorithm summarized in figure  [ fig : summary ] can be employed for jacobi conversion in this case .",
    "the jac2jac algorithm in chebfun and fasttransforms.jl implements this and the other three cases . in figure",
    "[ fig : jac2jacresults ] ( left ) we test the accuracy of our algorithm by using it to compute @xmath269 for various decay rates in the vector @xmath270 . in figure",
    "[ fig : jac2jacresults ] ( right ) we compare the execution time of this algorithm and a direct approach .",
    "we observe that our algorithm is faster in this case when @xmath271 .",
    "jac2jac_errors - eps - converted - to ( 50,0 ) @xmath1 ( 0,22 ) ( 45,57 ) ( 45,43 ) ( 45,31 )    jac2jac_timings - eps - converted - to ( 45,45 ) ( 60,30 ) ( 0,10 ) ( 50,0 ) @xmath1      this is a situation where the jacobi conversion matrix in @xmath257 can be written as a diagonally - scaled toeplitz - dot - hankel matrix ; however , the hankel part is not positive semidefinite ( see  ) .",
    "this is similar to what happens in section [ sec : cheb2leg ] .",
    "indeed , the submatrix @xmath198 is in fact positive semi - definite because @xmath273 .",
    "hence , we can do the same trick when we compute @xmath257 : apply the first row directly , and use the algorithm described in figure [ fig : summary ] on @xmath274 for the remaining entries .      if @xmath254 , then either @xmath276 or @xmath277 .",
    "if @xmath276 , then we convert the jacobi @xmath238 expansion to @xmath278 using case 1 , repeating if necessary until @xmath251 .",
    "similarly , if @xmath277 , then we convert the jacobi @xmath238 expansion to @xmath279 using case 1 . again , repeating until @xmath251 .",
    "thus , this case reduces the difference between @xmath280 and @xmath281 until the criterion for case @xmath93 or @xmath188 is applicable .",
    "the four cases above are implemented in the jac2jac commands in chebfun  @xcite and fasttransforms.jl  @xcite with the syntax jac2jac(v , a , b , g , d ) .",
    "based on the particular values of a , b , g , and d various cases above are exercised .",
    "for all parameter ranges the cost of the conversion is at most @xmath51 operations .",
    "this algorithm is also employed for the commands jac2cheb and cheb2jac by exploiting the fact that @xmath282  ( * ? ? ?",
    "* ( 18.7.3 ) ) .",
    "one can also compute the jacobi - to - chebyshev and chebyshev - to - jacobi conversions in @xmath175 operations using asymptotic expansions of jacobi polynomials  @xcite .",
    "we are not aware of major applications for laguerre - to - laguerre conversions , though related conversions are discussed in  @xcite . due to the simplicity of the conversion",
    ", we include it in this section .    for @xmath283 the generalized laguerre polynomial of degree @xmath6",
    "is given by @xmath284  ( * ? ? ?",
    "18.5.12 ) .",
    "the sequence @xmath285 forms a family of polynomials that are orthogonal with respect to the weight function @xmath286 on @xmath287 .",
    "suppose that @xmath288 and @xmath289 .",
    "then , there is an upper - triangular matrix , @xmath11 , that converts expansion coefficients in the @xmath290 basis to coefficients in the @xmath291 basis .",
    "if @xmath292 is an integer , then an @xmath174 complexity algorithm for computing the matrix - vector product is almost immediate thanks to the recurrence relation given in  ( * ? ? ?",
    "* ( 18.9.13 ) ) .",
    "if @xmath292 is not an integer , then there is an explicit formula for the entries of @xmath11 given by  ( * ? ? ?",
    "* ( 18.18.18 ) ) @xmath293 one observes that this conversion matrix is a diagonally - scaled toeplitz matrix , which is also a diagonally - scaled toeplitz - dot - hankel matrix by taking the hankel part as the matrix of all ones .",
    "a fast @xmath294 algorithm follows by a fast toeplitz matrix - vector product based on the fft  @xcite .",
    "many of the standard conversion matrices for converting between expansions coefficients in orthogonal polynomial basis can be written as a diagonally - scaled hadamard product between a toeplitz and hankel matrix .",
    "this leads to an @xmath51 complexity for basis conversion for a polynomial of degree @xmath1 .",
    "the resulting algorithm is conceptually simple , fft - based , and requires no precompution , while being competitive in terms of computational time with existing fast algorithms .",
    "we thank the school of mathematics and statistics at the university of sydney for awarding alex townsend a travel grant that allowed him to visit the school in february 2016 .",
    "we thank the cecil king foundation and the london mathematical society for awarding marcus webb the cecil king travel scholarship to visit the university of sydney from january to april 2016 .",
    "we thank laurent demanet and haihao lu for serendipitously discussing toeplitz - dot - hankel matrices with the first author a year earlier .",
    "we are grateful to bernhard beckermann for a discussion on the singular values of real , symmetric , and positive semidefinite hankel matrices .",
    "we also thank nick hale and mikal slevinsky for discussions on related topics over many years and providing excellent comments that improved this paper and our implementations of the algorithms .    3 , _ a fast algorithm for the evaluation of legendre expansions _ , siam j. sci .",
    ", 12 ( 1991 ) , pp .",
    "158179 . , _",
    "orthogonal polynomials and special functions _ , siam , 1975 . ,",
    "_ the condition number of real vandermonde , krylov and positive definite hankel matrices _ , numer .",
    ", 85 ( 2000 ) , pp .",
    ", _ julia : a fresh approach to numerical computing _ ,",
    "arxiv preprint arxiv:1411.1607 , ( 2014 ) . , _ on rapid computation of expansions in ultraspherical polynomials _",
    ", siam j.  numer .",
    ", 50 ( 2012 ) , pp .",
    "307327 . , _ harmonic analysis for engineers and applied scientists _",
    ", crc press , second edition , 2000 , _ the chebyshev ",
    "legendre method : implementing legendre methods on chebyshev points _",
    ", siam j.  numer .",
    ", 31 ( 1994 ) , pp .",
    "15191534 . , _ chebfun guide _",
    ", pafnuty publications , 2014 . ,",
    "_ the design and implementation of fftw3 _ , proc .",
    "ieee , 93 ( 2005 ) , pp .",
    "216231 . , _ matrix computations _ , 4th edition , johns hopkins university press , 2013 . ,",
    "_ implementing clenshaw - curtis quadrature , ii computing the cosine transformation _ ,",
    "acm , 15 ( 1972 ) , pp .",
    "343346 . , _ singular value bounds for the cauchy matrix and solutions of sylvester equations _ , technical report , university of kiel , 13 , 2001 .",
    ", _ strong rank revealing cholesky factorization _ , electr .",
    "anal . , 17 ( 2004 ) , pp .",
    "7692 . , _ a fast , simple , and stable chebyshev ",
    "legendre transform using an asymptotic formula _",
    ", siam j.  sci .",
    "comput . , 36 ( 2014 ) , a148a167 . , _ an algorithm for the convolution of legendre series _ , siam j. sci .",
    "comput . , 36 ( 2014 ) , a1207a1220 .",
    ", _ a fast fft - based discrete legendre transform _ , to appear in i m a numer .",
    ", _ on the low - rank approximation by the pivoted cholesky decomposition _ ,",
    ", 62 ( 2012 ) , pp .",
    ", _ analysis of the cholesky decomposition of a semi - definite matrix _ , in reliable numerical computation , 1990 .",
    ", _ a fast and simple algorithm for the computation of legendre coefficients _ , numer",
    ", 117 ( 2011 ) , pp .  529553 . , _ computing with expansions in gegenbauer polynomials _ , siam j. sci .",
    "comput . , 31 ( 2009 ) ,",
    "21512171 . , _ fast polynomial transforms _ , logos verlag berlin gmbh , 2011 . ,",
    "_ connection coefficients between orthogonal polynomials and the canonical sequence : an approach based on symbolic computation _ , numer .",
    ", 47 ( 2008 ) , pp .",
    "291314 . , _ chebyshev polynomials _ , taylor & francis , ( 2002 ) . ,",
    "_ equalities and inequalities for ranks of matrices _ , lin .",
    "alg . , 2 ( 1974 ) , pp .",
    "269292 . , _ an improvement on orszag s fast algorithm for legendre polynomial",
    "transform _ , trans .",
    "processing soc .",
    "japan , 40 ( 1999 ) , pp .",
    "36123615 . , _ iterative methods for toeplitz systems _ , oxford university press , 2004 .",
    ", r. m. slevinsky , et al .",
    ", https://github.com/approxfun/approxfun.jl , v0.1.0 , 2016 .",
    ", _ nist handbook of mathematical functions _ , cambridge university press , 2010 .",
    ", _ a fast and well - conditioned spectral method _ , siam review , 55 ( 2013 ) , pp .",
    "462489 . ,",
    "_ fast eigenfunction transforms _ , science and computers , academic press , new york , ( 1986 ) , pp .",
    "2330 . , _ hankel operators and their applications _ ,",
    "springer , 2012 .",
    ", _ fast algorithms for discrete polynomial transforms _ , math .  comp .",
    ", 67 ( 1998 ) , pp .",
    ", _ efficient spectral - galerkin method i. direct solvers of second- and fourth - order equations using legendre polynomials _ , siam j.  sci .  comput . , 15 ( 1994 ) , pp .",
    "14891505 . ,",
    "_ fast structured jacobi - jacobi transforms _ , preprint , 2016 . ,",
    "_ on the use of hahn s asymptotic formula and stabilized recurrence for a fast , simple , and stable chebyshev ",
    "jacobi transform _",
    ", arxiv preprint arxiv:1602.02618 , 2016 .",
    ", s. olver , et al .",
    "https://github.com/approxfun/approxfun.jl , v0.0.4 , 2016 .",
    ", _ continuous analogues of matrix factorizations _ , proc .",
    "a. , 471 ( 2015 ) . , _ approximation theory and approximation practice _ , siam , 2013 .",
    ", _ the asymptotic expansion of a ratio of gamma functions",
    "_ , pacific j. math .",
    ", 1 ( 1951 ) , pp .",
    "133142 . , _ fast and accurate computation of jacobi expansion coefficients of analytic functions _ , submitted , ( 2014 ) .",
    ", _ a fast symmetric svd algorithm for square hankel matrices _ , lin .",
    "appl . , 428 ( 2008 ) , pp ."
  ],
  "abstract_text": [
    "<S> many standard conversion matrices between coefficients in classical orthogonal polynomial expansions can be decomposed using diagonally - scaled hadamard products involving toeplitz and hankel matrices . </S>",
    "<S> this allows us to derive @xmath0 algorithms , based on the fast fourier transform , for converting coefficients of a degree @xmath1 polynomial in one polynomial basis to coefficients in another . </S>",
    "<S> numerical results show that this approach is competitive with state - of - the - art techniques , requires no precomputational cost , can be implemented in a handful of lines of code , and is easily adapted to extended precision arithmetic .    </S>",
    "<S> conversion matrix , toeplitz , hankel , hadamard product    65t50 , 65d05 , 15b05 </S>"
  ]
}