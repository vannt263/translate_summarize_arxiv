{
  "article_text": [
    "take a look at the image in figure  [ subfig1:tagit ] .",
    "might it be a flower petal , or a piece of fruit , or perhaps even an octopus tentacle ?",
    "the image on its own is ambiguous .",
    "take another look , but this time consider that the images in figure  [ subfig2:tagit ] share social - network metadata with figure  [ subfig1:tagit ] . now",
    "the answer is clear : all of these images show flowers .",
    "the context of additional unannotated images disambiguates the visual classification task .",
    "we build on this intuition , showing improvements in multilabel image annotation by exploiting image metadata to augment each image with a _ neighborhood _ of related images .",
    "most images on the web carry metadata ; the idea of using it to improve visual classification is not new .",
    "prior work takes advantage of user tags for image classification and retrieval @xcite , uses gps data @xcite to improve image classification , and utilizes timestamps @xcite to both improve recognition and study topical evolution over time .",
    "the motivation behind much of this work is the notion that images with similar metadata tend to depict similar scenes .",
    "one class of image metadata where this notion is particularly relevant is _ social - network metadata _ , which can be harvested for images embedded in social networks such as flickr .",
    "these metadata , such as user - generated tags and community - curated groups to which an image belongs , are applied to images by people as a means to communicate with other people ; as such , they can be highly informative as to the semantic contents of images .",
    "mcauley and leskovec  @xcite pioneered the study of multilabel image annotation using metadata , and demonstrated impressive results using only metadata and no visual features whatsoever .",
    "[ fig : pullfig ]    despite its significance , the applicability of mcauley and leskovec s method to real - world scenarios is limited due to the parametric method by which image metadata is modeled . in practice ,",
    "the vocabulary of metadata may shift over time : new tags may become popular , new image groups may be created , etc .",
    "an ideal method should be able to handle such changes , but their method assumes identical vocabularies during training and testing .    in this paper",
    "we revisit the problem of multilabel image annotation , taking advantage of both metadata and strong visual models .",
    "our key technical contribution is to generate _ neighborhoods _ of images ( as in figure  [ fig : pullfig ] ) nonparametrically using image metadata , then to operate on these neighborhoods with a novel parametric model that learns the degree to which visual information from an image and its neighbors should be trusted .",
    "in addition to giving state - of - the - art performance on multilabel image annotation ( section  [ sec : classification ] ) , this approach allows our model to perform tasks that are difficult or impossible using existing methods .",
    "specifically , we show that our model can do the following :    * * handle different types of metadata .",
    "* we show that the same model can give state - of - the - art performance using three different types of metadata ( image tags , image sets , and image groups ) .",
    "we also show that our model gives strong results when different metadata are available at training time and testing time .",
    "* * adapt to changing vocabularies . *",
    "our nonparametric approach to handling metadata allows our model to handle different vocabularies at train and test time .",
    "we show that our model gives strong performance even when the training and testing vocabulary of user tags are completely disjoint .",
    "[ [ automatic - image - annotation - and - image - search . ] ] automatic image annotation and image search .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our work falls in the broad area of image annotation and search @xcite .",
    "harvesting images from the web to train visual classifiers without human annotation is an idea that have been explored many times in the past decade @xcite",
    ". early work on image annotation used voting to transfer labels between visually similar images , often using simple nonparametric models @xcite .",
    "this strategy is well suited for multimodal data and large vocabularies of weak labels , but is very sensitive to the metric used to find visual neighbors .",
    "extensions use learnable metrics and weighted voting schemes @xcite , or more carefully select the training images used for voting  @xcite .",
    "our method differs from this work because we do not transfer labels from the training set ; instead we compute nearest - neighbors between _ test - set _ images using metadata .",
    "these approaches have shown good results , but are limited because they treat tags and visual features separately , and may be biased towards common labels .",
    "some authors instead tackle multilabel image annotation by learning parametric models over visual features that can make predictions @xcite or rank tags @xcite .",
    "gong @xcite recently showed state of the art results on nus - wide @xcite using cnns with multilabel ranking losses .",
    "these methods typically do not take advantage of image metadata .",
    "[ [ multimodal - representation - learning - images - and - tags . ] ] multimodal representation learning : images and tags .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a common approach for utilizing image metadata is to learn a joint representation of image and tags . to this end ,",
    "prior work generatively models the association between visual data and tags or labels @xcite or applies non - negative matrix factorization to model this latent structure @xcite .",
    "similarly , niu @xcite encode the text tags as relations among the images , and define a semi - supervised relational topic model for image classification .",
    "another popular approach maps images and tags to a common semantic space , using cca or kcca @xcite .",
    "this line of work is closely related to our task , however these approaches only model user tags and assume static vocabularies ; in contrast we show that our model can generalize to new types of metadata .",
    "[ [ beyond - images - and - tags . ] ] beyond images and tags .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    besides user tags , previous work uses gps and timestamps @xcite to improve classification performance in specific tasks such as landmark classification .",
    "some authors model the relations between images using multiple metadata @xcite .",
    "duan @xcite present a latent crf model in which tags , visual features and gps - tags are used jointly for image clustering .",
    "mcauley and leskovec model pairwise social relations between images and then apply a structural learning approach for image classification and labeling @xcite .",
    "they use this model to analyze the utility of different types of metadata for image labeling .",
    "our work is similarly motivated , but their method does not use any visual representation .",
    "in contrast , we use a deep neural network to blend the visual information of images that share similar metadata .",
    "we design a system that incorporates both visual features of images and the neighborhoods in which they are embedded .",
    "an ideal system should be able to handle different types of signals , and should be able to generalize to new types of image metadata and adapt to their changes over time ( e.g. users add new tags or add images to photo - sets ) . to this end",
    "we use metadata nonparametrically to generate image neighborhoods , then operate on images together with their neighborhoods using a parametric model .",
    "the entire model is summarized in figure  [ fig : graph_neighbor_model ] .",
    "let @xmath0 be a set of images , @xmath1 a set of possible labels , and @xmath2 a dataset associating each image with a set of labels .",
    "let @xmath3 be a set of possible neighborhoods for images ; in our case a neighborhood is a set of related images , so @xmath3 is the power set @xmath4 .",
    "we use metadata to associate images with neighborhoods .",
    "a simple approach would assign each image @xmath5 to a single neighborhood @xmath6 ; however there may be more than one useful neighborhood for each image . as such",
    ", we instead use image metadata to generate a set of _ candidate neighborhoods _ @xmath7 for each image @xmath8 .    at training time , each element of @xmath9 is a set of training images , and is computed using training image metadata . at test time , test image metadata is used to build @xmath9 from test images ; note that we do not use the training set at test time .        for an image @xmath5 and neighborhood @xmath10",
    ", we use a function @xmath11 parameterized by weights @xmath12 to predict label scores @xmath13 for the image @xmath8 .",
    "we average these scores over all candidate neighborhoods for @xmath8 , giving @xmath14 to train the model , we choose a loss @xmath15 and optimize : @xmath16 the set @xmath9 may be large , so for computational efficiency we approximate @xmath17 by sampling from @xmath9 . during training , we draw a single sample during each forward pass and at test time we use ten samples .",
    "we generate candidate neighborhoods using a nearest - neighbor approach .",
    "we use image metadata to compute a distance between each pair of images .",
    "we fix a _ neighborhood size _ @xmath18 and a _ max rank _ @xmath19 ; the candidate neighborhoods @xmath9 for an image @xmath8 then consist of all subsets of size @xmath20 of the @xmath21-nearest neighbors to @xmath8 .    the types of image metadata that we consider are user tags , image photo - sets , and image groups . sets are galleries of images collected by the same user ( e.g. pictures from the same event such as a wedding ) .",
    "image groups are community - curated ; images belonging to the same concept , scene or event are uploaded by the social network users .",
    "each type of metadata has a vocabulary @xmath22 of possible values , and associates each image @xmath5 with a subset @xmath23 of values . for tags",
    ", @xmath22 is the set of all possible user tags and @xmath24 are the tags for image @xmath8 ; for groups ( and sets ) , @xmath22 is the set of all groups ( sets ) , and @xmath24 are the groups ( sets ) to which @xmath8 belongs . for sets and groups , we use the entire vocabulary @xmath22 ; in the case of tags we follow @xcite and select only the @xmath25 most frequently occurring tags on the training set .",
    "we compute the distance between images using the jaccard similarity between their image metadata . concretely , for @xmath26 we compute    @xmath27 to prevent an image from appearing in its own neighborhoods , we set @xmath28 for all @xmath5 .    generating candidate neighborhoods introduces several hyperparameters , namely the neighborhood size @xmath20 , the max rank @xmath21 , the type of metadata used to compute distances , and the tag vocabulary size @xmath25 .",
    "we show in section  [ sec : hyperparameters ] that the type of metadata is the only hyperparameter that significantly affects our performance .          given an image @xmath5 and a neighborhood @xmath29",
    ", we design a model that incorporates visual information from both the image and its neighborhood in order to make predictions for the image .",
    "our model is essentially a fully - connected two layer neural network applied to features from the image and its neighborhood , except that we pool over the hidden states for the neighborhood images .",
    "we use a cnn @xcite @xmath30 to extract @xmath31-dimensional features from the images @xmath8 and @xmath32 .",
    "we compute an @xmath33-dimensional hidden state for each image by applying an affine transform and an elementwise relu nonlinearity @xmath34 to its features . to let the model treat hidden states for the image and its neighborhood differently , we apply distinct transforms to @xmath35 and @xmath36 , parameterized by @xmath37 and @xmath38 . at this point",
    "we have hidden states @xmath39 for @xmath8 and each @xmath40 ; to generate a single hidden state @xmath41 for the neighborhood @xmath42 we pool each @xmath43 elementwise so that @xmath44 . finally to compute label scores",
    "@xmath13 we concatenate @xmath45 and @xmath46 and pass them through a third affine transform parameterized by @xmath47 . to summarize :    @xmath48 the learnable parameters are @xmath49 , @xmath50 , @xmath51 , @xmath52 , @xmath53 , and @xmath54 .",
    "an example of a learned matrix @xmath53 is visualized in figure  [ fig : weights ] .",
    "the left and right sides multiply the hidden states for the image and its neighborhood respectively .",
    "both sides contain many nonzero weights , indicating that the model learns to use information from both the image and its neighborhood ; however the darker coloration on the left suggests that information from the image is weighted more heavily .",
    "we can follow this idea further , and use equation  [ eq : scores ] to compute for each image the portion of its score for each label that is due to the hidden state of the image @xmath45 and its neighborhood @xmath46 .",
    "the left side of figure  [ fig : weights ] shows examples of correctly labeled images whose scores are more due to the image , while the right shows images more influenced by their neighborhoods .",
    "the former show canonical views ( such as a bride and groom for _ wedding _ ) while the latter are more non - canonical ( such as a _ zebra _ crossing a road ) .",
    "we apply @xmath55 regularization to the matrices @xmath56 and @xmath53 and apply dropout @xcite with @xmath57 to the hidden layers @xmath58 and @xmath59 .",
    "we initialize all parameters using the method of @xcite and optimize using stochastic gradient descent with a fixed learning rate , rmsprop @xcite , and a minibatch size of 50 .",
    "we train all models for 10 epochs , keeping the model snapshot that performs the best on the validation set . for all experiments we use a learning rate of @xmath60 , @xmath55 regularization strength @xmath61 and hidden dimension @xmath62 ; these values were chosen using grid search .",
    "our image feature function @xmath30 returns the activations of the last fully - connected layer of the blvc reference caffenet @xcite , which is similar to the network architecture of @xcite .",
    "we ran preliminary experiments using features from the model of vgg @xcite , but this did not significantly change the performance of our model . for all models our loss function @xmath15 is a sum of independent one - vs - all logistic classifiers .",
    "in all experiments we use the nus - wide dataset  @xcite , which has been widely used for image labeling and retrieval .",
    "it consists of 269,648 images collected from flickr , each manually annotated for the presence or absence of 81 labels .",
    "following @xcite we augment the images with metadata using the flickr api , discarding images for which metadata is unavailable . following",
    "@xcite we also discard images for which all labels are absent .",
    "this leaves 190,253 images , which we randomly partition into training , validation , and test sets of 110k , 40k , and 40,253 images respectively .",
    "we generate 5 such splits of the data and run all experiments on all splits .",
    "statistics of the dataset can be found in table  [ tab : nuswide ] .",
    "we will make our data and features publicly available to facilitate future comparisons .",
    "+         [ tab : baselines ]      prior work uses a variety of metrics and experimental setups on nus - wide , making direct comparisons of results difficult . following prior work @xcite we assign a fixed number of labels to each image and report ( overall ) precision @xmath63 and recall @xmath64 ; we also compute the precision and recall for each label and report the mean across labels as the _ per - label _",
    "metrics @xmath65 .",
    "nus - wide has a highly uneven distribution of labels ; the most common ( _ sky _ ) has over 68,000 examples and the least common ( _ map _ ) has only 53 . as a result",
    "the overall precision and recall statistics are strongly biased towards the common labels .",
    "the precision and recall for uncommon labels are extremely noisy since they are based on only a handful of test - set examples , and the mean per - label statistics inherit this noise since they weight all classes equally .",
    "mean average precision ( map ) is another widely used metric  @xcite ; it directly measures ranking quality , so it naturally handles multiple labels and does not require choosing a fixed number of labels per image . as with other metrics , we report map both per - label ( map@xmath66 ) and per - image ( map@xmath67 ) .",
    "map@xmath66 is less noisy and hence preferable to other per - label metrics since it considers the full ranking of images instead of only the top labels for each image .",
    "we show that our model achieves state - of - the art results for multilabel image annotation on nus - wide .",
    "our best model computes neighborhoods using tags with a vocabulary size of @xmath68 , neighborhood size @xmath69 and max - rank @xmath70 .",
    "preliminary experiments at combining all types of metadata did not show improvements over using tags alone .",
    "we also show the result of augmenting the hidden state of our model with a binary indicator vector of image tags .",
    "all results are shown in table  [ tab : baselines ] .    [",
    "[ baselines . ] ] baselines .",
    "+ + + + + + + + + +    first we report the results of mcauley and leskovec  @xcite and gong  @xcite as in their original papers .",
    "then we compare our model with four baselines :    \\1 .",
    "tag - only + logistic : the tag - only model of @xcite represents each image with a sparse binary vector indicating its tags , while their full model uses all available metadata ( tags , groups , galleries , and sets ) and incorporates a graphical model to model pairwise interactions between these features .",
    "unfortunately these results are not directly comparable to ours , since they do not discard images without ground - truth labels ; as a result they use 244k images for their experiments while we use only 190k .",
    "we reimplement a version of their tag - only model by training one - vs - all logistic classifiers on top of binary tag indicator features .",
    "our reimplementation performs slightly worse than their reported numbers due to the difference in dataset size .",
    "cnn + logistic loss : the results of @xcite have been obtained using a deep convolutional neural networks in the style of @xcite equipped with various multilabel loss functions .",
    "again , these results are not directly comparable to ours because they train their networks from scratch on the nus - wide dataset , while we use networks that were pretrained on imagenet @xcite .",
    "we reimplement a version of their model by training one - vs - all logistic classifiers using the features extracted from our pretrained network .",
    "this is an extremely strong baseline ; note that it already outperforms @xcite , highlighting the power of the pretrained network .",
    "cnn + knn voting : as an additional baseline we implement a simple nearest neighbor approach . for each test image",
    "we compute the @xmath55 distance between its cnn features and the features of all images in the training set ; the ground - truth labels of the retrieved training images are then used in a voting scheme similar to @xcite .",
    "image neighborhoods + cnn - voting : for each test image we compute its @xmath21-nearest neighbors on the test set using user tags as in our full model , but instead of passing these neighbors to our parametric model we apply the cnn+logistic visual - only model to the image and its neighbors .",
    "then we set the label scores of the test image to be a weighted sum of its visual - only label scores and the mean of the visual - only label scores of its neighbors .     +    [",
    "fig : pr_class_aps ]    [ [ upper - bound . ] ] upper bound .",
    "+ + + + + + + + + + + +    as discussed in section  [ metrics ] , we assign the top @xmath71 labels to each image and report precision both per - class and per - image ( recall that the average number of labels per image is approximately @xmath72 ) .",
    "however many images do not have exactly 3 ground - truth labels ; this means that no classifier can achieve unit precision and recall . to estimate upper bounds for these metrics , we train one - vs - all logistic classifiers where each image its represented by a binary indicator vector encoding its ground - truth labels .",
    "as seen in table  [ tab : baselines ] , even this perfect classifier achieves far from perfect performance on many of the evaluation metrics .",
    "[ [ results . ] ] results .",
    "+ + + + + + + +    table  [ tab : baselines ] shows that our model outperforms prior work on nearly all metrics . the per - class precision and recall metrics display high variance ; as a result",
    "we do not believe them to be the best indicators of performance .",
    "the map metrics give a clearer picture of performance , since they display lower variance and do not rely on annotating each image with a fixed number of labels . on these metrics",
    "our model outperforms all baselines by a significant margin .    as an extension ,",
    "we append the binary tag vector to the representation learned by our model ( tag neighbors + tag vector ) ; this does not significantly change performance as measured by per - image metrics , but does show improvement on per - class metrics .",
    "this suggests that the binary tag vector is especially useful for rare classes which may have strong correlations with certain user tags .",
    "although it increases per - class performance , this extension significantly increases the number of learnable parameters and makes generalization to new types of metadata impossible . in order to qualitatively understand some of the cases where our model outperforms the baselines , figure  [ fig : labeling - examples ] compares the top three labels produced by our model and by the visual - only baseline .",
    "the additional visual information provided by the neighborhoods can help resolve ambiguities in non - canonical views ; for example in the image of swimmers the visual - only model appears to mistake the colorful swim caps for flowers , but the neighborhood provides canonical views of swimmers .    in few cases the neighborhood can hurt performance .",
    "for example in the image of the boy with a dog , the visual - only model correctly produces a _ dog _ label but our model replaces this with a _ water _ label , likely because no neighbors contain dogs but two neighbors contain visible bodies of water .",
    "however the aggregate metrics of table  [ tab : baselines ] make it clear that neighborhoods are beneficial more often than not .",
    "there are cases where both models fail ; for example see the lower right image of figure  [ fig : labeling - examples ] which shows a person crouching inside a statue of a rabbit .",
    "the ground - truth labels for this challenging image are _ statue _ and _ person _ , which are produced by neither model .",
    "th nearest neighbor of an image has a particular label given that the image has the label , as a function of @xmath73 and using different metadata .",
    "the dashed lines give the overall probability that an image has the label . across all metadata and all classes , an image and",
    "its neighbors are likely to share labels . ]",
    "more quantitatively , figure  [ subfig2:aps ] compares the average precision ( ap ) of both our model and the visual - only baseline for each label ; our model outperforms the baseline on all but three labels : _ map _ , _ earthquake _ , and _",
    "rainbow_. of these , _ map _ is the only label where our model is significantly outperformed by the baseline .",
    "figure  [ subfig2:aps ] also reveals that these three labels are among the most infrequent ; they have only 53 , 56 , and 397 instances respectively in the entire dataset , and an average of only 12.8 , 13.2 , and 82.0 instances respectively on the test sets .",
    "with so few test instances the performance of both models on these labels is highly susceptible to noise .",
    "it is also interesting to note that the middle frequencies are the ones in which our model gives the major boost in performance , while for the very frequent labels it is still able to give slight improvements .",
    "figure  [ subfig1:aps ] also shows two example precision - recall curves .",
    "the _ wedding _ label has high intra - class variability , making it difficult to recognize using visual features alone ; our model is able to give a large boost in performance by taking advantage of image metadata .",
    "our model also gives improvements on labels such as _ food _ where the performance of the visual - only baseline is already quite strong .",
    "our method for generating image neighborhoods introduces several hyperparameters : the type of metadata used , the size @xmath20 of each neighborhood , the max - rank @xmath21 for neighbors , and the tag - vocabulary size @xmath25 . here",
    "we explore the influence of these hyperparameters on our model .",
    ", max - rank @xmath21 , and tag vocabulary size @xmath25 . in all cases",
    "our model outperforms the baselines.,title=\"fig : \" ] , max - rank @xmath21 , and tag vocabulary size @xmath25 . in all cases",
    "our model outperforms the baselines.,title=\"fig : \" ]    [ [ sec : hyperparameters ] ] effects on performance .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    the most important hyperparameter for generating neighborhoods is the type of data used . we show in table  [ tab : our_models ] the performance of our model using different types of metadata : tags give the highest performance , followed by groups and then sets . in all cases",
    "our model outperforms the visual - only baseline .",
    "we also show the effect of using euclidean distance of visual features to build neighborhoods ( visual neighbors ) .",
    "this setup slightly outperforms the visual - only baseline but is outperformed when using metadata , showing both the ability of our method to handle a variety of neighbor types , and the importance of image metadata .    to study the effects of the neighborhood size @xmath20 , the max - rank @xmath21 , and the tag vocabulary size @xmath25 we show in figure  [ fig : neighborhood_params ] the performance of our model as we vary these hyperparameters .",
    "varying the max - rank @xmath21 gives the largest variation in performance , but in all cases we show improvements over the visual - only baseline and the results of @xcite .",
    "[ [ label - correlations . ] ] label correlations .",
    "+ + + + + + + + + + + + + + + + + + +    we can better interpret the influence of neighborhood hyperparameters by studying the correlations between the labels of images and their nearest neighbors .",
    "with strong correlations , visual evidence for a label among an image s neighbors is evidence that the image should have the same label ; as such , our model should perform better when these correlations are stronger .    to this end , we plot in figure  [ fig : prob_vs_k ] the probability that the @xmath73th nearest neighbor of an image has a particular label given that the image itself has the label ; on the same axis we show the baseline probability that a random image in the dataset has the label .",
    "this experiment shows that the nearest neighbors of images are indeed very likely to share labels with an image , and helps to explain the influence of various hyperparameters .",
    "an image s labels are most highly correlated with its tag neighbors , followed by groups and then sets ; this matches the results of table  [ tab : our_models ] .",
    "the flat shape of all curves in figure  [ fig : prob_vs_k ] suggests that the 20th nearest neighbor is nearly as informative as the 10th , suggesting that larger max - ranks @xmath21 may increase performance .",
    "one advantage of our model is that we only use metadata of images nonparametrically as a means to compute image neighborhoods . as a result ,",
    "our model can easily cope with situations where different types of metadata are available during training and testing .",
    "[ [ vocabulary - generalization . ] ] vocabulary generalization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    our best - performing model relies on user tags to generate image neighborhoods . in a real - world setting ,",
    "the vocabulary of user tags may change over time : new tags may become popular , and older tags may fall into disuse .",
    "any method that depends on user tags should be able to cope with these challenges .    ideally , to test our model s resilience to changes in user tags over time , we would train the model using a snapshot of flickr images at one point in time and test the model using a snapshot from a different point in time .",
    "unfortunately we do not have access to this type of data . as a proxy to such an experiment , we instead randomly divide the 10k most commonly occurring user tags into two sets . during training we use the first set of user tags to generate neighborhoods , and use the second during testing .",
    "we vary the degree to which the training tags and the testing tags overlap ; with an overlap of 0% there are no tags shared between training and testing , and an overlap of 100% uses the same vocabulary of user tags for training and testing .",
    "results are shown in figure  [ fig : overlap ] .",
    "we see that the performance of our model degrades as we decrease the overlap between the training and testing tags ; however even in the case of 0% overlap our model is able to outperform both the visual - only model and @xcite .",
    "[ [ metadata - generalization . ] ] metadata generalization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    as a test of our model s ability to generalize across different types of metadata , we perform an experiment where we use different types of metadata during training and testing .",
    "for example , we generate neighborhoods with tags during training and instead use sets during testing .",
    "table  [ tab : crosssig ] shows the quantitative results of this experiment ; in all cases our model outperforms the visual - only baseline .",
    "these results suggest that our model could be applied in cases where some types of metadata are unavailable during testing .",
    "we can explain the results of this experiment by again examining figure  [ fig : prob_vs_k ] .",
    "when we train using one signal and test using another , our train and test data are no longer drawn from the same distribution , breaking one of the core assumptions of supervised learning .",
    "however the parametric portion of our model only views image metadata through the lens of nearest neighbors ; figure  [ fig : prob_vs_k ] shows that changing the method of computing these neighbors does not drastically change the nature of the correlations between the labels of an image and its neighbors .",
    "[ tab : crosssig ]",
    "we have introduced a framework that exploits image metadata to generate neighborhoods of images , and uses a strong parametric visual model based on deep convolutional neural networks to blend visual information between an image and its neighbors .",
    "we use our model to achieve state - of - the - art performance for multilabel image annotation on the nus - wide dataset .",
    "we also show that our model gives impressive results even when it is forced to generalize to new types of metadata at test time .",
    "we thank j.  leskovec , j.  krause and o.  russakovsky for helpful comments and discussions .",
    "j.  johnson is supported by a magic grant from the brown institute for media innovation and l.  ballan is supported by a marie curie fellowship from the eu ( 623930 ) .",
    "we gratefully acknowledge the support of nvidia for their donation of gpus and yahoo for their donation of cluster machines used in this research ."
  ],
  "abstract_text": [
    "<S> some images that are difficult to recognize on their own may become more clear in the context of a _ neighborhood _ of related images with similar social - network metadata . </S>",
    "<S> we build on this intuition to improve multilabel image annotation . </S>",
    "<S> our model uses image metadata nonparametrically to generate neighborhoods of related images using jaccard similarities , then uses a deep neural network to blend visual information from the image and its neighbors . </S>",
    "<S> prior work typically models image metadata parametrically ; in contrast , our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing . </S>",
    "<S> we perform comprehensive experiments on the nus - wide dataset , where we show that our model outperforms state - of - the - art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata . </S>"
  ]
}