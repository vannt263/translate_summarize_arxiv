{
  "article_text": [
    "pattern recognition algorithms often involve testing the significance of a statistical dependency between two binary variables @xmath2 and @xmath3 , given the observed counts @xmath4 , @xmath5 , @xmath6 , and @xmath7 .",
    "if the test is done only a couple of times , the computation time is not crucial , but in an exhaustive search the test may repeated thousands or even millions of times .    for example , in data mining a classical problem is to search for the most significant classification rules of the form @xmath8 or @xmath9 , where @xmath2 is a set of binary attributes and @xmath3 is a binary class attribute .",
    "this problem is known to be @xmath10-hard with common significance measures like the @xmath0-measure @xcite and no polynomial time solutions are known .",
    "even a more complex problem is to search for all sufficiently significant dependency rules , where the consequence attribute is not fixed ( see e.g.  @xcite ) . in both problems",
    "the number of all tested patterns can be exponential and therefore each rule should be tested as fast as possible , preferrably in a constant time .",
    "the problem is that typically the mined data sets are very large ( the number of attributes can be tens of thousands and the number of rows millions ) .",
    "still , the most significant ( non - trivial ) dependency rules may be relatively infrequent and the corresponding distributions too skewed for fast but inaccurate asymptotic tests .",
    "for accurate results , one should test the significance of dependency @xmath8 with fisher s exact test , which evaluates the exact probability of the observed or a stronger dependency in the given data , if @xmath2 and @xmath3 were actually independent . for a positive dependency between @xmath2 and @xmath3 the probability ( @xmath1-value ) is defined by the cumulative hypergeometric distribution    @xmath11    where @xmath12 is the absolute frequency of set @xmath13 , @xmath14 is the data size ( number of rows ) , and @xmath15 .",
    "( for a negative dependency between @xmath2 and @xmath3 it suffices to replace @xmath3 by @xmath16 . )",
    "however , fisher s exact test is computationally demanding , when the data size @xmath14 is large . in each test",
    "one should evaluate @xmath17 terms , which means that the worst case time complexity is @xmath18 .",
    "for example , if @xmath19 and @xmath20 , we should evaluate 200 001 terms .",
    "in addition , each term involves binomial factors , but they can be evaluated in a constant time , if all factorials @xmath21 , @xmath22 have been tabulated in the beginning .    a common solution is to estimate the @xmath1-values from the @xmath0-measure , when the data size is moderate or large .",
    "the justification is that asymptotically ( when @xmath14 approaches @xmath23 ) @xmath24 can be approximated by the @xmath0-based @xmath1-values .",
    "however , in finite data sets the approximations are often quite inaccurate .",
    "the reason is that the @xmath0-measure is very sensitive to the data distribution . if the exact hypergeometric distribution is symmetric , the @xmath0-measure works well , but the more skewed the distribution is , the more inaccurate the approximated @xmath1-values are @xcite .",
    "classically @xcite , it is recommended that the @xmath0-approximation should not be used , if any of the expected counts ( under the assumption of independence between @xmath2 and @xmath3 ) @xmath25 , @xmath26 , @xmath27 , or @xmath28 is less than 5 .",
    "however , this rule of thumb prevents only the most extreme cases , which would lead to false discoveries . if the problem is to search for e.g.  the best 100 dependency rules , the @xmath0-measure produces quite different results than @xmath24 , even if all expected counts and the data size are large .    in this paper",
    ", we introduce better approximations for the exact @xmath24-values , which still can be calculated in a constant time .",
    "the approximations are upper bounds for the @xmath24 ( when the @xmath0-based values are usually lower bounds , i.e.  better than true values ) , but when the dependency is sufficiently strong , they give tight approximates to the exact values . in practice , they give identical results with the exact @xmath24-values , when used for rule ranking .",
    "the idea of the approximations is to calculate only the first or a couple of first terms from @xmath24 exactly and estimate an upper bound for the rest terms .",
    "the simplest upper bound evaluates only the first term exactly .",
    "it is also intuitively the most appealing as a goodness measure , because it is reminiscent to the existing dependency measures like the odds ratio . when the dependencies are sufficiently strong ( a typical data mining application ) , the results are also highly accurate .",
    "however , if the data set contains only weak and relatively insignificant dependencies , the simplest upper bound may produce too inaccurate results . in this case",
    ", we can use the tighter upper bounds , one of which can be adjusted to arbitrary accurate .",
    "however , the more accurate @xmath1-values we want to get , the more terms we have to calculate exactly .",
    "fortunately , the largest terms of @xmath24 are always the first ones , and in practice it is sufficient to calculate only a small fraction ( typically 210 ) of them exactly .    the rest of the paper is organized as follows . in section [ sec2 ]",
    "we introduce the upper bounds and give error bounds for the approximations . in section [ seceval ]",
    "we evaluate the upper bounds experimentally , concentrating on the weak ( and potentially the most problematic ) dependencies .",
    "the final conclusions are drawn in section [ secconcl ] .",
    "the following theorem gives two useful upper bounds , which can be used to approximate fisher s @xmath24 .",
    "the first upper bound is more accurate , but it contains an exponent , which makes it more difficult to evaluate .",
    "the latter upper bound is always easy to evaluate and also intuitively appealing .    [ ubtheorem ] let us notate @xmath29 and @xmath30 , @xmath31 . for positive dependency rule @xmath8 with lift @xmath32 @xmath33    each @xmath34 can be expressed as @xmath35 , where @xmath36 is constant .",
    "therefore , it is enough to show the result for @xmath37 .",
    "@xmath38 , where    @xmath39 since @xmath40 decreases when @xmath41 increases , the largest value is @xmath42 .",
    "we get an upper bound    @xmath43    the sum of geometric series is @xmath44 , which is the first upper bound . on the other hand , @xmath45 let us insert @xmath46 , and express the frequencies using lift @xmath47 . for simplicity , we use notations @xmath48 and @xmath49 .",
    "now @xmath50 , @xmath51 , @xmath52 and @xmath53 .",
    "we get    @xmath54    the nominator is @xmath55 , because @xmath56 @xmath57 .",
    "therefore @xmath58    in the following , we will denote the looser ( simpler ) upper bound by @xmath59 and the tighter upper bound ( sum of the geometric series ) by @xmath60 . in @xmath59 ,",
    "the first term of @xmath24 is always exact and the rest are approximated , while in @xmath60 , the first two terms are always exact and the rest are approximated .",
    "we note that @xmath59 can be expressed equivalently as @xmath61 where @xmath62 is the leverage .",
    "this is expression is closely related to the odds ratio @xmath63 which is often used to measure the strength of the dependency .",
    "the odds ratio can be expressed equivalently as @xmath64 we see that when the odds ratio increases ( dependency becomes stronger ) , the upper bound decreases . in practice , it gives a tight approximation to fisher s @xmath24 , when the dependency is sufficiently strong .",
    "the error is difficult to bind tightly , but the following theorem gives a loose upper bound for the error , when @xmath60 is used for approximation .",
    "[ ub2bound ] when @xmath24 is approximated by @xmath60 , the error is bounded by @xmath65    upper bound @xmath60 can cause error only , if @xmath66 . if @xmath67 , @xmath68 and if @xmath69 , @xmath70 .",
    "let us now assume that @xmath66 .",
    "the error is @xmath71 .",
    "it has an upper bound @xmath72    this leads to the following corollary , which gives good guarantees for the safe use of @xmath60 :    if @xmath73 , then @xmath74 .    according to theorem [ ub2bound ] , @xmath75 , if @xmath76",
    "this is true , when @xmath77 .    on the other hand , @xmath78 , when @xmath79 , because @xmath80    this is @xmath81 , because @xmath82 .",
    "a sufficient condition for @xmath77 is that @xmath83    this result also means that @xmath84 , when the lift is as large as required .",
    "the simpler upper bound , @xmath59 , can cause a somewhat larger error than @xmath60 , but it is even harder to analyze .",
    "however , we note that @xmath85 only , when @xmath67 . when @xmath69 , there is already some error , but in practice the difference is marginal .",
    "the following theorem gives guarantees for the accuracy of @xmath59 , when @xmath86 .",
    "if @xmath24 is approximated with @xmath59 and @xmath87 , the error is bounded by @xmath75 .",
    "the error is @xmath88 , where @xmath89 by theorem [ ub2bound ] .    when @xmath86 , @xmath59 ( being a decreasing function of @xmath90 ) is @xmath91 therefore , the error is bounded by    @xmath92    when @xmath86 , @xmath93 , and thus @xmath94 therefore , @xmath95    the latter factor is always @xmath96 , because @xmath97 .",
    "therefore @xmath75 .",
    "our experimental results support the theoretical analysis , according to which both upper bounds , @xmath59 and @xmath60 , give tight approximations to fisher s @xmath24 , when the dependency is sufficiently strong .",
    "however , if the dependency is weak , we may need a more accurate approximation .",
    "a simple solution is to include more larger terms @xmath98 to the approximation and estimate an upper bound only for the smallest terms @xmath99 using the sum of the geometric series .",
    "the resulting approximation and the corresponding error bound are given in the following theorem .",
    "we omit the proofs , because they are essentially identical with the previous proofs for theorems [ ubtheorem ] and [ ub2bound ] .",
    "[ generalub ] for positive dependency rule @xmath8 holds @xmath100 where @xmath101 and @xmath102 .",
    "the error of the approximation is @xmath103",
    "figure [ examplefig ] shows the typical behaviour of the new upper bounds , when the strength of the dependency increases ( i.e.  @xmath4 increases and @xmath104 and @xmath105 remain unchanged ) .",
    "in addition to upper bounds @xmath59 and @xmath60 , we consider a third upper bound , @xmath106 , based on theorem [ generalub ] , where the first three terms of @xmath24 are evaluated exactly and the rest is approximated .",
    "all three upper bounds approach to each other and the exact @xmath24-value , when the dependency becomes stronger .     and three upper bounds as functions of @xmath4 , when @xmath107 , @xmath108 , and @xmath109 .",
    "the strength of the dependency increases on the @xmath110-axes.,scaledwidth=70.0% ]    figure [ zoomfig ] shows a magnified area from figure [ examplefig ] . in this area ,",
    "the dependencies are weak , the upper bounds diverge from the exact @xmath24 .",
    "the reason is that in this area the number of approximated terms is also the largest .",
    "for example , when @xmath111 , @xmath24 contains 146 terms , and when @xmath112 , it contains @xmath113 terms . in these points",
    "the lift is @xmath114 and @xmath115 , respectively .",
    "the difference between @xmath59 and @xmath60 is marginal , but @xmath106 clearly improves @xmath59 .     showing the differences , when the dependency is weak.,scaledwidth=70.0% ]",
    "because the new upper bound gives accurate approximations for strong dependencies , we evaluate the approximations only for the potentially problematic weak dependencies . as an example , we consider two data sets , where the data size is either @xmath109 or @xmath116 . for both data sets , we have three test cases : 1 ) when @xmath117 , 2 ) when @xmath118 and @xmath119 , and 3 ) when @xmath120 and @xmath121 .",
    "( the second case with @xmath109 is shown in figures [ examplefig ] and [ zoomfig ] . ) for all test cases we have calculated the exact @xmath24 , three versions of the upper bound @xmath59 , @xmath60 and @xmath106 , and the @xmath1-value achieved from the one - sided @xmath0-measure .",
    "the @xmath0-based @xmath1-values were calculated with an online chi - square calculator @xcite .",
    "the values are reported for the cases , where @xmath122 , @xmath123 , and @xmath124 . because the data is discrete ,",
    "the exact @xmath24-values always deviate somewhat from the reference values .    the results for the first data set ( @xmath109 )",
    "are given in table [ pfapprcomp1000 ] and for the second data set ( @xmath116 ) in table [ pfapprcomp10000 ] .",
    "as expected , the @xmath0-approximation works best , when the data size is large and the distribution is balanced ( case 1 ) . according to the classical rule of a thumb ,",
    "the @xmath0-approximation can be used , when all expected counts are @xmath125 @xcite .",
    "this requirement is not satisfied in the third case in the smaller data set .",
    "the resulting @xmath0-based @xmath1-values are also the least accurate , but the @xmath0-test produced inaccurate approximations also for the case 2 , even if the smallest expected frequency was 50 .    in the smaller data set , the @xmath0-approximation overperformed the new upper bounds only in the first case , when @xmath122 .",
    "if we had calculated the first four terms exactly , the resulting @xmath126 would have already produced a better approximation .    in the larger data set ,",
    "@xmath0 gave more accurate results for the first two cases , when @xmath122 and for the case 1 , when @xmath123 .",
    "when @xmath124 , the new upper bounds gave always more accurate approximations .",
    "if we had calculated the first eight terms exactly , the resulting @xmath127 would have overperformed the @xmath0-approximation in case 1 with @xmath123 and case 2 with @xmath122 .",
    "calculating eight exact terms is quite reasonable compared to all 2442 terms , which have to be calculated for the exact @xmath24 in case 1 . with 15 exact terms , the approximation for the case 1 with @xmath128 would have also been more accurate than the @xmath0-based approximation",
    ". however , in so large data set ( especially with an exhaustive search ) , a @xmath1-value of 0.05 ( or even 0.01 ) is hardly significant",
    ". therefore , we can conclude that for practical search purposes the new upper bounds give better approximations to the exact @xmath24 than the @xmath0 .",
    "we have introduced a family of upper bounds , which can be used to estimate fisher s @xmath24 accurately . unlike the @xmath0-based approximations , these upper bounds are not sensitive to the data size , distribution , or small expected counts . in practical data mining purposes , the simplest upper bound produces already accurate results , but",
    "if all existing dependencies are weak and relatively insignificant , the results can be too inaccurate . in this case",
    ", we can use a general upper bound , whose accuracy can be adjusted freely . in practice ,",
    "it is usually sufficient to calculate only a couple of terms from the fisher s @xmath24 exactly . in large data sets ,",
    "this is an important concern , because the exact @xmath24 can easily require evaluation of thousands of terms for each tested dependency .",
    "w. hmlinen : efficient search for statistically significant dependency rules in binary data .",
    "ph.d . dissertation .",
    "series of publications a , report a-2010 - 2 department of computer science , university of helsinki , finland .",
    "2010 .",
    "s. morishita and j. sese : transversing itemset lattices with statistical metric pruning . in proceedings of the nineteenth acm sigmod - sigact - sigart symposium on principles of database systems ( pods00 ) , pages 226236 , acm press 2000 ."
  ],
  "abstract_text": [
    "<S> fisher s exact test is often a preferred method to estimate the significance of statistical dependence . </S>",
    "<S> however , in large data sets the test is usually too worksome to be applied , especially in an exhaustive search ( data mining ) . </S>",
    "<S> the traditional solution is to approximate the significance with the @xmath0-measure , but the accuracy is often unacceptable . as a solution , we introduce a family of upper bounds , which are fast to calculate and approximate fisher s @xmath1-value accurately . in addition , the new approximations are not sensitive to the data size , distribution , or smallest expected counts like the @xmath0-based approximation . according to both theoretical and experimental analysis , the new approximations produce accurate results for all sufficiently strong dependencies . </S>",
    "<S> the basic form of the approximation can fail with weak dependencies , but the general form of the upper bounds can be adjusted to be arbitrarily accurate .    </S>",
    "<S> keywords : fisher s exact test upper bound approximation dependency rule </S>"
  ]
}