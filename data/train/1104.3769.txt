{
  "article_text": [
    "we present a little known numerical method for computing characteristic polynomials of real matrices .",
    "the characteristic polynomial of a @xmath2 real matrix @xmath0 is defined as @xmath3 where @xmath4 is the identity matrix , @xmath5 and @xmath6 .",
    "the method was first introduced in 1956 by wallace givens at the third high speed computer conference at louisiana state university @xcite . according to givens ,",
    "the method was brought to his attention by his coder donald la budde ( * ? ? ?",
    "* p 302 ) . finding no earlier reference to this method , we credit its development to la budde and",
    "thus name it `` la budde s method '' .",
    "la budde s method consists of two stages : in the first stage it reduces @xmath0 to upper hessenberg form @xmath1 with orthogonal similarity transformations , and in the second stage it computes the characteristic polynomial of @xmath1 .",
    "the latter is done by computing characteristic polynomials of leading principal submatrices of successively larger order .",
    "because @xmath1 and @xmath0 are similar , they have the same characteristic polynomials .",
    "if @xmath0 is symmetric , then @xmath1 is a symmetric tridiagonal matrix , and la budde s method simplifies to the sturm sequence method @xcite .",
    "if @xmath0 is diagonal then la budde s method reduces to the summation algorithm , a horner - like scheme that is used to compute characteristic polynomials from eigenvalues @xcite .",
    "the summation algorithm is the basis for matlab s ` poly ` command , which computes the characteristic polynomial by applying the summation algorithm to eigenvalues computed with ` eig ` .",
    "we present recursions to compute the individual coefficients of the characteristic polynomial in the second stage of la budde s method .",
    "la budde s method has a number of advantages over ` poly ` .",
    "first , a householder reduction of @xmath0 to hessenberg form @xmath1 in the first stage is numerically stable , and it does not change the condition numbers @xcite of the coefficients @xmath7 with respect to changes in the matrix .",
    "in contrast to ` poly ` , la budde s method is not affected by potential illconditioning of eigenvalues .",
    "second , la budde s method allows the computation of individual coefficients @xmath7 ( in the process , @xmath8 are computed as well ) and is substantially faster if @xmath9 .",
    "this is important in the context of our quantum physics application , where only a small number of coefficients are required @xcite , @xcite .",
    "third , la budde s method is efficient , requiring only about @xmath10 floating point operations and real arithmetic .",
    "this is in contrast to ` poly ` which requires complex arithmetic when a real matrix has complex eigenvalues .",
    "most importantly , la budde s method can often be more accurate than ` poly ` , and can even compute coefficients of symmetric matrices to high relative accuracy .",
    "unfortunately we have not been able to derive error bounds that are tight enough to predict this accuracy .    in this paper",
    "we assume that the matrices are real .",
    "error bounds for complex matrices are derived in @xcite .      after reviewing existing numerical methods for computing characteristic polynomials in ",
    "[ c4 ] , we introduce la budde s method in  [ c_6 ] . then we present recursions for the second stage of la budde s method and running error bounds , for symmetric matrices in  [ s_sym ] and for nonsymmetric matrices in  [ s_nonsym ] . in ",
    "[ s_combined ] we present running error bounds for both stages of la budde s method .",
    "we end with numerical experiments in  [ s_exp ] that compare la budde s method to matlab s ` poly ` function and demonstrate the accuracy of la budde s method .",
    "in the nineteenth century and in the first half of the twentieth century characteristic polynomials were often computed as a precursor to an eigenvalue computation . in the second half of the twentieth century , however , wilkinson and others demonstrated that computing eigenvalues as roots of characteristic polynomials is numerically unstable @xcite . as a consequence , characteristic polynomials and methods for computing them fell out of favor with the numerical linear algebra community .",
    "we give a brief overview of these methods .",
    "they can be found in the books by faddeeva @xcite , gantmacher @xcite , and householder @xcite .",
    "the first practical method for computing characteristic polynomials was developed by leverrier in @xmath11 .",
    "it is based on newton s identities ( * ? ? ?",
    "* ( 7.19.2 ) ) @xmath12 the newton identities can be expressed recursively as @xmath13 leverrier s method and modifications of it have been rediscovered by faddeev and sominski , frame , souriau , and wegner , see @xcite , and also horst @xcite .",
    "although leverrier s method is expensive , with an operation count proportional to @xmath14 , it continues to attract attention .",
    "it has been proposed for computing @xmath15 , sequentially @xcite and in parallel @xcite .",
    "recent papers have focused on different derivations of the method @xcite , combinatorial aspects @xcite , properties of the adjoint @xcite , and expressions of @xmath16 in specific polynomial bases @xcite .",
    "in addition to its large operation count , leverrier s method is also numerically unstable .",
    "wilkinson remarks @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` we find that it is common for severe cancellation to take place when the @xmath17 are computed , as can be verified by estimating the orders of magnitudes of the various contributions to @xmath18 '' _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    wilkinson identified two factors that are responsible for the numerical instability of computing @xmath7 : errors in the computation of the trace , and errors in the previously computed coefficients @xmath19 .",
    "our numerical experiments on many test matrices corroborate wilkinson s observations .",
    "we found that leverrier s method gives inaccurate results even for coefficients that are well conditioned .",
    "for instance , consider the @xmath2 matrix @xmath0 of all ones .",
    "its characteristic polynomial is @xmath20 , so that @xmath21 . since @xmath0 has only a single nonzero singular value @xmath22 , the coefficients @xmath7",
    "are well conditioned ( because @xmath23 singular values are zero , the first order condition numbers with respect to absolute changes in the matrix are zero ( * ? ? ?",
    "* corollary 3.9 ) ) .",
    "however for @xmath24 , leverrier s method computes values for @xmath25 through @xmath26 in the range of @xmath27 to @xmath28 .",
    "the remaining methods described below have operation counts proportional to @xmath29 .      in 1931",
    "krylov presented a method that implicitly tries to reduce @xmath0 to a companion matrix , whose last column contains the coefficients of @xmath16 .",
    "explicitly , the method constructs a matrix @xmath30 from what we now call krylov vectors : @xmath31 where @xmath32 is an arbitrary vector .",
    "let @xmath33 be the _ grade _ of the vector , that is the smallest index for which the vectors @xmath34 are linearly independent , but the inclusion of one more vector @xmath35 makes the vectors linearly dependent .",
    "then the linear system @xmath36 has the unique solution @xmath37 .",
    "krylov s method solves this linear system @xmath38 for @xmath37 . in the fortunate case",
    "when @xmath39 the solution @xmath37 contains the coefficients of @xmath16 , and @xmath40 . if @xmath41 then @xmath37 contains only coefficients of a divisor of @xmath16 .",
    "the methods by danilevski , weber - voetter , and bryan can be viewed as particular implementations of krylov s method @xcite , as can the method by samuelson @xcite .    although krylov s method is quite general , it has a number of shortcomings .",
    "first krylov vectors tend to become linearly dependent , so that the linear system @xmath42 tends to be highly illconditioned .",
    "second , we do not know in advance the grade @xmath43 of the initial vector @xmath44 ; therefore , we may end up only with a divisor of @xmath16 .",
    "if @xmath0 is derogatory , i.e. some eigenvalues of @xmath0 have geometric multiplicity 2 or larger , then every starting vector @xmath44 has grade @xmath41 , and krylov s method does not produce the characteristic polynomial of @xmath0 . if @xmath0 is non derogatory , then it is similar to its companion matrix , and almost every starting vector should give the characteristic polynomial .",
    "still it is possible to start with a vector @xmath44 of grade @xmath41 , where krylov s method fails to produce @xmath16 for a non derogatory matrix @xmath0 ( * ? ?",
    "* example 4.2 ) .    the problem with krylov s method , as well as the related methods by danilevski",
    ", weber - voetter , samuelson , ryan and horst is that they try to compute , either implicitly or explicitly , a similarity transformation to a companion matrix .",
    "however , such a transformation only exists if @xmath0 is nonderogatory , and it can be numerically stable only if @xmath0 is far from derogatory .",
    "it is therefore not clear that remedies like those proposed for danilevski s method in @xcite , ( * ? ? ?",
    "* p 36 ) , @xcite , @xcite would be fruitful .",
    "the analogue of the companion form for derogatory matrices is the frobenius normal form .",
    "this is a similarity transformation to block triangular form , where the diagonal blocks are companion matrices .",
    "computing frobenius normal forms is common in computer algebra and symbolic computations e.g. @xcite , but is numerically not viable because it requires information about jordan structure and is thus an illposed problem .",
    "this is true also of wiedemann s algorithm @xcite , which works with @xmath45 , where @xmath46 is a vector , and can be considered a `` scalar version '' of krylov s method .",
    "hyman s method computes the characteristic polynomial for hessenberg matrices @xcite .",
    "the basic idea can be described as follows .",
    "let @xmath47 be a @xmath2 matrix , and partition @xmath48 if @xmath49 is nonsingular then @xmath50 . specifically , if @xmath51 where @xmath1 is an unreduced upper hessenberg matrix then @xmath49 is nonsingular upper triangular , so that @xmath52 is just the product of the subdiagonal elements .",
    "thus @xmath53 the quantity @xmath54 can be computed as the solution of a triangular system .",
    "however @xmath55 , @xmath49 , and @xmath56 are functions of @xmath57 .",
    "to recover the coefficients of @xmath58 requires the solution of @xmath59 upper triangular systems @xcite .",
    "a structured backward error bound under certain conditions has been derived in @xcite , and iterative refinement is suggested for improving backward accuracy . however , it is not clear that this will help in general .",
    "the numerical stability of hyman s method depends on the condition number with respect to inversion of the triangular matrix @xmath49 .",
    "since the diagonal elements of @xmath49 are @xmath60 , @xmath49 can be ill conditioned with respect to inversion if @xmath1 has small subdiagonal elements .      an obvious way to compute the coefficients of the characteristic polynomial",
    "is to compute the eigenvalues @xmath61 and then multiply out @xmath62 .",
    "the matlab function ` poly ` does this .",
    "it first computes the eigenvalues with ` eig ` and then uses a horner - like scheme , the so - called summation algorithm , to determine the @xmath7 from the eigenvalues @xmath63 as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ` c = [ 1 zeros(1,n ) ] ` + ` for j = 1:n ` + @xmath64`c(2:(j+1 ) ) = c(2:(j+1 ) ) - \\lambda_j.*c(1:j ) ` + ` end ` _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the accuracy of ` poly ` is highly dependent on the accuracy with which the eigenvalues @xmath63 are computed . in @xcite",
    "we present perturbation bounds for characteristic polynomials with regard to changes in the eigenvalues , and show that the errors in the eigenvalues are amplified by elementary symmetric functions in the absolute values of the eigenvalues . since eigenvalues of non - normal ( or nonsymmetric )",
    "matrices are much more sensitive than eigenvalues of normal matrices and are computed to much lower accuracy , ` poly ` in turn tends to compute characteristic polynomials of non - normal matrices to much lower accuracy . as a consequence , ` poly ` gives useful results only for the limited class of matrices with wellconditioned eigenvalues .",
    "la budde s method works in two stages . in the first stage it reduces a real matrix @xmath0 to upper hessenberg form @xmath1 by orthogonal similarity transformations . in the second stage",
    "it determines the characteristic polynomial of @xmath1 by successively computing characteristic polynomials of leading principal submatrices of @xmath1 .",
    "because @xmath1 and @xmath0 are similar , they have the same characteristic polynomials .",
    "if @xmath0 is symmetric , then @xmath1 is a symmetric tridiagonal matrix , and la budde s method simplifies to the sturm sequence method .",
    "the sturm sequence method was used by givens @xcite to compute eigenvalues of a symmetric tridiagonal matrix @xmath65 , and is the basis for the bisection method @xcite .",
    "givens said about la budde s method ( * ? ? ? *",
    "p 302 ) :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ since no division occurs in this second stage of the computation and the detailed examination of the first stage for the symmetric case [ ... ] was successful in guaranteeing its accuracy there , one may hope that the proposed method of getting the characteristic equation will often yield accurate results .",
    "it is , however , probable that cancellations of large numbers will sometimes occur in the floating point additions and will thus lead to excessive errors . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    wilkinson also preferred la budde s method to computing the frobenius form .",
    "he states @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ we have described the determination of the frobenius form in terms of similarity transformations for the sake of consistency and in order to demonstrate its relation to danilewski s method .",
    "however , since we will usually use higher precision arithmetic in the reduction to frobenius form than in the reduction to hessenberg form , the reduced matrices arising in the derivation of the former can not be overwritten in the registers occupied by the hessenberg matrix .",
    "it is more straightforward to think in terms of a direct derivation of the characteristic polynomial of @xmath1 .",
    "this polynomial may be obtained by recurrence relations in which we determine successively the characteristic polynomials of each of the leading principal submatrices @xmath66 ( @xmath67 ) of @xmath1 .",
    "[ ... ]    no special difficulties arise if some of the [ subdiagonal entries of @xmath1 ] are small or even zero . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    la budde s method has several attractive features .",
    "first , a householder reduction of @xmath0 to hessenberg form @xmath1 in the first stage is numerically stable @xcite , @xcite .",
    "since orthogonal transformations do not change the singular values , and the condition numbers of the coefficients @xmath7 to changes in the matrix are functions of singular values @xcite , the sensitivity of the @xmath7 does not change in the reduction from @xmath0 to @xmath1 .",
    "in contrast to the eigenvalue based method in  [ s_evalues ] , la budde s method is not affected by the conditioning of the eigenvalues .",
    "second , la budde s method allows the computation of individual coefficients @xmath7 ( in the process , @xmath8 are computed as well ) and is substantially faster if @xmath9 .",
    "this is important in the context of our quantum physics application , where only a small number of coefficients are required @xcite , @xcite .",
    "third , la budde s method is efficient .",
    "the householder reduction to hessenberg form requires @xmath69 floating point operations @xcite , while the second stage requires @xmath70 floating point operations @xcite ( or @xmath71 flops if @xmath0 is symmetric @xcite ) .",
    "if the matrix @xmath0 is real , then only real arithmetic is needed  in contrast to eigenvalue based methods which require complex arithmetic if a real matrix has complex eigenvalues .",
    "in the first stage , la budde s method reduces a real symmetric matrix @xmath0 to tridiagonal form @xmath65 by orthogonal similarity transformations . the second stage , where it computes the coefficients of the characteristic polynomial of @xmath65 , amounts to the sturm sequence method @xcite . we present recursions to compute individual coefficients in the second stage of la budde s method in",
    "[ s_ss ] , describe our assumptions for the floating point analysis in ",
    "[ s_ass1 ] , and derive running error bounds in  [ s_srun ] .",
    "we present an implementation of the second stage of la budde s method for symmetric matrices .",
    "let @xmath72 be a @xmath2 real symmetric tridiagonal matrix with characteristic polynomial @xmath73 . in the process of computing @xmath16",
    ", the sturm sequence method computes characteristic polynomials @xmath74 of all leading principal submatrices @xmath75 of order @xmath76 , where @xmath77 .",
    "the recursion for computing @xmath16 is @xcite , ( * ? ? ?",
    "* ( 8.5.2 ) ) @xmath78 in order to recover individual coefficients of @xmath16 from the recursion ( [ e_sturm ] ) , we identify the polynomial coefficients @xmath79 and @xmath80 where @xmath81 . equating like powers of @xmath57 on both sides of ( [ e_sturm ] ) gives recursions for individual coefficients @xmath7 , which are presented as algorithm 1 . in the process ,",
    "@xmath19 are also computed .    @xmath2 real symmetric tridiagonal matrix @xmath65 , index @xmath82 coefficient @xmath83 of @xmath16 @xmath84    @xmath85 , @xmath86 @xmath87 @xmath88",
    "@xmath89 @xmath90 @xmath87 @xmath88 @xmath89    if @xmath65 is a diagonal matrix then algorithm 1 reduces to the summation algorithm ( * ? ? ?",
    "* algorithm 1 ) for computing characteristic polynomials from eigenvalues .",
    "the summation algorithm is the basis for matlab s ` poly ` function , which applies it to eigenvalues computed by ` eig ` .",
    "the example in figure [ f_ex ] shows the coefficients computed by algorithm 1 when @xmath91 and @xmath92 .",
    "@xmath93      we assume that all matrices are real . error bounds for complex matrices are derived in @xcite .",
    "in addition , we make the following assumptions :    1 .",
    "the matrix elements are normalized real floating point numbers",
    "the coefficients computed in floating point arithmetic are denoted by @xmath94 .",
    "the output from the floating point computation of algorithms 1 and 2 is @xmath95\\equiv\\hat{c}_k^{(n)}$ ] .",
    "in particular , @xmath96\\equiv c_1=-\\alpha_1 $ ] .",
    "the error in the computed coefficients is @xmath97 so that @xmath98 and @xmath99 5 .",
    "the operations do not cause underflow or overflow .",
    "the symbol @xmath46 denotes the unit roundoff , and @xmath100 .",
    "standard error model for real floating point arithmetic @xcite : + if @xmath101 , and @xmath37 and @xmath102 are real normalized floating point numbers so that @xmath103 does not underflow or overflow , then @xmath104=(x \\op   y)(1+\\delta ) \\qquad \\mathrm{where } \\qquad |\\delta| \\>\\leq u,\\ ] ] and @xmath105=\\frac{x \\op y}{1+\\epsilon } \\qquad \\mathrm{where }   \\qquad |\\epsilon|\\leq u.\\ ] ]    the following relations are required for the error bounds .    [ theta ] let @xmath106 and @xmath107 be real numbers , @xmath108 , with @xmath109 and @xmath110 . if @xmath100 then    1 .",
    "@xmath111 , where @xmath112 2 .",
    "@xmath113      we derive running error bounds for algorithm 1 , first for @xmath114 , then for @xmath115 , and at last for the remaining coefficients @xmath116 , @xmath117 .",
    "the bounds below apply to lines 2 , 4 , and 14 of algorithm 1 .",
    "[ error_1 ] if the assumptions in  [ s_ass1 ] hold and @xmath118 , \\qquad 2\\leq i\\leq n,\\ ] ] then @xmath119    the model ( [ model2 ] ) implies @xmath120 where @xmath121 .",
    "writing the computed coefficients @xmath122 and @xmath123 in terms of their errors ( [ e_error ] ) and then simplifying gives @xmath124 hence @xmath125 .",
    "the bounds below apply to lines 2 , 6 , and 16 of algorithm 1 .",
    "[ error_2 ] if the assumptions in  [ s_ass1 ] hold , and @xmath126- \\fl\\left[\\beta_2 ^ 2\\right]\\biggr]\\\\ \\hat c_2^{(i ) } & = & \\fl\\biggl[\\fl\\left[\\hat c_2^{(i-1)}- \\fl\\left[\\alpha_i\\hat c_1^{(i-1)}\\right]\\right]-\\fl\\left[\\beta_i^2\\right]\\biggr ] , \\qquad 3\\leq i\\leq n,\\end{aligned}\\ ] ] then @xmath127    the model ( [ model1 ] ) implies for the multiplications @xmath128 , \\qquad \\mathrm{where}\\quad |\\delta|,|\\eta|\\leq u.\\ ] ] applying the model ( [ model2 ] ) to the subtraction gives @xmath129 now express @xmath130 in terms of the errors @xmath131 from ( [ e_error ] ) and simplify .    for @xmath132 , applying model ( [ model1 ] ) to the multiplications and the first subtraction gives @xmath133 $ ] , where @xmath134 @xmath135 , @xmath136 and @xmath137 .",
    "applying the model ( [ model2 ] ) to the remaining subtraction gives @xmath138 where @xmath121 .",
    "now express @xmath139 and @xmath140 in terms of their errors ( [ e_error ] ) to get @xmath141 and apply the triangle inequality .",
    "the bounds below apply to lines 8 , 10 , and 18 of algorithm 1 .",
    "[ error_3 ] if the assumptions in  [ s_ass1 ] hold , and @xmath142 + \\fl\\left[\\beta_i^2\\hat c_{i-2}^{(i-2)}\\right]\\biggr],\\qquad 3\\leq i\\leq k,\\\\ \\hat c_j^{(i)}&=&\\fl\\biggl[\\fl\\left[\\hat{c}_j^{(i-1)}- \\fl\\left[\\alpha_i\\hat c_{j-1}^{(i-1)}\\right]\\right]- \\fl\\left[\\beta_i^2\\hat c_{j-2}^{(i-2)}\\right]\\biggr ] , \\qquad 3\\leq j\\leq k , \\quad j+1\\leq i\\leq n,\\end{aligned}\\ ] ] then @xmath143 ) implies for the three multiplications that @xmath144,\\ ] ] where @xmath145 and @xmath146 . applying model ( [ model2 ] ) to the remaining addition gives @xmath147 as in the previous proofs , write @xmath148 , @xmath149 and @xmath150 in terms of their errors ( [ e_error ] ) , @xmath151 and apply the triangle inequality .    for @xmath152 , applying ( [ model2 ] ) to the two multiplications and",
    "the first subtraction gives @xmath153 $ ] where @xmath154 @xmath155 and @xmath156 , @xmath157 .",
    "applying model ( [ model2 ] ) to the remaining subtraction gives @xmath158 where @xmath121 .",
    "write the computed coefficients in terms of their errors ( [ e_error ] ) , @xmath159 and apply the triangle inequality .",
    "we state the bounds when the leading @xmath82 coefficients of @xmath16 are computed by algorithm 1 in floating point arithmetic .",
    "[ c_srunerr ] if the assumptions in  [ s_ass1 ] hold , then @xmath160-c_j|\\leq \\phi_j,\\qquad 1\\leq j\\leq k,\\ ] ] where @xmath161\\equiv \\hat{c}_j^{(n)}$ ] and @xmath162 are given in theorems [ error_1 ] , [ error_2 ] and [ error_3 ] .",
    "in the first stage , la budde s method @xcite reduces a real square matrix to upper hessenberg form @xmath1 . in the second stage",
    "it computes the coefficients of the characteristic polynomial of @xmath1 .",
    "we present recursions to compute individual coefficients in the second stage of la budde s method in  [ s_lab ] , and derive running error bounds in  [ s_labrun ] .",
    "we present an implementation of the second stage of la budde s method for nonsymmetric matrices .",
    "let @xmath163 be a real @xmath2 upper hessenberg matrix with diagonal elements @xmath164 , subdiagonal elements @xmath165 , and characteristic polynomial @xmath166 .",
    "la budde s method computes the characteristic polynomial of an upper hessenberg matrix @xmath1 by successively computing characteristic polynomials of leading principal submatrices @xmath167 of order @xmath76 @xcite .",
    "denote the characteristic polynomial of @xmath167 by @xmath168 , @xmath108 , where @xmath169 .",
    "the recursion for computing @xmath16 is ( * ? ? ?",
    "* ( 6.57.1 ) ) @xmath170 where @xmath171 .",
    "the recursion for @xmath172 is obtained by developing the determinant of @xmath173 along the last row of @xmath167 .",
    "each term in the sum contains an element in the last column of @xmath167 and a product of subdiagonal elements .    as in the symmetric case",
    ", we let @xmath79 and @xmath80 where @xmath81 .",
    "equating like powers of @xmath57 in ( [ e_uh ] ) gives recursions for individual coefficients @xmath7 , which are presented as algorithm 2 . in the process ,",
    "@xmath19 are also computed .",
    "@xmath174 real upper hessenberg matrix @xmath1 , index @xmath82 coefficient @xmath7 of @xmath16 @xmath175 @xmath176 , @xmath177 @xmath87 @xmath87    for the special case when @xmath1 is symmetric and tridiagonal , algorithm 2 reduces to algorithm 1 .",
    "figure [ f_ex1 ] shows an example of the recursions for @xmath91 and @xmath92 .",
    "@xmath178    algorithm 2 computes the characteristic polynomial of companion matrices exactly . to see this ,",
    "consider the @xmath2 companion matrix of the form @xmath179 algorithm 2 computes @xmath180 for @xmath181 and @xmath182 , so that @xmath183 .",
    "since only trivial arithmetic operations are performed , algorithm 2 computes the characteristic polynomial exactly .",
    "we present running error bounds for the coefficients of @xmath16 of a real hessenberg matrix @xmath1 .",
    "the bounds below apply to lines 2 , 4 , and 11 of algorithm 2 .",
    "[ t_1 ] if the assumptions in  [ s_ass1 ] hold and @xmath118 , \\qquad 2\\leq i\\leq n,\\ ] ] then @xmath119    the proof is the same as that of theorem [ error_1 ] .",
    "the following bounds apply to line 2 of algorithm 2 , as well as lines 6 and 14 for the case @xmath184 .",
    "[ t_2 ] if the assumptions in  [ s_ass1 ] hold , and @xmath185- \\fl\\left[h_{12}\\>\\beta_2\\right]\\biggr]\\\\ \\hat c_2^{(i)}&=&\\fl\\biggl[\\fl\\left[\\hat c_2^{(i-1)}- \\fl\\left[\\alpha_i\\hat{c}_1^{(i-1)}\\right]\\right]- \\fl\\left[h_{i-1,i}\\>\\beta_i\\right]\\biggr ] , \\qquad 3\\leq i\\leq n,\\end{aligned}\\ ] ] then @xmath186    the proof is the same as that of theorem [ error_2 ] .",
    "the bounds below apply to lines 6 , 8 , and 14 of algorithm 2 .",
    "[ t_3 ] if the assumptions in  [ s_ass1 ] hold , @xmath187 + \\fl\\left[\\sum_{m=1}^{i-2}{h_{i - m , i}\\>\\beta_i\\cdots \\beta_{i - m+1}\\ > \\hat c_{i - m-1}^{(i - m-1)}}+h_{1i}\\>\\beta_i\\cdots\\beta_2\\right]\\right],\\end{aligned}\\ ] ] @xmath188 , and @xmath189\\right]\\right.\\\\ & & \\qquad \\left .",
    "- \\fl\\left[\\sum_{m=1}^{j-2}{h_{i - m , i}\\>\\beta_i\\cdots \\beta_{i - m+1}\\ > \\hat c_{i - m-1}^{(i - m-1)}}-h_{i - j+1}\\>\\beta_i\\cdots \\beta_{i - j+2}\\right]\\right],\\end{aligned}\\ ] ] @xmath117 , @xmath190 , then @xmath191 and @xmath192    the big sum in @xmath193 contains @xmath194 products , where each product consists of @xmath195 numbers . for the @xmath196 multiplications in such a product ,",
    "the model ( [ model1 ] ) and lemma [ theta ] imply @xmath197 = h_{i - m , i}\\>\\beta_i\\cdots \\beta_{i - m+1}\\>\\hat c_{i - m-1}^{(i - m-1)}(1+\\theta_{m+1 } ) , \\end{aligned}\\ ] ] where @xmath198 and @xmath199 .",
    "the term @xmath200 is a product of @xmath76 numbers , so that @xmath201 = h_{1i}\\>\\beta_i\\cdots \\beta_2(1+\\theta_{i-1}),\\ ] ] where @xmath202 . adding the @xmath203 products @xmath204 from left to right , so that @xmath205+g_3\\right ] \\cdots + g_{i-1}\\right],\\ ] ] gives , again with ( [ model1 ] ) , the relation @xmath206 where @xmath207 and @xmath208 .",
    "for the very first term in @xmath148 we get @xmath209= \\alpha_i\\hat c_{i-1}^{(i-1)}(1+\\delta)$ ] , where @xmath145 . adding this term to @xmath210 and using model ( [ model2 ] ) yields @xmath211 where @xmath212 .",
    "write the computed coefficients in terms of their errors ( [ e_error ] ) @xmath213 and then apply the triangle inequality .    for @xmath214",
    ", @xmath215 contains the additional term @xmath216 , which is involved in the first subtraction .",
    "model ( [ model1 ] ) implies @xmath217 \\right]=\\hat c_j^{(i-1)}\\>\\left(1+\\delta^{(i)}\\right)- \\alpha_i\\hat c_{j-1}^{(i-1)}\\left(1+\\theta_2^{(i)}\\right),\\ ] ] where @xmath155 and @xmath218 . from this",
    "we subtract @xmath210 which is computed as in the case @xmath219 .",
    "finally we can state bounds when the leading @xmath82 coefficients of @xmath16 are computed by algorithm 2 in floating point arithmetic .",
    "[ c_runerr ] if the assumptions in  [ s_ass1 ] hold , then @xmath160-c_j|\\leq \\rho_j,\\qquad 1\\leq j\\leq k,\\ ] ] where @xmath161\\equiv \\hat c_j^{(n)}$ ] and @xmath220 are given in theorems [ t_1 ] , [ t_2 ] and [ t_3 ] .",
    "[ [ potential - instability - of - la - buddes - method ] ] potential instability of la budde s method + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the running error bounds reflect the potential instability of la budde s method . the coefficient @xmath221 is computed from the preceding coefficients @xmath222 .",
    "la budde s method can produce inaccurate results for @xmath221 , if the magnitudes of preceding coefficients are very large compared to @xmath221 so that catastrophic cancellation occurs in the computation of @xmath221 .",
    "this means the error in the computed coefficient @xmath223 can be large if the preceding coefficients in the characteristic polynomials of the leading principal submatrices are larger than @xmath116 .",
    "it may be that the instability of la budde s method is related to the illconditioning of the coefficients .",
    "unfortunately we were not able to show this connection .",
    "we present first order error bounds for both stages of la budde s method .",
    "the bounds take into the account the error from the reduction to hessenberg ( or tridiagonal ) form in the first stage , as well as the roundoff error from the computation of the characteristic polynomial of the hessenberg ( or tridiagonal ) matrix in the second stage .",
    "we derive bounds for symmetric matrices in ",
    "[ s_scombined ] , and for nonsymmetric matrices in ",
    "[ s_nscombined ] .",
    "this bound combines the errors from the reduction of a symmetric matrix @xmath0 to tridiagonal form @xmath65 with the roundoff error from algorithm 1 .",
    "let @xmath224 be the tridiagonal matrix computed in floating point arithmetic by applying householder similarity transformations to the symmetric matrix @xmath0",
    ". from @xcite follows that for some small constant @xmath225 one can bound the error in the frobenius norm by @xmath226 the backward error @xmath227 can be viewed as a matrix perturbation .",
    "this means we need to incorporate the sensitivity of the coefficients @xmath228 to changes @xmath227 in the matrix .",
    "the condition numbers that quantify this sensitivity can be expressed in terms of elementary symmetric functions of the singular values @xcite .",
    "let @xmath229 be the singular values of @xmath0 , and denote by @xmath230 the @xmath231th elementary symmetric function in all @xmath59 singular values .",
    "[ t_symerr ] if the assumptions in  [ s_ass1 ] hold , @xmath0 is real symmetric with @xmath232 for the constant @xmath233 in ( [ e_sym ] ) , @xmath234 are the coefficients of the characteristic polynomial of @xmath235 , then @xmath236-c_j|\\leq ( n - j+1)\\>s_{j-1}\\ > \\nu_1 n^2\\|a\\|_f\\>u+\\phi_j+ \\mathcal{o}\\left(u^2\\right),\\qquad 1\\leq j\\leq k,\\ ] ] where @xmath237 are the running error bounds from corollary [ c_srunerr ] .",
    "the triangle inequality implies @xmath236-c_j|\\leq |\\fl[\\tilde{c}_j]-\\tilde c_j|+|\\tilde c_j - c_j|.\\ ] ] applying corollary [ c_runerr ] to the first term gives @xmath238-\\tilde c_j|\\leq \\phi_j$ ] .",
    "now we bound the second term @xmath239 , and use the fact that @xmath0 and @xmath65 have the same singular values . if @xmath240 then the absolute first order perturbation bound ( * ? ? ?",
    "* remark 3.6 ) applied to @xmath65 and @xmath241 gives @xmath242 from @xmath243 follows @xmath244 .",
    "hence we need @xmath245 to apply the above perturbation bound .",
    "theorem [ t_symerr ] suggests two sources for the error in the computed coefficients @xmath246 $ ] : the sensitivity of @xmath228 to perturbations in the matrix , and the roundoff error @xmath247 introduced by algorithm 1 . the sensitivity of @xmath228 to perturbations in the matrix",
    "is represented by the first order condition number @xmath248 , which amplifies the error @xmath249 from the reduction to tridiagonal form .",
    "this bound combines the errors from the reduction of a nonsymmetric matrix @xmath0 to upper hessenberg form @xmath1 with the roundoff error from algorithm 2 .",
    "let @xmath250 be the upper hessenberg matrix computed in floating point arithmetic by applying householder similarity transformations to @xmath0",
    ". from @xcite follows that for some small constant @xmath251 @xmath252 the polynomial coefficients of nonsymmetric matrices are more sensitive to changes in the matrix than those of symmetric matrices .",
    "the sensitivity is a function of only the largest singular values , rather than all singular values @xcite .",
    "we define @xmath253 which is the @xmath254st elementary symmetric function in only the @xmath231 largest singular values .",
    "[ t_nonsymerr ] if the assumptions in  [ s_ass1 ] hold , @xmath255 for the constant @xmath256 in ( [ e_nonsym ] ) , and @xmath234 are the coefficients of the characteristic polynomial of @xmath257 , then @xmath258-c_j\\right|\\leq    { n \\choose j}s_{j-1}^{(j)}\\ > \\nu_2 n^2\\|a\\|_f\\>u+\\rho_j + \\mathcal{o}\\left(u^2\\right),\\qquad 1\\leq j\\leq k,\\ ] ] where @xmath247 are the running error bounds from corollary [ c_runerr ] .",
    "the proof is similar to that of theorem [ t_symerr ] .",
    "the triangle inequality implies @xmath236-c_j|\\leq |\\fl[\\tilde{c}_j]-\\tilde",
    "c_j|+|\\tilde c_j - c_j|.\\ ] ] applying corollary [ c_runerr ] to the first term gives",
    "@xmath238-\\tilde c_j|\\leq \\rho_j$ ] .",
    "now we bound the second term @xmath239 , and use the fact that @xmath0 and @xmath1 have the same singular values .",
    "if @xmath240 then the absolute first order perturbation bound ( * ? ? ?",
    "* remark 3.4 ) applied to @xmath1 and @xmath259 gives @xmath260 from @xmath261 follows @xmath262 .",
    "hence we need @xmath263 to apply the above perturbation bound .    as in the symmetric case , there are two sources for the error in the computed coefficients @xmath246 $ ] : the sensitivity of @xmath228 to perturbations in the matrix , and the roundoff error @xmath247 introduced by algorithm 2 . the sensitivity of @xmath228 to perturbations in the matrix",
    "is represented by the first order condition number @xmath264 , which amplifies the error @xmath265 from the reduction to hessenberg form .",
    "we compare the accuracy of algorithms 1 and 2 to matlab s ` poly ` function , and demonstrate the performance of the running error bounds from corollaries [ c_srunerr ] and [ c_runerr ] .",
    "the experiments illustrate that algorithms 1 and 2 tend to be more accurate than ` poly ` , and sometimes substantially so , especially when the matrices are indefinite or nonsymmetric .",
    "we do not present plots for the overall error bounds in theorems [ t_symerr ] and [ t_nonsymerr ] , because they turned out to be much more pessimistic than expected .",
    "we conjecture that the errors from the reduction to hessenberg form have a particular structure that is not captured by the condition numbers .",
    "the coefficients computed with algorithms 1 and 2 are denoted by @xmath266 and @xmath267 , respectively , while the coefficients computed by ` poly ` are denoted by @xmath268 .",
    "furthermore , we distinguish the characteristic polynomials of different matrices by using @xmath269 for the @xmath82th coefficient of the characteristic polynomial of the matrix @xmath270 .",
    "this example illustrates that algorithm 2 can compute the coefficients of a highly nonsymmetric matrix more accurately than ` poly ` , and that the running error bounds from corollary [ c_runerr ] approximate the roundoff error from algorithm 2 well .",
    "we choose a @xmath2 forsythe matrix , which is a perturbed jordan block of the form @xmath271 with characteristic polynomial @xmath272 .",
    "then we perform an orthogonal similarity transformation @xmath273 , where @xmath274 is an orthogonal matrix obtained from the qr decomposition of a random matrix .",
    "the orthogonal similarity transformation to upper hessenberg form @xmath275 is produced by @xmath276 ` hess`@xmath277 .",
    "we applied algorithm 2 to a matrix @xmath275 of order @xmath278 .",
    "figure [ f_forsythe3 ] shows that algorithm 2 produces absolute errors of about @xmath279 , and that the running error bounds from corollary [ c_runerr ] approximate the roundoff error from algorithm 2 well .",
    "in contrast , the absolute errors produced by ` poly ` are huge , as figure [ f_forsythe3poly ] shows .",
    "hansen s matrix ( * ? ? ?",
    "* p 107 ) is a rank one perturbation of a @xmath2 symmetric tridiagonal toeplitz matrix , @xmath280 hansen s matrix is positive definite , and the coefficients of its characteristic polynomial are @xmath281 figure [ f_hansen3rel ] illustrates for @xmath278 that the algorithm 1 computes the coefficients to machine precision , and that later coefficients have higher relative accuracy than those computed by ` poly ` . with regard to absolute errors , figure [ f_hansen3abs ]",
    "indicates that the running error bounds @xmath237 from corollary [ c_srunerr ] reflect the trend of the errors , but the bounds become more and more pessimistic for larger @xmath82 .",
    "this example illustrates that algorithm 1 can compute the characteristic polynomial of a symmetric indefinite matrix to high relative accuracy , and that the running error bounds in corollary [ c_srunerr ] capture the absolute error well .      for @xmath285 we obtained the exact coefficients",
    "@xmath286 with ` sym2poly(poly(sym(t ) ) ) ` from matlab s symbolic toolbox . algorithm 1 computes the coefficients with odd index exactly , i.e. @xmath287 for @xmath284 and @xmath108 .",
    "in contrast , as figure [ f_indef3abs ] shows , the coefficients computed by ` poly ` can have magnitudes as large @xmath288 .",
    "figure [ f_indef3releven ] illustrates that algorithm 1 computes the coefficients @xmath289 with even index to machine precision , while the coefficients computed with ` poly ` have relative errors that are many magnitudes larger .",
    "figure [ f_indef3abs ] also shows that the running error bounds approximate the true absolute error very well .",
    "what is not visible in figure [ f_indef3abs ] , but what one can show from theorems [ error_1 ] and [ error_2 ] is that @xmath290 .",
    "hence the running error bounds recognize that @xmath291 are computed exactly .",
    "the frank matrix @xmath292 is an upper hessenberg matrix with determinant 1 from matlab s ` gallery ` command of test matrices .",
    "the coefficients of the characteristic polynomial appear in pairs , in the sense that @xmath293 . for a frank matrix of order @xmath294",
    ", we used matlab s toolbox to determine the exact coefficients @xmath295 with the command ` sym2poly(poly(sym(u ) ) ) ` .",
    "figure [ f_frank3abs ] illustrates that algorithm 2 computes the coefficients at least as accurately as ` poly ` .",
    "in fact , as seen in figure [ f_frank3rel ] , algorithm 2 computes the first 20 coefficients to high relative accuracy .        the chow matrix is a matrix that is toeplitz as well as lower hessenberg from matlab s ` gallery ` command of test matrices .",
    "our version of the transposed chow matrix is an upper hessenberg matrix with powers of 2 in the leading row and trailing column , @xmath296 as before , we computed the exact coefficients with matlab s symbolic toolbox .",
    "figure [ f_chow3 ] illustrates that algorithm 2 computes all coefficients @xmath297 to high relative accuracy for @xmath294 .",
    "in contrast , the relative accuracy of the coefficients computed by ` poly ` deteriorates markedly as @xmath82 becomes larger .",
    "however , if we compute instead the characteristic polynomial of @xmath298 , then a preliminary reduction to upper hessenberg form is necessary .",
    "figure [ f_chowt3 ] illustrates that the computed coefficients have hardly any relative accuracy to speak of , and only the trailing coefficients have about 1 significant digit .",
    "the loss of accuracy occurs because the errors in the reduction to hessenberg form are amplified by the condition numbers of the coefficients , as the absolute bound in theorem [ t_nonsymerr ] suggests .",
    "unfortunately , in this case , the condition numbers in theorem [ t_nonsymerr ] are too pessimistic to predict the absolute error of algorithm 2 ."
  ],
  "abstract_text": [
    "<S> la budde s method computes the characteristic polynomial of a real matrix @xmath0 in two stages : first it applies orthogonal similarity transformations to reduce @xmath0 to upper hessenberg form @xmath1 , and second it computes the characteristic polynomial of @xmath1 from characteristic polynomials of leading principal submatrices of @xmath1 . if @xmath0 is symmetric , then @xmath1 is symmetric tridiagonal , and la budde s method simplifies to the sturm sequence method . </S>",
    "<S> if @xmath0 is diagonal then la budde s method reduces to the summation algorithm , a horner - like scheme used by the matlab function ` poly ` to compute characteristic polynomials from eigenvalues .    </S>",
    "<S> we present recursions to compute the individual coefficients of the characteristic polynomial in the second stage of la budde s method , and derive running error bounds for symmetric and nonsymmetric matrices . </S>",
    "<S> we also show that la budde s method can be more accurate than ` poly ` , especially for indefinite and nonsymmetric matrices @xmath0 . unlike ` poly ` , </S>",
    "<S> la budde s method is not affected by illconditioning of eigenvalues , requires only real arithmetic , and allows the computation of individual coefficients .    </S>",
    "<S> = 0pt    summation algorithm , hessenberg matrix , tridiagonal matrix , roundoff error bounds , eigenvalues    65f15 , 65f40 , 65g50 , 15a15 , 15a18 </S>"
  ]
}