{
  "article_text": [
    "networked distributed storage systems such as google file - system ( gfs )  @xcite , amazon s3  @xcite or hadoop file - system ( hdfs )  @xcite spread data among several storage nodes and allow to scale out from hundreds to thousands of commodity storage servers able to accommodate the ever - growing volume of data to be stored . to ensure that data survives failures of some of the storage nodes , all data needs",
    "to be redundantly stored .",
    "the simplest way to introduce redundancy is to store multiple copies ( or replicas ) of each data across the system .",
    "but erasure codes , a more sophisticated type of redundancy , can provide equivalent or even better fault - tolerance than replication for significantly lower storage overhead  @xcite , and hence have increasingly been embraced in recent times in systems such as microsoft azure @xcite , hadoop fs  @xcite and the new version of the google file system @xcite among others .",
    "typical choices of erasure codes used in these systems have an overall overhead of @xmath0@xmath1 the size of the original data  @xcite , which allows to reduce up to 50% the typical overhead of storing three replicas .",
    "although erasure codes have the potential to significantly reduce storage costs in distributed storage systems , there are still two advantages of using replication to store newly introduced data :    * _ pipelined insertion : _ replication allows to easily pipeline the redundancy generation process : data being stored in a node can be simultaneously forwarded to a second node , and from this second node to a third , and so on  @xcite .",
    "such pipelining process allows to distribute the redundancy generation costs among different nodes , achieving a high storage throughout as well as an immediate data reliability . *",
    "_ data locality : _ freshly introduced data in the system is very likely to be accessed and used , e.g. , in a batch process to carry out some analytics .",
    "replicating the data in several storage nodes allows the task scheduler to exploit data - locality : jobs are scheduled on the same nodes where data is located @xcite .",
    "such a scheduling strategy reduces network latencies and increases data and processing throughputs .    due to these properties ,",
    "distributed storage systems often store newly introduced data using replication , and rely on erasure codes to archive older and infrequently accessed data  @xcite .",
    "such a pragmatic design allows systems to enjoy the benefits of replication ( fast data insertion , data locality , etc . ) when the data is in frequent use , as well as that of erasure codes ( high fault - tolerance for lower storage cost ) when the data is not accessed regularly , but still needs to be preserved .",
    "the need to access a specific stored data reduces significantly within a short period of time @xcite , which justifies replacing the replicas by an erasure code based archival .",
    "this migration usually consists of an atomic operation where a single storage node obtains the entire data object ( by downloading blocks from different nodes if needed ) , encodes it , and finally uploads various parity blocks to different storage nodes  @xcite , after which the number of replicas can be safely reduced to one .",
    "although the encoding of one data object using this naive approach is inherently centralized , large datasets ( containing several data objects ) can sometimes be concurrently encoded by distributing individual encoding operations across different nodes .",
    "this does not change the fact that the limited network and computing capacity of individual nodes remain a bottleneck that slows down the whole archival process .    while different aspects of erasure coding based distributed storage systems",
    "have been studied recently , which include maximizing the fault - tolerance of erasure codes  @xcite , reducing the costs of repairing failures  @xcite , or deduplicating encoded data  @xcite , this paper instead looks at a relatively unexplored problem , that of the efficiency of the migration from replication to erasure codes , aiming at optimizing the data archival in distributed storage systems .",
    "our main contributions are three - fold .",
    "\\(1 ) we propose a novel coding strategy that splits the single - object encoding operation into different tasks that can be concurrently executed in different nodes , thus distributing the network and computing load of the archival process across multiple nodes , which in turn speeds up the archival process .",
    "our new encoding scheme is inspired by the _ pipelined insertions _ used in replication : first , the encoding process is distributed among those nodes storing replicated data of the object to be encoded , which exploits _ data locality _ and saves network traffic .",
    "we then arrange the encoding nodes in a pipeline where each node sends some partially encoded data to the next node , which creates parity data simultaneously on different storage nodes , avoiding the extra time required to distribute the parity after the encoding process is terminated .",
    "\\(2 ) we further present rapidraid codes , an explicit family of erasure codes that realizes the pipelined erasure coding idea and provides fast archival without compromising either on data reliability or on storage overhead .",
    "interestingly , rapidraid codes only require the existence of two object replicas to execute a pipeline encoding , which makes them suitable for archiving data in reduced redundancy systems .",
    "additionally , rapidraid codes offer flexible parameter choices to realize different storage overheads ( up to 2@xmath2 the size of the original data ) and different data reliability guarantees .",
    "\\(3 ) we finally provide a real implementation of rapidraid codes that we benchmark both in a small cluster of 50 hp thinclients as well as in a set of amazon ec2 virtual instances .",
    "our experimental results show that rapidraid coding reduces the coding time of single data objects by up to 90% , and by up to 20% for batch processing the coding of multiple objects .",
    "the benefits of rapidraid codes are also visible when part of the network is congested . the presence of congested nodes has less detrimental effects on rapidraid encoding times than on traditional encoding times .    the rest of the paper is organized as follows . in section  [",
    "s : background ] we provide the basic background on distributed storage systems and classical erasure codes . in section  [ s : times ] we estimate the coding times of classical erasure codes and show how pipelined erasure coding speeds up the coding time by exploiting data locality . in sections  [ s : rrex ] and  [ s : rrge ] we present the family of rapidraid codes and we experimentally evaluate its performance in section  [ s : eval ] .",
    "finally , sections  [ s : relwork ] and  [ s : concl ] respectively present the related work and our conclusions .",
    "distributed storage systems used in data centers have started to adopt a hybrid strategy for redundancy , where replicas of the newly inserted data are created , while erasure codes are preferred for archival of the same data once it does not need to be regularly accessed anymore , but still needs to be preserved .",
    "the number of replicas is then reduced . the use of erasure coding for archival increases the fault tolerance of the system while reducing storage overheads with respect to replication  @xcite , though replication remains so far the best form of redundancy for new data since it is likely to be frequently manipulated .",
    "formally , the encoding process takes @xmath3 blocks of data and computes @xmath4 parity blocks ( or redundancy blocks ) , which will be stored in @xmath4 other different storage nodes . in most cases , since it is unlikely to find data objects that were exactly split into @xmath3 blocks during the insertion process , the @xmath3 blocks used in the encoding process might belong to different data objects .",
    "for example , in some systems files from the same directory are jointly encoded  @xcite .    an optimal erasure code in terms of the trade - off between storage overhead and fault tolerance",
    "is called a maximum distance separable ( mds ) code , and has the property that the original object can be reconstructed from any @xmath3 out of the @xmath5 stored blocks , tolerating the loss of any @xmath6 blocks .",
    "the notation `` @xmath7 code '' is often used to emphasize the code parameters .",
    "examples of the most widely used mds codes are the reed - solomon codes",
    ". such codes will be referred to as _ classical erasure codes _ , to distinguish them from newly designed erasure codes .",
    "we will denote a data object to be stored by a vector @xmath8 of @xmath9 bits , that is each @xmath10 , @xmath11 , is a string of @xmath12 bits .",
    "operations are typically performed using finite field arithmetic , that is , the two bits @xmath13 are seen as forming the finite field @xmath14 of two elements , while @xmath10 , @xmath11 , then belong to the binary extension field @xmath15 containing @xmath16 elements .",
    "encoding of the object @xmath17 is performed using an ( @xmath18 ) generator matrix @xmath19 such that @xmath20 , to obtain an @xmath21-dimensional codeword @xmath22 , of size @xmath23 bits .",
    "when the generator matrix @xmath19 has the form @xmath24^t$ ] where @xmath25 is the identity matrix and @xmath26 is a @xmath27 matrix , the codeword @xmath28 becomes @xmath29 $ ] where @xmath17 is the original object , and @xmath30 contains the @xmath31 bits of redundancy .",
    "the code is then said to be systematic , in which case the @xmath3 parts of the original object remain unaltered after the coding process .",
    "the data can then still be read without requiring a decoding process .    due to the computational complexity of finite field arithmetic , erasure codes usually need to operate on small fields ( with small @xmath12 values ) to guarantee a fast coding process .",
    "usually @xmath32 or @xmath33 ( @xmath34 or @xmath35 ) are preferred due to their efficient manipulation using 8-bit and 16-bit cpu words .",
    "however , the size of the field also constrains the size of the object ( which is of @xmath36 bits ) to be either @xmath37 or @xmath38 bits long , for relatively small values of @xmath3 . in distributed storage systems where the @xmath3 blocks are usually tens of megabytes long",
    ", the coding is handled per part .",
    "the coding process iteratively takes @xmath3 input words ( @xmath12 bit words ) from each of the @xmath3 original blocks to form a small object of size @xmath36 , which can be easily encoded .",
    "one of the main drawbacks of classical erasure codes is that the encoding process is an atomic operation in that a single node has the responsibility to download @xmath3 blocks ( from any of the existing replicas ) , encode them , and finally upload the resulting @xmath4 parity blocks to @xmath4 other nodes @xcite . in this case",
    "the encoding node becomes a network and computing bottleneck that slows down the whole coding process .",
    "the @xmath39 symbol denotes a coding operation . ]    to understand better why such an atomicity results in long encoding times , we depict in fig .  [",
    "f : pec ] an example of an object encoding using a classical systematic ( @xmath40 ) erasure code : an object @xmath41 is encoded into an @xmath42-dimensional codeword @xmath43 . the node @xmath44 ( denoted by n@xmath44 on the figure ) stores a replica of the raw data block @xmath45 , @xmath46 . to migrate to an erasure encoded data",
    ", the node executing the encoding process ( denoted by @xmath39 ) downloads the @xmath47 original blocks from any of the existing replicas ( here from node @xmath48 ) , and computes the redundancy blocks @xmath49 which are then uploaded to nodes @xmath50 to @xmath42 .",
    "the number of transmitted blocks is @xmath51 , and it could have been reduced to @xmath52 if the coding process were run for example in node @xmath53 , which already stored @xmath54 locally . in this toy example , exploiting data locality could save a block transmission .    to analytically obtain an estimate of the time required for encoding one object using a classical erasure code , we consider the best possible scenario and assume that the coding process is done in a streamlined manner , meaning that the coding node downloads in parallel all the @xmath3 original blocks and starts to generate parity data immediately after receiving the first few bytes from each of the @xmath3 source nodes ( e.g. , once the first @xmath3 network buffers are filled ) . concurrently with the encoding of this data",
    ", the coding node continues to receive data from the @xmath3 source nodes , and uploads the partially generated parity data to the @xmath55 destination nodes .",
    "the time required to encode an object can then be approximated by :    @xmath56    where @xmath57 is the time needed to download a single data block under normal network conditions , and @xmath58 represents the time required to generate parity data from the first @xmath3 network buffers .",
    "since the size of the blocks to be encoded are relatively large we will assume that the time required to transfer a block between two nodes is several orders of magnitude longer than the time required to partially encode an amount of data equivalent to the size of a network buffer : i.e. , @xmath59    n8 .",
    "the @xmath39 symbol denotes a coding operation . ]",
    "one way to avoid the bottleneck of having a single coding node is to pipeline the creation of erasure code redundancy and distribute the redundancy generation costs among different storage nodes .",
    "the main idea behind our pipelined strategy is to take advantage of the fact that the data to be encoded is already spread and replicated over different nodes .",
    "then , each of the nodes with one of the replicas can combine the data it stores with data from other nodes to generate part of the final codeword @xmath60 . in fig .",
    "[ f : ppl ] we depict an example of this idea using the same code parameters ( 8,4 ) used in fig .  [ f : pec ] , though here we do not insist on the code being systematic .",
    "nodes @xmath61 to @xmath53 store together a replica of the stored object @xmath17 as before ( that is node @xmath44 stores @xmath10 , @xmath46 ) but this time nodes @xmath50 to @xmath42 store a second replica of the same object as well .",
    "the coding process proceeds as follows .",
    "the first node sends a multiple of @xmath62 to the second node .",
    "the node @xmath63 computes a linear combination of this multiple of @xmath62 with @xmath64 and forwards the result to the node @xmath65 .",
    "the node @xmath65 has now its own data @xmath66 , and again computes a linear combination of @xmath66 with what it received .",
    "the process is iteratively repeated from node @xmath44 to node @xmath67 , @xmath68 . simultaneously to this pipeline process , each node also generates its own redundancy block @xmath69 , based on what it owns and receives , which does not have to be the same linear combination as that sent to the next node .",
    "the set of all the locally generated blocks constitutes the final codeword @xmath70 .",
    "assuming that only two replicas of @xmath17 are used in the process , the maximum length of the final codeword should be constrained to @xmath71 , although we will see in next sections that any @xmath72 is possible .",
    "additionally , note that the coding process only requires to transmit seven temporal blocks ( in general @xmath73 blocks are transmitted ) which entails the same network traffic as a classical encoding process .",
    "however , the coding time for the pipelined strategy is significantly reduced . in this case",
    "we can measure the coding time as the time required to transmit one block @xmath57 , plus @xmath73 times the delay taken to receive and encode a network buffer , denoted by @xmath74 ( we assume here the same streamlined coding strategy ) :    @xmath75    similarly , due to the large size of the blocks being encoded ( of the order of tens of megabytes ) we can also assume that the time required to transfer a block between two nodes is several orders of magnitude longer than the time required to partially encode an amount of data equivalent to the size of a network buffer : @xmath76 .",
    "however , since @xmath77 and @xmath78 , it is easy to see when we compare ( [ e : tclassic ] ) and ( [ e : tpipe ] ) that the factor @xmath79 in ( [ e : tclassic ] ) makes @xmath80 several times larger than @xmath81 . in section",
    "[ s : eval ] we will support this claim with real experiments .",
    "one possible criticism of the pipelined coding strategy is that unlike in classical erasure codes , the generated codeword does not contain a raw copy of the original data ( i.e. , it is not a systematic code ) .",
    "the immediate consequence is that accessing stored data will always require a decoding operation , which always comes with an associated cpu overhead .",
    "however , the benefits of a fast and less cpu - demanding encoding process ( as we will see in section vi ) outweighs the relative inefficiency of data access , since the latter is infrequent . furthermore , empirical studies have shown how erasure encoded data can be accessed with relatively low latencies , even when data needs to be decoded  @xcite , and this latency can be further ameliorated by adopting pipelined decoding operations ( faster than classical decoding operations ) , which are not reported here because of space restrictions .",
    "in this section we present rapidraid codes , an explicit family of erasure codes that realize the idea of pipelined erasure codes presented in the previous section .",
    "we first illustrate the code construction through two simple examples , and in section  [ s : rrge ] we formalize the definition of rapidraid codes .",
    "we continue with an @xmath82 erasure code , as used in the previous section .",
    "an object @xmath83 , of @xmath47 blocks is stored over @xmath51 nodes using a codeword @xmath84 , and two replicas of @xmath85 are initially scattered as follows ( this is the same original placement as that of fig .  [",
    "f : ppl ] ) : @xmath86 based on this replica placement , we split the rapidraid coding process in two phases :    * phase 1 ( vertical coding ) * : following the pipeline depicted in fig .",
    "[ f : ppl ] , node @xmath61 forwards some multiple of @xmath62 to node 2 , which computes a linear combination of the received data with @xmath64 , and forwards it again to node 3 , and so on .",
    "more generally , node @xmath44 encodes the data it gets from the previous node together with the data it already has and forwards it to the next node .",
    "we denote the data forwarded from node @xmath44 to its successor , node @xmath87 , by @xmath88 , which is defined as follows : @xmath89 where @xmath90 , @xmath91 , are predetermined values .    * phase 2 ( horizontal coding ) * : each of the @xmath21 involved nodes also generates an element of the final codeword @xmath69 by encoding the received data together with the locally stored data as follows : @xmath92 where @xmath93 , @xmath94 , are also predetermined values .    although we defined the coding process using two logically different phases , we want to highlight that when the coding process is implemented as a streamlined process , both phases can be executed simultaneously : as soon as node @xmath44 receives the first few bytes of @xmath95 it can start generating the first bytes of @xmath69 , and concurrently forward @xmath96 to node @xmath67 .      using the notation of section  [ s : background ] , we can express the rapidraid coding process of the ( 8,4 ) example using the standard linear coding notation @xmath20 as @xmath97 \\xi_1 & 0 & 0 & 0 \\\\ \\psi_1 &",
    "\\xi_2 & 0 & 0 \\\\ \\psi_1 & \\psi_2 & \\xi_3 & 0 \\\\ \\psi_1 & \\psi_2 & \\psi_3 & \\xi_4 \\\\ \\psi_1+\\xi_5 & \\psi_2 & \\psi_3 & \\psi_4 \\\\ \\psi_1+\\psi_5 & \\psi_2+\\xi_6 & \\psi_3 & \\psi_4 \\\\ \\psi_1+\\psi_5 & \\psi_2+\\psi_6 & \\psi_3+\\xi_7 & \\psi_4 \\\\ \\psi_1+\\psi_5 & \\psi_2+\\psi_6 & \\psi_3+\\psi_7 & \\psi_4+\\xi_8 \\end{bmatrix * } \\cdot \\begin{bmatrix } o_1 \\\\ o_2 \\\\ o_3 \\\\ o_4 \\end{bmatrix } = \\begin{bmatrix } c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\\\ c_5 \\\\ c_6 \\\\ c_7 \\\\ c_8 \\end{bmatrix}.\\ ] ] it is easy to see that we can use the gauss elimination method to reconstruct the original object , @xmath17 , from any subset of four linearly independent symbols of @xmath60 . maximizing the number of linearly independent 4-subsets in @xmath60 can be done by exhaustive computational search of the values taken by @xmath98 and @xmath99 once the size @xmath16 of the field is fixed .",
    "when all @xmath3-subsets in @xmath60 are linearly independent , the code then becomes mds , which achieves the highest possible fault tolerance given any @xmath3 and @xmath21 .",
    "note that the larger the field @xmath100 , the more likely it is to remove the linear dependencies within @xmath60  @xcite .",
    "however , even by selecting the optimal values of @xmath98 and @xmath99 , there could be some intrinsic dependencies introduced by the pipelined coding process itself that can not be removed . in the example of",
    "the ( 8,4 ) code proposed , from all the @xmath101 possible 4-subsets , there is one single linearly dependent @xmath53-subset , namely @xmath102 , which can not be removed , no matter the values taken by @xmath99 and @xmath98 in @xmath100 , for any @xmath12 .",
    "recall that @xmath103 in @xmath100 .",
    "then the following linear combination of @xmath104 and @xmath105 always evaluates to zero : @xmath106 + c_2\\left[\\xi_6\\xi_2^{-1}\\right ] + c_5 + c_6 \\\\ \\!\\!&\\!\\!=&\\!\\!o_1\\xi_1(\\psi_1\\xi_6\\xi_2^{-2}+\\psi_5+\\xi_5)\\xi_1^{-1 } + ( o_1\\psi_1+o_2\\xi_2)\\xi_6\\xi_2^{-1 }",
    "\\\\ \\!\\!&\\!\\!&\\!\\!+(o_1\\psi_1+o_1\\xi_5+o_2\\psi_2+o_3\\psi_3+o_4\\psi_4 ) \\\\ \\!\\!&\\!\\!&\\!\\!+(o_1\\psi_1+o_1\\psi_5 + o_2\\psi_2+o_2\\xi_6+o_3\\psi_3+o_4\\psi_4)\\\\ \\!\\!&\\!\\!=&\\!\\!o_1\\xi_1\\psi_1\\xi_6\\xi_2^{-1}\\xi_1^{-1}\\!+o_1\\xi_1\\psi_5\\xi_1^{-1}+o_1\\xi_1\\xi_5 \\xi_1^{-1}\\!+ o_1\\psi_1\\xi_6\\xi_2^{-1}\\\\ \\!\\!&\\!\\!&\\!\\!+o_2\\xi_2\\xi_6\\xi_2^{-1}+o_1\\psi_1+o_1\\xi_5+o_2\\psi_2+o_3\\psi_3+o_4\\psi_4 \\\\ \\!\\!&\\!\\!&\\!\\!+ o_1\\psi_1+o_1\\psi_5 + o_2\\psi_2+o_2\\xi_6+o_3\\psi_3+o_4\\psi_4 = 0.\\end{aligned}\\ ] ] it shows that the code is not an mds code :",
    "if all the redundant blocks but @xmath102 fail , it will be impossible to recover the original data @xmath17 . in section  [ s : rrge ] we will analyze in detail which are the ( @xmath107 ) values that allow to obtain mds codes , and for the rest of ( @xmath107 ) values , we will quantify the impact that the non - mds property has on the overall data reliability .",
    "the previous ( 8,4 ) code example enjoys a symmetric construction inherited from @xmath71 , but we can extend the rapidraid coding scheme for @xmath109 . as an example we consider the case of a ( 6,4 ) code , which requires replicas of @xmath17 to be initially overlapped on the @xmath110 nodes as follows : @xmath111 the rest of the coding process continues as previously explained .",
    "the basic difference will be on the computation made by nodes 3 and 4 , which in this case corresponds to : @xmath112 note that some of the subindexes of coefficients @xmath113 and @xmath114 might need to be altered accordingly .",
    "inspired by the examples of previous section , we now present a general definition of rapidraid codes for any pair ( @xmath107 ) of parameters , where @xmath72 .",
    "we start by stating the requirements that rapidraid imposes on how data must be stored :    * as shown in the ( 6,4 ) example code , when @xmath115 two of the stored replicas should be overlapped between @xmath21 storage nodes : a replica of @xmath17 should be placed in nodes @xmath61 to @xmath3 , and a second replica of @xmath17 in nodes from @xmath116 to @xmath21 .",
    "* the final @xmath21 redundancy blocks forming @xmath60 have to be generated ( and finally stored ) in nodes that were already storing a replica of the original data .",
    "we then formally define the temporal redundant block that each node @xmath44 in the pipelined chain sends to its successor as : @xmath117 with @xmath118 , while the final redundant block @xmath69 generated / stored in each node @xmath44 is : @xmath119 where @xmath120 are static predetermined values specifically chosen to guarantee maximum fault tolerance .",
    "as we already mentioned , the fault tolerance of the code depends on the number of linearly independent blocks within the codeword @xmath60 .",
    "optimally , if the code is mds , all the @xmath121 @xmath3-subsets of @xmath60 are linearly independent . in practice , achieving the mds property is not always possible due to different types of linear dependencies generated during the construction of the rapidraid code .",
    "we distinguish two different types of these linear dependencies :    1 .   *",
    "natural dependencies * are introduced by the pipelined coding process itself and can not be removed , no matter the values taken by @xmath99 and @xmath98 .",
    "* accidental dependencies * appear due to a bad choice of the values of @xmath99 and @xmath98 .    )",
    "rapidraid codewords .",
    "we consider three @xmath21 values with all the possible @xmath3 values such that @xmath122.,title=\"fig:\"][f : perc ] ) rapidraid codewords .",
    "we consider three @xmath21 values with all the possible @xmath3 values such that @xmath122.,title=\"fig:\"][f : num ]    to evaluate the fault tolerance of an @xmath7 rapidraid code , we need to count the different linear dependencies in its codewords .",
    "we first analytically detect _ natural dependencies _ by enumerating all the possible @xmath3-subsets , and for each @xmath3-subset , we determine by symbolic computation whether it contains linear dependencies .",
    "once we know that there is no linear dependency , we pick values of @xmath98 and @xmath99 so as to avoid _",
    "accidental dependencies_. this can be done at random for relatively large fields such as @xmath33 , where almost any random set of coefficients guarantees the absence of accidental dependencies  @xcite . for",
    "small fields like @xmath123 , finding a set of coefficients without accidental dependencies might require long exhaustive searches .",
    "such an enumeration of all possible @xmath3-subsets is feasible only for small values of @xmath21 , due to the fast growth of the number @xmath121 of @xmath3-subsets to test . in fig .",
    "[ f : numeric ] , we computed the number of natural linear dependencies of @xmath7 rapidraid codes with @xmath124 , and all the possible values of @xmath3 , @xmath122 . in fig .",
    "[ f : perc ] we show the percentage of linearly independent @xmath3-subsets and in fig .",
    "[ f : num ] the absolute number of linearly dependent @xmath3-subsets .",
    "we observe that rapidraid codes achieve the mds property when @xmath125 .    after analyzing all the rapidraid codes for @xmath126",
    ", we propose the following conjecture :    an @xmath7 rapidraid code as defined by ( [ e : pfwd ] ) and ( [ e : pcode ] ) is maximum distance separable ( mds ) if @xmath125 .",
    "however , we would like to highlight that some of the non - mds codes ( when @xmath127 ) still achieve high percentages of linearly independent @xmath3-subsets .",
    "this is the case for example of a ( 16,11 ) rapidraid code , evaluated later in this paper .",
    "to complete the fault tolerance analysis of @xmath7 rapidraid codes , we now consider their static resilience , which is the probability of being able to reconstruct a given stored object when a fraction @xmath128 of random storage nodes fail .",
    "this static resilience for different node failure probabilities using the `` number of 9 s '' metric is shown in table  [ t : reliab ] , where we compare three different codes : ( i ) a ( 16,11 ) rapidraid code , which is non - mds , ( ii ) a ( 16,11 ) classical mds code , and ( iii ) the standard replication scheme with three replicas .",
    "we see that although the static resilience of the rapidraid code is slightly lower than the classical erasure code , for storage systems with low node failure probabilities ( @xmath129 ) , rapidraid codes achieve at least the same resiliency as the de - facto standard 3-way replication scheme .",
    "according to data center studies published in  @xcite , the annualized failure rate ( afr ) of modern hard disk drives ( hdd ) is in the range of 2% to 5% , depending on the age of the disk . since the time required to repair a disk failure ( which includes the time to detect the disk failure plus the time to repair and restore the missing data ) is in the range of minutes or a few hours  @xcite , it is reasonable to expect less than 1% of simultaneous disk failures , making the rapidraid codes family an attractive alternative to replace classical erasure codes in data centers . besides , the actual trend in datacenters is to use solid state disks ( ssd ) , which have even lower afrs as compared to traditional hdd .",
    "further note that the actual chance of data loss is much lower than the values indicated by static resilience analysis if the system is repaired and thus faults are not allowed to accumulate .",
    ".static resiliency of three different redundancy schemes ( in number of 9 s ) for different probabilities of node failure @xmath128 .",
    "[ cols=\"<,^,^,^,^\",options=\"header \" , ]",
    "in this section we evaluate the coding performance of a rapidraid code and compare it with that of classical erasure coding .",
    "the code that we choose for the evaluation is a ( 16,11 ) code , with parameters similar to those used in real distributed storage systems  @xcite , which offer a data reliability comparable to a ( 16,11 ) classical erasure code ( see table  [ t : reliab ] ) .      in order to fairly compare coding times of rapidraid codes with those of classical erasure codes",
    ", we developed an experimental distributed storage system which consists of a fast python server infrastructure providing basic store / retrieve operations , as well as finite field arithmetic required to encode and forward data in pipelined erasure codes .",
    "the finite field arithmetic is implemented using the jerasure  @xcite library , which contains a fast set of functions ( optimized c code ) designed to construct efficient erasure codes .    over our distributed storage system",
    "we integrated two different erasure codes :    * a ( 16,11 ) classical reed - solomon erasure code using cauchy generator matrices , as it is already implemented in the jerasure library .",
    "we adjust the erasure code parameters to guarantee maximum performance as it is suggested in  @xcite , which makes the cauchy reed solomon code to clearly outperform other open source erasure coding libraries  @xcite",
    ". we will refer to this code implementation as _ cec _ ( _ classical erasure code _ ) . *",
    "a ( 16,11 ) rapidraid code implemented using the finite field arithmetic from jerasure .",
    "this implementation can either work with 8 bit or 16 bit arithmetic , with operations in @xmath32 or @xmath33 respectively . in each case",
    "the values of all @xmath130 and @xmath99 coefficients are chosen to maximize the obtained fault tolerance .",
    "we will refer to the 8bits and 16bits rapidraid implementations as _",
    "rr8 _ and _ rr16 _ respectively .    in the case of _",
    "rr8 _ the use of a small finite field makes it very difficult to find coefficient values guaranteeing the absence of accidental linear dependencies . in this case , the 8bit ( 16,11 ) rapidraid implementation achieves data reliability values slightly lower than the ones depicted in table  [ t : reliab ] . despite this lower reliability ,",
    "we include the 8bit implementation in our evaluation to show the effects that the word size has in coding times .",
    "note that our rapidraid implementation also includes a fast pipelined decoding mechanism that is not discussed here because of space restrictions .",
    "we evaluate the three coding settings , _ cec _ , _ rr8 _ and _ rr16 _ in both a small cluster of 50 hp t5745 thinclient computers , and a set of 16 small instances in the amazon ec2 cloud computing service .",
    "we will refer to the thinclient and amazon ec2 testbed as _",
    "tpc _ and _ ec2 _ respectively .",
    "finally , in all the experiments we assume that the size of all the @xmath131 original blocks is of 64 mb , which is the default block size in gfs and hdfs  @xcite .",
    "it means that the size of the original object to be stored is of 704 mb ( 11@xmath264 mb ) , and the final erasure encoded object takes 1024 mb ( 16@xmath264 mb ) , which represents a storage overhead of approximately @xmath132 the size of the original data .",
    "m4cmccc & * cec * & * rr8 * & * rr16 * + intel atom ( n280 ) 1.66ghz ; 512 kb cache & 17.81 & 5.06 & 27.33 + intel xeon ( e5645 ) 2.40ghz ; 12,288 kb cache & 5.20 & 3.50 & 4.31 + intel core2 quad ( q9400 ) 2.66ghz ; 3,072 kb cache & 4.13 & 1.47 & 1.95 +    before evaluating coding times we will measure the overall computing requirements of the three evaluated codes .",
    "this metric is of special interest in datacenters where an archiving process requiring little overall computing resources is preferred due to the low interference it has on the normal datacenter operations .    to measure the overall computing requirements of the _ cec _ implementation",
    "we execute an encoding process where the @xmath131 original blocks and the @xmath133 parity are all stored in the local file system , avoiding all the network i / o . in that case",
    "the encoding time corresponds basically to the time the cpu is dedicated to execute the coding operations .",
    "similarly , to measure the overall computing requirements of the _ rr8 _ and _ rr16 _ implementations , we run an encoding process where the execution of the @xmath134 nodes occur in a single node , avoiding also all the network i / o .    in table  [ t : cpus ] we depict the average encoding time of the three encoding implementations when all the computing is executed in a single node and no network communication is involved .",
    "we show the results for three different cpus .",
    "the first case ( intel atom ) corresponds to the execution time in the thinclient computers , the second case ( intel xeon ) is an amazon ec2 _ small instance _ , and the last one ( intel core2 ) a personal desktop computer .",
    "except in the case of atom , both rapidraid implementations require less cpu time to encode the same amount of data ( i.e. , 704 mb ) than the _ cec _ implementation . in the case of the atom cpu , due to the small size of the cache memory ,",
    "the jerasure library can not allocate the whole lookup table required to perform @xmath33 arithmetic , which increases _",
    "rr16 _ coding times as compared to _",
    "we observe that rapidraid codes can be computed faster than even one of the fastest implementation of classical erasure codes , and thus its impact on cpu usage is favorable .",
    "[ f : single ] [ f : parallel ]    in fig .",
    "[ f : generaltimes ] we measure the encoding times of the three different implementations for a single object encoding , as well as multiple object encodings .    in fig .",
    "[ f : single ] a single data object is encoded in a totally idle system .",
    "we see how the two rapidraid implementations have of the order of 90% shorter coding times as compared to the classical erasure code implementation . in this case , by distributing the network and computing load of the encoding process across 16 different nodes , rapidraid",
    "codes significantly speed up the single data object s archival process .",
    "however , this speedup is obtained at the expense of involving 16 nodes in the encoding process .",
    "it is then interesting to measure the encoding throughput of a classical erasure code involving the same number of nodes , i.e. , when 16 encoding process are executed in parallel . in fig .",
    "[ f : parallel ] we depict the per - object encoding times obtained by executing 16 concurrent classical encoding processes and 16 rapidraid encoding processes on a group of 16 nodes . in the _",
    "ec2 _ setting , the two rapidraid implementations achieve a reduction of the overall coding time by up to 20% . on the thinclients ,",
    "the 16bit rapidraid implementation requires around 50% longer coding times than classical erasure codes due to problems with the small cache size .      in practice",
    ", storage nodes might be executing other tasks concurrently with data archival processes , which might cause some nodes to experience network congestions that in turn might affect the coding times .",
    "although nodes in the _ ec2 _ setting are already virtual computers subjected to real network congestions , we needed to be able to arbitrary reduce the network capacity of some nodes to evaluate the potential effects that severe network congestions can have on rapidraid coding times .",
    "to evaluate such effects of congestion , we use the linux _ netem _ driver to introduce arbitrary congestions in our cluster of thinclients .",
    "specifically , we use _ netem _ to reduce the network bandwidth of some nodes from 1gbps to 500mbps , and add to these nodes a 100ms network latency ( with a deviation of up to @xmath135ms ) .    in fig .",
    "[ f : cong ] we depict the effects that different network congestion levels have in coding times of the _ cec _ and _ rr8 _ implementations .",
    "note that we only use the 8bits rapidraid implementation due to the impossibility to run efficient @xmath33 arithmetic in the thinclient cluster . in fig .",
    "[ f : congsingle ] we show the time required to encode a single object . in the case of rapidraid codes , coding times have a quasi - linear behavior when the number of congested nodes increases .",
    "however , in the case of classical erasure codes , we can see how a single congested node has major impacts to the coding times .",
    "similarly , in fig .  [",
    "f : congparallel ] we depict the per - object coding times of 16 concurrently encoded objects .",
    "compared with the single object coding time , the presence of a single congested node has even more impact on the coding times of classical erasure codes . in general these results",
    "show how classical erasure codes have a worse resilience to congested networks than rapidraid codes .",
    "error bars depict the standard deviation value.,title=\"fig:\"][f : congsingle ] 10ms .",
    "error bars depict the standard deviation value.,title=\"fig:\"][f : congparallel ]",
    "despite widespread use of erasure coding for archiving data in distributed storage systems , existing literature does not explore the process of migration from replication based redundancy to erasure code based redundancy .",
    "we thus discuss some peripherally related works .",
    "the most relevant related work is that of fan et al .",
    "@xcite , who propose to distribute the task of erasure coding using the hadoop infrastructure , as mapreduce tasks .",
    "any individual object is however encoded at a single node , and hence the parallelism achieved in their approach is only at the granularity of individual data objects .",
    "we note from our experiments that distributing the individual encoding tasks provide further performance benefits .",
    "decentralized erasure coding has also been explored in the context of sensor networks  @xcite .",
    "however , in such a setting , the ( disjoint ) data generated by @xmath3 sensors is jointly stored over @xmath136 storage sensors based on erasure coding redundancy .",
    "this is achieved using network coding techniques , and is relatively straight forward to achieve , since random linear combinations of the already distributed data needs to be stored over the additional nodes .",
    "such a technique is inapplicable for the problem considered in this paper .",
    "li et al .",
    "@xcite also used a similar pipelining based encoding strategy over a tree - structured topology to reduce the traffic required to repair lost redundancy .",
    "redundancy replenishment is a very important and vigorously researched topic  @xcite , however , as noted previously , it is an unrelated problem .",
    "in this paper we introduced a novel pipelined erasure coding strategy to speedup the archival of data in distributed storage systems .",
    "we also presented rapidraid , an explicit family of erasure codes that realizes the idea of pipelined erasure coding without compromising either data reliability or storage overheads .",
    "in particular , we showed that for equivalent storage overhead , rapidraid codes can achieve a fault tolerance similar to that of existing erasure codes , and higher than replicated systems .",
    "finally , we presented a real implementation of rapidraid codes , and experiments with real system benchmarks demonstrate the efficacy of our proposed solution . for coding a single object ,",
    "our approach achieved up to 90% reduction in time , while even when multiple objects are encoded , our approach is up to 20% faster than distributing classical erasure coding tasks for different objects .",
    "the benefits of rapidraid codes are visible even when part of the network is congested , where rapidraid codes enable shorter coding times and have a better scalability as compared to existing erasure codes when the network congestion increases .",
    "the current implementation source code is available at .",
    "the design of pipelined erasure coding based rapidraid codes is an important step towards more efficient mechanisms to archive `` big - data '' in distribute storage systems . as part of our future research ,",
    "we aim to explore the performance of rapidraid codes under different choice of code parameters @xmath3 and @xmath21 .",
    "it is specially challenging for large values of @xmath21 where numerical evaluation of the fault tolerance becomes intractable .",
    "we also aim to explore how rapidraid codes can be generalized to exploit the existence of more than two replicas , and particularly for the special case of three replicas , which is the de facto redundancy scheme used in most production systems .              c.  huang , h.  simitci , y.  xu , a.  ogus , b.  calder , p.  gopalan , j.  li , and s.  yekhanin , `` erasure coding in windows azure storage , '' in _ proceedings of the usenix annual technical conference ( atc ) _ , 2012 .",
    "a.  thusoo , z.  shao , s.  anthony , d.  borthakur , n.  jain , j.  sen  sarma , r.  murthy , and h.  liu , `` data warehousing and analytics infrastructure at facebook , '' in _ proceedings of the 2010 acm sigmod international conference on management of data _ , ser .",
    "sigmod 10 , 2010 .",
    "d.  ford , f.  labelle , f.  i. popovici , m.  stokely , v .- a .",
    "truong , l.  barroso , c.  grimes , and s.  quinlan , `` availability in globally distributed storage systems , '' in _ the 9th usenix conference on operating systems design and implementation ( osdi ) _",
    ", 2010 .",
    "a.  thusoo , z.  shao , s.  anthony , d.  borthakur , n.  jain , j.  sen  sarma , r.  murthy , and h.  liu , `` data warehousing and analytics infrastructure at facebook , '' in _ proceedings of the 2010 international conference on management of data ( sigmod ) _ , 2010 .",
    "l.  x. bin  fan , wittawat  tantisiriroj and g.  gibson , `` diskreduce : replication as a prelude to erasure coding in data - intensive scalable computing , '' carnegie mellon univsersity , parallel data laboratory , tech .",
    "technical report cmu - pdl-11 - 112 , 2011 .",
    "l.  pamies - juarez , p.  garca - lpez , m.  snchez - artigas , and b.  herrera , `` towards the design of optimal data redundancy schemes for heterogeneous cloud storage infrastructures , '' _ computer networks _ , vol .",
    "55 , no .  5 , pp .",
    "11001113 , 2011 .",
    "j.  li , s.  yang , x.  wang , and b.  li , `` tree - structured data regeneration in distributed storage systems with regenerating codes , '' in _ the 29th ieee intl .",
    "conference on computer communications ( infocom ) _",
    ", 2010 .",
    "x.  li , m.  lillibridge , and m.  uysal , `` reliability analysis of deduplicated and erasure - coded storage , '' in _ proceedings of the workshop on hot topics in measurement modeling of computer systems ( hotmetrics ) _ , 2010 .          j.  s. plank , s.  simmerman , and c.  d. schuman , `` jerasure : a library in c / c++ facilitating erasure coding for storage applications , '' university of tennessee , tech",
    "technical report cs-08 - 627 , 2008 .    j.  s. plank , j.  luo , c.  d. schuman , l.  xu , and z.  wilcox - ohearn , `` a performance evaluation and examination of open - source erasure coding libraries for storage , '' in _ proccedings of the 7th conference on file and storage technologies ( fast ) _ , 2009 ."
  ],
  "abstract_text": [
    "<S> to achieve reliability in distributed storage systems , data has usually been replicated across different nodes . </S>",
    "<S> however the increasing volume of data to be stored has motivated the introduction of erasure codes , a storage efficient alternative to replication , particularly suited for archival in data centers , where old datasets ( rarely accessed ) can be erasure encoded , while replicas are maintained only for the latest data . </S>",
    "<S> many recent works consider the design of new storage - centric erasure codes for improved repairability . </S>",
    "<S> in contrast , this paper addresses the _ migration _ from replication to encoding : traditionally erasure coding is an _ atomic operation _ in that a single node with the whole object encodes and uploads all the encoded pieces . </S>",
    "<S> although large datasets can be concurrently archived by distributing individual object encodings among different nodes , the network and computing capacity of individual nodes constrain the archival process due to such atomicity .    </S>",
    "<S> we propose a new _ pipelined coding strategy _ that distributes the network and computing load of single - object encodings among different nodes , which also speeds up multiple object archival . </S>",
    "<S> we further present _ rapidraid codes _ , an explicit family of pipelined erasure codes which provides fast archival without compromising either data reliability or storage overheads . </S>",
    "<S> finally , we provide a real implementation of rapidraid codes and benchmark its performance using both a cluster of 50 nodes and a set of amazon ec2 instances . </S>",
    "<S> experiments show that rapidraid codes reduce a single object s coding time by up to 90% , while when multiple objects are encoded concurrently , the reduction is up to 20% .    </S>",
    "<S> archival , migration , erasure codes , distributed storage </S>"
  ]
}