{
  "article_text": [
    "during the last decades , convex optimization methods have been shown to be very effective for solving inverse problems . on the one hand ,",
    "algorithms such as projection onto convex sets ( pocs ) @xcite have become popular for finding a solution in the intersection of convex sets .",
    "pocs was used in data recovery problems @xcite in order to incorporate prior information on the target image ( e.g. smoothness constraints ) .",
    "some variants of pocs such as art ( algebraic reconstruction technique ) @xcite or ppm ( parallel projection method ) @xcite were also proposed to achieve iteration parallelization .",
    "additional variants of pocs can be found in @xcite .",
    "other parallel approaches such as block - iterative surrogate constraint splitting methods were considered to solve a quadratic minimization problem under convex constraints @xcite which may include a total variation constraint ( see also @xcite ) . however",
    "the method in @xcite based on subgradient projections is not applicable to non - differentiable objective functions .",
    "on the other hand , some denoising approaches were based on wavelet transforms @xcite , and more generally on frame representations @xcite . in @xcite ,",
    "algorithms which belong to the class of forward - backward algorithms were proposed in order to restore images degraded by a linear operator and a noise perturbation .",
    "forward - backward iterations allow us to minimize a sum of two functions assumed to be in the class @xmath0 of lower semicontinuous convex functions defined on a hilbert space @xmath1 , and taking their values in @xmath2-\\infty,+\\infty]$ ] , which are proper ( i.e. not identically equal to @xmath3 ) .",
    "in addition , one of these functions must be lipschitz differentiable on @xmath1 . in @xcite ,",
    "this algorithm was investigated by making use of proximity operator tools @xcite firstly proposed by moreau in @xcite . in @xcite , applications to frame representations were developed and a list of closed form expressions of several proximity operators was provided . typically , forward - backward methods are appropriate when dealing with a smooth data fidelity term e.g. a quadratic function and a non - smooth penalty term such as an @xmath4-norm promoting sparsity in the considered frame @xcite .",
    "the computation of the proximity operator associated with the @xmath4-norm indeed reduces to a componentwise soft - thresholding @xcite .",
    "another optimization method known as the douglas - rachford algorithm @xcite was then proposed for signal / image recovery problems @xcite to relax the lipschitz differentiablity condition required in forward - backward iterations . in turn",
    ", the latter algorithm requires the knowledge of the proximity operators of both functions .",
    "this algorithm was then extended to the minimization of a sum of a finite number of convex functions @xcite , the proximity operator of each function still being assumed to be known .",
    "one of the main advantages of this algorithm called parallel proximal algorithm ( ppxa ) is its parallel structure which makes it easily implementable on multicore architectures .",
    "ppxa is well suited to deconvolution problems in the presence of additive gaussian noise , where the proximity operator associated with the fidelity term takes a closed form @xcite . to minimize a sum of two functions one of which is quadratic ,",
    "another interesting class of parallel convex optimization algorithms was proposed by fornasier et al . in @xcite . however , in a more general context , particularly when the noise is not additive gaussian and a wider class of degradation operators is considered , the proximity operator associated with data fidelity term does not have a closed form , which prevents the direct use of ppxa and the algorithms in @xcite and @xcite .",
    "therefore , other solutions have to be looked for . for poisson noise ,",
    "a first solution is to resort to the anscombe transform @xcite , while a second one consists of approximating the poisson data fidelity term with a gradient lipschitz function @xcite .",
    "both approaches require the use of a nested iterative algorithm @xcite , combining forward - backward and douglas - rachford iterations .",
    "nested algorithms may however appear limited for two main reasons : the parallelization of the related iterations is difficult , and the number of functions to be minimized is in practice limited to three .",
    "more recently , approaches related to augmented lagrangian techniques @xcite have been considered in @xcite .",
    "these methods are well - adapted when the linear operator is a convolution and fourier diagonalization techniques can thus be used , but for more general linear degradation operators , a large - size linear system of equations has to be solved numerically at each iteration of the algorithm .    the objective of this paper is to propose an adaptation of ppxa to minimize criteria used in a wide panel of restoration problems such as those involving a convolution or decimated convolution operator using a finite - support kernel and non - necessarily additive gaussian noise .",
    "decimated convolutions are important in practice since they are often encountered in super - resolution problems .",
    "to apply proximal methods , it seems that we should be able to compute the proximity operator associated with the fidelity term for a large class of noise distributions . when the proximity operator can not be easily computed",
    ", we will show that a splitting approach may often be employed to circumvent this difficulty .",
    "this is the main contribution of this paper .",
    "moreover , based on similar splitting techniques and in the spirit of existing works on deconvolution in the presence of gaussian noise @xcite , a twofold regularization composed of a sparsity term and a total variation term is performed in order to benefit from each regularization .",
    "we will consider this type of hybrid regularization by investigating different discrete forms of the total variation .",
    "the paper is organized as follows : first , in section  [ sec : context ] , we present the considered restoration problem and the general form of the associated criterion to be minimized . then , in section  [ sec : prox ] , the definition and some properties of proximity operators as well as explicit forms related to the data fidelity term in a restoration context and to a discretization of the total variation are provided .",
    "section  [ sec : algo ] introduces an accelerated version of ppxa which allows us to efficiently solve frame - based image recovery problems .",
    "section  [ se : hybrid ] shows how the results obtained in the two previous sections can be used for solving restoration problems where a regularization is performed both in the spatial and in the wavelet domains .",
    "finally , in section  [ sec : res ] , the effectiveness of the proposed approach is demonstrated by experiments for the restoration of images degraded by a blur ( or a decimated blur ) with finite - support kernel and by a poisson noise . some conclusions are drawn in section  [ se : conclu ] .",
    "the degradation model considered throughout this paper is the following : @xmath5 where @xmath6 denotes the original image of global size @xmath7 degraded by a non - negative valued convolutive operator @xmath8 and contaminated by a noise non necessarily additive , the effect of which is denoted by @xmath9 . here",
    ", @xmath10 is a positive parameter which characterizes the noise intensity .",
    "the vector @xmath11 represents the observed data of size @xmath12 . for example",
    ", @xmath13 may denote the addition of a zero - mean laplacian noise with standard - deviation @xmath10 , or the corruption by an independent poisson noise with scaling parameter @xmath10 .",
    "@xmath14 represents a convolution or a decimated convolution operator using a finite - support kernel .",
    "our objective is to recover the image @xmath6 from the observation @xmath15 by using some prior information on its frame coefficients and its spatial properties .      in inverse problems , certain physical properties of the target solution @xmath6",
    "are most suitably expressed in terms of the coefficients @xmath16 of its representation @xmath17 with respect to a family of vectors @xmath18 in the euclidean space @xmath19 .",
    "recall that a family of vectors @xmath18 in @xmath19 constitutes a frame if there exist two constants @xmath20 and @xmath21 in @xmath220,+\\infty\\right[}}$ ] such that @xmath23 the associated frame operator is the injective linear operator @xmath24 , the adjoint of which is the surjective linear operator @xmath25 .",
    "when @xmath26 in , @xmath18 is said to be a tight frame . in this case , we have    @xmath27    where @xmath28 denotes the identity matrix .",
    "a simple example of a tight frame is the union of @xmath29 orthonormal bases , in which case @xmath30 .",
    "for instance , a 2d real ( resp .",
    "complex ) dual - tree wavelet decomposition is the union of two ( resp .",
    "four ) orthonormal wavelet bases @xcite .",
    "curvelets @xcite constitute another example of tight frame .",
    "historically , gabor frames @xcite have played an important role in many inverse problems . under some conditions ,",
    "contourlets @xcite also constitute tight frames . when @xmath31 , an orthonormal basis is obtained .",
    "further constructions as well as a detailed account of frame theory in hilbert spaces can be found in @xcite .    in such a framework ,",
    "the observation model becomes @xmath32 where @xmath33 represents the frame coefficients of the original data ( @xmath34 is the target data of size @xmath7 ) .",
    "our objective is now to recover @xmath33 from the observation @xmath15 .      in the context of inverse problems ,",
    "the original image can be restored by solving a convex optimization problem of the form : @xmath35 where @xmath36 are functions of @xmath37 ( see @xcite and references therein ) and the restored image is @xmath38 .",
    "a particular popular case is when @xmath39 ; the minimization problem thus reduces to the minimization of the sum of two functions which , under a bayesian framework , can be interpreted as a fidelity term @xmath40 linked to noise and an a priori term @xmath41 related to some prior probabilistic model put on the frame coefficients ( some examples will be given in section  [ sec : problem ] ) .    in this paper",
    ", we are especially interested in the case when @xmath42 , which may be fruitful for imposing additional constraints on the target solution . at the same time ,",
    "when considering a frame representation ( which , as already mentioned , often allows us to better express some properties of the target solution ) , the convex optimization problem can be re - expressed as : @xmath43 where @xmath44 are functions of @xmath45 and @xmath46 are functions of @xmath37 , related to the image or to the frame coefficients , respectively .",
    "the terms for @xmath47 related directly to the pixel values may be the data fidelity term , or a pixel range constraint term , whereas , the functions of indices @xmath48 defined on frame coefficients are often chosen from some classical prior probabilistic model .",
    "for example , they may correspond to the minus log - likelihood of independent variables following generalized gaussian distributions @xcite .",
    "we will now present convex analysis tools which are useful to deal with such minimization problems .",
    "a fundamental tool which has been widely employed in the recent convex optimization literature is the proximity operator @xcite first introduced by moreau in 1962 @xcite .",
    "the proximity operator of @xmath49 is defined as @xmath50 thus , if @xmath51 is a nonempty closed convex set of @xmath52 , and @xmath53 denotes the indicator function of @xmath51 , _",
    "i.e. _ , @xmath54 , @xmath55 if @xmath56 , @xmath3 otherwise , then , @xmath57 reduces to the projection @xmath58 onto @xmath51 .",
    "other examples of proximity operators corresponding to the potential functions of standard log - concave univariate probability densities have been listed in @xcite .",
    "some of them will be used in the paper and we will thus recall the proximity operators of the potentials associated with a gamma distribution ( which is closely related to the kullback - leibler divergence @xcite ) and with a generalized gaussian distribution , before dealing with the euclidean norm in dimension 2 .",
    "@xcite [ ex : gamd ] let @xmath59 and set @xmath60-\\infty,+\\infty\\right]}}\\nonumber \\\\",
    "& \\eta\\mapsto \\begin{cases } -\\chi\\ln(\\eta)+\\alpha\\eta,&\\text{if}\\;\\;\\chi>0\\;\\;\\text{and}\\;\\;\\eta>0;\\\\ \\alpha\\eta,&\\text{if}\\;\\;\\chi=0\\;\\;\\text{and}\\;\\;\\eta\\ge 0;\\\\ { \\ensuremath{+\\infty } } , & \\text{otherwise}. \\end{cases}\\end{aligned}\\ ] ] then , for every @xmath61 , @xmath62    @xcite [ ex : gg ] let @xmath63 , @xmath64 , and set @xmath65-\\infty,+\\infty\\right]}}\\colon\\eta\\mapsto \\chi |\\eta|^p.\\ ] ] then , for every @xmath61 , @xmath66 is given by @xmath67    where @xmath68 denotes the signum function .",
    "in example [ ex : gg ] , it can be noticed that the proximity operator associated with @xmath69 reduces to a soft thresholding .",
    "@xcite [ ex : sqrt ] let @xmath70 and set @xmath71 then , for every @xmath72 , @xmath73      we will now study the problem of determining the proximity operator of a function @xmath74 where @xmath75 is a linear operator , @xmath76-\\infty,+\\infty\\right]}}\\!\\colon\\ !",
    "( u^{(m)})_{1 \\le m \\le m}\\mapsto \\sum_{m=1}^m \\psi_m(u^{(m)})\\ ] ] and , for every @xmath77 , @xmath78 .",
    "as will be shown next , the proximity operator of this function can be determined in a closed form for specific cases only .",
    "however , @xmath79 can be decomposed as a sum of functions for which the proximity operators can be calculated explicitly .",
    "firstly , we introduce a property concerning the determination of the proximity operator of the composition of a convex function and a linear operator , which constitutes a generalization of ( * ? ? ?",
    "* proposition 11 ) for separable convex functions .",
    "the proof of the following proposition is provided in appendix [ ap : proxcomp ] .",
    "[ p : proxcomp ] let @xmath80 , @xmath81 , and let @xmath82 be an orthonormal basis of @xmath83 .",
    "let @xmath84 be a function such that @xmath85 where @xmath86 are functions in @xmath87 .",
    "let @xmath88 be a matrix in @xmath89 such that @xmath90 where @xmath91 is a sequence of positive reals .",
    "+ then @xmath92 and , for every @xmath93 @xmath94 where @xmath95 is the function defined by @xmath96    the function @xmath97 defined in is separable in the canonical basis of @xmath98",
    ". however , for an abitrary convolutive ( or decimated convolutive ) operator @xmath99 , is generally not satisfied .",
    "nevertheless , assume that @xmath100 is a partition of @xmath101 in nonempty sets .",
    "for every @xmath102 , let @xmath103 be the number of elements in @xmath104 ( @xmath105 ) and let @xmath1060,+\\infty\\right[}}\\,:\\,(u^{(m)})_{m\\in { \\ensuremath{\\mathbb i}}_i }   \\mapsto \\sum_{m\\in { \\ensuremath{\\mathbb i}}_i } \\psi_m(u^{(m)})$ ] . if , for every @xmath107 , @xmath108 is the vector of @xmath19 corresponding to the @xmath109-th row vector of @xmath14 , we have then @xmath110 where @xmath111 is a linear operator from @xmath19 to @xmath112 associated with a matrix @xmath113 and @xmath114 .",
    "the following assumption will play a prominent role in the rest of the paper :    [ a : parti ] for every @xmath102 , @xmath115 is a family of non zero orthogonal vectors .",
    "then , @xmath79 can be decomposed as a sum of @xmath116 functions @xmath117 where , for every @xmath102 , @xmath118 is associated with an invertible diagonal matrix @xmath119 . according to proposition  [ p : proxcomp ] , we have then , for every @xmath120 , @xmath121    1 .",
    "note that assumption [ a : parti ] is obviously satisfied when @xmath122 , that is when , for every @xmath102 , @xmath104 reduces to a singleton .",
    "it can be noticed that the application of @xmath111 or @xmath123 reduces to standard operations in signal processing .",
    "for example , when @xmath14 corresponds to a convolutive operator , the application of @xmath111 consists of two steps : a convolution with the impulse response of the degradation filter and a decimation for selected locations ( @xmath124 ) .",
    "the application of @xmath123 also consists of two steps : an interpolation step ( by inserting zeros between data values of indices @xmath124 ) followed by a convolution with the filter with conjugate frequency response .",
    "the fundamental idea behind the previously introduced partition @xmath125 , is to form groups of non - overlapping  and thus orthogonal",
    " shifts of the convolution kernel so as to be able to compute the corresponding proximity operators . to reduce the number of proximity operators to be computed , one usually wants to find the smallest integer @xmath116 such that , for every @xmath102 , @xmath115 is an orthogonal family . for the sake of simplicity",
    ", we will consider the case of a 1d deconvolution problem , where @xmath7 represents the original signal size whereas @xmath12 corresponds to the degraded signal size , the extension to 2d deconvolution problems being straightforward .",
    "different configurations concerning the impact of boundary effects on the convolution operator will be studied : first , we will consider the case when no boundary effect occurs .",
    "then , boundary effects introduced by zero padding and by a periodic convolution will be taken into account .",
    "finally , the special case of decimated convolution will be considered .",
    "@xmath126 designates in the sequel the length of the kernel and @xmath127 its values .    1 .   one - dimensional convolutive models without boundary effect .",
    "+ we typically have the following tplitz structure : where @xmath128 . + in order to satisfy assumption  [ a : parti ] , we can choose @xmath129 and , for every @xmath130 , @xmath131 hence , we have for all @xmath130 , @xmath132 in this case , @xmath79 can be decomposed as a sum of @xmath126 functions , whose proximity operators can be easily calculated .",
    "2 .   one - dimensional zero - padded convolutive models .",
    "+ the following tplitz matrix is considered : where @xmath133 . in this case , @xmath116 can be chosen equal to @xmath126 and the index sets @xmath100 are still given by .",
    "however , the diagonal parameters are not all equal as in the previous example .",
    "we have indeed , for every @xmath130 , @xmath134 3 .",
    "one - dimensional periodic convolutive models .",
    "+ in this case , a matrix having a circulant structure @xcite is involved : where @xmath135 . in order to satisfy assumption  [ a : parti ] , we subsequently set @xmath136 and , for every @xmath102 , @xmath137 + the diagonal parameters are then given by .",
    "+ another choice which was made in @xcite is to set @xmath138 and to proceed as in and .",
    "this solution may be preferred due to its simplicity , when the resulting value of @xmath116 is small .",
    "4 .   one - dimensional @xmath139-decimated zero - padded convolutive models .",
    "+ we get the following matrix of @xmath140 where @xmath141 .    * in order to satisfy assumption  [ a : parti ] , we subsequently set @xmath142 and the index sets @xmath100 are still given by .",
    "we have indeed , for every @xmath130 , @xmath143 note that , when @xmath144 , @xmath145 is an orthogonal family , and thus @xmath146 .    in the previous example ( the non - decimated example being a special case when",
    "@xmath147 ) , the computational complexity of applying each operator @xmath111 or @xmath123 with @xmath102 is @xmath148 and we have about @xmath149 proximity operators @xmath150 to compute . assuming a complexity @xmath151 for computing @xmath152 , the overall computational complexity is @xmath153 . in turn , if we choose @xmath122 , the complexity of computation of @xmath111 or @xmath123 is @xmath154 , but we have about @xmath12 proximity operators @xmath150 to compute .",
    "thus , the overall computational complexity remains of the same order as previously .",
    "this means that limiting the number of proximity operators to be computed has no clear advantage in terms of computational complexity , but it allows us to reduce the memory requirement ( gain of a factor @xmath155 for the storage of the results of the proximity operators ) .",
    "total variation @xcite represents a powerful regularity measure in image restoration for recovering piecewise homogeneous areas with sharp edges @xcite .",
    "different versions of discretized total variation can be found in the literature @xcite .",
    "our objective here is to consider discrete versions for which the proximity operators can be easily computed .",
    "the main idea will be to split the total variation term in a sum of functions the proximity operators of which have a closed form .",
    "the considered form of the total variation of a digital image @xmath156 is @xmath157 where @xmath158 , and @xmath159 and @xmath160 are two discrete gradients computed in orthogonal directions through fir filters with impulse responses of size @xmath161 .",
    "more precisely , in the above expression , we have @xmath162 where @xmath163 and @xmath164 are the filter kernel matrices here assumed to have unit frobenius norm , and for every @xmath165 , @xmath166 denotes a block of @xmath161 neighbouring pixels .",
    "since the proximity operator associated with the so - defined total variation does not take a simple expression in general , can be split in `` block terms '' by following an approach similar to that in section  [ ss : proxconv ] : @xmath167 where , for every @xmath168 and @xmath169 , @xmath170 and the notation @xmath171 has been used .",
    "a closed form expression for the proximity operator of the latter function can be derived as shown below ( the proof is provided in appendix  [ ap : proxtv ] ) .",
    "[ p : proxtv ] under the assumption that @xmath172 , for every @xmath173 and @xmath174 , we have @xmath175 where , for every @xmath176 , @xmath177 with @xmath178 and @xmath179 @xmath180    the result in proposition  [ p : proxtv ] basically means that , for a given value of @xmath181 , the image is decomposed into non - overlapping blocks @xmath182@xmath183 of @xmath161 pixels .",
    "then provides the expression of the proximity operator associated with each one of these blocks , whereas deals with boundary effects .",
    "the above result offers some degrees of freedom in the definition of the discretized total variation for the choices of the function @xmath184 and of the gradient filters .    *",
    "two classical choices for the function @xmath184 @xcite are the following : 1 .",
    "if @xmath185 then , an anisotropic form is obtained .",
    "according to example [ ex : gg ] , reduces to @xmath186 2 .   if @xmath187 , then the standard isotropic form is found .",
    "the proximity operator involved in is given in example [ ex : sqrt ] .",
    "* some examples of kernel matrices @xmath188 and @xmath189 satisfying the assumptions of proposition [ p : proxtv ] are as follows : 1 .",
    "roberts filters such that @xmath190 and @xmath191 were investigated in @xcite .",
    "2 .   finite difference filters can be used , which are such that @xmath192 .",
    "prewitt filters also satisfy the required assumptions .",
    "they are defined by + @xmath193 .",
    "4 .   sobel filters such that + @xmath194 are possible choices too .",
    "in the class of convex optimization methods , an algorithm recently proposed in @xcite appears well - suited to solve the class of the minimization problems formulated as in problem  .",
    "however , when synthesis frame representations are considered ( problem  ) and when the function number @xmath195 is large , the frame analysis and synthesis operators have to be applied several times in the algorithm which induces a long computation time . in this section , we briefly recall the parallel proximal algorithm and its convergence properties .",
    "then , we propose an improved version of ppxa to efficiently solve problem",
    ".      an equivalent formulation of the convex optimization problem   is : @xmath196 this formulation was used in @xcite to derive algorithm  [ algo : ppa ] .    set @xmath1970,+\\infty\\right[}}$ ] .",
    "for every @xmath198 , set @xmath1990,1]^j$ ] such that @xmath200 .",
    "set @xmath201 and @xmath202 .",
    "@xmath203 0,2 [ \\\\ \\mbox{for } \\ ; j=1,\\ldots , j \\\\",
    "\\lfloor \\quad u_{j,\\ell+1 } = u_{j,\\ell } + \\lambda_\\ell \\ ; ( 2\\ ; p_\\ell - x_\\ell -p_{j,\\ell } ) \\\\ x_{\\ell+1 } = x_\\ell + \\lambda_\\ell   \\;(p_\\ell -x_\\ell )    \\end{array } \\right.$ ]    ppxa involves real constants @xmath204 and @xmath205 , and , at each iteration @xmath206 , a relaxation parameter @xmath207 .",
    "it also includes possible error terms @xmath208 in the computation of the proximity operators , which shows the numerical stability of the algorithm .",
    "the sequence @xmath209 generated by algorithm [ algo : ppa ] can be shown to converge to a solution to problem   ( or equivalently to problem  ) under the following assumption @xcite .       1",
    ".   [ a:1 ] @xmath210 .",
    "[ a:2 ] @xmath211 . of @xmath52",
    "is designated by @xmath212 and the domain of a function @xmath213-\\infty,+\\infty]$ ] is @xmath214 . ]",
    "[ a:3 ] @xmath215 .",
    "[ a:4 ] @xmath216 .",
    "[ ass : conv_spingarn ]    the fact that the algorithm involves several parameters should not be viewed as a weakness since the convergence is guaranteed for any choice of these parameters under the previous assumption .",
    "these parameters bring out flexibility in ppxa in the sense that an appropriate choice of them ( typical values will be indicated in section  [ sec : res ] ) may be beneficial to the convergence speed .",
    "consider now problem where a tight frame is employed ( @xmath217 ) . by setting @xmath218 @xmath219 and by invoking proposition [ p : proxcomp ] with @xmath220 and @xmath221 ,",
    "the iterations of algorithm  [ algo : ppa ] become as described in algorithm  [ algo : ppa_frame ] .",
    "@xmath222 0,2[\\\\ \\mbox{for } \\;j=1,\\ldots ,",
    "j\\\\ \\lfloor   \\quad u_{j,\\ell+1 } = u_{j,\\ell } + \\lambda_\\ell\\ ; ( 2\\ ; p_\\ell - x_\\ell -p_{j,\\ell})\\\\ x_{\\ell+1 } = x_\\ell + \\lambda_\\ell(p_\\ell -x_\\ell )   \\end{array } \\right.$ ]    however , the first loop can be costly in terms of computational complexity because it requires to apply @xmath223 times the operators @xmath224 and @xmath225 at each iteration .",
    "we will now see how it is possible to speed up these iterations .",
    "in algorithm [ algo : accppa ] , we propose an adaptation of ppxa in order to reduce its computational load by limiting the number of times the operators @xmath224 and @xmath225 are applied .",
    "details concerning the derivation of this algorithm can be found in appendix [ algop : ppa_acc ] .",
    "let @xmath1970,+\\infty\\right[}}$ ] . for every @xmath198 ,",
    "set @xmath1990,1]^j$ ] such that @xmath200 . set @xmath201 and @xmath226 . for every @xmath227 , set @xmath228 and @xmath229 . @xmath230",
    "0,2[\\\\          \\mbox{for } \\;j=1,\\ldots,{\\ensuremath{s}}\\\\          \\left\\lfloor          \\begin{tabular}{c }          $ ] u_j,+1^ = u_j,^",
    "+ _ ( r_^ -u_j,^)@xmath231v_j,+1 = v_j , + _ ( _ - q_j,)@xmath232    let us make the following assumption :     [ as : errorterms ]    1 .",
    "[ a:1acc ] @xmath233 .",
    "[ a:2acc ] @xmath234 .",
    "[ a:3acc ] @xmath235 and @xmath236 .",
    "[ a:4acc ] @xmath216 .",
    "then , algorithm [ algo : accppa ] converges to a solution to problem .",
    "in addition , this algorithm requires only 3 applications of @xmath224 or @xmath225 at each iteration .",
    "hence , a gain w.r.t .",
    "algorithm [ algo : ppa_frame ] is obtained as soon as @xmath237 .",
    "this fact will be illustrated by our simulation results in section  [ se : simspeed ] .",
    "in restoration problems , one of the terms in the criterion to be minimized usually is a fidelity term measuring some distance between the image degraded by the operator @xmath14 and the observed data @xmath15 .",
    "we will assume that this function takes the form @xmath238 where @xmath239 . in the case of data corrupted by a additive zero - mean white gaussian noise with variance",
    "@xmath10 , a standard choice for @xmath97 is a quadratic function such that @xmath240 .",
    "then , the associated proximity operator of @xmath79 can be computed explicitly ( see @xcite ) . in the case of data contaminated by an independent poisson noise with scaling parameter @xmath10 ,",
    "a standard choice is @xmath241 where @xmath242 is the generalized kullback - leibler divergence @xcite such that , @xmath243 and @xmath244 the proximity operator of @xmath97 can then be derived from example [ ex : gamd ] .",
    "concerning regularization functions , a standard choice of penalty function in the wavelet domain is : @xmath245 , @xmath246 where , for every @xmath247 , @xmath248 is a finite function of @xmath87 such that @xmath249 .",
    "power functions as in example  [ ex : gg ] are often chosen for @xmath250 ( see e.g. @xcite ) .",
    "the main problem with wavelet regularization is the occurence of some visual artefacts ( e.g. ringing artefacts ) , some of which can be reduced by increasing the redundancy of the representation .",
    "another popular type of regularization that can be envisaged consists of employing a total variation measure @xcite .",
    "its major drawback is the generation of staircase - like effects in the recovered images . to combine the advantages of both regularizations , we propose to : @xmath251 as already mentioned",
    ", @xmath252 corresponds to the regularization term operating in the wavelet domain .",
    "@xmath253 represents a discrete total variation term as defined by .",
    "finally , @xmath53 is the indicator function of a nonempty closed convex set @xmath51 of @xmath19 ( for example , related to support or value range contraints ) .",
    "this kind of objective function was also recently investigated in @xcite but the approach was restricted to the use of a quadratic data fidelity term and of a specific form of the total variation term .",
    "the non - negative real parameters @xmath254 and @xmath255 control the degree of smoothness in the wavelet and in the space domains , respectively .",
    "the main difficulty in applying algorithm [ algo : ppa ] to our restoration problem is that it requires to compute the proximity operators associated with each of the four terms in . in general ,",
    "closed forms of the proximity operators are known only for the indicator function @xmath53 and for @xmath252 @xcite . however , as explained in section [ ss : proxconv ] , provided that the function @xmath97 is separable , the data fidelity term can be decomposed as a sum of @xmath116 functions @xmath256 for which the proximity operators can be calculated according to .",
    "similarly , by using the results in section  [ ss : proxtv ] , the @xmath253 function can be split in @xmath257 functions @xmath258 , the proximity operators of which are given by proposition [ p : proxtv ] .",
    "algorithm  [ algo : accppa ] can then be applied with @xmath259 and @xmath260 . in the present case",
    ", it can be noticed that if @xmath261 , assumption [ as : errorterms ]  [ a:1 ] ) is satisfied .",
    "in addition , assumption [ as : errorterms ]  [ a:2 ] ) is fulfilled @xmath262 ( since @xmath263 and @xmath264 @xmath265 ) .",
    "this condition is verified if @xmath20,{\\ensuremath{+\\infty}}[^m\\subset{\\ensuremath{\\mathrm{dom}\\,}}\\psi$ ] and @xmath266^n$ ] since for every @xmath267 , @xmath111 has been assumed non - negative real valued in section [ se : restpb ] , and with non - zero lines ( see assumption  [ a : parti ] ) .      in our simulations",
    ", we will be first interested in studying the performance in terms of convergence rate of the accelerated version of ppxa .",
    "algorithms [ algo : ppa_frame ] and [ algo : accppa ] are implemented by setting @xmath268 , @xmath269 and , for every @xmath270 , @xmath271 if @xmath272 are the functions corresponding to the decomposition of the data fidelity term and@xmath273 correspond to the decomposition of @xmath253 .",
    "the weights are thus chosen to provide equal contributions to the four functions in criterion ( 42 ) .",
    "for the first and second functions which are splitted , the corresponding 1/4 weight is further subdivided in a uniform manner .",
    "note however that the behaviour of the algorithm did not appear to be very sensitive to an accurate choice of these parameters . a comparison between the different total variation regularization terms defined in section  [ ss : accppxa ] will also be made .",
    "another discussion will be held concerning the boundary effects .",
    "two cases will be considered : the use of a periodic convolution and then , of a convolution with zero - padding .",
    "results for a decimated convolution will also be presented .",
    "finally , the interest in combining total variation and wavelet regularization terms will be shown with respect to classical regularizations . a tight frame version of the dual - tree transform ( dtt ) proposed in @xcite ( @xmath274 ) using symlets of length 6 over 3 resolution levels is employed .",
    "we choose potential functions of the form : for every @xmath247 , @xmath275 where @xmath276 and @xmath277 , the proximity operators of which are given by example [ ex : gg ] .",
    "table [ tab : compvitesseppxa ] gives the iteration numbers and the cpu times for the original ppxa algorithm and the proposed accelerated one in order to reach convergence when considering different image sizes ( `` sebal '' : @xmath278 , `` peppers '' : @xmath279 and `` marseille '' : @xmath280 ) and various kernel blur sizes . the stopping criterion is based on the relative error between the objective function computed at the current iteration and at the previous one .",
    "the stopping tolerance has been set to @xmath281 .",
    "these results have been obtained with an intel core2 6700 , 2.66 ghz .",
    "the last line of table  [ tab : compvitesseppxa ] illustrates the gain in cpu - time when using algorithm [ algo : accppa ] .",
    "moreover , in figure  [ fig : compcpu ] , the mean square error on the image iterates @xmath282 is plotted as a function of computation time , where @xmath283 denotes the sequence generated by algorithm  [ algo : ppa_frame ] or algorithm  [ algo : accppa ] .",
    "[ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]",
    "a new convex regularization approach to restore data degraded by a ( possibly decimated ) convolution operator and a non necessarily additive noise has been proposed .",
    "the main advantages of the method are ( i ) to deal directly with the `` true '' noise likelihood ( i.e. the kullback - leibler divergence in the case of poisson noise ) without requiring any approximation of it ; ( ii ) to permit the use of sophisticated regularization functions , e.g. one promoting sparsity in a wavelet frame domain and a total variation penalization . in addition",
    ", the proposed algorithm has a parallel structure which makes it easily implementable on multicore architectures .",
    "numerical and visual results demonstrate the effectiveness of the proposed approach .",
    "one can note that , even if the paper is devoted to the case of convolutive operators , this approach could be generalized to more general linear operators .",
    "note that the primal - dual approaches @xcite can offer alternative solutions to the ones developed in this paper",
    ". however , one of the advantages of ppxa is that it easily leads to efficient parallel implementations",
    "since @xmath284 is the matrix associated with a bijective operator , @xmath88 is associated with a surjective one and @xmath285 .",
    "this allows us to conclude that @xmath286 is a function of @xmath287 .",
    "+ to calculate the proximity operator of @xmath288 , we now come back to the definition of this operator .",
    "we have thus , for every @xmath289 , @xmath290 we can write any vector @xmath291 as a sum of an element @xmath292 and @xmath293 . we have then @xmath294 . similarly , we can write @xmath295 where @xmath296 and @xmath297 .",
    "so , @xmath298 can be determined by finding @xmath299 this yields @xmath300 and it remains to find @xmath301 by using the separability of @xmath84 , this is equivalent to finding @xmath302 it can be deduced from ( * ? ? ?",
    "* lemma  2.6 ) that , for every @xmath303 , @xmath304 which , according to ( * ? ? ?",
    "* proposition  2.10 ) , leads to @xmath305 altogether , and yield @xmath306 in addition , since @xmath307 is the projection of @xmath308 onto @xmath309 , @xmath310 and follows . +      by using the proximity operator definition , @xmath311 minimizes @xmath312 where @xmath313 denotes the frobenius norm , for every @xmath314 , @xmath315 and @xmath316 it is then clear that holds since the variables @xmath317 with @xmath318 are not elements of the matrices @xmath319 with @xmath320 and @xmath321 .",
    "in addition , since it has been assumed that @xmath322 and @xmath323 , the matrices @xmath319 and @xmath324 can be decomposed in an orthogonal manner as follows : @xmath325 where @xmath326 and @xmath327 is given by . after some simplications , we have thus to minimize , for every @xmath320 and @xmath321 , @xmath328 this shows that is satisfied and that @xmath329 eq . straightforwardly follows .",
    "let @xmath330 denote the projector on @xmath331 .",
    "for every @xmath332 , we have @xmath333 where @xmath334 is the projection error and there exists @xmath335 such that @xmath336 by combining this with the fact that @xmath337 , we obtain the relation , @xmath338 which allows us to deduce from that @xmath339 now , consider the first step of algorithm [ algo : ppa_frame ] : @xmath340 @xmath341 where @xmath342 is assumed to belong to @xmath331 , i.e. @xmath343 with @xmath344 . defining @xmath345 similarly to yields @xmath346 . according to",
    ", @xmath347 is such that @xmath348            in the new formulation , the last steps of the algorithm consist of updating @xmath354 and @xmath355 , for all @xmath356 .",
    "we propose to define @xmath357 , @xmath358 and @xmath359 , which yields @xmath360 and @xmath361 . by using and",
    ", these relations can be simplified as      which leads to algorithm [ algo : accppa ] .",
    "+ we finally note that assumption [ as : errorterms ] [ a:3 ] ) implies that assumption [ ass : conv_spingarn ]  [ a:4 ] ) is satisfied since , for every @xmath227 , @xmath363 this allows us to transpose the convergence results concerning algorithm [ algo : ppa_frame ] to algorithm [ algo : accppa ] .",
    "n.  pustelnik , c.  chaux , and j .- c .",
    "pesquet , `` hybrid regularization for data restoration in the presence of poisson noise , '' in _ proc .",
    "sig . and image proc .",
    "conference _ , glasgow , scotland , aug .",
    "24 - 28 2009 , pp . x+5 .",
    "j.  bect , l.  blanc - fraud , g.  aubert , and a.  chambolle , `` a @xmath365-unified variational framework for image restoration , '' in _ proc .",
    "european conference on computer vision _ , t.  pajdla and j.  matas , eds . ,",
    "prague , czech republic , may 2004 , vol .",
    "lncs 3024 , pp .",
    "113 , springer .",
    "p.  l. combettes and j .- c .",
    "pesquet , `` proximal splitting methods in signal processing , '' in _ fixed - point algorithms for inverse problems in science and engineering _ , h.  h. bauschke , r.  burachik , p.  l. combettes , v.  elser , d.  r. luke , and h.  wolkowicz , eds . , pp .",
    "springer - verlag , new york , 2010 .",
    "j.  bioucas - dias and m.  a.  t. figueiredo , `` an iterative algorithm for linear inverse problems with compound regularizers , '' in _ proc .",
    "int . conf . on image proces .",
    "_ , san diego , ca , usa , oct . 1215 2008 , pp . 685688 .                                          n.  p. galatsanos and a.  k. katsaggelos , `` methods for choosing the regularization parameter and estimating the noise variance in image restoration and their relation , '' , vol . 1 , no",
    ". 3 , pp . 322336 , jul . 1992 ."
  ],
  "abstract_text": [
    "<S> regularization approaches have demonstrated their effectiveness for solving ill - posed problems . however , in the context of variational restoration methods , a challenging question remains , namely how to find a good regularizer . </S>",
    "<S> while total variation introduces staircase effects , wavelet domain regularization brings other artefacts , e.g. ringing . </S>",
    "<S> however , a trade - off can be made by introducing a hybrid regularization including several terms non necessarily acting in the same domain ( e.g. spatial and wavelet transform domains ) . </S>",
    "<S> while this approach was shown to provide good results for solving deconvolution problems in the presence of additive gaussian noise , an important issue is to efficiently deal with this hybrid regularization for more general noise models . to solve this problem </S>",
    "<S> , we adopt a convex optimization framework where the criterion to be minimized is split in the sum of more than two terms . for spatial domain regularization , isotropic or anisotropic </S>",
    "<S> total variation definitions using various gradient filters are considered . </S>",
    "<S> an accelerated version of the parallel proximal algorithm is proposed to perform the minimization . </S>",
    "<S> some difficulties in the computation of the proximity operators involved in this algorithm are also addressed in this paper . </S>",
    "<S> numerical experiments performed in the context of poisson data recovery , show the good behaviour of the algorithm as well as promising results concerning the use of hybrid regularization techniques . </S>"
  ]
}