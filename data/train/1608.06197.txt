{
  "article_text": [
    "in the light of problems caused due to poor crowd management , such as crowd crushes and blockages , there is an increasing need for computational models which can analyse highly dense crowds using video feeds from surveillance cameras .",
    "crowd counting is a crucial component of such an automated crowd analysis system .",
    "this involves estimating the number of people in the crowd , as well as the distribution of the crowd density over the entire area of the gathering .",
    "identifying regions with crowd density above the safety limit can help in issuing prior warnings and can prevent potential crowd crushes .",
    "estimating the crowd count also helps in quantifying the significance of the event and better handling of logistics and infrastructure for the gathering .    in this work",
    ", we propose a deep learning based approach for estimating the crowd density as well as the crowd count from still images .",
    "counting crowds in highly dense scenarios ( > 2000 people ) poses a variety of challenges .",
    "highly dense crowd images suffer from severe occlusion , making the traditional face / person detectors ineffective .",
    "crowd images can be captured from a variety of angles introducing the problem of perspective .",
    "this results in non - uniform scaling of the crowd necessitating the estimation model to be scale - invariant to large scale changes .",
    "furthermore , unlike other vision problems , annotating highly dense crowd images is a laborious task .",
    "this makes the creation of large - scale crowd counting datasets infeasible and limits the amount of training data available for learning - based approaches .     + actual count : 1115 estimated : 1143     +     + actual count:440 estimated:433        hand - crafted image features ( sift  @xcite , hog etc .",
    "@xcite ) often fail to provide robustness to challenges of occlusion and large scale variations .",
    "our approach for crowd counting relies instead on deep learnt features using the framework of fully convolutional neural networks(cnn ) .",
    "we tackle the issue of scale variation in the crowd images using a combination of a shallow and deep convolutional architectures .",
    "further , we perform extensive data augmentation by sampling patches from the multi - scale image representation to make the system robust to scale variations .",
    "our approach is evaluated on the challenging ucf_cc_50 dataset  @xcite and has achieved state of the art results .",
    "some works in the crowd counting literature experiment on datasets having sparse crowd scenes  @xcite , such as ucsd dataset  @xcite , mall dataset  @xcite and pets dataset  @xcite .",
    "in contrast , our method has been evaluated on highly dense crowd images which pose the challenges discussed in the previous section .",
    "methods introduced in @xcite and @xcite exploit patterns of motion to estimate the count of moving objects . however , these methods rely on motion information which can be obtained only in the case of continuous video streams with a good frame rate , and do not extend to still image crowd counting .",
    "the algorithm proposed by idrees _",
    "et al . _",
    "@xcite is based on the understanding that it is difficult to obtain an accurate crowd count using a single feature . to overcome this",
    ", they use a combination of handcrafted features : hog based head detections , fourier analysis , and interest points based counting .",
    "the post processing is done using multi - scale markov random field .",
    "however , handcrafted features often suffer a drop in accuracy when subjected to variances in illumination , perspective distortion , severe occlusion etc .",
    "though zhang _ et al . _",
    "@xcite utilize a deep network to estimate crowd count , their model is trained using perspective maps of images . generating these perspective maps is a laborious process and is infeasible .",
    "we use a simpler approach for training our model , yet obtain a better performance .",
    "et al . _",
    "@xcite also train a deep model for crowd count estimation .",
    "their model however is trained to determine only the crowd count and not the crowd density map , which is crucial for crowd analysis .",
    "our network estimates both the crowd count as well as the crowd density distribution .",
    "crowd images are often captured from varying view points , resulting in a wide variety of perspectives and scale variations .",
    "people near the camera are often captured in a great level of detail i.e. , their faces and at times their entire body is captured . however , in the case of people away from camera or when images are captured from an aerial viewpoint , each person is represented only as a head blob .",
    "efficient detection of people in both these scenarios requires the model to simultaneously operate at a highly semantic level ( faces / body detectors ) while also recognizing the low - level head blob patterns .",
    "our model achieves this using a combination of deep and shallow convolutional neural networks .",
    "an overview of the proposed architecture is shown in fig .  [",
    "fig : architecture ] . in the following subsections",
    ", we describe these networks in detail .",
    "our deep network captures the desired high - level semantics required for crowd counting using an architectural design similar to the well - known vgg-16  @xcite network .",
    "although the vgg-16 architecture was originally trained for the purpose of object classification , the learned filters are very good generic visual descriptors and have found applications in a wide variety of vision tasks such as saliency prediction  @xcite , object segmentation  @xcite etc .",
    "our model efficiently builds up on the representative power of the vgg network by fine - tuning its filters for the problem of crowd counting .",
    "however , crowd density estimation requires per - pixel predictions unlike the problem of image classification , where a single discrete label is assigned for an entire image .",
    "we obtain these pixel - level predictions by removing the fully connected layers present in the vgg architecture , thereby making our network fully convolutional in nature .",
    "the vgg network has 5 max - pool layers each with a stride of 2 and hence the resultant output features have a spatial resolution of only @xmath0 times the input image . in our adaptation of the vgg model , we set the stride of the fourth max - pool layer to @xmath1 and remove the fifth pooling layer altogether .",
    "this enables the network to make predictions at @xmath2 times the input resolution .",
    "we handle the receptive - field mismatch caused by the removal of stride in the fourth max - pool layer using the technique of holes introduced in @xcite .",
    "convolutional filters with holes can have arbitrarily large receptive fields irrespective of their kernel size .",
    "using holes , we double the receptive field of convolutional layers after the fourth max - pool layer , thereby enabling them to operate with their originally trained receptive field .      in our model",
    ", we aim to recognize the low - level head blob patterns , arising from people away from the camera , using a shallow convolutional network . since blob detection does not require the capture of high - level semantics , we design this network to be shallow with a depth of only 3 convolutional layers .",
    "each of these layers has @xmath3 filters with a kernel size of @xmath4 . to make the spatial resolution of this network s prediction equal to that of its deep counterpart",
    ", we use pooling layers after each convolution layer .",
    "our shallow network is primarily used for the detection of small head - blobs . to ensure that there is no loss of count due to max - pooling , we use average pooling layers in the shallow network .",
    "we concatenate the predictions from the deep and shallow networks , each having a spatial resolution of @xmath2 times the input image , and process it using a 1x1 convolution layer .",
    "the output from this layer is upsampled to the size of the input image using bilinear interpolation to obtain the final crowd density prediction .",
    "the total count of the people in the image can be obtained by a summation over the predicted density map .",
    "the network is trained by back - propagating the @xmath52 loss computed with respect to ground - truth .",
    "training a fully convolutional network using the ground - truth of head annotations , marked as a binary dot corresponding to each person , would be difficult .",
    "the exact position of the head annotations is often ambiguous , and varies from annotator to annotator ( forehead , centre of the face etc . ) , making cnn training difficult .    in @xcite ,",
    "the authors have trained a deep network to predict the total crowd count in an image patch .",
    "but using such a ground truth would be suboptimal , as it would nt help in determining which regions of the image actually contribute to the count and by what amount .",
    "et al . _",
    "@xcite have generated ground truth by blurring the binary head annotations , using a kernel that varies with respect to the perspective map of the image .",
    "however , generating such perspective maps is a laborious task and involves manually labelling several pedestrians by marking their height .",
    "we generate our ground truth by simply blurring each head annotation using a gaussian kernel normalized to sum to one .",
    "this kind of blurring causes the sum of the density map to be the same as the total number of people in the crowd .",
    "preparing the ground truth in such a fashion makes the ground truth easier for the cnn to learn , as the cnn no longer needs to get the exact point of head annotation right .",
    "it also provides information on which regions contribute to the count , and by how much .",
    "this helps in training the cnn to predict both the crowd density as well as the crowd count correctly .",
    "as cnns require a large amount of training data , we perform an extensive augmentation of our training dataset .",
    "we primarily perform two types of augmentation .",
    "the first type of augmentation helps in tackling the problem of scale variations in crowd images , while the second type improves the cnn s performance in regions where it is highly susceptible to making mistakes i.e. , highly dense crowd regions .        in order to make the cnn robust to scale variations , we crop patches from the multi - scale pyramidal representation of each training image .",
    "we consider scales of @xmath6 to @xmath7 , incremented in steps of @xmath8 , times the original image resolution ( as shown in fig.[fig : imagepyramid_1 ] ) for constructing the image pyramid .",
    "we crop @xmath9 patches with @xmath10 overlap from this pyramidal representation . with this augmentation ,",
    "the cnn is trained to recognize people irrespective of their scales .",
    "we observed that cnns find highly dense crowds inherently difficult to handle . to overcome this ,",
    "we augment the training data by sampling high density patches more often .",
    "we evaluate our approach for crowd counting on the challenging ucf_cc_50  @xcite dataset .",
    "this dataset contains 50 gray scale images , each provided with head annotations .",
    "the number of people per image varies between 94 and 4543 , with an average of 1280 individuals per image .",
    "the dataset comprises of images from a wide range of scenarios such as concerts , political rallies , religious gatherings , stadiums etc .    in a manner similar to recent works",
    "@xcite , we evaluate the performance of our approach using 5-fold cross validation .",
    "we randomly divide the dataset into five splits with each split containing 10 images . in each fold of the cross validation , we consider four splits ( 40 images ) for training the network and the remaining split ( 10 images ) for validating its performance .",
    "we sample @xmath9 patches from each of the 40 training images following the previously described data augmentation method .",
    "this procedure yields an average of 50,292 training patches per fold .",
    "we train our deep convolutional network using the deeplab  @xcite version of caffe  @xcite deep learning framework , using titan x gpus .",
    "our network was trained using stochastic gradient descent ( sgd ) optimization with a learning rate of @xmath11 and momentum of @xmath12 .",
    "the average training time per fold is about 5 hours .",
    "we use mean absolute error ( mae ) to quantify the performance of our method .",
    "mae computes the mean of absolute difference between the actual count and the predicted count for all the images in the dataset .",
    "the results of the proposed approach along with other recent methods are shown in table .",
    "[ tab : results ] .",
    "the results shown do not include any post - processing methods .",
    "the results illustrate that our approach achieves state - of - the - art performance in crowd counting .",
    "[ tab : results ]    .quantitative results of our approach along with other state - of - the - art methods on ucf_cc_50 dataset . [ cols=\"^,^\",options=\"header \" , ]",
    "in this paper , we proposed a deep learning based approach to estimate the crowd density and total crowd count from highly dense crowd images .",
    "we showed that using a combination of a deep network as well as a shallow network is essential for detecting people under large scale variations and severe occlusion .",
    "we also show that the challenge of varying scales , and inherent difficulties in highly dense crowds , can be effectively tackled by augmenting the training images .",
    "our method outperforms the state - of - the - art methods on the challenging ucf_cc_50 dataset .",
    "this work was supported by science and engineering research board ( serb ) , department of science and technology ( dst ) , govt .",
    "of india ( proj no .",
    "sb / s3/eece/0127/2015 ) .",
    "a.  b. chan , z .- s .",
    "j. liang , and n.  vasconcelos .",
    "privacy preserving crowd monitoring : counting people without people models or tracking . in _ ieee conference on computer vision and pattern recognition _ , 2008 .",
    "n.  dalal and b.  triggs .",
    "histograms of oriented gradients for human detection . in _",
    "ieee computer society conference on computer vision and pattern recognition , 2005 .",
    "_ , volume  1 , pages 886893 .",
    "ieee , 2005 ."
  ],
  "abstract_text": [
    "<S> our work proposes a novel deep learning framework for estimating crowd density from static images of highly dense crowds . </S>",
    "<S> we use a combination of deep and shallow , fully convolutional networks to predict the density map for a given crowd image . </S>",
    "<S> such a combination is used for effectively capturing both the high - level semantic information ( face / body detectors ) and the low - level features ( blob detectors ) , that are necessary for crowd counting under large scale variations . as most crowd datasets have </S>",
    "<S> limited training samples ( < 100 images ) and deep learning based approaches require large amounts of training data , we perform multi - scale data augmentation . augmenting the training samples in such a manner </S>",
    "<S> helps in guiding the cnn to learn scale invariant representations . </S>",
    "<S> our method is tested on the challenging ucf_cc_50 dataset , and shown to outperform the state of the art methods . </S>"
  ]
}