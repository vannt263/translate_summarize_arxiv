{
  "article_text": [
    "estimating the maximum likelihood gradient of boltzmann machines requires samples to be drawn from the model .",
    "this is typically done through gibbs sampling where the state of the markov chain is preserved in between parameter updates , leading to the stochastic maximum likelihood ( sml ) algorithm @xcite . while popular in practice , this algorithm can be brittle as it relies on the markov chain mixing properly during training :",
    "if the sampling process fails to adequately explore the space of allowed configurations ( e.g. by getting trapped in regions of high - probability ) , the negative phase statistics can become biased and cause the learning procedure to diverge .",
    "great care must therefore be taken in decreasing the learning rate or increasing the number of gibbs steps to offset the loss of ergodicity incurred by learning .",
    "an attractive and well studied alternative is to augment the boltzmann distribution with a temperature parameter in order to perform a simulation in the joint temperature - configuration space . as the temperature increases",
    ", the distribution becomes more uniform , which in turn allows particles to quickly explore the energy landscape and escape configurations corresponding to local minima in the original target distribution .",
    "as particles decrease to the nominal temperature , they are guaranteed to be proper samples of the target distribution and can thus be used to estimate the sufficient statistics of the model .",
    "@xcite have all explored variations on this idea , by considering either serial or parallel implementations of this basic tempering strategy .",
    "this generally results in increased robustness to the choice of hyper - parameters and faster convergence when used in conjunction with higher learning rates .    despite these benefits ,",
    "tempering methods remain computationally expensive .",
    "while sml requires a mini - batch of @xmath0 markov chains to be simulated , parallel tempering ( pt ) requires @xmath1 chains , where @xmath2 is the number of temperatures .",
    "while the serial variants appear to address this issue , they may be doing so at the expense of sampling efficiency .",
    "when using tempered transitions , long - distance jumps can suffer from high - rejection rates , resulting in the use of possibly stale samples in the gradient estimation step .",
    "similarly , coupled adaptive simulated tempering ( cast ) @xcite relies mostly on non - tempered chains to perform local mode exploration , with non - local moves occurring only occasionally , when simulated tempering particles are sampled back down to the nominal temperature . while the relative merits of each method can be discussed at great length , they unfortunately all share the same achilles heel : _ their tempering parametrization is inefficient_. they invariably require many interpolating distributions between the target distribution and the high temperature distribution for which rapidly mixing sampling is possible .",
    "our proposed solution starts with the idea that we may be able to _ learn _ more effective interpolating distributions , allowing us to use far fewer intermediate distributions than with traditional tempering methods , while still maintaining a high level of ergodicity in the mcmc simulation of the extended system .",
    "recent findings @xcite have shown that when training deep feature hierarchies , such as deep belief networks ( dbn ) @xcite as few as 2 - 3 layers are required for the upper layers to mix properly between the modes of the distribution .",
    "it appears as though successive nonlinear transformations to ever more abstract latent variables ( i.e. ever more removed from the data ) play a crucial role in achieving efficient mixing .",
    "the goal of this work is to leverage these learned intermediate distributions to facilitate mixing in the lower levels of the hierarchy .",
    "while this procedure is generally applicable to the learning of any energy - based model featuring latent variables , this paper considers the particular case of sampling from a restricted boltzmann machine ( rbm ) @xcite for the purpose of training . in this",
    "setting , our method can equivalently be seen as way to perform joint - training of dbns .",
    "[ [ restricted - boltzmann - machines . ] ] restricted boltzmann machines .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    rbms define a probability distribution @xmath3 $ ] over a set of visible variables @xmath4 and latent variables @xmath5 .",
    "@xmath6 is referred to as the energy function , which in the case of binary visible and latent units is defined as @xmath7 .",
    "@xmath8 is the partition function defined as @xmath9}$ ] .",
    "the parameters @xmath10 of the model include a weight matrix @xmath11 and bias vectors @xmath12 and @xmath13 .",
    "rbms owe much of their success to this parametrization as it renders its conditional distributions factorial , enabling trivial inference and efficient sampling through block gibbs sampling .",
    "this property also allows us to analytically compute the un - normalized probability of a given configuration @xmath14 or @xmath15 , through the free - energy function @xmath16 such that @xmath17 $ ] .",
    "[ [ stochastic - maximum - likelihood . ] ] stochastic maximum likelihood",
    ". + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the maximum likelihood gradient of an rbm can be derived as the difference of two expectations , @xmath18 -    \\mathbb{e}_p \\left [ \\frac{\\partial f(v)}{\\partial \\theta } \\right]$ ] , where @xmath19 is the empirical distribution .",
    "stochastic maximum likelihood ( sml ) @xcite approximates the expectation under the model distribution ( negative phase ) through mcmc , the simplest instantiation being a gibbs chain which alternates @xmath20 , @xmath21 .",
    "the particularity of sml is that it avoids the expensive burn - in process associated with each parameter update by maintaining the state of the markov chains in between learning iterations .",
    "while asymptotic convergence of sml has been established under certain conditions@xcite , the method can often diverge in practice when faced with complex multi - modal distributions . as @xmath22 becomes increasingly peaked",
    ", mode hops can become exponentially less likely for a gibbs sampler .",
    "no practical learning rate annealing schedules can then ensure that the model s sufficient statistics are accurately reflected by the sampler .",
    "for this reason , several authors have proposed using tempering - based samplers to estimate the negative phase gradient , as these methods are known to be efficient at escaping from regions of high - probability .",
    "[ [ parallel - tempering . ] ] parallel tempering .",
    "+ + + + + + + + + + + + + + + + + + +    we focus here on the parallel tempering ( pt ) strategy of @xcite as it is the most relevant to our work . instead of simulating a single markov chain",
    ", pt simulates an ensemble of @xmath2 models @xmath23 , with @xmath24 $ ] . the sole difference in the parametrization of @xmath25 wrt .",
    "@xmath26 is the introduction of the inverse temperature parameter @xmath27 $ ] which acts to scale the energy values .",
    "low values of @xmath28 act to make the distributions @xmath25 closer to uniform , and will thus facilitate sampling . ] . to leverage these fast - mixing chains",
    ", pt incorporates an additional transition operator ( on top of the standard gibbs operator applied independently to each chain ) : swaps ( or _ replica exchanges _ ) are proposed between samples at neighboring temperature . if @xmath29 , pt accepts this move with probability @xmath30 defined as : @xmath31 note that the partition functions are not required to compute this quantity , as @xmath32 and @xmath33 appear both in the numerator and denominator .",
    "a gibbs steps at each markov chain , followed by swap proposals between a subset of ( neighboring ) chains constitutes a single pt iteration .",
    "it is easy to see that repeating this process will cause fast - mixing samples to eventually be swapped into the lower temperatures , thus ( from the point of view of @xmath34 ) performing a long - distance mcmc move . in this manner",
    ", the samples of @xmath34 can move much faster across configurations and more accurately reflect the model statistics .",
    "[ [ deep - belief - networks . ] ] deep belief networks .",
    "+ + + + + + + + + + + + + + + + + + + + +    rbms are typically trained and stacked in a greedy layer - wise fashion to form a deep belief network ( dbn ) .",
    "the result is a model which learns a joint distribution over multiple layers of latent variables .",
    "latent layers defines a joint - distribution @xmath35 , where @xmath25 is the distribution of the @xmath36-th level ] these can then be used as input to a classifier , most often outperforming features extracted by shallow models ( i.e. a single rbm ) .",
    "another interesting property of dbns however , is that the last level rbm is known to mix well across its input configurations .",
    "on mnist , @xcite showed that drawing samples from the top - level rbm via gibbs sampling and then deterministically projecting these back to the input space , led to samples which mixed very well across digit classes .",
    "this important result would appear to be somewhat under - appreciated , especially considering the mixing issues typically associated with single layer models , even when used in conjunction with tempering .",
    "this phenomenon has received renewed attention of late . @xcite",
    "have found that depth is crucial in sampling and learning better texture models in the rbm family .",
    "the relationship between depth and mixing was further explored in @xcite , where the authors hypothesize that good mixing is the result of deep networks learning to disentangle the underlying factors of variation .",
    "our proposed solution starts with the idea of _ learning _ the set of interpolating distributions . a possible realization of this idea would be to simulate an ensemble @xmath37 , with each distribution @xmath25 having its own set of parameters @xmath38 and @xmath39 .",
    "each @xmath25 would be trained independently to model the empirical distribution @xmath40 , but regularized so that @xmath25 s become easier to sample from ] with index @xmath36 . by construction , @xmath41",
    "would be kept small and should thus allow for frequent replica exchange . while such an algorithm would certainly lower the computational cost of simulating `` higher temperatures '' , it does little to address the large number of interpolating distributions often required by tempering methods .    exploiting the observation that good mixing can be achieved by remapping the data into a high - level latent space , we instead _ learn _ interpolating distributions @xmath25 via stacked latent variable models and train each @xmath25 to model the posterior distribution of @xmath42 . training this ensemble jointly , we can then propose replica exchanges between neighboring models and exploit samples of the upper layers to perform long distance mcmc moves in the visible space of lower layers .",
    "while this procedure is generally applicable to the learning of any energy - based model featuring latent variables , this paper considers the particular case where @xmath25 is a restricted boltzmann machine ( rbm ) @xcite .",
    "similar to parallel tempering , deep tempering simulates an ensemble @xmath43 of models defined by equation  [ eq : dt_ensemble ] .",
    "the key difference is that models @xmath25 do not share the same parametrization .",
    "each @xmath25 defines a joint distribution over its own set of visible variables @xmath44 and latent variables @xmath45 .",
    "the support of these variables is constrained such that @xmath46 .",
    "the parameters @xmath38 of these distributions are learned jointly , in order for @xmath25 to accurately reflect the posterior distribution of the lower layer .",
    "defining @xmath47 to be the empirical distribution and @xmath48 , the @xmath36-th layer aggregate posterior distribution defined as @xmath49 $ ] , parameters of the @xmath36-th layer are trained simultaneously , e.g. through maximum likelihood , as shown in equation  [ eq : dt_learning ] .",
    "@xmath50.\\end{aligned}\\ ] ] the particular form of proposal models @xmath25 is left free , with the only constraint that @xmath51 and @xmath52 can be evaluated up to a normalization constant , meaning to say @xmath53 and @xmath54 can each be marginalized analytically .",
    "this last fact allows us to perform cross - temperature state swaps between neighboring distributions .",
    "replica exchanges can then be considered between @xmath55 and @xmath56 , with a swap acceptance probability @xmath57 given by : @xmath58    the deep tempering method as a whole is detailed in the algorithm of figure  [ fig : alg ] .    while our proposed method shares many similarities to parallel tempering , there are nonetheless important differences which will have consequences for training rbms .",
    "as previously described , parallel tempering exploits a temperature parameter to be able to simulate samples from a high - temperature ( i.e. smoothed - out ) version of the model distribution . in our case ,",
    "the analogous high - temperature distribution is not only simulating samples in a transformed space ( that corresponding to the rbm latent variables ) , but the upper layer model is actually trained not to reflect the model distribution , but rather a transformation of the data distribution given by the rbm conditional @xmath59 . despite these differences , accept - reject step still ensures that the samples accepted to swap into the lower - layer rbm do properly reflect the lower - layer rbm model distribution    [ [ deep - belief - network - joint - training . ] ] deep belief network joint - training .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    viewing the proposed approach as simply a means of training the lower - layer rbm ignores one important point : at the end of the training process , we effectively have not a single trained rbm , but a stack of rbms trained together as a whole deep belief network ( dbn ) . from this perspective , the proposed approach can be seen as a means of joint - training a dbn .",
    "this form of training is in stark contrast to the common practice of greedily training the constituent rbms , in a bottom - up sequential manner .    in the proposed approach ,",
    "joint training plays a crucial role in ensuring that at every point in training , upper - layer rbms faithfully represent the distribution over the lower layer hidden units in order to ensure adequate acceptance rate of the state - swapping mcmc moves . from this perspective , if our goal is to learn a dbn , the proposed approach is doubly efficient .",
    "not only is the learned - tempering model also a viable dbn in its own right , but we are able to exploit the rapid mixing of the upper - layer rbms to ensure rapid ( and consistent ) mixing of the rbms at every - layer of the dbn .",
    "it is worthwhile to ask if this style of joint - training results in any important differences compared to the traditional greedy layer - wise training strategy .",
    "we will return to this question in the experimental section .",
    "for now it suffices to point to some interesting and potentially important consequences for the proposed style of joint training .",
    "first , without swapping between neighboring rbms , the proposed approach is simply the simultaneous application of the standard layer - wise rbm training procedure with one notable exception .",
    "the upper - layer rbms are effectively training to reach a moving target : the distribution that they are being asked to learn is constantly changing as the lower - layer rbms learn .",
    "second , each rbm is trained using sml where the negative phase samples are drawn using mcmc over an ensemble of models , mimicking the effect of traditional tempered mcmc chains .",
    "typically , this chain will consist of small jumps that reflect the local structure of the rbm around negative phase samples , and long distance jumps that correspond to particle swaps between the states of neighboring rbms .",
    "the fact that all rbms share both the positive phase data samples as well as the negative phase samples will help maintain consistency between the models , ensuring acceptable trans - rbm swap ratios .    as rbm training proceeds and the energy surface begins to reflect the distribution of the data , the low - level rbms will increasingly rely on the upper - layer rbms to provide  through swaps",
    " a steady supply of diverse particles for use in the negative phase of training .",
    "this is similar to the situation when rbms are trained with a tempering strategy .",
    "however unlike in true tempering , here the upper - layer rbms do not share the same parametrization as the lower - layer rbm . as a result",
    ", replica exchanges may not help escape `` spurious '' local minimas in the energy which may emerge as a result of training ( and which are not shared across layers ) .",
    "instead , we expect the long distance mcmc moves offered by dt to be biased towards modes which are well supported by the data .",
    "we start our evaluation of deep tempering on the `` artificial modes '' dataset first introduced in @xcite .",
    "the dataset is characterized by five basic @xmath60 binary random noise images .",
    "training data is generated online as a mixture model over these five modes , permuting each pixel independently with a mode dependent probability .",
    "this dataset is difficult from a sampling perspective , as it combines modes of low probability having large spatial support , and narrow modes with high probability . on the other hand ,",
    "the structure of the data is relatively simple , allowing us to use models with low capacity and thus measure likelihood exactly during training .",
    "likelihood is reported on a test set of @xmath61k images .",
    "[ [ rbm - vs .- t - dbn ] ] rbm vs. t - dbn + + + + + + + + + + + + +    figure  [ fig : modes1 ] compares the test - set likelihood curve ( during training ) of a @xmath62-hidden unit rbm , vs the likelihood of the first - level rbm in a 2 and 3-layer tempered dbn ( t - dbn ) , with @xmath62 hidden units per layer .",
    "we therefore evaluate dt as a better training / sampling algorithm for rbms .",
    "gradients were computed by averaging over a small mini - batch of size @xmath61 , kept small so as to prevent models from covering all modes through a large mini - batch of ( effectively ) stationary particles . to simplify the analysis and given the interesting dynamic between learning and mixing ( the mixing rate of a sampler constrains the set of feasible learning rates )",
    ", we focus our hyper - parameter search to constant learning rates in @xmath63 .",
    "results are averaged over @xmath61 seeds , with the shaded area representing @xmath64 .",
    "as we can see , the single rbm is unable to learn a meaningful model of the dataset for any setting of the learning rate .",
    "sml diverges when using high learning rates and yields an unimpressive score of around @xmath65 nats when using a learning rate of @xmath66 .",
    "in contrast , a stack of two rbms trained via deep tempering achieves a score of @xmath67 nats with a learning rate of @xmath68 . pushing the learning rate",
    "further to @xmath69 causes the two layer model to diverge , a fact which seems attenuated by moving to a stack of 3 rbms , shown in figure  [ fig : modes1c ] .",
    "cross - model state swaps occurred rather frequently on this dataset , with the best performing t - dbn2 model swapping @xmath70 of the time on average .",
    "[ [ pt - cast - vs - t - dbn . ] ] pt , cast vs t - dbn .",
    "+ + + + + + + + + + + + + + + + + +    dt also compares favorably to state - of - the - art tempering methods . in @xcite ,",
    "parallel tempering required around @xmath71 tempered chains spaced uniformly in the unit interval , to achieve a likelihood of around @xmath72 nats .",
    "optimizing the inverse temperature parameters through an adaptive tempering mechanism improved this likelihood to around @xmath73 nats , while requiring fewer tempered chains ( @xmath74 ) .",
    "results obtained with the original cast formulation @xcite ( cast 1:1 ) , using @xmath75 tempered chains spaced uniformly in the unit interval , are shown in figure  [ fig : modes_cast ] .",
    "the model originally performs well but eventually diverges despite ast converging to the proper ( log ) adaptive weights ( given by the log - partition functions at each temperature ) . suspecting insufficient replica exchanges between tempered and non - tempered chains ( given the large number of temperatures being simulated )",
    ", we also trained a one layer rbm using a modified version of cast , where we increase the `` coupling ratio '' , i.e. the ratio of non - tempered to tempered chains , denoted as cast 1:x .",
    "a ratio of 1:10 led to notable improvements in likelihood , achieving @xmath76 nats after @xmath77 updates .",
    "cast 1:100 achieved a new state - of - the - art for single - layer models : @xmath78 nats .",
    "the reader should keep in mind however that cast @xmath79 with a mini - batch of size @xmath61 requires simulating @xmath80 markov chains in total , compared to the @xmath62 markov chains required by a t - dbn2 .",
    "[ [ dbn - lower - bound . ] ] dbn lower - bound .",
    "+ + + + + + + + + + + + + + + +    as mentioned in section  [ sec : deeptempering ] , dt applied to a stack of rbms can equivalently be viewed as performing joint - training of a dbn .",
    "it is thus interesting to study the effect of dt on the lower - bound of the dbn likelihood , as formulated in @xcite .",
    "the results are shown in figure  [ fig : modes_dbn ] .",
    "the optimal learning rate achieved a lower - bound of @xmath81 nats for a 2-layer dbn ( shown in green ) , and @xmath82 nats for the 3-layer variant ( red ) .",
    "one may also wonder about the effect of joint - training vs. the typical greedy layer - wise training procedure employed by dbns . to avoid the extreme instability of sml trained rbms on this dataset , we adjusted the greedy layer - wise training to incorporate early - stopping , based on detecting oscillations in training likelihood .",
    "@xmath83 layers were pre - trained in this manner , followed by joint - training across all layers . the results clearly show worse performance for the pre - trained models .",
    "this suggests that the greedy layer - wise pre - training scheme of dbns leads to poor local minima .",
    "deep tempering combines ideas from traditional mcmc tempering methods , with new observations from the field of deep learning . from the field of mcmc",
    ", we borrow the idea of simulating an ensemble of distributions , where the fast - mixing properties of high - temperature chains is leveraged to facilitate training of the lower layers , which are often prone to getting stuck in regions of high probability . from the world of deep learning",
    ", we exploit recent findings which show that models at deeper layers in the hierarchy seem to naturally exhibit good mixing .",
    "these two ideas are combined through a joint - training strategy , which considers individual models in a deep hierarchy as interpolating distributions for the lower layers . while we have concentrated on the application of deep tempering to rbms",
    "the method is much more widely applicable .",
    "perhaps most interesting is that it can be readily applied to deep boltzmann machines ( dbm ) @xcite both to promote mixing in the negative phase of learning as well as to draw diverse samples from a fully trained model .",
    "we simply construct a latent variable model over all the even layers of the dbm and use this model to propose swaps with the dbm state over the even layers of the model .",
    "the method that we describe in this paper can be considered an example of an auxiliary variable method markov chain monte carlo sampling of the lower layer rbm .",
    "similar to other such methods , including the celebrated sweden - wang method @xcite , one can interpret the addition of the random variables in the upper - layer rbms ( both visibles and hiddens ) as auxiliary variables that are added to augment the simulated system with the goal of achieving more efficient sampling .",
    "where the proposed approach differs from more standard auxiliary variable methods for mcmc is the richness in structure relating these auxiliary variables both to themselves and to the target lower - layer rbm .",
    "while swendsen - wang can be thought of as imposing a clustering on the variables of the simulated system , our approach uses a dbn ( with @xmath83 layers ) to model the dependency structures between the latent variables of the lower - layer rbm ."
  ],
  "abstract_text": [
    "<S> restricted boltzmann machines ( rbms ) are one of the fundamental building blocks of deep learning . </S>",
    "<S> approximate maximum likelihood training of rbms typically necessitates sampling from these models . in many training scenarios , </S>",
    "<S> computationally efficient gibbs sampling procedures are crippled by poor mixing . in this work </S>",
    "<S> we propose a novel method of sampling from boltzmann machines that demonstrates a computationally efficient way to promote mixing . </S>",
    "<S> our approach leverages an under - appreciated property of deep generative models such as the deep belief network ( dbn ) , where gibbs sampling from deeper levels of the latent variable hierarchy results in dramatically increased ergodicity . </S>",
    "<S> our approach is thus to train an auxiliary latent hierarchical model , based on the dbn . </S>",
    "<S> when used in conjunction with parallel - tempering , the method is asymptotically guaranteed to simulate samples from the target rbm . </S>",
    "<S> experimental results confirm the effectiveness of this sampling strategy in the context of rbm training . </S>"
  ]
}