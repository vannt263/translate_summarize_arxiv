{
  "article_text": [
    "_ active learning",
    "_ refers to a family of powerful supervised learning protocols capable of producing more accurate classifiers while using a smaller number of labeled data points than traditional ( passive ) learning methods .",
    "here we study a variant known as _ pool - based _ active learning , in which a learning algorithm is given access to a large pool of unlabeled data ( i.e. , only the covariates are visible ) , and is allowed to sequentially request the label ( response variable ) of any particular data points from that pool .",
    "the objective is to learn a function that accurately predicts the labels of new points , while minimizing the number of label requests .",
    "thus , this is a type of sequential design scenario for a function estimation problem .",
    "this contrasts with passive learning , where the labeled data are sampled at random . in comparison , by more carefully selecting which points should be labeled , active learning can often significantly decrease the total amount of effort required for data annotation",
    ". this can be particularly interesting for tasks where unlabeled data are available in abundance , but label information comes only through significant effort or cost .    recently , there have been a series of exciting advances on the topic of active learning with arbitrary classification noise ( the so - called _ agnostic _ pac model @xcite ) , resulting in several new algorithms capable of achieving improved convergence rates compared to passive learning under certain conditions .",
    "the first , proposed by balcan , beygelzimer and langford @xcite was the @xmath0 ( agnostic active ) algorithm , which provably never has significantly worse rates of convergence than passive learning by empirical risk minimization .",
    "this algorithm was later analyzed in detail in @xcite , where it was found that a complexity measure called the _ disagreement coefficient _ characterizes the worst - case convergence rates achieved by @xmath0 for any given hypothesis class , data distribution and best achievable error rate in the class .",
    "the next major advance was by dasgupta , hsu and monteleoni @xcite , who proposed a new algorithm , and proved that it improves the dependence of the convergence rates on the disagreement coefficient compared to @xmath0 .",
    "both algorithms are defined below in section [ sec : algorithms ] .",
    "while all of these advances are encouraging , they are limited in two ways .",
    "first , the convergence rates that have been proven for these algorithms typically only improve the dependence on the magnitude of the noise ( more precisely , the noise rate of the hypothesis class ) , compared to passive learning .",
    "thus , in an asymptotic sense , for nonzero noise rates these results represent at best a constant factor improvement over passive learning .",
    "second , these results are limited to learning with a fixed hypothesis class of limited expressiveness , so that convergence to the bayes error rate is not always a possibility .    on the first of these limitations , recent work by castro and nowak @xcite on learning threshold classifiers discovered that if certain parameters of the noise distribution are _ known _ ( namely , parameters related to tsybakov s margin conditions ) , then we can achieve strict improvements in the asymptotic convergence rate via a specific active learning algorithm designed to take advantage of that knowledge for thresholds .",
    "subsequently , balcan , broder and zhang @xcite proved a similar result for linear separators in higher dimensions , and castro and nowak @xcite showed related improvements for the space of boundary fragment classes ( under a somewhat stronger assumption than tsybakov s ) .",
    "however , these works left open the question of whether such improvements could be achieved by an algorithm that does not explicitly depend on the noise conditions ( i.e. , in the _ agnostic _ setting ) , and whether this type of improvement is achievable for more general families of hypothesis classes , under the usual complexity restrictions ( e.g. , vc class , entropy conditions , etc . ) . in a personal communication ,",
    "john langford and rui castro claimed @xmath0 achieves these improvements for the special case of threshold classifiers ( a special case of this also appeared in @xcite ) .",
    "however , there remained an open question of whether such rate improvements could be generalized to hold for arbitrary hypothesis classes . in section [ sec : rates ] , we provide this generalization .",
    "we analyze the rates achieved by @xmath0 under tsybakov s noise conditions @xcite ; in particular , we find that these rates are strictly superior to the known rates for passive learning , when the disagreement coefficient is finite",
    ". we also study a novel modification of the algorithm of dasgupta , hsu and monteleoni @xcite , proving that it improves upon the rates of @xmath0 in its dependence on the disagreement coefficient .",
    "additionally , in section [ sec : aggregation ] , we address the second limitation by proposing a general model selection procedure for active learning with an arbitrary structure of nested hypothesis classes .",
    "if the classes have restricted expressiveness ( e.g. , vc classes ) , the error rate for this algorithm converges to the best achievable error by any classifier in the structure , at a rate that adapts to the noise conditions and complexity of the optimal classifier . in general ,",
    "if the structure is constructed to include arbitrarily good approximations to any classifier , the error converges to the bayes error rate in the limit .",
    "in particular , if the bayes optimal classifier is in some class within the structure , the algorithm performs nearly as well as running an agnostic active learning algorithm on that single hypothesis class , thus preserving the convergence rate improvements achievable for that class .",
    "in the active learning setting , there is an _ instance space _ @xmath1 , a _ label space _ @xmath2 and some fixed distribution @xmath3 over @xmath4 , with marginal @xmath5 over @xmath6 .",
    "the restriction to binary classification ( @xmath2 ) is intended to simplify the discussion ; however , everything below generalizes quite naturally to multiclass classification ( where @xmath7 ) .",
    "there are two sequences of random variables : @xmath8 and @xmath9 where each @xmath10 pair is independent of the others , and has joint distribution @xmath3 . however , the learning algorithm is only permitted direct access to the @xmath11 values ( unlabeled data points ) , and must request the @xmath12 values one at a time , sequentially .",
    "that is , the algorithm picks some index @xmath13 to observe the @xmath12 value , then after observing it , picks another index @xmath14 to observe the @xmath15 label value , etc .",
    "we are interested in studying the rate of convergence of the error rate of the classifier output by the learning algorithm , in terms of the number of label requests it has made . to simplify the discussion",
    ", we will think of the data sequence as being essentially inexhaustible , and will study @xmath16-confidence bounds on the error rate of the classifier produced by an algorithm permitted to make at most @xmath17 label requests , for a fixed value @xmath18 .",
    "the actual number of ( unlabeled ) data points the algorithm uses will be made clear in the proofs ( typically close to the number of points needed by passive learning to achieve the stated error guarantee ) . a _ hypothesis class",
    "_ @xmath19 is any set of measurable classifiers @xmath20 .",
    "we will denote by @xmath21 the vc dimension of @xmath19 ( see , e.g. , @xcite ) . for any measurable @xmath22 and distribution @xmath23 over @xmath24 ,",
    "define the _ error rate _ of @xmath25 as @xmath26 ; when @xmath27 , we abbreviate this as @xmath28 .",
    "this simply represents the risk under the @xmath29@xmath30 loss .",
    "we also define the _ conditional error rate _ , given a set @xmath31 , as @xmath32 .",
    "let @xmath33 , called the _ noise rate _ of @xmath19 . for any @xmath34 ,",
    "let @xmath35 , let @xmath36 - 1 $ ] and let @xmath37 .",
    "we call @xmath38 the _ bayes optimal classifier _ and @xmath39 the _ bayes error rate_. additionally , define the _ diameter _ of any set of classifiers",
    "@xmath40 as @xmath41 , and for any @xmath42 , define the diameter of the _ @xmath43-minimal set _ of @xmath40 as @xmath44 .    for a classifier @xmath25 , and a sequence @xmath45 , let @xmath46 $ ] denote the _ empirical error rate _ on @xmath47 , [ and define @xmath48 by convention ]",
    ". it will often be convenient to make use of sets of ( index , label ) pairs , where the index is used to uniquely refer to an element of the @xmath49 sequence ( while conveniently also keeping track of relative ordering information ) ; in such contexts , we will overload notation as follows . for a classifier  @xmath25 , and a finite set of ( index , label ) pairs @xmath50 ,",
    "let @xmath51 $ ] , ( and @xmath52 , as before ) .",
    "thus , @xmath53 , where @xmath54 .",
    "for the indexed _ true _ label sequence , @xmath55 , we abbreviate this @xmath56 , the empirical error on the first @xmath57 data points .    in addition to the independent interest of understanding the rates achievable here , another primary interest in this setting",
    "is to quantify the achievable _ improvements _ , compared to _",
    "passive learning_. in this context , a passive learning algorithm can be formally defined as a function mapping the sequence @xmath58 to a classifier @xmath59 ; for instance , perhaps the most widely studied family of passive learning methods is that of _ empirical risk minimization _",
    "( e.g. , @xcite ) , which return a classifier @xmath60 .",
    "for the purpose of this comparison , we review known results on passive learning in several contexts below .",
    "here we describe a particular parametrization of noise distributions , relative to a hypothesis class , often referred to as tsybakov s noise conditions @xcite , or margin conditions .",
    "these noise conditions have recently received substantial attention in the passive learning literature , as they describe situations in which the asymptotic minimax convergence rate of passive learning is faster than the worst case @xmath61 rate ( e.g. , @xcite ) .",
    "[ con : tsybakov ] there exist finite constants",
    "@xmath62 and @xmath63 , s.t .",
    "@xmath64 , @xmath65 .",
    "this condition is satisfied when , for example , @xmath66 @xcite .",
    "it is also satisfied when the bayes optimal classifier is in @xmath19 and @xmath67 where @xmath68 and @xmath69 are functions of @xmath70 and @xmath71 @xcite ; in particular , @xmath72 . as we will see , the case where @xmath73 is particularly interesting ; for instance , this is the case when @xmath74 and @xmath75 for some constant @xmath76 .",
    "informally , in many cases condition [ con : tsybakov ] can be realized in terms of the relation between magnitude of noise and distance to the optimal decision boundary ; that is , since in practice the amount of noise in a data point s label is often inversely related to the distance from the decision boundary , a small @xmath68 value may often result from having low density near the decision boundary ( i.e. , large margin ) ; when this is not the case , the value of @xmath68 is often determined by how quickly @xmath77 changes as @xmath78 approaches the decision boundary .",
    "see @xcite for further interpretations of this condition .",
    "it is known that when this condition is satisfied for some @xmath79 and @xmath62 , the passive learning method of empirical risk minimization achieves a convergence rate guarantee , holding with probability @xmath80 , of @xmath81 where @xmath82 is a ( @xmath68 and @xmath69-dependent ) constant ( this follows from @xcite ; see appendix b of the supplementary material @xcite , especially ( 17 ) and lemma 5 , for the details ) .",
    "furthermore , for some hypothesis classes , this is known to be a tight bound ( up to the log factor ) on the minimax convergence rate , so that there is _ no _ passive learning algorithm for these classes for which we can guarantee a faster convergence rate , given that the guarantee depends on @xmath3 only through @xmath69 and @xmath68 @xcite ( see also appendix d of the supplementary material @xcite ) .",
    "the disagreement coefficient , introduced in @xcite , is a measure of the complexity of an active learning problem , which has proven quite useful for analyzing the convergence rates of certain types of active learning algorithms : for example , the algorithms of @xcite . informally , it quantifies how much disagreement there is among a set of classifiers relative to how close to some @xmath25 they are .",
    "the following is a version of its definition , which we will use extensively below . for any hypothesis class @xmath19 and @xmath83 ,",
    "let @xmath84 for @xmath85 $ ] and measurable @xmath22 , let @xmath86    [ def : disagreement - coefficient ] the _ disagreement coefficient _ of @xmath25 with respect to @xmath19 under @xmath5 is defined as @xmath87 where @xmath88 ( though see appendix [ subsec : r0 ] for alternative possibilities for @xmath89 ) .",
    "[ def : global - disagreement - coefficient ] we further define the disagreement coefficient for the hypothesis class @xmath19 with respect to the target distribution @xmath3 as @xmath90}}$ ] , where @xmath91}\\}$ ] is any sequence in @xmath19 with @xmath92})$ ] monotonically decreasing to @xmath93 ; [ by convention , take every @xmath94}\\in\\arg\\min _ { h \\in{\\mathbb c } } { \\mathit{er}}(h)$ ] if the minimum is achieved ] .    in definition [ def : disagreement - coefficient ] , it is conceivable that @xmath95 may sometimes not be measurable . in such cases , we can define @xmath96 as the _ outer _ measure @xcite , so that it remains well defined .",
    "we continue this practice below , letting @xmath97 and @xmath98 ( and indeed any reference to `` probability '' ) refer to the outer expectation and measure in any context for which this is necessary.=1    because of its simple intuitive interpretation , measuring the amount of disagreement in a local neighborhood of some classifier @xmath25 , the disagreement coefficient has the wonderful property of being relatively simple to calculate for a wide range of learning problems , especially when those problems have a natural geometric representation . to illustrate this",
    ", we will go through a few simple examples from @xcite .",
    "consider the hypothesis class of thresholds @xmath99 on the interval @xmath100 $ ] [ for @xmath101 , where @xmath102 iff @xmath103 .",
    "furthermore , suppose @xmath5 is uniform on @xmath100 $ ] . in this case",
    ", it is clear that the disagreement coefficient is @xmath104 , since for sufficiently small @xmath105 , the region of disagreement of @xmath106 is @xmath107 , which has probability mass @xmath108 . in other words , since the disagreement region grows with @xmath105 in two disjoint directions , each at rate @xmath30 , we have @xmath109 .    as a second example , consider the disagreement coefficient for _ intervals _ on @xmath100 $ ] .",
    "as before , let @xmath110 $ ] and @xmath5 be uniform , but this time @xmath19 is the set of intervals @xmath111}$ ] such that for @xmath112 $ ] , @xmath111}(x ) = + 1 $ ] iff @xmath113 $ ] ( for @xmath114 ) .",
    "in contrast to thresholds , the disagreement coefficients @xmath115}}$ ] for the space of intervals vary widely depending on the particular @xmath111}$ ] .",
    "specifically , we have @xmath116 } } = \\max\\{\\frac{1}{b - a } , 4\\}$ ] . to see this , note that when @xmath117 , every interval in @xmath118},r)$ ] has its lower and upper boundaries within @xmath105 of @xmath119 and @xmath120 , respectively ; thus , @xmath121},r ) ) ) \\leq4 r$ ] , with equality for sufficiently small @xmath105 .",
    "however , when @xmath122 , _ every _ interval of @xmath123 is in @xmath118},r)$ ] , so that @xmath121},r ) ) ) = 1 $ ] .    as a slightly more involved example",
    ", @xcite studies the scenario where @xmath6 is the surface of the origin - centered unit sphere in @xmath124 for @xmath125 , @xmath19 is the space of all linear separators whose decision surface passes through the origin , and @xmath126 is the uniform distribution on @xmath6 ; in this case , it turns out @xmath127 the disagreement coefficient @xmath128 satisfies @xmath129 the disagreement coefficient has many interesting properties that can help to bound its value for a given hypothesis class and distribution .",
    "we list a few elementary properties below . their proofs , which are quite short and follow directly from the definition ,",
    "are left as easy exercises .",
    "[ lem : close ] suppose @xmath130 $ ] s.t . for any measurable set @xmath131 , @xmath132 .",
    "let @xmath22 be a measurable classifier , and suppose @xmath128 and @xmath133 are the disagreement coefficients for @xmath25 with respect to @xmath19 under @xmath126 and @xmath134 , respectively .",
    "then @xmath135    [ lem : mixtures ] suppose @xmath136 $ ] s.t . for any measurable set @xmath131 , @xmath137 . for a measurable @xmath22 ,",
    "let @xmath138 be the disagreement coefficient with respect to @xmath19 under @xmath139 , @xmath140 be the disagreement coefficient with respect to @xmath19 under @xmath141 , and @xmath128 be the disagreement coefficient with respect to @xmath19 under @xmath5 .",
    "then @xmath142    [ lem : unions ] suppose @xmath143 is a classifier s.t .",
    "the disagreement coefficient with respect to @xmath144 under @xmath126 is @xmath138 and with respect to @xmath145 under @xmath126 is @xmath140 . then if @xmath128 is the disagreement coefficient with respect to @xmath146 under @xmath126 , we have that @xmath147 in fact , even if @xmath148 , we still have @xmath149 .",
    "see @xcite for further discussions of various uses of the disagreement coefficient and related notions and extensions in active learning .",
    "in particular , friedman @xcite proves that any hypothesis class and distribution satisfying certain general regularity conditions will admit finite constant bounds on @xmath150 .",
    "also , wang @xcite bounds the disagreement coefficient for certain nonparametric hypothesis classes , characterized by smoothness of their decision surfaces .",
    "additionally , beygelzimer , dasgupta and langford @xcite present an interesting analysis using a natural extension of the disagreement coefficient to study active learning with a larger family of loss functions beyond @xmath29@xmath30 loss .",
    "the disagreement coefficient has deep connections to several other quantities , such as doubling dimension @xcite and vc dimension @xcite .",
    "additionally , a related quantity , referred to as the `` capacity function , '' was studied in the 1980s by alexander in the passive learning literature , in the context of ratio - type empirical processes @xcite and recently was further developed by gin and koltchinskii @xcite ; interestingly , in this latter work , gin and koltchinskii study a localized version of the capacity function , which in our present context can essentially be viewed as the function @xmath151 , so that @xmath152 .",
    "we begin the discussion of the algorithms we will analyze by noting the underlying inspiration that unifies them .",
    "specifically , at this writing , all of the published general - purpose agnostic active learning algorithms achieving nontrivial improvements are derivatives of a basic technique proposed by cohn , atlas and ladner @xcite for the realizable active learning problem . under the assumption that there exists a perfect classifier in @xmath19 , they proposed an algorithm which processes unlabeled data points in sequence , and for each one it determines whether there is a classifier in @xmath19 consistent with all previously observed labels that predicts @xmath1531 for this new point _ and _ one that predicts @xmath1541 for this new point",
    "; if so , the algorithm requests the label , and otherwise it does not request the label ; after @xmath17 label requests , the algorithm returns any classifier consistent with all observed labels . in some sense , this algorithm corresponds to the very least we could expect of an active learning algorithm , as it never requests the label of a point it can derive from known information , but otherwise makes no effort to search for informative data points .",
    "the idea is appealing , not only for its simplicity , but also for its extremely efficient use of unlabeled data ; in fact , under the stated assumption , the algorithm produces a classifier consistent with the labels of _ all _ of the unlabeled data it processes , including those it does _ not _ request the labels of .",
    "we can equivalently think of this algorithm as maintaining two sets : @xmath155 is the set of candidate hypotheses still under consideration , and @xmath156 is their region of disagreement .",
    "we can then think of the algorithm as requesting a random labeled point from the conditional distribution of @xmath3 given that @xmath157 , and subsequently removing from @xmath40 any classifier inconsistent with the observed label .",
    "a formal definition of the algorithm is given as follows .",
    "* algorithm 0 * + input : hypothesis class @xmath19 , label budget @xmath17 + output : classifier @xmath158 + -2mm(1,0)346 + 0 .",
    "@xmath159 , @xmath160 + 1 . for @xmath161 + 2 .",
    "if @xmath162 , + 3 .",
    "request @xmath163 + 4 .",
    "@xmath164 + 5 .",
    "@xmath165 + 6 .",
    "if @xmath166 or @xmath167 , return any @xmath168    the algorithms described below for the problem of active learning with label noise each represent noise - robust variants of this basic idea .",
    "they work to reduce the set of candidate hypotheses , while only requesting the labels of points in the region of disagreement of these candidates .",
    "the trick is to only remove a classifier from the candidate set once we have high statistical confidence that it is worse than some other candidate classifier so that we never remove the best classifier .",
    "however , the two algorithms differ somewhat in the details of how that confidence is calculated .",
    "the first noise - robust algorithm we study , originally proposed by balcan , beygelzimer and langford @xcite , is typically referred to as @xmath0 for _ agnostic active_. this was historically the first general - purpose agnostic active learning algorithm shown to achieve improved error guarantees for certain learning problems in certain ranges of @xmath17 and @xmath93 .",
    "below is a variant of this algorithm .",
    "it is defined in terms of two functions : @xmath169 and @xmath170 .",
    "these represent upper and lower confidence bounds on the error rate of a classifier from @xmath19 with respect to an arbitrary sampling distribution , as a function of a labeled sequence sampled according to that distribution . some steps in the algorithm",
    "require calculating certain probabilities , such as @xmath171 or @xmath172 ; later , we discuss replacing these with appropriate estimators .",
    "* algorithm 1 * + input : hypothesis class @xmath19 , label budget @xmath17 , confidence @xmath173 , functions @xmath169 and @xmath170 + output : classifier @xmath59 + -2mm(1,0)346 + 0 .",
    "@xmath174 , @xmath175 , @xmath176 , @xmath177 + 1 . for @xmath178 + 2 .",
    "if @xmath179 + 3 . @xmath180 ; @xmath176 + 4 .",
    "if @xmath181 , return any @xmath182 + 5 .",
    "@xmath183 + 6 .",
    "request @xmath163 and let @xmath184 + 7 . @xmath185 + 8 .",
    "@xmath186 + 9 .",
    "@xmath187 + 10 .",
    "return @xmath188 , where @xmath189    the intuitive motivation behind the algorithm is the following .",
    "it focuses on reducing the set of candidate hypotheses @xmath40 , while being careful not to throw away the best classifier @xmath190 ( supposing , for this informal explanation , that @xmath191 exists ) .",
    "given that this is satisfied at any given time in the algorithm , it makes sense to focus our samples to the region @xmath192 , since a classifier @xmath193 has smaller error rate than another classifier @xmath194 if and only if it has smaller conditional error rate given @xmath192 .",
    "for this reason , on each round , we seek to remove from @xmath40 any @xmath25 for which our confidence bounds indicate that @xmath195 .",
    "however , so that we can make use of known results for i.i.d .",
    "samples , we freeze the sampling region @xmath196 and collect an i.i.d . sample from the conditional given this region , updating the region only when doing so allows us to further significantly focus the samples ; for this same reason , we also reset the collection of samples @xmath197 every time we update the region @xmath198 , so that it represents samples from the conditional given @xmath198 .",
    "finally , we maintain the values @xmath199 , which represent confidence upper bounds on @xmath200 , and we return the @xmath201 minimizing this confidence bound ; note that it does not suffice to return @xmath202 , since the final @xmath197 set might be small .    as long as the confidence bounds @xmath169 and @xmath170 satisfy ( overloading notation in the natural way ) @xmath203 for any distribution @xmath23 over @xmath24 and any @xmath204 , and @xmath169 and @xmath170 converge to each other as @xmath57 grows , it is known that a @xmath205 confidence bound on @xmath206 converges to @xmath29 @xcite .",
    "for instance , balcan , beygelzimer and langford @xcite suggest defining these functions based on classic results on uniform convergence rates in passive learning @xcite , such as @xmath207\\\\[-8pt ] { \\mathit{lb}}(h , q,\\delta^\\prime ) & = & \\max\\{{\\mathit{er}}_q(h ) - g(|q|,\\delta^\\prime),0\\ } , \\nonumber\\end{aligned}\\ ] ] where @xmath208 for @xmath209 , and by convention @xmath210 for @xmath211 .",
    "this choice of @xmath169 and @xmath170 is motivated by the following lemma , due to vapnik @xcite .",
    "[ lem : uniform ] for any distribution @xmath23 over @xmath24 , and any @xmath212 and @xmath213 , with probability @xmath214 over the draw of @xmath215 , every @xmath216 satisfies @xmath217    to avoid computational issues , instead of explicitly representing the sets @xmath40 and  @xmath198 , we may implicitly represent them as a set of constraints imposed by the condition in step 7 of previous iterations .",
    "we may also replace @xmath171 and @xmath172 by estimates , since these quantities can be estimated to arbitrary precision with arbitrarily high confidence using only _ unlabeled _ data . specifically , the convergence rates proven below can be preserved up to constant factors by replacing these quantities with confidence bounds based on a finite number of unlabeled data points ; the details of this are included in appendix c of the supplementary material  @xcite . as for the number of unlabeled data points required by the above algorithm itself , note that if @xmath171 becomes small , it will use a large number of unlabeled data points ; however , @xmath171 being small also indicates @xmath218 is small ( and indeed @xmath199 ) . in particular , to get an excess error rate of @xmath219 , the algorithm will generally require a number of unlabeled data points only polynomial in @xmath220 ; also , the condition in step 4 guarantees the total number of unlabeled data points used by the algorithm is bounded with high probability . for comparison , recall that passive learning typically requires a number of _ labeled _ data points polynomial in @xmath220 .",
    "the second noise - robust algorithm we study was originally proposed by dasgupta , hsu and monteleoni @xcite .",
    "it uses a type of constrained passive learning subroutine , learn , defined as follows for two sets of labeled data points , @xmath221 and @xmath197 .",
    "@xmath222 by convention , if no @xmath216 has @xmath223 , @xmath224 .",
    "the algorithm is formally defined below , in terms of a sequence of estimators @xmath225 , defined later .",
    "* algorithm 2 * + input : hypothesis class @xmath19 , label budget @xmath17 , confidence @xmath173 , functions @xmath225 + output : classifier @xmath59 , sets of ( index , label ) pairs @xmath221 and @xmath197 + -2mm(1,0)346 + 0 .",
    "@xmath226 , @xmath227 + 1 . for @xmath161 + 2 . if @xmath228 or @xmath229 , return @xmath230 along with @xmath221 and @xmath197 + 3 . for each @xmath231 , let @xmath232 + 4 . if some @xmath233 has @xmath234 or + 2.5@xmath235 + 5.then @xmath236 + 6 .",
    "else request the label @xmath163 and let @xmath237    the algorithm maintains two sets of labeled data points : @xmath221 and @xmath197 .",
    "the set @xmath197 represents points of which we have requested the labels .",
    "the set @xmath221 represents the remaining points , and the labels of points in @xmath221 are _",
    "inferred_. specifically , suppose ( inductively ) that at some time @xmath57 we have that every @xmath238 has @xmath239 , where @xmath240 ( supposing the min is achieved , for this informal motivation ) .",
    "at any point , we can be fairly confident that @xmath191 will have relatively small empirical error rate .",
    "thus , if all of the classifiers @xmath25 with @xmath241 and @xmath242 have relatively large empirical error rates compared to some @xmath25 with @xmath241 and @xmath243 , we can confidently infer that @xmath244 .",
    "note that this is not the _ true _",
    "label @xmath245 , but a sort of `` denoised '' version of it .",
    "once we infer this label , since we are already confident that this is the @xmath191 label , and @xmath191 is the classifier we wish to compete with , we simply add this label as a _ constraint _ : that is , we require every classifier under consideration in the future to have @xmath246 .",
    "this is how elements of @xmath221 are added . on the other hand ,",
    "if we can not confidently infer @xmath247 , because some classifiers labeling @xmath248 as @xmath249 also have relatively small empirical error rates , then we simply request the label @xmath245 and add it to the set @xmath197 .",
    "note that in order to make this comparison , we needed to be able to calculate the differences of empirical error rates ; however , as long as we only consider the set of classifiers @xmath25 that _ agree _ on the labels in @xmath221 , we will have @xmath250 , for any two such classifiers @xmath251 and  @xmath252 , where @xmath253 .",
    "the key to the above argument is carefully choosing a threshold for how large the difference in empirical error rates needs to be before we can confidently infer the label .",
    "for this purpose , algorithm 2 is defined in terms of a function , @xmath254 , representing a threshold for a type of hypothesis test .",
    "this threshold must be set carefully , since the sequence of labeled data points corresponding to @xmath255 is not actually an i.i.d .",
    "sample from @xmath3 .",
    "dasgupta , hsu and monteleoni @xcite suggest defining this function as @xmath256 where @xmath257 and @xmath258 is the shatter coefficient ( e.g. , @xcite ) ; this suggestion is based on a confidence bound they derive , and they prove the correctness of the algorithm with this definition , meaning that the @xmath205 confidence bound on its error rate converges to @xmath93 as @xmath259 .",
    "for now we will focus on the first return value ( the classifier ) , leaving the others for section [ sec : aggregation ] , where they will be useful for chaining multiple executions together .",
    "in both of the above cases , one can prove guarantees stating that neither algorithm s convergence rates are ever significantly worse than passive learning by empirical risk minimization @xcite . however , it is even more interesting to discuss situations in which one can prove error rate guarantees for these algorithms significantly _",
    "better _ than those achievable by passive learning . in this section , we begin by reviewing known results on these potential improvements , stated in terms of the disagreement coefficient ; we then proceed to discuss new results for algorithm 1 and a novel variant of algorithm 2 , and describe the convergence rates achieved by these methods in terms of the disagreement coefficient and tsybakov s noise conditions .    to simplify the presentation , for the remainder of this paper we will restrict the discussion to situations with @xmath260 ( and therefore @xmath19 with @xmath261 too ) .",
    "handling the extra case of @xmath262 is a trivial matter , since @xmath263 would imply that any proper learning algorithm achieves excess error @xmath29 for all values of @xmath17 .      before going into the results for general distributions @xmath3 on @xmath24",
    ", it will be instructive to first look at the special case when the noise rate is zero .",
    "understanding how the disagreement coefficient enters into the analysis of this simpler case may aid in digestion of the theorems and proofs for the general case presented later , where it plays an essentially analogous role .",
    "most of the major ingredients of the proofs for the general case can be found in this special case , albeit in a much simpler form .",
    "although this result has not previously been published , the proof is essentially analogous to ( one case of ) the analysis of algorithm 1 in @xcite .",
    "[ thm : cal - upper ] let @xmath264 be such that @xmath265 and @xmath266 . @xmath267 and @xmath268 , with probability @xmath269 over the draw of the unlabeled data , the classifier @xmath59 returned by algorithm 0 after @xmath17 label requests satisfies",
    "@xmath270    as in the algorithm , let @xmath271 denote the set of classifiers in @xmath19 consistent with the first @xmath272 label requests . if @xmath273 for all values of @xmath272 in the algorithm , then with probability @xmath30 the algorithm uses all @xmath17 label requests .",
    "technically , each claim below should be followed by the phrase , `` unless @xmath274 for some @xmath275 , in which case @xmath276 so the bound trivially holds . ''",
    "however , to simplify the presentation , we will make this special case implicit , and will not mention it further .",
    "the high - level outline of this proof is to use @xmath277 as an upper bound on @xmath278 , and then show @xmath277 is halved roughly every @xmath279 label requests .",
    "thus , after roughly @xmath280 label requests , any @xmath281 should have @xmath282 .    specifically , let @xmath283 .",
    "if @xmath284 , the bound in the theorem statement trivially holds , since the right - hand side exceeds @xmath30 ; otherwise , consider some nonnegative @xmath285 and @xmath286 .",
    "let @xmath287 denote the point corresponding to the @xmath272th label request , and let @xmath288 denote the point corresponding to label request number @xmath289 .",
    "it must be that @xmath290 which means there is an i.i.d .",
    "sample of size @xmath291 , with distribution equivalent to the conditional of @xmath292 given @xmath293 , contained in @xmath294 : namely , the first @xmath291 points in this subsequence that are in @xmath295 .",
    "now recall that , by classic results from the passive learning literature ( e.g. , @xcite ) , this implies that on an event @xmath296 holding with probability @xmath297 , @xmath298 also note that @xmath291 was defined ( with express purpose ) so that @xmath299 recall that , since @xmath265 , we have @xmath300 . since @xmath301 , this means for any @xmath302 we have @xmath303 , and thus @xmath304 so @xmath305 , and therefore by monotonicity of @xmath306 and the definition of @xmath307 @xmath308 by a union bound , @xmath296 holds for every @xmath309 with probability @xmath80 . on these events ,",
    "if @xmath310 , then ( by induction ) @xmath311 solving for @xmath219 in terms of @xmath17 gives the result ( with a slight increase in constants due to relaxing the ceiling functions ) .",
    "we will now describe the known results for agnostic active learning algorithms , starting with algorithm 1 .",
    "the key to the potential convergence rate improvements of algorithm 1 is that , as the region of disagreement @xmath198 decreases in measure , the error difference @xmath312 of any classifiers @xmath313 under the _ conditional _ sampling distribution ( given @xmath198 ) can become significantly larger [ by a factor of @xmath314 than @xmath315 , making it significantly easier to determine which of the two is worse using a sample of labeled data . in particular , @xcite developed a technique for analyzing this type of algorithm , and adapting that analysis to the above definition of algorithm 1 results in the following guarantee .",
    "let @xmath59 be the classifier returned by algorithm 1 when allowed @xmath17 label requests , using the bounds ( [ eqn : bbl - bounds ] ) and confidence parameter @xmath316 .",
    "then there exists a finite universal constant @xmath82 such that , with probability @xmath80 , @xmath317 , @xmath318    similarly , the key to improvements from algorithm 2 is that as the number @xmath57 of processed unlabeled data points increases , we only need to request the labels of those data points in the region of disagreement of the set of classifiers with near - optimal empirical error rates .",
    "thus , if the region of disagreement of classifiers with excess error @xmath319 shrinks as @xmath219 shrinks , we expect the frequency of label requests to shrink as @xmath57 increases .",
    "since we are careful not to discard the best classifier , and the excess error rate of a classifier can be bounded in terms of the @xmath320 function , we end up with a bound on the excess error which is converging in @xmath57 , the number of _ unlabeled _ data points processed , even though we request a number of labels growing slower than @xmath57 .",
    "when this situation occurs , we expect algorithm 2 will provide an improved convergence rate compared to passive learning .",
    "dasgupta , hsu and monteleoni @xcite prove the following convergence rate guarantee .",
    "let @xmath59 be the classifier returned by algorithm 2 when allowed @xmath17 label requests , using the threshold ( [ eqn : dhm - bound ] ) , and confidence parameter @xmath316 .",
    "then there exists a finite universal constant @xmath82 such that , with probability @xmath80 , @xmath267 , @xmath321    note that , among other changes , this bound improves the dependence on the disagreement coefficient @xmath150 , compared to the bound for algorithm 1 . in both cases , for certain ranges of @xmath150 , @xmath93 and @xmath17",
    ", these bounds can represent significant improvements in the excess error guarantees , compared to the corresponding guarantees possible for passive learning .",
    "however , in both cases , when @xmath322 these bounds have an _ asymptotic _ dependence on @xmath17 of @xmath323 , which is no better than the convergence rates achievable by passive learning ( e.g. , by empirical risk minimization ) .",
    "thus , there remains the question of whether either algorithm can achieve asymptotic convergence rates strictly superior to passive learning for distributions with nonzero noise rates .",
    "this is the topic we turn to next .",
    "it is known that for most nontrivial @xmath19 , for any @xmath17 and @xmath322 , for every active learning algorithm there is some distribution with noise rate @xmath93 for which we can guarantee excess error no better than @xmath324 @xcite ; that is , the @xmath61 asymptotic dependence on @xmath17 in the above bounds matches the corresponding minimax rate , and thus can not be improved as long as the bounds depend on @xmath3 only via @xmath93 ( and @xmath150 ) .",
    "therefore , if we hope to discover situations in which these algorithms have strictly superior asymptotic dependence on @xmath17 , we will need to allow the bounds to depend on a more detailed description of the noise distribution than simply the noise rate @xmath93 .",
    "as previously mentioned , one way to describe a noise distribution using a more detailed parametrization is to use tsybakov s noise conditions ( condition [ con : tsybakov ] ) . in the context of passive learning ,",
    "this allows one to describe situations in which  the rate of convergence is between @xmath325 and @xmath61 , even when @xmath322 .",
    "this raises the natural question of how these active learning algorithms perform when the noise distribution satisfies this condition with finite @xmath69 and @xmath68 parameter values . in many ways",
    ", it seems active learning is particularly well - suited to exploit these more favorable noise conditions , since they imply that as we eliminate suboptimal classifiers , the diameter of the remaining set shrinks ; thus , for finite @xmath150 values , the region of disagreement should also be shrinking , allowing us to focus the samples in a smaller region and accelerate the convergence .",
    "focusing on the special case of learning one - dimensional threshold classifiers under a certain uniform marginal distribution , castro and nowak @xcite studied conditions related to condition [ con : tsybakov ] .",
    "in particular , they studied a threshold - learning algorithm that , unlike the algorithms described here , takes @xmath68 as _ input _ , and found its convergence rate to be @xmath326 when @xmath327 , and @xmath328 for some ( @xmath69-dependent ) constant @xmath82 , when @xmath329 .",
    "note that this improves over the @xmath330 rates achievable in passive learning @xcite .",
    "subsequently , balcan , broder and zhang @xcite proved an analogous positive result for higher - dimensional linear separators , and castro and nowak @xcite additionally showed a related result for boundary fragment classes ( see below ) ; in both cases , the algorithm depends explicitly on the noise parameters .",
    "later , in a personal communication , langford and castro claimed that in fact algorithm 1 achieves this rate ( up to log factors ) for the one - dimensional thresholds problem , leading to speculation that perhaps these improvements are achievable in the general case as well ( under conditions on the disagreement coefficient ) .",
    "castro and nowak @xcite also prove that a value @xmath331 ( or @xmath332 , for some @xmath333 , when @xmath334 ) is also a _ lower bound _ on the minimax rate for the threshold learning problem .",
    "in fact , a similar proof to theirs can be used to show this same lower bound holds for any nontrivial @xmath19 . for completeness ,",
    "a proof of this more general result is included in appendix d of the supplementary material @xcite .",
    "other than the few specific results mentioned above , it was not previously known whether algorithm 1 or algorithm 2 , or indeed _ any _ active learning algorithm , generally achieves convergence rates that exhibit these types of improvements .",
    "the above observations open the question of whether these algorithms , or variants thereof , improve this asymptotic dependence on @xmath17 .",
    "it turns out this is indeed possible .",
    "specifically , we have the following result for algorithm 1 .",
    "[ thm : bbl - adaptive ] let @xmath59 be the classifier returned by algorithm 1 when allowed @xmath17 label requests , using the bounds ( [ eqn : bbl - bounds ] ) and confidence parameter @xmath316 .",
    "suppose further that @xmath3 satisfies condition [ con : tsybakov ] .",
    "then there exists a finite ( @xmath68- and @xmath69-dependent ) constant @xmath82 such that , for any @xmath335 , with probability @xmath269 , @xmath336    we will proceed by bounding the _ label complexity _ , or size of the label budget @xmath17 that is sufficient to guarantee , with high probability , that the excess error of the returned classifier will be at most @xmath219 ( for arbitrary @xmath42 ) ; with this in hand , we can simply bound the inverse of the function to get the result in terms of a bound on excess error .    throughout this proof ( and proofs of later results in this paper ) , we will make frequent use of basic facts about @xmath337 . in particular , for any classifiers @xmath338 and set @xmath339 , we have @xmath340 ; also , if @xmath341 , we have @xmath342 and therefore @xmath343 .",
    "note that , by lemma [ lem : uniform ] and a union bound , on an event of probability @xmath205 , ( [ eqn : uniform ] )  holds with @xmath344 for every set @xmath197 , relative to the conditional distribution given its respective @xmath198 set , for any value of @xmath17 . for the remainder of this proof , we assume that this @xmath205 probability event occurs . in particular",
    ", this means that for every @xmath216 and every @xmath197 set in the algorithm , @xmath345 , for the set @xmath198 that @xmath197 is sampled under .",
    "our first task is to show that we never remove the `` good '' classifiers from @xmath40 .",
    "we only remove a classifier @xmath25 from @xmath40 if @xmath346 has @xmath347 .",
    "each @xmath348 has @xmath349 , so that @xmath350 thus , for any @xmath348 with @xmath351 , @xmath352 , so that on any given round of the algorithm , the set @xmath353 is not removed from @xmath40 . in particular , since we always have @xmath354 , by induction this implies the invariant @xmath355 , and therefore also @xmath356 where again the second equality is due to the fact that @xmath357 , @xmath358 @xmath359 .",
    "we will spend the remainder of the proof bounding the size of @xmath17 sufficient to guarantee some @xmath360 .",
    "in particular , similar to the proof of theorem  [ thm : cal - upper ] , we will see that as long as @xmath361 , we will halve @xmath171 roughly every @xmath362 label requests , so that the total number of label requests before some @xmath363 is at most roughly @xmath364",
    ".    recalling the definition of @xmath94}$ ] ( from definition [ def : global - disagreement - coefficient ] ) , let @xmath365}(x)\\bigr ) > \\frac{{\\mathbb p}(r)}{2\\theta}\\biggr\\}.\\ ] ] note that after step 7 , if @xmath366 , then @xmath367}(x)\\bigr ) \\leq { \\mathbb p}(r)/(2\\theta)\\bigr\\}\\bigr)\\bigr ) \\\\ &",
    "= & \\lim_{k^\\prime\\rightarrow\\infty } { \\mathbb p}\\biggl(\\operatorname{dis } \\biggl(\\bigcap_{k > k^\\prime } b\\bigl(h^{[k]},{\\mathbb p}(r)/(2\\theta ) \\bigr)\\biggr)\\biggr)\\\\ & \\leq&\\lim_{k^\\prime\\rightarrow\\infty } { \\mathbb p}\\biggl(\\bigcap _ { k > k^\\prime } \\operatorname{dis}\\bigl(b\\bigl(h^{[k ] } , { \\mathbb p}(r)/(2\\theta ) \\bigr)\\bigr)\\biggr)\\\\ & \\leq&\\liminf_{k \\rightarrow\\infty } { \\mathbb p}\\bigl(\\operatorname{dis } \\bigl(b\\bigl(h^{[k ] } , { \\mathbb p}(r)/(2\\theta)\\bigr)\\bigr)\\bigr ) \\\\ & \\leq & \\liminf _ { k\\rightarrow\\infty } \\theta_{h^{[k ] } } \\frac{{\\mathbb p}(r)}{2\\theta } = \\frac{{\\mathbb p}(r)}{2},\\end{aligned}\\ ] ] so that we will satisfy the condition in step 2 on the next round .",
    "here we have used the definition of @xmath150 in the final inequality and equality . on the other hand ,",
    "if after step 7 , we have @xmath368 , then @xmath369}(x)\\bigr ) > \\frac{{\\mathbb p}(r)}{2\\theta}\\biggr\\ } \\\\ & = & \\biggl\\{h \\in v \\dvtx\\biggl(\\frac{\\limsup_{k \\rightarrow \\infty } { \\mathbb p}(h(x ) \\neq h^{[k]}(x))}{\\mu}\\biggr)^{\\kappa } > \\biggl(\\frac{{\\mathbb p}(r)}{2\\mu\\theta}\\biggr)^{\\kappa}\\biggr\\}\\\\ & \\subseteq & \\biggl\\{h \\in v \\dvtx\\biggl(\\frac{{\\operatorname{diam}}({\\mathit{er}}(h)-\\nu;{\\mathbb c})}{\\mu } \\biggr)^{\\kappa } > \\biggl(\\frac{{\\mathbb p}(r)}{2\\mu\\theta } \\biggr)^{\\kappa}\\biggr\\}\\\\ & \\subseteq & \\biggl\\{h \\in v \\dvtx { \\mathit{er}}(h)-\\nu >",
    "\\biggl(\\frac{{\\mathbb p}(r)}{2\\mu \\theta}\\biggr)^{\\kappa}\\biggr\\}\\\\ & = & \\bigl\\{h \\in v \\dvtx { \\mathit{er}}(h|r)-\\inf_{h^\\prime\\in v } { \\mathit{er}}(h^\\prime| r ) > { \\mathbb p}(r)^{\\kappa-1}(2\\mu\\theta)^{-\\kappa}\\bigr\\}\\\\ & \\subseteq & \\bigl\\{h \\in v \\dvtx { \\mathit{ub}}(h , q,\\delta / n)-\\min_{h^\\prime\\in v } { \\mathit{lb}}(h^\\prime , q,\\delta / n ) > { \\mathbb p}(r)^{\\kappa-1}(2\\mu\\theta)^{-\\kappa } \\bigr\\}\\\\ & \\subseteq & \\bigl\\{h \\in v \\dvtx { \\mathit{lb}}(h , q,\\delta / n)-\\min_{h^\\prime\\in v } { \\mathit{ub}}(h^\\prime , q,\\delta / n ) \\\\ & & \\hspace*{40.2pt } > { \\mathbb p}(r)^{\\kappa-1}(2\\mu\\theta)^{-\\kappa } - 4g(|q|,\\delta / n)\\bigr\\}.\\end{aligned}\\ ] ] here , the third line follows from the fact that @xmath92 } ) \\leq { \\mathit{er}}(h)$ ] for all sufficiently large @xmath370 , the fourth line follows from condition [ con : tsybakov ] , and the final line follows from the definition of @xmath169 and @xmath170 . by definition , every @xmath348 has @xmath371 , so for this last set to be nonempty after step 7 , we must have @xmath372 .    combining these two cases ( @xmath366 and @xmath373 ) ,",
    "since @xmath374 gets reset to @xmath29 upon reaching step 3 , we have that after every execution of step 7 , @xmath375    if @xmath376 , then certainly @xmath360 ( by definition of @xmath377 ) .",
    "so on any round for which @xmath361 , we must have @xmath378 combining ( [ eqn : pg - bound-1 ] ) and ( [ eqn : pg - bound-2 ] ) , on any round for which @xmath361 , @xmath379 solving for @xmath380 reveals that when @xmath361 , @xmath381 basic algebra shows that when @xmath382 , we have @xmath383 combining this with ( [ eqn : geps - bound ] ) , solving for @xmath374 and adding @xmath21 to handle the case @xmath384 , we have that on any round for which @xmath361 , @xmath385 since @xmath386 by definition , and @xmath172 is at least halved each time we reach step  3 , we need to reach step 3 at most @xmath387 times before we are guaranteed some @xmath360 . thus , any @xmath388 suffices to guarantee either some @xmath374 exceeds ( [ eqn : q - bound ] ) or we reach step 3 at least @xmath389 times , either of which implies the existence of some @xmath360 .",
    "the stated result now follows by basic inequalities to bound the smallest value of @xmath219 satisfying ( [ eqn : label - complexity ] ) for a given value of @xmath17 .",
    "if the disagreement coefficient is finite , theorem [ thm : bbl - adaptive ] can often represent a significant improvement in convergence rate compared to passive learning , where we typically expect rates of order @xmath390 @xcite ; this gap is especially notable when the disagreement coefficient and @xmath68 are small .",
    "furthermore , the bound matches ( up to logarithmic factors ) the form of the minimax rate _ lower bound _ proved by castro and nowak @xcite for threshold classifiers ( where @xmath391 ) ; as mentioned , that lower bound proof can be generalized to any nontrivial @xmath19 ( see appendix d of the supplementary material @xcite ) , so that the rate of theorem [ thm : bbl - adaptive ] is nearly minimax optimal for any nontrivial @xmath19 with _ bounded _ disagreement coefficients .",
    "also note that , unlike the upper bound analysis of castro and nowak @xcite , we do not require the algorithm to be given any extra information about the noise distribution , so that this result is somewhat stronger ; it is also more general , as this bound applies to an arbitrary hypothesis class .",
    "a refined analysis and minor tweaks to the algorithm should be able to reduce the log factors in this result .",
    "for instance , defining @xmath169 and @xmath170 using the uniform convergence bounds of alexander @xcite , and using a slightly more complicated algorithm closer to the original definition @xcite  taking multiple samples between bound evaluations , allowing a larger confidence argument to the @xmath169 and @xmath170 evaluations  the @xmath392 factor should reduce at least to @xmath393 , if not further .",
    "also , as previously mentioned , it is possible to replace the quantities @xmath172 and @xmath171 in algorithm 1 with estimators of these quantities based on a finite sample of unlabeled data points , while preserving the results of theorem [ thm : bbl - adaptive ] up to constant factors .",
    "we include an example of such estimators in appendix c of the supplementary material @xcite , along with a sketch of how to modify the proof of theorem [ thm : bbl - adaptive ] to compensate for using these estimated probabilities .",
    "note that , as before , @xmath17  gets divided by @xmath394 in the rates achieved by algorithm 1 .",
    "as before , it is not clear whether any modification to the definitions of @xmath169 and @xmath170 can reduce this exponent on @xmath150 from @xmath104 to @xmath30 . as such , it is natural to investigate the rates achieved by algorithm 2 under condition [ con : tsybakov ] ; we know that it does improve the dependence on @xmath150 for the worst case rates over distributions with any given noise rate , so we might hope that it does the same for the rates over distributions with any given values of @xmath69 and @xmath68 .",
    "unfortunately , we do not presently know whether the original definition of algorithm 2 achieves this improvement",
    ". however , we now present a slight modification of the algorithm , and prove that it does indeed provide the desired improvement in dependence on @xmath150 , while maintaining the improvements in the asymptotic dependence on @xmath17 . specifically , consider the following definition for the threshold in algorithm 2 : @xmath395 where @xmath396 is defined in the , based on a notion of local rademacher complexity studied by koltchinskii @xcite . in particular , the quantity @xmath397 is known to be adaptive to tsybakov s noise conditions , in the sense that more favorable noise conditions yield smaller values of @xmath397 . using this definition",
    ", we have the following theorem ; due to space limitations , its proof is not presented here , but is included in appendix b of the supplementary material @xcite .",
    "[ thm : tight - agnostic ] suppose @xmath59 is the classifier returned by algorithm 2 with threshold as in ( [ eqn : dhm - tight ] ) , when allowed @xmath17 label requests and given confidence parameter @xmath316 .",
    "suppose further that @xmath3 satisfies condition [ con : tsybakov ] with finite parameter values @xmath68 and @xmath69 .",
    "then there exists a finite ( @xmath68 and @xmath69-dependent ) constant @xmath82 such that , with probability @xmath80 , @xmath267 , @xmath398    note that this does indeed improve the dependence on @xmath150 , reducing its exponent from @xmath104 to @xmath30 ; we do lose some in that there is now a square root in the exponent of the @xmath334 case ; however , as with theorem [ thm : bbl - adaptive ] , it is likely that slight refinements to the definition of @xmath320 would reduce this ( though we may also need to weaken the theorem statement to hold for any single @xmath17 , rather than simultaneously for all @xmath17 ) .",
    "the bound in theorem [ thm : tight - agnostic ] is stated in terms of the vc dimension @xmath21 .",
    "however , for certain nonparametric hypothesis classes , it is sometimes preferable to quantify the complexity of the class in terms of a constraint on the _ entropy _ of the class , relative to the distribution @xmath3 ( see e.g. , @xcite ) .",
    "specifically , for @xmath399 $ ] , define @xmath400    [ con : entropy ] there exist finite constants @xmath401 and @xmath402 s.t . @xmath403 and @xmath399 $ ] , @xmath404 .",
    "in particular , the entropy with bracketing condition used in the original minimax analysis of tsybakov @xcite implies condition [ con : entropy ] @xcite , as does the analogous condition for random entropy @xcite . in passive learning",
    ", it is known that empirical risk minimization achieves a rate of order @xmath405 under conditions  [ con : tsybakov ] and [ con : entropy ] @xcite ( see also appendix b of the supplementary material @xcite , especially ( 19 ) and lemma 5 ) , and that this is sometimes minimax optimal @xcite .",
    "the following theorem gives a bound on the rate of convergence of the same version of algorithm  2 as in theorem [ thm : tight - agnostic ] , this time in terms of the entropy condition which , as before , is faster than the passive learning rate when the disagreement coefficient is finite .",
    "the proof of this result is included in appendix b of the supplementary material @xcite .",
    "[ thm : tight - agnostic - entropy ] suppose @xmath59 is the classifier returned by algorithm 2 with threshold as in ( [ eqn : dhm - tight ] ) , when allowed @xmath17 label requests and given confidence parameter @xmath316 .",
    "suppose further that @xmath3 satisfies condition [ con : tsybakov ] with finite parameter values @xmath68 and @xmath69 , and condition [ con : entropy ] with parameter values @xmath70 and @xmath406 .",
    "then there exists a finite ( @xmath68 , @xmath69 , @xmath70 and @xmath406-dependent ) constant @xmath82 such that , with probability @xmath80 , @xmath267 , @xmath407    again , it is likely that refinements to the @xmath320 definition may lead to improvements in the log factor .",
    "also , although this result is stated for algorithm 2 , it is conceivable that , by modifying algorithm 1 to use definitions of @xmath40 and @xmath199 based on @xmath408 , an analogous result might be possible for algorithm 1 as well .",
    "it is worth mentioning that castro and nowak @xcite proved a minimax lower bound for the hypothesis class of _ boundary fragments _ , with an exponent having a similar dependence on related definitions of @xmath68 and @xmath406 parameters to that of theorem [ thm : tight - agnostic - entropy ] .",
    "their result does provide a valid lower bound here ; however , it is not clear whether their lower bound , theorem [ thm : tight - agnostic - entropy ] , both , or neither is tight in the present context , since the value of @xmath150 is not presently known for that particular problem , and the matching upper bound of @xcite was proven under a stronger restriction on the noise than condition [ con : tsybakov ] . however , see @xcite for an analysis of the disagreement coefficient for other nonparametric hypothesis classes , characterized by smoothness of the decision surface .",
    "while the previous sections address adaptation to the noise distribution , they are still restrictive in that they deal with hypothesis classes of limited expressiveness .",
    "that is , the assumption of finite vc dimension implies a strong restriction on the variety of classifiers one can represent ( or approximate ) in the class ; the entropy conditions allow slightly more flexibility , but under nontrivial distributions , even the entropy conditions imply a significant restriction on the expressiveness of the class .",
    "thus , for algorithms restricted to classifiers from such a restricted hypothesis class , it is often unrealistic to expect convergence to the bayes error rate .",
    "we address this issue in this section by developing a general algorithm for learning with a sequence of nested hypothesis classes of increasing complexity , similar to the setting of structural risk minimization in passive learning @xcite .",
    "the objective is to adapt , not only to the noise conditions , but also to the complexity of the optimal classifier .",
    "the starting point for this discussion is the assumption of a structure on @xmath19 , in the form of a sequence of nested hypothesis classes : @xmath409 each class has an associated noise rate @xmath410 , and we define @xmath411 .",
    "we also let @xmath412 and @xmath413 be the disagreement coefficient and vc dimension , respectively , for the set @xmath414 .",
    "we are interested in an algorithm that guarantees convergence in probability of the error rate to @xmath415 .",
    "we are particularly interested in situations where @xmath416 , a condition which is realistic in this setting since the sets @xmath414 can be defined so that it is always satisfied , even while maintaining each @xmath417 ( see , e.g. , @xcite ) . additionally ,",
    "if we are so lucky as to have some @xmath418 , then we would like the convergence rate achieved by the algorithm to be not significantly worse than running one of the above agnostic active learning algorithms with hypothesis class @xmath414 alone . in this context",
    ", we can define a structure - dependent version of tsybakov s noise condition as follows .",
    "[ con : tsybakov - dependent ] for some nonempty @xmath419 , for each @xmath420 , there exist finite constants @xmath421 and @xmath422 , such that @xmath423 .",
    "note that we do not require every @xmath414 , @xmath424 , to have finite @xmath425 and @xmath426 , only some nonempty set @xmath427 ; this is important , since we might not expect @xmath414 to satisfy condition [ con : tsybakov ] for small indices @xmath13 , where the expressiveness is quite restricted .    in passive learning",
    ", there are several methods for this type of model selection which are known to preserve the convergence rates of each class @xmath414 under condition [ con : tsybakov - dependent ] ( e.g. , @xcite ) . in particular , koltchinskii @xcite develops a method that performs this type of model selection ; it turns out we can modify koltchinskii s method to suit our present needs in the context of active learning ; this results in a general active learning model selection method that preserves the types of improved rates discussed in the previous section .",
    "this modification is presented below , based on using algorithm 2 as a subroutine .",
    "( it should also be possible to define an analogous method that uses algorithm 1 as a subroutine instead . )    * algorithm 3 * + input : nested sequence of classes @xmath428 , label budget @xmath17 , confidence parameter @xmath173 + output : classifier @xmath59 + -2mm(1,0)346 + 0 .",
    "for @xmath429 + 1 .",
    "let @xmath430 and @xmath431 be the sets returned by algorithm 2 run with @xmath414 and the + threshold ( [ eqn : dhm - tight ] ) , allowing @xmath432 label requests , and confidence @xmath433 + 2 .",
    "let @xmath434 + 3 .",
    "if @xmath435 and @xmath436 s.t .",
    "@xmath437 , @xmath438 + 4 .",
    "@xmath439 + 5 .",
    "return @xmath59    the function @xmath440 is defined in the .",
    "this method can be shown to have a confidence bound on its error rate converging to @xmath441 at a rate never significantly worse than the original passive learning method of koltchinskii @xcite , as desired . additionally , we have the following guarantee on the rate of convergence under condition [ con : tsybakov - dependent ] .",
    "the proof is similar in style to koltchinskii s original proof , though some care is needed due to the altered sampling distribution and the constraint set @xmath442 .",
    "the proof is included in appendix b of the supplementary material  @xcite .",
    "[ thm : structure - dependent ] suppose @xmath59 is the classifier returned by algorithm 3 , when allowed @xmath17 label requests and confidence parameter @xmath18 .",
    "suppose further that @xmath3 satisfies condition [ con : tsybakov - dependent ] .",
    "then there exist finite ( @xmath426 and @xmath443-dependent ) constants @xmath444 such that , with probability @xmath269 , @xmath267 , @xmath445    in particular , if we are so lucky as to have @xmath418 for some finite @xmath13 , then the above algorithm achieves a convergence rate not significantly worse than that guaranteed by theorem [ thm : tight - agnostic ] for applying algorithm 2 directly , with hypothesis class @xmath414 .",
    "note that the algorithm itself has no dependence on the set @xmath446 , nor has it any dependence on each class s complexity parameters @xmath447 ; the adaptive behavior of the data - dependent bound @xmath448 allows the algorithm to adaptively ignore the returned classifier from the runs of algorithm 2 for which convergence is slow , thus automatically selecting an index for which the error rate is relatively small .",
    "as in the previous section , we can also show a variant of this result when the complexities are quantified in terms of the entropy . specifically , consider the following condition and theorem ; the proof is in appendix b of the supplementary material @xcite . again , this represents an improvement over known results for passive learning when the disagreement coefficients are finite .",
    "[ con : entropy - aggregation ] for each @xmath424 , there exist finite constants @xmath449 , @xmath450 s.t .",
    "@xmath403 and @xmath399 $ ] , @xmath451 .",
    "[ thm : structure - dependent - entropy ] suppose @xmath59 is the classifier returned by algorithm 3 , when allowed @xmath17 label requests and confidence parameter @xmath316 .",
    "suppose further that @xmath3 satisfies conditions [ con : tsybakov - dependent ] and [ con : entropy - aggregation ] .",
    "then there exist finite ( @xmath426 , @xmath425 , @xmath452 and @xmath453-dependent ) constants @xmath444 such that , with probability @xmath80 , @xmath454 , @xmath455    in addition to these theorems for this structure - dependent version of tsybakov s noise conditions , we also have the following result for a structure - independent noise condition , in the sense that the noise condition does not depend on the particular choice of @xmath414 sets , but only on the distribution @xmath3 ( and in some sense , the full class @xmath456 ) ; it may be particularly useful when the class @xmath19 is universal , in the sense that it can approximate any classifier .",
    "[ thm : structure - independent ] suppose the sequence @xmath428 is constructed so that @xmath457 , and @xmath59 is the classifier returned by algorithm 3 , when allowed @xmath17 label requests and confidence parameter @xmath316 .",
    "suppose that there exists a constant @xmath62 s.t . for all measurable @xmath22 , @xmath458 .",
    "then there exists a finite ( @xmath69-dependent ) constant @xmath82 such that , with probability @xmath80 , @xmath317 , @xmath459    the condition @xmath460 is quite easy to satisfy : for example , @xmath414 could be axis - aligned decision trees of depth @xmath13 , or thresholded polynomials of degree @xmath13 , or multi - layer neural networks with @xmath13 internal units , etc . as for the noise condition in theorem [ thm : structure - independent ]",
    ", this would be satisfied whenever @xmath461 for some constant @xmath462 $ ] .",
    "the case where @xmath463 for @xmath327 can be studied analogously , though the rate improvements over passive learning are more subtle .",
    "under tsybakov s noise conditions , active learning can offer improved asymptotic convergence rates compared to passive learning when the disagreement coefficient is finite .",
    "it is also possible to preserve these improved convergence rates when learning with a nested structure of hypothesis classes , using an algorithm that adapts to both the noise conditions and the complexity of the optimal classifier .",
    "[ app : bound ]",
    "we define the quantity @xmath397 following koltchinskii s analysis of excess risk in terms of local rademacher complexity @xcite .",
    "the general idea is to construct a bound on the excess risk achieved by a given algorithm , such as empirical risk minimization , via an application of talagrand s inequality .",
    "such a bound should be based on a measure of the expressiveness of the set of functions @xmath19 ; however , to bound the excess risk achieved by a particular algorithm given a number of data points , we need only measure the expressiveness of the set of functions the algorithm is likely to select from . for reasonable algorithms , such as empirical risk minimization , this means the set of functions with reasonably small excess risk .",
    "thus , we can bound the excess risk of the algorithm in terms of a measure of expressiveness of the set of functions with relatively small risk , typically referred to as a _ local _ complexity measure .",
    "this reasoning is somewhat circular , in that first we must decide how small to expect the excess risk of the returned function to be before we can calculate the local complexity measure , which itself is used to calculate a bound on the risk of the returned function .",
    "thus , we define the bound on the excess risk as a kind of fixed point .",
    "furthermore , we can estimate these quantities using data - dependent confidence bounds , so that the excess risk bound can be calculated without direct access to the distribution .",
    "for the data - dependent measure of the expressiveness of the function class , we can use a rademacher process .",
    "a detailed motivation and derivation can be found in @xcite .    for our purposes , we add an additional constraint , by requiring the functions we calculate the complexity of to agree with the labels of a labeled set @xmath221 .",
    "this is helpful for us , since given a set @xmath197 of labeled data with true labels , for any two functions @xmath251 and @xmath252 that agree on the labels of @xmath221 , it is always true that @xmath465 equals the difference of the true empirical error rates .",
    "as we prove in the supplement , as long as the set @xmath221 is chosen carefully ( i.e. , as in algorithm 2 ) , the addition of this constraint is essentially inconsequential , so that @xmath397 remains a valid excess risk bound .",
    "the detailed definitions are stated as follows .    for any function @xmath466 , and @xmath467 a sequence of independent random variables with distribution uniform in @xmath468 , define the _ rademacher process _ for @xmath469 under a finite set of ( index , label ) pairs @xmath470 as @xmath471 the @xmath472 should be thought of as internal variables in the learning algorithm , rather than being fundamental to the learning problem .",
    "for any two finite sets @xmath473 and @xmath474 , define @xmath475 & = & \\{h \\in{\\mathbb c}\\dvtx { \\mathit{er}}_{{\\mathcal l}}(h ) = 0\\},\\\\ \\hat{{\\mathbb c}}(\\varepsilon ; { \\mathcal",
    "l } , s ) & = & \\bigl\\{h \\in{\\mathbb c}[{\\mathcal l } ] \\dvtx { \\mathit{er}}_{s}(h ) - \\min _ { h^\\prime\\in{\\mathbb c}[{\\mathcal l } ] } { \\mathit{er}}_{s}(h^\\prime ) \\leq\\varepsilon\\bigr\\},\\\\",
    "\\hat{d}_{{\\mathbb c}}(\\varepsilon ; { \\mathcal l } , s ) & = & \\sup_{h_1,h_2 \\in\\hat{{\\mathbb c}}(\\varepsilon ; { \\mathcal l } , s ) } \\frac{1}{|s|}\\sum_{(i , y ) \\in s } \\mathbh { 1}[h_1(x_i ) \\neq h_2(x_i)]\\end{aligned}\\ ] ] and @xmath476 for @xmath477 , @xmath213 , define @xmath478 and @xmath479 , and for any set @xmath480 , define the set @xmath481 .",
    "we use the following definitions from koltchinskii @xcite with only minor modifications .",
    "[ def : bound ] for @xmath399 $ ] , and finite sets @xmath482 , define @xmath483 and @xmath484 where , for our purposes , we can take @xmath485 and @xmath486 , though there seems to be room for improvement in these constants . for completeness , we also define @xmath487 by convention",
    ".    we will also define a related quantity , representing a distribution - dependent version of @xmath464 , also explored by koltchinskii @xcite . specifically , for @xmath42 , define @xmath488 for @xmath213 , let @xmath489 and @xmath490 where , for our purposes , we can take @xmath491 and @xmath492 . for completeness",
    ", we also define @xmath493 .      in definition [ def : disagreement - coefficient ]",
    ", we took @xmath88 .",
    "if @xmath494 , then this choice is usually relatively harmless .",
    "however , in some cases , setting @xmath88 results in a suboptimal , or even infinite , value of @xmath150 , which is undesirable . in these cases , we would like to set @xmath89 as large as possible while maintaining the validity of the bounds .",
    "if we do this carefully enough , we should be able to establish bounds that , even in the worst case when @xmath495 , are never worse than the bounds for some analogous passive learning method ; however , to do this requires @xmath89 to depend on the parameters of the learning problem : namely , @xmath17 , @xmath173 , @xmath19 and @xmath3 .",
    "the effect of a larger @xmath89 can sometimes be dramatic , as there are scenarios where @xmath496  @xcite ; we certainly wish to distinguish between such scenarios , and those where @xmath497 .",
    "generally , depending on the bound we wish to prove , different values of @xmath89 may be appropriate . for the tightest bound in terms of @xmath150 proven in the appendices ( namely , lemma 7 of appendix b in the supplementary material @xcite )",
    ", the definition of @xmath498 in ( [ eqn : r0 ] ) below gives a good bound .",
    "for the looser bounds ( namely , theorems [ thm : tight - agnostic ] and [ thm : tight - agnostic - entropy ] ) , a larger value of @xmath89 may provide better bounds ; however , this same general technique can be employed to define a good value for @xmath89 in these looser bounds as well , simply using upper bounds on ( [ eqn : r0 ] ) analogous to how the theorems themselves are derived from lemma 7 in appendix b @xcite .",
    "likewise , one can state analogous refinements of @xmath89 for theorems [ thm : cal - upper][thm : bbl - adaptive ] , though for brevity these are left for the reader s independent consideration .",
    "[ def : r0 ] define @xmath499 and @xmath500    we use this definition of @xmath498 in all of the main proofs .",
    "in particular , with this definition , lemma 7 of appendix b @xcite is never significantly worse than the analogous known result for passive learning ( though it can be significantly better when @xmath501 ) .",
    "i extend my sincere thanks to larry wasserman for numerous helpful discussions and also to john langford for initially pointing out to me the possibility of @xmath0 adapting to tsybakov s noise conditions for threshold classifiers .",
    "i would also like to thank the anonymous referees for extremely helpful suggestions on earlier drafts .",
    "balcan , m .- f . ,",
    "broder , a. and zhang , t. ( 2007 ) .",
    "margin based active learning . in _ proceedings of the 20th conference on learning theory_. _ lecture notes in computer science _ * 4539 * 3550 .",
    "springer , berlin ."
  ],
  "abstract_text": [
    "<S> we study the rates of convergence in generalization error achievable by active learning under various types of label noise . </S>",
    "<S> additionally , we study the general problem of model selection for active learning with a nested hierarchy of hypothesis classes and propose an algorithm whose error rate provably converges to the best achievable error among classifiers in the hierarchy at a rate adaptive to both the complexity of the optimal classifier and the noise conditions . </S>",
    "<S> in particular , we state sufficient conditions for these rates to be dramatically faster than those achievable by passive learning .    .    </S>"
  ]
}