{
  "article_text": [
    "we consider as given a system of @xmath0 polynomial equations in @xmath1 variables .",
    "the coefficients of the polynomials are complex numbers .",
    "besides  @xmath0 and  @xmath1 , two other factors determine the complexity of the system : the number  @xmath2 of monomials that appear with nonzero coefficient and the largest degree  @xmath3 that occurs as an exponent of a variable .",
    "the tuple @xmath4 determines the cost of evaluating and differentiating the system accurately . as the degrees increase ,",
    "standard double precision arithmetic becomes insufficient to solve polynomial systems with homotopy continuation methods ( see for example  @xcite for an introduction ) . in the problem setup for this paper",
    "we consider the tracking of one difficult solution path in extended precision .",
    "the extended precision arithmetic we perform with the quad double library qd  2.3.9  @xcite , and in particular on a gpu using the software in  @xcite . for the numerical properties ,",
    "we refer to  @xcite and  @xcite .",
    "our development of massively parallel algorithms is motivated by the desire to offset the extra cost of double double and quad double arithmetic .",
    "we strive for a quality up  @xcite factor : if we can afford to keep the execution time constant , how much can we improve the quality of the solution ?    using double double or quad double arithmetic",
    "we obtain predictable cost overheads . in  @xcite we experimentally determined that the overhead factors of double double over standard double arithmetic is indeed similar to the overhead of complex over standard double arithmetic . in terms of quality",
    ", the errors are expected to decrease proportionally to the increase in the precision . in  @xcite we described a multicore implementation of a path tracker and we implemented our methods used to evaluate and differentiate systems of polynomials",
    "were implemented on the nvidia tesla c2050 , as described in  @xcite .",
    "the focus of this paper is on the solving of the linear systems , needed to run newton s method .    because of the limited bandwidth of cpu",
    "/ gpu communication we can not afford to transfer the evaluated system and its jacobian matrix from the gpu to the cpu and perform the linear system solving on the cpu .",
    "although the evaluation and differentiation of a polynomial system often dominates the cost of newton s method  @xcite , the cost of linear system solving increases relative to the parallel run times of evaluation and differentiation so that even with minor speedups , using a parallel version of the linear system solver matters in the overall execution time .    in the next section we state our problem ,",
    "mention related work and list our contributions . in the third section",
    "we summarize the mathematical definition and properties of the modified gram - schmidt method and we illustrate the higher cost of complex multiprecision arithmetic . then we describe our parallel version of the modified gram - schmidt algorithm and give computational results .",
    "our problem originates from the application of homotopy continuation methods to solve polynomial systems .",
    "while the tracking of many solution paths is a pleasingly parallel computation for which message passing works well , see for example  @xcite , it often occurs that there is one difficult solution path for which the double precision is insufficient to reach the solution at the end of the path .",
    "the goal is to offset the extra cost of extended precision using a gpu .    in this paper",
    "we focus on the solving of a linear system ( which may have more equations than unknowns ) on a gpu .",
    "the linear system occurs in the context of newton s method applied to a polynomial system . because the system could have more equations than unknowns and because of increased numerical stability",
    ", we decided to solve the linear system with a least squares method via a qr decomposition of the matrix .",
    "the algorithm we decided to implement is the modified gram - schmidt algorithm , see  @xcite for its definition and a discussion of its numerical stability",
    ". a computational comparison between gaussian elimination and orthogonal matrix decomposition can be found in  @xcite .    because the overhead factor of the computation cost of extended precision arithmetic",
    ", we can afford to apply a fine granularity in our parallel algorithm .",
    "comparing qr with householder transformations and with the modified gram - schmidt algorithm , the authors of  @xcite show that on message passing systems , a parallel modified gram - schmidt algorithm can be much more efficient than a parallel householder algorithm , and is never slower .",
    "mpi implementations of three versions of gram - schmidt orthonormalizations are described in  @xcite .",
    "the performance of different parallel modified gram - schmidt algorithms on clusters is described in  @xcite . because the modified gram - schmidt method can not be expressed by level-2 blas operations , in  @xcite the authors proposed an efficient implementation of the classical gram - schmidt method .    in  @xcite",
    "is a description of a parallel qr with classical gram - schmidt on gpu and results on an implementation with the nvidia geforce 295 are reported . a report on qr decompositions using householder transformations on the nvidia tesla c2050",
    "can be found in  @xcite .",
    "a high performance implementation of the qr algorithm on gpus is described in  @xcite .",
    "the authors of  @xcite did not consider to implement the modified gram - schmidt method on a gpu because the vectors in the inner products are large and the many synchronizations incur a prohibitive overhead . according to  @xcite , a blocked version is susceptible to precision problems . in our setting , the length @xmath1 of the vectors is small ( our @xmath1 may coincide with the warp size ) and similar to what is reported in  @xcite , we expect the cost of synchronizations to be modest for a small number of threads . because of our small dimensions , we do not consider a blocked version .    in  @xcite , the problem of solving many small independent qr factorizations on a gpu is investigated .",
    "although our qr factorizations are also small , in our application of newton s method in the tracking of one solution path , the linear systems are not independent and must be solved in sequence .",
    "after the qr decomposition , we solve an upper triangular linear system .",
    "the solving of dense triangular systems on multicore and gpu accelerators is described in  @xcite .",
    "triple precision ( double + single float ) implementations of blas routines on gpus were presented in  @xcite .",
    "related to polynomial system solving on a gpu , we mention two recent works . in  @xcite , a subresultant method with a cuda implementation of the fft is described to solve systems of two variables . the implementation with cuda of a multidimensional bisection algorithm on an nvidia gpu",
    "is presented in  @xcite .      1 .",
    "we show that the extra cost of multiprecision arithmetic in the modified gram - schmidt orthogonalization method can be compensated by a gpu .",
    "2 .   combined with projected speedups of our massively parallel evaluation and differentiation implementation  @xcite , the results pave the way for a path tracker that runs entirely on a gpu .",
    "roots of polynomial systems are typically complex and we calculate with complex numbers . following notations in  @xcite , the inner product of two complex vectors @xmath5 is denoted by @xmath6 . in particular : @xmath7 , where @xmath8 is the complex conjugate of @xmath9 .",
    "figure  [ figalgmgs ] lists pseudo code of the modified gram - schmidt orthogonalization method .",
    "= input : @xmath10 .",
    "+ output : = @xmath11 , @xmath12 : @xmath13 , + @xmath14 is upper triangular , and @xmath15 .",
    "+ let @xmath16 be column  @xmath17 of  @xmath18 + for @xmath17 from 1 to @xmath1 do + @xmath19 + @xmath20 , @xmath21 is column @xmath17 of  @xmath22 + for @xmath23 from @xmath24 to @xmath1 do + @xmath25 + @xmath26    given the qr decomposition of a matrix  @xmath18 , the system  @xmath27 is equivalent to @xmath28 . by the orthogonality of  @xmath22 ,",
    "solving @xmath27 is reduced to the upper triangular system @xmath29 .",
    "this solution minimizes @xmath30 .    instead of computing  @xmath31 separately , for numerical stability as recommended in  @xcite , we apply the modified gram - schmidt method to the matrix  @xmath18 augmented with  @xmath32 :    @xmath33     =      \\left [        \\begin{array}{cc }           q & { { \\bf q}}_{n+1 }        \\end{array }     \\right ]     \\left [        \\begin{array}{cc }           r & { { \\bf y}}\\\\           0 & z        \\end{array }     \\right].\\ ] ]    as @xmath34 is orthogonal to the column space of  @xmath22 , we have @xmath35 and @xmath36 .",
    "as reported in  @xcite , the number of flops in the algorithm in figure  [ figalgmgs ] equals @xmath37 . in computations we experience the cubic behavior of the running time : doubling @xmath1 and @xmath0 multiplies the running time by a factor of about  8 .",
    "with user cpu times of runs with the modified gram - schmidt algorithm on random data in table  [ tabprecision ] we illustrate the overhead factor of using complex double , complex double double , and complex quad double arithmetic over standard double arithmetic .",
    "computations in this section were done on one core of an 3.47 ghz intel xeon x5690 and with version 2.3.70 of phcpack  @xcite .",
    "going from double to complex quad double arithmetic , 3.7 seconds increase to 2916.8 seconds ( more than 48 minutes ) , by a factor of 788.3 .",
    ".user cpu times for 10,000 qr decompositions with @xmath38 , for increasing levels of precision . [ cols=\">,>,>\",options=\"header \" , ]",
    "using a massively parallel algorithm for the modified gram - schmidt orthogonalization on a nvidia tesla c2050 computing processor we can compensate for the cost of one extra level of precision , even for modest dimensions , using a fine granularity . for larger dimensions",
    "we obtain double digit speedups and the gpu computes solutions , twice as accurate faster than the  cpu .",
    "m.  anderson , g.  ballard , j.  demmel , and k.  keutzer .",
    "communication - avoiding qr decomposition for gpus . in _ proceedings of the 2011 ieee international parallel distributed processing symposium ( ipdps 2011 ) _ , pages 4858 .",
    "ieee computer society , 2011 .",
    "m.j anderson , d.  sheffield , and k.  keutzer . a predictive model for solving small linear algebra problems in gpu registers .",
    "in _ proceedings of the 2012 ieee international parallel distributed processing symposium ( ipdps 2012 ) _ , pages 213 .",
    "ieee computer society , 2012 .",
    "y.  hida , x.s .",
    "li , and d.h .",
    "algorithms for quad - double precision floating point arithmetic . in _",
    "15th ieee symposium on computer arithmetic ( arith-15 2001 ) _ , pages 155162 .",
    "ieee computer society , 2001 .",
    "n.j . higham . .",
    "siam , 1996 .",
    "li . numerical solution of polynomial systems by homotopy continuation methods . in _ handbook of numerical analysis .",
    "volume xi .",
    "special volume : foundations of computational mathematics _ , pages 209304 .",
    "north - holland , 2003 .",
    "m.  lu , b.  he , and q.  luo . supporting extended precision on graphics processors . in _ proceedings of the sixth international workshop on data management on new hardware ( damon 2010 ) _ , pages 1926 , 2010 .",
    "maza and w.  pan . solving bivariate polynomial systems on a gpu .",
    ", 341 , 2011 . proceedings of high performance computing symposium 2011 , montreal , 15 - 17 june 2011 .",
    "b.  milde and m.  schneider .",
    "parallel implementation of classical gram - schmidt orthogonalization on cuda graphics cards .",
    "available via https://www.cdc.informatik.tu-darmstadt.de /de",
    "/ cdc / personen / michael - schneider .",
    "d.  mukunoki and d.  takashashi .",
    "implementation and evaluation of triple precision blas subroutines on gpus . in _ proceedings of the 2012 ieee 26th international parallel and distributed processing symposium workshops _ , pages 13721380 .",
    "ieee computer society , 2012 .",
    "d.n . priest . .",
    "phd thesis , university of california at berkeley , 1992 .",
    "g.  rnger and m.  schwind .",
    "comparison of different parallel modified gram - schmidt algorithms . in _ proceedings of the 11th international euro - par conference on parallel processing ( euro - par05 )",
    "_ , pages 826836 .",
    "springer - verlag , 2005 .",
    "s.  tomov , r.  nath , h.  ltaief , and j.  dongarra .",
    "dense linear algebra solvers for multicore with gpu accelerators . in _ proceedings of the ieee international symposium on parallel and distributed processing workshops ( ipdsw 2010 ) _ , pages 18 .",
    "ieee computer society , 2010 .",
    "j.  verschelde and g.  yoffe . evaluating polynomials in several variables and their derivatives on a gpu computing processor . in _ proceedings of the",
    "2012 ieee 26th international parallel and distributed processing symposium workshops _ , pages 13911399 .",
    "ieee computer society , 2012 .",
    "t.  yokozawa , d.  takahashi , t.  boku , and m.  sato .",
    "efficient parallel implementation of classical gram - schmidt orthogonalization using matrix multiplication . in _",
    "parallel matrix algorithms and applications ( pmaa 2006 ) _ , pages 3738 , 2006 ."
  ],
  "abstract_text": [
    "<S> our problem is to accurately solve linear systems on a general purpose graphics processing unit with double double and quad double arithmetic . </S>",
    "<S> the linear systems originate from the application of newton s method on polynomial systems . </S>",
    "<S> newton s method is applied as a corrector in a path following method , so the linear systems are solved in sequence and not simultaneously . </S>",
    "<S> one solution path may require the solution of thousands of linear systems . in previous work we reported good speedups with our implementation to evaluate and differentiate polynomial systems on the nvidia tesla c2050 . </S>",
    "<S> although the cost of evaluation and differentiation often dominates the cost of linear system solving in newton s method , because of the limited bandwidth of the communication between cpu and gpu , we can not afford to send the linear system to the cpu for solving during path tracking .    </S>",
    "<S> because of large degrees , the jacobian matrix may contain extreme values , requiring extended precision , leading to a significant overhead . </S>",
    "<S> this overhead of multiprecision arithmetic is our main motivation to develop a massively parallel algorithm . to allow overdetermined linear systems we solve linear systems in the least squares sense , computing the qr decomposition of the matrix by the modified gram - schmidt algorithm . </S>",
    "<S> we describe our implementation of the modified gram - schmidt orthogonalization method for the nvidia tesla c2050 , using double double and quad double arithmetic . </S>",
    "<S> our experimental results show that the achieved speedups are sufficiently high to compensate for the overhead of one extra level of precision </S>",
    "<S> .    * keywords . * double double arithmetic , </S>",
    "<S> general purpose graphics processing unit , massively parallel algorithm , modified gram - schmidt method , orthogonalization , quad double arithmetic , quality up . </S>"
  ]
}