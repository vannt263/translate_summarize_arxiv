{
  "article_text": [
    "cognitive radio is a wireless communication technology that addresses the inefficiency of the radio resource usage via computational intelligence @xcite .",
    "cognitive radios have both civilian and military applications @xcite .",
    "a major task in such radios is the classification of the modulation format of unknown received signals . as the pairing of multiple - antenna ( mimo ) transmission and orthogonal frequency division multiplexing ( ofdm )",
    "data modulation has become central to fourth generation ( 4 g ) and fifth generation ( 5 g ) wireless technologies , a need has arisen for the development of new classification algorithms capable of handling mimo - ofdm signals .",
    "modulation classification methods are generally classified as inference - based or pattern recognition - based @xcite .",
    "the inference - based approaches fall into two categories , namely bayesian and non - bayesian methods @xcite .",
    "bayesian methods model unknown parameters as random variables with some prior distributions , and aim to evaluate the posterior probability of the modulation type .",
    "non - bayesian methods , instead , model unknown parameters as nuisance variables that need to be estimated before performing modulation classification . with pattern recognition - based methods ,",
    "specific features are extracted from the received signal and then used to discriminate among the candidate modulations . compared to the pattern recognition - based approaches ,",
    "inference - based methods generally achieve better classification performance at the cost of a higher computational complexity @xcite .",
    "there is ample literature on classification algorithms for single - antenna ( siso ) systems @xcite , @xcite-@xcite . among them , a bayesian method using gibbs sampling is proposed in @xcite . in @xcite , a systematic bayesian solution based on the latent dirichlet bayesian network ( bn ) is proposed , which generalizes and improves upon the work in @xcite .",
    "a preprocessor for modulation classification is developed in @xcite , whereby the timing offset is estimated using grid - based gibbs sampling to be sampled . then the conditional probability density function ( normalized or non - normalized ) of variable @xmath0",
    "is computed at each selected point , which is used for sampling @xmath0 . ] .",
    "we note that , while most algorithms rely on the assumption that the channels are flat fading or additive white gaussian noise channels , the approaches in @xcite-@xcite are well - suited also for frequency selective fading channels .",
    "only few publications address modulation classification in mimo systems @xcite-@xcite .",
    "the task of modulation classification for mimo is more challenging than for siso due to the mutual interference between the received signals and to the multiplicity of unknown channels . in @xcite , a non - bayesian approach , referred to as independent component analysis ( ica)-phase correction ( pc ) ,",
    "is proposed , where the channel matrix required for the calculation of the hypotheses test is estimated blindly by ica @xcite .",
    "several related pattern recognition - based algorithms are introduced in @xcite-@xcite , where source streams are separated by ica - pc or a constant modulus algorithm , and diverse higher - order signal statistics are used as discriminating features . moreover ,",
    "a pattern recognition - based algorithm using spatial and temporal correlation functions as distinctive features is reported in @xcite for mimo frequency - selective channels with time - domain transmission .",
    "the approach is not applicable to modulation classification for mimo - ofdm systems . as for mimo - ofdm systems ,",
    "a non - bayesian approach is proposed in @xcite based on ica - pc and an assumed invariance of the frequency - domain channels across the coherence bandwidth .",
    "it is finally noted that the results in this paper were partially presented in @xcite .",
    "_ main contributions : _ in this work , we develop bayesian modulation classification techniques for mimo - ofdm systems operating over frequency - selective fading channels , assuming unknown channels and signal - to - noise ratio ( snr ) .",
    "our main contributions are as follows",
    "a modulation classification technique is proposed based on gibbs sampling for mimo - ofdm systems .",
    "inspired by the latent dirichlet models in machine learning @xcite , this approach leverages a novel selection of the prior distributions for the unknown variables , the modulation format and the transmitted symbols .",
    "this selection was first adopted by some of the authors in @xcite for siso systems .",
    "as compared to siso systems , a gibbs sampling implementation such as in @xcite may have an impractically slow convergence due to the high - dimensional and multimodal distributions in mimo systems .",
    "the strategy of annealing @xcite-@xcite combined with multiple random restarts @xcite-@xcite is hence proposed here to improve the convergence speed .",
    "an alternative bayesian solution for modulation classification in mimo - ofdm systems that leverages mean field variational inference @xcite is proposed , based on the same latent dirichlet prior selection .",
    "3 .   a hybrid approach that switches from gibbs sampling to mean field variational inference is proposed for modulation classification in mimo - ofdm systems .",
    "the hybrid approach is motivated by the fact that the gibbs sampler is superior to mean field as a method for exploring the global solution space , while the mean field algorithm has better convergence speed in the vicinity of a local optima @xcite-@xcite .",
    "extensive numerical results demonstrate that the proposed gibbs sampling method converges to an effective solution , and its accuracy improves for small sample sizes when switching to the mean field variational inference technique after a number of iterations .",
    "moreover , the speed of convergence is seen to be generally improved by multiple random restarts and annealing @xcite-@xcite .",
    "overall , while most of the reviewed existing modulation classification algorithms for mimo - ofdm systems work under the assumptions that the channels are flat fading @xcite-@xcite , that the number of receive antennas is no less than the number of transmit antennas @xcite-@xcite , and/or that the number of samples is large ( as for pattern recognition - based methods ) @xcite-@xcite , the proposed method achieves satisfactory performance under more general conditions .",
    "the rest of the paper is organized as follows .",
    "the signal model is introduced in sec .",
    "[ sec : system - model ] . in sec .",
    "[ sec : preliminaries ] , we briefly review some necessary preliminary concepts , including bayesian inference and bns , while in sec .",
    "[ sec : bayesian - inference - for ] , we formulate the modulation classification problem under study as a bayesian inference task , and propose solutions based on gibbs sampling and on mean field variational inference .",
    "numerical results of the proposed methods are presented in sec .",
    "[ sec : numerical - results - and . ] .",
    "finally , conclusions are drawn in sec .",
    "[ sec : conclusions ] .    _ notation _ : the superscripts @xmath1 and @xmath2 denote matrix or vector transpose and hermitian , respectively .",
    "the @xmath3-th row of the matrix @xmath4 is denoted as @xmath5 and the @xmath6-th column is denoted as @xmath7 .",
    "lower case bold letters and upper case bold letters are used to denote column vectors and matrices , respectively .",
    "the notation @xmath8 , where @xmath9^{t}$ ] and @xmath10 , denotes the vector composed of all the elements of @xmath11 except @xmath12 .",
    "we use an angle bracket @xmath13 to represent the expectation with respect to the random variables indicated in the subscript . for notational simplicity",
    ", we do not indicate the variables in the subscript when the expectation is taken with respect to all the random variables inside the bracket @xmath13 the notations @xmath14 and @xmath15 stand for the digamma function @xcite and the indicator function , respectively .",
    "the cardinality of a set @xmath16 is denoted @xmath17 .",
    "we use the same notation , @xmath18 , for both probability density functions ( pdf ) and probability mass function ( pmf ) .",
    "moreover , we will identify a pdf or pmf by its arguments , e.g. , @xmath19 represents the distribution of random variable @xmath20 given the random variable @xmath21 .",
    "the notations @xmath22 and @xmath23 represent the the circularly symmetric complex gaussian distribution with mean vector @xmath24 and covariance matrix @xmath25 and the inverse gamma distribution with shape parameter @xmath26 and scale parameter @xmath27 , respectively .",
    "consider a mimo - ofdm system operating over a frequency - selective fading channel with @xmath28 subcarriers , @xmath29 transmit antennas , @xmath30 receive antennas and a coherence period of @xmath31 ofdm symbols .",
    "all frequency - domain symbols transmitted during the coherence period are taken from a finite constellation @xmath32 , such as @xmath33-psk or @xmath33-qam , where @xmath34 is the ( finite ) set containing all possible constellations . without loss of generality ,",
    "@xmath35 is assumed to be a constellation of symbols with average power equal to 1 .",
    "the number of transmit antennas @xmath29 is assumed known .",
    "we observe that several algorithms have been proposed for the estimation of @xmath29 @xcite .",
    "we focus on the problem of detecting the constellation @xmath35 in the absence of information about the snr , the transmitted symbols and the fading channel coefficients .    after matched filtering and sampling , assuming that time synchronization has been successfully performed at least within the error margin afforded by the cyclic prefix , the frequency - domain samples @xmath36=[y_{1}[n , k], ...",
    ",y_{m_{r}}[n , k]]^{t}$ ] , received across the @xmath30 receive antennas , at the @xmath37-th subcarrier of the @xmath38-th ofdm frame , are expressed as @xmath39=\\mathbf{h}[n]\\mathbf{s}[n , k]+\\mathbf{z}[n , k],\\label{eq : instantaneous mixtures}\\ ] ] where @xmath40 $ ] is the @xmath41 frequency - domain channel matrix associated with the @xmath37-th subcarrier ; @xmath42 $ ] is the @xmath43 vector composed of the symbols transmitted by the @xmath29 antennas , i.e. , @xmath42=[s_{1}[n , k], ... ,s_{m_{t}}[n , k]]^{t}$ ] , with @xmath44\\in a$ ] being the symbol transmitted by the @xmath45-th transmit antenna over the @xmath37-th subcarrier of the @xmath38-th ofdm symbol ; and @xmath46=[z_{1}[n , k], ... ,z_{m_{r}}[n , k]]^{t}\\sim\\mathcal{cn}(\\mathbf{0},\\sigma^{2}\\mathbf{i})$ ] is complex white gaussian noise , which is independent over indices @xmath37 and @xmath38 .",
    "the frequency - domain channel matrix @xmath40 $ ] can be written @xmath47=\\left[\\begin{array}{ccc } \\tilde{h}_{1,1}\\left[n\\right ] & \\cdots & \\tilde{h}_{m_{t},1}\\left[n\\right]\\\\ \\vdots & \\ddots & \\vdots\\\\ \\tilde{h}_{1,m_{r}}\\left[n\\right ] & \\cdots & \\tilde{h}_{m_{t},m_{r}}\\left[n\\right ] \\end{array}\\right],\\label{eq : h_n}\\ ] ] where @xmath48, ... ,\\tilde{h}_{m_{t},m_{r}}\\left[n\\right]]^{t}$ ] denotes the @xmath49 frequency - domain channel vector between the @xmath45-th transmit antenna and the @xmath50-th receive antenna .",
    "assuming that the channel for any pair @xmath51 has at most @xmath52 symbol - spaced taps , we write @xmath53 , with @xmath54 the @xmath55 time - domain channel vector and @xmath56 the @xmath57 matrix composed of the first @xmath52 columns of the dft matrix of size @xmath28 .",
    "note that the channel is fixed within the coherence frame of @xmath31 ofdm symbols .    according to ( [ eq : instantaneous mixtures ] ) and ( [ eq : h_n ] ) ,",
    "the @xmath58 received frequency - domain signals @xmath59^{t}, ... ,$]@xmath60^{t}]^{t}$ ] at the @xmath50-th receive antenna is given by @xmath61 where @xmath62=[y_{m_{r}}[1,k], ... ,y_{m_{r}}[n , k]]^{t}$ ] ; @xmath63^{t}$ ] is an @xmath64 matrix representing the transmitted symbols with @xmath65 an @xmath66 diagonal matrix whose @xmath67 element is @xmath44 $ ] ; and @xmath68^{t}, ... ,\\mathbf{z}_{m_{r}}[k]^{t}]^{t}$ ] with @xmath69=[z_{m_{r}}[1,k], ...",
    ",z_{m_{r}}[n , k]]^{t}$ ] .    let us further define the @xmath70 vector @xmath71^{t}$ ] containing all the transmitted symbols with @xmath72^{t}, ... ,\\mathbf{s}[n , k]^{t}]^{t}$ ] ; the @xmath73 vector @xmath74^{t}$ ] for the time domain channels associated with all the transmit - receive antenna pairs , where @xmath75^{t}$ ] ; and the @xmath76 receive signal vector @xmath77^{t}$ ] .",
    "the task of modulation classification is for the receiver to correctly detect the modulation format @xmath35 given only the received samples @xmath78 , while being uninformed about the symbols @xmath79 , the channel @xmath80 and the noise power @xmath81 . using ( [ eq : instantaneous mixtures ] ) and ( [ eq : sig_mod_1 ] ) , the likelihood function @xmath82 of the observation is given by @xmath83\\big|\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2}\\right)\\nonumber \\\\ = & \\prod_{m_{r}}p\\left(\\mathbf{y}_{m_{r}}\\big|\\mathbf{s},\\mathbf{h}_{m_{r}}\\mathrm{,\\sigma^{2}}\\right),\\label{eq : joint likelihood-1}\\end{aligned}\\ ] ] with @xmath84 @xmath85 @xmath86 and @xmath36|(\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2})$ ] @xmath85@xmath87\\mathbf{s}[n , k],\\sigma^{2}\\mathbf{i})$ ] .",
    "as formalized in the next section , in this paper we formulate the modulation classification problem as a bayesian inference task . in this section ,",
    "we review some necessary preliminary concepts .",
    "specifically , we start by introducing the general task of bayesian inference in sec . [ sub : bayesian - inference ] ; we review the definition of bn , which is a useful graphical tool to represent knowledge about the structure of a joint distribution , in sec .",
    "[ sub : bayesian - network ] ; and , finally , we review approximate solutions to the bayesian inference task , namely , gibbs sampling in sec . [ sub : gibbs - sampling ] , and mean field variational inference in sec . [",
    "sub : mean - field - variational ] .",
    "bayesian inference aims to compute the posterior probability of the variables of interest given the evidence , where the evidence is a subset of the random variables in the model .",
    "specifically , given the values of some evidence variables @xmath88 , one wishes to estimate the posterior distribution of a subset of the unknown variables @xmath89^{t}$ ] .",
    "we assume here for simplicity of exposition that all variables are discrete with finite cardinality . however , the extension to continuous variables with pdfs is immediate , as it will be argued .",
    "the conditional pmf of @xmath90 given the evidence @xmath88 is proportional to the product of a prior distribution @xmath91 on the unknown variables @xmath90 and of the likelihood of the evidence @xmath92 : @xmath93 if one is interested in computing the posterior distribution of the unknown variable @xmath94 , then a direct approach would be to write @xmath95 the inference task ( [ eq : general marginal ] ) is made difficult in practice by the multidimensional summation over all the values of the variables @xmath96 .",
    "note also that , if the variables are continuous , the operation of summation is replaced by integration and a similar discussion applies .",
    "next , we discuss the bn model .",
    "a bn is an acyclic graph that can be used to represent useful aspects of the structure of a joint distribution .",
    "each node in the graph represents a random variable , while the directed edges between the nodes encode the probabilistic influence of one variable on another .",
    "node @xmath97 is defined to be a parent of @xmath94 , if an edge from node @xmath97 to node @xmath94 exists in the graph . according to the bn s chain rule @xcite ,",
    "the influence encoded in a bn for a set of variables @xmath98^{t}$ ] can be interpreted as the factorization of the joint distribution in the form @xmath99 where we use @xmath100 to denote the set of parent variables of variable @xmath94 .",
    "note that ( [ eq : bn s chain rule ] ) encodes the fact that each variable @xmath94 is independent of its ancestors in the bn , when conditioning on its parent variables @xmath100 . in the following",
    ", we will find it useful to rewrite ( [ eq : bn s chain rule ] ) in a more abstract way as @xcite @xmath101 where the product is taken over all @xmath102 factor @xmath103 with @xmath104 .",
    "markov chain monte carlo ( mcmc ) techniques provide effective iterative approximate solutions to the bayesian inference task ( [ eq : general marginal ] ) that are based on randomization and can obtain increasingly accurate posterior distributions as the number of iterations increases .",
    "the goal of these techniques is to generate @xmath33 random samples @xmath105 from the desired posterior distribution @xmath106 .",
    "this is done by running a markov chain whose equilibrium distribution is @xmath106 . as a result , according to the law of large numbers , the multidimensional summation ( or integration ) ( [ eq : general marginal ] ) can be approximated by an ensemble average .",
    "in particular , the marginal distribution of any particular variable @xmath94 in @xmath90 can be estimated as @xmath107 where @xmath108 is the @xmath109-th sample for @xmath94 generated by the markov chain , and @xmath110 denotes the number of samples used as burn - in period to reduce the correlations with the initial values @xcite .",
    "gibbs sampling is a classical mcmc algorithm that defines the aforementioned markov chain by sampling all the variables in @xmath90 one - by - one .",
    "specifically , the algorithm begins with a set of arbitrary feasible values for @xmath90 .",
    "then , at step @xmath109 , a sample for a given variable @xmath94 is drawn from the conditional distribution @xmath111 . whenever a sample is generated for a variable ,",
    "the value of that variable is updated within the vector @xmath90 .",
    "it can be shown that the required conditional distributions @xmath111 may be calculated by multiplying all the factors in the factorization ( [ eq : newfactorization ] ) that contain the variable of interest and then normalizing the resulting distribution , i.e. , we have @xmath112 where the right - hand side of ( [ eq : fullcondionaleq ] ) is the product of the factors in ( [ eq : newfactorization ] ) that involve the variable @xmath94 .    _",
    "remark 1 _ : in order for markov chain monte carlo algorithms to converge to a unique equilibrium distribution , the associated markov chain needs to be irreducible and aperiodic ( * ? ? ?",
    "in the context of the gibbs sampling , a sufficient condition for asymptotic correctness of gibbs sampling is that the distributions @xmath111 are strictly positive in their domains for all @xmath6 .",
    "_ remark 2 _ : when applying gibbs sampling to practical problems , in particular those with high - dimensional and multimodal posterior distribution @xmath111 , slow convergence may be encountered due to the local nature of the updates . one approach to address this issue is to run gibbs sampling with _",
    "multiple random restarts _ that are initialized with different feasible solutions @xcite-@xcite . moreover , within each run , _ simulated annealing _ may be used to avoid low - probability `` traps . ''",
    "accordingly , the prior probability , or the likelihood , may be parametrized by a temperature parameter @xmath1 , such that a large temperature implies a lower reliance on the evidence aimed at exploring more thoroughly the range of the variables .",
    "samples are generated , starting with a high temperature and ending with a low temperature @xcite-@xcite .",
    "mean field variational inference provides an alternative way to approach the bayesian inference problem of calculating @xmath113 .",
    "the key idea of this method is that of searching for a distribution @xmath114 that is closest to the desired posterior distribution @xmath115 , in terms of the kullback - leibler ( kl ) divergence @xmath116 , within the class @xmath117 of distributions that factorize as the product of marginals , i.e. , @xmath118 ( * ? ? ?",
    "the corresponding variational problem is given as @xmath119 by imposing the necessary optimality conditions for problem ( [ eq : mf_optimi ] ) , one can prove that the mean field approximation @xmath114 is locally optimal only if the proportionality @xcite @xmath120 holds for all @xmath121 , where the expectation in ( [ eq : fixed_point ] ) is taken with respect to the distribution @xmath122 .",
    "the idea of the mean field variational inference is to solve ( [ eq : fixed_point ] ) by means of fixed - point iterations ( see @xcite for details ) .",
    "it can be shown that each iteration of ( [ eq : fixed_point ] ) results in a better approximation @xmath123 to the target distribution @xmath115 , hence guaranteeing convergence to a local optimum of problem ( [ eq : mf_optimi ] ) ( * ? ? ?",
    "once an approximating distribution @xmath114 is obtained , an approximation of the desired posterior @xmath115 can be obtained as @xmath124 and the marginal posterior distribution may be approximated as @xmath125",
    "in this section , we solve the problem of detecting the modulation @xmath32 by adopting a bayesian inference formulation .",
    "first , in sec . [ sub : modulation - classification - via ]",
    ", we discuss the problem of selecting a proper prior distribution , and argue that a latent dirichlet model inspired by @xcite and first used for modulation classification in @xcite , provides an effective choice .",
    "then , based on this prior model , we develop two solutions , one based on gibbs sampling , in sec . [",
    "sub : modulation - classification - via-1 ] , and the other based on mean field variational inference , in sec .",
    "[ sub : modulation - classification - via ] .      according to ( [ eq : bayes theorem ] ) , the joint distribution of the unknown variables @xmath126",
    "may be expressed @xmath127 where the likelihood function @xmath82 is given in ( [ eq : joint likelihood-1 ] ) , and the term @xmath128 stands for the prior information on the unknown quantities . the prior is assumed to factorize as @xmath129|a\\right)\\bigg\\}\\cdot\\nonumber \\\\   & \\cdot\\prod_{m_{t},m_{r}}p\\left(\\boldsymbol{\\mathbf{h}}_{m_{t},m_{r}}\\right)p\\left(\\sigma^{2}\\right).\\label{eq : prior_factorization-1}\\end{aligned}\\ ] ]      a natural choice for the prior distribution of the unknown variables @xmath126 is given by @xmath130 , @xmath44|a\\sim\\mathrm{uniform}(a)$ ] , @xmath131 , and @xmath132 with fixed parameters @xmath133 @xcite . recall that the inverse gamma distribution is the conjugate prior for the gaussian likelihood at hand , and that uninformative priors can be obtained by selecting sufficiently large @xmath134 and @xmath135 and sufficiently small @xmath136 @xcite .",
    "the factorization ( [ eq : prior_factorization-1 ] ) implies that , under the prior information , the variables @xmath35 , @xmath54 and @xmath81 are independent of each other , and that the transmitted symbols @xmath44 $ ] are independent of all the other variables in ( [ eq : prior_factorization-1 ] ) when conditioned on the modulation @xmath35 . the bn _",
    "@xmath137 _ that encodes the factorization given by ( [ eq : posterior_1 ] ) , along with ( [ eq : joint likelihood-1 ] ) and ( [ eq : prior_factorization-1 ] ) , is shown in fig .",
    "[ fig : dirr_bn-1 ] .",
    "the bayesian inference task for modulation classification of mimo - ofdm is to compute the posterior probability of the modulation @xmath35 conditioned on the received signal @xmath78 , namely @xmath138 following the discussion in sec .",
    "[ sec : preliminaries ] , the calculation in ( [ eq : inference_problem-1 ] ) is intractable because of the multidimensional summation and integration .",
    "gibbs sampling ( sec . [ sub : gibbs - sampling ] ) and mean field variational inference ( sec .",
    "[ sub : mean - field - variational ] ) offer feasible alternatives .",
    "however , the prior distribution ( [ eq : prior_factorization-1 ] ) does not satisfy the sufficient condition mentioned in _ remark _ _ 1 _ , since some of the conditional distributions required for gibbs sampling are not strictly positive in their domains .",
    "in particular , the required conditional distribution for modulation @xmath35 can be expressed as @xmath139|a\\right).\\label{eq : new}\\ ] ] the conditional distribution term @xmath140|a = a)$ ] in ( [ eq : new ] ) is zero for all values of @xmath44 $ ] not belonging to the constellation @xmath26 , i.e. , @xmath140|a)=0 $ ] for @xmath44\\notin a$ ] .",
    "therefore , the conditional distribution @xmath141 is equal to zero if the transmitted symbols @xmath79 do not belong to @xmath26 . as a result , the gibbs sampler may fail to converge to the posterior distribution ( see , e.g. , @xcite ) . in order to alleviate the problem outlined above",
    ", we propose to adopt a prior distribution encoded on a latent dirichlet bn @xmath142 shown in fig .",
    "[ fig : dirr_bn ] .",
    "next , we introduce the gibbs sampler based on on a latent dirichlet bn @xmath142 in details . accordingly ,",
    "each transmitted symbol @xmath44 $ ] is distributed as a random mixture of uniform distributions on the different constellations in the set @xmath34 .",
    "specifically , a random vector @xmath143 of length @xmath144 is introduced to represent the mixture weights , with @xmath145 being the probability that each symbol @xmath44 $ ] belongs to the constellation @xmath146 .",
    "given the mixture weights @xmath143 , the transmitted symbols @xmath44 $ ] are mutually independent and distributed according to a mixture of uniform distributions , i.e. , @xmath147|\\mathbf{p}_{a}\\right)=\\sum_{a:\\,s_{m_{t}}[n , k]\\in a}\\mathbf{p}_{a}\\left(a\\right)/\\left|a\\right|$ ] . the dirichlet distribution is selected as the prior distribution of @xmath143 in order to simplify the development of the proposed solutions , as shown in the following subsections . in particular , given a set of nonnegative parameters @xmath148^{t}$ ] , we have @xmath149 @xcite .",
    "we recall that the parameter @xmath150 has an intuitive interpretation as it represents the number of symbols in constellation @xmath146 observed during some preliminary measurements .",
    "the bn @xmath142 encodes a factorization of the conditional distribution @xmath151 @xmath152|\\mathbf{p}_{a}\\right)\\bigg\\}\\cdot\\nonumber \\\\   & \\cdot\\prod_{m_{t},m_{r}}p\\left(\\boldsymbol{\\mathbf{h}}_{m_{t},m_{r}}\\right)p\\left(\\sigma^{2}\\right),\\label{eq : joint_posterior_2}\\end{aligned}\\ ] ] where we have @xmath149 with a set of nonnegative parameters @xmath148^{t}$ ] @xcite , @xmath147|\\mathbf{p}_{a}\\right)=\\sum_{a:\\,s_{m_{t}}[n , k]\\in a}\\mathbf{p}_{a}\\left(a\\right)/\\left|a\\right|$ ] , and the other distributions are as in ( [ eq : joint likelihood-1 ] ) and ( [ eq : prior_factorization-1 ] ) .",
    "the bayesian inference task for modulation classification is to compute the posterior probability of the mixture weight vector @xmath143 conditional on the received signal @xmath78 , namely @xmath153 and then to estimate @xmath35 as the value that maximize the a posteriori mean of @xmath143 : @xmath154    the proposed approach guarantees that all the conditional distributions needed for gibbs sampling based on the bn @xmath142 are non - zero , and therefore the aforementioned convergence problem for the inference based on bn @xmath137 is avoided .      in this subsection",
    ", we elaborate on gibbs sampling for modulation classification . as explained in sec .",
    "[ sub : gibbs - sampling ] , in order to sample from the joint posterior distribution ( [ eq : joint_posterior_2 ] ) , the distribution of each variable conditioned on all other variables is needed . according to ( [ eq : fullcondionaleq ] ) ,",
    "we have ( for derivations see appendix ii ) :    1 .",
    "the conditional distribution of the vector @xmath143 , given @xmath79 , @xmath80 , @xmath155 and @xmath78 can be expressed as @xmath156 where @xmath157^{t}$ ] , and @xmath158 is the number of samples of transmitted symbols in constellation @xmath146 ; 2 .",
    "the distribution of transmitted symbols @xmath44 $ ] , conditioned on @xmath143 , @xmath159 $ ] , @xmath80 , @xmath155 and @xmath78 , is given by @xmath160\\big|\\mathbf{p}_{a},\\mathbf{s}\\diagdown s_{m_{t}}[n , k],\\mathbf{h\\mathrm{,\\sigma^{2},}}\\mathbf{y}\\right)\\nonumber \\\\ \\propto & p\\left(s_{m_{t}}[n , k]|\\mathbf{p}_{a}\\right)p\\left(\\mathbf{y}[n , k]\\big|\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2}\\right),\\label{eq : tx_symbol_gb}\\end{aligned}\\ ] ] where we recall that when a new sample is generated for @xmath44 $ ] , the new value is updated and used in computing subsequent samples in @xmath79 ; 3 .   the required distribution for channel vector @xmath54 is given by @xmath161 where we have @xmath162 and @xmath163 4 .   the conditional distribution for @xmath81 , conditioned on @xmath143 , @xmath79 , @xmath80 , and @xmath78 , is given by @xmath164 where @xmath165 and @xmath166",
    "note that ( [ eq : p_a posterior ] ) is a consequence of the fact that dirichlet distribution is the conjugate prior of the categorical likelihood @xcite ; ( [ eq : channel_gb ] ) can be derived by following from standard mmse channel estimation results @xcite ; and ( [ eq : sigma_sq_gb ] ) follows the fact that the inverse gamma distribution is the conjugate prior for the gaussian distribution @xcite .",
    "we summarize the proposed gibbs sampler for modulation classification in algorithm 1 .    * initialize @xmath167 from prior distributions as discussed in sec .",
    "[ sub : modulation - classification - via ] * * for * each iteration @xmath168 * * given @xmath169 draw a sample @xmath170 from @xmath171 in ( [ eq : p_a posterior ] ) ; * * given @xmath172)^{(m)}\\}$ ] , draw a sample @xmath173 $ ] from @xmath140|\\mathbf{p}_{a},\\mathbf{s}\\diagdown s_{m_{t}}[n , k],\\mathbf{h\\mathrm{,\\sigma^{2},}}\\mathbf{y})$ ] in ( [ eq : tx_symbol_gb ] ) ; * * given @xmath174 and the current sample values for , draw a sample @xmath175 from @xmath176 in ( [ eq : channel_gb ] ) ; * * given @xmath177 draw sample @xmath178 from @xmath179 in ( [ eq : sigma_sq_gb ] ) ; * * * * end for *    _ remark _ _ 3 : _ in @xcite , an alternative gibbs sampling approach based on a `` superconstellation '' is proposed to solve the convergence problem at hand for modulation classification in siso .",
    "the gibbs sampling scheme in @xcite can be viewed as an approximation of the approach based on the latent dirichlet bn obtained by setting the prior distribution @xmath149 such that @xmath180 and by setting the sample value of @xmath143 to be equal to the mean of the conditional distribution @xmath171 , i.e. , @xmath181 @xcite , where we recall that @xmath158 is the number of symbols that belong to constellation @xmath146 .",
    "the performance of the `` superconstellation '' approach extended to mimo ofdm is discussed in sec .",
    "[ sec : numerical - results - and . ] .",
    "_ remark 4 _ : when the snr is high , the convergence speed is severely limited by the close - to - zero probabilities in the conditional distribution ( [ eq : tx_symbol_gb ] ) .",
    "specifically , as the gibbs sampling proceeds with its iterations , the samples of @xmath81 tend to be small , making the relationship between @xmath36 $ ] and @xmath44 $ ] almost deterministic .",
    "in particular , the term @xmath182|\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2})$ ] in ( [ eq : tx_symbol_gb ] ) satisfies @xmath182|\\mathbf{s}^{(m)}[n , k],\\mathbf{h}^{(m)}[n],(\\sigma^{2})^{(m)})\\simeq1 $ ] for the selected sample value @xmath183 $ ] , and @xmath182|\\mathbf{s}^{(m)}[n , k],\\mathbf{h}^{(m)}[n],(\\sigma^{2})^{(m)})\\simeq0 $ ] for all other possible values for @xmath42 $ ] . as a result",
    ", transition between states with different values in the markov chain occurs with a very low probability leading to extremely slow convergence . as discussed in _ remark 2 _ ,",
    "the strategy of gibbs sampling with multiple random restarts and annealing may be adopted to address this issue . for simulated annealing",
    ", we substitute the conditional distribution ( [ eq : sigma_sq_gb ] ) for @xmath81 with an iteration dependent prior given as @xcite @xmath184 where we have @xmath185 , with @xmath109 denoting the current iteration index , @xmath186 and @xmath187 , where @xmath33 is the total number of iterations . for multiple restarts ,",
    "we propose to use the entropy of the pmf @xmath188 , estimated in a run as the metric , to choose among the @xmath189 runs of gibbs sampling which one should be used in ( [ eq : modulation_estimate ] ) .",
    "specifically , the run with the minimum entropy estimate @xmath188 is selected .",
    "the rationale of this choice is that an estimate @xmath188 with a low entropy identifies a specific modulation type with a smaller uncertainty than an estimate @xmath188 with higher entropy ( i.e. , closer to a uniform distribution ) .      following the discussion in sec .",
    "[ sub : mean - field - variational ] , the goal of mean field variational inference applied to the problem at hand is that of searching for a distribution @xmath190 that is closest to the desired posterior distribution @xmath191 , in terms of the kullback - leibler ( kl ) divergence @xmath192 , within the class @xmath117 of distributions that factorize as @xmath193 where @xmath194)$ ] , and @xmath195 .",
    "next , we present the fixed - point equations for mean field variational inference . these update equations may be derived by applying ( [ eq : fixed_point ] ) to the joint pdf ( [ eq : joint_posterior_2 ] ) .",
    "we recall that ( [ eq : fixed_point ] ) requires taking expectations of the relevant variables with respect to updated distribution @xmath123 .",
    "if all distributions @xmath196 , @xmath197 and @xmath198 are initialized by choosing from the conjugate exponential prior family @xcite , @xcite in a way being consistent with the priors in ( [ eq : joint_posterior_2 ] ) , namely @xmath196 being a dirichlet distribution , @xmath197 being a circularly complex gaussian distribution , and @xmath198 being an inverse gamma distribution , these fixed point update equations can be calculated , using a similar approach as in the appendix i , as follows .    1 .",
    "the fixed point update equation for the mixture weight vector @xmath143 can be expressed as @xmath199 where @xmath200^{t}$ ] and @xmath201\\in a}q(s_{m_{t}}[n , k])$ ] .",
    "the update equation for the transmitted symbol @xmath44 $ ] is given by @xmath202\\right)\\propto\\exp\\big[\\big\\langle\\ln p\\left(s_{m_{t}}[n , k]|\\mathbf{p}_{a}\\right)\\big\\rangle_{q\\left(\\mathbf{p}_{a}\\right)}+\\nonumber \\\\   & \\big\\langle\\ln p\\left(\\mathbf{y}[n , k]|\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2}\\right)\\big\\rangle_{q\\left(\\mathbf{s}[n , k]\\diagdown s_{m_{t}}[n , k],\\mathbf{h},\\sigma^{2}\\right)}\\big]\\label{eq : tx_symbol_mf_1}\\end{aligned}\\ ] ] where the detailed expression of ( [ eq : tx_symbol_mf_1 ] ) is shown in appendix iii .",
    "the equation for the channel vector @xmath203 is given by @xmath204 where @xmath205 + @xmath206 + @xmath207 + and @xmath208 is a diagonal matrix whose@xmath209 element is equal to @xmath210|^{2}\\big\\rangle$ ] .",
    "4 .   the fixed point update equation for the noise variance @xmath81 can be expressed as @xmath211 where @xmath165 , and @xmath212 with @xmath213+\\nonumber \\\\   & + \\sum_{m_{t}}\\mathbf{\\psi}_{m_{t},m_{r}}+\\sum_{m_{t}}\\mathbf{\\xi}_{m_{t},m_{r}},\\end{aligned}\\ ] ] @xmath214+\\nonumber \\\\   & \\hat{\\mathbf{h}}_{m_{t},m_{r}}^{h}\\mathbf{w}^{h}\\big\\langle\\mathbf{d}_{m_{t}}^{h}\\mathbf{d}_{m_{t}}\\big\\rangle\\mathbf{w}\\hat{\\mathbf{h}}_{m_{t},m_{r}},\\end{aligned}\\ ] ] and @xmath215 where we use @xmath216 $ ] to denote the trace of a matrix .",
    "we summarize the proposed iterative mean field variational inference for modulation classification in algorithm 2 .",
    "* initialize @xmath196 , @xmath217 , @xmath197 and @xmath198 * * for * each iteration @xmath168 * * given the most updated distribution @xmath217 , @xmath197 and @xmath198 , update the distribution @xmath196 using ( [ eq : p_a_mf_2 ] ) ; * * given the most updated distribution @xmath196 , @xmath218\\right)$ ] , @xmath197 and @xmath198 , update the distribution @xmath219\\right)$ ] using ( [ eq : tx_symbol_mf_1 ] ) ; * * given the most updated distribution @xmath196 , @xmath217 , @xmath220 and @xmath198 , update the distribution @xmath221 using ( [ eq : channel_mf_2 ] ) ; * * given the most updated distribution @xmath196 , @xmath217 , and @xmath197 , update the distribution @xmath198 according to ( [ eq : mf_sig ] ) ; * * end for *    _ remark 5 _ : while gibbs sampler is generally known to be superior to mean field as a method for exploring the global solution space , the mean field algorithm is known to have better convergence speed in the vicinity of a local optima @xcite , @xcite , @xcite .",
    "following @xcite , we then propose a hybrid strategy strategy that switches from gibbs sampling to mean field variational inference to `` zoom in '' on the local minimum of the optimization problem ( [ eq : mf_optimi ] ) .",
    "additional discussion on this point can be found in the next section .",
    "a break - down of the contribution to the computational complexity of each iteration for the proposed gibbs sampler are shown in table [ tab : complexity ] . in summary ,",
    "the gibbs sampler requires @xmath222\\}$ ] basic arithmetic operations , where @xmath223 is the total number of iterations and @xmath224 is the total number of states of all possible constellations , i.e. , @xmath225 . at each iteration , the number of the basic arithmetic operations required for mean field variational inference is of the same order of magnitude as that for gibbs sampling .",
    "this similarity of the computational complexity of gibbs sampling and mean field variational inference is also reported in @xcite , @xcite and @xcite .",
    "variable +   @xmath143 & @xmath226 +   @xmath79 & @xmath227 +   @xmath80 & @xmath228\\}$ ] +   @xmath81 & @xmath229 +   total & @xmath230@xmath222\\}$ ] +",
    "in this section , we evaluate the performance of the proposed modulation classification schemes for the detection of three possible modulation formats within a mimo - ofdm system .",
    "the performance criterion is the probability of correct classification assuming that all the modulations are equally likely .",
    "normalized rayleigh fading channels are assumed such that @xmath231=1.$ ] we define the average snr as @xmath232 .",
    "unless stated otherwise , the following conditions are assumed : _ i _ ) @xmath34=\\{qpsk , 8-psk , 16qam } ; _ ii _ ) @xmath233 antennas ; _ iii _ ) @xmath234 ofdm symbols ; and _ iv _ ) @xmath235 taps with relative powers given by @xmath236 @xmath237 $ ] .",
    "we first investigate the performance of the proposed gibbs sampling algorithms with or without multiple random restarts and simulated annealing within each run ( see _ remark 4 _ ) .",
    "the number of runs in each process of gibbs sampling with multiple random restarts is selected to be @xmath238 , and the number of iterations in each run is @xmath239 , where @xmath240 initial samples are used as burn - in period . ) . ]",
    "note here that the total number of iterations required for gibbs sampling is @xmath241 .",
    "all elements of the vector parameter @xmath242 of the prior distribution @xmath149 are selected to be equal to a parameter @xmath243 . as also reported in @xcite ,",
    "it may be shown , via numerical results , that the modulation classification performance is not sensitive to the choice of parameter @xmath243 as long as the value of the virtual observation @xmath243 ( see sec .",
    "[ sub : latent - dirichlet - bn ] ) is not very small ( @xmath244 ) . for the numerical experiments in this paper , we select the values of @xmath243 to be equal to @xmath245 of the total number of symbols , e.g. , in this example @xmath246=40 $ ] .    in fig .",
    "[ fig : gibbs ] , the probabilities of correct classification for regular gibbs sampling , gibbs sampling with multiple random restarts , gibbs sampling with annealing and gibbs sampling with both multiple random restarts and annealing are plotted as a function of snr .",
    "we also show for reference the performance of the superconstellation scheme , with both multiple random restarts and annealing , of @xcite ( see _ remark _ _ 3 _ ) extended to mimo ofdm systems . from fig .",
    "[ fig : gibbs ] , it can be seen that the proposed strategy outperforms the approach in @xcite . moreover , both strategies of multiple random restarts and annealing improve the success rate , and that the best performance is achieved by gibbs sampling with both random restarts and annealing . as discussed in _ remark 4",
    "_ , annealing is seen to be especially effective in the high - snr regime .",
    "next , we study the effect of incorrect channel length estimates .",
    "the relative powers of the considered channel taps are @xmath247 $ ] .",
    "we considered the performance of the proposed scheme under overestimated , correctly estimated , or underestimated channel lengths .",
    "specifically , the channel length estimates take three possible values , namely @xmath248 , @xmath249 or @xmath250 , while @xmath251 . the same values for the parameters of gibbs sampler",
    "are used as in sec .",
    "[ sub : gibbs - sampling - with ] .",
    "[ fig : gibbs-1 ] shows the probabilities of correct classification for gibbs sampling with both random restarts and annealing versus snr .",
    "it is observed that there is a minor performance degradation with an overestimated channel length . here",
    ", for this example , the degradation caused by the overfitting when a more complex model with @xmath250 is used is minor .",
    "in contrast , a more severe performance degradation is observed for the case of the underestimated channel length .",
    "this significant degradation is caused by the bias introduced by the simpler model with @xmath248 .",
    "we also carried out the experiments for @xmath235 taps with relative powers of @xmath252 @xmath253@xmath254 $ ] .",
    "the performance is very similar to that for the case of @xmath251 shown in fig .",
    "[ fig : gibbs-1 ] and is hence not reported here .",
    "to study the effect of modulation pool @xmath34 with a larger size , besides the three modulations considered above , we added 16-psk into the modulation pool , i.e. , @xmath34=\\{qpsk , 8-psk , 16qam , 16-psk}. the relative powers of the multi - path components and the parameters of the gibbs sampler take the same value as in sec .",
    "[ sub : performance - under - incorrect ] .",
    "the performance of the proposed gibbs sampler for the cases of three and four modulation formats is shown in fig .",
    "[ fig : gibbs-1 ] .",
    "as expected , some performance degradation is observed for a larger set of possible modulation schemes . to gain more insight into the classifier behavior , the confusion matrices for both cases of three and four modulation schemes for snr of 5 db",
    "are shown in tables [ tab:3_modulations ] and [ tab:4_modulations ] , respectively .",
    "the confusion matrices show the probabilities of deciding for a given modulation format when another format is the correct one .",
    "for instance , the element associated with row 8-psk and column qpsk in table [ tab:3_modulations ] indicates that , when the actual modulation scheme is 8-psk , the probability that the estimated modulation is qpsk is 9.2% . comparing table [ tab:3_modulations ] with table [ tab:4_modulations ]",
    ", it can be seen that the decreased accuracy is mainly caused by the confusion between the two most similar modulation formats , namely 8-psk and 16-psk .",
    ".[tab:3_modulations]confusion matrix of the proposed gibbs sampler for three modulation formats at 5 db [ cols=\"^,^,^,^,^ \" , ]      here , we study the performance of a hybrid scheme , inspired by @xcite , that starts with a gibbs sampler in order to perform a global search in the parameter space and then switches to mean field variational inference to speed up the convergence to a nearby local optima . in the switching iteration",
    ", all the needed marginal distributions for mean field variational inference are initialized as the conditional distributions for gibbs sampling in the previous iteration , namely the marginal distributions are initialized as follows , @xmath255 @xmath256)\\nonumber \\\\ = & p^{(m_{s}-1)}(s_{m_{t}}[n , k]|\\mathbf{p}_{a},\\mathbf{s}\\diagdown s_{m_{t}}[n , k],\\mathbf{h\\mathrm{,\\sigma^{2},}}\\mathbf{y}),\\end{aligned}\\ ] ] @xmath257 and @xmath258 where @xmath259 denotes the index of the switching iteration .    to demonstrate the effectiveness of this approach",
    ", we consider modulation classification using received samples within one ofdm frame ( @xmath260 ) over rayleigh fading channels with two taps ( @xmath261 ) and with the relative powers @xmath262 $ ] .",
    "the snr is @xmath263 and the dft size is @xmath264 or @xmath265 .",
    "after the first eight iterations of regular gibbs sampling ( without random restarts and annealing ) , we switch to the mean field variational inference . the probability of correct classification of both regular gibbs sampling and the hybrid approach versus the number of iterations @xmath33 with @xmath266 burn - in samples is shown in fig . [ fig : gb=000026mf ] . it can be observed that the hybrid approach outperforms gibbs sampling especially when the number of iterations is small .      here",
    ", we compare the classification results achieved by the proposed gibbs sampling scheme with the ica - pc approach of @xcite , which extends to mimo - ofdm the techniques studied in @xcite .",
    "the approach in @xcite exploits the invariance of the frequency - domain channels across the coherence bandwidth to perform classification .",
    "specifically , the subcarriers are grouped in sets of @xmath267 adjacent subcarriers whose frequency - domain channel matrices are assumed to be identical .",
    "let us denote the frequency - domain channel matrix and the received samples for the @xmath3-th group by @xmath268 and @xmath269 respectively , @xmath270 . to compute the likelihood function @xmath271 of the received samples @xmath269 over the subcarriers within group @xmath3 , an estimate @xmath272 of the channel matrix @xmath268",
    "is first obtained using ica - pc , and then the likelihood @xmath273 is approximated as @xmath274 .",
    "accordingly , the likelihood function @xmath275 of all the received samples @xmath78 is approximated as @xmath276 , where @xmath277 .",
    "the detected modulation is selected as @xmath278 .    in fig .",
    "[ fig : ica=000026gibbs ] , we plot the performance of the approach based on ica - pc with different values of @xmath267 and gibbs sampling with random restarts and annealing .",
    "the number of runs are @xmath238 , and the annealing schedule is ( [ eq : tempering schedule ] ) .",
    "it can be seen from fig .",
    "[ fig : ica=000026gibbs ] that gibbs sampling significantly outperforms ica - pc . in this regard , note that , with @xmath279 , the accuracy in ica - pc is poor due to the insufficient number of observed data samples ; while with @xmath280 , the model mismatch problem becomes more severe due to the assumption of equal channel matrices in each subcarrier group .    to further emphasize the advantages of the proposed bayesian techniques , in fig .",
    "[ fig : ica=000026gibbs ] , we consider the case in which there are fewer receive antennas than transmit antennas by setting @xmath281 and all other parameters as above . while ica - pc can not be applied due to the ill - posedness of the ica problem , it can be observed that a success rate of @xmath282 can be attained by the proposed gibbs scheme at @xmath283 .",
    "it should be mentioned however , that the complexity of ica - pc of the order of @xmath284 @xcite is smaller than that of gibbs sampling ( see _ remark 5 _ ) , where @xmath285 denotes the maximum number of states of a constellation among all the possible modulation types , i.e. , @xmath286 .      to investigate the performance of the proposed gibbs sampling approach in the presence of a model mismatch ,",
    "we study here the case of coded ofdm .",
    "specifically , we assume that the information bits are first encoded using a convolutional code , and then modulated .",
    "we apply gibbs sampling with multiple random restarts and annealing with all relevant parameters being the same as in sec . [",
    "sub : performance - of - gibbs ] .",
    "the code rates are @xmath287 @xmath288 and @xmath289 , respectively . in fig .",
    "[ fig : codedofdm ] , the probability of correct classification is shown versus snr .",
    "it can be seen from the figure that the success rate decreases only slightly ( up to 6% ) as the code rate decreases .",
    "the degradation is caused by the fact that the coded transmitted symbols are not mutually independent due to the convolutional coding , and hence their prior distribution is mismatched with respect to the model ( [ eq : joint_posterior_2 ] ) .",
    "as the snr increases , the performance degradation for coded signals become minor , because , in this regime , gibbs sampling relies more on the observed samples than on the priors .",
    "we also studied the performance of the proposed gibbs sampling for ofdm systems with space - frequency coded symbols ( sf - ofdm ) @xcite . compared to the uncoded case , minor performance degradation ( 2% at @xmath283 and up to 8% at @xmath290 )",
    "is observed also due to the presence of a model mismatch .",
    "in this paper , we have proposed two bayesian modulation classification schemes for mimo - ofdm systems based on a selection of the prior distributions that adopts a latent dirichlet model and on the bayesian network formalism .",
    "the proposed gibbs sampling method converges to an effective solution and , using numerical results , its accuracy is demonstrated to improve for small sample sizes when switching to the mean field variational inference technique after a number of iterations .",
    "the speed of convergence is shown to improve via multiple random restarts and annealing .",
    "the techniques are seen to overcome the performance limitation of state - of - the - art non - bayesian schemes based on ica and bayesian schemes based on `` superconstellation '' methods .",
    "in fact , while most of the mentioned existing modulation classification algorithms rely on the assumptions that the channels are flat fading , that the number of receive antennas is no less than the number of transmit antenna , and/or that a large amount of samples are available ( as for pattern recognition - based methods ) , the proposed schemes achieve satisfactory performance under more general conditions .",
    "for example , with @xmath291 transmit antennas and under frequency selective fading channels with @xmath235 taps , a correct classification rate of above @xmath292 may be attained with @xmath293 receive antennas and with 256 received samples at each antenna ; and a success rate of above @xmath294 may be achieved with @xmath281 receive antenna and 256 received samples at the antenna .",
    "moreover , the proposed gibbs sampler presents a graceful degradation in the presence of a model mismatch caused by channel coding , e.g. , a decrease in the success rate by @xmath295 with a code rate of @xmath296 .",
    "future works include devising a gibbs sampling scheme that accounts for the effects of the timing and carrier frequency offsets for mimo systems following , e.g. , @xcite , @xcite , @xcite .",
    "in addition , the development of bayesian classification techniques that address non - gaussian noise is also a topic for further investigation .",
    "in this appendix , we give the expressions for all standard distributions that are useful to derive the conditional distributions for gibbs sampling in appendix ii .    1 .",
    "dirichlet distribution : @xmath297 @xmath298 where @xmath299 denotes the length of the vector @xmath300 , and @xmath301 stands for the gamma function @xcite .",
    "2 .   circular complex gaussian distribution : @xmath302 , @xmath303 where we use @xmath304 to denote the determinant of a matrix .",
    "3 .   inverse gamma distribution : : @xmath305 , @xmath306",
    "in this appendix , the required conditional distributions for gibbs sampling are derived .      applying ( [ eq : fullcondionaleq ] ) to ( [ eq : joint_posterior_2 ] ) ,",
    "we have @xmath307|\\mathbf{p}_{a}\\right)\\bigg\\}\\nonumber \\\\ \\propto & \\prod_{a\\in\\mathcal{a}}\\left[\\mathbf{p}_{a}\\left(a\\right)\\right]^{\\boldsymbol{\\gamma}\\left(a\\right)-1}\\prod_{n , k , m_{t}}\\bigg[\\sum_{a:\\,s_{m_{t}}[n , k]\\in a}\\mathbf{p}_{a}\\left(a\\right)/\\left|a\\right|\\bigg]\\nonumber \\\\ = & \\prod_{a\\in\\mathcal{a}}\\frac{1}{\\left|a\\right|}\\left[\\mathbf{p}_{a}\\left(a\\right)\\right]^{\\boldsymbol{\\gamma}\\left(a\\right)-1+c_{a}}\\nonumber \\\\ \\sim & \\mathrm{dirichlet}\\left(\\boldsymbol{\\gamma}+\\mathbf{c}\\right),\\end{aligned}\\ ] ] where @xmath157^{t}$ ] , and @xmath158 is the number of samples of transmitted symbols in constellation @xmath146 .",
    "applying ( [ eq : fullcondionaleq ] ) to ( [ eq : joint_posterior_2 ] ) , we have @xmath308\\bigg\\}\\nonumber \\\\ \\sim & \\mathcal{cn}(\\hat{\\mathbf{h}}_{m_{t},m_{r}},\\hat{\\boldsymbol{\\sigma}}_{m_{t},m_{r}}),\\end{aligned}\\ ] ] where @xmath309 and @xmath310 where the approximation in ( [ eq : h_sig ] ) follows from the fact that @xmath311 is very large such that the term @xmath312 can be neglected .      applying ( [ eq : fullcondionaleq ] ) to ( [ eq : joint_posterior_2 ] )",
    ", we have @xmath314 where @xmath165 and @xmath166 .",
    "by taking expectation with respect to updated distribution @xmath315 and @xmath316\\diagdown s_{m_{t}}[n , k],\\mathbf{h},\\sigma^{2}\\right)$ ] receptively , it can be shown that the expression for @xmath317|\\mathbf{p}_{a}\\right)\\big\\rangle_{\\mathbf{p}_{a}}$ ] is @xmath318|\\mathbf{p}_{a}\\right)\\big\\rangle_{q\\left(\\mathbf{\\mathbf{\\mathbf{p}_{a}}}\\right)}\\nonumber \\\\ = & \\sum_{a\\in\\mathcal{a}}\\mathrm{\\mathbf{1}}\\left(s_{m_{t}}[n , k]\\in a\\right)\\left[\\psi\\left(\\gamma_{a}\\right)-\\psi\\left(\\gamma_{0}\\right)-\\ln\\left|a\\right|\\right],\\end{aligned}\\ ] ] where @xmath319 ; and the expression for @xmath320|\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2}\\right)\\big\\rangle$ ] is @xmath321|\\mathbf{s}[n , k],\\mathbf{h}[n],\\sigma^{2}\\right)\\big\\rangle_{q\\left(\\mathbf{s}[n , k]\\diagdown s_{m_{t}}[n , k],\\mathbf{h},\\sigma^{2}\\right)}\\nonumber \\\\ \\propto & \\frac{\\alpha}{\\beta}\\bigg\\{2\\mathrm{real}\\left[\\mathbf{y}^{h}[n , k]\\right]\\big\\langle\\mathbf{h}[n]\\big\\rangle\\big\\langle\\mathbf{s}[n , k]\\big\\rangle_{q\\left(\\mathbf{s}[n , k]\\diagdown s_{m_{t}}[n , k]\\right)}\\nonumber \\\\   & -\\mathrm{tr}\\left(\\big\\langle\\mathbf{h}^{h}[n]\\mathbf{h}[n]\\big\\rangle\\mathrm{cov\\left(\\mathbf{s}[n , k]\\right)}\\right)\\nonumber \\\\   & -\\big\\langle\\mathbf{s}[n , k]\\big\\rangle_{q\\left(\\mathbf{s}[n , k]\\diagdown s_{m_{t}}[n , k]\\right)}^{h}\\cdot\\nonumber \\\\   & \\cdot\\big\\langle\\mathbf{h}^{h}[n]\\mathbf{h}[n]\\big\\rangle\\big\\langle\\mathbf{s}[n , k]\\big\\rangle_{q\\left(\\mathbf{s}[n , k]\\diagdown s_{m_{t}}[n , k]\\right)}\\bigg\\},\\end{aligned}\\ ] ] with the @xmath322 element of the covariance matrix of the vector @xmath42 $ ] @xmath323\\right)_{(m_{t}^{\\prime},m_{t}^{\\prime\\prime})}\\nonumber \\\\ = & \\mathrm{var(}s_{m_{t}^{\\prime}}[n , k])\\nonumber \\\\ = & \\begin{cases } \\big\\langle\\left|s_{m_{t}^{\\prime}}[n , k]\\right|^{2}\\big\\rangle-\\big\\langle\\left|s_{m_{t}^{\\prime}}[n , k]\\right|\\big\\rangle^{2 } , & \\mathrm{if}\\:m_{t}^{\\prime}=m_{t}^{\\prime\\prime},\\\\   & m_{t}^{\\prime\\prime}\\neq m_{t};\\\\ 0 , & \\mathrm{otherwise}. \\end{cases}\\end{aligned}\\ ] ] the @xmath324-th element of @xmath325\\big\\rangle_{q\\left(\\mathbf{s}[n , k]\\diagdown s_{m_{t}}[n , k]\\right)}$ ] being @xmath326\\big\\rangle_{(m_{t}^ { , } ) } & = \\begin{cases } \\big\\langle s_{m_{t}^{\\prime}}[n , k]\\big\\rangle , & \\mathrm{if\\:}m_{t}^{\\prime}\\neq m_{t}\\\\ s_{m_{t}}[n , k ] , & \\mathrm{if}\\:m_{t}^{\\prime}=m_{t } \\end{cases},\\end{aligned}\\ ] ] the @xmath327 element of @xmath328\\big\\rangle$ ] being the @xmath37-th element of the matrix product @xmath329 , and the @xmath322 element of @xmath330\\mathbf{h}[n]\\big\\rangle$ ] being @xmath331\\mathbf{h}[n]\\big\\rangle\\right]_{\\left(m_{t}^{\\prime},m_{t}^{\\prime\\prime}\\right)}=\\nonumber \\\\   & \\begin{cases } \\sum_{m_{r}=1}^{m_{r}}\\bigg\\{\\mathrm{tr}\\left[\\left[\\mathbf{w}_{(n,\\cdot)}\\right]^{h}\\mathbf{w}_{(n,\\cdot)}\\mathbf{\\boldsymbol{\\sigma}}_{m_{t}^{\\prime\\prime},m_{r}}\\right]+\\\\ \\big\\langle\\mathbf{h}_{m_{t}^{\\prime\\prime},m_{r}}\\big\\rangle^{h}\\left[\\mathbf{w}_{(n,\\cdot)}\\right]^{h}\\mathbf{w}_{(n,\\cdot)}\\big\\langle\\mathbf{h}_{m_{t}^{\\prime\\prime},m_{r}}\\big\\rangle\\bigg\\ } , & \\mathrm{if\\:}m_{t}^{\\prime}=m_{t}^{\\prime\\prime};\\\\ \\big\\langle\\mathbf{h}[n]\\big\\rangle_{\\left(\\cdot , m_{t}^{\\prime}\\right)}\\big\\langle\\mathbf{h}[n]\\big\\rangle_{\\left(\\cdot , m_{t}^{\\prime\\prime}\\right ) } , & \\mathrm{if}\\:m_{t}^{\\prime}\\neq m_{t}^{\\prime\\prime}. \\end{cases}\\end{aligned}\\ ] ]      o. a. dobre , s. rajan and r. inkol , `` joint signal detection and classification based on first - order cyclostationarity for cognitive radios , '' _ eurasip j. adv .",
    "signal process_. , vol .",
    "2009 , no .",
    "1 , p. 656719",
    "o. a. dobre , a. abdi , y. bar - ness and w. su , a survey of automatic modulation classification techniques : classical approaches and new developments , _ iet communications _ ,",
    "vol.1 , no .",
    "137 - 156 , apr . 2007 .",
    "f. hameed , o. a. dobre , and d. c. popescu , `` on the likelihood - based approach to modulation classification , '' _ ieee transactions on wireless communications _ , vol . 8 , no . 12 , pp . 5884 - 5892 , dec .",
    "2009 .",
    "o. a. dobre and f. hameed , likelihood - based algorithms for linear digital modulation classification in fading channels ,  in _ proc .",
    "19th ieee canadian conf .",
    "electrical computer engineering _ , pp .",
    "1347 - 1350 , ottawa , ontario , canada , may 2006 .",
    "y. liu , o. simeone , a. haimovich and w. su , modulation classification via gibbs sampling based on a latent dirichlet bayesian network , _ ieee signal proc .",
    "letters _ ,",
    "9 , pp . 1135 - 1139 , sep .",
    "s. amuru and c. r. c. m. da silva , `` a blind preprocessor for modulation classification applications in frequency - selective non - gaussian channels '' , _ ieee trans . communications _ , vol .",
    "156 - 169 , jan . 2015 .",
    "m. s. muhlhaus , m. oner , o. a. dobre , h. u. jakel and f. k. jondral , `` automatic modulation classification for mimo systems using forth - order cumulatns , '' in _ proc .",
    "ieee vehic .",
    "technology conf .",
    "_ fall , pp . 1 - 5 , quebec city , qc , can . , sep .",
    "m. s. muhlhaus , m. oner , o. a. dobre and f. k. jondral , `` a low complexity modulation classification algorithm for mimo systems , '' _ ieee communications letters _ ,",
    "1881 - 1884 , oct . 2013 .",
    "k. hassan , i. dayoub , w. hamouda , c. n. nzeza and m. berineau , `` blind digital modulation identification for spatially - correlated mimo systems , '' _ ieee trans .",
    "wireless commun .",
    "683 - 693 , feb . 2012 .",
    "m. marey and o. a. dobre , `` blind modulation classification algorithm for single and multiple - antenna systems over frequency - selective channels , '' _ ieee signal proc . letters _ , vol .",
    "9 , pp . 1098 - 1102 , sep .",
    "2014 .",
    "a. agirman - tosun , y. liu , a. m. haimovich , o. simeone , w. su , j. dabin and e. kanterakis , modulation classification of mimo - ofdm signals by independent component analysis and support vector machines , _",
    "ieee asilomar conference _ ,",
    "y. liu , o. simeone , a. haimovich and w. su , modulation classification of mimo - ofdm signals by gibbs sampling , in _ proc .",
    "the 49th conference on information sciences and systems ( ciss ) _ , no .",
    "1 , pp . 1 - 6 , baltimore ,",
    "18 - 20 , 2015 .",
    "s. aouda , a m. zoubir , c. m. s. see , `` a comparative study on source number detection '' , in _ proc .",
    "international symposium on signal processing and its applications _ , vol .",
    "173 - 176 , paris , france , jul . 2003 .",
    "f. z. merli , x. wang , g. m. vitetta , a bayesian multiuser detection algorithm for mimo - ofdm systems affected by multipath fading , carrier frequency offset , and phase noise , _ ieee j. on selected .",
    "areas in commun .",
    "3 , pp . 506- 516 , april 2008 .",
    "b. lu , x. wang , `` bayesian blind turbo receiver for coded ofdm systems with frequency offset and frequency - selective fading '' , _ ieee j. selected .",
    "areas in commun_. , vol .",
    "19 , no . 12 , pp . 2516 - 2527 , dec ."
  ],
  "abstract_text": [
    "<S> the problem of modulation classification for a multiple - antenna ( mimo ) system employing orthogonal frequency division multiplexing ( ofdm ) is investigated under the assumption of unknown frequency - selective fading channels and signal - to - noise ratio ( snr ) . </S>",
    "<S> the classification problem is formulated as a bayesian inference task , and solutions are proposed based on gibbs sampling and mean field variational inference . </S>",
    "<S> the proposed methods rely on a selection of the prior distributions that adopts a latent dirichlet model for the modulation type and on the bayesian network formalism . </S>",
    "<S> the gibbs sampling method converges to the optimal bayesian solution and , using numerical results , its accuracy is seen to improve for small sample sizes when switching to the mean field variational inference technique after a number of iterations . </S>",
    "<S> the speed of convergence is shown to improve via annealing and random restarts . </S>",
    "<S> while most of the literature on modulation classification assume that the channels are flat fading , that the number of receive antennas is no less than that of transmit antennas , and that a large number of observed data symbols are available , the proposed methods perform well under more general conditions . finally , the proposed bayesian methods are demonstrated to improve over existing non - bayesian approaches based on independent component analysis and on prior bayesian methods based on the ` superconstellation ' method .    </S>",
    "<S> bayesian inference ; modulation classification ; mimo - ofdm ; gibbs sampling ; mean field variational inference ; latent dirichlet model . </S>"
  ]
}