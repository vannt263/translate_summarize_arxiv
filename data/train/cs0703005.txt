{
  "article_text": [
    "a channel @xmath1 with noncausal state information at the sender has capacity @xmath7 as shown by gelfand and pinsker  @xcite .",
    "transmitting at capacity , however , obscures the state information @xmath3 as received by the receiver @xmath4 . in some instances we wish to convey the state information @xmath3 itself , which could be time - varying fading parameters or an original image that we wish to enhance .",
    "for example , a stage actor with face @xmath8 uses makeup @xmath9 to communicate to the back row audience @xmath10 . here",
    "@xmath9 is used to enhance and exaggerate @xmath8 rather than to communicate new information .",
    "another motivation comes from cognitive radio systems  @xcite with the additional assumption that the secondary user @xmath11 communicates its own message and at the same time facilitates the transmission of the primary user s signal @xmath3 .",
    "how should the transmitter communicate over the channel to `` amplify '' his knowledge of the state information to the receiver ? what is the optimal tradeoff between state amplification and independent information transmission ?    to answer these questions , we study the communication problem depicted in figure  [ fig : setup ] .    here",
    "the sender has access to the channel state sequence @xmath12 , independent and identically distributed ( i.i.d . ) according to @xmath13 , and wishes to transmit a message index @xmath14 : = \\{1 , 2 , \\ldots , 2^{nr}\\}$ ] , independent of @xmath3 , as well as to help the receiver reduce the uncertainty about the channel state in @xmath15 uses of a state dependent channel @xmath16 .",
    "based on the message @xmath17 and the channel state @xmath3 , the sender chooses @xmath18 and transmits it across the channel . upon observing the channel output @xmath4",
    ", the receiver guesses @xmath19 $ ] and forms a list @xmath20 that contains likely candidates of the actual state sequence @xmath3 .    without any observation @xmath4 ,",
    "the receiver would know only that the channel state @xmath3 is one of @xmath21 typical sequences ( with almost certainty ) and we can say the uncertainty about @xmath3 is @xmath22 . now upon observing @xmath4 and forming a list @xmath23 of likely candidates for @xmath3 , the receiver s list size is reduced from @xmath24 to @xmath25 .",
    "thus we define the _ channel state uncertainty reduction rate _ to be @xmath26 as a natural measure for the amount of information the receiver learns about the channel state . in other words ,",
    "the uncertainty reduction rate @xmath27 $ ] captures the difference between the original channel state uncertainty and the residual state uncertainty after observing the channel output . later in section  [ sec : gaussian ] we will draw a connection between the list size reduction and the conventional information measure @xmath2 that also captures the amount of information @xmath4 learns about @xmath3 .",
    "more formally , we define a @xmath28 code as the encoder map @xmath29",
    "\\times \\mathcal{s}^n \\to \\mathcal{x}^n\\ ] ] and decoder maps @xmath30\\\\ l_n & : \\mathcal{y}^n \\to 2^{\\mathcal{s}^n}\\end{aligned}\\ ] ] with list size @xmath31 the probability of a message decoding error @xmath32 and the probability of a list decoding error @xmath33 are defined respectively as @xmath34 where the message index @xmath17 is chosen uniformly over @xmath35 $ ] and the state sequence @xmath3 is drawn i.i.d .",
    "@xmath36 , independent of @xmath17 .",
    "a pair @xmath37 is said to be achievable if there exists a sequence of @xmath28 codes with @xmath38 and @xmath39 as @xmath40 . finally , we define the optimal @xmath41 tradeoff region , or the _ tradeoff region _ in short , to be the closure of all achievable @xmath41 pairs , and denote it by @xmath42 .",
    "this paper shows that the tradeoff region @xmath42 can be characterized as the union of all @xmath37 pairs satisfying @xmath43 for some joint distribution of the form @xmath44 .    as a special case ,",
    "if the encoder s sole goal is to `` amplify '' the state information ( @xmath45 ) , then the maximum uncertainty reduction rate @xmath46 is given by @xmath47 the maximum uncertainty reduction rate @xmath48 is achieved by designing the signal @xmath11 to enhance the receiver s estimation of the state @xmath3 while using the remaining pure information bearing freedom in @xmath11 to provide more information about the state .",
    "more specifically , there are three different components involved in reducing the receiver s uncertainty about the state :    1 .",
    "the transmitter uses the channel capacity to convey the state information . in section  [ sec : noncausal ] , we study the classical setup  @xcite of coding for memory with defective cells ( example  [ ex : memory ] ) and show that this `` source - channel separation '' scheme is optimal when the memory defects are symmetric .",
    "2 .   the transmitter gets out of the way of the receiver s view of the state .",
    "for instance , the maximum uncertainty reduction for the binary multiplying channel @xmath49 ( example  [ ex : binary ] in section  [ sec : noncausal ] ) with binary input @xmath50 and binary state @xmath51 is achieved by sending @xmath52 .",
    "the transmitter actively amplifies the state . in example",
    "[ ex : wdp ] in section  [ sec : gaussian ] , we consider the gaussian channel @xmath53 with gaussian state @xmath8 and gaussian noise @xmath54 . here",
    "the optimal transmitter amplifies the state as @xmath55 under the given power constraint @xmath56 .",
    "it is interesting to note that the maximum uncertainty reduction rate @xmath48 is the information rate @xmath57 that could be achieved if both the state @xmath8 and the signal @xmath9 could be freely designed , instead of the state @xmath8 being generated by nature .",
    "this rate also appears in the sum rate of the capacity region expression for the cooperative multiple access channel  ( * ? ? ?",
    "* problem 15.1 ) and the multiple access channel with cribbing encoders by willems and van der meulen  @xcite .",
    "when the state information is only _ causally _ available at the transmitter , that is , when the channel input @xmath58 depends on only the past and the current channel channel state @xmath59 , we will show that the tradeoff region @xmath42 is given as the union of all @xmath37 pairs satisfying @xmath60 over all joint distributions of the form @xmath61 .",
    "interestingly , the maximum uncertainty reduction rate @xmath48 stays the same as in the noncausal case  .",
    "that causality incurs no cost on the ( sum ) rate is again reminiscent of the multiple access channel with cribbing encoders  @xcite .    the problem of communication over state - dependent channels with state information known at the sender has attracted a great deal of attention .",
    "this research area was first pioneered by shannon  @xcite , kuznetsov and tsybakov  @xcite , and gelfand and pinsker  @xcite .",
    "several advancements in both theory and practice have been made over the years .",
    "for instance , heegard and el gamal  @xcite characterized the channel capacity and devised practical coding techniques for computer memory with defective cells .",
    "costa @xcite studied the now famous `` writing on dirty paper '' problem and showed that the capacity of an additive white gaussian noise channel is not affected by additional interference , as long as the entire interference sequence is available at the sender prior to the transmission .",
    "this fascinating result has been further extended with strong motivations from applications in digital watermarking ( see , for example , moulin and osullivan  @xcite , chen and wornell  @xcite , and cohen and lapidoth  @xcite ) and multi - antenna broadcast channels ( see , for example , caire and shamai  @xcite , weingarten , steinberg , and shamai  @xcite , and mohseni and cioffi  @xcite ) .",
    "readers are referred to caire and shamai  @xcite , lapidoth and narayan  @xcite , and jafar  @xcite for more complete reviews on the theoretical development of the field . on the practical side , erez , shamai , and",
    "zamir  @xcite proposed efficient coding schemes based on lattice strategies for binning .",
    "more recently , erez and ten brink  @xcite report efficient coding techniques that almost achieve the capacity of costa s dirty paper channel .    in @xcite , we formulated the problem of simultaneously transmitting pure information and helping the receiver estimate the channel state under a distortion measure .",
    "although the characterization of the optimal rate - distortion tradeoff is still open in general ( cf .",
    "@xcite ) , a complete solution is given for the gaussian case ( the writing on dirty paper channel ) under quadratic distortion  @xcite .",
    "in this particular case , optimality was shown for a simple power - sharing scheme between pure information transmission via costa s original coding scheme and state amplification via simple scaling .",
    "recently , merhav and shamai  @xcite considered a related problem of transmitting pure information , but this time under the additional requirement of _ minimizing _ the amount of information the receiver can learn about the channel state . in this interesting work , the optimal tradeoff between pure information rate @xmath0 and the amount of state information @xmath62 is characterized for both causal and noncausal setups .",
    "furthermore , for the gaussian noncausal case ( writing on dirty paper ) , the optimal rate - distortion tradeoff is given under quadratic distortion .",
    "( this may well be called `` writing dirty on paper '' . )    the current paper thus complements @xcite in a dual manner",
    ". it is refreshing to note that our notion of uncertainty reduction rate @xmath5 is essentially equivalent to merhav and shamai s notion of @xmath62 ; both notions capture the normalized mutual information @xmath63 .",
    "( see the discussion in section  [ sec : gaussian ] . )",
    "the crucial difference is that @xmath5 is to be maximized while @xmath62 is to be minimized .",
    "both problems admit single - letter optimal solutions .    the rest of this paper is organized as follows . in the next section ,",
    "we establish the optimal @xmath41 tradeoff region for the case in which the state information @xmath3 is noncausally available at the transmitter before the actual communication .",
    "section  [ sec : gaussian ] extends the notion of state uncertainty reduction to continuous alphabets , by identifying the list decoding requirement @xmath64 with the mutual information rate @xmath65 .",
    "in particular , we characterize the optimal @xmath37 tradeoff region for costa s `` writing on dirty paper '' channel .",
    "since the intuition gained from the study of the noncausal setup carries over when the transmitter has causal knowledge of the state sequence , the causal case is treated only briefly in section  [ sec : causal ] , followed by concluding remarks in section  [ sec : conc ] .",
    "in this section , we characterize the optimal tradeoff region between the pure information rate @xmath0 and the state uncertainty reduction rate @xmath5 with state information noncausally available at the transmitter , as formulated in section  [ sec : intro ] .",
    "[ thm : noncausal ] the tradeoff region @xmath42 for a state - dependent channel @xmath66 with state information @xmath3 noncausally known at the transmitter is the union of all @xmath41 pairs satisfying @xmath67 for some joint distribution of the form @xmath44 , where the auxiliary random variable @xmath68 has cardinality bounded by @xmath69 .    as will be clear from the proof of the converse , the region given by  is convex .",
    "( we can merge the time - sharing random variable into @xmath68 . )",
    "since the auxiliary random variable @xmath68 affects the first inequality only , the cardinality bound on @xmath70 follows directly from the usual technique ; see gelfand and pinsker  @xcite or a general treatment by salehi  @xcite .",
    "finally , we can take @xmath9 as a deterministic function of @xmath71 without reducing the region , but at the cost of increasing the cardinality bound of @xmath68 ; refer to the proof of lemma  [ lemma : equiv ] below .",
    "it is easy to see that we can recover the gelfand ",
    "pinsker capacity formula @xmath72 for the other extreme case of pure state amplification , we have the following result .",
    "[ corollary : noncausal ] under the condition of theorem  [ thm : noncausal ] , the maximum uncertainty reduction rate @xmath73 is given by @xmath74    thus the receiver can learn about the state @xmath3 essentially at the maximal cut - set rate @xmath75    before we prove theorem  [ thm : noncausal ] , we need the following two lemmas .",
    "the first one extends fano s inequality  ( * ? ? ?",
    "* lemma 7.9.1 ) to list decoding .",
    "[ lemma : gen - fano ] for a sequence of list decoders @xmath76 @xmath77 with list size @xmath78 fixed for each @xmath15 , let @xmath79 be the sequence of corresponding probabilities of list decoding error . if @xmath80 , then @xmath81 where @xmath82 as @xmath83 .",
    "define an error random variable @xmath62 as @xmath84 we can then expand @xmath85 note that @xmath86 and @xmath87 .",
    "we can also bound @xmath88 as @xmath89 where the inequality follows because when there is no error , the remaining uncertainty is at most @xmath90 , and when there is an error , the uncertainty is at most @xmath91 .",
    "this implies that @xmath92 taking @xmath93 proves the desired result .",
    "the second lemma is crucial to the proof of theorem  [ thm : noncausal ] and contains a more interesting technique than lemma  [ lemma : gen - fano ] .",
    "this lemma shows that the third inequality can be replaced by a tighter inequality below ( recall that @xmath94 since @xmath95 ) , which becomes crucial for the achievability proof of theorem  [ thm : noncausal ] .",
    "[ lemma : equiv ] let @xmath96 be the union of all @xmath37 pairs satisfying . let @xmath97 be the closure of the union of all @xmath37 pairs satisfying @xmath98 for some joint distribution @xmath99 , where the auxiliary random variable @xmath68 has finite cardinality . then @xmath100    since @xmath95 forms a markov chain , it is trivial to check that @xmath101 for the other direction of inclusion , we need some notation .",
    "let @xmath102 be the set of all distributions of the form @xmath103 consistent with the given @xmath13 and @xmath1 , where the auxiliary random variable @xmath68 is defined on an arbitrary finite set .",
    "further let @xmath104 be the restriction of @xmath102 such that @xmath105 for some function @xmath106 , i.e. , @xmath107 takes values @xmath108 or @xmath109 only .",
    "if we define @xmath110 to denote the closure of all @xmath37 pairs satisfying , , and over @xmath104 , or equivalently , if @xmath110 is defined to be the restriction of @xmath111 over a smaller set of distributions @xmath104 , then clearly @xmath112 let @xmath113 be defined as the closure of @xmath41 pairs satisfying . since @xmath114 forms a markov chain on @xmath104 , we have @xmath115    to complete the proof , it now suffices to show that @xmath116 to see this , we restrict @xmath113 to the distributions of the form @xmath117 with @xmath118 independent of @xmath119 , namely , @xmath120 with deterministic @xmath121 , i.e. , @xmath122 is a function of @xmath123 , and call this restriction @xmath124 . since @xmath9 is a deterministic function of @xmath125 and at the same time @xmath126 form a markov chain , @xmath124 can be written as the closure of all @xmath41 pairs satisfying @xmath127 for some distribution of the form @xmath128 satisfying .",
    "but we have @xmath129 and the set of conditional distributions on @xmath130 given @xmath8 satisfying is as rich as any @xmath131 .",
    "( indeed , any conditional distribution @xmath132 can be represented as @xmath133 for appropriately chosen @xmath134 and _ deterministic _ distribution @xmath135 with cardinality of @xmath136 upper bounded by @xmath137 ; see also ( * ? ? ?",
    "* eq .  ( 44 ) ) . )",
    "therefore , we have @xmath138 which completes the proof .",
    "now we are ready to prove theorem  [ thm : noncausal ] .    for the proof of achievability , in the light of lemma  [ lemma : equiv ] , it suffices to prove that any pair @xmath139 satisfying , , for some @xmath140 is achievable . since the coding technique is quite standard , we only sketch the proof here .",
    "for fixed @xmath140 , the result of gelfand ",
    "pinsker  @xcite shows that the transmitter can send @xmath141 bits reliably across the channel .",
    "now we allocate @xmath142 bits for sending the pure information and use the remaining @xmath143 bits for sending the state information by random binning .",
    "more specifically , we assign typical @xmath3 sequences to @xmath144 bins at random and send the bin index of the observed @xmath3 using @xmath145 bits . at the receiving end ,",
    "the receiver is able to decode the codeword @xmath146 from @xmath4 with high probability . using joint typicality of @xmath147",
    ", the state uncertainty can be first reduced from @xmath148 to @xmath149 . indeed , the number of typical @xmath3 sequences jointly typical with @xmath150 is bounded by @xmath151 .",
    "in addition , using @xmath143 bits of independent refinement information from the hash index of @xmath3 , we can further reduce the state uncertainty by @xmath152 .",
    "hence , by taking the list of all @xmath3 sequences jointly typical with @xmath150 satisfying the hash check , we have the total state uncertainty reduction rate @xmath153 by varying @xmath142 , it can be readily seen that all @xmath37 pairs satisfying @xmath154 for any fixed @xmath155 are achievable .    for the proof of converse , we have to show that given any sequence of @xmath28 codes with @xmath156 the @xmath41 pairs must satisfy @xmath157 for some joint distribution @xmath158 .",
    "the pure information rate @xmath0 can be readily bounded from the previous work by gelfand and pinsker  ( * ? ? ?",
    "* proposition 3 ) .",
    "here we repeat a simpler proof given in heegard  ( * ? ? ?",
    "* appendix 2 ) for completeness ; see also ( * ? ?",
    "? * lecture  13 ) . starting with fano s inequality",
    ", we have the following chain of inequalities : @xmath159 where ( a ) follows from the csiszr sum formula @xmath160 and ( b ) follows because @xmath161 is independent of @xmath162 . by recognizing the auxiliary random variable @xmath163 and noting that @xmath164 form a markov chain",
    ", we have @xmath165    on the other hand , since @xmath166 , we can trivially bound @xmath5 by lemma  [ lemma : gen - fano ] as @xmath167 similarly , we can bound @xmath168 as @xmath169 where ( a ) follows since @xmath17 is independent of @xmath3 and conditioning reduces entropy , ( b ) follows from the data processing inequality ( both directions ) , and ( c ) follows from the memorylessness of the channel .",
    "we now introduce the usual time - sharing random variable @xmath170 uniform over @xmath171 , independent of everything else . then implies @xmath172 on the other hand , implies @xmath173 where the last equality follows since @xmath174 form a markov chain .",
    "finally , we recognize @xmath175 @xmath176 and note that @xmath177 , @xmath178 , and @xmath179 , which completes the proof of the converse",
    ".    roughly speaking , the optimal coding scheme is equivalent to sending the codeword @xmath146 reliably at the gelfand  pinsker rate @xmath180 and reducing the receiver s uncertainty by @xmath181 from @xmath4 and the decoded codeword @xmath146 .",
    "it should be noted that @xmath182 has the same form as the achievable region for the dual tradeoff problem between pure information rate @xmath0 and ( minimum ) normalized mutual information rate @xmath183 studied in @xcite .",
    "but we can reduce the uncertainty about @xmath3 further by allocating part @xmath152 of the pure information rate @xmath184 to convey independent refinement information ( hash index of @xmath3 ) . by varying @xmath185 $ ]",
    "we can trace the entire tradeoff region @xmath186 .",
    "it turns out an alternative coding scheme based on wyner ",
    "ziv source coding with side information  @xcite , instead of random binning , also achieves the tradeoff region @xmath42 . to see this ,",
    "fix any @xmath140 and @xmath187 satisfying @xmath188 and consider the wyner  ziv encoding of @xmath3 with covering codeword @xmath189 and side information @xmath190 at the decoder .",
    "more specifically , we can generate @xmath191 @xmath189 codewords and assign them into @xmath144 bins . as before we use the gelfand ",
    "pinsker coding to convey a message of rate @xmath141 reliably over the channel .",
    "since the rate @xmath192 is sufficient to reconstruct @xmath189 at the receiver with side information @xmath4 and @xmath146 , we can allocate the rate @xmath152 for conveying @xmath189 and use the remaining rate @xmath193 for extra pure information . forming a list of @xmath3 jointly typical with @xmath194 results in the uncertainty reduction rate @xmath5 given by @xmath195",
    "thus the tradeoff region @xmath196 can be achieved via the combination of two fundamental results in communication with side information : channel coding with side information by gelfand and pinsker  @xcite and rate distortion with side information by wyner and ziv  @xcite .",
    "it is also interesting to note that the information about @xmath3 can be transmitted in a manner completely independent of geometry ( random binning ) or completely dependent on geometry ( random covering ) ; refer to @xcite for a similar phenomenon in a relay channel problem .",
    "when @xmath10 is a function of @xmath197 , it is optimal to identify @xmath198 and theorem  [ thm : noncausal ] simplifies to the following corollary .",
    "[ coro : deterministic ] the tradeoff region @xmath42 for a deterministic state - dependent channel @xmath199 with state information @xmath3 noncausally known at the transmitter is the union of all @xmath41 pairs satisfying @xmath200 for some joint distribution of the form @xmath201 . in particular , the maximum uncertainty reduction rate is given by @xmath202    the next two examples show different flavors of optimal state uncertainty reduction .    [",
    "ex : memory ] consider the problem of conveying information using a write - once memory device with stuck - at defective cells @xcite as depicted in figure  [ fig : memory ] .",
    "here each memory cell has probability @xmath203 of being stuck at @xmath108 , probability @xmath204 of being stuck at @xmath109 , and probability @xmath205 of being a good cell , with @xmath206 .",
    "it is easy to see that the channel output @xmath10 is a simple deterministic function of the channel input @xmath9 and the state @xmath8 .",
    "now it is easy to verify that the tradeoff region @xmath42 is given by @xmath207 where @xmath208 can be chosen arbitrarily ( @xmath209 ) .",
    "this region is achieved by choosing @xmath210 . without loss of generality , we can choose @xmath211 independent of @xmath8 , because the input @xmath9 affects @xmath10 only when @xmath212 .",
    "there are two cases to consider .    1 .",
    "if @xmath213 , then the choice of @xmath214 maximizes both and , and hence achieves the entire tradeoff region @xmath42 .",
    "the optimal transmitter splits the full channel capacity @xmath215 to send both the pure information and the state information .",
    "( see figure  [ fig : graph](a ) for the case @xmath216 . ) 2 .",
    "on the other hand , when @xmath217 , there is a clear tradeoff in our choice of @xmath208 .",
    "for example , consider the case @xmath218 .",
    "if the goal is to communicate pure information over the channel , we should take @xmath214 to maximize the number of distinguishable input preparations .",
    "this gives the channel capacity @xmath219 .",
    "if the goal is , however , to help the receiver reduce the state uncertainty , we take @xmath220 , i.e. , we transmit a fixed signal @xmath221 .",
    "this way , the transmitter can minimize his interference with the receiver s view of the state @xmath8 .",
    "the entire tradeoff region is given in figure  [ fig : graph](b ) .",
    "[ ex : binary ] consider the binary multiplying channel @xmath222 , where the output @xmath10 is the product of the input @xmath50 and the state @xmath223 .",
    "we assume that the state sequence @xmath3 is drawn i.i.d . according to @xmath224 .",
    "it can be easily shown that the optimal tradeoff region is given by @xmath225 this is achieved by @xmath210 , independent of @xmath8 .",
    "as in example  [ ex : memory](b ) , there is a tension between the pure information transmission and the state amplification . when the goal is to maximize the pure information rate , we should choose @xmath226 to achieve the capacity @xmath227 .",
    "but when the goal is to maximize the state uncertainty reduction rate , we should choose @xmath228 ( @xmath52 ) to achieve @xmath229 . in words , to maximize the state uncertainty reduction rate ,",
    "the transmitter simply clears the receiver s view of the state .",
    "the previous section characterized the tradeoff region @xmath42 between the pure information rate @xmath0 and the state uncertainty reduction rate @xmath230 .",
    "apparently the notion of uncertainty reduction rate @xmath5 is meaningful only when the channel state @xmath8 has finite cardinality ( i.e. , @xmath231 ) , or at least when @xmath232 .",
    "however , from the proof of theorem  [ thm : noncausal ] ( the generalized fano s inequality in lemma  [ lemma : gen - fano ] ) , along with the fact that the optimal region is single - letterizable , we can take an alternative look at the notion of state uncertainty reduction as reducing the list size from @xmath21 to @xmath233 .",
    "we will show shortly in proposition  [ prop : noncausal ] that the difference @xmath234 of the normalized list size is essentially equivalent to the normalized mutual information @xmath235 , which is well - defined for an arbitrary state space @xmath236 and captures the amount of information the receiver @xmath4 can learn about the state @xmath3 ( or lack thereof  @xcite ) .",
    "hence , the physically motivated notion @xmath5 of list size reduction is consistent with the mathematical information measure @xmath237 , and both notions of state uncertainty reduction can be used interchangeably , especially when @xmath236 is finite .    to be more precise ,",
    "we define a @xmath238 code by an encoding function @xmath29 \\times \\mathcal{s}^n \\to \\mathcal{x}^n\\ ] ] and a decoding function @xmath239.\\ ] ] then the associated state uncertainty reduction rate for the @xmath240 code is defined as @xmath241 where the mutual information is with respect to the joint distribution @xmath242 induced by @xmath243 with message @xmath17 distributed uniformly over @xmath35 $ ] , independent of @xmath3 .",
    "similarly , the probability of error is defined as @xmath244 a pair @xmath37 is said to be achievable if there exists a sequence of @xmath238 codes with @xmath245 and @xmath246 the closure of all achievable @xmath37 pairs is called the tradeoff region @xmath247 .",
    "( here we use the notation @xmath247 instead of @xmath42 to temporarily distinguish this from the original problem formulated in terms of the list size reduction . )",
    "we now show that the optimal tradeoff @xmath248 between the information transmission rate @xmath0 and the mutual information rate @xmath5 has the same solution as the optimal tradeoff @xmath42 between @xmath0 and the list size reduction rate @xmath5 .",
    "[ prop : noncausal ] the tradeoff region @xmath247 for a state - dependent channel @xmath66 with state information @xmath3 noncausally known at the transmitter is the closure of all @xmath41 pairs satisfying @xmath249 for some joint distribution of the form @xmath44 with auxiliary random variable @xmath68 .",
    "hence , @xmath248 has the identical characterization as @xmath42 in theorem  [ thm : noncausal ] .",
    "let @xmath250 be the region described by . we provide a sandwich proof @xmath251 , which is given implicitly in the proof of theorem  [ thm : noncausal ] .",
    "more specifically , consider a finite partition and @xmath10 is defined as @xmath252_p ; [ y]_q),$ ] where the supremum is over all finite partitions @xmath253 and @xmath170 ; see kolmogorov  @xcite and pinsker  @xcite . ] to quantize the state random variable @xmath8 into @xmath254 $ ] . under this partition ,",
    "let @xmath255}^{**}$ ] be the set of all @xmath37 pairs satisfying @xmath256 ) \\\\",
    "\\delta & \\le h([s ] ) \\\\",
    "r + \\delta & \\le i(x , [ s ] ; y)\\end{aligned}\\ ] ] for some joint distribution of the form @xmath257)p(u , x|[s])p(y|x,[s])$ ] with auxiliary random variable @xmath68 .",
    "consider the original list size reduction problem with state information @xmath254 $ ] and let @xmath255}^*$ ] denote the tradeoff region .",
    "then theorem  [ thm : noncausal ] shows that @xmath255}^ { * * } = \\mathcal{r}_{[s]}^*$ ] . in particular , for any @xmath258 and @xmath259}^{**}$",
    "] , there exists a sequence of @xmath260 codes @xmath261 such that @xmath262 and @xmath263^n \\ne l_n(y^n ) ) \\to 0 $ ] .",
    "now from the generalized fano s inequality ( lemma  [ lemma : gen - fano ] ) , the achievable list size reduction rate @xmath264 should satisfy @xmath265^n ; y^n ) + n { \\epsilon}_n \\le i(s^n ; y^n ) + n{\\epsilon}_n\\ ] ] with @xmath82 as @xmath83 . hence by letting @xmath83 and @xmath266 we have from the definition of @xmath248 that @xmath267}^ { * * } = \\mathcal{r}_{[s]}^ * \\subseteq \\mathcal{r}_i^*.\\ ] ] also it follows trivially from repeating the intermediate steps in the converse proof of theorem  [ thm : noncausal ] that @xmath268 .    finally taking a sequence of partitions with mesh @xmath269 and hence letting @xmath255}^ { * * } \\to \\mathcal{r}^{**}$ ] , we have the desired result .",
    "since both notions of state uncertainty reduction , the list size reduction @xmath270 and the mutual information @xmath63 , lead to the same answer , we will subsequently use them interchangeably and denote the tradeoff region by the same symbol @xmath42 .",
    "[ ex : wdp ] consider costa s writing on dirty paper model depicted in figure  [ fig : wdp ] as the canonical example of a continuous state - dependent channel . here",
    "the channel output is given by @xmath271 , where @xmath18 is the channel input subject to a power constraint @xmath272 , @xmath273 is the additive white gaussian state , and @xmath274 is the white gaussian noise .",
    "we assume that @xmath3 and @xmath275 are independent .    for the writing on dirty paper model , we have the following tradeoff between the pure information transmission and the state uncertainty reduction .",
    "[ prop : gaussian ] the tradeoff region @xmath196 for the gaussian channel depicted in figure  [ fig : wdp ] is characterized by the boundary points @xmath276 where @xmath277    the achievability follows from proposition  [ prop : noncausal ] with trivial extension to the input power constraint .",
    "in particular , we use the simple power sharing scheme proposed in  @xcite , where a fraction @xmath278 of the input power is used to transmit the pure information using costa s writing on dirty paper coding technique , while the remaining @xmath279 fraction of the power is used to amplify the state . in other words ,",
    "@xmath280 with @xmath281 independent of @xmath8 , and @xmath282 with @xmath283 evaluating @xmath284 and @xmath285 for each @xmath278 , we recover and .",
    "the proof of converse is essentially the same as that of ( * ? ? ?",
    "* theorem 2 ) , which we do not repeat here .    as an extreme point of the @xmath37",
    ", we recover costa s writing on dirty paper result @xmath286 by taking @xmath287 . on the other hand ,",
    "if state uncertainty reduction is the goal , then all of the power should be used for state amplification . the maximum uncertainty reduction rate @xmath288 is achieved with @xmath289 and @xmath290 .",
    "in ( * ? ? ? * theorem 2 ) , the optimal tradeoff was characterized between the pure information rate @xmath0 and the receiver s state estimation error @xmath291 .",
    "although the notion of state estimation error @xmath292 in @xcite and our notion of the uncertainty reduction rate @xmath5 appear to be distinct objectives at first sight , the optimal solutions to both problems are identical , as shown in the proof of proposition  [ prop : gaussian ] .",
    "there is no surprise here .",
    "because of the quadratic gaussian nature of both problems , minimizing the mean squared error @xmath293 can be recast into maximizing the mutual information @xmath294 , and vice versa .",
    "also the optimal state uncertainty reduction rate @xmath48 ( or equivalently , the minimum state estimation error @xmath295 is achieved by the symbol - by - symbol amplification @xmath296 .",
    "finally , it interesting to compare the optimal coding scheme to the optimal coding scheme when the goal is to minimize ( instead of maximizing ) the uncertainty reduction  @xcite , which is essentially based on coherent subtraction of @xmath9 and @xmath8 with possible randomization .",
    "the previous two sections considered the case in which the transmitter has complete knowledge of the state sequence @xmath3 prior to the actual communication . in this section ,",
    "we consider another model in which the transmitter learns the state sequence on the fly , i.e. , the encoding function @xmath297 \\times \\mathcal{s}^i \\to \\mathcal{x } , \\qquad i = 1,2,\\ldots , n,\\ ] ] depends causally on the state sequence .",
    "we state our main theorem .",
    "[ thm : causal ] the tradeoff region @xmath42 for a state - dependent channel @xmath66 with state information @xmath3 causally known at the transmitter is the union of all @xmath41 pairs satisfying @xmath298 for some joint distribution of the form @xmath61 , where the auxiliary random variable @xmath68 has cardinality bounded by @xmath299 .    as in the noncausal case , the region is convex . since the auxiliary random variable @xmath68 affects the first inequality only , the cardinality bound @xmath300 ; see shannon  @xcite . ) finally , we can take @xmath9 as a deterministic function of @xmath71 without decreasing the region .",
    "compared to the noncausal tradeoff region @xmath301 in theorem  [ thm : noncausal ] , the causal tradeoff region @xmath302 in theorem  [ thm : causal ] is smaller in general .",
    "more precisely , @xmath302 is characterized by the same set of inequalities   as in @xmath301 , but the set of joint distributions is restricted to those with auxiliary variable @xmath68 independent of @xmath8 . indeed , from the independence between @xmath68 and @xmath8 , we can rewrite as @xmath303 which is exactly the same as . thus the inability to use the future state sequence decreases the tradeoff region .",
    "however , only the inequality , or equivalently , the inequality , is affected by the causality , and the sum rate does not change from .",
    "since the proof of theorem  [ thm : causal ] is essentially identical to that of theorem  [ thm : noncausal ] , we skip most of the steps .",
    "the least straightforward part is the following lemma .",
    "[ lemma : equiv2 ] let @xmath96 be the union of all @xmath41 pairs satisfying . let @xmath97 be the closure of the union of all @xmath37 pairs satisfying , , and @xmath304 for some joint distribution @xmath305 where the auxiliary random variable @xmath68 has finite cardinality . then @xmath100    the proof is a verbatim copy of the proof of lemma  [ lemma : equiv ] , except that here @xmath68 is independent of @xmath8 , i.e. , @xmath306 .",
    "the final step follows since the set of conditional distributions on @xmath307 given @xmath8 of the form @xmath308 with deterministic @xmath121 is as rich as any @xmath309 , and @xmath310 with this replacement , the desired proof follows along the same lines as the proof of lemma  [ lemma : equiv ] .",
    "as one extreme point of the tradeoff region @xmath42 , we recover the shannon capacity formula  @xcite for channels with causal side information at the transmitter as follows : @xmath311 on the other hand , the maximum uncertainty reduction rate @xmath48 for pure state amplification is identical to that for the noncausal case given in corollary  [ corollary : noncausal ] .    [",
    "corollary : causal ] under the condition of theorem  [ thm : causal ] , the maximum uncertainty reduction rate @xmath48 is given by @xmath312    thus the receiver can learn about the state essentially at the maximum cut - set rate , even under the causality constraint .",
    "for example , the symbol - by - symbol amplification strategy @xmath289 is optimal for the gaussian channel ( example  [ ex : wdp ] ) for both causal and noncausal cases .",
    "finally , we compare the tradeoff regions @xmath302 and @xmath301 with a communication problem that has a totally different motivation , yet has a similar capacity expression . in (",
    "* situations 3 and 4 ) , willems and van der meulen studied the multiple access channel with cribbing encoders . in this communication problem , the multiple access channel @xmath313 has two inputs and one output .",
    "the primary transmitter @xmath8 and the secondary transmitter @xmath9 wish to send independent messages @xmath314 $ ] and @xmath315 $ ] respectively to the common receiver @xmath10 .",
    "the difference from the classical multiple access channel is that either the secondary transmitter @xmath9 learns the primary transmitter s signal @xmath8 on the fly ( @xmath316 ( * ? ? ?",
    "* situation 3 ) ) or @xmath9 knows the entire signal @xmath3 ahead of time ( @xmath317 ( * ? ? ?",
    "* situation 4 ) ) .",
    "the capacity region @xmath318 for both cases is given by all @xmath37 pairs satisfying @xmath319 for some joint distribution @xmath320 .",
    "this capacity region @xmath318 looks almost identical to the tradeoff regions @xmath301 and @xmath302 in theorems  [ thm : noncausal ] and [ thm : causal ] , except for the first inequality .",
    "moreover , has the same form as the capacity expression for channels with state information available at _ both _ the encoder and decoder , either causally or noncausally .",
    "( the causality has no cost when both the transmitter and the receiver share the same side information ; see , for example , caire and shamai  ( * ? ?",
    "* proposition 1 ) . )",
    "it should be stressed , however , that the problem of cribbing multiple access channels and our state uncertainty reduction problem have a fundamentally different nature .",
    "the former deals with encoding and decoding of the signal @xmath3 , while the latter deals with uncertainty reduction in an uncoded sequence @xmath3 specified by nature . in a sense ,",
    "the cribbing multiple access channel is a detection problem , while the state uncertainty reduction is an estimation problem .",
    "because the channel is state dependent , the receiver is able to learn something about the channel state from directly observing the channel output .",
    "thus , to help the receiver narrow down the uncertainty about the channel state at the highest rate possible , the sender must jointly optimize between facilitating state estimation and transmitting refinement information , rather than merely using the channel capacity to send the state description .",
    "in particular , the transmitter should summarize the state information in such a way that the summary information results in the maximum uncertainty reduction when coupled with the receiver s initial estimate of the state . more generally , by taking away some resources used to help the receiver reduce the state uncertainty",
    ", the transmitter can send additional pure information to the receiver and trace the entire @xmath37 tradeoff region .",
    "there are three surprises here .",
    "first , the receiver can learn about the channel state and the independent message at a maximum cut - set rate @xmath57 over all joint distributions @xmath321 consistent with the given state distribution @xmath13 .",
    "second , to help the receiver reduce the uncertainty in the initial estimate of the state ( namely , to increase the mutual information from @xmath322 to @xmath323 ) , the transmitter can allocate the achievable information rate @xmath324 in two alternative methods ",
    "random binning and its dual , random covering .",
    "thirdly , as far as the sum rate @xmath325 and the maximum uncertainty reduction rate @xmath48 are concerned , there is no cost associated with restricting the encoder to learn the state sequence on the fly .",
    "b.  chen and g.  w. wornell , `` quantization index modulation : a class of provably good methods for digital watermarking and information embedding , '' _ ieee trans .",
    "inf . theory _ ,",
    "it-47 , no .  4 , pp . 14231443 , 2001 .",
    "a.  sutivong , t.  m. cover , m.  chiang , and y .- h .",
    "kim , `` rate vs. distortion trade - off for channels with state information , '' in _ proc .",
    "inform . theory _ , lausanne , switzerland , june / july 2002 , p. 226 .",
    "h.  weingarten , y.  steinberg , and s.  shamai , `` the capacity region of the gaussian multiple - input multiple - output broadcast channel , '' _ ieee trans .",
    "inf . theory _ , vol .",
    "it-52 , no .  9 , pp .",
    "39363964 , sept ."
  ],
  "abstract_text": [
    "<S> we consider the problem of transmitting data at rate @xmath0 over a state dependent channel @xmath1 with state information available at the sender and at the same time conveying the information about the channel state itself to the receiver . </S>",
    "<S> the amount of state information that can be learned at the receiver is captured by the mutual information @xmath2 between the state sequence @xmath3 and the channel output @xmath4 . </S>",
    "<S> the optimal tradeoff is characterized between the information transmission rate @xmath0 and the state uncertainty reduction rate @xmath5 , when the state information is either causally or noncausally available at the sender . in particular , </S>",
    "<S> when state transmission is the only goal , the maximum uncertainty reduction rate is given by @xmath6 . </S>",
    "<S> this result is closely related and in a sense dual to a recent study by merhav and shamai , which solves the problem of _ masking _ the state information from the receiver rather than conveying it . </S>"
  ]
}