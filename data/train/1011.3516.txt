{
  "article_text": [
    "relationships between information theory and statistical physics have been widely recognized in the last few decades , from a wide spectrum of aspects .",
    "these include conceptual aspects , of parallelisms and analogies between theoretical principles in the two disciplines , as well as technical aspects , of mapping between mathematical formalisms in both fields and borrowing analysis techniques from one field to the other .",
    "one example of such a mapping , is between the paradigm of random codes for channel coding and certain models of magnetic materials , most notably , ising models and spin glass models ( see , e.g. , @xcite,@xcite,@xcite,@xcite , and many references therein ) .",
    "today , it is quite widely believed that research in the intersection between information theory and statistical physics may have the potential of fertilizing both disciplines .",
    "this paper is more related to the former aspect mentioned above , namely , the relationships between the two areas in the conceptual level .",
    "however , it has also ingredients from the second aspect . in particular , let us consider two questions in the two fields , which at first glance , may seem completely unrelated , but will nevertheless turn out later to be very related",
    ". these are special cases of more general questions that we study later in this paper .",
    "the first is a simple question in statistical mechanics , and it is about a certain extension of a model described in ( * ? ? ?",
    "* , problem 13 ) : consider a one  dimensional chain of @xmath0 connected elements ( e.g. , monomers or whatever basic units that form a polymer chain ) , arranged along a straight line ( see fig .",
    "[ chain ] ) , and residing in thermal equilibrium at fixed temperature @xmath1 .",
    "the are two types of elements , which will be referred to as type ` 0 ' and type ` 1 ' . the number of elements of each type @xmath2 ( with @xmath2 being either ` 0 ' or ` 1 ' ) is given by @xmath3 , where @xmath4 ( and so , @xmath5 ) .",
    "each element of each type may be in one of two different states , labeled by @xmath6 , where @xmath6 also takes on the values ` 0 ' and ` 1 ' .",
    "the length and the internal energy of an element of type @xmath2 at state @xmath6 are given by @xmath7 and @xmath8 ( independently of @xmath2 ) , respectively .",
    "a contracting force @xmath9 is applied to one edge of the chain while the other edge is fixed .",
    "what is the minimum amount of mechanical work @xmath10 that must be carried out by this force , along an isothermal process at temperature @xmath1 , in order to shrink the chain from its original length @xmath11 ( when no force was applied ) into a shorter length , @xmath12 , where @xmath13 is a given constant ?",
    "the second question is in information theory . in particular",
    ", it is the classical problem of lossy source coding , and some of the notation here will deliberately be chosen to be the same as before : an information source emits a string of @xmath0 independent symbols , @xmath14 , where each @xmath15 may either be ` 0 ' or ` 1 ' , with probabilities @xmath16 and @xmath17 , respectively .",
    "a lossy source encoder maps the source string , @xmath18 , into a shorter ( compressed ) representation of average length @xmath19 , where @xmath20 is the coding rate ( compression ratio ) , and the compatible decoder maps this compressed representation into a reproduction string , @xmath21 , where each @xmath22 is again , either ` 0 ' or ` 1 ' . the fidelity of the reproduction is measured in terms of a certain distortion ( or distance ) function , @xmath23 , which should be as small as possible , so that @xmath24 would be as ` close ' as possible to @xmath25 .",
    "is required to be strictly identical to @xmath24 , in which case @xmath26 . however , in some applications , one might be willing to trade off between compression and fidelity , i.e. , slightly increase the distortion at the benefit of reducing the compression ratio @xmath20 . ] in the limit of large @xmath0 , what is the minimum coding rate @xmath27 for which there exists an encoder and decoder such that the average distortion , @xmath28 , would not exceed @xmath12 ?",
    "it turns out , as we shall see in the sequel , that the two questions have intimately related answers . in particular , the minimum amount of work @xmath10 , in the first question , is related to @xmath29 ( a.k.a .",
    "the _ rate  distortion function _ ) , of the second question , according to @xmath30 provided that the hamiltonian , @xmath8 , in the former problem , is given by @xmath31 where @xmath32 is boltzmann s constant , and @xmath33 is the relative frequency ( or the empirical probability ) of the symbol @xmath34 in the reproduction sequence @xmath24 , pertaining to an optimum lossy encoder  decoder with average per  symbol distortion @xmath35 ( for large @xmath0 ) .",
    "moreover , the minimum amount of work @xmath10 , which is simply the free energy difference between the final equilibrium state and the initial state of the chain , is achieved by a reversible process , where the compressing force @xmath36 grows very slowly from zero , at the beginning of the process , up to a final level of @xmath37 where @xmath38 is the derivative of @xmath29 ( see fig .  [ rd ] ) .",
    "thus , physical compression is strongly related to data compression , and the fundamental physical limit on the minimum required work is intimately connected to the fundamental information ",
    "theoretic limit of the minimum required coding rate .",
    "this link between the the physical model and the lossy source coding problem is obtained from a large deviations perspective .",
    "the exact details will be seen later on , but in a nutshell , the idea is this : on the one hand , it is possible to represent @xmath29 as the large deviations rate function of a certain rare event , but on the other hand , this large deviations rate function , involves the use of the legendre transform , which is a pivotal concept in thermodynamics and statistical mechanics . moreover , since this legendre transform is applied to the ( logarithm of the ) moment generating function ( of the distortion variable ) , which in turn , has the form a partition function , this paves the way to the above described analogy .",
    "the legendre transform is associated with the optimization across a certain parameter , which can be interpreted as either inverse temperature ( as was done , for example , in @xcite,@xcite,@xcite,@xcite ) or as a ( generalized ) force , as proposed here .",
    "the interpretation of this parameter as force is somewhat more solid , for reasons that will become apparent later .",
    "one application of this analogy , between the two models , is a parametric representation of the rate ",
    "distortion function @xmath29 as an integral of the minimum mean square error ( mmse ) in a certain bayesian estimation problem , which is obtained in analogy to a certain variant of the fluctuation ",
    "dissipation theorem .",
    "this representation opens the door for derivation of upper and lower bounds on the rate ",
    "distortion function via bounds on the mmse , as was demonstrated in a companion paper @xcite .",
    "another possible application is demonstrated in the present paper : when the setup is extended to allow information sources with memory ( non i.i.d .",
    "processes ) , then the analogous physical model consists of interactions between the various particles . when these interactions are sufficiently strong ( and with high enough dimension ) , then the system exhibits phase transitions . in the information  theoretic domain , these phase transitions mean irregularities and threshold effects in the behavior of the relevant information  theoretic function , in this case , the rate  distortion function .",
    "thus , analysis tools and physical insights are ` imported ' from statistical mechanics to information theory .",
    "a particular model example for this is worked out in section 4 .",
    "the outline of the paper is as follows . in section 2 ,",
    "we provide some relevant background in information theory , which may safely be skipped by readers that possess this background . in section 3 ,",
    "we establish the analogy between lossy source coding and the above described physical model , and discuss it in detail . in section 4",
    ", we demonstrate the analysis for a system with memory , as explained in the previous paragraph .",
    "finally , in section 5 we summarize and conclude .",
    "one of the most elementary roles of information theory is to provide fundamental performance limits pertaining to certain tasks of information processing , such as data compression , error ",
    "correction coding , encryption , data hiding , prediction , and detection / estimation of signals and/or parameters from noisy observations , just to name a few ( see e.g. , @xcite ) .    in this paper , our focus is on the first item mentioned  data compression , a.k.a .  _ source coding _",
    ", where the mission is to convert a piece of information ( say , a long file ) , henceforth referred to as the _ source data _ , into a shorter ( normally , binary ) representation , which enables either perfect recovery of the original information , as in the case of _ lossless compression _ , or non  perfect recovery , where the level of reconstruction errors ( or distortion ) should remain within pre - specified limits , which is the case of _ lossy data compression_.    lossless compression is possible whenever the statistical characterization of the source data inherently exhibits some level of _ redundancy _ that can be exploited by the compression scheme , for example , a binary file , where the relative frequency of 1 s is much larger than that of the 0 s , or when there is a strong statistical dependence between consecutive bits .",
    "these types of redundancy exist , more often than not , in real  life situations .",
    "if some level of errors and distortion are allowed , as in the lossy case , then compression can be made even more aggressive .",
    "the choice between lossless and lossy data compression depends on the application and the type of data to be compressed .",
    "for example , when it comes to sensitive information , like bank account information , or a piece of important text , then one may not tolerate any reconstruction errors at all .",
    "on the other hand , images and audio / video files , may suffer some degree of harmless reconstruction errors ( which may be unnoticeable to the human eye or ear , if designed cleverly ) and thus allow stronger compression , which would be very welcome , since images and video files are typically enormously large .",
    "the _ compression ratio _ , or the _ coding rate _",
    ", denoted @xmath20 , is defined as the ( average ) ratio between the length of the compressed file ( in bits ) and the length of the original file .",
    "the basic role of information theory , in the context of lossless / lossy source coding , is to characterize the fundamental limits of compression : for a given statistical characterization of the source data , normally modeled by a certain random process , what is the minimum achievable compression ratio @xmath20 as a function of the allowed average distortion , denoted @xmath35 , which is defined with respect to some distortion function that measures the degree of proximity between the source data and the recovered data .",
    "the characterization of this minimum achievable @xmath20 for a given @xmath35 , denoted as a function @xmath29 , is called the _ rate  distortion function _ of the source with respect to the prescribed distortion function .",
    "for the lossless case , of course , @xmath39 .",
    "another important question is how , in principle , one may achieve ( or at least approach ) this fundamental limit of optimum performance , @xmath29 ? in this context , there is a big gap between lossy compression and lossless compression .",
    "while for the lossless case , there are many practical algorithms ( most notably , adaptive huffman codes , lempel  ziv codes , arithmetic codes , and more ) , in the lossy case , there is unfortunately , no constructive practical scheme whose performance comes close to @xmath29 .",
    "the simplest non  trivial model of an information source is that of an i.i.d.process , a.k.a .",
    "a _ discrete memoryless source _ ( dms ) , where the source symbols , @xmath14 , take on values in a common finite set ( alphabet ) @xmath40 , they are statistically independent , and they are all drawn from the same probability mass function , denoted by @xmath41 . the source string @xmath42 is compressed into a binary representation depends on @xmath25 , the code should be designed such that the running bit - stream ( formed by concatenating compressed strings corresponding to successive @xmath0blocks from the source ) could be uniquely parsed in the correct manner and then decoded . to this end , the lengths @xmath43 must be collectively large enough so as to satisfy the kraft inequality . the details can be found , for example , in @xcite .",
    "] of length @xmath44 ( which may or may not depend on @xmath25 ) , whose average is @xmath45 , and the compression ratio is @xmath46 . in the decoding ( or decompression )",
    "process , the compressed representation is mapped into a reproduction string @xmath47 , where each @xmath22 , @xmath48 , takes on values in the _ reproduction alphabet _",
    "@xmath49 ( which is typically either equal to @xmath40 or to a subset of @xmath40 , but this is not necessary ) .",
    "the fidelity of the reconstruction string @xmath24 relative to the original source string @xmath6 is measured by a certain distortion function @xmath50 , where the function @xmath51 is defined additively as @xmath52 , @xmath53 being a function from @xmath54 to the non  negative reals .",
    "the average distortion per symbol is @xmath55 .",
    "as said , @xmath29 is defined ( in general ) as the infimum of all rates @xmath20 for which there exist a sufficiently large @xmath0 and an encoder  decoder pair for @xmath0blocks , such that the average distortion per symbol would not exceed @xmath35 . in the case of a dms @xmath56 ,",
    "an elementary coding theorem of information theory asserts that @xmath29 admits the following formula @xmath57 where @xmath2 is a random variable that represents a single source symbol ( i.e. , it is governed by @xmath56 ) , @xmath58 is the mutual information between @xmath2 and @xmath6 , i.e. , @xmath59 @xmath60 being the marginal distribution of @xmath6 , which is associated with a given conditional distribution @xmath61 , and the minimum is over all these conditional probability distributions for which @xmath62 for @xmath39 , @xmath6 must be equal to @xmath2 with probability one ( unless @xmath63 also for some @xmath64 ) , and then @xmath65 the shannon entropy of @xmath2 , as expected . as mentioned earlier",
    ", there are concrete compression algorithms that come close to @xmath66 for large @xmath0 . for @xmath67 , however , the proof of achievability of @xmath29 is non  constructive .",
    "the idea for proving the existence of a sequence of codes ( indexed by @xmath0 ) whose performance approach @xmath29 as @xmath68 , is based on the notion of _ random coding _ : if we can define , for each @xmath0 , an ensemble of codes of ( fixed ) rate @xmath20 , for which the average per  symbol distortion ( across both the randomness of @xmath6 and the randomness of the code ) is asymptotically less than or equal to @xmath35 , then there must exist at least one sequence of codes in that ensemble , with this property .",
    "the idea of random coding is useful because if the ensemble of codes is chosen wisely , the average ensemble performance is surprisingly easy to derive ( in contrast to the performance of a specific code ) and proven to meet @xmath29 in the limit of large @xmath0 .    for a given @xmath0 ,",
    "consider the following ensemble of codes : let @xmath69 denote the conditional probability matrix that achieves @xmath29 and let @xmath70 denote the corresponding marginal distribution of @xmath6 .",
    "consider now a random selection of @xmath71 reproduction strings , @xmath72 , each of length @xmath0 , where each @xmath73 , @xmath74 , is drawn independently ( of all other reproduction strings ) , according to @xmath75 this randomly chosen code is generated only once and then revealed to the decoder . upon observing an incoming source string @xmath25",
    ", the encoder seeks the first reproduction string @xmath76 that achieves @xmath77 , and then transmits its index @xmath78 using @xmath79 bits , or equivalently , @xmath80 _ nats_. has the obvious interpretation of the number of bits needed to specify a number between @xmath81 and @xmath82 , the natural base logarithm is often mathematically more convenient to work with . the quantity @xmath83 can also be thought of as the description length , but in different units , called nats , rather than bits , where the conversion is according to @xmath81 nat @xmath84 bits . ]",
    "if no such codeword exists , which is referred to as the event of _ encoding failure _ , the encoder sends an arbitrary sequence of @xmath19 nats , say , the all  zero sequence .",
    "the decoder receives the index @xmath78 and simply outputs the corresponding reproduction string @xmath76 .    obviously , the per  symbol distortion would be less than @xmath35 whenever the encoder does not fail , and so , the main point of the proof is to show that the probability of failure ( across the randomness of @xmath25 and the ensemble of codes ) is vanishingly small for large @xmath0 , provided that @xmath20 is slightly larger than ( but can be arbitrarily close to ) @xmath29 , i.e. , @xmath85 for an arbitrarily small @xmath86 .",
    "the idea is that for any source string that is _ typical _ to @xmath56 ( i.e. , the empirical relative frequency of each symbol in @xmath25 is close to its probability ) , one can show ( see , e.g. , @xcite ) that the probability that a single , randomly selected reproduction string @xmath24 would satisfy @xmath87 , decays exponentially as @xmath88 $ ] .",
    "thus , the above described random selection of the entire codebook together with the encoding operation , are equivalent to conducting @xmath82 independent trials in the quest for having at least one @xmath78 for which @xmath77 , @xmath74 .",
    "if @xmath89}$ ] , the number of trials is much larger ( by a factor of @xmath90 ) than the reciprocal of the probability of a single ` success ' , @xmath88 $ ] , and so , the probability of obtaining at least one such success ( which is case where the encoder succeeds ) tends to unity as @xmath68 .",
    "we took the liberty of assuming that source string is typical to @xmath56 because the probability of seeing a non ",
    "typical string is vanishingly small .      from the foregoing discussion",
    ", we see that @xmath29 has the additional interpretation of the exponential rate of the probability of the event @xmath87 , where @xmath25 is a given string typical to @xmath56 and @xmath24 is randomly drawn i.i.d .  under @xmath70 .",
    "consider the following chain of equalities and inequalities for bounding the probability of this event from above .",
    "letting @xmath91 be a parameter taking an arbitrary non  positive value , we have : @xmath92\\right\\}\\right>\\nonumber\\\\ & = & e^{-nsd}\\left<\\prod_{i=1}^n e^{sd(x_i,{\\hat{x}}_i)}\\right>\\nonumber\\\\ & = & e^{-nsd}\\prod_{i=1}^n\\left < e^{sd(x_i,{\\hat{x}}_i)}\\right>\\nonumber\\\\ & = & e^{-nsd}\\prod_{x\\in{{\\cal x}}}\\prod_{i:~x_i = x}\\left < e^{sd(x,{\\hat{x}}_i)}\\right>\\nonumber\\\\ & = & e^{-nsd}\\prod_{x\\in{{\\cal x}}}\\left[\\left < e^{sd(x,{\\hat{x}})}\\right>\\right]^{np(x)}\\nonumber\\\\ & = & e^{-ni(d , s)}\\end{aligned}\\ ] ] where @xmath93 is defined as @xmath94\\right\\}.\\ ] ] the tightest upper bound is obtained by minimizing it over the range @xmath95 , which is equivalent to maximizing @xmath93 in that range .",
    "i.e. , the tightest upper bound of this form is @xmath96 , where @xmath97 ( the chernoff bound ) . while this is merely an upper bound , the methods of large deviations theory ( see , e.g. , @xcite ) can readily be used to establish the fact that the bound @xmath96 is tight in the exponential sense , namely , it is the correct asymptotic exponential decay rate of @xmath98 .",
    "accordingly , @xmath99 is called the _ large deviations rate function _ of this event . combining this with the foregoing discussion",
    ", it follows that @xmath100 , which means that an alternative expression of @xmath29 is given by @xmath101.\\ ] ] interestingly , the same expression was obtained in ( * ? ? ?",
    "* corollary 4.2.3 ) using completely different considerations ( see also @xcite ) . in this paper",
    ", however , we will also concern ourselves , more generally , with the rate  distortion function , @xmath102 , pertaining to a given reproduction distribution @xmath103 , which may not necessarily be the optimum one , @xmath70 .",
    "this function is defined similarly as in eq .",
    "( [ rdc ] ) , but with the additional constraint that the marginal distribution that represents the reproduction would agree with the given @xmath103 , i.e. , @xmath104 . by using the same large deviations arguments as above , but for an arbitrary random coding distribution @xmath103 , one readily observes that @xmath102 is of the same form as in eq .",
    "( [ ldrdo ] ) , except that @xmath70 is replaced by the given @xmath103 ( see also @xcite ) .",
    "this expression will now be used as a bridge to the realm of equilibrium statistical mechanics .",
    "consider the parametric representation of the rate ",
    "distortion function @xmath102 , with respect to a given reproduction distribution @xmath103 : @xmath105.\\ ] ] the expression in the inner brackets , @xmath106 can be thought of as the partition function of a single particle of `` type '' @xmath2 , which is defined as follows . assuming a certain fixed temperature @xmath107 , consider the hamiltonian @xmath108 imagine now that this particle may be in various states , indexed by @xmath109 . when a particle of type @xmath2 lies in state @xmath6",
    "its internal energy is @xmath8 , as defined above , and its length is @xmath7 .",
    "next assume that instead of working with the parameter @xmath91 , we rescale and redefine the free parameter as @xmath36 , where @xmath110 .",
    "then , @xmath36 has the physical meaning of a force that is conjugate to the length .",
    "this force is stretching for @xmath111 and contracting for @xmath9 . with a slight abuse of notation , the gibbs partition function ( * ? ? ?",
    "* section 4.8 ) pertaining to a single particle of type @xmath2 is then given by @xmath112\\right\\},\\ ] ] and accordingly , @xmath113 is the gibbs free energy per particle of type @xmath2 .",
    "thus , @xmath114 is the average per  particle gibbs free energy ( or the gibbs free energy density ) pertaining to a system with a total of @xmath0 non  interacting particles , from @xmath115 different types , where the number of particles of type @xmath2 is @xmath116 , @xmath117 .",
    "the helmholtz free energy per particle is then given by the legendre transform @xmath118.\\ ] ] however , for @xmath119 ( which is the interesting range , where @xmath120 ) , the maximizing @xmath36 is always non  positive , and so , @xmath121.\\ ] ] invoking now eq .",
    "( [ ldrd ] ) , we readily identify that @xmath122 which supports the analogy between the lossy data compression problem and the behavior of the statistical  mechanical model of the kind described in the third paragraph of the introduction : according to this model , the physical system under discussion is a long chain with a total of @xmath0 elements , which is composed of @xmath115 different types of shorter chains ( indexed by @xmath2 ) , where the number of elements in the short chain of type @xmath2 is @xmath116 , and where each element of each chain can be in various states , indexed by @xmath6 . in each state @xmath6 ,",
    "the internal energy and the length of each element are @xmath123 and @xmath7 , as described above .",
    "the total length of the chain , when no force is applied , is therefore @xmath124 . upon applying a contracting force @xmath9 ,",
    "states of shorter length become more probable , and the chain shrinks to the length of @xmath12 , where @xmath35 is related to @xmath36 according to the legendre relation is concave and @xmath125 is convex , the inverse legendre transform holds as well , and so , there is one  to  one correspondence between @xmath36 and @xmath35 . ]",
    "( [ legendre ] ) between @xmath125 and @xmath126 , which is given by @xmath127 where @xmath128 and @xmath129 are , respectively , the derivatives of @xmath125 and @xmath102 relative to @xmath35 .",
    "the inverse relation is , of course , @xmath130 where @xmath131 is the derivative of @xmath126 .",
    "since @xmath102 is proportional to the free energy , where the system is held in equilibrium at length @xmath12 , it also means the minimum amount of work required in order to shrink the system from length @xmath11 to length @xmath12 , and this minimum is obtained by a reversible process of slow increase in @xmath36 , starting from zero and ending at the final value given by eq .",
    "( [ fl ] ) .",
    "+ _ discussion _",
    "+ this analogy between the lossy source coding problem and the statistical  mechanical model of a chain , may suggest that physical insights may shed light on lossy source coding and vice versa .",
    "we learn , for example , that the contribution of each source symbol @xmath2 to the distortion , @xmath132 , is analogous to the length contributed by the chain of type @xmath2 when the contracting force @xmath36 is applied .",
    "we have also learned that the local slope of @xmath102 is proportional to a force , which must increase as the chain is contracted more and more aggressively , and near @xmath39 , it normally tends to infinity , as @xmath133 in most cases .",
    "this slope parameter also plays a pivotal role in theory and practice of lossy source coding : on the theoretical side , it gives rise to a variety of parametric representations of the rate ",
    "distortion function @xcite,@xcite , some of which support the derivation of important , non  trivial bounds . on the more practical side ,",
    "often data compression schemes are designed by optimizing an objective function with the structure of @xmath134 thus @xmath36 plays the role of a lagrange multiplier .",
    "this lagrange multiplier is now understood to act like a physical force , which can be ` tuned ' to the desired trade  off between rate and distortion . as yet another example , the convexity of the rate  distortion function can be understood from a physical point of view , as the helmholtz free energy is also convex , a fact which has a physical explanation ( related to the fluctuation  dissipation theorem ) , in addition to the mathematical one .    at this point ,",
    "two technical comments are in order :    1 .",
    "we emphasized the fact that the reproduction distribution @xmath103 is fixed . for a given target value of @xmath35 ,",
    "one may , of course , have the freedom to select the optimum distribution @xmath70 that minimizes @xmath102 , which would yield the rate ",
    "distortion function , @xmath29 , and so , in principle , all the foregoing discussion applies to @xmath29 as well .",
    "some caution , however , must be exercised here , because in general , the optimum @xmath103 may depend on @xmath35 ( or equivalently , on @xmath91 or @xmath36 ) , which means , that in the analogous physical model , the internal energy @xmath8 depends on the force @xmath36 ( in addition to the linear dependence of the term @xmath135 ) .",
    "this kind of dependence does not support the above described analogy in a natural manner .",
    "this is the reason that we have defined the rate ",
    "distortion problem for a fixed @xmath103 , as it avoids this problem .",
    "thus , even if we pick the optimum @xmath70 for a given target distortion level @xmath35 , then this @xmath70 must be kept unaltered throughout the entire process of increasing @xmath36 from zero to its final value , given by ( [ fl ] ) , although @xmath70 may be sub  optimum for all intermediate distortion values that are met along the way from @xmath136 to @xmath35 .",
    "an alternative interpretation of the parameter @xmath91 , in the partition function @xmath137 , could be the ( negative ) inverse temperature , as was suggested in @xcite ( see also @xcite ) . in this case",
    ", @xmath7 would be the internal energy of an element of type @xmath2 at state @xmath6 and @xmath33 , which does not include a power of @xmath91 , could be understood as being proportional to the degeneracy ( in some coarse  graining process ) . in this case , the distortion would have the meaning of internal energy , and since no mechanical work is involved , this would also be the heat absorbed in the system , whereas @xmath102 would be related to the entropy of the system .",
    "the legendre transform , in this case , is the one pertaining to the passage between the microcanonical ensemble and the canonical one .",
    "the advantage of the interpretation of @xmath91 ( or @xmath36 ) as force , as proposed here , is that it lends itself naturally to a more general case , where there is more than one fidelity criterion .",
    "for example , suppose there are two fidelity criteria , with distortion functions @xmath138 and @xmath139 . here , there would be two conjugate forces , @xmath36 and @xmath140 , respectively ( for example , a mechanical force and a magnetic force ) , and the physical analogy carries over .",
    "on the other hand , this would not work naturally with the temperature interpretation approach since there is only one temperature parameter in physics .",
    "we end this section by providing a representation of @xmath102 and @xmath35 in an integral form , which follows as a simple consequence of its representation as the legendre transform of @xmath141 , as in eq .",
    "( [ ldrd ] ) .",
    "since the maximization problem in ( [ ldrd ] ) is a convex problem ( @xmath141 is convex in @xmath91 ) , the minimizing @xmath91 for a given @xmath35 is obtained by taking the derivative of the r.h.s .",
    ", which leads to @xmath142 this equation yields the distortion level @xmath35 for a given value of the minimizing @xmath91 in eq .",
    "( [ ldrd ] ) .",
    "let us then denote @xmath143 which means that @xmath144 taking the derivative of ( [ ds ] ) , we readily obtain @xmath145\\nonumber\\\\ & = & \\sum_{x\\in{{\\cal x}}}p(x ) \\left[\\frac{\\sum_{{\\hat{x}}\\in\\hat{{{\\cal x}}}}q({\\hat{x}})d^2(x,{\\hat{x}})e^{sd(x,{\\hat{x } } ) } } { \\sum_{{\\hat{x}}\\in\\hat{{{\\cal x}}}}q({\\hat{x}})e^{sd(x,{\\hat{x}})}}-\\right.\\nonumber\\\\ & & \\left.\\left(\\frac{\\sum_{{\\hat{x}}\\in\\hat{{{\\cal x}}}}q({\\hat{x}})d(x,{\\hat{x}})e^{sd(x,{\\hat{x } } ) } } { \\sum_{{\\hat{x}}\\in\\hat{{{\\cal x}}}}q({\\hat{x}})e^{sd(x,{\\hat{x}})}}\\right)^2\\right]\\nonumber\\\\ & = & \\sum_{x\\in{{\\cal x}}}p(x)\\cdot\\mbox{var}_s\\{d(x,{\\hat{x}})|x\\}\\nonumber\\\\ & \\equiv&\\mbox{mmse}_s\\{d(x,{\\hat{x}})|x\\},\\end{aligned}\\ ] ] where @xmath146 is the variance of @xmath7 w.r.t.the conditional probability distribution @xmath147 the last line of eq .",
    "( [ derds ] ) means that the expectation of @xmath146 w.r.t .",
    "@xmath56 is exactly the mmse of estimating @xmath7 based on the ` observation ' @xmath2 using the conditional mean of @xmath7 given @xmath2 as an estimator . differentiating both sides of eq .",
    "( [ rds ] ) , we get @xmath148 or , equivalently , @xmath149 and @xmath150 in @xcite , this representation was studied extensively and was found quite useful . in particular , simple bounds on the mmse were shown to yield non ",
    "trivial bounds on the rate  distortion function in some cases where an exact closed form expression is unavailable .",
    "the physical analogue of this representation is the fluctuation ",
    "dissipation theorem , where the conditional variance , or equivalently the mmse , plays the role of the fluctuation , which describes the sensitivity , or the linear response , of the length of the system to a small perturbation in the contracting force .",
    "if @xmath91 is interpreted as the negative inverse temperature , as was mentioned before , then the mmse is related to the specific heat of the system .",
    "the theoretical framework established in the previous section extends , in principle , to information sources with memory ( non i.i.d .",
    "sources ) , with a natural correspondence to a physical system of interacting particles .",
    "while the rate  distortion function for a general source with memory is unknown , the maximum rate achievable by random coding can still be derived in many cases of interest . unlike the case of the memoryless source , where the best random coding distribution is memoryless as well ,",
    "when the source exhibits memory , there is no apparent reason to believe that good random coding distributions should remain memoryless either , but it is not known what the form of the optimum random coding distribution is .",
    "for example , there is no theorem that asserts that the optimum random coding distribution for a markov source is markov too .",
    "one can , however examine various forms of the random coding distributions and compare them .",
    "intuitively , the stronger is the memory of the source , the stronger should be the memory of the random coding distribution .    in this section ,",
    "we demonstrate one family of random coding distributions , with a very strong memory , which is inspired by the curie ",
    "weiss model of spin arrays , that possesses long range interactions .",
    "consider the random coding distribution @xmath151 where @xmath152 , @xmath153 and @xmath154 are parameters , and @xmath155 is the appropriate normalization constant .",
    "using the identity , @xmath156 we can represent @xmath103 as a mixture of i.i.d .",
    "distributions as follows : @xmath157 where @xmath158 is the memoryless source @xmath159^n}\\ ] ] and the weighting function @xmath160 is given by @xmath161\\right]\\right\\}.\\ ] ] next , we repeat the earlier derivation for each @xmath158 individually : @xmath162 where @xmath163 is a short  hand notation for @xmath164 , which is well defined from the previous section since @xmath158 is an i.i.d .  distribution . at this point ,",
    "two observations are in order : first , we observe that a separate large deviations analysis for each i.i.d .",
    "component @xmath158 is better than applying a similar analysis directly to @xmath103 itself , without the decomposition , since it allows a different optimum choice of @xmath91 for each @xmath165 , rather than one optimization of @xmath91 that compromises all values of @xmath165 .",
    "moreover , since the upper bound is exponentially tight for each @xmath158 , then the corresponding mixture of bounds is also exponentially tight .",
    "the second observation is that since @xmath158 is i.i.d . , @xmath166 depends on the source @xmath56 only via the marginal distribution of a single symbol @xmath167 , which is assumed here to be independent of @xmath78 .",
    "a saddle  point analysis gives rise to the following expression for @xmath102 , the random  coding rate distortion function pertaining to @xmath103 , which is the large deviations rate function : @xmath168+r_{\\theta}(d)\\right\\ } + \\phi(b , j)\\ ] ] where @xmath169 we next have a closer look at @xmath166 , assuming @xmath170 , and using the hamming distortion function , i.e. , @xmath171 since @xmath172 we readily obtain @xmath173\\nonumber\\\\ & & + \\ln\\cosh(b+\\theta).\\end{aligned}\\ ] ] on substituting this expression back into the expression of @xmath102 , we obtain the formula @xmath174\\right\\}\\right ) + \\phi(b , j),\\end{aligned}\\ ] ] which requires merely optimization over two parameters .",
    "in fact , the maximization over @xmath91 , for a given @xmath165 , can be carried out in closed form , as it boils down to the solution of a quadratic equation .",
    "specifically , for a symmetric source ( @xmath175 ) , the optimum value of @xmath91 is given by @xmath176-\\ln[2(1-d)],\\ ] ] where @xmath177 the details of the derivation of this expression are omitted as they are straightforward .",
    "as the curie ",
    "weiss model is well known to exhibit phase transitions ( see , e.g. , @xcite,@xcite ) , it is expected that @xmath102 , under this model , would consist of phase transitions as well . at the very least , the last term @xmath178 is definitely subjected to phase transitions in @xmath153 ( the magnetic field ) and @xmath154 ( the coupling parameter ) . the first term , that contains the minimization over @xmath165 , is somewhat more tricky to analyze in closed form .",
    "in essence , considering @xmath179 as a function of @xmath165 , substituting it back into the expression of @xmath102 , and finally , differentiating w.r.t .",
    "@xmath165 and equating to zero ( in order to minimize ) , then it turns out that the ( internal ) derivative of @xmath180 w.r.t .",
    "@xmath165 is multiplied by a vanishing expression ( by the very definition of @xmath181 as a solution to the aforementioned quadratic equation ) .",
    "the final result of this manipulation is that the minimizing @xmath165 should be a solution to the equation @xmath182 this is a certain ( rather complicated ) variant of the well  known magnetization equation in the mean field model , @xmath183 , which is well known to exhibit a first order phase transition in @xmath153 whenever @xmath184 .",
    "it is therefore reasonable to expect that the former equation in @xmath165 , which is more general , will also have phase transitions , at least in some cases .",
    "in this paper , we have drawn a conceptually simple analogy between lossy compression of memoryless sources and statistical mechanics of a system of non  interacting particles . beyond the belief that this analogy may be interesting on its own right",
    ", we have demonstrated its usefulness in several levels . in particular , in the last section , we have observed that the analogy between the information ",
    "theoretic model and the physical model is not merely on the pure conceptual level , but moreover , analysis tools from statistical mechanics can be harnessed for deriving information  theoretic functions .",
    "moreover , physical insights concerning phase transitions , in systems with strong interactions , can be ` imported ' for the understanding possible irregularities in these functions , in this case , non  smooth dependence on @xmath153 and @xmath154 ."
  ],
  "abstract_text": [
    "<S> we draw a certain analogy between the classical information  theoretic problem of lossy data compression ( source coding ) of memoryless information sources and the statistical mechanical behavior of a certain model of a chain of connected particles ( e.g. , a polymer ) that is subjected to a contracting force . </S>",
    "<S> the free energy difference pertaining to such a contraction turns out to be proportional to the rate  </S>",
    "<S> distortion function in the analogous data compression model , and the contracting force is proportional to the derivative this function . </S>",
    "<S> beyond the fact that this analogy may be interesting on its own right , it may provide a physical perspective on the behavior of optimum schemes for lossy data compression ( and perhaps also , an information  theoretic perspective on certain physical system models ) . </S>",
    "<S> moreover , it triggers the derivation of lossy compression performance for systems with memory , using analysis tools and insights from statistical mechanics . </S>"
  ]
}