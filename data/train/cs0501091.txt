{
  "article_text": [
    "when dealing with high volumes of vector - valued data of some large dimension @xmath0 , it is often assumed that the data possess some intrinsic geometric description in a space of unknown dimension @xmath1 and that the high dimensionality arises from an unknown stochastic mapping of @xmath2 into @xmath3 .",
    "we can pose the problem of _ nonlinear dimensionality reduction _ ( nldr ) @xcite as follows : given raw data with values in @xmath3 , we wish to obtain optimal estimates of the intrinsic dimension @xmath4 and of the stochastic map with the purpose of modeling the intrinsic geometry of the data in @xmath2 .",
    "one typically considers the following set - up : we are given a sample @xmath5 , where @xmath6 are i.i.d . according to an unknown absolutely continuous distribution @xmath7 .",
    "the corresponding pdf @xmath8 has to be estimated from the observation as @xmath9 .",
    "the intrinsic dimension @xmath4 of the data may not be known in advance and would also have be estimated as @xmath10 .",
    "since the pdf @xmath8 is assumed to arise from a stochastic map of the low - dimensional space @xmath2 into the high - dimensional space @xmath3 , we can use our knowledge about @xmath4 and @xmath8 in order to make inferences about the intrinsic geometry of the data . in the absence of such knowledge",
    ", any such inference has to be made based on the estimates @xmath11 and @xmath12 . in this paper",
    "we introduce a complexity - regularized quantization approach to nldr , assuming that the intrinsic dimension @xmath4 of the data is given ( e.g. , as a maximum - likelihood estimate @xcite ) .",
    "we begin with a quick sketch of some notions about smooth manifolds @xcite .",
    "a _ smooth manifold _ of dimension @xmath4 is a set @xmath13 together with a collection @xmath14 , where the sets @xmath15 cover @xmath13 and each map @xmath16 is a bijection of @xmath17 onto an open set @xmath18 , such that for all @xmath19 with @xmath20 the map @xmath21 is smooth .",
    "the pairs @xmath22 are called _ charts _ of @xmath13 , and the entire collection @xmath23 is referred to as an _ atlas_. intuitively , the charts describe the points of @xmath13 by _ local _ coordinates : given @xmath24 and a chart @xmath25 , @xmath16 maps any point @xmath26 `` near @xmath27 '' ( i.e. , @xmath28 ) to an element of @xmath29 .",
    "smoothness of the transition maps @xmath30 ensures that local coordinates of a point transform differentiably under a change of chart .    assuming that @xmath13 is compact , we can always choose the atlas @xmath23 in such a way that the indexing set @xmath31 is finite and each @xmath32 is an open ball of radius @xmath33 ( * ? ? ?",
    "* thm .  3.3 ) ( one can always set @xmath34 for all @xmath35 , but we choose not to do this for greater flexibility in modeling ) .",
    "the next notion we need is that of a _ tangent space _ to @xmath13 at point @xmath27 , denoted by @xmath36 .",
    "let @xmath37 be an open interval such that @xmath38 .",
    "consider the set of all curves @xmath39 such that @xmath40 .",
    "then for any chart @xmath25 we have a function @xmath41 , such that @xmath42 for all @xmath43 in a sufficiently small neighborhood of @xmath44 .",
    "we say that two such curves @xmath45 are equivalent iff @xmath46 , @xmath47 , for all @xmath35 such that @xmath48 , where @xmath49 are the components of @xmath50 .",
    "the resulting set of equivalence classes has the structure of a vector space of dimension @xmath4 , and is precisely the tangent space @xmath36 .",
    "intuitively , @xmath36 allows us to `` linearize '' @xmath13 around @xmath27 .",
    "note that , although all the tangent spaces @xmath51 are isomorphic to each other and to @xmath2 , there is no meaningful way to add elements of @xmath36 and @xmath52 with @xmath53 distinct .",
    "next , we specify the class of stochastic embeddings dealt with in this paper .",
    "consider three random variables @xmath54 , where @xmath55 takes values in the finite set @xmath31 with @xmath56 , @xmath57 takes values in @xmath2 , and @xmath58 takes values in @xmath3 .",
    "conditional distributions of @xmath57 given @xmath55 and of @xmath58 given @xmath59 are assumed to be absolutely continuous and described by densities @xmath60 and @xmath61 , respectively .",
    "since for a compact @xmath13 the images @xmath32 of charts in @xmath23 are open balls of radii @xmath33 , let us suppose that the conditional mean @xmath62 $ ] is the center of @xmath32 [ we can therefore take @xmath63 for all @xmath64 and that the largest eigenvalue of the conditional covariance matrix @xmath65 $ ] of @xmath57 given @xmath66 is equal to @xmath67 .",
    "it is convenient to think of the eigenvectors @xmath68 of @xmath69 as giving a basis of the tangent space @xmath70 .",
    "the unconditional density @xmath71 of @xmath58 is the finite mixture @xmath72 , where @xmath73 .",
    "the resulting pdf follows the local structure of the manifold @xmath13 and accounts both for low- and high - dimensional noise .    as an example @xcite ,",
    "let all @xmath74 be @xmath4-dimensional zero - mean gaussians with unit covariance matrices , @xmath75 , and @xmath76 , @xmath77 , for some means @xmath78 , covariance matrices @xmath79 , and @xmath80 matrices @xmath81 , so that @xmath82 .",
    "consider a random vector @xmath83 with an absolutely continuous distribution @xmath84 , described by a pdf @xmath85 .",
    "we wish to find a mixture model that would not only yield a good `` local '' approximation to @xmath85 , but also have low complexity , where the precise notion of complexity depends on application .    in order to set this up quantitatively",
    ", we use a complexity - regularized adaptation of the quantizer mismatch approach of gray and linder @xcite .",
    "we seek a finite collection @xmath86 of pdf s from a class @xmath87 of `` admissible '' models and a measurable partition @xmath88 of @xmath3 that would minimize the objective function@xmath89 , \\label{eq : ibar}%\\vspace{-10pt}\\ ] ] where @xmath90 is the pdf defined as @xmath91 , @xmath92 is the relative entropy , @xmath93 is a regularization functional that quantifies the complexity of the @xmath94th model pdf relative to the entire collection @xmath95 , and @xmath96 is the parameter that controls the trade - off between the relative - entropy ( mismatch ) term and the complexity term .    this minimization problem can be posed as a _ complexity - constrained quantization problem _ with an encoder @xmath97 corresponding to the partition @xmath98 through @xmath99 if @xmath100 , a decoder @xmath101 defined by @xmath102 , and a length function @xmath103 satisfying the kraft inequality @xmath104 . in order to describe the encoder and to quantify the performance of the quantization scheme , we need to choose a distortion measure between an input vector and an encoder output in such a way that minimizing average distortion would yield the @xmath105-functional ( [ eq : ibar ] ) of the corresponding partition and codebook .",
    "consider the distortion @xmath106 ( this is not a distortion measure in the strict sense since it can be negative , but its expectation with respect to @xmath85 is nonnegative by the divergence inequality ) . for a given codebook @xmath95 and length function @xmath107 , the optimal encoder is the minimum - distortion encoder @xmath108 with ties broken arbitrarily .",
    "the resulting partition @xmath109 yields the average distortion @xmath110,\\end{aligned}\\ ] ] where @xmath111",
    ". then @xmath112\\\\ & & \\qquad \\ge   \\sum_{m \\in \\cm}p_m\\big[d(f_m\\|g_m ) + \\mu\\phi_\\gamma(g_m)\\big],\\end{aligned}\\ ] ] with equality if and only if @xmath113 .",
    "thus , the optimal decoder and length function for a given partition are such that the average @xmath114-distortion is precisely the @xmath105-functional",
    ". we can therefore iterate the optimality properties of the encoder , decoder and length function in a lloyd - type descent algorithm ; this can only decrease average distortion and thus the @xmath105-functional .",
    "note that the @xmath115 term in @xmath116 does not affect the minimum - distortion encoder .",
    "thus , as far as the encoder is concerned , the distortion measure @xmath117 is equivalent to @xmath114 .",
    "when the distribution of @xmath58 is unknown , we can take a sufficiently large training sample @xmath118 and use a lloyd descent algorithm to empirically design a mixture model for the data :    \\1 ) * initialization : * begin with an initial codebook @xmath119 , where @xmath87 is the class of admissible models , and a length function @xmath120 .",
    "set iteration number @xmath121 , pick a convergence threshold @xmath122 , and let @xmath123 be the average @xmath124-distortion of the initial codebook .",
    "\\2 ) * minimum - distortion encoder : * encode each sample @xmath6 into the index @xmath125 .",
    "\\3 ) * centroid decoder : * update the codebook by minimizing over all @xmath126 the empirical conditional expectation @xmath127 \\equiv \\frac{1}{n^{(r)}_m } \\sum_{i : \\alpha^{(r)}(x_i ) = m } \\rho_0(x_i , g),\\ ] ] where @xmath128 , i.e. , set @xmath129 $ ] .",
    "\\4 ) * optimal length function : * if @xmath130 , let @xmath131 , where @xmath132 . if @xmath133 , remove the corresponding cell from the code and decrease @xmath134 by 1 .",
    "\\5 ) * test : * compute the average @xmath114-distortion @xmath135 with the code @xmath136 . if @xmath137 , quit .",
    "otherwise , go to step 2 and continue .    with a judicious choice of the initial codebook and length function , this algorithm yields a finite mixture model @xmath138 as a good `` fit '' to the empirical distribution of the data in the sense of near - optimal trade - off between the local mismatch and complexity .",
    "given a training sample @xmath139 of `` raw '' @xmath0-dimensional data and assuming its intrinsic dimension @xmath1 is known , our goal is to determine two mappings , @xmath140 and @xmath141 , where @xmath142 maps high - dimensional vectors to their dimension - reduced versions and @xmath143 maps back to the high - dimensional space .",
    "in general , the dimension - reducing map @xmath142 entails loss of information , so @xmath144 . therefore we will be interested in the average distortion incurred by our scheme , @xmath145 $ ] , where @xmath146 is a suitable distortion measure on pairs of @xmath0-vectors , e.g. , the squared euclidean distance , and the expectation is w.r.t . the empirical distribution of the sample .",
    "the first step is to use the above quantization scheme to fit a complexity - regularized gaussian mixture model to the training sample",
    ". our class @xmath87 of admissible model pdf s will be the set of all @xmath0-dimensional gaussians with nonsingular covariance matrices , @xmath147 , and for each finite set @xmath148 we shall define a regularization functional @xmath149 that penalizes those @xmath150 that are `` geometrically complex '' relative to the rest of @xmath95 .",
    "the idea of `` geometric complexity '' can be motivated @xcite by the example of the gaussian mixture model from sect .",
    "[ sec : manifolds ] .",
    "the covariance matrix of the @xmath151th component , @xmath152 , is invariant under the mapping @xmath153 , where @xmath154 is a @xmath155 orthogonal matrix , i.e. , @xmath156 . in geometric terms , a copy of the orthogonal group @xmath157 associated with the @xmath151th component of the mixture is the group of rotations and reflections in the tangent space to @xmath13 at @xmath158 .",
    "thus , the log - likelihood term in @xmath124 is not affected by assigning arbitrary and independent orientations to the tangent spaces associated with the components of the mixture . however , since our goal is to model the intrinsic _ global _ geometry of the data , it should be possible to smoothly glue together the local data provided by our model .",
    "we therefore require that the orientations of the tangent spaces at `` nearby '' points change smoothly as well .",
    "( in fact , one has to impose certain continuity requirements on the orientation of the tangent spaces in order to define measure and integration on the manifold ( * ? ? ?",
    "xi ) . )",
    "given a finite set @xmath159 , we shall define the regularization functional @xmath149 as @xmath160 where @xmath161 is a smooth positive symmetric kernel such that @xmath162 as @xmath163 , and @xmath164 is the relative entropy between two gaussians .",
    "possible choices for the kernel @xmath165 are the inverse euclidean distance @xmath166 @xcite , a gaussian kernel @xmath167 for a suitable value of @xmath168 @xcite or a compactly supported `` bump '' @xmath169 , where @xmath170 is an infinitely differentiable reflection - symmetric function that is identically zero everywhere outside a closed ball of radius @xmath171 and one everywhere inside an open ball of radius @xmath172 .",
    "the relative entropy serves as a measure of position and orientation alignment of the tangent spaces , while the smoothing kernel ensures that more weight is assigned to `` nearby '' components .",
    "this complexity functional is a generalization of the `` global coordination '' prior of brand @xcite to mixtures with unequal component weights .    with these definitions of @xmath87 and @xmath173",
    ", the @xmath124-distortion for a codebook @xmath174 and a length function @xmath107 is @xmath175 where we have also removed the @xmath176 term as it does not affect the encoder .",
    "the effect of the geometric complexity term is to curve the boundaries of the partition cells according to locally interpolated `` nonlocal information '' about the rest of the codebook . determining the lloyd centroids for the decoder will involve solving @xmath134 simultaneous nonlinear equations for the means and the same number of equations for the covariance matrices . for computational efficiency we can use the kernel data from the previous iteration , which would sacrifice optimality but avoid nonlinear equations .",
    "the output of the previous step is a gauss mixture model @xmath177 and a partition @xmath98 of @xmath3 .",
    "suppose that for each @xmath178 the eigenvectors @xmath179 of @xmath180 are numbered in the order of decreasing eigenvalues , @xmath181 .",
    "the next step is to design the dimension - reducing map @xmath142 and the reconstruction map @xmath143 .",
    "one method , proposed by brand @xcite , is to use the mixture model of the underlying pdf [ obtained in his case by an em algorithm with a prior corresponding to the average of the complexity @xmath182 over the entire codebook and with equiprobable components of the mixture ] to construct a mixture of local affine transforms , preceded by local karhunen-  transforms , as a solution to a weighted least - squares problem .",
    "however , we can use the encoder partition @xmath183 directly : for each @xmath184 , let @xmath185 , where @xmath186 is the projection onto the first @xmath4 eigenvectors of @xmath180 , and then define @xmath187 .",
    "this approach is similar to local principal component analysis of kambhatla and leen @xcite , except that their quantizer was not complexity - regularized and therefore the shape of the resulting voronoi regions was determined only by local statistical data .",
    "we can describe the operation of dimension reduction ( feature extraction ) as an encoder @xmath188 , so that @xmath189 , where @xmath190 is the minimum - distortion encoder for the @xmath124-distortion .",
    "the corresponding reconstruction operation can be designed as a decoder @xmath191 which receives a pair @xmath192 , @xmath193 , and computes @xmath194 , where @xmath195 denotes the usual scalar product in @xmath2 .",
    "this encoder - decoder pair is a composite karhunen-  transform coder matched to the mixture source @xmath196 .",
    "if the data alphabet @xmath197 is compact , then the squared - error distortion is bounded by some @xmath198 , and the mismatch due to using this composite coder on the disjoint mixture source @xmath199 can be bounded from above by @xmath200 , where @xmath201 is the @xmath202 norm . provided that the mixture @xmath203 is optimal for @xmath85 in the sense of minimizing the @xmath114-distortion , we can use pinsker s inequality ( * ? ? ?",
    "* ch .  5 ) @xmath204 and convexity of the relative entropy to further bound the mismatch by @xmath205 .",
    "note that the maps @xmath142 and @xmath143 are not smooth , unlike the analogous maps of brand @xcite .",
    "this is an artifact of the hard partitioning used in our scheme .",
    "however , hard partitioning has certain advantages : it allows for use of composite codes @xcite and nonlinear interpolative vector quantization @xcite if additional compression of dimension - reduced data is required .",
    "moreover , the lack of smoothness is not a problem in our case because we can use kernel interpolation techniques to model the geometry of dimension - reduced data by a smooth manifold , as explained next .",
    "our use of mixture models has been motivated by certain assumptions about the structure of stochastic embeddings of low - dimensional manifolds into high - dimensional spaces .",
    "in particular , given an @xmath0-dimensional gaussian mixture model @xmath206 , we can associate to each component of the mixture a chart of the underlying manifold , such that the image of the chart in @xmath2 is an open ball of radius @xmath207 centered at the origin , and we can take the first @xmath4 eigenvectors of the covariance matrix of @xmath208 as coordinate axes in the tangent space to the manifold at the inverse image of @xmath209 under the @xmath94th chart . owing to geometric complexity regularization ,",
    "the orientations of tangent spaces change smoothly as a function of position .",
    "ideally , one would like to construct a smooth manifold consistent with the given descriptions of charts and tangent spaces .",
    "however , this is a fairly difficult task since we not only have to define a smooth coordinate map @xmath210 for each chart , but also make sure that these maps satisfy the chart compatibility condition .",
    "instead , we can construct the manifold _",
    "implicitly _ by gluing the coordinate frames of the tangent spaces into an object having a smooth inner product .    specifically , let us fix a sufficiently small @xmath211 , and let @xmath212 be an infinitely differentiable function that is identically zero everywhere outside a closed ball of radius @xmath213 and one everywhere inside an open ball of radius @xmath214 , with both balls centered at @xmath215 .",
    "let @xmath216 .",
    "the inner product of two vectors @xmath217 , treated as elements of the tangent space @xmath218 , is given by @xmath219 .",
    "then for each @xmath220 the map @xmath221 , @xmath222 is a symmetric form , which is positive definite whenever @xmath223 for at least one value of @xmath94 . in addition",
    ", the map @xmath224 is smooth . in this way",
    ", we have implicitly defined a _",
    "riemannian metric _",
    "vii ) on the underlying manifold .",
    "the functions @xmath225 form a so - called _ smooth partition of unity _ , which is the only known way of gluing together local geometric data to form smooth objects ( * ? ? ?",
    "ii ) .    in geometric terms , @xmath226 for all @xmath94 if and only if @xmath220 is an image under the dimension - reduction map of a point in @xmath3 whose first @xmath4 principal components w.r.t . each gaussian in the mixture model",
    "fall outside the covariance ellipsoid of that gaussian .",
    "if the mixture model is close to optimum , this will happen with negligible probability .",
    "a practical advantage of this feature of our scheme is in rendering it robust to outliers .",
    "our mixture modeling scheme can also be used to estimate the `` true '' but unknown pdf @xmath8 of the high - dimensional data , if we assume that @xmath8 belongs to some fixed class @xmath227 .",
    "indeed , the empirically designed codebook @xmath174 of gaussian pdf s , the corresponding component weights @xmath228 , and the mixture @xmath229 are random variables since they depend on the training sample @xmath230 .",
    "we are interested in the quality of approximation of @xmath8 by the mixture @xmath231 .    following moulin and liu @xcite",
    ", we use the relative - entropy loss function @xmath232",
    ". we shall give an upper bound on the loss in terms of the _ index of resolvability _",
    "@xcite @xmath233,\\ ] ] where @xmath234 , which quantifies how well @xmath8 can be approximated , in the relative - entropy sense ( and , by pinsker s inequality , in @xmath202 sense ) , by a gaussian of moderate geometric complexity relative to the rest of the codebook .",
    "we have the following result :    let the codebook @xmath174 of gaussian pdf s be such that the log - likelihood ratios @xmath235 uniformly satisfy the _",
    "bernstein moment condition _",
    "@xcite , i.e. , there exists some @xmath236 such that @xmath237 for all @xmath238",
    ". let @xmath239 be the smallest number such that @xmath240 for all @xmath241 ( owing to the bernstein condition , it is nonnegative and finite ) . then , for any @xmath242 and @xmath211 , @xmath243 where @xmath244 .",
    "the expected loss satisfies @xmath245 \\le \\frac{1+\\alpha}{1-\\alpha}r_{\\mu , n}(f^ * ) + \\frac{4\\abs{\\cm}\\mu}{(1-\\alpha)n}. \\label{eq : lossbound2}\\ ] ] the probabilities and expectations are all w.r.t .",
    "the pdf @xmath8 .",
    "due to the fact that @xmath246 for all @xmath241 , the composite complexity @xmath247 satisfies the kraft inequality .",
    "then we can use a strategy similar to that of moulin and liu @xcite to prove that @xmath248 for each @xmath178 .",
    "hence , by the union bound @xmath249 for all @xmath178 , except for an event of probability at most @xmath250 . by convexity of the relative entropy ,",
    "@xmath251 for all @xmath178 implies that @xmath252 for @xmath253 .",
    "therefore @xmath254 with probability at least @xmath255 . to prove ( [ eq : lossbound1 ] ) , we use the fact @xcite that if @xmath256 is a random variable with @xmath257 , then @xmath258 \\le \\int^\\infty_0 \\pr[z\\ge t]dt$ ] .",
    "we let @xmath259 and choose @xmath260 .",
    "then @xmath258 \\le \\frac{4\\abs{\\cm}\\mu}{(1-\\alpha)n}$ ] , which proves ( [ eq : lossbound2 ] ) .    to discuss consistency in the large - sample limit ,",
    "consider a sequence of empirically designed mixture models @xmath261 .",
    "this is different from the usual empirical quantizer design , where we increase the training set size but keep the number of quantizer levels fixed .",
    "the scheme is consistent in the relative - entropy sense if @xmath262 as @xmath263 , where @xmath264 and the expectation is with respect to @xmath8 .",
    "a sufficient condition for consistency can be determined by inspection of the upper bound in eq .",
    "( [ eq : lossbound2 ] ) .",
    "specifically , we require that the codebooks @xmath265 satisfy : ( a ) @xmath266 , ( b ) @xmath267 for all @xmath268 , and ( c ) @xmath269",
    ". condition ( c ) can be satisfied by initializing the lloyd algorithm by a codebook of size much smaller than the training set size @xmath270 , which is usually done in practice in order to ensure good training performance .",
    "the first two conditions can also be easily met in many practical settings .",
    "consider , for instance , the class @xmath227 of all pdf s supported on a compact @xmath271 and lipschitz - continuous with lipschitz constant @xmath272 . then",
    ", if we take as our class of admissible gaussians @xmath273 for suitably chosen constants @xmath274 independent of @xmath270 , the relative entropy @xmath275 of any two @xmath276 can be bounded independently of @xmath270 , and condition ( a ) will be met with proper choice of the component weights .",
    "condition ( b ) is likewise easy to meet since the maximum value of any @xmath268 depends only on the set @xmath197 , the lipschitz constant @xmath272 , and the dimension @xmath0 .    in general , the issue of optimal codebook design is closely related to the problem of universal vector quantization @xcite : we can consider , e.g. , a class @xmath227 of pdf s with disjoint supports contained in a compact @xmath271 . then a sequence of gaussian codebooks that yields a consistent estimate of each @xmath268 in the large - sample limit is weakly minimax universal @xcite for @xmath227 and can also be used to quantize any source contained in the @xmath202-closed convex hull of @xmath227 .",
    "we have introduced a complexity - regularized quantization approach to nldr .",
    "one advantage of this scheme over existing methods for nldr based on gaussian mixtures , e.g. , @xcite , is that , instead of fitting a gauss mixture to the entire sample , we design a codebook of gaussians that provides a good trade - off between local adaptation to the data and global geometric coherence , which is key to robust geometric modeling .",
    "complexity regularization is based on a kernel smoothing technique that allows for a meaningful geometric description of dimension - reduced data by means of a riemannian metric and is also robust to outliers .",
    "moreover , to our knowledge , the consistency proof presented here is the first theoretical asymptotic consistency result applied to nldr .",
    "work is currently underway to implement the proposed scheme for applications to image processing and computer vision .",
    "also planned is future work on a quantization - based approach to estimating the intrinsic dimension of the data and on assessing asymptotic _ geometric _ consistency of our scheme in terms of the gromov - hausdorff distance between compact metric spaces @xcite .",
    "e.  levina and p.  bickel , `` maximum likelihood estimation of intrinsic dimension , '' in _ adv .",
    "neural inform . processing systems _ , l.  saul , y.  weiss , and l.  bottou , eds .",
    "17.1em plus 0.5em minus 0.4emcambridge , ma : mit press , 2005 .",
    "s.  roweis , l.  saul , and g.  hinton , `` global coordination of locally linear models , '' in _ adv .",
    "neural inform .",
    "processing systems _ , t.  dietterich , s.  becker , and z.  ghahramani , eds . ,",
    "14.1em plus 0.5em minus 0.4emcambridge , ma : mit press , 2002 , pp .",
    "889896 .",
    "m.  brand , `` charting a manifold , '' in _ adv .",
    "neural inform . processing systems _ ,",
    "s.  becker , s.  thrun , and k.  obermayer , eds .",
    "15.1em plus 0.5em minus 0.4emcambridge , ma : mit press , 2003 , pp ."
  ],
  "abstract_text": [
    "<S> we consider the problem of nonlinear dimensionality reduction : given a training set of high - dimensional data whose `` intrinsic '' low dimension is assumed known , find a feature extraction map to low - dimensional space , a reconstruction map back to high - dimensional space , and a geometric description of the dimension - reduced data as a smooth manifold . </S>",
    "<S> we introduce a complexity - regularized quantization approach for fitting a gaussian mixture model to the training set via a lloyd algorithm . </S>",
    "<S> complexity regularization controls the trade - off between adaptation to the local shape of the underlying manifold and global geometric consistency . </S>",
    "<S> the resulting mixture model is used to design the feature extraction and reconstruction maps and to define a riemannian metric on the low - dimensional data . </S>",
    "<S> we also sketch a proof of consistency of our scheme for the purposes of estimating the unknown underlying pdf of high - dimensional data . </S>"
  ]
}