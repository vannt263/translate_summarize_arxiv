{
  "article_text": [
    "representing words by dense , real - valued vector embeddings , also commonly called `` distributed representations , '' helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles .",
    "this has been shown dramatically in state - of - the - art results on language modeling @xcite as well as improvements in other natural language processing tasks @xcite .",
    "substantial benefit arises when embeddings can be trained on large volumes of data .",
    "hence the recent considerable interest in the cbow and skip - gram models of ; relatively simple log - linear models that can be trained to produce high - quality word embeddings on the entirety of english wikipedia text in less than half a day on one machine .",
    "there is rising enthusiasm for applying these models to improve accuracy in natural language processing , much like brown clusters @xcite have become common input features for many tasks , such as named entity extraction @xcite and parsing @xcite . in comparison to brown clusters , the vector embeddings have the advantages of substantially better scalability in their training , and intriguing potential for their continuous and multi - dimensional interrelations .",
    "in fact , present new state - of - the - art results in conll 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of skip - gram that injects supervision with lexicons .",
    "similarly show results in dependency parsing using skip - gram embeddings .",
    "they have also recently been applied to machine translation @xcite .",
    "a notable deficiency in this prior work is that each word type ( _ e.g. _ the word string plant ) has only one vector representation  polysemy and hononymy are ignored .",
    "this results in the word plant having an embedding that is approximately the average of its different contextual semantics relating to biology , placement , manufacturing and power generation . in moderately high - dimensional spaces a vector can be relatively `` close '' to multiple regions at a time , but this does not negate the unfortunate influence of the triangle inequality , @xmath0 . ] here : words that are not synonyms but are synonymous with different senses of the same word will be pulled together .",
    "for example , pollen and refinery will be inappropriately pulled to a distance not more than the sum of the distances plant  pollen and plant  refinery .",
    "fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities .",
    "discovering embeddings for multiple senses per word type is the focus of work by and .",
    "they both pre - cluster the contexts of a word type s tokens into discriminated senses , use the clusters to re - label the corpus tokens according to sense , and then learn embeddings for these re - labeled words .",
    "the second paper improves upon the first by employing an earlier pass of non - discriminated embedding learning to obtain vectors used to represent the contexts .",
    "note that by pre - clustering , these methods lose the opportunity to jointly learn the sense - discriminated vectors and the clustering .",
    "other weaknesses include their fixed number of sense per word type , and the computational expense of the two - step process ",
    "the method took one week of computation to learn multiple embeddings for a 6,000 subset of the 100,000 vocabulary on a corpus containing close to billion tokens .",
    "this paper presents a new method for learning vector - space embeddings for multiple senses per word type , designed to provide several advantages over previous approaches .",
    "( 1 ) sense - discriminated vectors are learned jointly with the assignment of token contexts to senses ; thus we can use the emerging sense representation to more accurately perform the clustering .",
    "( 2 ) a non - parametric variant of our method automatically discovers a varying number of senses per word type .",
    "( 3 ) efficient online joint training makes it fast and scalable .",
    "we refer to our method as _ multiple - sense skip - gram _ , or _ mssg _ , and its non - parametric counterpart as _ np - mssg_.    our method builds on the skip - gram model @xcite , but maintains multiple vectors per word type . during online training with a particular token , we use the average of its context words vectors to select the token s sense that is closest , and perform a gradient update on that sense . in the non - parametric version of our method , we build on _ facility location _",
    "@xcite : a new cluster is created with probability proportional to the distance from the context to the nearest sense .",
    "we present experimental results demonstrating the benefits of our approach .",
    "we show qualitative improvements over single - sense skip - gram and , comparing against word neighbors from our parametric and non - parametric methods .",
    "we present quantitative results in three tasks .",
    "on both the scws and wordsim353 data sets our methods surpass the previous state - of - the - art .",
    "the google analogy task is not especially well - suited for word - sense evaluation since its lack of context makes selecting the sense difficult ; however our method dramatically outperforms on this task .",
    "finally we also demonstrate scalabilty , learning multiple senses , training on nearly a billion tokens in less than 6 hours  a 27x improvement on huang et al .",
    "much prior work has focused on learning vector representations of words ; here we will describe only those most relevant to understanding this paper .",
    "our work is based on neural language models , proposed by , which extend the traditional idea of @xmath1-gram language models by replacing the conditional probability table with a neural network , representing each word token by a small vector instead of an indicator variable , and estimating the parameters of the neural network and these vectors jointly .",
    "since the model is quite expensive to train , much research has focused on optimizing it .",
    "replaces the max - likelihood character of the model with a max - margin approach , where the network is encouraged to score the correct @xmath1-grams higher than randomly chosen incorrect @xmath1-grams . replaces the global normalization of the bengio model with a tree - structured probability distribution , and also considers multiple positions for each word in the tree .",
    "more relevantly , and propose extremely computationally efficient log - linear neural language models by removing the hidden layers of the neural networks and training from larger context windows with very aggressive subsampling .",
    "the goal of the models in and is not so much obtaining a low - perplexity language model as learning word representations which will be useful in downstream tasks .",
    "neural networks or log - linear models also do not appear to be necessary to learn high - quality word embeddings , as estimate word vector representations using canonical correlation analysis ( cca ) .    word vector representations or embeddings have been used in various nlp tasks such as named entity recognition @xcite , dependency parsing @xcite , chunking @xcite , sentiment analysis @xcite , paraphrase detection @xcite and learning representations of paragraphs and documents @xcite .",
    "the word clusters obtained from brown clustering @xcite have similarly been used as features in named entity recognition @xcite and dependency parsing @xcite , among other tasks .",
    "there is considerably less prior work on learning multiple vector representations for the same word type .",
    "introduce a method for constructing multiple sparse , high - dimensional vector representations of words .",
    "extends this approach incorporating global document context to learn multiple dense , low - dimensional embeddings by using recursive neural networks .",
    "both the methods perform word sense discrimination as a pre - processing step by clustering contexts for each word type , making training more expensive .",
    "while methods such as those described in and use token - specific representations of words as part of the learning algorithm , the final outputs are still one - to - one mappings between word types and word embeddings .",
    "the skip - gram model learns word embeddings such that they are useful in predicting the surrounding words in a sentence . in the skip - gram model",
    ", @xmath2 is the vector representation of the word @xmath3 , where @xmath4 is the words vocabulary and @xmath5 is the embedding dimensionality .    given a pair of words @xmath6 , the probability that the word @xmath7 is observed in the context of word @xmath8 is given by , @xmath9 the probability of not observing word @xmath7 in the context of @xmath8 is given by , @xmath10    given a training set containing the sequence of word types @xmath11 , the word embeddings are learned by maximizing the following objective function : @xmath12 where @xmath13 is the @xmath14 word in the training set , @xmath15 is the set of observed context words of word @xmath13 and @xmath16 is the set of randomly sampled , noisy context words for the word @xmath13 .",
    "@xmath17 consists of the set of all observed word - context pairs @xmath18 ( @xmath19 ) .",
    "@xmath20 consists of pairs @xmath21 ( @xmath19 ) where @xmath16 is the set of randomly sampled , noisy context words for the word @xmath13 .",
    "for each training word @xmath8 , the set of context words @xmath22 includes @xmath23 words to the left and right of the given word as shown in figure [ fig1 ] .",
    "@xmath23 is the window size considered for the word @xmath8 uniformly randomly sampled from the set @xmath24 , where n is the maximum context window size .",
    "the set of noisy context words @xmath16 for the word @xmath13 is constructed by randomly sampling @xmath25 noisy context words for each word in the context @xmath26 .",
    "the noisy context words are randomly sampled from the following distribution , @xmath27 where @xmath28 is the unigram distribution of the words and @xmath29 is the normalization constant .    .",
    "context @xmath26 of word @xmath8 consists of @xmath30 . ]",
    "to extend the skip - gram model to learn multiple embeddings per word we follow previous work @xcite and let each sense of word have its own embedding , and induce the senses by clustering the embeddings of the context words around each token .",
    "the vector representation of the context is the average of its context words vectors . for every word type , we maintain clusters of its contexts and the sense of a word token is predicted as the cluster that is closest to its context representation . after predicting the sense of a word token",
    ", we perform a gradient update on the embedding of that sense .",
    "the crucial difference from previous approaches is that word sense discrimination and learning embeddings are performed jointly by predicting the sense of the word using the current parameter estimates .     and",
    "context @xmath26 of word @xmath8 consists of @xmath30 .",
    "the sense is predicted by finding the cluster center of the context that is closest to the average of the context vectors . ]    in the mssg model , each word @xmath3 is associated with a global vector @xmath32 and each sense of the word has an embedding ( sense vector ) @xmath33 ( @xmath34 ) and a context cluster with center @xmath35 ( @xmath34 ) .",
    "the @xmath36 sense vectors and the global vectors are of dimension @xmath5 and @xmath36 is a hyperparameter .    consider the word @xmath8 and let @xmath37 be the set of observed context words .",
    "the vector representation of the context is defined as the average of the global vector representation of the words in the context .",
    "let @xmath38 be the vector representation of the context @xmath26 .",
    "we use the global vectors of the context words instead of its sense vectors to avoid the computational complexity associated with predicting the sense of the context words .",
    "we predict @xmath39 , the sense of word @xmath13 when observed with context @xmath15 as the context cluster membership of the vector @xmath40 as shown in figure [ fig2 ] .",
    "more formally , @xmath41 the hard cluster assignment is similar to the @xmath42-means algorithm .",
    "the cluster center is the average of the vector representations of all the contexts which belong to that cluster .",
    "for @xmath43 we use cosine similarity in our experiments .    here , the probability that the word @xmath7 is observed in the context of word @xmath8 given the sense of the word @xmath13 is , @xmath44 the probability of not observing word @xmath7 in the context of @xmath8 given the sense of the word @xmath13 is , @xmath45    given a training set containing the sequence of word types @xmath46 , the word embeddings are learned by maximizing the following objective function : @xmath47 where @xmath8 is the @xmath14 word in the sequence , @xmath26 is the set of observed context words and @xmath48 is the set of noisy context words for the word @xmath8 .",
    "@xmath17 and @xmath20 are constructed in the same way as in the skip - gram model .    after predicting the sense of word @xmath8",
    ", we update the embedding of the predicted sense for the word @xmath8 ( @xmath49 ) , the global vector of the words in the context and the global vector of the randomly sampled , noisy context words .",
    "the context cluster center of cluster @xmath50 for the word @xmath8 ( @xmath51 ) is updated since context @xmath26 is added to the cluster @xmath50 .",
    "input : @xmath46 , @xmath5 , @xmath36 , @xmath52 .",
    "initialize @xmath33 and @xmath32 , @xmath53 randomly , @xmath54 @xmath53 to 0 .",
    "@xmath55 @xmath56 @xmath38 @xmath57 \\ {    @xmath58 }    update context cluster center @xmath51 since context @xmath26 is added to context cluster @xmath50 of word @xmath8 .",
    "@xmath59 gradient update on @xmath49 , global vectors of words in @xmath26 and @xmath48 .",
    "output : @xmath33 , @xmath32 and context cluster centers @xmath35 , @xmath53",
    "the mssg model learns a fixed number of senses per word type . in this section , we describe a non - parametric version of mssg , the np - mssg model , which learns varying number of senses per word type .",
    "our approach is closely related to the online non - parametric clustering procedure described in .",
    "we create a new cluster ( sense ) for a word type with probability proportional to the distance of its context to the nearest cluster ( sense ) .",
    "each word @xmath3 is associated with sense vectors , context clusters and a global vector @xmath32 as in the mssg model .",
    "the number of senses for a word is unknown and is learned during training .",
    "initially , the words do not have sense vectors and context clusters .",
    "we create the first sense vector and context cluster for each word on its first occurrence in the training data . after creating the first context cluster for a word , a new context cluster and a sense vector",
    "are created online during training when the word is observed with a context were the similarity between the vector representation of the context with every existing cluster center of the word is less than @xmath60 , where @xmath60 is a hyperparameter of the model .",
    "consider the word @xmath8 and let @xmath37 be the set of observed context words .",
    "the vector representation of the context is defined as the average of the global vector representation of the words in the context .",
    "let @xmath38 be the vector representation of the context @xmath26 .",
    "let @xmath61 be the number of context clusters or the number of senses currently associated with word @xmath8 .",
    "@xmath50 , the sense of word @xmath8 when @xmath62 is given by @xmath63 where @xmath64 is the cluster center of the @xmath65 cluster of word @xmath8 and @xmath66 .",
    "the cluster center is the average of the vector representations of all the contexts which belong to that cluster . if @xmath67 , a new context cluster and a new sense vector are created for the word @xmath8 .",
    "the np - mssg model and the mssg model described previously differ only in the way word sense discrimination is performed .",
    "the objective function and the probabilistic model associated with observing a ( word , context ) pair given the sense of the word remain the same .",
    "to evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in , which is the april 2010 snapshot of the wikipedia corpus @xcite .",
    "it contains approximately 2 million articles and 990 million tokens . in all our experiments we remove all the words with less than 20 occurrences and",
    "use a maximum context window ( @xmath52 ) of length 5 ( 5 words before and after the word occurrence ) .",
    "we fix the number of senses ( @xmath36 ) to be 3 for the mssg model unless otherwise specified .",
    "our hyperparameter values were selected by a small amount of manual exploration on a validation set . in np - mssg",
    "we set @xmath60 to -0.5 . the skip - gram model , mssg and np - mssg models sample one noisy context word ( @xmath25 ) for each of the observed context words .",
    "we train our models using adagrad stochastic gradient decent @xcite with initial learning rate set to 0.025 . similarly to , we do nt use a regularization penalty .",
    "below we describe qualitative results , displaying the embeddings and the nearest neighbors of each word sense , and quantitative experiments in two benchmark word similarity tasks .    .training time results .",
    "first five model reported in the table are capable of learning multiple embeddings for each word and skip - gram is capable of learning only single embedding for each word . [",
    "cols=\"<,>\",options=\"header \" , ]     figure 3 shows the distribution of number of senses learned per word type in the np - mssg model .",
    "we learn the multiple embeddings for the same set of approximately 6000 words that were used in for all our experiments to ensure fair comparision .",
    "these approximately 6000 words were choosen by huang et al .  mainly from the top 30,00 frequent words in the vocabulary .",
    "this selection was likely made to avoid the noise of learning multiple senses for infrequent words .",
    "however , our method is robust to noise , which can be seen by the good performance of our model that learns multiple embeddings for the top 30,000 most frequent words .",
    "we found that even by learning multiple embeddings for the top 30,000 most frequent words in the vocubulary , mssg model still achieves state - of - art result on scws task with an @xmath68 score of 69.2 as shown in table [ table:30k - word - sim - task ] .",
    "we present an extension to the skip - gram model that efficiently learns multiple embeddings per word type .",
    "the model jointly performs word sense discrimination and embedding learning , and non - parametrically estimates the number of senses per word type .",
    "our method achieves new state - of - the - art results in the word similarity in context task and learns multiple senses , training on close to billion tokens in less than 6 hours .",
    "the global vectors , sense vectors and cluster centers of our model and code for learning them are available at https://people.cs.umass.edu/~arvind/emnlp2014wordvectors . in future work",
    "we plan to use the multiple embeddings per word type in downstream nlp tasks .",
    "this work was supported in part by the center for intelligent information retrieval and in part by darpa under agreement number fa8750 - 13 - 2 - 0020 .",
    "government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon .",
    "any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor ."
  ],
  "abstract_text": [
    "<S> there is rising interest in vector - space word embeddings and their use in nlp , especially given recent methods for their fast estimation at very large scale </S>",
    "<S> . nearly all this work , however , assumes a single vector per word type  ignoring polysemy and thus jeopardizing their usefulness for downstream tasks . </S>",
    "<S> we present an extension to the skip - gram model that efficiently learns multiple embeddings per word type . </S>",
    "<S> it differs from recent related work by jointly performing word sense discrimination and embedding learning , by non - parametrically estimating the number of senses per word type , and by its efficiency and scalability . </S>",
    "<S> we present new state - of - the - art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours . </S>"
  ]
}