{
  "article_text": [
    "in high - dimensional regression problems where the number of potential model parameters greatly exceeds the number of training samples , the use of an @xmath0 penalty which augments standard objective functions with a term that sums the absolute effect sizes of all parameters in the model has emerged as a hugely successful and intensively studied variable selection technique , particularly for the ordinary least squares ( ols ) problem ( e.g. @xcite ) .",
    "generalised linear models ( glms ) relax the implicit ols assumption that the response variable is normally distributed and can be applied to , for instance , binomially distributed binary outcome data or poisson distributed count data @xcite .",
    "however , the most popular and efficient algorithm for @xmath0-penalised regression in glms uses a quadratic approximation to the log - likelihood function to map the problem back to an ols problem and although it works well in practice , it is not guaranteed to converge to the optimal solution @xcite .",
    "here it is shown that calculating the maximum likelihood coefficient estimates for @xmath0-penalised regression in generalised linear models can be done via a coordinate descent algorithm consisting of successive soft - thresholding operations on the _ unpenalised _ maximum log - likelihood function without requiring an intermediate ols approximation . because this algorithm can be expressed entirely in terms of the natural formulation of the glm , it is proposed to call it the _ natural coordinate descent algorithm_.    to make these statements precise , let us start by introducing a response variable @xmath1 and predictor vector @xmath2 .",
    "it is assumed that @xmath3 has a probability distribution from the exponential family , written in canonical form as @xmath4 where @xmath5 is the natural parameter of the distribution , @xmath6 is a dispersion parameter and @xmath7 , @xmath8 and @xmath9 convex are known functions . the expectation value of @xmath3 is a function of the natural parameter , @xmath10 , and linked to the predictor variables by the assumption of a linear relation @xmath11 , where @xmath12 is the vector of regression coefficients .",
    "it is tacitly assumed that @xmath13 such that @xmath14 represents the intercept parameter .",
    "suppose now that we have @xmath15 observation pairs @xmath16 ( with @xmath17 fixed for all @xmath18 ) .",
    "the minus log - likelihood of the observations for a given set of regression coefficients @xmath19 under the glm is given by @xmath20 where any terms not involving @xmath19 have been omitted , @xmath21 is a convex function , @xmath22 , and the dependence of @xmath23 and @xmath24 on the data @xmath16 has been suppressed for notational simplicity . in the penalised regression",
    "setting , this cost function is augmented with @xmath0 and @xmath25 penalty terms to achieve regularity and sparsity of the minimum - energy solution , i.e. @xmath26 is replaced by @xmath27 where @xmath28 and @xmath29 are the @xmath25 and @xmath0 norm , respectively , and @xmath30 and @xmath31 are positive constants .",
    "the @xmath25 term merely adds a quadratic function to @xmath23 which serves to make its hessian matrix non - singular and it will not need to be treated explicitly in our analysis . furthermore a slight generalisation is made where instead of a fixed parameter @xmath31 , a vector of predictor - specific penalty parameters @xmath32 is used .",
    "this allows for instance to account for the usual situation where the intercept coefficient is unpenalised ( @xmath33 ) .",
    "the problem we are interested in is thus to find @xmath34 with @xmath26 a function of the form @xmath35 where @xmath36 is a smooth convex function , @xmath37 is an arbitrary vector and @xmath38 , @xmath39 is a vector of non - negative parameters . the notation @xmath40 is used to indicate that @xmath41 for all @xmath42 and likewise the notation @xmath43 will be used to indicate elementwise multiplication , i.e. @xmath44 . the maximum of the _ unpenalised _ log - likelihood , considered as a function of @xmath24 , is of course the legendre transform of the convex function @xmath23 , @xmath45 and the unpenalised regression coefficients satisfy @xmath46 where @xmath47 is the usual gradient operator ( see lemma [ lem : legendre ] in appendix [ sec : proof - theorem - app ] ) .",
    "this leads to the following key result :    [ thm : main ] the solution @xmath48 of @xmath49 is given by @xmath50 where @xmath51 is the solution of the constrained convex optimisation problem @xmath52 furthermore the sparsity patterns of @xmath53 and @xmath54 are complementary , @xmath55    the proof of this theorem consists of an application of fenchel s duality theorem and is provided in appendix [ sec : proof - theorem - app ] .",
    "two special cases of theorem [ thm : main ] merit attention .",
    "firstly , in the case of lasso or elastic net penalised linear regression , @xmath56 is a quadratic function of @xmath19 , with @xmath57 a positive definite matrix , such that @xmath58 and @xmath59 . if furthermore @xmath60 is diagonal , with diagonal elements @xmath61 , then eq . reduces to the @xmath62 independent problems @xmath63 with solution @xmath64 and @xmath65 .",
    "this is the well - known analytic solution of the lasso with uncorrelated predictors @xcite , which forms the basis for numerically solving the case of arbitrary @xmath60 as well @xcite .",
    "secondly , in the case of penalised covariance matrix estimation , @xmath66 for non - negative definite matrices @xmath67 , and @xmath68 for @xmath69 negative definite ( and @xmath70 otherwise ) @xcite",
    ". eq . is then exactly the dual problem studied by @xcite .",
    "it is well - known that a cyclic coordinate descent algorithm for the @xmath0-penalised optimisation problem in eq . converges @xcite .",
    "when only one variable is optimised at a time , keeping all others fixed , the equivalent variational problem in eq . reduces to a remarkably simple soft - thresholding mechanism illustrated in figure [ fig : thm-1d ] .",
    "more precisely , let @xmath71 be a smooth convex function of a single variable @xmath72 , @xmath73 and @xmath74 .",
    "the solution of the one - variable optimisation problem @xmath75 with @xmath76 , can be expressed as follows . if @xmath77 then @xmath78 and hence @xmath79 .",
    "otherwise we must have @xmath80 and @xmath81 .",
    "hence the solution takes the form of a generalised ` soft - thresholding ' @xmath82 see also figure [ fig : thm-1d ] . in other words , compared to the multivariate problem in theorem [ thm : main ] where there remains ambiguity about the signs @xmath83 , in the one - variable case the sign is uniquely determined by the relative position of @xmath24 and @xmath84 .     in one dimension .",
    "* a. * the unpenalised cost function @xmath85 is a convex function of @xmath19 ; the maximum - likelihood estimate @xmath86 is its unique minimiser . *",
    "b. * the maximum - likelihood estimate is also equal to the slope of the tangent to the legendre transform of @xmath23 at @xmath24 . * c. * every value of the @xmath0 penalty parameter @xmath31 leads to a different cost function ; for @xmath87 sufficiently small , the maximum - likelihood estimate @xmath88 is non - zero while for sufficiently large @xmath89 it is exactly zero .",
    "* d. * the penalised problem can also be solved by minimising the _ unpenalised _ legendre transform over the interval @xmath90 $ ] ; for @xmath91 and @xmath92 the absolute minimiser of @xmath93 is not included in this interval such that the constrained minimiser is the boundary value @xmath94 and the the maximum - likelihood estimate @xmath95 equals the slope of the tangent at @xmath94 , while for @xmath96 , the constrained minimiser is always the absolute minimiser which has a tangent with slope zero .",
    "note that because @xmath93 is convex , the slope at @xmath94 is always smaller than the slope at @xmath24 ( i.e. @xmath88 ) .",
    "similar reasoning applies when @xmath97 . ]",
    "numerically solving the unpenalised one - variable problem is usually straightforward .",
    "first note that by assumption , @xmath23 is differentiable and therefore it is itself the legendre transform of @xmath93 .",
    "hence @xmath98 likewise , and assuming there exists no analytic expression for @xmath93 , @xmath99 can be found as the zero of the function @xmath100 for @xmath23 convex , this is a monotonically increasing function of @xmath19 and conventional one - dimensional root - finding algorithms converge quickly .",
    "the @xmath62-dimensional natural coordinate descent algorithm simply consists of iteratively applying the above procedure to the one - dimensional functions @xmath101 where @xmath102 are the current coefficient estimates , i.e. @xmath103 where @xmath104 and @xmath105 .",
    "standard techniques can be used to make the algorithm more efficient by organising the calculations around the set of non - zero coefficients @xcite , that is , after every complete cycle through all coordinates , the current set of non - zero coefficients is updated until convergence before another complete cycle is run ( see pseudocode in appendix [ sec : code ] ) .",
    "an alternative method for updating @xmath106 in the preceding algorithm is to use a quadratic approximation to @xmath107 around the current estimate of @xmath106 in eq . .",
    "this leads to a linear approximation for @xmath108 , i.e. if @xmath109 , then @xmath110 this approximation differs from the standard quadratic approximation @xcite by the fact that it still uses the _ exact _ thresholding rule from . to be precise ,",
    "given current estimates @xmath53 , the standard approximation updates the @xmath111 coordinate by minimizing the approximate quadratic cost function @xmath112 \\beta_j +    \\mu_j|\\beta_j|,\\end{aligned}\\ ] ] which has the solution @xmath113 where @xmath114 .",
    "hence , compared to the exact coordinate update rule , the standard algorithm not only uses a quadratic approximation to the cost function , but also a linear approximation @xmath115    the following result shows that , under certain conditions , the approximate and exact thresholding will return the same result :    [ prop : threshold ] let @xmath71 be a smooth convex function of a single variable @xmath72 , and let @xmath53 be the solution of @xmath116 with @xmath117 and @xmath118 . denote @xmath119 and @xmath120",
    ". then @xmath121    the proof of this proposition can be found in appendix [ sec : proof - theorem - app ] .",
    "note that in the coordinate descent algorithms the single - coordinate functions @xmath23 change from step to step , that @xmath122 is calculated on the _ current _ instead of the _ new _ solution , and that , in the quadratic approximation algorithm , both the current and new solutions are only approximate minimisers .",
    "hence this result only shows that if all these errors are sufficiently small , then both thresholding rules will agree .",
    "i implemented the natural coordinate descent algorithm for logistic regression in c with a matlab interface ( source code available from http://tmichoel.github.io/glmnat/ ) . the penalised cost function for @xmath12 in this case is given by @xmath123 where @xmath124 and @xmath125 , @xmath126 are the observations .",
    "recall from section [ sec : introduction ] that @xmath14 is regarded as the ( unpenalised ) intercept parameter and therefore a fixed value of one ( @xmath17 ) is added to every observation . as convergence criterion i used @xmath127 , where @xmath128 is a fixed parameter .",
    "the difference is calculated at every iteration step when a single coefficient is updated and the maximum is taken over a full iteration after all , resp .",
    "all active , coefficients have been updated once .    to test the algorithm i used gene expression levels for 17,814 genes in 540 breast cancer samples ( brca dataset ) @xcite and 20,531 genes in 266 colon and rectal cancer samples ( coad dataset )",
    "@xcite as predictors for estrogen receptor status ( brca ) and early - late tumor stage ( coad ) , respectively ( see appendix [ sec : cancer - genome - atlas ] for details , processed data available from http://tmichoel.github.io/glmnat/ ) .",
    "i compared the implementation of the natural coordinate descent algorithm against ` glmnet ` ( version dated 30 aug 2013 ) @xcite , a fortran - based implementation for matlab of the coordinate descent algorithm for penalised regression in generalised linear models proposed by @xcite , which was found to be the most efficient in a comparison to various other softwares by the original authors @xcite as well as in an independent study @xcite .",
    "all analyses were run on a laptop with 2.7  ghz processor and 8  gb ram using matlab v8.2.0.701 ( r2013b ) .",
    "following @xcite , i considered a geometric path of regularisation parameters @xmath129 where @xmath130 , and @xmath131 , corresponding to the default choice in ` glmnet ` .",
    "note that @xmath132 is the smallest penalty that yields a solution where only the intercept parameter is non - zero .",
    "such a path of parameters evenly spaced on log - scale typically corresponds to models with a linearly increasing number of non - zero coefficients @xcite . to compare the output of two different algorithms over the entire regularisation path , i considered the maximum relative score difference @xmath133 where @xmath134 and @xmath135 are the coefficient estimates obtained by the respective algorithms for the @xmath136th penalty parameter .    a critical issue when comparing algorithm runtimes is to match convergence threshold settings .",
    "figure [ fig : comp]a shows the runtimes of the exact natural coordinate descent algorithm ( using eq . ) and its quadratic approximation ( using eq . ) , and their maximum relative score difference for a range of values of the convergence threshold @xmath137 .",
    "the quadratic approximation algorithm is about twice as fast as the exact algorithm and , as expected , both return numerically identical results within the accepted tolerance levels . for subsequent analyses only the quadratic approximation algorithm was used . because ` glmnet ` uses a different convergence criterion than the one used here , i ran the natural coordinate descent algorithm with a range of values for @xmath137 and calculated the maximum relative score difference over the entire regularisation path with respect to the output of ` glmnet ` with default settings",
    ". figure [ fig : comp]b shows that there is a dataset - dependent value for @xmath137 where this difference is minimised and that the minimum difference is within the range observed when running ` glmnet ` with randomly permuted order of predictors .",
    "these minimising values @xmath138 and @xmath139 were used for the subsequent comparisons .",
    "first , i compared the natural coordinate descent algorithms with exact and approximate thresholding rules ( cf .",
    "eqs . and ) . for both datasets and all penalty parameter values",
    ", no differences were found between the two rules during the entire course of the algorithm , indicating that the error terms discussed after proposition [ prop : threshold ] are indeed sufficiently small in practice .",
    "since there is as yet no analytical proof extending proposition [ prop : threshold ] to the algorithmic setting , the exact thresholding rule was used for all subsequent analyses .    .",
    "the inset shows the maximum relative score difference between both algorithms for the same convergence thresholds . *",
    "b. * maximum relative score difference between the natural coordinate descent algorithm and ` glmnet ` vs. convergence threshold parameter @xmath137 for the brca ( blue circles ) and coad ( red squares ) dataset .",
    "the horizontal lines indicate the minimum , mean and maximum of the relative score difference over 10 comparisons between the original ` glmnet ` result and ` glmnet ` applied to data with randomly permuted order of predictors . * c , d .",
    "* runtime in seconds of the natural coordinate descent algorithm with cold ( blue squares ) and warm ( red circles ) starts on the brca ( * c * ) and coad ( * d * ) dataset vs. index @xmath136 of the penalty parameter vector . * e , f . *",
    "runtime in seconds of ` glmnet ` with cold ( blue squares ) and warm ( red circles ) starts on the brca ( * e * ) and coad ( * f * ) dataset vs. index @xmath136 of the penalty parameter vector .",
    "see main text for details . ]",
    "next , i compared the natural coordinate descent algorithm to ` glmnet ` considering both `` cold '' and `` warm '' starts .",
    "for the cold starts , the solution for the @xmath140 penalty parameter value , @xmath141 , was calculated starting from the initial vector @xmath142 . for the warm starts ,",
    "@xmath141 was calculated along the path of penalty parameters @xmath143 , each time using @xmath144 as the initial vector for the calculation of @xmath145 .",
    "this scenario was designed to answer the question : if a solution @xmath146 is sought for some fixed value of @xmath147 , is it best to run the coordinate descent algorithm once starting from the initial vector @xmath142 ( cold start ) , or to run the coordinate descent algorithm multiple times along a regularization path , each time with an initial vector that should be close to the next solution ( warm start ) ? clearly , if a solution is needed for all values of a regularization path it is always better to run through the path once using warm starts at each step .    for ` glmnet `",
    ", there is a clear advantage to using warm starts and , as also observed by @xcite , for smaller values of @xmath31 , it can be faster to compute along a regularisation path down to @xmath31 than to compute the solution at @xmath31 directly ( figure [ fig : comp]e , f ) . in contrast",
    ", the natural coordinate descent algorithm is much less sensitive to the use of warm starts ( i.e. to the choice of initial vector for @xmath53 ) and it is considerably faster than ` glmnet ` when calculating solutions at single penalty parameter values ( figure [ fig : comp]c , d ) .    to investigate whether this qualitative difference between both algorithms is a general property ,",
    "the following process was repeated 1,000 times : a gene was randomly selected from the brca dataset , a binary response variable was defined from the sign of its expression level , and penalised logistic regression with penalty parameter @xmath148 ( cf .",
    "was performed using 5,000 randomly selected genes as predictors , using both cold start ( with initial vector @xmath149 and warm start ( along the regularization path @xmath150 ) ; the response gene was constrained to have at least @xmath151 samples of either sign and the predictor genes were constrained to not contain the response gene .",
    "this scheme ensured that datasets with sufficient variability in the correlation structure among the predictor variables and between the predictor and the response variables were represented among the test cases .",
    "as expected , total runtime correlated well with the size of the model , defined here as the number of predictors with @xmath152 , more strongly so for the natural coordinate descent algorithm ( pearson s @xmath153 , @xmath154 ) than for ` glmnet ` ( @xmath155 , @xmath156 ) .",
    "consistent with these high linear correlations the difference in speed ( runtime@xmath157 ) between cold and warm start was inversely proportional to model size ( spearman s @xmath158 ; figure [ fig : speed]a ) .",
    "furthermore , cold start outperformed warm start ( @xmath159 ) in all 1,000 datasets .",
    "for ` glmnet ` the opposite was true : warm start always outperformed cold start .",
    "however the speed difference ( @xmath160 ) did not correlate as strongly with model size ( spearman s @xmath161 ; figure [ fig : speed]b ; note that the opposite sign of the correlation coefficient is merely due to the opposite sign of the speed differences ) .",
    "this consistent qualitative difference between both algorithms with respect to the choice of initial vector was unexpected in view of the results in section [ sec : quadr - appr - with ] . upon closer inspection",
    ", it was revealed that the natural coordinate descent algorithm uses a scheme whereby the parameters of the quadratic approximation for coordinate @xmath42 ( i.e. , @xmath162 and @xmath163 ) are updated whenever there is a change in @xmath164 for some @xmath165 .",
    "in contrast , ` glmnet ` uses two separate loops , called `` middle '' and `` inner '' loop in @xcite . in the middle loop , the quadratic approximation to @xmath23 at the current solution @xmath166 is calculated , i.e. @xmath167 where @xmath168 in the inner loop , a penalised least squares coordinate descent algorithm is run until convergence using the approximation , i.e. keeping the values of @xmath169 and @xmath26 fixed",
    ". a poor choice of initial vector will therefore result in values of @xmath169 and @xmath26 that are far from optimal , and running a coordinate descent algorithm until convergence without updating these values would therefore result in a loss of efficiency .",
    "it is therefore plausible that the continuous updating of the quadratic approximation parameters in the natural coordinate descent algorithm explains its robustness with respect to the choice of initial vector .    ) vs. model size between cold start ( @xmath170 ) and warm start ( @xmath171 ) on sub - sampled datasets for the natural coordinate descent algorithm ( * a * ) , logistic regression using ` glmnet ` ( * b * ) and linear regression using ` glmnet ` ( * c * ) .",
    "see main text for details . ]",
    "if this reasoning is correct , then the warm - start advantage should not be present if ` glmnet ` is used to solve penalised least squares problems , since in this case there is no middle loop to be performed . to test this hypothesis , i performed penalised linear regression ( lasso ) with ` glmnet ` on the same sub - sampled datasets , using the same binary response variable and the same penalty parameter values as in the previous logistic regressions .",
    "although the speed difference @xmath160 between cold and warm start now indeed followed a similar pattern as the natural coordinate descent algorithm ( spearman s @xmath172 ; figure [ fig : speed]c ) , in all but 20 datasets , warm start still outperformed cold start .",
    "this suggests that not updating the quadratic approximation at every step during logistic regression in ` glmnet ` may explain in part why it is more sensitive to the choice of initial vector , but additional , undocumented optimizations of the code must be in place to explain its warm - start advantage .",
    "the popularity of @xmath0-penalised regression as a variable selection technique owes a great deal to the availability of highly efficient coordinate descent algorithms . for generalised linear models ,",
    "the best existing algorithm uses a quadratic least squares approximation where the coordinate update step can be solved analytically as a linear soft - thresholding operation .",
    "this analytic solution has been understood primarily as a consequence of the quadratic nature of the problem .",
    "here it has been shown however that in the dual picture where the penalised optimisation problem is expressed in terms of its legendre transform , this soft - thresholding mechanism is generic and a direct consequence of the presence of an @xmath0-penalty term .",
    "incorporating this analytic result in a standard coordinate descent algorithm leads to a method that is not only theoretically attractive and easy to implement , but also appears to offer practical advantages compared to the existing implementations of the quadratic - approximation algorithm . in particular",
    "it is more robust to the choice of starting vector and therefore considerably more efficient when it is cold - started , i.e. when a solution is computed at set values of the @xmath0-penalty parameter as opposed to along a regularisation path of descending @xmath0-penalties .",
    "this can be exploited for instance in situations where prior knowledge or other constraints dictate the choice of @xmath0-penalty parameter or in data - intensive problems where distributing the computations for sweeping the @xmath0-penalty parameter space over multiple processors can lead to significant gains in computing time .",
    "future work will focus on developing such parallellized implementations of the natural coordinate descent algorithm and providing implementations for additional commonly used generalised linear models .",
    "with the notations introduced in section [ sec : introduction ] , let @xmath173 and @xmath174 .",
    "@xmath175 and @xmath169 are convex functions on @xmath176 which satisfy fenchel s duality theorem @xcite @xmath177 where @xmath178 and @xmath179 are the legendre transforms of @xmath175 and @xmath169 respectively .",
    "we have @xmath180 and @xmath181 . if @xmath182 then the @xmath111 term in this sum is @xmath183 , otherwise it is @xmath184 , i.e. @xmath185 if @xmath186 and @xmath187 otherwise .",
    "it follows that @xmath188 where @xmath189 . denoting @xmath190",
    ", the minimiser @xmath191 must satisfy the optimality conditions @xcite : @xmath192 and @xmath193 for any index @xmath42 , choose @xmath194 .",
    "then @xmath195 and by eq .",
    ", @xmath196 assume @xmath197 and @xmath198 , where @xmath199 .",
    "then there exists @xmath128 such that @xmath200 , but this contradicts eq . .",
    "by lemma [ lem : legendre ] below , if @xmath201 , then @xmath202 .",
    "hence we have shown that @xmath203 denote @xmath204 .",
    "we find @xmath205 \\hat    \\beta_{0,j } = w^t\\hat\\beta_0 - \\sum_{j=1}^p \\mu_j |\\hat\\beta_{0,j}|,\\end{aligned}\\ ] ] and hence by eq .",
    ", @xmath206 i.e. @xmath86 is also the unique minimiser of the penalised cost function @xmath26 . this concludes the proof of theorem [ thm : main ] .",
    "[ lem : legendre ] for all @xmath37 , we have @xmath207    for a given @xmath208 , let @xmath209 or @xmath210 from fenchel s inequality ( @xmath211 for all @xmath212 , cf .",
    "@xcite ) it follows that @xmath213 i.e. @xmath214 by assumption @xmath23 is differentiable and hence so is @xmath93 .",
    "it follows that ( see @xcite ) @xmath215      first , assume @xmath216 .",
    "by theorem [ thm : main ] we have @xmath142 , and hence @xmath217 and @xmath218 .",
    "this establishes the direction @xmath219 . if @xmath220 , then again by theorem [ thm : main ] and using the notation from section [ sec : natur - coord - desc ] , @xmath221 hence @xmath222 and @xmath223 . if @xmath224 , @xmath225 and , by convexity of u , @xmath226 similarly , if @xmath227 , we have @xmath228 . this establishes the direction @xmath229 .",
    "processed data files were obtained from https://tcga-data.nci.nih.gov/docs/publications/brca_2012/ :    * normalised expression data for 17,814 genes in 547 breast cancer samples ( file ` brca.exp.547.med.txt ` ) . * clinical data for 850 breast cancer samples ( file ` brca_clinical.tar.gz ` ) .",
    "540 samples common to both files had an estrogen receptor status reported as positive or negative in the clinical data .",
    "estrogen receptor status was used as the binary response data @xmath230 , @xmath231 , and gene expression for all genes ( @xmath232 one constant predictor ) was used as predictor data @xmath233 , @xmath234 .",
    "processed data files were obtained from https://tcga-data.nci.nih.gov/docs/publications/coadread_2012/ :    * normalised expression data for 20,531 genes in 270 colon and rectal cancer samples ( file ` crc_270_gene_rpkm_datac.txt ` ) . * clinical data for 276 colon and rectal cancer samples ( file ` crc_clinical_sheet.txt ` ) .",
    "266 samples common to both files had a tumor stage ( from i to iv ) reported in the clinical data .",
    "early ( i  ii ) and late ( iii  iv ) stages were grouped and used as the binary response data @xmath230 , @xmath235 , and gene expression for all genes ( @xmath232 one constant predictor ) was used as predictor data @xmath233 , @xmath236 .",
    "initialise @xmath142 ."
  ],
  "abstract_text": [
    "<S> the problem of finding the maximum likelihood estimates for the regression coefficients in generalised linear models with an @xmath0 sparsity penalty is shown to be equivalent to minimising the unpenalised maximum log - likelihood function over a box with boundary defined by the @xmath0-penalty parameter . in one - parameter models or </S>",
    "<S> when a single coefficient is estimated at a time , this result implies a generic soft - thresholding mechanism which leads to a novel coordinate descent algorithm for generalised linear models that is entirely described in terms of the natural formulation of the model and is guaranteed to converge to the true optimum . </S>",
    "<S> a prototype implementation for logistic regression tested on two large - scale cancer gene expression datasets shows that this algorithm is efficient , particularly so when a solution is computed at set values of the @xmath0-penalty parameter as opposed to along a regularisation path . </S>",
    "<S> source code and test data are available from http://tmichoel.github.io / glmnat/. </S>"
  ]
}