{
  "article_text": [
    "efficient neural network modelling requires an autonomous functioning independent from external constraints or control mechanisms . for fixed - point retrieval by an attractor associative memory model this requirement is mainly expressed by the robustness of its learning and retrieval capabilities against external noise , against malfunctioning of some of the connections and so on .",
    "indeed , a model which embodies this robustness is able to perform as a content - adressable memory having large basins of attraction for the memorized patterns .",
    "intuitively , one can imagine that these basins of attraction become smaller when the storage capacity gets larger .",
    "this might occur , e.g. , in sparsely coded models ( okada , 1996 and references cited therein ) .",
    "therefore , the necessity of a control of the activity of the neurons has been emphasized such that the latter stays the same as the activity of the memorized patterns during the recall process .",
    "this has led to several discussions imposing external constraints on the dynamics .",
    "however , the enforcement of such a constraint at every time step destroys part of the autonomous functioning of the network . to solve this problem , quite recently",
    ", a self - control mechanism has been introduced in the dynamics through the introduction of a time - dependent threshold in the transfer function ( dominguez & boll , 1998 ; boll , dominguez & amari 2000 ) .",
    "this threshold is determined as a function of both the cross - talk noise and the activity of the memorized patterns in the network and adapts itself in the course of the time evolution .    up to now only neural network models without synaptic noise have been considered in this context .",
    "the purpose of the present work is precisely to generalise this self - control mechanism when synaptic noise is allowed .",
    "let us consider a network of @xmath0 binary neurons . at a discrete time step @xmath1 the neurons @xmath2 are updated synchronously according to the rule @xmath3 where @xmath4 are the synaptic couplings",
    ", @xmath5 is the activity of the memorized patterns and @xmath6 is usually called the `` local field '' of neuron @xmath7 at time @xmath1 . in general , the transfer function @xmath8 can be a monotonic function with @xmath9 a time - dependent threshold . later on it",
    "will be chosen as @xmath10\\ , .",
    "\\label{transfer}\\ ] ] the `` temperature '' @xmath11 controls the thermal fluctuations , which are a measure for the synaptic noise ( hertz et al . , 1991 ) . in the sequel , for theoretical simplicity in the methods",
    "used , the number of neurons @xmath0 will be taken to be sufficiently large .",
    "the synaptic couplings @xmath4 themselves are determined by the covariance rule @xmath12 the memorized patterns @xmath13 are independent identically distributed random variables ( iidrv ) with respect to @xmath7 and @xmath14 chosen according to the probability distribution @xmath15 the coefficients @xmath16 are iidrv with probability @xmath17 \\delta_{d,0 } +            ( { c}/{n } ) \\delta_{d,1 }    \\nonumber \\\\",
    "pr\\{c_{ij}=c_{ji}\\}=(c / n)^2,\\quad ( { c}/{n})\\ll 1 , \\quad c>0 . \\end{aligned}\\ ] ] this introduces the so - called extremely diluted asymmetric architecture with @xmath18 measuring the average connectivity of the network ( derrida et al . , 1987 ) .",
    "at this point we remark that the couplings ( [ 2.ji ] ) are of infinite range ( each neuron interacts with infinitely many others ) such that our model allows a so - called mean - field theory approximation .",
    "this essentially means that we focus on the dynamics of a single neuron while replacing all the other neurons by an average background local field . in other words ,",
    "no fluctuations of the other neurons are taken into account , not even in response to changing the state of the chosen neuron . in our case this approximation becomes exact because , crudely speaking , @xmath6 is the sum of very many terms and a central limit theorem can be applied ( hertz et al . , 1991 ) .",
    "it is standard knowledge by now that synchronous mean - field theory dynamics can be solved exactly for these diluted architectures ( e.g. , boll , 2004 ) .",
    "hence , the big advantage is that this will allow us to determine the precise effects from self - control in an exact way .",
    "we recall that the relevant parameters describing the solution of this dynamics are the retrieval overlap , @xmath19 , between the memorized pattern , @xmath20 , and the microscopic network state , @xmath21 , and the neural activity , @xmath22 , given by , respectively @xmath23 we remark that the @xmath19 are normalized parameters within the interval @xmath24 $ ] which attain the maximal value @xmath25 whenever the model succeeds in a perfect recall , i.e. , @xmath26 for all @xmath7 .    in order to measure the retrieval quality of the recall process",
    ", we use the mutual information function ( boll , dominguez & amari , 2000 ; nadal , brunel & parga , 1998 ; schultz & treves , 1998 and references therein ) . in general",
    ", it measures the average amount of information that can be received by the user by observing the signal at the output of a channel ( blahut , 1990 ; shannon , 1948 ) . for the recall process of memorized patterns that we are discussing here , at each time step the process",
    "can be regarded as a channel with input @xmath27 and output @xmath21 such that this mutual information function can be defined as ( forgetting about the pattern index @xmath14 and the time index @xmath1 ) @xmath28 ,               \\label{3.ss } \\\\      & & s(\\sigma_i|\\xi_i )         \\equiv -\\sum_{\\sigma_i}p(\\sigma_i|\\xi_i)\\ln[p(\\sigma_i|\\xi_i ) ] .",
    "\\label{3.sx}\\end{aligned}\\ ] ] here @xmath29 and @xmath30 are the entropy and the conditional entropy of the output , respectively .",
    "these information entropies are peculiar to the probability distributions of the output .",
    "the term @xmath31 is also called the equivocation term in the recall process .",
    "the quantity @xmath32 denotes the probability distribution for the neurons at time @xmath1 , while @xmath33 indicates the conditional probability that the @xmath34 neuron is in a state @xmath35 at time @xmath1 , given that the @xmath34 pixel of the memorized pattern that is being retrieved is @xmath36 .",
    "hereby , we have assumed that the conditional probability of all the neurons factorizes , i.e. , @xmath37 , which is a consequence of the mean - field theory character of our model explained above .",
    "we remark that a similar factorization has also been used in schwenker et al .",
    "( 1996 ) .",
    "the calculation of the different terms in the expression ( [ 3.is ] ) proceeds as follows . formally writing @xmath38 for an arbitrary quantity @xmath39 the conditional probability can be obtained in a rather straightforward way by using the complete knowledge about the system : @xmath40 .",
    "the result reads ( we forget about the index @xmath7 ) @xmath41\\,\\delta(\\sigma-1 ) \\nonumber\\\\       & + &       [ 1-\\gamma_{0}-(m-\\gamma_{0})\\xi]\\,\\delta(\\sigma ) ,       \\nonumber\\\\       \\gamma_{0}&= & \\frac{q - am}{1-a } \\label{3.ps}\\end{aligned}\\ ] ] one can simply verify that this satisfies the averages @xmath42 and those are precisely equal , for large @xmath0 , to the parameters @xmath43 and @xmath44 mentioned above ( eq .  ( [ parmq ] ) ) . using the probability distribution of the patterns ( eq.([2.px ] ) ) , we furthermore obtain @xmath45 hence the expressions for the entropies defined above become @xmath46          \\nonumber\\\\      & & \\hspace*{0.7 cm }          -(1-a ) [ \\gamma_0 \\ln \\gamma_0 + ( 1-\\gamma_0)\\ln(1-\\gamma_0 ) ] .",
    "\\label{3.hs}\\end{aligned}\\ ] ] recalling eq .",
    "( [ 3.is ] ) this completes the calculation of the mutual information content of the present model .",
    "it is standard knowledge ( e.g. , derrida et al . , 1987 ; boll , 2004 ) that the synchronous dynamics for diluted architectures can be solved exactly following the method based upon a signal - to - noise analysis of the local field ( [ 2.si ] ) ( e.g. , amari , 1977 ; amari & maginu , 1988 ; okada , 1996 ; boll , 2004 and references therein ) . without loss of generality",
    "we focus on the recall of one pattern , say @xmath47 , meaning that only @xmath48 is macroscopic , i.e. , of order @xmath25 and the rest of the patterns causes a cross - talk noise at each time step of the dynamics .",
    "supposing that the initial state of the network model , @xmath49 , is a collection of iidrv with mean zero and neural activity @xmath50 and correlated only with memorized pattern @xmath25 with an overlap @xmath51 , then the full time evolution can be shown to be given by @xmath52 \\rangle_{\\omega }       \\label{3.m1}\\\\    q_{t+1}=a m_{t+1}^1 +           ( 1-a)\\langle f_{\\theta_{t},\\beta}(-am^1_{t }                         + \\omega_{t } ) \\rangle_{\\omega}\\ , ,                \\label{3.q}\\end{aligned}\\ ] ] with @xmath53 where we have averaged over the first pattern @xmath54 and where the angular brackets indicate that we still have to average over the residual ( cross - talk ) noise @xmath55 which can be written as @xmath56^{1/2 } { \\cal n}(0,1 ) ,    \\quad q_{t}=(1 - 2a)q_{t } + a^2\\ ] ] with @xmath57 a gaussian random variable with mean zero and variance unity and the ( finite ) loading defined by @xmath58 . recalling the specific form of the transfer function ( [ transfer ] ) we explicitly have @xmath59 \\rangle_{\\omega }       \\nonumber\\\\    & & \\hspace*{-1 cm }      = \\int_{-\\infty}^{\\infty }        \\frac{dy \\,\\,e^{-y^2/ \\alpha q_t}}{2\\sqrt{2\\pi \\alpha q_t } }    [ 1 + \\tanh[\\beta ( -am_t - \\theta_{t } + y ) ] ]   \\label{transtemp }   \\end{aligned}\\ ] ] and an analogous expression for @xmath60 \\rangle_{\\omega}$ ] .",
    "of course , it is known that the quality of the recall process is influenced by the cross - talk noise at each time step of the dynamics .",
    "a novel idea is then to let the network itself autonomously counter this cross - talk noise at each time step by introducing an adaptive , hence time - dependent , threshold .",
    "this has been studied for neural network models at zero temperature , i.e. , without synaptic noise where @xmath61 . for sparsely coded models , meaning that the pattern activity @xmath5 is very small and tends to zero for @xmath0 large ,",
    "it has been found ( dominguez & boll , 1998 ; boll , dominguez & amari , 2000 ) that @xmath62 makes the second term on the r.h.s of eq.([3.q ] ) asymptotically vanish faster than @xmath5 such that @xmath63 .",
    "it turns out that the inclusion of this self - control threshold considerably improves the quality of the fixed - point retrieval dynamics , in particular the storage capacity , the basins of attraction and the information content . as an example we present in fig .  1 the basin of attraction for the whole retrieval phase @xmath64 for the self - control model with @xmath65 given by eq .",
    "( [ 2.tt ] ) and initial value @xmath66 , compared with a model where the threshold @xmath67 is selected for every loading @xmath68 by hand in an optimal way meaning that the information content @xmath69 is maximized .",
    "the latter is non - trivial because it is even rather difficult , especially in the limit of sparse coding , to choose a threshold interval by hand such that @xmath7 is non - zero .",
    "the basin of attraction is clearly enlarged with this self - control threshold choice and even near the border of critical storage the results are still improved . for more details",
    "we refer to dominguez & boll ( 1998 ) and boll , dominguez & amari ( 2000 ) .     for @xmath70 and initial @xmath71 for the self - control model ( full line ) and the optimal threshold model ( dashed line ) at zero temperature .",
    ", width=264 ]    a similar threshold also works for sparsely coded sequential patterns ( kitani & aoyagi , 1998 ) and even for non - sparse architectures as well ( boll & dominguez carreta , 2000 ) .",
    "it is then worthwhile to examine whether such a self - control threshold can be found for networks with synaptic noise .",
    "no systematic study has been done in this case .",
    "the specific problem to be posed in analogy with the zero - temperature case is the following one .",
    "can one determine a form for the threshold @xmath72 in eq .",
    "( [ transtemp ] ) such that the integral vanishes asymptotically faster than @xmath5 ?",
    "in contrast with the zero - temperature case , where due to the simple form of the transfer function , this threshold could be determined analytically ( recall eq .",
    "( [ 2.tt ] ) , a detailed study of the asymptotics of the integral in eq .",
    "( [ transtemp ] ) gives no satisfactory analytic solution .",
    "therefore , we have designed a systematic numerical procedure through the following steps :    * choose a small value for the activity @xmath73 . *",
    "determine through numerical integration the threshold @xmath74 such that @xmath75 for different values of the variance @xmath76 .",
    "* determine , as a function of the temperature @xmath77 , the value for @xmath78 such that @xmath79 \\leq a ' \\nonumber \\\\                   & & \\mbox{for } \\quad \\theta >",
    "\\theta ' + \\theta'_t.\\end{aligned}\\ ] ]    the second step leads , as expected , precisely to a threshold having the zero - temperature form eq .",
    "( [ 2.tt ] ) .",
    "the third step determining the temperature dependent part @xmath78 leads to the results shown in fig .  2 .",
    "as a function of @xmath80 for several values of @xmath73,width=245 ]    intuitively it is seen that @xmath78 behaves quadratically .",
    "indeed , making a polynomial fit of these results we find that the linear term is negligable and that the quadratic term is of the form @xmath81 .",
    "furthermore , the dependence of the coefficient of this quadratic term on the variance is very weak .",
    "hence , we propose the following self - control threshold @xmath82 together with eqs.([3.m1])-([3.q ] ) this relation describes the self - control dynamics of the network model with synaptic noise .",
    "this dynamical threshold is again a macroscopic parameter , thus no average must be taken over the microscopic random variables at each time step @xmath1 .    at this point",
    "we want to make two remarks .",
    "first , for a binary layered network ( boll & massolo , 2000 ) the inclusion of a threshold of the form ( [ 2.tt ] ) , although not designed for non - zero temperatures , is shown to still improve the retrieval quality for low pattern activities and low temperatures , in comparison with an optimal threshold model analogous to the one mentioned above . secondly , in a recent study of an extremely diluted three - state neural network ( dominguez et al . , 2002 )",
    "based on information theoretic and mean - field theory arguments , a self - control threshold with a linear temperature correction term with coefficient @xmath25 has been mentioned without any further details . in that specific model",
    "this self - control threshold is shown to improve the retrieval quality for low temperatures but it is not specified how much of the improvement is really due to the linear correction itself .     for @xmath70 and several values of the temperature with ( full lines ) and without ( dashed lines ) the temperature correction @xmath78 in the threshold.,width=245 ]",
    "we have solved this self - control dynamics , eqs.([3.m1])-([3.q ] ) and eq .",
    "( [ threstemp ] ) , for our model with synaptic noise , in the limit of sparse coding , numerically .",
    "in particular , we have studied in detail the influence of the temperature dependent part of the threshold . of course , we are only interested in the retrieval solutions with @xmath83 and carrying a non - zero information  @xmath84 .",
    "we remark that all numerical calculations presented here are done for an appropriate number of time steps ( at least of the order of a few hundred ) in order to assure that a stable equilibrium point is reached .     for several initial values",
    "@xmath85 with @xmath70 , @xmath86 and @xmath87 without ( left ) and with ( right ) the temperature correction @xmath78 in the threshold.,width=245 ]    the important features of the solution are illustrated in figs .",
    "3 - 5 . in fig .",
    "3 we show the basin of attraction for the whole retrieval phase for the model with the temperature - zero threshold ( [ 2.tt ] ) ( dashed curves ) compared to the model with the temperature dependent threshold ( [ threstemp ] ) ( full curves ) ( compare also fig .  1 ) .",
    "we see that there is no clear improvement for low temperatures but there is a substantial one for higher temperatures . even near the border of critical storage",
    "the results are still improved such that also the storage capacity itself is larger .",
    "this is further illustrated in fig .",
    "4 where we compare the time evolution of the retrieval overlap @xmath88 starting from several initial values , @xmath85 , for the model with ( right figure ) and without ( left figure ) the quadratic temperature correction in the threshold . here",
    "this temperature correction is absolutely crucial to force some of the overlap trajectories to go to the retrieval attractor @xmath89 .",
    "it really makes the difference between retrieval and non - retrieval in the model . at this point",
    "we remark that the influence of a linear temperature correction term has been examined also here but no real improvement has been found of the results for the temperature - zero threshold .     as a function of @xmath80 for several values of the loading @xmath68 and @xmath90 with ( full lines ) and without ( dashed lines ) the temperature correction @xmath78 in the threshold.,width=245 ]    in fig .",
    "5 we plot the information content @xmath7 as a function of the temperature for the self - control dynamics with the threshold ( [ threstemp ] ) ( full curves ) , respectively ( [ 2.tt ] ) ( dashed curves ) . we see that , especially for small loading @xmath68 a substantial improvement of the information content is obtained .",
    "in this work we have generalized complete self - control in the dynamics of sparsely coded associative memory networks to models with synaptic noise . we have proposed an analytic form for the relevant macroscopic threshold consisting out of the known form for temperature zero plus a quadratic temperature correction term dependent on the pattern activity .",
    "the consequences of this self - control mechanism on the quality of the recall process by the network have been studied .",
    "we find that the basins of attraction of the retrieval solutions as well as the storage capacity are enlarged and that the mutual information content is maximized .",
    "this confirms the considerable improvement of the quality of recall by self - control , also for network models with synaptic noise .",
    "this allows us to conjecture that this idea of self - control , allowing the network to function autonomously , might be relevant for other architectures in the presence of synaptic noise , and for dynamical systems in general , when trying to improve the basins of attraction and convergence times .",
    "we are indebted to s. goossens for some contributions at the initial stages of this work .",
    "one of the authors ( db ) would like to thank d. dominguez for stimulating discussions .",
    "this work has been supported by the fund for scientific research- flanders ( belgium ) .",
    "boll d ( 2004 ) .",
    "multi - state neural networks based upon spin - glasses : a biased overview in _ advances in condensed matter and statistical mechanics _ eds .",
    "korutcheva e and cuerno r. , nova science publishers , new - york , p. 321 - 349 ."
  ],
  "abstract_text": [
    "<S> for the retrieval dynamics of sparsely coded attractor associative memory models with synaptic noise the inclusion of a macroscopic time - dependent threshold is studied . </S>",
    "<S> it is shown that if the threshold is chosen appropriately as a function of the cross - talk noise and of the activity of the memorized patterns , adapting itself automatically in the course of the time evolution , an autonomous functioning of the model is guaranteed . </S>",
    "<S> this self - control mechanism considerably improves the quality of the fixed - point retrieval dynamics , in particular the storage capacity , the basins of attraction and the mutual information content . </S>"
  ]
}