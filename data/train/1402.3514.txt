{
  "article_text": [
    "principal component analysis ( pca ) is widely used to explore high - dimensional data .",
    "it centers and rotates the original @xmath0-dimensional measurements to construct a small number @xmath1 of new orthonormal variables , called _ principal components _ , that account for most of the variation in the data . however , classical pca is very sensitive to outliers .",
    "outliers are observations that are inconsistent with the multivariate pattern of the majority of the data .",
    "if left unchecked , they influence the estimated parameters by disproportionately pulling the fit towards themselves . in this way , outliers obscure the main relationships in the data and their true outlyingness . in practice , we want to find the outliers to bound their influence on the fit and to study as objects of interest in their own right . for these reasons ,",
    "we need robust pca methods that meet the following criteria :    like classical pca , a robust pca method should handle cases where the number of variables exceeds the number of observations ,    and it should be shift and rotation equivariant , meaning that if the data are shifted or rotated the estimated parameters should transform accordingly",
    ".    it should be computable for high - dimensional data .",
    "it should accurately describe the multivariate pattern of the majority of the observations , even when the data is heavily contaminated by outliers .",
    "it should have a high breakdown point ; a measure an estimator s robustness to outliers in the data .",
    "it should be insensitive to the dimensionality of the data .",
    "criteria ( 1)-(3 ) are natural for any pca method .",
    "criteria ( 4)-(5 ) relate to robustness .",
    "criterion ( 6 ) is related to both concerns .",
    "we find that state - of - the - art robust pca algorithms have most of these properties , but that , surprisingly , many instances can be found where they fail to satisfy criterion ( 4 ) . in this paper , we introduce a robust pca algorithm , fasthcs , to meet these criteria ( hcs for high - dimensional congruent subset ) . in the next section we outline fasthcs .",
    "then , in sections   and   we compare it to several state - of - the - art methods on simulated data and three real data applications which show that in many settings only fasthcs can be relied upon to provide a robust pca solution .",
    "given an @xmath2 data matrix @xmath3 and for a fixed @xmath4 , the fasthcs algorithm searches for a subset of size at least @xmath5 free of outliers ( this is the minimal value of @xmath6 such that there are at least @xmath7 clean observations in each candidate subset ) .",
    "if @xmath8 , fasthcs computes the mean - centered data matrix @xmath9 , and performs the kernel eigenvalue decomposition of @xmath10 where @xmath11 is an @xmath12 matrix , @xmath13 is an @xmath14 matrix , @xmath15 and @xmath16 is a diagonal matrix with the eigenvalues on the diagonal .",
    "then , fasthcs works with the @xmath17 matrix @xmath18 .",
    "the transformation from @xmath19 to @xmath20 causes no loss of information or robustness since we retain all of the components corresponding to non - zero eigenvalues .",
    "however , this transformation reduces the computational cost of the subsequent steps of the algorithm . at the end of the algorithm",
    ", fasthcs reverses these transformations so that the returned parameter estimates are consistent with conventional pca .",
    "when @xmath21 , we simply set @xmath22 .",
    "the @xmath23-index is a subset selection criterion first introduced in  @xcite where it is used to identify an outlier free subset to serve as the basis of the robust pcs location and scatter estimator .",
    "the @xmath23-index was designed to be insensitive to the configuration of the outliers and consequently , as we show in that article , the fit found by fastpcs is nearly unaffected by the presence of outliers in the data ( we refer to this as quantitative robustness ) . in  @xcite we further show that the pcs estimates also have the maximum possible breakdown point ( we refer to this as qualitative robustness ) .",
    "robust location and scatter estimation are also important for pca . in the pcs context , the @xmath23-index is applied to the observations in their original dimensionality , and one approach to achieving robust pca would be to use the robust pcs covariance estimate as a starting point for pca",
    ". however , this approach does not satisfy criterion ( 3 ) for robust pca since it is not possible to perform pcs when the number of dimensions is greater than the number of observations .",
    "this subsection describes how the @xmath23-index can be extended to the pca context by applying it to projections of the data on to subspaces .",
    "to begin , fasthcs draws @xmath24 random subsets of size @xmath7 from @xmath20 without replacement , where @xmath24 is given by : @xmath25 and where @xmath26 is an integer specifying the number of uncontaminated observations , so that the probability of getting at least one uncontaminated starting subset is at least 99%  @xcite . by default we set @xmath27 .",
    "however , if the user is sure that the contamination rate of the sample is lower than @xmath28 , we offer the possibility ( as in  @xcite ) of using this information to reduce the computational cost of running fasthcs .",
    "denote these @xmath7-subsets as @xmath29 .",
    "the svd decomposition of the observations indexed by @xmath30 is : @xmath31 where @xmath32 is the estimated center , @xmath33 is a diagonal matrix for which the non - zero elements @xmath34 are the descending eigenvalues of the pca model fited to @xmath35 , and the eigenvectors @xmath36 are the first @xmath1 loadings of this model .",
    "next , we compute the score matrix @xmath37 with @xmath38 rows @xmath39 :",
    "@xmath40 which is the projection of the re - centered rows of @xmath41 on to the subspace spanned by the first @xmath1 loadings of @xmath42 . to measure the outlyingness of an @xmath43 to the members of @xmath44 , we will use its squared orthogonal distance to @xmath45 , the direction normal to the hyperplane through @xmath1 members of @xmath44 drawn at random : @xmath46 and , to remove the dependence of this measure on the direction @xmath47 , we average it over @xmath48 such directions @xmath49",
    ": @xmath50 ( in remark 1 below we discuss how we set the value of the parameter @xmath48 ) .",
    "the denominator in equation   normalizes these distances across the directions @xmath47",
    ".    we can now describe the computation of the first step of fasthcs . for a given a @xmath7-subset @xmath30 of @xmath51 and its corresponding matrix @xmath37 , algorithm 1 returns and @xmath6-subset @xmath52 of indexes of @xmath51 using an iterative procedure we call _ growing steps_. in each step @xmath53",
    ", @xmath54 is updated and contains the indexes of the @xmath55 observations with smallest values of @xmath56 .",
    "the value of @xmath55 itself increases incrementally from @xmath57 to @xmath6 in @xmath58 steps .",
    "these steps do not have a convergence criterion , so the number of iterations @xmath58 must be set in advance ( in remark 1 below we discuss how we set the value of the parameter @xmath58 ) .",
    "0.15 cm    ' '' ''    0.15 cm @xmath59    ' '' ''    for = @xmath60 to @xmath58 do : + @xmath61 + set @xmath62 + set @xmath63 + end for + @xmath64    ' '' ''    0.3 cm    after growing @xmath24 candidate @xmath52 s , fasthcs evaluates each using a criterion we call the @xmath23-index , and fits a robust pca model to the @xmath52 having smallest value of the @xmath23-index . for a given @xmath6-subset @xmath52 and direction @xmath65",
    ", we define a subset @xmath66 that is optimal with respect to @xmath65 in the sense that it indexes the @xmath6 observations with the smallest values of @xmath67 .",
    "more precisely , denoting @xmath68 the @xmath69 order statistic of a vector @xmath70 , we have : @xmath71 then , we define the @xmath23-index of an @xmath52 along @xmath45 as @xmath72with the convention that @xmath73 .",
    "the measure @xmath74 is always positive and increases the fewer members @xmath52 shares with @xmath66 along the direction @xmath45 .",
    "this is because , for a given direction @xmath45 , the members of @xmath66 not in @xmath52 will decrease the denominator in equation   without affecting the numerator , increasing the overall ratio . as in the growing steps ,",
    "we remove the dependence of equation on the directions @xmath45 by considering the average over @xmath48 directions : @xmath75 finally , fasthcs selects as @xmath76 the candidate @xmath6-subset @xmath52 with lowest @xmath23-index .",
    "given @xmath77 , we denote the pca parameters corresponding to @xmath77 as @xmath78 and obtain them as follows : @xmath79 where @xmath80 .",
    "fasthcs computes these parameters on the full space of the data set , @xmath81 , rather than on the space of @xmath82 , to increase their accuracy .",
    "algorithm 2 summarizes the i - index step of fasthcs .",
    "0.15 cm    ' '' ''    0.15 cm @xmath83    ' '' ''    for = @xmath84 to @xmath24 do : + @xmath85 + @xmath86 + @xmath87 + end for + @xmath88 + return @xmath89    ' '' ''    0.3 cm    through experiments , we find that increasing @xmath48 above 25 or @xmath58 above 5 does not noticeably improve the performance of the algorithm ( though it increases its computational cost ) , so we set these parameters to those values .",
    "those experiments were carried on the outlier configurations discussed in sections 3 and 4 as well as additional configurations enabled by the simulation suite provided with the online resources ( section 4 ) . because such experiments can not cover all possible configurations of outliers , we focused on those configurations singled out as most challenging in the literature on robust pca .",
    "* exact fit : * when the @xmath6 members of a subset @xmath90 lie on a subspace @xmath91 with @xmath92 the numerator and denominator of @xmath93 will be the same for any direction @xmath94 through members of @xmath90  @xcite so that @xmath95 . then , @xmath96 and @xmath97 will correspond with @xmath98 .",
    "in such situations , fasthcs will return the index of the members of @xmath99 .",
    "this behavior is called exact fit  @xcite .    *",
    "breakdown point : * the ( finite sample ) breakdown point of an estimator referred to in criterion ( 5 ) is the smallest proportion of observations that need to be replaced by arbitrary value to drive the estimates to arbitrary values  @xcite .",
    "naturally , a higher breakdown point is better , and the maximal breakdown point achievable in the pca context is essentially fifty percent .",
    "both the growing step and the i - index use distances computed on subspaces to derive a measure of outlyingness .",
    "since they are restricted to this view of the data , they are vulnerable to outliers that appear consistent with the majority on a subspace , but are outlying with respect to it ( appendix a details the specific configurations of outliers giving rise to this issue ) .",
    "therefore , fits based on @xmath77 alone will not have maximum breakdown and the procedure presented above must be combined with a second , computational expedient , ancillary procedure to ensure that the final fasthcs estimates do .",
    "although experiments , such as those in sections  [ hcs : s3 ] and  [ hcs : s4 ] , show that the @xmath23-index rarely selects contaminated subsets , it is vulnerable to specific configurations of outliers ( see appendix a ) . to guard against these",
    ", fasthcs uses a robust projection - pursuit ( pp ) approach to identify a second subset of observations , @xmath100 .",
    "the pp approach proceeds by assigning an outlyingness score to each observation : @xmath101 where @xmath102 contains 1000 directions @xmath103 , each given by two data points drawn randomly from the sample , @xmath104 is the median of @xmath105 and @xmath106 .",
    "the pp method is orthogonaly invariant and computationally expedient .",
    "a version of the pp algorithm is used as an initial step in robpca  @xcite , a popular robust pca algorithm .      consider the subset @xmath107 . because @xmath108 + 1 $ ]",
    ", it holds that @xmath109 and @xmath110 is free of outliers whenever either one of @xmath77 or @xmath100 is .",
    "we propose to exploit this fact to select between the i - index and pp - based models .",
    "denote @xmath111 and @xmath112 with the assumption that @xmath113 .",
    "when @xmath114 ( or if @xmath115 ) the final fasthcs parameters @xmath116 will be equal to @xmath117 and the final fasthcs subset @xmath118 is set as @xmath100 . as we show in appendix  [ app : ab ] , this selection rule ensures that the fasthcs fit has a high breakdown point .",
    "our approach is similar to the robpca algorithm which also selects from among two candidate subsets in the final stage of the algorithm .",
    "robpca selects the subset whose eigenvalues have the smallest product .",
    "however , depending on the configuration of the outliers and the rate of contamination , it is possible for a contaminated subset to have smaller eigenvalues than an uncontaminated one  @xcite , and so to end up being selected by the criterion used in robpca . in contrast ,",
    "the selection criterion we propose controls ( through the denominators in equation  ) for the relative scatter of the two subsets and so it is not biased towards subsets containing many concentrated outliers . naturally , criterion is designed to favor the @xmath23-index based model whenever doing so does not cause breakdown .",
    "two concepts of distance are used to assess the outlyingness of an observation with respect to a pca model , and cut - off values for both of these can be used to classify outliers  @xcite .",
    "the first is the orthogonal distance ( @xmath119 ) of the observation to the pca model space : @xmath120 assuming multivariate normality of the observations on which the pca model is fitted , a cut - off can be obtained for the @xmath119 statistics using the wilson - hilferty transformation of the @xmath119s into approximately normally distributed random variables : @xmath121 where @xmath122 is the @xmath123 quantile of the @xmath124 distribution with one degree of freedom , and @xmath125 indexes a subset of observations .",
    "the second measure of outlyingness is the score distance ( @xmath126 ) of the observation on the pca model space : @xmath127 a 97.5% cut - off for the @xmath126 statistics can be obtained using a @xmath128 distribution .    in inferential applications ,",
    "pca theory typically assumes multivariate normality , though ellipticity is sufficient for many of the hypotheses of interest to pca - based inference , see  @xcite and  @xcite . in any case ,",
    "robust pca performs inference with a model fitted on the non - outlying observations , so the distributional assumption pertains to only this subset of the data .",
    "conversely , no assumptions are made on the distribution(s ) of the outliers .",
    "the computational complexity of fasthcs is determined by the @xmath23-index and pp subset selection components .",
    "the complexity of the pp - based approach is @xmath129 .",
    "this is dominated by the time complexity of the @xmath23-index , which scales as @xmath130 for each starting @xmath7-subset .",
    "except when @xmath1 and @xmath38 are small ( smaller than about 5 and 2000 in our tests ) fasthcs is not the quickest of the robust pca methods we considered ( in general , we find that pcal is ) . the fast qualification in this context ( `` fasthcs '' ) is used to distinguish the algorithm based on random sub - sampling from the nave one based on exhaustive enumeration of all possible starting points , the latter being usually not computable in practice .",
    "the computing time of fasthcs grows similarly in @xmath38 to other methods we discuss in this paper , while it is the most sensitive to increases in @xmath1 , with computation times being comparable until around @xmath131 . for higher",
    "@xmath1 , fasthcs is the slowest overall . in practice , fasthcs becomes impractical for values of @xmath1 much larger than 25 .",
    "nevertheless , the overall time complexity of fasthcs grows with @xmath1 , instead of @xmath0 , making it a suitable candidate for high - dimensional applications , and satisfying criterion ( 3 ) for a robust pca method .",
    "moreover , fasthcs belongs to the class of so called ` embarrassingly parallel ' algorithms , i.e. its time complexity scales as the inverse of the number of processors , meaning it is well suited to benefit from modern computing environments . to enhance user experience",
    ", we implemented fasthcs in c-0.2ex@xmath132 and wrapped it in an portable , open source ` r ` package  @xcite distributed through ` cran ` ( package ` fasthcs ` )",
    "in this section we evaluate the behavior of fasthcs against three other robust pca methods : the robpca @xcite , pcapp  @xcite and pcal  @xcite algorithms .",
    "although other methods for high - dimensional outlier detection exist , these are particularly comparable with fasthcs : all three are pca algorithms , satisfying criteria ( 1)-(3 ) of a robust pca method .",
    "robpca , pcapp and pcal were computed using the ` r `  @xcite package ` rrcov `  @xcite with default settings except for the robustness parameter ` alpha ` for robpca which we set to 0.5 , the value yielding maximum robustness and the value of @xmath133 which we set to @xmath1 for all the algorithms .",
    "our evaluation criterion is the empirical bias , a quantitative measure of robustness of a fit . in appendix c",
    "we explain the motivation for this choice ( in the online resources we also report the results obtained using alternative evaluation criteria ) .      given an elliptical distribution @xmath134 with location vector @xmath135 and covariance matrix @xmath136 ( the superscript @xmath137 stands for uncontaminated ) and an arbitrary distribution @xmath138 ( the superscript @xmath139 stands for contaminated ) , consider the @xmath140-contaminated model @xmath141 for a fixed @xmath142 denote @xmath143 the rank @xmath1 approximation of @xmath144 and @xmath145 an estimator of @xmath143 .",
    "the ( empirical ) bias measures the difference between @xmath146 and @xmath147 .",
    "for this , we will consider more specifically the shape component of this difference which is called the shape bias .",
    "given these two ( rank reduced ) covariance matrices , recall that the corresponding shape matrices are defined by @xmath148 and @xmath149 . for an estimator of @xmath150 ,",
    "all the information about the shape bias is contained in the matrix @xmath151 , or equivalently its condition number  @xcite : @xmath152 where @xmath153 ( @xmath154 ) is the largest ( @xmath155 ) eigenvalue of the positive - semidefinite matrix @xmath151 . evaluating",
    "the maximum bias of @xmath146 is an empirical matter : for a given sample , it depends on the dimensionality of the data , the rate of contamination by outliers , the distance separating them from the uncontaminated observations , and the spatial configuration of the outliers ( @xmath138 ) .",
    "however , because all the algorithms we compare are rotation and shift equivariant , w.l.o.g . we can focus on configurations where @xmath144 is diagonal , and @xmath156 ( a @xmath0-vector of zeros ) , greatly reducing the number of scenarios we need to consider .",
    "since the effect of contamination is presumably most harmful when the outlier belongs to the subspace spanned by @xmath157 ( the orthogonal complement of @xmath158 ) we , concentrate on the class of outlier configurations satisfying these conditions  @xcite . in the simulation results shown in section  [ mcs : s5 ] ,",
    "the outliers belong to the subspace spanned by the eigenvector corresponding to the @xmath7-th eigenvalue of @xmath144 ( as in @xcite ) whereas the simulation settings shown in the online resources the outliers belong to the subspace spanned by all the components of @xmath157 ( as is done in  @xcite ) .      to quantify the robustness of the four algorithms , we generate many contaminated data sets @xmath159 of size @xmath38 with @xmath160 where @xmath161 and @xmath162 are the uncontaminated and contaminated part of the sample . in all simulations , the center of the uncontaminated data @xmath163 its @xmath144 is either @xmath144 or @xmath164 .",
    "we show results where @xmath165 , @xmath166 , @xmath167 , and @xmath140 is one of @xmath168 to facilitate comparison , we consider a generalization to arbitrary values of @xmath1 of the parametrization of @xmath136 used in  @xcite .",
    "to define this matrix , @xmath13 , we set the values of the first @xmath1 elements of the diagonal of @xmath144 so that they decrease exponentially and do not drop abruptly before the remaining , smaller , entries .",
    "more precisely , the first @xmath1 entries of the diagonal of @xmath136 are the first @xmath1 fibonacci numbers and the entries @xmath169 are linearly decreasing as ( @xmath170 ) . in section 2 of the online resources , we also provide results using a covariance matrix proprosed by  @xcite .    our measure of robustness , the bias , depends on the distance between the outliers and the non outlying observations which we will measure by @xmath171 where @xmath172 is an indicator for the observations coming from @xmath162 .",
    "( a more detailed description of how we set the location of the outliers can be found in section 3 of the online resources . ) in the simulations , the distance @xmath173 separating the outliers from the good data is one of @xmath174    we consider two outlier configurations frequently used in the robust pca literature  @xcite :    shift outliers : @xmath175 and @xmath176 chosen to satisfy equation  ;    point - mass outliers : all the outliers are concentrated around a single point at a distance @xmath173 from @xmath161 . to obtain this effect , we set @xmath177",
    ".    both of these outlier configurations are relevant in practical applications where they are , for example , similar to certain types of sensor faults and contamination scenarios .    for fasthcs ,",
    "the number of initial @xmath7-subsets @xmath24 is given as in equation  , with @xmath178 .",
    "the ` rrcov ` implementations for robpca and pcapp include hardcoded values for the number of starting subsets presumed by their authors to be sufficient for these methods .",
    "pcal does not require starting subsets .",
    "section 4 of the online resources explains how the reader can use code we supply to replicate all results from this section .    in figures [ hcs :",
    "sim1 ] to [ hcs : sim2 ] , we display the bias curves as lattice plots @xcite for discrete combinations of @xmath0 , @xmath1 and @xmath140 .",
    "in all cases , we expect the outlier detection problem to become monotonically harder as we increase @xmath1 and @xmath140 , so little information will be lost by considering a discrete grid of a few values for these parameters .",
    "the configurations also depend on the distance separating the data from the outliers . here , the effects of @xmath173 on the bias are harder to foresee : clearly nearby outliers will be harder to detect but misclassifying distant outliers will increase the bias more .",
    "therefore , we will test the algorithms for many values ( and chart the results as a function ) of @xmath173 . for each algorithm ,",
    "a solid colored line will depict the median , and a dotted line ( of the same color ) the 75th percentile of @xmath179 .",
    "each figure is based on 12000 simulations .",
    "figure  [ hcs : sim1 ] displays the bias curves corresponding to the fits found by the algorithms for @xmath180 for the shift ( right ) and point - mass ( left ) configurations .",
    "regardless of the spatial configuration of the outliers or the value of @xmath140 , the fits found by pcapp and pcal generally have high values of @xmath179 .",
    "as it turns out , pcapp and pcal will show poor performance on all of the remaining simulations as well .",
    "since this poor performance is consistent , we will not discuss it in detail .",
    "the performance of robpca is substantially better than the previous two algorithms , but it becomes increasingly unreliable as @xmath1 increases .",
    "fasthcs shows almost no bias .",
    "furthermore , we see that in some cases even after the bias curves of robpca have re - descended , they are still above those of fasthcs .",
    "given that this gap increases with @xmath140 , we infer that the eigenvalue estimation of robpca is still influenced by the outliers , even when the eigenvectors are correctly estimated .",
    "fasthcs estimates both correctly .     for @xmath180 , shift ( top ) and point - mass ( bottom ) as a function of @xmath173 .",
    ", , , .,title=\"fig:\",scaledwidth=95.0% ]   for @xmath180 , shift ( top ) and point - mass ( bottom ) as a function of @xmath173 . ,",
    ", , .,title=\"fig:\",scaledwidth=95.0% ]    we next consider the high dimensional case of @xmath8 .",
    "figure  [ hcs : sim2 ] . across all scenarios ,",
    "the results are comparable to those in seen in figure  [ hcs : sim1 ] .",
    "fasthcs is the best performing method , being unaffected by the outliers , while the other methods show high biases on most settings .     for @xmath181 , shift ( top ) and point - mass ( bottom ) as a function of @xmath173 .",
    ", , , .,title=\"fig:\",scaledwidth=95.0% ]   for @xmath181 , shift ( top ) and point - mass ( bottom ) as a function of @xmath173 .",
    ", , , .,title=\"fig:\",scaledwidth=95.0% ]    over all of the scenarios , fasthcs shows almost no bias , despite challenging outlier configurations .",
    "furthermore , the bias curves corresponding to the fits found by fasthcs are also less variable : throughout , the 75th percentile of the bias corresponding to the fasthcs fit is typically closer to the median bias than is the case for the other algorithms .",
    "these findings indicate that fasthcs meets criteria ( 4)-(5 ) for a robust pca method .",
    "in contrast , we find that the performance of the other methods vary with the configuration of the outliers , the rate of the contamination , and the dimensionality of the @xmath1-subspace . in section 5 of the online resources , we re - analyze these simulation results , giving similar plots for a measure of outlier misclassification , as well as the principal angle measure and @xmath182 , two measures of quality of fit focusing on the loadings .",
    "an extended simulation study shows that the results we present above are robust the choice of different simulation settings ( for example , those used in  @xcite ) , and different bias measurments criterions",
    ". however , since the outcome of the extended study is nearly identical to the one we present in this section , we have relegated these results in the online resources .",
    "we next apply the algorithms to three real data examples . we selected these examples because in each the observations in the data can be separated into two subgroups from which we construct a majority and an outlier group .",
    "they are taken from three fields that regularly use pca : character recognition , chemometrics and genetics .",
    "a feature shared by all of these data sets is that the variables within each are measured on the same scale .",
    "data sets with this property were selected to remove the ancillary problem of data standardization . if the variables are not on the same scale , it is common practice in pca modelling to standardize the data , but the choice of how to do so robustly adds a layer of subjectivity to the results . for the interested reader ,",
    "the data sets used in this section are included in the ` fasthcs ` package .",
    "section 6 of the online resources explains how the reader can use code we provide to replicate all results in this section .",
    "the implementations of robpca and pcapp we use do not have an option to set the seed , but to ensure reproducibility of the results for fasthcs , we set ` seed=1 ` .",
    "pcal is a deterministic algorithm and uses no seeds . as in the simulations",
    ", we run each algorithm with default settings , except for the ` alpha ` parameter in robpca which we set to 0.5 .",
    "to illustrate the outlier detection capabilities of the algorithms , we display diagnostics plots .",
    "these show the @xmath119 and @xmath126 values for each observation , divided by the cut - off values in equations and   to put each of the methods on a comparable scale .",
    "we recommend using as large a value of @xmath1 as possible for fasthcs , since this improves its outlier detection performance .",
    "however , to avoid the curse of dimensionality , it is also advised to set @xmath183  @xcite . in all the examples that follow ,",
    "we select a relatively high number of components , @xmath184 , to strike a balance between computation time and accuracy .",
    "once the outliers have been detected , components with large eigenvales can be analyzed and used to construct a pca model of the good data .",
    "one may also wish to use a selection criterion , such as the scree chart or contribution to variance . in section 7 of the online resources",
    ", we also show results using the latter of these approaches in an extended analysis . in that analysis , the chemometrics data set illustrates how robust pca methods parametrized based on a parsimonious , eigenvalue - based criterion , may miss outliers on minor components , even when the majority of the data may be modelled using a parsimonious model .",
    "[ ]        [ ]        [ ]        [ ]    [ ]        [ ]      the multiple features data set  @xcite contains many replications of hand written numerals ( 0-9 ) extracted from nine original maps of a dutch public utility . for each numeral , we have 200 replications ( the observations ) expressed as a vector of 76 of fourier coefficients ( the features ) describing its shape .",
    "finally , each numeral has been manually identified , yielding an extra vector of class labels . in this application",
    ", we will combine the vectors of fourier coefficients corresponding to the 200 replications of the digit 1 to the vector of fourier coefficients corresponding to the first 150 replications of the digit 0 ( so that @xmath185 and @xmath186 ) .",
    "the goal of the methods will be to distinguish the 0 s and the 1 s .",
    "to give an impression of the differences between the two groups , we plot the fourier coefficients corresponding to the main ( outlier ) subgroup in the bottom ( top ) panels of figure  [ hcs : rd1 ] as dark blue ( light orange ) curves . in general , the curves corresponding to the members of the two groups are visually similar . in particular , the vertical ranges of both largely overlap , and both sets of curves exhibit a similar pattern of variance clustering where the central 40 fourier coefficients have systematically less dispersion than higher or lower ones .",
    "figure  [ hcs : rd2 ] depicts the four resulting diagnostic plots .",
    "we assign to each observation a color ( dark blue or light orange ) and a plot symbol ( round or triangle ) depending on whether the corresponding curve describes a member of class 1 or 0 , respectively .",
    "the outlier plots of pcapp and pcal show that neither method makes any distinction between the two digits , and observations from both groups influence the corresponding pca models .",
    "robpca discovers a different structure in the data , confounding the 0 s as the majority group and the 1 s as outliers on the model space . since only a few 1",
    "s are @xmath119 outliers , almost all of the observations influence the fitted loadings .",
    "in contrast to the other methods , fasthcs correctly identifies all of the 0 s as outliers and identifies some 1 s that might warrant additional scrutiny .      the tablet data  @xcite contains the results of an analysis on escitalopram@xmath187 tablets from the pharmaceutical company h. lundbeck a / s using near - infrared ( nir ) spectroscopy .",
    "the study includes tablets of four different dosages from pilot , laboratory and full scale production settings are included .",
    "each tablet ( the observations ) is measured along 404 wavelengths ( the variables ) . from this data",
    ", we extract two subsets of observations which we combine to obtain a new data set formed of two heterogeneous subgroups . tablets of 80 mg will make up the majority group and the rows corresponding to the first 50 tablets with a nominal weight of 250 mg will serve as the outliers .",
    "this gives a high - dimensional data set ( i.e. @xmath8 ) with @xmath188 , @xmath189 and a contamination rate of @xmath190 .",
    "figure  [ hcs : tablet1 ] , depicts the spectra of the 250 mg ( light orange ) and 80 mg ( dark blue ) tablets .",
    "the spectra of the 250 mg tablets follow a different multivariate pattern than those of the 80 mg tablets .",
    "for example , the spectra of the former are generally lower and more spread out than the spectra of the latter .",
    "@xcite explain that accurate models for nir analyses of medical tablets are valuable for quality control purposes , since they are fast , nondestructive , noninvasive , and require little preparation .",
    "the goal of the algorithms will be to fit a model to the 80 mg tablets , despite the presence in the sample of many 250 mg tablets .",
    "figure  [ hcs : tablet2 ] depicts the diagnostic plots of the scaled outlyingness measures obtained from each of the algorithms . to enhance the distinction between the two groups in our data , we show the 80 mg tablets as ( dark ) blue circles and the 250 mg tablets as ( light ) orange triangles .",
    "these results are similar to those we saw when we examined the multiple features data set .",
    "again pcapp and pcal do not distinguish between the two groups and robpca uses both groups to fit the loadings parameters and confuses the outliers with the majority group on the model space .",
    "in contrast , the diagnostic plot derived from the fasthcs fit establishes that the 250 mg tablets do not follow the same multivariate patterns as 80 mg tablets and , in fact , depart significantly from it . in the plot",
    ", we see that fasthcs assigns the outliers high @xmath119 values ; excluding them from loadings and eigenvalue estimation .",
    "it also assigns many of them high @xmath126 values ; revealing their distance on the model space .      in our final case study",
    ", we examine another high - dimensional data set ; the dna alteration data set  @xcite .",
    "this data set consists of cytosine methylation @xmath191 values collected at 1413 autosomal cpg loci ( the variables ) in a sample of 217 non - pathological human tissue specimens ( the observations ) taken from 10 different anatosites . in @xcite , the authors show that the tissue samples in this data set form three well separated subgroups .",
    "the first of these constitutes all 113 observations corresponding to cytosine methylation @xmath191 values measured on `` non - blood , non - placenta '' ( henceforth , simply `` non - blood '' ) tissues .",
    "a second subgroup of data points comprises the 85 cytosine methylation @xmath191 measurements taken on blood tissues .    in this application",
    ", we will combine the 113 measurements of cytosine methylation @xmath191 values corresponding to the samples `` non - blood '' tissue with 85 measurements taken from blood tissues ( so that @xmath192 and @xmath193 ) . in figure",
    "[ hcs : rd4 ] , we plot the 1413 @xmath191 values corresponding to each blood ( light orange ) and non - blood ( dark blue ) observation .",
    "visually , the curves of these two groups appear difficult to distinguish from one another",
    ". in particular , the vertical range of both overlap and the groups do not exhibit any pronounced difference in the variability of the variables .",
    "the diagnostic plot for pcapp ( figure  [ hcs : geneticdiagnostic ] ) reveals that the fitted model regards blood and non - blood tissue to be quantitatively similar .",
    "robpca and pcal also detect almost none of the outliers , but additionally consider a number of the non - blood observations to be @xmath126 outliers . as in the previous case studies",
    ", fasthcs correctly identifies all of the outliers . as a consequence ,",
    "the parameter estimates corresponding to this model are more likely to reflect the true structure of non - blood tissue than those fitted by the other algorithms .",
    "in this article we introduced fasthcs to satisfy a number of criteria we expect a robust pca method to have . in both the simulations and real data examples we performed , fasthcs met all of these criteria .",
    "in contrast , state - of - the - art methods did not , and often produced results one would expect from a non - robust method .",
    "this may seem like an extreme outcome , but it is in fact the very nature of dealing with outliers : if a method fails to identify them , the resulting model fit is often profoundly changed .",
    "it is interesting to compare the performance of fasthcs and robpca because these methods both use variants of projection pursuit .",
    "while fasthcs compares the fit produced by the @xmath23-index to that from the projection pursuit criterion , robpca relies completely on the projection pursuit criterion to construct its initial subset .",
    "thus , the difference in performance between fasthcs and robpca that we observe in our simulations and real data examples arises from the fact that fasthcs nearly always chooses the @xmath23-index subset over the projection pursuit one .    in most applications ,",
    "admittedly , data settings and contamination patterns will not be as difficult as those we featured in our simulations and real data examples , and in these easier cases the different methods will , hopefully , concur .",
    "nevertheless , in three real data examples from fields where pca is widely used , we were able to establish that real world situations can be challenging enough to push current state - of - the - art outlier detection procedures to their limits and beyond , justifying the development of better solutions . in any case , given that in practice we do not know the configuration of the outliers , as data analysts , we prefer to carry out our inferences while planning for the worst contingencies .",
    "the authors wish to acknowledge the helpful comments from two anonymous referees and the editor which improved this paper .",
    "throughout this appendix , let @xmath81 be an @xmath194 data matrix of uncontaminated observations drawn from a rank @xmath1 distribution @xmath195 , with @xmath1 and integer satisfying @xmath196 .",
    "however , we do not observe @xmath81 but an @xmath194 ( potentially ) corrupted data matrix @xmath197 that consists of @xmath198 observations from @xmath81 and @xmath199 arbitrary values with @xmath200 denoting the ( unknown ) rate of contamination . throughout ,",
    "@xmath201 and the pca estimates @xmath202 are defined as in section   with @xmath203 will denoting the @xmath204-th diagonal entry of @xmath205 .",
    "we will consider the finite sample breakdown  @xcite in the context of pca following  @xcite : @xmath206 equation   defines the so - called finite sample explosion breakdown point and equation   the so - called finite sample implosion breakdown point of pca estimates @xmath207 , and the general finite sample breakdown point is @xmath208 .",
    "the following assumptions ( as per , for example  @xcite ) all pertain to the original , uncontaminated , data set @xmath81 .",
    "we will consider the case whereby the point cloud formed by @xmath81 lies in _ general position _ in @xmath209 .",
    "the following definition of _ general position _ is adapted from @xcite :    definition 1 : _ general position in @xmath209_. @xmath81 is in general position in @xmath209 if no more than @xmath1-points of @xmath81 lie in any @xmath210-dimensional affine subspace . for @xmath1-dimensional data , this means that there are no more than @xmath1 points of @xmath81 on any hyperplane , so that any @xmath211 points of @xmath81 always determine a @xmath1-simplex with non - zero determinant .",
    "if at least @xmath6 rows of @xmath197 are in general position in @xmath209 , any subset of @xmath6 observations will contain at least @xmath211 observations in general position .",
    "this guarantees that the @xmath155 eigenvalue corresponding to any @xmath6-subset is non - zero  @xcite .",
    "thus , it follows that @xmath217 .",
    "denote @xmath218 the outlying entries of @xmath197 and @xmath219 .",
    "the only outliers capable of causing explosion breakdown must satisfy : @xmath220 for any bounded scalar @xmath221 and @xmath222 depending only on the uncontaminated observations .",
    "suppose that the outliers do not satisfy equation   so that @xmath223 , but that the pca estimates @xmath202 break down .",
    "this leads to a contradiction since @xmath224 therefore , for a contaminated @xmath6-subset to cause explosion breakdown , the outliers must satisfy equation  .",
    "assume that an outlier @xmath225 does not satisfy condition  .",
    "@xcite showed that any @xmath6 subset @xmath52 indexing @xmath225 will have an unbounded value of @xmath226 if and only if @xmath227 is unbounded .",
    "but for the uncontaminated data , it holds that @xmath228 so if the contaminated data set @xmath197 contains at least @xmath6 entries from the original data matrix @xmath81 , then it is always possible to construct a subset @xmath52 of entries of @xmath197 for which @xmath229 is bounded so that @xmath52 will never be selected over @xmath230 .",
    "in this appendix , we derive the finite sample breakdown point of fasthcs .",
    "define @xmath81 , @xmath231 and @xmath232 as in appendix a. recall that @xmath233 where @xmath111 . then ,",
    "if @xmath234 or if @xmath115 then the final fasthcs estimates are based on @xmath100 . otherwise , they are based on @xmath235 .    if @xmath236 and @xmath237 , then @xmath238 .",
    "@xcite showed that the population breakdown point of @xmath239 is 50% , which corresponds to a finite sample breakdown point of @xmath240 .",
    "consequently , @xmath100 will not index any data point for which @xmath236 . since @xmath241 indexes the overlap between @xmath76 and @xmath100 , if @xmath236 , then @xmath238 .",
    "when @xmath81 is in general position , @xmath242 , and @xmath243 .",
    "we will proceed by showing that the denominators in equation   are bounded , while only the numerator dependent on @xmath100 is bounded .",
    "lemma 1 implies there exists a fixed constant @xmath244 such that @xmath245 for any orthogonal matrix @xmath246 .",
    "similarly , since the projection pursuit approach has a breakdown point of @xmath240 , there exists a fixed @xmath247 such that @xmath248 as a consequence of and , there exists a fixed constant @xmath249 such that : @xmath250 next , note that @xmath251 ( equation follows from appendix a , theorem 1 ) , so that @xmath252 is not bounded from above .",
    "conversely , @xmath239 has an explosion breakdown point of @xmath240 , so that there exists a fixed @xmath253 such that : @xmath254    from equations   and the unboundedness of   it follows that the left - hand side in equation   is unbounded .",
    "however , by equations   and  , the right - hand side of equation   is bounded from above so that in cases where outliers cause explosion breakdown of @xmath202 , criterion will select @xmath255 .",
    "since the breakdown point of @xmath239 is @xmath240 , we have that @xmath256 .",
    "when @xmath81 is in general position , @xmath242 , and @xmath257 , then @xmath258 .    by appendix",
    "a , theorem 1 , we have that the implosion breakdown point of @xmath202 is @xmath216 . the implosion breakdown point of @xmath239 is @xmath240 , which is higher , so it follows that @xmath259 .    for @xmath260 ,",
    "the finite sample breakdown point of @xmath261 is @xmath262    the finite sample breakdown point of @xmath263 .",
    "given lemmas 2 and 3 , @xmath264 .",
    "the objective of the simulation studies in section  [ mcs : s5 ] is to measure how much the fitted pca parameters @xmath265 obtained by four robust pca methods deviate from the true @xmath266 when they are exposed to outliers .",
    "one way to compare pca fits is with respect to their eigenvectors , as in the _ maxsub _ criterion  @xcite : @xmath267 where @xmath268 is the smallest eigenvalue of the matrix @xmath269 .",
    "the maxsub has an appealing geometrical interpretation as it represents the maximum angle between a vector in @xmath270 and the vector most parallel to it in @xmath271 .",
    "however , it does not exhaustively account for the dissimilarity between two sets of eigenvectors . as an alternative to the _ maxsub _",
    ", @xcite proposes the total dissimilarity : @xmath272 which is an exhaustive measure of dissimilarity for orthogonal matrices . furthermore , because @xmath273 and @xmath274  @xcite , it is readily seen that   is a measure of sphericity of @xmath275 ( it is proportional to the likelihood ratio test statistics for non - sphericity of @xmath275  @xcite ) . however , note that   now forfeits the geometric interpretation enjoyed by the _",
    "maxsub_.    in any case , measures of dissimilarity based solely on eigenvectors , such as the _ maxsub _ or _ sumsub _ , necessarily fail to account for bias in the estimation of the eigenvalues .",
    "this is problematic when used to evaluate robust fits because it is possible for outliers to exert substantially more influence on @xmath261 than on @xmath271 .",
    "an extreme example is given by the so - called good leverage type of contamination in which the outliers lie on the subspace spanned by @xmath270 so that even the classical pca estimate ( whose eigenvalues can be made arbitrarely bad by such outliers ) will have low values of @xmath276 .",
    "in contrast , we are interested in an exhaustive measure of dissimilarity ; one that summarizes the the effects of the outliers on all the parameters of the pca fit into a single number , so that the algorithms can be ranked in terms total dissimilarity . to construct such a measure , it is logical to base it on @xmath277 and its estimate @xmath278 because they contain all the parameters of the fitted model . for our purposes ,",
    "one need to only consider the effects of outliers on @xmath149 , the shape component of @xmath150  @xcite .",
    "this is because to rank the observations in a contaminated sample in terms of their true outlyingness ( and thus reveal the outliers ) , it is sufficient to estimate the shape component of @xmath279 correctly .",
    "consequently , an exhaustive measure of dissimilarity between @xmath280 and @xmath281 is given by @xmath282 , where @xmath283 is any measure of non - sphericity of its argument .",
    "in practice several choices of @xmath283 are possible , the simplest being the condition number of @xmath284 which is defined as the ratio of the largest to the smallest eigenvalue of @xmath284  @xcite , explaining the definition of @xmath285 .",
    "00 bjrck , . and golub , g. h. ( 1973 ) .",
    "numerical methods for computing angles between linear subspaces .",
    "mathematics of computation , 27 , 2 , 579594 .",
    "christensen , b.c houseman , e.a .",
    "marsit , c.j .",
    "zheng , s. wrench , m.r .",
    "wiemels , j.l .",
    "nelson , h.h .",
    "karagas , m.r .",
    "padbury , j.f .",
    "bueno , r. sugarbaker , d.j yeh , r. , wiencke , j.k .",
    "kelsey , k.t .",
    "aging and environemental exposure alter tissue - specific dna methylation dependent upon cpg island context .",
    "plos genetics 5(8 ) , e1000602 .",
    "croux , c. and ruiz - gazen , a. ( 2005 ) .",
    "high breakdown estimators for principal components : the projection - pursuit approach revisited .",
    "journal of multivariate analysis , 95 , 206226 .",
    "donoho , d.l .",
    "breakdown properties of multivariate location estimators . ph.d .",
    "qualifying paper harvard university .",
    "debruyne , m. and hubert , m. ( 2009 ) . the influence function of the stahel - donoho covariance estimator of smallest outlyingness .",
    "statistics & probability letters 79(3 ) , 275282 .",
    "deepayan , s. ( 2008 ) .",
    "lattice : multivariate data visualization with r. springer , new york .",
    "dyrby , m. engelsen , s.b .",
    "nrgaard , l. bruhn , m. and lundsberg nielsen , l. ( 2002 ) .",
    "chemometric quantitation of the active substance in a pharmaceutical tablet using near infrared ( nir ) transmittance and nir ft raman spectra applied spectroscopy 56(5 ) : 579585 .",
    "hubert , m. rousseeuw , p. j. and vanden branden , k. ( 2005 ) .",
    "robpca : a new approach to robust principal components analysis .",
    "technometrics , 47 , 6479 .",
    "hubert , m. , rousseeuw , p. and vakili , k. ( 2014 ) .",
    "shape bias of robust covariance estimators : an empirical study .",
    "statistical papers , volume 55 , issue 1 , pp 1528 .",
    "jensen , d. r. ( 1986 ) , the structure of ellipsoidal distributions , ii .",
    "principal components .",
    "biometrical journal , 28 : 363369 .",
    "jolliffe , i.t .",
    "( 2002 ) . principal component analysis .",
    "springer , new york .",
    "second edition .",
    "krzanowski , w.j .",
    "( 1979 ) . between - groups comparison of principal components .",
    "journal of the american statistical association , vol .",
    "li , g. , chen , z. ( 1985 ) . projection - pursuit approach to robust dispersion matrices and principal components : primary theory and monte carlo .",
    "joural of the american statistical association , 80 , pp .",
    "locantore , n. , marron , j. s. , simpson , d. g. , tripoli , n. , zhang , j. t. , and cohen , k. l. ( 1999 ) .",
    "robust principal component analysis for functional data . test .",
    "8(1 ) , 173 .",
    "maronna r. a. and yohai v.j .",
    "( 1995 ) . the behavior of the stahel - donoho robust multivariate estimator .",
    "journal of the american statistical association 90 ( 429 ) , 330341 .",
    "maronna , r. ( 2005 ) .",
    "principal components and orthogonal regression based on robust scales .",
    "technometrics , 47 , 264273 .",
    "maronna , r. a. ; martin , r. d. and yohai , v. j. ( 2006 ) .",
    "robust statistics : theory and methods .",
    "wiley , new york .",
    "muirhead , r.j .",
    "aspects of multivariate statistical theory .",
    "john wiley and sons , new york .",
    "r core team ( 2014 ) .",
    "r : a language and environment for statistical computing .",
    "r foundation for statistical computing .",
    "vienna , austria .",
    "rousseeuw , p.j . and leroy , a.m. ( 1987 ) .",
    "robust regression and outlier detection .",
    "wiley , new york .",
    "schmitt , e. llerer , v. and vakili , k. ( 2014 ) . the finite sample breakdown point of pcs .",
    "statistics & probability letters , 94 , 214220 .",
    "seber , g. a. f. ( 2008 ) .",
    "matrix handbook for statisticians .",
    "wiley series in probability and statistics .",
    "wiley , new york .",
    "stahel w. ( 1981 ) .",
    "breakdown of covariance estimators .",
    "research report 31 , fachgrupp fr statistik , e.t.h .",
    "todorov v. and filzmoser p. ( 2009 ) .",
    "an object - oriented framework for robust multivariate analysis .",
    "journal of statistical software , 32 , 147 .",
    "tyler , d.e . ( 1994 ) .",
    "finite sample breakdown points of projection based multivariate location and scatter statistics .",
    "vakili , k. and schmitt , e. ( 2014 ) .",
    "finding multivariate outliers with fastpcs .",
    "computational statistics & data analysis , vol .",
    "69 , 5466 .",
    "van breukelen , m. duin , r.p.w .",
    "tax , d.m.j . and den hartog , j.e .",
    "handwritten digit recognition by combined classifiers .",
    "kybernetika , 34 , 381386 .",
    "wu , w. , massart , d. l. , and de jong , s. ( 1997 ) , the kernel pca algorithms forwide data .",
    "part i : theory and algorithms",
    ". chemometrics and intelligent laboratory systems , 36 , 165172 .",
    "yohai , v.j . and maronna , r.a .",
    "the maximum bias of robust covariances .",
    "communications in statistics  theory and methods , 19 , 29252933 ."
  ],
  "abstract_text": [
    "<S> principal component analysis ( pca ) is widely used to analyze high - dimensional data , but it is very sensitive to outliers . </S>",
    "<S> robust pca methods seek fits that are unaffected by the outliers and can therefore be trusted to reveal them . </S>",
    "<S> fasthcs ( high - dimensional congruent subsets ) is a robust pca algorithm suitable for high - dimensional applications , including cases where the number of variables exceeds the number of observations . </S>",
    "<S> after detailing the fasthcs algorithm , we carry out an extensive simulation study and three real data applications , the results of which show that fasthcs is systematically more robust to outliers than state - of - the - art methods .    </S>",
    "<S> _ keywords : _ high - dimensional data , outlier detection , computational statistics , exploratory data analysis </S>"
  ]
}