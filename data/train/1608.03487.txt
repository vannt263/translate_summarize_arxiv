{
  "article_text": [
    "in this paper , we aim at solving the following convex constrained optimization problem : @xmath6 where @xmath7 is a smooth or non - smooth convex function and @xmath8 is a convex function .",
    "the problem can find applications in machine learning , signal processing , statistics , marketing optimization , and etc .",
    "for example , in distance metric learning one needs to learn a positive semi - definite matrix such that similar examples are close to each other and dissimilar examples are far from each other  @xcite , where the positive semi - definite ( psd ) constraint can be cast into a convex inequality constraint .",
    "another example arising in compressive sensing is to minimize the @xmath5 norm of high - dimensional vector subject to a measurement constraint  @xcite .",
    "although general interior - point methods can be applied to solve the problem with linear convergence , they suffer from exceedingly high computational cost per - iteration . another solution is to employ the projected gradient ( pg ) method  @xcite or the conditional gradient ( cg ) method  @xcite , where the pg method needs to compute the projection into the constrained domain at each iteration and cg needs to solve a linear optimization problem under the constraint .",
    "however , for many constraints ( e.g. , psd , a quadratic constraint ) both projection into the constrained domain and the linear optimization under the constraint are time - consuming , which restrict their capabilities to solving these problems .",
    "recently , there emerges a new direction towards addressing the challenge of expensive projection that is to reduce the number of projections . in the seminal paper  @xcite ,",
    "the authors have proposed two algorithms with only one projection at the end of iterations for non - smooth convex and strongly convex optimization , respectively .",
    "the idea of both algorithms is to move the constraint function into the objective function and to control the violation of constraint for intermediate solutions .",
    "while their developed algorithms enjoy an optimal convergence rate for non - smooth optimization ( i.e. , @xmath1 iteration complexity ) and a close - to - optimal convergence rate for strongly convex optimization ( i.e. , @xmath9   suppresses a logarithmic factor . ] ) , there still lack of theory and algorithms with reduced projections for smooth convex optimization . in this paper , we bridge the gap by developing a general theory of optimization with only one projection , which gives an iteration complexity of @xmath0 for smooth optimization with only one projection . beyond that",
    ", we make non - trivial improvements on the convergence rates for both non - smooth and smooth non - stronlgy convex optimization at the price of a logarithmic number of projections . in particular , we show that under a mild local error bound condition , the iteration complexities can be improved to @xmath2 for non - smooth optimization and @xmath3 for smooth optimization , where @xmath4 $ ] is a constant in the local error bound condition . to our knowledge , these are the best convergence rates with only a logarithmic number of projections for non - strongly convex optimization .",
    "[ sec : rw ] the issue of high projection cost in projected gradient descent has received increasing attention in recent years . most work are based on the frank - wolfe technique that eschews the projection in favor of a linear optimization over the constrained domain  @xcite .",
    "it happens that for many bounded domains ( e.g. , bounded balls for vectors and matrices , a psd constraint with a bounded trace norm ) the linear optimization over the constrained domain is much cheaper than projection into the constrained domain  @xcite .",
    "however , there still exist many constraints that render both projection into the constrained domain and linear optimization under the constraint are comparably expensive .",
    "examples include polyhedral constraints , quadratic constraints and a psd constraint .    to tackle these complex constraints , the idea of optimization with reduced number of projections was proposed .",
    "in  @xcite , the authors developed two algorithms for solving non - smooth problems as in  ( [ eqn : prob ] ) , one for non - strongly convex optimization and another for strongly convex optimization , which achieve ( almost ) optimal convergence rates for the two settings in the worst case .",
    "however , it does not address smooth optimization problems . in a recent work",
    "@xcite , the authors studied the smooth and strongly convex optimization and they proposed an algorithm with @xmath10 projections and proved an @xmath11 convergence rate , where @xmath12 is the condition number and @xmath13 is the total number of iterations .",
    "nonetheless , if the condition number is high the number of projections could be very large .",
    "it still remains an open problem to develop optimization algorithms with only one projection for smooth problems .",
    "additionally , if trading a logarithmic number of projections for improved convergence rates is attractive , can we achieve better convergence ? in this paper",
    ", we provide affirmative answers to these questions .",
    "it is worth mentioning that although the present work focuses on deterministic optimization instead of stochastic optimization as considered in  @xcite , the general theory of optimization with only one projection developed here can be easily extended to stochastic optimization by combining with recent advances in stochastic smooth optimization for a finite sum problem  @xcite .",
    "another major contribution of this paper is to develop improved convergence for optimization with reduced projections under the local error bound condition .",
    "several recent works also exploit different forms of error bound conditions to improve the convergence  @xcite .",
    "most notably , the technique used in our work is closely related to  @xcite .",
    "however , for constrained optimization problems the methods in  @xcite still need to conduct projections at each iteration .",
    "we present some preliminaries in this section .",
    "let @xmath14 denote the constrained domain , @xmath15 denote the optimal solution set and @xmath16 denote the optimal objective value .",
    "we denote by @xmath17 the gradient and by @xmath18 the subgradient of a smooth or non - smooth function , respectively . when @xmath7 is a non - smooth function , we consider the problem as non - smooth constrained optimization . when both @xmath7 and @xmath8 are smooth , we consider the problem as smooth constrained optimization .",
    "a function @xmath7 is @xmath19-smooth if it has a lipschitz continuous gradient , i.e. , @xmath20 where @xmath21 denotes the euclidean norm .",
    "a function @xmath7 is @xmath22-strongly convex if it satisfies @xmath23 in the sequel , @xmath24 denotes the distance of @xmath25 to a set @xmath26 , i.e. , @xmath27 .    in the sequel , we make the following assumptions to facilitate the development of our algorithms and theory .    [",
    "ass : rsg ] for a convex minimization problem  ( [ eqn : prob ] ) , we assume ( i ) there exist @xmath28 and @xmath29 such that @xmath30 ; ( ii ) @xmath15 is a non - empty convex compact set ; ( iii ) there exists a positive value @xmath31 such that @xmath32 or more generally there exists a constant @xmath31 for any @xmath33 , such that @xmath34 satisfies @xmath35_+\\end{aligned}\\ ] ] where @xmath36_+$ ] is a hinge operator that is defined as @xmath36_+=s$ ] if @xmath37 , and @xmath36_+=0 $ ] if @xmath38 ; ( iv ) there exists a strictly feasible solution such that @xmath39 ; ( v ) both @xmath7 and @xmath8 are defined everywhere and are lipschitz continuous with their lipschitz constants denoted by @xmath40 and @xmath41 , respectively .",
    "* remark : * we make several remarks about the assumptions . assumptions ( i ) and ( ii ) are only exploited in developing improved convergence rates in section  [ sec : imp ] , where assumption ( i ) supposes there is a lower bound of the optimal value @xmath16 . the inequality in  ( [ eqn : keyi0 ] ) is also assumed in  @xcite , which is to ensure the distance of the final solution before projection to constrained domain @xmath26 is not too large .",
    "note that the inequality in  ( [ eqn : keyi ] ) is a more general condition than  ( [ eqn : keyi0 ] ) as revealed shortly .",
    "we present more discussions about assumption ( iii ) in subsection  [ sec : dis ] and exhibit the value of @xmath42 for a number of commonly seen constraints .",
    "the strict feasibility assumption ( iv ) allows us to explore the kkt condition of the projection problem shown below .",
    "traditional projected gradient descent methods need to solve the following projection at each iteration @xmath43 = \\arg\\min_{\\u\\in\\r^d , c(\\u)\\leq 0}\\|\\u - \\x\\|^2\\end{aligned}\\ ] ] conditional gradient methods ( a.k.a .",
    "the frank - wolfe technique ) need to solve the following linear optimization at each iteration @xmath44 . for many constraint functions ( see examples given below ) , solving the projection problem and",
    "the linear optimization could be very expensive .",
    "we first show that the inequality in  ( [ eqn : keyi ] ) is weaker than  ( [ eqn : keyi0 ] ) since the inequality  ( [ eqn : keyi0 ] ) implies the inequality  ( [ eqn : keyi ] ) , which is stated in the following lemma .",
    "[ lem:0 ] for any @xmath33 , let @xmath45 .",
    "if  ( [ eqn : keyi0 ] ) holds , then  ( [ eqn : keyi ] ) holds .",
    "below , we demonstrate that several commonly seen constraints in practice satisfy the assumption ( iii ) . specially , we discuss three types of constraints , polyhedral constraints , a quadratic constraint and a psd constraint .",
    "[ [ polyhedral - constraints . ] ] polyhedral constraints .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    first , we show that when @xmath8 is a polyhedral function , i.e. , its epigraph is a polyhedron , the inequality in  ( [ eqn : keyi ] ) is satisfied . to this end , we explore the polyhedral error bound ( peb ) condition  @xcite . in particular , if we consider an optimization problem , @xmath46 , where the epigraph of @xmath47 is polyhedron .",
    "let @xmath48 denote the optimal set and @xmath49 denote the optimal value of the problem above .",
    "the peb says that there exists @xmath31 such that for any @xmath33 @xmath50 to show that the inequality  ( [ eqn : keyi ] ) holds for a polyhedral function @xmath8 , we can consider the optimization problem @xmath51_+$ ] . the optimal set of the above problem is given by @xmath52 .",
    "when @xmath53 , @xmath54 is the closest point in the optimal set to @xmath25 .",
    "therefore if @xmath8 is a polyhedral function , by the peb condition there exists a @xmath31 such that @xmath55_+   - \\min_\\x [ c(\\x)]_+ ) = \\frac{1}{\\rho}[c(\\x)]_+\\ ] ] let us consider a concrete example , where the problem has a set of affine inequalities @xmath56 .",
    "there are two methods to encode this into a single constraint function @xmath57 .",
    "the first method is to use @xmath58 , which is a polyhedral function and therefore satisfies  ( [ eqn : keyi ] ) .",
    "the second method is to use @xmath59_+\\|$ ] , where @xmath60_+=\\max(0 , \\a)$ ] and @xmath61 . thus @xmath62_+",
    "=   \\|[c\\x - \\b]_+\\|$ ] .",
    "the inequality in  ( [ eqn : keyi ] ) is then guaranteed by the hoffman s bound and the parameter @xmath42 is given by the minimum non - zero eigenvalue of @xmath63  @xcite .",
    "note that the projection onto a polyhedron is a linear constrained quadratic programming problem , and the linear optimization over a polyhedron is a linear programming problem .",
    "both have polynomial time complexity that would be high if @xmath64 and @xmath65 are large  @xcite .    [",
    "[ quadratic - constraint . ] ] quadratic constraint .",
    "+ + + + + + + + + + + + + + + + + + + + +    a quadratic constraint can take the form of @xmath66 , where @xmath67 and @xmath68 .",
    "such a constraint appears in compressive sensing  @xcite norm . ] , where the goal is to reconstruct a sparse high - dimensional vector @xmath25 from a small number of noisy measurements @xmath69 with @xmath70 .",
    "the corresponding optimization problem is @xmath71 where @xmath72 is an upper bound on the magnitude of the noise . to check the assumption ( iii ) , we note that @xmath73 and @xmath74 .",
    "let us consider that @xmath75 has a full row rank  . ] and denote by @xmath76 , then on the boundary @xmath77 we have @xmath78 and @xmath79 , where @xmath80 is the minimum eigenvalue of @xmath81 .",
    "therefore the assumption ( iii ) is satisfied with @xmath82 .",
    "it is notable that the projection and the linear optimization under the quadratic constraint require solving a quadratic programming problem and therefore could be expensive .",
    "[ [ psd - constraint . ] ] psd constraint .",
    "+ + + + + + + + + + + + + + +    a psd constraint @xmath83 for @xmath84 can be written as an inequality constraint @xmath85 , where @xmath86 denotes the minimum eigen - value of @xmath87 .",
    "the subgradient of @xmath88 when @xmath89 is given by @xmath90 , i.e. , the convex hull of the outer products of normalized vectors in the null space of the matrix @xmath87 .",
    "to show the subgradient is lower bounded , we have @xmath91 which is the set of positive semi - definite matrices that are orthogonal to @xmath87 and have a trace of one .",
    "if the dimension of the null space of @xmath87 is @xmath92 with @xmath93 , we can show that the subgradient of @xmath94 is lower bounded by @xmath95 .",
    "we postpone the details into the appendix . finally , we note that both projection and linear optimization under a psd constraint need to conduct a singular value decomposition , which is time demanding for a large matrix .",
    "in this section , we extend the idea of only one projection proposed in  @xcite to a general theory , and then present optimization algorithms with only one projection for non - smooth and smooth optimization , respectively . to tackle the constraint ,",
    "we introduce a penalty function @xmath96 parameterized by @xmath97 such that @xmath98_+\\\\ h_\\gamma(\\x)&\\leq c\\gamma , \\forall \\x \\text { such that } c(\\x)\\leq 0 \\end{aligned}\\ ] ] where @xmath99 is a constant , @xmath100 is a constant such that @xmath101 .",
    "it is notable that the penalty function @xmath96 will also depends on @xmath100 ; however since it will be set to a constant value , thus the dependence on @xmath100 is omitted .",
    "we will construct such a penalty function @xmath96 for non - smooth and smooth optimization in next subsections .",
    "we propose to optimize the following augmented objective function @xmath102 we can employ any applicable optimization algorithms to optimize @xmath103 pretending that there is no constraint , and finally obtain a solution @xmath104 that is not necessarily feasible . in order to obtain a feasible solution , we perform one projection to get @xmath105 the following theorem allows us to convert the convergence of @xmath104 for @xmath103 to that of @xmath106 for @xmath7 .",
    "[ thm:1 ] let @xmath107 be any iterative optimization algorithm applied to @xmath108 with @xmath13 iterations , which starts with @xmath109 and returns @xmath104 as the final solution .",
    "assume @xmath7 is @xmath40-lipschitz continuous and @xmath107 enjoys the following convergence of @xmath104 for any @xmath33 @xmath110 where @xmath111 when @xmath112 .",
    "then @xmath113 where @xmath114 is an optimal solution to  ( [ eqn : prob ] ) .",
    "* remark : * it is worth mentioning that we omit some constant factors in the convergence bound @xmath115 that are irrelevant to our discussions .",
    "the notation @xmath116 emphasizes that it is a function of @xmath97 and depends on @xmath109 and @xmath25 and it will be referred to as @xmath117 . in the next several subsections , we will see that by carefully choosing the penalty function @xmath96 we are able to provide nice convergence for smooth and non - smooth optimization with only one projection . in the above theorem",
    ", we assume the optimization algorithm @xmath118 is deterministic .",
    "however , a similar result can be extended to a stochastic optimization algorithm , which we will leave to future work for exploration .",
    "first , we consider @xmath119 , which implies that @xmath120",
    ". then @xmath121 and @xmath122 , therefore @xmath123 .",
    "then  ( [ eqn : conv ] ) follows due to @xmath124 .",
    "next , we assume @xmath125 .",
    "inequality  ( [ eqn : key0 ] ) implies that @xmath126_+\\leq f(\\x _ * ) + c\\gamma   +   b_t(\\gamma ; \\x _ * ,   \\x_1)\\end{aligned}\\ ] ] by assumption ( iii ) , we have @xmath127_+\\geq \\rho\\|\\widehat\\x_t - \\widetilde\\x_t\\|$ ] .",
    "combined with  ( [ eqn : b0 ] ) we have @xmath128 where the last inequality follows that fact @xmath129 because the lipschitz property and @xmath130 .",
    "therefore we have @xmath131 finally , we obtain @xmath132      we discuss the application of the general theory of one projection optimization to non - smooth optimization . for non - smooth optimization , we can choose @xmath133_+$ ] , therefore @xmath134 .",
    "we will use subgradient descent as an example to demonstrate the convergence for @xmath7 , though many other optimization algorithms designed for non - smooth optimization are applicable ( e.g. ,  @xcite ) .",
    "the update of subgradient descent method is given by the following for @xmath135 @xmath136 following the standard analysis for subgradient descent , we can establish the convergence of @xmath137 , in particular @xmath138 for a non - strongly convex function @xmath7 and @xmath139 for a @xmath22-strongly convex function @xmath7 .",
    "combining that with theorem  [ thm:1 ] , we have the following convergence result with the omitted proof included in the appendix .",
    "[ cor:1 ] suppose that assumption ( iii ) , ( iv ) hold and @xmath140 .",
    "set @xmath141_+$ ] with @xmath142 .",
    "let  ( [ eqn : sg ] ) run for @xmath13 iterations and @xmath143 .",
    "if @xmath7 is a convex function , we can set @xmath144 and achieve @xmath145 if @xmath7 is a @xmath22-strongly convex function , we can set @xmath146 and achieve @xmath147    * remark : * the convergence rate remains the same as projected subgradient method as applied to  ( [ eqn : prob ] ) even though only one projection is conducted at the end .",
    "the same order of convergence results was established in  @xcite for non - smooth optimization .",
    "it should be noted that the @xmath148 factor for strongly convex optimization can be removed using suffix averaging  @xcite or polynomial - decay averaging  @xcite .",
    "in next section , we will develop improved convergence rates for non - smooth optimization .      for smooth optimization ,",
    "we consider both @xmath7 and @xmath8 to be smooth   is non - smooth but its proximal mapping can be easily solved . ] .",
    "let the smoothness parameter of @xmath7 and @xmath8 be @xmath149 and @xmath150 , respectively . in order to ensure",
    "the augmented function @xmath103 is still a smooth function , we construct the following penalty function @xmath151 the following proposition shows that @xmath96 is a smooth function and obeys the condition in  ( [ eqn : pen ] ) .",
    "[ prop:1 ] suppose @xmath8 is @xmath150-smooth and @xmath41-lipschitz continuous . the penalty function in  ( [ eqn : pens ] ) is a @xmath152-smooth function and satisfies ( i ) @xmath153_+$ ] and ( ii ) @xmath154 , @xmath155 such that @xmath57 .",
    "then @xmath103 is a smooth function and its the smoothness parameter is given by @xmath156 .",
    "next , we will establish the convergence for @xmath7 using the nesterov s optimal accelerated gradient ( nag ) methods .",
    "the update of one variant of nag can be written as follows for @xmath157 @xmath158 where the value of @xmath159 can be set to different values depending on whether @xmath7 is strongly convex or not ( see corollary  [ cor:2 ] ) .",
    "previous work have established the convergence of @xmath160 for @xmath103 , in particular @xmath161 for smooth non - strongly convex optimization and @xmath162 for smooth and strongly convex optimization . by combining these results with theorem  [ thm:1 ] and appropriately setting @xmath97",
    ", we can achieve the following convergence of @xmath106 for @xmath7 .",
    "[ cor:2 ] suppose that assumption ( iii ) , ( iv ) hold , @xmath163 , @xmath164 , @xmath7 is @xmath149-smooth and @xmath8 is @xmath150-smooth",
    ". let  ( [ eqn : nag ] ) run for @xmath13 iterations and @xmath165 . if @xmath7 is convex , we can set @xmath166 , @xmath167 , where @xmath168 with @xmath169 , and achieve @xmath170 if @xmath7 is @xmath22-strongly convex , we can set @xmath171 with @xmath172 and @xmath173 , and achieve @xmath174 as long as @xmath175 .",
    "* remark : * the converges results above indicate an @xmath0 iteration complexity for smooth optimization and @xmath176 with @xmath172 for smooth and strongly convex optimization with only one projection .",
    "in this section , we will show that the iteration complexity can be further reduced for both smooth and non - smooth optimization at a price of a logarithmic number of projections . to facilitate the presentation",
    ", we first introduce some notations .",
    "the @xmath177-sublevel set @xmath178 and @xmath177-level set @xmath179 of @xmath7 are defined as @xmath180 note that assumption ( ii ) guarantees that @xmath178 is also bounded  @xcite .",
    "let @xmath181 denote the closest point in the @xmath177-sublevel set @xmath178 to @xmath182 , i.e. , @xmath183 let @xmath184 denote the closest optimal solution in @xmath15 to @xmath25 , i.e. , @xmath185 .",
    "our development relies on the following key result  @xcite .",
    "we include the proof in the appendix .",
    "[ lem : key ] let @xmath186 .",
    "then for any @xmath182 and @xmath187 we have @xmath188    in order to develop improved convergence , we explore the local error bound condition to further bound @xmath189 .    a problem @xmath190 satisfies a local error bound condition if there exist @xmath4 $ ] and @xmath191 such that for any @xmath192 we have @xmath193 where @xmath15 denotes the optimal set and @xmath16 denotes the optimal value .",
    "* remark : * we would like to remark here that the local error bound condition is a mild condition .",
    "studies have shown that many problems enjoy this condition .",
    "for example , @xcite shows that the kurdyka - ojasiewicz ( kl ) property , a property that is satisfied for many convex functions , implies the local error bound condition .",
    "additionally , @xcite shows that problems that have a polyhedral epigraph satisfy the local error bound condition with @xmath194 .",
    "it is also easy to see that a locally strongly convex functions satisfy the local error bound condition with @xmath195 . in the next two subsections",
    ", we will develop improved convergence for non - strongly convex optimization with a logarithmic number of projections based on the local error bound condition .      to establish an improved convergence for non - smooth optimization",
    ", we develop a new algorithm shown in algorithm 1 based on subgradient descent ( gd ) method , to which we refer as lopgd .",
    "the algorithm runs for @xmath196 epochs and each epoch employs gd for minimizing @xmath137 with a feasible solution @xmath197 as a starting point and @xmath198 iterations of updates . at the end of each epoch , the averaged solution @xmath199 is projected into the constrained domain @xmath26 and the solution @xmath200 will be used as the starting point for next epoch .",
    "the step size @xmath201 is decreased by half every epoch starting from a given value @xmath202 .",
    "the theorem below establishes the iteration complexity of lopgd for finding an @xmath177-optimal solution for  ( [ eqn : prob ] ) and also exhibits the values of @xmath196 , @xmath198 and @xmath202 . to simplify notations , we let @xmath203 and @xmath204 .",
    "[ thm:2 ] suppose assumptions ( i)@xmath205(iv ) hold .",
    "let @xmath206 , @xmath207 and @xmath208 , where @xmath209 and @xmath210 are constants appearing in the local error bound condition .",
    "then there exists at least one @xmath211 such that @xmath212 .",
    "* remark : * since the projection is only conducted at the end of each epoch and the total number of epochs is at most @xmath213 , so the total number of projections is only a logarithmic number .",
    "the iteration complexity in  theorem  [ thm:2 ] is @xmath2 that improves the one in corollary  [ cor:1 ] without strong convexity .",
    "* input * : @xmath214 , @xmath215 , @xmath202 * initialization * : @xmath216 let @xmath217 update @xmath218    let @xmath219 compute @xmath220 $ ] update @xmath221    * input * : @xmath214 and a sequence of numbers @xmath222 , @xmath223 * initialization * : @xmath216 let @xmath224 update @xmath225 update @xmath226    let @xmath227 compute @xmath220 $ ] update @xmath228      similar to non - smooth optimization , we also develop a new algorithm based on nag shown in algorithm 2 , where @xmath229 is the smoothness parameter of @xmath230 and @xmath231 is a sequence with @xmath232 updated as in corollary  [ cor:2 ] .",
    "we refer to this algorithm as lopnag .",
    "the theorem below exhibits the iteration complexity of lopnag and reveals the values of @xmath196 , @xmath223 and @xmath233 . to simplify notations ,",
    "we let @xmath234 .",
    "[ thm:3 ] suppose assumptions ( i)@xmath205(iv ) hold and @xmath7 is @xmath149-smooth and @xmath8 is @xmath150-smooth .",
    "let @xmath235 , @xmath207 and @xmath236 , where @xmath209 and @xmath210 are constants appearing in the local error bound condition .",
    "then there exists at least one @xmath211 such that @xmath212 .",
    "* remark : * it is not difficult to show that the total number of iterations is bounded by @xmath3 , which improves the one in corollary  [ cor:2 ] without strong convexity .",
    "we present an experimental result in this section to demonstrate the effectiveness of the proposed algorithms for solving the compressive sensing problem in  ( [ eqn : cs ] ) .",
    "we generate a synthetic data for testing .",
    "in particular , we generate a random measurement matrix @xmath67 with @xmath237 and @xmath238 .",
    "the entries of the matrix @xmath75 are generated independently with the uniform distribution over the interval @xmath239 $ ] .",
    "the vector @xmath240 is generated with the same distribution at @xmath241 randomly chosen coordinates .",
    "the noise @xmath242 is a dense vector with independent random entries with the uniform distribution over the interval @xmath243 $ ] , where @xmath210 is the noise magnitude and is set to @xmath244 .",
    "finally the vector @xmath245 was obtained as @xmath246 .",
    "we compare with the start - of - the - art optimization algorithm proposed in  @xcite ( i.e. , the nesterov s smoothing plus the nesterov s optimal method for the smoothed problem ) known as nesta .",
    "we use the matlab package of nesta  . for fair comparison",
    ", we also use the fast projection code in the nesta package for conducting projection .",
    "we implement the proposed lopnag algorithm . to handle the unknown smoothness parameter in the proposed algorithm",
    ", we use the backtracking technique  @xcite .",
    "the parameter @xmath97 is initially set to @xmath247 and decreased by half every @xmath248 iterations after a projection and the target smoothing parameter @xmath22 in nesta is set to @xmath249 . for the value of @xmath100 in lopnag , we tune it from its theoretical value to several smaller values and choose the one that yields the fastest convergence .",
    "we report the results in table  [ tab : cs ] , which include different number of iterations , the corresponding number of projections , the recovery error of the found solution compared to the underlying true sparse solution , the objective value ( i.e. , the @xmath5 norm of the found solution ) and the running time .",
    "note that each iteration of nesta requires two projections because it maintains two extra sequence of solutions . from the results",
    ", we can see that lopnag converges significantly faster than nesta .",
    "even with only one projection , we are able to obtain a better solution than that of nesta after running @xmath250 iterations .    [ tab : cs ]",
    "in this paper , we have considered a convex optimization problem subject to a convex inequality constraint .",
    "we have developed a general theory of optimization with only one projection that yields an improved iteration complexity for smooth optimization compared with non - smooth optimization . by exploring the local error bound condition ,",
    "we further develop new algorithms with a logarithmic number of projections and achieve better convergence for both smooth and non - smooth optimization . as a future work",
    ", we plan to explore the application of the general theory to stochastic optimization .",
    "when @xmath57 , @xmath251 .",
    "there is nothing to prove .",
    "therefore we consider @xmath53 and @xmath252 . by kkt conditions",
    ", there exists @xmath253 and @xmath254 such that @xmath255 since @xmath252 , then @xmath256 , @xmath257 and @xmath258 .",
    "therefore , @xmath259 is the same direction as @xmath260 . on the other hand , @xmath261 where the second equality uses the fact that @xmath262 is the same direction as @xmath260 and the last inequality uses the inequality  ( [ eqn : keyi0 ] ) .",
    "we first show that @xmath263    in fact , given any @xmath264 with @xmath265 and @xmath266 , we can show @xmath267 , @xmath268 , @xmath269 and @xmath270 , which means @xmath271 belongs to the set on the right . since the set on the left is the convex hull of all such @xmath271 , the set on the left",
    "is included in the set of the right .    on the other hand , given any element @xmath272 from the set of the right",
    ", we can represent it as @xmath273 where @xmath274 , @xmath275 , @xmath276 for some @xmath277 , @xmath278 and @xmath279 for @xmath280 .",
    "these three properties of @xmath281 imply @xmath282 and @xmath283 so that @xmath272 is a convex combination of some elements of the set on the left .",
    "therefore , the set on the right is included in the set of the left .",
    "next , we want to show @xmath284 it is easy to see that the set on the left is a subset of the set on the right . to show the opposite , given any element @xmath272 from the set of the right",
    ", we consider its eigenvalue decomposition @xmath285 where @xmath286 and @xmath287 and @xmath277 are the eigenvalue and the corresponding eigenvector with @xmath288 .",
    "since @xmath87 is psd , the property @xmath289 implies @xmath290 so that @xmath291 must be zero for @xmath280 . as a result , @xmath273 with @xmath276 being an element in the set on the left .",
    "note that @xmath292 .",
    "this means @xmath272 is in the set on the left also .",
    "if the dimension of the null space of @xmath87 is @xmath92 with @xmath93 then we can write @xmath293 , where @xmath294 is a diagonal matrix with @xmath295 . we can set the constant @xmath42 to be the solution of the following optimization problem .",
    "@xmath296 to simplify the problem , we note that @xmath297 let @xmath298 where @xmath299 and @xmath300 .",
    "because @xmath301 is a diagonal matrix with nonnegative entries , it then leads to that the diagonal entries of @xmath302 are all zeros , as a result @xmath303 and consequentially @xmath304 due to that @xmath305 . as a result , @xmath306 and @xmath307 .",
    "therefore , we get @xmath308 as a result , @xmath309 .",
    "the two inequalities are straightforward to prove .",
    "we prove the smoothness property .",
    "let @xmath310 .",
    "it is not difficult to see that @xmath311 is @xmath312-lipschtiz continuous function .",
    "the gradient of @xmath96 is given by @xmath313 then @xmath314",
    "to prove the corollary , we need the convergence results of subgradient descent methods .",
    "we first prove for non - smooth convex optimization .",
    "let @xmath315 run for @xmath13 iterations .",
    "assume @xmath316 .",
    "then for any @xmath33 @xmath317 where @xmath143 .    since @xmath141_+$",
    "] , we can let @xmath318 . to prove the result for convex optimization , we let @xmath319 , the closest optimal solution in @xmath15 to @xmath109 , in the above convergence",
    ". then we have @xmath320 where the last inequality is due to the value of @xmath321 . then by combining with the result in theorem  [ thm:1 ] ,",
    "we have @xmath322 to the prove convergence for strongly convex optimization , we need the following result .",
    "assume @xmath137 is @xmath22-strongly convex and @xmath316 .",
    "let @xmath323 .",
    "if @xmath324 , we have @xmath325 where @xmath326 .    then the convergence of @xmath106 for @xmath7 is similarly proved .",
    "the following proposition shows the convergence of @xmath137 .",
    "@xcite assume @xmath137 is @xmath327-smooth .",
    "let  ( [ eqn : nag ] ) run for @xmath13 iterations . if @xmath137 is convex , we can set @xmath328 , where @xmath329 with @xmath169",
    ". then for any @xmath33 we have @xmath330 if @xmath137 is @xmath22-strongly convex , we can set @xmath331 .",
    "then for any @xmath33 we have @xmath332    we first prove the convergence for a smooth convex function @xmath7 . from theorem  [ thm:1 ] and the construction of @xmath96 in  ( [ eqn : pens ] ) , we have @xmath333 where @xmath334 . by proposition",
    "[ prop:1 ] , we have @xmath335",
    ". then @xmath336 where the last equality is due to the value of @xmath97 .",
    "next , we prove the convergence for a smooth and strongly convex function @xmath7 .",
    "first , we have @xmath337 note that @xmath338 is not the optimal solution to @xmath137 , hence @xmath339 and we use its lipschitz continuity property where @xmath340 . @xmath341 to avoid clutter",
    ", we will consider the dominating term .",
    "consider @xmath342 . to bound the second term ,",
    "we have @xmath343 consider @xmath13 to be sufficiently larger such that @xmath175 , then @xmath344 therefore @xmath345 similarly we can show the last two terms are dominated by @xmath346 . as a result ,",
    "the proof duplicates that in  @xcite .",
    "we consider @xmath348 , otherwise the conclusion holds trivially . by the first - order optimality conditions of  ( [ eqn : ec ] )",
    ", we have for any @xmath349 , there exists @xmath253 ( the lagrangian multiplier of problem  ( [ eqn : ec ] ) ) @xmath350 let @xmath351 in the first inequality we have @xmath352 we argue that @xmath256 , otherwise @xmath353 contradicting to the assumption @xmath348 .",
    "therefore @xmath354 next we prove that @xmath355 is upper bounded . since @xmath356 where @xmath357 is the closest point to @xmath358 in the optimal set .",
    "let @xmath359 in the inequality of  ( [ eqn : o2 ] ) , we have @xmath360 thus @xmath361 therefore @xmath362 combining the above inequality with  ( [ eqn : b1 ] ) we have @xmath363 which completes the proof .",
    "let @xmath364 .",
    "we assume @xmath365 ; otherwise the result holds trivially .",
    "let @xmath366 denote the closest point to @xmath200 in the sublevel set @xmath178 of @xmath7 .",
    "then @xmath367 , which is because we assume that @xmath365",
    ". we will prove by induction that @xmath368 .",
    "it holds for @xmath369 because of assumption ( i ) .",
    "we assume it holds for @xmath370 and prove it is true for @xmath371 .",
    "we consider the @xmath372-th epoch of lopgd . for",
    "any @xmath33 @xmath373 let @xmath374 .",
    "then @xmath375 since @xmath376_+ = f(\\x_{k-1,\\epsilon}^\\dagger ) \\\\",
    "f(\\xh_k)&= f(\\xh_k ) + \\lambda[c(\\xh_k)]_+\\end{aligned}\\ ] ] then @xmath377_+ - f(\\x_{k-1,\\epsilon}^\\dagger)\\leq   \\underbrace { \\left(\\frac{\\eta_k \\bar g^2}{2 } + \\frac{\\|\\x_{k-1 }",
    "- \\x_{k-1,\\epsilon}^\\dagger\\|^2}{2\\eta_k t}\\right)}\\limits_{b_t}\\end{aligned}\\ ] ] then @xmath378_+\\le f(\\x_{k-1,\\epsilon}^\\dagger ) + b_t\\end{aligned}\\ ] ] then @xmath379 assume @xmath380 ( otherwise the proof is done ) , thus @xmath381 .",
    "then @xmath382 leading to @xmath383 then @xmath384 where the third inequality uses lemma  [ lem : key ] and the last inequality uses the local error bound condition . since we assume @xmath385 .",
    ". then @xmath387 by noting the values of @xmath388 and @xmath389 , we have @xmath390 therefore @xmath391 due to the assumption @xmath392 and @xmath393 . by induction , we therefore show that with at most @xmath394 epochs , we have @xmath395",
    "following a similar analysis and using the convergence of nag and proposition  [ prop:1 ] , we have @xmath396 by using lemma  [ lem : key ] and the local error bound condition , we have @xmath397 plugging the values of @xmath398 , @xmath399 into the above inequality yields @xmath400 then @xmath401 therefore the total number of iterations is @xmath402"
  ],
  "abstract_text": [
    "<S> this paper focuses on convex constrained optimization problems , where the solution is subject to a convex inequality constraint . in particular , we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time - consuming , which render both projected gradient methods and conditional gradient methods ( a.k.a . the frank - wolfe algorithm ) expensive . in this paper , we develop projection reduced optimization algorithms for both smooth and non - smooth optimization with improved convergence rates . </S>",
    "<S> we first present a general theory of optimization with only one projection . </S>",
    "<S> its application to smooth optimization with only one projection yields @xmath0 iteration complexity , which can be further reduced under strong convexity and improves over the @xmath1 iteration complexity established before for non - smooth optimization . </S>",
    "<S> then we introduce the local error bound condition and develop faster convergent algorithms for non - strongly convex optimization at the price of a logarithmic number of projections . in particular , we achieve a convergence rate of @xmath2 for non - smooth optimization and @xmath3 for smooth optimization , where @xmath4 $ ] is a constant in the local error bound condition . an experiment on solving the constrained @xmath5 minimization problem in compressive </S>",
    "<S> sensing demonstrates that the proposed algorithm achieve significant speed - up . </S>"
  ]
}