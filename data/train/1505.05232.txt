{
  "article_text": [
    "deep convolutional neural nets ( cnns ) , pioneered by lecun and collaborators  @xcite , now produce state - of - the - art performance on many visual recognition tasks  @xcite .",
    "an attractive property is that appear to serve as universal feature extractors , either as `` off - the - shelf '' features or through a small amount of `` fine tuning '' .",
    "cnns trained on particular tasks such as large - scale image classification  @xcite _ transfer _ extraordinarily well to other tasks such as object detection  @xcite , scene recognition  @xcite , image retrieval  @xcite , etc @xcite .",
    "* hierarchical chain models : * cnns are hierarchical feed - forward architectures that compute progressively invariant representations of the input image . however , the appropriate level of invariance might be task - dependent .",
    "distinguishing people and dogs requires a representation that is robust to large spatial deformations , since people and dogs can articulate .",
    "however , fine - grained categorization of car models ( or bird species ) requires fine - scale features that capture subtle shape cues .",
    "we argue that a universal architecture capable of both tasks must employ some form of multi - scale features for output prediction .",
    "* multi - scale representations : * multi - scale representations are a classic concept in computer vision , dating back to image pyramids  @xcite , scale - space theory  @xcite , and multiresolution models  @xcite .",
    "though somewhat fundamental notions , they have not been tightly integrated with contemporary feed - forward approaches for recognition .",
    "we introduce multi - scale cnn architectures that use features at multiple scales for output prediction ( fig .",
    "[ fig : splash ] ) . from one perspective ,",
    "our architectures are quite simple .",
    "typical approaches train a output predictor ( e.g. , a linear svm ) using features extracted from a single output layer .",
    "instead , one can train an output predictor using features extracted from _ multiple _ layers .",
    "note that these features come `` for free '' ; they are already computed in a standard feed - forward pass .    *",
    "spatial pooling : * one difficulty with multi - scale approaches is feature dimensionality - the total number of features across all layers can easily reach hundreds of thousands .",
    "this makes training even linear models difficult and prone to over - fitting .",
    "instead , we use marginal activations computed from sum ( or max ) pooling across spatial locations in a given activation layer . from this perspective ,",
    "our models are similar to those that compute multi - scale features with spatial pooling , including multi - scale templates  @xcite , orderless models@xcite , spatial pyramids  @xcite , and bag - of - words  @xcite .",
    "our approach is most related to @xcite , who also use spatially pooled cnn features for scene classification .",
    "they do so by pooling together multiple cnn descriptors ( re)computed on various - sized patches within an image .",
    "instead , we perform a single cnn encoding of the entire image , extracting multiscale features `` for free '' .",
    "* end - to - end training : * our multi - scale model differs from such past work in another notable aspect . our entire model is still a feed - forward cnn that is no longer chain - structured , but a directed - acyclic graph ( dag ) .",
    "dag - structured cnns can still be discriminatively trained in an end - to - end fashion , allowing us to directly learn multi - scale representations .",
    "dag structures are relatively straightforward to implement given the flexibility of many deep learning toolboxes  @xcite .",
    "our primary contribution is the demonstration that structures can capture multiscale features , which in turn allow for transfer learning between coarse and fine - grained classification tasks .",
    "* dag neural networks : * dag - structured neural nets were explored earlier in the context of recurrent neural nets @xcite .",
    "recurrent neural nets use feedback to capture dynamic states , and so typically can not be processed with feed - forward computations .",
    "more recently , networks have explored the use of `` skip '' connections between layers @xcite , similar to our multi - scale connections .",
    "@xcite show that such connections are useful for a single binary classification task , but we motivate multiscale connections through multitask learning : different visual classification tasks require features at different image scales .",
    "finally , the recent state - of - the - art model of @xcite make use of skip connections for training , but does not use them at test - time .",
    "this means that their final feedforward predictor is not a dag .",
    "our results suggest that adding multiscale connections at testtime might further improve their performance .",
    "* overview : * we motivate our multi - scale dag - cnn model in sec .  [ sec : motivation ] , describe the full architecture in sec .",
    "[ sec : approach ] , and conclude with numerous benchmark results in sec .",
    "[ sec : exp ] .",
    "we evaluate multi - scale dag - structured variants of existing cnn architectures ( , caffe  @xcite , deep19  @xcite ) on a variety of scene recognition benchmarks including sun397  @xcite , mit67  @xcite , scene15  @xcite .",
    "we observe a consistent improvement regardless of the underlying cnn architecture , producing state - of - the - art results on all 3 datasets .",
    "in this section , we motivate our multi - scale architecture with a series of empirical analysis . we carry out an analysis on existing cnn architectures , namely caffe and deep19 .",
    "caffe  @xcite is a broadly used cnn toolbox .",
    "it includes a pre - trained model `` alexnet ''  @xcite model , learned with millions of images from the imagenet dataset  @xcite .",
    "this model has 6 conv .",
    "layers and 2 fully - connected ( fc ) layers .",
    "deep19  @xcite uses very small @xmath0 receptive fields , but an increased number of layers  19 layers ( 16 conv . and 3 fc layers ) .",
    "this model produced state - of - the - art performance in ilsvrc-2014 classification challenge  @xcite .",
    "we evaluate both `` off - the - shelf '' pre - trained models on the heavily benchmarked mit indoor scene ( mit67 ) dataset  @xcite , using 10-fold cross - validation .",
    "* image retrieval : * recent work has explored sparse reconstruction techniques for visualizing and analyzing features  @xcite . inspired by such techniques , we use image retrieval to begin our exploration .",
    "we attempt to `` reconstruct '' a query image by finding @xmath1 closest images in terms of l2-distance , when computed with mean - pooled layer - specific activations .",
    "results are shown for two query images and two caffe layers in fig .",
    "[ fig : moti ] .",
    "the florist query image tends to produces better matches when using mid - level features that appear to capture _ objects _ and _ parts_. on the other hand , the church - inside query image tends to produce better matches when using high - level features that appear to capture more global _ scene _ statistics .        *",
    "single - scale classification : * following past work @xcite , we train a linear svm classifier using features extracted from a particular layer .",
    "we specifically train @xmath2 1-vs - all linear classifiers .",
    "we plot the performance of single - layer classifiers in fig .",
    "[ fig : layer_mit67 ] . the detailed parameter options for both caffe model are described later in sec .",
    "[ sec : exp ] . as past",
    "work has pointed out , we see a general increase in performance as we use higher - level ( more invariant ) features .",
    "we do see a slight improvement at each nonlinear activation ( relu ) layer .",
    "this makes sense as this layer introduces a nonlinear rectification operation @xmath3 , while other layers ( such an convolutional or sum - pooling ) are linear operations that can be learned by a linear predictor .        * scale - varying classification : * the above experiment required training @xmath4 1-vs - all classifiers , where @xmath5 is the number of classes and @xmath6 is the number of layers .",
    "we can treat each of the @xmath7 classifiers as binary predictors , and score each with the number of correct detections of its target class .",
    "we plot these scores as a matrix in fig .",
    "[ fig : level_perf ] .",
    "we tend to see groups of classes that operate best with features computed from particular high - level or mid - level layers .",
    "most categories tend to do well with high - level features , but a significant fraction ( over a third ) do better with mid - level features .    *",
    "spatial pooling : * in the next section , we will explore multi - scale features . one practical hurdle to including all features from all layers is the massive increase in dimensionality . here , we explore strategies for reducing dimensionality through pooled features .",
    "we consider various pooling strategies ( sum , average , and max ) , pooling neighborhoods , and normalization post - processing ( with and without l2 normalization ) .",
    "we saw good results with average pooling over all spatial locations , followed by l2 normalization .",
    "specifically , assume a particular layer is of size @xmath8 , where @xmath9 is the height , @xmath10 is the width , and @xmath11 is the number of filter channels .",
    "we compute a @xmath12 feature by averaging across spatial dimensions .",
    "we then normalize this feature to have unit norm .",
    "we compare this encoding versus the unpooled full - dimensional feature ( also normalized ) in fig .",
    "[ fig : full_marg ] .",
    "pooled features always do better , implying dimensionality reduction actually helps performance .",
    "we verified that this phenomena was due to over - fitting ; the full features always performed better on training data , but performed worse on test data .",
    "this suggests that with additional training data , less - aggressive pooling ( that preserves some spatial information ) may perform better .",
    "* multiscale classification : * we now explore multi - scale predictors that process pooled features extracted from multiple layers . as before ,",
    "we analyze `` off - the - shelf '' pre - trained models .",
    "we evaluate performance as we iteratively add more layers .",
    "[ fig : layer_mit67 ] suggests that the last relu layer is a good starting point due to its strong single - scale performance .",
    "fig  [ fig : add_back_caffe ] plots performance as we add previous layers to the classifier feature set .",
    "performance increases as we add intermediate layers , while lower layers prove less helpful ( and may even hurt performance , likely do to over - fitting ) .",
    "our observations suggest that high and mid - level features ( , _ parts _ and _ objects _ ) are more useful than low - features based on _ edges _ or _ textures _ in scene classification .",
    "* multi - scale selection : * the previous results show that adding all layers may actually hurt performance .",
    "we verified that this was an over - fitting phenomena ; additional layers always improved training performance , but could decrease test performance due to over - fitting .",
    "this appears especially true for multi - scale analysis , where nearby layers may encoded redundant or correlated information ( that is susceptible to over - fitting ) .",
    "ideally , we would like to search for the `` optimal '' combination of relu layers that maximize performance on validation data .",
    "since there exists an exponential number of combinations ( @xmath13 for @xmath6 relu layers ) , we find an approximate solution with a greedy forward - selection strategy .",
    "we greedily select the next - best layer ( among all remaining layers ) to add , until we observe no further performance improvement .",
    "as seen in fig .",
    "[ fig : forward_select_caffe ] , the optimal results of this greedy approach rejects the low - level features .",
    "this is congruent with the previous results in fig .",
    "[ fig : add_back_caffe ] .",
    "our analysis strongly suggest the importance ( and ease ) of incorporating multi - scale features for classification tasks . for our subsequent experiments , we use scales selected by the forward selection algorithm on mit67 data ( shown in fig .  [",
    "fig : forward_select_caffe ] ) . note that we use them for all our experimental benchmarks , demonstrating a degree of cross - dataset generalization in our approach .",
    "in this section , we show that the multi - scale model examined in fig .  [",
    "fig : forward_select_caffe ] can be written as a dag - structured , feed - forward cnn .",
    "importantly , this allows for end - to - end gradient - based learning . to do so",
    ", we use standard calculus constructions  specifically the chain rule and partial derivatives  to generalize back - propagation to layers that have multiple `` parents '' or inputs .",
    "though such dag structures have been previously introduced by prior work , we have not seen derivations for the corresponding gradient computations .",
    "we include them here for completeness , pointing out several opportunities for speedups given our particular structure .",
    "* model : * the run - time behavior of our multi - scale predictor from the previous section is equivalent to feed - forward processing of the dag - structured architecture in fig .  [",
    "fig : forward_select_caffe ] . note that we have swapped out a margin - based hinge - loss ( corresponding to a svm ) with a softmax function , as the latter is more amenable to training with current toolboxes .",
    "specifically , typical cnns are grouped into collections of four layers , , conv . ,",
    "relu , contrast normalization ( norm ) , pooling layers ( with the norm and pooling layers being optional ) .",
    "the final layer is usually a @xmath5-way softmax function that predicts one of @xmath5 outputs .",
    "we visualize these layers as a chain - structured `` backbone '' in fig .",
    "[ fig : model ] .",
    "our dag - cnn simply links each relu layer to an average - pooling layer , followed by a l2 normalization layer , which feeds to a fully - connected ( fc ) layer that produces @xmath5 outputs ( represented formally as a @xmath14 matrix ) .",
    "these outputs are element - wise added together across all layers , and the resulting @xmath5 numbers are fed into the final softmax function .",
    "the weights of the fc layers are equivalent to the weights of the final multi - scale @xmath5-way predictor ( which is a softmax predictor for a softmax loss output , and a svm for a hinge - loss output ) .",
    "note that all the required operations are standard modules except for the _",
    "add_.    * training : * let @xmath15 be the cnn model parameters at @xmath16-th layer , training data be ( @xmath17 ) , where @xmath18 is the @xmath19-th input image and @xmath20 is the indicator vector of the class of @xmath18",
    ". then we intend to solve the following optimization problem    @xmath21    as is now commonplace , we make use of stochastic gradient descent to minimize the objective function . for a traditional _ chain _ model , the partial derivative of the output with respect to any one weight can be recursively computed by the chain rule , as described in the back - prop algorithm . * multi - output layers ( relu ) : * our dag - model is structurally different at the relu layers ( since they have multiple outputs ) and the _ add _ layer ( since it has multiple inputs ) .",
    "we can still compute partial derivatives by recursively applying the chain rule , but care needs to be taken at these points .",
    "let us consider the @xmath19-th relu layer in fig .",
    "[ fig : backprop_eq ] .",
    "let @xmath22 be its input , @xmath23 be the output for its @xmath24-th output branch ( its @xmath25 child in the dag ) , and let @xmath26 is the final output of the softmax layer .",
    "the gradient of @xmath26 with respect to the input of the @xmath19-th relu layer can be computed as    @xmath27    where @xmath28 for the example in fig .",
    "[ fig : backprop_eq ] .",
    "one can recover standard back - propagation equations from the above by setting @xmath29 : a single back - prop signal @xmath30 arrives at relu unit @xmath19 , is multiplied by the local gradient @xmath31 , and is passed on down to the next layer below . in our dag , _ multiple _ back - prop signals arrive @xmath32 from each branch @xmath24 , each is multiplied by an branch - specific gradient @xmath33 , and their total sum is passed on down to the next layer .",
    "-th relu . ]    * multi - input layers ( add ) : * let @xmath34 represents the output of a layer with multiple inputs .",
    "we can compute the gradient along the layer by applying the chain rule as follows : @xmath35 one can similarly arrive at the standard back - propagation by setting @xmath29 .    *",
    "special case ( relu ) : * our particular dag architecture can further simplify the above equations",
    ". firstly , it may be common for multiple - output layers to duplicate the same output for each child branch .",
    "this is true of our relu units ; they pass the same values to the next layer in the chain and the current - layer pooling operation .",
    "this means the output - specific gradients are identical for those outputs @xmath36 , which simplifies to @xmath37 this allows us to add together multiple back - prop signals before scaling them by the local gradient , reducing the number of multiplications by @xmath38 .",
    "we make use of this speed up to train our relu layers .",
    "* special case(add ) : * similarly , our multi - input add layer reuses the same partial gradient for each input @xmath39 which simplifies even further in our case to @xmath40 . the resulting back - prop equations that simplify are given by @xmath41 implying that one can similarly save @xmath38 multiplications .",
    "the above equations have an intuitive implementation ; the standard chain - structured back - propagation signal is simply replicated along each of the parents of the add layer .    *",
    "implementation : * we use the excellent matconnet codebase to implement our modifications @xcite .",
    "we implemented a custom add layer and a custom dag data - structure to denote layer connectivity .",
    "training and testing is essentially as fast as the chain model .    * vanishing gradients : * we point out an interesting property of our multiscale models that make them easier to train . vanishing gradients",
    "@xcite refers to the phenomena that gradient magnitudes decrease as they are propogated through layers , implying that lower - layers can be difficult to learn because they receive too small a learning signal . in our dag - cnns ,",
    "lower layers are _ directly _ connected to the output layer through multi - scale connections , ensuring they receive a strong gradient signal during learning .",
    "[ fig : grad ] experimentally verifies this claim .",
    "larger , implying that they receive a stronger supervised signal from the target label during gradient - based learning . ]",
    "we explore dag - structured variants of two popular deep models , caffe  @xcite and deep19  @xcite .",
    "we refer to these models as caffe - dag and deep19-dag .",
    "we evaluate these models on three benchmark scene datasets : sun397  @xcite , mit67  @xcite , and scene15  @xcite . in absolute terms",
    ", we achieve the best performance ever reported on all three benchmarks , sometimes by a significant margin .",
    "* feature dimensionality : * most existing methods that use cnns as feature extractors work with the last layer ( or the last fully connected layer ) , yielding a feature vector of size 4096",
    ". forward feature selection on caffe - dag selects layers @xmath42 , making the final multiscale feature 9216-dimensional .",
    "deep19-dag selects layers@xmath43 , for a final size of 6144 .",
    "we perform feature selection by cross - validating on mit67 , and use the same multiscale structure for all other datasets .",
    "dataset - dependant feature selection may further improve performance .",
    "our final multiscale dag features are _ only 2x larger _ than their single - scale counter part , making them practically easy to use and store .",
    "* training : * we follow the standard image pre - processing steps of fixing the input image size to @xmath44 by scaling and cropping , and subtracting out the mean rgb value ( computed on imagenet ) .",
    "we initialize filters and biases to their pre - trained values ( tuned on imagenet ) and initialize multi - scale fully - connected ( fc ) weights to small normally - distributed numbers .",
    "we perform 10 epochs of learning .",
    "* baselines : * we compare our dag models to published results , including two additional baselines .",
    "we evaluate the best single - scale `` off - the - shelf '' model , using both caffe and deep19 .",
    "we pass l2-normalized single - scale features to liblinear  @xcite to train @xmath5-way one - vs - all classifiers with default settings .",
    "finally , sec .",
    "[ sec : diag ] concludes with a detailed diagnostic analysis comparing off - the - shelf and fine - tuned versions of chain and dag structures .",
    "* sun397 : * we tabulate results for all our benchmark datasets in table  [ table : all ] , and discuss each in turn .",
    "sun397  @xcite is a large scene recognition dataset with 100k images spanning 397 categories , provided with standard train - test splits .",
    "our dag models outperform their single - scale counterparts . in particular",
    ", deep19-dag achieves the highest @xmath45 accuracy .",
    "our results are particularly impressive given that the next - best method of  @xcite ( with a score of @xmath46 ) makes use of a imagenet - trained cnn and a custom - trained cnn using a new 7-million image dataset with 400 scene categories .    * mit67 : * mit67 consists of 15k images spanning 67 indoor scene classes  @xcite , provided with standard train / test splits .",
    "indoor scenes are interesting for our analysis because some scenes are well characterized by high - level spatial geometry (  church and cloister ) , while others are better described by mid - level objects (  wine celler and operating room ) in various spatial configurations .",
    "we show qualitative results in fig .",
    "[ fig : more_eg ] .",
    "deep19-dag produces a classification accuracy of @xmath47 , reducing the best - previously reported error  @xcite by * 23.9%*. interestingly  @xcite also uses multi - scale cnn features , but do so by first extracting various - sized patches from an image , rescaling each to canonical size .",
    "single - scale cnn features extracted from these patches are then vector - quantized into a large - vocabulary codebook , followed by a projection step to reduce dimensionality .",
    "our multi - scale representation , while similar in spirit , is an end - to - end trainable model that is computed `` for free '' from a single ( dag ) cnn .",
    "* scene15 : * the scene15  @xcite includes both indoor scene ( , store and kitchen ) and outdoor scene ( , mountain and street ) .",
    "it is a relatively small dataset by contemporary standards ( 2985 test images ) , but we include here for completeness .",
    "performance is consistent with the results above .",
    "our multi - scale dag model , specifically deep19-dag , outperforms all prior work . for reference ,",
    "the next - best method of  @xcite uses a new custom 7-million image scene dataset for training .      in this section , we analyze `` off - the - shelf '' ( ots ) and `` fine - tuned '' ( ft ) versions of both single - scale chain and multi - scale dag models .",
    "we focus on the caffe model , as it is faster and easier for diagnostic analysis .",
    "* chain : * chain - ots uses single - scale features extracted from cnns pre - trained on imagenet .",
    "these are the baseline caffe results presented in the previous subsections .",
    "chain - ft trains a model on the target dataset , using the pre - trained model as an initialization . this can be done with standard software packages  @xcite . to ensure consistency of analysis , in both cases features",
    "are passed to a k - way multi - class svm to learn the final predictor .",
    "* dag : * dag - ots is obtained by fixing all internal filters and biases to their pre - trained values , and only learning the multiscale fully - connected ( fc ) weights .",
    "because this final stage learning is a convex problem , this can be done by simply passing off - the - shelf multi - scale features to a convex linear classification package ( e.g. , svm ) .",
    "we compare this model to a fine - tuned version that is trained end - to - end , making use of the modified backprop equation from sec .",
    "[ sec : approach ] .",
    "* comparison : * fig .",
    "[ fig : comp_otf ] compares off - the - shelf and fine - tune variants of chain and dag models .",
    "we see two dominant trends .",
    "first , as perhaps expected , fine - tuned ( ft ) models consistently outperform their off - the - shelf ( ots ) countparts .",
    "even more striking is the large improvement from chain to dag models , indicating the power of multi - scale feature encodings .",
    "* dag - ots : * perhaps most impressive is the strong performance of dag - ots . from a theoretical perspective , this validates our underyling hypothesis that multi - scale features allow for better transfer between recognition tasks  in this case , imagenet and scene classification . an interesting question is whether multi - scale features , when trained with gradient - based dag - learning on imagenet , will allow for even more transfer .",
    "we are currently exploring this .",
    "however even with current cnn architectures , our results suggest that _ any system making use of off - the - shelf cnn features should explore multi - scale variants as a  cheap \" baseline . _ compared to their single - scale counterpart , multiscale features require no additional time to compute , are only a factor of 2 larger to store , and consistently provide a noticeable improvement .",
    "* conclusion : * we have introduced multi - scale cnns for image classification .",
    "such models encode scale - specific features that can be effectively shared across both coarse and fine - grained classification tasks .",
    "importantly , such models can be viewed as dag - structured feedforward predictors , allowing for end - to - end training . while fine - tuning helps performance ,",
    "we empirically demonstrate that even `` off - the - self '' multiscale features perform quite well .",
    "we present extensive analysis and demonstrate state - of - the - art classification performance on three standard scene benchmarks , sometimes improving upon prior art by a significant margin ."
  ],
  "abstract_text": [
    "<S> we explore multi - scale convolutional neural nets ( cnns ) for image classification . </S>",
    "<S> contemporary approaches extract features from a single output layer . by extracting features from multiple layers </S>",
    "<S> , one can simultaneously reason about high , mid , and low - level features during classification . </S>",
    "<S> the resulting multi - scale architecture can itself be seen as a feed - forward model that is structured as a directed acyclic graph ( dag - cnns ) . </S>",
    "<S> we use dag - cnns to learn a set of multiscale features that can be effectively shared between coarse and fine - grained classification tasks . while fine - tuning such models helps performance , </S>",
    "<S> we show that even `` off - the - self '' multiscale features perform quite well . </S>",
    "<S> we present extensive analysis and demonstrate state - of - the - art classification performance on three standard scene benchmarks ( sun397 , mit67 , and scene15 ) . in terms of the heavily benchmarked mit67 and scene15 datasets , </S>",
    "<S> our results reduce the lowest previously - reported error by * 23.9% * and * 9.5% * , respectively . </S>"
  ]
}