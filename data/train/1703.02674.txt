{
  "article_text": [
    "a variety of applications share the core task of selecting a subset of columns from a short , wide matrix @xmath3 with @xmath4 rows and @xmath5 columns",
    ". the criteria for selecting these columns typically aim at preserving information about the span of @xmath3 while generating a well - conditioned submatrix .",
    "classical and recent examples include experimental design , where we select observations or experiments  @xcite ; preconditioning for solving linear systems and constructing low - stretch spanning trees ( here @xmath3 is a version of the node - edge incidence matrix and we select edges in a graph )  @xcite ; matrix approximation  @xcite ; feature selection in @xmath0-means clustering  @xcite ; sensor selection  @xcite and graph signal processing  @xcite .    in this work ,",
    "we study an elegant randomized approach that holds promise for all of these applications .",
    "this approach relies on sampling columns of @xmath3 according to a probability distribution defined over its submatrices : the probability of selecting a set @xmath6 of @xmath0 columns from @xmath3 , with @xmath7 , is @xmath8 where @xmath9 is the submatrix consisting of the selected columns .",
    "this distribution is reminiscent of _ volume sampling _",
    ", where @xmath10 columns are selected with probability proportional to the determinant @xmath11 of a @xmath12 matrix , i.e. , the squared volume of the parallelepiped spanned by the selected columns .",
    "( volume sampling does _ not _ apply to @xmath13 as the involved determinants vanish . ) in contrast , @xmath14 uses the determinant of an @xmath15 matrix and uses the volume spanned by the _ rows _ formed by the selected columns .",
    "hence we refer to @xmath14-sampling as _ dual volume sampling ( dvs)_.    [ [ contributions . ] ] contributions .",
    "+ + + + + + + + + + + + + +    despite the ostensible similarity between volume sampling and dvs , and despite the many practical implications of dvs outlined below , efficient algorithms for dvs are not known and were raised as open questions in @xcite . in this work ,",
    "we make two key contributions :    =2em    we develop polynomial - time randomized sampling algorithms and their derandomization for dvs .",
    "surprisingly , our proofs require only elementary ( but involved ) matrix manipulations .",
    "we establish that @xmath14 is a _ strongly rayleigh _",
    "measure @xcite , a remarkable property that captures a specific form of negative dependence .",
    "our proof relies on the theory of real - stable polynomials , and the ensuing result implies a provably fast - mixing , practical mcmc sampler .",
    "moreover , this result implies concentration properties for dual volume sampling .",
    "the selection of @xmath16 columns from a short and wide matrix has many applications .",
    "our algorithms for dvs hence have several implications and connections ; we note a few below .",
    "* experimental design .",
    "* the theory of optimal experiment design explores several criteria for selecting the set of columns ( experiments ) @xmath6 .",
    "popular choices are @xmath17 here , @xmath18 denotes the moore - penrose pseudoinverse of @xmath3 , and @xmath6 the minimization ranges over all @xmath6 such that @xmath9 has full row rank @xmath4 .",
    "a - optimal design , for instance , is statistically optimal for linear regression @xcite .",
    "finding an optimal solution for these design problems is np - hard ; and most discrete algorithms use local search @xcite .",
    "3.1 ) show that dual volume sampling yields an approximation guarantee for both a- and e - optimal design : if @xmath6 is sampled from @xmath14 , then @xmath19 \\le {",
    "m - n + 1\\over k - n+1}\\|a^\\dagger\\|_f^2;\\quad   \\mathbb{e}\\left[\\|a_s^\\dagger\\|_2 ^ 2\\right ] \\le \\left(1 + { n(m - k)\\over k - n+1}\\right ) \\|a^\\dagger\\|_2 ^ 2.\\end{aligned}\\ ] ] @xcite provide a polynomial time sampling algorithm only for the case @xmath20 .",
    "our sampling algorithms achieve the bound in expectation , and the derandomization in section  [ sec : derand ] achieves the bound deterministically .",
    "@xcite recently ( in parallel ) achieved approximation bounds for a - optimality via a different algorithm combining a convex relaxation and greedy method .",
    "other methods for experimental design includes leverage score sampling  @xcite and predictive length sampling  @xcite .    * low - stretch spanning trees and applications . * objectives [ eq : opt ] also arise in the construction of low - stretch spanning trees , which have important applications in graph sparsification , preconditioning and solving symmetric diagonally dominant ( sdd ) linear systems  @xcite , among others @xcite . in the node - edge incidence matrix @xmath21 of an undirected graph @xmath22 with @xmath4 nodes and @xmath23 edges ,",
    "the column corresponding to edge @xmath24 is @xmath25 .",
    "let @xmath26 be the svd of @xmath27 with @xmath28 .",
    "the stretch of a spanning tree @xmath29 in @xmath22 is then given by @xmath30 @xcite . in those applications ,",
    "we hence search for a set of edges with low stretch .",
    "* network controllability . *",
    "the problem of sampling @xmath31 columns in a matrix also arise in network controllability problems .",
    "for example , in @xcite , the authors consider selecting control nodes @xmath6 ( under certain constraints ) over time in complex networks to control a linear time - invariant network . after transforming the problem into a column subset selection problem from a short and wide controllability matrix ,",
    "the objective becomes essentially an e - optimal design problem , for which the authors use greedy heuristics .",
    "we focus on a specific , widely applicable setup of column subset selection : _ picking columns from a short and wide matrix _ ( to optimize certain criteria ) .",
    "this setup was analyzed by  @xcite where they minimize a norm of the pseudoinverse of the induced submatrix .",
    "formally , let @xmath32 with @xmath5 be a full row - rank matrix , and @xmath0 a sampling parameter with @xmath2 .",
    "let @xmath33 denote the rank of @xmath3 , thus @xmath34 .",
    "let @xmath35 denote the submatrix of @xmath3 with rows and columns indexed by @xmath36 $ ] and @xmath37 $ ] , respectively .",
    "for brevity we use @xmath9 to denote columns of @xmath3 indexed by @xmath37 $ ] .",
    "the objective is to find a subset @xmath38 $ ] , such that @xmath39 @xmath40 denotes the frobenius norm or spectral norm , respectively , and @xmath18 denotes the moore - penrose pseudoinverse of @xmath3 .",
    "the initial motivation of  @xcite for using this objective was to find low - stretch spanning trees in an undirected graph  @xcite , a problem that reduces to   with @xmath41 .",
    "however , problem   arises much more widely . for example",
    ", it is known that a solution to   is the statistically optimal solution for experimental design in linear regression  @xcite .",
    "additional applications include low - rank matrix approximation  @xcite , feature selection in @xmath0-means clustering  @xcite and sensor selection  @xcite , among many others .",
    "given its many applications , finding efficient methods to solve   is valuable .",
    "a promising approach advanced in  @xcite is a sampling algorithm based on determinants of submatrices .",
    "although @xcite refer to their approach as `` volume sampling , '' it is different from the original volume sampling method of  @xcite  therefore , to avoid confusion , we will refer to the method as _ dual volume sampling ( dvs ) _",
    "( see  [ sec : setup ] ) .",
    "it has been shown that dvs solves   well .",
    "specifically , if we sample a subset @xmath6 of cardinality @xmath31 via dvs , then the approximation guarantees hold @xmath19 \\le { m - n + 1\\over k - n+1}\\|a^\\dagger\\|_f^2;\\quad   \\mathbb{e}\\left[\\|a_s^\\dagger\\|_2 ^ 2\\right ] \\le \\left(1 + { n(m - k)\\over k - n+1}\\right ) \\|a^\\dagger\\|_2 ^ 2.\\end{aligned}\\ ] ] unfortunately , @xcite provide a polynomial time sampling algorithm only for the case @xmath20 .",
    "no polynomial time algorithm for dvs for the more important setting @xmath13 is currently known .",
    "further , no _",
    "derandomized _ algorithm that satisfies the dvs bounds   deterministically is known either .",
    "@xcite raise development of efficient algorithms for both these problems as open problems .",
    "the main focus of this paper is to provide positive answers to these open problems . as a result",
    ", dvs becomes a practical method for column subset selection ( in the harder @xmath42 regime ) .",
    "in particular , we provide two polynomial time sampling methods for dvs : ( i ) an exact algorithm polynomial in @xmath23 , @xmath4 ; and ( ii ) an efficient approximate sampling algorithm via markov chains .",
    "moreover , we also present a derandomized version of our exact sampling algorithm that deterministically samples a subset @xmath6 satisfying  .",
    "our exact sampling algorithm uses a perturbation argument to compute the partition function and marginal probabilities .",
    "this computation , if done naively , will take time exponential in @xmath0 .",
    "we refer to the perturbation as the `` _",
    "_ @xmath43-trick _ _ '' .",
    "further , based on computation of marginal probabilities , we propose a derandomization of our first algorithm to obtain a method that samples @xmath6 deterministically . to obtain our fast mixing markov chain sampler",
    ", we appeal to the powerful theory of stable polynomials and _ strongly rayleigh _",
    "measures  @xcite .",
    "in particular , we first establish that the measure induced by dual volume sampling is ( homogeneously ) strongly rayleigh .",
    "thereafter , fast mixing follows from the recent result of  @xcite , that provides a fast mixing ( for suitably initialized ) markov chain for sampling from strongly rayleigh measures .",
    "* contributions . * in summary ,",
    "the key contributions of this paper are :    1 .",
    "the _ first _ polynomial - time exact dvs algorithm ( section  [ sec : dualvol ] ) 2 .",
    "the _ first _ polynomial - time derandomized dvs algorithm that deterministically generates a subset @xmath6 satisfying   ( section  [ sec : derand ] ) .",
    "3 .   a fast - mixing approximate dvs sampler built on markov chains for strongly rayleigh measures ;",
    "this yields a method that runs in time _",
    "linear _ in the dataset size @xmath23 ( section  [ sec : mcmc ] ) .",
    "[ [ notation . ] ] notation . + + + + + + + + +    from a matrix @xmath44 with @xmath45 columns , we sample a set @xmath38 $ ] of @xmath0 columns ( @xmath7 ) , where @xmath46 : = \\{1,2,\\ldots , m\\}$ ] .",
    "we denote the singular values of @xmath3 by @xmath47 , in decreasing order .",
    "we will assume @xmath3 has full row rank @xmath48 , so @xmath49 .",
    "we also assume that @xmath50 for every @xmath38 $ ] where @xmath51 .",
    "by @xmath52 , we denote the @xmath0-th elementary symmetric polynomial of @xmath3 , i.e. , the @xmath0-th coefficient of the characteristic polynomial @xmath53 .    similarly , one can define the distribution @xmath54 induced by _ volume sampling _ , that is , @xmath55 in this case , one samples a subset @xmath38 $ ] such that @xmath56 .",
    "observe that while ostensibly similar , these distributions are quite different : one is for sampling @xmath51 and the other for @xmath56 .",
    "both are related to volumes induced by submatrices  @xmath14 is proportional to the square of the volume of the parallelepiped spanned by the _ rows _ of @xmath9 , while @xmath54 is proportional to the volume spanned by the _ columns _ of @xmath9 .",
    "these differences and connections make @xmath14 closely related yet different from @xmath54 , which explains our choice of name `` dual volume sampling . ''",
    "we describe in this section our method to sample from the distribution @xmath14 .",
    "our first method relies on the key insight that , as we show , the marginal probabilities for dvs can be computed in polynomial time . to demonstrate this , we begin with the partition function and then derive marginals .",
    "the partition function has a convenient simple closed form , which follows from the cauchy - binet formula and was also derived in @xcite .",
    "[ lem : partition ] for @xmath32 with @xmath34 and @xmath57 , we have @xmath58}\\det(a_s a_s^\\top ) = \\binom{m - n}{k - n}\\det(a a^\\top).\\ ] ]    next , we will need the marginal probability @xmath59 that a given set @xmath60 $ ] is a subset of the random set @xmath6 . in the following theorem",
    ", the set @xmath61\\setminus t$ ] denotes the ( set ) complement of @xmath29 , and @xmath62 denotes the orthogonal complement of @xmath63 .",
    "[ thm : marginal ] let @xmath64 $ ] , @xmath65 , and @xmath66 .",
    "let @xmath67 be the singular value decomposition of @xmath68 where @xmath69 , and @xmath70 .",
    "further define the matrices @xmath71 q^\\top a_{t_c } \\in\\mathbb{r}^{r(a_t)\\times ( m-|t|)}.\\end{aligned}\\ ] ] let @xmath72 be the eigenvalue decomposition of @xmath73 where @xmath74 .",
    "moreover , let @xmath75 $ ] and @xmath76 .",
    "then the marginal probability of @xmath29 in dvs is @xmath77 \\times \\left[\\prod_{j=1}^{r(b)}\\sigma_j^2(b)\\right ] \\times \\gamma \\over z_a}.\\end{aligned}\\ ] ]    we prove theorem  [ thm : marginal ] via a perturbation argument that connects dvs to volume sampling .",
    "specifically , observe that for @xmath78 and @xmath79 it holds that @xmath80 carefully letting @xmath81 bridges volumes with `` dual '' volumes .",
    "the technical remainder of the proof further relates this equality to singular values , and exploits properties of characteristic polynomials .",
    "a similar argument yields an alternative proof of lemma  [ lem : partition ] .",
    "we show the proofs in detail in appendix  [ app : sec : partition ] and [ app : sec : marginal ] respectively .    in this section ,",
    "we show how to compute the partition function and marginal distributions for @xmath82 .",
    "we base our analysis on a simple perturbation argument , which we call the _ @xmath43-trick_.",
    "this serves as a bridge between dual volume sampling and volume sampling , based on which we can perform the desired computations in polynomial time .",
    "theorem  [ thm : partition ] illustrates the @xmath43-trick , which is subsequently used in theorem  [ thm : marginal ] to obtain polytime computation of the marginal distribution .",
    "this theorem is the key building block for our final polynomial time exact dvs algorithm of section  [ sec : dualvol ] .",
    "before we dive into partition functions and marginals , we recall two easily verified facts about determinants that will be useful in our analysis : @xmath83    the partition function of @xmath82 , happens to have a pleasant closed - form formula . although this formula is known  @xcite , and follows immediately by an application of the cauchy - binet identity , we present an alternative proof based on the @xmath43-trick perturbation for its conceptual value and subsequent use .",
    "[ thm : partition ] given @xmath32 where @xmath34 and @xmath57 , we have @xmath84}\\det(a_s a_s^\\top ) = \\left(\\begin{array}{c}m - n\\\\k - n\\end{array}\\right ) \\det(a a^\\top).\\end{aligned}\\ ] ]    first note that for @xmath57 and any @xmath66 , by   we have @xmath85 taking limits as @xmath86 on both sides we have @xmath87 let us focus on @xmath88 .",
    "we construct an identity matrix @xmath89 , then we have @xmath90}\\right ) .",
    "\\end{split}\\ ] ] in other words , this value is proportional to the probability of sampling columns from @xmath91}$ ] using volume sampling .",
    "therefore , using the definition of @xmath92 we have @xmath93}&\\det(a_s^\\top a_s + { \\varepsilon}i_k ) = { 1\\over { \\varepsilon}^{k - n } } e_k ( a^\\top a + { \\varepsilon}i_m)\\\\ & = { 1\\over { \\varepsilon}^{k - n } } e_k ( \\operatorname{diag}([(\\sigma_1 ^ 2(a ) + { \\varepsilon } ) , ( \\sigma_2 ^ 2(a ) + { \\varepsilon}),\\ldots , ( \\sigma_n^2(a ) + { \\varepsilon } ) , { \\varepsilon},\\ldots,{\\varepsilon}]))\\\\ & = \\binom{m - n}{k - n}\\prod\\nolimits_{i=1}^n ( \\sigma_i^2(a ) + { \\varepsilon } ) + o({\\varepsilon}).\\end{aligned}\\ ] ] now taking the limit as @xmath86 we obtain @xmath84}\\det(a_s a_s^\\top ) & = \\lim_{{\\varepsilon}\\to 0}\\binom{m - n}{k - n}\\prod\\nolimits_{i=1}^n ( \\sigma_i^2(a ) + { \\varepsilon } ) + o({\\varepsilon } ) = \\binom{m - n}{k - n}\\det(a a^\\top).\\end{aligned}\\ ] ]    we can see that in this proof , the @xmath43-trick works as a bridge connecting dual volume sampling and volume sampling , whose partition function is easy to compute .",
    "this perturbation trick proves essential in computing the marginal distribution , as shown below .",
    "[ thm : marginal ] let @xmath64 $ ] , @xmath65 , and let @xmath94\\backslash t$ ] . we write @xmath95 as the singular value decomposition of @xmath68 where @xmath96 , @xmath97 be the complement column space of @xmath98 . further we define the two matrices @xmath99 q_t^\\top a_{t_c } \\in\\mathbb{r}^{r(a_t)\\times ( m-|t|)}.\\end{aligned}\\ ] ]",
    "let @xmath100 be the eigenvalue decomposition of @xmath101 where @xmath102 .",
    "moreover , let @xmath103 $ ] and @xmath104 .",
    "then the marginal probability of sampling @xmath29 via dual volume sampling is given by @xmath105 \\times \\left[\\prod_{j=1}^{r(b_{t_c})}\\sigma_j^2(b_{t_c})\\right ] \\times \\gamma_t \\over \\binom{m - n}{k - n}\\det(a a^\\top)}.\\end{aligned}\\ ] ]    the proof is an application of the @xmath43-trick ; due to its length we defer it to appendix  [ app : sec : marginal ] .    [ [ complexity . ] ] complexity .",
    "+ + + + + + + + + + +    the numerator of @xmath106 in theorem  [ thm : marginal ] requires @xmath107 time to compute the first term , @xmath107 to compute the second and @xmath108 to compute the third .",
    "the denominator takes @xmath107 time , amounting in a total time of @xmath108 to compute the marginal probability .",
    "the marginal probabilities derived above directly yield a polynomial - time _ exact _",
    "dvs algorithm . instead of @xmath0-sets",
    ", we sample ordered @xmath0-tuples @xmath109^k$ ] .",
    "we denote the @xmath0-tuple variant of the dvs distribution by @xmath110 : @xmath111 sampling @xmath112 is now straightforward . at the @xmath113th step we sample @xmath114 via @xmath115 ; these probabilities are easily obtained from the marginals in theorem  [ thm : marginal ] .",
    "[ coro : cond ] let @xmath116 , and @xmath106 as in thm .",
    "[ thm : marginal ] . then , @xmath117 as a result , it is possible to draw an exact dual volume sample in time @xmath118 .",
    "the full proof may be found in the appendix .",
    "the running time claim follows since the sampling algorithm invokes @xmath119 computations of marginal probabilities , each costing @xmath108 time .",
    "we are now ready to state our polynomial time exact dvs algorithm . instead of focusing on @xmath0-subsets",
    ", the key idea here is to use dvs for @xmath0-tuples , and to decompose the tuple distribution into a product of conditional distributions that allow us to recover the desired @xmath0-subset . a similar strategy",
    "is followed by  @xcite .",
    "denote @xmath0-tuple dvs distribution as @xmath110 , and let @xmath109^k$ ] be the sampled tuple .",
    "since each permutation of the @xmath120 is equally likely , it follows that @xmath121 the joint probability of the tuple can be decomposed into a product of conditional probabilities as @xmath122 given these conditional probabilities , sampling @xmath123 is straightforward : in the @xmath113-th iteration , we calculate the conditional probability of @xmath124 given @xmath125 for each @xmath126 $ ] , and then sample @xmath127 with this conditional probability .",
    "the full algorithm is outlined as algorithm  [ algo : dualvol ] .",
    "thus , a polynomial - time algorithm for computing conditional distributions directly implies a polynomial - time _",
    "exact _ dual volume sampling algorithm .",
    "the computation of the conditional probability is based on theorem  [ thm : marginal ] , and is given in the following corollary :    * input : * matrix @xmath32 to sample columns from , @xmath2 the target size * output : * set @xmath6 such that @xmath128 and @xmath129 .",
    "initialize @xmath112 as empty tuple compute the conditional distribution @xmath130 ( corollary  [ coro : cond ] ) sample @xmath131 with probability proportional to @xmath132 @xmath133 output @xmath112 as a set @xmath6 .",
    "[ coro : cond ] let @xmath116 , then    @xmath134 \\times \\left[\\prod_{j=1}^{r(b_{t_c\\backslash\\{i\\}})}\\sigma_j^2(b_{t_c\\backslash\\{i\\}})\\right ] \\times \\gamma_{t\\cup\\{i\\ } } \\over ( k - t+1)\\left[\\prod_{i=1}^{r(a_t)}\\sigma_i^2(a_t)\\right ] \\times \\left[\\prod_{j=1}^{r(b_{t_c})}\\sigma_j^2(b_{t_c})\\right ] \\times \\gamma_t}.\\end{aligned}\\ ] ]    let @xmath116 . by the definition of conditional probability and theorem  [ thm : marginal ] we have @xmath135,r\\cap t = \\emptyset}\\det(a_{t\\cup\\{i\\}\\cup r}a_{t\\cup\\{i\\}\\cup r}^\\top)\\over{(k - t+1)!\\over k!}\\sum_{|r| = k - t+1,r\\subseteq[n],r\\cap t = \\emptyset}\\det(a_{t\\cup r}a_{t\\cup r}^\\top)}\\\\ & = { \\left[\\prod_{i=1}^{r(a_{t\\cup\\{i\\}})}\\sigma_i^2(a_{t\\cup\\{i\\}})\\right ] \\times \\left[\\prod_{j=1}^{r(b_{t_c\\backslash\\{i\\}})}\\sigma_j^2(b_{t_c\\backslash\\{i\\}})\\right ] \\times \\gamma_{t\\cup\\{i\\ } }",
    "\\over(k - t+1 ) \\left[\\prod_{i=1}^{r(a_t)}\\sigma_i^2(a_t)\\right ] \\times \\left[\\prod_{j=1}^{r(b_{t_c})}\\sigma_j^2(b_{t_c})\\right ] \\times \\gamma_t}.\\end{aligned}\\ ] ]    [ [ computational - cost . ] ] computational cost .",
    "+ + + + + + + + + + + + + + + + + + +    it takes @xmath108 time to compute the conditional probability for one element given @xmath136 already selected elements .",
    "since this probability has to be computed for @xmath137 elements and there are a total of @xmath0 such iterations , the net running time of algorithm  [ algo : dualvol ] is @xmath118 .",
    "[ [ remark ] ] remark + + + + + +    a potentially more efficient approximate algorithm could be derived by noting the relations between volume sampling and dvs .",
    "specifically , we add a small perturbation to dvs as in equation  [ eq : eps ] to transform it into a volume sampling problem , and apply random projection for more efficient volume sampling as in  @xcite .",
    "please refer to appendix  [ app : sec : approx ] for more details .",
    "[ [ alternative - random - projections . ] ] alternative : random projections .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given the connection between dvs and volume sampling via perturbation , one may consider an alternative sampling method that performs volume sampling on @xmath138^\\top$ ] .",
    "doing so introduces a small distortion factor of order @xmath139 .",
    "this procedure can be further accelerated by applying volume - preserving random projections  @xcite , resulting in an algorithm with running time @xmath140 .",
    "this time is linear in @xmath23 but of high order in @xmath0 , and thus efficient only if @xmath141 . for sdd matrices ,",
    "recent work may yield improvements to .",
    "[ thm : approx ] for any @xmath142 and @xmath143 there is an algorithm that , in time @xmath144 , samples a subset from an approximate distribution @xmath145 with @xmath146 and @xmath147.\\end{aligned}\\ ] ]    in summary , we obtain two sampling algorithms : an exact one with running time @xmath118 , and an approximate one with time @xmath140 .",
    "next , we derandomize the above sampling algorithm to _ deterministically _ select a subset that satisfies the bound   for the frobenius norm , thereby answering another question in @xcite .",
    "the key insight for derandomization is that conditional expectations can be computed in polynomial time , given the marginals in theorem  [ thm : marginal ] :    [ cor : conditional ] let @xmath148^{t-1}$ ] be such that the marginal distribution satisfies @xmath149 .",
    "the conditional expectation can be expressed as @xmath150 = \\frac{\\sum_{j=1}^n p ' ( \\{i_1 , \\ldots , i_{t-1}\\ } \\subseteq s | s\\sim p(s;a_{[n]\\setminus \\{j\\ } } ) ) } { p ' ( \\{i_1 , \\ldots , i_{t-1}\\ } \\subseteq s | s\\sim p(s;a ) ) } ,    \\end{aligned}\\ ] ] where @xmath151 are the unnormalized marginal distributions , and it can be computed in @xmath152 time .",
    "we show the full derivation in appendix  [ app : sec : derand ] .",
    "corollary  [ cor : conditional ] enables a greedy derandomization procedure . starting with the empty tuple @xmath153 , in the @xmath154th iteration , we greedily select @xmath155 $ ] and append it to our selection : @xmath156 .",
    "the final set is the non - ordered version @xmath157 of @xmath158 .",
    "theorem  [ thm : derandbounds ] shows that this greedy procedure succeeds , and implies a deterministic version of the bound  ( [ eq : bounds ] ) .",
    "[ thm : derandbounds ] the greedy derandomization selects a column set @xmath6 satisfying @xmath159    the proof is done by constructing an iterative algorithm where in each iteration , we go through all elements that have not been selected and compute the expectation conditioned on this element included to current set .",
    "then we choose the element with the lowest conditional expectation to actually include into current set .",
    "such inclusion of elements will only decrease the conditional expectation , thus retaining the bound in theorem  [ thm : derandbounds ] .",
    "detailed proof is deferred to appendix  [ app : sec : greedy ] .    * complexity . * at each iteration , greedy selection requires @xmath152 to compute the conditional expectation for @xmath160 items .",
    "thus , the total running time for @xmath0 iterations is @xmath161 .",
    "the approximation bound for the spectral norm is slightly worse than that in  , but of the same order if @xmath162 .",
    "[ [ complexity.-1 ] ] complexity .",
    "+ + + + + + + + + + +    at each iteration , algorithm  [ algo : derand ] takes @xmath152 to compute the conditional expectation for @xmath160 items , thus the total running time for @xmath0 iterations is @xmath161 . also note that the approximation bound for spectral norm is slightly worse than that in  , though it will of the same order if @xmath162",
    "next , we investigate dvs more deeply and discover that it possesses a remarkable structural property , namely , the _ strongly rayleigh ( sr ) _",
    "@xcite property .",
    "this property has proved remarkably fruitful in a variety of recent contexts , including recent progress in approximation algorithms  @xcite , fast sampling  @xcite , graph sparsification  @xcite , extensions to the kadison - singer problem  @xcite , and certain concentration of measure results  @xcite , among others .    for dvs",
    "the sr property has two major consequences : it leads to a fast mixing practical mcmc sampler , and it implies results on concentration of measure .    [ [ strongly - rayleigh - measures . ] ] strongly rayleigh measures .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    sr measures were introduced in the landmark paper of  @xcite , who develop a rich theory of negatively associated measures . in particular , we say that a probability measure @xmath163}\\to { \\mathbf{r}}_+$ ] is _ negatively associated _ , if @xmath164 , for @xmath165 increasing functions on @xmath166}$ ] with _ disjoint _ support .",
    "this property reflects a `` repelling '' nature of @xmath167 , a property that occurs more broadly across probability , combinatorics , physics , and other fields  see @xcite and references therein .",
    "the negative association property turns out to be quite subtle in general ; the class of sr measures captures a strong notion of negative association and provides a framework for analyzing such measures .",
    "specifically , sr measures are defined via their connection to real - stable polynomials  @xcite .",
    "a multivariate polynomial @xmath168 $ ] where @xmath169 is called _ real stable _ if all its coefficients are real and @xmath170 whenever @xmath171 for @xmath172 .",
    "a measure is called an _ sr measure _ if its multivariate generating polynomial @xmath173 } \\mu(s)\\prod_{i\\in s}z_i$ ] is real - stable .",
    "notable examples of sr measures are determinantal point processes  @xcite , balanced matroids  @xcite , bernoullis conditioned on their sum , among others .",
    "it is known ( see  ( * ? ? ?",
    "* pg .  523 ) ) that the class of sr measures is exponentially larger than the class of determinantal measures .",
    "theorem  [ thm : sr ] establishes the sr property for dvs and is the main result of this section . here and",
    "in the following , we use the notation @xmath174 .",
    "[ thm : sr ] let @xmath175 and @xmath2 .",
    "then the multiaffine polynomial @xmath176 } \\det(a_s a_s^\\top)\\prod_{i\\in s}z_i\\quad=\\quad \\sum_{|s|=k , s\\subseteq",
    "[ m ] } \\det(a_s a_s^\\top)z^{s},\\ ] ] is real stable .",
    "consequently , @xmath14 is an sr measure .",
    "the proof of theorem  [ thm : sr ] relies on key properties of real stable polynomials and sr measures established in  @xcite .",
    "essentially , the proof demonstrates that the generating polynomial of @xmath177 can be obtained by applying a few carefully chosen stability preserving operations to a polynomial that we know to be real stable .",
    "stability , although easily destroyed , is closed under several operations noted in the important proposition below .",
    "[ prop : basic ] let @xmath178 be a stable polynomial .",
    "the following properties preserve stability :    * substitution * : @xmath179 for @xmath180 ;    * differentiation * : @xmath181 for any @xmath38 $ ] ;    * diagonalization * : @xmath182 is stable , and hence @xmath183 ; and    * inversion * : @xmath184 .",
    "in addition , we need the following two propositions for proving theorem  [ thm : sr ] .    [ prop : det ] let @xmath185 be hermitian , @xmath186 and @xmath187 ( @xmath188 be hermitian semidefinite matrices .",
    "then , the following polynomial is stable : @xmath189    [ prop : elsym ] for @xmath190 and @xmath191 , we have @xmath192 .",
    "let @xmath193_{i=1}^m)$ ] be an diagonal matrix . using the cauchy - binet identity we have @xmath194 }         \\det ( ( ay)_{:,t})\\det((a^\\top)_{t , : } ) = { \\sum\\nolimits}_{|t|=n , t\\subseteq [ m]}\\det(a_t^\\top a_t)y^t .",
    "\\end{split}\\ ] ] thus , when @xmath195 , the ( diagonal ) indicator matrix for @xmath6 , we obtain @xmath196 .",
    "consequently , in the summation above only terms with @xmath197 survive , yielding @xmath198    we are now ready to sketch the proof of theorem  [ thm : sr ] .",
    "notationally , it is more convenient to prove that the `` complement '' polynomial @xmath199}\\det(a_sa_s^\\top)z^{s_c}$ ] is stable ; subsequently , an application of prop .",
    "[ prop : basic]-(iv ) yields stability of  . using matrix notation @xmath200 , @xmath201 , our starting stable polynomial ( this stability follows from prop .  [ prop : det ] ) is @xmath202 which can be expanded as @xmath203}\\det(w_s+l_s)z^{s_c }    =     { \\sum\\nolimits}_{s\\subseteq [ m ] }    \\left({\\sum\\nolimits}_{t\\subseteq s}w^{s\\setminus t}\\det(l_{t , t})\\right)z^{s_c}.\\ ] ] thus , @xmath204 is real stable in @xmath205 variables , indexed below by @xmath6 and @xmath206 where @xmath207 . instead of the form",
    "above , we can sum over @xmath208 $ ] but then have to constrain the support to the case when @xmath209 and @xmath210 .",
    "in other words , we may write ( using iverson - brackets @xmath211 ) @xmath212}{\\llbracket s_c\\cap r=\\emptyset \\wedge s_c\\cap t=\\emptyset \\rrbracket}\\det(l_{t , t})z^{s_c}w^{r}.\\ ] ] next , we truncate polynomial   at degree @xmath213 by restricting @xmath214 . by  ( * ? ? ?",
    "* corollary 4.18 ) this truncation preserves stability , whence @xmath215 \\\\ |s_c\\cup r|=m - n}}{\\llbracket s_c\\cap r=\\emptyset \\rrbracket}\\det(l_{s\\backslash r , s\\backslash r})z^{s_c}w^{r},\\ ] ] is also stable . using prop .",
    "[ prop : basic]-(iii ) , setting @xmath216 retains stability ; thus @xmath217 \\\\",
    "r|=m - n } }       { \\llbracket s_c\\cap r=\\emptyset \\rrbracket}\\det(l_{s\\backslash r , s\\backslash r})z^{s_c}y^{|r|}\\\\    & = \\sum_{s\\subseteq [ m]}\\bigl({\\sum\\nolimits}_{|t|=n , t\\subseteq s}\\det(l_{t , t})\\bigr)y^{|s|-|t|}z^{s_c }    = \\sum_{s \\subseteq [ m ] } e_n(l_{s , s})y^{|s|-n}z^{s_c},\\end{aligned}\\ ] ] is also stable .",
    "next , differentiating @xmath218 , @xmath219 times with respect to @xmath220 and evaluating at @xmath221 preserves stability ( prop .",
    "[ prop : basic]-(ii ) and ( i ) ) .",
    "in doing so , only terms corresponding to @xmath222 survive , resulting in @xmath223}e_n(l_{s , s})z^{s_c } = ( k - n)!\\sum_{|s|=k , s\\subseteq[m]}\\det(a_s a_s^\\top ) z^{s_c},\\ ] ] which is just @xmath224 ( up to a constant ) ; here , the last equality follows from prop .",
    "[ prop : elsym ] .",
    "this establishes stability of @xmath224 and hence of @xmath225 .",
    "since @xmath225 is in addition multiaffine , it is the generating polynomial of an sr measure , completing the proof .",
    "the sr property of @xmath14 established in theorem  [ thm : sr ] implies a fast mixing markov chain for sampling @xmath6 .",
    "the states for the markov chain are all sets of cardinality @xmath0 .",
    "the chain starts with a randomly - initialized active set @xmath6 , and in each iteration we swap an element @xmath226 with an element @xmath227 with a specific probability determined by the probability of the current and proposed set .",
    "the stationary distribution of this chain is the one induced by dvs , by a simple detailed - balance argument .",
    "the chain is shown in algorithm  [ algo : mcmc ] .",
    "* input : * @xmath32 the matrix of interest , @xmath0 the target cardinality * output : * @xmath228 initialize @xmath38 $ ] such that @xmath222 and @xmath229 draw @xmath230 uniformly pick @xmath226 and @xmath231\\backslash s$ ] uniformly randomly + @xmath232 @xmath233 with probability @xmath234    the convergence of the markov chain is measured via its mixing time : the _ mixing time _ of the chain indicates the number of iterations @xmath235 that we must perform ( starting from @xmath236 ) before we can consider @xmath237 as an approximately valid sample from @xmath14 .",
    "formally , if @xmath238 is the total variation distance between the distribution of @xmath237 and @xmath14 after @xmath235 steps , then @xmath239 is the _ mixing time _ to sample from a distribution @xmath43-close to @xmath14 in terms of total variation distance .",
    "we say that the chain mixes fast if @xmath240 is polynomial in the problem size .",
    "the fast mixing result for algorithm  [ algo : mcmc ] is a corollary of theorem  [ thm : sr ] combined with a recent result of  @xcite on fast - mixing markov chains for homogeneous sr measures .",
    "theorem  [ thm : fastmc ] states this more precisely .",
    "[ thm : fastmc ] the mixing time of markov chain shown in algorithm  [ algo : mcmc ] is given by @xmath241    since @xmath14 is @xmath0-homogeneous sr by theorem  [ thm : sr ] , the chain constructed for sampling @xmath6 following that in  @xcite mixes in @xmath242 time .",
    "[ [ implementation . ] ] implementation .",
    "+ + + + + + + + + + + + + + +    to implement algorithm  [ algo : mcmc ] we need to compute the transition probabilities @xmath243 .",
    "let @xmath244 and assume @xmath245 .",
    "by the matrix determinant lemma we have the acceptance ratio @xmath246 thus , the transition probabilities can be computed in @xmath247 time .",
    "moreover , one can further accelerate this algorithm by using the quadrature techniques of  @xcite to compute lower and upper bounds on this acceptance ratio to determine early acceptance or rejection of the proposed move .",
    "[ [ initialization . ] ] initialization .",
    "+ + + + + + + + + + + + + + +    a remaining question is initialization .",
    "since the mixing time involves @xmath248 , we need to start with @xmath236 such that @xmath249 is sufficiently bounded away from @xmath221 .",
    "we show in appendix  [ app : sec : init ] that by a simple greedy algorithm , we are able to initialize @xmath6 such that @xmath250 , and the resulting running time for algorithm  [ algo : mcmc ] is @xmath251 , which is _ linear _ in the size of data set @xmath23 and is efficient when @xmath0 is not too large .      [ [ concentration . ] ] concentration .",
    "+ + + + + + + + + + + + + +    @xcite show concentration results for strong rayleigh measures . as a corollary of our theorem  [ thm : sr ] together with their results , we directly obtain tail bounds for dvs .    [ [ algorithms - for - experimental - design . ] ] algorithms for experimental design .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    widely used , classical algorithms for finding an approximate optimal design include fedorov s exchange algorithm @xcite ( a greedy local search ) and simulated annealing @xcite .",
    "both methods start with a random initial set @xmath6 , and greedily or randomly exchange a column @xmath252 with a column @xmath253 .",
    "apart from very expensive running times , they are known to work well in practice @xcite .",
    "yet so far there is no theoretical analysis , or a principled way of determining when to stop the greedy search .    curiously , our mcmc sampler is essentially a randomized version of fedorov s exchange method .",
    "the two methods can be connected by a unified , simulated annealing view , where we define @xmath254 with temperature parameter @xmath255 .",
    "driving @xmath255 to zero essentially recovers fedorov s method , while our results imply fast mixing for @xmath256 , together with approximation guarantees . through this lens",
    ", simulated annealing may be viewed as initializing fedorov s method with the fast - mixing sampler . in practice",
    ", we observe that letting @xmath257 improves the approximation results , which opens interesting questions for future work .",
    "[ thm : fastmc ] the mixing time of markov chain shown in algorithm  [ algo : mcmc ] is given by @xmath241    the proof of the fast mixing time bound relies on the theory of strongly rayleigh measures  @xcite , for which the following important result was recently shown in  @xcite :    [ thm : srmix ] for any strongly rayleigh k - homogeneous probability distribution @xmath258}\\to \\mathbb{r}_+$ ] , @xmath259 $ ] where @xmath260 and @xmath66 , we have @xmath261 .    * ( theorem  [ thm : fastmc ] ) .",
    "* since @xmath177 is @xmath262-homogeneous strongly rayleigh by theorem  [ thm : sr ] , the chain constructed for sampling @xmath263 following that in  @xcite mixes in @xmath242 time .",
    "one can first sample @xmath263 and take the complement to get @xmath6 when the chain has mixed well , resulting in algorithm  [ algo : mcmc ] .",
    "it remains to prove theorem  [ thm : sr ] .",
    "let @xmath264\\backslash s$ ] be the complement of @xmath6 .",
    "theorem  [ thm : sr ] proves that the distribution @xmath265 , which is closely related to @xmath14 , is strongly rayleigh .",
    "note that @xmath177 samples @xmath266\\backslash s$ ] .",
    "we also use the notation @xmath174 .",
    "[ thm : sr ] let @xmath175 and @xmath2 .",
    "then the multiaffine polynomial @xmath176 } \\det(a_s a_s^\\top)\\prod_{i\\in s_c}z_i\\quad=\\quad \\sum_{|s|=k , s\\subseteq",
    "[ m ] } \\det(a_s a_s^\\top)z^{s_c},\\ ] ] is real stable . in other words",
    ", @xmath177 is a @xmath0-homogeneous strongly rayleigh ( sr ) measure .",
    "the proof of theorem  [ thm : sr ] relies on key properties of real stable polynomials and sr measures established in the remarkable work of  @xcite .",
    "a polynomial @xmath168 $ ] where @xmath169 is called real stable if all its coefficients are real and @xmath170 whenever @xmath171 for @xmath172",
    ". a measure is called sr measure if its generating polynomial is real stable .",
    "we have the following properties on real stable polynomial that are useful in our proof :    [ prop : det ] let @xmath185 be hermitian , @xmath186 and @xmath187 ( @xmath188 be hermitian semidefinite matrices .",
    "then , the following polynomial is stable : @xmath189    stability can be subtle ; although easily destroyed , it can sometimes be preserved , as noted below .",
    "[ prop : basic ] let @xmath178 be a stable polynomial .",
    "the following closure properties hold :    * substitution * : @xmath179 for @xmath180 is stable    * differentiation * : @xmath181 for any @xmath38 $ ] is stable ;    * diagonalization * : @xmath182 is stable , and hence @xmath183 is stable .    with propositions  [ prop : det ] and  [ prop : basic ]",
    "in hand , we are now ready to prove theorem  [ thm : sr ] .    * ( theorem  [ thm : sr ] ) .",
    "* let @xmath193_{i=1}^m)$ ] be an @xmath267 diagonal matrix of indeterminates . using the cauchy - binet identity we obtain @xmath194 }         \\det ( ( ay)_{:,t})\\det((a^\\top)_{t , : } ) = { \\sum\\nolimits}_{|t|=n , t\\subseteq [ m]}\\det(a_t^\\top a_t)y^t .    \\end{split}\\ ] ] thus ,",
    "when @xmath268 is a diagonal matrix with ones for indices in @xmath6 and zeros otherwise , we obtain @xmath196 . consequently , in the summation above only terms with @xmath197 survive , yielding @xmath269 where we introduced @xmath191 .",
    "thus we can rewrite the polynomial   as @xmath270 } \\det(a_sa_s^\\top)z^{s_c } = \\sum_{|s|=k , s\\subseteq [ m ] } e_n(l_{s , s})z^{s_c}.    \\end{split}\\ ] ] our aim is to prove stability of  .",
    "to that end , we introduce the determinantal polynomial @xmath202 which can be expanded as @xmath271}\\det(w_s+l_s)z^{s_c }    =     \\sum_{s\\subseteq [ m ] }    \\left({\\sum\\nolimits}_{t\\subseteq s}w^{s\\setminus t}\\det(l_{t , t})\\right)z^{s_c}.\\ ] ] this is an sr polynomial in @xmath205 variables , indexed by @xmath6 and @xmath206 , where @xmath207 .",
    "it has support @xmath209 and @xmath210 . in other words",
    ", we have ( using iverson - brackets @xmath211 ) @xmath272}{\\llbracket s_c\\cap r=\\emptyset \\wedge s_c\\cap t=\\emptyset \\rrbracket}\\det(l_{t , t})z^{s_c}w^{r}.\\ ] ] now we truncate this sr polynomial at @xmath213 by restricting @xmath214 . by  (",
    "* corollary 4.18 ) this truncation preserves the sr property , whence @xmath215 \\\\ |s_c\\cup",
    "r|=m - n}}{\\llbracket s_c\\cap r=\\emptyset \\rrbracket}\\det(l_{s\\backslash r , s\\backslash r})z^{s_c}w^{r}\\ ] ] is also stable . using prop .",
    "[ prop : basic]-(iii ) , setting @xmath216 retains stability , yielding @xmath217 \\\\ |s_c\\cup",
    "r|=m - n } }       { \\llbracket s_c\\cap r=\\emptyset \\rrbracket}\\det(l_{s\\backslash r , s\\backslash r})z^{s_c}y^{|r|}\\\\    & = \\sum_{s\\subseteq [ m]}\\bigl({\\sum\\nolimits}_{|t|=n , t\\subseteq s}\\det(l_{t , t})\\bigr)y^{|s|-|t|}z^{s_c }    = \\sum_{s \\subseteq [ m ] } e_n(l_{s , s})y^{|s|-n}z^{s_c},\\end{aligned}\\ ] ] where the second equality follows since @xmath214 and @xmath273 imply that @xmath274 .",
    "differentiating @xmath218 , @xmath219 times with respect to @xmath220 and evaluating at @xmath221 preserves stability ( prop .",
    "[ prop : basic]-(ii ) and ( i ) ) . in doing so , only terms corresponding to @xmath222",
    "survive , resulting in @xmath223}e_n(l_{s , s})z^{s_c } = ( k - n)!\\sum_{|s|=k , s\\subseteq[m]}\\det(a_s a_s^\\top ) z^{s_c},\\ ] ] which is just  ( [ eq:1 ] ) ( up to a constant ) .",
    "thus , we have shown that @xmath225 is stable .",
    "since it is multiaffine , it is actually an sr polynomial , and its corresponding measure @xmath177 is sr .",
    "[ [ implementation - notes . ] ] implementation notes . + + + + + + + + + + + + + + + + + + + + +    to implement algorithm  [ algo : mcmc ] we need to compute the transition probabilities @xmath243 .",
    "let @xmath244 and assume @xmath245 .",
    "[ eq:4 ] we have the acceptance ratio @xmath246 thus , the transition probabilities can be computed in @xmath247 time .",
    "moreover , one can further accelerate the algorithm by using the techniques developed in  @xcite to compute lower and upper bounds on this quantity to determine early acceptance or rejection of the proposed move .",
    "a remaining question is initialization .",
    "since the mixing time is related to @xmath248 , we need to start with @xmath236 such that @xmath249 is bounded away from @xmath221 . here again the @xmath43-trick proves useful .",
    "set @xmath275 , whereby @xmath276^\\top\\bigr).\\end{aligned}\\ ] ] the rhs is a distribution induced by volume sampling .",
    "greedily choosing columns of @xmath3 one by one gives a @xmath277 approximation to the maximum volume submatrix  @xcite .",
    "this results in a set @xmath6 such that @xmath278 thus , @xmath250 , and the resulting running time for algorithm  [ algo : mcmc ] is @xmath251 , which is _ linear _ in the size of data set @xmath23 and is efficient when @xmath0 is not large . in practice",
    "it is hard to set @xmath43 to be exactly @xmath279 , but a small value will approximately suffice .",
    "we report selection performance of dvs on real regression data  ( compact(s ) , abalone and bank32nh ] ) for experimental design .",
    "we use 4,000 samples from each dataset for estimation .",
    "we compare against various baselines , including uniform sampling  ( ` unif ` ) , leverage score sampling  ( ` lev ` )  @xcite , predictive length sampling  ( ` pl ` )  @xcite , the sampling  ( ` smpl`)/greedy  ( ` greedy ` ) selection methods in  @xcite and fedorov s exchange algorithm  @xcite .",
    "we run the mcmc sampler for dvs for 10,000 iterations , which empirically yields selections that are sufficiently good .",
    "we measure performances via ( 1 ) the prediction error @xmath280 , and 2 ) running times .",
    "figure  [ fig : cpu ] shows the results for these three measures with sample sizes @xmath0 varying from @xmath281 to @xmath282 .",
    "further experiments ( including for the interpolation @xmath283 ) , may be found in the appendix .        in terms of prediction error",
    ", dvs performs well and is comparable with ` lev ` .",
    "its strength compared to the greedy and relaxation methods ( ` smpl ` , ` greedy ` , ` fedorov ` ) is running time , leading to good time - error tradeoffs .",
    "these tradeoffs are illustrated in figure  [ fig : cpu ] for @xmath284 .",
    "in other experiments ( shown in appendix  [ app : sec : exp ] ) we observed that in some cases , the optimization and greedy methods  ( ` smpl ` , ` greedy ` , ` fedorov ` ) yield better results than sampling , however with much higher running times . hence , given time - error tradeoffs , dvs may be an interesting alternative in situations where time is a very limited resource and results are needed quickly .",
    "we show full results on compact(s ) , abalone and bank32nh datasets in figure  [ app : fig : cpu ] ,  [ app : fig : abalone ] and  [ app : fig : bank ] respectively .",
    "we also run dvs- * , which is @xmath375-generalized dvs algorithm .",
    "we observe that decreasing @xmath255 sometimes helps but sometimes not . in figure",
    "[ app : fig : bank ] we observe that optimization- or greedy - based methods , while taking a huge amount of time to run , perform better than all sampling - based methods , thus for these selection methods , one is not always superior than another .",
    "in this paper , we study the problem of dvs and develop an exact ( randomized ) polynomial time sampling algorithm as well as its derandomization .",
    "thereafter , we study dual volume sampling via the theory of real - stable polynomials and prove that its distribution satisfies the `` strong rayleigh '' property .",
    "this result has remarkable consequences , especially because it implies a provably fast - mixing markov chain sampler that makes dual volume sampling much more attractive to practitioners .",
    "finally , we observe connections to classical , computationally more expensive experimental design methods ( fedorov s method and sa ) ; together with our results here , these could be a first step towards a better theoretical understanding of those methods . in this paper",
    ", we develop polynomial time algorithms for dual volume sampling , a promising method for column subset selection problems .",
    "we propose the first polynomial time exact sampling algorithm and its derandomized alternative , thereby closing two open problems raised in  @xcite .",
    "further , we show that a close variant of the dual volume distribution under consideration is strongly rayleigh . as a result",
    "we immediately obtain a fast mixing markov chain for approximate dual volume sampling .",
    "our algorithms thus enhance the applicability of dual volume sampling in its downstream applications like column subset selection and experimental design .",
    "j.  borcea and p.  brndn .",
    "applications of stable polynomials to mixed determinants : johnson s conjectures , unimodality , and symmetrized fischer products .",
    "_ duke mathematical journal _ , pages 205223 , 2008 .",
    "a.  magen and a.  zouzias .",
    "near optimal dimensionality reductions that preserve volumes . in _",
    "approximation , randomization and combinatorial optimization .",
    "algorithms and techniques _ , pages 523534 .",
    "springer , 2008 .",
    "the partition function of @xmath82 , happens to have a pleasant closed - form formula . although this formula is known  @xcite , and follows immediately by an application of the cauchy - binet identity , we present an alternative proof based on the perturbation argument for its conceptual value and subsequent use .      first note that for @xmath57 and any @xmath66 , by   we have @xmath85 taking limits as @xmath86 on both sides we have @xmath87 let us focus on @xmath88 .",
    "we construct an identity matrix @xmath89 , then we have @xmath90}\\right ) .",
    "\\end{split}\\ ] ] in other words , this value is proportional to the probability of sampling columns from @xmath91}$ ] using volume sampling .",
    "therefore , using the definition of @xmath92 we have @xmath93}&\\det(a_s^\\top a_s + { \\varepsilon}i_k ) = { 1\\over { \\varepsilon}^{k - n } } e_k ( a^\\top a + { \\varepsilon}i_m)\\\\ & = { 1\\over { \\varepsilon}^{k - n } } e_k ( \\operatorname{diag}([(\\sigma_1 ^ 2(a ) + { \\varepsilon } ) , ( \\sigma_2 ^ 2(a ) + { \\varepsilon}),\\ldots , ( \\sigma_n^2(a ) + { \\varepsilon } ) , { \\varepsilon},\\ldots,{\\varepsilon}]))\\\\ & = \\binom{m - n}{k - n}\\prod\\nolimits_{i=1}^n ( \\sigma_i^2(a ) + { \\varepsilon } ) + o({\\varepsilon}).\\end{aligned}\\ ] ] now taking the limit as @xmath86 we obtain @xmath84}\\det(a_s a_s^\\top ) & = \\lim_{{\\varepsilon}\\to 0}\\binom{m - n}{k - n}\\prod\\nolimits_{i=1}^n ( \\sigma_i^2(a ) + { \\varepsilon } ) + o({\\varepsilon } ) = \\binom{m - n}{k - n}\\det(a a^\\top).\\end{aligned}\\ ] ]",
    "the marginal probability of a set @xmath60 $ ] for dual volume sampling is @xmath285 theorem  [ thm : partition ] shows how to compute the denominator , thus our main effort is devoted to the nominator .",
    "we have @xmath286 using the @xmath43-trick we have @xmath287 by decomposing @xmath288 we have @xmath289    now we let @xmath95 be the singular value decomposition of @xmath68 where @xmath96 , @xmath290 and @xmath291 . plugging the decomposition in the equation we obtain @xmath292 q_t^\\top a_r\\\\ & = a_r^\\top q_t q_t^\\top a_r - { \\varepsilon}a_r^\\top q_t   \\left[\\begin{array}{cccc } { 1\\over \\sigma_1 ^ 2(a_t ) + { \\varepsilon } } & 0 & \\ldots & 0\\\\ 0 & { 1\\over \\sigma_2 ^ 2(a_t ) + { \\varepsilon } } & \\ldots & 0\\\\   \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0 & \\ldots & { 1\\over \\sigma_{r(a_t)}^2(a_t)+ { \\varepsilon } } \\end{array}\\right ] q_t^\\top a_r.\\end{aligned}\\ ] ]",
    "thus it follows that @xmath293 q_t^\\top a_r + { \\varepsilon}i_{|r|}\\\\ & = b_r^\\top b_r + { \\varepsilon}c_r^\\top c_r + { \\varepsilon}i_{|r|},\\end{aligned}\\ ] ] where @xmath294 is the projection of columns of @xmath295 on the orthogonal space of columns of @xmath68 .",
    "let @xmath97 be the complement column space of @xmath98 , then we have @xmath296 .",
    "moreover , @xmath297",
    "q_t^\\top a_r \\in\\mathbb{r}^{r(a_t)\\times |r|}.\\end{aligned}\\ ] ] we further let @xmath298 and @xmath299 q_t^\\top a_{t_c } \\in\\mathbb{r}^{r(a_t)\\times ( m-|t|)}\\end{aligned}\\ ] ] where @xmath61\\backslash t$ ] .",
    "then we have @xmath300}{\\left[\\begin{array}{c}b_{t_c}\\\\ \\sqrt{{\\varepsilon}}u_{t_c } \\\\",
    "\\sqrt{{\\varepsilon}}c_{t_c}\\end{array}\\right]}^\\top\\right)\\end{aligned}\\ ] ] where we construct an orthonormal matrix @xmath301 whose columns are basis vectors",
    ". since we are free to chose any orthonormal @xmath302 , we simply let it be @xmath303 .",
    "let @xmath103 $ ] , we have @xmath304}{\\left[\\begin{array}{c}b_{t_c}\\\\ \\sqrt{{\\varepsilon}}u_{t_c }",
    "\\\\ \\sqrt{{\\varepsilon}}c_{t_c}\\end{array}\\right]}^\\top\\right ) & =   \\left({\\left[\\begin{array}{c}b_{t_c}\\\\ \\sqrt{{\\varepsilon}}w_{t_c } \\end{array}\\right]}{\\left[\\begin{array}{c}b_{t_c}\\\\ \\sqrt{{\\varepsilon}}w_{t_c } \\end{array}\\right]}^\\top\\right ) \\\\ & = f_{t_c } \\in\\mathbb{r}^{(m+n-|t|)\\times ( m+n-|t|)}\\end{aligned}\\ ] ] the properties of characteristic polynomials imply that @xmath305 where @xmath306 $ ] and @xmath307\\backslash s_1 $ ] .",
    "further we have @xmath308 hence it follows that @xmath309 \\times \\\\&\\sum_{|s| = k-|t|}{\\varepsilon}^{k-|t|-|s_1|}\\det((b_{t_c})_{s_1}(b_{t_c})_{s_1}^\\top ) \\times\\\\ & \\quad\\quad\\quad\\quad \\det((w_{t_c})_{s_2 } ( w_{t_c})_{s_2}^\\top - ( w_{t_c})_{s_2 } ( b_{t_c})_{s_1}^\\top((b_{t_c})_{s_1}(b_{t_c})_{s_1}^\\top)^{-1}(b_{t_c})_{s_1 } ( w_{t_c})_{s_2}^\\top ) \\end{aligned}\\ ] ] ( since @xmath310 and @xmath311 ) @xmath312 \\times \\\\&\\sum_{|s| = k-|t|}{\\varepsilon}^{k-|t|-r(b_{t_c})}\\det(b_{t_c}b_{t_c}^\\top ) \\det((w_{t_c})_{s_2 } ( w_{t_c})_{s_2}^\\top - ( w_{t_c})_{s_2 } b_{t_c}^\\top(b_{t_c}b_{t_c}^\\top)^{-1}b_{t_c } ( w_{t_c})_{s_2}^\\top ) + o({\\varepsilon})\\\\ & = \\left[\\prod_{i=1}^{r(a_t)}\\sigma_i^2(a_t)\\right ] \\times \\left[\\prod_{j=1}^{r(b_{t_c})}\\sigma_j^2(b_{t_c})\\right ] \\sum_{s_2}\\det((w_{t_c})_{s_2 } ( w_{t_c})_{s_2}^\\top - ( w_{t_c})_{s_2 } b_{t_c}^\\top(b_{t_c}b_{t_c}^\\top)^{-1}b_{t_c } ( w_{t_c})_{s_2}^\\top ) \\end{aligned}\\ ] ] where @xmath313\\backslash [ r(b_{t_c})]$ ] and @xmath314 .",
    "let @xmath315 be the eigenvalue decomposition of @xmath101 where @xmath102 .",
    "further , let @xmath316 be the complement column space of @xmath317 , thus we have @xmath318   \\left[\\begin{array}{c c } q_{b_{t_c } } & q_{b_{t_c}}^\\perp \\end{array}\\right ] = i_{|t_c| } = i_{n-|t|}\\end{aligned}\\ ] ] then for any @xmath313\\backslash [ r(b_{t_c})]$ ] we have @xmath319 it follows that @xmath320 combining all the above derivations , we obtain that @xmath321 \\times \\left[\\prod_{j=1}^{r(b_{t_c})}\\sigma_j^2(b_{t_c})\\right ] \\times \\gamma_t \\over \\left(\\begin{array}{c}n - m\\\\k - m\\end{array}\\right ) \\det(a a^\\top)}.\\end{aligned}\\ ] ]",
    "it may happen in practice that @xmath322 but @xmath0 is of the same order as @xmath4 .",
    "in such case we can transform the dual volume sampling to slightly distorted volume sampling based on   and then take the advantage of determinant - preserving projections to accelerate the sampling procedure .    concretely , instead of sampling column subset @xmath6 with probability proportional to @xmath323",
    ", we sample with probability proportional to a distorted value @xmath324 for small @xmath66 . denoting this distorted distribution as @xmath325 ,",
    "we have @xmath326 letting @xmath327 be the minimum singular value , we have @xmath328 we further let @xmath329 when @xmath43 sufficiently small . sampling from @xmath330 will yield @xmath331-approximate dual volume sampling ( in the sense of @xcite and our theorem  [ thm : approx ] ) .",
    "we can sample from @xmath330 via _ volume sampling _ with distribution @xmath332})$ ] . with the volume sampling algorithm proposed in  @xcite ,",
    "the resulting running time would be @xmath333 .",
    "[ thm : projection ] for any @xmath334 , @xmath335 and @xmath336 , the random gaussian projection of @xmath337 where @xmath338 satisfies @xmath339 for all @xmath340 $ ] and @xmath341 where @xmath342 is the projected matrix .",
    "( corollary  [ thm : approx ] ) the idea is to project @xmath91}$ ] to a lower - dimensional space in a way that the values for submatrix determinants are preserved up to a small multiplicative factor .",
    "then we perform volume sampling .",
    "we project columns of @xmath91}$ ] , which is in @xmath346 , to vectors in @xmath347 where @xmath348 so as to achieve a @xmath349 approximation by thm .",
    "[ thm : projection ] .",
    "let @xmath22 be a @xmath350-dimensional i.i.d .",
    "gaussian random matrix , then we have @xmath351 } = g_a a + \\sqrt{{\\varepsilon}}g_a'\\end{aligned}\\ ] ] where @xmath352 and @xmath353 are two independent gaussian random matrix .",
    "the projected matrix can be computed in @xmath354 time .",
    "after that , if we use volume sampling algorithm proposed in  @xcite the resulting running time would be @xmath355 .",
    "thus the total running time would be @xmath345 .",
    "an interesting observation is that the resulting running time is independent of @xmath356 , which means one can set @xmath43 arbitrarily small so as to make the approximation in the first step as accurate as possible , without affecting the running time .",
    "however , in practice , a very small @xmath43 can result in numerical problems .",
    "in addition , the dimensionality reduction is only efficient if @xmath357 .",
    "we use @xmath358 denote the matrix @xmath359\\backslash\\{j\\},:}$ ] , namely matrix @xmath3 with row @xmath113 deleted .",
    "we have @xmath360\\\\ & = \\sum_{(i_t,\\ldots , i_k)\\in[m]^{k - t+1 } } \\|a_s^\\dagger\\|_f^2 \\overrightarrow{p}(s_1=i_1,\\ldots , s_k = i_k ; a \\mid s_1=i_1,\\ldots , s_{t-1}=i_{t-1})\\\\ & = \\sum_{(i_t,\\ldots , i_k)\\in[m]^{k - t+1 } } \\|a_s^\\dagger\\|_f^2 { \\overrightarrow{p}(s_1=i_1,\\ldots , s_k = i_k ; a ) \\over \\overrightarrow{p}(s_1=i_1,\\ldots , s_{t-1}=i_{t-1 } ; a)}\\\\ & = { \\sum_{(i_t,\\ldots , i_k)\\in[m]^{k - t+1}}\\det(a_{\\{i_1,\\ldots , i_k\\ } } a_{\\{i_1,\\ldots , i_k\\}}^\\top ) \\|a_{\\{i_1,\\ldots , i_k\\}}^\\dagger\\|_f^2 \\over \\sum_{(i_t,\\ldots , i_k)\\in[m]^{k - t+1}}\\det(a_{\\{i_1,\\ldots , i_k\\ } } a_{\\{i_1,\\ldots , i_k\\}}^\\top ) } \\\\ & = { \\sum_{j=1}^n\\sum_{(i_t,\\ldots , i_k)\\in[m]^{k - t+1 } } \\det(a_{\\{i_1,\\ldots , i_k\\}}^j ( a_{\\{i_1,\\ldots , i_k\\}}^j)^\\top ) \\over \\sum_{(i_t,\\ldots , i_k)\\in[m]^{k - t+1}}\\det(a_{\\{i_1,\\ldots , i_k\\ } } a_{\\{i_1,\\ldots , i_k\\}}^\\top ) } \\\\\\end{aligned}\\ ] ] while the denominator is the ( unnormalized ) marginal distribution @xmath361 , the numerator is the summation of ( unnormalized ) marginal distribution @xmath362 for @xmath363 . by theorem  [ thm : marginal ]",
    "we can compute this expectation in @xmath152 time .",
    "* input : * matrix @xmath32 to sample columns from , @xmath364 the target size * output : * set @xmath6 such that @xmath128 with the guarantee @xmath365 initialize @xmath112 as empty tuple compute conditional expectation @xmath366 $ ] with corollary  [ cor : conditional ] .",
    "choose @xmath367 @xmath133 output @xmath112 as a set @xmath6      observe that at each iteration @xmath235 , we have @xmath368\\\\ & = { \\sum\\nolimits}_{j\\notin \\overrightarrow{s}}\\overrightarrow{p}(t_i = j | t_1=s_1,\\ldots , t_{i-1}=s_{i-1})\\mathbb{e}\\left[\\|a_t^\\dagger\\|_f^2 \\mid t_1 = s_1,\\ldots , t_{i-1}=s_{i-1 } , t_i = j\\right],\\end{aligned}\\ ] ] and we choose @xmath113 such that @xmath369 $ ] is minimized . since at the beginning we have @xmath370 \\le { m - n + 1\\over k - n+1}\\|a^\\dagger\\|_f^2 ; \\quad t\\sim p(t;a),\\end{aligned}\\ ] ] it follows that the conditional expectation satisfies @xmath371 \\le { m - n + 1\\over k - n+1}\\|a^\\dagger\\|_f^2.\\end{aligned}\\ ] ] hence we have @xmath372 \\le { m - n + 1\\over k - n+1}\\|a^\\dagger\\|_f^2.\\end{aligned}\\ ] ] further , by using standard bounds relating the operator norm to the frobenius norm , we obtain @xmath373",
    "set @xmath275 , whereby @xmath374^\\top\\bigr).\\end{aligned}\\ ] ] the rhs is a distribution induced by volume sampling .",
    "greedily choosing columns of @xmath3 one by one gives a @xmath277 approximation to the maximum volume submatrix  @xcite .",
    "this results in a set @xmath6 such that @xmath278 thus , @xmath250 .",
    "note that in practice it is hard to set @xmath43 to be exactly @xmath279 , but a small approximate value suffices ."
  ],
  "abstract_text": [
    "<S> we study _ dual volume sampling _ , a method for selecting @xmath0 columns from an @xmath1 short and wide matrix ( @xmath2 ) such that the probability of selection is proportional to the _ volume _ spanned by the rows of the induced submatrix . </S>",
    "<S> this method was proposed by  @xcite ( @xcite ) , who showed it to be a promising method for column subset selection and its multiple applications . </S>",
    "<S> however , its wider adoption has been hampered by the lack of polynomial time sampling algorithms . </S>",
    "<S> we remove this hindrance by developing an exact ( randomized ) polynomial time sampling algorithm as well as its derandomization . </S>",
    "<S> thereafter , we study dual volume sampling via the theory of real - stable polynomials and prove that its distribution satisfies the `` strong rayleigh '' property . </S>",
    "<S> this result has remarkable consequences , especially because it implies a provably fast - mixing markov chain sampler that makes dual volume sampling much more attractive to practitioners . </S>",
    "<S> this sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well . </S>"
  ]
}