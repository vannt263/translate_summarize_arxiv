{
  "article_text": [
    "finding similar items is a fundamental problem that underlies numerous problems in data analysis .",
    "link prediction in a graph can be cast as finding similar nodes in the graph  @xcite ; customers are recommended similar products  @xcite ; text analysis often involves finding similar texts  @xcite ; data cleaning requires removal of entries that are essentially identical  @xcite . in these settings ,",
    "entities are represented as vectors in high - dimensional feature space , i.e. , @xmath9 , for some large @xmath10 .",
    "many notions of similarity involve dot products , so a measure of distance between @xmath11 and @xmath12 is @xmath13 .",
    "this subsumes cosine similarity , common - neighbors , database join operations @xcite , frequent itemset mining , data cleaning @xcite , etc",
    ". motivated by these applications , we study the maximum all - pairs dot - product ( mad ) problem .    given two sets of @xmath10-dimensional vectors @xmath14 and @xmath15 : find the index pair @xmath7 that maximizes @xmath16 . more generally , given additional parameter @xmath2 , find the @xmath2 index pairs @xmath17 corresponding to the @xmath2 largest dot products .",
    "it is convenient to think of @xmath5 and @xmath6 as matrices ( @xmath18 and @xmath19 ) , where the columns are the corresponding vectors .",
    "the mad problem is exactly finding the largest entries in the product @xmath20 .",
    "the mad formulation subsumes many existing problems in the literature .",
    "for the special case where @xmath5 is a single column ( equivalently , @xmath21 ) , this is the exactly the mips ( maximum inner product search ) problem  @xcite . here , we maximize the dot product with @xmath22 among all columns of @xmath6 . when @xmath23 is the adjacency matrix of a graph , this is equivalent to finding pairs of nodes with the most common neighbors , a fundamental link prediction operation .",
    "the most obvious approach is to simply compute @xmath24 exhaustively .",
    "there is a rich history on algorithms for dense and sparse matrix multiplication  @xcite , with implementations in libraries like intel mkl s blas  @xcite and csparse  @xcite .",
    "however , existing algorithms become prohibitive as the sizes of @xmath5 and @xmath6 grow , even if they are sparse .",
    "there is also much literature in approximate matrix multiplication , which quickly computes an approximate product @xmath25 .",
    "drineas , kannan , and mahoney @xcite introduced an approach based on sampling rows of @xmath5 and @xmath6 to minimize frobenius norm of the error .",
    "much study has been done on various sampling strategies  @xcite .",
    "these methods are also not suited for mad , since we only care for a few entries ( at most , say , 1000 ) despite the matrix having dimensions in the millions .",
    "an alternate , popular approach for high - dimensional nearest neighbor search is some form of dimension reduction , the most famous approach being indyk and motwani s locality sensitive hashing ( lsh )  @xcite .",
    "recent results by shrivastava and li extend lsh methods for the mips problem  @xcite . these approaches usually involve randomly hashing the vectors into a few  buckets \" , and then searching for vector pairs that share many common buckets . for high - dimensional data ,",
    "the maximum dot product is often small in comparison to the vector norms , so the similarities are quite small .",
    "this means that many hashes are required to find the nearest neighbors , leading to a storage blowup ( typically , two orders of magnitude more than the data )",
    ". this can be quite prohibitive even for small data sets .",
    "we take a different route , and apply _ index sampling _ methods .",
    "the idea is to sample pair @xmath7 proportional to some function of the dot product @xmath26 . with enough samples , we hope to find the large entries of @xmath27 .",
    "the earliest application of this idea is by cohen and lewis , who constructed a sampling algorithm for the mips problem  @xcite .",
    "their approach samples pairs @xmath7 proportional to @xmath28  @xcite .",
    "campagna and pagh give sampling approaches for a variety of distance measures  @xcite .",
    "we stress that no previous result ( either sampling based , lsh based , or otherwise ) applies directly to the mad problem .",
    "* diamond sampling : * our main contribution is _ diamond sampling _ , a new randomized approach to the mad problem .",
    "this is inspired by recent work by jha et al for 4-vertex motif detection in large graphs  @xcite .",
    "their idea is to sample 3-paths in graphs to estimate counts of 4-vertex motifs .",
    "we generalize that idea to the matrix product setting , to design a sampling procedure for the mad problem .",
    "diamond sampling is able to sample pairs @xmath7 proportional to @xmath29 .",
    "the square term is a critical improvement over cohen and lewis ; it allows for faster convergence to the top entries of @xmath27 .    * theoretical analysis : * we give a theoretical analysis of diamond sampling , and prove concentration bounds on its behavior .",
    "our analysis shows the eventual convergence of the sampling to squared entries , with no assumption whatsoever .",
    "previous sampling work required nonnegativity assumption , or assumed structural correlations among positive entries  @xcite .",
    "we give strong storage bounds on diamond sampling , and show that it requires very little overhead .",
    "* empirical validation : * we apply diamond sampling on six real - world datasets and show that is extremely efficient .",
    "diamond sampling is orders of magnitude faster than exact computation and requires far fewer samples compared to other matrix sampling approaches .",
    "[ fig : awesome ] shows the results of diamond sampling on a song dataset with 48 m user - song entries .",
    "we consider @xmath23 to be the matrix where songs are columns and attempt to find the top correlated songs .",
    "we can get the top 100 pairs in an order of magnitude less time than exhaustive computation .",
    "we consider numerous applications in product recommendation and link prediction , and consistently get to top 10 - 100 dot products , with a speedup of 10 - 100x over exact computation .",
    "furthermore , the number of samples required is much smaller than the cohen - lewis approach  @xcite .",
    "* application to mips : * given recent interest in the mips problem , we also apply diamond sampling to a mips problem , as used in  @xcite .",
    "we focus on the movielens dataset  @xcite , a collaborative filtering application .",
    "we get significantly better precision - recall curves with a maximum precision of 90 - 100% as opposed to 30 - 65% with asymmetric lsh methods .",
    "our running time is a fraction of a second per query .",
    "our diamond sampling requires minimal storage overhead and much less than lsh methods , which require large amounts of memory , easily running into hundred times the dataset size ( requiring a large - memory machine ) .",
    "we use the standard notation that @xmath30=\\set{1,\\dots , n}$ ] . for @xmath31 , the function @xmath32 if @xmath33 and @xmath34 otherwise ;",
    "i.e. , @xmath35 . let @xmath36 denote a vector and @xmath37 be a matrix .",
    "we denote the vector and matrix @xmath38-norms as follows : @xmath39 note that the matrix @xmath38-norms are entrywise rather than the induced norms .",
    "we let @xmath40 denote the number of nonzeros in @xmath41 ; if @xmath41 is dense , then we suppose without loss of generality that @xmath42 .",
    "we assume throughout that @xmath18 and @xmath43 .",
    "we use @xmath44 $ ] to index rows of @xmath5 and @xmath6 .",
    "the @xmath45th rows of @xmath5 and @xmath6 ( transposed to column vectors ) are denoted by @xmath46 and @xmath47 respectively .",
    "we use @xmath48 $ ] to index columns of @xmath5 , whose @xmath49th column is denoted by @xmath50 or @xmath51 .",
    "we use @xmath52 $ ] to index columns of @xmath6 , whose @xmath53th column is denoted by @xmath54 or @xmath55 . since , by definition , @xmath56 ,",
    "we have @xmath57,\\ ;   j \\in [ n].\\ ] ]    if @xmath5 and @xmath6 are binary , i.e. , unweighted adjacency matrices , then we can consider them as representing a tripartite graph on @xmath58 nodes ; see [ fig : tripartite ] . from this interpretation",
    ", we define the neighbor sets , @xmath59 | a_{ki } = 1 } , &    \\mathcal{n}^a_k & = \\set { i \\in [ m ] | a_{ki } = 1 } , \\\\",
    "\\mathcal{n}^b_j & = \\set { k \\in [ d ] | b_{kj } = 1 } , &    \\mathcal{n}^b_k & = \\set { j \\in [ n ] | b_{kj } = 1}.\\end{aligned}\\ ] ] correspondingly , we can define degrees of the nodes , i.e. , @xmath60 random selection is uniform , i.e. , equal probability for all elements of a discrete set , unless stated otherwise .",
    "complexity of diamond sampling depends on the two input matrices .",
    "we start our discussion with the special case of binary matrices @xmath5 and @xmath6 .",
    "we follow with the general case and then discuss other special cases such as nonnegative inputs and computing the maximum in @xmath61 , i.e. , @xmath62 .      to motivate our procedure , we start with the case where @xmath5 and @xmath6 are binary matrices",
    ". we can represent this as a tripartite graph where the @xmath63 columns of @xmath5 , indexed by @xmath49 , correspond to nodes on the left ; the @xmath64 columns of @xmath6 , indexed by @xmath53 , correspond to nodes on the right ; and the @xmath10 common rows of @xmath5 and @xmath6 , indexed by @xmath45 or @xmath65 , correspond to nodes in the center .",
    "edge @xmath66 exists iff @xmath67 ; likewise , edge @xmath68 exists iff @xmath69 .",
    "therefore , @xmath70 is simply the number of common neighbors of @xmath49 and @xmath53 : @xmath71        if node @xmath45 has an @xmath5-neighbor @xmath49 and a @xmath6-neighbor @xmath53 , then we call @xmath72 a `` wedge . ''",
    "the existence of such a wedge implies that @xmath73 .",
    "in fact , there are exactly @xmath70 distinct wedges connecting pair @xmath7 ; see [ fig : tpg - wedge ] .",
    "the probability of selecting a random wedge with endpoints @xmath7 can be shown to be proportional to @xmath70 @xcite .    in diamond sampling ,",
    "our goal is find a `` diamond '' @xmath74 formed by _ two _ intersecting wedges , i.e. , @xmath72 and @xmath75 ; see [ fig : tpg - diamond ] .",
    "note that any pair @xmath7 participates in @xmath76 diamonds ( note that we are not requiring @xmath45 and @xmath65 to be different ) .",
    "hence , the probability of selecting a random diamond of the form @xmath74 is proportional to @xmath76 .",
    "sampling random diamonds will expedite identifying the largest dot products as compared to sampling random wedges ; however , sampling random diamonds is more complex .",
    "thankfully , we can adapt the arguments of jha et al .",
    "@xcite for this purpose . here",
    ", the goal is to find a random three - path of the form @xmath74 . if it closes to form a four - cycle , then it is a random diamond .",
    "moreover , these samples will be uncorrelated .",
    "that is , given a set of random 3-paths , those that complete to a diamond will form a uniform sample of the diamonds .",
    "see [ fig : tpg - diamond ] for a three - path that closes to form a diamond and [ fig : tpg - threepath ] for one that does not",
    ".    finding a random three - path of the form @xmath74 is a multi - step procedure , shown in [ alg : diamond - binary ] and illustrated in [ fig : diamond ] . in [ line : dbw ] , we weight each edge @xmath66 according to the number of three paths it is the center of , i.e. , @xmath77 ( again we do not require @xmath78 ) , and store the weights in a matrix @xmath79 . observe that @xmath79 has the same sparsity pattern as @xmath5 .",
    "in [ line : dbs1 ] , we select a random edge @xmath66 proportional to its weight ( see [ fig : step1 ] ) . to complete the three - path",
    ", we select a random neighbor of @xmath45 in @xmath6 , labeled @xmath53 in [ line : dbs2 ] ( see [ fig : step2 ] ) and a random neighbor of @xmath49 in @xmath5 , labeled @xmath65 in [ line : dbs3 ] ( see [ fig : step3 ] ) .",
    "this yields a uniform random three - path .",
    "if edge @xmath80 exists , i.e. , @xmath81 , then the three - path is a diamond and so we increment the counter @xmath82 in [ line : dbs4 ] ; obviously , @xmath83 .    given matrices @xmath84 and @xmath85 .",
    "+ let @xmath86 be the number of samples .",
    "@xmath87 @xmath88 all - zero matrix of size @xmath89 sample @xmath90 with probability @xmath91 sample @xmath53 from @xmath92 sample @xmath65 from @xmath93 @xmath94 postprocessing ( see [ alg : post ] )        the largest values in @xmath95 correspond to the ( likely ) largest dot products , but we do some further postprocessing to obtain the final answer , as shown in [ alg : post ] .",
    "we are seeking the top-@xmath2 dot products .",
    "we have a budget of @xmath96 dot products , where we assume @xmath97 .",
    "we let @xmath98 denote the indices of all the nonzeros in @xmath95 and @xmath99 denote the top-@xmath100 entries in @xmath95 ; this requires a sort in [ line : post1 ] of at most @xmath86 items ( and generally many fewer , depending on the proportion of three - paths that close into diamonds ) .",
    "we compute the @xmath100 dot products in [ line : post2a , line : post2b , line : post2c ] at a cost of @xmath101 .",
    "finally , we let @xmath102 denote the top-@xmath2 dot products from @xmath99 in [ line : post3 ] , requiring a sort of @xmath100 items .    given @xmath103 .",
    "let @xmath2 be the number of top dot products , and @xmath104 be the budget of dot products .",
    "extract top-@xmath100 entries of @xmath95 , i.e. , @xmath105 and @xmath106 @xmath107 all - zero matrix of size @xmath89 @xmath108 extract top-@xmath2 entries of @xmath109 , i.e. , @xmath110 and @xmath111      we present the binary version as general motivation , but our implementation and analysis are based on the diamond sampling algorithm for general real - valued @xmath5 and @xmath6 in [ alg : diamond ] . in this case , we define the matrix of weights @xmath112 such that @xmath113 , \\ ,",
    "i \\in [ m].\\ ] ] the weight @xmath114 correspond to the weight of all three paths with edge @xmath66 at its center .",
    "this is computed in [ line : dw ] .",
    "the sampling in [ line : ds1 ] has the same complexity as in the binary case , but the sampling in [ line : ds2,line : ds3 ] now has a nonuniform distribution and so has higher complexity than in the binary case .",
    "the postprocessing is unchanged .",
    "given matrices @xmath115 and @xmath116 .",
    "+ let @xmath86 be the number of samples .",
    "@xmath117 @xmath88 all - zero matrix of size @xmath89 sample @xmath90 with probability @xmath91 sample @xmath53 with probability @xmath118 sample @xmath65 with probability @xmath119 @xmath120 postprocessing ( see [ alg : post ] )      if @xmath5 and @xmath6 are nonnegative , the only change is that the sign computations can be ignored in computing the sample increment in [ line : dx ] in [ alg : diamond ] .",
    "this avoids potentially expensive random memory accesses .      if @xmath62 , then @xmath121 is symmetric .",
    "the matrix @xmath95 is not symmetric , although @xmath122}$ ] is .",
    "hence , we modify @xmath95 before by inserting the following step before the postprocessing in [ line : post ] in [ alg : diamond ] : @xmath123 now @xmath95 is symmetric , and the forthcoming analysis is unaffected .      if @xmath62 and @xmath5 is symmetric , then @xmath124 and we can replace [ line : dx ] in [ alg : diamond ] with the following two lines : @xmath125 this exploits the fact that we can swap the role of @xmath45 and @xmath49 in the initial edge sample .",
    "again , @xmath95 may not be symmetric , so we insert [ eq : xsym ] before the postprocessing in [ line : post ] .",
    "let @xmath126 and @xmath127 . in the dense case , @xmath128 and @xmath129 .",
    "the total work is @xmath130 the total storage ( not counting the inputs @xmath5 and @xmath6 ) is @xmath131 we give detailed arguments below and in the implementation discussion in [ sec : implementation ] .",
    "* preprocessing . * for the sampling in [ line : dbs2,line : dbs3 ] , we precompute cumulative , normalized column sums for @xmath6 and the same for rows of @xmath5 , requiring storage of @xmath132 and computation of @xmath133 .",
    "the matrix @xmath79 has the same nonzero pattern as @xmath5 , so the cost to store it is equal to @xmath134 and to compute it is @xmath135 .    * sampling .",
    "* for a straightforward implementation , the cost per sample in [ line : ds1 ] is @xmath136 . for [ line : ds2 ] ,",
    "the cost per sample is @xmath137 ; here , we have used the approximation @xmath138 .",
    "a similar analysis applied for @xmath5 and [ line : ds3 ] .",
    "so , the cost per sample is @xmath139 . without loss of generality",
    ", we assume that we need to store the three - paths and the summand in [ line : dbs4 ] for a total storage of @xmath140 .",
    "* postprocessing . *",
    "conservatively , we require @xmath141 storage for the @xmath142 triples in @xmath99 and @xmath143 storage for the @xmath144 triples in @xmath145 .",
    "the sorting requires at most @xmath146 time , and usually much less since @xmath147 may be much less than @xmath86 due to only some three - paths forming diamonds and concentration , i.e. , picking the same @xmath7 pair multiple times .",
    "this section provides a theoretical analysis of diamond sampling .",
    "we first prove that the expected value of @xmath82 is @xmath148 , and then we prove error bounds on our estimate as a function of the number of samples . unless stated otherwise , our analysis applies to the general version of the diamond - sampling algorithm ( [ alg : diamond ] ) .      for a single instance of [ line : ds1,line : ds2,line : ds3 ] of [ alg : diamond ] ,",
    "we define the event @xmath149    @xmath150 .",
    "the probability of choosing three - path @xmath74 is ( by independence of these choices ) the product of the following probabilities : that of choosing the center edge @xmath66 , then picking @xmath53 , and then picking @xmath65 .",
    "@xmath151    in what follows , we use @xmath152 to be the following random variable : if @xmath153 are the respective indices updated in the @xmath154th iteration , @xmath155 .",
    "otherwise , @xmath156 .",
    "observe that @xmath157 .",
    "[ lem : dexp ] for diamond sampling , @xmath158 } = c_{ij}^2 / \\|w\\|_1 $ ] .",
    "we note that @xmath159 } = { \\mathbb{e}[\\sum_\\ell x_{i , j,\\ell}]}/s = { \\mathbb{e}[x_{i , j,1}]}$ ] .",
    "( we use linearity of expectation and the fact that the @xmath152 are i.i.d . for fixed @xmath153 and varying @xmath154 . )",
    "@xmath160 }      & = \\sum_{k } \\sum_{k ' } \\operatorname{pr}\\bigl ( \\mathcal{e}_{k'ikj } \\bigr ) \\cdot      \\operatorname{sgn}(a_{ki } b_{ki } a_{k'j } ) \\ , b_{k'j } \\\\      & = \\sum_{k } \\sum_{k ' }",
    "\\frac { |a_{ki } b_{kj } a_{k'i}| } { \\|w\\|_1 } \\cdot      \\operatorname{sgn}(a_{ki } b_{kj } a_{k'i } ) \\",
    ", b_{k'j } \\\\      & = \\frac{1}{\\|w\\|_1 }       \\sum_k \\sum_{k ' } a_{ki } b_{kj } a_{k'i }   b_{k'j } \\\\      & = \\frac{1}{\\|w\\|_1 }       \\bigl ( \\sum_k a_{ki } b_{kj } \\bigr )       \\bigl ( \\sum_{k ' } a_{k'i } b_{k'j }   \\bigr ) \\\\      & = \\frac{1}{\\|w\\|_1 }       \\bigl ( \\sum_k a_{ki } b_{kj } \\bigr ) ^2 = \\frac{c_{ij}^2}{\\|w\\|_1}.    \\end{aligned}\\ ] ]      we now provide some concentration bounds when all entries in @xmath5 and @xmath6 are nonnegative .",
    "[ lem : conc ] fix @xmath161 and error probability @xmath162 .",
    "assume all entries in @xmath5 and @xmath6 are nonnegative and at most @xmath163 .",
    "if the number of samples @xmath164 then @xmath165 \\leq \\delta.\\ ] ]    observe that @xmath152 is in the range @xmath166 $ ] .",
    "thus , @xmath167 is in @xmath168 $ ] .",
    "set @xmath169 .",
    "since @xmath170 is the sum of random variables in @xmath168 $ ] , we can apply the standard multiplicative chernoff bound ( theorem 1.1 of  @xcite ) .",
    "this yields @xmath171 } ] < \\exp(-{\\varepsilon}^2{\\mathbb{e}[y_{ij}]}/3)$ ] . by [ lem : dexp ] , @xmath172 } = ( s/{k})(c^2_{ij}/\\|w\\|_1)$ ] , which is at least @xmath173 by choice of @xmath86 .",
    "hence , @xmath171 } ] < \\delta/2 $ ] . note that @xmath174 .",
    "we multiply the expression inside the @xmath175 $ ] by @xmath176 to get the event @xmath177 .",
    "using the chernoff lower tail bound and identical reasoning , we get @xmath178 \\leq \\delta/2 $ ] .",
    "a union bound completes the proof .",
    "the following theorem gives a bound on the number of samples required to distinguish ",
    "large \" dot products from  small \" ones .",
    "the constant @xmath179 that appears is mostly out of convenience ; it can be replaced with anything @xmath180 with appropriate modifications to @xmath86 .",
    "[ thm : sep ] fix some threshold @xmath181 and error probability @xmath162 . assume all entries in @xmath5 and @xmath6 are nonnegative and at most @xmath163 .",
    "suppose @xmath182 .",
    "then with probability at least @xmath183 , the following holds for all indices @xmath153 and @xmath184 : if @xmath185 and @xmath186 , then @xmath187 .",
    "first consider some dot product @xmath70 with value at least @xmath181",
    ". we can apply [ lem : conc ] with @xmath188 and error probability @xmath189 , so with probability at least @xmath190 , @xmath191 .",
    "now consider dot product @xmath192 .",
    "define @xmath193 and @xmath194 as in the proof of [ lem : conc ] .",
    "we can apply the lower tail bound of theorem 1.1 ( third part ) of  @xcite : for any @xmath195}$ ] , @xmath196 < 2^{-b}$ ] .",
    "we set @xmath197 . from [ lem : dexp ] and the assumption that @xmath198 and @xmath199 } \\!= \\!{\\mathbb{e}[x_{i'j'}]}/{k}\\!= \\!sc^2_{i'j'}/{k}\\|w\\|_1 \\!\\leq \\!s\\tau^2/(16{k}\\|w\\|_1 ) \\ !",
    "< \\!b/2e$ ] . plugging in our bound for @xmath86 , @xmath200 @xmath201 .",
    "hence , @xmath202\\ ! < \\!\\delta/(2mn)$ ] .",
    "equivalently , @xmath203 \\ !",
    "< \\!\\delta/(2mn)$ ] .",
    "we take a union bound over all the error probabilities ( there are at most @xmath4 pairs @xmath153 or @xmath184 ) .    in conclusion , with probability at least @xmath183 , for any pair of indices @xmath153 : if @xmath185 , then @xmath204 . if @xmath205 , then @xmath206 .",
    "this completes the proof .    to get a useful interpretation of [ lem : conc ] and [ thm : sep ]",
    ", we ignore the parameters @xmath207 and @xmath208 .",
    "let us also assume that @xmath209 , which is a reasonable assumption for most of our experiments .",
    "basically , to get a reasonable estimate of @xmath70 , we require @xmath210 samples . if the value of the @xmath2-th largest entry in @xmath109 is @xmath181 , we require @xmath211 samples to find the @xmath2-largest entries .",
    "for instance , on a graph , if we want to identify pairs of vertices with at least 200 common neighbors , we can set @xmath212 , and @xmath213 will be the number of ( non - induced ) 3-paths in the graph .",
    "the square in the denominator is what makes this approach work . in [ tab : stats ] of [ sec : experiments ] , we show some of the values of @xmath211 for particular datasets , where @xmath181 is the magnitude of the largest entry .",
    "we discuss the implementation details for reproducibility , but we stress that the implementation is not our primary contribution .",
    "nevertheless , careful thought has gone into the process and we show that a clever implementation of the sampling can improve performance by almost @xmath214 ; see [ fig : perf_lo ] .",
    "we consider two alternative schemes for drawing @xmath86 samples from an arbitrary discrete distribution defined by the vector @xmath215^p$ ] such that @xmath216 .",
    "the choice of schemes is based on the relative sizes of @xmath86 and @xmath38 .",
    "if the size of the distribution , @xmath38 , is smaller than the number of samples , @xmath86 , then we use binary search on the cumulative sums of @xmath217 to determine each sample .",
    "this requires @xmath218 comparisons , plus @xmath219 work for the preprocessing to compute the cumulative sum .",
    "we note that using the alias method @xcite yields a constant time per search at the same cost for preprocessing and storage ( up to a constant ) .",
    "the cumulative sum requires @xmath219 space , and sampled events are stored as counts in space @xmath219 .",
    "note that both the binary search and sample counter increments involve random ( not contiguous ) memory access .",
    "this returns a count vector @xmath220 of length @xmath38 such that @xmath221 is the number of occurrences of event @xmath45 and @xmath222 .",
    "if , on the other hand , the number of samples , @xmath86 , is less than the size of the distribution , @xmath38 , we can avoid the binary search by doing a single sort of the samples , as shown in [ alg : sampling_smalls ] .",
    "this is essentially a variation on merge sort , but we sort only one of the two lists and compute the other on the fly .",
    "the preprocessing involves sorting @xmath86 random numbers , requiring @xmath146 comparisons and @xmath223 space .",
    "sampled events are determined by walking through the sorted samples and the probability distribution , computing the cumulative sums along the way , requiring @xmath224 computations .",
    "sampled events are stored explicitly in space @xmath223 .",
    "note that all memory accesses in this approach are contiguous ( reads and writes ) .",
    "this returns an explicit sample list @xmath225 of length @xmath86 such that @xmath226 .    given probability distribution @xmath227^p$ ] and @xmath86 = # samples    @xmath228 for all @xmath229 $ ] sort the vector @xmath230 so that @xmath231 @xmath232 , @xmath233 @xmath234 , @xmath235 @xmath236    thus , the cost of sampling and storing @xmath86 events from a discrete distribution of size @xmath38 is @xmath237 computations and @xmath238 space .",
    "the implementation of [ alg : diamond ] takes advantage of the specialized sorting mentioned above as well as locality , as we explain .    in the preprocessing , in anticipation of the sampling in [ line : ds2 ] , we compute a matrix @xmath239 such that each row is a normalized cumulative sum , i.e. , @xmath240 .",
    "we store the matrix @xmath239 in compressed sparse row ( csr ) format , so that the entries of @xmath241 are contiguous in memory .",
    "similarily , for [ line : ds3 ] , we compute a matrix @xmath242 such that each column is a normalized cumulative sum , i.e. , @xmath243 .",
    "we store the matrix @xmath244 in compressed sparse column ( csc ) format , so that the entries of @xmath245 are contiguous in memory .",
    "note that storage @xmath242 in csc format is equivalent to storage @xmath246 in csr format , so we need only one data structure .",
    "we separate lines [ line : ds1,line : ds2,line : ds3,line : dx ] into four separate loops , first computing @xmath86 pairs of the form @xmath90 , then @xmath86 @xmath6-neighbors , etc .    for the samples in [ line :",
    "ds1 ] , we use the search via sample sorting in [ alg : sampling_smalls ] for choosing the samples from @xmath79 since typically @xmath247 .",
    "because of the way that [ alg : sampling_smalls ] works , the pairs @xmath248 for @xmath229 $ ] are conveniently sorted according to @xmath249 .",
    "the sorted values yield data locality for [ line : ds2 ] , where we use standard binary search to choose the values @xmath250 for @xmath251 $ ] .",
    "we rearrange the @xmath86 samples in @xmath223 time so that they are ordered according to @xmath252 .",
    "then we use standard binary search to choose the values @xmath253 for @xmath229 $ ] from [ line : ds3 ] .    finally , we reorder the samples in @xmath223 time so that they are sorted according to @xmath253 , enabling efficient lookups for @xmath254 values in [ line : dx ] .    [",
    "fig : perf_lo ] shows a 2.7 times speed - up for our optimized implementation of [ alg : diamond ] versus a straightforward implementation .",
    "in particular , we note the drastic reduction in time for center , left , and right samples and setting the output entry ( which involves searching for the existence of the 4th edge ) in the optimized implementation .",
    "this is due to achieving better data locality ( i.e. , cache performance ) , and the overheads of the reorderings to attain this locality are amortized .    ) . ]",
    "mad corresponds to find the highest entries in a matrix - matrix product .",
    "general high - performance implementations are available . in the dense case ,",
    "the blas interface allows access to vendor - tuned libraries like intel s math kernel library @xcite or nvidia s cublas @xcite . in the sparse case , matrix multiplication is available in csparse @xcite , an efficient open - source library that is used by matlab for many sparse computations .",
    "the computational cost of matrix multiplication is @xmath255 in the dense case ( assuming the classical algorithm is used ) and @xmath256 , i.e. , the number of wedges in the tripartite graph , in the sparse case .",
    "the storage cost of library implementations of matrix multiplication , @xmath4 in the dense case and up to @xmath4 in the sparse case , is generally the limiting factor .    to adapt these high - performance libraries",
    ", we perform a series of matrix - vector products , @xmath257 for @xmath258 $ ] , to compute the columns of @xmath109 one at a time .",
    "we do not save the columns but instead use a priority queue to track the top-@xmath2 entries . because csparse is open source , we were able to modify the code to minimize the memory footprint , achieving @xmath259 , with little loss in performance . in the dense case",
    ", we compute @xmath109 in column blocks to size @xmath260 , computing and processing the output matrix in chunks using the ` dgemm ` interface to mkl , so that the memory footprint is @xmath259 .",
    "all experiments are run on an intel xeon e5 - 2650 `` ivy bridge '' 2.0 ghz machine with 32 gb of memory .",
    "our codes are written in c / c++ with a mex - interface to matlab ( version 8.3.0.532 ) .",
    "the codes are all single - threaded .",
    "we experiment on real - world datasets described below .    * _ as - skitter @xmath61 _ : skitter is an internet topology graph from the `` as - skitter '' dataset from snap @xcite .",
    "this yields a sparse binary symmetric @xmath261 matrix @xmath5 with @xmath64 = 1,696,415 nodes and 11,095,298 nonzeros . *",
    "_ movielens @xmath24 _ : the movielens-10 m data set @xcite comprises a sparse movie - user matrix , @xmath262 , of size @xmath263 with @xmath63 = 65,133 movies and @xmath64 = 71,567 users . following shrivastava and li @xcite , who in turn followed @xcite ,",
    "we compute the low - rank svd of @xmath262 using @xmath264 components , so that @xmath265 where @xmath18 and @xmath19 are dense real - valued matrices . *",
    "_ live journal @xmath61 _ : livejournal is a free online community , and we use the `` soc - livejournal1 '' dataset from snap @xcite .",
    "the corresponding `` friendship '' sparse symmetric binary adjacency matrix @xmath5 has dimension @xmath64 = 4,847,571 and 68,993,773 nonzeros . *",
    "_ asic @xmath61 _ : the asic dataset is a xyce circuit simulation matrix ; we use the `` sandia / asic_680k '' matrix from the florida matrix collection @xcite .",
    "the @xmath266 real - valued matrix @xmath5 is nonsymmetric ( though it is structurally symmetric ) with dimension @xmath64 = 682,862 and 2,638,997 nonzeros . * _ amazon kindle @xmath24 _ : the amazon product data consists of review and product data from amazon @xcite .",
    "the kindle category data includes @xmath63 = 1,406,916 reviewers and @xmath64 = 430,532 books .",
    "the reviewer - book rating matrix @xmath5 contains numeric scores from 15 for reviewed books and has a total of 3,205,546 entries .",
    "the book - book similarity matrix @xmath6 contain numeric scores of 14 to indicate the relationship ( i.e. , 4 indicates the book have been purchased together by someone ) with a total of 11,012,558 entries . * _ million song @xmath61 _ : the echo nest taste profile subset of the million song dataset contains 48 m user - song play counts from real users @xcite .",
    "the resulting user - song matrix @xmath5 has 1,019,318 users , 384,546 songs , and 48,373,586 user - song play counts .",
    "we present time and accuracy results for six data sets in [ fig : topids1,fig : topids2 ] . in this study , we vary @xmath86 as an axis , set @xmath267 , and plot results for @xmath268 .",
    "the top row plots the recall , i.e. , the percentage of the top-@xmath2 entries identified , versus the number of samples , @xmath86 .",
    "( not all samples close to form diamonds ; see [ tab : stats ] . )",
    "the bottom row plots the wall - clock computation time versus the number of samples , including the time for exact computation as described in [ sec : exact - impl ] .",
    "because the budget @xmath100 does not depend on @xmath2 ( we set @xmath267 ) , the timing is the same for all runs .",
    "contains some additional data about the sampling , including the size of the largest entry , the size of @xmath213 , the ratio @xmath269 ( which is proportional to the number of samples needed to recover the largest entry according to [ thm : sep ] ) , and the closure rate of the three paths to form diamonds .",
    ".summary statistics for datasets .",
    "the column labeled `` est .",
    "samples '' reports the ratio @xmath269 , the estimated number of samples required to find the top entry .",
    "the column labeled `` closure '' is the percentage of sampled three paths that correspond to a successful diamond sample . [",
    "cols=\"^,^,^,^,^\",options=\"header \" , ]     [ 2for1 ] a retailer wants to select , say , 100 pairs of products for a 2-for - the - price - of-1 promotion .",
    "if we assume each product has a dense or sparse representation in some feature space , this becomes a mad search with @xmath62 . in the case of the million song dataset ,",
    "we let @xmath5 denote a song - by - user matrix where entry @xmath7 denotes the number of plays of song @xmath49 by user @xmath53 . we want to find pairs of distinct songs that have the highest number of common plays .    for this",
    ", we find the top-@xmath2 pairs of similar songs in the million song dataset , based on being played by the same users .",
    "we calculate the top-@xmath2 all - pairs dot products for columns of @xmath5 as explained in [ sec : gram ] . additionally , we ignore the diagonal entries that pair songs with themselves . the top song pair using this",
    "metric is `` undo '' by bjrk and `` revelry '' by kings of leon , which are both in the alternative genre .",
    "we want to find members of a social network that should be connected but are not .",
    "these can be used as recommendations for new `` friends . ''",
    "we use the live journal data .",
    "the top entry in @xmath270 is not an edge in @xmath5 ; 6 out of top 10 are not ; 55 out of top 100 are not ; and 511 out of top 1000 are not .",
    "the top-10 new - connection suggestions have over 1800 common neighbors per pair .",
    "the most similar related work is that of cohen and lewis @xcite , which we refer to as wedge sampling . in this section , we compare diamond sampling to wedge sampling and present results for the skitter dataset , which showed the most distinction between the methods .",
    "we generalized cohen and lewis approach to @xmath2-mad and implemented wedge sampling with similar optimizations to those described in [ sec : lo ] .",
    "in general , using diamonds requires fewer ( three - path ) samples than wedge samples to identify top entries in the output matrix . in our implementation ,",
    "the preprocessing cost for diamonds is greater than for wedges , but the per - sample costs of each method are roughly the same .    in [ fig : wedge ] , we present top-@xmath2 scores and times for wedge and diamond sampling on the skitter dataset . in these experiments",
    ", we set the budget of dot products to be @xmath271 . for fewer than @xmath272 samples ,",
    "the diamond sampling has much better accuracy , but because the time is dominated by preprocessing , diamond sampling is also more expensive . for greater than @xmath272 samples ,",
    "the time is dominated by sampling and computing dot products , so the running times of diamond and wedge sampling approach are roughly the same .",
    "however , diamond sampling has identified all top entries by @xmath272 samples , while wedge sampling needs @xmath273 or @xmath274 samples to identify all top entries , requiring an order of magnitude more time .",
    "we provide a comparison to some experiments in the paper by shrivastava and li @xcite ( additional details in @xcite ) for the movielens-10 m data set @xcite .",
    "user @xmath53 corresponds to column @xmath54 . for each user @xmath53",
    ", we want to find the top-10 movie recommendations . in other words , for each user , we want to solve the @xmath45-mips problem .",
    "precision - recall results over 2000 random users using asymmetric lsh and an increasing number of hash functions , @xmath275 , are reported in @xcite . the amount of storage increases with @xmath276 . for comparison , we reproduce the curves reports in their figures , although we did not redo the experiments .",
    "we also pick 2000 random users and apply our diamond sampling approach in [ alg : diamond ] repeatedly , using just a _ single column _ of @xmath6 in each application .",
    "the @xmath5 matrix is approximately 79 mb in size .",
    "we need to keep one object that is the size of @xmath5 and a few vectors of length @xmath63 or @xmath86 , for a total of less that 100 mb extra storage .",
    "while it is hard to pin down the exact storage of lsh methods , it is on the order of one to two magnitudes more than the dataset .",
    "it is well - known that lsh is memory intensive ( this is explicitly called out in the e2-lsh manual  @xcite ) .",
    "we use an increasing number of samples , @xmath277 .",
    "we set the dot - product budget to be @xmath267 .",
    "average precision - recall curves are shown in [ fig : lsh ] .",
    "it is difficult to compare the methods directly , so we can not say that using 64 samples is comparable to using 64 hash functions .",
    "however , we can say that the storage per sample is much less than the storage per hash .",
    "thanks to madhav jha and kevin matulef for helpful discussions regarding this work .",
    "ballard s work is funded by a harry s. truman postdoctoral fellowship .",
    "this material is based upon work supported by the u.s .",
    "department of energy , office of science , office of advanced scientific computing research , complex interconnected distributed systems ( cids ) program and the darpa graphs program .",
    "sandia national laboratories is a multi - program laboratory managed and operated by sandia corporation , a wholly owned subsidiary of lockheed martin corporation , for the u.s .",
    "department of energy s national nuclear security administration under contract de - ac04 - 94al85000 .",
    "p.  cremonesi , y.  koren , and r.  turrin , `` performance of recommender algorithms on top - n recommendation tasks , '' _ proc .",
    "recsys 10 _ , 2010 .",
    "http://dx.doi.org/10.1145/1864708.1864721[doi:10.1145/1864708.1864721 ]",
    "g.  salton , j.  allan , and c.  buckley , `` approaches to passage retrieval in full text information systems , '' in _ proc .",
    "sigir 93 _ 1em plus 0.5em minus 0.4em http://dx.doi.org/http://doi.acm.org/10.1145/160688.160693[doi:10.1145/160688.160693 ] .",
    "isbn 0 - 89791 - 605 - 0 pp",
    ". 4958 .",
    "m.  w. berry , s.  t. dumais , and g.  w. obrien , `` using linear algebra for intelligent information retrieval , '' _ siam review _",
    "37 , no .  4 , pp . 573595 , 1995 .",
    "http://dx.doi.org/10.1137/1037127[doi:10.1137/1037127 ]    d.  v. kalashnikov , s.  mehrotra , and z.  chen , `` exploiting relationships for domain - independent data cleaning , '' in _ proc .",
    "sdm05 _ , apr .",
    "http://dx.doi.org/10.1137/1.9781611972757.24[doi:10.1137/1.9781611972757.24 ] pp . 262273 .",
    "f.  angiulli and c.  pizzuti , `` an approximate algorithm for top-@xmath45 closest pairs join query in large high dimensional data , '' _ data & knowledge engineering _ , vol .",
    "53 , no .  3 , pp .",
    "263281 , jun .",
    "]    e.  cohen and d.  d. lewis , `` approximating matrix multiplication for pattern recognition tasks , '' _ j. algorithms _ , vol .",
    "30 , no .  2 ,",
    "pp . 211252 , 1999 .",
    "]      t.  bertin - mahieux , d.  p.  w. ellis , b.  whitman , and p.  lamere , `` the million song dataset , '' in _ proc .",
    "ismir 2011_.1em plus 0.5em minus 0.4emuniversity of miami , 2011 .",
    "b.  mcfee , t.  bertin - mahieux , d.  p. ellis , and g.  r. lanckriet , `` the million song dataset challenge , '' in _",
    "www12 companion : proc .",
    "21st intl .",
    "companion on world wide web _ ,",
    "http://dx.doi.org/10.1145/2187980.2188222[doi:10.1145/2187980.2188222 ] pp . 909916 .",
    "f.  g. gustavson , `` two fast algorithms for sparse matrices : multiplication and permuted transposition , '' _ acm t. math . soft . _ ,",
    "vol .  4 , no .  3 , pp . 250269 , sep .",
    "http://dx.doi.org/10.1145/355791.355796[doi:10.1145/355791.355796 ]",
    "p.  drineas and r.  kannan , `` fast monte - carlo algorithms for approximate matrix multiplication , '' in _ proc .",
    "focs01 _ , oct .",
    "http://dx.doi.org/10.1109/sfcs.2001.959921[doi:10.1109/sfcs.2001.959921 ] pp . 452459 .",
    "p.  drineas , r.  kannan , and m.  w. mahoney , `` fast monte carlo algorithms for matrices i : approximating matrix multiplication , '' _",
    "siam j. computing _ ,",
    "36 , no .  1 ,",
    "132157 , jan . 2006 .",
    "http://dx.doi.org/10.1137/s0097539704442684[doi:10.1137/s0097539704442684 ]",
    "t.  sarls , `` improved approximation algorithms for large matrices via random projections , '' in _ proc .",
    "focs 06 _ , oct .",
    "http://dx.doi.org/10.1109/focs.2006.37[doi:10.1109/focs.2006.37 ] pp . 143152 .",
    "belabbas and p.  wolfe , `` on sparse representations of linear operators and the approximation of matrix products , '' in _ proc .",
    "ciss 2008 _ , mar .",
    "http://dx.doi.org/10.1109/ciss.2008.4558532[doi:10.1109/ciss.2008.4558532 ] pp . 258263 .",
    "a.  magen and a.  zouzias , `` low rank matrix - valued chernoff bounds and approximate matrix multiplication , '' in _ proc . soda 11 _ , 2011 .",
    "http://dx.doi.org/10.1137/1.9781611973082.109[doi:10.1137/1.9781611973082.109 ] pp . 14221436 .",
    "r.  pagh , `` compressed matrix multiplication , '' _ acm transactions on computation theory ( toct ) _ , vol .  5 , no .  3 , pp .",
    "117 , aug .",
    "http://dx.doi.org/10.1145/2493252.2493254[doi:10.1145/2493252.2493254 ]",
    "j.  t. holodnak and i.  c.  f. ipsen , `` randomized approximation of the gram matrix : exact computation and probabilistic bounds , '' _ siam j. matrix analysis and applications _ , vol .",
    "36 , no .  1 , pp . 110137 , 2015 .",
    "http://dx.doi.org/10.1137/130940116[doi:10.1137/130940116 ]",
    "a.  campagna and r.  pagh , `` finding associations and computing similarity via biased pair sampling , '' _ knowledge and information systems _ , vol .",
    "31 , no .  3 , pp .",
    "505526 , jun . 2011 .",
    "http://dx.doi.org/10.1007/s10115-011-0428-y[doi:10.1007/s10115-011-0428-y ]",
    "m.  vose , `` a linear algorithm for generating random numbers with a given distribution , '' _ ieee t. soft .",
    "_ , vol .",
    "17 , no .  9 , pp . 972975 , 1991 .",
    "http://dx.doi.org/10.1109/32.92917[doi:10.1109/32.92917 ]        t.  a. davis and y.  hu , `` the university of florida sparse matrix collection , '' _ acm t. math",
    "_ , vol .",
    "38 , no .  1 ,",
    "1:11:25 , dec . 2011 .",
    "http://dx.doi.org/10.1145/2049662.2049663[doi:10.1145/2049662.2049663 ] .",
    "j.  mcauley , c.  targett , j.  shi , and a.  van  den hengel , `` image - based recommendations on styles and substitutes , '' in _ proc .",
    "sigir15 _ , 2015 ."
  ],
  "abstract_text": [
    "<S> given two sets of vectors , @xmath0 and @xmath1 , our problem is to find the top-@xmath2 dot products , i.e. , the largest @xmath3 among all possible pairs . </S>",
    "<S> this is a fundamental mathematical problem that appears in numerous data applications involving similarity search , link prediction , and collaborative filtering . </S>",
    "<S> we propose a sampling - based approach that avoids direct computation of all @xmath4 dot products . </S>",
    "<S> we select diamonds ( i.e. , four - cycles ) from the weighted tripartite representation of @xmath5 and @xmath6 . </S>",
    "<S> the probability of selecting a diamond corresponding to pair @xmath7 is proportional to @xmath8 , amplifying the focus on the largest - magnitude entries . </S>",
    "<S> experimental results indicate that diamond sampling is orders of magnitude faster than direct computation and requires far fewer samples than any competing approach . </S>",
    "<S> we also apply diamond sampling to the special case of maximum inner product search , and get significantly better results than the state - of - the - art hashing methods . </S>"
  ]
}