{
  "article_text": [
    "compressed sensing is a framework that enables an @xmath1-dimensional sparse signal @xmath4 to be recovered from @xmath5 linear measurements of its elements , @xmath6 , by exploiting the prior knowledge that @xmath7 contains many zero elements @xcite . a simple consideration guarantees that @xmath8-recovery",
    ", @xmath9 where @xmath10 denotes the number of non - zero elements in @xmath11 , is theoretically optimal in terms of minimizing the number of measurements @xmath12 necessary for perfectly recovering any original signal @xmath7 .",
    "however , carrying out @xmath8-recovery for a general measurement matrix @xmath13 is np - hard .",
    "to avoid such computational difficulties , an alternative approach , @xmath14-recovery @xmath15 where @xmath16 , is widely employed , as ( [ l1 ] ) is generally converted into a linear programming problem , and therefore , signal recovery is mathematically guaranteed in an @xmath17 computational time through the use of the interior point method",
    ". nevertheless , the @xmath17 cost of computation can still be unacceptably high in many practical situations , and much effort is being put into finding more computationally feasible and accurate recovery schemes @xcite .    among such efforts ,",
    "the recovery scheme recently proposed by krzakala et al .",
    "@xcite is worth special attention .",
    "their scheme basically follows the bayesian approach .",
    "namely , the signal recovery problem is formulated as one of statistical inference from the posterior distribution , @xmath18 where @xmath19 is a normalization factor imposing the condition @xmath20 and a component - wise prior distribution @xmath21 $ ] is assumed .",
    "@xmath22 and @xmath23 represent the density of the non - zero signal elements and a gaussian distribution , respectively . exactly inferring @xmath7 from ( [ posterior ] )",
    "is np - hard , similarly to ( [ l0 ] ) .",
    "however , by employing the belief propagation ( bp ) in conjunction with the expectation - maximization ( em ) algorithm for estimating @xmath22 and the parameters of @xmath24 , they developed an approximation algorithm , termed em - bp , which has better recovery performance than ( [ l1 ] ) with a computational cost of only @xmath25 .",
    "furthermore , they showed that , by employing a peculiar type of `` seeded '' matrix @xmath13 , the threshold of the compression rate @xmath26 of em - bp , above which the original signal is typically recovered successfully , can approach very close to that of @xmath8-recovery , @xmath27 , where @xmath0 is the actual signal density of @xmath7 and @xmath28 stands for ` seeded em - bp '",
    ". the seeded matrix is composed of blocks along the diagonal densely filled with gaussian random variables .",
    "it is important to remember that this result is achieved for the first time with an approach different from @xmath8-recovery , being the threshold for @xmath14-recovery that is much higher than the optimal one : @xmath29 .",
    "the optimality of the @xmath8-recovery is guaranteed for em - bp , which means that this scheme can practically achieve the theoretically optimal threshold of signal recovery with an @xmath25 computational cost .",
    "this remarkable property was recently proved in a mathematically rigorous manner in the case that the matrix entries satisfy certain conditions concerning their statistics  @xcite .",
    "however , it is still unclear whether their scheme is optimal in terms of the computational complexity ; there might be a certain design of the measurement matrix @xmath13 that makes it possible to further reduce the computational cost while keeping the same signal recovery threshold .",
    "the purpose of the present study is to explore such a possibility . for this",
    ", we focus on a class of matrices that are characterized by the following properties :    * * sparsity : * the matrix @xmath13 has only @xmath30 non - zero elements per row and column .",
    "this implies that the measurements can be performed in a time linear in the signal length .",
    "this situation is highly preferred for the sake of practicality , given that such an operation typically needs to be done in real time , during data acquisition . *",
    "* integer values : * the matrix elements are not real valued , but take on small integer values .",
    "this means that an optimized code for the measurements can work with bitwise operations , thus achieving much better performance without any loss of precision . *",
    "* no block structure : * the block structure used in @xcite may not be necessary for reaching the optimal threshold . as an alternative possibility , we study a structure made of a square matrix in the upper left corner ( the seed ) plus a stripe along the diagonal .",
    "this structure is much more amenable for analytic computations , since it corresponds to a one - dimensional model homogeneous in space .",
    "the use of sparse matrices for compressed sensing has already been suggested in several earlier studies @xcite .",
    "our approach is particularly similar to that of @xcite in the sense that both sides are based on the bayesian framework and use integer - valued sparse measurement matrices . nevertheless , these two approaches differ considerably in the following two points .",
    "firstly , we adopt em - bp , which updates only a few variables per node for the signal recovery , while the recovery algorithm of @xcite involves functional updates and needs significantly more computational time than ours .",
    "secondly , we thoroughly explored a simple design of @xmath13 that achieves nearly optimal recovery performance . in contrast , the problem of the matrix design is not fully examined in @xcite . by carrying out extensive numerical experiments in conjunction with an analysis based on density evolution @xcite , we show that a threshold close to the theoretical limit @xmath31 can be achieved by using matrices with the above properties with an almost _ linear _ computational cost in the measurement and recovery stages .",
    "this paper is organized as follows . in section [ sec : em - bp ] , the em - bp algorithm is briefly explained and the results for dense matrices are summarized . the algorithm is applied to homogeneously sparse matrices in section [ sec : sparse ] and to structured sparse block matrices in section [ sec : block ] .",
    "a new type of `` striped '' sparse matrix without blocks is introduced in section [ sec : stripe ] .",
    "the last section summarizes our work , focusing on its importance for practical use , and touches on future issues .",
    "the new algorithm based on bp in conjunction with the em proposed in ref .",
    "@xcite starts from eq .",
    "( [ posterior ] ) .",
    "a similar idea was also proposed in ref .",
    "in order to solve it with bp , @xmath32 messages for the probability distributions of the variables @xmath33 are constructed in the following way : @xmath34 \\prod_{\\gamma\\neq \\mu } m_{\\gamma\\rightarrow i}(x_i)\\end{aligned}\\ ] ] where @xmath35 and @xmath36 are normalization factors .",
    "this em - bp equations are very complicated because the messages are distribution functions . in order to make them simpler",
    ", the messages can be approximated by assuming that they are gaussian , thus obtaining the equations for the mean @xmath37 and the variance @xmath38 of @xmath39 .",
    "this approximation was introduced for sparse matrices in refs .",
    "@xcite , and it becomes asymptotically exact if @xmath13 is dense .",
    "in fact , it is derived from an expansion in small @xmath40 , and in the dense case @xmath41 . supposing that the elements of the original signal follow a bernoulli - gaussian distribution with parameters @xmath0 , @xmath42 and @xmath43 , the update rules for the messages are the following : @xmath44 where @xmath45 and @xmath46 are some analytical functions depending on the parameters @xmath22 , @xmath47 and @xmath48",
    ". for details , see ref .",
    "@xcite .    in general ,",
    "the original @xmath0 , @xmath42 and @xmath43 are not known , but one can use em to derive the update rules for them , using the property that the partition function @xmath49 is the likelihood of the parameters @xmath50 and is maximized by the true parameters @xmath0 , @xmath42 , and @xmath43 . thus , after the update of all the messages , the inferred parameters of the original distribution are updated following these rules : @xmath51^{-1}}\\;,\\end{aligned}\\ ] ] with @xmath52 and @xmath53 .",
    "if the algorithm converges to the correct solution , @xmath54 and @xmath55 .    to reduce the number of messages from @xmath56 to @xmath57 , one can see that in the large @xmath1 limit , the messages @xmath37 and @xmath38 are nearly independent of @xmath58 .",
    "thus , we can derive the equations involving only a variable per each measurement node and a variable per signal node , if we are careful to keep the correcting onsager reaction term as in the tap equations of statistical physics @xcite .",
    "this method was introduced in the context of compressed sensing in ref .",
    "@xcite and is called approximated message passing ( amp ) .    in general",
    ", the correct distribution of the original signal is unknown .",
    "however , in ref .",
    "@xcite , it is demonstrated that if @xmath59 , the most probable configuration of @xmath11 with respect to @xmath21 $ ] with @xmath60 , restricted to the subspace @xmath61 , is the original signal @xmath7 , even if the signal is not distributed according to @xmath62 .",
    "so our choice of a gaussian distribution for @xmath24 should be perfectly fine even if the original signal has a different distribution .",
    "the free entropy @xmath63 at a fixed mean square error @xmath64 can be computed if a dense matrix is used . for @xmath59 ,",
    "the global maximum of the function @xmath63 is at @xmath65 , that corresponds to the correct solution . however , below a certain threshold @xmath66 that depends on the distribution @xmath67 , the free entropy develops a secondary , local maximum at @xmath68 . as a consequence",
    ", the em - bp algorithm can not converge to the correct solution for @xmath69 , because a dynamical transition occurs .",
    "nonetheless , the threshold @xmath70 is lower than @xmath71 .",
    "top : probability of perfect recovery versus the signal sparsity @xmath0 using sparse matrices with @xmath72 and @xmath73 .",
    "the threshold is the same as with dense matrices .",
    "bottom : probability of perfect recovery computed with density evolution has the same threshold . here",
    ", @xmath1 is the population size.,title=\"fig : \" ] top : probability of perfect recovery versus the signal sparsity @xmath0 using sparse matrices with @xmath72 and @xmath73 .",
    "the threshold is the same as with dense matrices .",
    "bottom : probability of perfect recovery computed with density evolution has the same threshold . here",
    ", @xmath1 is the population size.,title=\"fig : \" ]    first of all , we want to verify if the use of a sparse matrix can reach the same results as the use of a dense one . for a sparse random matrix",
    ", the amp equations can not be used ; thus , we can use the update rules in eq .",
    "( [ eq : updates ] ) for inferring the original signal @xmath7 . in particular , we choose the matrix @xmath13 to have only @xmath74 elements different from zero in each row and @xmath75 elements in each column , extracting them from the distribution , @xmath76 with @xmath77",
    ". the use of the messages @xmath37 and @xmath38 instead of the amp equations does not involve an extra cost in memory , because the number of the messages is @xmath57 from the sparsity of the matrix . in principle , the messages @xmath78 are not gaussian if the matrix is sparse , so the use of only the two parameters @xmath37 and @xmath38 is not exact .",
    "however , the convolution of @xmath79 messages ( with @xmath73 in a typical matrix we use ) is not far from a gaussian , and indeed , we can verify a posteriori that this approximation is valid , because it gives good results .    in all our numerical simulations , we use a bernoulli - gaussian distributed signal and a compression rate @xmath72 .    figure  [ sparse ] ( top ) shows the probability of perfect recovery as a function of the sparsity of the signal @xmath0 for different sizes , by applying the em - bp algorithm using a sparse matrix with @xmath73 .",
    "the threshold for perfect recovery in the thermodynamic limit ( @xmath80 ) is @xmath81 , which is the same as the one obtained in ref .",
    "@xcite with a dense matrix .",
    "we can not analytically compute the free entropy @xmath63 as in @xcite , because we use sparse matrices and can not use methods such as the saddle point one .",
    "however , we performed a numerical density evolution analysis , as shown in fig .",
    "[ sparse ] ( bottom ) , and found that the threshold is almost the same as the one computed with the matrices @xmath13 .",
    "stability check of the solution determined by the em - bp message passing algorithm .",
    "starting the recovery process with a sparse matrix from an initial condition differing less than @xmath82 from the correct solution , the latter is recovered as long as @xmath83 . in the limit @xmath84 , the stability limit @xmath85 tends to the theoretical bound @xmath2 ( which is 0.5 ) . ]    next , we will verify that the correct solution is always the global maximum of @xmath63 and it is locally stable up to @xmath86 when using em - bp with a sparse matrix .",
    "since we can not analytically compute the free entropy , we must resort to a numerical method .",
    "we start em - bp with an initial condition very close to the correct solution : @xmath87 , with @xmath88 a random number uniformly distributed in @xmath89 $ ] . in this way",
    ", we have verified ( see fig .  [ stability ] ) that if @xmath82 is sufficiently small , the correct solution can be found up to @xmath86 , as in the case of a dense matrix .    for the algorithms based on the @xmath14 minimization",
    ", it is known that the threshold with a sparse matrix is lower than that with a dense one",
    ". however , these algorithms are not optimal , because the correct solution disappears below the threshold @xmath71 . in this sense ,",
    "the em - bp algorithm is optimal , because the global maximum of the free entropy is always on the correct solution .",
    "thus , one can expect that , if the rank of the sparse matrix is the same as that of the dense one , a similar threshold can be reached ( as we have demonstrated numerically ) .    in summary",
    ", we can say that the em - bp algorithm of ref .",
    "@xcite seems to reach the same threshold @xmath70 , either using a dense gaussian matrix or a sparse binary one if the numbers of non - zero elements per row / column are @xmath30 but sufficiently large .",
    "however , the use of a sparse matrix is computationally much faster than the use of a dense one .",
    "moreover , the use of binary elements , instead of gaussian real values , allows for better code optimization and eventually for hard - wired encoding of the compression process .",
    "@xmath90 with a sparse , structured matrix with blocks , for different sizes @xmath1 and values of @xmath91 ( see text ) . the thresholds for the @xmath14-recovery and for em - bp without any structure",
    "are also drawn .",
    "for comparison with the data in fig .",
    "[ unid ] , we used the exponent @xmath92 best fitting those data . ]    to avoid the secondary maximum of the free entropy , in ref .",
    "@xcite , the authors use a structured block matrix that helps to nucleate the correct solution .",
    "the idea is that the correct solution is found for the first variables , and then it propagates to the whole signal .",
    "this idea is similar to the so - called spatial coupling that is very useful for solving many different problems @xcite . with this trick , the authors of ref .",
    "@xcite reach perfect recovery for almost any @xmath93 in the large @xmath1 limit while ref .",
    "@xcite reports that the gain is quite small when different recovery algorithms are used . here",
    ", we try to use a matrix with the same block structure , but sparsely filled .",
    "we divide the @xmath1 variables into @xmath91 groups of size @xmath94 and @xmath12 measurements into @xmath91 groups of size @xmath95 in such a way that @xmath96 and @xmath97 .    in this way",
    ", the matrix @xmath98 is divided into @xmath99 blocks , labeled with indices @xmath100 .",
    "each block is a sparse binary matrix with @xmath101 elements different from zero for each row and @xmath102 elements for each column , distributed according to eq .",
    "( [ eq : pf ] ) , with @xmath103 . as in ref .",
    "@xcite , we choose @xmath104 , @xmath105 , @xmath106 , and @xmath107 otherwise . the important ingredient to nucleate",
    "the correct solution is that in the first block @xmath108 holds . for simplicity",
    ", we can choose @xmath109 and @xmath110 for @xmath111 .",
    "the recovery strongly depends on the parameters @xmath112 and @xmath113 , and the best results for @xmath72 are obtained around @xmath114 and @xmath115 .",
    "moreover , we used these two values in the experiments described below because we wanted to work with matrices with elements having small integer values .    similarly to the dense case , the use of a sparse structured matrix with blocks allows to overcome the dynamical transition at @xmath70 and to nucleate the correct solution until @xmath2 is very close to @xmath0 .",
    "figure  [ structured ] shows the mean critical threshold @xmath90 for different signal lengths at a fixed compression rate @xmath72 .",
    "the @xmath116 axis uses the same scaling variable as in fig .",
    "[ unid ] , and the best parameter @xmath117 obtained from the fit of data in fig .",
    "[ unid ] also interpolates the data in fig .",
    "[ structured ] quite well . in the thermodynamic limit , @xmath118 extrapolates to a value compatible with the optimal one , @xmath2 , and it is certainly much higher than the thresholds for @xmath14-recovery and for em - bp without any structure .",
    "we have also done a density evolution analysis that confirms this result .    for each value of @xmath1 and @xmath91 ,",
    "the mean critical threshold @xmath118 is computed as follows .",
    "we randomly generate a block structured matrix @xmath98 with the given @xmath1 and @xmath91 .",
    "we start with a sufficiently sparse original signal @xmath7 , which has been recovered by the algorithm ; we then add non - zero entries to the signal and check whether the new signal can be recovered by the algorithm ; we go on adding non - zero elements to the signal until a failure in a perfect recovery occurs .",
    "the previous to the last value for @xmath0 is the critical threshold for the matrix @xmath98 .",
    "the mean critical threshold is obtained by averaging over many different random matrices and signals , with the same values of @xmath1 and @xmath91 .",
    "the number of such random extractions goes from @xmath119 for the largest @xmath1 value up to @xmath120 for the smallest @xmath1 value .",
    "the values of @xmath121 used for the simulations shown in fig .  [",
    "structured ] are the following : @xmath122 , @xmath123 , @xmath124 , and @xmath125 .",
    "we need to increase both @xmath1 and @xmath91 if we want to obtain good results in the thermodynamic limit . however , if we change @xmath91 , we must change @xmath101 too . indeed , in order to have the same number of elements per row and column for each of the @xmath99 blocks",
    ", we must satisfy the conditions : @xmath126 with @xmath101 and @xmath127 integer valued . here , we have used the smaller possible value for @xmath101 , that is @xmath128 . the fact that it is impossible to keep @xmath101 constant while increasing @xmath91 implies that these kinds of block - structured matrices always become dense in the thermodynamic limit .",
    "this is a limitation of the block structure that we want to eliminate with the matrix proposed in the following section .",
    "the matrix proposed in ref .",
    "@xcite is not the only one that allows the optimal threshold to be reached .",
    "reference @xcite analyzes the use of other good dense , block - structured matrices .",
    "however , the block - structure is not so simple to handle if one wants to do analytical calculations in the continuum limit .",
    "moreover , in making these block - structured matrices sparse , one has to be careful to find the right values of @xmath129 . for these reasons ,",
    "we want to know if the block structure is crucial , and , if not , we want to eliminate it .",
    "a nearly one - dimensional sparse matrix with a square block of size @xmath130 at the top left and non - zero elements in the stripes around the diagonal can achieve compression and perfect recovery close to the theoretical bound in linear time . ]",
    "we tried a different structured sparse matrix ( see fig .",
    "[ matrixunid ] ) , that we called a striped matrix .",
    "it has one sparse square block of size @xmath91 on the top left of the matrix with @xmath74 elements for each row and column extracted from ( [ eq : pf ] ) with @xmath77 .",
    "this arrangement is fundamental for nucleating the correct solution .",
    "apart from this first block , the residual compression rate is @xmath131 .",
    "then , we construct a one - dimensional structure around the diagonal of the remaining matrix . for each column @xmath132",
    ", we randomly place @xmath133 non - zero elements , again extracted from ( [ eq : pf ] ) , in the interval of the width @xmath134 around the diagonal .",
    "one element with @xmath77 is always placed on the diagonal ( actually on the position closest to the diagonal ) . for the remaining elements , we use the following rules . if the element is at a distance @xmath135 from the diagonal , we use @xmath136 .",
    "otherwise , if its distance is @xmath137 , we use @xmath138 below the diagonal and @xmath139 above the diagonal . in this way , the number of elements per column is constant , while the number of elements per row is a truncated poisson random variable with mean @xmath140 : indeed , there are no empty rows , thanks to the rule of placing the first element of each column closest to the diagonal .",
    "when constructing the matrix , we apply exactly the same rule to each column , but in the last @xmath91 columns it may happen that a non - zero element has a row index larger than @xmath12 : these elements are then moved below the first square matrix by changing the row and column indices as follows : @xmath141 and @xmath142 .",
    "@xmath90 with a sparse , striped matrix , as described in section [ sec : stripe ] for different sizes ( from @xmath143 to @xmath144 ) .",
    "the thresholds for @xmath14-recovery and for em - bp without structure are drawn for comparison .",
    "the best fitting parameter is @xmath92 , and it leads to an extrapolation of @xmath90 in the thermodynamic limit compatible with @xmath2 . ]    in this way , we have some kind of continuous one - dimensional version of the block - structured matrix discussed in the previous section . within this striped matrix ensemble , the thermodynamic limit at a fixed matrix sparsity can be calculated without any problem , by sending @xmath145 at a fixed @xmath146 and fixed @xmath74 . in fig .",
    "[ unid ] , we show the mean critical threshold reached by using striped matrices with a fixed ratio @xmath147 ( the same used in the plot of fig .  [ matrixunid ] ) and different signal lengths .",
    "perfect decoding up to @xmath118 is again achieved by using the em - bp algorithm .",
    "we extrapolated the @xmath90 data to the thermodynamic limit by assuming the following behavior in the large @xmath1 limit : @xmath148 the data in fig .",
    "[ unid ] are plotted with the best fitting parameter @xmath92 , and the extrapolated value @xmath149 is perfectly compatible with the theoretical bound @xmath2 .",
    "hence , we can conclude that the important ingredient to reach optimality is not the block structure , but the nearly one - dimensional structure , associated with the initial block with @xmath150 to nucleate the correct solution .",
    "it is worth noticing that the corresponding statistical mechanics model for these striped random matrices is a one - dimensional disordered model with an interaction range growing with the signal length , as in a kac construction .",
    "models of this kind are analytically solvable and have shown very interesting results @xcite .",
    "actual time ( in seconds ) for recovery of a signal with dense and sparse matrices for different data lengths @xmath1 .",
    "the data are fitted respectively by a quadratic and a linear function . ]",
    "the use of our striped sparse matrices allows for a great reduction in computational complexity .",
    "indeed , the measurement and recovery times grow linearly with the size of the signal if sparse matrices are used , while they grow quadratically if dense matrices are used .",
    "figure [ fig : time ] shows the measurement and recovery times of a signal for different signal lengths @xmath1 . for this test , we used dense block - structured matrices and sparse striped matrices .",
    "the number of em - bp iterations to reach the solution is roughly constant for different @xmath1 .",
    "a quadratic fit for the dense case and a linear fit for the sparse one perfectly interpolate the data .",
    "we introduced an ensemble of sparse random matrices @xmath98 that , thanks to their particular structure ( see figure [ matrixunid ] ) , allow us to perform the following operations in linear time : + ( i ) measurement of a @xmath0-sparse vector @xmath151 of length @xmath1 by using a linear transformation , @xmath152 , to a vector @xmath153 of length @xmath154 + ( ii ) perfect recovery of the original vector @xmath151 by using a message passing algorithm ( expectation maximization belief propagation ) for almost any parameter satisfying the theoretical bound @xmath155 .",
    "these striped sparse matrices that have such good performance because there is a ` seeding ' sparse square matrix in the upper left corner that nucleates a seed for the right solution and the one - dimensional structure along the diagonal propagates the initial seed to the complete right solution . both seeding and a one - dimensional structure",
    "have been used in the past @xcite , but in our new ensemble , the matrices are sparse , and this permits us to perform all the operations in a time linear in the signal length .",
    "we also checked that sparse matrices perform as well as dense ones in the case of block - structured matrices and for matrices with no structure at all .",
    "apart from the compressed sensing case , several other applications require a sparse matrix or equivalently linear time complexity @xcite    in data streaming computing , one is typically interested in doing very quick measurements in constant time .",
    "for example , if the task is to measure the number of packets @xmath156 with destination @xmath157 passing through a network router , it is not possible to keep a vector @xmath7 because it is generally too long . instead",
    ", a much shorter sketch of it , @xmath6 , is measured in such a way that a very sparse vector @xmath7 can be recovered from @xmath158 .",
    "the matrix @xmath13 must be sparse in order to be able to update the sketch @xmath158 in a constant time for each new packet passing through the router .",
    "another interesting application is the problem of group testing , where a very sparse vector @xmath159 is given and one is interested in performing the fewest linear measurements , @xmath6 , that allow for detection of the defective elements ( @xmath160 ) . in this case , the experimental constraints require a sparse matrix @xmath13 : only if the tested compound @xmath161 is made of a very few elements of @xmath7 , the linear response holds and non - linear effects can be ignored .",
    "however , in the more general case , one does not directly observe the sparse signal @xmath7 but rather a linear transformation of it , @xmath162 , made with a dictionary matrix @xmath163 ( which is typically a fourier or wavelet transformation , and thus is a dense matrix ) . in this more difficult case",
    ", one would like to design a sparse measurement matrix @xmath164 such that the measurement / compression operation , @xmath165 , is fast , and the resulting observed data @xmath158 is short , thanks to the sparseness of @xmath7 .",
    "the conflicting requirement is to have a fast recovery scheme , because , now , to recover the original signal one should solve @xmath166 subject to @xmath167 , where @xmath168 is typically dense ( e.g.  in case of fourier and wavelet transformations ) .",
    "so a very interesting future development of the present approach is to extend it to this more complex case .",
    "fr - t is grateful for his useful discussions with f.  krzakala , m.  mzard and l.  zdeborova and financial support from the italian research minister through the firb project no .",
    "rbfr086nn1 on `` inference and optimization in complex systems : from the thermodynamics of spin glasses to message passing algorithms '' .",
    "m.  a.  t.  figueiredo , r.  d.  nowak and s.  j.  wright , `` gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , '' _ ieee journal of selected topics in signal processing _ vol .",
    "1 , pp . 586597 , 2007 .",
    "chung , g.  d.  forney jr .",
    ", t.  j.  richardson and r.  urbanke , `` on the design of low - density parity - check codes within 0.0045 db of the shannon limit , '' _ ieee comm . lett .",
    "58 no.2 , pp .",
    "58 - 60 , 2000 .",
    "f.  krzakala , m.  mzard , f.  sausset , y.  sun and l.  zdeborova , `` probabilistic reconstruction in compressed sensing : algorithms , phase diagrams , and threshold achieving matrices , '' arxiv:1206.3953 , 2012 ."
  ],
  "abstract_text": [
    "<S> in the context of the compressed sensing problem , we propose a new ensemble of sparse random matrices which allow one ( i ) to acquire and compress a @xmath0-sparse signal of length @xmath1 in a time linear in @xmath1 and ( ii ) to perfectly recover the original signal , compressed at a rate @xmath2 , by using a message passing algorithm ( expectation maximization belief propagation ) that runs in a time linear in @xmath1 . in the large @xmath1 limit , the scheme proposed here closely approaches the theoretical bound @xmath3 , and so it is both optimal and efficient ( linear time complexity ) . </S>",
    "<S> more generally , we show that several ensembles of dense random matrices can be converted into ensembles of sparse random matrices , having the same thresholds , but much lower computational complexity . </S>"
  ]
}