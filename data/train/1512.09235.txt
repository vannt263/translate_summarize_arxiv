{
  "article_text": [
    "in this paper , we aim to design a primal - dual fixed - point algorithmic framework for solving the following minimization problem : @xmath1 where @xmath2 and @xmath3 are three proper lower semi - continuous convex functions , and @xmath4 is differentiable on @xmath5 with a @xmath6-lipschitz continuous gradient for some @xmath7 $ ] , while @xmath8 is a bounded linear transformation .",
    "this formulation covers a wide application in image processing and signal recovery with multi - regularity terms .",
    "for instance , in many imaging and data processing applications , the functional @xmath9 corresponds to a data - fidelity term , and the last two terms are related to regularity terms . as a direct example of , we can consider the fused lasso penalized problems @xcite defined by @xmath10 on the other hand , in the imaging science , total variation regularization with @xmath11 being the discrete gradient operator together with @xmath0 regularization has been adopted in some image restoration applications , for example in @xcite .    as far as we know , condat @xcite tackled a problem with the same form as given in and proposed a primal dual splitting scheme .",
    "extensions to multi - block composite functions are also discussed in detail .",
    "for the special case @xmath12 ( @xmath13 denotes the usual identity operator ) , davis and yin @xcite proposed a three block operator splitting scheme based on monotone operators .",
    "when the problem reduces to two - block separable functions , many splitting and proximal algorithms have been proposed and studied in the literature . among them , extensive research have been conducted on the alternating direction of multiplier method ( admm ) @xcite ( also known as split bregman @xcite , see for example @xcite and the references therein ) .",
    "the primal - dual hybrid gradient method ( pdhg ) @xcite , also known as chambolle - pock algorithm @xcite , is another class of popular algorithm , largely adopted in imaging applications . in @xcite , several completely decoupled schemes , such as inexact uzawa solver and primal - dual fixed - point algorithm , are proposed to avoid subproblem solving for some typical @xmath0 minimization problems .",
    "komodakis and pesquet @xcite recently gave a nice overview of recent primal - dual approaches for solving large - scale optimization problems .",
    "a general class of multi - step fixed - point proximity algorithms is proposed in @xcite , which covers several existing algorithms @xcite as special cases . in the preparation of this paper",
    ", we notice that li and zhang @xcite also studied the problem and introduced a quasi - newton based scheme as preconditioned operators and the overrelaxation strategies to accelerate the algorithms .",
    "both algorithms can be viewed as a generalization of condat s algorithm @xcite .",
    "the theoretical analysis is established based on the multi - step techniques present in @xcite .    in the following",
    ", we mainly review some most relevant work for a concise presentation .",
    "we first consider a constrained regularization problem @xmath14 where @xmath15 is a closed convex set arising from physical requirements of the solutions .",
    "this problem can be reformulated as the form by introducing a set indicator function @xmath16 ( see ) as @xmath17 .",
    "for example , this problem has been studied in @xcite in the context of maximum a posterior ect reconstruction , and a preconditioned alternating projection algorithm ( papa ) is proposed for solving the resulted regularization problem . for @xmath18 in , we proposed a primal - dual fixed - point algorithm pdfp@xmath19o ( primal - dual fixed - point algorithm based on proximity operator ) in @xcite . based on the fixed point theory , we have shown the convergence of the scheme pdfp@xmath19o and its convergence rate under suitable conditions .    in this work",
    ", we aim to extend the ideas of pdfp@xmath19o in @xcite and papa in @xcite for solving without subproblem solving and provide a convergence analysis on the primal dual sequences .",
    "the specific algorithm , namely primal - dual fixed - point ( pdfp ) algorithm , is formulated as follows :    [ formbasic3b ]    ( ) y^k+1=_f_3(x^k-(x^k)- b^t v^k),[formbasic3ba ] + v^k+1=(i-_)(by^k+1+v^k)[formbasic3bb ] , + x^k+1=_f_3(x^k-(x^k)- b^t v^k+1),[formbasic3bc ]    where @xmath20 , @xmath21 . here",
    "@xmath22 is the proximity operator @xcite of a function @xmath23 , see .",
    "when @xmath24 , the proposed algorithm is reduced to papa proposed in @xcite , see .",
    "for the special case @xmath18 , we obtain pdfp@xmath19o proposed in @xcite .",
    "the convergence analysis of the proposed pdfp algorithm is built upon fixed point theory on the primal and dual pairs .",
    "the overall scheme is completely explicit , which allows an easy implementation and parallel computing for many large scale applications .",
    "this will be further illustrated through application to the problems arising in statistics learning and image restoration .",
    "the pdfp is a symmetric form and it is different from condat s algorithm proposed in @xcite .",
    "in addition , we point out that the ranges of the parameters are larger than those of @xcite and may lead to a significant advantage of parameter selection in practice . this will be further discussed in section [ seq : connections ] .",
    "the rest of the paper is organized as follows . in section [ seq :",
    "derivation ] , we will present some preliminaries and notations , and deduce pdfp from the first order optimality condition . in section [ seq : convergence ]",
    ", we will provide the convergence results and the linear convergence rate results for some special cases . in section [ seq : connections ]",
    ", we will make a comparison on the form of the pdfp algorithm with some existing algorithms . in section [ sec : numerical_experiments ] , we will show the numerical performance and the efficiency of pdfp through some examples on fused lasso and pmri ( parallel magnetic resonance image ) reconstruction .",
    "for the self completeness of this work , we list some relevant notations , definitions , assumption and lemmas in convex analysis .",
    "one may refer to @xcite and the references therein for more details .    for the ease of presentation , we restrict our discussion in the euclidean space @xmath25 , equipped with the usual inner product @xmath26 and norm @xmath27 .",
    "we first assume that the problem has at least one solution and @xmath28 satisfy @xmath29 where the symbol @xmath30 denotes the strong relative interior of a convex subset , and the effective domain of @xmath23 is defined as @xmath31    the @xmath0 norm of a vector @xmath32 is denoted by @xmath33 and the spectral norm of a matrix is denoted by @xmath34 .",
    "let @xmath35 be the collection of all proper lower semi - continuous convex functions from @xmath25 to @xmath36 $ ] .",
    "for a function @xmath37 , the proximity operator of @xmath23 : @xmath22 @xcite is defined by @xmath38 for a nonempty closed convex set @xmath39 , let @xmath16 be the indicator function of @xmath40 , defined by @xmath41 let @xmath42 be the projection operator onto @xmath40 , i.e. @xmath43 it is easy to see that @xmath44 for all @xmath45 , and the proximity operator is a generalization of projection operator .",
    "note that many efficient splitting algorithms rely on the fact that @xmath46 has a closed form solution .",
    "for example , when @xmath47 , the proximity solution is given by element - wised soft - shrinking .",
    "we refer the reader to @xcite for more details about proximity operators .",
    "let @xmath48 be the subdifferential of @xmath23 , i.e. @xmath49 and @xmath50 be the convex conjugate function of @xmath51 , defined by @xmath52    an operator @xmath53 is nonexpansive if @xmath54 and @xmath55 is firmly nonexpansive if @xmath56 it is obvious that a firmly nonexpansive operator is nonexpansive .",
    "an operator @xmath55 is @xmath57-strongly monotone if there exists a positive real number @xmath57 such that @xmath58    [ lem_additivity ] for any two functions @xmath59 and @xmath60 , and a bounded linear transformation @xmath8 , satisfying that @xmath61 there holds @xmath62    [ lem_proximity ] let @xmath63",
    ". then @xmath22 and @xmath64 are firmly nonexpansive .",
    "in addition , there hold @xmath65 if @xmath23 has @xmath6-lipschitz continuous gradient further , there holds @xmath66    [ lem : convergence ] let @xmath55 be an operator , and @xmath67 be a fixed - point of @xmath55 .",
    "let @xmath68 be the sequence generated by the fixed point iteration @xmath69 .",
    "suppose ( i ) @xmath55 is continuous , ( ii ) @xmath70 is non - increasing , ( iii ) @xmath71",
    ". then the sequence @xmath68 is bounded and converges to a fixed - point of @xmath55 .",
    "the proof of lemma [ lem : convergence ] is standard , and one may refer to the proof of theorem 3.5 in @xcite for more details .",
    "let @xmath72 and @xmath73 be two positive numbers . to simplify the presentation , we use in what follows the following notations :",
    "@xmath74 denote @xmath75 when @xmath20 , @xmath76 is a positive symmetric definite matrix , so we can define a norm @xmath77 for a pair @xmath78 , we also define a norm on the product space @xmath79 as @xmath80      on extending the ideas of papa proposed in @xcite and pdfp@xmath19o proposed in @xcite , we derive the following primal - dual fixed - point algorithm for solving the minimization problem .    under the assumption , by using the first order optimality condition of and lemma [ lem_additivity ] ,",
    "we have @xmath81 where @xmath82 is an optimal solution . let @xmath83 by applying , we have @xmath84 by inserting into , we get @xmath85 or equivalently , @xmath86 . next ,",
    "replacing @xmath87 in by @xmath88 , we can get @xmath89 .",
    "in other words @xmath90 for @xmath91 . meanwhile , if @xmath90 , we can get that @xmath82 meets the first order optimality condition of and thus @xmath82 is a minimizer of .    to sum up , we have the following theorem .",
    "[ theorem_fixedpoint ] suppose that @xmath82 is a solution of and @xmath92 is defined as .",
    "then we have    v^*=t_1(v^*,x^ * ) , + x^*=t_2(v^*,x^ * ) ,    i.e. @xmath93 is a fixed - point of @xmath55 .",
    "conversely , if @xmath94 is a fixed - point of @xmath55 , then @xmath82 is a solution of .",
    "it is easy to confirm that the sequence @xmath95 generated by pdfp algorithm is the picard iteration @xmath96 .",
    "so we will use the operator @xmath55 to analyze the convergence of pdfp in section [ seq : convergence ] .",
    "in the following , we denote @xmath93 as a fixed - point of the operator @xmath55 .",
    "let @xmath97 be the sequence generated by the operator @xmath55 .       there hold the following estimates : @xmath98    we first prove . by lemma [ lem_proximity ] ,",
    "we know @xmath99 is firmly nonexpansive , and use and we further have @xmath100 which implies @xmath101 thus @xmath102    next we prove . by the optimality condition of ( cf . )",
    ", we have @xmath103 by the property of subdifferentials ( cf . ) , @xmath104 i.e. , @xmath105 therefore , @xmath106 on the other hand , by the optimality condition of , it follows that @xmath107 thanks to the property of subdifferentials , there holds @xmath108 so @xmath109 thus @xmath110 replacing the term @xmath111 in with the right side term of the above inequality , we immediately obtain .    [",
    "lem : sequencekeyineq ] there holds @xmath112    summing the two inequalities and and re - arranging the terms , we have @xmath113 where @xmath114 is given in and .",
    "meanwhile , by the optimality condition of , we have @xmath115 which implies @xmath116 on the other hand , it follows from that @xmath117 recalling , we immediately obtain in terms of - .",
    "[ lem : sequence ] let @xmath118 and @xmath21 .",
    "then the sequence @xmath119 is non - increasing and @xmath120    if @xmath121 and @xmath21 , it follows from that @xmath122 , i.e. the sequence @xmath119 is non - increasing .",
    "moreover , summing the inequalities from @xmath123 to @xmath124 , we get @xmath125 the combination of and gives @xmath126 noting that @xmath118 , we know @xmath76 is positive symmetric definite , so is equivalent to @xmath127 hence , we have from the above inequality and that @xmath128 the combination of and then gives rise to @xmath129 according to , and , we have @xmath120    as a direct consequence of lemma [ lem : sequence ] and lemma [ lem : convergence ] , we obtain the convergence of pdfp as follows .",
    "[ theorem : convergence ] let @xmath118 and @xmath21 .",
    "then the sequence @xmath68 is bounded and converges to a fixed - point of @xmath55 , and @xmath130 converges to a solution of .    by lemma [ lem_proximity ] , both @xmath131 and @xmath132 are firmly nonexpansive , thus the operator @xmath55 defined by - is continuous . from lemma [ lem : sequence ] , we know that the sequence @xmath133 is non - increasing and @xmath134 . by using lemma [ lem : convergence ] , we know that the sequence @xmath68 is bounded and converges to a fixed - point of @xmath55 . by using theorem [ theorem_fixedpoint ] , @xmath130 converges to a solution of .    [",
    "remark : thereason ] for the special case @xmath18 , the pdfp reduces naturally to pdfp@xmath19o proposed in @xcite , where the conditions for the parameters are @xmath135 , @xmath21 . in theorem [",
    "theorem : convergence ] , the condition for the parameter @xmath73 is slightly more restricted as @xmath20 .",
    "it is easy to see when @xmath18 , the equation in lemma [ lem : sequencekeyineq ] reduces to @xmath136 and the conditions in the proof of lemma [ lem : sequence ] can be relaxed to @xmath135 .",
    "however , for a general @xmath17 , the condition can not be relaxed to ensure the positive definitiveness of the matrix @xmath76 , which is needed for the uniform convergence .",
    "[ remark : f10 ] for the special case @xmath137 , the problem reduces to two - block proper lower semicontinuous convex functions without lipschitz continuous gradient assumptions .",
    "the condition @xmath21 in pdfp becomes @xmath138 .",
    "although , @xmath72 is an arbitrary positive number in theory , but the range of @xmath72 will affect the convergence speed and it is also a difficult problem to choose a best value in practice .      in the following",
    ", we will show the convergence rate results with some additional assumptions on the basic problem .",
    "in particular , for @xmath18 , the algorithm reduces to pdfp@xmath19o proposed in @xcite . the conditions for a linear convergence given there as condition 3.1 in @xcite is as followed : for @xmath135 and @xmath139 , there exist @xmath140 , @xmath141 such that @xmath142 where @xmath143 is given in .",
    "it is easy to see that a strongly convex function @xmath9 satisfies the condition . for a general @xmath17",
    ", we need stronger conditions on the functions .",
    "[ prop_contraction_ratio ] suppose that holds and @xmath144 is strongly convex .",
    "then , we have @xmath145 where @xmath146 is the convergence rate ( indicated in the proof ) and @xmath147 is a parameter describing the strongly monotone property of @xmath148 ( cf . ) .",
    "use moreau s identity ( cf . ) to get @xmath149 so is equivalent to @xmath150 according to the optimality condition of , @xmath151 similarly , according to the optimality condition of , @xmath152 observing that @xmath148 is @xmath57-strongly monotone , we have by and that @xmath153 i.e. , @xmath154 thus @xmath155    summing the two inequalities and , and then using the same argument for driving , we arrive at @xmath156 where we have also used the condition .",
    "let @xmath157 , @xmath158 .",
    "it is clear that @xmath146 .",
    "hence , according to the notation , the estimate can be rewritten as required .",
    "we note that a linear convergence rate for strongly convex @xmath144 and @xmath17 are obtained in @xcite .",
    "they introduced two preconditioned operators for accelerating the algorithm , while a clear relation between the convergence rate and the preconditioned operators is still missing .",
    "meanwhile , introducing preconditioned operators could be beneficial in practice , and we can also introduce a preconditioned operator to deal with @xmath159 in our scheme . since the analysis is rather similar to the current one , we will omit it in this paper .",
    "in this section , we present the connections of the pdfp algorithm to some algorithms proposed previously in the literature . in particular , when @xmath24 , due to @xmath160 , the proposed algorithm is reduced to papa proposed in @xcite    [ formbasicc2 ] & \\ {    & y^k+1=_c(x^k-(x^k)- b^t v^k ) , + & v^k+1=(i-_)(by^k+1+v^k ) , + & x^k+1=_c(x^k-(x^k)- b^t v^k+1 ) , +    . &    where @xmath20 , @xmath21 .",
    "we note that the conditions of the parameters for the convergence of pdfp are larger than those in @xcite . here",
    "we still refer as pdfp , since papa originally proposed in @xcite incorporates other techniques such as diagonal preconditioning . for the special case @xmath18 , due to @xmath161",
    ", we obtain the pdfp@xmath19o scheme proposed in @xcite    [ formbasicpdfp2o ] & \\ {    & y^k+1=x^k-(x^k)- b^t v^k , + & v^k+1=(i-_)(by^k+1+v^k ) , + & x^k+1=x^k-(x^k)- b^t v^k+1 , +    . &    where @xmath135 , @xmath21 . based on pdfp@xmath19o",
    ", we also proposed pdfp@xmath19o@xmath162 in @xcite for @xmath24 as    [ formbasicc1 ] & \\ {    & y^k+1=x^k-(x^k)- b^t v_1^k - v_2^k , + & v_1^k+1=(i-_)(by^k+1+v_1^k ) , + & v_2^k+1=(i-_c)(y^k+1+v_2^k ) , + & x^k+1=x^k-(x^k)- b^t v_1^k+1-v_2^k+1 , +    . &    where @xmath163 , @xmath21 . similar technique of extension to multi composite functions have also been used in @xcite . compared to pdfp , the algorithm pdfp@xmath19o@xmath162 introduces an extra variable , while pdfp requires two times projections .",
    "most importantly , the primal variable at each iterate of pdfp is feasible , but maybe not for that of pdfp@xmath19o@xmath162 .",
    "in addition , the permitted ranges of the parameters are also tighter in pdfp@xmath19o@xmath162 .",
    "the other most related algorithm to the pdfp algorithm is the algorithm proposed by l. condat in @xcite . for the special case with @xmath137 ,",
    "condat s algorithm reduces to pdhg method in @xcite . by grouping multi - block as a single block",
    ", the authors in @xcite extended the pdhg algorithm @xcite to multi composite functions penalized problems .",
    "the authors in @xcite proposed a class of multi - step fixed - point proximity algorithms , including several existing algorithms as special examples , for example the algorithms in @xcite .",
    "the three - block method proposed by davis and yin in @xcite are based on operator splitting but subproblem solving is required when it is applied to solve for the case @xmath164 .",
    "li and zhang @xcite also introduce preconditioned operators based on the techniques present in @xcite and including condat s algorithm in @xcite as a special case , and further introduce quasi - newton and the overrelaxation strategies to accelerate the algorithms .",
    "specifically , we compare the pdfp algorithm with the basic algorithm 3.2 in @xcite .    in the following , we mainly compare pdfp to condat s algorithm @xcite for a simple presentation .",
    "we first change the form of pdfp algorithm by using moreau s identity , see , i.e. @xmath165 where @xmath166 .",
    "a direct comparison is presented in table [ table : cmp_condat_pdfp2o ] . from table [",
    "table : cmp_condat_pdfp2o ] , we can see that the ranges of the parameters in condat s algorithm are relatively smaller than pdfp . also since the condition for condat s algorithm is mixed with all the parameters , it is not always easy to choose them in practice .",
    "this is also pointed out in @xcite . while the rules for the parameters in pdfp are separate , and they can be chosen independently according to the lipschitiz constant and the operator norm of @xmath167 . in this sense ,",
    "our parameter rules are relatively more practical . in the numerical experiments",
    ", we can set @xmath73 to be close to @xmath168 and @xmath72 to be close to @xmath169 for most of tests .",
    "nevertheless , pdfp has an extra step compared to condat s algorithm and the computation cost may increase due to the computation of @xmath131 . in practice , this step is often related to @xmath0 shrinkage , so the cost could be still ignorable in practice .",
    ".the comparison between condat ( @xmath170 ) and pdfp",
    ". [ cols= \" < , < , < \" , ]      a general image restoration problem with nonnegative constraint and sparse regularization can be written as @xmath171 where @xmath172 is some bounded linear operator describing the image formation process , @xmath173 is the usual @xmath0 based regularization in order to promote sparsity under the transform @xmath11 , @xmath174 is the regularization parameter . here",
    "we use isotropic total variation as the regularization functional , thus the matrix @xmath11 represents for the discrete gradient operator . for this example",
    ", we can set @xmath175 , @xmath176 , and @xmath177 .",
    "we consider pmri reconstruction , where @xmath178 for each @xmath179 is composed of a diagonal downsampling operator @xmath180 , fourier transform @xmath181 and a diagonal coil sensitivity mapping @xmath182 for receiver @xmath183 , i.e. @xmath184 and @xmath182 are often estimated in advance .",
    "it is well known in total variation application that @xmath185 .",
    "the related lipschitz constant of @xmath186 can be estimated as @xmath187 .",
    "therefore the two parameters in pdfp are set as @xmath188 and @xmath189 .",
    "the same simulation setting as in @xcite is used in this experiment and we still use artifact power ( ap ) and two - region signal to noise ratio ( snr ) to measure image quality .",
    "one may refer to @xcite for more details .    in the following , we compare pdfp algorithm with the previous proposed algorithms pdfp@xmath19o and pdfp@xmath19o@xmath190 .",
    "from figure [ figure : spine_pmri_cmp ] and [ figure : brain_pmri_cmp ] , we can first see that the introduction of nonnegative constraint in the model is beneficial and we can recover a better solution with higher two - region snr and lower ap value .",
    "the nonnegative constraint leads to a faster convergence for a stable recovery .",
    "secondly , pdfp@xmath19o@xmath190 and pdfp are both efficient . for a subsampling rate @xmath191 , pdfp@xmath19o@xmath190 and pdfp can both recover better solutions in terms of ap values compared to pdfp@xmath19o under the same iterative numbers . for @xmath192 ,",
    "the solutions of pdfp@xmath19o@xmath190 and pdfp have better ap values than those of pdfp@xmath19o , but only use half iteration numbers of pdfp@xmath19o .",
    "the computation time for pdfp is slightly less than pdfp@xmath19o@xmath190 .",
    "finally , the iterative solutions of pdfp are always feasible , which could be useful in practice .",
    "cccccc & pdfp@xmath19o & pdfp@xmath19o@xmath190 & + r=2 & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & + ap & 0.002523 & 0.001294 & 0.001021 + snr & 34.94 & 35.35 & 36.01 + @xmath194 & 8 & 8 & 8 + time & 0.73 & 0.75 & 0.67 +   + r=4 & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 . ,",
    "title=\"fig:\",scaledwidth=27.0% ] & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 . ,",
    "title=\"fig:\",scaledwidth=27.0% ] & + ap & 0.040011 & 0.009718 & 0.009802 + snr & 38.06 & 39.55 & 39.57 + @xmath194 & 500 & 250 & 250 + time & 43.76 & 22.35 & 19.30 +    cccccc & pdfp@xmath19o & pdfp@xmath19o@xmath190 & + r=2 & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & + ap & 0.000822 & 0.000469 & 0.000465 + snr & 39.36 & 39.75 & 40.38 + @xmath194 & 25 & 25 & 25 + time & 3.96 & 4.01 & 3.63 +   + r=4 & . for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & .",
    "for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 .",
    ", title=\"fig:\",scaledwidth=27.0% ] & .",
    "for pdfp@xmath19o and pdfp , @xmath188 , @xmath189 and for pdfp@xmath19o@xmath190 , @xmath193 , @xmath189 . , title=\"fig:\",scaledwidth=27.0%",
    "] & + ap & 0.002502 & 0.001528 & 0.001535 + snr & 43.06 & 43.86 & 44.13 + @xmath194 & 150 & 75 & 75 + time & 23.02 & 11.74 & 10.79 +",
    "we have extended the algorithm papa @xcite and pdfp@xmath19o @xcite to derive a primal - dual fixed - point algorithm pdfp ( see ) for solving the minimization problem of three - block convex separable functions .",
    "the proposed pdfp algorithm is a symmetric and fully splitting scheme , only involving explicit gradient and linear operators without any inversion and subproblem solving , when the proximity operator of nonsmooth functions can be easily handled .",
    "the scheme can be easily adapted to many inverse problems involving many terms minimization and it is suitable for large scale parallel implementation .",
    "in addition , the parameter range determined by the convergence analysis is rather simple and clear , and it could be useful for practical application . finally as discussed in section 5 in @xcite , we can also extend the current pdfp algorithm to solve multi - block composite ( more than three ) minimization problems . preconditioning operators , as proposed in @xcite can be also introduced to accelerate pdfp , which could be a future work for some specific applications .",
    "p. chen was partially supported by the phd research startup foundation of taiyuan university of science and technology ( no .",
    ". j. huang was partially supported by nsfc ( no . 11171219 ) .",
    "x. zhang was partially supported by nsfc ( no . 91330102 and gz1025 ) and 973 program ( no .",
    "2015cb856000 ) .",
    "jim x ji , jong bum son , and swati d rane . .",
    ", 31(1):24 - 36 , 2007 .",
    "nikos komodakis and jean - christophe pesquet . playing with duality : an overview of recent primal - dual approaches for solving large - scale optimization problems . , 2014 .",
    "jun liu , lei yuan , and jieping ye . an efficient algorithm for a class of fused lasso problems . in _ proceedings of the 16th acm sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 323332 .",
    "acm , 2010 .",
    "thomas pock and antonin chambolle .",
    "diagonal preconditioning for first order primal - dual algorithms in convex optimization . in _ computer vision ( iccv ) , 2011 ieee international conference on _ , pages 17621769 .",
    "ieee , 2011 ."
  ],
  "abstract_text": [
    "<S> many problems arising in image processing and signal recovery with multi - regularization can be formulated as minimization of a sum of three convex separable functions . </S>",
    "<S> typically , the objective function involves a smooth function with lipschitz continuous gradient , a linear composite nonsmooth function and a nonsmooth function . in this paper </S>",
    "<S> , we propose a primal - dual fixed - point ( pdfp ) scheme to solve the above class of problems . </S>",
    "<S> the proposed algorithm for three block problems is a fully splitting symmetric scheme , only involving explicit gradient and linear operators without inner iteration , when the nonsmooth functions can be easily solved via their proximity operators , such as @xmath0 type regularization . </S>",
    "<S> we study the convergence of the proposed algorithm and illustrate its efficiency through examples on fused lasso and image restoration with non - negative constraint and sparse regularization .    </S>",
    "<S> keywords : primal - dual fixed - point algorithm , convex separable minimization , proximity operator , sparsity regularization . </S>"
  ]
}