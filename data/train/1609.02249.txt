{
  "article_text": [
    "@xmath0-simplex is a @xmath0-dimensional polytope which is the convex hull of its @xmath1 vertices .",
    "suppose @xmath2 are @xmath1 affinely independent vertices .",
    "suppose the convex - hull generated by these vertices is called @xmath3 .",
    "then all points in the @xmath0-simplex @xmath3 can be described by the set @xmath4 since every vector in a simplex can be represented in an unique way as a convex combination of its extreme points ( see game theory maschler , solan , zamin , pg 924 ) , for each point in @xmath5 , there exist one and only one combination of @xmath6 satisfying the conditions @xmath7 for @xmath8 and @xmath9 . + consider a case where a black - box function is needed to be optimized whose constrained parameter space is given by a collection of independent simplex blocks .",
    "suppose there are @xmath10 simplex blocks given by @xmath11 where @xmath12-th simplex block @xmath13 comes from @xmath14 where @xmath15 is given by @xmath16 @xmath17 denotes the number of elements in the @xmath12-th simplex block .",
    "the objective function @xmath18 is to be minimized over its domain . in other words ,",
    "the problem can be re - stated as @xmath19 + optimization problems based on simplex parameter space arises in many problems in the field of combinatorial geometry , hidden markov models , probability vector estimation ( e.g. , mixture models ) , b - spline modeling , genetics ( e.g. , @xcite ) etc . among the existing methods for constrained optimization ,",
    "most of them are designed for general linear or non - linear constrained space .",
    "but there is scarcity of algorithms which are designed specially for simplex parameter space .",
    "+ most of the algorithms for constrained optimization method works fine for convex functions . among them , ` interior point ( ip ) ' ( see @xcite , @xcite , @xcite , @xcite ) ) and ` sequential quadratic programming ( sqp ) ' ( see @xcite , @xcite , @xcite ) algorithms are widely used .",
    "both of these methods use gradient - based approach .",
    "but the problem with gradient based methods is that they might be time consuming for optimizing the functions with complex structure . in case ,",
    "the closed form expression of the derivative of the function is not available ( or provided ) , taking approximation of the derivative might be erroneous if the function has a lot of spikes in a small neighborhood .",
    "lastly , although derivative - based methods works pretty fast for constrained optimazation problems , these methods are prone to get struck at local solutions for non - convex functions . + for optimizing non - convex functions ,",
    "many deterministic and stochastic methods have been developed over last few decades . among them",
    "` genetic algorithm ( ga ) ' ( see @xcite , @xcite , @xcite ) and ` simulated annealing ( sa ) ' ( see @xcite , @xcite ) are widely used nowadays .",
    "although these methods were first developed for unconstrained global optimization , later they were extended for optimizing objective functions with constraints ( see @xcite , @xcite ) . during optimizing the objective function , both of these methods jumps within the parameter domain for better solutions . if our interest is to optimize a function on simplex blocks , using general constrained optimization versions of ga and sa , while looking for better solution , a good proportion of points checked by the above - mentioned algorithms are prone to be generated outside the constrained space or in the infeasible region .",
    "thus the efficiency of the algorithm might be affected .",
    "ga does not scale well with complexity because in higher dimensional optimization problems there is often an exponential increase in the search space size ( @xcite , page 21 ) . + as mentioned in @xcite , with increasing access to high - performance modern computers and clusters , some approaches like monte carlo methods and multi - start strategy have a great advantage .",
    "although they perform well for lower dimensional problems , since their requirement of palatalization grows exponentially with dimension of the parameter space , these strategies are prone to fail solve high - dimensional optimization problems efficiently .",
    "+ @xcite proposed ` greedy co - ordinate descent with varying step - size on simplex ' ( gcdvss ) algorithm which optimizes non - convex problems efficiently when the parameter is a unit simplex - block . since that method",
    "was designed specifically for simplex parameter space , unlike constrained ga or sa , most of the jumps performed in the update step while looking for better solution fall in the feasible region , which makes it more efficient .",
    "the number of operations required in each iteration step for gcdvss algorithm is of the order of square of the number of parameters in the simplex block which is a significant improvement over the ga whose complexity increases exponentially with the dimension ( @xcite ) .",
    "another advantage of using gcdvss algorithm is during an iteration , it evolves the function at @xmath20 ( where @xmath21 is the number of parameters in the unit simplex - block ) independent directions.thus incorporation of parallel computing makes it even faster and the requirement of parallelization increases linearly with the dimension of the simplex block .",
    "+ in this paper we extend gcdvss algorithm for the case when parameter space has multiple unit - simplex blocks ( of possibly different sizes ) instead of single simplex block . in gcdvss algorithm , at each iteration the value of the objective function is evaluated at @xmath20 sites ( considering there are @xmath21 variables on unit simplex @xmath22 ) in the neighborhood of the current optimal solution ( see das(2016 + ) for details ) and the best movement is selected .",
    "similar to gcdvss , here also in each iteration the function value is evaluated at @xmath23 sites ( assuming there are total @xmath24 parameters in the multiple simplex - blocks ) in the neighborhood of the current optimal solution .",
    "this algorithm is named ` greedy co - ordinate descent with varying step - size on multiple simplex ' ( gcdvsms ) .",
    "suppose our objective function is @xmath18 which need to be minimized over its domain .",
    "this can be re - stated as the problem given in equation ( [ opt_problem ] ) .",
    "the main idea of movement while looking for better solution in this algorithm is quite similar to that of gcdvss ( @xcite ) .",
    "this algorithm consists of several _",
    "runs_. inside each _ run _ , iterations are performed to look for optimal solution and based on convergence criteria 1 ( cc1 ) ( see below for details ) , iterations inside a _ run _ stop and a solution is returned at the end .",
    "the next _ run _ starts from the solution returned by the previous _ run _ and tries to improve the solution .",
    "hence only for the first _ run _ , the initial guess should be provided by the user .",
    "if the solution returned by two consecutive _ runs _ are close enough based on another convergence criteria ( named convergence criteria 2 ( cc2 ) , see below for details ) , the algorithm stops returning the final solution .",
    "the strategy of running several _ runs _ helps to jump from the local solutions to a better local solution or the global solution .",
    "+ in each _ run _ , there are four tuning parameters _ initial global step size _ ( @xmath25 ) , _ step decay rate _ ( @xmath26 ) , _ step size threshold _ ( @xmath27 ) and _ sparsity threshold _ ( @xmath28 ) . like gcdvss , except the _ step decay rate _ , we keep the values of all other parameters same throughout all",
    "_ runs_. for the first _ run _ , _ step decay rate _ is taken to be @xmath29 and for the other _ runs _ , it is taken equal to @xmath30 .",
    "other than these five parameters @xmath25 , @xmath29 , @xmath30 , @xmath27 and @xmath28 , the two convergence criteria cc1 and cc2 are controlled by _",
    "tol_fun_1 _ and _ tol_fun_2 _ respectively .",
    "lastly , the maximum number of iterations inside a _ run _ and the maximum number of allowed _ runs _ are fixed to be equal to _ max_iter _ and _ max_runs _ respectively . + as mentioned in section [ sec1 ] , in the parameter space consists of @xmath1 unit - simplex blocks and the @xmath12-th simplex block has @xmath17 elements in it and is denoted by @xmath31 which belongs to @xmath14 for @xmath32",
    ". hence the total number of variables is @xmath33 . inside each",
    "_ run _ there is a parameter named _ global step size _ @xmath34 and @xmath23 parameters named _ local step sizes _ which are denoted by @xmath35 and @xmath36 for @xmath37 and @xmath38 .",
    "the values of these above - mentioned @xmath39 parameter values evolve based of the values of the previously mentioned tuning parameters and other strategies of this algorithm .",
    "hence , the user does not need to provide or control their values . inside",
    "a _ run _ , in the first iteration the value of the _ global step size _ is set to be @xmath40 ( @xmath41 denotes the value of _ global step size _ in the @xmath42-th iteration in a _ run _ ) .",
    "the value of @xmath34 is kept same throughout an iteration .",
    "but , at the end of each iteration , based on a convergence criteria ( call convergence criteria 3 ( cc3 ) ) , its value is either kept same or divided by @xmath26 ( _ step decay rate _ ) .",
    "hence , @xmath43 will be either equal to @xmath41 or @xmath44 depending of the cc3 checked at the end of @xmath42-th iteration ( see below and the algorithm summary for details ) . at the beginning of an iteration , the values of _",
    "local step sizes _ @xmath35 and @xmath36 are set to be equal to the value of the _ global step size _",
    "@xmath34 of that iteration .",
    "for example , at the start of @xmath42-th iteration of a _ run _ , we set @xmath45",
    ". suppose the current value of the variable at the beginning of @xmath42-th iteration is @xmath46 where @xmath47 for @xmath37 .",
    "during the iteration step , the objective function value is evaluated at @xmath23 feasible points in the neighborhood of @xmath48 .",
    "these feasible points are obtained by making the movement from the current solution based on the local step - sizes @xmath49 for @xmath37 and @xmath50 ( see below for details ) .",
    "the values of these local step - sizes @xmath49 are subjected to be updated several times ( if required ) within an iteration ( see below for details ) .",
    "+ the above - mentioned @xmath23 movements can be divided into @xmath24 ` positive movements ' @xmath51 and @xmath24 ` negative movements ' @xmath52 for @xmath32 and @xmath50 .",
    "we call a position of a unit - simplex box to be ` significant ' if its value is greater than the sparsity control parameter @xmath28 . for example , since the @xmath12-th unit - simplex block has @xmath17 positions ( or elements ) , it can have atleast @xmath53 and atmost @xmath17 ` significant ' positions . during the @xmath42-th iteration , for performing @xmath51-th movement",
    ", @xmath54 is incremented by @xmath55 and this quantity is subtracted equally from the other ` significant ' positions of the simplex block @xmath56 , keeping the values of the variables of the other simplex - blocks unchanged .",
    "thus the sum of the values in the @xmath12-th simplex block remains constant after this operation . after this update",
    "is made , it is checked whether the updated @xmath56 is feasible or not . in case",
    "it is feasible , the corresponding objective function value is evaluated .",
    "if the point is not feasible , @xmath55 is divided by @xmath26 ( i.e. , set @xmath57 ) until a feasible point is achieved .",
    "similarly , for performing @xmath52-th movement , @xmath54 is decremented by @xmath58 and this quantity is divide by the number of other ` significant ' positions and added to those positions of the simplex block @xmath56 , keeping the values of the variables of the other simplex - blocks unchanged and keeping the sum of the values of this simplex block constant , i.e. , @xmath53 .",
    "similar to the update procedure of @xmath55 , @xmath58 is also subjected to be updated within the iteration until a feasible point is yielded .",
    "thus , inside each iteration , the objective function value is evaluated at @xmath23 feasible points in the neighborhood of current solution . only the best possible movement out of these @xmath23 possible movements",
    "is performed at the end of the iteration which explains the greedy nature of this proposed algorithm .",
    "to control sparsity , the ` insignificant ' positions of the simplex blocks of the finally accepted movement are set equal to 0 and the sum of the values of the ` insignificant ' positions is a simplex block is equally divided by the number of ` significant ' positions inside that block and added to those positions .",
    "thus while controlling the sparsity , the sum of the elements inside every simplex block of the finally accepted movement is kept constant , i.e. , 1 .",
    "the minimum allowable value for @xmath55 and @xmath58 is @xmath27 ( _ step size threshold _ ) . at the end of an iteration , the values of the @xmath34 ( _ global step size _ ) might be changed or kept same based of convergence criteria 3 ( cc3 ) ( see below for details ) .",
    "the minimum allowable value of @xmath34 is @xmath27 ( _ step size threshold _ ) . at the beginning of the next iteration , the values of the _ local step sizes _",
    "are set equal to the _ global step size _ of the next iteration . +",
    "as mentioned earlier , the value of @xmath34 is changed or kept unchanged based on cc3 .",
    "cc3 is described as if the difference of the value of the objective function after an iteration with the value at the previous iteration is less than _ tol_fun_1 _ , @xmath34 is divided by @xmath26 .",
    "otherwise , it is kept unchanged .",
    "cc1 determines the stopping criteria of iterations in a _",
    "run_. cc1 is given by if at the end of an iteration cc3 is satisfied and @xmath59 , the iterations stop inside that _ run _ and the solution returned by the last iteration of that _ run _ serves as the starting point of the next _",
    "run_. cc2 determines the final stopping criteria of the _ runs _ which is if the difference of the objective function value returned by a _ run _ with",
    "that returned by the previous _ run _ is less than _ tol_fun_2 _ , no further _ run _ is performed and the final solution is returned .",
    "+ below , the proposed algorithm has been noted dividing into two parts named stage 1 and stage 2 .",
    "stage 1 describes the iterations within a _ run _ and stage 2 describes the changes of tuning parameter values and other steps performed while moving from one _ run _ to the next _ run_. at the beginning , set @xmath60 ( @xmath61 denotes the number of _ runs _ ) , @xmath62 and initial guess of the solution @xmath63 where @xmath64 for @xmath37 . + * stage : 1 *    1 .   set @xmath65 .",
    "set @xmath66 go to step ( 2 ) .",
    "2 .   set @xmath67 and @xmath68 for @xmath37 and @xmath69 .",
    "set @xmath70 and go to step ( 3 ) .",
    "3 .   if @xmath71 , go to step ( 7 ) . else go to step ( 4 ) 4 .",
    "if @xmath72 , set @xmath73 and go to step ( 5 ) .",
    "else , find @xmath74 where @xmath75 .",
    "if @xmath76 , go to step ( 4.1 ) , else set @xmath77 and go to step ( 4 ) .",
    "1 .   if @xmath78 , set @xmath79 and go to step ( 4 ) . else",
    "( if @xmath80 ) , evaluate vector @xmath81 such that @xmath82 go to step ( 4.2 ) .",
    "2 .   check whether @xmath83 or not . if @xmath83 , go to step ( 4.3 ) .",
    "else , set @xmath84 and go to step ( 4.1 ) 3 .",
    "evaluate @xmath85 .",
    "set @xmath86 and go to step ( 4 ) .",
    "if @xmath72 , set @xmath73 and go to step ( 6 ) .",
    "else , find @xmath87 where @xmath88 .",
    "if @xmath89 , go to step ( 5.1 ) , else set @xmath77 go to step ( 5 ) .",
    "1 .   if @xmath90 , set @xmath79 and go to step ( 5 ) . else",
    "( if @xmath91 ) , evaluate vector @xmath92 such that @xmath93 go to step ( 5.2 ) 2 .",
    "check whether @xmath94 or not . if @xmath94 , go to step ( 5.3 ) .",
    "else , set @xmath95 and go to step ( 5.1 ) 3 .",
    "evaluate @xmath96 .",
    "set @xmath86 and go to step ( 5 ) .",
    "set @xmath97 and @xmath98 . if @xmath99 , go to step ( 6.1 ) .",
    "else , set @xmath100 and go to step ( 6.2 ) .",
    "1 .   if @xmath101 , set @xmath102 , else ( if @xmath103 ) , set @xmath104 .",
    "go to step ( 6.2 ) .",
    "2 .   define @xmath105 and set @xmath106 .",
    "set @xmath107 and go to step ( 3 ) .",
    "set @xmath108 .",
    "set @xmath109 .",
    "go to step ( 8) . 8 .",
    "find @xmath110 where @xmath111 .",
    "go to step ( 8.1 ) ( @xmath112 denotes the value at the @xmath113-th co - ordinate of @xmath114 ) .",
    "1 .   if @xmath115 , set @xmath116 and go to step ( 9 ) . else",
    "( if @xmath117 ) go to step ( 8.2 ) 2 .   set @xmath118 . @xmath119",
    "go to step ( 9 ) .",
    "set @xmath120 and @xmath121 .",
    "if @xmath122 , set @xmath123 and go to step ( 11 ) . else ( if @xmath124 ) go to step ( 10 ) .",
    "if @xmath125 set @xmath126 .",
    "go to step ( 11 ) .",
    "else go to step ( 12 )",
    ". 11 . if @xmath127 , go to step ( 12 ) .",
    "else set @xmath128 and go to step ( 2 ) . 12 .",
    "* stop * execution .",
    "set @xmath129 . set @xmath130 .",
    "go to stage 2 .",
    "* stage : 2 *    1 .",
    "if @xmath131 , set @xmath132 keeping other tuning parameters ( @xmath133 and @xmath25 ) fixed .",
    "repeat algorithm described in stage 1 setting @xmath134 .",
    "else , go to step ( 2 ) ( of stage 2 ) .",
    "if @xmath135 and @xmath136 , repeat the algorithm described in stage 1 setting @xmath134 . else @xmath137 is the final solution .",
    "* stop * and * exit*.    the default values of the parameters are @xmath138 , @xmath139 and @xmath140 .",
    "the values of @xmath141 and @xmath142 are taken to be @xmath143 and @xmath144 respectively .",
    "it should be noted that taking lower values of @xmath27 and @xmath28 results in more precise solution in the cost of higher computation time .",
    "the default values of _",
    "tol_fun_1 _ and _ tol_fun_2 _ are taken to be @xmath145 .",
    "in section [ sec_algo ] , note that at every iteration , after the best move is selected in a greedy manner out of @xmath20 possible moves in the neighborhood , all the values of the selected ( or updated ) unit - simplex block which are less than @xmath28 are replaced by zeros with some following adjustment ( see step ( 8) of stage 1 in section [ sec_algo ] ) .",
    "hence if there is prior knowledge of sparsity in the final solution , the value of @xmath28 should be taken bigger than it s default value .",
    "thus introduction of this sparsity control parameter @xmath28 in thhis proposed algorithm helps in using the prior knowledge of sparse solution while solving the optimization problem .",
    "the greatest challenge of solving a non - convex optimization problem is no algorithm can be designed which always reach the global minimum while optimizing it .",
    "however , it is a desirable property of any algorithm that it should reach a global minimum when the function is convex . in this section",
    "it has been shown that under some basic regularity conditions , taking the values of the parameters @xmath27 , _",
    "tol_fun_1 _ and _ tol_fun_2 _ significantly small , the stopping criteria of the proposed algorithm ensures that the solution obtained is a global minimum in case the objective function is convex .    [ theorem ] suppose @xmath146 and @xmath147 is convex , continuous and differentiable on @xmath148 .",
    "suppose @xmath149 and @xmath150 for @xmath32 and each of it s co - ordinates are non - zero .",
    "consider a sequence @xmath151 for @xmath152 , @xmath153 , @xmath154 for all @xmath32 .",
    "define @xmath155 and @xmath156 for @xmath32 and @xmath50 .",
    "if for all @xmath152 , @xmath157 and @xmath158 ( whenever @xmath159 ) for @xmath32 and @xmath50 , @xmath160 is a point of global minimum of @xmath147 .",
    "+    for all @xmath161 , define , @xmath162 define @xmath163 such that @xmath164 where @xmath165 and @xmath166 for @xmath32 .",
    "note that @xmath167 is the first @xmath168 co - ordinates of @xmath169 .",
    "consider the map @xmath170 such that @xmath171 .",
    "it can be seen that @xmath172 is a bijection for any @xmath37 .",
    "+ define @xmath173 such that @xmath174 since @xmath175 is the cartesian product of bijective functions @xmath176 , @xmath175 is also a bijection .",
    "hence , to prove that @xmath177 is the global minimum of @xmath147 on @xmath178 , it is enough to show that @xmath179 is the global minimum of @xmath180 on @xmath181 .",
    "the definition of @xmath182 reveals that @xmath183 .",
    "hence it will be sufficient to show that @xmath179 is a global minimum of @xmath182 on @xmath181 .",
    "+ the convexity of @xmath182 follows from the convexity of @xmath147 .",
    "consider any two points @xmath184 and @xmath185 in @xmath186 .",
    "define @xmath187 and @xmath188 where @xmath167 and @xmath189 denotes the first @xmath168 co - ordinates of @xmath169 and @xmath190 respectively for @xmath37 .",
    "take any constant @xmath191 .",
    "now @xmath192 hence @xmath182 is convex . + fix any @xmath193 . define @xmath194 such that @xmath195 where @xmath196 for @xmath32 . note that @xmath197 implies @xmath198 .",
    "define @xmath199 and @xmath200 note that @xmath201 and @xmath202 are the first @xmath203 co - ordinates of @xmath204 and @xmath205 respectively .",
    "+ following the argument in the proof of theorem 4.1 in @xcite , under the given conditions , there exists a positive integer @xmath206 such that for all @xmath207 , @xmath208 .",
    "hence for @xmath207 , @xmath209 . define @xmath210 such that @xmath211 hence we have @xmath212 and @xmath213 for @xmath214 .",
    "define @xmath215 for @xmath216 . again following the arguments made in the proof of theorem 4.1 in @xcite , under the given conditions , @xmath217 for @xmath218 .",
    "now , @xmath219 hence for each @xmath32 , @xmath220 for @xmath214 . that implies the partial derivative of @xmath182 with respect to each co - ordinate is zero at @xmath179 . since",
    "@xmath182 has been shown to be convex , @xmath179 is a global minimum of @xmath182 .",
    "hence , @xmath177 is a global minimum of @xmath147 .    in section [ sec_algo ]",
    ", it should be noted that taking",
    "_ step size threshold _ @xmath27 small enough , the allowable values of _ local step sizes _ @xmath55 and @xmath58 can taken as close to zero as required . also note that in theorem [ theorem ] , the role of @xmath221 is analogous to that of @xmath55 and @xmath58 in section [ sec_algo ] .",
    "in other words , in the proposed algorithm if we take @xmath222 and @xmath223 , the iterations within a _ run _ stops when for very small value of @xmath55 and @xmath58 for @xmath37 and @xmath38 , corresponding movements ( as described in step ( 4 ) and ( 5 ) of stage 1 in section [ sec_algo ] ) in the neighborhood do not yield better solution than the current solution .",
    "hence , it that scenario , the obtained solution by the proposed algorithm is a global minimum if the objective function is convex with the desired minimal regularity conditions as described in theorem [ theorem ] .",
    "note that , for a convex function satisfying the regularity conditions , the convergence criteria ensures that at the end of any _ run _ the solution obtained is a global minimum . hence , in this case , evaluation of only one _ run _ will be enough to find the global minimum .",
    "for simulation purpose , some multidimensional benchmark functions have been considered on transformed unit - simplex blocks parameter space .",
    "the transformations made on the parameter spaces of these benchmark functions are similar to that considered in @xcite .",
    "suppose a function @xmath147 is to be minimized on a @xmath224-dimensional hypercube @xmath225 where @xmath226 $ ] for some constants @xmath227 in @xmath228 .",
    "consider the map @xmath229 $ ] such that @xmath230 for @xmath231 .",
    "clearly @xmath232 is a bijection . after replacing the original parameters of the problem with the transformed parameters we get @xmath233 where @xmath234 for @xmath231 .",
    "define @xmath235^d \\mapsto \\mathbb{r}$ ] such that @xmath236 consider the set @xmath237 .",
    "note that @xmath238^{d } \\subset s$ ] .",
    "define @xmath239 which is equal to function @xmath42 considered on the extended domain @xmath240 . since @xmath241 $ ] for @xmath231 and @xmath242 , hence @xmath243 .",
    "define @xmath244 .",
    "hence we can conclude that @xmath245\\in \\delta^d$ ] where @xmath246 and @xmath247 now define @xmath248 such that @xmath249 for @xmath250 . it can be seen that @xmath250 implies @xmath251 .",
    "suppose the global minimum of the function @xmath147 occurs at @xmath252 in @xmath225 .",
    "hence , the function @xmath253 will have the global minimum at @xmath254 in @xmath255 . +",
    "now consider there are @xmath256 blocks of simplexes @xmath257 for @xmath258 .",
    "define , @xmath259 here the comparative study of performances of the proposed algorithm and various existing methods of constrained optimization has been shown for optimizing some benchmark unconstrained global optimization problems on transformed parameter space which is given by @xmath256 blocks of @xmath224-dimensional simplexes . among the other methods we considered the ` interior - point ' ( ip ) algorithm , ` sequential quadratic programming ' ( sqp ) and constrained ` genetic algorithm ' ( ga ) .",
    "ip and sqp search for local minimum while optimizing any function and in general they are less time consuming . on the other hand ga tries to find global minimum being more time consuming .",
    "these above - mentioned well - known algorithms are available in matlab r2014a ( the mathworks ) via the optimization toolbox functions _ fmincon _",
    "( for ip and sqp algorithm ) and _ ga _ ( for ga ) . for gcdvsms algorithm",
    ", the values of all the tuning parameters have been taken to be same as mentioned in section [ sec_algo ] . while using ip and sqp algorithms , the upper bound for maximum number of iterations and function evaluations have been set to be infinity each .",
    "for ga , the default options of ` ga ' function in matlab r2014a has been considered .",
    "gcdvsms algorithm is implemented in matlab r2014a .",
    "the comparative study has been performed for the cases @xmath260 and @xmath261 for all the above - mentioned algorithms . to check the performance of the proposed algorithm in higher dimensional problems ,",
    "additional simulation studies have been performed . for each cases ,",
    "all the algorithms have been initialized from 100 randomly generated starting points .",
    "the average time ( in seconds ) and minimum value of the objective function for each cases have been noted down in table [ compare_1 ] .",
    "all the computations have been performed in a computer with 64-bit windows 8.1 , intel i7 3.6ghz processor , 32 gb ram .",
    "@xmath224-dimensional rastrigin function is given by @xmath262 , \\ ; \\mathbf{x } = ( x_1,\\ldots , x_d)\\in d\\end{aligned}\\ ] ] where @xmath263 , the domain of the parameters are typically taken to be @xmath264^{d}$ ] .",
    "hence we have @xmath265 . after performing the above - mentioned transformations ,",
    "we obtain    @xmath266 , \\ ; \\bar{\\mathbf{y } } = ( y_1,\\ldots , y_{d+1 } ) \\in \\delta^{d}.\\end{aligned}\\ ] ]    we consider the case where we need to minimize @xmath267 for @xmath268 for @xmath269 . in table",
    "[ compare_1 ] , it is noted that gcdvsms outperformed other algorithms and the average computation time of gcdvsms is 4 - 5 folds smaller than that of ga .",
    "@xmath224-dimensional ackley s function is given by @xmath270 where @xmath263 , the domain of the parameters are typically taken to be @xmath271^{d}$ ] .",
    "after necessary transformations , we get @xmath272 where @xmath273 .",
    "our objective is to minimize @xmath267 over @xmath268 for @xmath269 . in this case",
    "also , gcdvsms outperforms all other algorithms with 2 - 3 fold time improvement over ga .",
    "all of the above - mentioned functions being non - convex , we consider the sphere function which is convex .",
    "@xmath224-dimensional sphere function is given by @xmath274 here the domain of the parameters @xmath263 is taken to be @xmath264^{d}$ ] .",
    "the modified sphere function on simplex is given by , @xmath275 we minimize @xmath267 over @xmath268 for @xmath269 .",
    "it should be noted that , the sphere function being convex , ip and sqp functions are expected to work better than gcdvsms and ga while minimizing it .",
    "note that , gcdvsms yields significantly better solution than ga with 3 - 4 folds improvement in computation time .      @xmath224-dimensional griewank function is given by @xmath276 here the domain of the parameters @xmath263 is taken to be @xmath277^{d}$ ] .",
    "after transformation , the griewank function on simplex is given by , @xmath278 we minimize @xmath267 over @xmath268 for @xmath269 .",
    "for the case @xmath279 , gcdvsms performs significantly better than others . in the other case ,",
    "ip and sqp performs better than ga and gcdvsms . in this case",
    "also , gcdvsms outperforms ga with 4 - 7 folds improvement in computation time .",
    ".comparative study of gcdvsms , ip , sqp and ga for optimizing modified rastrigin , ackley , sphere and griewank function starting from 100 randomly generated initial points .",
    "[ cols=\"^,^,^,^,^,^ \" , ]",
    "this paper proposes a black - box optimization technique where the parameter space is given by a collection of independent simplex blocks . in the comparative study provided in table [ compare_1 ]",
    ", it is noted that the proposed algorithm outperformed all the other considered existing algorithms .",
    "also it should be noted that the proposed algorithm works upto 7 times faster than genetic algorithm yielding better solution under each scenarios considered . in table [ tab_high_dim ] some other higher dimensional simulation studies have been also provided on modified rastrigin , ackley , sphere and griewank function where the proposed algorithm reaches significantly close to the true solution in reasonable time .",
    "i would personally like to thank my ph.d .",
    "advisor dr .",
    "subhashis ghoshal who motivated me into this work as the major part of my ph.d",
    ". project .",
    "i would also like to thank dr .",
    "hua zhou who enriched my knowledge regarding optimization and efficient coding techniques .",
    "das , p. , derivative - free efficient global optimization on high - dimensional simplex , arxiv:1604.08636 ( 2016 ) blei , d.m . and",
    "lafferty , j.d . , topic models , http://www.cs.columbia.edu/ blei / papers / bleilaerty2009.pdf ( 2009 ) potra , f.a . and wright , s.j . , interior - point methods",
    ". journal of computational and applied mathematics , 4 , 281 - 302 ( 2000 ) karmakar , n. , new polynomial - time algorithm for linear programming , combinatorica , 4 , 373 - 395 ( 1984 ) boyd , s. and vandenberghe , l. , convex optimization .",
    "cambridge : cambridge university press ( 2006 ) wright , m. h. , the interior - point revolution in optimization : history , recent developments , and lasting consequences , bulletin of american mathematical society , 42 , 39 - 5 ( 2005 ) nocedal , j. and wright , s.j . , numerical optimization , 2nd edition . , operations research series , springer , ( 2006 ) boggs , p.t . and tolle , j.w . , sequential quadratic programming , acta numerica 4 , 1 - 51 ( 1996 ) fraser , a.s . , simulation of genetic systems by automatic digital computers i. introduction .",
    "australian journal of biological sciences , 10 , 484 - 491 ( 1957 ) bethke , a.d . , genetic algorithms as function optimizers , ( 1980 ) goldberg , d.e . , genetic algorithms in search , optimization , and machine learning , operations research series .",
    "addison - wesley publishing company ( 1989 ) kirkpatrick , s. , gelatt , c.d . and vecchi , m.p .",
    ", optimization by simulated annealing .",
    "australian journal of biological sciences , 220 , 671 - 680 , ( 1983 ) granville , v. , krivanek , m. and rasson , j.p .",
    ", simulated annealing : a proof of convergence .",
    "ieee transactions on pattern analysis and machine intelligence , 16 , 652 - 656 ( 1994 ) smith , r.l . and",
    "romeijn , h.e . , simulated annealing for constrained global optimization , journal of global optimization , journal of global optimization , 5(2 ) , 101 - 126 ( 1994 ) reid , d.j . , genetic algorithms in constrained optimization , mathematical and computer modelling , 23(5 ) , 87 - 111 ( 1996 ) geris , l. , computational modeling in tissue engineering , springer ( 2012 ) hilbert , m. and lopez , p. , the worlds technological capacity to store , communicate , and compute information , science , 332 , 60 - 65 ( 2011 )"
  ],
  "abstract_text": [
    "<S> black - box optimization of objective function of parameters belonging to simplex arises in many inference and predictive models . </S>",
    "<S> @xcite introduced greedy co - ordinate descent of varying step - sizes on simplex ( gcdvss ) which efficiently optimizes any black - box function whose parameters belong to a simplex . in this paper , that method has been modified and extended for the case where the set of parameters may belong to multiple simplex block of different sizes . the main principle of this algorithm is to make jumps of varying step - sizes within each simplexes simultaneously and searching for the best direction for movement . </S>",
    "<S> since this algorithm is designed specially for multiple simplex blocks parameter space , the proportion of movements made within the parameter space during the update step of a iteration is relatively higher for the proposed algorithm . starting from a single initial guess , unlike genetic algorithm or simulated annealing , requirement of parallelization for this algorithm grows linearly with the dimension of the parameter space which makes it more efficient for higher dimensional optimization problems . </S>",
    "<S> comparative studies with some existing algorithms have been provided based on modified well - known benchmark functions . </S>",
    "<S> upto 7 folds of improvement in computation time has been noted for using the proposed algorithm over genetic algorithm , yielding significantly better solution in all the cases considered .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore    = 1 </S>"
  ]
}