{
  "article_text": [
    "the past two decades has witnessed the wide application of @xmath0-minimization models in signal and image processing , compressive sensing , machine learning , statistic , and more .",
    "the success of @xmath0 minimization is mainly due to that the @xmath0-norm can well reflect sparse prior .",
    "recently , it was observed that other auxiliary information of sparse solutions , such as partial support set @xcite and nonnegative sparsity @xcite , could help fit practical models . in this paper , instead of studying the theoretical benefit of modeling auxiliary priors , we are interested in designing efficient algorithms to solve the @xmath0-minimization problems with auxiliary box constraints : @xmath1 and @xmath2 where @xmath3 are given , @xmath4 is an augmented parameter , and @xmath5 is some box - constrained set .",
    "the above problems are obtained separately by imposing box constraints to the basis pursuit model @xcite : @xmath6 and the augmented @xmath0 norm model @xcite : @xmath7 both of which have been proved powerful for sparse recovery .",
    "adding box constraints to classical @xmath0 minimization on one hand extends the range of models and to include more practical models in application , and on the other hand can help improve the ability of sparse recovery of them when correct box constraints are available ; the second point of view shall be demonstrated numerically later on .",
    "similar benefit of adding box constraints to classical matrix completion has been observed in a recent paper @xcite which was posted on arxiv at the time of the writing of the present paper .    due to the existing of the strongly convex term @xmath8 , it has been explained in several papers @xcite that models and have computational advantages over their correspondences and . besides , applying the proximal point algorithm @xcite to models or generates a series of subproblems similar to or .",
    "therefore , the center assignment of solving problems - reduces to studying the following generalized problem @xmath9 where @xmath10 is a given vector . with the help of the lagrange dual analysis and",
    "by noticing the strong convexity of the objective function , in this study we derive a projected shrinkage ( proshink ) algorithm for solving . by the nesterov techniques @xcite ,",
    "the proposed algorithm can be speeded up ; we present an accelerated scheme as well .",
    "theoretically , we prove the convergence of both the primal and dual point sequences of proshink . a key contribution in our technique",
    "is that a complicated proximal operator appeared in the deduction can be equivalently simplified into a projected shrinkage operator .",
    "this can also be applied to simplifying standard forward - backward splitting algorithm for the boxed - constrained basis pursuit denoising problem : @xmath11 where @xmath12 is a positive paramter .",
    "the rest of paper is organized as follows . in section 2",
    ", we introduce some basis concepts of constrained convex optimization and obtain important properties about the shrinkage operator . in section 3 , under the lagrange dual analysis , we propose the proshrink algorithm and prove its convergence , and meanwhile we present detailed iteration schemes for solving models , , and . in section 4 , we do sparse recovery experiments to demonstrate the benefit of adding box constrains .",
    "in this paper , we restrict our attention onto two classes of intervals .",
    "the first class is :    @xmath13 , c<0 , ~~\\textrm{or } \\\\ & i=[c , d ] , 0<c < d ~~\\textrm{or}~~i=[d , c ] , d < c<0\\};\\end{aligned}\\ ] ]    the second class is @xmath14 , c<0<d\\}.\\ ] ] the box constraint @xmath5 appeared in all models mentioned before is defined as @xmath15 , where @xmath16 . throughout this paper , we assume that @xmath17 .",
    "first , we introduce proximal point operator and its important properties .",
    "let @xmath18 be a closed proper convex function .",
    "the proximal operator @xcite @xmath19 is defined by @xmath20    since the objective function is strongly convex and proper , @xmath21 is properly defined for every @xmath22 .",
    "the following properties @xcite will be used in our analysis .",
    "[ lem1 ] let @xmath18 be a closed proper convex function .",
    "then , for all @xmath23 the proximal operator @xmath24 satisfies the followings :    1 .",
    "firmly nonexpansive : @xmath25 2 .",
    "lipschitz continuous : @xmath26",
    ".    if @xmath27 is fully separable , meaning that @xmath28 , then @xmath29    second , we need to introduce convex projected operator and projected subgradient to deal with box constraints .",
    "@xmath30_\\mathcal{x}^+:=\\arg\\min_{y\\in\\mathcal{x}}\\|x - y\\|$ ] .",
    "the following property of projected operator shall be often encountered in our deduction .",
    "[ projp ] for any interval @xmath31 , we have that @xmath32_i^+=\\tau\\cdot[w]^+_{i/\\tau}$ ] holds for arbitrary @xmath33 and positive parameter @xmath4 .",
    "we begin with the definition of projected operator and derive that    @xmath34_i^+   = \\arg\\min_{y\\in i}\\|y-\\tau w\\| = \\arg\\min_{y\\in i}\\|\\tau^{-1}y- w\\|   = \\tau\\cdot\\arg\\min_{z\\in i/\\tau}\\|z- w\\|   = \\tau\\cdot[w]^+_{i/\\tau}.\\end{aligned}\\ ] ]    this completes the proof .",
    "define @xmath35_\\mathcal{x}^+ , h\\in \\partial f(x)\\}$ ] without confusion , we also denote @xmath36 by @xmath37_\\mathcal{x}^+ $ ] .    with projected subgradient",
    ", we can state a necessary and sufficient condition which guarantees a vector to be a minimizer to a class of constrained convex optimization problems .",
    "[ lem : optcond ] let @xmath38 be proper convex and @xmath5 nonempty , closed , and convex . then",
    ", we have @xmath39    the following two facts will be used in our deduction :    * fact 1 .",
    "* @xmath40^+_\\mathcal{x } \\leftrightarrow   \\langle z - x , z - y\\rangle\\leq 0 , \\forall y\\in \\mathcal { x}$ ] ;    * fact 2 . * @xmath41 .    with these two facts , we derive that    @xmath42_\\mathcal{x}^+   \\\\ & \\leftrightarrow    \\exists h\\in\\partial f(x^ * ) , \\textrm{such that}~~x^*=[x^*- h]_\\mathcal{x}^+ \\\\ &",
    "\\leftrightarrow    \\langle x^*-(x^*- h ) , x^*-y\\rangle \\geq 0 , \\forall y\\in\\mathcal{x } \\\\ & \\leftrightarrow    \\langle h , y - x^*\\rangle \\geq 0 , \\exists h\\in \\partial f(x^ * ) , \\forall y\\in\\mathcal{x}\\\\ & \\leftrightarrow     x^*\\in\\arg\\min_{x\\in\\mathcal{x}}f(x).\\end{aligned}\\ ] ]    this completes the proof .      in this part",
    ", we build an important formulation that links the proximal point operator and the projected shrinkage operation . in order to establish that formulation ,",
    "we need two lemmas .",
    "[ shrinkproj ] let @xmath43 be the shrinkage operator defined by @xmath44 and let @xmath45 .",
    "then , we always have that @xmath46_i^+=[q-{\\mathrm{sign}}(c)]_i^+\\ ] ] holds for arbitrary @xmath47 , where @xmath48 appears in the definition of @xmath49 .",
    "recall that @xmath50 with @xmath51 , or @xmath52 $ ] with @xmath53 , or @xmath54 $ ] with @xmath55 , or @xmath56,$ ] with @xmath57 .",
    "so it is easy to observe the following fact :    if @xmath51 , then @xmath58_i^+\\equiv c$ ] for each @xmath59 and @xmath60_i^+\\equiv c$ ] ;    if @xmath53 , then @xmath61_i^+\\equiv c$ ] for each @xmath62 and @xmath60_i^+\\equiv c$ ] .",
    "thus , together with the definition of the shrinkage operator , for @xmath51 we have that @xmath63_i^+ & = & \\left\\{\\begin{array}{lc } [ q-1]_i^+ , & q>1 \\\\ { [ 0]_i^+ } , & -1\\leq q\\leq 1 \\\\ { [ q+1]_i^+ } , & q<-1 \\end{array }   \\right .",
    "\\\\ & = & \\left\\{\\begin{array}{lc } [ q-1]_i^+ , & q>1 \\\\ { c } , & q\\leq 1 \\end{array }   \\right.=[q-1]_i^+\\end{aligned}\\ ] ]    and for @xmath53 have that @xmath64_i^+ = \\left\\{\\begin{array}{lc } [ q+1]_i^+ , & q<-1 \\\\ { c } , & q\\geq -1 \\end{array}\\right.=[q+1]_i^+.\\ ] ] on the other hand , @xmath65_i^+=[q-1]_i^+$ ] when @xmath51 and @xmath65_i^+=[q+1]_i^+$ ] when @xmath53 .",
    "so the relationship holds .",
    "[ shrinkproj2 ] let @xmath31 and @xmath66 where @xmath67 is the indicator function .",
    "then , we always have that @xmath68_i^+={{\\mathbf{prox } } } _ { i_\\tau(\\cdot)}(q)\\ ] ] holds for arbitrary @xmath47 .    by the definition of proximal point operator",
    ", we derive that    @xmath69    if @xmath45 , then @xmath70 and hence    @xmath71    applying lemma [ lem : optcond ] yields to @xmath72_i^+$ ] .",
    "together with lemmas [ projp ] and [ shrinkproj ] , we derive that @xmath73_i^+=\\tau\\cdot[\\tau^{-1}q-{\\mathrm{sign}}(c)]_{i/\\tau}^+=\\tau\\cdot [ { { \\mathbf{shrink } } } ( \\tau^{-1}q)]_{i/\\tau}^+=[\\tau\\cdot{{\\mathbf{shrink } } } ( \\tau^{-1}q)]_i^+.\\ ] ] so relationship holds when @xmath45 .    if @xmath74 , then again invoking lemma [ lem : optcond ] yields to @xmath75_i^+$ ] .",
    "such @xmath76 must be the unique solution to problem because its objective functions is strongly convex .",
    "thus , it suffices to show that @xmath77_i^+$ ] satisfies the following inclusion : @xmath78_i^+.\\ ] ] now , we check @xmath79 case - by - case :    * case 1 : @xmath80*. since interval @xmath81\\in t_2 $ ] satisfies @xmath82 , condition @xmath80 implies @xmath83 and hence @xmath84 .",
    "then , by lemmas [ projp ] and [ shrinkproj ] , and together with the definition of shrinkage operator we derive that @xmath85^+_i=[q-\\tau]^+_i= [ \\tau\\cdot(\\tau^{-1}q-1)]^+_i=\\tau\\cdot[\\tau^{-1}q-1]^+_{i/\\tau } = \\tau\\cdot[{{\\mathbf{shrink } } } ( \\tau^{-1}q)]_{i/\\tau}^+=[\\tau\\cdot{{\\mathbf{shrink } } } ( \\tau^{-1}q)]_i^+=p(q).\\ ] ]    * case 2 : @xmath86*. condition @xmath86 implies @xmath87 .",
    "then , similarly to the argument in case 1 , we have that @xmath85^+_i=[q-\\tau]^+_i= [ \\tau\\cdot(\\tau^{-1}q+1)]^+_i=\\tau\\cdot[\\tau^{-1}q+1]^+_{i/\\tau } = \\tau\\cdot[{{\\mathbf{shrink } } } ( \\tau^{-1}q)]_{i/\\tau}^+=[\\tau\\cdot{{\\mathbf{shrink } } } ( \\tau^{-1}q)]_i^+=p(q).\\ ] ]    * case 3 : @xmath88*. condition @xmath88 implies @xmath89 . then , noting the fact that @xmath90=\\partial \\|0\\|_1 $ ] , we have that @xmath91^+_i= [ q- \\tau\\cdot\\partial \\|0\\|_1]^+_i=\\tau\\cdot [ \\tau^{-1}q- \\cdot\\partial \\|0\\|_1]^+_{i/\\tau}\\ni 0.\\ ] ] this completes the proof .",
    "now , we are ready to build the most important formulation in this study .",
    "[ proshr ] define the projected shrinkage operator @xmath92_\\mathcal{x}^+$ ] for a vector @xmath22 via @xmath93_\\mathcal{x}^+\\right)_i=[{{\\mathbf{shrink } } } ( v_i)]_{i_i}^+ , i= 1 , 2 , \\cdots , n.\\ ] ] and let @xmath94 .",
    "then , it holds @xmath95_\\mathcal{x}^+={{\\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(v)}.\\ ] ]    noting that @xmath96 and the property , together with lemma [ shrinkproj2 ] , the conclusion follows .",
    "the significance of formulation is two - fold : the expression based on proximal point operator will be used for convergence analysis ; whilst that expressed by the projected shrinkage operator is for computational consideration due to its simplicity .",
    "in this section , we derive a lagrange dual problem of and the proshrink algorithm for solving it . following the line of proof thought in paper @xcite ,",
    "we prove the convergence of both the primal and dual point sequences of the proshrink algorithm .",
    "the lagrangian of the augmented convex model ( [ cent ] ) is @xmath97 the lagrange dual function is @xmath98 for any vector @xmath99 , the @xmath100-minimization problem above is a strongly convex program and hence has a unique solution @xmath101 that satisfies    @xmath102    where @xmath94 . by formulation in corollary [ proshr ] , we obtain @xmath103_\\mathcal{x}^+.\\ ] ]    now , @xmath104^+_\\mathcal{x } , y)$ ] .",
    "thus , we can write down the lagrange dual problem of ( [ cent ] ) as follows : @xmath105^+_\\mathcal{x } , y).\\ ] ] it is well known in convex analysis @xcite that the dual objective function @xmath106^+_\\mathcal{x } , y)$ ] is gradient - lipschitz - continuous due to the strong convexity of the primal objective function @xmath107 . and moreover , the gradient of dual objective function is given by @xmath108^+_\\mathcal{x}\\ ] ] each solution to the dual problem can generate the unique solution to the primal problem via formulation .",
    "this fact is stated in the following lemma .",
    "[ dset ] let @xmath109 be the unique solution to problem and @xmath17 . then the dual solution set to problem is @xmath110^+_\\mathcal{x}\\right\\},\\ ] ] which is nonempty and convex .    by lemma [ lem1 ] , for @xmath111 we have that    @xmath112^+_{\\mathcal{x}}-[\\tau \\cdot { { \\mathbf{shrink } } } ( \\tau^{-1}u + a^t\\tilde{y})]^+_{\\mathcal{x}},a^ty - a^t\\tilde{y}\\rangle \\\\ = & \\tau^{-1 } \\langle { { \\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^ty)-{{\\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^t\\tilde{y}),\\tau\\cdot a^ty-\\tau\\cdot a^t\\tilde{y}\\rangle \\\\ \\geq & \\tau^{-1 }   \\|{{\\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^ty)-{{\\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^t\\tilde{y})\\|_2 ^ 2\\geq 0,\\end{aligned}\\ ] ]    which implies that the dual objective function @xmath113 is convex .",
    "thus , the dual solution set is    @xmath114^+_\\mathcal{x}=b\\},\\end{aligned}\\ ] ]    which must be nonempty and convex by assumption @xmath17 and the convexity of @xmath113 .",
    "now , it suffices to show @xmath115 .",
    "on one hand , we have @xmath116 since @xmath117 . on the other hand ,",
    "let @xmath118 , i.e. , @xmath119 is some dual solution .",
    "then , @xmath120^+_\\mathcal{x}$ ] is a primal solution and it must equal @xmath109 by uniqueness .",
    "so @xmath121 and hence @xmath122 , which completes the proof .",
    "applying the gradient iteration to the dual objective @xmath113 gives : @xmath123_\\mathcal{x}^+),\\ ] ] where @xmath124 is the step size whose range shall be studied later for convergence . by setting @xmath125_\\mathcal{x}^+$ ] ,",
    "we obtain the equivalent iteration in the primal - dual form : @xmath126_\\mathcal{x}^+ \\\\ y^{k+1}=y^{k}+h ( b - ax^{k+1 } ) . \\end{array } \\right.\\end{aligned}\\ ] ] because the projected shrinkage operator is involved , we call projected shrinkage algorithm . recall that the linearized bregman ( lbreg ) algorithm @xcite has the following form : @xmath127 therefore , the proshrink algorithm can be viewed as a generalization of the lbreg algorithm .    applying nesterov",
    "s accelerated scheme @xcite , we obtain an accelerated proshrink algorithm with the following form : @xmath128^+_\\mathcal{x } ; \\\\",
    "z^{k+1}=y^{(k)}+h ( b - ax^{k+1});\\\\ \\gamma_k=(\\sqrt{\\theta_k+4}-\\theta_k)/2;\\\\ \\beta_{k+1 } = ( 1-\\theta_k)\\gamma_k , \\theta_{k+1}=\\theta_k\\gamma_k ; \\\\",
    "y^{k+1}= z^{k+1}+\\beta_{k+1}(z^{k+1}-z^k ) .",
    "\\end{array } \\right.\\end{aligned}\\ ] ] in addition , it is predictable that the adaptive restart technique developed in @xcite can further accelerate the scheme ; such acceleration for the lbreg algorithm was observed in paper @xcite .",
    "now , let us return to models and .",
    "model can be solved by proshrink or its acceleration with @xmath129 . to solve model ,",
    "we apply the proximal point algorithm and obtain a series of subproblems as follows : @xmath130 where @xmath131 are positive parameters .",
    "each subproblem above can be well solved by proshrink as well .",
    "we write down the iteration scheme without detailed derivation : @xmath132_\\mathcal{x}^+ \\\\ y^{i+1}=y^{i}+h ( b - ax^{i+1 } ) . \\end{array } \\right.\\end{aligned}\\ ] ] the subproblem can also be solved by the accelerated proshrink scheme .    at last ,",
    "the standard forward - backward splitting algorithm for model is @xmath133 where @xmath134 are the step sizes .",
    "the main difficulty of the above iteration is to compute the proximal point operator of @xmath135 . utilizing formulation , this can be overcome and the iteration can be simplified into @xmath136^+_\\mathcal{x}.\\ ] ]      in this part , we prove the convergence of primal sequence @xmath137 and dual sequence @xmath138 in iteration ( [ mainalg ] ) .",
    "[ thm : cvg ] set step size @xmath139 and @xmath140 in iteration ( [ mainalg ] ) .",
    "let @xmath109 be the unique minimizer to problem ( [ aug ] ) and @xmath141 be the solution set to problem .",
    "then , @xmath142 , and there exists a point @xmath143 such that @xmath144 .",
    "this theorem can be proved in the same manner as that in paper @xcite . for completeness",
    ", we provide a proof below .",
    "let @xmath145 .",
    "by lemma [ dset ] , we have @xmath146^+_\\mathcal{x}$ ] .",
    "together with @xmath125_\\mathcal{x}^+$ ] and lemma [ lem1 ] and corollary [ proshr ] , we derive    @xmath147_{\\mathcal{x}}^+ - [ \\tau\\cdot{{\\mathbf{shrink } } } ( \\tau^{-1}u + a^t\\hat{y})]^+_{\\mathcal{x } } \\rangle \\\\ = & \\langle a^ty^k - a^t\\hat{y } , { { \\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^ty^{k } ) - { { \\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^t\\hat{y } ) \\rangle",
    "\\\\ \\geq&\\tau^{-1}\\cdot   \\|{{\\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^ty^{k } ) - { { \\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^t\\hat{y } ) \\rangle\\|_2 ^ 2\\\\ = & \\tau^{-1 } \\cdot \\| x^{k+1}- x^*\\|_2 ^ 2\\end{aligned}\\ ] ]    using this inequality , we have    @xmath148    therefore , under the assumption @xmath149 we can make the following claims :    * claim 1 : * @xmath150 is monotonically nonincreasing in @xmath151 and thus converges to a limit ;    * claim 2 : * @xmath152 converges to 0 as @xmath151 tends to @xmath153 , i.e. , @xmath154 .    from claim 1",
    ", it follows that @xmath138 is bounded and thus has a converging subsequence @xmath155 .",
    "let @xmath156 . by the lipschitz continuity of proximal point operator in lemma [ lem1 ] and corollary [ proshr ] , we have    @xmath157_\\mathcal{x}^+\\\\ \\nonumber & = \\lim_{i\\rightarrow \\infty } { { \\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^ty^{k_i+1})= { { \\mathbf{prox } } } _ { \\mathcal{x}_\\tau(\\cdot)}(u+\\tau\\cdot a^t\\bar{y})=   [ \\tau \\cdot { { \\mathbf{shrink } } } ( \\tau^{-1}u + a^t\\bar{y})]_\\mathcal{x}^+,\\nonumber\\end{aligned}\\ ] ]    so @xmath158 by lemma [ dset ] .",
    "recall @xmath159 is arbitrary .",
    "hence , claim 1 holds for @xmath160 .",
    "if @xmath138 had another limit point , then @xmath161 would fail to be monotonic .",
    "so , @xmath162 converges to @xmath143 ( in norm ) .",
    "in the section , we do sparse recovery experiments to demonstrate that adding box constraints can help improve recovery of sparse signals considerably . it was shown in @xcite when the augmented parameter @xmath163 , the augmented @xmath0-norm model is equivalent to classical basis pursuit if the sensing matrix @xmath164 satisfies certain properties such as null - space property , or restricted isometry property .",
    "so we only test models and to observe possible advantages of adding box constraints . in the test",
    ", model was solved by the lbreg algorithm and model by the proshrink algorithm .",
    "we used 100 random pairs @xmath165 with matrices @xmath164 of size @xmath166 and vectors @xmath100 with 400 entries , out of which @xmath167 were nonzero entries set to @xmath168 uniformly randomly for @xmath169 .",
    "each entry of the sensing matrix @xmath164 was sampled independently from the standard gaussian distribution .",
    "thus , @xmath170 are given vectors .",
    "a relative error of @xmath171 was considered as an exact recovery ; the relative error is defined as @xmath172 where @xmath173 is finally generated by the lbreg or the proshrink algorithms .",
    "the box - constrained set @xmath5 for the proshrink algorithm was set as @xmath90^{400}$ ] .",
    "we plot the exact recovery rate via sparsity levels in figure [ fig4 ] from which we see that proshrink performs remarkably better than lbreg as the sparse level increases .",
    "more precisely , when the sparse level is low , both lbreg and proshrink can well recover sparse signals ; but when the sparse level becomes high , the recovery rate by lbreg is worse than that by proshrink that indicates adding box constraints to the augmented @xmath0-norm model indeed improves the recovery rate .",
    "in this paper , we proposed the projected shrinkage algorithm for boxed - constrained @xmath0-minimization .",
    "the most important factor in our study should be the deduction of formulation that establishes the relationship between projected shrinkage operator and proximal point operator .",
    "numerically , we demonstrated that adding box constraints to classical @xmath0-minimization can obtain better performance .",
    "however , giving theoretical explanation for this phenomenon is open . we leave it for future work",
    "we would like to thank professor wotao yin ( ucla ) for his comments and suggestion on numerical verification and professor jian - feng cai ( iowa u ) for his insight of the projected shrinkage operator ."
  ],
  "abstract_text": [
    "<S> box - constrained @xmath0-minimization can perform remarkably better than classical @xmath0-minimization when correction box constraints are available . and </S>",
    "<S> also many practical @xmath0-minimization models indeed involve box constraints because they take certain values from some interval . in this paper </S>",
    "<S> , we propose an efficient iteration scheme , namely projected shrinkage ( proshrink ) algorithm , to solve a class of box - constrained @xmath0-minimization problems . a key contribution in our technique </S>",
    "<S> is that a complicated proximal point operator appeared in the deduction can be equivalently simplified into a projected shrinkage operator . </S>",
    "<S> theoretically , we prove that proshrink enjoys a convergence of both the primal and dual point sequences . on the numerical level </S>",
    "<S> , we demonstrate the benefit of adding box constraints via sparse recovery experiments .    </S>",
    "<S> * keywords : * proximal point operator ; projected shrinkage ; box constraints ; @xmath0-minimization ; sparse recovery </S>"
  ]
}