{
  "article_text": [
    "this is to certify that :    1 .",
    "the thesis comprises only my original work towards the mphil ; 2 .",
    "due acknowledgement has been made to all other material used ; and 3 .",
    "the thesis is less than 50,000 words in length .        [",
    "cols=\"^\",options=\"header \" , ]      in this section , we will prove that under certain conditions , the information state converges in distribution .",
    "this fact is already known for classical hidden markov models , and is quite robust : legland and mevel @xcite prove geometric ergodicity of the information state even when calculated from incorrectly specified parameters , while capp , moulines and rydn @xcite prove harris recurrence of the information state for certain uncountable state underlying chains .",
    "we will present a mostly elementary proof of convergence in the case of multiple observation processes .    to determine the limiting behaviour of the information state , we begin by finding an explicit form for its one - step time evolution .    [ def : rfunction ] for each observation process @xmath0 and each observed state @xmath1 , the * @xmath2-function * is the function @xmath3 given by @xmath4 where @xmath5 is the dirac measure on @xmath6 and @xmath7 is the @xmath8th component of @xmath9 .",
    "[ lem : informationstaterecurrence ] in a hidden markov model with multiple observation processes and a fixed policy @xmath10 , the information state satisfies the recurrence relation @xmath11    let @xmath12 and @xmath13 . by the markov property as in definition [ def : observationprocess ] , and the simplification ( [ eqn : noidependence ] ) , @xmath14 & \\hspace{3cm}\\times{\\mathbb p}\\big(x_{t+1}=x{\\,\\big|\\,}x_t = j\\big){\\mathbb p}\\big(x_t = j , y^{(i_{(t)})}_{(t)}=y_{(t)}\\big)\\nonumber\\\\ & = \\frac1{k_{t+1}}\\sum_jm^{(i_{t+1})}_{x , y_{t+1}}t_{j , x}k_tz_t(y_{(t)})_j\\nonumber\\\\ & = \\frac{k_t}{k_{t+1}}\\sum_jm^{(i_{t+1})}_{x , y_{t+1}}t_{j , x}z_t(y_{(t)})_j\\nonumber\\\\ & = \\frac{\\sum_jm^{(i_{t+1})}_{x , y_{t+1}}t_{j , x}z_t(y_{(t)})_j}{\\sum_x\\sum_jm^{(i_{t+1})}_{x , y_{t+1}}t_{j , x}z_t(y_{(t)})_j},\\end{aligned}\\ ] ] since @xmath15 does not depend on @xmath16 and @xmath17 .",
    "note that for each information state @xmath18 and each observation process @xmath0 , there are at most @xmath19 possible information states at the next step , which are given explicitly by @xmath20 for each observation @xmath21 .",
    "[ lem : informationdistributionrecurrence ] the information distribution satisfies the recurrence relation @xmath22 where the sum is taken over all observation processes @xmath0 and all observation states @xmath1 , @xmath23 is the dirac measure on @xmath24 , and @xmath25 is the matrix product considering @xmath9 as a row vector .",
    "since @xmath26 is a deterministic function of @xmath27 , given that @xmath28 , @xmath29 this depends only on @xmath30 and @xmath31 , so given that @xmath32 and @xmath33 , @xmath34 integration over @xmath35 gives @xmath36 by definition [ def : informationstate ] , @xmath37 is the posterior distribution of @xmath38 given the observations up to time @xmath39 , so @xmath40 , the @xmath16th coordinate of the vector @xmath9 . since @xmath37 is a function of @xmath27 , which is a function of @xmath41 and the observation randomness @xmath42 , by the markov property as in definition [ def : observationprocess ] , @xmath43 substituting ( [ eqn : pygivenz ] ) into ( [ eqn : mutplusone ] ) completes the proof .    note",
    "that lemma [ lem : informationdistributionrecurrence ] shows that the information distribution is given by a linear dynamical system on @xmath44 , and therefore the information state is a markov chain with state space @xmath24 .",
    "we will use tools in markov chain theory to analyse the convergence of the information state , for which it will be convenient to give a name to this recurrence .",
    "[ def : transitionfunction ] the * transition function * of the information distribution is the deterministic function @xmath45 given by @xmath46 , extended linearly to all of @xmath44 by the recurrence in lemma [ lem : informationdistributionrecurrence ] .",
    "the coefficients @xmath47 are called the * @xmath48-functions*.    we now give a criterion under which the information state is always positive recurrent .    a discrete state markov chain @xmath38 is called * ergodic * if it is irreducible , aperiodic and positive recurrent .",
    "such a chain has a unique invariant measure @xmath49 , which is a limiting distribution in the sense that @xmath38 converges to @xmath49 in total variation norm @xcite .",
    "a discrete state markov chain @xmath38 is called * positive * if every transition probability is strictly positive , that is , for all @xmath50 , @xmath51 .",
    "this is a stronger condition than ergodicity .",
    "[ def : anchored ] we shall call a hidden markov model * anchored * if the underlying markov chain @xmath38 is ergodic , and for each observation process",
    "@xmath0 , there is a state @xmath52 and an observation @xmath53 such that @xmath54 and @xmath55 for all @xmath56 .",
    "the pair @xmath57 is called an * anchor pair*.    heuristically , the latter condition allows for perfect information @xmath58 whenever the observation @xmath53 is made using observation process @xmath0 .",
    "this anchors the information chain in the sense that this state can be reached with positive probability from any other state , thus resulting in a recurrent atom in the uncountable state chain @xmath37 . on the other hand , since each information state can make a transition to only finitely many other information states , starting the chain at @xmath58 results in a discrete state markov chain , for which it is much easier to prove positive recurrence .",
    "[ lem : anchor ] in an anchored hidden markov model , for any anchor pair @xmath57 , @xmath59 for all @xmath60 .    when @xmath56 , @xmath55 by definition [ def : anchored ] , so every term in the numerator of definition [ def : rfunction ] is zero except the coefficient of @xmath58 .",
    "since we know the coefficients have sum 1 , it follows that @xmath61 .",
    "[ lem : alpha ] in a positive , anchored hidden markov model , the @xmath48-functions @xmath62 , for each @xmath63 , are uniformly bounded below by some @xmath64 , that is , @xmath65 for all @xmath0 and @xmath18 .",
    "we can write @xmath66 by definitions [ def : transitionfunction ] and [ def : anchored ] , which is bounded below by @xmath67 since @xmath68 . since each @xmath54 , if all the entries of @xmath69 are positive , then @xmath70 is bounded below uniformly in @xmath18 for fixed @xmath0 , which then implies a uniform bound in @xmath18 and @xmath0 since there are only finitely many @xmath0 .",
    "[ def : orbit ] for each state @xmath71 , the * orbit * @xmath72 of @xmath73 under the @xmath2-functions is @xmath74 by requiring the @xmath48-functions to be positive , we exclude points in the orbit which are reached with zero probability .",
    "let @xmath75 .",
    "[ prop : discreteness ] in a positive , anchored hidden markov model , there exists a constant @xmath76 such that for all measures @xmath77 , the mass of the measure @xmath78 outside @xmath79 is bounded by @xmath80 , that is , @xmath81",
    ".    we can rewrite definition [ def : transitionfunction ] as @xmath82 where @xmath83 in this notation , the integral is the lebesgue integral of the function @xmath84 with respect to the measure @xmath85 . since @xmath84 takes values in the @xmath44 and @xmath85",
    "is a probability , the integral also takes values in @xmath86 , thus @xmath87 maps the information state space @xmath44 to itself .",
    "since @xmath88 is a measure supported on the set of points reachable from @xmath18 via an @xmath2-function , and @xmath79 is a union of orbits of @xmath2-functions and therefore closed under @xmath2-functions , it follows that all mass in @xmath79 is mapped back into @xmath79 under the evolution function , that is @xmath89 on the other hand , by lemma [ lem : anchor ] , @xmath90 for all @xmath18 , hence @xmath91 putting these together gives @xmath92 setting @xmath93 gives @xmath94 , hence @xmath81 by induction . by lemma [ lem : alpha ] , @xmath95 , while @xmath96 since we can always choose a larger value .",
    "up to this point , we have considered the evolution function as a deterministic function @xmath45 .",
    "however , we can also consider it as a probabilistic function @xmath97 . by definition [ def : transitionfunction ]",
    ", @xmath87 maps points in @xmath79 to @xmath79 , hence the restriction @xmath98 gives a probabilistic function , and therefore a markov chain , with countable state space @xmath79 .    by proposition [ prop : discreteness ] , the limiting behaviour of the information chain takes place almost entirely in @xmath79 in some sense , so we would expect that convergence of the restricted information chain @xmath99 is sufficient for convergence of the full information chain @xmath87 .",
    "this is proved below .",
    "[ prop : positiverecurrent ] in a positive , anchored hidden markov model , under any policy , the chain @xmath99 has at least one state of the form @xmath58 which is positive recurrent , that is , whose expected return time is finite .",
    "construct a markov chain @xmath100 on the set @xmath101 , with transition probabilities @xmath102 for all @xmath0 , @xmath103 whenever @xmath104 is nonempty , and all other transition probabilities zero .",
    "we note that this is possible since we allow each state a positive probability transition to some other state .",
    "since @xmath100 is a finite state markov chain , it must have a recurrent state .",
    "each state @xmath72 can reach some state @xmath105 , so some state @xmath105 is recurrent ; call it @xmath106 .",
    "consider a state @xmath107 of the chain @xmath87 which is reachable from @xmath108 , where @xmath109 is a composition of @xmath110 @xmath2-functions with corresponding @xmath48-functions nonzero .",
    "since the @xmath105 partition @xmath24 , one of them must contain @xmath111 ; call it @xmath112 .",
    "we will assume @xmath113 ; the proof follows the same argument and is simpler in the case when @xmath114 .    by definition of the @xmath48-functions , @xmath115 this means that @xmath116 is reachable from @xmath108 in the chain @xmath87 , hence in the chain @xmath100 , @xmath112 is reachable from @xmath106 , and by recurrence of @xmath106 , @xmath106 must also be reachable from @xmath112 via some sequence of positive probability transitions @xmath117 by definition of @xmath118 , @xmath119 is nonempty , and thus contains some point @xmath120 , where @xmath121 is a composition of @xmath122 @xmath2-functions with corresponding @xmath48 nonzero .    by definition [ def : orbit ] , each transition @xmath123 to @xmath124 in the information chain occurs with positive probability , so @xmath125 since @xmath126 , by anchoredness and positivity , @xmath127 the markov property then gives @xmath128 continuing via the sequence ( [ eqn : path ] ) , we obtain @xmath129 thus , for every state @xmath130 reachable from @xmath108 , we have found constants @xmath131 and @xmath132 such that @xmath133 by lemma [ lem : alpha ] , @xmath134 is uniformly bounded below , while @xmath135 depends only on the directed path ( [ eqn : path ] ) and not on @xmath18 , and thus is also uniformly bounded below since there are only finitely many @xmath105 , and hence it suffices to choose finitely many such paths .",
    "similarly , @xmath136 also depends only on the directed path ( [ eqn : path ] ) , and thus is uniformly bounded above . in particular , it is possible to pick @xmath136 and @xmath137 such that @xmath138 and @xmath139 .",
    "let @xmath140 be the first entry time into the state @xmath108 . by the above bound",
    ", we have @xmath141 for any initial state @xmath18 reachable from @xmath108 .",
    "letting @xmath142 and @xmath143 be independent copies of @xmath140 and @xmath85 , the time - homogeneous markov property gives @xmath144 & \\hspace{2cm}\\times{\\mathbb p}\\big(z_{ks}=z'\\big|\\tau > ks , z_0=z\\big)\\nonumber\\\\[.2 cm ] & \\le\\sup_{z'}{\\mathbb p}\\big(\\tau>(k+1)s\\big|\\tau > ks , z_{ks}=z'\\big)\\nonumber\\\\ & = \\sup_{z'}{\\mathbb p}\\big(\\tau'>s\\big|z'_0=z'\\big)\\nonumber\\\\ & \\le1-c.\\end{aligned}\\ ] ] by induction , @xmath145 for all @xmath146 .",
    "dropping the condition on the initial distribution @xmath147 for convenience , we have @xmath148 & = \\sum_{k\\in{\\mathbb z}^+}{\\mathbb p}(\\tau > k)=\\sum_{k\\in{\\mathbb z}^+}\\sum_{0\\le t < s}{\\mathbb p}(\\tau > ks+t)\\nonumber\\\\ & \\le\\sum_{k\\in{\\mathbb z}^+}\\sum_{0\\le t < s}{\\mathbb p}(\\tau > ks)\\le\\sum_{k\\in{\\mathbb",
    "z}^+}s(1-c)^k=\\frac sc<\\infty.\\end{aligned}\\ ] ] in particular , @xmath149<\\infty$ ] , so @xmath108 is a positive recurrent state .    [",
    "lem : linearity ] the transition function @xmath87 , considered as an operator on the real banach space @xmath86 of signed radon measures on @xmath24 with the total variation norm , is linear with operator norm @xmath150 .",
    "linearity follows immediately from the fact that @xmath87 is defined as a finite sum of integrals .    for each @xmath151",
    ", let @xmath152 be the hahn decomposition , so that @xmath153 by definition of the total variation norm .    if @xmath154 , then @xmath155 by linearity of @xmath87 .",
    "otherwise , let @xmath156 , so that @xmath157 .",
    "since @xmath87 maps probability measures to probability measures , @xmath158 , and similarly for @xmath159 , hence by linearity of @xmath87 and the triangle inequality , @xmath160 this shows that @xmath161 .",
    "picking @xmath162 to be any probability measure gives @xmath163 , hence @xmath150 .",
    "[ thm : convergence2 ] in a positive , anchored hidden markov model , irreducibility and aperiodicity of the restricted information chain @xmath99 is a sufficient condition for convergence in distribution of the information state @xmath37 to some discrete invariant measure @xmath164 .    in this case , the restricted chain @xmath99 is irreducible , aperiodic and positive recurrent  that is , ergodic  and hence has a unique invariant probability measure @xmath165 , which can be considered as an element of @xmath44 supported on @xmath166 .",
    "we will show that the information distribution @xmath167 converges in total variation norm to @xmath165 . fix @xmath168 and @xmath64 , and pick @xmath169 such that @xmath170 , with @xmath171 as defined in proposition [ prop : discreteness ] .",
    "let @xmath172 be the restriction of the probability measure @xmath173 to @xmath79 , which is a positive measure with total mass @xmath174 , hence we can divide to obtain the probability measure @xmath175 supported on @xmath79 .",
    "since @xmath99 is ergodic and @xmath176 is a probability measure , @xmath177 converges to @xmath165 in total variation norm , so pick @xmath178 such that for all @xmath179 , @xmath180 .",
    "for any @xmath181 , we have the triangle inequality bound @xmath182 by lemma [ lem : linearity ] , @xmath87 is linear with operator norm 1 , so by proposition [ prop : discreteness ] , @xmath183 since @xmath184 , again using linearity and the fact that @xmath185 , @xmath186 finally , again using proposition [ prop : discreteness ] @xmath187 we see that for all @xmath181 , @xmath188 , so the information state @xmath167 converges to @xmath165 in total variation norm and therefore in distribution .",
    "the conditions of theorem [ thm : convergence2 ] can be weakened to the case when the underlying chain is only ergodic .",
    "given an ergodic finite transition matrix @xmath69 , some power @xmath189 will be positive , and thus we should still have convergence by taking time in @xmath146-step increments , since the information state will not fluctuate too much within those @xmath146 steps .",
    "the difficulty lies in the fact that the information state taken in @xmath146-step increments is not the same as the information state of the @xmath146-step chain .",
    "it is our belief that the information state converges in all but a small number of pathological examples ; however , we are only able to prove it in the above cases .",
    "if the information state does not converge , then it does not make sense to consider a limiting expected entropy .",
    "however , it is possible that a cesro sum of the expected entropy converges , and the limsup and liminf will certainly exist .",
    "alternatively , we could simply work with a finite time horizon .",
    "continuing further within the general case has proven to be quite difficult , so we will restrict the remainder of our results to a special case , where there are two states and two observation processes with two possible observations each , and each observation process observes a different underlying state perfectly .",
    "formally , @xmath190 , and the transition and observation matrices are @xmath191 in order to exclude trivial cases , we will require that the parameters @xmath192 , @xmath193 , @xmath194 and @xmath195 are contained in the open interval @xmath196 , and @xmath197 .",
    "note that this special case exactly corresponds to the problem of searching for a moving target studied by macphee and jordan @xcite , although our cost function , limiting expected entropy , is very different from theirs , expected cumulative sum of prescribed costs until the first zero - entropy observation .",
    "we will begin by proving that given this restriction , the information state always converges in distribution , except for one case which is pathological in a sense that will be explained later .",
    "this proof is both a correction and an improvement of the proof given in @xcite .    in the special case ,",
    "the space @xmath24 is a 1-simplex embedded in @xmath198 , which we can identify with the interval @xmath199 $ ] , via equating each point @xmath200 $ ] with the point @xmath201 , so that @xmath18 represents the mass at 0 in the distribution .    by substituting these parameters into into definition [ def :",
    "transitionfunction ] , the transition function in the special case is @xmath202 where :    * @xmath203 ; * @xmath204 ; * @xmath205 ; and * @xmath206 .",
    "note that the second line of ( [ eqn : specialcase ] ) consists of two point masses at 0 and 1 , which is a feature of the anchoredness condition . in the special case",
    ", it allows us to represent the location of masses by only two @xmath2-functions .",
    "we will continue to use the symbols @xmath207 , @xmath208 , @xmath209 and @xmath210 in their meaning above for the remainder of our discourse .",
    "note that we have simplified the notation , in that @xmath209 represents the @xmath2-function @xmath211 , while @xmath212 does not appear since @xmath213 identically , and similarly for the other symbols .    since the special case satisfies the conditions of positivity and anchoredness , proposition [ prop : positiverecurrent ] applies , so irreducibility and aperiodicity of the information chain are sufficient for ergodicity .",
    "we now show that this occurs in all but two exceptional cases :    * case 1 : each orbit is contained entirely in the policy region which maps to that orbit , that is , @xmath214 and @xmath215 .",
    "note that by ( [ eqn : specialcase ] ) , the @xmath48-functions are always strictly positive , so we must have @xmath216 and @xmath217 .",
    "* case 2 : the orbits alternate periodically between policy regions , that is , @xmath218 and @xmath219 , where @xmath2 is the combined @xmath2-function @xmath220 .",
    "let case 0 denote the general case when neither case 1 nor case 2 occurs .",
    "[ lem : irreducible ] the chain @xmath99 has only one irreducible recurrent class , except in case 1 , where it splits into two irreducible recurrent classes , both of which are positive recurrent .    by proposition [ prop : positiverecurrent ] , without loss of generality , assume that the state 0 is positive recurrent .",
    "if 1 is reachable from 0 , that is , there is some @xmath39 such that @xmath221 , then 0 is also reachable from 1 since 0 is recurrent , hence 0 and 1 are in the same irreducible recurrent class . by ( [ eqn : specialcase ] )",
    ", either 0 or 1 is reachable from every state , so there can not be any other recurrent classes .    otherwise , if 0 is reachable from 1 but 1 is not reachable from 0 , then 1 is transient , and furthermore , all of @xmath222 is transient since any @xmath223 is reachable only via the transient state 1 , while all of @xmath224 is reachable from the recurrent state 0 and hence forms the only irreducible recurrent class .    finally ,",
    "if 0 and 1 are both unreachable from each other , then it must be the case that @xmath214 and @xmath215 , in which case the chain splits into two irreducible classes @xmath224 and @xmath222 , both of which are positive recurrent by the argument in proposition [ prop : positiverecurrent ] .    [",
    "lem : aperiodic ] the recurrent classes are aperiodic , except in case 2 , where the chain consists of a single irreducible recurrent class with period 2 .",
    "if any recurrent class is periodic , then at least one of 0 or 1 is recurrent and periodic ; without loss of generality suppose it is 0 . since 0 is periodic , it can not reach itself in 1 step , so must be contained in @xmath106 , thus 1 is reachable from 0 and hence is in the same irreducible recurrent class .",
    "note that by the same argument , @xmath225 , thus 0 reaches itself in 2 steps and hence the period must be 2 .",
    "this means 0 can not reach itself in an odd number of steps , so @xmath226 when @xmath146 is even and @xmath227 when @xmath146 is odd , and similarly for the orbit of 1 , which is the only possibility of periodicity .",
    "thus , there is one exception in which the information chain is periodic , and another in which it is reducible .",
    "as will be evident , both exceptions can be remedied .",
    "we begin by giving a class of policies under which reducibility can not occur .",
    "this class of policies is simple enough to be easily analysed , and as will be conjectured later , always includes the optimal policy in any hidden markov model within the special case .",
    "[ def : thresholdpolicy ] a policy @xmath10 is called a * threshold policy * if its preimages @xmath228 and @xmath229 are both intervals .",
    "a threshold policy is indeed given by a threshold , since there must be some boundary point between @xmath230 and @xmath106 , such that one observation process is used on one side and the other is used on the other side . outside the threshold case ,",
    "it is unclear what the equivalent definition would be , since the concept of an interval does not generalise easily to higher dimensions .",
    "[ lem : r0bigger ] the linear fractional transformations @xmath209 and @xmath210 satisfy the inequality @xmath231 for all @xmath200 $ ] .",
    "we can write @xmath232 .",
    "note that the coefficient @xmath233 of @xmath194 is strictly positive and in the denominator , while @xmath234 is exactly the same with @xmath235 instead of @xmath194 . since @xmath236 , @xmath237 , hence @xmath231 .",
    "[ lem : monotonic ] the linear fractional transformations @xmath209 and @xmath210 are both strictly increasing when @xmath238 and both strictly decreasing when @xmath239 .",
    "the derivative of @xmath209 is @xmath240 , which is positive everywhere if @xmath238 and negative everywhere if @xmath239 . the same holds for @xmath210 , since it is identical with @xmath235 instead of @xmath194 .",
    "[ lem : fixedpoint ] the linear fractional transformations @xmath209 and @xmath210 have unique fixed points @xmath241 and @xmath242 , which are global attractors of their respective dynamical systems .",
    "split the interval @xmath199 $ ] into open subintervals at its interior fixed points , noting that by inspection , the two boundary points are not fixed .",
    "since linear fractional transformations have at most two fixed points , there must be between one and three such subintervals , all non - degenerate .    by continuity , in any such subinterval @xmath243 , either @xmath244 for all @xmath245 , or @xmath246 for all @xmath245 .",
    "since @xmath247 , @xmath244 for all points @xmath18 in the leftmost subinterval , and since @xmath248 , @xmath246 for all points @xmath18 in the rightmost subinterval .",
    "this means that there are at least two subintervals .",
    "if there were three subintervals , @xmath249 , @xmath250 and @xmath251 in that order , then either @xmath244 for all @xmath252 , or @xmath246 for all @xmath252 .",
    "in the first case , the fixed point between @xmath249 and @xmath250 is attracting on the left and repelling on the right , and in the second case , the fixed point between @xmath250 and @xmath251 is repelling on the left and attracting on the right .",
    "however , such fixed points can only occur for parabolic linear fractional transformations with only one fixed point .",
    "this is a contradiction , hence there can not be three subintervals .",
    "thus , there are exactly two subintervals , and exactly one fixed point , which must then be an attractor across all of @xmath199 $ ] .",
    "[ lem : eta0bigger ] the fixed points satisfy @xmath253 .    by lemma [",
    "lem : r0bigger ] , @xmath254 .",
    "first consider the case @xmath238 .",
    "applying lemma [ lem : monotonic ] @xmath146 times gives @xmath255 , so the orbit of @xmath241 under @xmath210 is monotonically decreasing , but it also converges to @xmath242 by lemma [ lem : fixedpoint ] , hence @xmath253 .    in the remaining case @xmath239 , suppose @xmath256 .",
    "then by lemma [ lem : monotonic ] , @xmath257 , which is a contradiction , hence @xmath253 .",
    "[ prop : exception ] the first exception to theorem [ thm : convergence2 ] , case 1 , can not occur under a threshold policy .",
    "suppose @xmath214 , @xmath215 , and the policy is threshold .",
    "since @xmath258 and @xmath259 , it follows that every point of @xmath224 is less than every point of @xmath222 .",
    "since @xmath224 and @xmath222 are the orbits of 0 and 1 under @xmath209 and @xmath210 respectively , by lemma [ lem : fixedpoint ] , they have limit points @xmath241 and @xmath242 respectively , hence @xmath260 .",
    "this contradicts lemma [ lem : eta0bigger ] , hence this can not occur .",
    "the remaining exception is when the information chain is periodic with period 2 , in which case the expected entropy oscillates between two limit points .",
    "the limiting expected entropy can still be defined in a sensible way , by taking the average , minimum or maximum of the two limit points , depending on which is most appropriate for the situation .",
    "thus , for threshold policies , it is possible to define optimality without exception .",
    "we conclude this section by writing down a closed form general expression for the limiting expected entropy .",
    "[ prop : entropyformula ] under the conditions of theorem [ thm : convergence2 ] , that is , in case 0 , the limiting expected entropy of a policy is given by @xmath261 where , for @xmath262 :    * @xmath263 is the entropy function and @xmath264 is the constant function with value 1 ; * @xmath220 and @xmath265 are the combined @xmath2-function and combined @xmath48-function respectively , with @xmath209 , @xmath210 , @xmath207 and @xmath208 defined as in ( [ eqn : specialcase ] ) ; * @xmath266 $ ] are defined by the recursion @xmath267 * @xmath268 are defined by the recursion @xmath269 * @xmath270 is the linear functional @xmath271    by theorem [ thm : convergence2 ] , a unique invariant probability measure @xmath272 exists , so it suffices to show that @xmath273 where @xmath274 is the dirac measure , and @xmath275 is extended in the obvious way to a linear functional @xmath276 .    by proposition [ prop : discreteness ] , the invariant measure is supported in the combined orbit set @xmath79 , and @xmath277 is the only point which can make a one - step transition under the restricted information chain @xmath99 to @xmath278 , with probability @xmath279 .",
    "since the masses at @xmath277 and @xmath278 are invariant , we must have @xmath280 it then follows that for some constants @xmath281 , @xmath282 the mass at @xmath277 is @xmath283 , which makes a transition under @xmath99 to 0 in one step with probability @xmath284 . since @xmath272 is an invariant measure ,",
    "mass is conserved at @xmath285 , hence @xmath286 since @xmath287 as @xmath288 , by telescoping series , @xmath289 multiplying the right hand side of ( [ eqn : conservationofmass ] ) by ( [ eqn : telescope ] ) with @xmath290 , then collecting coefficients of @xmath291 and @xmath292 , yields @xmath293 note that conservation of mass at @xmath294 is now automatic , since mass is conserved on all of @xmath79 and at every other point in @xmath79 .",
    "the second equation comes from requiring @xmath272 to have total mass 1 , that is , @xmath295 the solution to ( [ eqn : conservationofmass2 ] ) and ( [ eqn : totalmassone ] ) is exactly the required result .",
    "we note that the denominator is zero only when @xmath296 and @xmath297 for all @xmath298 , which is exactly the excluded case @xmath214 and @xmath215 .",
    "this proof can be generalised easily to the case of more than two observation processes , as long as each one is anchored with only two states .",
    "we now present computational results , again in the special case , which will illustrate the nature of the optimal policy in the case of minimising limiting expected entropy . since any such discussion must cover how best to calculate the limiting expected entropy given a hidden markov model and a policy , this is a natural place to start .",
    "the simpliest approach is to use the formula in proposition [ prop : entropyformula ] , noting that each of @xmath299 , @xmath300 , @xmath301 and @xmath302 is a product of two infinite series , each of which is bounded by a geometric series and hence well - approximated by truncation .",
    "specifically , we write the limiting expected entropy as @xmath303 where :    * @xmath304 ; * @xmath305 ; * @xmath306",
    ".    we can simplify the calculation slightly by recursively updating @xmath307 and @xmath308 , storing them as real - valued variables @xmath309 and @xmath310 .",
    "[ alg : entropy ] estimation of limiting expected entropy .    1 .",
    "define as functions the entropy @xmath263 , the mixed @xmath2-function @xmath311 and the mixed @xmath48-function @xmath312 ; 2 .",
    "pick a large number @xmath313 ; 3 .",
    "initialise variables @xmath314 , @xmath315 , @xmath316 , @xmath317 , @xmath318 and @xmath319 to 0 , @xmath320 and @xmath321 to 1 , @xmath111 to 0 and @xmath322 to 1 ; 4 .",
    "repeat the following @xmath323 times : 1 .   add @xmath320 to @xmath318 and @xmath321 to @xmath319 ; 2 .   add @xmath324 to @xmath316 and @xmath325 to @xmath317 ; 3 .   if @xmath114 , then add @xmath326 to @xmath314 , and if @xmath327 , then add @xmath328 to @xmath315 ; 4 .",
    "let @xmath329 and @xmath330 ; 5 .",
    "multiply @xmath320 by @xmath331 and @xmath321 by @xmath332 ; 5 .",
    "the limiting expected entropy @xmath333 is estimated by @xmath334    [ prop : errorbound ] the estimate @xmath335 satisfies the error bound @xmath336 where @xmath337 is the denominator of @xmath335 , and @xmath338    the formula for @xmath48 follows from definition [ def : transitionfunction ] , since the @xmath48-functions are linear and hence their maxima occur at the endpoints @xmath339 or @xmath340 .",
    "note that @xmath341 follows from the requirement that @xmath342 . since @xmath343 is monotonic increasing",
    ", this implies that the error bound is finite and vanishes as @xmath344 .",
    "since each series has non - negative summands , each truncated series is smaller than the untruncated series , hence @xmath345 the @xmath146th summand in each series is bounded by @xmath346 , hence @xmath347 the same bound holds for @xmath348 , hence @xmath349 similarly , since @xmath350 , @xmath351 combining via the triangle inequality gives the required bound .",
    "note that this bound depends only on the quantities @xmath48 and @xmath343 , which are easily calculated . in particular , this allows us to prescribe a precision and calculate the limiting expected entropy to within that precision by running the algorithm with unbounded @xmath313 and terminating when error bound reaches the desired precision .",
    "furthermore , since @xmath343 appears only in the denominator and grows monotonically with @xmath313 , we can replace it with @xmath352 for some small , fixed @xmath353 to calculate a prior a sufficient number of steps for any given precision .",
    "[ eg : errorbound ] in later simulations , we will pick @xmath354 $ ] , so that @xmath355 .",
    "this gives an error bound of @xmath356 while the constant appears daunting at first glance , solving for a prescribed error of @xmath357 gives @xmath358 hence , we require @xmath359 for any realistic value of @xmath343 , this is easily within computational bounds , as each iteration requires at most 36 arithmetic operations , 2 calls to the policy function , and 4 calls to the logarithm function .    an alternative approach to estimating limiting expected",
    "entropy would be to simulate observation of the hidden markov model under the given policy .",
    "the major drawback of this method is that it requires working with the information state @xmath37 , which takes values in @xmath360)$ ] , the set of probability measures on the unit interval , which is an infinite dimensional space .",
    "one possibility is to divide @xmath199 $ ] into subintervals and treat each subinterval as a discrete atom , but this produces a very imprecise result . even using an unrealistically small subinterval width of @xmath361 , the entropy function has a variation of over @xmath362 across the first subinterval , restricting the error bound to this order of magnitude regardless of the number of iterations . in comparison ,",
    "example [ eg : errorbound ] shows that the direct estimation method has greatly superior performance .",
    "an improvement is to use the fact that the limiting distribution @xmath272 is discrete , and store a list of pairs containing the locations and masses of discrete points .",
    "since any starting point moves to either 0 or 1 in one step , at the @xmath313th iteration , the list of points must contain at least the first @xmath313 points in the orbit of either 0 or 1 .",
    "each such point requires a separate calculation at each iteration , and thus the number of computations is @xmath363 rather than @xmath364 as for algorithm [ alg : entropy ] .",
    "since the number of iterations @xmath313 corresponds to the last point in the orbit of 0 or 1 which is represented , for any given @xmath313 , this method differs from the direct computation method only in the masses on these points , thus we would expect the relationship between precision and number of iterations to be similar . since the simulation method has quadratically growing number of computations , this would suggest that it is slower than the direct computation method , and indeed , this is also indicated by empirical trials .",
    "we will use the direct computation method of estimating limiting expected entropy for all of our results .      the problem of finding the policy which minimises limiting expected entropy is made much easier by restricting the search space to the set of threshold policies , as these can be represented by a single number representing the threshold , and a sign representing which observation process is used on which side of the threshold .",
    "the simplest approach is to pick a collection of test thresholds uniformly in @xmath199 $ ] , either deterministically or randomly , and test the limiting expected entropy at these thresholds , picking the threshold with minimal entropy as the optimal threshold policy .",
    "however , this method is extremely inefficient .",
    "proposition [ prop : discreteness ] shows that the policy only matters on the countable set @xmath365 $ ] , so moving the threshold does not change the policy as long as it does not move past a point in @xmath79 .    as shown in figures [ fig : regioni][fig : regionvi ] , points in @xmath79 tend to be quite far apart , and thus the naive approach will cause a large number of equivalent policies to be tested . on the other hand , points in @xmath79 close to the accumulation points are closely spaced , so even with a very fine uniform subset , some policies will be missed when the spacing between points in @xmath79 becomes less than the spacing between test points .    a better way is to decide on a direction in which to move the threshold , and select the test point exactly at the next point in the previous realisation of @xmath79 in the chosen direction , so that every threshold in between the previous point and the current point gives a policy equivalent to the previous policy",
    "this ensures that each equivalence class of policies is tested exactly once , thus avoiding the problem with the naive method .",
    "however , a new problem is introduced in that the set of test points depends on the iteration number @xmath313 , which determines the number of points of @xmath79 that are considered .",
    "this creates a circular dependence , in that the choice of @xmath313 depends on the desired error bound , the error bound depends on the policy , and the set of policies to be tested depends on @xmath313 .",
    "we can avoid this problem by adapting proposition [ prop : errorbound ] to a uniform error bound across all threshold policies .",
    "[ prop : errorboundthreshold ] for a threshold policy and @xmath366 , the error is @xmath367 where @xmath368 is the smallest integer such that @xmath369 , and @xmath370    first note that @xmath368 exists , since by lemma [ lem : eta0bigger ] , iterations of @xmath209 and @xmath210 converge to @xmath371 respectively .    using proposition [ prop : errorbound ] , it suffices to prove that for @xmath366 , @xmath372 it is not possible for @xmath373 and @xmath374 , since this would mean @xmath375 and @xmath376 , which gives the ordering @xmath377 , but @xmath230 and @xmath106 are intervals for a threshold policy .",
    "hence , either @xmath378 or @xmath379 for some @xmath380 .",
    "if @xmath381 , then @xmath382 .",
    "since @xmath383 , this gives @xmath384 , as required .",
    "a similar argument holds in the case @xmath385 .",
    "the existence of this uniform bound for @xmath343 in the threshold case is closely related to proposition [ prop : exception ] , which states that the exception case 1 , where @xmath386 and @xmath387 , can not occur in a threshold policy . in this exceptional case ,",
    "proposition [ prop : entropyformula ] does not hold , as the denominator is zero , and hence @xmath388 for all @xmath313 .",
    "the fact that this can not occur in a threshold policy is the key ingredient of this uniform bound .",
    "now that we have an error bound which does not depend on the policy , we can determine a uniform number of iterations @xmath313 that will suffice for estimating the limiting expected entropy for all threshold policies .",
    "this reduces the search space to a finite one , as each point in the orbits of 0 and 1 must be in one of the two policy regions , hence , there are at most @xmath389 policies",
    ". most of these will not be threshold policies , but since orbit points need not be ordered , there is no obvious bound on the number of threshold policies that need to be checked .",
    "simulation results later in this section will show that in most cases , the number of such policies is small enough to be computationally feasible .",
    "[ def : orientation ] the * orientation * of a threshold policy is the pair of whether @xmath230 is to the left or right of @xmath106 , and whether the threshold is included in the left or right interval .",
    "let @xmath390 $ ] , @xmath391(a_1]$ ] , @xmath392 $ ] and @xmath393(a_0]$ ] denote the four possibilities , with the square bracket indicating inclusion of the threshold and round bracket indicating exclusion .    our strategy for simplifying the space of threshold policies that need to be considered is to note that the policy only matters on @xmath79 , the support of the invariant measure .",
    "although @xmath79 depends on the policy , for a given orientation and threshold @xmath39 , any policy with the same orientation and some other threshold @xmath394 such that no points of @xmath79 lie between @xmath39 and @xmath394 is an equivalent policy , in that sense that the invariant measure is the same , since no mass exists in the region where the two policies differ .",
    "thus , for each orientation , we can begin with @xmath395 , and at each iteration , move the threshold left past the next point in @xmath79 , since every threshold in between is equivalent to the previous threshold .",
    "although @xmath79 changes at each step , this process must terminate in finite time since we already showed that there are only finitely many policies for any given @xmath313 , and by testing equivalence classes of policies only once , it is likely to that far fewer steps are required than the @xmath389 bound .",
    "furthermore , since @xmath79 is a discrete set , every threshold policy has an interval of equivalent threshold policies , so we can assume without loss of generality that the threshold is contained in the interval to the right , that is , only test the orientations @xmath390 $ ] and @xmath392 $ ] .",
    "[ alg : optimalthreshold ] finding the optimal threshold policy .    1 .",
    "find @xmath368 , the smallest integer such that @xmath369 , by repeated application of @xmath209 and @xmath210 to 0 and 1 respectively ; 2 .",
    "prescribe an error @xmath396 and determine the number of iterations @xmath397 3 .",
    "start with the policy @xmath398 and @xmath399 $ ] with @xmath395 , that is , @xmath230 is the whole interval and @xmath106 is empty but considered as being to the right of @xmath230 , and loop until @xmath400 : 1 .",
    "run algorithm [ alg : entropy ] , and let the next value of @xmath39 be the greatest @xmath308 which is strictly less than the previous value of @xmath39 ; 2 .",
    "if entropy is less than the minimum entropy for any policy encountered so far , record it as the new minimum ; 4 .",
    "repeat with @xmath230 considered to be on the right of @xmath106 .",
    "we now calculate the location of the optimal threshold for a range of values of the parameters @xmath192 , @xmath193 , @xmath194 and @xmath195 .",
    "the results of this calculation , which will be presented in figure [ fig : thresholdregions ] , is the primary content of this section , as it will give an empirical description of the optimal threshold policy .",
    "since we have not been able to prove optimality analytically , except in the symmetric case of proposition [ prop : symmetric ] , this empirical description will provide our most valuable insight into the problem .    in order to facilitate understanding",
    ", we will define six classes of threshold policies , depending on the location and orientation of the threshold .",
    "diagrams of the invariant measure under each of these classes , presented in figures [ fig : regioni][fig : regionvi ] , will demonstrate that these classes of thresholds are qualitatively different , which will further manifest itself in the next section .",
    "we partition the interval into three subintervals with endpoints at the attractors @xmath241 and @xmath242 , noting that this produces three regions which consist entirely of a single equivalence class of policy .",
    "we also assign colours to these regions to accommodate the data presented in figure [ fig : thresholdregions ] .",
    "[ def : thresholdregions ] the six * threshold regions * are defined by partitioning the set of all threshold policies first by whether the orientation is @xmath390 $ ] or @xmath392 $ ] , then by the location of the threshold @xmath39 in relation to the accumulation points @xmath242 and @xmath241 , with inclusion of endpoints defined more precisely overleaf .",
    "we number them using roman numerals i through vi .",
    "note that if either @xmath401 or @xmath402 , then either @xmath230 or @xmath106 occupies the whole interval , depending on the orientation .",
    "in particular , @xmath390 $ ] with @xmath400 is equivalent to @xmath392 $ ] with @xmath395 , since in both cases @xmath106 is the whole interval , and similarly @xmath390 $ ] with @xmath395 is equivalent to @xmath392 $ ] with @xmath400 .",
    "thus , the space of all possible threshold policies consists of two disjoint intervals @xmath403 $ ] , each of whose endpoints is identified with an endpoint of the other interval , which is topologically equivalent to a circle . to be technically correct",
    ", we note that identifying 0 and @xmath404 does not present a problem , since we can simply extend the interval to @xmath405 $ ] for some small @xmath64 , and identify 0 and @xmath406 instead . while this results in a subinterval of the circle corresponding to the same policy , this does not add additional complexity since every threshold policy has an interval of equivalent policies .",
    "this topology is illustrated overleaf in figure [ fig : thresholddomain ] , followed by precise definitions and examples of the six threshold regions in figures [ fig : regioni][fig : regionvi ] , and finally our computational results in figure [ fig : thresholdregions ] .",
    "( i )  ( ii )  ( iii ) + $ ] , while the bottom line represents the orientation @xmath392 $ ]",
    ". the right end of the top line and the left end of the bottom line are both the policy @xmath407 $ ] and @xmath408 , so we can paste them together , and similarly for the left end of the top line and the right end of the bottom line .",
    "hence , we see that the set of threshold policies is topologically a circle.,title=\"fig : \" ] + 0  @xmath242   @xmath241  1 + $ ] , while the bottom line represents the orientation @xmath392 $ ] .",
    "the right end of the top line and the left end of the bottom line are both the policy @xmath407 $ ] and @xmath408 , so we can paste them together , and similarly for the left end of the top line and the right end of the bottom line .",
    "hence , we see that the set of threshold policies is topologically a circle.,title=\"fig : \" ] + ( iii )  ( iv )  ( v )   ( vi )  ( i )   +    * region i * ( represented by red in figure [ fig : thresholdregions ] ) : @xmath390 $ ] with @xmath409 or @xmath392 $ ] with @xmath402 . when @xmath238 , every policy here is equivalent to the all-@xmath106 policy , as the mass in the orbit of 0 approaches 0 .    , @xmath410 , @xmath411 , @xmath412 .",
    "entropy is 0.3145 . under the evolution function ,",
    "any mass eventually enters the grey region since it contains both accumulation points in its interior , after which it can not escape , hence in the limit , there is zero mass in the orbit of 0 , and the policy is equivalent the all-@xmath106 policy . ]",
    "* region ii * ( yellow ) : @xmath390 $ ] with @xmath413 .",
    "this is the most difficult to understand of the threshold policies , as the orbits do not converge to the accumulation points @xmath241 and @xmath242 , but rather , oscillate around the threshold @xmath39 .    , @xmath410 , @xmath411 , @xmath412 .",
    "entropy is 0.3251 .",
    "note that the masses do not converge to the accumulation points . ]    * region iii * ( green ) : @xmath390 $ ] with @xmath414 or @xmath392 $ ] with @xmath401 .",
    "when @xmath238 , every policy here is equivalent to the all-@xmath230 policy , since the mass in the orbit of 1 approaches 0 .",
    ", @xmath410 , @xmath411 , @xmath412 .",
    "entropy is 0.3337 . under the evolution function",
    ", any mass eventually enters the white region since it contains both accumulation points in its interior , after which it can not escape , hence in the limit , there is zero mass in the orbit of 1 , and the policy is equivalent to the all-@xmath230 policy . ]",
    "* region iv * ( cyan ) : @xmath392 $ ] with @xmath415 . for a policy in this region ,",
    "the first finitely many points of the orbit of 0 belongs to @xmath106 , while every other point lies in @xmath230 .",
    "note that @xmath401 is included in region iii .",
    ", @xmath410 , @xmath411 , @xmath412 .",
    "entropy is 0.3275 .",
    "the orbit of 1 approaches @xmath241 , while the orbit of 0 follows an approach sequence to @xmath242 for a finite number of steps ( in this case 2 steps ) before also approaching @xmath241 . ]",
    "* region v * ( blue ) : @xmath392 $ ] with @xmath416 .",
    "when @xmath238 , every policy here is equivalent to the policy @xmath417 and @xmath418 .    , @xmath410 , @xmath411 , @xmath412 .",
    "entropy is 0.3265 .",
    "note that the orbit of 0 converges to @xmath242 while the orbit of 1 converges to @xmath241 . ]",
    "* region vi * ( magenta ) : @xmath392 $ ] with @xmath419 . for a policy in this region ,",
    "the first finitely many points of the orbit of 0 belongs to @xmath106 , while every other point lies in @xmath230 . note that @xmath402 is included in region i. the lack of symmetry with region vi in terms of strict and non - strict inequalities",
    "is due to the choice that the threshold itself be included in the right region .    , @xmath410 , @xmath411 , @xmath412 .",
    "entropy is 0.3182 .",
    "the orbit of 0 approaches @xmath242 , while the orbit of 1 follows an approach sequence to @xmath241 for a finite number of steps ( in this case 1 step ) before also approaching @xmath242 . ]     on the major horizontal axis increasing rightwards , @xmath193 on the major vertical axis increasing downwards , @xmath194 on the minor horizontal axis increasing rightwards and @xmath195 on the minor vertical axis increasing downwards .",
    "each parameter is sampled by @xmath420 , omitting the trivial cases when @xmath421 , resulting in 152000 sample points .",
    "colours representing regions are defined in figures [ fig : regioni][fig : regionvi ] . ]",
    "the bound in proposition [ prop : errorboundthreshold ] is consistent with the empirical running time for the calculations used to generate figure [ fig : thresholdregions ] .",
    "our program experienced significant slowdowns when either @xmath192 and @xmath194 , or @xmath193 and @xmath195 were large , with the slowest occuring at the extreme point @xmath422 .",
    "this suggests that a uniform bound when @xmath423 is impossible , and that algorithm [ alg : optimalthreshold ] will fail for sufficiently large values of @xmath48 . on the other hand , either @xmath424 or @xmath425 results in trivial limiting behaviour of the underlying chain , so this situation is unlikely to occur in any practical application .",
    "note that in figure [ fig : thresholdregions ] , the half of the main diagonal where @xmath238 is entirely blue .",
    "this can be proven analytically .",
    "we note that the condition @xmath238 corresponds to the underlying markov chain having positive one - step autocorrelation , which is a reasonable assumption when the frequency of observation is greater than the frequency of change in the observed system , since in this case , one would not expect the system to oscillate with each observation .",
    "[ prop : symmetric ] in the symmetric case @xmath426 and @xmath427 , with @xmath238 , the optimal general policy is the unique region v policy @xmath428 given by @xmath417 and @xmath418 .",
    "proposition [ prop : entropyformula ] gives the limiting expected entropy as a convex combination of two quantities , hence @xmath429 note that equality is realised when the two quantities above are equal , which occurs under @xmath428 , since in that case @xmath430 and @xmath431 .",
    "let @xmath432 , @xmath433 and @xmath434 .",
    "in order to prove that @xmath428 is optimal , by symmetry , it suffices to prove that it minimises @xmath435 .    first , we show that for each @xmath146 and any other policy @xmath10 , @xmath436 , where the notation @xmath437 makes the dependence on the policy explicit .    by lemma [",
    "lem : r0bigger ] , @xmath438 . by lemma [ lem : monotonic ] and our assumption that @xmath238 , iterations of @xmath209 and @xmath210 approach their limits monotonically , hence @xmath439 and @xmath440 .",
    "these inequalities are illustrated in figure [ fig : symmetric ] .",
    "combining these inequalities gives @xmath441 .",
    "entropy is concave on the interval @xmath199 $ ] , and therefore on the subinterval @xmath442 $ ] , hence @xmath443 since @xmath444 and entropy is increasing on @xmath445 $ ] , @xmath446 , which is equal to @xmath447 by symmetry .",
    "hence , the inequality ( [ eqn : symmetricineq ] ) reduces to @xmath448 , that is , @xmath449 .     and",
    "@xmath450 in relation to 0 , @xmath242 , @xmath241 and 1 .",
    "all positions are fixed except that @xmath450 may be to the left of @xmath242 . since @xmath451 lies to the left of @xmath242 and the diagram is symmetric , @xmath451 has lower entropy than @xmath241 . since @xmath450 lies between @xmath451 and @xmath241 and entropy is concave , @xmath450 has higher entropy than @xmath451 .",
    "hence the @xmath428 minimises the entropy at @xmath452.,title=\"fig : \" ] + 0  @xmath451  @xmath242  @xmath450  @xmath241  1     next , we show that for each @xmath146 and any other policy @xmath10 , @xmath453 let @xmath454 . since @xmath238 , by ( [ eqn : specialcase ] )",
    ", @xmath208 is decreasing , while @xmath455 is increasing .",
    "we have already established that @xmath441 and @xmath456 , which implies @xmath457 and @xmath458 respectively .",
    "this shows that @xmath459 .    for @xmath460 ,",
    "write @xmath461 since @xmath462 and @xmath463 , decreasing @xmath464 increases @xmath465 , and the same is true for @xmath466 , since in that case we can write the expression in the same way with @xmath467 .",
    "this proves ( [ eqn : coeffdominance ] ) .    using ( [ eqn : coeffdominance ] ) and @xmath468 , for any @xmath469 , @xmath470 since @xmath471 identically , the second series vanishes as @xmath472 , while the first series is always non - negative by ( [ eqn : coeffdominance ] ) , hence @xmath473 .",
    "thus , @xmath474 this proves the required minimisation .",
    "note that the proof above relies heavily on the fact that equality is attained in ( [ eqn : symmetrich ] ) .",
    "this occurs only in the symmetric case , and thus this approach does not generalise readily .",
    "the complexity of the proof in the symmetric case is indicative of the difficulty of the problem in general , and thus highlights the importance of the empirical description provided by figure [ fig : thresholdregions ] .",
    "in the course of performing the computations to generate figure [ fig : thresholdregions ] , we noticed that entropy is unimodal with respect to threshold , with threshold considered as a circle as in figure [ fig : thresholddomain ] . while we can not prove this analytically , it is true for each of the 152000 points in the parameter space considered .",
    "this allows some simplification in finding the optimal threshold policy , since finding a local minimum is sufficient .",
    "thus , we can alter algorithm [ alg : optimalthreshold ] to begin by testing only two policies , then testing policies in the direction of entropy decrease until a local minimum is found .",
    "however , the running time improvement is only a constant factor ; if we model entropy as a standard sinusoid with respect to threshold , then the running time decreases by a factor of 3 on average .",
    "the problem of determining the optimal general policy is much more difficult , due to the complexity of the space of general policies .",
    "since a policy is uniquely determined by the value of the policy function at the orbit points , this space can be viewed as a hypercube of countably infinite dimension , which is much more difficult to study than the space of threshold policies , which is a circle .",
    "one strategy is to truncate the orbit and consider a finite dimensional hypercube , justified by the fact that orbit points have masses which decay geometrically , and thus the tail contributes very little .",
    "however , a truncation at @xmath313 ( that is , force the policy to be constant on @xmath475 , and similarly for the orbit of 1 ) gives @xmath389 possible policies , which is still far too large to determine optimality by checking the entropy of each policy .",
    "the next approximation is to only look for locally optimal policies , in the sense that changing the policy at each of the @xmath476 truncated orbit points increases entropy , and hope that by finding enough such locally optimal policies , the globally optimal policy will be among them .",
    "since a hypercube has very high connectivity , regions of attraction tend to be large , which heuristically suggests that this strategy will be effective .",
    "[ alg : localoptimum ] finding a locally optimal truncated policy .    1 .",
    "pick @xmath313 , and a starting policy , expressed as a pair of sequences of binary digits @xmath477 , with @xmath478 ; 2 .",
    "cycle through the digits @xmath479 , flipping the digit if it gives a policy with lower entropy , otherwise leaving it unchanged ; 3 .",
    "if the previous step required any changes , repeat it , otherwise a locally optimal truncated policy has been found .",
    "we picked @xmath480 since this allows a policy to be easily expressed as two unsigned 64-bit integers , and for each of the 152000 uniformly spaced parameters of figure [ fig : thresholdregions ] , we generated 10 policies uniformly on the hypercube and applied algorithm [ alg : localoptimum ] .",
    "none of the locally optimal policies for any of the parameter values had lower entropy than the optimal threshold policy from figure [ fig : thresholdregions ] , and on average 98.3% of them were equivalent to the optimal threshold policy , up to a precision of 0.1% , indicating that the optimal threshold policy is locally optimal with a very large basin of attraction , which strongly suggests that it is also the globally optimal policy .    in the special case ,",
    "the infimum of entropy attainable under threshold policies is the same as that under general policies .",
    "the fact that a large proportion of locally optimal policies have globally optimal entropy gives a new method for finding the optimal policy . by picking 10 random truncated policies and running algorithm [ alg : localoptimum ]",
    ", at least one of them will yield an optimal policy with very high probability .",
    "empirical observations suggest that this method is slower than algorithm [ alg : optimalthreshold ] on average , but since the success rate remains high while algorithm [ alg : optimalthreshold ] becomes significantly slower as @xmath48 approaches 1 , this method is a better alternative for some parameter values .    .",
    "darkness increases with the proportion of simulated locally optimal policies which have the same entropy as the optimal threshold policy , up to a precision of 0.1% .",
    "the average is 9.83 out of 10 , but the distribution is far from uniform  local optima are exceedingly likely to be the same as the threshold optimum for some parameter values and exceedingly unlikely for others .",
    "the boundaries are approximately those of the threshold regions ( see figure [ fig : thresholdregions ] ) , with some imprecision due to the non - deterministic nature of the simulation data . ]",
    "one last policy of interest is the greedy policy . in the previous sections",
    ", we considered a long term optimality criterion in the minisation of limiting expected entropy , but in some cases , it may be more appropriate to set a short term goal . in particular , one may instead desire to minimise expected entropy at the next step , in an attempt to realise maximal immediate gain while ignoring future implications .",
    "[ def_greedy ] the * greedy * policy is the policy such that the expected entropy after one observation is minimised .",
    "up to an exchage of strict and non - strict inequalities , this is given by @xmath481:\\alpha_0(z)h(r_0(z))<\\alpha_1(z)h(r_1(z))\\},\\\\ & a_1=\\{z\\in[0,1]:\\alpha_0(z)h(r_0(z))\\ge\\alpha_1(z)h(r_1(z))\\}.\\end{aligned}\\ ] ]    the greedy policy has the benefit of being extremely easy to use , as it only requires a comparison of two functions at the current information state .",
    "since these functions are smooth , efficient numerical methods such as newton - raphson can be used to determine the intersection points , thus allowing the policy to be described by a small number of thresholds .",
    "in fact , only one threshold is required , as computational results show the greedy policy always a threshold policy . using the 152000 uniformly distributed data points from before , in each case the two functions defining the greedy policy crossed over at most once .",
    "the greedy policy is always a threshold policy .",
    "note that @xmath482 .",
    "it may appear at first glance that the factor of @xmath195 violates symmetry , but recall that @xmath18 maps to @xmath483 under relabelling .    using this identity",
    ", the intersection points that define the greedy policy satisfy @xmath484 , where @xmath485 .",
    "it is easy to see that @xmath486 is monotonic decreasing on @xmath199 $ ] with range @xmath487 $ ] , hence @xmath488 is a well - defined one - parameter family of functions mapping @xmath199 $ ] to itself with fixed points at 0 and 1 .    on the other hand",
    ", @xmath489 is also a one - parameter family of functions mapping @xmath199 $ ] to itself with fixed points at 0 and 1 .",
    "since the range of @xmath209 is contained in @xmath196 , we can discount the endpoints @xmath490 and @xmath491 , hence it suffices to show that the equation @xmath492 has at most one solution for @xmath493 .",
    "convexity methods may help in this last step but we have not been able to complete the proof .    even when the greedy policy is not optimal , it is very close to optimal .",
    "of the 152000 uniformly distributed data points in figure [ fig : greedyoptimal ] below , the greedy policy is non - optimal at only 6698 points , or 4.41% , up to an error tolerance of @xmath494 . on average ,",
    "the greedy policy has entropy 0.0155% higher than the optimal threshold policy , with a maximum error of 5.15% occuring at the sample point @xmath495 , @xmath496 , @xmath497 and @xmath498 .",
    "thus the greedy polices provides an alternative suboptimal policy which is very easy to calculate and very close to optimal .    .",
    "light grey indicates that the greedy policy is the optimal threshold policy ; darker points indicate suboptimality with darkness proportional to error .",
    "similarly to figure [ fig : localoptimum ] , the suboptimal points lie on the boundaries of the threshold regions . ]    we make a final remark that the likelihood of a locally optimal policy being globally optimal as shown in figure [ fig : localoptimum ] , and the closeness of the greedy policy to the optimal threshold policy as shown in figure [ fig : greedyoptimal ] , both exhibit a change in behaviour at the boundaries of the threshold regions as shown in figure [ fig : thresholdregions ] .",
    "this suggests that these regions are indeed qualitatively different , and are likely to be interesting objects for further study .",
    "this thesis presents initial groundwork in the theory of hidden markov models with multiple observation processes .",
    "we prove a condition under which the information state converges in distribution , and give algorithms for finding the optimal policy in a special case , which provides strong evidence that the optimal policy is a threshold policy .      *",
    "the information state converges for an anchored hidden markov model with only ergodicity rather than positivity ; * the greedy policy is always a threshold policy ; * among threshold policies , the limiting expected entropy is unimodal with respect to threshold ; and * the optimal threshold policy is also optimal among general policies .",
    "possible approaches to these problems are likely to be found in @xcite , @xcite and @xcite .",
    "the author was not aware of these papers under after the date of submission , and thus was unable to incorporate their methods into this thesis .",
    "better algorithms and error bounds for finding the optimal policy are also a worthwhile goal .",
    "although our algorithms are computationally feasible with reasonable prescribed errors , our focus was on finding workable rather than optimal algorithms , and thus there is plenty of room for improvement .",
    "baum and t.  ptrie . statistical inference for probabilistic functions of finite state markov chains .",
    "_ annals of mathematical statistics_. volume 6 , number 37 , pages 15541563 , 1966 .",
    "o.  capp , e.  moulines and t.  rydn .",
    "_ inference in hidden markov models_. springer , new york , 2005 .",
    "g.  casella and r.l .",
    "_ statistical inference _ , second edition .",
    "thomson , pacific grove , 2002 .",
    "t.m .  cover and j.a .",
    "_ elements of information theory_. wiley , new york , 1991 .",
    "elliott , l.  aggoun and j.b .",
    "_ hidden markov models : estimation and control_. springer - verlag , new york , 1995 .",
    "evans and v.  krishnamurthy .",
    "optimal sensor scheduling for hidden markov model state estimation .",
    "_ international journal of control_. volume 18 , number 74 , pages 17371742 , 2001 .",
    "l.a .  johnston and v.  krishnamurthy",
    ". opportunistic file transfer over a fading channel : a pomdp search theory formulation with optimal threshold policies .",
    "_ ieee transactions on wireless communications_. volume 5 , number 2 , pages 394205 , 2006",
    ". v.  krishnamurthy .",
    "algorithms for optimal scheduling and management of hidden markov model sensors .",
    "_ ieee transactions on signals processing_. volume 6 , number 50 , pages 13821397 , 2002",
    ". v.  krishnamurthy and d.  djonin .",
    "structured threshold policies for dynamic sensor scheduling  a pomdp approach .",
    "_ ieee transactions on signals processing_. volume 5 , number 10 , pages 49384957 , 2007 .",
    "f.  le  gland and l.  mevel .",
    "exponential forgetting and geometric ergodicity in hidden markov models . _ mathematics of control , signals and systems_. volume 13 , pages 6393 , 2000 .",
    "macphee and b.p .",
    "optimal search for a moving target .",
    "_ probability in the engineering and informational sciences_. volume 9 , number 2 , pages 159182 , 1995 .",
    "meyn and r.l .",
    "tweedie . _ markov chains and stochastic stability_. springer - verlag , london , 1993 .",
    "rabiner . a tutorial on hidden markov models and selected applications in speech recognition .",
    "_ proceedings of the ieee_. volume 2 , number 77 , pages 257286 , 1989 .",
    "sensor scheduling for optimal observability using estimation entropy .",
    "_ proceedings of the fifth annual ieee international conference on pervasive computing and communications workshops_. march 2007 .",
    "d.  sinno and d.  cochran .",
    "dynamic estimation with selectable linear measurements .",
    "_ proceedings of the 1998 ieee international conference on accoustics , speech and signal processing_. may 1998 .",
    "_ supermodularity and complementarity_. princeton university press , 1998 .",
    "separation of estimation and control for discrete time systems .",
    "_ proceedings of the ieee_. volume 59 , number 11 , 1971 .",
    "_ hidden markov models with multiple observation processes_. honours thesis , university of melbourne , 2007 ."
  ],
  "abstract_text": [
    "<S> we consider a hidden markov model with multiple observation processes , one of which is chosen at each point in time by a policy  a deterministic function of the information state  and attempt to determine which policy minimises the limiting expected entropy of the information state . </S>",
    "<S> focusing on a special case , we prove analytically that the information state always converges in distribution , and derive a formula for the limiting entropy which can be used for calculations with high precision . using this fomula </S>",
    "<S> , we find computationally that the optimal policy is always a threshold policy , allowing it to be easily found . </S>",
    "<S> we also find that the greedy policy is almost optimal . </S>"
  ]
}