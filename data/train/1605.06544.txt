{
  "article_text": [
    "in everyday life we constantly face tasks we must perform in the presence of sensory uncertainty . a natural and efficient strategy",
    "is then to use probabilistic computation .",
    "behavioral experiments have established that humans and animals do in fact use probabilistic rules in sensory , motor and cognitive domains @xcite . however , the implementation of such computations at the level of neural circuits is not well understood .    in this work ,",
    "we ask how distributed neural computations can consolidate incoming sensory information and reformat it so it is accessible for many tasks .",
    "more precisely , how can the brain simultaneously infer marginal probabilities in a probabilistic model of the world ?",
    "previous efforts to model marginalization in neural networks using distributed codes invoked limiting assumptions , either treating only a small number of variables @xcite , allowing only binary variables @xcite , or restricting interactions @xcite .",
    "real - life tasks are more complicated and involve a large number of variables that need to be marginalized out , requiring a more general inference architecture .    here",
    "we present a distributed , nonlinear , recurrent network of neurons that performs inference about many interacting variables .",
    "there are two crucial parts to this model : the representation and the inference algorithm .",
    "we assume that brains represent probabilities over individual variables using probabilistic population codes ( ppcs ) @xcite , which were derived from using bayes rule on experimentally measured neural responses to sensory stimuli . here for the first time we link multiple ppcs together to construct a large - scale graphical model . for the inference algorithm ,",
    "many researchers have considered loopy belief propagation ( lbp ) to be a simple and efficient candidate algorithm for the brain @xcite . however",
    ", we will discuss one particular feature of lbp that makes it neurally implausible .",
    "instead , we propose that an alternative formulation of lbp known as tree - based reparameterization ( trp ) @xcite , with some modifications for continuous - time operation at two timescales , is well - suited for neural implementation in population codes .",
    "we describe this network mathematically below , but the main conceptual ideas are fairly straightforward : multiplexed patterns of activity encode statistical information about subsets of variables , and neural interactions disseminate these statistics to all other encoded variables for which they are relevant .",
    "in section [ ppc ] we review key properties of our model of how neurons can represent probabilistic information through probabilistic population codes .",
    "section [ trp ] reviews graphical models , loopy belief propagation , and tree - based reparameterization . in section [ sec : neuraltrp ] , we merge these ingredients to model how populations of neurons can represent and perform inference on large multivariate distributions .",
    "section [ experiments ] describes experiments to test the performance of network .",
    "we summarize and discuss our results in section [ conclusion ] .",
    "neural responses @xmath0 vary from trial to trial , even to repeated presentations of the same stimulus @xmath1 .",
    "this variability can be expressed as the likelihood function @xmath2 .",
    "experimental data from several brain areas responding to simple stimuli suggests that this variability often belongs to the exponential family of distributions with linear sufficient statistics @xcite : @xmath3 where @xmath4 depends on the stimulus - dependent mean and fluctuations of the neuronal response and @xmath5 is independent of the stimulus . for a conjugate prior @xmath6",
    ", the posterior distribution will also have this general form , @xmath7 .",
    "this neural code is known as a linear ppc : it is a probabilistic population code because the population activity collectively encodes the stimulus probability , and it is linear because the log - likelihood is linear in @xmath0 . in this paper , we assume responses are drawn from this family , although incorporation of more general ppcs with nonlinear sufficient statistics @xmath8 is possible : @xmath9 .",
    "an important property of linear ppcs , central to this work , is that different projections of the population activity encode the natural parameters of the underlying posterior distribution .",
    "for example , if the posterior distribution is gaussian ( figure [ fig : ppcfig ] ) , then @xmath10 , with @xmath11 and @xmath12 encoding the linear and quadratic natural parameters of the posterior .",
    "these projections are related to the expectation parameters , the mean and variance , by @xmath13 and @xmath14 .",
    "a second important property of linear ppcs is that the variance of the encoded distribution is inversely proportional to the overall amplitude of the neural activity .",
    "intuitively , this means that more spikes means more certainty ( figure [ fig : ppcfig ] ) .",
    "the most fundamental probabilistic operations are the product rule and the sum rule .",
    "linear ppcs can perform both of these operations while maintaining a consistent representation @xcite , a useful feature for constructing a model of canonical computation . for a log - linear probability code like linear ppcs",
    ", the product rule corresponds to weighted summation of neural activities : @xmath15 .",
    "in contrast , to use the sum rule to marginalize out variables , linear ppcs require nonlinear transformations of population activity . specifically , a quadratic nonlinearity with divisive normalization performs near - optimal marginalization in linear ppcs @xcite .",
    "quadratic interactions arise naturally through coincidence detection , and divisive normalization is a nonlinear inhibitory effect widely observed in neural circuits @xcite .",
    "alternatively , near - optimal marginalizations on ppcs can also be performed by more general nonlinear transformations @xcite . in sum ,",
    "ppcs provide a biologically compatible representation of probabilistic information .     and @xmath12 encode the natural parameters of the posterior",
    "( * b * ) corresponding posteriors over stimulus variables determined by the responses in panel a. the gain or overall amplitude of the population code is inversely proportional to the variance of the posterior distribution.,scaledwidth=70.0% ]",
    "to generalize ppcs , we need to represent the joint probability distribution of many variables .",
    "a natural way to represent multivariate distributions is with probabilistic graphical models . in this work ,",
    "we use the formalism of factor graphs , a type of bipartite graph in which nodes representing variables are connected to other nodes called factors representing interactions between ` cliques ' or sets of variables ( figure [ fig : trp2]a ) . the joint probability over all variables",
    "can then be represented as a product over cliques , @xmath16 , where @xmath17 are nonnegative compatibility functions on the set of variables @xmath18 in the clique , and @xmath19 is a normalization constant .",
    "the distribution of interest will be a posterior distribution @xmath20 that depends on neural responses @xmath0 .",
    "since the inference algorithm we present is unchanged with this conditioning , for notational convenience we suppress this dependence on @xmath0 .    in this paper , we focus on pairwise interactions , although our main framework generalizes naturally to richer , higher - order interactions . in a pairwise model ,",
    "we allow singleton factors @xmath21 for variable nodes @xmath22 in a set of vertices @xmath23 , and pairwise interaction factors @xmath24 for pairs @xmath25 in the set of edges @xmath26 that connect those vertices . the joint distribution is then @xmath27      the inference problem of interest in this work is to compute the marginal distribution for each variable , @xmath28 .",
    "this task is generally intractable .",
    "however , the factorization structure of the distribution can be used to perform inference efficiently , either exactly in the case of tree graphs , or approximately for graphs with cycles .",
    "one such algorithm is called belief propagation ( bp ) @xcite .",
    "bp iteratively passes information along the graph in the form of messages @xmath29 from node @xmath22 to @xmath30 , using only local computations that summarize the relevant aspects of other messages upstream in the graph : @xmath31 where @xmath32 is the time or iteration number , and @xmath33 is the set of neighbors of node @xmath22 on the graph .",
    "the estimated marginal , called the ` belief ' @xmath34 at a node @xmath22 , is proportional to the local evidence at that node @xmath35 and all the messages coming into node @xmath22 .",
    "similarly , the messages themselves are determined self - consistently by combining incoming messages  except for the previous message from the target node @xmath30 .",
    "this message exclusion is critical because it prevents evidence previously passed by the target node from being counted as if it were new evidence .",
    "this exclusion only prevents overcounting on a tree graph , and is unable to prevent overcounting of evidence passed around loops .",
    "for this reason , bp is exact for trees , but only approximate for general , loopy graphs .",
    "if we use this algorithm anyway , it is called ` loopy ' belief propagation ( lbp ) , and it often has quite good performance @xcite .",
    "multiple researchers have been intrigued by the possibility that the brain may perform lbp @xcite , since it gives `` a principled framework for propagating , in parallel , information and uncertainty between nodes in a network '' @xcite . despite the conceptual appeal of lbp , it is important to get certain details correct : in an inference algorithm described by nonlinear dynamics , deviations from ideal behavior could in principle lead to very different outcomes .",
    "one critically important detail is that each node must send different messages to different targets to prevent overcounting .",
    "this exclusion can render lbp neurally implausible , because neurons can not readily send different output signals to many different target neurons .",
    "some past work simply ignores the problem @xcite ; the resultant overcounting destroys much of the inferential power of lbp , often performing worse than more nave algorithms like mean - field inference .",
    "one better option is to use different readouts of population activity for different targets @xcite , but this approach is inefficient because it requires many readout populations for messages that differ only slightly , and requires separate optimization for each possible target .",
    "other efforts have avoided the problem entirely by performing only unidirectional inference of low - dimensional variables that evolve over time @xcite .",
    "appealingly , one can circumvent all of these difficulties by using an alternative formulation of lbp known as tree - based reparameterization ( trp ) .",
    "insightful work by wainwright , jakkola , and willsky @xcite revealed that belief propagation can be understood as a convenient way of refactorizing a joint probability distribution , according to approximations of local marginal probabilities .",
    "for pairwise interactions , this can be written as @xmath36 where @xmath37 is a so - called ` pseudomarginal ' distribution of @xmath38 and @xmath39 is a joint pseudomarginal over @xmath38 and @xmath40 ( figure [ fig : trp2]a  b ) , where @xmath41 and @xmath42 are the outcome of loopy belief propagation .",
    "the name pseudomarginal comes from the fact that these quantities are always locally consistent with being marginal distributions , but they are only globally consistent with the true marginals when the graphical model is tree - structured .    these pseudomarginals can be constructed iteratively as the true marginals of a different joint distribution @xmath43 on an isolated tree - structured subgraph @xmath44 .",
    "compatibility functions @xmath45 from factors remaining outside of the subgraph are collected in a residual term @xmath46 .",
    "this regrouping leaves the joint distribution unchanged : @xmath47 the factors of @xmath48 are then rearranged by computing the true marginals on its subgraph @xmath44 , again preserving the joint distribution . in subsequent updates ,",
    "we iteratively refactorize using the marginals of @xmath48 along different tree subgraphs @xmath44 ( figure [ fig : trp2]c ) .     on a tree graph .",
    "( * b * ) an alternative parameterization of the same distribution in terms of the marginals @xmath49 .",
    "( * c * ) two trp updates for a @xmath50 nearest - neighbor grid of variables.,scaledwidth=100.0% ]    typical lbp can be interpreted as a sequence of local reparameterizations over just two neighboring nodes and their corresponding edge @xcite .",
    "pseudomarginals are initialized at time @xmath51 using the original factors : @xmath52 and @xmath53 . at iteration @xmath54 ,",
    "the node and edge pseudomarginals are computed by exactly marginalizing the distribution built from previous pseudomarginals at iteration @xmath32 : @xmath55 notice that , unlike the original form of lbp , operations on graph neighborhoods @xmath56 do not differentiate between targets .",
    "trp s operation only requires updating pseudomarginals , in place , using local information .",
    "these are appealing properties for a candidate brain algorithm .",
    "this representation is also nicely compatible with the structure of ppcs : different projections of the neural activity encode the natural parameters of an exponential family distribution .",
    "it is thus useful to express the pseudomarginals and the trp inference algorithm using vectors of sufficient statistics @xmath57 and natural parameters @xmath58 for each clique : @xmath59 . for a model with at most pairwise interactions",
    ", the trp updates ( [ bptrp ] ) can be expressed in terms of these natural parameters as @xmath60 where @xmath61 is the number of neighbors of @xmath22 , and @xmath62 , @xmath63 and @xmath64 are matrices and nonlinear functions ( for vertices @xmath23 and edges @xmath26 ) that are determined by the particular graphical model ( see below ) .",
    "since the natural parameters reflect log - probabilities , the product rule for probabilities becomes a linear sum in @xmath65 , while the sum rule for probabilities must be implemented by nonlinear operations @xmath66 on @xmath65 .    in the concrete case of a gaussian graphical model ,",
    "the joint distribution is given by @xmath67 , where @xmath68 and @xmath69 are the natural parameters , and the linear and quadratic functions @xmath70 and @xmath71 are the sufficient statistics .",
    "when we reparameterize this distribution by pseudomarginals , we again have linear and quadratic sufficient statistics : two for each node , @xmath72 , and five for each edge , @xmath73 .",
    "each of these vectors of sufficient statistics has its own vector of natural parameters , @xmath74 and @xmath75 .    to approximate the marginal probabilities ,",
    "the trp algorithm initializes the pseudomarginals to @xmath76 and @xmath77 . to update @xmath65",
    ", we must extract the matrices @xmath78 and nonlinear functions @xmath66 that recover the univariate marginal distribution of a bivariate gaussian @xmath42 . for @xmath79 , this marginal",
    "is @xmath80 using this , we can determine the form of the weight matrices and the nonlinear functions in the trp updates ( [ paramupdates ] ) .",
    "@xmath81 where @xmath82 is the @xmath83 element of @xmath75 .",
    "notice that these nonlinear functions are all quadratic functions with a linear divisive normalization .",
    "an important feature of the trp updates is that they circumvent the ` message exclusion ' problem of lbp .",
    "the trp update for the singleton terms , ( [ bptrp ] ) and ( [ paramupdates ] ) , includes contributions from _ all the neighbors _ of a given node .",
    "there is no free lunch , however , and the price is that the updates at time @xmath54 depend on previous pseudomarginals at two different times , @xmath32 and @xmath54 .",
    "the latter update is therefore instantaneous information transmission , which is not biologically feasible .    to overcome this limitation ,",
    "we observe that the brain can use fast and slow timescales @xmath84 instead of instant and delayed signals .",
    "we convert the update equations to continuous time , and introduce auxiliary variables @xmath85 which are lowpass - filtered versions of @xmath65 on a slow timescale : @xmath86 .",
    "the nonlinear dynamics of ( [ paramupdates ] ) are then updated on a faster timescale @xmath87 according to @xmath88 where the nonlinear terms @xmath66 depend only on the slower , delayed activity @xmath85 . by concatenating these two sets of parameters , @xmath89 , we obtain a coupled multidimensional dynamical system which represents the approximation to the trp iterations : @xmath90 here the weight matrix @xmath91 and the nonlinear function @xmath92 inherit their structure from the discrete - time updates and the lowpass filtering at the fast and slow timescales .      to complete our neural inference network , we now embed the nonlinear dynamics ( [ dseq ] ) into the population activity @xmath0 . since different projections of the neural activity in a linear ppc encode natural parameters of the underlying distribution , we map neural activity onto @xmath93 by @xmath94 where @xmath95 is a rectangular @xmath96 embedding matrix that projects the natural parameters and their low - pass versions into the neural response space .",
    "these parameters can be decoded from the neural activity as @xmath97 , where @xmath98 is the pseudoinverse of @xmath95 .    applying this basis transformation to ( [ dseq ] ) , we have @xmath99 .",
    "we then obtain the general form of the updates for the neural activity @xmath100 where @xmath101 and @xmath102 correspond to the linear and nonlinear computational components that integrate and marginalize evidence , respectively .",
    "the nonlinear function on @xmath0 inherits the structure needed for the natural parameters , such as a quadratic polynomial with a divisive normalization used in low - dimensional gaussian marginalization problems @xcite , but now expanded to high - dimensional graphical models .",
    "figure [ nn ] depicts the network architecture for the simple graphical model from figure [ fig : trp2]a , both when there are distinct neural subpopulations for each factor ( figure [ nn]a ) , and when the variables are fully multiplexed across the entire neural population ( figure [ nn]b ) .",
    "these simple , biologically - plausible neural dynamics ( [ eq : neuraltrp ] ) represent a powerful , nonlinear , fully - recurrent network of ppcs which implements the trp update equations on an underlying graphical model .",
    "a. ( * b * ) a cartoon shows how the same distribution can be represented as distinct projections of the distributed neural activity , instead of as distinct populations . in both cases , since the neural activities encode log - probabilities , linear connections are responsible for integrating evidence while nonlinear connections perform marginalization.,scaledwidth=90.0% ]",
    "we evaluate the performance of our neural network on a set of small gaussian graphical models with up to 400 interacting variables .",
    "the networks time constants were set to have a ratio of @xmath103 .",
    "figure [ fig : dynamics ] shows the neural population dynamics as the network performs inference , along with the temporal evolution of the corresponding node and pairwise means and covariances .",
    "the neural activity exhibits a complicated timecourse , and reflects a combination of many natural parameters changing simultaneously during inference .",
    "this type of behavior is seen in neural activity recorded from behaving animals @xcite .",
    "a.,scaledwidth=80.0% ]    figure [ fig : performance ] shows that our recurrent neural network accurately infers the marginal probabilities , and reaches almost the same conclusions as loopy belief propagation .",
    "the data points are obtained from multiple simulations with different graph topologies , including graphs with many loops .",
    "figure [ fig : noiseperformance ] verifies that the network is robust to noise even when there are few neurons per inferred parameter ; adding more neurons improves performance since the noise can be averaged away .     and",
    "densely connected graphs with up to 25 variables .",
    "the expectation parameters ( means , covariances ) of the pseudomarginals closely match the corresponding parameters for the true marginals . ]",
    "square grid , in the presence of independent spatiotemporal gaussian noise of standard deviation 0.1 times the standard deviation of each signal .",
    "( * b * ) expectation parameters ( means , variances ) of the node pseudomarginals closely match the corresponding parameters for the true marginals , despite the noise .",
    "results are shown for one or five neurons per parameter in the graphical model , and for no noise ( i.e. infinitely many neurons).,scaledwidth=100.0% ]",
    "we have shown how a biologically - plausible nonlinear recurrent network of neurons can represent a multivariate probability distribution using population codes , and can perform inference by reparameterizing the joint distribution to obtain approximate marginal probabilities .",
    "our network model has desirable properties beyond those lauded features of belief propagation .",
    "first , it allows for a thoroughly distributed population code , with many neurons encoding each variable and many variables encoded by each neuron .",
    "this is consistent with neural recordings in which many task - relevant features are multiplexed across a neural population @xcite .",
    "second , the network performs inference in place , without using a distinct neural representation for messages , and avoids the biological implausibility associated with sending different messages about every variable to different targets .",
    "this virtue comes from exchanging multiple messages for multiple timescales .",
    "it is noteworthy that allowing two timescales prevents overcounting of evidence on loops of length two ( target to source to target ) .",
    "this suggests a novel role of memory in static inference problems : a longer memory could be used to discount past information sent at more distant times , thus avoiding the overcounting of evidence that arises from loops of length three and greater",
    ". it may therefore be possible to develop reparameterization algorithms with all the convenient properties of lbp but with improved performance on loopy graphs .",
    "previous results show that the quadratic nonlinearity with divisive normalization is convenient and biologically plausible interpretable , but this precise form is not necessary : other pointwise neuronal nonlinearities are capable of producing the same high - quality marginalizations in ppcs @xcite . in a distributed code ,",
    "the precise nonlinear form at the neuronal scale is not important as long as the effect on the parameters is the same .",
    "more generally , however , different nonlinear computations on the parameters implement different approximate inference algorithms .",
    "the distinct behaviors of such algorithms as mean - field inference , generalized belief propagation , and others arise from differences in their nonlinear transformations .",
    "even gibbs sampling can be described as a noisy nonlinear message - passing algorithm .",
    "although lbp and its generalizations have strong appeal , we doubt the brain will use this algorithm exactly",
    ". the real nonlinear functions in the brain may implement even smarter algorithms .    to identify the brain s algorithm",
    ", it may be more revealing to measure how information is represented and transformed in a low - dimensional latent space embedded in the high - dimensional neural responses than to examine each neuronal nonlinearity in isolation .",
    "the present work is directed toward this challenge of understanding computation in this latent space .",
    "it provides a concrete example showing how distributed nonlinear computation can be distinct from localized neural computations .",
    "learning this computation from data will be a key challenge for neuroscience . in",
    "future work we aim to recover the latent computations of our network from artificial neural recordings generated by the model .",
    "successful model recovery would encourage us to apply these methods to large - scale neural recordings to uncover key properties of the brain s distributed nonlinear computations .",
    "* acknowledgments : * xp and rr were supported by a grant from the mcnair foundation and by the intelligence advanced research projects activity ( iarpa ) via department of interior / interior business center ( doi / ibc ) contract number d16pc00003 ."
  ],
  "abstract_text": [
    "<S> behavioral experiments on humans and animals suggest that the brain performs probabilistic inference to interpret its environment . here </S>",
    "<S> we present a new general - purpose , biologically - plausible neural implementation of approximate inference . </S>",
    "<S> the neural network represents uncertainty using probabilistic population codes ( ppcs ) , which are distributed neural representations that naturally encode probability distributions , and support marginalization and evidence integration in a biologically - plausible manner . by connecting multiple ppcs together as a probabilistic graphical model </S>",
    "<S> , we represent multivariate probability distributions . </S>",
    "<S> approximate inference in graphical models can be accomplished by message - passing algorithms that disseminate local information throughout the graph . </S>",
    "<S> an attractive and often accurate example of such an algorithm is loopy belief propagation ( lbp ) , which uses local marginalization and evidence integration operations to perform approximate inference efficiently even for complex models . </S>",
    "<S> unfortunately , a subtle feature of lbp renders it neurally implausible . </S>",
    "<S> however , lbp can be elegantly reformulated as a sequence of tree - based reparameterizations ( trp ) of the graphical model . </S>",
    "<S> we re - express the trp updates as a nonlinear dynamical system with both fast and slow timescales , and show that this produces a neurally plausible solution . by combining all of these ideas , we show that a network of ppcs can represent multivariate probability distributions and implement the trp updates to perform probabilistic inference </S>",
    "<S> . simulations with gaussian graphical models demonstrate that the neural network inference quality is comparable to the direct evaluation of lbp and robust to noise , and thus provides a promising mechanism for general probabilistic inference in the population codes of the brain . </S>"
  ]
}