{
  "article_text": [
    "total variation ( tv ) denoising is a nonlinear filtering method based on the assumption that the underlying signal is piecewise constant ( equivalently , the derivative of the underlying signal is _ sparse _ ) @xcite .",
    "such signals arise in geoscience , biophysics , and other areas @xcite .",
    "the tv denoising technique is also used in conjunction with other methods in order to process more general types of signals @xcite .",
    "total variation denoising is prototypical of methods based on sparse signal models .",
    "it is defined by the minimization of a convex cost function comprising a quadratic data fidelity term and a non - differentiable convex penalty term .",
    "the penalty term is the composition of a linear operator and the @xmath0 norm .",
    "although the @xmath0 norm stands out as the convex penalty that most effectively induces sparsity @xcite , non - convex penalties can lead to more accurate estimation of the underlying signal @xcite .",
    "a few recent papers consider the prescription of non - convex penalties that maintain the convexity of the tv denoising cost function @xcite .",
    "( the motivation for this is to leverage the benefits of both non - convex penalization and convex optimization , e.g. , to accurately estimate the amplitude of jump discontinuities while guaranteeing the uniqueness of the solution . )",
    "the penalties considered in these works are separable ( additive ) .",
    "but non - separable penalties can outperform separable penalties in this context .",
    "this is because preserving the convexity of the cost function is a severely limiting requirement .",
    "non - separable penalties can more successfully meet this requirement because they are more general than separable penalties @xcite .",
    "this paper proposes a non - separable non - convex penalty for total variation denoising that generalizes the standard penalty and maintains the convexity of the cost function to be minimized .",
    "the new penalty , which is based on the moreau envelope , can more accurately estimate the amplitudes of jump discontinuities in an underlying piecewise constant signal .",
    "numerous non - convex penalties and algorithms have been proposed to outperform @xmath0-norm regularization for the estimation of sparse signals e.g. , @xcite .",
    "however , few of these methods maintain the convexity of the cost function .",
    "the prescription of non - convex penalties maintaining cost function convexity was pioneered by blake , zisserman , and nikolova @xcite , and further developed in refs .",
    "these works rely on the presence of both strongly and weakly convex terms , which is also exploited in @xcite .",
    "the proposed penalty is expressed as a differentiable convex function subtracted from the standard penalty ( i.e. , @xmath0 norm ) .",
    "previous works also use this idea @xcite .",
    "but the differentiable convex functions used therein are either separable @xcite or sums of bivariate functions @xcite .    in parallel with the submission of this paper",
    ", carlsson has also proposed using moreau envelopes to prescribe non - trivial convex cost functions @xcite . while the approach in @xcite starts with a given non - convex cost function ( e.g. , with the @xmath1 pseudo - norm penalty ) and",
    "seeks the convex envelope , our approach starts with the @xmath0-norm penalty and seeks a class of convexity - preserving penalties .",
    "some forms of generalized tv are based on infimal convolution ( related to the moreau envelope ) @xcite .",
    "but these works propose convex penalties suitable for non - piecewise - constant signals , while we propose non - convex penalties suitable for piecewise - constant signals .",
    "given @xmath2 and @xmath3 , total variation denoising is defined as @xmath4 where @xmath5 is the @xmath6 matrix @xmath7    as indicated in , tv denoising is the proximity operator @xcite of the function @xmath8 .",
    "it is convenient that tv denoising can be calculated exactly in finite - time @xcite .",
    "before we define the non - differentiable non - convex penalty in sec .  [ sec : pen ] , we first define a differentiable convex function .",
    "we use the moreau envelope from convex analysis @xcite .",
    "let @xmath9 .",
    "we define @xmath10 as @xmath11 where @xmath5 is the first - order difference matrix .    if @xmath12 , then @xmath13 is the _ moreau envelope _ of index @xmath14 of the function @xmath15 .",
    "[ prop : calcs ] the function @xmath13 can be calculated by @xmath16 @xmath17    for @xmath18 : setting @xmath18 and @xmath19 in gives . for @xmath12 : by the definition of tv denoising , the @xmath20 minimizing the function in is the tv denoising of @xmath21 , i.e. , @xmath22 .",
    "let @xmath9 .",
    "the function @xmath13 satisfies @xmath23    from , we have @xmath24 for all @xmath20 . in particular , @xmath25 leads to @xmath26 .",
    "also , @xmath27 since @xmath28 is defined as the minimum of a non - negative function .",
    "let @xmath9 .",
    "the function @xmath13 is convex and differentiable .",
    "it follows from proposition 12.15 in ref .",
    "@xcite .",
    "[ prop : sgrad ] let @xmath9 .",
    "the gradient of @xmath13 is given by @xmath29 where @xmath30 denotes total variation denoising .",
    "since @xmath13 is the moreau envelope of index @xmath14 of the function @xmath15 when @xmath12 , it follows by proposition 12.29 in ref .",
    "@xcite that @xmath31 this proximity operator is tv denoising , giving .",
    "to strongly induce sparsity of @xmath32 , we define a non - convex generalization of the standard tv penalty . the new penalty is defined by subtracting a differentiable convex function from the standard penalty .",
    "let @xmath9 .",
    "we define the penalty @xmath33 as @xmath34 where @xmath5 is the matrix and @xmath13 is defined by .",
    "the proposed penalty is upper bounded by the standard tv penalty , which is recovered as a special case .",
    "let @xmath9 .",
    "the penalty @xmath35 satisfies @xmath36 and @xmath37    it follows from and .",
    "when a convex function is subtracted from another convex function [ as in ] , the resulting function may well be negative on part of its domain .",
    "inequality states that the proposed penalty @xmath35 avoids this fate .",
    "this is relevant because the penalty function should be non - negative .",
    "figures in the supplemental material show examples of the proposed penalty @xmath35 and the function @xmath13 .",
    "we define ` moreau - enhanced ' tv denoising .",
    "if @xmath12 , then the proposed penalty penalizes large amplitude values of @xmath32 less than the @xmath0 norm does ( i.e. , @xmath38 ) , hence it is less likely to underestimate jump discontinuities .    given @xmath2 , @xmath3 , and @xmath9 , we define moreau - enhanced total variation denoising as @xmath39 where @xmath35 is given by .",
    "the parameter @xmath40 controls the non - convexity of the penalty . if @xmath18 , then the penalty is convex and moreau - enhanced tv denoising reduces to tv denoising .",
    "greater values of @xmath40 make the penalty more non - convex .",
    "what is the greatest value of @xmath40 that maintains convexity of the cost function ?",
    "the critical value is given by theorem [ thm : cond ] .",
    "[ thm : cond ] let @xmath3 and @xmath9 .",
    "define @xmath41 as @xmath42 where @xmath35 is given by . if @xmath43 then @xmath44 is convex . if @xmath45 then @xmath44 is strongly convex .",
    "we write the cost function as @xmath46 @xmath47 where @xmath48 is affine in @xmath21 .",
    "the last term is convex as it is the point - wise maximum of a set of convex functions .",
    "hence , @xmath44 is a convex function if @xmath49 .",
    "if @xmath50 , then @xmath44 is strongly convex ( and strictly convex ) .",
    "let @xmath2 , @xmath3 , and @xmath51 .",
    "then @xmath52 produced by the iteration    [ eq : alg ] @xmath53    converges to the solution of the moreau - enhanced tv denoising problem .",
    "if the cost function is strongly convex , then the minimizer can be calculated using the forward - backward splitting ( fbs ) algorithm @xcite .",
    "this algorithm minimizes a function of the form @xmath54 where both @xmath55 and @xmath56 are convex and @xmath57 is additionally lipschitz continuous .",
    "the fbs algorithm is given by    [ eq : fbs ] @xmath58      \\\\      x { ^{(k+1 ) } } & = \\arg \\min_x \\big\\ { { \\tfrac{1}{2}}\\norm { z { ^{(k ) } } - x } _ 2 ^ 2 + \\mu f_2 ( x ) \\big\\}\\end{aligned}\\ ] ]    where @xmath59 and @xmath60 is the lipschitz constant of @xmath57 . the iterates @xmath52 converge to a minimizer of @xmath61 .    to apply the fbs algorithm to the proposed cost function ,",
    "we write it as @xmath62 where    [ eq : deff12 ] @xmath63    the gradient of @xmath55 is given by @xmath64 using proposition [ prop : sgrad ] .",
    "subtracting @xmath13 from @xmath55 does not increase the lipschitz constant of @xmath57 , the value of which is 1 .",
    "hence , we may set @xmath65 .",
    "using , the fbs algorithm becomes    @xmath66      \\\\",
    "\\label{eq : xupdate }      x { ^{(k+1 ) } } & = \\arg \\min_x \\big\\ { { \\tfrac{1}{2}}\\norm { z { ^{(k ) } } - x } _ 2 ^ 2 + \\mu { { \\lambda } } \\norm { d x } _ 1 \\big\\}.     \\end{aligned}\\ ] ]    @xmath67      \\\\      \\label{eq : xupdate }      x { ^{(k+1 ) } } & = \\arg \\min_x \\big\\ { { \\tfrac{1}{2}}\\norm { z { ^{(k ) } } - x } _ 2 ^ 2 + \\mu { { \\lambda } } \\norm { d x } _ 1 \\big\\}.     \\end{aligned}\\ ] ]    note that is tv denoising . using the value @xmath68",
    "gives iteration .",
    "( experimentally , we found this value yields fast convergence . )    each iteration of entails solving two standard tv denoising problems . in this work , we calculate tv denoising using the fast exact c language program by condat @xcite . like the iterative shrinkage / thresholding algorithm ( ista ) @xcite , algorithm can be accelerated in various ways .",
    "we suggest not setting @xmath40 too close to the critical value @xmath69 because the fbs algorithm generally converges faster when the cost function is more strongly convex ( @xmath70 ) .",
    "in summary , the proposed moreau - enhanced tv denoising method comprises the steps :    1 .",
    "set the regularization parameter @xmath71 ( @xmath3 ) .",
    "2 .   set the non - convexity parameter @xmath40 ( @xmath45 ) .",
    "3 .   initialize @xmath72 .",
    "4 .   run iteration until convergence .",
    "to avoid terminating the iterative algorithm too early , it is useful to verify convergence using an optimality condition .",
    "[ prop : opt ] let @xmath2 , @xmath3 , and @xmath51 . if @xmath21 is a solution to , then @xmath73_n       \\in      \\operatorname{sign } ( [ d x ] _ n ) \\ ] ] for @xmath74 , where @xmath75 is given by @xmath76_n = \\sum _ { m { \\leqslant}n } x_m\\ ] ] and @xmath77 is the set - valued signum function @xmath78 , & t = 0          \\\\          \\ { 1 \\ } , & t > 0 .",
    "\\end{cases}\\ ] ]    according to , if @xmath79 is a minimizer , then the points @xmath80_n , u_n ) \\in { \\mathbb{r}}^2 $ ] must lie on the graph of the signum function , where @xmath81 denotes the value on the left - hand side of .",
    "hence , the optimality condition can be depicted as a scatter plot .",
    "figures in the supplemental material show how the points in the scatter plot converge to the signum function as the algorithm progresses .",
    "a vector @xmath21 minimizes a convex function @xmath61 if @xmath82 where @xmath83 is the subdifferential of @xmath61 at @xmath21 .",
    "the subdifferential of the cost function is given by @xmath84 which can be written as @xmath85_n ) , \\ , u \\in { \\mathbb{r}}^{n-1 } \\}.",
    "\\end{gathered}\\ ] ] @xmath86_n ) , \\ , u \\in { \\mathbb{r}}^{n-1 } \\}.\\ ] ] hence , the condition @xmath87 can be written as @xmath88_n ) , \\ , u \\in { \\mathbb{r}}^{n-1 }   \\}.",
    "\\end{gathered}\\ ] ] @xmath89_n ) , \\ , u \\in { \\mathbb{r}}^{n-1 }   \\}.\\ ] ]    let @xmath90 be a matrix of size @xmath6 such that @xmath91 , e.g. , .",
    "it follows that the condition @xmath87 implies that @xmath92_n       \\in      \\operatorname{sign } ( [ d x]_n ) \\ ] ] for @xmath74 .",
    "using proposition [ prop : sgrad ] gives .",
    "this example applies tv denoising to the noisy piecewise constant signal shown in fig .",
    "[ fig : example1](a ) .",
    "this is the ` blocks ' signal ( length @xmath93 ) generated by the wavelab @xcite function ` makesignal ` with additive white gaussian noise ( @xmath94 ) .",
    "we set the regularization parameter to @xmath95 following a discussion in ref .",
    "@xcite . for moreau - enhanced tv denoising",
    ", we set the non - convexity parameter to @xmath96 .",
    "figure  [ fig : example1 ] shows the result of tv denoising with three different penalties . in each case , a _",
    "convex _ cost function is minimized .",
    "figure  [ fig : example1](b ) shows the result using standard tv denoising ( i.e. , using the @xmath0-norm ) .",
    "this denoised signal consistently underestimates the amplitudes of jump discontinuities , especially those occurring near other jump discontinuities of opposite sign .",
    "figure  [ fig : example1](c ) shows the result using a separable non - convex penalty @xcite .",
    "this method can use any non - convex scalar penalty satisfying a prescribed set of properties . here",
    "we use the minimax - concave ( mc ) penalty  @xcite with non - convexity parameter set to maintain cost function convexity .",
    "this result significantly improves the root - mean - square error ( rmse ) and mean - absolute - deviation ( mae ) , but still underestimates the amplitudes of jump discontinuities .",
    "moreau - enhanced tv denoising , shown in fig .",
    "[ fig : example1](d ) , further reduces the rmse and mae and more accurately estimates the amplitudes of jump discontinuities .",
    "the proposed non - separable non - convex penalty avoids the consistent underestimation of discontinuities seen in figs .",
    "[ fig : example1](b ) and [ fig : example1](c ) .    to further compare the denoising capability of the considered penalties , we calculate the average rmse as a function of the noise level .",
    "we let the noise standard deviation span the interval @xmath97 .",
    "for each @xmath98 value , we calculate the average rmse of 100 noise realizations .",
    "figure  [ fig : rmse ] shows that the proposed penalty yields the lowest average rmse for all @xmath99 .",
    "however , at low noise levels , separable convexity - preserving penalties @xcite perform better than the proposed non - separable convexity - preserving penalty .",
    "this paper demonstrates the use of the moreau envelope to define a non - separable non - convex tv denoising penalty that maintains the convexity of the tv denoising cost function .",
    "the basic idea is to subtract from a convex penalty its moreau envelope .",
    "this idea should also be useful for other problems , e.g. , analysis tight - frame denoising @xcite .",
    "separable convexity - preserving penalties @xcite outperformed the proposed one at low noise levels in the example .",
    "it is yet to be determined if a more general class of convexity - preserving penalties can outperform both across all noise levels .",
    "10    f.  astrom and c.  schnorr . on coupled regularization for non - convex variational image enhancement .",
    "in _ iapr asian conf . on pattern recognition ( acpr ) _ , pages 786790 , november 2015 .",
    "h.  h. bauschke and p.  l. combettes . .",
    "springer , 2011 .",
    "i. bayram .",
    "penalty functions derived from monotone mappings . , 22(3):265269 , march 2015 .",
    "i.  bayram . on the convergence of the iterative shrinkage",
    "/ thresholding algorithm with a weakly convex penalty . ,",
    "64(6):15971608 , march 2016 .",
    "s.  becker and p.  l. combettes .",
    "an algorithm for splitting parallel sums of linearly composed monotone operators , with applications to signal recovery .",
    ", 15(1):137159 , 2014 .",
    "a.  blake and a.  zisserman . .",
    "mit press , 1987 .",
    "m.  burger , k.  papafitsoros , e.  papoutsellis , and c .- b .",
    "infimal convolution regularisation functionals of bv and lp spaces . , 55(3):343369 , 2016 .",
    "e.  j. cands , m.  b. wakin , and s.  boyd . enhancing sparsity by reweighted l1 minimization . , 14(5):877905 ,",
    "december 2008 .",
    "m.  carlsson .",
    "on convexification / optimization of functionals including an l2-misfit term .",
    "september 2016 .",
    "m.  castella and j .- c .",
    "optimization of a geman - mcclure like criterion for sparse signal deconvolution . in _ ieee int .",
    "workshop comput . adv .",
    "multi - sensor adaptive proc .",
    "_ , pages 309312 , december 2015",
    ".    a.  chambolle and p .- l . lions . image recovery via total variation minimization and related problems .",
    "76:167188 , 1997 .    r.  chartrand .",
    "shrinkage mappings and their induced penalty functions . in _ proc .",
    "ieee int .",
    "speech , signal processing ( icassp ) _ , pages 10261029 , may 2014 .    l.  chen and y.  gu . the convergence guarantees of a non - convex approach for sparse recovery .",
    ", 62(15):37543767 , august 2014 .",
    "p .- y . chen and i.  w. selesnick .",
    "group - sparse signal denoising : non - convex regularization , convex optimization . ,",
    "62(13):34643478 , july 2014 .",
    "e.  chouzenoux , a.  jezierska , j.  pesquet , and h.  talbot .",
    "a majorize - minimize subspace approach for @xmath100 image regularization .",
    ", 6(1):563591 , 2013 .",
    "p.  l. combettes and j .- c .",
    "proximal splitting methods in signal processing . in h.",
    "h. bauschke et  al . ,",
    "editors , _ fixed - point algorithms for inverse problems in science and engineering _ , pages 185212 .",
    "springer - verlag , 2011 .",
    "l.  condat . a direct algorithm for 1-d total variation denoising .",
    ", 20(11):10541057 , november 2013 .",
    "j.  darbon and m.  sigelle .",
    "image restoration with discrete constrained total variation part i : fast and exact optimization .",
    ", 26(3):261276 , 2006 .",
    "i.  daubechies , m.  defrise , and c.  de mol . an iterative thresholding algorithm for linear inverse problems with a sparsity constraint . , 57(11):14131457 , 2004 .",
    "y.  ding and i.  w. selesnick .",
    "artifact - free wavelet denoising : non - convex sparse regularization , convex optimization .",
    ", 22(9):13641368 , september 2015 .",
    "d.  donoho , a.  maleki , and m.  shahram .",
    "wavelab 850 , 2005 . ` http://www-stat.stanford.edu/%7ewavelab/ ` .",
    "l.  dmbgen and a.  kovac .",
    "extensions of smoothing via taut strings . , 3:4175 , 2009 .",
    "s.  durand and j.  froment .",
    "reconstruction of wavelet coefficients using total variation minimization .",
    ", 24(5):17541767 , 2003 .",
    "g.  r. easley , d.  labate , and f.  colonna .",
    "shearlet - based total variation diffusion for denoising .",
    ", 18(2):260268 , february 2009 .",
    "m.  figueiredo and r.  nowak .",
    "an em algorithm for wavelet - based image restoration .",
    ", 12(8):906916 , august 2003 .",
    "a.  gholami and s.  m. hosseini . a balanced combination of tikhonov and total variation regularizations for reconstruction of piecewise - smooth signals .",
    ", 93(7):19451960 , 2013 .",
    "t.  hastie , r.  tibshirani , and m.  wainwright . .",
    "crc press , 2015 .",
    "w.  he , y.  ding , y.  zi , and i.  w. selesnick .",
    "sparsity - based algorithm for detecting faults in rotating machines .",
    ", 72 - 73:4664 , may 2016 .",
    "n.  a. johnson . a dynamic programming algorithm for the fused lasso and @xmath101-segmentation .",
    ", 22(2):246260 , 2013 .",
    "a.  lanza , s.  morigi , and f.  sgallari .",
    "convex image denoising via non - convex regularization with parameter selection . ,",
    "pages 126 , 2016 .",
    "m.  a. little and n.  s. jones .",
    "generalized methods and solvers for noise removal from piecewise constant signals : part i  background theory . , 467:30883114 , 2011 .",
    "m.  malek - mohammadi , c.  r. rojas , and b.  wahlberg . a class of nonconvex penalties preserving overall convexity in optimization - based mean filtering . , 64(24):66506664 ,",
    "december 2016 .",
    "y.  marnissi , a.  benazza - benyahia , e.  chouzenoux , and j .- c .",
    "generalized multivariate exponential power prior for wavelet - based multichannel image restoration . in _ proc .",
    "ieee int .",
    "image processing ( icip ) _ , pages 24022406 , september 2013 .",
    "h.  mohimani , m.  babaie - zadeh , and c.  jutten . a fast approach for overcomplete sparse decomposition based on smoothed l0 norm . , 57(1):289301 , january 2009 .",
    "t.  mllenhoff , e.  strekalovskiy , m.  moeller , and d.  cremers .",
    "the primal - dual hybrid gradient method for semiconvex splittings .",
    "8(2):827857 , 2015 .    m.  nikolova .",
    "estimation of binary images by minimizing convex criteria . in _ proc",
    ".  ieee int .",
    "image processing ( icip ) _ , pages 108112 vol . 2 , 1998 .",
    "m.  nikolova",
    ". local strong homogeneity of a regularized estimator .",
    ", 61(2):633658 , 2000 .",
    "m.  nikolova .",
    "analysis of the recovery of edges in images and signals by minimizing nonconvex regularized least - squares .",
    ", 4(3):960991 , 2005 .",
    "m.  nikolova .",
    "energy minimization methods .",
    "in o.  scherzer , editor , _ handbook of mathematical methods in imaging _ , chapter  5 , pages 138186 .",
    "springer , 2011 .",
    "m.  nikolova , m.  k. ng , and c .- p . tam .",
    "fast nonconvex nonsmooth minimization methods for image restoration and reconstruction . , 19(12):30733088 , december 2010 .",
    "a.  parekh and i.  w. selesnick .",
    "convex denoising using non - convex tight frame regularization .",
    ", 22(10):17861790 , october 2015 .",
    "a.  parekh and i.  w. selesnick .",
    "enhanced low - rank matrix approximation . , 23(4):493497 , april 2016 .",
    "j.  portilla and l.  mancera . -based sparse approximation : two alternative methods and some applications . in _ proceedings of spie",
    "_ , volume 6701 ( wavelets xii ) , san diego , ca , usa , 2007 .    p.  rodriguez and b.  wohlberg .",
    "efficient minimization method for a generalized total variation functional . , 18(2):322332 , february 2009 .",
    "l.  rudin , s.  osher , and e.  fatemi .",
    "nonlinear total variation based noise removal algorithms .",
    ", 60:259268 , 1992 .",
    "i.  w. selesnick and i.  bayram .",
    "sparse signal estimation by maximally sparse convex optimization . ,",
    "62(5):10781092 , march 2014 .",
    "i.  w. selesnick and i.  bayram . enhanced sparsity by non - separable regularization . , 64(9):22982313 , may 2016 .",
    "i.  w. selesnick , a.  parekh , and i.  bayram .",
    "convex 1-d total variation denoising with non - convex regularization .",
    ", 22(2):141144 , february 2015 .",
    "s.  setzer , g.  steidl , and t.  teuber .",
    "infimal convolution regularizations with discrete l1-type functionals .",
    ", 9(3):797827 , 2011 .",
    "m.  storath , a.  weinmann , and l.  demaret .",
    "jump - sparse and sparse recovery using potts functionals . , 62(14):36543666 , july 2014 .",
    "d.  p. wipf , b.  d. rao , and s.  nagarajan .",
    "latent variable bayesian models for promoting sparsity .",
    ", 57(9):62366255 ,",
    "september 2011 .",
    "nearly unbiased variable selection under minimax concave penalty .",
    ", pages 894942 , 2010 .",
    "h.  zou and r.  li .",
    "one - step sparse estimates in nonconcave penalized likelihood models .",
    ", 36(4):15091533 , 2008 .",
    "to gain intuition about the proposed penalty function and how it induces sparsity of @xmath32 while maintaining convexity of the cost function , a few illustrations are useful .",
    "figure  [ fig : pen1 ] illustrates the proposed penalty , its sparsity - inducing behavior , and its relationship to the differentiable convex function @xmath13 .",
    "figure  [ fig : pen2 ] illustrates how the proposed penalty is able to maintain the convexity of the cost function .",
    "figure  [ fig : pen1 ] shows the proposed penalty @xmath35 defined in for @xmath102 and @xmath103 .",
    "it can be seen that the penalty approximates the standard tv penalty @xmath104 for signals @xmath21 for which @xmath32 is approximately zero .",
    "but it increases more slowly than the standard tv penalty as @xmath105 . in that sense",
    ", it penalizes large values less than the standard tv penalty .    as shown in fig .",
    "[ fig : pen1 ] , the proposed penalty is expressed as the standard tv penalty minus the differentiable convex non - negative function @xmath13 .",
    "since @xmath13 is flat around the null space of @xmath5 , the penalty @xmath35 approximates the standard tv penalty around the null space of @xmath5 .",
    "in addition , since @xmath13 is non - negative , the penalty @xmath35 lies below the standard tv penalty .",
    "figure  [ fig : pen2 ] shows the differentiable part of the cost function @xmath44 for @xmath102 , @xmath106 , and @xmath103 .",
    "the differentiable part is given by @xmath55 in .",
    "the total cost function is obtained by adding the standard tv penalty to @xmath55 , see .",
    "hence , @xmath44 is convex if the differentiable part @xmath55 is convex . as can be seen in fig .",
    "[ fig : pen2 ] , the function @xmath55 is convex .",
    "we note that the function @xmath55 in this figure is not strongly convex .",
    "this is because we have used @xmath107 .",
    "if @xmath108 , then the function @xmath55 will be strongly convex ( and hence @xmath44 will also be strongly convex and have a unique minimizer ) .",
    "we recommend @xmath108 .",
    "figure  [ fig : pen3 ] shows the differentiable part @xmath55 of the cost function @xmath44 for @xmath109 , @xmath106 , and @xmath103 . here",
    ", the function @xmath55 is non - convex because @xmath110 which violates the convexity condition .    in order to simplify the illustration ,",
    "we have set @xmath111 in fig .",
    "[ fig : pen2 ] . in practice @xmath112 .",
    "but the only difference between the cases @xmath111 and @xmath112 is an additive affine function which does not alter the convexity properties of the function .    in practice",
    "we are interested in the case @xmath113 , i.e. , signals much longer than two samples . however , in order to illustrate the functions , we are limited to the case of @xmath103 .",
    "we note that the case of @xmath103 does not fully illustrate the behavior of the proposed penalty .",
    "in particular , when @xmath103 the penalty is simply a linear transformation of a scalar function , which does not convey the non - separable behavior of the penalty for @xmath114 .",
    "a separate document has additional supplemental figures illustrating the convergence of the iterative algorithm .",
    "these figures show the optimality condition as a scatter plot .",
    "the points in the scatter plot converge to the signum function as the algorithm converges ."
  ],
  "abstract_text": [
    "<S> total variation denoising is a nonlinear filtering method well suited for the estimation of piecewise - constant signals observed in additive white gaussian noise . </S>",
    "<S> the method is defined by the minimization of a particular non - differentiable convex cost function . </S>",
    "<S> this paper describes a generalization of this cost function that can yield more accurate estimation of piecewise constant signals . </S>",
    "<S> the new cost function involves a non - convex penalty ( regularizer ) designed to maintain the convexity of the cost function . </S>",
    "<S> the new penalty is based on the moreau envelope . </S>",
    "<S> the proposed total variation denoising method can be implemented using forward - backward splitting . </S>"
  ]
}