{
  "article_text": [
    "machine learning usually assumes that the training data are from the same domain @xcite .",
    "recently , transfer learning has been a well - studied topic in machine learning community .",
    "it refers to the problem of learning a machine learning modeling for a target domain with help from a source domain @xcite . for a machine learning problem ,",
    "a source domain is a domain with sufficient training data , which are well labeled , and thus it is easy to train models in this domain .",
    "a target domain is usually in lack of training data , or in lack of labels . because of the lack of data and/or label",
    ", the training in the target domain is difficult , and has the problem of over - fitting .",
    "the source domain and target domains share the same feature space and label space , thus it would be very helpful to use the source domain data to help the learning problem in the target domain",
    ". however , the data distributions of the source and target domains are usually very different , if we simply use the models trained by using the source domain training set , the prediction performance over the target domain can be very inferior .",
    "the reason for this is that the difference between the two different domain distributions can even be more significant than the difference between different classes . to solve this problem",
    ", we need to transfer the source domain model to the target domain by adapting the source domain predictor to the target domain data set .",
    "this is called domain transfer learning .",
    "given a source domain predictor , its train a target domain predictor by adapting it to the target domain over the target domain training set , even when the target domain training set itself is insufficient to train a new target domain predictor .",
    "one example of domain transfer learning is spoken arabic digit recognition @xcite . in a spoken arabic digit data set",
    ", there are two domains of data , which are male voice signal , and female voice signal . for each domain , there are voice data of ten classes , and each class is a one digit , varying from one to ten .",
    "although the voice signal of both domains shares the same feature space and class label space , however , the classifier trained with male domain data can not be used directly in the female voice signal domain , due to the significant difference between male and female voice signal domains .",
    "thus an adaptation is necessary to transfer the classifier for the male domain classifier to the female domain .",
    "another example of transfer learning is spam detection problem @xcite . for one email user , we can train a spam detector from the well labeled spam and normal emails . however",
    ", it can not be used to detect the spams of another user because the emails of these two users may are significantly different from each other .",
    "thus to use the first user s detector to the second user s email , we need to perform an adaptation to the detector .    up to now",
    ", most of the domain transfer learning methods are focused on single instance data @xcite .",
    "that is , one data point is only presented as one single instance .",
    "however , in many machine learning applications , one data point can contains multiple instances , and is presented as a bag of instances .",
    "this type of data is called multi - instance data . in domain transfer learning problem , many applications require to learn domain transfer classifier for multi - instance data . however , only a few works are done to this direction , while most existing works ignore the nature of multi - instance data and only treat them as one single feature vector . to overcome this problem ,",
    "two methods have been proposed for domain transfer learning with multi - instance data .    * zhang and si @xcite proposed the problem of domain transfer learning for multi - instance data , and a novel method to solve it within the framework of multi - task learning .",
    "more than one source domains are considered and each domain is considered as a task .",
    "the target domain classifier is obtained by a weighted linear combination of the classifiers of the multiple tasks . *",
    "wang et al .",
    "@xcite proposed an adaptive knowledge transfer learning framework for multi - instance data .",
    "this method adapts the source - target domain cross class knowledge to the target domain for the multi - instance data to boost the learning . it also build a data - dependent mixture model to combine the knowledge from the source domain to enhance the target domain weak classifier",
    "the objective of this model is optimized by an iterative coordinate descent method as a constraint concave - convex programming problem .",
    "it has been shown that the most effective way for multi - instance learning method is using multi - instance dictionary to map a bag of instances to a bag - level feature vector @xcite .",
    "this representation method is based on bag - instance similarity , and the critical component of this method is the learning of the dictionary . in this paper ,",
    "we invest the problem of domain transfer multi - instance learning problem and propose a novel solution based on the dictionary . according to our knowledge , this is the first work toward the direction of learning multi - instance dictionary in the scene of domain transfer learning .",
    "we assume that in the source domain , we have a well - trained multi - instance dictionary and a corresponding classifier . using this dictionary",
    ", we can map a bag of instances to a bag - level feature by using the bag - instance similarity , and then in the bag - level feature space , we can apply the classifier to classify the bag .",
    "however , the source domain dictionary and classifier can not be directly applied to the target domain , we propose to add an adaptive term to the source domain classifier to construct the target domain classifier to transfer the knowledge of the source domain to the target domain . to construct the domain adaptive term",
    ", we propose to learn a domain transfer dictionary to represent a target domain bag to the bag - level feature space , and the adaptive term is designed to be a linear function in this space .",
    "the problem is transferred to the problem of learning the domain transfer dictionary and adaptive function parameter . to this end , we propose to optimize both the mentioned parameters by learn from a limited target domain training set .",
    "we propose to minimize the hinge loss of the target domain training bags , the complexity of the adaptive function , and the complexity of the domain transfer dictionary . to solve the minimization problem , we develop an iterative algorithm .",
    "the contributions of this paper are listed as follows .    * * contribution # 1*. we propose a novel problem of transfer domain dictionary learning .",
    "this problem is beyond simple domain transfer learning or dictionary learning .",
    "it is proposed to explore the cross - domain knowledge in the aspect of dictionary learning .",
    "its significance is to learn the critical dictionary which can bridge the source and target domains .",
    "it only learns a good predictor in the target domain with help of source domain , but also reveals the nature of the multi - instance data which connects the to domains .",
    "* * contribution # 2*. we build a novel learning model to solve this problem .",
    "this model based on a joint optimization problem of both the domain transfer dictionary and the domain adaptation function .",
    "the objective is composed of three terms .",
    "the first term is the average classification error over the target domain training bags , measured by the hinge loss , the second term is the complexity term of the adaptation function , and the last term is the complexity term of the domain transfer dictionary . in this way ,",
    "our problem is modeled as a minimization problem . * * contribution # 3*. we also develop an effective algorithm to solve the problem proposed in * contribution # 2*. we use the lagrange multiplier method and an alternate optimization strategy .",
    "the optimization of the adaptation function parameter is transferred to the optimization of some lagrange multipliers , and the optimization of domain transfer dictionary and the lagrange multipliers are conducted alternately .",
    "the optimization of lagrange multipliers is performed as a quadratic programming problem , while the domain transfer dictionary is updated by gradient descent algorithm .",
    "this paper is organized in the following way . in section",
    "[ sec : method ] , we introduce the proposed domain transfer dictionary learning method , in section [ sec : experiment ] , we give the experimental results of the proposed method over several benchmark data sets , and finally in section [ sec : conclusion ] , we conclude the paper with some future works .",
    "in this section , we introduce the newly proposed multi - instance dictionary learning method for domain transfer learning problem . in section [ sec : objective ] , we model the learning problem as a minimization problem , and the problem is solved in section [ sec : optimization ] .",
    "furthermore , in section [ sec : algorithm ] , we propose an iterative algorithm to implement the solution of the problem .      in the problem of domain transfer multi - instance learning ,",
    "we suppose we have two domains , which are a source domain and a target domain . for the source domain",
    ", we have a training set , and we have learned a well - trained multi - instance dictionary , @xmath0 , where @xmath1 is the vector of the @xmath2-th word of the dictionary , and @xmath3 is the number of the words in this dictionary . with this dictionary , we can represent a bag of instances as a feature vector of @xmath3 dimensions .",
    "suppose we have a bag denoted as @xmath4 , where @xmath5 is @xmath6-dimensional feature vector the @xmath7-th instance of the bag , and @xmath8 is the number of the instances of this bag , we can represent as    @xmath9^\\top \\in \\mathbb{r}^\\iota \\end{aligned}\\ ] ]    where the @xmath2-th dimension is the maximum dot - produce between the @xmath2-th codeword and the instances of @xmath10 .",
    "the motive to use the maximum dot - product to measure the similarly between an instance and a bag is it is simple and parameter - free .",
    "some other similarity measures such as gaussian kernel requires additional parameters such as the band - width parameter .",
    "the tuning of these parameters is time - consuming and has the problem of over - fitting .",
    "the chosen similarity maximum dot - product does not have such problems because it is parameter - free . with this bag - level feature vector",
    ", we can classify the bags in the bag - level space . to this end",
    ", we also have a well - trained classifier , @xmath11 , which is learned from the source domain , to map the bag @xmath10 to its true binary class label , @xmath12 , from its bag - level features ,    @xmath13    where @xmath14 is the parameter of the source domain classifier .",
    "the source domain dictionary , @xmath15 , and its corresponding bag - level classifier , @xmath16 are trained over the source domain , and are supposed work well in the source domain .",
    "moreover , we have a target domain , and we also want to present and classify the bags of the target domain .",
    "one direct method is to apply the source domain dictionary and classifier to the target domain data .",
    "however , this is not suitable due to the significant difference between the distributions of the source domain and target domain .",
    "we propose to design the target domain multi - instance classifier , @xmath17 , by adapting the source domain dictionary classifier , @xmath18 , to the target domain .",
    "more specifically , the target domain classifier is the combination of the source domain classifier and an adaptation term , @xmath19 ,    @xmath20    where @xmath21 and @xmath22 are parameters of the adaptation term which will be specified as follows .",
    "@xmath21 is the domain transfer multi - instance dictionary , and it contains @xmath23 codewords , @xmath24 , where @xmath25 is its @xmath2-th codeword .",
    "we use it to represent a bag @xmath10 to a @xmath23-dimensional bag - level feature vector ,    @xmath26^\\top \\in \\mathbb{r}^\\kappa . \\end{aligned}\\ ] ]    the adaptation term is a linear function in this bag - level feature space constructed by the domain transfer dictionary , @xmath21 ,    @xmath27    @xmath28 \\in \\mathbb{r}^\\kappa$ ] is the @xmath23-dimensional adaptation function parameter vector , and @xmath29 is its @xmath2-th element . substituting the definition of the adaptation term @xmath19 to ( [ equ : g ] )",
    ", we can rewrite the target domain classifier @xmath17 as follows ,    @xmath30    and the problem of domain transfer multi - instance dictionary learning is to learn both @xmath21 and @xmath22 by using a target domain training set .",
    "the target domain training set is denoted as @xmath31 , where @xmath32 is the bag of the @xmath33-th target domain training data point , and @xmath34 is the binary class label of the @xmath33-th target domain training data point .",
    "@xmath32 contains @xmath35 instances , @xmath36 , where @xmath37 is the @xmath7-th instance of the @xmath33-th bag .",
    "given a dictionary @xmath21 , without confusion , the bag - level vector of @xmath32 , @xmath38 is simply denoted as @xmath39 .",
    "moreover , the classification response of the source domain classifier over this bag is denoted as    @xmath40    in this way , the response of the target domain classifier over @xmath32 is given as    @xmath41    to learn both the domain transfer dictionary @xmath21 and the classifier parameter @xmath22 over the target domain training set , we consider the following three minimization problems .    *",
    "* minimization of the classification errors*. the classification error of the @xmath33-th training bag is measure by the hinge loss , + @xmath42 + to seek the optimal classifier and dictionary , this classification error term should be minimized .",
    "we propose to minimize the average hinge loss over the target domain training set , + @xmath43 + the minimization of this problem is difficult because it is couple with a maximization problem within the hinge loss function . to solve this problem",
    ", we introduce a slack variable @xmath44 to present the maximum variable between @xmath45 and @xmath46 , + @xmath47 + with this slack variable , we can rewrite the problem in ( [ equ : hinge1 ] ) as a constrained minimization problem as follows , + @xmath48 * * reducing the complexity of the adaptation function*. to prevent the over - fitting problem , we want to keep the adaptation function as simply as possible , and reduce the complexity of the adaptation function .",
    "the complexity of the adaptation function is measured by the squared @xmath49 norm of the adaptation function parameter , + @xmath50 + to this end , we propose to minimize this regularization term as follows , + @xmath51 + the solution of this single problem is an all - zero vector @xmath52 $ ] .",
    "this solution is not optimal for the overall problem , but when this minimization problem is combined with the other problems , this term can bring a tradeoff between the classification error and the adaptation function simplicity . * * reducing the complexity of the dictionary*. we also hope the dictionary can be as simple as possible .",
    "the complexity is also measure by the squared @xmath49 norms of the codewords , + @xmath53 + similarly , the solution for this single problem is @xmath23 all - zero codewords , @xmath54 , k=1,\\cdots , \\kappa$ ] .",
    "so we also use it as a regularization term to tradeoff the learning of the dictionary .",
    "the overall minimization problem is the weighted linear combination of the problems in ( [ equ : hinge2 ] ) , ( [ equ : w1 ] ) , and ( [ equ : psi ] ) .",
    "@xmath55    where @xmath56 is the weight of the regularization term of @xmath22 , and @xmath57 is the weight of the regularization term of @xmath21 .",
    "please note that in the objective function , the first term is the classification error term of the target domain , which is critical for the learning of optimal domain transfer multi - instance dictionary .",
    "this term grantees that the learned dictionary and its corresponding classifier can lead to a good classification accuracy over the training set of the target domain .",
    "the second and third terms are regularization terms which try to generalize the learned dictionary and classifier to the data beyond the training set .      in this section ,",
    "we discuss how to solve the overall problem in ( [ equ : hinge2 ] ) .",
    "we use the lagrange multiplier method to solve this problem .",
    "we first define a lagrange multiplier variable @xmath58 for each constraint @xmath59 , and a lagrange multiplier variable @xmath60 for each constraint @xmath61 .",
    "the lagrange function of this constrained problem is ,    @xmath62    thus the dual form of the problem is given as    @xmath63    to solve this problem , we set the partial derivatives of @xmath64 with regard to @xmath22 , @xmath44 to zeros ,    @xmath65    we substitute the results of ( [ equ : lagrange2 ] ) to ( [ equ : lagrange1 ] ) , and obtain the following problem ,    @xmath66    to solve this problem , we use an iterative algorithm with alternate optimization strategy .",
    "we only consider two variables in ( [ equ : lagrange3 ] ) , which are @xmath67 and @xmath21 , and all other variables have vanished .",
    "to optimize them , in an iteration , we first fix @xmath21 and update @xmath67 , then with the updated @xmath67 , we update @xmath21 .",
    "when @xmath21 is fixed , the problem in ( [ equ : lagrange3 ] ) is reduced to    @xmath68    this is a maximization problem with regard to @xmath67 .",
    "the objective is a quadratic function and the constraints are linear functions . thus it is a linear constrained quadratic programming problem .",
    "we can solve it by active set algorithm .",
    "when @xmath67 is fixed , the problem in ( [ equ : lagrange3 ] ) is reduced to    @xmath69    we further rewrite the bag - level feature vector as    @xmath70^\\top \\\\ & = \\left[{{\\boldsymbol{\\psi}}}_1^\\top{{\\textbf{x}}}^i_{\\pi^i_1 } , \\cdots,{{\\boldsymbol{\\psi}}}_\\kappa^\\top{{\\textbf{x}}}^i_{\\pi^i_\\kappa}\\right]^\\top , \\end{aligned}\\ ] ]    where    @xmath71    is the index of the instance which gives the maximum product between @xmath72 and the instance .",
    "thus we can rewrite @xmath73 in ( [ equ : lagrange3 ] ) as follows ,    @xmath74    substituting ( [ equ : zz ] ) to ( [ equ : psi1 ] ) , we can rewrite it as follows ,    @xmath75\\\\ & = \\sum_{k=1}^\\kappa   \\left [ \\frac{c_2}{2 } \\|{{\\boldsymbol{\\psi}}}_k\\|_2 ^ 2 - \\frac{1}{2c_1 } { { \\boldsymbol{\\psi}}}_k^\\top \\left ( \\sum_{i , j=1}^n\\beta_i \\beta_j y_i y_j { { \\textbf{x}}}^i_{\\pi^i_k } { { { \\textbf{x}}}^j_{\\pi^j_k}}^\\top \\right ) { { \\boldsymbol{\\psi}}}_k   \\right ] \\\\ & \\left .+   \\sum_{i=1}^n\\beta_i \\left ( 1 - y_i   f_i\\right ) \\right \\}. \\end{aligned}\\ ] ]    it is apparently that the objective function is composed of a submission of sub - functions over individual codewords and a term irrelevant to the codewords . thus we can optimize each codewords one by one independently .",
    "we define the objective for each @xmath72 as follows ,    @xmath76    which is a quadratic function of @xmath72 .",
    "the problem of ( [ equ : psi2 ] ) can be rewritten as follows ,    @xmath77    ignoring the last term irrelevant to the dictionary learning , this problem can be decomposed to @xmath23 independent sub - problems ,    @xmath78    to solve each of these problems , we use the sub - gradient deselect algorithm .",
    "firstly , we update the instance indexes according to ( [ equ : pi ] ) by using previous updated codewords , and then fix them to update the codewords themselves . to update them ,",
    "we fist calculate the sub - gradient function of @xmath79 with regard to @xmath72 ,    @xmath80    then we descent each codeword to the sub - gradient direction ,    @xmath81    where @xmath82 is the step size of the descent .",
    "the algorithm of updating the @xmath2-th codeword is summarized in algorithm 1 .",
    "* algorithm 1 : * updating the @xmath2-th codeword*. 1 .",
    "@xmath83^\\top$ ] ; 2 .   * for * @xmath84 3 .",
    "@xmath85 for @xmath86 ; 4 .",
    "@xmath87 ; 5 .",
    "* end of for * 6 .",
    "output @xmath72 .",
    "the overall algorithm for the learning of domain transfer dictionary and its corresponding adaptation function is summarized in algorithm 2 . in this algorithm ,",
    "the domain transfer dictionary and the lagrange multiplier variables are updated alternately in a while loop .",
    "after the while loop is completed , the adaptation function parameter is recovered from the lagrange multiplier variables .    * algorithm 2 : * learning domain transfer dictionary and adaptation function parameter ( dtc)*. 1 .",
    "* input * : target training set , @xmath88 ; 2 .",
    "* input * : source training dictionary and its corresponding classifier parameter , @xmath15 and @xmath16 ; 3 .   * input * : size of the domain transfer dictionary , @xmath23 ; 4 .",
    "* input * : weights of regularization terms , @xmath56 and @xmath57 .",
    "* initialization * : @xmath89 for @xmath86",
    "* initialization * : @xmath90 ; 7 .",
    "* initialization * : initialize the domain transfer dictionary @xmath91",
    "* repeat * 1 .   * for @xmath86 * 2 .",
    "update @xmath92^\\top$ ] ; 3 .",
    "* end of for * 4 .",
    "update @xmath93 by solve the quadratic programming problem by fixing @xmath94 ; + @xmath95 5 .",
    "* for @xmath96 * 6 .",
    "update @xmath97 by fixing @xmath93 and using algorithm 1 ; 7 .",
    "* end of for * 8 .",
    "9 .   * until convergency * 10 . update @xmath99 ; 11 . *",
    "output * : @xmath100 , @xmath22 .",
    "in this section , we use three benchmark data sets to evaluate the performance of the proposed algorithm .",
    "the method is compared to state - of - the - arts domain adaptation algorithms , and also some single domain dictionary learning algorithms .",
    "the running time of the algorithm is reported , and the convergency of the iterative algorithm is also plot .",
    "we use two benchmark domain transfer multi - instance data sets in our experiments , which are introduced as follows .      the first data set used is the trecvid data set @xcite .",
    "this data set is a set of key frames of video programs .",
    "the total number of the key frames is 61,901 , and they belongs to 36 classes of concepts .",
    "this data set is composed of two sub - sets , which are trecvid 2005 data set and trecvid 2007 data set .",
    "the differences of program structure and production values between these two sub - sets are significantly , thus we treat them as two different domains .",
    "moreover , we choose the key frames of the chinese channel cctv4 from trecvid 2005 as source domain data , and the entire trecvid 2007 data set as the target domain @xcite .",
    "to present each key frame , we extract the sift local features , and treat each key frame as a bag of the local features .",
    "each local feature is treated as an instance .",
    "thus this is a multi - instance learning problem .",
    "the seconde data is the combination of the mrsc data set and voc .",
    "the mrsc data is an image data set containing 4,323 images of 18 classes , and the voc data set is also an image data set containing 5,011 images of 20 classes .",
    "the mrsc data set is publicly accessible at http://research.microsoft.com/enu + s / projects / objectclassrecognition , and the voc data set is publicly accessible at http://pascallin.ecs.soton.ac.uk/challenges/voc/voc2007 .",
    "both the two data sets share 6 common classes , which are listed as follows : aeroplane , bicycle , bird , car , cow , and sheep . however , the distractions of the data of these two data sets are significantly different , and we tree them as two different domains .",
    "thus we can combine the images of these 6 classes of both the data sets to one cross - domain data set .",
    "the cross - domain data set is composed of a source domain set of msrc containing 1,269 images , and a target domain of voc containing 1,530 images @xcite .",
    "thus the total number of the images of this cross - domain data set is 2,799 . to present each image ,",
    "we also extract a set of sift local features , and treat each local feature as an instance .",
    "each image is treated as a bag of instances .",
    "the third data set is a subset of the 20 newsgroups data set .",
    "this data set is a set of 18,774 documents .",
    "the documents belong to 6 main classes , and 20 sub - classes . to construct the setting of domain learning , we set the main class  comp \" to be the positive class , and the main class  rec \" to be the negative class . in the positive class ,",
    "we further select two sub - classes to be source domain and target domain respectively , i.e. ,  comp.windows.x \" and  comp.sys.ibm.pc \" . for the negative class , we also select two sub - classes to be source domain and target domain , i.e. ,  rec.sport.hockey \" and  rec.motorcycles \" . thus the source domain contains the data of  comp.windows.x \" and  rec.sport.hockey \" , and the target domain contains the data of  comp.sys.ibm.pc \" and  rec.motorcycles \" . to represent each document",
    ", we treat each paragraph as a instance , and use the word - frequency feature as the feature of the instance .",
    "thus each document is a bag of instances .",
    "to conduct the experiment , we use the 10-fold cross - validation .",
    "the target domain data set is split to 10 sub - sets .",
    "each sub - set is used as a training set , and the other 9 sub - sets are used as testing set .",
    "we fist train a source domain dictionary and classifier over the entire source domain data set , and then use the target training set to train the domain - transfer dictionary and its corresponding adaptive function parameter .",
    "then the target domain predictor is obtained as the combination of the source domain predictor and the adaptive function .",
    "the target domain predictor is finally tested over the target domain test data set .",
    "please note that only one fold is used as target domain training set .",
    "this is insufficient for training a good target domain predictor without the help of the source domain data .",
    "this setting makes it necessary to perform domain transfer learning .",
    "the average classification accuracy over the ten folds of training set is evaluated as performance measure .      in this section ,",
    "we report the experimental results of the proposed algorithm .",
    "we first compare the proposed algorithm , dtc , against both some common domain transfer learning methods and two domain transfer multi - instance learning methods .",
    "the compared domain transfer learning methods are listed as follows ,    * domain transfer svm ( dt - svm ) @xcite , * domain transfer multiple kernel learning ( dt - mkl ) @xcite , * deep low - rank coding for transfer learning ( dlrc ) @xcite , and * cross - domain sparse coding ( cdsc ) @xcite .    for these methods , the dictionary used to represent the bags are not learned cross domains .",
    "the dictionary is learned before the transfer learning is conducted and it is fixed during the transfer process .",
    "we also compare the proposed domain transfer multi - instance learning to two other such algorithms , which are    * the original multiple instance transfer learning ( mitl ) @xcite , and * knowledge transfer learning for multiple instance learning ( aktl - mil ) @xcite .",
    "please note that for the mitl algorithm , it assumes that there are several source domains , and the algorithm learns the predictors of different source domains and the combination coefficients of the predictors .",
    "however , in our experimental setting , we have only one source domain .",
    "thus we only learn one source domain predictor and set its coefficient to one .",
    "the comparisional results of different domain transfer learning algorithms are given in table [ tab : transfer ] . from table [",
    "tab : transfer ] we observe that the proposed method outperforms all the other four compared domain transfer learning methods , including linear classifier @xcite , multi - kernel classifier @xcite , deep coding @xcite , and sparse coding @xcite .",
    "this indicates that for multi - instance domain transfer learning , using a dictionary is the most effective method to represent the multi - instance data .",
    "moreover , the outperforming of the proposed method over the compared method is even more significant over the data sets of trecvid and mrsc+voc .",
    "for example , over trecvid data set , dtc is the only algorithm that achieves an average accuracy higher than 0.300 , and over mrsc+voc , dtc is also the only algorithm that has a higher average accuracy higher than 0.400 .",
    "both these two data sets are image sets with local features .",
    "this implies that the proposed domain - transfer dictionary learning is especially suitable for computer vision tasks which represent images as collections of visual local features .",
    ".average classification accuracy of comparison among domain transfer learning methods [ cols=\"<,^,^,^\",options=\"header \" , ]       and @xmath57.,title=\"fig:\",scaledwidth=65.0% ] +    we perform the analysis of the sensitivity of the proposed algorithm to the two tradeoff parameters , @xmath56 , and @xmath57 .",
    "we plot the average classification accuracy of the proposed algorithm with different values of @xmath56 and @xmath57 over the trecvid data set in fig .",
    "[ fig : c1c2 ] . according to the figure ,",
    "the proposed algorithm are stable to the changes of both the two parameters .",
    "in this paper , we study the problem of domain transfer learning with multi - instance learning . we proposed to use the multi - instance dictionary to represent the bag of instances .",
    "however , due to the significant difference between the distributions of source and target domains , the dictionary learned from the source domain is not suitable to the target domain .",
    "we propose to learn a domain transfer dictionary to solve this problem .",
    "a target domain bag is represented by both the source domain and the domain transfer dictionaries , and then classified by the source domain classifier and an adaptive function simultaneously .",
    "the target domain classification response is the combination of the source domain response and the result of the domain adaptive function . to learn the domain transfer dictionary and its corresponding adaptive function parameter",
    ", we model a minimization problem , which minimizes the complexities of the adaptive function parameters , the domain transfer dictionary , and the average classification error jointly .",
    "the optimization problem is solved by an iterative algorithm .",
    "our proposed method is shown to be effective by experiments over three benchmark data sets . in the future",
    ", we will also use the proposed algorithm to applications of bioinformatics @xcite and data analysis of nanotechnology @xcite .",
    "chen , a. , eberle , m. , lunt , e. , liu , s. , leake , k. , rudenko , m. , hawkins , a. , schmidt , h. : dual - color fluorescence cross - correlation spectroscopy on a planar optofluidic chip .",
    "lab on a chip * 11*(8 ) , 15021506 ( 2011 )          duan , l. , tsang , i.w .",
    ", xu , d. , maybank , s.j . :",
    "domain transfer svm for video concept detection . in : computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on , pp .",
    "ieee ( 2009 )    fan , x. , malone , b. , yuan , c. : finding optimal bayesian network structures with constraints learned from data . in : proceedings of the 30th annual conference on uncertainty in artificial intelligence ( uai-14 ) , pp .",
    "200209 ( 2014 )              hammami , n. , bedda , m. : improved tree model for arabic speech recognition .",
    "in : computer science and information technology ( iccsit ) , 2010 3rd ieee international conference on , vol .  5 , pp .",
    "521526 . ieee ( 2010 )    hammami , n. , sellam , m. : tree distribution classifier for automatic spoken arabic digit recognition . in : internet technology and secured transactions , 2009 .",
    "icitst 2009 . international conference for , pp . 14 .",
    "ieee ( 2009 )      ling , x. , dai , w. , xue , g.r .",
    ", yang , q. , yu , y. : spectral domain - transfer learning . in : proceedings of the 14th acm sigkdd international conference on knowledge discovery and data mining , pp .",
    "488496 . acm ( 2008 )          liu , s. , zhao , y. , parks , j.w . , deamer , d.w . ,",
    "hawkins , a.r . , schmidt , h. : correlated electrical and optical analysis of single nanoparticles and biomolecules on a nanopore - gated optofluidic chip .",
    "nano letters * 14*(8 ) , 48164820 ( 2014 )    liu , x. , wang , j. , yin , m. , edwards , b. , xu , p. : supervised learning of sparse context reconstruction coefficients for data representation and classification . neural computing and applications",
    "pp . 19 ( 2015 )",
    "long , m. , ding , g. , wang , j. , sun , j. , guo , y. , yu , p. : transfer sparse coding for robust image representation . in : proceedings of the ieee conference on computer vision and pattern recognition , pp . 407414 ( 2013 )    lu , g. , yan , y. , ren , l. , saponaro , p. , sebe , n. , kambhamettu , c. : where am i in the dark : exploring active transfer learning on the use of indoor localization based on thermal imaging .",
    "neurocomputing * 173 * , 8392 ( 2016 )      oomen , j. , over , p. , kraaij , w. , smeaton , a. : symbiosis between the trecvid benchmark and video libraries at the netherlands institute for sound and vision . international journal on digital libraries * 13*(2 ) , 91104 ( 2013 )    pan , s.j . , ni , x. , sun , j.t . , yang , q. , chen , z. : cross - domain sentiment classification via spectral feature alignment . in : proceedings of the 19th international conference on world wide web , pp .",
    "751760 . acm ( 2010 )    peng , b. , liu , y. , zhou , y. , yang , l. , zhang , g. , liu , y. : modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics .",
    "nanoscale research letters * 10*(1 ) , 19 ( 2015 )      wang , h. , wang , j. : an effective image representation method using kernel classification . in :",
    "2014 ieee 26th international conference on tools with artificial intelligence ( ictai 2014 ) , pp .",
    "853858 ( 2014 )    wang , j. , wang , h. , zhou , y. , mcdonald , n. : multiple kernel multivariate performance learning using cutting plane algorithm . in : systems ,",
    "man , and cybernetics ( smc ) , 2015 ieee international conference on , pp .",
    "18701875 ( 2015 )    wang , j. , zhou , y. , duan , k. , wang , j.j.y . , bensmail , h. : supervised cross - modal factor analysis for multiple modal data classification . in : systems ,",
    "man , and cybernetics ( smc ) , 2015 ieee international conference on , pp .",
    "18821888 ( 2015 )    wang , j.j.y . , bensmail , h. : cross - domain sparse coding . in : cikm 2013 ,",
    "proceedings of the 22nd acm international conference on conference on information & knowledge management , pp .",
    "14611464 . acm ( 2013 )        wang , s. , zhou , y. , tan , j. , xu , j. , yang , j. , liu , y. : computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field .",
    "computational mechanics * 53*(3 ) , 403412 ( 2014 )            zhao , s. , cao , q. , chen , j. , zhang , y. , tang , j. , duan , z. : a multi - atl method for transfer learning across multiple domains with arbitrarily different distribution .",
    "knowledge - based systems * 94 * , 6069 ( 2016 )    zhou , y. , hu , w. , peng , b. , liu , y. : biomarker binding on an antibody - functionalized biosensor surface : the influence of surface properties , electric field , and coating density .",
    "the journal of physical chemistry c * 118*(26 ) , 14,58614,594 ( 2014 )"
  ],
  "abstract_text": [
    "<S> in this paper , we invest the domain transfer learning problem with multi - instance data . </S>",
    "<S> we assume we already have a well - trained multi - instance dictionary and its corresponding classifier from the source domain , which can be used to represent and classify the bags . </S>",
    "<S> but it can not be directly used to the target domain . </S>",
    "<S> thus we propose to adapt them to the target domain by adding an adaptive term to the source domain classifier . </S>",
    "<S> the adaptive function is a linear function based a domain transfer multi - instance dictionary . </S>",
    "<S> given a target domain bag , we first map it to a bag - level feature space using the domain transfer dictionary , and then apply a the linear adaptive function to its bag - level feature vector . to learn the domain - transfer dictionary and the adaptive function parameter </S>",
    "<S> , we simultaneously minimize the average classification error of the target domain classifier over the target domain training set , and the complexities of both the adaptive function parameter and the domain transfer dictionary . </S>",
    "<S> the minimization problem is solved by an iterative algorithm which update the dictionary and the function parameter alternately . </S>",
    "<S> experiments over several benchmark data sets show the advantage of the proposed method over existing state - of - the - art domain transfer multi - instance learning methods .    </S>",
    "<S> example.eps    gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}