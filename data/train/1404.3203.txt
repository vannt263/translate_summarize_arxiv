{
  "article_text": [
    "a decade of powerful results in compressed sensing and related fields have demonstrated that many signals that have low - dimensional latent structure can be recovered from very few compressive measurements .",
    "building on this work , many researchers have shown that classification tasks can also be run on compressive measurements , provided that either the data or classifier is sparse in an appropriate basis  @xcite .",
    "however , classification is a considerably simpler task than reconstruction , as there may be a large number of hyperplanes which successfully cleave the same data set .",
    "the question remains :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ can we successfully classify data from even fewer compressive measurements than required for signal reconstruction ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    prior work on compressive classification has focused on preserving distances or inner products between data points .",
    "indeed , since popular classifiers including the support vector machine and logistic regression only depend on dot products between data points , it makes sense that if dot products are preserved under a compressive measurement , then the resulting decision hyperplane should be close to the one computed on the uncompressed data .    in this paper",
    ", we take a different view of the compressive classification problem , and for some special cases , we are able to show that data can be classified with extremely few compressive measurements . specifically , we assume that our data classes are circumscribed by disjoint convex bodies , and we seek to avoid intersection between distinct classes after projection . by studying the set of separating hyperplanes , we provide a general way to estimate the minimal dimension under which two bodies remain disjoint after random projection . in section  3 , we specialize these results to study ellipsoidal classes and give our main theoretical result  that @xmath0 ellipsoids of sufficient pairwise separation remain separated after randomly projecting onto @xmath1 dimensions . here , the geometry of the ellipsoids plays an interesting and intuitive role in the notion of sufficient separation .",
    "our results differ from prior work insofar as they can be applied to _",
    "full _ dimensional data sets and are independent of the number of points in each class . we provide a comparison with principal component analysis in section  4 by considering different toy examples of classes to illustrate strengths and weaknesses , and then by applying both approaches to hyperspectral imaging data .",
    "we conclude in section  5 with a discussion of future work .",
    "in this section , we discuss our model for the classes as well as the underlying assumptions we apply throughout this paper .",
    "consider an ensemble of classes @xmath2 that we would like to classify .",
    "we assume that these classes are pairwise _ linearly separable _ , that is , for every pair @xmath3 with @xmath4 , there exists a hyperplane in @xmath5 which separates @xmath6 and @xmath7 .",
    "equivalently , we assume that the convex hulls @xmath8 are disjoint , and for simplicity , we assume these convex hulls are closed sets .    linear separability is a particularly useful property in the context of classification , since to demonstrate non - membership , it suffices to threshold an inner product with the vector normal to a separating hyperplane . of course",
    ", in many applications , classes do not enjoy this ( strong ) property , but the property can be weakened to _ near _ linear separability , in which there exists a hyperplane that mostly distinguishes of a pair of classes .",
    "one may also lift to a tensored version of the vector space and find linear separability there .",
    "since linear separability is so useful , we use this property as the basis for our notion of distortion : we seek to project the classes @xmath9 in such a way that their images are linearly separable .",
    "our assumptions on the @xmath6 s and our notion of distortion both lead to a rather natural problem in convex geometry ( see figure  [ figure.rare eclipse problem ] for an illustration ) :    given a pair of disjoint closed convex sets @xmath10 and @xmath11 , find the smallest @xmath12 such that a random @xmath13 projection @xmath14 satisfies @xmath15 with probability @xmath16 .",
    "two sufficiently separated convex sets remain separated when projected onto a subspace .",
    "the rare eclipse problem asks for the smallest @xmath12 such that this happens when projecting onto a random subspace of dimension @xmath12 .",
    "solving this problem for a given ensemble of classes enables dimensionality reduction in a way that ensures linear separability for classification . , scaledwidth=50.0% ]    at this point , we discuss some related work in the community .",
    "it appears that compressive classification was studied as early as 2006 , when  @xcite considered a model in which each class is a point in euclidean space .",
    "interestingly , this bears some resemblance to the celebrated work in  @xcite , which used random projections to quickly approximate nearest - neighbor search .",
    "the work in  @xcite considered a more exotic family of classes , namely low - dimensional manifolds  this is particularly applicable to the classification of images according to the primary object featured in each image .",
    "along these lines of low - dimensional classes , there has since been some work in the case where classes are low - dimensional subspaces  @xcite , or unions thereof  @xcite .",
    "specifically , @xcite considers a gaussian mixture model in which each gaussian is supported on a different subspace . from a slightly dual view",
    ", researchers have also shown that if the classifier is known to be sparse , then we can subsample the data itself , and the separating hyperplane can be determined from a number of examples roughly proportional to the sparsity of the hyperplane  @xcite .",
    "it is striking that , to date , all of the work in compressive classification has focused on classes of low dimension .",
    "this is perhaps an artifact of the mindset of compressed sensing , in which the projection preserves all information on coordinate planes of sufficiently small dimension .",
    "however , classification should not require nearly as much information as signal reconstruction does , and so we expect to be able to compressively classify into classes of full dimension ; indeed , we allow two points in a common class to be mapped to the same compressive measurement , as this will not affect the classification . a boolean version of this idea is studied in @xcite , which considers both random and optimality constructed projections . in the continuous setting ,",
    "the closest existing work is that of dasgupta  @xcite , which uses random projections to learn a mixture of gaussians . in particular",
    ", dasgupta shows that sufficiently separated gaussians stay separated after random projection . in the next section ,",
    "we prove a similar result about ellipsoids , but with a sharper notion of separation .",
    "given two disjoint closed convex bodies @xmath10 and a projection dimension @xmath12 , the rare eclipse problem asks whether a random @xmath13 projection @xmath14 of these bodies avoids collision , i.e. , whether @xmath17 is typically empty .",
    "this can be recast as a condition on the @xmath18-dimensional null space of @xmath14 : @xmath19 where @xmath20 denotes the minkowski difference of @xmath21 and @xmath22 .",
    "of course , the null space of @xmath14 is closed under scalar multiplication , and so avoiding @xmath20 is equivalent to avoiding the normalized versions of the members of @xmath20 . indeed ,",
    "if we take @xmath23 to denote the intersection between the unit sphere in @xmath5 and the cone generated by @xmath20 , then @xmath24 now suppose @xmath14 is drawn so that its entries are iid @xmath25 . then by rotational invariance , the distribution of its null space is uniform over the grassmannian . as such",
    ", the rare eclipse problem reduces to a classical problem in convex geometry : given a `` mesh '' ( a closed subset of the unit sphere ) , how small must @xmath26 be for a random @xmath26-dimensional subspace to `` escape through the mesh , '' i.e. , to avoid collision ? it turns out that for this problem , the natural way to quantify the size of a mesh is according to its _ gaussian width _ : @xmath27,\\ ] ] where @xmath28 is a random vector with iid @xmath25 entries .",
    "indeed , gaussian width plays a crucial role in the following result , which is an improvement to the original ( corollary  3.4 in  @xcite ) ; the proof is given in the appendix , and follows the proof of corollary  3.3 in  @xcite almost identically .",
    "take a closed subset @xmath23 of the unit sphere in @xmath5 , and denote @xmath29 , where @xmath28 is a random @xmath12-dimensional vector with iid @xmath25 entries .",
    "if @xmath30 , then an @xmath18-dimensional subspace @xmath31 drawn uniformly from the grassmannian satisfies @xmath32    it is straightforward to verify that @xmath33 , and so rearranging leads to the following corollary :    [ corollary.gordon ] take disjoint closed convex sets @xmath10 , and let @xmath34 denote the gaussian width of the intersection between the unit sphere in @xmath5 and the cone generated by the minkowski difference @xmath20 .",
    "draw an @xmath13 matrix @xmath14 with iid @xmath25 entries .",
    "then @xmath35    now that we have a sufficient condition on @xmath12 , it is natural to wonder how tight this condition is .",
    "recent work by amelunxen , lotz , mccoy and tropp  @xcite shows that the gordon s results are incredibly tight .",
    "indeed , by an immediate application theorem  i and proposition  10.1 in  @xcite , we achieve the following characterization of a phase transition for the rare eclipse problem :    [ corollary.akf ] take disjoint closed convex sets @xmath10 , and let @xmath34 denote the gaussian width of the intersection between the unit sphere in @xmath5 and the cone generated by the minkowski difference @xmath20 .",
    "draw an @xmath13 matrix @xmath14 with iid @xmath25 entries .",
    "then @xmath36    considering the second part of corollary  [ corollary.akf ] , the bound in corollary  [ corollary.gordon ] is essentially tight . also , since corollary  [ corollary.akf ] features an additional @xmath37 factor in the error term of the phase transition , the bound in corollary  [ corollary.gordon ] is stronger than the first part of corollary  [ corollary.akf ] when @xmath38 , which corresponds to the regime where we can compress the most : @xmath39 .",
    "corollaries  [ corollary.gordon ] and  [ corollary.akf ] demonstrate the significance of gaussian width to the rare eclipse problem . in this subsection , we observe these quantities to solve the rare eclipse problem in the special case where @xmath21 and @xmath22 are balls .",
    "since each ball has its own parameters ( namely , its center and radius ) , in this subsection , it is more convenient to write @xmath40 and @xmath41 .",
    "the following lemma completely characterizes the difference cone @xmath42 :    [ lemma.circcone lower bound ] for @xmath43 , take balls @xmath44 , where @xmath45 , @xmath46 such that @xmath47 , and @xmath48 denotes the ball centered at @xmath49 of radius @xmath50 . then the cone generated by the minkowski difference @xmath42 is the circular cone @xmath51 where @xmath52 is the angle such that @xmath53 .    in three dimensions ,",
    "the fact that the difference cone is circular makes intuitive sense .",
    "the proof of lemma  [ lemma.circcone lower bound ] is routine and can be found in the appendix .    considering the beginning on this section",
    ", it now suffices to bound the gaussian width of the circular cone s intersection with the unit sphere @xmath54 .",
    "luckily , this computation is already available as proposition  4.3 in  @xcite : @xmath55 see figure  [ figure_circcone_phasetransition ] for an illustration of the corresponding phase transition . by lemma",
    "[ lemma.circcone lower bound ] ( and corollaries  [ corollary.gordon ] and  [ corollary.akf ] ) , this means a random @xmath13 projection will keep two balls from colliding provided @xmath56 note that there is a big payoff in the separation @xmath57 between the balls . indeed , doubling the separation decreases the required projection dimension by a factor of @xmath58 .",
    "( plot ) at ( 3,3 )   phase transition for a random null space to avoid a circular cone . fixing the ambient dimension to be @xmath59 ,",
    "then for each @xmath60 and @xmath61 , we randomly drew @xmath62 @xmath13 matrices with iid @xmath25 entries and plotted the proportion whose null spaces avoided the circular cone with angle @xmath63 .",
    "as expected , if @xmath63 is large , then so must @xmath12 so that the null space is small enough to avoid the cone . in red",
    ", we plot the curve @xmath64 , which captures the phase transition by theorem  i and proposition  4.3 in  @xcite . by lemma",
    "[ lemma.circcone lower bound ] , the circular cone is precisely the difference cone of two balls , and so this phase transition solves the rare eclipse problem in this special case . ,",
    "title=\"fig:\",width=226 ] ; ( 0,0 ) ",
    "( 6,0 ) ; ( 0,0 ) ",
    "( 0,6 ) ; ( 1.5,0 ) ",
    "( 1.5,0.1 ) ; ( 3,0 ) ",
    "( 3,0.1 ) ; ( 4.5,0 ) ",
    "( 4.5,0.1 ) ; ( 6,0 ) ",
    "( 6,0.1 ) ; ( 0,1.5 ) ",
    "( 0.1,1.5 ) ; ( 0,3 )  ( 0.1,3 ) ; ( 0,4.5 ) ",
    "( 0.1,4.5 ) ; ( 0,6 ) ",
    "( 0.1,6 ) ; ( 0,-0.2 ) node ; ( 1.5,-0.2 ) node ; ( 3,-0.2 ) node ; ( 4.5,-0.2 ) node ; ( 6,-0.2 ) node ; ( 3,-0.75 ) node @xmath63 : angle of cone ; ( -0.15,0 ) node ; ( -0.2,1.5 ) node ; ( -0.2,3 ) node ; ( -0.2,4.5 ) node ; ( -0.25,6 ) node ; ( -0.75,3 ) node [ rotate=90 ] @xmath12 : rank of random projection ;      now that we have solved the rare eclipse problem for balls , we consider the slightly more general case of ellipsoids . actually , this case is somewhat representative of the general problem with arbitrary convex sets .",
    "this can be seen by appealing to the following result of paouris  @xcite :    [ theorem.concentration of volume ] there is an absolute constant @xmath65 such that the following holds : given a convex set @xmath66 , draw a random vector @xmath67 uniformly from @xmath26 .",
    "suppose @xmath26 has the property that @xmath68=0 $ ] and @xmath69=i$ ] .",
    "then @xmath70    in words , the above theorem says that the volume of an isotropic convex set is concentrated in a round ball .",
    "the radius of the ball of concentration is @xmath71 , which corresponds to the fact that @xmath72=n$ ] .",
    "this result can be modified to describe volume concentration of any convex set ( isotropic or not ) . to see this , consider any convex set @xmath66 of full dimension ( otherwise the volume is zero ) . then taking @xmath31 to be a random vector drawn uniformly from @xmath26 , we define the centroid @xmath73 $ ] .",
    "also , since @xmath26 has full dimension , the inertia matrix @xmath74 $ ] is symmetric and positive definite , and we can take @xmath75)^{1/2}$ ] .",
    "it is straightforward to verify that @xmath76 is distributed uniformly over @xmath77 , and that @xmath78 satisfies the hypotheses of theorem  [ theorem.concentration of volume ] .",
    "we claim that @xmath31 is concentrated in an ellipsoid defined by @xmath79 for some @xmath80 , where @xmath48 denotes the ball centered at @xmath49 of radius @xmath50 .",
    "indeed , theorem  [ theorem.concentration of volume ] gives @xmath81 overall , the vast majority of any convex set is contained in an ellipsoid defined by its centroid and inertia matrix , and so two convex sets are _ nearly _ linearly separable if the corresponding ellipsoids are linearly separable .",
    "( a similar argument relates the case of two ellipsoids to a mixture of two gaussians . )",
    "note that any ellipsoid has the following convenient form : @xmath82 where @xmath83 is the center of the ellipsoid , @xmath21 is some @xmath84 symmetric and positive semidefinite matrix , and @xmath48 denotes the ball centered at the origin of radius @xmath50 .",
    "intuitively , the difference cone of any two ellipsoids will not be circular in general , as it was in the case of two balls .",
    "indeed , the oblong shape of each ellipsoid ( determined by its shape matrix @xmath21 ) precludes most of the useful symmetries in the difference cone , and as such , the analysis of the size of the cone is more difficult .",
    "still , we established the following upper bound on the gaussian width in the general case , which by corollaries  [ corollary.gordon ] and  [ corollary.akf ] , translates to a sufficient number of rows for a random projection to typically maintain separation :    [ theorem.two ellipsoids ] for @xmath43 , take ellipsoids @xmath85 , where @xmath45 , @xmath86 is symmetric and positive semidefinite , and @xmath48 denotes the ball centered at @xmath49 of radius @xmath50 .",
    "let @xmath34 denote the gaussian width of the intersection between the unit sphere in @xmath5 and the cone generated by the minkowski difference @xmath42 .",
    "then @xmath87 provided @xmath88 ; here , @xmath89 and @xmath90 .",
    "the proof is technical and can be found in the appendix , but the ideas behind the proof are interesting .",
    "there are two main ingredients , the first of which is the following result :    [ proposition 3.6 ] let @xmath91 be any non - empty convex cone in @xmath5 , and let @xmath28 be an @xmath92-dimensional vector with iid @xmath25 entries",
    ". then @xmath93,\\ ] ] where @xmath94 denotes the euclidean projection onto the dual cone @xmath95 of @xmath91",
    ".    proposition  [ proposition 3.6 ] is essentially a statement about convex duality , and while it provides an upper bound on @xmath34 , in our case , it is difficult to find a closed form expression for the right - hand side .",
    "however , the bound is in terms of distance to the dual cone , and so any point in this cone provides an upper bound on this distance .",
    "this leads to the second main ingredient in our analysis : we choose a convenient mapping @xmath96 that sends any vector @xmath28 to a point in @xmath95 ( but not necessarily the closest point ) , while at the same time allowing the expectation of @xmath97 to have a closed form .",
    "since @xmath98 for every possible instance of @xmath28 , this produces a closed - form upper bound on the bound in proposition  [ proposition 3.6 ] .",
    "cc    ( -1.15,-0.9 )  ( -0.4,1.8 ) ; ( -2,0.15 ) ",
    "( 3.8,-0.23 ) ; ( 0,0 ) ellipse [ x radius=3 cm , y radius=1 cm ] ; ellipse [ x radius=1.5 cm , y radius=1 cm ] ; ( -2.5,-1 )  ( 2.5,1 ) ; ( 2.4,1.3 ) node ; ( 4,0.5 ) node ; ( -3.2,-1.5 ) node ;    &    ( 0,0 ) rectangle ( 0.3,0.3 ) ; ( 0,0 ) rectangle ( 0.3,0.3 ) ; ( 0,0 )  ( 2.9,-0.19 ) ",
    "( 0.75,2.7 ) cycle ; ( 0,0 )  ( -0.19,-2.9 )  ( -2.7,0.75 ) cycle ; ( 0,0 ) ",
    "( 0.844,0.3125 ) ; ( -0.25,1.25 ) circle [ radius=0.07 cm ] ; ( -0.25,1.6 ) node @xmath28 ; ( -0.25,1.25 )  ( -2,0.553 ) ; ( -2,0.553 ) circle [ radius=0.07 cm ] ; ( -2.2,1.2 ) node ; ( 1.1,0.4 ) node ; ( 1,1.5 ) node ; ( -0.6,-1.5 ) node ;     +    ( -1.25,1.3 ) ",
    "( 1.25,-1.3 ) ; ( -2.5,0.2 )  ( 2.5,-0.2 ) ; ( 0,0 ) ellipse [ x radius=3 cm , y radius=1 cm ] ; ellipse [ x radius=3 cm , y radius=1 cm ] ; ( -1.5,-1 ) ",
    "( 1.5,1 ) ; ( 1.4,1.3 ) node ; ( 3,0.5 ) node ; ( -3.2,-0.5 ) node ;    &    ( 0,0 ) rectangle ( 0.3,0.3 ) ; ( 0,0 ) rectangle ( 0.3,0.3 ) ; ( 0,0 )  ( 3.607,-0.289 )  ( -2.5,2.6 ) cycle ; ( 0,0 )  ( -0.289,-3.607 )  ( -2.6,-2.5 ) cycle ; ( 0,0 )  ( 0.75,0.5 ) ; ( -0.25,1 ) circle [ radius=0.07 cm ] ; ( -0.5,1.2 ) node ; ( -0.25,1 ) ",
    "( -1.75,0 ) ; ( 0.95,0.45 ) node ; ( 2,0.1 ) node ; ( -0.8,-2 ) node ;    figure  [ figure.pseudoprojection ] illustrates how we chose the pseudoprojection @xmath96 .",
    "interestingly , this pseudoprojection behaves more like the true projection when the ellipsoids are more distant from each other . at the other extreme ,",
    "note that theorem  [ theorem.two ellipsoids ] does not hold if the ellipsoids are too close , i.e. , if @xmath99 .",
    "this occurs , for example , if the two ellipsoids collide after projecting onto the span of @xmath100 ; indeed , taking @xmath101 and @xmath102 to be unit - norm vectors such that @xmath103 , then rearranging gives @xmath104 as figure  [ figure.pseudoprojection ] illustrates , our pseudoprojection even fails to be well - defined when the ellipsoids collide after projecting onto the span of @xmath100 .",
    "so why bother using a random projection to maintain linear separability when there is a rank-@xmath50 projection available ?",
    "there are two reasons : first , calculating this rank-@xmath50 projection requires access to the centers of the ellipsoids , which are not available in certain applications ( e.g. , unsupervised or semi - supervised learning , or if the projection occurs blindly during the data collection step ) .",
    "second , the use of a random projection is useful when projecting multiple ellipsoids simultaneously to preserve pairwise linear separability  as we will detail in the next subsection , randomness allows one to appeal to the union bound in a way that permits several ellipsoids to be projected simultaneously using particularly few projected dimensions .    at this point , we compare theorem  [ theorem.two ellipsoids ] to the better understood case of two balls . in this case , @xmath105 and @xmath106 , and so theorem  [ theorem.two ellipsoids ] gives that @xmath107 if we consider the regime in which @xmath108 , then we recover the case of two balls to within a factor of @xmath109 , suggesting that the analysis is tight ( at least in this case ) . for a slightly more general lower",
    "bound , note that a projection maintains separation between two ellipsoids only if it maintains separation between balls contained in each ellipsoid .",
    "the radius of the largest ball in the @xmath110th ellipsoid is equal to the smallest eigenvalue @xmath111 of the shape matrix @xmath86 , and the center of this ball coincides with the center @xmath112 of its parent ellipsoid . as such , we can again appeal to the case of two balls to see that theorem  [ theorem.two ellipsoids ] is reasonably tight for ellipsoids of reasonably small eccentricity @xmath113 .",
    "closed form bounds for general ellipses with high eccentricity are unwieldy , but figure  [ figure_ellipsoids_phasetransition ] illustrates that our bound is far from tight when the ellipsoids are close to each other .",
    "still , the bound improves considerably as the distance increases . as such",
    ", we leave improvements to theorem  [ theorem.two ellipsoids ] as an open problem ( in particular , finding a closed - form characterization of the phase transition in terms of the @xmath112 s and @xmath86 s ) .",
    "cc    ( plot ) at ( 3.07,3 )   phase transition for a random projection to keep ellipsoids separated .",
    "( a ) fixing the ambient dimension to be @xmath114 , then for each @xmath115 and @xmath116 , we conducted @xmath117 trials . for each trial",
    ", we randomly drew @xmath118 and @xmath119 as iid standard wishart - distributed @xmath84 matrices with @xmath92 degrees of freedom ( i.e. , @xmath120 , where @xmath67 is @xmath84 with iid @xmath25 entries ) , along with an @xmath13 matrix @xmath14 with iid @xmath25 entries .",
    "plotted is the proportion of trials for which the ellipsoids are disjoint after applying @xmath14 ( we did not record whether the ellipsoids were separated before projection ) . for each of the 160,000 trials ,",
    "the shape matrices satisfied @xmath121 , thereby rendering theorem  [ theorem.two ellipsoids ] irrelevant .",
    "( b ) next , we performed the same experiment , except we changed the distribution of @xmath118 and @xmath119 so that @xmath100 is in the null space of both , and in the orthogonal complement of @xmath100 , they are iid standard wishart - distributed @xmath122 matrices with @xmath123 degrees of freedom .",
    "as such , the corresponding ellipsoids resided in parallel hyperplanes , and @xmath124 so that theorem  [ theorem.two ellipsoids ] applies .",
    "for each trial , we stored the bound on @xmath34 from theorem  [ theorem.two ellipsoids ] and calculated the sample average of the squares of these bounds corresponding to each @xmath115 .",
    "the red curve plots these sample averages ( or 40 , whichever is smaller)think of this as an upper bound on the phase transition .",
    "as one might expect , this bound appears to sharpen as the distance increases .",
    ", title=\"fig:\",height=226 ] ; ( 0,0 )  ( 6.135,0 ) ; ( 0,0 )  ( 0,6 ) ; ( 1.53375,0 )  ( 1.53375,0.1 ) ; ( 3.0675,0 )  ( 3.0675,0.1 ) ; ( 4.60125,0 )  ( 4.60125,0.1 ) ; ( 6.135,0 )  ( 6.135,0.1 ) ; ( 0,1.5 )  ( 0.1,1.5 ) ; ( 0,3 )  ( 0.1,3 ) ; ( 0,4.5 )  ( 0.1,4.5 ) ; ( 0,6 )  ( 0.1,6 ) ; ( 0,-0.2 ) node ; ( 1.53375,-0.2 ) node ; ( 3.0675,-0.2 ) node ; ( 4.60125,-0.2 ) node ; ( 6.135,-0.2 ) node ; ( 3.0675,-0.75 ) node @xmath125 : distance between ellipsoid centers ; ( -0.15,0 ) node ; ( -0.2,1.5 ) node ; ( -0.2,3 ) node ; ( -0.2,4.5 ) node ; ( -0.2,6 ) node ; ( -0.75,3 ) node [ rotate=90 ] @xmath12 : rank of random projection ;    &    ( plot ) at ( 3.07,3 )   phase transition for a random projection to keep ellipsoids separated . (",
    "a ) fixing the ambient dimension to be @xmath114 , then for each @xmath115 and @xmath116 , we conducted @xmath117 trials . for each trial , we randomly drew @xmath118 and @xmath119 as iid standard wishart - distributed @xmath84 matrices with @xmath92 degrees of freedom ( i.e. , @xmath120 , where @xmath67 is @xmath84 with iid @xmath25 entries ) , along with an @xmath13 matrix @xmath14 with iid @xmath25 entries .",
    "plotted is the proportion of trials for which the ellipsoids are disjoint after applying @xmath14 ( we did not record whether the ellipsoids were separated before projection ) . for each of the 160,000 trials , the shape matrices satisfied @xmath121 , thereby rendering theorem  [ theorem.two ellipsoids ] irrelevant .",
    "( b ) next , we performed the same experiment , except we changed the distribution of @xmath118 and @xmath119 so that @xmath100 is in the null space of both , and in the orthogonal complement of @xmath100 , they are iid standard wishart - distributed @xmath122 matrices with @xmath123 degrees of freedom . as such , the corresponding ellipsoids resided in parallel hyperplanes , and @xmath124 so that theorem  [ theorem.two ellipsoids ] applies . for each trial , we stored the bound on @xmath34 from theorem  [ theorem.two ellipsoids ] and calculated the sample average of the squares of these bounds corresponding to each @xmath115 .",
    "the red curve plots these sample averages ( or 40 , whichever is smaller)think of this as an upper bound on the phase transition .",
    "as one might expect , this bound appears to sharpen as the distance increases .",
    ", title=\"fig:\",height=226 ] ; ( 0,0 )  ( 6.135,0 ) ; ( 0,0 )  ( 0,6 ) ; ( 1.53375,0 )  ( 1.53375,0.1 ) ; ( 3.0675,0 )  ( 3.0675,0.1 ) ; ( 4.60125,0 )  ( 4.60125,0.1 ) ; ( 6.135,0 )  ( 6.135,0.1 ) ; ( 0,1.5 )  ( 0.1,1.5 ) ; ( 0,3 )  ( 0.1,3 ) ; ( 0,4.5 )  ( 0.1,4.5 ) ; ( 0,6 )  ( 0.1,6 ) ; ( 0,-0.2 ) node ; ( 1.53375,-0.2 ) node ; ( 3.0675,-0.2 ) node ; ( 4.60125,-0.2 ) node ; ( 6.135,-0.2 ) node ; ( 3.0675,-0.75 ) node @xmath125 : distance between ellipsoid centers ; ( -0.15,0 ) node ; ( -0.2,1.5 ) node ; ( -0.2,3 ) node ; ( -0.2,4.5 ) node ; ( -0.2,6 ) node ;      various classification tasks require one to distinguish between several different classes , and so one might ask for a random projection to maintain pairwise linear separability . for",
    "a fixed projection dimension @xmath12 , let @xmath126 denote the probability that convex classes @xmath127 and @xmath128 collide after projection . then the union bound gives that the probability of maintaining separation is @xmath129 .    this use of the union bound helps to illustrate the freedom which comes with a random projection .",
    "recall that theorem  [ theorem.two ellipsoids ] requires that projecting the ellipsoids onto the line spanned by the difference @xmath130 of their centers maintains separation . in the case of multiple ellipsoids , one",
    "might then be inclined to project onto the span of @xmath131 .",
    "generically , such a choice of projection puts @xmath132 , where @xmath26 is the total number of classes . on the other hand , suppose each pairwise distance @xmath133 is so large that the @xmath134th gaussian width satisfies @xmath135 then by corollary  [ corollary.gordon ] , taking @xmath136 ensures that classes @xmath127 and @xmath128 collide after projection with probability @xmath137 , and so the probability of maintaining overall separation is @xmath138 .",
    "of course , we will not save so much in the projection dimension when the convex bodies are closer to each other , but we certainly expect @xmath139 in reasonable cases .    at this point , we note the similarity between the performance @xmath140 and what the johnson  lindenstrauss lemma guarantees when the classes are each a single point .",
    "indeed , a random projection of @xmath141 dimensions suffices to ensure that pairwise distances are preserved to within a factor of @xmath142 with constant probability ; this in turn ensures that pairwise separated points remain pairwise separated after projection .",
    "in fact , the proof technique for the johnson ",
    "lindenstrauss lemma is similar : first prove that a random projection typically preserves the norm of any vector , and then perform a union bound over all @xmath143 difference vectors .",
    "one might be inspired to use johnson  lindenstrauss ideas to prove a result analogous to theorem  [ theorem.two ellipsoids ] ( this was actually an initial attempt by the authors ) .",
    "unfortunately , since johnson  lindenstrauss does not account for the shape matrices @xmath86 of the ellipsoids , one is inclined to consider worst - case orientations , and so terms like @xmath144 are replaced by spectral norms @xmath145 in the analysis , thereby producing a strictly weaker result .",
    "dasgupta  @xcite uses this johnson ",
    "lindenstrauss approach to project a mixture of gaussians while maintaining some notion of separation .",
    "in this section , we compare the performance of random projection and principal component analysis ( pca ) for dimensionality reduction .",
    "first , we should briefly review how to perform pca .",
    "consider a collection of data points @xmath146 , and define the empirical mean by @xmath147 .",
    "next , consider the empirical inertia matrix @xmath148 the eigenvectors of @xmath149 with the largest eigenvalues are identified as the _ principal components _ , and the idea of pca is to project @xmath150 onto the span of these components for dimensionality reduction .    in this section , we will compare random projection with pca in a couple of ways .",
    "first , we observe some toy examples of data sets that illustrate when pca is better , and when random projection is better .",
    "later , we make a comparison using a real - world hyperspectral data set .      here",
    ", we consider a couple of extreme data sets which illustrate when pca outperforms random projection and vice versa",
    ". our overarching model for the data sets will be the following : given a collection of disjoint balls @xmath151 in @xmath5 , we independently draw @xmath152 data points uniformly from @xmath153 .",
    "when @xmath152 is large , we can expect @xmath149 to be very close to @xmath154 by the law of large numbers ; here , @xmath155 denotes the mean of the distribution .",
    "recall that the projection dimension for pca is the number of large eigenvalues of @xmath149 .",
    "since the operator spectrum is a continuous function of the operator , we can count large eigenvalues of @xmath156 to estimate this projection dimension .",
    "the following lemma will be useful to this end :    [ lemma.pca ] consider a ball of the form @xmath157 , where @xmath158 denotes the ball centered at @xmath49 of radius @xmath50 .",
    "define the operator @xmath159 then the span of @xmath160 and its orthogonal complement form the eigenspaces of @xmath161 with eigenvalues @xmath162 respectively , where @xmath163 is some constant depending on @xmath92 .",
    "pick any vector @xmath164 of unit norm . then",
    "@xmath165 notice that the operator @xmath166 is invariant under conjugation by any rotation matrix .",
    "as such , this operator is a constant @xmath163 multiple of the identity operator .",
    "thus , @xmath167 is maximized at @xmath168 when @xmath169 is a normalized version of @xmath160 , and minimized at @xmath170 whenever @xmath169 is orthogonal to @xmath160 .",
    "we start by considering the case where @xmath23 is composed of two balls , namely @xmath171 and @xmath172 .",
    "as far as random projection is concerned , in this case , we are very familiar with the required projection dimension : @xmath173 . in particular , as @xmath174 approaches @xmath175",
    ", a random projection can not provide much dimensionality reduction . to compare with pca , note that in this case",
    ", @xmath156 is a scalar multiple of @xmath176 , where @xmath177 moreover , it is easy to show that @xmath178 . by lemma  [ lemma.pca ] , the dominant eigenvector of @xmath179 is @xmath160 , and so pca would suggest to project onto the one - dimensional subspace spanned by @xmath160 .",
    "indeed , this projection always preserves separation , and so in this case , pca provides a remarkable savings in projection dimension .",
    "now consider the case where @xmath23 is composed of @xmath180 balls @xmath181 defined by @xmath182 and @xmath183 , where @xmath184 denotes the @xmath185th identity basis element .",
    "then @xmath156 is a scalar multiple of @xmath186 , where @xmath187 recall that @xmath188 .",
    "then @xmath156 is simply a scalar multiple of @xmath189 . by lemma  [ lemma.pca ] ,",
    "the @xmath190 s are all diagonal , and their diagonals are translates of each other .",
    "as such , their sum ( and therefore @xmath156 ) is a scalar multiple of the identity matrix  in this case , pca would choose to not project down to fewer dimensions .",
    "on the other hand , if we take @xmath191 then by corollary  [ corollary.gordon ] , a random projection maintains separation between any fixed pair of balls from @xmath181 with probability @xmath192 , and so by the union bound , the balls are pairwise separated with probability @xmath193 . in particular , if @xmath194 , then we can take @xmath195 .",
    "overall , random projection performs poorly when the classes are close , but when there are multiple sufficiently separated classes , you can expect a dramatic dimensionality reduction . as for pca",
    ", we have constructed a toy example for which pca performs well ( the case of two balls ) , but in general , the performance of pca seems difficult to describe theoretically .",
    "whereas the performance of random projection can be expressed in terms of `` local '' conditions ( e.g. , pairwise separation ) , as the last example illustrates , the performance of pca can be dictated by more `` global '' conditions ( e.g. , the geometric configuration of classes ) . in the absence of theoretical guarantees for pca",
    ", the following subsection provides simulations with real - world hyperspectral data to illustrate its performance compared to random projection .",
    "one specific application of dimensionality reduction is the classification of hyperspectral data . for this application ,",
    "the idea is to distinguish materials by observing them across hundreds of spectral bands ( like the red , green and blue bands that the human eye detects ) .",
    "each pixel of a hyperspectral image can be viewed as a vector of spectral information , capturing how much light of various frequencies is being reradiated from that portion of the scene . a hyperspectral image is naturally represented as a data cube with two spatial indices and one spectral index , and a common task is to identify the material observed at each pair of spatial indices .",
    "to do this , one might apply _ per - pixel classification _ , in which a classifier simply identifies the material in a given pixel from its spectral content , ignoring any spatial context .",
    "since the spectral information is high - dimensional , it is natural to attempt dimensionality reduction before classification .",
    "a popular choice for this task is pca  @xcite , and in this subsection , we provide some preliminary simulations to compare its performance with random projection .",
    "all experiments described in this subsection were conducted using the indian pines hyperspectral data set  @xcite .",
    "this data set consists of a hyperspectral image with @xmath196 pixels , each containing spectral reflectance data represented by a vector of length @xmath197 .",
    "each pixel corresponds to a particular type of vegetation or crop , such as corn or wheat , with a total of @xmath198 different classes ( see figure  [ figureindianpines ] for an illustration ) .",
    "the indian pines hyperspectral data set  @xcite .",
    "each pixel corresponds to a different type of vegetation or crop .",
    "the ground truth image of labels is depicted on the left , and a sample band of the data set is displayed on the right .",
    ", scaledwidth=70.0% ]    for our simulations , the task will consist of using the known labels of a training set ( a small subset of the @xmath199 pixels ) to make accurate predictions for the remainder of the pixels . to keep the simulations fast",
    ", each simulation considers a small patch of pixels .",
    "more precisely , given a patch of @xmath152 pixels and a prescribed training ratio @xmath175 , we pick a random subset of the pixels of size @xmath200 to be the training set .",
    "we use the labels from this training set to train a classifier that will then attempt to guess the label of each of the other @xmath201 pixels from the location of its spectral reflectance in @xmath202-dimensional space .",
    "the classifier we use is matlab s built - in implementation of multinomial logistic regression .",
    "performance is measured by classification error and runtime .    given this setting , for different values of projection dimension @xmath12",
    ", we draw an @xmath13 matrix @xmath14 with iid @xmath25 entries and replace every spectral reflectance data point @xmath101 by @xmath203 . in the degenerate case @xmath204 , we simply take @xmath14 to be the identity matrix . for comparison , we also use principal component analysis ( pca ) for dimensionality reduction , which will interrogate the training set to identify @xmath12 principal components before projecting the data set onto the span of these components . an immediate advantage of random projection",
    "is that it allows the sensing mechanism to blindly compress the data , as it does not need a training set to determine the compression function .",
    "figure  [ figureindianpines2_trainningratio0.5and0.2 ] uses different patches of the indian pines dataset and different training ratios to compare both the classification accuracy and runtime of multinomial logistic regression when applied to various projections of the data set .",
    "the first experiment focuses on a small patch of @xmath205 pixels , and the second considers a patch of @xmath206 pixels .",
    "these experiments reveal a few interesting phenomena .",
    "first of all , dimensionality reduction leads to impressive speedups in runtime .",
    "perhaps more surprising is the fact that there seems to be an improvement in classification performance after projecting the data .",
    "we are far from completely understanding this behavior , but we suspect it has to do with regularization and overfitting .     the performance of classification by multinomial logistic regression after projecting onto subspaces of various dimensions @xmath12 .",
    "depicted are two particular patches of the entire indian pines data set  the top uses a patch of @xmath205 pixels , while the bottom uses a patch of @xmath206 pixels . in each case , the first two plots in the first row depict the ground truth labels in the patch , as well as the random training set we selected .",
    "the third plot compares , for different values of projection dimension @xmath12 , the classification error incurred with random projection and with principal component analysis .",
    "the fourth plot shows the runtime ( in seconds ) for different values of @xmath12 .",
    "the second and third rows depict the classification outcomes when using random projection and pca , respectively .",
    "one can see that dimensionality reduction not only speeds up the algorithm , but also improves the classification performance by discouraging overfitting .",
    ", title=\"fig:\",scaledwidth=80.0% ]   the performance of classification by multinomial logistic regression after projecting onto subspaces of various dimensions @xmath12 . depicted",
    "are two particular patches of the entire indian pines data set  the top uses a patch of @xmath205 pixels , while the bottom uses a patch of @xmath206 pixels . in each case , the first two plots in the first row depict the ground truth labels in the patch , as well as the random training set we selected .",
    "the third plot compares , for different values of projection dimension @xmath12 , the classification error incurred with random projection and with principal component analysis .",
    "the fourth plot shows the runtime ( in seconds ) for different values of @xmath12 .",
    "the second and third rows depict the classification outcomes when using random projection and pca , respectively .",
    "one can see that dimensionality reduction not only speeds up the algorithm , but also improves the classification performance by discouraging overfitting .",
    ", title=\"fig:\",scaledwidth=80.0% ]    it is also interesting how similar random projection and pca perform .",
    "note that the pca method has an unfair advantage since it is data - adaptive , meaning that it uses the training data to select the projection , and in practical applications for which the sensing process is expensive , one might be interested in projecting in a non - adaptive way , thereby allowing for less sensing .",
    "our simulations suggest that the expense is unnecessary , as a random projection will provide almost identical performance .",
    "as indicated in the previous subsection , random projection is also better understood as a means to maintain linear separability , and so there seems to be little benefit in choosing pca over random projection ( at least for this sort of classification task ) .",
    "one of the main points of this paper is that random projections can maintain separation between sufficiently separated sets , and this is useful for classification in the projected domain . given the mindset of compressed sensing , it is impressive that the sets need not be low - dimensional to enjoy separation in the projected domain .",
    "what this suggests is a more general notion of simplicity that is at play , of which low - dimensionality and sufficient separation are mere instances . obviously , understanding this general notion is a worthy subject of future work .    from a more applied perspective",
    ", it would be worth investigating alternative notions of distortion .",
    "indeed , linear separability is the best - case scenario for classification , but it is not at all necessary . after identifying any worthy notion of distortion , one might study how much distortion is incurred by random projection , and hopefully some of the ideas contained in this paper will help .",
    "one of our main results ( theorem  [ theorem.two ellipsoids ] ) provides a sufficient number of rows for a random projection to maintain separation between ellipsoids .",
    "however , as illustrated in figure  [ figure_ellipsoids_phasetransition ] , this bound is far from optimal .",
    "considering this case of two ellipsoids is somewhat representative of the more general case of two convex sets ( as we identified using theorem  [ theorem.concentration of volume ] ) , improvements to theorem  [ theorem.two ellipsoids ] would be rather interesting . in particular , it would be nice to characterize the phase transition in terms of the ellipsoids parameters , as we already have in the case of two balls .",
    "finally , the random projections we consider here all have iid @xmath25 entries , but real - world sensing systems may not enjoy this sort of flexibility .",
    "as such , it would be interesting to extend the results of this paper to more general classes of random projections , in particular , random projections which can be implemented with a hyperspectral imager ( say ) .",
    "let @xmath23 be a closed subset of @xmath207 .",
    "draw an @xmath13 matrix @xmath14 with iid @xmath25 entries .",
    "then @xmath208 \\geq\\lambda_m - w(s),\\ ] ] where @xmath29 , and @xmath28 is a random @xmath12-dimensional vector with iid @xmath25 entries .    to prove the escape theorem ,",
    "consider the function @xmath209 gordon s comparison theorem gives that @xmath210\\geq\\lambda_m - w(s)$ ] , and so @xmath211-\\big(\\lambda_m - w(s)\\big)\\big).\\end{aligned}\\ ] ] next , we note that @xmath212 is lipschitz with respect to the frobenius norm with constant @xmath50 , and so we can appeal to  ( 1.6 ) of  @xcite to get @xmath213-t\\big)\\geq 1-e^{-t^2/2 } \\qquad \\forall t>0.\\ ] ] taking @xmath214 and applying to then gives the result .",
    "we begin by finding the smallest @xmath217 $ ] for which @xmath218 . by the definition of @xmath219 , this containment is equivalent to @xmath220 to find the smallest such @xmath63 , we solve this optimization problem .",
    "taking @xmath221 , then @xmath222 , and so we seek to @xmath223 quickly note that the objective function is well defined over the feasibility region due to the assumption @xmath224 .",
    "we first claim that @xmath225 is minimized on the boundary , i.e. , where @xmath226 . to see this ,",
    "suppose @xmath227 , and letting @xmath228 denote the orthogonal projection onto the orthogonal complement of the span of @xmath229 , take @xmath230 such that @xmath231 .",
    "then @xmath232 lies on the boundary and @xmath233 as such , it suffices to minimize subject to @xmath226 . at this point ,",
    "the theory of lagrange multipliers can be applied since the equality constraint @xmath234 is a level set of a function whose gradient @xmath235 does not vanish on the level set .",
    "thus , the minimizers of @xmath236 with @xmath237 satisfy @xmath238 for some lagrange multiplier @xmath239 .    to continue",
    ", we calculate @xmath240 .",
    "it is actually easier to calculate the gradient of @xmath241 : @xmath242 note that",
    "@xmath243 only if @xmath244 is a nontrivial multiple of @xmath229 , i.e. , only if @xmath244 maximizes @xmath245 ( by cauchy  schwarz ) . also , it is easy to verify that @xmath246 .",
    "overall , changing variables @xmath247 gives that any minimizer @xmath248 of @xmath236 subject to @xmath226 satisfies @xmath249 at this point , and together imply that @xmath250 is a nontrivial multiple of @xmath248 , and so combining with gives @xmath251 as such , @xmath49 , @xmath229 and @xmath252 form vertices of a right triangle with hypotenuse @xmath253 , and the smallest @xmath63 satisfying is the angle between @xmath229 and @xmath252 .",
    "thus , @xmath254 .",
    "it remains to prove the reverse containment , @xmath255 , for this particular choice of @xmath63 .",
    "define @xmath256 then @xmath219 is the cone generated by @xmath257 , and so it suffices to show that @xmath258 . to this end , pick any @xmath259 , and consider the triangle with vertices @xmath49 , @xmath229 and @xmath260 . by definition ,",
    "the angle between @xmath229 and @xmath260 is @xmath63 , and the side @xmath260 has length @xmath261 . as such , by the side - angle - side postulate , this triangle is congruent to the triangle with vertices at @xmath49 , @xmath229 and @xmath252 .",
    "this implies that the side between @xmath260 and @xmath229 has length @xmath262 , and so @xmath263 , as desired .",
    "let @xmath266 be the singular value decomposition of @xmath21 .",
    "since the gaussian is isotropic , @xmath267 , and since the function @xmath268 is convex , jensen s inequality gives @xmath269 similarly , since @xmath270 is convex , we can also use jensen s inequality to get @xmath271 which completes the proof .    to prove theorem  [ theorem.two ellipsoids ] , let @xmath215 denote the cone generated by the minkowski difference @xmath42 .",
    "we will exploit proposition  [ proposition 3.6 ] , which gives the following estimate in terms of the polar cone @xmath272 : @xmath273,\\ ] ] where @xmath28 has iid @xmath25 entries and @xmath274 denotes the euclidean projection onto @xmath275 .",
    "instead of directly computing the distance between @xmath28 and its projection onto @xmath275 , we will construct a mapping @xmath96 which sends @xmath28 to some member of @xmath275 , but for which distances are easier compute ; indeed @xmath97 will be an upper bound on @xmath276 . consider the polar decomposition @xmath277 , where @xmath278 .",
    "then we can decompose @xmath279 , and we define @xmath280 to be the point in @xmath275 of the form @xmath281 which is closest to @xmath28 . with this definition",
    ", we have @xmath282 to simplify this constraint , we find a convenient representation of the polar cone : @xmath283 where the last step uses the fact that each @xmath86 is symmetric .",
    "the constraint @xmath284 is then equivalent to @xmath285 at this point , we focus on the case in which the projection @xmath286 is disjoint from @xmath287 . in this case",
    ", we have the following strict inequality : @xmath288 and rearranging then gives @xmath289 as such , taking @xmath290 produces a point @xmath291 , considering @xmath292 where the last step follows from the triangle inequality . note that if @xmath293 , then we can take @xmath294 to get @xmath295",
    "otherwise , @xmath296 .",
    "overall , we have @xmath297 by the monotonicity of expectation , we then have @xmath298 \\leq\\mathbb{e}_g(\\alpha^*-g_1)_+ = \\mathbb{e}_{g_2}\\big[\\mathbb{e}_{g_1}\\big[(\\alpha^*-g_1)_+\\big|g_2\\big]\\big].\\ ] ] to estimate the right - hand side , we first have @xmath299 = \\int_{-\\infty}^\\infty(\\alpha^*-z)_+d\\phi(z ) = \\alpha^*\\phi(\\alpha^*)+\\frac{1}{\\sqrt{2\\pi}}e^{-(\\alpha^*)^2/2},\\ ] ] which lies between @xmath300 and @xmath301 since @xmath302 .",
    "let @xmath303 denote the @xmath264 orthogonal projection onto the orthogonal complement of the span of @xmath100 .",
    "appealing to lemma  [ lemma.applying jensens twice ] with @xmath304 then gives @xmath305 where the last inequality follows from the fact that each row of @xmath306 is a projection of the corresponding row in @xmath86 , and therefore has a smaller @xmath109-norm .",
    "considering , this implies @xmath307 which combined with and then gives @xmath308\\big ] \\leq\\frac{\\|a_1\\|_f+\\|a_2\\|_f}{\\zeta-\\big(\\|a_1e\\|_2+\\|a_2e\\|_2\\big)}+\\frac{1}{\\sqrt{2\\pi}}.\\ ] ]      the authors thank matthew fickus and katya scheinberg for insightful discussions .",
    "a.  s.  bandeira was supported by afosr award fa9550 - 12 - 1 - 0317 , d.  g.  mixon was supported by nsf award dms-1321779 , and b.  recht was supported by onr award n00014 - 11 - 1 - 0723 and nsf awards ccf-1139953 and ccf-11482 .",
    "the views expressed in this article are those of the authors and do not reflect the official policy or position of the united states air force , department of defense , or the u.s .",
    "government .",
    "e.  abbe , n.  alon , a.  s.  bandeira , linear boolean classification , coding and `` the critical problem '' , ieee international symposium on information theory ( isit 2014 ) , to appear .",
    "d.  amelunxen , m.  lotz , m.  b.  mccoy , j.  a.  tropp , living on the edge : a geometric theory of phase transitions in convex optimization , available online : arxiv:1303.6672                      y.  gordon , on milman s inequality and random subspaces which escape through a mesh in @xmath309 , geometric aspects of functional analysis , israel seminar 198687 , lecture notes in mathematics 1317 ( 1988 ) 84106 .",
    "r.  j.  johnson , improved feature extraction , feature selection , and identification techniques that create a fast unsupervised hyperspectral target detection algorithm , master s thesis , air force institute of technology , 2008 .",
    "w.  b.  johnson , j.  lindenstrauss , extensions of lipschitz mappings into a hilbert space , conference in modern analysis and probability ( new haven , conn . , 1982 ) , contemporary mathematics 26 , providence , ri : american mathematical society , 1984 , pp ."
  ],
  "abstract_text": [
    "<S> this paper addresses the fundamental question of when convex sets remain disjoint after random projection . </S>",
    "<S> we provide an analysis using ideas from high - dimensional convex geometry . for ellipsoids , </S>",
    "<S> we provide a bound in terms of the distance between these ellipsoids and simple functions of their polynomial coefficients . as an application </S>",
    "<S> , this theorem provides bounds for compressive classification of convex sets . rather than assuming that the data to be classified is sparse , </S>",
    "<S> our results show that the data can be acquired via very few measurements yet will remain linearly separable . </S>",
    "<S> we demonstrate the feasibility of this approach in the context of hyperspectral imaging . </S>"
  ]
}