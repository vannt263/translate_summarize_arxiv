{
  "article_text": [
    "whether for function approximation , classification , or density estimation , probabilistic models relying on random fields have been increasingly used in recent works from various research communities . finding their roots in geostatistics and spatial statistics with optimal linear prediction and kriging @xcite , random field models for prediction have become a main stream topic in machine learning ( under the _ gaussian process regression _ terminology , see , e.g. , @xcite ) , with a spectrum ranging from metamodeling and adaptive design approaches for time - consuming simulations in science and engineering @xcite ) to theoretical bayesian statistics in function spaces ( see @xcite and references therein ) . often , a gaussian random field model is assumed for the function of interest , and so all prior assumptions on this function are incorporated through the corresponding mean function and covariance kernel . here we focus on random field models for which the covariance kernel exists , and we discuss some mathematical properties of associated realisations ( or _ paths _ ) depending on the kernel , both in the gaussian case and in a more general second - order framework .",
    "a number of well - known random field properties driven by the covariance kernel ( say in the centred case ) are in the mean square sense @xcite , e.g. @xmath0 continuity and differentiability @xcite .",
    "such results are quite general in the sense that they hold in a variety of cases ( gaussian or not ) , but they generally are nt informative about the pathwise beahviour of underlying random fields . in the gaussian case , however , much can be said about path regularity properties of stationary random field paths ( cf .",
    "classical results in @xcite and subsequent works ) based on the behaviour of the covariance kernel in the neighbourhood of the origin . likewise , for non - stationary gaussian fields , results connecting path regularity to kernel properties can be found in @xcite .",
    "more recently , @xcite studied path regularity of second - order random fields , and could draw conclusions about a.s .",
    "continuous differentiability in non - gaussian settings .",
    "also , we refer to the thesis @xcite for an enlightening exposition of state - of - the - art results concerning regularity properties of random field sample paths in various frameworks .    in a different settings , links between invariances of kernels under operations like translations and rotations ( that is to say , the notions of _ stationarity _ and _ isotropy _ @xcite ) and invariances in distribution of the corresponding random fields have been covered extensively in spatial statistics and throughout the literature of probability theory @xcite . however , such properties are to be understood _ in distribution _ , and do not directly concern random field paths .",
    "our main focus in the present work is on _ pathwise _ algebraic and geometric properties of random fields , such as invariances under group actions or sparse function decompositions of multivariate paths .",
    "we first establish in a quite general framework , that for a centred random field @xmath1 possessing a covariance ( i.e. such that the variance is finite at any location in the index space @xmath2 ) , @xmath3 has paths invariant under @xmath4 with probability @xmath5 if and only if @xmath6 , where @xmath4 belongs to the class of linear combinations of composition operators . the presented results generalise @xcite , where random fields with paths invariant under the action of a finite group were studied . here",
    "we also extend recent works on additive kernels for high - dimensional kriging @xcite , and we provide a simple characterization of the class of kernels leading to squared - integrable centred random fields with additive paths .",
    "furthermore , in the particular case of gaussian random fields , a more general class of invariances can be covered through the link between operators on the paths and operators on the _ reproducing kernel hilbert space _ @xcite associated with the random field .",
    "section @xmath7 presents a general result characterizing path invariance in terms of argumentwise invariance of covariance kernels , in the case of combinations of composition operators . in section @xmath8",
    ", we discuss how the gaussianity assumption enables extending the results of section @xmath7 to more general operators .",
    "the obtained results are applied to gaussian process regression in section @xmath9 where the potential of argumentwise invariant kernels is demonstrated through various examples .",
    "section @xmath10 is dedicated to an overall conclusion of the article , and a discussion on some research perspectives .",
    "designing kernels imposing some structural constraints on the associated random field models is of interest in various situations .",
    "one of those situations is the high - dimensional function approximation framework , where simplifying assumptions are needed in order to guarantee a reasonable inference despite the curse of dimensionality .",
    "following its successful use in multidimensional nonparametric smoothing @xcite , the _",
    "additivity assumption _ has become a very popular simplifying assumption for dealing with high - dimensional problems , and has recently inspiring further work in mathematical statistics @xcite .    a class of kernels leading to random fields with additive paths , in the sense detailed below ( see also @xcite ) ,",
    "has recently been considered in @xcite .",
    "calling a function @xmath11 ( with multidimensional source space @xmath12 where @xmath13 ) additive when there exists @xmath14 such that @xmath15 , it was indeed shown in @xcite that    [ propadd ] if a random field @xmath3 possesses a kernel of the form @xmath16 where the @xmath17 s are arbitrary positive definite kernels over the @xmath18 s , then @xmath3 is additive up to a modification , i.e. there exists a random field @xmath19 which paths are additive functions such that @xmath20 . [ prop1 ]",
    "one may wonder whether kernels of the form @xmath21 are the only ones giving birth to centred random fields with additive paths .",
    "the answer to this question turns out to be negative , as will be established in corollary [ additive_dependent ] .    in a similar fashion",
    ", @xcite gives a characterization of kernels which associated centred random fields have their paths invariant under a finite group action .",
    "let @xmath22 be a finite group acting on d via a measurable action @xmath23",
    "@xmath3 has invariant paths under @xmath24 ( _ up to a modification _ ) if and only if @xmath25 is argumentwise invariant : @xmath26 .",
    "[ prop2 ]    we show in proposition [ prop : kernelprocess ] that both propositions [ prop1 ] and [ prop2 ] are sub - cases of a general result on square - integrable random fields invariant under the class of _ combination of composition operators _ ( see definition  [ def : comofcom ] ) .",
    "a characterization of kernels leading to random fields possessing additive paths is given in corollary [ additive_dependent ] , and it is then shown that having the form of eq .",
    "[ eq1_add_kernels ] is not necessary .",
    "another by - product of proposition [ prop : kernelprocess ] is a new proof of proposition  [ prop2 ] relying on a particular class of combination of composition operators , as illustrated in example  [ ex_cco_groupinvariance ] .",
    "let us now introduce the set up of composition operators ( see , e.g. , @xcite ) and their combinations .",
    "let us consider an arbitrary function @xmath27 .",
    "the _ composition operator @xmath28 with symbol @xmath29 _ is defined as follows : @xmath30    such operators can be naturally extended to random fields indexed by @xmath2 : @xmath31    we call _ combination of composition operators with symbols @xmath32 and weights @xmath33 _ the operator @xmath34 [ def : comofcom ]      [ prop : kernelprocess ] let @xmath3 be a square - integrable centred random field with covariance kernel @xmath25 .",
    "then @xmath3 equals @xmath35 up to a modification , i.e. @xmath36 if and only if @xmath25 is @xmath4-invariant , i.e. @xmath37    @xmath38 : let us fix arbitrary @xmath39 .",
    "since @xmath40 is a modification of @xmath41 , we have @xmath42 , and so : @xmath43 @xmath44 : using @xmath45 , we get @xmath46 , so @xmath47 .",
    "since @xmath3 is centred , so is @xmath35 , and hence @xmath48 .    as noted in @xcite , two processes modifications of each other",
    "that are almost surely continuous are indistinguishable .",
    "almost sure results may then directly be obtained for processes with almost surely continuous paths .",
    "[ prop2 ] now appears as a special case of prop .",
    "[ prop : kernelprocess ] with @xmath49 where @xmath50 .",
    "for instance , let us consider the following functions over @xmath51 ^ 2 \\times [ -1,1]^2 $ ] : @xmath52 where @xmath53 ( resp .",
    "@xmath54 ) are the polar coordinates of @xmath55 ( resp .",
    "@xmath56 ) and where @xmath57 .",
    "@xmath58 and @xmath59 are positive definite kernels ( in the loose sense ) as admissible covariances ( respectively those of the brownian sheet and the brownian motion ) composed with a change of index , i.e. @xmath60 with @xmath61 from @xmath51 ^ 2 $ ] onto @xmath62 ^ 2 $ ] ( resp . from @xmath63 onto @xmath64 ) . by construction @xmath65 and @xmath66",
    "are argumentwise invariant with respect to rotations around the origin ( with angles multiple of @xmath67 for @xmath65 ) .",
    "as illustrated in figure  [ fig : ex_brown_rot ] , proposition  [ prop : kernelprocess ] ensures that the sample paths of centred ( gaussian or non - gaussian ) random fields with these kernels inherit their invariance properties . note that @xmath58 belongs to the class of kernels argumentwise invariant under the action of a finite group treated above , while the argumentwise invariance of @xmath59 under the action of an infinite group can actually be seen as invariance under any composition operator with symbol of the form @xmath68 where @xmath69 is an arbitrary point on the unit circle .",
    "0.45     0.45     [ ex_cco_groupinvariance ]    beyond group - invariance , proposition  [ prop : kernelprocess ] also has implications concerning the sparsity of multivariate random field paths , as detailed in the following section on additivity , leading to a generalization of proposition  [ propadd ] .",
    "let us first show how the additivity property boils down to an invariance property under some specific class of combination of composition operators .",
    "[ rmk : additivefun ] assuming @xmath70 , a function @xmath71 is additive if and only if @xmath72 is invariant under the following combination of composition operators : @xmath73 where @xmath74 , and @xmath75 .    a centred random field @xmath3 possessing a covariance kernel @xmath25 has additive paths ( up to a modification ) if and only if @xmath25 is a positive definite kernel of the form @xmath76    if @xmath3 has additive paths up to a modification , there exists a random field @xmath77 with additive paths such that @xmath78 , and so @xmath3 and @xmath19 have the same covariance kernel",
    "now , @xmath19 having additive paths , remark [ rmk : additivefun ] implies that @xmath79 , where @xmath80 , so equation [ additive_dependent ] holds with @xmath81 .",
    "reciprocally , from proposition @xmath8 , we know that it suffices for @xmath3 to have additive paths that @xmath82 is additive @xmath83 . for a kernel @xmath25 such as in eq .",
    "[ additive_dependent ] and an arbitrary @xmath84 , setting @xmath85 we get @xmath86 , so @xmath82 is additive .",
    "let us consider the following kernel over @xmath87 : @xmath88 where the @xmath89 are smoothing kernels over @xmath90 .",
    "previous results on vector - valued random fields ensure that @xmath25 is a valid covariance function @xcite .",
    "furthermore , the structure of @xmath25 corresponds to an additive kernel in the sense of equation  [ additive_dependent ] . according to corollary  [ additive_dependent ] , a random field with such kernel has additive paths ( up to a modification ) , with univariate marginals exhibiting possible cross - correlations .",
    "composition operators constitute a remarkable class of linear maps since they can be defined on function spaces without any restriction . in particular , they similarly apply to random field paths or to kernel functions ( with one argument fixed ) , so that taking out of a covariance a ( combination of ) composition(s ) applied to a random field and turning it into a ( combination of ) composition(s ) on the covariance kernel appears as a natural operation .    for more general classes of operators ,",
    "however , operators on paths and operators on kernels are two different mathematical objects : it is a priori not obvious how to transform operators on paths into operators on the kernel , and even less straightforward to know when and how it is possible to define an operator on paths corresponding to a given operator on the kernel space .    given a linear operator @xmath91 and a second - order centred process @xmath3 such that @xmath35 is second order , generalizing the approach that lead to prop .",
    "[ prop : kernelprocess ] enables us to characterize pathwise invariances of @xmath3 by @xmath4 relying on second - order properties of the joint process @xmath92 , without any additional assumption concerning @xmath3 s probability distribution :    @xmath93 up to a modification if and only if @xmath94    under the square - integrability and zero - mean hypotheses on @xmath3 and @xmath35 , @xmath95 is equivalent to @xmath96 .    in the particular case of combinations of composition operators covered by prop .",
    "[ prop : kernelprocess ] , it was possible to take @xmath4 out of the covariance and variance in the right hand side of eq .",
    "[ eq_toto ] , leading to a further equivalence between pathwise invariance of @xmath3 and invariance of @xmath25 under @xmath4 . in greater generality , however , it is not straightforward how @xmath4 can be taken out of terms such as @xmath97 .",
    "we show in section  [ subsec : rkhs ] below that in case @xmath3 is gaussian and @xmath4 satisfies some technical condition with respect to @xmath3 , there exists an operator @xmath98 defined over the reproducing kernel hilbert space associated with @xmath3 , such that @xmath99 , for all @xmath100 .",
    "this construction based on the celebrated _ love isometry _",
    "@xcite then enables us extending prop .",
    "[ prop : kernelprocess ] to a broader class of operators .",
    "numerical examples are presented throughout the current section , including simulated paths of gaussian random fields with argumentwise invariant kernels under various ( integral and differential ) operators , that subsequently serve as a basis to original applications in gaussian process regression , presented in section [ sec : gpr ] .",
    "we focus here on a centred gaussian random field @xmath101 defined over a compact set @xmath102 , with covariance kernel @xmath103 .",
    "@xmath25 is here assumed continuous , so that the paths of @xmath101 belong to some subspace of the space @xmath104 of continuous functions over @xmath2 , and are in particular square - integrable ( with respect to lebesgue s measure on @xmath2 , say ) by compacity of @xmath2 .",
    "let us further consider a linear map @xmath105 acting on the paths of @xmath3 and such that @xmath41 is centred and square - integrable for all @xmath106 .    in proposition",
    "[ prop : tronde ] below , the so - called _ love isometry _ @xcite allows us to define an operator , derived from @xmath4 , acting on the reproducing kernel hilbert space ( rkhs ) associated with @xmath25 .",
    "let us first recall some useful definitions and the isometry in question .",
    "the rkhs @xmath107 associated with @xmath25 @xcite can be defined as functional completion of the function space spanned by the @xmath108 s : @xmath109 equipped with the scalar product defined by @xmath110 .",
    "a crucial state - of - the - art result is that @xmath107 is isometric to the hilbert space generated by the random field @xmath3 @xcite : @xmath111 where the adherence is taken with respect to the usual @xmath112 topology on the space of ( equivalence classes of ) square - integrable random variables .",
    "( love isometry ) [ loeve ] the map @xmath113 defined by : @xmath114 for all @xmath115 and extended by linearity and continuity , is an isometry from @xmath116 to @xmath117 .    as shown below",
    ", the love isometry allows us to link operators on the paths of @xmath3 to corresponding operators on the rkhs , provided that the random variables @xmath118 ( @xmath115 ) belong to @xmath119 :    [ prop : tronde ] let @xmath120 be such that for any @xmath106 , @xmath121 .",
    "then , there exists a unique operator @xmath122 satisfying @xmath123 and such that @xmath124 for all @xmath115 and @xmath125 .",
    "let @xmath122 be an operator satisfying ( [ eq : tronde ] ) and the pointwise convergence condition .",
    "since @xmath126 , we have : @xmath127 this is immediately extended in a unique way to @xmath107 by linearity and continuity of the isometry @xmath128 , leading to : @xmath129 conversely , using again properties of @xmath128 , one easily checks that ( [ eq : tronde_h ] ) defines a linear map satisfying ( [ eq : tronde ] ) and the pointwise convergence condition .",
    "the construction proposed above will serve as basis for an invariance result , given in prop .",
    "[ prop : invrkhs ] . before stating it ,",
    "let us examine and discuss in more detail the assumptions made in prop .",
    "[ prop : tronde ] and the relation between @xmath4 and @xmath98 , both through examples and analytical considerations .",
    "[ ex : trondeoplincocoop ] let @xmath4 be a linear combination of composition operators , @xmath130 , such as introduced in def .",
    "[ def : comofcom ] .",
    "recall that @xmath4 similarly applies to random field paths or to kernel functions , with @xmath131 and @xmath132 .",
    "in particular , we directly obtain that the condition @xmath121 is fulfilled , so that prop .",
    "[ prop : tronde ] can be applied .",
    "it is then easy to check that @xmath133 , implying that @xmath98 is the unique representer of @xmath4 on @xmath107 satisfying the pointwise convergence condition of prop .",
    "[ prop : tronde ] .",
    "in other words , here @xmath134 .",
    "note that this example also illustrates that @xmath135 may differ from @xmath107 .",
    "indeed , choosing a composition operator @xmath136 and fixing @xmath137 , we have @xmath138 and so @xmath139 .",
    "taking for instance the @xmath5-dimensional rkhs @xmath107 spanned by the 1st order polynomial @xmath140 on @xmath141 $ ] ( with kernel @xmath142 ) and choosing @xmath143 , we see that @xmath144 is a second order polynomial , and thus not in @xmath107 .",
    "[ ex : trondeopint ] let us now consider a measure @xmath145 on @xmath2 such that @xmath146 and define @xmath147 for all @xmath115 . then , relying on the fubini - tonelli theorem , @xmath148 .",
    "in other words , @xmath134 again .      in both examples [ ex : trondeoplincocoop ] and [ ex : trondeopint ] , we found out that @xmath134 . from this , we may wonder under which circumstances @xmath98 is a restriction of @xmath4 .",
    "the spectral framework , and more specifically the _ karhunen - love ( kl ) expansion _ , is a suitable setting to investigate such question . as a preliminary to a sufficient condition for @xmath134 to hold ,",
    "let us recall some useful basics concerning the kl expansion .    in a nutshell ,",
    "the starting point of kl is the _ mercer decomposition _ ( see @xcite , with generalizations in @xcite ) of the continuous covariance kernel @xmath25 : given any finite measure @xmath145 on the borel algebra of @xmath2 whose support is @xmath2 ( typically the lebesgue measure @xmath149 ) , there exists an orthonormal basis @xmath150 of @xmath151 and a sequence of non - negative real numbers @xmath152 such that : @xmath153 where the convergence is absolute and uniform on @xmath2 .",
    "note that the finite trace hypothesis @xmath154 often given as prerequisite of the mercer theorem is automatically fulfilled here , considering the assumptions made on @xmath25 . relying on eq .",
    "[ eq : kl_kernel ] , it is well - known ( see , e.g. , @xcite ) that the rkhs @xmath107 can then be represented as a subspace of @xmath151 , in the following way : @xmath155 furthermore , relying on the love isometry ( cf . prop .  [ loeve ] ) , the random field @xmath3 itself can be expanded with respect to the @xmath156 s , leading to the kl expansion : @xmath157 where the @xmath158 s are independent standard gaussian random variables , and the series is uniformly convergent with probability @xmath5 @xcite . in particular , noting that @xmath159   = \\int_{d }   k(\\mathbf{u},\\mathbf{u } ) \\mathrm{d}\\nu ( \\mathbf{u } )   < + \\infty$ ] , we get ( with probability @xmath5 ) both that the paths of @xmath3 are in @xmath151 and that the series of eq .",
    "[ eq : kl_process ] converges normally .",
    "consequently , in case of a bounded operator @xmath4 from @xmath151 to itself , @xmath160 with probability @xmath5 , where the convergence is normal .",
    "note that in cases such as the one of the differentiation operator ( see , e.g. , for a differentiation of the kl expansion ) , @xmath4 is not bounded with respect to the usual @xmath151 norm , but similar normal convergence results may be obtained by considering a source space of differentiable elements equipped with an _ ad hoc _ topology ( e.g. , sobolev spaces ) .",
    "concerning our question on operators on paths vs on the rkhs , we obtain by substituting @xmath3 and @xmath35 by their respective expansions in @xmath98 s definition : @xmath161 now , using the mercer decomposition ( [ eq : kl_kernel ] ) of @xmath25 and the boundedness of @xmath4 , @xmath162 , so we conclude that @xmath134 . besides , on may also notice that @xmath163 since @xmath164 using that @xmath165 < + \\infty$ ] , we finally also get that @xmath166 @xmath145-a.e .",
    "coming back to invariances , we now give a characterization result , that generalizes those of section @xmath7 in the particular case of gaussian fields :    [ prop : invrkhs ] under the assumptions of prop .",
    "[ prop : tronde ] , the three following conditions are equivalent :    * @xmath93 _ up to a modification _ * @xmath167 * @xmath168    by the pointwise convergence condition on @xmath98 , ( ii ) and ( iii ) are equivalent .",
    "now , let us prove the equivalence between ( i ) and ( ii ) . since for any arbitrary @xmath115 , @xmath169 , we have by duality : @xmath170    proposition [ prop : invrkhs ] can be used to define families of centred gaussian field models satisfying linear - type properties , simply by looking at their kernel .",
    "this includes for instance the case of gaussian random fields with centred paths ( or _ mean - centered _ fields , to use the terminology of @xcite ) and gaussian random fields whose paths are solutions of linear differential equations , as illustrated below ( see also @xcite for recent results on vector fields with divergence - free and curl - free paths ) .",
    "let @xmath145 be a probability measure on @xmath171 such that @xmath172 .",
    "then @xmath3 has centered paths ",
    "i.e. @xmath173  if and only if @xmath174",
    ". indeed , define @xmath4 by @xmath175 for all @xmath115 .",
    "following example [ ex : trondeopint ] , we have @xmath176 , and the result comes from proposition [ prop : invrkhs ] .    for instance , the kernel @xmath177 defined by @xmath178 satisfies the above condition .",
    "figure  [ fig : ex_invpath ] ( a ) shows some sample paths of a centred gaussian random process based possessing a kernel of that form .    we illustrate here the case where the sample paths of a gaussian process are solution to the differential equation : @xmath179 the solutions of the associated homogeneous equation are the functions satisfying @xmath180 so they correspond to invariant functions with respect to @xmath181 .",
    "the solutions of the homogeneous equation are well - known to be in @xmath182 which can be endowed with the following kernel @xmath183 where @xmath184 is a symmetric positive semi - definite @xmath185 matrix .",
    "@xmath186 is solution to the homogeneous equation ( i.e. @xmath186 is @xmath4-invariant ) for all @xmath187 so the sample paths of a centred gaussian process with such kernel inherit this property . let @xmath188 be a gaussian process with mean @xmath189 and covariance @xmath25 . since @xmath190 is a particular solution of eq .",
    "[ eq : de ] , @xmath188 has sample paths satisfying this differential equation .",
    "this is illustrated in figure  [ fig : ex_invpath].b .",
    "0.3     0.3     0.3     in the previous example , the solutions of the ode belong to a 2-dimensional space .",
    "we consider here another ode , the _ laplace equation _",
    "@xmath191 , for which the space of solutions is infinite dimensional .",
    "the solutions to this equation are called harmonic functions and we will call harmonic kernels any positive definite function satisfying the ode argumentwise : @xmath192 @xmath193 . examples of such harmonic kernels can be found in the recent literature ( see respectively  @xcite for 2d and 3d input spaces ) .",
    "we will focus here on the following kernel over @xmath194 : @xmath195 proposition  [ prop : invrkhs ] can be applied to the operator @xmath196 so the sample paths obtained with @xmath197 also are harmomic functions .",
    "this can be seen in the right panel of figure  [ fig : ex_invpath ] where the sample path shows some special features of harmonic functions such as the absence of local minimum .",
    "the aim of this section is to discuss and illustrate the use of argumentwise invariant kernels in gaussian process regression ( gpr ) .",
    "the main idea behind this approach is to incorporate invariance assumptions within gpr . as we will see , using such kernels",
    "can significantly improve the predictivity of gpr in cases where _ structural priors _ on the function to approximate , involving invariances under bounded linear operators , are available .",
    "gpr gives a very convenient stochastic framework for modelling a function @xmath71 based on a finite set of observations @xmath198 , @xmath199 and a gaussian process prior on @xmath72 .",
    "the literature of gpr and related methods is scattered over several fields including statistics and geostatistics  @xcite , machine learning  @xcite and functional analysis  @xcite .",
    "depending on the scientific community , the predictor of @xmath72 is either defined as best linear unbiased predictor of a square - integrable ( or intrinsic ) random field , conditional expectation of a gaussian process , or interpolator with minimal norm in rkhs settings .",
    "one striking fact is that , given any positive definite kernel @xmath25 , the approaches end up with the same expressions for the best predictor and for the  conditional  kernel describing the remaining uncertainty on @xmath72 :    @xmath200    where @xmath201 and @xmath202 .",
    "we will discuss in the next section the influence of using invariant kernel in such models .",
    "we consider here a bounded operator @xmath4 on the paths as defined in section  [ inv_gaussian_case ] , and we use for convenience the same letter to denote t s restriction to the rkhs @xmath107 . assuming that @xmath203 , it was already established that the paths of a centred gaussian random field with kernel @xmath25 are invariant under @xmath4 .",
    "we now establish further that both the gpr predictor and the conditional distribution of such random field knowing response values at a finite set of points are invariant as well .",
    "let @xmath3 be a centred gaussian field with argumentwise @xmath4-invariant kernel @xmath25 and @xmath204 @xmath205 be a finite set of observations .",
    "then ,    * the gpr predictor @xmath206 is @xmath4-invariant * the  conditional covariance kernel ",
    "@xmath207 is argumentwise @xmath4-invariant * @xmath3 conditioned on the evaluation results is @xmath4-invariant , up to a modification .",
    "consequently , conditional simulations of @xmath3 are @xmath4-invariant .",
    "the properties @xmath208 and @xmath209 are a direct consequence of the linearity of @xmath4 .",
    "for example , we have for @xmath209 : @xmath210 furthermore the conditional distribution of @xmath3 knowing evaluation results simplifies as @xmath211 , where @xmath212 stands for the distribution of a centred gaussian random field with covariance kernel @xmath207 .",
    "according to proposition  [ prop : invrkhs ] , a random field with distribution @xmath212 is @xmath4-invariant up to a modification .",
    "@xmath213 follows using the linearity of @xmath4 .",
    "we now consider invariant kernels introduced in the examples of the previous sections and study associated gpr models and predictions .",
    "more precisely , we focus on @xmath8 case studies involving various priors : zero - mean functions , solutions to @xmath214 and solutions to @xmath215 .    [ [ gpr - with - centred - paths ] ] gpr with centred paths + + + + + + + + + + + + + + + + + + + + + +    here we assume that @xmath216 $ ] and that the function to approximate is @xmath217 . assuming that for some reason , it is known _ a priori _ that the integral of @xmath72 over @xmath2 is zero , it is of particular interest to incorporate this knowledge into the model . to get an insight of how gpr is improved by incoporating this structural prior , we compare predictions based on the following kernels : @xmath218 the integral of @xmath219 with respect to any of its variables is zero , so the paths of the associated centred gaussian process will inherit this property . as a consequence , choosing a kernel such as @xmath219 allows incorporating the prior information @xmath220 in gpr modeling , as illustrated in figure  [ fig : zero - mean_gpr ] ( a ) .    in a second time , we assume that evaluation results @xmath221 at @xmath222 distinct points @xmath223 are available , and we compare gaussian process conditional distributions based on both kernels . as seen in figure  [ fig : zero - mean_gpr ] the use of @xmath219 improves considerably gpr predictions since @xmath206 recovers the large peak in the center of the domain .",
    "this is reflected by root integrated squared errors , with values of 0.04 and 1.06 for @xmath219 and @xmath25 , respectively .",
    "0.45     0.45     [ [ gpr - of - a - solution - to - a - univariate - linear - ode ] ] gpr of a solution to a univariate linear ode + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we saw in section 3 that a gaussian process @xmath188 with mean @xmath224 and kernel given by eq .",
    "[ eq : odekern ] is equivalent to a process with paths satisfying the ode @xmath214 .",
    "figure  [ fig : ex_decond ] shows the conditional distribution of @xmath188 given evaluations at one or two points .",
    "it can be seen on the right panel that the prediction uncertainty collapses as soon as @xmath188 is evaluated at two distinct points . with this behaviour ,",
    "the model reflects the unicity of the solution to such ode under two equality conditions .    0.45   satisfying @xmath214 .",
    "in this example , the matrix @xmath184 of eq .",
    "[ eq : odekern ] is set to identity.,title=\"fig : \" ]    0.45   satisfying @xmath214 . in this example , the matrix @xmath184 of eq .",
    "[ eq : odekern ] is set to identity.,title=\"fig : \" ]    [ [ gpr - of - an - harmonic - function ] ] gpr of an harmonic function + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now consider the function @xmath225 , a solution to @xmath226 . as seen previously , the kernel given in eq .",
    "[ eq : kharm ] satisfies this equation argumentwise so it allows to incorporate a structural prior of harmonicity in the gpr model .",
    "figure  [ fig : ex_biharm ] shows the resulting predictions based on four observations , and the associated prediction error .",
    "0.45     0.45     since the best predictor @xmath206 and @xmath72 are harmonic , so is also the prediction error @xmath227 .",
    "it implies that the maximum error is located on the boundary of the domain ( see figure  [ fig : ex_biharm].b ) .",
    "this property may be of interest for the construction of design of experiments for learning harmonic functions .",
    "[ [ sparse - anova - kernels ] ] sparse anova kernels + + + + + + + + + + + + + + + + + + + +    given a product probability measure @xmath228 over @xmath229 , high dimensional model representation ( hdmr , see , e.g. , @xcite ) corresponds to the decomposition of any @xmath230 as the sum of a constant , univariate effects , and interactions terms with increasing orders : @xmath231 where the @xmath232 s ( @xmath233 ) satisfy @xmath234 for all @xmath235  @xcite .",
    "this decomposition is of great interest for defining _ variance - based _",
    "global sensitivity indices ( usually referred to as _ sobol indices _ ) quantifying the influence of each variable or group of variables on the response : @xmath236}{\\mathrm{var}[f(\\mathbf{x})]}\\ ] ] where @xmath237 is a random vector with probability distribution @xmath145 .",
    "sparsity of @xmath72 , in the sense of having many terms equal to zero in eq .",
    "[ eq : hdmr ] , can be interpreted as invariance with respect to operators of the form @xmath238 , where @xmath239 denotes the projection operator mapping @xmath72 to @xmath232 .",
    "we will now illustrate on a popular function from the sensitivity analysis literature how taking prior knowledge of such sparsity into account may highly benefit prediction .",
    "sobol s @xmath240-function  @xcite is defined on @xmath62^d$ ] as @xmath241 where the @xmath242 s are arbitrary positive parameters . beyond the convenient fact that the dimensionality @xmath243 is tunable",
    ", one particular feature of @xmath240 is that the global sensitivity indices have a closed form expression  @xcite : @xmath244 prior knowledge of sobol indices allows defining a subset @xmath245 of main effects and interactions with a significant influence on the output .",
    "we will consider hereafter that terms explaining less than 1e-3 % of variance are non significant .",
    "we consider the @xmath240-function in ten dimensions ( @xmath246 ) with parameter values @xmath247 . in such settings , @xmath245 is a set of 23 subsets of indices including all the main effects except the one of @xmath248 , and some first and second order interaction terms .",
    "let us now compare prediction performances obtained with four gpr models respectively based on the following kernels : @xmath249 where the @xmath250 correspond to argumentwise centred gaussian kernels as in eq .",
    "[ eq : k0 ] , parameterized by variances @xmath251 and lengthscales @xmath252 .",
    "the kernels @xmath253 , @xmath254 , @xmath255 and @xmath256 are respectively parametrized by @xmath257 , @xmath258 , @xmath257 and @xmath259 parameters . since the product in the expression of @xmath255",
    "can be expanded as a sum of @xmath260 kernels with increasing interaction orders , @xmath253 and @xmath254 can be seen as sparse variations of @xmath255 where most of the terms are set to zero .",
    "the learning set is made of @xmath261 uniformly distributed points over the input space .",
    "the parameters @xmath251 and @xmath252 as well as the observation noise @xmath262 are estimated by maximum likelihood .",
    "furthermore , a test set of 1000 uniformly distributed points is considered for assessing the model accuracies .",
    "the obtained results are summarized in table  [ tab : last ] .",
    "it can be seen that the model based on @xmath256 performs rather poorly on this example .",
    "this can be explained by the fact that @xmath256 does not include any bias term ( i.e. a constant in the kernel expression ) . as a consequence",
    ", the associated model tends to come back to @xmath263 when the prediction point is far from the training points . on the other hand , models based on @xmath253 , @xmath254 and @xmath255 perform significantly better since they explain at least half of the variance of @xmath240 .",
    "the sum of sensitivity indices associated to the main effects shows that 66% of the variance of @xmath240 is explained by its additive part so the additive structure assumed by @xmath253 , though not completely unrealistic , is a strong assumption that disadvantages the model .",
    "conversely , the structures of @xmath254 and @xmath255 are well - suited to the problem at hand and the associated models give the best results .",
    "this is particularly true for @xmath254 which only includes the relevant terms for approximating @xmath240 .",
    ".comparison of the predictivity of models based on various kernels . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     in this example",
    ", the best model has been obtained by using a sparse kernel obtained from the knowledge of sensitivity indices . since the latter are usually not available , the issue of automatic sparsity detection is of great importance in practice .",
    "various methods based on a trade - off between a @xmath0-norm and a @xmath264-norm can be found in the literature ( see for example  @xcite ) .",
    "this article focuses on the control of pathwise invariances of square - integrable random field through their covariance structure .",
    "it is illustrated in various examples how a number features one may wish to impose on paths such as multivariate sparsity , symmetries , or being solution to homogeneous odes may be cast as invariance properties under bounded linear operators .",
    "one of the main results of this work , given in proposition  [ prop : kernelprocess ] , relates sample path invariances to the _ argumentwise invariance _ of the covariance kernel , in cases where @xmath4 is a combination of composition operators .",
    "although conceptually simple , such class of operators suffices to describe various mathematical properties on functions such as invariances under finite group actions , or additivity ( i.e. , being sum of univariate functions ) .",
    "this result allows us in particular to extend recent results from @xcite by giving a complete characterization of kernels leading to centred random fields with additive paths , and also to retrieve another result from @xcite on kernels leading to random fields with paths invariant under the action of a finite group .",
    "perhaps surprisingly , the obtained results linking sample paths properties to the covariance apply to squared - integrable random fields and do not restrict to the gaussian case .    turning then to the particular case of gaussian random fields , we obtain in proposition  [ prop : invrkhs ] a generalization of proposition  [ prop : kernelprocess ] to a broader class of operators , that enables constructing gaussian fields with paths invariant under various integral and differential operators .",
    "the core results essentially base on the love isometry between the hilbert space generated by the field and its reproducing kernel hilbert space .",
    "perspectives include revisiting those invariance results in measure - theoretic settings .",
    "taking invariances into account in random field modelling and prediction is of huge practical interest , as illustrated in section [ sec : gpr ] .",
    "various examples involving different kinds of structural priors show how gaussian process regression models may be drastically improved by designing an appropriate invariant kernel .",
    "one striking fact is that invariances assumptions may increase the accuracy of the model even if the function to approximate is not perfectly invariant .",
    "this can be seen on the last example where the assumed sparsity allows to improve the model by avoiding the curse of dimensionality .",
    "a.  van  der vaart , j.  van  zanten , reproducing kernel hilbert spaces of gaussian priors , in : pushing the limits of contemporary statistics : contributions in honor of jayanta k. ghosh , institute of mathematical statistics collect 3 , 2008 , pp . 200222 .",
    "r.  j. adler , an introduction to continuity , extrema , and related topics for general gaussian processes , vol .",
    "12 of lecture notes - monograph series , published by : institute of mathematical statistics , 1990 .",
    "j.  mercer , functions of positive and negative type and their connection with the theory of integral equations , philosophical transactions of the royal society a 209 ( 1909 ) 415446 .",
    "http://dx.doi.org/10.1098/rsta.1909.0016 [ ] .",
    "s.  gunn , m.  brown , supanova : a sparse , transparent modelling approach , in : neural networks for signal processing ix , 1999 .",
    "proceedings of the 1999 ieee signal processing society workshop , ieee , 1999 , pp . 2130 ."
  ],
  "abstract_text": [
    "<S> we study pathwise invariances of centred random fields that can be controlled through the covariance . a result involving composition operators </S>",
    "<S> is obtained in second - order settings , and we show that various path properties including additivity boil down to invariances of the covariance kernel . these results are extended to a broader class of operators in the gaussian case , via the love isometry . </S>",
    "<S> several covariance - driven pathwise invariances are illustrated , including fields with symmetric paths , centred paths , harmonic paths , or sparse paths . </S>",
    "<S> the proposed approach delivers a number of promising results and perspectives in gaussian process regression .    </S>",
    "<S> covariance kernels , composition operators , rkhs , bayesian function learning , structural priors . 60g60 , 60g17 , 62j02 . </S>"
  ]
}