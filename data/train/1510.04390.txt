{
  "article_text": [
    "principal component analysis ( pca ) is one of the oldest @xcite and most fundamental techniques in data analysis , enjoying ubiquitous applications in modern science and engineering @xcite .",
    "given a data matrix @xmath1 of @xmath2 data points of dimension @xmath3 , pca gives a closed form solution to the problem of fitting , in the euclidean sense , a @xmath4-dimensional linear subspace to the columns of @xmath5 . even though the optimization problem associated with pca is non - convex ,",
    "it does admit a simple solution by means of the singular value decomposition ( svd ) of @xmath5 .",
    "in fact , the @xmath4-dimensional subspace @xmath6 of @xmath7 that is closest to the column span of @xmath5 is precisely the subspace spanned by the first @xmath4 left singular vectors of @xmath5 .",
    "using @xmath6 as a model for the data is meaningful when the data are known to have an approximately linear structure of underlying dimension @xmath4 , i.e. they lie close to a @xmath4-dimensional subspace @xmath8 . in practice ,",
    "the principal components of @xmath5 are known to be well - behaved under mild levels of noise , i.e. , the angle between @xmath6 and @xmath8 is relatively small and more importantly @xmath6 is optimal when the noise is gaussian @xcite .",
    "however , in the presence of even a few outliers in @xmath5 , i.e. , points whose angle from the underlying ground truth subspace @xmath8 is large , the angle between @xmath8 and its estimate @xmath6 will in general be large .",
    "this is to be expected since , by definition , the principal components are orthogonal directions of maximal correlation with _ all _ the points of @xmath5 .",
    "this phenomenon , together with the fact that outliers are almost always present in real datasets , has given rise to the important problem of outlier detection in pca .",
    "traditional outlier detection approaches come from robust statistics and include _ influence - based detection _ , _ multivariate trimming _ , @xmath9__-estimators _ _ , _ iteratively _ _ weighted recursive least squares _ and _ random sampling consensus _ ( ransac ) @xcite .",
    "these methods are usually based on non - convex optimization problems , admit limited theoretical guarantees and have high computational complexity ; for example , in the case of ransac many trials are required .",
    "recently , two attractive methods have appeared @xcite with tight connections to _ compressed sensing _",
    "@xcite and _ low - rank representation _ @xcite .",
    "both of these methods are based on convex optimization problems and admit theoretical guarantees and efficient implementations .",
    "remarkably , the self - expressiveness method of @xcite does not require an upper bound on the number of outliers as the method of @xcite does .",
    "however , they are both guaranteed to succeed only in the low - rank regime : the dimension @xmath4 of the underlying subspace @xmath8 associated to the inliers should be small compared to the ambient dimension @xmath3 .    in this paper",
    "we adopt a _ dual _ approach to the problem of robust pca in the presence of outliers , which allows us to transcend the low - rank regime of modern methods such as @xcite .",
    "the key idea of our approach comes from the fact that , in the absence of noise , the inliers lie inside any hyperplane @xmath10 that contains the underlying linear subspace @xmath8 .",
    "this suggests that , instead of attempting to fit directly a low - dimensional linear subspace to the entire data set , as done e.g. in @xcite , we can search for a hyperplane @xmath11 that contains as many points of the dataset as possible .",
    "when the inliers are in general position inside the subspace , and the outliers are in general position outside the subspace , this hyperplane will ideally contain the entire set of inliers together with possibly a few outliers . after removing the points that do not lie in that hyperplane",
    ", the robust pca problem is reduced to one with a potentially much smaller outlier percentage than in the original dataset .",
    "in fact , the number of outliers in the new dataset will be at most @xmath12 , an upper bound that can be used to dramatically facilitate the outlier detection process using existing methods .",
    "we think of the direction @xmath13 of the normal to the hyperplane @xmath11 as a _ dual principal component _ of @xmath5 , as ideally it is an element of @xmath14 .",
    "naturally , one can continue by finding a second dual principal component by searching for a hyperplane @xmath15 , with @xmath16 , that contains as many points as possible from @xmath17 , and so on , leading to a _ dual principal component analysis _ of @xmath5 .",
    "we pose the problem of searching for such hyperplanes as an @xmath18 cosparsity - type problem , which we relax to a non - convex @xmath0 problem on the sphere .",
    "we provide theoretical guarantees under which every global solution of that problem is a dual principal component .",
    "more importantly , we relax this non - convex optimization problem to a sequence of linear programming problems , which , after a finite number of steps , yields a dual principal component . experiments on synthetic data demonstrate that the proposed method is able to handle more outliers and higher dimensional subspaces than the state - of - the - art methods @xcite .",
    "we begin by establishing our data model in section [ subsection : datamodel ] , then we formulate our dpcp problem conceptually and computationally in sections [ subsection : conceptualformulation ] and [ subsection : computationalformulation ] , respectively .",
    "we employ a deterministic noise - free data model , under which the inliers consist of @xmath19 points @xmath20 \\in \\re^{d \\times n}$ ] that lie in the intersection of the unit sphere @xmath21 with an unknown proper subspace @xmath8 of @xmath7 of unknown dimension @xmath4 . accordingly , the outliers consist of @xmath9 arbitrary points @xmath22 \\in \\re^{d \\times m}$ ] that lie on @xmath21 .",
    "the dataset , that we assume given , is @xmath23 \\boldsymbol{\\gamma } \\in \\re^{d \\times l}$ ] , where @xmath24 and @xmath25 is some permutation , indicating that the partition of the columns of @xmath26 into @xmath5 and @xmath27 is unknown .",
    "we further assume that the columns of @xmath26 are in _ general position _ in the following sense : first , any @xmath4-tuple of inliers and any @xmath3-tuple of outliers is linearly indepenent .",
    "second , for any @xmath28 , of which at most @xmath29 come from @xmath5 , the hyperplane of @xmath7 spanned by @xmath30 does not contain any of the remaining points .",
    "notice that in our data model we have made no assumption about the dimension of @xmath8 : indeed , @xmath8 can be anything from a line to a @xmath31-dimensional hyperplane .",
    "ideally , we would like to be able to partition the columns of @xmath26 into those that lie in @xmath8 and those that do nt .",
    "but under such generality , this is not a well - posed problem since @xmath5 lies inside every subspace that contains @xmath8 , which in turn may contain some elements of @xmath27 . in other words ,",
    "given @xmath26 and without any other a - priori knowledge , it may be impossible to correctly partition @xmath26 into @xmath5 and @xmath27 .",
    "instead , we formulate the following well - posed problem :    [ prb : partition ] partition the columns of @xmath32 into two groups , such that one of the groups is a subset of @xmath26 with maximal cardinality , with respect to the property of lying inside a @xmath31-dimensional hyperplane of @xmath7 .",
    "the usefulness of this formulation is that for large values of @xmath33 , where known methods for outlier detection in pca fail , one of the groups , say @xmath34 will contain the entire @xmath5 together with precisely @xmath35 columns of @xmath27 , while the other group , say @xmath36 , will contain the remaining @xmath37 columns of @xmath27 .",
    "note that the first group is _ structured _ in the sense that it must lie in a hyperplane and so in general @xmath38 .",
    "having the partition @xmath39 , we can reject the unstructured group @xmath36 and reconsider the robust pca problem on the group @xmath34 .",
    "but now the number of outliers has decreased from @xmath40 to @xmath35 .",
    "in fact , we can use the upper bound @xmath12 on the number of outliers to dramatically facilitate the outlier detection process using other existing methods .",
    "a natural approach towards solving problem [ prb : partition ] is to solve @xmath41 the idea behind is that a hyperplane @xmath42 contains a maximal number of columns of @xmath26 if and only if @xmath43 is as sparse as possible . since is intractable ,",
    "consider @xmath44 notice that the objective in is convex , while the constraint @xmath45 is non - convex , thus leading to a non - smooth and non - convex optimization problem .",
    "[ prb : nonconvex ] when is every global solution @xmath46 of orthogonal to @xmath47 ? how can we efficiently solve ?    in this paper , we propose to relax by a sequence of linear programs of the form @xmath48 where @xmath49 is some arbitrary vector and @xmath50 indicates normalization to unit @xmath51-norm .",
    "we naturally ask :    [ prb : convexrelaxations ] under what conditions does the sequence of converge to a vector @xmath52 that is orthogonal to @xmath47 ?",
    "in this section , we aim to familiarize the reader with the state - of - the - art of outlier detection in modern single subspace learning ( section [ subsection : outlierpca ] ) , as well as give a brief overview ( section [ subsection : dl ] ) of existing work , that relates technically to the problems of interest of this paper , i.e. problems and .",
    "one of the oldest and most popular outlier detection methods in pca is _ random sampling consensus ( ransac ) _",
    "the idea behind ransac is simple : alternate between randomly sampling @xmath53 points from the dataset and computing a subspace model for these points , until a model is found that fits a maximal number of points in the entire dataset within some error @xmath54 .",
    "ransac is usually characterized by high performance , when not both @xmath53 and the oultier percentage are large ; otherwise it requires a high computational time , particularly when @xmath4 is unknown and @xmath53 is allowed to vary , since exponentially many trials are required in order to sample outlier - free subsets , and thus obtain reliable models . moreover , its performance is very sensitive on in the input parameters @xmath53 and @xmath54 .    among many other outlier detection methods ( see section [ section : introduction ] ) , in the remaining of this section",
    "we will focus on the modern low - rank / sparse - representation theoretic methods of @xcite and @xcite , which we will later use experimentally to compare against our proposed method .",
    "the first method @xcite , referred to as l21 , is a variation of the _ robust pca _ algorithm of @xcite , which computes a @xmath55-norm decomposition denotes the nuclear norm of a matrix , i.e. , the sum of its singular values , and @xmath56 is defined as the sum of the euclidean norms of the columns of a matrix . ] of the data matrix , instead of the @xmath57-decomopsition in @xcite .",
    "more specifically , l21 solves the convex optimization problem @xmath58 it is shown in @xcite that , under certain conditions , the optimal solution to this problem is of the form @xmath59 \\boldsymbol{\\gamma}$ ] and @xmath60 \\boldsymbol{\\gamma}$ ] .",
    "that is , the nonzero columns of the @xmath61 matrix give the inliers and the nonzero columns of the @xmath62 matrix give the outliers .",
    "however , the theoretical conditions require the intrinsic dimension @xmath63 and the outlier percentage to be small enough .",
    "the second method that we consider , referred to as se , is based on the _ self - expressiveness _ property of the data matrix , a notion popularized by the work of @xcite in the area of subspace clustering @xcite .",
    "more specifically , if a column of @xmath26 is an inlier , then it can in principle be expressed as a linear combination of @xmath4 other columns of @xmath26 , which are inliers .",
    "if the column is instead an outlier , then it will in principle be expressible as a linear combination of not less than @xmath3 other columns . to encourage each point to express itself as a linear combination of the smallest number of other data points ,",
    "the following convex optimization problem is solved : @xmath64 if @xmath4 is small enough with respect to @xmath3 , an element is declared as an outlier if the @xmath0 norm of its coefficient vector in @xmath65 is large ; see @xcite for an explicit formula .",
    "se admits theoretical guarantees @xcite and efficient admm implementations @xcite .",
    "however , as it is clear from its description , it is expected to succeed only when @xmath4 is sufficiently small .",
    "in contrast though to l21 , se has the remarkable property that it can , in principle , handle an arbitrary number of outliers .",
    "problems of the form @xmath66 and variants of its relaxations have appeared on several occasions and in diverse contexts in the literature , but are much less understood than the now classic sparse @xcite and cosparse @xcite problems of the form @xmath67 respectively .",
    "the main source of difficulty is that , in contrast to , obtaining tight convex relaxations of is a hard problem .",
    "one of the first instances where was considered was in the context of blind source separation @xcite , where it was proposed to relax it with the problem @xmath68 this is still a non - convex problem , and a heuristic based on quadratic programming was proposed to solve it .",
    "it was not until very recently , that the convex relaxation @xmath69 was proposed , with @xmath70 taken to be a row or a sum of two rows of @xmath71 , and theorems of correctness were given in the context of dictionary learning @xcite . notice that our proposed convex relaxations can be seen as a generalization of . in the context of finding the sparsest vector in a subspace , which is intrinsically related to dictionary learning",
    ", an alternating direction minimization scheme was proposed in @xcite to solve a relaxation of the form @xmath72 remarkably , under some mild conditions , this was shown to converge with high probability to a global solution of @xmath73 the geometry of was further studied in a probabilistic framework in the recent @xcite , after replacing the @xmath0-norm with a smooth surrogate .",
    "in this section we state and discuss our main theoretical results , regarding problems and . before doing so though",
    ", we need to introduce additional notation and draw some interesting connections with the field of numerical integration on the sphere ( section [ subsection : integrationperspective ] ) .      to begin with , for a vector @xmath45 , denote by @xmath74 the function @xmath75 .",
    "then given a set of @xmath2 points @xmath76 , the quantity @xmath77 is a discrete approximation of the integral @xmath78 where @xmath79 is the uniform measure on @xmath21 and @xmath80 is the mean height of the unit hemisphere of @xmath7 , given in closed form by @xmath81 where the double factorial is defined as @xmath82 a useful fact is that @xmath80 is a decreasing function of @xmath3 and in fact tends to zero as @xmath3 goes to infinity .",
    "now , observe that because of the symmetry of @xmath21 , the integral in does not depend on @xmath83 .",
    "however , the _ integration error _ @xmath84 does depend both on the direction of @xmath83 as well as the distribution of the points @xmath85 on @xmath21 .",
    "it is clear though , that the more uniformly the points are distributed , the smaller will be the dependence of the integration error on the direction of @xmath83 .",
    "we note here that the notion of uniform point set distribution on the sphere is a non - trivial one . in a deterministic setting",
    ", this is an active subject of study in the fields of combinatorial geometry and numerical integration on the sphere @xcite .",
    "a widely used measure of the uniformity of a point set on the sphere is the so - called _ point set discrepancy _",
    "@xmath86 of the set , which can be defined in terms of spherical harmonics as @xmath87 where @xmath88 is the dimension of the vector space of spherical harmonics of order @xmath89 , and @xmath90 is the @xmath91-th basis element .",
    "it is then a fact that the integration error is small if and only if @xmath86 is small .",
    "as before , for any @xmath45 we define a vector valued function @xmath92 by @xmath93 .",
    "note that the image of @xmath94 is @xmath95 and that points that are orthogonal to @xmath83 are mapped to @xmath96 .",
    "moreover ,    [ lem : vectorintegral ] @xmath97 .",
    "this result suggests that the quantity @xmath98 can be interpreted as a discrete approximation of the integral @xmath99 and so the more uniformly distributed are the points @xmath85 , the closer @xmath100 is to the quantity @xmath101 .",
    "the above discussion motivates defining the quantities @xmath102 and @xmath103 , to capture the uniformity of outliers and inliers , respectively : @xmath104      before we consider the _ discrete _ non - convex problem , it is instructive to examine its continuous counterpart @xmath105 where @xmath106 is the uniform measure on @xmath107 .",
    "of course this problem is only of theoretical interest and serves in establishing a first intuition for the idea behind .",
    "in fact ,    [ thm : continuousnonconvex ] any global solution to problem must be orthogonal to @xmath8 .",
    "the proof of the above theorem follows easily from the symmetry of the sphere , since the first integral appearing in does not depend on @xmath83 , while the second integral depends only on the angle of @xmath83 from @xmath8 .",
    "theorem [ thm : continuousnonconvex ] suggests that under sufficiently well - distributed point sets of inliers and outliers , any global solution to the discrete problem should _ also _ be orthogonal to the span of the inliers . before stating the precise result",
    ", we need one last piece of notation :    for a set @xmath108 \\subset \\cs^{d-1}$ ] and integer @xmath109 , define @xmath110 to be the maximum circumradius among all polytopes @xmath111 , where @xmath112 are distinct integers in @xmath113 $ ] , and @xmath114 indicates the convex hull operator .",
    "[ thm : discretenonconvex ] suppose that the quantity @xmath115 satisfies @xmath116 for all positive integers @xmath117 such that @xmath118 .",
    "then any global solution @xmath46 to will be orthogonal to @xmath47 .    towards interpreting this result ,",
    "consider first the asymptotic case where we allow @xmath19 and @xmath9 to go to infinity , while keeping the ratio @xmath119 constant",
    ". under point set uniformity , i.e. under the hypothesis that @xmath120 and @xmath121 , we will have that @xmath122 and @xmath123 , in which case is satisfied .",
    "this suggests the interesting fact that when the number of inliers is a linear function of the number of outliers , then will always give a normal to the inliers even for arbitrarily large number of outliers and irrespectively of the subspace dimension @xmath4 .",
    "along the same lines , for a given @xmath119 and under the point set uniformity hypothesis , we can always increase the number of inliers and outliers ( thus decreasing @xmath103 and @xmath102 ) , while keeping @xmath119 constant , until is satisfed , once again indicating that is possible to yield a normal to the space of inliers irrespectively of their intrinsic dimension .",
    "+   +   +     +   +     +      in this section we consider the sequence of convex relaxations ; in particular , there are two important issues to be addressed .",
    "first , note that relaxing the constraint @xmath124 in with a linear constraint @xmath125 as in , has already been found to be of limited theoretical guarantees @xcite .",
    "so it is natural to ask whether the idea of considering a sequence of such relaxations @xmath126 has an intrinsic merit or not , irrespectively of the data distribution .",
    "for example , if the data is _ perfectly well distributed _ , yet the sequence does not yield vectors orthogonal to the inlier space , then we will know that a - priori the method is limited .",
    "fortunately , this is not the case : when the data is perfectly well distributed , i.e. when we restrict our attention to the continuous analog of , given by @xmath127 , \\label{eq : convexrelaxationscontinuousraw}\\end{aligned}\\ ] ] then the sequence @xmath128 achieves the property of interest :    [ thm : convexrelaxationscontinuous ] consider the sequence of vectors @xmath128 generated by recursion , where @xmath129 is arbitrary .",
    "let @xmath130 be the corresponding sequence of angles from @xmath8 .",
    "then @xmath131 , provided that @xmath132 .",
    "this result suggests that relaxing @xmath124 with the sequence @xmath133 is intrinsically the right idea .",
    "the second issue is how the distribution of the data affects the ability of this sequence of relaxations to give vectors orthogonal to @xmath8 .",
    "the answer is given by theorem [ thm : discreteconvexrelaxations ] , which says that when the angle between @xmath49 and @xmath8 is large enough and the data points are well distributed , the sequence will consist of vectors orthogonal to the inlier space , for sufficiently large indices @xmath134 .",
    "[ thm : discreteconvexrelaxations ] let @xmath135 be the angle between @xmath49 and @xmath8 .",
    "suppose that condition on the outlier ratio @xmath119 holds true and consider the vector sequence @xmath136 generated by recursion .",
    "then after a finite number of terms @xmath137 , for some @xmath109 , every term of @xmath136 will be orthogonal to @xmath138 , providing that @xmath139    first note that if is true , then the expression of always defines an angle between @xmath140 and @xmath141 .",
    "second , theorem [ thm : discreteconvexrelaxations ] can be interpreted using the same asymptotic arguments as theorem [ thm : discretenonconvex ] . in particular , notice that the lower bound on the angle @xmath135 tends to zero as @xmath142 go to infinity with @xmath119 constant .",
    "note also that this result does not show convergence of the sequence @xmath143 : it only shows that this sequence will eventually satisfy the desired property of being orthogonal to the space of inliers ; a convergence result remains yet to be established .",
    "so far we have established a mechanism of obtaining an element @xmath13 of @xmath14 , where @xmath144 : run the sequence of linear programs until the function @xmath145 converges within some small @xmath146 ; then assuming no pathological point set distributions , any vector @xmath143 can be taken as @xmath13 .",
    "there are two possibilities : either @xmath8 is a hyperplane of dimension @xmath147 or @xmath148 . in the first case , @xmath13 is the unique up to scale element of @xmath14 , which proves that in this case the sequence of in fact converges . in such a case , we can identify our subspace model with the hyperplane defined by the normal @xmath13 .",
    "next , if @xmath148 , we can proceed to find a second element @xmath149 of @xmath14 that is orthogonal to @xmath13 and so on .",
    "this naturally leads to the _ dual principal component pursuit _ shown in algorithm [ alg : dpcp ] .",
    "@xmath150 ; @xmath151 ; @xmath152 ; @xmath153 ; @xmath154 [ dpcp : update ] ; @xmath155 ; @xmath156 ; @xmath157 ; @xmath158 ;    a few comments are in order . in algorithm",
    "[ alg : dpcp ] , @xmath159 is an estimate for the codimension @xmath160 of the inlier subspace @xmath47 .",
    "if @xmath159 is rather large , then in the computation of each @xmath161 , it is more efficient to reduce the coordinate representation of the data by replacing @xmath26 with @xmath162 , where @xmath163 is the orthogonal projection onto @xmath164 , and solve the linear program in step [ dpcp : update ] in the projected space .    notice further how the algorithm initializes",
    "@xmath49 : this is effectively the right singular vector of @xmath165 , that corresponds to the smallest singular value . as it will be demonstrated in section [ section : experiments ] , this choice has the effect that the angle of @xmath49 from the inlier subspace is typically large , in particular , larger than the smallest initial angle required for the success of the principal component pursuit of .",
    "in this section we investigate experimentally the proposed dpcp alg .",
    "[ alg : dpcp ] . using both synthetic ( subsection [ subsection : synthetic ] ) and real data ( subsection [ subsection : real ] ) , we compare dpcp to the three methods se , l21 and ransac discussed in section [ subsection : outlierpca ] as well as to the method of eq . discussed in section [ subsection : dl ] , which we will refer to as svs ( _ sparsest vector in a subspace _ ) .",
    "the parameters of the methods are set to fixed values , chosen such that the methods work well across all tested dimension and outlier configurations .",
    "in particular , we use @xmath166 and @xmath167 ; see @xcite and @xcite for details . regarding dpcp , we fix @xmath168 , and unless otherwise noted , we set @xmath159 equal to the true codimension of the subspace .",
    "to begin with , we evaluate the performance of dpcp in the absence of noise , for various subspace dimensions @xmath169 and outlier percentages @xmath170 .",
    "we fix the ambient dimension @xmath171 , sample @xmath172 inliers uniformly at random from @xmath173 and @xmath9 outliers uniformly at random from @xmath21 .",
    "we are interested in examining the ability of dpcp to recover a single normal vector ( @xmath174 ) to the subspace , by means of recursion .",
    "the results are shown in fig .",
    "[ figure : dpcp ] for @xmath175 independent trials .",
    "[ figure : dpcp_theory ] shows whether the theoretical conditions of are satisfied or not . in checking these conditions , we estimate the abstract quantities @xmath176 by monte - carlo simulation .",
    "whenever these conditions are satisfied , we choose @xmath177 in a controlled fashion , so that its angle @xmath135 from the subspace is larger than the minimal angle @xmath178 of , and then we run dpcp ; if the conditions are not true , we do not run dpcp and report a @xmath140 .",
    "fig [ figure : dpcp_controlled ] shows the angle of @xmath179 from the subspace .",
    "we see that whenever is true , dpcp returns a normal after only @xmath175 iterations .",
    "fig [ figure : dpcp_random ] shows that if we initialize @xmath177 randomly , then its angle @xmath135 from the subspace becomes less than the minimal angle @xmath178 , as @xmath4 increases .",
    "even so , fig .",
    "[ figure : dpcp_randomsuccess ] shows that dpcp still yields a numerical normal , except for the regime where both @xmath4 and @xmath180 are very high . notice that this is roughly the regime where we have no theoretical guarantees in fig .",
    "[ figure : dpcp_theory ] . fig .",
    "[ figure : dpcp_smart ] shows that if we initialize @xmath177 as the right singular vector of @xmath181 corresponding to the smallest singular value , then @xmath182 is true for most cases , and the corresponding performance of dpcp in fig .",
    "[ figure : dpcp_smartsuccess ] improves further . finally , fig .",
    "[ figure : dpcp_angle ] plots @xmath178 .",
    "we see that for very low @xmath4 this angle is almost zero , i.e. dpcp does not depend on the initialization , even for large @xmath180 . as @xmath4 increases though , so does @xmath178 , and in the extreme case of the upper rightmost regime , where @xmath4 and @xmath180 are very high , @xmath178 is close to @xmath183 , indicating that dpcp will succeed only if @xmath177 is very close to @xmath14 .",
    "next , for the same range of @xmath180 and @xmath4 , and still in the absence of noise , we examine the potential of each of se , l21 , svs , ransac and dpcp to perfectly distinguish outliers from inliers .",
    "note that each of these methods returns a _ signal _ @xmath184 , which can be thresholded for the purpose of declaring outliers and inliers . for se",
    ", @xmath185 is the @xmath0-norm of the columns of the coefficient matrix @xmath65 , while for l21 it is the @xmath51-norm of the columns of @xmath62 . since ransac ,",
    "svs and dpcp directly return subspace models , for these methods @xmath185 is simply the distances of all points to the estimated subspace model . in fig .",
    "[ figure : separation ] we depict success versus failure , where success is interpreted as the existence of a threshold on @xmath185 that perfectly separates outliers and inliers .",
    "as expected , the low - rank methods se and l21 can not cope with large dimensions even in the presence of @xmath186 outliers . as expected , ransac is very successful irrespectively of dimension , when @xmath180 is small , since the probability of sampling outlier - free subsets is high .",
    "but as soon as @xmath180 increases , its performance drops dramatically . moving on , svs is the worst performing method , which we attribute to its approximate nature .",
    "remarkably , dpcp performs perfectly irrespectively of dimension for up to @xmath187 outliers .",
    "note that we use the true codimension @xmath159 of the subspace as input to dpcp ; this is to ascertain the true limits of the method . certainly , in practice only an estimate for @xmath159 can be used . as we have observed from experiments ,",
    "the performance of dpcp typically does not change much if the codimension is underestimated ; however performance can deteriorate significantly if the true @xmath159 is overestimated .",
    "moreover , we note that while se , l21 and svs are extremely fast , as they rely on admm implementations , dpcp is much slower , even if we use an optimizer such as gurobi @xcite .",
    "speeding up dpcp is the subject of current research .    finally , in fig .",
    "[ figure : rocsynthetic ] we show roc curves associated with the thresholding of @xmath185 for varying levels of noise and outliers .",
    "when @xmath4 is small , fig .",
    "[ figure : roc_synthetic_ld ] shows that se , l21 and dpcp are equally robust giving perfect separation between outliers and inliers , while svs and ransac perform poorly .",
    "interestingly , for large @xmath4 ( fig .",
    "[ figure : roc_synthetic_ld ] ) , dpcp gives considerably less false positives ( fp ) than all other methods across all cases , indicating once again its unique property of being able to handle large subspace dimensions in the presence of many outliers .      in this subsection",
    "we consider an outlier detection scenario in pca using real images .",
    "the inliers are taken to be all @xmath188 face images of a single individual from the extended yale b dataset @xcite , while the @xmath9 outliers are randomly chosen from caltech101 @xcite .",
    "all images are cropped to size @xmath189 as was done in @xcite . for a fair comparison ,",
    "we run se on the raw @xmath190-dimensional data , while all other methods use projected data onto dimension @xmath191 . since it is known that face images of a single individual under different lighting conditions lie close to an approximately @xmath192-dimensional subspace @xcite , we choose the codimension parameter of dpca to be @xmath193 . we perform @xmath175 independent trials for each individual across all @xmath194 individuals for a different number of outliers @xmath195 and report the ensemble roc curves in fig .",
    "[ figure : rocfaces ] . as is evident , dpca is the most robust among all methods .",
    "we presented _ dual principal component pursuit ( dpcp ) _ , a novel @xmath0 outlier detection method , which is based on solving an @xmath0 problem on the sphere by linear programs over a sequence of tangent spaces on the sphere .",
    "dpcp is able to handle subspaces of as low codimension as @xmath196 in the presence of as many outliers as @xmath187 .",
    "future research will be concerned with speeding up the method as well as extending it to multiple subspaces and other types of data corruptions , such as missing entries and entry - wise errors .",
    "this work was supported by grant nsf 1447822 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of outlier rejection in single subspace learning . </S>",
    "<S> classical approaches work directly with a low - dimensional representation of the subspace . </S>",
    "<S> our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement . </S>",
    "<S> we pose this problem as an @xmath0-minimization problem on the sphere and show that , under certain conditions on the distribution of the data , any global minimizer of this non - convex problem gives a vector orthogonal to the subspace . </S>",
    "<S> moreover , we show that such a vector can still be found by relaxing the non - convex problem with a sequence of linear programs </S>",
    "<S> . experiments on synthetic and real data show that the proposed approach , which we call dual principal component pursuit ( dpcp ) , outperforms state - of - the art methods , especially in the case of high - dimensional subspaces . </S>"
  ]
}