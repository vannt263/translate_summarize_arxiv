{
  "article_text": [
    "in the one - body approximation , the study of disordered systems amounts to the study of random schrdinger operators of the form @xmath0 where @xmath1 is a kinetic term ( _ i.e. _ a self - adjoint or essentially self - adjoint operator corresponding to some dispersion relation , typically a regularized version of @xmath2 ) and @xmath3 is a real random potential ( in the simplest case , @xmath3 is a white noise ) .",
    "we work on a ultra - violet regular subspace of @xmath4 and we restrict ourselves to @xmath5 small so as to see @xmath6 as a kind of perturbation of the free hamiltonian .    the properties of @xmath7 are usually established through the behavior of the kernel of the resolvent operator or green s function ( @xcite , @xcite , @xcite ) @xmath8 for instance , the density of states is given by @xmath9    the important point is that , in the thermodynamic limit , the system is self - averaging , _",
    "i.e. _ mean properties are often almost sure ones .",
    "thus the problem can be seen as a statistical field theory with respect to the random field @xmath3 . in statistical mechanics , functional integrals in the weakly coupled regime",
    "are controled through a cluster expansion ( or polymer expansion ) with small field versus large field conditions , the problem being then to control a boltzmann weight ( @xcite , @xcite ) .    in the first part of this paper ,",
    "we derive a resolvent cluster expansion with large field versus small field conditions assuming that @xmath3 satisfies some large deviation estimates .",
    "this would allow to prove the existence and the regularity of the mean green s function ( theorem [ thre ] ) and to get an asymptotic expansion for the density of states .",
    "in the second part , we show that the hypothesis of theorem [ thre ] are satisfied in the case of a 2 dimensional model with a rotation invariant dispersion relation and an infra - red cut - off on the potential . from the point of view of _ renormalization group _",
    "analysis , our results allows to control the model away from the singularity , _",
    "i.e. _ to perform the first renormalization group steps and therefore to generate a fraction of the expected `` mass '' .",
    "in @xmath10 we consider @xmath0 where @xmath3 is a gaussian random field with covariance @xmath11 whose smooth translation invariant kernel is rapidly decaying ( we will note the associated measure @xmath12 ) . because @xmath11 is smooth",
    ", @xmath12 as a measure on tempered distributions is in fact supported on @xmath13 functions .",
    "we suppose also that @xmath14 has compact support so that we do not have to deal with ultra - violet problems .",
    "we construct the finite volume model in @xmath15 by replacing @xmath11 and @xmath1 by their `` @xmath16-periodization '' @xmath17    then we define @xmath18 where @xmath19 can be considered either as a measure on @xmath20 or as a measure on @xmath21 which is supported by the space of @xmath16-periodic functions . in the same way ,",
    "@xmath22 will be considered as an operator either on @xmath23 or on @xmath24 .",
    "one can note that in momentum space , because of the cut - off , the problem reduces to a finite dimensional one .    because @xmath3 is almost surely regular ,",
    "its operator norm as a multiplicative operator is equal to its @xmath25 norm ( it is easy to see that @xmath26 , equality can be obtained by taking test functions @xmath27 such that @xmath28 ) . therefore @xmath3 is bounded and self - adjoint .",
    "then @xmath29 is almost surely an analytic operator - valued function of @xmath5 in a small domain ( depending on @xmath3 ) around the origin .",
    "this domain can be extended to a @xmath3-dependent neighborhood of the real axis thanks to the identity ( for @xmath30 small enough ) @xmath31^{n } \\right\\}\\ ] ] in the same way , @xmath32 is analytic in @xmath33 .",
    "one can also check that @xmath32 has a smooth kernel and is integrable with respect to @xmath19 .",
    "furthermore , @xmath34 will have a translation invariant kernel because @xmath19 is translation invariant .",
    "we introduce a function @xmath35 which satisfies    * @xmath35 is an odd @xmath13 function , increasing and bounded * for any @xmath36 , @xmath37 * for any @xmath38 , @xmath39 * the @xmath25 norm of its derivatives does not grow too fast    then for @xmath40 , we define the operators @xmath41 , @xmath42 and @xmath43 through the fourier transform of their kernel @xmath44 - i\\mu \\right|^{1/2 } } \\\\",
    "\\hat{u}_{\\lambda , \\mu}^{-1}(p ) & = & \\hat{d}_{\\lambda , \\mu}^{2}(p )      \\hat{c}_{\\lambda , \\mu}^{-1}(p ) \\end{aligned}\\ ] ]    given any characteristic length @xmath45 we can divide the space into cubes @xmath46 of side @xmath45 and construct an associated @xmath47 partition of unity @xmath48 where @xmath49 has support in a close neighborhood of the cube @xmath46 ( _ e.g. _ on @xmath46 and its nearest neighbors ) .",
    "this decomposition induces an orthogonal decomposition of @xmath3 into a sum of fields @xmath50 with covariance @xmath51    for simplicity we will pretend that @xmath11 and @xmath52 have compact support , so that @xmath50 is almost surely supported on a close neighborhood of @xmath46 , moreover we will take that it is restricted to @xmath46 and its nearest neighbors .",
    "the generalization to a fast decaying @xmath11 can be easily obtained by decomposing each @xmath50 over the various cubes and write more complicated small / large field conditions that test the size of @xmath50 in the various cubes .",
    "this leads to lengthy expressions that we want to avoid .",
    "finally , we note @xmath53 the distance in @xmath15 @xmath54    in the following , @xmath55 or @xmath56 will stand as generic names for constants in order to avoid keeping track of the numerous constants that will appear",
    ". furthermore we will not always make the distinction between a function and its fourier transform but we will use @xmath36 , @xmath57 and @xmath58 as space variables and @xmath59 and @xmath60 as momentum variables .",
    "[ thre ]   + suppose that    * @xmath11 is smooth and has fast decay * @xmath61^{d } } \\ ! \\xi^{-1}_{\\lambda}(x ,",
    "y ) \\ , dx \\ , dy$ ] exists * for all @xmath62 $ ] and all @xmath63 , @xmath64 , @xmath65 and @xmath66 have smooth kernels with fast decay over a length scale @xmath45 . * for all @xmath67 , we have @xmath68 such that for all @xmath16 and all triplets @xmath69 @xmath70^{n_{1 } } \\left[1 + l^{-1 }      d_{\\lambda}(\\delta_{2 } , \\delta_{3})\\right]^{n_{1}}}\\ ] ] * there are constants @xmath71 , @xmath72 , @xmath73 and @xmath74 such that @xmath75 where @xmath76 denote the probability with respect to the measure @xmath77 ( @xmath78 ) @xmath79    then let @xmath80 , @xmath81 and @xmath82    for all @xmath83 and for all @xmath84 small enough ( in a @xmath5-dependent way ) , @xmath85 is uniformly bounded in @xmath16 and admits the following development ( in the operator norm sense ) @xmath86 where @xmath87^{d}$ ] , and @xmath88 is the characteristic function of @xmath89 .",
    "furthermore we have the following properties    * @xmath90 has a smooth , translation invariant kernel * @xmath91 and @xmath90 have high power decay @xmath92^{n_{0}}}\\ ] ] and a similar relation for @xmath90 with @xmath53 being replaced by @xmath93 .",
    "* @xmath94 is an analytic operator valued function of @xmath33 for all @xmath33 in @xmath95e_{1 } , e_{2}[$ ] with a small @xmath5-dependent radius of analyticity .",
    "* @xmath94 is a @xmath13 operator - valued function of @xmath5 and admits an asymptotic expansion to all orders in @xmath5 . which is the formal perturbative expansion of @xmath96 ( @xmath97 denotes the scalar product , _",
    "i.e. _ @xmath98 )    this theorem is formulated in a rather general way so as to apply with minimum transformation to various situations ( lattice or continuous models ) and in any dimension .",
    "then we construct a concrete example with a two - dimensional model .",
    "one can also refer to @xcite for a @xmath99 case .",
    "we consider @xmath100 where    * @xmath101 is a ultra - violet regularized inverse laplacian , _",
    "i.e. _ there is a @xmath102 function @xmath103 equal to 1 on `` low '' momenta such that @xmath104 we will note @xmath105 instead of @xmath106 , the uv - cutoff being then implicit .",
    "* we are interested in the mean green s function for an energy @xmath107 * @xmath108 is an infra - red cut - off which enforces @xmath109 for some large constant @xmath110 * @xmath3 has covariance @xmath11 which is a @xmath47 approximation of a @xmath111-function    this corresponds to the model away from the singularity @xmath112 in a multi - scale renormalization group analysis , we will show that it generates a small fraction of the expected imaginary part which is @xmath113 .",
    "let @xmath114 be an even integer greater than 2 , we define @xmath115 such that @xmath116    next , we construct a smooth partition of unity into cubes of side @xmath117 ( they form a lattice @xmath118 ) and we construct the fields @xmath50 s accordingly .",
    "[ thprob2d ]   + there exist constants @xmath71 and @xmath72 such that for any @xmath16 , @xmath119 and @xmath120 we have @xmath121 furthermore theorem [ thre ] applies and @xmath122 is asymptotic to its perturbative expansion so that it behaves more or less like @xmath123    it is easy to extend this result to the case of a rotation invariant dispersion relation and for energies outside the free spectrum not too close to the band edge . in this case , the cut - off is no longer needed so that the result apply to the full model .",
    "we give here the global strategy for proving theorem [ thre ] , the main ingredient being the polymer expansion that we will detail in the following .",
    "first we recall ( without proving them ) some quite standard properties of gaussian measures .",
    "[ lemtrans ] complex translation + let @xmath124 be a gaussian random field with covariance @xmath55 and let @xmath125 be the associated measure .",
    "for any regular functional @xmath126 and any function @xmath127 , we have the following identity @xmath128    [ lemipp ] integration by part + with the same notations than above we have @xmath129    those lemmas could for instance be easily proved for polynomial functionals and extended through a density argument to a wide class of functionals .",
    "our starting point is obtained by applying lemma [ lemtrans ] with @xmath131 .",
    "@xmath132    on one hand we earned something because now the resolvent operator in the integral is bounded in norm independently of @xmath84 ( in the following we will note @xmath58 instead of @xmath133 and show convergence for any @xmath58 such that @xmath134 , this would allow to prove analyticity in @xmath58 ) .",
    "but on the other hand we have a huge normalization factor to pay .",
    "however , we can remark that this normalization factor is in fact equivalent to a factor @xmath135 per @xmath45-cube .",
    "most of the demonstration amounts to a polymer expansion of @xmath136 , _",
    "i.e. _ we write @xmath91 as a sum over polymers of polymer activities @xmath137^{n_{0 } } }        \\sum_{y \\in { \\cal a } }   \\lambda^{c_{2}|y| } \\gamma_{y } \\ , t(y ) \\right ]        \\chi_{\\delta_{in}}\\end{aligned}\\ ] ] where @xmath138 and @xmath139 are small constants , @xmath140 has decay in the spatial extension of @xmath141 and @xmath142 is bounded .",
    "furthermore , @xmath143 is given by a functional integration over fields @xmath50 s corresponding to cubes in the support of the polymer @xmath141 .",
    "this show that @xmath91 is bounded and has a high power decay uniformly in @xmath16 .",
    "next , when we consider @xmath144 we can divide the sum over polymers into a sum over polymers with a large spatial extension ( say @xmath145 ) and sum over `` small '' polymers .",
    "the large polymers will have a total contribution small as @xmath146 to some large power .",
    "for the small polymers , since we are far away from the boundaries , their contribution calculated with @xmath19 will be equal to their contribution calculated with @xmath12 up to a factor @xmath147 . in this way we can prove the development ( [ lambdadev ] ) .",
    "smoothness of the kernel will be obtained because we will show that we can write @xmath148 the convergence for any @xmath149 allows to show analyticity ( we write @xmath58-derivatives as cauchy integrals so that we can show that they all exists and do not grow too fast ) .",
    "then an asymptotic expansion can be generated through the repeated use of resolvent identity .    finally , for the density of states , we just need to remark that @xmath150 where @xmath151 is a regularized @xmath111-function because of the presence of the ultra - violet cut - off .",
    "thus an asymptotic expansion for @xmath152 with respect to the operator norm will yield an asymptotic expansion for the density of states .",
    "cluster expansions in constructive field theory lay heavily on a clever application of the taylor formula with integral remainder .",
    "writing the full taylor series would amount to completely expand the perturbation series , which most often diverges , and therefore should be avoided .",
    "a rather instructive example of minimal convergent expansion is the brydges - kennedy forest formula : you have a function defined on a set of links between pair of cubes and you expand it not on all possible graphs but only on forests ( cf @xcite ) .    for more complex objects",
    "a way to generalize such a formula can be found in @xcite , and we refer the reader to it for a more careful treatment and for various proofs .",
    "let us assume that we have a set of objects that we call monomers .",
    "a sequence of monomers will be called a polymer , then we will expand a function defined on a set of monomers into a sum over allowed polymers .    to be more precise ,",
    "let @xmath153 be a set of monomers , we define the set @xmath154 of polymers on @xmath153 as the set of all finite sequences ( possibly empty ) of elements of @xmath153 .",
    "then a monomer can be identified to a polymer of length 1 . the empty sequence or empty polymer",
    "will be noted @xmath155 .",
    "we define on @xmath154    * a concatenation operator : for @xmath156 and @xmath157 , we define @xmath158 * the notion of starting sequence : we say that @xmath159 is a starting sequence of @xmath141 ( equivalently that @xmath141 is a continuation of @xmath159 ) and we note @xmath160 iff there exists @xmath161 such that @xmath162    then we call allowed set ( of polymers ) any finite subset @xmath163 such that    * @xmath164 * @xmath165    the first condition implies that @xmath166 whenever @xmath167 is non - empty .",
    "finally , for @xmath141 belonging to some allowed set @xmath167 , a monomer @xmath124 is said to be admissible for @xmath141 ( according to @xmath167 ) iff @xmath168 .",
    "[ lemclust ]   + let @xmath169 be a set of @xmath170 monomers and @xmath154 the set of polymers on @xmath153 .",
    "we assume that we have an indexation of @xmath171 by @xmath153 , _",
    "i.e. _ a bijection from @xmath153 to @xmath172 so that an element of @xmath173 can be noted @xmath174 .    for @xmath175 a regular function from @xmath173 to some banach space @xmath176 and an allowed set @xmath177",
    ", the polymer expansion of @xmath175 according to @xmath167 is given through the following identity @xmath178\\end{aligned}\\ ] ] where @xmath179 is given by @xmath180    _ proof _ + the proof is made through an inductive iteration of a first order taylor formula . we start with @xmath181 and put a common interpolating parameter @xmath182 on all admissible monomers for the empty set , i.e. we make a first order taylor expansion with integral remainder of @xmath183 $ ] between 0 and 1 , with @xmath184 being the vector with entries 1 or 0 according to whether the corresponding monomer is admissible or not .",
    "then each partial derivative acting on @xmath175 can be seen as taking down the corresponding monomer so that terms can be seen as growing polymers .",
    "the iteration goes as follow : for a term of order @xmath185 corresponding to a given polymer @xmath141 and having @xmath185 interpolating parameters @xmath186 we put a common parameter @xmath187 interpolating between @xmath188 and @xmath189 on all monomers admissible for @xmath141 .",
    "it is easy to check that the process is finite since @xmath190 is finite and that one obtains the desired formula .",
    "@xmath130    in the following our monomers are sets of cubes ( that we call the support of the monomer ) and links between those cubes .",
    "when we take down a polymer , we connect all the cubes in its support and maybe some more cubes . thus a polymer is made of several connected regions , we will say that it is connected if it has a single connected component",
    ". the rules of admissibility will be to never take down a monomer whose support is totally contained in a connected region .    in this case",
    ", one can show that the interpolating parameters depend only of the connected component to which the corresponding monomer belongs so that one can think to `` factorize '' the connected components .",
    "we define generalized polymers as sets of connected polymers .",
    "then a generalized polymer @xmath191 is allowed if the polymer @xmath192 is allowed ( this does not depend of the order of the @xmath193 .",
    "equation ( [ eqclust0 ] ) becomes @xmath194\\ ] ] where the sum extends on all allowed generalized polymers , and @xmath195 is given by @xmath196      semi - perturbative expansion ( like cluster expansions ) are convergent only when the `` perturbation '' is small ( in our case the operators @xmath50 s ) .",
    "thus it is very important to distinguish between the so called _ small field regions _ where perturbations will work and the _ large field regions _ where we must find other estimates ( they will come mostly from the exponentially small probabilistic factor attached to those regions ) .",
    "we take a @xmath47 function @xmath84 such that    * @xmath197 * @xmath198 $ ] * @xmath199 } } = 1 $ ]    then for each @xmath46 we define @xmath200 where @xmath201",
    ". then we can expand @xmath202 where @xmath203 is the large field region whose contribution will be isolated through the following lemma .",
    "[ lemlargef ]   + let @xmath203 be a large field region made of @xmath170 cubes @xmath204 ,  , @xmath205 and @xmath110 any operator such that @xmath206 ( @xmath207 stands for @xmath208 ) .",
    "we have the following identity @xmath209 where @xmath210    _ proof _ + the proof relies on resolvent expansion identities @xmath211 we show by induction that for all @xmath212 we have @xmath213    the case @xmath214 is obtained by a resolvent expansion @xmath215    then we go from @xmath216 to @xmath217 with 2 steps of resolvent expansion .",
    "we write @xmath218    finally , for @xmath219 we make a last resolvent expansion on the rest term @xmath220 by writing @xmath221",
    "@xmath130    if we look at @xmath222 and fix @xmath223 , we can see that summing over the sequences @xmath224 and choosing a particular term for each @xmath225 amounts to construct a tree on @xmath226 .",
    "we define an oriented link @xmath227 as a couple of cubes that we note @xmath228 , then @xmath229 is the set of oriented links . given two cubes @xmath230 and @xmath231 and a set of cubes @xmath232 we construct the set @xmath233 of oriented trees going form @xmath230 to @xmath231 through @xmath203 as the sequences @xmath234 which satisfy    * @xmath235 * @xmath236 * @xmath237 * @xmath238 * @xmath239    then we have the following equivalent formulation of lemma [ lemlargef ] .",
    "[ ldecoupling ]   + let @xmath203 be a large field region made of @xmath170 cubes @xmath204 ,  , @xmath205 and @xmath110 any operator such that @xmath206    we have the following identity @xmath240 where @xmath241    the proof being just a rewriting of lemma [ lemlargef ] is quite immediate . @xmath130    thanks to this lemma we can factorize out the contribution of the large field region , then we need to extract spatial decay for the resolvent in the small field region",
    ". however a kind of combes - thomas estimate ( @xcite ) would not be enough because of the normalization factor that we must pay .",
    "for this reason , we will make a polymer expansion to determine which region really contributes to the resolvent .",
    "for some large field region @xmath203 , we want to prove the decay of @xmath242 and get something to pay for the normalization factor .",
    "we define the set @xmath243 of links as the set of pair of cubes , and @xmath244 as the set of links which do not connect two cubes of @xmath203 .",
    "then for @xmath245 we define @xmath246 with the convention that @xmath247 if @xmath248 .    for any fixed @xmath249 we expand @xmath250 on @xmath244 with the rule that for any growing polymer    * if we have two adjacent connected components @xmath159 and @xmath161 ( such that @xmath251 ) we connect the two components * we connect @xmath252 ( resp .",
    "@xmath253 ) to any adjacent polymer component    this allows to take into account that the operators localized on a pair of cubes have their support extending to the neighboring cubes .",
    "let us notice that if @xmath110 and @xmath254 have disjoint support , we have @xmath255    then it is easy to see that the expansion of @xmath250 involves only totally connected polymers which connect @xmath252 to @xmath253 , because the other terms necessarily contain a product of two operators with disjoint supports which gives zero .",
    "we note @xmath256 the corresponding set of polymers which is a decreasing function of @xmath203 , _",
    "@xmath257 then , according to ( [ eqclust ] ) , our expansion looks like    @xmath258 \\\\     & = & \\sum_{y \\in { \\cal a}(\\omega , l_{0 } ) } \\int \\prod_{i } dh_{i }      \\left(\\prod_{x \\in y } \\frac{\\partial } { \\partial u_{x}}\\right )       \\frac{1}{\\displaystyle i + \\sum_{x \\in { \\cal l}(\\omega ) } z_{x } q_{x }      + \\sum_{x \\in y } u_{x } q_{x } }        \\left[\\vec{z}(y , \\{h_{i}\\ } ) , \\vec{0}\\right ] \\end{aligned}\\ ] ]    then in the second expression , we rewrite the derivatives as cauchy integrals so that @xmath259 where @xmath260    we suppose that we fixed @xmath67 the power rate of decay in @xmath261 of @xmath262 and @xmath263 , then we have the following lemma    [ lemresexp ]   + for @xmath264 and @xmath5 small enough , we have @xmath265^{n_{2 } } } \\gamma(y )     \\mbox { with } \\sum_{y \\in { \\cal a}^{*}(l_{0 } ) } \\gamma(y ) \\leqslant 1\\end{aligned}\\ ] ] where @xmath266 is the number of monomers in @xmath141 .",
    "_ proof _ + since we are in the small field region @xmath267^{n_{1}-(d+1)}}\\ ] ] then in ( [ eqresexp ] ) we can integrate each @xmath268 on a circle of radius @xmath269^{n_{1 } -      2 ( d+1)}\\ ] ] while staying in the domain of analyticity for @xmath268 and have a resolvent bounded in norm by say @xmath270 ( if @xmath5 is small enough ) . thus @xmath271^{n_{1 } - 2(d+1 ) } } \\right ) \\\\     & \\leqslant & \\frac{\\lambda^{|y|/4}}{[1 + l^{-1 } d_{\\lambda}(\\delta_{0 } , \\delta_{0'})]^{n_{2 } } } \\ , \\frac{\\left[o(1 ) \\lambda^{1/4}\\right]^{|y|}}{|y| ! }   \\left(\\prod_{{x \\in y } \\atop { x=\\{\\delta_{x } , \\delta_{x}'\\ } } }   \\frac{1}{[1+l^{-1 } d_{\\lambda}(\\delta_{x } , \\delta_{x}')]^{d+1 } } \\right ) \\end{aligned}\\ ] ] this demonstrates the first part of the lemma with @xmath272^{|y|}}{|y| ! }    \\left(\\prod_{x \\in y } \\gamma_{x }",
    "\\right ) \\quad \\mbox { and } \\sum_{x \\ni \\delta } \\gamma_{x } = o(1)\\ ] ]    a link @xmath273 can either be a true link when @xmath274 or a tadpole when both cubes collapse .",
    "our expansion rules insure that there is at most 1 tadpole per cube of @xmath275 the support of @xmath141 .",
    "if we forget about proximity links ( that we connect also adjacent cubes ) then a polymer with @xmath216 true links and @xmath59 tadpoles has a support of @xmath217 cubes ( 2 of them being @xmath252 and @xmath252 ) and the true links make a tree on the support of @xmath141 . if we take into account the proximity links then two connected links in the tree on @xmath141 are adjacent instead of sharing a common cube , we will forget about this since it would induce at most a factor @xmath276 .",
    "the links in @xmath141 are ordered , but we can take them unordered by eating up the @xmath277 we have in @xmath278 .",
    "then the sum over @xmath141 can be decomposed as    * choose @xmath279 * choose @xmath280 cubes @xmath281 * chose a tree @xmath282 on @xmath283 * choose @xmath284 * place @xmath59 tad - poles on @xmath285    we can perform the sum on tadpole configurations because for @xmath59 tadpoles , we have a factor @xmath286^{p}$ ] coming from the tadpoles and at most @xmath287 configurations .",
    "thus @xmath288^{m+1 } \\nonumber \\\\   & & \\quad   \\left[o(1 ) \\lambda^{1/4}\\right]^{m } \\left (   \\prod_{x \\in { \\cal t } } \\gamma_{x}\\right)\\end{aligned}\\ ] ] then we fix first the form of @xmath282 then we sum over the positions of @xmath204 ,  , @xmath289 .",
    "but since the cubes are now labeled we get @xmath290 the desired sum . @xmath291^{m } \\frac{1}{(m-1 ) ! }   \\sum_{\\cal t } \\sum_{(\\delta_{1 } , \\ldots , \\delta_{m-1 } ) }   \\left ( \\prod_{x \\in { \\cal t } } \\gamma_{x}\\right)\\ ] ] we choose @xmath252 as the root of our tree and suppose that the position of @xmath253 is not fixed .",
    "then the sum over the position of the cubes is made starting from the leaves thanks to the decaying factors @xmath292 ( cf .",
    "@xcite ) , this costs a factor @xmath293 .    finally , the sum over @xmath282 , which is a sum over unordered trees , is performed using cayley s theorem which states that there are @xmath294 such trees .",
    "@xmath291^{m } \\frac{(m+1)^{m-1}}{(m-1 ) ! } \\leqslant o(1 ) \\lambda^{1/4 } \\leqslant 1\\ ] ] for @xmath5 small enough .",
    "@xmath130    we note that we can perform the same expansion on @xmath295      we define @xmath296 we can combine equations ( [ complextrans ] ) , ( [ lsdecomp ] ) , ( [ eqldecoupling ] ) and ( [ yexpan ] ) to write @xmath297 where we pretend that the @xmath49 s are sharp otherwise we would have to deal with adjacent cubes but it s an irrelevant complication .",
    "furthermore , for the leftmost term we made a polymer expansion of @xmath298 instead of @xmath299 so that we can write @xmath300 as @xmath301    the crucial point here is to notice that for any cube @xmath46 , each term where @xmath46 appears in @xmath203 but not in @xmath302 pairs with a corresponding term where @xmath303 and @xmath304 ( _ i.e. _ @xmath46 has been killed in every polymer expansion ) .",
    "then the corresponding @xmath305 and @xmath306 add up back to 1 so that    @xmath307    the factor @xmath308 correponds to the translation of @xmath3 by @xmath309 , this is equivalent to have translated all the @xmath50 s by @xmath310 therefore we can write it as @xmath311 then we can perform the integration on all @xmath312 so that the normalization factor reduces to @xmath313 this amounts to pay a constant per cube of @xmath314 , this is done in @xmath203 with a fraction of the probabilistic factor coming from the large field condition and in @xmath315 with a fraction of the factor @xmath316 coming from the @xmath317 .",
    "the sums over the various @xmath318 s are controled by lemma [ lemresexp ] and we are left with a sum over a tree that we perform much in the same way we did in lemma [ lemresexp ] .",
    "indeed one can check that spatial decay appears through factors of the form @xmath319 thus we can extract decay in @xmath320 time a bound in @xmath321 when we combine all these factors .    yet we need some extra features to deal with the product of @xmath322 s , each of them being bounded in norm by @xmath323 for some @xmath324 .",
    "the factor @xmath325 can be controled with a small fraction of the probabilistic factor attached to the cube @xmath326    if a given @xmath327 appears at a large power it has necessarily a large number of links attached to it .",
    "because of the tree structure , the links must go further and further so that the decay of the links together with the gaussian measure allow to control the factorial coming from the accumulation of fields .",
    "this is quite standard and the reader can refer to @xcite for instance .    finally we can write @xmath300 as a sum over polymers of the form @xmath328^{n_{3 } } }      \\sum_{y \\in { \\cal a}^{*}(\\delta_{in } , \\delta_{out } ) } \\lambda^{c_{2}|y| }      \\gamma_{y } t(y ) \\end{aligned}\\ ] ] where @xmath138 and @xmath139 are small constant , @xmath140 has decay in the spatial extension of @xmath141 and @xmath329 is bounded .",
    "we are interested now in the particular case @xmath100 where    * @xmath101 is a ultra - violet regularized inverse laplacian , that we will note @xmath331 * @xmath108 is an infra - red cut - off that forces @xmath332 * @xmath3 has covariance @xmath11 which is a @xmath47 approximation of a @xmath111-function * @xmath114 is an even integer greater than 2 , and @xmath333 is such that @xmath116    for each @xmath334 , we construct a smooth partition of unity into cubes of side @xmath335 which form a lattice @xmath336 . it follows a decomposition of @xmath3 in fields @xmath337 and we will assume for simplicity that for @xmath338 and @xmath339 @xmath340 even if it is not totally true because of irrelevant border effects .",
    "we make a partition of unity according to the size of @xmath341 thanks to a function @xmath342 which satisfies    * @xmath342 is in @xmath343 with value in @xmath344 $ ] * @xmath342 has its support inside @xmath345 $ ] and is equal to 1 on @xmath344 $ ] * the @xmath25 norm of the derivatives of @xmath342 does not grow too fast    then we construct @xmath346",
    "\\\\      \\hat{\\eta}_{j}(p)&=&\\hat{\\eta } \\left[m^{2j}(p^{2}-e)^{2}\\right ] -         \\hat{\\eta } \\left[m^{2(j+1 ) } ( p^{2}-e)^{2 } \\right ]         \\quad \\mbox{for } j > 0 \\\\",
    "\\end{array }    \\right.\\ ] ] in order to shorten expressions , we assume that @xmath347    we expect that most of the physics will come from the neighborhood of the singularity @xmath348 of the free propagator . as an operator in momentum space , @xmath3 has a kernel @xmath349 .",
    "but since @xmath59 and @xmath60 have more or less the same norm , there are only two configurations which give the sum @xmath350 ( cf @xcite ) .",
    "we can see this in another way .",
    "if we make perturbations and integrate on @xmath3 we will get feynman graphs with four - legged vertices where the incoming momenta have a fixed norm and must add to zero ( or almost zero ) because of ( approximate ) translation invariance .",
    "then the four momenta approximately form a rhombus which happens to be a parallelogram .",
    "it implies that they must be more or less opposite 2 by 2 .",
    "thus the problem looks like a vectorial model because the angular direction of the momentum is preserved .    in order to have this feature more explicit , we decompose the slice @xmath351 into @xmath352 angular sectors .",
    "we introduce @xmath353 with    * @xmath353 is an even function in @xmath354 with value in @xmath344 $ ] * @xmath353 has its support inside @xmath355 $ ] and is equal to 1 on @xmath356 $ ] * @xmath357 for @xmath358 * the @xmath25 norm of the derivatives of @xmath353 does not grow too fast    then we define @xmath359 and construct sectors @xmath360 of angular width @xmath361 centered around @xmath362 ( identifying @xmath363 and @xmath364 ) , with @xmath365 .",
    "@xmath366    afterwards , we define the operators @xmath367 s by their kernel @xmath368 they form a positive , self - adjoint partition of identity .",
    "we will map our problem to an operator - valued matrix problem with the following lemma whose proof is quite obvious .",
    "+ let @xmath370 be an hilbert space and suppose that we have a set of indices @xmath371 and a partition of unity @xmath372 where @xmath373 is the identity in @xmath374 and the @xmath375 s are self - adjoint positive operators .    for all @xmath376 ,",
    "we define @xmath377 then @xmath370 and @xmath374 are naturally isomorphic to @xmath378 and @xmath379 thanks to @xmath380    in our case , we define @xmath381 as the set of sectors in the slice @xmath382 and @xmath383 so that we can construct the operator - valued matrices @xmath384 s as @xmath385    for a slice @xmath386 , we define the enlarged slice @xmath387 then an angular sector @xmath388 of @xmath386 has a natural extension into an angular sector @xmath389 of @xmath390 and we have the corresponding operator @xmath391 .",
    "let @xmath393 be defined by @xmath394 where the @xmath111 are kronecker s ones .",
    "we can remark that @xmath395    then we have the following large deviation result    [ thprobv ]   + there exists constants @xmath55 and @xmath396 such that for all @xmath16 , @xmath397 , @xmath398 and @xmath399 @xmath400 where @xmath401 behaves like @xmath402 .",
    "_ proof _ + we use the bound @xmath403^{m_{0}}\\ ] ] where @xmath404 thus for any @xmath405 @xmath406^{m_{0 } }       d\\mu_{\\xi_{\\lambda } } ( v)\\end{aligned}\\ ] ]    let us note @xmath407^{m_{0 } }       \\!\\!\\!\\ !",
    "=       \\mbox{tr } \\left[\\sum_{\\alpha } ( \\eta_{\\alpha}^{j})^{2 } \\ ,",
    "v_{\\delta } \\sum_{\\beta } ( \\eta_{\\beta}^{k})^{2 } \\ ,        v_{\\delta } \\right]^{m_{0}}\\ ] ]",
    "we have the following lemma    [ lemproba ]   + there exists a constant @xmath55 such that for all @xmath405 we have the following bound @xmath408\\ ] ]    this lemma is the core of the demonstration but its proof is quite long so that we postpone it until the end of this part .",
    "it leads to @xmath409\\ ] ] we take @xmath410 and use the rough bound @xmath411 to get the desired estimate .",
    "@xmath130    in fact , in the proof of lemma [ lemproba ]",
    "it is easy to see that @xmath412 and @xmath413 can be replaced by @xmath414 and @xmath415 with the same result .",
    "furthermore , thanks to the locality of @xmath3 and to the decay of the @xmath416 s , the sum of several @xmath417 s is more or less an orthogonal sum .",
    "more precisely , for any cube @xmath252 we define @xmath418 as the set of cubes of @xmath419 which are contained in @xmath252 . then given two sets @xmath420 and @xmath421 and their smoothed characteristic functions @xmath422 and",
    "@xmath423 we have    [ lemortho ]   + for any @xmath185 and @xmath55 there is a constant @xmath424 such that for any @xmath397 and @xmath425 @xmath426\\left[1 + m^{-nk } d(\\omega_{2},\\delta_{0})^{n } \\right ] }        \\nonumber \\\\     & & \\quad \\max \\left [ \\|\\bar{\\eta}_{j }       v_{\\delta_{0 } } \\bar{\\eta}_{k}\\| ,       \\sup_{{m < j } \\atop { n < k } } \\sup_{\\delta \\in d_{m \\wedge n}(\\delta_{0 } ) }       \\!\\ !",
    "m^{-c(j - m+k - n ) } \\|\\eta_{m } v_{\\delta } \\eta_{n}\\| \\right ] \\end{aligned}\\ ] ] where @xmath427 .",
    "_ proof _ + we introduce @xmath428 a @xmath47 function equal to 1 on the support of @xmath429 then we write @xmath430    afterwards we introduce the sectors and the matrix formulation and we notice that when we want to compute for instance the norm of the function @xmath431 , momentum conservation tells us that we can convolve @xmath428 by a function which is restricted in momentum space to the neighborhood of @xmath432 . in this way it is quite easy to see that we can extract at the same time spatial decay and momentum conservation decay .",
    "let @xmath433 , we call @xmath434 and @xmath435 the events @xmath436 \\\\    y_{c_{y } , a } & = & \\left[\\| d_{\\lambda , \\mu } \\eta_{e } v_{\\delta_{0 } } \\eta_{e }       d_{\\lambda , \\mu}\\| \\geqslant a c_{y } j_{0 } m^{j_{0}/2 } \\right]\\end{aligned}\\ ] ] we will note @xmath437 the contrary event of @xmath438 .    theorem [ thprobv ] tells us that @xmath439 one can see that thanks to lemma [ lemortho ] , @xmath440 implies @xmath441 .",
    "thus if we call @xmath442 @xmath443    furthermore , if we work with respect to @xmath440 which is stronger than @xmath444 everything goes as if one had @xmath70^{n_{1 } } \\left[1 + l^{-1 }      d_{\\lambda}(\\delta_{2 } , \\delta_{3})\\right]^{n_{1}}}\\ ] ] thus we will be able to apply theorem [ thre ] with an effective coupling constant @xmath445 and a length scale @xmath446 .",
    "if we want to make perturbations it is clever to perturb around the expected green s function without cut - off , _",
    "i.e. _ we write @xmath447 where @xmath448 is the expected contribution of the tadpole given by the self - consistent condition @xmath449    afterwards , when we compute the perturbative expansion , the tadpole with cut - off will eat up a fraction @xmath450 of the counter - term so that @xmath451 @xmath130    in fact since the tadpole has a real part , it implies that we should also renormalize the energy by a shift @xmath452 \\right)\\ ] ]      we will note @xmath453 , @xmath454 and @xmath124 as either @xmath455 or @xmath456 .",
    "we can perform the integration on @xmath50 so that @xmath457 appears as a sum of feynman graphs .",
    "@xmath458    where a solid line stands for a @xmath459 , a dashed line stands for a @xmath460 and a wavy line represents the insertion of a @xmath50 . in the following",
    ", we will prove the theorem in infinite volume with @xmath3 having a covariance @xmath111 in order to have shorter expressions .",
    "the proof can then easily be extended to short range covariances and finite volume except for the first few slices where one must pay attention to the ultra - violet cut - off but this is irrelevant because it will cost only a factor @xmath56 .",
    "the integration on @xmath50 consists in contracting the wavy lines together , then both ends are identified and bear an extra @xmath49 which restricts their position .",
    "the @xmath124 s will stand as propagators and the contraction of the @xmath50 s will give birth to 4-legged vertices .",
    "first , we notice that if we note @xmath461 the opposite sector of @xmath462 @xmath463    then we put an orientation on each propagator , so that if a @xmath464 goes from a vertex at @xmath58 to a vertex at @xmath465 it gives a @xmath466 , _",
    "i.e. _ it is equivalent to have an incoming @xmath467 at @xmath58 and an incoming @xmath464 at @xmath465 .",
    "now , for a given vertex with incoming propagators @xmath468 ,",
    "@xmath469 , @xmath470 , @xmath471 , the spatial integration over its position gives a term of the form @xmath472    in momentum space , it becomes @xmath473 where we use the same notation for a function and its fourier transform .    in @xmath36-space",
    ", @xmath49 is a @xmath47 function with support inside a box of side @xmath474 , it means that in momentum space , it is a @xmath13 function with fast decay over a scale @xmath475 .",
    "thus for all @xmath185 there exists @xmath476 such that @xmath477    we make a decomposition of @xmath49 @xmath478 where @xmath479 has its support inside the ball of radius @xmath480 , @xmath481 has its support outside the ball of radius @xmath482 and @xmath483 forces @xmath484 to be in the interval @xmath485 $ ] . in this way",
    ", we can decompose each vertex @xmath486 into a sum of vertices @xmath487 , where a vertex @xmath487 forces momentum conservation up to @xmath488 and has a factor coming from @xmath489 we split the factor in order to have a small factor per vertex and yet retain some decay to perform the sum on @xmath490 .",
    "a graph will present tadpoles when two neighboring @xmath50 s contract together thus yielding a @xmath491 .",
    "suppose that we have a @xmath382-tadpole then at the corresponding vertex we will have something of the form @xmath492 between the two @xmath456 s , momentum will be preserved up to @xmath493 which in most case is much smaller than @xmath494 so that @xmath495 is very close to @xmath496 .",
    "then we would like to forget about @xmath497 by summing over @xmath462 and see the whole thing as a kind of new @xmath498 . now if per chance the new @xmath498 makes a tadpole we will erase it , and so on recursively .",
    "first , we define the propagators as propagators ( or links ) of order @xmath188 @xmath499    then we define links of order @xmath500 @xmath501 where we do nt write the momentum conservation indices for shortness .",
    "we have a similar definition for @xmath502 ( obtained by erasing @xmath60 @xmath456-tadpoles of order 0 ) .",
    "we will note @xmath503 to indicate that momentum is preserved up to @xmath504 between the leftmost and the rightmost @xmath124 s or @xmath505 if momentum conservation is worse than @xmath494 .",
    "now , we can iterate the process in an obvious way .",
    "yet , we must add an important restriction : we will erase a @xmath506 tadpole only if it is attached to a @xmath507 vertex .     + there exist constants @xmath72 and @xmath508 ( independent of @xmath382 and @xmath509 ) such that for any tadpole obtained by erasing a total of @xmath59 @xmath455-tadpoles and @xmath60 @xmath456-tadpoles we have the following bond @xmath510    where @xmath126 is a small factor coming from the various @xmath483 that appear in the expression of @xmath124 .",
    "thus , @xmath175 gets smaller as momentum conservation gets worse .",
    "_ proof _ + first we will prove this result when momentum is well preserved , _",
    "i.e. _ up to @xmath494 at worst , then we will see what has to be adapted when there is a bad momentum conservation .",
    "the proof is by induction on the order of the tadpole .",
    "we define @xmath72 , @xmath508 and @xmath511 such that @xmath512 it is easy to see that for level 0 tadpoles @xmath513    now , consider @xmath514 a @xmath455-tadpole of order @xmath216 and weight @xmath515 obtained by erasing @xmath185 @xmath456-tadpoles of order @xmath280 and weights @xmath516 ,  , @xmath517 .",
    "we have @xmath518 the expression of @xmath519 will be of the form @xmath520    since we supposed that we have momentum conservation up to @xmath494 , the @xmath521 s will be either @xmath522 or one of its neighbors and @xmath523 will be either @xmath524 or one of its neighbors .",
    "thus the sum on sector attribution will give a factor @xmath525 .",
    "we have @xmath526 @xmath455 s but only @xmath185 spatial integrations because we have a tadpole .",
    "this gives a factor @xmath527 ( we forget about the momentum conservation factor for the moment ) .",
    "finally the @xmath528 s bring their factor so that @xmath529 which is precisely what we want .",
    "then we can do the same for the @xmath530 s .",
    "now we must consider the cases with bad momentum conservation .",
    "first , let us suppose that momentum conservation is bad overall for @xmath519 but was good for the @xmath528 s , then the previous argument will work except if there are some @xmath507 vertices . in this case",
    "we will have to pay a factor @xmath352 to find the following @xmath521 instead of a factor 3 .",
    "but from the corresponding @xmath531 we have a small factor @xmath532 from which we can take a fraction to pay the @xmath352 and retain a small factor for @xmath126",
    ".    finally , if a @xmath528 has a bad momentum conservation it is necessarily attached to a @xmath507 vertex ( otherwise we would not erase it ) . in this case",
    "we must pay a factor @xmath533 ( to find @xmath523 and @xmath521 ) but again we can take a fraction of the factor of @xmath531 to do so .",
    "when tadpole elimination has been completed , we have erased @xmath534 @xmath455-tadpoles and @xmath535 @xmath456-tadpoles and we are left with @xmath536 vertices linked together by @xmath537 @xmath455 s and @xmath537 @xmath456 s ( a tadpole which has not been erased being seen as a propagator ) .    for a @xmath538 ,",
    "it is quite easy to see that to integrate on @xmath57 with fixed @xmath36 amounts more or less to the same problem for @xmath539 and that to find @xmath540 knowing @xmath462 costs a factor @xmath541 .",
    "[ sconserv ]   + _ let ( @xmath542 ,  , @xmath543 ) be a quadruplet of sectors of the enlarged slice @xmath390 and @xmath544 such that there are @xmath545 , ",
    ", @xmath546 verifying @xmath547",
    "_    then we can find @xmath548 satisfying @xmath549 where @xmath550 and @xmath551 are some constants independent of @xmath227 and @xmath552 .",
    "_ proof _ + if we can prove the result for @xmath553 then we will be able to enlarge the result to any @xmath227 provided maybe we take some slightly bigger @xmath550 and @xmath551 .",
    "therefore we assume that this is the case in the following .",
    "we define @xmath554 by    * @xmath555 * @xmath556 * @xmath557 * @xmath558    then , if @xmath559 we exchange @xmath560 and @xmath561 .",
    "a sector @xmath562 is included in a tube , of center @xmath563 and whose direction is orthogonal to the direction @xmath564 , of size @xmath565    we define @xmath566    if we can prove that @xmath549 then we will be able to conclude , with @xmath567 and @xmath568 .",
    "it is easy to check that by construction , we have    * @xmath569 * @xmath570    we have a trivial bound @xmath571    therefore @xmath572    we can see that @xmath35 is very well conserved .",
    "if @xmath573 then @xmath574 .    otherwise , let us remark that @xmath575 is at a distance at most @xmath576 from a rhombus @xmath577 of center @xmath578 and of diagonals @xmath579    then , @xmath580 is at a distance at most @xmath581 from a rectangle @xmath582 of center @xmath583 and of sides @xmath584    since @xmath585 , we have @xmath586 . we define a @xmath58 axis in the direction @xmath587 .",
    "@xmath588 has a @xmath58 coordinate @xmath589 .",
    "this leads to the condition @xmath590    let us note that @xmath591 is an increasing function of @xmath592 and that we have @xmath593 we take @xmath594 .",
    "@xmath595 since we are in the case @xmath596 , for @xmath597 and @xmath598 large enough we will have @xmath599    therefore we must have @xmath600 which allows us to conclude .",
    "the previous section shows that , at each vertex , momenta come approximately by pairs of opposite sectors .",
    "thus , for all the vertices which havent been erased by the tadpole elimination process , we can choose by a factor @xmath601 how to pair the sectors .",
    "then we split the vertices in two half - vertices according to this pairing .",
    "we represent graphically this as    this gives @xmath602 ( split ) graphs that we will consider as our basic graphs in the following .",
    "a graph is decomposed into a number of _ momentum cycles _ connected together by wavy lines .",
    "we will follow those cycles to fix momentum sectors . finding the enlarged sectors ( of level @xmath382 ) will cost a factor @xmath352 per cycle times a constant per vertex",
    ". then we will pay an extra @xmath603 for each @xmath456 propagator to find its sector .",
    "we define @xmath604 the total number of momentum cycles that we decompose into @xmath605 tadpoles , @xmath551 bubbles ( with 2 vertices ) and @xmath227 large cycles ( with 3 or more vertices ) .",
    "we have @xmath606 and the sector attribution costs @xmath607 notice that the constant has an exponent @xmath405 because of the tadpole elimination process .",
    "the spatial integration of the vertices will be made with the short @xmath455 links whenever possible .",
    "we can decompose each graph into @xmath455-cycles linked together by @xmath456 links ( because there are @xmath270 incoming @xmath455 s at each vertex ) , this allows to integrate all the vertices but one per cycle with a @xmath455 link .",
    "the total cost is ( noticing that the last vertex is integrated in the whole cube @xmath46 ) @xmath608 where @xmath609 is the total number of short cycles that we decompose into @xmath610 short tadpoles , @xmath611 short bubbles and @xmath612 large short cycles . @xmath613",
    "the scaling of the tadpoles and the propagators give a factor @xmath614    tadpoles that have been obtained by erasing a few vertices ( say @xmath615 for instance ) will have an extra small factor because they strongly violate momentum conservation , we can take it to be a power of @xmath616 .",
    "tadpoles with higher weights will not have this good factor but we will see that they bring a better combinatoric .",
    "the @xmath605 momentum tadpoles will consist in @xmath617 low weight ones and @xmath618 others while the @xmath610 short tadpoles split into @xmath619 low weight ones and @xmath620 others .",
    "we can manage to have a factor @xmath621    if we have a short bubble we will have four incoming long propagators whose momenta must add up to zero up to @xmath622 .",
    "if we apply lemma [ sconserv ] , we can see that knowing 3 of these momenta it cost only a factor @xmath623 to find the fourth momentum sparing us a factor @xmath603 obtained by navely fixing first the enlarged sector at slice @xmath382 .",
    "if the bubble has a weight @xmath59 , _ i.e. _ the two short propagators have been obtained after erasing @xmath59 vertices , and a momentum conservation worse than @xmath624 then the small factor of bad momentum conservation will pay for the @xmath625 .",
    "we will have @xmath626 such good bubbles , each of them bringing a factor @xmath627 .",
    "in addition , we will have @xmath628 bad bubble of weight greater than @xmath629 for which we earn nothing and @xmath630 bad bubbles of weight @xmath631 bringing a factor @xmath632 .",
    "this gives a factor @xmath633    finally we have the following bound for the contribution of a graph @xmath634    if we use equations ( [ momcycle ] ) and ( [ shortcycle ] ) we obtain @xmath635    we will take @xmath636 so that @xmath637 .",
    "furthermore , @xmath605 and @xmath610 are at most equal to @xmath638 thus @xmath639 .",
    "it allows us to rewrite the bound @xmath640       + let @xmath641 be the number of ways to contract @xmath642 adjacent @xmath3 s so as to make only generalized tadpoles .",
    "we have @xmath643    _ proof _ + it is easy to see that a good contraction scheme , _",
    "i.e _ one that gives only generalized tadpoles , corresponds to have no crossing contractions .",
    "it means that if we label the fields @xmath644 according to their order and if @xmath645 and @xmath646 contract respectively to @xmath647 and @xmath648 then @xmath649 we have @xmath650 . for @xmath651",
    ", we contract first @xmath652 to some @xmath645 .",
    "@xmath653 will necessarily contract among themselves making only generalized tadpoles and so will do @xmath654 .",
    "thus @xmath655 is necessarily odd and we have @xmath656 where by convention @xmath657 .",
    "we introduce the generating function @xmath658 the recursion formula ( [ trecursion ] ) can be translated into an equation for @xmath605 which is @xmath659 whose resolution yields @xmath660 since the second solution is analytic around @xmath661 , we can take it as @xmath662 and the coefficients of its power expansion will give us @xmath641 .",
    "an easy computation leads to the desired formula .",
    "@xmath130     + the number @xmath663 of graphs with @xmath254 possible momentum bubbles obtained in the contraction of a cycle of @xmath664 @xmath3 s has the following bound @xmath665    _ proof _ + first lets us remark that bubbles come in chains ( possibly with a tadpole at one end ) of two possible types    * type 1 : * type 2 :    where a solid line stands here either for a @xmath455 or a @xmath456 .",
    "we have two special cases    * which can be seen as a type 1 chain * which can generate only one momentum bubble so that we can see it as a type 2 chain of length 1 .    having chosen the @xmath3 s there are only two contraction schemes that yield a type 1 chain and a unique contraction scheme for a type 2 chain .",
    "if we fix explicitly the subgraphs corresponding to @xmath254 bubbles and contract the remaining @xmath3 s in any way we will get all the desired graphs plus some extra ones so that we can bound @xmath663 .",
    "we construct @xmath666 type 1 chains of lengths @xmath667 ,  , @xmath668 and @xmath669 type 2 chains of lengths @xmath670 , ",
    ", @xmath671 .",
    "we set @xmath672    to count the contraction schemes , first we cut the cycle of @xmath664 @xmath3 s into a sequence of @xmath3 s ( there are @xmath664 ways to do so ) . then it is easy to check that in order to build a type 1 chain we must choose two sets @xmath673 and @xmath674 of @xmath675 adjacent @xmath3 s while for a type 2 we need a set @xmath676 of @xmath677 adjacent @xmath3 s .",
    "we distribute those @xmath678 objects in @xmath679 boxes in an ordered way , and for the @xmath680 type 1 chain the respective order of @xmath673 and @xmath674 will fix the contraction scheme . then , there remain @xmath681 @xmath3 s to contract so that we have the following number of configurations @xmath682 we can compute this @xmath683!}{[2(m - b)-2r_{1}-r_{2 } ] ! ( 2r_{1 } ) ! r_{2 } ! } \\nonumber \\\\    & & \\frac{(2 r_{1})!}{r_{1 } ! } \\frac{[2(m - b - r_{1})]!}{2^{m - b - r_{1 } }      ( m - b - r_{1 } ) ! } \\\\     & \\leqslant & 2 m \\!\\ !",
    "\\sum_{{r_{1}\\leqslant      b_{1 } } \\atop { r_{2}\\leqslant b_{2 } } }       { b_{1}-1 \\choose r_{1}-1 } { b_{2}-1 \\choose r_{2}-1 } 3^{2(m - b ) }       2^{2r_{1 } } r_{1 }",
    "! 2^{m - b - r_{1 } } ( m - b - r_{1 } ) ! \\\\   & \\leqslant & 2 m \\!\\ !",
    "\\sum_{b_{1}+b_{2}=b } 2^{b_{1}-1 } 2^{b_{2}-1 } 9^{m - b }      2^{m } ( m - b ) ! \\\\   & \\leqslant & 2 m ( b+1 ) 18^{m } ( m - b ) !   \\end{aligned}\\ ] ] @xmath130      now we can achieve the proof of lemma [ lemproba ] in bounding @xmath684    in order to compute this sum , we fix first @xmath534 , @xmath535 and @xmath685 , where @xmath685 is the number of possible momentum bubbles and therefore is greater than @xmath551 .",
    "then we define the set @xmath686 has the set of graphs with the corresponding @xmath534 , @xmath535 and @xmath685 and for which the erased tadpoles form @xmath185 sets of @xmath687 adjacent @xmath3 s .",
    "we can write @xmath688 to bound @xmath689 we notice that when a graph has a bad bubble of weight @xmath631 it means that we have erased two set of generalized tadpoles @xmath690 and @xmath691 on the two propagators of the bubble with @xmath692 .",
    "thus we have a corresponding factor @xmath693 which control the bad factor @xmath694 of the bad bubble so that @xmath695    the number of graphs in @xmath696 has the following bound @xmath697 \\leqslant 2^{m_{0 } } \\prod t(q_{i } ) 3^{m'_{0 } } c^{m'_{0 } } ( m'_{0}-\\bar{b } ) ! \\leqslant c^{m_{0 } } \\prod \\frac{2^{2q_{i}}}{q_{i}+1 } ( m'_{0}-\\bar{b})!\\ ] ]    this leads to @xmath698    summing on @xmath534 and @xmath535 is equivalent to sum over @xmath537 and @xmath535 with @xmath699 . the sum over @xmath685 is roughly evaluated by taking the supremum over @xmath685 , the result depends whether @xmath537 is greater than @xmath700 or not .",
    "@xmath701   \\\\    & \\leqslant & c^{m_{0 } } m^{-m_{0}j } \\sum_{m'_{0 } , t_{k } }      m^{-m'_{0}(k - j)/2 } m^{-t_{k}(k - j ) } \\left [ 1 + 1_{(m'_{0 } > m^{j/6 } ) }      m^{-m'_{0}j/6 } ( m'_{0}-\\bar{b } ) ! \\right]\\end{aligned}\\ ] ]    finally , the sum over @xmath535 is easy and we bound the sum over @xmath537 by finding the supremum .",
    "one can check that it gives the announced result .",
    "understanding the effect of perturbations on the free spectrum of hamiltonian operators is an outstanding challenge .",
    "we think that for the two - dimensional case , this paper can help to control the model up to quite close to the scale of the expected `` mass '' ( _ i.e. _ imaginary part ) so that in studying the full model , one can focus on the thin slice of momentum @xmath702 .",
    "then we think that a key of the problem lies in the fact that in this case the potential is very close ( in momentum space ) to a large hermitian random matrix with almost independent entries and therefore we can connect our problem to the much better understood domain of random matrices or equivalently use the vector model picture of the problem .",
    "one can note that in dimension @xmath99 , the interpretation in term of random matrices can also be helpful ( cf .",
    "@xcite ) , but then one has to deal with constrained matrices whose entries are no longer independent .",
    "this paper is part of a larger program in collaboration with j. magnen and v. rivasseau to apply rigorous renormalization group methods to the study of disordered systems , the ultimate goal being to devise tools to construct extended states in weakly disordered systems in dimension @xmath703 .",
    "they played an important part in getting these results and i thank them very much for their help .",
    "fmrt2 m. aizenman , in _ the state of matter _ , world scientific ( 1994 ) 367 a. abdesselam and v. rivasseau `` an explicit large versus small field multiscale cluster expansion '' ( hep - th/9605094 ) to appear in _ reviews in math .",
    "_ d.brydges , `` a short course on cluster expansion '' , in _ critical phenomena , random systems , gauge theories _",
    ", les houches session xliii , 1984 , elsevier science publishers ( 1986 ) j.m .",
    "combes and l. thomas `` asymptotic behaviour of eigenfunctions for multiparticle schrdinger operators '' , _ comm .",
    "* 34 * ( 1973 ) 251 - 270 j.feldman , j. magnen , v. rivasseau and e. trubowitz , in _ the state of matter _ , world scientific ( 1994 ) 293 j.feldman , j. magnen , v. rivasseau and e. trubowitz , _ europhys .",
    "* 24 * ( 1993 ) 521 e. fradkin , in _ dveloppement rcents en thorie des champs et physique statistique _ , les houches xxxix , north holland ( 1984 ) m.l .",
    "mehta , _ random matrices _ , academic press , boston ( 1991 ) j. magnen , g. poirot and v. rivasseau `` the anderson model as a matrix model '' ( cond - mat/9611236 ) to appear in _ advanced quantum field theory _ , la londe les maures ( in memory of c. itzykson ) elsevier ( 1996 ) g. parisi , in _ dveloppement rcents en thorie des champs et physique statistique _ , les houches xxxix , north holland ( 1984 ) v. rivasseau , `` cluster expansions with large / small field conditions '' , minicourse in vanvouver summer school ( 1993 ) v. rivasseau , _ from perturbative to constructive renormalization _ , princeton university press ( 1991 )",
    "thouless , _ phys . report _",
    "* 13 * ( 1974 ) 93"
  ],
  "abstract_text": [
    "<S> in this paper we develop a polymer expansion with large / small field conditions for the mean resolvent of a weakly disordered system . then we show that we can apply our result to a two - dimensional model , for energies outside the unperturbed spectrum or in the free spectrum provided the potential has an infra - red cut - off . </S>",
    "<S> this leads to an asymptotic expansion for the density of states . </S>",
    "<S> we believe this is an important first step towards a rigorous analysis of the density of states in the free spectrum of a random schrdinger operator at weak disorder .    </S>",
    "<S> = -0.54 truecm = -0.54 truecm = 0 truecm = 0 truecm = 0 truecm = 0 truecm = 23 truecm = 17 truecm    amssym.def amssym.tex </S>"
  ]
}