{
  "article_text": [
    "monte carlo methods are among the core techniques for studying the statics and dynamics of particle systems in classical and quantum physics , in particular for systems in statistical physics @xcite .",
    "although for a few problems simple sampling is reasonably efficient , most applications are based on importance sampling techniques . among them , markov chain monte carlo ( mcmc ) is by far the most widely used approach in statistical physics . in quantum monte carlo ,",
    "on the other hand , one can evaluate the wave function in a path - integral formulation in imaginary time by a swarm of particles diffusing in configuration space that undergo a sequence of birth - death processes @xcite .",
    "this is a special case of a procedure more generally known as sequential monte carlo @xcite .",
    "such procedures have been more reluctantly adopted in statistical physics applications , but they have gained some traction recently , for example in variants of `` go with the winners '' simulations @xcite . in sequential monte carlo , configurations are gradually built in possibly biased steps , sequentially accumulating weights that multiply configurations in the final averages . in many applications such weights",
    "fluctuate wildly , thus leading to rather unstable results . in",
    "the `` go with the winners '' approaches , configurations are selectively cloned or pruned in accordance with their weight to tame these fluctuations , a procedure often referred to as population control .",
    "one recent approach of this type is the population annealing ( pa ) algorithm @xcite .",
    "there , a large number of configurations are prepared in independent equilibrium configurations , for instance at infinite temperature .",
    "each configuration evolves according to a standard mcmc approach at the given temperature .",
    "the population is then gradually cooled , and each configuration sequentially builds up a weight depending on its energy at the instant of temperature change .",
    "population control is used to keep weight fluctuations under control .",
    "we here focus on a variant where `` perfect '' population control is used at each temperature step such that all weights remain equal to unity at all times @xcite .",
    "the approach has been successfully used for equilibrium simulations of spin - glass systems @xcite , and also for finding ground states in spin glasses and other systems with frustrated interactions @xcite .",
    "recently , we have studied the behavior of pa for simulations of the 2d ising model , analyzing systematically the dependence on the population size and annealing protocol , and proposed a number of improvements @xcite .",
    "the era of serial computing came to an end in the early 2000s when cpu clock frequencies first hit the `` wall '' of about 3.5 ghz , beyond which heat dissipation becomes unfeasible with conventional techniques and the power consumption increases too steeply .",
    "while moore s law @xcite predicting an exponential growth of the number of transistors in an integrated circuit continues to hold , the resulting exponential growth of computational power seen for cpus essentially stopped being an increase of serial performance ( for instance through the increase of clock speeds ) and now translates into a corresponding increase in the number of parallel cores or other compute units .",
    "thus the comfortable situation where the same old code or , at least , the same old algorithm could be run on more modern hardware with exponentially decreasing run times with every new generation of machine , has come to an end .",
    "instead , it has become necessary to design and implement solutions to our computational problems that scale well up to thousands or maybe even millions of cores @xcite . a computational environment that recently proved to be a particularly useful pathway towards massively parallel computing",
    "are graphics processing units ( gpus ) and similar accelerator devices .",
    "they feature a much higher density of actual compute units than cpus , at the expense of reduced cache memories and control logic units that are mostly useful for accelerating serial and unpredictable loads , and are hence very well suited for the needs of scientific computing @xcite . in statistical physics ,",
    "significant speed - ups have been observed for ising model simulations with local @xcite as well as non - local @xcite udpate algorithms ; for continuous - spin systems @xcite ; for spin glasses @xcite and random - field models @xcite ; for potts systems @xcite ; for polymers @xcite and many other applications .    the pa algorithm that requires",
    "the parallel simulation of a population of tens of thousands up to millions of replicas appears to be a perfect match for this new type of computational resource .",
    "the quality of approximation increases with population size @xcite such that a higher parallel load is clearly advantageous . as we will show below",
    ", we observe a gpu speed - up of up to 230 times over a serial cpu based code , thus bringing the wall - clock time for typical calculations of the 2d ising system considered here down to minutes in many cases . for such models with ising spins ,",
    "the additional application of multi - spin coding yields a further up to 10-fold speed - up , such that we reach a peak performance of 10 ps per spin flip of the whole pa simulation code , including the resampling and measurement parts .",
    "we provide a flexible implementation that can be configured using command - line switches and should be easily adaptable to simulations of related models such as 3d ising systems , potts and @xmath8 models , and spin glasses . in extension to the standard pa algorithm , our code also allows for the adaptive choice of inverse temperature steps and an analysis of the simulation results with a multi - histogram approach .",
    "the rest of the paper is organized as follows . in sec .",
    "[ sec : algorithm ] we summarize the pa algorithm and the extensions employed here .",
    "section [ sec : implementation ] discusses our implementation on gpu , while sec .",
    "[ sec : msc ] introduces the program variant that employs multi - spin coding . in sec .",
    "[ sec : performance ] we investigate the performance and reliability of our code .",
    "finally , sec .",
    "[ sec : conclusions ] contains our conclusions .",
    "the population annealing method was first discussed by iba @xcite in the general context of population - based algorithms and later applied to spin glasses by hukushima and iba @xcite .",
    "more recently , machta @xcite used a method that avoids the recording of weight functions through population control in every step .",
    "this is the variant we discuss and implement here .",
    "as outlined above , the approach is a hybrid of sequential algorithm and mcmc that simulates a population of configurations at each time , updating them with mcmc methods and resampling the population periodically as the temperature is gradually lowered .",
    "the algorithm can be summarized as follows :    1 .",
    "set up an equilibrium ensemble of @xmath9 independent copies ( replicas ) of the system at inverse temperature @xmath10 .",
    "typically @xmath11 , where this can be easily achieved .",
    "2 .   to create an approximately equilibrated sample at @xmath12 , resample configurations with their relative boltzmann weight @xmath13/q_i$ ] , where @xmath14 .",
    "\\label{eq : q}\\ ] ] 3 .",
    "update each replica by @xmath15 rounds of an mcmc algorithm at inverse temperature @xmath16 .",
    "4 .   calculate estimates for observable quantities @xmath17 as population averages @xmath18 .",
    "goto step 2 unless the target temperature @xmath19 has been reached .",
    "if we choose @xmath11 , equilibrium configurations for the replicas can be generated by simple sampling , i.e. , by assigning independent , purely random spin configurations to each copy .",
    "the resampling process in step 2 can be realized in different ways @xcite .",
    "here we use the following approach @xcite . for each replica",
    "@xmath20 in the population at inverse temperature @xmath21 we draw a random number @xmath22 uniformly in @xmath23 .",
    "the number of copies of replica @xmath20 in the new population is then taken to be @xmath24 where @xmath25 is renormalized to ensure that the population size stays close to the target value @xmath26 . here , @xmath27 denotes the largest integer that is less than or equal to @xmath28 ( i.e. , rounding down ) . the new population size is @xmath29 .",
    "this method requires only a single call to the random number generator for each replica in the current population and leads to very small fluctuations in the total population size .",
    "note that it is possible that @xmath30 , in which case the corresponding replica disappears from the population , while other configurations will be replicated several times . in the standard setup ,",
    "steps of equal size in inverse temperature are taken , i.e. , @xmath31 and @xmath32 is an adjustable parameter .",
    "we discuss an adaptive , automatic choice of temperature steps below in sec .",
    "[ sec : adaptive ] . in the code",
    "presented here , we use @xmath15 sweeps of metropolis single - spin flip updates to equilibrate each replica in each temperature step .",
    "other updates such as heat - bath dynamics or even non - local cluster moves could be employed easily as well .",
    "measurements are taken as population averages , and our code produces estimates for the following quantities , @xmath33 here , @xmath34 denotes the configurational energy and @xmath35 the configurational magnetization of replica @xmath20 , and @xmath36 is the number of spins .",
    "additionally , pa provides a natural estimate of the free energy , @xmath37 where @xmath38 is the partition function at inverse temperature @xmath10 , @xmath39 for ising spins and @xmath11 , and @xmath40 is the reweighting factor used in the resampling . from eqs .",
    "( [ eq : observables ] ) and ( [ eq : free - energy ] ) we can also compute the entropy per site via @xmath41      it was shown in ref .",
    "@xcite that one of the strengths of the pa approach is that by combining the data from independent runs not only statistical errors are decreased , but also systematic deviations can be reduced .",
    "this is the case if one uses weighted averages of results of independent runs . as was shown in refs .",
    "@xcite , an unbiased way of combining the results of @xmath42 independent runs of pa for the same system and target population size is to weight them by the free energies as estimated according to eq .",
    "( [ eq : free - energy ] ) , @xmath43 with @xmath44 where @xmath45 denotes the average of observable @xmath46 in simulation @xmath47 and @xmath48 the corresponding free - energy estimator according to eq .",
    "( [ eq : free - energy ] ) .",
    "the concept of weighted averages allows for an additional parallelization in splitting the total simulation into independent parts .",
    "the weighting ensures that this does not substantially degrade the quality of the results @xcite .",
    "note that the concept of weighted averages is more general than the pa approach @xcite , but for the present algorithm the necessary free - energy estimates are a free by - product of the simulation according to eq .  . in the implementation provided here ,",
    "multiple runs can be requested on the command line , but the weighted averaging of results is left to the user to perform separately .",
    "while an annealing cycle of the population is valid for any choice of the temperature sequence @xmath10 , @xmath49 , @xmath50 , and given a sufficiently large number @xmath15 of mcmc sweeps employed at each temperature it also leads to essentially unbiased estimates of observables , the resampling step is only effective if @xmath51 is sufficiently small @xcite .",
    "the optimal size of temperature steps will itself depend on temperature , and a uniform stepping is not in general ideal . as was recently shown in ref .",
    "@xcite uniform effectiveness of resampling is achieved by ensuring a constant overlap of the energy histograms of population members between the neighboring temperatures .",
    "this overlap can be computed from the reweighting factors before actually performing the resampling step , and one finds @xmath52}{r_{i-1}q(\\beta_{i-1},\\beta ' ) }    \\right).\\ ] ] clearly , @xmath53 , and one can use numerical root finding techniques such as , for instance , bisection search , to find @xmath54 such that @xmath55 and then set @xmath56 .",
    "values of @xmath57 provide sufficient histogram overlap without an unnecessary proliferation of temperature steps . in practice ,",
    "if @xmath42 runs are performed for additional averaging , our code used in adaptive mode decides about temperature steps only in the first run and keeps the temperature sequence fixed for the remaining passes .      as",
    "a pa sweep produces samples at a large number of closely spaced temperatures ( typically at least 100 , even for small systems ) , it is natural to combine these data to increase the accuracy and reduce statistical fluctuations in the spirit of the multi - histogram analysis of ferrenberg and swendsen @xcite . neglecting correlations between the data at different temperatures as well as the effect of autocorrelations , an optimized combination of histograms to yield an estimate of the density of states is given by @xcite @xmath58}.    \\label{eq : mhreq1}\\ ] ] here",
    ", @xmath59 denotes the total number of temperatures , and we assumed a normalization of the histogram at inverse temperature @xmath16 such that @xmath60 .",
    "we note that the storage requirements are moderate as at each time one only needs to store the sum of histograms up to the current temperature and not each histogram individually .",
    "generalizations to other quantities such as magnetizations are possible @xcite .",
    "for definiteness we focus on an implementation for the ferromagnetic , zero - field ising model on the square lattice with hamiltonian @xmath61 here , interactions are only between nearest neighbors @xmath62 and periodic boundary conditions are assumed . as is well known",
    ", this model undergoes a continuous phase transition at the inverse temperature @xmath63 @xcite .",
    "the question of how well suited population annealing is as a simulation technique to study this model and its transition is discussed in ref .",
    "@xcite . here , we are not concerned with this aspect , but we use this model as a convenient starting point since a wealth of exact or extremely accurate results are available for it as reference points , and a generalization of the code to other spin models and even more general systems such as polymers or particle systems should be rather straightforward .    [ [ general - considerations ] ] general considerations + + + + + + + + + + + + + + + + + + + + + +    inspecting the algorithm given in sec .",
    "[ sec : main - algorithm ] , one identifies three computationally demanding steps : a resampling of the population that involves the determination of weight factors and the copying of replicas , the update of individual configurations with mcmc moves ( i.e. , spin flips ) , and the measurement of observables . as we shall see below when reporting the performance results , most time ( on cpu or gpu ) is normally spent on spin flips ( see also ref .",
    "@xcite ) , while for typical choices of @xmath15 ( @xmath64 , say ) the resampling step and the measurements of the elementary quantities listed in eqs .",
    " are much less time consuming .",
    "these observations suggest to also choose the effort for optimization of each of these parts correspondingly .",
    "we hence first focus on the spin updates .",
    "one of the basic features of present day gpu devices that is of paramount importance for performance is the technique of _ latency hiding _ implemented in the scheduling algorithm @xcite .",
    "each time an elementary group of threads ( given by a warp of 32 threads on current nvidia gpus ) accesses some data in memory that is currently not cached , there is a latency of hundreds or even thousands of clock cycles until the read or write operation completes . instead of leaving the compute units idle",
    ", the scheduler puts the present warp in the queue and allows another warp that has already completed its data transaction to use the compute units .",
    "if only enough such thread groups are available , the compute cores will be kept constantly busy and hence the memory latencies are hidden away .",
    "good gpu performance thus requires to break the work into many threads , optimal performance is often only reached for thread numbers in excess of ten times the number of available physical cores @xcite .    a second crucial requirement for exploiting the full potential of gpus relates to the minimization of costly global memory accesses .",
    "this includes a reasonable level of compression of the data to be transferred for the updates . for the present problem with ising variables",
    "@xmath65 it suggests to use the narrowest available native data type to represent spins , which is an 8-bit integer , or to revert to a multi - spin coding approach .",
    "a discussion of the latter technique is postponed until the next section .",
    "further , the relative slowness of memory makes it useful to cache and re - use data as much as possible , which could involve using automatic caches or the user - managed cache known as shared memory @xcite .",
    "finally , it implies optimization geared towards increasing the locality of memory operations as each direct access to global memory ( implying a cache miss ) fetches a full cache line of 128 bytes . ideally , the threads in a warp access memory locations in the same cache line(s ) , thus making use of all of the data that is actually loaded .",
    "this concept is known as _ memory coalescence_.",
    "[ [ spin - updates ] ] spin updates + + + + + + + + + + + +    by construction population annealing suggests to parallelize the calculations for different members of the population .",
    "one particularly simple code setup is hence to assign one thread to the updating of each replica such that in total @xmath66 threads are used for the mcmc part of the algorithm , i.e. , for flipping spins .",
    "each thread then goes sequentially through the lattice . to ensure good memory coalescence in this case , the same spins of each replica",
    "should be placed next to each other in memory , so configurations should be stored in replica - major order . in practice",
    "this code setup , which we denote as _ replica - parallel _ , does show good but not optimal performance , especially for smaller population sizes where it does not provide enough parallelism . where this approach does not provide optimal performance",
    ", it still has the advantage of being completely general , and it could consequently be applied unaltered to pa simulations of any other model .",
    "we hence mention it here as a safe fall - back solution especially for problems for which it is not possible or straightforward to implement a domain decomposition ( for instance for systems with long - range interactions ) .     spins . to this end",
    "it flips spins on one checkerboard sub - lattice first , moving the tiles over the lattice until it is covered , sychronizes and then updates the other sub - lattice . ]    to increase the amount of parallel work , for the present code for the 2d ising model we opted to additionally parallelize the updates for each single replica , using a domain decomposition of the lattice .",
    "this was extensively used previously for simulations using mcmc ( single - spin flips ) only .",
    "the basic step consists of a checkerboard decomposition of the lattice which allows for independent updates of all spins of one sub - lattice .",
    "we denote the corresponding scheme that parallelizes over replicas and spins as _ spin - parallel_. as the number of threads per block is limited to 1024 on current cuda devices , one either lets each thread update a certain range of spins or employs a further decomposition of the lattice , be it in strips @xcite , a second layer of checkerboard tiles @xcite or some other form of subdivision @xcite . for the present code , we used one of the simplest solutions and let the ` eqthreads ` threads of a block employed for the spin - updating kernel ` checkkerall ( ) ` handle the spins of a full replica in the following way ( cf .",
    "[ fig : checker ] ) : the first ` eqthreads ` spins of the blue sub - lattice are updated in parallel , then the next ` eqthreads ` blue spins and so forth until all blue spins of the replica have been dealt with . after a synchronization of all threads they update the white spins of the current configuration in the same way , followed by another synchronization of threads .",
    "finally , this whole procedure is repeated @xmath15 times until all spin updates have been implemented .",
    "this setup is illustrated in fig .",
    "[ fig : checker ] for an @xmath67 lattice and @xmath68 . to increase memory coalescence",
    "we store the spins of each sub - lattice together , separate from the spins of the other sub - lattice .",
    "note that for this setup the spins are stored in a _ spin - major _ order as the threads of a block work on spins in the same replica .",
    "we are not explicitly using shared memory for the spin flips as it was not found to improve performance on the devices tested here .",
    "a further optimization could consist of storing the spin arrays in texture memory as suggested in ref .",
    "@xcite which simplifies index arithmetics and allows to make use of the separate texture cache , but for the sake of simplicity we refrain from such additional optimizations that are expected to yield only quite moderate further speed improvements .    in this spin - parallel setup",
    "utilizing additional parallel work inside of each replica , different replicas are handled by different thread blocks .",
    "we request @xmath66 thread blocks at kernel invocation which will cause no problems for realistic population sizes on recent devices where the maximum number of blocks is @xmath69 . for the actual spin updates , we use pre - calculated tables of the metropolis factors",
    "@xmath70 , stored in texture memory @xcite . for deciding about",
    "the acceptance of proposed spin flips the algorithm requires one random number per spin update .",
    "random number generation on gpus and in other massively parallel contexts requires a way of producing many uncorrelated ( sub-)sequences , and certain parameters such as the memory footprint make some of the standard generators in serial environments unsuitable for a massively parallel application .",
    "some of the related issues are discussed in refs .",
    "a suite of generators loosely based on cryptographic algorithms turned out to be particularly competitive in this context , namely the series of philox generators of ref .",
    "@xcite . in the tests conducted in ref .  @xcite it combined excellent gpu performance with a passing of all tests of the testu01 suite @xcite .",
    "also , in the meantime it has been included as one of the generators in the ` curand ` library that is part of nvidia s cuda distribution .",
    "it hence requires no further code to be used for the present application .",
    "additionally , users can readily replace it by any of the alternative rngs included in ` curand ` if they so desire .",
    "one of the important advantages of the philox generator is that it does not require the transfer of a generator state between gpu main memory and the multiprocessors doing the actual calculations .",
    "this is a consequence of it being a counter - based generator , i.e. , the generation of the number @xmath71 in the sequence does not require knowledge of @xmath72 or any other previous state .",
    "we use one instance of the philox_4x32_10 generator per thread , which is initialized in the kernel with a sequence number determined from the grid and block indices of the thread and a global iteration parameter .",
    "the required numbers in @xmath73 are then generated by in - line calls to ` curand_uniform ( ) ` in the spin - updating kernel ` checkkerall ( ) ` .",
    "this inline production of random numbers is faster than a pre - generation in dedicated arrays in a separate kernel and also much more efficient in terms of the memory footprint as no arrays are required .",
    "[ [ resampling - and - measurements ] ] resampling and measurements + + + + + + + + + + + + + + + + + + + + + + + + + + +    the resampling process is also fully handled on gpu . to determine the resampling factors @xmath74 of eq .",
    "one first needs the normalization constant @xmath75 of eq .  , which equals the sum of all ( unnormalized ) resampling factors .",
    "there are @xmath66 summands , and the corresponding kernel ` qker ( ) ` is called with @xmath76 threads best chosen to equal the maximum block size ( 1024 for current nvidia gpus ) and , correspondingly , @xmath77 blocks .",
    "( here , @xmath78 denotes the smallest integer that is larger or equal to @xmath28 , i.e. , rounding up . ) within each block , we use the standard parallel reduction method that adds elements pairwise in several generations until only one element ( the sum ) is left  a scheme that can be visualized as a binary tree @xcite .",
    "this approach stores the intermediate results in shared memory . as threads from different blocks",
    "can not directly communicate , the sum of partial results of each thread block is typically determined by an additional kernel invocation @xcite .",
    "alternatively , one can make use of the ` atomicadd ( ) ` device function provided by cuda to complete the reduction in the same kernel call , or use ` _ _ threadfence ( ) ` to ensure synchronization across blocks . as for the solution using ` atomicadd ( )",
    "` the order of summation is not well defined , different runs with the same parameters and random number seeds could potentially lead to slightly different values of @xmath75 ( at the level of the floating - point precision ) and hence , ultimately , to a divergence of trajectories of the associated markov chains through different decisions in the resampling kernel .",
    "although we never observed such a case in practice , we decided to use a deterministic version of parallel reduction for the calculation of @xmath75 to rule out this possibility . for the calculation of averages discussed below",
    ", we use the semantically simpler code with ` atomicadd ( ) ` .",
    "a second kernel , ` calctauker ( ) ` is used with the same execution configuration to determine the number of copies of each replica to be created according to eq .  . here",
    ", another random number is used for each replica in the current population to determine whether the number of copies is @xmath79 or @xmath80 . to facilitate the parallel placement of new copies in the vector storing the resampled population",
    ", we also calculate the partial sums @xmath81 , i.e. , the offsets into that vector , again using the same parallel reduction approach .",
    "this calculation is completed in the kernel ` calcparsum ( ) ` . in the end , ` resampleker ( ) ` is used to copy the selected individual replicas into the previously calculated locations of the new population vector , using one thread per spin in a tile of size @xmath82 and a number of blocks that covers the full population and each individual lattice with tiles .",
    "finally , measurements of the quantities of eqs .   are computed using a parallel reduction algorithm to first calculcate the configurational energy and magnetization of each replica in the kernel ` energyker ( ) ` . as only one block",
    "is assigned to each replica in this case , no further reduction of block values is required here .",
    "finally , another parallel reduction is used in the kernel ` calcaverages ( ) ` to determine the population averages , employing ` atomicadd ( ) ` for the inter - block reduction .",
    "[ [ further - optimization - and - parameters ] ] further optimization and parameters + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we note that through the fluctuations of @xmath66 the execution configuration of the kernels changes with each temperature step . for a fully loaded gpu",
    "this causes only negligible variations in the total performance , however .",
    "an important feature of the provided implementation is that all calculations are performed on gpu , so no significant memory transfers to or from cpu occur during the pa run time .",
    "a number of further optimizations have been employed to achieve good performance .",
    "we request a larger l1 cache over shared memory using the ` cudadevicesetcacheconfig ( ) ` command as this turns out to be beneficial for the memory accesses in the main ` checkkerall ( ) ` kernel that does not make use of shared memory .",
    "there is a maximal number of threads that can be resident on a multiprocessor at any given time , and in general it is found that latency hiding works better the more threads are resident .",
    "this _ occupancy _ of multiprocessors can be limited by the number of available registers , however .",
    "depending on the gpu employed , it can be beneficial to request a maximum number of registers to be consumed per thread using the command - line option ` maxregcount ` of the ` nvcc ` compiler .",
    "the occupancy achieved with a given setup and register usage can be determined using the occupancy calculator spreadsheet that comes with the cuda distribution .",
    "we provide here two separate codes , one for single - spin coding and one for multi - spin coding ( see below ) .",
    "the relevant parameters such as @xmath26 , @xmath15 , @xmath10 and @xmath19 , as well as the number of runs @xmath42 can be specified either through constants ( ` # define`s ) at the beginning of the source code or through command - line arguments .",
    "there are two gpu specific parameters , ` eqthreads ` and ` nthreads ` , which decide about the block size in the different kernels .",
    "these can be adjusted by changing the values in the ` # define`s in the source code , but the default choices , @xmath83 and @xmath84 , are virtually always ( near ) optimal on modern cards .",
    "the seed of the rng can be changed by adapting ` rngseed ` , and in the default setup it is initialized using the system time .",
    "the results of each run of the algorithm are stored in a separate output file in text format .",
    "each line of the output contains the values @xmath1 , @xmath85 , @xmath86 , @xmath87 , @xmath88 , @xmath89 , @xmath90 , @xmath91 , @xmath26 , @xmath92 , i.e. , the inverse temperature , energy per site , specific heat , magnetization per site and its moments , free energy density divided by temperature , entropy per site , population size , and logarithm of partition function ratio , respectively .",
    "it is clear from the general design principles for efficient gpu code as discussed above in sec .",
    "[ sec : implementation ] that a minimization of memory transfers will often result in more efficient code .",
    "more specifically , this will always be the case for code that is _ memory - bound _",
    ", i.e. , for which the mix of memory transactions and arithmetic operations is such that the performance limiting factor is the latency and bandwidth of memory transactions @xcite .",
    "since the metropolis update of the ising model used in the equilibrating subroutine is arithmetically very light , especially when using a precomputed table for the exponential function , this is indeed the case for the present application . under these circumstances any modification that reduces memory transfers",
    "can be expected to increase performance .",
    "since an ising spin is a single - bit variable , it is clear that storing it in a standard built - in variable ( even if it is of 8-bit length ) is not ideal and an explicit one - bit representation promises some performance improvement .",
    "this can be implemented using multi - spin coding ( msc ) , i.e. , by storing the states of @xmath93 spin variables in a single machine word of @xmath93 bits @xcite .",
    "natural choices for the architecture are @xmath94 , @xmath95 , @xmath96 and @xmath97 . while for simulations of single systems as discussed in refs .",
    "@xcite the spins represented by @xmath93 bits in a word correspond to different lattice sites ( synchronous msc ) , for the present application it is more convenient to have the bits in a word represent the spins on the same lattice site but in different replicas ( asynchronous msc ) @xcite . quite efficient",
    "bitwise operations are available to implement a parallel metropolis update of the spins coded in a @xmath93-bit word .",
    "this approach has been extensively used in simulations , in particular , of spin - glass models @xcite .",
    "@xmath98 & lcg & & + 8 & 8 & no & 1.82 & 1 + 16 & 16 & no & 1.78 & 1 + 32 & 32 & no & 1.83 & 1 + 64 & 64 & no & 1.76 & 1 + 8 & 1 & no & 6.74 & 8 + 16 & 1 & no & 10.56 & 16 + 32 & 1 & no & 13.61 & 32 + 64 & 1 & no & 9.40 & 64 + 8 & 1 & yes & 6.09 & 1 + 16 & 1 & yes & 8.94 & 1 + 32 & 1 & yes & 10.22 & 1 + 64 & 1 & yes & 8.16 & 1 +     2d ising system with @xmath3 , @xmath99 and population size @xmath100 using asynchronous multi - spin coding ( msc ) with @xmath94 , @xmath95 , @xmath96 and @xmath97 bits .",
    "the same random numbers are used to decide about spin flips in all replicas coded in the same words .",
    "( b ) relative error of the specific heat for @xmath101 with @xmath102 and for @xmath103 with additional reordering of replicas and with using different random numbers for each replica coded in the same word , produced by an in - line linear congruential generator ( lcg ) seeded by the main generator ( philox ) . the data for this last variant and that of @xmath104 are practically indistinguishable . ( c ) ratio of the estimated variances @xmath105 of the multi - spin coded ( msc ) variants as compared to the single - spin coded ( ssc ) reference implementation .",
    ", title=\"fig : \" ]   2d ising system with @xmath3 , @xmath99 and population size @xmath100 using asynchronous multi - spin coding ( msc ) with @xmath94 , @xmath95 , @xmath96 and @xmath97 bits .",
    "the same random numbers are used to decide about spin flips in all replicas coded in the same words .",
    "( b ) relative error of the specific heat for @xmath101 with @xmath102 and for @xmath103 with additional reordering of replicas and with using different random numbers for each replica coded in the same word , produced by an in - line linear congruential generator ( lcg ) seeded by the main generator ( philox ) . the data for this last variant and that of @xmath104 are practically indistinguishable . ( c ) ratio of the estimated variances @xmath105 of the multi - spin coded ( msc ) variants as compared to the single - spin coded ( ssc ) reference implementation . , title=\"fig : \" ]   2d ising system with @xmath3 , @xmath99 and population size @xmath100 using asynchronous multi - spin coding ( msc ) with @xmath94 , @xmath95 , @xmath96 and @xmath97 bits .",
    "the same random numbers are used to decide about spin flips in all replicas coded in the same words .",
    "( b ) relative error of the specific heat for @xmath101 with @xmath102 and for @xmath103 with additional reordering of replicas and with using different random numbers for each replica coded in the same word , produced by an in - line linear congruential generator ( lcg ) seeded by the main generator ( philox ) . the data for this last variant and that of @xmath104 are practically indistinguishable . ( c ) ratio of the estimated variances @xmath105 of the multi - spin coded ( msc ) variants as compared to the single - spin coded ( ssc ) reference implementation .",
    ", title=\"fig : \" ]    the resulting msc variant of the code shows increased performance over a single - spin coded ( ssc ) version , with an improvement that depends only weakly on the number of spins @xmath93 coded together .",
    "this is illustrated in the data in the first section of table  [ tab : msc ] , where a different random number is drawn using the base generator for each of the @xmath93 spins coded in a word , i.e. , @xmath106 .",
    "the relatively moderate and mostly @xmath93 independent improvement is a result of the fact that the time taken per spin update is in this setup limited by the time it takes to generate the random numbers used to decide about the acceptance of spin flips .",
    "a number of implementations of this scheme for spin glasses @xcite have used the _ same _ random number for deciding about flipping all of the @xmath93 spins in a word .",
    "this introduces some correlations , however , and while it is argued that this effect is minor for spin - glass problems due to the property of bond chaos in such systems @xcite , we expect it to be much more relevant for the case of the ferromagnet studied here .",
    "if the same random numbers are used for deciding about flips of the @xmath93 spins coded together , this implies that these replicas develop in a correlated manner .",
    "in particular , if ( some of ) these @xmath93 replicas have identical spin configurations as is the case if they are copies of the same parent configuration in the resampling process , they are coupled and remain identical for all future times .",
    "this clearly interferes with the goal of fair sampling . to illustrate this effect ,",
    "we show in fig .",
    "[ fig : msc](a ) the relative errors of the specific heat of the 2d ising model sampled with the msc pa implementation with @xmath94 , @xmath95 , @xmath96 and @xmath97 spins coded together , respectively , while using the same random numbers to flip spins in all @xmath93 replicas coded together .",
    "as is clearly seen , the errors in this setup increase with @xmath93 , and a rescaling of the @xmath107 axis reveals that @xmath108 in fact increases proportional to @xmath109 as expected from general statistical arguments ( not shown ) . on the other hand , the performance of this variant using only one random number for @xmath93 spins is found to be excellent , cf .  the data in the second section of table  [ tab : msc ] .",
    "note that here in contrast to the case with @xmath110 the speedup varies considerably with @xmath93 , and we find the best result for @xmath111 .    in an attempt to alleviate the correlation effect",
    ", we introduced a rearrangement of replicas after resampling in such a way as to avoid placing offspring of the same parent configuration in the same word .",
    "this is achieved by the following procedure .",
    "if we enumerate all replicas as @xmath112 , @xmath113 @xmath66 , then for a given @xmath114 , @xmath115 , the spins of the replicas @xmath116 , @xmath117 , @xmath118 , @xmath119 are originally stored in the same words . or , equivalently",
    ", replicas with the same value of @xmath120 are stored in the same word .",
    "the population is then rearranged such that replicas with the same value of @xmath121 are stored in the same word , i.e. , when initially replicas @xmath122 , @xmath123 , @xmath50 , @xmath93 occupy the first word , this now contains replicas @xmath122 , @xmath124 , @xmath50 , @xmath125 .",
    "this process can be pictured as transposing a @xmath126 matrix , followed by reshaping the result to again occupy @xmath93 rows .",
    "unless a parent has more than @xmath127 children ( which is unlikely for sufficiently large populations ) , this setup ensures that descendants from the same parent configuration are placed in different words . as a result",
    ", their next spin updates are governed by independent random number samples .",
    "however , it is clear that at a lower temperature some of these sibling replicas could again end up in the same machine word and hence remain correlated .",
    "the behavior of statistical errors of the resulting improved algorithm is illustrated in fig .",
    "[ fig : msc](b ) .",
    "it leads to a slight reduction of the inflation of statistical errors against the non - msc implementation , but by no means removes it .",
    "additionally , the improvement appears to vanish for temperatures below the transition point @xmath128 .     of the single - spin coded gpu and cpu codes as a function of the number @xmath15 of equilibration sweeps relative to the time @xmath129 achieved in the fastest case considered , namely for @xmath130 .",
    "the different lines show different population sizes .",
    "( b ) the same comparison for the multi - spin coded gpu code .",
    "the reference line for cpu is again for the single - spin coded algorithm .",
    ", title=\"fig : \" ]   of the single - spin coded gpu and cpu codes as a function of the number @xmath15 of equilibration sweeps relative to the time @xmath129 achieved in the fastest case considered , namely for @xmath130 .",
    "the different lines show different population sizes .",
    "( b ) the same comparison for the multi - spin coded gpu code .",
    "the reference line for cpu is again for the single - spin coded algorithm .",
    ", title=\"fig : \" ]    a method that provides high performance without compromising the statistical quality of data can be constructed by combining the underlying rng with a particularly fast in - line generator used to supply the @xmath93 random numbers used to flip the spins stored in the same word . for this purpose",
    ", we use a simple linear - congruential generator ( lcg ) of the form @xmath131 with @xmath132 and @xmath133 @xcite . for each call to the spin - updating kernel and each word of @xmath93 spins ,",
    "this generator is seeded by a call to the underlying , high - quality generator ( philox ) .",
    "although lcg generators are no longer recommended for general purpose applications in simulations ( see , e.g. , ref .",
    "@xcite and references therein ) , we believe that this does not cause any problems in the present context as the lcg is only used to multiply a sample of the underlying rng and additionally the resampling is done with the base rng alone .",
    "empirical testing confirms this assumption as no biases or increases in statistical fluctuations are observed .",
    "the corresponding results shown in fig .  [ fig : msc](b ) and ( c ) reveal that the relative error for this approach is identical to that of simulations using @xmath93 samples from the base rng . as the data in the last section of table  [ tab : msc ]",
    "illustrate , the performance of this combined approach is excellent , providing an about 10-fold speed - up of the simulations with msc and @xmath111 as compared to the simulations with ssc ( both running on gpu ) .",
    "in order to compare performance across different choices of the algorithmic parameters @xmath26 , @xmath15 and @xmath32 , we normalize the time for a full pa run by the total number of spin flips performed , @xmath134 where @xmath59 denotes the number of temperature steps . we compare the gpu codes proposed here to our reference cpu implementation which is a scalar program , so only uses one core . instead of discussing the performance of a range of different cpus and gpus",
    ", we here restrict ourselves to the gpus and cpus available in an hpc cluster machine recently installed at the home institution of one of us ( coventry university ) , which are intel xeon e5 - 2683 v4 cpus and nvidia tesla k80 gpu cards , which should be fairly representative of present - day hpc cluster configurations . the k80 is a double card , of which only one card is actually used for the measurements at a time , featuring 2880 cores and 12 gb of ram .",
    "rccccc & cpu & +   + & & & + @xmath0 & @xmath135 [ ns ] & @xmath135 [ ns ] & speedup & @xmath135 [ ns ] & speedup + 16 & 23.1 & 0.092 & 251 & 0.0096 & 2406 + 32 & 22.9 & 0.094 & 243 & 0.0095 & 2410 + 64 & 22.6 & 0.095 & 238 & 0.0098 & 2306 + 128 & 22.6 & 0.098 & 230 & 0.0098 & 2306 + 256 & 22.5 & 0.099 & 227 & 0.0098 & 2295 +    in general , the best performance in terms of the metric is achieved when minimizing the frequency of resampling steps , such that practically all time is invested in flipping spins . first focusing on this optimal case , we collect in table [ tab : peak - performance ] the times @xmath135 for @xmath130 and a population size @xmath136 for different system sizes .",
    "it is seen that in the range @xmath137 considered the performance of all three codes is almost independent of system size . for the gpu codes",
    "this is an indication that through the replica parallelism and the additional spin - parallelism there is enough parallel work to saturate the device already for moderate system sizes .",
    "the single - spin coded gpu code is found to be about 230 times faster than the cpu case .",
    "the multi - spin coding with @xmath111 and the additional combination with an lcg generator yields a further factor of 10 , resulting in a total peak speedup of the msc code of about 2300 as compared to the scalar program .",
    ".times @xmath135 per spin flip ( in ns ) for ssc and msc gpu codes run on the tesla k80 gpu for a @xmath138 system .",
    "[ cols=\"^ , > , < , < , < , < \" , ]     while we provide code for the 2d ising ferromagnet , we hope it to be used as a template for the simulation also of further problems with the same algorithm .",
    "generalizations for models on different lattices in various dimensions , the case of different couplings including spin glasses and random - field systems as well as more general spin systems such as potts @xcite or o(@xmath114 ) models are straightforward . applications to off - lattice systems are also straightforward conceptually @xcite , although the optimization of the code in such cases might be a little bit more difficult .",
    "large populations can be simulated on standard gpus .",
    "for the present implementation , for @xmath2 it is possible on the k80 gpus to simulate @xmath139 replicas using the ssc variant and @xmath140 for the msc version .",
    "it is worthwhile noting that one can combine simulations to effectively achieve the precision expected from a single run with the combined population size by using the weighted averaging scheme as discussed above in sec .",
    "[ sec : weighted ] @xcite .",
    "additionally , it should be possible to combine gpu parallelization with mpi and run very large populations on a cluster of gpu enabled nodes , and we expect population annealing to show excellent scaling properties for such setups .",
    "the work of l.b . m.b .  and l.s .",
    "is supported by the grant 14 - 21 - 00158 from the russian science foundation .",
    "was also supported by the scientific grant agency of the ministry of education of the slovak republic ( grant no .",
    "1/0331/15 ) .",
    "the authors acknowledge support from the european commission through the irses network dionicos under contract no .",
    "pirses - ga-2013 - 612707 .",
    "the simulations were performed on the hpc facilities of coventry university and the science center in chernogolovka .",
    "m.w.acknowledges fruitful and pleasant discussions with jon machta and helmut katzgraber on the subject of population annealing .",
    "w.  wang , j.  machta , h.  g. katzgraber , comparing monte carlo methods for finding ground states of ising spin glasses : population annealing , simulated annealing , and parallel tempering , phys .",
    "e 92 ( 2015 ) 013303 .",
    "k.  asanovic , r.  bodik , b.  c. catanzaro , j.  j. gebis , p.  husbands , k.  keutzer , d.  a. patterson , w.  l. plishker , j.  shalf , s.  w. williams , k.  a. yelick , the landscape of parallel computing research : a view from berkeley , tech .",
    "rep . , technical report ucb / eecs-2006 - 183 , eecs department , university of california , berkeley ( 2006 ) .",
    "e.  e. ferrero , j.  p. de  francesco , n.  wolovick , s.  a. cannas , @xmath141-state potts model metastability study using optimized gpu - based monte carlo algorithms , comput .",
    ". commun . 183",
    "( 2012 ) 15781587 .",
    "l.  y. barash , l.  n. shchur , prand : gpu accelerated parallel random number generation library : using most reliable algorithms and applying parallelism of modern gpus and cpus , comput .",
    "commun . 185",
    "( 2014 ) 13431353 .",
    "j.  k. salmon , m.  a. moraes , r.  o. dror , d.  e. shaw , parallel random numbers : as easy as 1 , 2 , 3 , in : proceedings of 2011 international conference for high performance computing , networking , storage and analysis , sc 11 , acm , new york , ny , usa , 2011 .",
    "n.  ito , y.  kanada , monte carlo simulation of the ising model and random number generation on the vector processor , in : proceedings of the 1990 acm / ieee conference on supercomputing , ieee computer society press , 1990 , pp .",
    "753763 .",
    "f.  belletti , m.  cotallo , a.  cruz , l.  a. fernndez , a.  g. guerrero , m.  guidetti , a.  maiorano , f.  mantovani , e.  marinari , v.  martn - mayor , a.  muoz sudupe , d.  navarro , g.  parisi , s.  p. gaviro , m.  rossi , j.  j. ruiz - lorenzo , s.  f. schifano , d.  sciretti , a.  tarancn , r.  l. tripiccione , janus : an fpga - based system for high - performance scientific computing , comput .",
    "11 ( 2009 ) 4858 .",
    "y.  fang , s.  feng , k .-",
    "tam , z.  yun , j.  moreno , j.  ramanujam , m.  jarrell , parallel tempering simulation of the three - dimensional edwards  anderson model with compact asynchronous multispin coding on gpu , comput .",
    ". commun . 185",
    "( 2014 ) 24672478 ."
  ],
  "abstract_text": [
    "<S> population annealing is a promising recent approach for monte carlo simulations in statistical physics , in particular for the simulation of systems with complex free - energy landscapes . </S>",
    "<S> it is a hybrid method , combining importance sampling through markov chains with elements of sequential monte carlo in the form of population control . </S>",
    "<S> while it appears to provide algorithmic capabilities for the simulation of such systems that are roughly comparable to those of more established approaches such as parallel tempering , it is intrinsically much more suitable for massively parallel computing . here </S>",
    "<S> , we tap into this structural advantage and present a highly optimized implementation of the population annealing algorithm on gpus that promises speed - ups of several orders of magnitude as compared to a serial implementation on cpus . while the sample code is for simulations of the 2d ferromagnetic ising model , it should be easily adapted for simulations of other spin models , including disordered systems . </S>",
    "<S> our code includes implementations of some advanced algorithmic features that have only recently been suggested , namely the automatic adaptation of temperature steps and a multi - histogram analysis of the data at different temperatures .    </S>",
    "<S> * program summary *    _ manuscript title : _ gpu accelerated population annealing algorithm + _ authors : _ lev yu . </S>",
    "<S> barash , martin weigel , michal borovsk , wolfhard janke , lev n. shchur + _ program title : _ paising + _ journal reference : _ </S>",
    "<S> + _ catalogue identifier : _ </S>",
    "<S> + _ licensing provisions : _ creative commons attribution license ( cc by 4.0 ) + _ programming language : </S>",
    "<S> _ c , cuda + _ computer : _ system with an nvidia cuda enabled gpu + _ operating system : _ linux , windows , macos + _ ram : _ 200 mbytes + _ number of processors used : _ 1 gpu + _ supplementary material : _ </S>",
    "<S> + _ keywords : _ population annealing ; monte carlo simulation ; ising model ; parallel computing ; gpu ; multi - spin coding + _ classification : _ 23 + _ external routines / libraries : _ nvidia cuda toolkit 6.5 or newer + _ subprograms used : _ </S>",
    "<S> + _ nature of problem : _ the program calculates the internal energy , specific heat , several magnetization moments , entropy and free energy of the 2d ising model on square lattices of edge length @xmath0 with periodic boundary conditions as a function of inverse temperature @xmath1 . </S>",
    "<S> +   + _ solution method : _ the code uses population annealing , a hybrid method combining markov chain updates with population control . the code is implemented for nvidia gpus using the cuda language and employs advanced techniques such as multi - spin coding , adaptive temperature steps and multi - histogram reweighting . </S>",
    "<S> +   + _ restrictions : _ the system size and size of the population of replicas are limited depending on the memory of the gpu device used . </S>",
    "<S> +   + _ unusual features : _ +   + _ additional comments : _ +   + _ running time : _ for the default parameter values used in the sample programs , @xmath2 , @xmath3 , @xmath4 , @xmath5 , @xmath6 , @xmath7 , a typical run time on an nvidia tesla k80 gpu is 156 seconds for the single spin coded ( ssc ) and 18 seconds for the multi - spin coded ( msc ) program ( see sec .  [ </S>",
    "<S> sec : algorithm ] for a description of these parameters ) . </S>"
  ]
}