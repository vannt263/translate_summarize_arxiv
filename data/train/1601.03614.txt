{
  "article_text": [
    "let @xmath0 @xmath1 be a bivariate two - sided brownian motion such that @xmath2 , @xmath3=e[(b^2_1)^2]=1 $ ] and @xmath4=\\rho$ ] for some @xmath5 .",
    "also , let @xmath6 ( @xmath7 ) be a sequence of i.i.d .",
    "bivariate standard normal variables independent of @xmath8 . for each @xmath9 , we denote by @xmath10 the law of the vector @xmath11 when it is generated by the following model : @xmath12 where @xmath13 is a non - negative number .",
    "the aim of this paper is to study the asymptotic structure of the sequence of experiments @xmath14 , @xmath15 , as @xmath16 when the time lag parameter @xmath17 is asymptotically infinitesimal , i.e.  @xmath17 tends to 0 as @xmath16 ( here and below @xmath18 denotes the borel @xmath19-field of @xmath20 for @xmath21 ) .",
    "more precisely , we study the limit experiment of @xmath22 as @xmath16 for the proper convergence rate @xmath23 .    if @xmath24 , model is a special case of the hoffmann - rosenbaum - yoshida ( hry ) model introduced in @xcite to describe lead - lag effects in high - frequency financial data .",
    "a similar model has also been studied in @xcite with an asymptotic regime different from the current setting . here",
    ", the lead - lag effect refers to a situation where one time series is correlated to another time series at a later time , which has especially drawn attention in analysis of economic time series data for a long time , and associated econometric methods have been developed by many authors ; see section 1 of @xcite , section 3 of @xcite and references therein .",
    "the practicality of the hry model in empirical work has recently been established by several authors such as @xcite and @xcite for financial data and @xcite for social media data .",
    "these empirical studies show that time lag parameters are typically comparable to the observation frequencies in their scales .",
    "this motivates us to study the hry model when @xmath17 is infinitesimal .",
    "in such a situation , one would especially be interested in how small lag parameters can be identified in principle . to the author s knowledge ,",
    "however , there is few theoretical study for the hry model and , in particular , nothing has been known about the optimality of statistical inferences for the hry model .",
    "the purpose of this paper is trying to fill in this gap .    in this paper , as well as ( a special case of ) the hry model , we also consider a situation where the model contains measurement errors .",
    "this is motivated by the recent studies for the volatility estimation from ultra high frequency financial data , which is typically modeled as a discretely observed semimartingale with market microstructure noise .",
    "we refer to chapter 7 of @xcite for a brief description of this subject .",
    "in particular , the asymptotic structure and the asymptotic efficiency bound for estimating the scale parameter @xmath25 from the discrete observations @xmath26 where @xmath27}$ ] is a one - dimensional standard wiener process and @xmath28 is a sequence of centered i.i.d .",
    "standard normal variables independent of @xmath29 , has been established in the work of @xcite ( see also @xcite ) .",
    "they proved the lan property for the above model and constructed asymptotically efficient estimators for @xmath19 ( they indeed considered a more general setting ) .",
    "extensions of their lan result to a multivariate setting have also been studied by several authors . the correlation estimation in a bivariate setting",
    "is studied in @xcite , while a more general setting containing non - synchronous sampling case is studied in @xcite . on the other hand",
    ", @xcite has studied the asymptotic structure of model when @xmath19 is a function of time rather than a constant and established the asymptotic equivalence between such a model and a gaussian white noise model .",
    "the result has been extended to the bivariate case by @xcite and a multivariate setting containing non - synchronous case by @xcite .",
    "another type of extension , replacing the wiener process @xmath29 by a different continuous - time process , has also been studied .",
    "for example , @xcite consider the efficient estimation of @xmath19 in a situation where @xmath29 is a more general gaussian process , especially a fractional brownian motion .",
    "the main contribution of this paper is ( i ) to determine the proper convergence rate @xmath23 , and ( ii ) to derive a stochastic expansion for the likelihood ratio process for @xmath22 .",
    "analogously to @xcite , the proper convergence rate @xmath23 depends on the behavior of the sequence @xmath30 .",
    "this is intuitively natural because @xmath31=n^{-1}$ ] and thus the behavior of @xmath30 determines how strongly the measurement errors ( locally ) dominate the nature of the observed returns . in particular",
    ", we find that @xmath32 if @xmath24 , or more generally if @xmath30 is bounded .",
    "the rate @xmath33 is much faster than the usual parametric rate @xmath34 and even faster than the rate @xmath35 .",
    "this is surprising because the time resolution of our model is @xmath35 , so our result suggests that we could estimate lag parameters smaller than the time resolution of observation data .",
    "this implication is at least true for our restrictive situation , as shown in section [ section : application ] .",
    "since the convergence rate of the estimator for the lag parameter @xmath17 proposed in @xcite can not be faster than @xmath35 ( see proposition 2 of @xcite and the discussion after this proposition ) , our result shows that their estimator is suboptimal in the setting considered in this paper ( although their estimator works in a more general setting ) .    0",
    "given the proper convergence rate , we show that the likelihood ratio process @xmath36 converges finite dimensionally in law to the process @xmath37 under @xmath38 as @xmath16 , where @xmath39 and @xmath40 are two mutually independent standard normal variables and @xmath41 and @xmath42 are non - negative numbers determined by the asymptotic behavior of @xmath30 , which will precisely be defined by and . in particular , @xmath41 is always positive , while @xmath42 is positive if @xmath30 is bounded and @xmath43 otherwise . 0 setting @xmath44 we can easily check that the process is equal to @xmath45 .",
    "therefore , by le cam s first lemma and theorem 61.6 of @xcite the localized experiment @xmath46 converges to the experiment @xmath47 .",
    "that is , for any @xmath48 and any finite subset @xmath49 of @xmath50 , it holds that @xmath51 as @xmath16 .",
    "the experiment @xmath52 is non - standard unless @xmath43 . from this result",
    "we can deduce the limit experiment of the localized experiment @xmath46 by a contiguity argument ( see corollary [ limit ] ) .",
    "the case that @xmath43 corresponds to the situation where the measurement errors locally dominate the signal and in this case the limit experiment is a gaussian shift experiment .",
    "this result is of interest because model evidently exhibits irregularity about the parameter @xmath17 and the limit experiment of such a model is typically different from the gaussian shift experiment .",
    "our result means that the measurement errors have a kind of regularizing effect on the asymptotic structure of model . on the other hand ,",
    "if @xmath53 , which corresponds to the cases where the signal dominates or is balanced with the measurement errors , the limit experiment does not result in well - studied cases .",
    "given the proper convergence rate , we have the following stochastic expansion for the likelihood ratio process : there are random variables @xmath54 and @xmath55 and non - negative numbers @xmath41 and @xmath42 such that @xmath56 for any bounded sequence @xmath57 of real numbers and @xmath58 therefore , by a contiguity argument we deduce that the experiments @xmath59 converge weakly to the experiment @xmath60 in the le cam sense , where @xmath61 ( see corollary [ limit ] ) . the numbers @xmath41 and @xmath42 are determined by the asymptotic behavior of @xmath30 and precisely defined by . in particular , @xmath41 is always positive , while @xmath42 is positive if @xmath30 is bounded and @xmath43 otherwise .",
    "the case @xmath43 corresponds to the situation where the measurement errors locally dominate the signal , and in this case our model enjoys the lan property which commonly appears in regular experiments .",
    "this result is of interest because model exhibits irregularity in the sense that its likelihood function is not smooth in @xmath17 , and the limit experiment of such a model typically deviates from the lan structure as illustrated in chapters v  vii of @xcite .",
    "our result means that the measurement errors have a kind of regularizing effect on the asymptotic structure of model . on the other hand ,",
    "if @xmath53 , which corresponds to the cases where the signal dominates or is balanced with the measurement errors , in addition to an observation from a usual gaussian shift experiment @xmath62 , the limit experiment contains an extra observation from the experiment @xmath63 .",
    "although this experiment looks simple , to the author s knowledge it does not result in well - studied cases ( such as in @xcite ) , so the definition of asymptotically efficient estimators in this case is not obvious . to obtain the asymptotic efficiency bound for estimating the lag parameter in this case , in section [ section : application ] we apply @xcite s theory to our problem , which is a common approach to establish asymptotic efficiency bounds for experiments generated by diffusion type processes ( see @xcite for details ) . as a result , we find that bayesian estimators are asymptotically efficient , while the maximum likelihood estimator is not always asymptotically efficient .",
    "this is a common phenomenon in irregular models ; see chapters v  vii of @xcite , @xcite , chapter 3 of @xcite , @xcite and chapter 9 of @xcite for example .",
    "this paper is organized as follows .",
    "section [ section : main ] presents the main result of the paper . in section [ section : application ] we discuss the efficient estimation of the lag parameter in our setting .",
    "section [ section : technical ] is devoted to the proof of an auxiliary technical result .      @xmath64 denotes the @xmath65-identity matrix . for a matrix @xmath66",
    ", we denote by @xmath67 and @xmath68 its spectral norm and the frobenius norm , respectively .",
    "that is , @xmath69 and @xmath70 .",
    "also , we denote by @xmath71 the @xmath72-th entry of @xmath66 .",
    "we start with completing the definitions of the quantities @xmath23 , @xmath41 and @xmath42 appearing in the introduction .",
    "first , following @xcite we assume that the sequence @xmath30 converges in @xmath73 $ ] and set @xmath74 we also assume @xmath75 as in @xcite .",
    "then we set @xmath76 @xmath77 can be considered as an `` effective '' sample size in the sense that the proper convergence rate for estimating @xmath19 from model is given by @xmath78 , which is seen as the regular parametric rate if we regard @xmath77 as the sample size . using this effective sample size @xmath77",
    ", we define our proper convergence rate as @xmath79 .",
    "the constants @xmath41 and @xmath42 appearing in  are defined by @xmath80 and @xmath81 where for any @xmath82 we set @xmath83    [ thm : main ] there are two sequences @xmath54 and @xmath55 of random variables satisfying  for any bounded sequence @xmath57 of real numbers .",
    "we can explicitly give the variables @xmath54 and @xmath55 in theorem [ thm : main ] by below .",
    "theorem [ thm : main ] has some immediate consequences .",
    "the first one is the direct consequence of the definition of the lan property .    if @xmath84 , @xmath85 has the lan property at @xmath86 with rate @xmath23 and asymptotic fisher information @xmath41 .",
    "the second one follows from le cam s first lemma .",
    "[ contiguity ] @xmath87 and @xmath38 are mutually contiguous if the sequence @xmath88 of real numbers satisfies @xmath89 .",
    "the third one is derived from corollary [ contiguity ] and theorem 61.6 of @xcite ( we refer to @xcite , @xcite , chapter 10 of @xcite and chapters 89 of @xcite for the definition and applications of the weak convergence of experiments ) .",
    "[ limit ] the sequence @xmath90 of experiments converges weakly to the experiment @xmath60 as @xmath16 , where @xmath61 .",
    "0 @xmath91_t=\\rho t,\\qquad i=1,\\dots , n\\end{aligned}\\ ] ]    @xmath92    0 @xmath93 is gaussian    @xmath94 @xmath95=0,&e[\\widetilde{\\epsilon}^2_{i , n}\\delta^n_jb^2]=-\\vartheta1_{\\{i = j\\}}&\\textrm{if } \\vartheta\\geq0,\\\\ e[\\widetilde{\\epsilon}^1_{i , n}\\delta^n_jb^1]=\\vartheta1_{\\{i = j\\}},&e[\\widetilde{\\epsilon}^2_{i , n}\\delta^n_jb^2]=0&\\textrm{if } \\vartheta<0 \\end{array}\\right.\\end{aligned}\\ ] ]    now we turn to the proof of theorem [ thm : main ] .",
    "although @xmath85 consists of gaussian distributions , the problem is not simple because the covariance matrix @xmath96 of @xmath10 is a complicated function of the lag parameter @xmath17 .",
    "for this reason we first transfer from the model @xmath10 to a more tractable model defined as follows : for each @xmath9 , set @xmath97 .",
    "then , for each @xmath98 we denote by @xmath99 the law of the vector @xmath100 when it is generated by @xmath101 where @xmath102 for @xmath103 .",
    "we denote by @xmath104 the covariance matrix of @xmath99 .    in the following",
    "we will show that @xmath10 is well - approximated by @xmath99 . to be precise",
    ", the hellinger distance between @xmath10 and @xmath99 tends to 0 as @xmath16 , provided that @xmath17 tends to 0 sufficiently fast . here ,",
    "the hellinger distance @xmath105 between two probability measures @xmath106 and @xmath107 on a measurable space @xmath108 is defined by @xmath109 where @xmath110 is a @xmath19-finite measure dominating both @xmath106 and @xmath107 ( @xmath111 for example ) .",
    "it can easily be checked that @xmath105 does not depend on the choice of @xmath110 .",
    "see appendix a.1 of @xcite , section 2 of @xcite and section 2.4 of @xcite for more information about the hellinger distance . throughout the paper ,",
    "we denote by @xmath112 ( resp .",
    "@xmath113 ) expectation with respect to @xmath10 ( resp .",
    "@xmath99 ) .",
    "[ prop : hellinger ] ( a ) if @xmath114 , then @xmath115 .",
    "\\(b ) if @xmath116 , @xmath117 for any @xmath9 and any @xmath98 .",
    "\\(c ) if a sequence @xmath88 of positive numbers satisfies @xmath118 as @xmath16 , then @xmath119 .    claim ( c ) immediately follows from ( a ) and ( b ) , so we focus on ( a ) and ( b ) . by symmetry",
    "we may assume @xmath120 .",
    "then we have @xmath121=\\widetilde{\\mathbb{e}}_{n,\\vartheta}[x_ix_j]$ ] for all @xmath122 .",
    "moreover , a simple computation shows that 0 @xmath123=\\frac{i\\wedge j}{n}+v_n1_{\\{i = j\\}},\\qquad \\mathbb{e}_{n,\\vartheta}[y_iy_j]=\\left(\\frac{i\\wedge j}{n}-\\vartheta\\right)+v_n1_{\\{i = j\\}},\\\\ & \\mathbb{e}_{n,\\vartheta}[x_iy_j]=\\rho\\frac{i\\wedge ( j - n\\vartheta)}{n}\\end{aligned}\\ ] ] 0 @xmath124=\\left(\\frac{i\\wedge j}{n}-\\vartheta\\right)+v_n1_{\\{i = j\\}},\\qquad \\mathbb{e}_{n,\\vartheta}[x_iy_j]=\\rho\\frac{i\\wedge ( j - n\\vartheta)}{n}\\end{aligned}\\ ] ] and @xmath125&=\\left(\\frac{i\\wedge j}{n}-\\vartheta1_{\\{i\\geq j\\}}-\\vartheta1_{\\{i\\leq j\\}}\\right)+(v_n+\\vartheta)1_{\\{i = j\\ } } = \\mathbb{e}_{n,\\vartheta}[y_iy_j],\\\\ \\widetilde{\\mathbb{e}}_{n,\\vartheta}[x_iy_j]&=\\rho\\left(\\frac{i\\wedge j}{n}-\\vartheta1_{\\{i\\geq j\\}}\\right).\\end{aligned}\\ ] ] in particular , we have @xmath125&=\\left(\\frac{i\\wedge j}{n}-\\vartheta1_{\\{i\\geq j\\}}-\\vartheta1_{\\{i\\leq j\\}}\\right)+(v_n+\\vartheta)1_{\\{i = j\\ } } = \\left(\\frac{i\\wedge j}{n}-\\vartheta\\right)+v_n1_{\\{i = j\\ } } = \\mathbb{e}_{n,\\vartheta}[y_iy_j]\\end{aligned}\\ ] ] and 0 @xmath123=\\widetilde{\\mathbb{e}}_{n,\\vartheta}[x_ix_j]=\\frac{i\\wedge j}{n}+v_n1_{\\{i = j\\}},\\qquad \\mathbb{e}_{n,\\vartheta}[y_iy_j]=\\widetilde{\\mathbb{e}}_{n,\\vartheta}[y_iy_j]=\\left(\\frac{i\\wedge j}{n}-\\vartheta\\right)+v_n1_{\\{i = j\\}}\\end{aligned}\\ ] ] and @xmath126= \\left\\ { \\begin{array}{ll } \\rho i / n&\\textrm{if } i < j - n\\vartheta,\\\\ \\rho(j / n-\\vartheta)&\\textrm{otherwise } , \\end{array } \\right . \\qquad",
    "\\widetilde{\\mathbb{e}}_{n,\\vartheta}[x_iy_j]= \\left\\ { \\begin{array}{ll } \\rho i / n&\\textrm{if } i < j,\\\\ \\rho(j / n-\\vartheta)&\\textrm{otherwise}. \\end{array } \\right.\\ ] ] therefore , we have @xmath127 if @xmath128 .",
    "this yields claim ( a ) because both @xmath10 and @xmath99 are centered gaussian . on the other hand , from the above identities we also have @xmath129 now if @xmath116 , @xmath96 is positive semidefinite and satisfies @xmath130 by the monotonicity theorem for eigenvalues ( corollary 4.3.3 of @xcite ) , for @xmath131 is positive semidefinite .",
    "therefore , by eqs.(a.4 ) and ( a.6 ) from @xcite we obtain @xmath132 hence claim ( b ) holds true .    in the following",
    "we will frequently use the fact that the hellinger distance dominates the total variation distance : @xmath133 where @xmath134 .",
    "see e.g.  lemma 2.3 of @xcite for the proof .",
    "the following properties of the total variation distance are immediate consequences of the definition and important for our purpose . for each @xmath9 , let @xmath135 and @xmath136 be two sequences of probability measures on a measurable space @xmath137 , and let @xmath138 be a random variable on @xmath137 taking its value in a metric space @xmath139 .",
    "then , for any @xmath140 and any probability measure @xmath110 on @xmath139 , the following statements hold true : @xmath141 0 @xmath142 , @xmath143 @xmath144    @xmath145 - \\vartheta\\left [ \\begin{array}{cc } 0   & \\rho j_n    \\\\ \\rho j_n^\\top   & 1_n1_n^\\top \\end{array } \\right ] = : \\bar{k}-\\vartheta\\bar{j}\\ ] ]    next we express the covariance matrix @xmath104 of @xmath99 as a tractable form . for this purpose",
    "we introduce some notation .",
    "the @xmath146 matrix @xmath147 denotes the backward difference operator , i.e. @xmath148.\\ ] ] we set @xmath149 and denote by @xmath150 the covariance matrix of @xmath151 under @xmath99 , i.e.  @xmath152 .",
    "@xmath150 can explicitly be expressed as @xmath153 , where @xmath154,\\qquad \\bar{\\nabla}_n^+=\\left [ \\begin{array}{cc } 0   & \\nabla_n^\\top",
    "\\\\ \\nabla_n   & r_n \\end{array } \\right],\\qquad \\bar{\\nabla}_n^-=-\\left [ \\begin{array}{cc } r_n   & \\nabla_n    \\\\ \\nabla_n^\\top   & 0 \\end{array } \\right]\\ ] ] with @xmath155 , @xmath156 and @xmath157.\\ ] ] it is more convenient to rewrite the expression @xmath158 as follows .",
    "let @xmath159 and @xmath160 be the symmetric and skew - symmetric parts of @xmath161 , respectively .",
    "that is , @xmath162 and @xmath163",
    ". then we set @xmath164,\\qquad \\bar{t}_n=\\frac{1}{2}\\left [ \\begin{array}{cc } -r_n   & t_n    \\\\",
    "-t_n   & r_n \\end{array } \\right].\\ ] ] we can easily check @xmath165 , so we obtain @xmath166 .",
    "this is a simple function of @xmath17 , so @xmath150 is more tractable than @xmath96 .",
    "in fact , it turns out that the following result is sufficient for our purpose .",
    "[ prop : main ] for any @xmath167 , we have @xmath168 as @xmath16 and @xmath169 0 @xmath170    the proof of proposition [ prop : main ] consists of elementary but complicated calculations , so we postpone it to section [ section : technical ] .",
    "note that proposition [ prop : main ] yields the invertibility of @xmath171 for sufficiently large @xmath172 if @xmath173 , for @xmath174 .",
    "set @xmath175 by virtue of proposition [ prop : hellinger ] and  , it suffices to prove the following statements : @xmath176 follows from proposition [ prop : main ] and proposition 2 of @xcite . on the other hand , setting @xmath177 , we have @xmath178 by proposition [ prop : main ] . therefore , by proposition [ prop : hellinger ] , and proposition 2 from chapter 4 of @xcite we obtain once we show that @xmath179    the strategy of the proof of is the same as that of theorem 4.2 from @xcite .",
    "first , by eq.(4.3 ) of @xcite , for sufficiently large @xmath172 we have @xmath180&=-\\frac{1}{2}\\left\\{\\log\\det v_n(r_nu_n)-\\log\\det v_n(0)+\\operatorname{tr}(v_n(0)(v_n(r_nu_n)^{-1}-v_n(0)^{-1}))\\right\\}+\\frac{\\|a_n\\|_f^2}{4}\\\\ & = -\\frac{1}{2}\\left[\\left\\{\\log\\det(e_{2n}-a_n)+\\operatorname{tr}(a_n)+\\frac{1}{2}\\operatorname{tr}(a_n^2)\\right\\ } + \\operatorname{tr}((e_{2n}-a_n)^{-1}-e_{2n}-a_n - a_n^2)\\right].\\end{aligned}\\ ] ] note that it holds that @xmath181 for sufficiently large @xmath172 because @xmath182 as @xmath16 by proposition [ prop : main ] .",
    "combining this fact with inequality ( v ) from appendix ii of @xcite , we obtain @xmath183 for sufficiently large @xmath172 .",
    "hence proposition [ prop : main ] yields @xmath184\\to0 $ ] .",
    "next , eq.(4.4 ) of @xcite yields @xmath185&=\\left\\|v_n(0)^{\\frac{1}{2}}(v_n(\\vartheta)^{-1}-v_n(0)^{-1})v_n(0)^{\\frac{1}{2}}-a_n\\right\\|_f^2 = \\left\\|(e_{2n}-a_n)^{-1}-e_{2n}-a_n\\right\\|_f^2.\\end{aligned}\\ ] ] therefore , using the identity @xmath181 again we obtain @xmath186\\leq\\|a_n\\|_\\mathrm{sp}^2\\|a_n\\|_f^2(1-\\|a_n\\|_\\mathrm{sp})^{-2}$ ] for sufficiently large @xmath172 . hence proposition [ prop : main ] again yields @xmath187\\to0 $ ] , and we obtain .",
    "we finish this section with some remarks .    from an econometric point of view , proposition [ prop : hellinger ] is of independent interest because the model given by  has an economic interpretation different from model .",
    "this model contains measurement errors correlated to the latent returns @xmath188 .",
    "the integrated volatility estimation in the presence of this type of measurement error has been studied by @xcite for example . in the market microstructure theory ,",
    "such a correlation is often explained as an effect of asymmetric information ( e.g.  @xcite ) .",
    "interestingly , some economic arguments suggest that such an information asymmetry would cause a lead - lag effect ; see @xcite and @xcite for instance .",
    "it would also be worth emphasizing that @xcite connect this type of model with the investigation of price discovery , for price discovery processes are closely related to lead - lag effects , as seen in @xcite and @xcite .",
    "0    if @xmath189 , in addition to an observation from a usual gaussian shift experiment @xmath62 , the limit experiment contains an extra observation from the experiment @xmath63 . from ",
    "we see that @xmath190 as @xmath191 , so this extra observation becomes more informative as the absolute value of the correlation increases .",
    "our proof of the main result heavily depends on the gaussianity of the model , and especially we require the gaussianity of the measurement errors .",
    "it is obvious that we need some restriction on the distribution of the measurement errors to derive a specific limit experiment .",
    "in fact , if @xmath192 and @xmath193 s take their values only in integers , then we can completely recover the signal for sufficiently large @xmath172 .",
    "apart from such a trivial example , the recent study of @xcite has showed that another ( non - trivial ) specification for the distribution of the measurement errors @xmath194 s in can improve the convergence rate for estimating the scale parameter @xmath19 . in the light of the connection between the convergence rates for models  , we naturally conjecture that a similar specification for the measurement errors would affect the convergence rate for our model .",
    "this issue is beyond the scope of this paper and left to future research .",
    "as an application of the results from the previous section , we construct efficient estimators for the lag parameter @xmath17 in the models @xmath85 at @xmath86 . here we consider a slightly extended setup as follows : letting @xmath195 be a sequence of positive numbers tending to 0 and @xmath196 be a bounded open interval in @xmath50 , we construct efficient estimators for the parameter @xmath197 in the models @xmath198 at every @xmath199 . to make use of the results from the previous section , we impose the following condition on @xmath195 : @xmath200 under",
    "there is a positive integer @xmath201 such that @xmath202 and @xmath203 is invertible for any @xmath204 and @xmath205 due to proposition [ prop : main ] . throughout this section",
    ", we always assume that @xmath172 is larger than such an @xmath201 .",
    "we start with generalizing proposition [ prop : main ] by a matrix perturbation argument .",
    "[ lemma : local ] for any @xmath167 we have @xmath206 as @xmath16 .    setting @xmath207 , we have @xmath208 for any @xmath204 .",
    "therefore , ostrowski s theorem ( theorem 4.5.9 of @xcite ) implies that @xmath209 and @xmath210 hence , proposition [ prop : main ] implies that the proof is completed once we show that @xmath211 and @xmath212 as @xmath16 .",
    "since @xmath213 and @xmath214 share the same eigenvalues ( theorem 1.3.20 of @xcite ) and @xmath215 , the desired results follow from proposition [ prop : main ] , and the neumann series representation of @xmath214 .    using the above result",
    ", we can prove a uniform version of theorem [ thm : main ] .",
    "[ prop : infinitesimal ] let @xmath54 and @xmath55 be defined by",
    ". then @xmath216 under @xmath217 as @xmath16 uniformly in @xmath204 for any bounded sequence @xmath57 of real numbers .",
    "moreover , @xmath218 under @xmath217 as @xmath16 uniformly in @xmath204 .",
    "we can prove the first claim in a similar manner to the proof of theorem [ thm : main ] using lemma [ lemma : local ] instead of proposition [ prop : main ] . to prove the second claim",
    ", it suffices to show that @xmath219 under @xmath220 as @xmath16 for any sequence @xmath221 of numbers in @xmath222 , which follows from lemma [ lemma : local ] and inequality ( 13 ) from @xcite .",
    "now we construct our estimators . as in the previous section",
    ", we would like to work with the tractable model @xmath99 rather than the original model @xmath10 .",
    "for this reason we consider the quasi - likelihood function based on the former as follows : @xmath223 then we define the quasi maximum likelihood estimator ( qmle ) @xmath224 as a solution of the equation @xmath225 note that the above equation always has at least one solution belonging to the closure of @xmath196 because @xmath226 is continuous .",
    "we also define the quasi bayesian estimator ( qbe ) @xmath227 for a prior density @xmath228 with respect to the quadratic loss by @xmath229 where the prior density @xmath230 is assumed to be continuous and satisfy @xmath231 .",
    "the corresponding qmle and qbe in the experiments @xmath85 are given by @xmath232 and @xmath233 , respectively .",
    "to describe the limit distribution of these estimators , we introduce the likelihood ratio process for the limit experiment @xmath234 where @xmath39 and @xmath40 are two mutually independent variables such that @xmath235 and @xmath236 .",
    "then we set @xmath237 and @xmath238    we first give the asymptotic behavior of the estimators @xmath224 and @xmath227 in the experiments @xmath239 using the general scheme of @xcite .",
    "note that in this situation @xmath224 and @xmath227 are true maximum likelihood and bayesian estimators , respectively .",
    "[ prop : ih ] for any compact subset @xmath240 of @xmath196 , uniformly in @xmath241 it holds that @xmath242 converges in law to @xmath243 under @xmath244 and @xmath245\\to e[|\\hat{u}|^p]$ ] for any @xmath246 as @xmath16 . also , uniformly in @xmath241 it holds that @xmath247 converges in law to @xmath248 under @xmath244 and @xmath249\\to e[|\\tilde{u}|^p]$ ] for any @xmath246 as @xmath16 .    for every @xmath199 , we set @xmath250 and define @xmath251 for each @xmath252 . according to theorems i-10.1 and i-10.2 from @xcite",
    ", it suffices to prove the following statements :    1 .",
    "@xmath253<\\infty$ ] , [ ih1 ] 2 .",
    "there is a constant @xmath254 such that @xmath255<\\infty$ ] , [ ih2 ] 3 .",
    "the marginal distributions of @xmath256 converge in law to the marginal distributions of @xmath257 uniformly in @xmath241 as @xmath16 .",
    "[ ih3 ]    [ ih3 ] is an immediate consequence of proposition [ prop : infinitesimal ] . on the other hand , by eq.(a.4 ) from @xcite",
    "we obtain @xmath258 = h^2(\\widetilde{\\mathbb{p}}_{n , c\\eta_n+r_nu},\\widetilde{\\mathbb{p}}_{n , c\\eta_n+r_nv})\\\\ & \\leq2\\rho^2r_n^2\\|v_n(c\\eta_n+r_nu)^{-\\frac{1}{2}}((u - v)\\bar{t}_n+|u - v|\\bar{s}_n)v_n(c\\eta_n+r_nu)^{-\\frac{1}{2}}\\|_f^2\\\\ & \\leq4\\rho^2r_n^2(u - v)^2\\left(\\|v_n(c\\eta_n+r_nu)^{-\\frac{1}{2}}\\bar{t}_nv_n(c\\eta_n+r_nu)^{-\\frac{1}{2}}\\|_f^2+\\|v_n(c\\eta_n+r_nu)^{-\\frac{1}{2}}\\bar{s}_nv_n(c\\eta_n+r_nu)^{-\\frac{1}{2}}\\|_f^2\\right),\\end{aligned}\\ ] ] hence lemma [ lemma : local ] yields claim [ ih1 ] .",
    "now we consider [ ih2 ] . by corollary 3.2a.1 from",
    "@xcite we have 0 @xmath259 = \\left\\{\\frac{\\det v_n(c\\eta_n)}{\\det v_n(c\\eta_n+r_nu)}\\right\\}^{1/4}\\det\\left(e_{2n}+\\frac{1}{2}(v_n(c\\eta_n+r_nu)^{-1}-v_n(c\\eta_n)^{-1})v_n(c\\eta_n)\\right)^{-\\frac{1}{2}},\\ ] ] hence it holds that @xmath260 = -\\frac{1}{4}\\log\\det[e_{2n}-a_{n , c}(u ) ] -\\frac{1}{2}\\log\\det\\left(e_{2n}+\\frac{1}{2}\\left((e_{2n}-a_{n , c}(u))^{-1}-e_{2n}\\right)\\right),\\end{aligned}\\ ] ] where @xmath261 .",
    "then we consider the following decomposition : @xmath262\\\\ & = -\\frac{1}{4}\\left\\{\\log\\det[e_{2n}-a_{n , c}(u)]+\\operatorname{tr}[a_{n , c}(u)]+\\frac{1}{2}\\|a_{n , c}(u)\\|_f^2\\right\\}\\\\ & \\quad-\\frac{1}{2}\\left\\{\\log\\det\\left(e_{2n}+\\frac{1}{2}b_{n , c}(u)\\right)-\\frac{1}{2}\\operatorname{tr}\\left(b_{n , c}(u)\\right)+\\frac{1}{8}\\|b_{n , c}(u)\\|_f^2\\right\\}\\\\ & \\quad+\\frac{1}{4}\\left\\{\\operatorname{tr}[a_{n , c}(u)]+\\|a_{n , c}(u)\\|_f^2-\\operatorname{tr}\\left(b_{n , c}(u)\\right)\\right\\}-\\frac{1}{8}\\left\\{\\|a_{n , c}(u)\\|_f^2-\\frac{1}{2}\\|b_{n , c}(u)\\|_f^2\\right\\}\\\\ & = : \\mathbb{i}_{n , c}(u)+\\mathbb{ii}_{n , c}(u)+\\mathbb{iii}_{n , c}(u)+\\mathbb{iv}_{n , c}(u),\\end{aligned}\\ ] ] where @xmath263 .",
    "let us set @xmath264 and @xmath265 .",
    "then we have @xmath266 , hence it holds that @xmath267 in particular , we have @xmath268 for sufficiently large @xmath172 by and lemma [ lemma : local ] .",
    "for such an @xmath172 we have @xmath269 and thus @xmath270 where we use the inequality @xmath271 for @xmath272 to obtain the latter estimate .",
    "therefore , for sufficiently large @xmath172 we have for any @xmath199 and any @xmath252 @xmath273 by appendix ii-(v ) from @xcite and @xmath274 by the inequality @xmath275 for @xmath276 , as well as @xmath277 consequently , there is a constant @xmath278 such that for sufficiently large @xmath172 it holds that @xmath279 \\leq-\\kappa_0\\|a_{n , c}(u)\\|_f^2\\ ] ] for any @xmath199 and any @xmath252 .",
    "now we consider giving an upper bound for @xmath280 .",
    "we have @xmath281 therefore , noting @xmath282 , by lemma [ lemma : local ] for sufficiently large @xmath172 we have @xmath283 for any @xmath199 and any @xmath252 .",
    "consequently , we obtain [ ih2 ] by setting @xmath284 .    if @xmath189 , is equivalent to the condition that @xmath285 as @xmath16 .",
    "therefore , proposition [ prop : hellinger](a ) yields the following result :    [ coincide ] if @xmath189 , the statement of proposition [ prop : ih ] still holds true while @xmath244 are replaced by @xmath217 s .",
    "now we return to the efficient estimation of the parameter @xmath197 in the model @xmath198 .",
    "first we consider the case @xmath84 . in this case we know that @xmath198 enjoys the lan property at every @xmath199 by proposition [ prop : infinitesimal ] , so the definition of the asymptotic efficiency of an estimator sequence is well - established , which is found in section ii-11 of @xcite for example .",
    "if @xmath84 , both @xmath224 and @xmath227 are asymptotically efficient at every @xmath204 in the experiments @xmath198 .",
    "that is , both @xmath242 and @xmath247 converge in law to @xmath286 under @xmath217 for any @xmath199 as @xmath16 . in particular , both @xmath287 and @xmath288 are asymptotically efficient at @xmath86 in the experiments @xmath289 .",
    "next we turn to the case @xmath189 . in this case the experiments @xmath198 no longer enjoy the lan property , so the definition of the asymptotic efficiency is not obvious . here",
    "we follow the approach of @xcite to define the asymptotic efficiency for our experiments , which is based on an asymptotically minimax lower bound derived from the asymptotic properties of the bayesian estimators .",
    "we obtain the following result by virtue of corollary [ coincide ] and theorem i-9.1 of @xcite :    [ thm : minimax ] if @xmath189 , we have @xmath290\\geq e[\\tilde{u}^2]\\ ] ] for any @xmath291 and any estimator sequence @xmath292 in the experiments @xmath198 .",
    "in particular , we also have @xmath293\\geq e[\\tilde{u}^2]\\ ] ] for any estimator sequence @xmath294 in the experiments @xmath289 .",
    "thanks to theorem [ thm : minimax ] , an estimator sequence @xmath292 is said to be asymptotically efficient at @xmath291 in the experiments @xmath198 if it holds that @xmath290=e[\\tilde{u}^2].\\ ] ] similarly , an estimator sequence @xmath294 is said to be asymptotically efficient at @xmath86 in the experiments @xmath289 if it holds that @xmath293=e[\\tilde{u}^2].\\ ] ] the following result is an immediate consequence of this definition .    if @xmath189 , the sequence @xmath227 is asymptotically efficient at every @xmath204 in the experiments @xmath198 .",
    "in particular , the sequence @xmath288 is asymptotically efficient at @xmath86 in the experiments @xmath85 .",
    "in contrast , there is no guarantee of the asymptotic efficiency of the ( q)mle @xmath224 if @xmath189 .",
    "in fact , @xmath227 may perform much better than @xmath224 if @xmath295 , as shown in the following proposition .",
    "it holds that @xmath296 & = \\frac{1}{i_\\gamma+j_\\gamma}\\left(1-\\frac{1}{\\pi}\\arctan\\left(\\sqrt{\\frac{j_\\gamma}{i_\\gamma}}\\right)+\\frac{\\sqrt{i_\\gamma j_\\gamma}}{\\pi(i_\\gamma+j_\\gamma)}\\right),\\label{mle : var}\\\\ e\\left[\\tilde{u}^2\\right ] & = \\frac{1}{i_\\gamma+j_\\gamma}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty\\left(\\frac{x\\psi(x)-y\\psi(y)}{\\psi(x)+\\psi(y)}\\right)^2\\psi_r(x , y)\\mathrm{d}x\\mathrm{d}y,\\label{bayes : var}\\end{aligned}\\ ] ] where @xmath297 and @xmath298 denotes the bivariate normal density with standard normal marginals and correlation @xmath299 .",
    "in particular , @xmath300/e\\left[\\hat{u}^2\\right]=0 $ ] if @xmath295 .",
    "let us denote by @xmath301 the normal density with mean 0 and variance @xmath302 .",
    "then , a simple calculation yields @xmath296&=\\frac{2}{(i_\\gamma+j_\\gamma)^2}\\int_0^\\infty z^2\\mathrm{d}z\\int_{0}^\\infty \\phi_{i_\\gamma}(x)\\phi_{j_\\gamma}(z - x)\\mathrm{d}x.\\end{aligned}\\ ] ] by formulae ( 3.322.2 ) and ( 6.292 ) from @xcite we have @xmath303 hence we obtain .",
    "next , by a change of variable we obtain @xmath304 moreover , formulae ( 3.462.5 ) and ( 3.322.2 ) form @xcite imply that @xmath305 since the distribution of the vector @xmath306 has the density @xmath298 , we obtain .    finally , we prove the latter statement .",
    "define the functions @xmath307 and @xmath308 on @xmath309 by @xmath310 then we have @xmath311=f(r)/(i_\\gamma+j_\\gamma)$ ] and @xmath312=g(r)/(i_\\gamma+j_\\gamma)$ ] . since @xmath313 as @xmath191 if @xmath295 and @xmath314 , it suffices to prove @xmath315 . because we have @xmath316 the dominated convergence theorem yields @xmath315 , which completes the proof",
    "before starting the proof , we introduce some notation .",
    "we set @xmath317 then we define the @xmath146 matrix @xmath318 by @xmath319=\\frac{2}{\\sqrt{2n+1}}\\cos\\left[\\xi_i(2j-1)\\right],\\ ] ] which is often referred to as the discrete cosine transform ( dct ) of type - viii ( see @xcite and references therein ) . note that @xmath320 and @xmath321 is real orthogonal .",
    "it is known that @xmath321 diagonalizes @xmath322 as follows : @xmath323.\\ ] ] see lemma 1 of @xcite or lemma c.2 of @xcite for the proof .    for each @xmath82",
    "we define the functions @xmath324 and @xmath325 on @xmath50 by @xmath326 we also set @xmath327 .",
    "from we have @xmath328    for a square matrix @xmath66 , @xmath329 denotes the spectral radius of @xmath66 .",
    "we will frequently use the identity @xmath330 holding if @xmath66 is a normal matrix .",
    "now we start the main body of the proof .",
    "we will frequently use the following well - known inequality for the sine function , @xmath331    [ lemma : calculus ] for any @xmath82 , we have @xmath332    the claim immediately follows from the identity @xmath333    [ smallnoise ] let @xmath334\\to\\mathbb{r}$ ] be continuous .",
    "also , let @xmath335 be a sequence of positive integers such that @xmath336 and @xmath337 $ ] as @xmath16 .",
    "then , for any @xmath246 we have @xmath338 provided that @xmath295 .    by the fundamental theorem of calculus , we have @xmath339 hence the desired result follows from the standard riemann sum approximation .",
    "0 @xmath340    [ lambda.inv ] let @xmath335 be a sequence of positive integers such that @xmath336 and @xmath337 $ ] as @xmath16 .",
    "then @xmath341 for any @xmath342 such that @xmath343 .",
    "first , using the lower and upper darboux sums of the integral @xmath344 , we obtain @xmath345 now , formula ( 2.553.3 ) in @xcite yields @xmath346 hence we obtain .",
    "next , a simple calculation yields @xmath347 therefore , if @xmath295 , lemma [ smallnoise ] implies that @xmath348 on the other hand , if @xmath349 , for sufficiently large @xmath172 we have @xmath350 , hence @xmath351 therefore , we obtain @xmath352 hence the desired result follows from .",
    "0 @xmath353    [ lemma : bs ] let @xmath335 be a sequence of positive integers such that @xmath336 and @xmath354 as @xmath16 .",
    "then @xmath355    since @xmath356 , we have @xmath357 therefore , using the formula @xmath358 , we can decompose the target quantity as @xmath359    first we prove @xmath360 . using the monotonicity of the tangent function and assumption @xmath336 , we obtain @xmath361 since formula ( 2.526.21 ) in @xcite yields @xmath362 , we conclude that @xmath360 .",
    "next we prove @xmath363 .",
    "our proof relies on the following inequality for the tangent function : @xmath364 the lower estimate of is well - known , and the upper estimate is known as the becker - stark inequality ( eq.(2 ) of @xcite ) .",
    "now , using , we obtain @xmath365 therefore , using formula @xmath366 , we conclude that @xmath363 . 0",
    "the becker - stark inequality : @xmath367 for @xmath368 $ ] @xmath369    @xmath370    [ lemma : normr ] for any @xmath342 , it holds that @xmath371 and @xmath372 as @xmath16 . 0    1 .",
    "@xmath373 , 2 .   @xmath374 .    first , by definition we have @xmath375 , hence and theorem 5.6.9 of @xcite yield @xmath376 therefore , lemma [ lambda.inv ] implies that @xmath371 .",
    "moreover , since it holds that @xmath377 lemma [ lambda.inv ] again yields the desired result .",
    "[ lemma : lemmas ] for any @xmath82 , we have 0 @xmath378 @xmath379 as @xmath16 .    since @xmath380 , by lemma [ lemma : normr ] it suffices to prove in the case where @xmath159 is replaced by @xmath322 .",
    " imply that @xmath381 and @xmath382 the first equation in immediately follows from . in order to prove the second equation in",
    ", we will prove the right side of converges to @xmath383 as @xmath16 .",
    "first , if @xmath295 , the desired result follows from lemma [ smallnoise ] .",
    "next , if @xmath384 , noting that @xmath385 , we have @xmath386 hence by lemma [ lambda.inv ] we obtain the desired equation once we prove @xmath387 the monotonicity of the cosine function yields @xmath388 since formula ( 3.661.4 ) in @xcite implies that @xmath389 we obtain the desired result .",
    "finally , if @xmath84 , using the inequality @xmath390 we obtain @xmath391 hence we deduce the desired result .    0    @xmath392 as @xmath16 for any @xmath342 .",
    "it suffices to prove @xmath393 because of lemma [ lemma : normr ] . since @xmath394",
    ", we have @xmath395 using the trigonometric identities @xmath396 and",
    "@xmath397 , we obtain 0 @xmath398 @xmath399 then , using summation formula ( 1.342.3 ) of @xcite , we have 0 @xmath400 @xmath401 0 @xmath402 now , using and inequalities  , we obtain @xmath403 hence lemma [ lemma : calculus ] yields the desired result .",
    "[ lemma : lemmat ] if @xmath302 and @xmath404 are positive numbers such that @xmath343 , we have @xmath405    since @xmath394 , we have @xmath406 using the trigonometric identities @xmath396 and @xmath397 , we obtain @xmath407 then , using summation formula ( 1.342.3 ) of @xcite , we have @xmath408 now , since @xmath409 and @xmath410 , by , and the unitary invariance of the frobenius norm , we obtain @xmath411 first we consider @xmath412 .",
    "using inequalities and @xmath413 ( @xmath414 ) , we have @xmath415 and thus lemma [ lemma : calculus ] yields @xmath416 .",
    "next we consider @xmath417 .",
    "first we prove @xmath418 , where @xmath419 lemma [ lemma : calculus ] and yield @xmath420 if @xmath84 , by the property of @xmath325 we have @xmath421 hence lemma [ lemma : calculus ] yields @xmath422 .",
    "this also holds true in the case that @xmath189 due to lemma [ lambda.inv ] because @xmath423 .",
    "consequently , @xmath424 .",
    "now , since @xmath425 , for any @xmath426 we have @xmath427 therefore , lemma [ lemma : bs ] implies that @xmath428 then , letting @xmath429 , by lemma [ lambda.inv ] we obtain @xmath430 .    by symmetry we have @xmath431 , hence we complete the proof due to and lemma [ lemma : normr ] .",
    "set @xmath432.\\end{aligned}\\ ] ] then we have 0 @xmath433\\bar{u}_n = \\frac{1}{2}\\left [ \\begin{array}{cc } u(a+b+c+d)u & -u(a - b+c - d)u \\\\ -u(a+b - c - d)u & u(a - b - c+d)u \\end{array } \\right],\\end{aligned}\\ ] ] hence we obtain @xmath434\\end{aligned}\\ ] ] and @xmath435\\end{aligned}\\ ] ] and @xmath436.\\end{aligned}\\ ] ] therefore , it holds that @xmath437\\bar{u}_n^\\top\\end{aligned}\\ ] ] and @xmath438\\bar{u}_n^\\top,\\end{aligned}\\ ] ] where @xmath439 and @xmath440 .",
    "hence we obtain @xmath441 therefore , lemmas [ lemma : lemmas ] and [ lemma : lemmat ] yield . on the other hand , since @xmath442 , lemma [ lemma : lemmas ] also yields @xmath443 .",
    "hence the proof is completed once we prove @xmath444 .",
    "note that @xmath445 is positive semidefinite and @xmath446 for any @xmath98 .",
    "note also that both @xmath447 and @xmath448 are symmetric .",
    "therefore , if @xmath449 is an eigenvalue of @xmath450 , by the monotonicity theorem for eigenvalues ( corollary 4.3.3 of @xcite ) we have @xmath451 for any @xmath98 .",
    "since we can take @xmath452 , this inequality implies that @xmath453 , which yields the desired result .",
    "the author thanks the participants at asc2013 asymptotic statistics and computations , statistics for stochastic processes and analysis of high frequency data , statistique asymptotique des processus stochastiques x , and statistics for stochastic processes and analysis of high frequency data iv for valuable comments .",
    "this work was supported by crest , jst ."
  ],
  "abstract_text": [
    "<S> this paper considers two brownian motions in a situation where one is correlated to the other with a slight delay . </S>",
    "<S> we study the problem of estimating the time lag parameter between these brownian motions from their high - frequency observations , which are possibly subject to measurement errors . </S>",
    "<S> the measurement errors are assumed to be i.i.d . </S>",
    "<S> , centered gaussian and independent of the latent processes . </S>",
    "<S> we investigate the asymptotic structure of the likelihood ratio process for this model when the lag parameter is asymptotically infinitesimal . </S>",
    "<S> we show that the structure of the limit experiment depends on the level of the measurement errors : if the measurement errors locally dominate the latent brownian motions , the model enjoys the lan property . </S>",
    "<S> otherwise , the limit experiment does not result in typical ones appearing in the literature . </S>",
    "<S> the proof is based on the fact that the model is asymptotically equivalent to discrete observations of two brownian motions under endogenous measurement errors . </S>",
    "<S> we also discuss the efficient estimation of the lag parameter to highlight the statistical implications .    </S>",
    "<S> _ keywords _ : asymptotic efficiency ; endogenous noise ; lead - lag effect ; local asymptotic normality ; microstructure noise . </S>"
  ]
}