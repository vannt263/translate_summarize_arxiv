{
  "article_text": [
    "with the rapid development of deep learning , neural networks start to show its great capability in nlp tasks[auli _ et al .",
    "_ , 2013 ] and recent research revealed that recurrent neural networks(rnn ) significantly outperforms popular statistical algorithms like hidden markov model(hmm)[zhang _ el al . _ , 2003 ] , crf(conditional random field)[peng _ et al .",
    "_ , 2004 ] and neural probabilistic models[bengio _ et al .",
    "_ , 2003 ] .",
    "as a special kind of rnn , lstm neural networks[hochreiter _ et al . _ ,",
    "1997 ] is verified to be efficient in modeling sequential data like speech and text [ sundermeyer _ et al .",
    "_ , 2015 ] .",
    "more over , blstm neural network[schuster _",
    "et al . _ ,",
    "1997 ] , which is derived from lstm network , has advantages in memorizing information for long periods in both directions , making great improvement in linguistic computation .",
    "sundermeyer et al .",
    "analyzed lstm neural network by modeling english and french[sundermeyer _ et al .",
    "_ , 2012 ] .",
    "wang et al . used bi - directional lstm into pos tagging , chunking and ner tasks and internal representations are learnt from unlabeled text for all tasks[wang _ et al .",
    "_ , 2015 ] .",
    "huang et al . combined lstm with crf and verified the efficiency and robustness of their model in sequential tagging[huang _ et al . _ ,",
    "ling et al focus on constructing compact vector representations of words with bi - directional lstm , which yield state - of - the - art performance in contrast to other word - to - vector algorithms like cbow and skip - n - gram[ling _ et al .",
    "_ , 2015 ] .",
    "the study that is close to ours is chen et al , which introduced lstm neural network into chinese word segmentation[chen _ et al .",
    "_ , 2015 ] , while lstm can just memorize the past contextual information from the context . due to the complicated and changeable structure of chinese sentence",
    ", it s intuitive that both future and past information need to be considered when training segmentation model .    in this study , in order to improve the performance of the chinese word segmentation , we applied bi - directional lstm networks to word segmentation task , we also constructed higher - level features of characters with blstm network and our contributions can be listed as follows : + 1 ) our work is the first to apply and improve bi - directional lstm network to chinese word segmentation benchmark data sets + 2 ) the training framework can be regarded as an integration of generating embeddings and tagging characters and it does not need any external datasets the paper is organized as follows . section 2 describes basic idea and architecture of blstm network .",
    "next , we introduce our training framework based on blstm network in section 3 .",
    "section 4 details our experiments on chinese dataset and summarizes our experimental results with previous research .",
    "finally , in section 5 we summarize key conclusions .",
    "blstm neural network is similar to lstm network in structure because both of them are constructed with lstm units[schuster _ et al .",
    "_ , 1997 ] .",
    "the special unit of this network is capable of learning long - term dependencies without keeping redundant context information .",
    "they work tremendously well on sequential modeling problems , and are now widely used in nlp tasks .      the basic structure of lstm memory unit",
    "is composed of three essential gates and a cell state .",
    "as shown in figure 1 , the memory cell contains the information it memorized at time @xmath0 , the state of the memory cell is bound up together with three gates , the input vector of each gate is composed of input part and recurrent part .",
    "forget gate controls what to abandon from the last moment , input gate decides what new information will be stored in the cell state , the output gate decides which part of the cell state will be output and the recurrent part will be updated by current cell state and fed into next iteration[hochreiter _ et al .",
    "_ , 1997 ] .",
    "the formal formulas for updating each gate and cell state are defined as follows : @xmath1    here @xmath2 and @xmath3 are input and output vector of the unit at time@xmath0 , @xmath4 and @xmath5 are weight matrices for input part and recurrent part of different gates , @xmath6 denotes bias vector and the functions @xmath7 , @xmath8 and @xmath9 are non - linear functions such as sigmoid or tanh , @xmath10 means point - wise calculation of two vectors . for completeness , we add @xmath11 to the formulas , which denote peephole connection and is mostly used in lstm variants .",
    "blstm network is designed to capture information of sequential dataset and maintain contextual features from past and future .",
    "different from lstm network , blstm network has two parallel layers propagating in two directions , the forward and backward pass of each layer are carried out in similar way of regular neural networks , these two layers memorize the information of sentences from both directions.[schuster _ et al .",
    "_ , 1997 ] since there are two lstm layers in our network , the vector formula should be also adjusted .",
    "@xmath12 @xmath13 and @xmath14 denotes the output vector of forward layer and backward layer respectively , different from former research , the final output in our work @xmath15 $ ] is the concatenation of these two parts , which means @xmath16 .",
    "we define the combination of forward and backward layers as a single blstm layer .",
    "in order to convert the segmentation problem into a tagging problem , we assign a label for each character to indicate the segmentation .",
    "there are four kinds of labels : _ b , m , e , s _ , corresponding to the beginning , middle , end of a word , and a single - character word , respectively .",
    "the basic procedure of language modeling in our study is shown in figure 2 .",
    "each character has an i d which is defined in a lookup dictionary , the dictionary is constructed by collecting unique characters in the training set . instead of one - hot representation",
    ", the characters are projected into a d - dimension space and initialized as dense vectors @xmath17 , we regard this initialization step as constructing embeddings for characters . every embedding is stored in a matrix @xmath18 and can be retrieved by its character i d . as embeddings are efficient in describing word - level features[miklov _ et al . _ ,",
    "2013 ] , we hope character - level embeddings can also achieve good performance in cws",
    ".        then he embeddings are fed into blstm network and the final output of blstm network is finally passed to a hidden layer and the softmax layer determines the tag with maximum probability of the character .      in order to further improve the structure of blstm network , we stack blstm layers based on the method of constructing rnn[pascanu _ et al . _ ,",
    "expecting to extract contextual features in higher level .",
    "however , the output of a blstm layer is in double size of the input vector since it is composed of two lstm layers , and its dimension will expand dramatically when the network goes deeper , here we use a transformation matrix to compress the dimension of output vectors , and keep it the same size with input vectors .",
    "assume that the output vector of blstm layer @xmath20 , the transformation matrix @xmath21 convert the vectors into lower dimension and thus the output of each blstm layer is kept the same dimension .",
    "as our blstm network gets more and more complicated , the number of parameters grows rapidly and we used dropout during training in order to avoid overfitting[srivastava _ et al .",
    "_ , 2014 ] .",
    "* setup * the dataset we used for evaluating our model on word segmentation is from backoff 2005 , which contains benchmark datasets for both simplified chinese(pku and msra ) and traditional chinese(as and hk ) .",
    "all the models are trained on nvidia gtx geforce 970 , it took about 16 to 17 hours to train a model on gpu while more than 4 days to train on cpu , in contrast .",
    "we also changed batch size during our training process because of the limit of gpu memory .",
    "* results * in this section , we will state the procedure of our experiments and how we get the model with the best performance , and we will also compare the performance of our network with state - of - the - art approaches .    * 13c * models & & & & + & p & r & f & p & r & f & p & r & f & p & r & f + bi - rnn & 94.2 & 92.5 & 93.3 & 95.7 & 94.8 & 95.2 & 96.1 & 96.4 & 96.2 & 96.8 & 95.9 & 96.3 + lstm * & 95.3 & 94.6 & 94.9 & 96.1 & 95.3 & 95.7 & 94.2 & 93.2 & 93.7 & 97.2 & 96.6 & 96.9 + blstm * & 96.5 & 95.3 & 95.9 & 96.6 & 97.1 & 96.9 & 97.3 & 96.9 & 97.1 & 97.4 & 97.2 & 97.3 + blstm2 * & 96.6 & 95.9 & 96.2 & 97.3 & 97.1 & 97.2 & 97.9 & 97.5 & 97.7 & 97.5 & * 97.4 & 97.4 + blstm3 * & * 96.8 & * 96.3 & * 96.5 & * 97.4 & * 97.3 & * 97.3 & * 98.0 & * 97.6 & * 97.8 & * 97.7 & 97.3 & * 97.5 + * * * * * * * * * * * *    since the dimension of embedding vector will directly influence the number of parameters and model complexity , we conducted related experiments on hkcityu dataset , and try to get a suitable size of embedding first .",
    "table 2 illustrate the performance of our model on hkcityu dataset with embedding vectors of different dimensions .",
    "when the dimension grows higher , the error rate becomes bigger and unstable .",
    "we can conclude that the model gets the best performance when the embedding dimension is 200 , which also indicates that an efficient representation of chinese words should not be too long .",
    ".[font - table ] performance of blstm networks with different embedding dimensions [ cols=\"^,^,^,^\",options=\"header \" , ]     table 3 lists the performances of our model as well as previous research .",
    "( zhao et al . , 2006 )",
    "is a crf model with rich feature template , ( sun and xu , 2011 ) improved supervised word segmentation by exploiting features of unlabeled data and the system of ( zhang _ et al .",
    "_ , 2013 ) applied semi - supervised approach to extract representations of label distributions from unlabeled and labeled datasets[zhang _",
    "et al . _ ,",
    "nevertheless , all the models or systems above focus on feature engineering , while our approach do not depend on any predesigned features thanks to the strong ability of blstm network in automatic feature learning .",
    "our model achieved competitive performance compared to the work of chen s , which also used character embeddings but applied lstm network to cws , and the results suggest that blstm may get better performance than lstm on segmentation , and indicates that both past and future information should be taken into account for segmentation task .",
    "in this paper , we propose to use bi - direction lstm neural network to train the model for chinese word segmentation , blstm network is quite efficient for sequential tagging task .",
    "the model learn to extract discriminative character - level features automatically and it do not require any hand - craft features for segmentation or prior knowledge .",
    "experiments conducted on sighan backoff 2005 datasets show that our model has good performance and generalization on both simplified chinese and traditional chinese .",
    "our results suggest that deep neural networks work well on segmentation tasks and blstm networks with word embedding is an effective tagging solution and worth further exploration .",
    "pi - chuan chang , michel galley , and christopher  d manning .",
    "optimizing chinese word segmentation for machine translation performance . in _ proceedings of the third workshop on statistical machine translation _ , pages 224232 .",
    "association for computational linguistics , 2008 .",
    "xinchi chen , xipeng qiu , chenxi zhu , pengfei liu , and xuanjing huang .",
    "long short - term memory neural networks for chinese word segmentation . in _ proceedings of the conference on empirical methods in natural language processing _ , 2015 .",
    "wang ling , tiago lus , lus marujo , ramn  fernandez astudillo , silvio amir , chris dyer , alan  w black , and isabel trancoso .",
    "finding function in form : compositional character models for open vocabulary word representation . , 2015 .",
    "tomas mikolov , ilya sutskever , kai chen , greg  s corrado , and jeff dean . distributed representations of words and phrases and their compositionality . in",
    "_ advances in neural information processing systems _ , pages 31113119 , 2013 .",
    "fuchun peng , fangfang feng , and andrew mccallum .",
    "chinese segmentation and new word detection using conditional random fields . in _ proceedings of the 20th international conference on computational linguistics _",
    ", page 562 .",
    "association for computational linguistics , 2004 .",
    "weiwei sun . a stacked sub - word model for joint chinese word segmentation and part - of - speech tagging . in _ proceedings of the 49th annual meeting of the association for computational linguistics : human language technologies - volume 1 _ , pages 13851394 .",
    "association for computational linguistics , 2011 .",
    "hua - ping zhang , hong - kui yu , de - yi xiong , and qun liu .",
    "hhmm - based chinese lexical analyzer ictclas . in _ proceedings of the second sighan workshop on chinese language processing - volume 17 _ , pages 184187 .",
    "association for computational linguistics , 2003 .",
    "longkai zhang , wang houfeng , sun xu , and mairgup mansur .",
    "exploring representations from unlabeled data with co - training for chinese word segmentation . in _ proceedings of the conference on empirical methods in natural language processing _ , 2013 .",
    "hai zhao , chang - ning huang , and mu  li .",
    "an improved chinese word segmentation system with conditional random field . in",
    "_ proceedings of the fifth sighan workshop on chinese language processing _ ,",
    "volume 1082117 .",
    "sydney : july , 2006 ."
  ],
  "abstract_text": [
    "<S> recurrent neural network(rnn ) has been broadly applied to natural language processing(nlp ) problems . </S>",
    "<S> this kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks . in this paper </S>",
    "<S> , we propose to use bi - directional rnn with long short - term memory(lstm ) units for chinese word segmentation , which is a crucial preprocess task for modeling chinese sentences and articles . </S>",
    "<S> classical methods focus on designing and combining hand - craft features from context , whereas bi - directional lstm network(blstm ) does not need any prior knowledge or pre - designing , and it is expert in keeping the contextual information in both directions . </S>",
    "<S> experiment result shows that our approach gets state - of - the - art performance in word segmentation on both traditional chinese datasets and simplified chinese datasets . </S>"
  ]
}