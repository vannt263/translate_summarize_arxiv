{
  "article_text": [
    "the basic machinery for olfaction , our ability to smell , is an array of a few hundred different types of sensory neurons .",
    "each of these expresses molecular receptors , that belong to a single type .",
    "when this small neuronal assembly is exposed to external stimuli , its cooperative response is capable to detect and recognize a wide variety of _ odorants _ and to measure their concentrations .",
    "we use the terminology  odorant \" to describe any chemically homogenous substance ( ligand ) which elicits a response from the olfactory system .",
    "the response of the array of neurons to any particular odorant is determined by the responses of the individual constituent neurons .",
    "this response is , however , governed by the extent to which the receptors expressed by the particular neuron bind the odorant , i.e. by the _ affinity _",
    "@xmath4 of the neuron s receptors to the odorant .",
    "according to a recently proposed model  @xcite , these affinities can be viewed as independent random variables , drawn from a single receptor affinity distribution ( rad ) , denoted by @xmath5 .",
    "once a set of affinities ( for all odorants and all sensory neurons ) has been generated , the response of the entire sensory assembly to any odorant is determined .",
    "this information is transferred from the sensory neurons to the olfactory bulb , onto which the axons of the sensory neurons project .",
    "they form synapses on secondary neurons ( mitral and tufted cells ) .",
    "this integration of the sensory input , that takes place in the olfactory bulb , forms the first step of the information processing that takes place in the olfactory pathway .",
    "interneurons of two major types ( periglomerular and granule cells ) are believed to play a role in computing the pattern transmitted from the olfactory bulb to higher brain centers .    in this paper",
    "we evaluate , on the basis of a very simple model , some of the potential computational characteristics of the olfactory bulb , as it performs this initial integration .",
    "we hope some of our quantitative results could be biologically relevant . our simple model for the sensory array and a single processing unit",
    "is depicted in fig .",
    "[ fig1 ] .",
    "the model we introduce is , however , interesting also from a mathematical point of view .",
    "the problem of linear separability ( * ls * ) of points in @xmath0 dimensional space has received considerable attention since the 19th century @xcite . in the mathematics literature",
    "cover studied the problem of * ls * of independent dichotomies using combinatorial methods  @xcite . in computer science the perceptron , introduced by rosenblatt  @xcite and analyzed in detail by minsky and papert  @xcite , gave a major boost to the field of neural networks .",
    "more recently , by introducing statistical mechanics techniques gardner  @xcite extended cover s results to cases where there are correlations between the points that have to be linearly separated .",
    "we generalize the problem of separating ( zero - dimensional ) _ points _ , to the separability of ( one - dimensional ) _ strings _ or _ curves _ , embedded in @xmath0-dimensional space . in the context of our problem the curves that need be separated are parametrized continuously by the odorant concentration .    in principle one",
    "can address the separability of curves by placing a discrete set of points on each curve , thereby mapping the problem onto the previously solved one , of separating points .",
    "one should note , however , that points that lie on the same curve are not independent ; in fact they are correlated in ways that render the previously developed analytical methods unapplicable .",
    "therefore we present an extensive numerical analysis of the capacity of this special neural network , of @xmath0 sensory neurons that provide input to a single processing unit .",
    "the capacity we calculate is interpreted as follows .",
    "the sensory system is exposed to @xmath6 odorants , _ one at a time_. one of these is the  target \" ; the aim is to distinguish the target from all the other @xmath7 odorants that form a  noisy olfactory background \" .",
    "the model , based on a single layer perceptron , is introduced and discussed in detail in section 2.1",
    ". then we turn to describe the method we have developed in order to determine numerically the capacity . to do this we had to adapt and use several different techniques .",
    "one of these , a learning algorithm introduced by nabutovsky and domany  @xcite , is described in sec 2.2 .",
    "this algorithm , like all other perceptron learning rules , finds the separation plane ( if the problem _ is _ * ls * ) ; however , unlike other learning algorithms , it provides a rigorous signal to the fact that a sample of examples is _ not _ * ls*.    another technique we had to adapt to our purposes is finite size scaling ( fss ) analysis of the data . the main results are presented in sec 3 as curves of capacity as a function of odorant concentration in the thermodynamic ( @xmath8 ) limit , obtained by extrapolation , using fss , from data obtained at a sequence of @xmath0 values .",
    "this large @xmath0 limit is quite natural from both practical and theoretical points of view . in practice ,",
    "for @xmath0 of the order of a few hundred , the results can hardly be distinguished numerically from those at the @xmath8 limit . as to the theoretical side ,",
    "the situation in this limit is much cleaner and easier to analyze .",
    "the final section 4 contains a critical discussion of the results from a biological point of view .    _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ our central finding is summarized in fig [ fig : alphac ] ; if we fix the range of concentrations in which the system operates , and increase the number of background odorants , we will reach a critical number @xmath2 beyond which the system fails to discriminate the target .",
    "this critical number is proportional to the number of sensory neurons @xmath0 , i.e. @xmath9 , and it decreases when the concentration range increases . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "the simple neural assembly that is considered here consists of a single secondary neuron , which receives inputs from an array of @xmath0 units that model the sensory neurons .",
    "the single secondary neuron represents a  grandmother cell \" , whose task is to detect one particular `` target '' odorant , labeled 0 .",
    "the sensory scenario we consider allows exposure of the neuronal assembly to a single odorant , which may either be the target odorant or one of @xmath7 background odorants .",
    "the odorant provides simultaneous stimuli to the @xmath0 sensory neurons .",
    "the aim of the single secondary neuron is to determine whether the odorant that generated the incoming signal from the sensory array is the target odorant 0 or not .",
    "we assume that all odorants , background and target , are presented to the sensory array in concentrations @xmath10 that lie within a range @xmath11 we pose the following , well defined quantitative question :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ what is the maximal number @xmath2 of different background odorants that our neuron can distinguish from the target , for any concentration within the prescribed range _ ?",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    to sharpen the question , we put it in a more precise mathematical form .",
    "consider @xmath12 background odorants with respective concentrations @xmath13 in the range ( [ eq : range ] ) .",
    "odorant @xmath14 is characterized by the @xmath15 affinities @xmath16 of the @xmath0 receptors . according to the rad model",
    ", these affinities are selected independently from a distribution @xmath5 [ 1 ] .",
    "all our numerical results were obtained using for @xmath5 the form ( note : @xmath17 ) @xmath18 the average and variance of this distribution are given by @xmath19 the distributions suggested in @xcite were poisson and binomial . with regard to the computational limitations of our model , the important idea behind",
    "the rad model lies not in the exact form of the distribution , but on the fact that the affinities can be thought of as independent random variables .",
    "the main computational features of our model will not be altered as long as the distribution has the following features : it is zero for negative affinities and it has finite first and second moments .",
    "we have used @xmath20 since it satisfies the previous constraints and is easier to deal with in analytical calculations .    when receptor @xmath21 is exposed to odorant @xmath14 , at concentration @xmath13 ,",
    "its response is given by @xmath22 where @xmath23 is a sigmoid shaped function ; we use @xmath24 throughout this paper .    the value taken by the affinity @xmath16 sets the particular concentration scale",
    "at which odorant @xmath25 affects the @xmath21th sensory neuron . from this point on",
    "we set @xmath26 in eq .",
    "( [ eq : psi ] ) ; this means that the concentrations are measured in inverse units of the parameter @xmath27    the set of values @xmath28 constitute a vector of signals * s*@xmath29 , generated by the entire sensory array , when it is exposed to odorant @xmath14 .",
    "the @xmath30 serve as inputs to our secondary neuron , which we model as a linear threshold element or perceptron ; its output signal is given by @xmath31    the simple neural network described above is schematically presented in fig .",
    "the sensory neurons are represented by boxes and the secondary neuron by a circle .",
    "we require the output of this neuron to differentiate the target odorant from the background , i.e. yield    @xmath32    for * any * odorant concentration in the allowed range ( [ eq : range ] ) .    to understand the geometrical meaning of this requirement , note that when the concentration of odorant @xmath14 is varied in the allowed range ( [ eq : range ] ) , the corresponding vector * s*@xmath29 traces a _ curve _ ( or _ string _ ) in the @xmath0-dimensional space of sensory responses .",
    "the requirement ( [ eq : smu ] ) means that there exists a hyperplane , such that the entire curve that corresponds to the target odorant lies on one side of it , while the curves that correspond to _ all _ @xmath1 background odorants lie on the other side .",
    "this explains our statement , made in the introduction , that the problem we solve deals with the _ linear separability of curves_.    we show that a solution to this classification problem can be found , provided @xmath33 .",
    "we estimate the critical capacity @xmath34 numerically .",
    "this is done by extrapolating results obtained for various values of @xmath0 , using finite size scaling techniques , to the limit @xmath8 .",
    "the value of @xmath35 is evaluated as a function of the limiting odorant concentrations .    in order to obtain these results using existing methodology",
    ", the most natural and straightforward thing to do is to place a discrete set of @xmath36 points @xmath37 on each curve , corresponding to different concentrations , and to require that the @xmath38 points that lie on the curve of the target odorant are linearly separable from the @xmath39 points that represent the background .",
    "that is , equations ( [ eq : smu ] ) become @xmath40 this raises the technical question of how many ( discrete ) representatives of the same odorant should be included in the learning set .",
    "we show below , that while the critical number of odorants @xmath41 , scales linearly with @xmath0 , the number of representatives of a single odorant , @xmath38 , has to grow at least as fast as @xmath42 .",
    "this ensures that increasing @xmath38 further does not change the results of the calculation ( e.g. the value of @xmath41 ) and hence the @xmath38 discrete points indeed represent correctly the continuous curves on which they lie .",
    "our problem has been turned into one of learning @xmath43  patterns \" , that constitute our training set @xmath44 . for technical reasons it is convenient to introduce and work with normalized patterns , @xmath45 with @xmath46 running over the @xmath38 discrete concentrations and @xmath47 over all odorants .",
    "note that we also multiplied each pattern * s*@xmath48 by its desired output , @xmath49 ; after this change of representation the condition of linear separability ( [ eq : smuzeta ] ) becomes @xmath50      the question posed above , whether the target odorant can or can not be distinguished from the background , has been reduced to the following one : is there a set of weights @xmath51 , for which all @xmath43 inequalities ( [ eq : pos ] ) are satisfied ?",
    "this problem is of the type studied by rosenblatt@xcite , and is an example of classification by a single layer perceptron .",
    "a solution exists if one can find a weight vector @xmath52 ( that parametrizes the perceptron ) such that for all the patterns @xmath53 in the training set @xmath44 the  field \" @xmath54 i.e. the projection of the weight vector @xmath55 onto all patterns @xmath56 is positive .",
    "we wish to determine the size of the training set @xmath44 , i.e. the number of background odorants @xmath1 , for which a solution @xmath52 can be found .",
    "this is done by executing a search for a solution @xmath52 by means of a _ learning algorithm_. there are several learning algorithms ( e.g. rosenblatt@xcite , abbott and kepler@xcite ) in the literature ; all are guaranteed to find such a weight vector , in a finite number of steps , _ provided a solution exists_. if , however , the problem is _ not _ * ls * and a solution _ does not _ exist , most learning algorithms will just run ad infinitum .",
    "an exception to this is the algorithm of nabutovsky and domany ( nd )  @xcite which detects , in finite time , that a problem is non - learnable .",
    "this is a batch perceptron learning algorithm ,    presenting sequentially the entire training set @xmath44 in one  sweep \" and repeating the process until either a solution is found or non - learnability is established .",
    "we found that this algorithm is efficient and convenient to use ( see @xcite for other algorithms that detect non-*ls * problems ) ..    nd introduced a parameter @xmath57 which they called _ despair _ , which is calculated  on line \" in the course of the learning process .",
    "@xmath57 is bounded if the training set @xmath44 is * ls*. since the nd algorithm can be shown to either find a solution @xmath55 , or transgress the bound for @xmath57 in a finite number of learning iterations , @xmath57 effectively signals if the learning set @xmath44 fails to be linearly separable .",
    "the theorem they proved can be easily extended to the distribution of examples in our problem .",
    "we introduced a halting criterion , which is probably more stringent than necessary , since no attempt has been made to determine an optimal lower bound .",
    "in figures [ fig : dsino]a and [ fig : dsino]b typical evolutions of the despair are shown for an * ls * case and for a non-*ls * case , respectively .",
    "the behavior of @xmath57 is strikingly different in the two cases , showing that indeed @xmath57 is a good indicator of learnability . in the learnable cases @xmath57 grows linearly with the number of learning sweeps until a solution is found ( and the curves terminate ) . in the non-*ls",
    "* cases @xmath57 grows exponentially with the number of sweeps and would continue to grow ; the process is halted when it s value exceeds a known bound , that must be satisfied if the problem is * ls*.    we now describe the nd algorithm used in the simulations .",
    "the patterns of the learning set @xmath44 are presented one at a time ( one cycle constitutes a sweep ) .",
    "nd have shown  @xcite that for binary valued patterns ( @xmath58 ) , i.e. patterns on vertices of a unit hypercube , an upper bound @xmath59 exists iff the training set is * ls*. on the other hand the dynamics is shown to take @xmath57 beyond that bound in a finite ( linear in @xmath0 ) number of iterations unless a solution exists and the algorithm halts . initialize the process with @xmath60 go to the next example .",
    "if it is correctly classified , do nothing to the current weight vector and go to the next example .",
    "once a misclassified example @xmath56 is found , update the weight vector as well as the parameter @xmath57 , according to @xmath61    @xmath62    @xmath63 is not just a learning rate parameter but an effective modulation function , chosen in order to maximize the increase of the despair as    @xmath64    the learning dynamics halts if all patterns are correctly classified or alternatively , if the value of @xmath57 exceeds an upper bound , given by @xmath65 this is guaranteed to happen in at most @xmath0[upp]@xmath66 steps .",
    "since there is a large number of parameters that are to be varied , we present first a detailed description of the manner in which we deal with every one of them .",
    "there are two random elements in our studies .",
    "the first is in the selection of @xmath38 concentrations for each odorant , within the range ( [ eq : range ] ) ; the second is the choice of @xmath67 , the affinity of receptor @xmath21 to odorant @xmath14 , selected at random from the distribution @xmath20 of eq .",
    "( [ eq : psi ] ) . for every choice of the remaining variables we generate an ensemble of experiments and average the object we are measuring over these two random elements .",
    "we select @xmath68 times the set of affinities and for each of these perform @xmath69 times the random selection of concentrations .",
    "the object we wish to estimate numerically is the probability @xmath70 , that the @xmath1 curves described in the introduction are * ls*. to this end we place @xmath38 points on each curve and measure the corresponding probability @xmath71 . as we will see , for large enough values , @xmath72 , this probability _ becomes independent of m _ ; beyond @xmath73 the set of @xmath38 discrete points represents the corresponding curves faithfully and hence the limiting value @xmath74 is our estimate for @xmath75 .",
    "finally , we are interested in this function in the large @xmath0 limit , i.e. when @xmath8 and @xmath76 , while @xmath77 is fixed .",
    "this limit is obtained by extrapolating our finite @xmath0 results , using finite size scaling methods .",
    "our first task is to determine how @xmath73 scales with @xmath0 ; that is , how dense a set of concentrations is to be used so that @xmath38 discrete points represent accurately the continuous curves @xmath78 of eq .",
    "( [ eq : si ] ) ?",
    "we choose values for @xmath0 ( number of receptor cells ) , @xmath1 ( number of odorants ) and @xmath79 ( limiting concentrations ) .",
    "we also set some value for @xmath38 , the number of concentrations by which every odorant is represented ( @xmath38 will will be varied ) .",
    "we proceeded according to the following steps :    1 .",
    "draw from the distribution @xmath80 a set of affinities @xmath67 for all @xmath0 receptors and @xmath1 odorants .",
    "2 .   generate for each odorant @xmath38 concentration values , from a uniform distribution in the allowed range @xmath81 and construct the set @xmath82 of normalized patterns .",
    "3 .   run the nd learning algorithm until it stops ; register whether the set @xmath82 was * ls * or not .",
    "steps 2,3 are repeated @xmath69 times for each set of affinities ; the whole process 1 - 3 is repeated for @xmath68 different sets of affinities .",
    "we used @xmath83 ; increasing it further made no difference .",
    "with such a value of @xmath68 the results did not depend on @xmath69 ; having tried @xmath84 we used @xmath85 in our simulations .    at this point",
    "we have @xmath86 experiments , out of which a fraction of @xmath87 cases were linearly separable . keeping @xmath88 fixed , we increase @xmath38 and repeat the entire process , obtaining the probability functions @xmath87 , that are plotted in fig . [",
    "fig : mnvsp ] versus @xmath89 .",
    "clearly the curves saturate when @xmath90 . from this point on we have fixed the value of @xmath38 at @xmath91 . this numerical result can be estimated by using the analysis of gardner and derrida @xcite for the capacity of biased patterns , using for the  magnetization \" @xmath92 the value @xmath93 .",
    "this gives , in addition to the leading behavior @xmath94 , logarithmic corrections as well .",
    "we can not rule out the possibility of such logarithmic corrections to the scaling we found here .      in all our experiments we fixed the value of @xmath96 and",
    "hence the dependence of the probability on this variable has been suppressed . for various values of @xmath0 , @xmath1 and @xmath97",
    "we calculate @xmath95 in the manner described above . keeping @xmath0 and @xmath97",
    "fixed , we increase @xmath1 .",
    "for @xmath98 we have @xmath99 and the probability of * ls * decreases as @xmath1 increases .",
    "we stop increasing @xmath1 when @xmath95 becomes smaller than some @xmath100    the variation of @xmath95 vs @xmath101 is presented , for three values of @xmath97 and four values of @xmath0 , in fig .",
    "[ fig : gfh ] .",
    "the results presented in these figures are discussed in the next subsection .",
    "we should mention here that for large @xmath0 we used a heuristic modification of the nd halting criterion , to label a problem as non-*ls*. typical evolutions of the despair parameter are shown in figures [ fig : dsino ] .",
    "each curve represents the history for a single learning set .",
    "notice the huge difference in scales for the learnable and the unlearnable cases .",
    "the wide separation in final values of @xmath57 suggests that a more practical , e.g. smaller , upper bound be used . for @xmath102 ( the largest value treated here ) we used a different halting criterion in order to escape from the need to reach an exponentialy high upper bound . after a small number of successful trial runs ( that did produce linear separabitiy ) we identified the highest value of the despair @xmath103 that was reached for a learnable set .",
    "this value was used to define our new heuristic halting criterion , @xmath104 @xmath103 .",
    "as expected , for small @xmath105 the probability for linear separability is close to 1 , and it decreases as @xmath106 increases .",
    "the curves obtained for fixed @xmath97 become sharper as @xmath0 increases .",
    "note that curves obtained for different @xmath0 values cross at approximately the same value of @xmath106 .",
    "similar behavior of the corresponding probability functions has been observed for random uncorrelated patterns  @xcite .",
    "notice , however , that the crossing point is at some probability @xmath107 .",
    "similar curves , obtained for other architectures , such as the parity and commitee machines @xcite crossed at @xmath108 .",
    "if there is a sharp transition in the thermodynamic limit ( @xmath8 ) , these curves should approach a step - function , with @xmath109 that is , for @xmath110 below a certain @xmath111 a learning set will be * ls * with probability one and conversely , it will be * ls * with probability zero for @xmath112 @xmath113 the manner in which such a step function is approached as @xmath8 can be described by a finite size scaling analysis ( e.g. @xcite ) .    for each value of @xmath97",
    "( keeping @xmath114 fixed ) we tried a simple rescaling of the @xmath110 variable , with two adjustable parameters , @xmath35 and @xmath115 ; @xmath116 for the proper choice of @xmath35 and @xmath115 we expect _ data collapse _ ; that is , curves obtained for different values of @xmath0 are expected to fall onto a single function , provided @xmath117 is plotted versus the scaled variable @xmath118 . as can be seen on figures [ fig : scalh]a , b and c ,",
    "this expectation is borne out ; the evidently good data colapse indeed substantiates the idea of a sharp transition at @xmath34 . as @xmath0 increases , the function @xmath117 becomes increasingly sharper ; its width near @xmath35 decreases at a rate governed by the exponent @xmath115 .    finally , we present in figure [ fig : alphac ] the behavior of @xmath34 as a function of @xmath119 ( for fixed @xmath114 ) . as @xmath97 increases , separation of the curves becomes an increasingly difficult task and hence @xmath120 decreases .",
    "we find that it saturates at a low value close to @xmath121 , which is exactly the cover result .",
    "this interesting point is explained in the appendix .",
    "note that even though we deal here with linear separability of _ curves _ , which one would expect to be a more difficult task than separating points , we found that our @xmath35 exceeds the value derived for points , @xmath122 .",
    "the reason is that this is the critical capacity for separating _",
    "random , independent _ points ; the curves we are trying to separate are _ not independent of each other_. in fact by construction we have @xmath123 for all the background odorants ; hence all these curves lie on one side of an entire family of planes .",
    "the target odorant , which also satisfies @xmath124 , should lie on the other side of the separating plane .",
    "the curve @xmath125 is , in effect , a _ phase boundary _ ;",
    "on one side we have a  phase \" in which the problem is * ls * , while on the other ( high @xmath106 ) region it is not .",
    "we present now a brief description of the manner in which linear separability breaks down as we cross this phase boundary by increasing @xmath97 at fixed @xmath106 .",
    "the manner in which * ls * breaks down as @xmath97 increases beyond the phase boundary is nicely illustrated by the set of figures [ fig : ls ] and [ fig : nls ] .",
    "consider the @xmath1 curves , in an @xmath0-dimensional space , which represent the odorants , in a linearly separable case .",
    "we present in figure [ fig : ls](a ) a projection of these curves onto a randomly chosen plane .",
    "one of these ( indicated by an arrow ) is the target odorant ; it seems to be entangled with the other curves .",
    "the point at which all curves seem to converge corresponds to the maximal concentration .",
    "the purpose of the learning dynamics is to find a particular direction * w * , along which one is able to separate the target curve from the others .",
    "denote by @xmath126 the hyperplane that passes through the origin and is _ perpendicular _ to * w * ; this is the linear manifold that separates the target from all the background curves .",
    "select now any plane @xmath127 , that contains * w * , and project all curves onto @xmath127 ; this produces fig .",
    "[ fig : ls](b ) .",
    "the horizontal dotted line shown here is the intersection of the hyperplane @xmath126 with the plane @xmath127 .",
    "the projected background odorant curves lie on one side of this line and the target on the other .",
    "the situation depicted here is * ls*.    consider now what happens when we turn the problem into non - * ls * by increasing @xmath97 beyond the phase boundary .",
    "as we increase the maximal concentration , the target odorant s curve penetrates to the  wrong \" side of the hyperplane @xmath126 . a picture of this situation is shown in fig .",
    "[ fig : nls](a ) .",
    "this is a non * ls * problem - which means that no matter how long we run our learning algorithm , we will never find a hyperplane @xmath126 that separates the target from all the background .",
    "if nevertheless we keep running our learning algorithm , the direction of our candidate for * w * will keep changing as we  learn \" ,",
    "but since the critical capacity curve of figure [ fig : alphac ] has been crossed , no amount of further learning will produce a separating plane .",
    "the density of points near the high concentration limit is much larger than for low concentrations .",
    "hence further learning will perhaps be able to separate the target from the background at high concentrations - but then separability breaks down at low concentrations ( see fig .",
    "[ fig : nls](b ) ) .",
    "in the olfactory bulb of most vertebrates , each secondary neuron ( mitral or tufted cell ) receives input from only one glomerulus , which in turn is innervated , in all likelihood , by axons stemming from olfactory epithelial sensory cells that all express the same olfactory receptor protein .",
    "thus , the grandmother cell modeled here may not simply represent a mitral or tufted cell . however , when the network of periglomerular and granule cells ( interneurons ) is taken into account , then it is fair to state that each mitral cell receives ( indirect ) input from a large number of different olfactory receptor types .",
    "thus , the present analysis may be relevant to the kind of neuronal processing that takes place in the first neuronal relay station of the olfactory pathway , the olfactory bulb .",
    "alternatively , it may represent , in abstract fashion , information processing that takes place both in the olfactory bulb and at higher olfactory central nervous system centers .",
    "previously , several studies have been published that analyze neuronal networks for the olfactory system @xcite @xcite@xcite @xcite @xcite @xcite . however , none of these was based on a quantitative model for the affinity relationships within the entire olfactory receptor repertoire . here , we use the receptor affinity distribution ( rad ) model , which was developed , based on general biochemical considerations , for receptor repertoires , including that of olfactory receptors .",
    "the power of this approach is in utilizing a global knowledge about the repertoire to analyze the fidelity of discrimination among odorants .",
    "it has been pointed out in the past , that the rad model may be used to analyze the signal to noise ratio in systems in which specific binding to a receptor has to be distinguished from the background of numerous other receptors which constitute `` non - specific binding '' @xcite @xcite . here",
    ", we apply a similar concept to an analysis of signal to noise discrimination in the case of a neuronal network whose input stems from a receptor repertoire .",
    "the results presented here suggest that for a fixed number of background odorants there is a maximal odorant concentration beyond which odorant discrimination becomes impossible .",
    "this is not surprising , since olfactory receptors are saturable , and at very high concentrations weak affinity receptors as well as high affinity ones will generate comparable signals .",
    "however , it is noteworthy that despite the fact that information capacity for odorant discrimination rapidly declines as odorant concentration goes up , the presently analyzed network is still capable of discrimination even at concentrations for which @xmath128 is of the order of a few hundred ( where @xmath129 is the average affinity ) .",
    "the model network consists of @xmath0 sensory neurons , each of which is characterized by a set of affinities to a number of odorants .",
    "when any particular odorant , @xmath14 , is present , sensory neuron @xmath21 produces a ( nonlinear ) response , @xmath130 .",
    "these responses constitute the inputs to a single processing unit ( secondary neuron ) , which performs weighted summation of all the @xmath0 inputs .",
    "the secondary neuron s output is the sign of this weighted sum .",
    "the aim of this single processing unit is to identify _ one single odorant _ separate it from all the others that may be sensed by the system .",
    "this secondary neuron plays the role of a `` grandmother cell '' for a particular target odorant .",
    "an assemply of @xmath131 such secondary neurons may constitute , together with the sensory neurons , a system that is able to clearly identify the presence of @xmath131 target odorants , from a background of @xmath70 odorants .",
    "we posed a well defined quantitative question : given that each odorant may appear with a concentration @xmath132 that lies in a certain range , @xmath133 , what is the maximal number of background odorants @xmath134 , from which a single target can be separated with probability 1 ?",
    "the answer is summarized in fig .",
    "5 , where @xmath35 , the critical capacity , is plotted vs. @xmath97 .",
    "the result is obtained in the limit of large @xmath0 ( i.e. many sensory neurons - in fact , for @xmath135 this result should already give excellent precision ) . for a dynamic range of @xmath136 of about 100",
    "we find @xmath137 .",
    "that is , for say @xmath138 sensory neurons we can distinguish the target from about 750 background odorants .",
    "hence if we assemble 750 odorants and appoint a grandmother cell for each , we will be able to identify them one by one .    in order to get this quantitative answer we had to generalize an old problem , of _ linear separability _ of @xmath70 points on an @xmath139 dimensional hypersphere , to the new problem of linerly separating @xmath70 _ curves _ that lie on the same hypershere .",
    "we have shown that in order to represent a curve by discrete points that lie on it , we have to place @xmath140 points on each curve .",
    "the results were obtained by a perceptron learning algorithm that signals when a problem is _ unlearnable _ , i.e. non - linearly - separable .",
    "the behavior of the phase boundary for large concentrations @xmath141 ( figure 5 ) is quite surprising since the network may be expected to enter a totally confused state due to the saturation of the nonlinear sensory neurons .",
    "this could be expected to lead instead to @xmath142that the cover result @xmath143 is recovered in the high concentration regime can be in fact be understood by the following argument .",
    "we first calculate the probability @xmath144 that a sensory unit gives a response @xmath145 to the presentation of an odorant in the range ( 1 ) , by @xmath146 where the average is taken over possible concentrations @xmath10 uniformly distributed in range ( 1 ) and according to the rad model , over the affinities , @xmath20 of equation ( 2 ) .",
    "@xmath147 is given by equation ( 6 ) .",
    "the integrals lead to @xmath148 where @xmath149 is the complementary error function this probability has one peak which sharpens and moves to higher values of @xmath145 as @xmath97 grows .",
    "however at the very ends of the interval , @xmath150or @xmath151 the probability is zero . that @xmath152 for every @xmath97 is the source of the surprise .",
    "the peak which concentrates all the probability , gets arbitrarily close to @xmath150 , as the concentration increases , but never makes it to the extreme of the interval .",
    "in fact @xmath153 .",
    "therefore the components of the vectors @xmath37 will be with overwhelming probability at the peak position , which can be written as @xmath154 with all @xmath155small but strictly positive .",
    "neglecting second order terms in @xmath156 the normalized patterns will then be : @xmath157 therefore the @xmath158 vectors are unbiasedly distributed around @xmath159 .",
    "we are taken back to the original cover - gardner problem of separating @xmath1 unbiased patterns with a hyperplane and the result @xmath160 is no longer a surprise .",
    "this argument does nt deal with the asymptotic behavior of the capacity in the presence of any kind of noise . in that case",
    "the naive expectations that @xmath161 for @xmath162 are probably borne out .",
    "* acknowledgements * we thank ido kanter for most useful discussions .",
    "the research of ed was supported by the germany - israel science foundation ( gif ) , the minerva foundation and the us - israel binational science foundation ( bsf ) .",
    "the work reported here was initiated during visits of nc to the weizmann institute , that were supported by grants from the so paulo society of friends of the weizmann institute and by the gorodesky foundation .",
    "jept s research was supported by a graduate fellowship of the fundao de amparo  pesquisa do estado de so paulo ( fapesp ) .",
    "nc received partial support from the conselho nacional de desenvolvimento cientfico e tecnolgico ( cnpq )    d. lancet , e. sadovsky and e. seidman , proc .",
    "usa , * 90 * 3715 ( 1993 ) l schlfli , theorie der vielfachen kontinuitt , gesammelte matematische abhandlungen , ed .",
    "steiner - schlfli - komittee basel , birkhuser p171 ( 1852 ) _ apud _",
    "w kinzel _ phil . mag .",
    "_ * b 77 * , 1455 ( 1998 ) e gardner , j. phys .",
    "a * 21 * , 257 ( 1988 ) e. gardner and b. derrida , j. phys . a * 21 * , 271 ( 1988 ) d. nabutovsky and e. domany , neural computation , * 3 * , 604 ( 1991 ) e.g. v. privman , ed .",
    "_ finite - size scaling and numerical simulations of statistical systems _ ,",
    "world scientific , singapore , 1990 w. nadler and w. fink , phys .",
    "* 78 * , 555 ( 1997 ) f. rosenblatt , _ principles of neurodynamics _ , spartan books , new york ( 1962 ) l. f. abbott and t. b. kepler j. phys .",
    "a. * 22 * , l711 ( 1989 ) t. cover , ieee tran .",
    "comput , * 14 * , 326 ( 1965 ) minsky and papert _ perceptrons _ mit press , cambridge , ma ( 1969 ) d. lancet , a horovitz and e katchalski - katzir molecular recognition in biology : models for analysis of protein - ligand interactions .",
    "behr , j.p . , ed . , john wiley and sons ltd .",
    "25 - 71 ( 1994 ) w. freeman , http://sulcus.berkeley.edu/flm/ms/wjfmm.html j. hopfield _ proceedings of the nat .",
    "* 88 * 6462 ( 1991 ) m. a. wilson and j. m. bower _",
    "j. neurophysiol _ * 67 * 981 ( 1992 ) z. li an j. hopfield _ biol",
    ". cybern _ * 61 * 379 ( 1989 ) z. li _ biol . cybern _ * 62 * 349 ( 1990 ) and modeling the sensory computations of the olfactory bulb published in models of neural networks vol . 2 , eds .",
    "j. l. van hemmen , e. domany , and k. schulten , springer - verlag new york , ( 1995 ) z. li and j. hertz _ network : computation in neural systems _ * 11 . * 83 ( 2000 )"
  ],
  "abstract_text": [
    "<S> we introduce and study an artificial neural network , inspired by the probabilistic receptor affinity distribution model of olfaction . </S>",
    "<S> our system consists on @xmath0 sensory neurons whose outputs converge on a single processing linear threshold element . </S>",
    "<S> the system s aim is to model discrimination of a single target odorant from a large number @xmath1 of background odorants , within a range of odorant concentrations . </S>",
    "<S> we show that this is possible provided @xmath1 does not exceed a critical value @xmath2 , and calculate the critical capacity @xmath3 . </S>",
    "<S> the critical capacity depends on the range of concentrations in which the discrimination is to be accomplished . </S>",
    "<S> if the olfactory bulb may be thought of as a collection of such processing elements , each responsible for the discrimination of a single odorant , our study provides a quantitative analysis of the potential computational properties of the olfactory bulb . </S>",
    "<S> the mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves , embedded in a large dimensional space . </S>",
    "<S> this is accomplished here by a numerical study , using a method that signals whether the discrimination task is realizable or not , together with a finite size scaling analysis . </S>"
  ]
}