{
  "article_text": [
    "recently , much attention has been drawn to the problem of recovering a target matrix from a small set of linear measurements .",
    "the estimated matrix is a superposition of low - complexity structure .",
    "it can be found in many different fields , such as medical imaging @xcite , seismology @xcite , information retrieval @xcite and machine learning @xcite .",
    "this problem regained great attention after the publication of the pioneering works of e.j .",
    "cands et al @xcite .",
    "according to paper @xcite , we can build the data model as follows : there exists a large - scale data matrix @xmath0 , where @xmath1 has low - rank , and @xmath2 is sparse component . the main question is how to recover a low - rank matrix @xmath3 and sparse matrix @xmath2 from a small set of linear measurements . in the paper @xcite , e.j .",
    "cands et.al proved that most low - rank matrices and the sparse components can be recovered , provided that the rank of the low - rank component is not too large , and the sparse component is reasonably sparse ; and more importantly they proved that it can be done by solving a simple convex optimization problem , i.e. most matrices of low - rank and the sparse components can be perfectly recovered by solving the optimization problem @xmath4 provided that the rank of the matrix @xmath5 and the cardinality of the sparse component @xmath6 obey @xmath7 where @xmath8 and @xmath9 are positive numerical constants .    in practice",
    ", it is necessary to develop efficient and effective tools to process , analyze , and extract useful information from such high dimensional data ( in application , dimensional of data is always very high ) .",
    "because strongly convex optimizations have many advantages , such as optimal solution is unique , many scholars suggest solving their strongly convex approximations , see , e.g. , @xcite , instead of directly solving the original convex optimizations . pertaining to problem ( 1 )",
    ", the authors gave the suitable sufficient conditions under which the strongly convex models lead to the exact low - rank and sparse matrix recovery .",
    "some suggestions were given on how to choose suitable parameters in practical algorithms in the paper @xcite .",
    "however , the results of paper @xcite are limited in a special case , i.e. @xmath10 . in this paper",
    ", we extend this result to the principal component pursuit with reduced linear measurements , i.e. @xmath11 is a @xmath12-dimensional random subspace .",
    "it s easy to note that results of paper @xcite is only a special case of ours .      in this subsection",
    ", we will interpret an important strongly convex programming which will be addressed in this paper and list its existence and uniqueness theorems . in the paper @xcite ,",
    "the authors have studied principal component pursuit with reduced linear measurements and given sufficient conditions under which @xmath3 and @xmath2 can be perfectly recovered .",
    "@xmath13    in this paper , we address a strongly convex programming .",
    "we prove it can guarantee exact low - rank matrix recovery .",
    "the proposed optimization is @xmath14 where @xmath15 is some positive penalty parameter and @xmath16 is the orthogonal projection onto the linear subspace @xmath17 .",
    "we also assume @xmath11 is a random subspace(the same assumption considered in paper @xcite ) . when @xmath18 in ( 2 ) , existence and uniqueness theorems is provided in the paper @xcite , as we list them below . in the end , how to choose suitable parameters in the optimization model ( 2 ) is discussed .",
    "@xcite [ t1 ] _ fix any @xmath19 , and let @xmath11 be a @xmath12-dimensional random subspace of @xmath20 ; @xmath3 obeys incoherence condition with parameter @xmath21 , and @xmath22 . then with high probability , the solution of problem(2 ) with @xmath23 is exact , i.e. @xmath24 and @xmath25 , provided that _",
    "@xmath26 _ where , @xmath27 , @xmath28 and @xmath29 are positive numerical constants and @xmath30 . _      we provide a brief summary of the notations which are used throughout the paper .",
    "we denote the operator norm of matrix by @xmath31 , the frobenius norm by @xmath32 , the nuclear norm by @xmath33 , and the dual norm of @xmath34 by @xmath35 . the euclidean inner product between two matrices is defined by the formula @xmath36 .",
    "note that @xmath37 .",
    "the cauchy - schwarz inequality gives @xmath38 , and it is well known that we also have @xmath39 , e.g.@xcite @xcite .",
    "linear transformations which act on the space of matrices are denoted by @xmath40 .",
    "it s easy to see that the operator of @xmath41 is high dimension matrix in substance .",
    "the operator norm of the operator is denoted by @xmath42 .",
    "it should be noted that @xmath43 .    the rest of the paper is organized as follows . in section 2",
    ", we list many important lemmas and prove a key lemma on which our main result depends .",
    "suggestions then is given in section 3 , which will guide us to choose suitable parameters in practical algorithms .",
    "conclusion and further works are discussed in section 4 .",
    "in this section , we first list some useful lemmas which will be used throughout this paper and then prove a main lemma . although the main lemma is similar to the corresponding one in the paper @xcite",
    ", the construction of @xmath44 is different .",
    "that leads to our necessary additional work .",
    "[ @xcite , lemma 1][l1 ] _ suppose that @xmath45 .",
    "let @xmath46 so that @xmath47 .",
    "assume that @xmath48 and @xmath49 .",
    "then , @xmath50 is the unique optimal solution to ( 2 ) if there exists a pair @xmath51 satisfying _ @xmath52 _ with @xmath53 , and @xmath54_.    [ @xcite , lemma 3][l2 ] _ assume that @xmath55 for some small @xmath56 and the other conditions of theorem 1.2 hold true .",
    "then , the matrix @xmath57 obeys , with high probability . + _",
    "@xmath58 + @xmath59 + @xmath60    [ @xcite , lemma 4][l3 ] _ in addition to the assumptions in the previous lemma , assume that the signs of the non - zero entries of @xmath2 are i.i.d .",
    "then , the matrix @xmath61 obeys , with high probability , + _ @xmath62 + @xmath63    the construction of @xmath57 and @xmath61 can be found in the paper @xcite .",
    "the authors also introduce a new scheme to construct @xmath44 for the principal component pursuit .",
    "however , the matrix @xmath44 constructed in the paper @xcite do not satisfy the requirement of our problem , so we have to modify this construction .",
    "we first give explicit construction of @xmath44 , and then , prove the modification of @xmath44 satisfies the corresponding property .",
    "_ construction of @xmath44 with least modification .",
    "_ we define @xmath44 by the following least squares problem : @xmath64 where @xmath65 .",
    "this construction of @xmath44 do nt satisfy theorem 2.6 only , but also has below lemma .",
    "[ l4 ] _ assume @xmath66 , and that @xmath55 for some small @xmath56 and the assumptions of theorem 1.2 hold true",
    ". then , the matrix @xmath44 obeys , with high probability .",
    "+ _ @xmath67 + @xmath68    in proof of lemma 2.4 , we have to use two important lemmas which are listed below .",
    "[ @xcite lemma 11 ] [ l5 ] let @xmath69 and @xmath70 be any three linear subspaces in @xmath20 satisfying @xmath71 , and @xmath72 and @xmath73 .",
    "we define @xmath74 . then , we have @xmath75    [ @xcite lemma 7 ] assume that @xmath76 .",
    "let @xmath11 be a linear subspace distributed according to the random subspace model .",
    "then , with high probability , we have @xmath77    * a , bounding the behavior of @xmath78*. for convenience , let @xmath79 . + according to triangle inequality , we have @xmath80 in the last equality , we have used @xmath81 .",
    "note that @xmath82 according to the derivation in the paper @xcite , with high probability , we can obtain @xmath83 putting those all together , we get @xmath84 combining with @xmath66 , we can obtain @xmath85 because @xmath44 is the optimum solution of least squares problem , we can use the convergent neumann series expansion .",
    "it s easy to note that @xmath86 according to triangle inequality , we have @xmath87 * b , estimating the first inequality of lemma 2.4*. in order to bound @xmath88 , we have to bound the behavior of @xmath89 .",
    "therefore , we have @xmath90 according to lemma 2.5 , we have , for any @xmath91 , with high probability , @xmath92 according to the paper@xcite , we have @xmath93 with high probability .    next",
    ", we will bound @xmath94 . according to the paper @xcite",
    ", @xmath95 has the same distribution as @xmath96 , where @xmath97 is a random gaussian matrix with i.i.d .",
    "entries @xmath98 .",
    "therefore , we can obtain @xmath99 together with lemma 2.6 , we can obtain @xmath100\\le e^{-\\frac{n^2}{32}}\\nonumber\\end{aligned}\\ ] ] it s easy to note that any entries of @xmath101 have the same distribution as @xmath102 , where @xmath103 are independent identically distributed .",
    "it is obvious to see that @xmath104 and @xmath105 therefore , @xmath102 is distributed according to @xmath106 , where @xmath79 . for simplicity , we define @xmath107 . using the jesen inequality , we have @xmath108\\le ( \\mathbb e[\\|z\\|_2 ^ 2])^{1/2}=\\sqrt{\\frac{p\\xi}{n^2}}\\nonumber\\end{aligned}\\ ] ] according to the proposition 2.18 in @xcite",
    ", we can obtain @xmath109 + t\\sqrt{\\frac{\\xi}{n^2}}\\right ] \\le e^{-t^2/2}\\nonumber\\end{aligned}\\ ] ] setting @xmath110 , after a simple calculation , we can obtain @xmath111 with high probability . for sufficiently large @xmath112 , the first inequality of lemma 2.4 is established . +",
    "* c , estimating the second inequality of lemma 2.4 * , note that @xmath113 similar to the paper @xcite , after a simple calculation , we can obtain @xmath114 where @xmath115 is some constant .",
    "note that for sufficiently large @xmath112 , the second inequality of lemma 2.4 is established .",
    "in this section , we shall provide sufficient conditions under which @xmath117 is the unique solution of the strongly convex programming ( 2 ) with high probability .",
    "afterwards , an explicit lower bound of @xmath116 will be given as well , which will guide us to choose suitable parameters in practical algorithms .",
    "[ result1 ] _ suppose that @xmath45 .",
    "let @xmath46 so that @xmath47 .",
    "assume that @xmath48 and @xmath49 .",
    "if there exists a pair @xmath51 and a matrix @xmath118 satisfying _",
    "@xmath119 _ with _ @xmath120 _ where @xmath121 , @xmath122 are positive parameters satisfying _ @xmath123",
    "_ then @xmath50 is the unique solution of the strongly convex programming ( 2)_.    for any feasible perturbation @xmath124 , it s easy to note that @xmath125 .",
    "according to the definition of @xmath126 , we have @xmath127 , therefore @xmath128 . for simplicity ,",
    "let @xmath129 , we can obtain @xmath130 in the second inequality above , we have used the facts @xmath131 in the third inequality above , we have used @xmath125 .",
    "+ we will bound @xmath132 . according to the definition of @xmath126 ,",
    "we have @xmath133 therefore @xmath134 putting those all together , we get @xmath135 this , together with ( 6 ) , implies that @xmath136 is a solution to ( 2 ) .",
    "the uniqueness follows from the strong convexity of the objective in ( 2 )",
    ".    we will provide the criterion of the value of @xmath116 .",
    "[ result1 ] _ let @xmath137 , @xmath138 , and @xmath139 .",
    "@xmath140 then , under the other assumptions of theorem 1.1 , @xmath50 is the unique solution to the strongly convex programming ( 2 ) with high probability .    in order to check the conditions in theorem 3.1 , we will prove there exists a matrix @xmath141 obeying @xmath142 note that @xmath143 with @xmath57 , @xmath61 and @xmath44 have analytical form constructed in the paper @xcite .",
    "we will check above conditions hold true one by one . for simplicity of proof",
    ", we denote @xmath144 without loss of generality , let @xmath145 . with the help of the construction @xcite of @xmath57 , @xmath61 and @xmath44 ,",
    "it is easy to check the first and second conditions hold true . with respect to the third condition , according to the paper @xcite , we have @xmath146 and @xmath147 . according to the modification of @xmath44 constructed in lemma 2.4",
    ", we have @xmath148 .",
    "it s easy to check that @xmath149 , which implies that the third condition holds true .",
    "consequently , we will provide the last two conditions also hold true under some suitable assumptions . pertaining to the fourth inequality",
    ", we have @xmath150 for the last inequality , noting that @xmath151 and @xmath152 as shown in @xcite , we can obtain @xmath153 in order to satisfy the condition ( 8) , we choose a @xmath116 obeying @xmath154 therefore @xmath155 combining ( 9 ) with ( 6 ) , we can obtain @xmath156 therefore @xmath157 together with ( 10 ) and ( 11 ) , the theorem 3.2 is established .    in order to simplify the formula ( 7 ) ,",
    "we suppose @xmath158 and @xmath159 , which satisfy the conditions above . therefore @xmath160 however , note that the exact lower bound is very hard to get , because we only have the information about the given data matrix @xmath161 . noting that @xmath162 and according to the paper @xcite , we have @xmath163 therefore , we can choose @xmath164 it s obvious that @xmath165 .",
    "therefore , we can obtain the result as follows .",
    "[ tau2 ] _  assuming _",
    "@xmath166 _ and the other assumptions of theorem 1.1 , @xmath50 is the unique solution to the strongly convex programming ( 2 ) with high probability . _",
    "in this paper , we have studied strongly convex programming for principal component pursuit with reduced linear measurements .",
    "we first provide sufficient conditions under which the strongly convex models lead to the exact low rank and sparse components recovery ; second , we give the criterion of the choice of the value of @xmath116 , which gives very useful advice on how to set the suitable parameters in designing efficient algorithms .",
    "especially , it is easy to note that the main results of paper @xcite is only the special case of ours . in some sense",
    ", we extend the result of choosing suitable parameters to the general problem .",
    "we would like to thank the reviewers very much for their valuable comments and suggestions .",
    "this research was supported by the national natural science foundation of china ( nsfc ) under grant 61172140 , and 985 key projects for excellent teaching team supporting ( postgraduate ) under grant a1098522 - 02 .",
    "yipeng liu is supported by fwo phd / postdoc grant : g0108.11(compressed sensing ) ."
  ],
  "abstract_text": [
    "<S> in this paper , we address strongly convex programming for principal component pursuit with reduced linear measurements , which decomposes a superposition of a low - rank matrix and a sparse matrix from a small set of linear measurements . </S>",
    "<S> we first provide sufficient conditions under which the strongly convex models lead to the exact low - rank and sparse matrix recovery ; second , we also give suggestions on how to choose suitable parameters in practical algorithms .    </S>",
    "<S> qingshan you and qun wan    yipeng liu    ( communicated by the associate editor name ) </S>"
  ]
}