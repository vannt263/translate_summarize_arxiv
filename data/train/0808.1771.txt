{
  "article_text": [
    "the general theme of `` scaling up for high dimensional data and high speed data streams '' is among the `` ten challenging problems in data mining research''@xcite .",
    "this paper focuses on a very efficient algorithm for estimating the entropy of data streams using a recently developed randomized algorithm called * _ compressed counting ( cc ) _ * by li @xcite .",
    "the underlying technique of cc is _ maximally - skewed stable random projections_. our experiments on real web crawl data demonstrate that cc can approximate entropy with very high accuracy . in particular , cc ( dramatically ) improves _ symmetric stable random projections _ ( indyk @xcite and li @xcite ) for estimating entropy , under the _ relaxed strict - turnstile _ model .      while traditional machine learning and",
    "mining algorithms often assume static data , in reality , data are often constantly updated .",
    "mining data streams@xcite in ( e.g. , ) 100 tb scale databases has become an important area of research , as network data can easily reach that scale@xcite .",
    "search engines are a typical source of data streams ( babcock _ et.al .",
    "_ @xcite ) .",
    "we consider the _ turnstile _ stream model ( muthukrishnan @xcite ) .",
    "the input stream @xmath9 , @xmath10 $ ] arriving sequentially describes the underlying signal @xmath11 , meaning @xmath12 = a_{t-1}[i_t ] + i_t,\\end{aligned}\\ ] ] where the increment @xmath13 can be either positive ( insertion ) or negative ( deletion ) .",
    "for example , in an online bookstore , @xmath14 $ ] may record the total number of books that user @xmath15 has ordered up to time @xmath16 and @xmath13 denotes the number of books that this user orders ( @xmath17 ) or cancels ( @xmath18 ) at @xmath19 .",
    "it is often reasonable to assume @xmath20\\geq 0 $ ] , although @xmath13 may be either negative or positive . restricting @xmath20\\geq 0 $ ] results in the _ strict - turnstile",
    "_ model , which suffices for describing almost all natural phenomena .",
    "for example , in an online store , it is not possible to cancel orders that do not exist .    *",
    "compressed counting ( cc ) * assumes a _ relaxed strict - turnstile _",
    "model by only enforcing @xmath20\\geq0 $ ] at the @xmath19 one cares about . at other times",
    "@xmath21 , cc allows @xmath22 $ ] to be arbitrary .",
    "this is more general than the _ strict - turnstile _ model .",
    "the @xmath0th frequency moment is a fundamental statistic : @xmath23^\\alpha .",
    "\\end{aligned}\\ ] ] when @xmath24 , @xmath25 is the sum of the stream .",
    "it is obvious that one can compute @xmath25 exactly and trivially using a simple counter , because @xmath26 = \\sum_{s=0}^t i_s$ ] .",
    "@xmath27 is basically a histogram and we can view @xmath28}{\\sum_{i=1}^d a_t[i]}$ ] as probabilities .",
    "an extremely useful ( especially in web and networks@xcite ) summary statistic is the shannon entropy : @xmath29}{f_{(1)}}\\log \\frac{a_t[i]}{f_{(1 ) } } , \\hspace{0.2 in } \\text{where } \\ \\",
    "f_{(1 ) } = \\sum_{i=1}^d a_t[i].\\end{aligned}\\ ] ]    various generalizations of the shannon entropy exist .",
    "the rnyi entropy@xcite , denoted by @xmath30 , is defined as @xmath31^\\alpha}{\\left(\\sum_{i=1}^d a_t[i]\\right)^\\alpha } = \\frac{1}{1-\\alpha } \\log \\frac{f_{(\\alpha)}}{f_{(1)}^\\alpha}.\\end{aligned}\\ ] ] the tsallis entropy@xcite , denoted by @xmath32 , is defined as , @xmath33 which was first introduced by havrda and charvt@xcite and later popularized by tsallis@xcite .",
    "it is easy to verify that , as @xmath2 , both the rnyi entropy and tsallis entropy converge to the shannon entropy .",
    "thus @xmath34 in the limit sense .",
    "for this fact , one can also consult http://en.wikipedia.org/wiki/renyi_entropy .    therefore",
    ", both the rnyi entropy and tsallis entropy can be computed from the @xmath0th frequency moment ; and one can approximate the shannon entropy from either @xmath30 or @xmath32 by using @xmath6 .",
    "in fact , several studies ( zhao _ et.al . _ @xcite and harvey _ et.al . _",
    "@xcite ) have used this idea to approximate the shannon entropy .",
    "we should mention that @xcite proposed estimating the logarithmic moment , @xmath35 $ ] , using @xmath36 with @xmath37 .",
    "their idea is very similar to that in estimating entropy .      because the elements , @xmath20 $ ] , are time - varying",
    ", a nave counting mechanism requires a system of @xmath38 counters to compute @xmath36 exactly ( unless @xmath39 ) .",
    "this is not always realistic when @xmath38 is large and the data are frequently updated at very high rate . for example , if @xmath20 $ ] records activities for each user @xmath15 , identified by his / her ip address , then potentially @xmath40 ( possibly much larger in the near future ) .",
    "due to the huge volume , streaming data are often not ( fully ) stored , even on disks@xcite .",
    "one common strategy is to store only a small `` sample '' the data ; and sampling has become an important topic in web search and data streams@xcite . while some modern databases ( e.g. , yahoo ! s 2-petabyte database ) and government agencies do store the whole data history , the data analysis often has to be conducted on a ( hopefully ) representative small sample of the data .",
    "as it is well - understood that general - purpose simple sampling - based methods often can not give reliable approximation guarantees@xcite , developing special - purpose ( and one - pass ) sampling / sketching techniques in streaming data has become an active area of research .",
    "pioneered by alon _",
    "et.al._@xcite , the problem of approximating @xmath36 in data streams has been heavily studied@xcite .",
    "the method of _ symmetric stable random projections _",
    "( indyk @xcite , li @xcite ) is regarded to be practical and accurate .",
    "we have mentioned that computing the first moment @xmath25 in _ strict - turnstile _",
    "model is trivial using a simple counter .",
    "one might naturally speculate that when @xmath6 , computing ( approximating ) @xmath36 should be also easy .",
    "however , none of the previous algorithms including _ symmetric stable random projections _ could capture this intuition .",
    "for example , figure [ fig_comp_var_factor ] in section [ sec_cc ] shows that the performance of _ symmetric stable random projections _ is roughly the same for @xmath39 and @xmath6 , even though @xmath39 should be trivial .    * * compressed counting ( cc)**@xcite was recently proposed to overcome the drawback of previous algorithms at @xmath6 .",
    "cc improves _",
    "symmetric stable random projections _",
    "uniformly for all @xmath41 and the improvement is in a sense `` infinite '' when @xmath2 as shown in figure [ fig_comp_var_factor ] in section [ sec_cc ] .",
    "however , no empirical studies on cc have been reported .",
    "et.al._@xcite applied _ symmetric stable random projections _ to approximate the shannon entropy .",
    "@xcite cited @xcite , as one application of _ compressed counting _ ( cc ) . a nice theoretical paper in focs08 by harvey _",
    "et.al._@xcite provided the criterion to choose the @xmath0 so that the shannon entropy can be approximated with a guaranteed accuracy , using the @xmath0th frequency moment .",
    "@xcite cited both _",
    "symmetric stable random projections_@xcite and _ compressed counting_@xcite .",
    "there are other methods for estimating entropy , e.g. , @xcite , which we do not compare with in this study .",
    "our main contribution is the first empirical study of compressed counting for estimating entropy .",
    "some theoretical analysis is also conducted .",
    "* we apply compressed counting ( cc ) to compute the rnyi entropy , the tsallis entropy , and the shannon entropy , on real web crawl data . *",
    "we empirically compare cc with _",
    "symmetric stable random projections _ and demonstrate the huge improvement .",
    "thus , our work helps establish cc as a promising practical tool in data stream computations .",
    "* we provide some theoretical analysis for approximating entropy , for example , the variance - bias trade - off . *",
    "our empirical work leads to practical recommendations for various estimators developed in @xcite .    for estimating the shannon entropy , the theoretical work by harvey _",
    "et.al._@xcite used _ symmetric stable random projections _ or cc as a subroutine ( a two - stage `` black - box '' approach ) .",
    "that is , they first determined at what @xmath3 value , @xmath30 ( or @xmath32 ) is close to @xmath42 within a required accuracy .",
    "then they used this chosen @xmath0th frequency moment to approximate the shannon entropy , independent of whether the frequency moments are estimated using cc or _",
    "symmetric stable random projections_.    in comparisons , we demonstrate that estimating shannon entropy is a variance - bias trade - off ; and hence the performance is highly coupled with the underlying estimators .",
    "the two - stage `` black - box '' approach @xcite may have some theoretical advantage ( e.g. , simplifying the analysis ) , while our variance - bias analysis directly reflects the real - world situation and leads to practical recommendations .",
    "* @xcite let @xmath8 and provided the procedures to compute @xmath43 ( or a series of @xmath43 s ) .",
    "if one actually carries out the calculation , their @xmath4 is very small ( like @xmath44 or smaller ) . consequently their theoretically calculated sample size may be ( impractically ) large , especially when using _ symmetric stable random projections_. * in comparison , we provide a practical recommendation for estimating shannon entropy : using cc with @xmath45 and the _ optimal quantile _ estimator .",
    "only a small sample ( e.g. , 20 ) can achieve a high accuracy ( e.g. , @xmath7 relative errors ) .",
    "* we demonstrate that due to the variance - bias trade - off , there will be an `` optimal '' @xmath0 value that could attain the best mean square errors for estimating the shannon entropy .",
    "this optimal @xmath0 can be quite away from 1 when using _ symmetric stable random projections_.      section [ sec_entropy ] reviews some applications of entropy .",
    "the basic methodologies of cc and various estimators for recovering the @xmath0th frequency moments are reviewed in section [ sec_cc ] .",
    "we analyze in section [ sec_entropy_est ] the biases and variances in estimating entropies .",
    "experiments on real web crawl data are presented in section [ sec_exp ] .",
    "finally , section [ sec_conclusion ] concludes the paper .",
    "the shannon entropy , @xmath42 defined in ( [ eqn_shannon ] ) , is a fundamental measure of randomness .",
    "a recent paper in wsdm08 ( mei and church @xcite ) was devoted to estimating the shannon entropy of msn search logs , to help answer some basic problems in web search , such as , _ how big is the web ? _",
    "the search logs can be naturally viewed as data streams , although @xcite only analyzed several `` snapshots '' of a sample of msn search logs .",
    "the sample used in @xcite contained 10 million < query , url , ip > triples ; each triple corresponded to a click from a particular ip address on a particular url for a particular query .",
    "@xcite drew their important conclusions on this ( hopefully ) representative sample .",
    "we believe one can ( quite easily ) apply compressed counting ( cc ) on the same task , on the whole history of msn ( or other search engines ) search logs instead of a ( static ) sample .",
    "+ using the shannon entropy as an important `` feature '' for mining anomalies is a widely used technique ( e.g. , @xcite ) . in imc07 , zhao _",
    "et.al._@xcite applied _ symmetric stable random projections _ to estimate the shannon entropy for all origin - destination ( od ) flows in network measurement , for clustering traffic and detecting traffic anomalies .    detecting anomaly events in real - time ( ddos attacks , network failures , etc . )",
    "is highly beneficial in monitoring network performance degradation and service disruptions .",
    "et.al._@xcite hoped to capture those events in real - time by examining the entropy of every od flow .",
    "they resorted to approximate algorithms because measuring the shannon entropy in real - time is not possible on high - speed links due to its memory requirements and high computational cost .",
    "+      the rnyi entropy , @xmath30 defined in ( [ eqn_renyi ] ) , is a generalization of the classical shannon entropy @xmath42 .",
    "@xmath30 is function of the frequency moment @xmath36 and approaches @xmath42 as @xmath46 .",
    "thus it is natural to use @xmath30 with @xmath6 to approximate @xmath42 .",
    "the rnyi entropy has other applications .",
    "it is a diversity index in ecology @xcite .",
    "it is used for analyzing expander graphs@xcite and other applications , e.g. @xcite .",
    "the tsallis entropy , @xmath47 defined in ( [ eqn_tsallis ] ) , is another generalization of the shannon entropy @xmath42 . since @xmath48 as @xmath2 ,",
    "the tsallis entropy provides another algorithm for approximating the shannon entropy .",
    "the tsallis entropy is widely used in statistical physics and mechanics .",
    "interested readers may consult the link + www.cscs.umich.edu/~crshalizi/notabene/tsallis.html .",
    "compressed counting ( cc ) assumes the _ relaxed strict - turnstile _ data stream model .",
    "its underlying technique is based on _ maximally - skewed stable random projections_.      a random variable @xmath49 follows a maximally - skewed @xmath0-stable distribution if the fourier transform of its density is@xcite @xmath50 where @xmath1 , @xmath51 , and @xmath52 .",
    "we denote @xmath53 . the skewness parameter @xmath54 for general stable distributions ranges in @xmath55 $ ] ; but cc uses @xmath52 , i.e. , maximally - skewed .",
    "previously , the method of _ symmetric stable random projections_@xcite used @xmath56 .    consider two independent variables , @xmath57 . for",
    "any non - negative constants @xmath58 and @xmath59 , the `` @xmath0-stability '' follows from properties of fourier transforms : @xmath60    note that if @xmath56 , then the above stability holds for any constants @xmath58 and @xmath59 .",
    "this is why _ symmetric stable random projections_@xcite can work on general data but cc only works on non - negative data ( i.e. , _ relaxed strict - turnstile model _ ) .",
    "since we are interested in the entropy , the non - negativity constraint is natural , because the probability should be non - negative .",
    "conceptually , one can generate a matrix @xmath61 and multiply it with the data stream @xmath27 , i.e. , @xmath62 .",
    "the resultant vector @xmath63 is only of length @xmath64 .",
    "the entries of @xmath65 , @xmath66 , are i.i.d .",
    "samples of a stable distribution @xmath67 .    by property of fourier transforms ,",
    "the entries of @xmath63 , @xmath68 @xmath69 to @xmath64 , are i.i.d .",
    "samples of a stable distribution @xmath70_j = \\sum_{i=1}^d r_{ij } a_t[i]\\\\   \\sim & s\\left(\\alpha,\\beta=1,f_{(\\alpha ) } = \\sum_{i=1}^d a_t[i]^\\alpha\\right),\\end{aligned}\\ ] ] whose scale parameter @xmath36 is exactly the @xmath0th frequency moment of @xmath27 .",
    "therefore , cc boils down to a statistical estimation problem .",
    "if we can estimate the scale parameter from @xmath64 samples , we can then estimate the frequency moments and entropies .    for real implementations , one should conduct @xmath71 incrementally .",
    "this is possible because the _ turnstile _ model ( [ eqn_turnstile ] ) is a linear updating model .",
    "that is , for every incoming @xmath9 , we update @xmath72 for @xmath69 to @xmath64",
    ". entries of @xmath65 are generated on - demand as necessary .",
    "ganguly and cormode @xcite commented that , when @xmath64 is large , generating entries of @xmath65 on - demand and multiplications @xmath73 , @xmath69 to @xmath64 , can be prohibitive when data arrive at very high rate .",
    "this can be a drawback of _ stable random projections_. an easy `` fix '' is to use @xmath64 as small as possible .    at the same @xmath64 , all procedures of cc and _ symmetric stable random projections _ are the same except the entries in @xmath65 follow different distributions .",
    "thus , both methods have the same efficiency in processing time at the same @xmath64 .",
    "however , since cc is much more accurate especially when @xmath6 , it requires a much smaller @xmath64 for reaching a specified level of accuracy .",
    "for example , while using _",
    "symmetric stable random projections _ with @xmath74 is prohibitive , using cc with @xmath75 only may be practically feasible .",
    "therefore , cc in a sense naturally provides a solution to the problem of processing efficiency .      in this study",
    ", we consider three estimators from @xcite , which are promising for good performance near @xmath39 .",
    "recall cc boils down to estimating the scale parameter @xmath36 from @xmath64 i.i.d .",
    "samples @xmath76 .",
    "@xmath77    @xmath78^k , \\\\\\notag & \\kappa(\\alpha ) = \\alpha , \\ \\ \\   \\",
    "\\text{if } \\   \\",
    "\\alpha<1 , \\    \\",
    "\\kappa(\\alpha ) = 2-\\alpha \\   \\ \\text{if } \\",
    "\\ \\alpha>1.\\end{aligned}\\ ] ]    this estimator is strictly unbiased , i.e. , @xmath79 , and its asymptotic ( i.e. , as @xmath80 ) variance is @xmath81 as @xmath2 , the asymptotic variance approaches zero .",
    "the geometric mean estimator is important for theoretical analysis .",
    "for example , @xcite showed that when @xmath82 ( i.e. , @xmath83 ) , the `` constant '' @xmath84 in its sample complexity bound @xmath85 approaches @xmath86 at the rate of @xmath87 .",
    "that is , as @xmath2 , the complexity becomes @xmath88 instead of @xmath89 .",
    "note that @xmath89 is the well - known large - deviation bound for _ symmetric stable random projections_. the sample complexity bound determines the sample size @xmath64 needed for achieving a relative accuracy within a @xmath90 factor of the truth .",
    "in many theory papers , the `` constants '' in tail bounds are often ignored .",
    "the geometric mean estimator for cc demonstrates that in special cases the `` constants '' may be so small that they should not be treated as `` constants '' any more .",
    "@xmath91    which is asymptotically unbiased and has variance @xmath92    @xmath93 is defined only for @xmath94 and is considerably more accurate than the geometric mean estimator @xmath95 .",
    "@xmath96    where @xmath97    to compute @xmath98 , one sorts @xmath99 , @xmath100 to @xmath64 and uses the @xmath101th smallest , i.e. , @xmath102 .",
    "@xmath101 is chosen to minimize the asymptotic variance .",
    "@xcite provides the values for @xmath101 , @xmath103 , as well as the asymptotic variances . for convenience ,",
    "we tabulate the values for @xmath104 $ ] in table [ tab_oq ] .",
    "the last column contains the asymptotic variances ( with @xmath105 ) without the @xmath106 factor .    .",
    "[ cols=\"<,<,<,<\",options=\"header \" , ]     [ tab_oq ]    compared with the geometric mean and harmonic mean estimators , @xmath95 and @xmath93 , the optimal quantile estimator @xmath98 has some noticeable advantages :    * when the sample size @xmath64 is not too small ( e.g. , @xmath107 ) , @xmath98 is more accurate then @xmath95 , especially for @xmath108 .",
    "it is also more accurate than @xmath93 , when @xmath0 is close to 1 .",
    "our experiments will verify this point .",
    "* @xmath98 is computationally more efficient because both @xmath95 and @xmath93 require @xmath64 fractional power operations , which are expensive .",
    "the drawbacks of the optimal quantile estimator are :    * for small samples ( e.g. , @xmath109 ) , @xmath98 exhibits bad behaviors when @xmath108 . * its theoretical analysis , e.g. ,",
    "variances and tail bounds , is based on the density function of skewed stable distributions , which do not have closed - forms . *",
    "the parameters , @xmath101 and @xmath103 , are obtained from the numerically - computed density functions . @xcite",
    "provided @xmath101 and @xmath103 values for @xmath110 and @xmath111 .      for _ symmetric stable random projections _ , the following geometric mean estimator is close to be statistically optimal when @xmath6 @xcite : @xmath112^k}\\\\\\label{eqn_f_gm_sym_var } & \\text{var}\\left(\\hat{f}_{(\\alpha),gm , sym}\\right ) \\frac{f_{(\\alpha)}^2}{k}\\frac{\\pi^2}{12}\\left(2+\\alpha^2\\right)+o\\left(\\frac{1}{k^2}\\right).\\end{aligned}\\ ] ] where @xmath113 .",
    "therefore , we only compare cc with this estimator , which was explicitly used in @xcite for the task of residual moment estimation for the general turnstile model .",
    "figure [ fig_comp_var_factor ] compares the variances of the three estimators for cc , as well as the geometric mean estimator for _ symmetric stable random projections_.      the standard procedure for sampling from skewed stable distributions is based on the chambers - mallows - stuck method@xcite .",
    "one first generates an exponential random variable with mean 1 , @xmath114 , and a uniform random variable @xmath115 , then , @xmath116^{1/\\alpha } } \\left[\\frac{\\cos\\left ( u - \\alpha(u + \\rho)\\right)}{w } \\right]^{\\frac{1-\\alpha}{\\alpha}}\\\\\\label{eqn_sampling_skewed } & \\sim s(\\alpha,\\beta=1,1),\\end{aligned}\\ ] ] where @xmath117 when @xmath94 and @xmath118 when @xmath108 .",
    "+ sampling from symmetric ( @xmath56 ) stable distributions uses the same procedure with @xmath119 .",
    "thus , the only difference is the @xmath120 term , which is a constant and can be removed out of the sampling procedure and put back to the estimates in the end , which in fact also provides better numerical stability when @xmath2 .",
    "note that the estimators ( [ eqn_f_gm ] ) and ( [ eqn_f_hm ] ) already contain @xmath121 in the numerators .",
    "thus , we can sample @xmath122 instead of @xmath123 and evaluate ( [ eqn_f_gm ] ) ( [ eqn_f_hm ] ) without @xmath121 .",
    "the basic procedure is to first estimate the @xmath0th frequency moment @xmath36 using cc and then compute various entropies using the estimated @xmath36 . here",
    "we use @xmath124 to denote a generic estimator of @xmath124 , which could be @xmath95 , @xmath93 , @xmath98 , or @xmath125 .    in the following subsections",
    ", we analyze the variances and biases in estimating the rnyi entropy @xmath30 , the tsallis entropy @xmath32 , and the shannon entropy @xmath42 .",
    "we denote a generic estimator of @xmath30 by @xmath126 : @xmath127 which becomes @xmath128 , @xmath129 , @xmath130 , and @xmath131 , respectively , when @xmath124 becomes @xmath95 , @xmath93 , @xmath98 , or @xmath125 .",
    "since @xmath25 can be computed exactly and trivially using a simple counter , we assume it is a constant .    since @xmath124 is unbiased or asymptotically unbiased , @xmath126 is also asymptotically unbiased .",
    "the asymptotic variance of @xmath126 can be computed by taylor expansions ( the so - called `` delta method '' in statistics ) : @xmath132      the generic estimator for the tsallis entropy @xmath32 would be @xmath133 which is asymptotically unbiased and has variance @xmath134      we use @xmath135 and @xmath136 to denote the estimators for shannon entropy using the estimated @xmath126 and @xmath137 , respectively .",
    "the variances remain unchanged , i.e. , @xmath138    however , @xmath135 and @xmath136 are no longer unbiased , even asymptotically ( unless @xmath2 ) .",
    "the biases would be @xmath139 the @xmath140 biases arise from the estimation biases in @xmath141 and @xmath142 and diminish quickly as @xmath64 increases .",
    "in fact , there are standard statistics procedures to reduce the @xmath140 bias to @xmath143 .",
    "however , the `` intrinsic biases , '' @xmath144 and @xmath145 , can not be removed by increasing @xmath64 ; they can only be reduced by letting @xmath0 close to 1 .",
    "the total error is usually measured by the mean square error : mse = bias@xmath146 + var .",
    "clearly , there is a variance - bias trade - off in estimating @xmath42 using @xmath30 or @xmath32 . for a particular data stream , at each sample size @xmath64 , there will be an optimal @xmath0 to attain the smallest mse .",
    "the optimal @xmath0 is data - dependent and hence some prior knowledge of the data is needed in order to determine it .",
    "the prior knowledge may be accumulated during the data stream process .",
    "alternatively , we could seek an estimator that is very accurate near @xmath39 to alleviate the variance - bias affect .",
    "the goal of the experimental study is to demonstrate the effectiveness of compressed counting ( cc ) for estimating entropies and to determine a good strategy for estimating the shannon entropy . in particular , we focus on the estimation accuracy and would like to verify the formulas for ( asymptotic ) variances in ( [ eqn_renyi_est_var ] ) and ( [ eqn_tsallis_est_var ] ) .      since the estimation accuracy is what we are interested in , we can simply use static data instead of real data streams .",
    "this is because the projected data vector @xmath147 is the same , regardless whether it is computed at once ( i.e. , static ) or incrementally ( i.e. , dynamic ) .",
    "as we have commented , the processing and storage cost of cc is the same as the cost of _ symmetric stable random projections _ at the same sample size @xmath64 .",
    "therefore , to compare these two methods , it suffices to compare their estimation accuracies .",
    "ten english words are selected from a chunk of web crawl data with @xmath148 pages : the , a , this , have , fun , friday , name , business , rice , and twist .",
    "the words are selected fairly randomly , except that we make sure they cover a whole range of sparsity , from function words ( e.g. , a , the ) , to common words ( e.g. , friday ) to rare words ( e.g. , twist ) .",
    "thus , as summarized in table [ tab_data ] , our data set consists of ten vectors of length @xmath149 and the entries are the numbers of word occurrences in each document .",
    "table [ tab_data ] indicates that the rnyi entropy @xmath30 provides a much better approximation to the shannon entropy @xmath42 , than the tsallis entropy @xmath32 does . on the other hand , if the purpose is to find a summary statistic that is different from the shannon entropy ( i.e. , sensitive to @xmath0 ) , then the tsallis entropy may be more suitable .",
    "l l l l l l l + word & nonzero & @xmath42 & @xmath150 & @xmath151 & @xmath152 & @xmath153 +   + twist & 274 & 5.4873 & 5.4962 & 5.4781 & 6.3256 & 4.7919 + rice & 490 & 5.4474 & 5.4997 & 5.3937 & 6.3302 & 4.7276 + friday & 2237 & 7.0487 & 7.1039 & 6.9901 & 8.5292 & 5.8993 + fun & 3076 & 7.6519 & 7.6821 & 7.6196 & 9.3660 & 6.3361 + business & 8284 & 8.3995 & 8.4412 & 8.3566 & 10.502 & 6.8305 + name & 9423 & 8.5162 & 9.5677 & 8.4618 & 10.696 & 6.8996 + have & 17522 & 8.9782 & 9.0228 & 8.9335 & 11.402 & 7.2050 + this & 27695 & 9.3893 & 9.4370 & 9.3416 & 12.059 & 7.4634 + a & 39063 & 9.5463 & 9.5981 & 9.4950 & 12.318 & 7.5592 + the & 42754 & 9.4231 & 9.4828 & 9.3641 & 12.133 & 7.4775 +    [ tab_data ]     +      the results for estimating frequency moments , rnyi entropy , tsallis entropy , and shannon entropy are presented in the following subsections , in terms of the normalized ( i.e. , relative ) mean square errors ( mses ) , e.g. , @xmath154 , @xmath155 , etc .",
    "after normalization , we observe that the results are quite similar across different words .",
    "to avoid boring the readers , not all words are selected for the presentation .",
    "however , we provides the experimental results for all 10 words , in estimating shannon entropy .    in our experiments ,",
    "the sample size @xmath64 ranges from @xmath156 to @xmath157 .",
    "we choose @xmath158 and @xmath159 .",
    "this is because @xcite only provided the optimal quantile estimator for @xmath110 and @xmath111 . for the geometric mean and harmonic mean estimators , we actually had no problem of using ( e.g. , ) @xmath160 or @xmath161 .",
    "figure [ fig_twist_f ] , figure [ fig_rice_f ] , and figure [ fig_friday_f ] provide the mses for estimating the @xmath0th frequency moments , @xmath36 , for twist , rice , and friday , respectively .",
    "* the errors of the three estimators for cc decrease ( to zero , potentially ) as @xmath2 , while the errors of _ symmetric stable random projections _ do not vary much near @xmath39 .",
    "the improvement of cc is enormous as @xmath2 .",
    "for example , when @xmath75 and @xmath162 , the mse of cc using the optimal quantile estimator is about @xmath163 while the mse of _ symmetric stable random projections _ is about @xmath164 , a 10000-fold error reduction .",
    "* the optimal quantile estimator @xmath98 is in general more accurate than the geometric mean and harmonic mean estimators near @xmath165 .",
    "however , for small @xmath64 ( e.g. , 20 ) and @xmath108 , @xmath98 exhibits some bad behaviors , which disappear when @xmath107 ( or even @xmath166 ) . *",
    "the theoretical asymptotic variances in ( [ eqn_f_gm_var ] ) , ( [ eqn_f_hm_var ] ) , ( [ eqn_f_gm_sym_var ] ) , and table [ tab_oq ] are accurate .",
    "+     +      figure [ fig_twist_h ] plots the mses for estimating the rny entropy for twist , with the curves for @xmath75 removed .",
    "the figure illustrates that : ( 1 ) cc improves _",
    "symmetric stable rand projections _",
    "enormously when @xmath2 ; ( 2 ) the generic variance formula ( [ eqn_renyi_est_var ] ) is accurate .",
    "+      figure [ fig_rice_t ] plots the mses for estimating the tsallis entropy for rice , illustrating that : ( 1 ) cc improves _",
    "symmetric stable rand projections _",
    "enormously when @xmath2 ; ( 2 ) the generic variance formula ( [ eqn_tsallis_est_var ] ) is accurate .",
    "+      figure [ fig_rice_hr ] illustrates the mses from estimating the shannon entropy using the rnyi entropy , for rice .    * using _ symmetric stable random projections _ with @xmath3 and very small @xmath4 is not a good strategy and not practically feasible because the required sample size is enormous .",
    "for example , using @xmath167 , we need @xmath74 in order to achieve a relative mse of @xmath168 .",
    "* there is clearly a variance - bias trade - off , especially for the geometric mean and harmonic mean estimator .",
    "that is , for each @xmath64 , there is an `` optimal '' @xmath0 which achieves the smallest mse . *",
    "using the optimal quantile estimator does not show a strong variance - bias trade - off , because its has very small variance near @xmath39 and its mses are mainly dominated by the ( intrinsic ) biases , @xmath144 . *",
    "the improvement of cc over _ symmetric stable random projections _ is very large when @xmath0 is close 1 .",
    "when @xmath0 is away from 1 , the improvement becomes less obvious because the mses are dominated by the biases .",
    "* using the optimal quantile estimator with @xmath0 very close to 1 ( preferably @xmath94 ) is our recommended procedure for estimating shannon entropy from rnyi entropy .    for a fixed @xmath0 and @xmath64 , we can see that cc improves _",
    "symmetric stable random projections _",
    "enormously when @xmath2 .",
    "if we follow the theoretical suggestion of @xcite by using ( e.g. ) @xmath161 , then the improvement of cc over _ symmetric stable random projections _ will be enormous .    as a practical recommendation",
    ", we do not suggest letting @xmath0 too close to 1 when using _ symmetric stable random projections_. instead , one should take advantage of the variance - bias trade - off by using @xmath0 away from 1 .",
    "there will be an `` optimal '' @xmath0 that attains the smallest mean square error ( mse ) , at each @xmath64 .    as illustrated in figure [ fig_rice_hr ] , cc is not affected much by the variance - bias trade - off and it is preferable to choose @xmath0 close to 1 when using the optimal quantile estimator .",
    "therefore , we will present the comparisons mainly in terms of the minimum mses ( i.e. , best achievable performance ) , which we believe actually heavyily favors _ symmetric stable random projections_.    figures [ fig_hr_min ] presents the minimum mses for all 10 words :    * the optimal quantile estimator is the most accurate .",
    "for example , using @xmath75 , the relative mse is only less than @xmath168 ( or even @xmath169 ) , which may be already accurate enough for some applications .",
    "* for every @xmath64 , cc reduces the ( minimum ) mse roughly by 20- to 50-fold , compared to _",
    "symmetric stable random projections_. this is comparing the curves in the vertical direction . * to achieve the same accuracy as _ symmetric stable random projections _ , cc requires a much smaller @xmath64 , a reduction by about 50-fold ( using the optimal quantile estimator ) .",
    "this is comparing the curves in the horizontal direction . *",
    "the results are quite similar for all 10 words . while it is boring to present all 10 words , the results deliver a strong hint that the performance of cc and its improvement over _ symmetric stable random projections _ should hold universally , not just for these 10 words .",
    "figure [ fig_rice_ht ] illustrates the mses from estimating shannon entropy using tsallis entropy , for rice :    * using _ symmetric stable random projections _ with @xmath3 and very small @xmath4 is not a good strategy and not practically feasible .",
    "for example , when @xmath167 , using @xmath74 can only achieve a relative mse of @xmath170 . * the effect of the variance - bias trade - off for geometric mean and harmonic mean estimators , is even more significant , because the ( intrinsic ) bias @xmath145 is large , as reported in table [ tab_data ] * the mses of the optimal quantile estimator is not affected much by @xmath64 , because its variance is negligible compared to the ( intrinsic ) bias .",
    "figures [ fig_ht_min ] presents the minimum mses for all 10 words :    * the optimal quantile estimator is the most accurate . with @xmath75 ,",
    "the relative mse is only less than @xmath168 ( or even @xmath169 ) .",
    "* when @xmath171 , using the optimal quantile estimator , cc reduces minimum mses by roughly 20- to 50-fold , compared to _",
    "symmetric stable random projections_. when @xmath172 , the reduction is about 5- to 15-fold . *",
    "even with @xmath173 , _ symmetric table random projections _ can not achieve the same accuracy as cc using the optimal quantile estimator with @xmath75 only .",
    "again , using the optimal quantile estimator with @xmath174 would be our recommended procedure for estimating shannon entropy from tsallis entropy .",
    "network data and web search data are naturally dynamic and can be viewed as data streams .",
    "the entropy is an extremely useful summary statistic and has numerous applications , for example , anomaly detection in web mining and network diagnosis .    efficiently and accurately computing",
    "the entropy in ultra - large and frequently updating data streams , in one - pass , is an active topic of research .",
    "a recent trend is to use the @xmath0th frequency moments with @xmath6 to approximate the entropy .",
    "for example , @xcite proposed using the @xmath8 frequency moments with very small @xmath4 ( e.g. , @xmath44 or smaller ) .    for estimating the @xmath0th frequency moments , the recently proposed _",
    "compressed counting ( cc ) _ dramatically improves the standard data stream algorithm based on _ symmetric stable random projections _ , especially when @xmath6 .",
    "however , it had never been empirically evaluated before this work .",
    "we experimented with cc to approximate the rnyi entropy , the tsallis entropy , and the shannon entropy . some theoretical analysis on the biases and variances",
    "was provided .",
    "extensive empirical studies based on some web crawl data were conducted .",
    "based on the theoretical and empirical results , important conclusions can be drawn :    * compressed counting ( cc ) is numerically stable and is capable of providing highly accurate estimates of the @xmath0th frequency moments .",
    "when @xmath0 is close to 1 , the improvements of cc over _ symmetric stable random projections _ in estimating frequency moments is enormous ; in fact , the improvements tend to `` infinity '' when @xmath2 .",
    "* when @xmath0 is close 1 , the optimal quantile estimator for cc is more accurate than the geometric mean and harmonic mean estimators , except when @xmath108 and the sample size @xmath64 is very small ( e.g. , @xmath175 ) . *",
    "it appears not a practical algorithm to approximate the shannon entropy using _ symmetric stable random projections _ with @xmath176 and very small @xmath4 .",
    "when we do need to use _ symmetric stable random projections _ , we should take advantage of the variance - bias trade - off by using @xmath0 away from 1 for achieving smaller mean square errors ( mses ) .",
    "* cc is able to provide highly accurate estimates of the shannon entropy using either the rnyi entropy or the tsallis entropy . in terms of the best achieable mses , the improvements over _ symmetric stable random projections",
    "_ can be about 20- to 50-fold .",
    "* when estimating shannon entropy from rnyi entropy , in order to reach the same accuracy as cc , _ symmetric stable random projections _ would need about 50 times more samples than cc .",
    "when estimating shannon entropy from tsallis entropy , _ symmetric stable random projections _ could not reach the same accuracy as cc even with 500 times more samples . *",
    "the rnyi entropy provides a better tool for estimating the shannon entropy than the tsallis entropy does . * our recommended procedure for estimating the shannon entropy is to use cc with the optimal quantile estimator and @xmath94 close 1 ( e.g. , @xmath177 ) . *",
    "since cc only needs a very small sample to achieve a good accuracy , the processing time of cc will be much reduced , compared to _",
    "symmetric stable random projections _",
    ", if the same level of accuracy is desired .    the technique of estimating shannon entropy using _ symmetric stable random projections _ has been applied with some success in practical applications , such as network anomaly detection and diagnosis@xcite .",
    "one major issue reported in @xcite ( also @xcite ) , is that the required sample size using _ symmetric stable random projections _ could be prohibitive for their real - time applications . since cc can dramatically reduce the required sample size , we are passionate that using compressed counting for estimating shannon entropy will be highly practical and beneficial to real - world web / network / data stream problems .",
    "this work is supported by grant nsf dms-0808864 and a gift from google .",
    "the author would like to thank jelani nelson for helpful communications .",
    "the author thanks kenneth church ."
  ],
  "abstract_text": [
    "<S> * _ compressed counting ( cc ) _ * was recently proposed for approximating the @xmath0th frequency moments of data streams , for @xmath1 . under the _ relaxed strict - turnstile _ model , cc dramatically improves the standard algorithm based on _ symmetric stable random projections _ , especially as @xmath2 . </S>",
    "<S> a direct application of cc is to estimate the entropy , which is an important summary statistic in web / network measurement and often serves a crucial `` feature '' for data mining . </S>",
    "<S> the rnyi entropy and the tsallis entropy are functions of the @xmath0th frequency moments ; and both approach the shannon entropy as @xmath2 . </S>",
    "<S> a recent theoretical work suggested using the @xmath0th frequency moment to approximate the shannon entropy with @xmath3 and very small @xmath4 ( e.g. , @xmath5 ) .    in this study , we experiment using cc to estimate frequency moments , rnyi entropy , tsallis entropy , and shannon entropy , on real web crawl data . </S>",
    "<S> we demonstrate the variance - bias trade - off in estimating shannon entropy and provide practical recommendations . </S>",
    "<S> in particular , our experiments enable us to draw some important conclusions :    * as @xmath2 , cc dramatically improves _ </S>",
    "<S> symmetric stable random projections _ in estimating frequency moments , rnyi entropy , tsallis entropy , and shannon entropy . </S>",
    "<S> the improvements appear to approach `` infinity . '' </S>",
    "<S> * cc is a highly practical algorithm for estimating shannon entropy ( from either rnyi or tsallis entropy ) with @xmath6 . </S>",
    "<S> only a very small sample ( e.g. , 20 ) is needed to achieve a high accuracy ( e.g. , @xmath7 relative errors ) . * using _ symmetric stable random projections _ and @xmath8 with very small @xmath4 does not provide a practical algorithm because the required sample size is enormous . * if we do need to use _ symmetric stable random projections _ for estimating shannon entropy , we should exploit the variance - bias trade - off by letting @xmath0 be away from 1 , for much better performance . </S>",
    "<S> * even in terms of the best achievable performance in estimating shannon entropy , cc still considerably improves _ </S>",
    "<S> symmetric stable random projections _ by one or two magnitudes , both in terms of the estimation accuracy and the required sample size ( storage space ) . </S>"
  ]
}