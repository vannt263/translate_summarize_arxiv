{
  "article_text": [
    "diffusion strategies @xcite-@xcite were first invented to solve distributed estimation problems in real - time environments where data are continuously streamed . here , all nodes employ adaptive filter algorithms to process the streaming data , and simultaneously share their instantaneous estimates with their neighbors .",
    "these approaches are also very useful to model many self - organizing systems @xcite .",
    "recently , in @xcite-@xcite , diffusion lms schemes have been used to estimate sparse vectors , or equivalently , to identify fir systems that have most of the impulse response coefficients either zero or negligibly small . in these papers , certain sparsity promoting norms of the filter coefficient vectors",
    "have been used to regularize the standard lms cost function , prominent amongst them being the @xmath0 norm of the coefficient vector that leads to the sparsity aware , zero attracting lms ( za - lms ) @xcite-@xcite form of weight adaptation .",
    "these diffusion sparse lms algorithms manifest superior performance in terms of lesser steady state network mean square deviation ( nmsd ) compared with the simple diffusion lms . in this paper , we show that the minimum level of the steady state nmsd achieved using za - lms based update at _ all _ the nodes of the network can also be obtained by a _",
    "heterogeneous _ network with only a fraction of the nodes using the za - lms update rule ( referred as sparsity aware nodes in this paper ) while the rest employing the standard lms update ( referred as sparsity agnostic nodes in this paper ) , provided the nodes using the za - lms are distributed over the network maintaining some  uniformity \" .",
    "note that reduction in the number of sparsity aware nodes reduces the overall computational burden of the network , especially when more complicated sparsity aware algorithms involving significant amount of computation are deployed to exploit sparsity . as shown in this paper , the only adjustment to be made to achieve the above reduction in the number of sparsity aware nodes is a proportional increase in the value of the optimum zero attracting coefficient .",
    "analytical expressions explaining the above behavior are provided and the claims made are validated via detailed simulation studies . finally , the proposed analysis ,",
    "though restricted to the @xmath0-norm regularized algorithm ( i.e. , za - lms ) only , can be trivially extended to the case of more general norms and thus similar behavior can also be expected from the corresponding heterogeneous networks .",
    "we consider a connected network consisting of @xmath1 nodes that are spatially distributed . at every time index @xmath2",
    ", each @xmath3 node collects some scalar measurement @xmath4 and some @xmath5 vector @xmath6 which are related by the following model : @xmath7 where @xmath8 is the measurement noise at the @xmath3 node and @xmath9 is the unknown @xmath10 vector , known a priori to be sparse , which is required to be estimated .",
    "both @xmath6 and @xmath8 are variates generated from some gaussian distributions , with @xmath6 and @xmath11 being mutually independent for all @xmath12 .    in the diffusion scheme , every @xmath13-th node , @xmath14 deploys a @xmath10 adaptive filter @xmath15 to estimate @xmath9 , which takes @xmath4 and @xmath16 respectively as the local desired response and input vectors .",
    "the estimates of @xmath9 , i.e. , @xmath17 for each @xmath13 are exchanged with the neighbors of the @xmath13-th node , i.e. , nodes directly connected to it , and are used to refine the estimates in one of the two following manners : ( a ) adapt - then - combine ( atc ) where @xmath17 is first updated to an intermediate estimate @xmath18 , which is then linearly combined with similar estimates received from the neighbors , and ( b ) combine - then - adapt ( cta ) where @xmath17 is first linearly combined with similar estimates received from the neighbors and then updated .",
    "originally , the diffusion schemes were proposed assuming lms form of weight adaptation at each node @xcite-@xcite . in the context of sparse estimation , certain sparsity exploiting norms of @xmath17 were added to the corresponding lms cost function @xcite-@xcite , the most popular of them being the @xmath0 norm penalty @xmath19 which results in the introduction of the zero attracting terms @xmath20 $ ] in the lms update equations @xcite-@xcite .",
    "the resulting diffusion za - lms algorithm for the atc scheme , popularly termed as za - atc diffusion algorithm @xcite , is shown in table i and is considered by us in this paper .",
    "the parameter @xmath21 in table i is the zero - attracting coefficient which is a very very small , positive constant taken same for all the nodes and @xmath22 denotes the set of nodes in the neighborhood of the node @xmath13 ( including itself ) .",
    "[ table : za_atc ]    the combining coefficients @xmath23 are non - negative constants which are usually chosen satisfying the following @xcite : @xmath24 there exist several standard schemes in the literature to choose the coefficients @xmath25 , e.g. , the uniform combination rule , the metropolis rule , the laplacian rule and the nearest neighbor rule to name a few . using these coefficients ,",
    "a combination matrix @xmath26 is defined for the network , where @xmath27_{i , j}=c_{i , j}^{'}.$ ]",
    "before presenting the proposed heterogeneous network and its at par behavior with the za - atc based diffusion network of @xcite , it will be useful to consider some of the major results of @xcite here .",
    "for this , we first define the average network mean - square deviation at the @xmath28 time index as , @xmath29 where @xmath30 is the individual mean - square deviation of the @xmath3 node at the @xmath28 time index , i.e. , @xmath31 = e[\\|{\\bf { \\tilde w}}_k(n)\\|^2]\\ ] ] where @xmath32 is the weight deviation vector for the @xmath13-th node at @xmath2-th index .",
    "the expression for steady - state @xmath33 ( i.e. , @xmath34 ) of the za - atc algorithm was derived analytically in @xcite . however , @xcite considered a more general form of diffusion , in which both @xmath35 and @xmath36 are also exchanged with the neighbors along with the local estimates @xmath37 .",
    "in contrast , in this paper , we consider exchange of only @xmath38 which is also the most common form of diffusion .",
    "additionally , we introduce a few more simplifications in @xcite .",
    "firstly , we assume same step - size @xmath39 for all nodes . next",
    ", both the input signal and noise at each node are assumed to be spatially and temporally i.i.d . under these , it is easy to check that the @xmath34 expression for the za - atc algorithm @xcite simplifies to the following :    @xmath40^t({\\bf i } - { \\bf f})^{-1}{\\bf q } \\nonumber\\\\                     & + &   \\frac{1}{n}(\\beta(\\infty ) - \\alpha(\\infty ) ) ,                     \\label{eq : msd}\\end{aligned}\\ ] ]    with @xmath41^t{\\boldsymbol \\varomega}{\\bf c}{\\bf c}^t({\\bf i } - \\mu{\\bf d}){\\bf { \\tilde w}}(\\infty)]\\ ] ] and @xmath42\\|_{{\\boldsymbol \\varomega}{\\bf c}{\\bf c}^t{\\boldsymbol   \\varomega}}^{2}],\\ ] ] where @xmath43 is an operator that stacks the columns of its argument matrix on top of each other , @xmath44 , @xmath45 and @xmath46 , with @xmath47 denoting an operator that carries out stacking of its argument column vectors on top of each other , and @xmath48 and @xmath49 are the variances of the noise and input signal respectively .",
    "the matrices @xmath50 , @xmath51 , @xmath52 and @xmath53 are defined as follows : + @xmath54 [ @xmath55 defines the right kronecker product . ]",
    "+ @xmath56 + @xmath57 , + @xmath58 .",
    "[ also note that for a vector @xmath59 and a matrix @xmath60 , @xmath61 indicates @xmath62 . ]",
    "it is noticed that the first term in the r.h.s . of ( 6 )",
    "is actually the steady - state network msd of simple atc diffusion lms @xcite and is independent of @xmath21 .",
    "let us denote the second term as @xmath63 , i.e. , @xmath64 .",
    "it is easy to see that one can express @xmath63 as @xmath65 , where , @xmath66^t{\\bf c}{\\bf c}^t({\\bf i } - \\mu{\\bf d}){\\bf { \\tilde w}}(\\infty)]\\ ] ] and @xmath67\\|_{{\\bf c}{\\bf c}^t}^{2}]~(>\\;0).\\ ] ] the function @xmath63 has two zero - crossing points , one at @xmath68 and the other at @xmath69 , and between them , @xmath63 takes only negative values with the minima occurring at @xmath70 , which , from ( 6 ) , also minimizes @xmath34 . for systems that are highly sparse",
    ", it follows from @xcite that @xmath71 and conversely , for non - sparse systems , @xmath72 . since for proper zero attraction ,",
    "@xmath21 must be positive , the optimum value of @xmath21 is then given by @xmath73.\\ ] ] the corresponding minimum value of @xmath74 ( when @xmath75 ) is then given as @xmath76 * the proposed heterogeneous diffusion network :* +   + in this section , we show that the same level of @xmath77 as given by ( 10 ) and therefore , the same @xmath78 $ ] can be reached by a heterogeneous network as well , where only a fraction of the nodes are sparsity aware and rest are sparsity agnostic , provided the network is designed satisfying the assumptions i.a and i.b as given in the box below where @xmath79 denotes the set of indices of the sparsity aware nodes and :     +   + in order to show the above , we replace the matrix @xmath80 by a new one defined as @xmath81,$ ] where @xmath82 .    using this and the fact that @xmath83 , @xmath84 and @xmath85 modify to @xmath86 and @xmath87 , given as follows : @xmath88^t{\\boldsymbol \\varomega}_s{\\bf c}{\\bf c}^{t}{\\bf { \\tilde w}}(\\infty)]\\end{aligned}\\ ] ] and @xmath89^t{\\boldsymbol \\varomega}_s{\\bf c}{\\bf c}^{t}{\\boldsymbol \\varomega}_s sgn[{\\bf w}(\\infty ) ] ]   \\end{aligned}\\ ] ] note that unlike @xmath84 and @xmath85",
    ", it is lot more difficult to express @xmath86 and @xmath87 as a function of @xmath21 , since unlike @xmath53 , @xmath90 can not be written simply as @xmath91 .",
    "instead , one needs to analyze the rhs of ( 11 ) and ( 12 ) to express @xmath86 and @xmath87 in terms of @xmath21 . towards this",
    ", we make the following assumptions : +    it is then possible to prove the following : +    for a network satisfying the @xmath92 and @xmath93 as given above , we have , @xmath94 n_s.\\end{aligned}\\ ] ] proof : skipped due to page limitation .",
    "+    for a network satisfying the @xmath95 , @xmath96 and @xmath97 as given above , we have , @xmath98n_{s}^{2}}{n}.\\end{aligned}\\ ] ] proof : skipped due to page limitation . +        substituting @xmath99 and @xmath100 in @xmath101 , then differentiating w.r.t . @xmath21 and equating the derivative to zero , we obtain , @xmath102n}{\\mu tr[{\\boldsymbol   \\psi}]n_s}].\\ ] ]    the corresponding minimum value of @xmath63 [ when @xmath75 , i.e. , the system is sparse ] , say , @xmath103 is given as @xmath104 ^ 2}{tr[{\\boldsymbol   \\psi}]}.\\ ] ] note that @xmath103 as given in ( 16 ) is independent of @xmath105 . therefore , _ its value remains same when @xmath106 , i.e. , when the network becomes _ _ homogeneous with all nodes being sparsity aware_. this also implies that if @xmath77 as given by ( 10 ) is analyzed using the assumptions i and ii , it would give rise to the same expression as that of @xmath103 ( i.e. , ( 16 ) ) . from this and ( 15 ) , we then make the following two conclusions : +   + @xmath107 the @xmath78 $ ] does not change when the network changes from being homogeneous to heterogeneous , with only @xmath105 of the total @xmath1 ( @xmath108)nodes employing sparsity aware adaptation .",
    "+ @xmath107 for sparse systems , the @xmath109 minimizing @xmath63 and thus @xmath110 $ ] ( i.e. , @xmath111n}{\\mu tr[{\\boldsymbol \\psi}]n_s}$ ] as given in ( 15 ) ) is inversely proportional to @xmath105 , meaning that while maintaining the same @xmath78 $ ] , one can reduce the number of sparsity aware nodes by introducing proportional increase in the value of @xmath21 .",
    "to test the performance of the heterogeneous networks , we use a strongly connected network of @xmath112 nodes placed randomly in a geographic region .",
    "the weights of the edges are determined by the uniform combination rule @xcite .",
    "the goal of the network is to estimate a @xmath113 vetor @xmath9 which is highly sparse ( only one coefficient being non - zero ) .",
    "we choose the same step - size @xmath114 for all the nodes . among these @xmath115 nodes ,",
    "@xmath105 number of nodes use the za - lms and rest of the nodes use simple lms update , with the former spaced uniformly ( i.e. , satisfying assumptions i.a and i.b ) over the network .",
    "the input signals and noise variables are drawn from gaussian distributions , and they are temporally and spatially independent .",
    "also , the input and noise statistics are same for all the nodes , with @xmath116 , and @xmath117 . to start with",
    ", the value of @xmath21 is kept fixed at @xmath118 for all the @xmath105 sparsity aware nodes .",
    "the simulation is then carried out for @xmath119 iterations and the network steady state msd is evaluated by taking ensemble average over @xmath120 independent runs .",
    "this is done for different values of @xmath105 ( ranging from @xmath121 to @xmath115 ) and based on this , the network steady state msd is plotted as a function of @xmath105 . the value of @xmath21 is then increased progressively to take the following five values : @xmath122 , one at a time for all the za - lms based nodes .",
    "1 displays the network steady state msd vs. @xmath105 plots with @xmath21 as a parameter .",
    "it is easily seen from fig .",
    "1 that ( i ) the minima reached by each msd - vs-@xmath105 plot is same for all the plots , and ( ii ) as @xmath21 increases , the value of @xmath105 where the minima occurs reduces and vice versa . in other words ,",
    "1 validates the theoretical conjectures made in the previous section .",
    "1 a. h. sayed , `` diffusion adaptation over networks , '' in _ e - reference signal processing _",
    ", r. chellapa and s. theodoridis , eds .",
    "amsterdam , the netherlands : elsevier , available online at http://arxiv.org/abs/1205.4220 , to be published .",
    "y. gu , y. chen and a. o. hero ,",
    " sparse lms for system identification \" , _ proc .",
    "ieee intl .",
    "taipei , taiwan , _ apr .",
    "k. shi and p. shi , `` convergence analysis of sparse lms algorithms with @xmath0-norm penalty based on white input signal , '' _ signal process .",
    "3289 - 3293 , dec . 2010 .",
    "bijit kumar das and m. chakraborty `` sparse adaptive filtering by an adaptive convex combination of the lms and the za - lms algorithms , '' _ ieee trans .",
    "circuits syst.i , reg.papers_ , vol .",
    "5 , pp . 1499 - 1507 , may 2014 ."
  ],
  "abstract_text": [
    "<S> in - network distributed estimation of sparse parameter vectors via diffusion lms strategies has been studied and investigated in recent years . in all the existing works , </S>",
    "<S> some convex regularization approach has been used at each node of the network in order to achieve an overall network performance superior to that of the simple diffusion lms , albeit at the cost of increased computational overhead . in this paper , we provide analytical as well as experimental results which show that the convex regularization can be selectively applied only to some chosen nodes keeping rest of the nodes sparsity agnostic , while still enjoying the same optimum behavior as can be realized by deploying the convex regularization at all the nodes . due to the incorporation of unregularized learning at a subset of nodes , </S>",
    "<S> less computational cost is needed in the proposed approach . </S>",
    "<S> we also provide a guideline for selection of the sparsity aware nodes and a closed form expression for the optimum regularization parameter .    * index terms*adaptive network , diffusion lms , sparse systems , excess mean square error , adaptive filter , @xmath0 norm . </S>"
  ]
}