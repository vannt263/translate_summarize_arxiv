{
  "article_text": [
    "in computer science , the notion of computational complexity serves as a measure of how difficult it is to compute a solution for a given problem .",
    "computations take time and complexity here means the time rate of growth to solve the problem",
    ". another related kind of complexity measure ( studied in theoretical computer science ) is the so - called algorithmic ( or kolmogorov ) complexity which measures how long a computer program ( on some generic computational machine ) needs to be in order that it produces a complete description of an object .",
    "interestingly , the theory says that if we consider as an object a system that can process input information ( available as a binary sequence of high entropy ) and which produces another sequence as an output then the amount of randomness in the output sequence is inversely proportional to the algorithmic complexity of the system .",
    "this has been traditionally studied in the context of algorithmic randomness ( see @xcite and references within ) and it has been only until recently unknown whether such a relationship between complexity and randomness exists for more general systems , for instance , those governed by physical laws . in @xcite the complexity of a general static system ( for instance , a physical solid ) is modeled algorithmically , i.e. , by its description length . using the model it is proposed that the stability of a static system ( from the physical perspective ) is related to its level of algorithmic complexity .",
    "this is explained by the relationship between the complexity of a system and its ability to distort the randomness in its environment . the first proof of this concept appeared in a recent paper @xcite where it was shown that this inverse relationship between system complexity and randomness exists also in a physical system .",
    "the particular system investigated consisted of a one - dimensional vibrating solid - beam to which a random sequence of external input forces is applied .",
    "the current paper is yet another proof of concept of the model of @xcite .",
    "we proceed along the line of @xcite but instead of considering a physical system ( the static solid with input force sequence ) we consider a _ decision _ system and study its influence on a random binary data sequence on which prediction decisions are made .",
    "the decision system is based on the maximum _ a posteriori _ probability decision where probabilities are defined by a statistical parametric model which is estimated from data .",
    "the learner of this model is a computer program that trains from a given random data sequence and then produces a decision rule by which it is able to predict ( or decide ) the value of the next bit in future ( yet unseen ) random binary sequences .",
    "while this paper is in the realm of machine - learning we are not proposing a new algorithm nor are we interested in the performance of the learner . but rather , our interest is in displaying a learning ( and decision ) system from the perspective of static system complexity and its influence on random inputs @xcite .",
    "let @xmath9 be a sequence of binary random variables drawn according to some unknown joint probability distribution @xmath10 . consider the problem of learning to predict the next bit in a binary sequence drawn according to @xmath11 . for training",
    ", the learner is given a finite sequence @xmath12 of bits @xmath13 @xmath14 , drawn according to @xmath11 and estimates a model @xmath15 that can be used to predict the next bit of a partially observed sequence .",
    "after training , the learner is tested on another sequence @xmath16 drawn according to the same unknown distribution @xmath11 . using @xmath15 he produces the bit @xmath17 as a prediction for @xmath18 , @xmath19 .",
    "denote by @xmath20 the corresponding binary sequence of mistakes where @xmath21 if @xmath22 and is @xmath23 otherwise . in @xcite",
    "the following question was posed : how random is @xmath20 ?",
    "it is clear that the sequence of mistakes should be random since the test sequence @xmath16 is random .",
    "it may also be that because the learner is using a model of a finite structure ( or a finite description - length ) that it may somehow introduce dependencies and cause @xmath20 to be less random than @xmath16 . and",
    "yet by another intuition , perhaps the fact that the learner is of a finite complexity limits its ability to deform ( or distort ) randomness of @xmath16 ?",
    "these are all valid initial guesses that relate to this main question .",
    "we note that our basis for saying that @xmath15 has a finite structure stems from it being an element of some regular hypothesis class , for instance , having a finite vc - dimension as is often the case in a learning setting ( see for instance structural risk minimization of @xcite ) . in the current paper",
    ", we are not interested in the learner s performance ( as modeled for instance by valiant s pac framework @xcite ) but instead we take a black - box view of a learner and ask how much influence does he has on the stochastic properties of the errors .",
    "we view the learner as an entity that interferes with the randomness that is inherent in the sequence to be predicted and through his predictions creates a sequence of mistakes that has a different stochastic character .",
    "this view in a broader sense is taken in @xcite and is shown ( empirically ) in @xcite to explain how static structures may deform random external forces .    the question raised above",
    "was answered in @xcite for a particular learning setting where the teacher uses a probability distribution @xmath11 based on a markov model with a certain complexity .",
    "the learner has access to a hypothesis class of boolean decision rules that are based on markov models .",
    "hence , learning amounts to the estimation of parameters of a finite - order markov model ( see for instance @xcite ) .",
    "the answer shows theoretically that the random characteristics of the subsequence of mistakes corresponding to the @xmath23-predictions of a learner changes in accordance with the complexity of the learner s decision rule s complexity . the more complex the rule the higher the possibility of distortion of randomness , i.e.",
    ", the farther away it is from being truly - random .    in the current paper",
    "we take an experimental approach to answering the above question . as in @xcite",
    "we focus on markov source and a markov learner whose orders may differ . in the next section",
    "we describe the setup .",
    "the learning problem consists of predicting the next bit in a given sequence generated by a markov chain ( model ) @xmath24 of order @xmath1 .",
    "there are @xmath25 states in the model each represented by a word of @xmath1 bits . during a learning problem ,",
    "the source s model is fixed .",
    "a learner , unaware of the source s model , has a markov model of order @xmath0 .",
    "we denote by @xmath26 the probability of transiting from state @xmath4 whose binary @xmath0-word is @xmath27 $ ] to the state whose word is @xmath28 $ ] .",
    "given a random sequence of length @xmath29 generated by the source the learner estimates its own model s parameters @xmath26 by @xmath30 , @xmath31 , which is the frequency of the event `` @xmath32 is followed by a @xmath33 '' in the training sequence .",
    "we denote by @xmath34 the learnt model with parameters @xmath30 , @xmath31 .",
    "we denote by @xmath35 the transition probability from state @xmath4 of the source model , @xmath31 .",
    "a simulation run is characterized by the parameters , @xmath0 and @xmath29 .",
    "it consists of a training and testing phases . in the training phase",
    "we show the learner a binary sequence of length @xmath29 and he estimates the transition probabilities . in the testing phase we show the learner another random sequence ( generated by the same source ) of length @xmath36 and test the learner s predictions on it . for each bit in the test sequence",
    "we record whether the learner has made a mistake .",
    "when a mistake occurs we indicate this by a @xmath33 and when there is no mistake we write a @xmath23 .",
    "the resulting sequence of length @xmath36 is the generalization mistake sequence @xmath20 .",
    "we denote by @xmath37 the binary subsequence of @xmath20 that corresponds to the mistakes that occured only when the learner predicted a @xmath23 .    for a fixed @xmath0 denote by @xmath38 the number of runs with a learner of order @xmath0 and training sample of size @xmath29 .",
    "the experimental setup consists of @xmath39 runs with @xmath40 , @xmath41 with a total of @xmath42 runs .",
    "the testing sequence is of length @xmath43 .",
    "each run results in a file called _ system _ which contains a binary vector @xmath44 whose @xmath3 bit represents the maximum _ a posteriori _ decision made at state @xmath4 of the learner s model , i.e. , @xmath45 for @xmath31 .",
    "let us denote by @xmath46 , thus @xmath47 are bernouli random variables with parameters @xmath48 , @xmath31 .",
    "the learner s system is its decision rule at every possible state .",
    "another file generated is the _",
    "errort0 _ which contains the mistake subsequence @xmath37 .",
    "at the end of each run we measure the lengths of the _ system _ file and its compressed length where compression is obtained via the gzip algorithm ( a variant of @xcite ) and compute the _ sysratio _ ( denoted as @xmath49 which is the ratio of the compressed to uncompressed length of the system file . note that @xmath2 is a measure of information density since it captures the number of bits of useful information ( useful for describing the system ) there are per bit of representation ( in the uncompressed file ) .",
    "we do similarly for the mistake - subsequence @xmath37 obtaining the length @xmath50 of the compressed file that contains @xmath37 ( henceforth referred to as the estimated algorithmic complexity of @xmath37 since it is an approximation of the kolmogorov complexity of @xmath37 , see @xcite ) .",
    "we measure the kl - divergence @xmath51 between the probability distribution @xmath52 of binary words @xmath53 of length @xmath54 and the empirical probability distribution @xmath55 as measured from the mistake subsequence @xmath37 .",
    "note , @xmath52 is defined according to the bernouli model with parameter @xmath56 , that is , @xmath57 for a word @xmath53 with @xmath4 ones , where @xmath56 is the frequency of ones in the subsequence @xmath37 .",
    "the distribution @xmath55 equals the frequency of a word @xmath53 in @xmath37 .",
    "hence @xmath51 reflects by how much @xmath37 deviates from being random according to a bernoulli sequence .",
    "we are interested in the determining the following relationships : ( 1 ) the system ratio @xmath2 versus the learner s model order @xmath0 , ( 2 ) the estimated algorithmic complexity @xmath50 of the subsequence @xmath37 versus the @xmath2 , and ( 3 ) the deviation @xmath51 versus @xmath2 .",
    "we choose four different levels of learning problems , controlled by the order of the source model @xmath58 , @xmath54 , @xmath59 , @xmath60 . for each problem",
    "we choose for the source model a transition matrix of probabilities @xmath61 , @xmath62 , where for some of the states @xmath4 we set @xmath63 and for others @xmath64 , @xmath65 .",
    "thus the bayes optimal error is @xmath66 . to ensure that the problem is sufficiently challenging we set the first half of the states ( those ranging from the @xmath1-dimensional vector @xmath67 to @xmath68 ) to have @xmath63 and the second half ( @xmath69 to @xmath70 ) to have @xmath64 .",
    "this ensures that a markov model of order @xmath71 can not approximate the true transition probabilities well , i.e. , the infinite - sample limit estimate based on a markov model of order @xmath0 which is smaller than @xmath1 will still be @xmath72 , @xmath31 .",
    "but for a markov model of order @xmath73 the infinite - sample size estimates will converge to the true values of @xmath74 or @xmath75 .    before we start to investigate the three relationships stated above we perform a sanity check to see how the prediction generalization error ( for any of the two prediction types , not just when predicting a zero ) varies with respect to the model complexity @xmath0 .",
    "figure [ fig : generalization - error - versus ] displays this relationship for a learning problem with @xmath58 . the curve ( with @xmath76 ) is the mean error over all learning runs of a fixed @xmath0 value , the upper and lower curves are the standard deviation above and below the mean , respectively . as seen , when the learner s model order @xmath0 is smaller than @xmath1 his generalization error stays at the maximum level of @xmath77 . at @xmath78",
    "there is a drop to an error close to the bayes error of @xmath66 then as @xmath0 increases beyond @xmath1 the mean ( as well as the standard deviation ) of the generalization error start to increase .",
    "this is due to overfitting of the model to the training data and also because the variance of the error estimate increases with @xmath0 due to the fact that the maximum sample size of any run is fixed at @xmath79 and is not increasing with respect to @xmath0 .",
    "generalization error versus @xmath0 for @xmath58 ]    we now proceed to describe the first result which concerns the relationship between the sysratio @xmath2 and @xmath0 .",
    "figure [ fig : sysratio  versus ] shows the mean and standard deviation of the sysratio @xmath2 as a function of @xmath0 .",
    "the mean decreases as the learner s model order @xmath0 increases . to explain this , first note that the uncompressed length of the system is always @xmath80 for some constant @xmath81 since the vector @xmath44 is of length @xmath82 ( see section [ sec : experimentl - setup ] ) .",
    "the length of the compressed system file also grows , but at a slower rate with respect to @xmath0 and this gives rise to the decrease in @xmath2 with respect to @xmath0",
    ". why is the rate of the compressed system file growing more slowly ?",
    "the reason is that for values of @xmath71 the learner s model is incapable ( by design of the learning problem ) of estimating the bayes optimal prediction and the probability of the events `` @xmath32 is followed by a @xmath33 '' is @xmath83 , @xmath31 .",
    "thus the average value @xmath30 of the indicators of such events is a binomial random variable with a distribution symmetric at @xmath84 and hence from ( [ eq : zi ] ) the probability @xmath48 that @xmath85 equals @xmath84 .",
    "the components of the random vector @xmath44 are independent bernouli random variables with parameter @xmath48 when conditioned on the sample size vector @xmath86 ( this is the vector whose components @xmath87 are the number of times that @xmath32 appeared in the training sequence , see @xcite for details ) .",
    "since in this case @xmath88 then each component has a maximum entropy @xmath89 and hence the expected value of the entropy of the vector @xmath44 ( with respect to the random sample size vector @xmath86 ) is maximal and equals @xmath90 hence the expected compressed length of the system file ( which contains the vector @xmath44 ) is large as the expected description length of any random variable is at least as large as its entropy .",
    "as @xmath0 increases beyond @xmath1 the model becomes more capable of estimating the true transition probabilities ( recall , these are either @xmath66 or @xmath91 ) and the probability @xmath26 of the events `` @xmath32 is followed by a @xmath33 '' get farther away from @xmath84 in the direction of @xmath66 or @xmath91 , depending on the particular state @xmath4 , @xmath31 .",
    "thus the average value @xmath30 of the indicators of such events is a binomial random variable with an asymmetric distribution with a mean @xmath92 ) .",
    "hence from ( [ eq : zi ] ) the probability @xmath48 that @xmath85 gets either very close to @xmath23 or @xmath33 as the training size @xmath29 increases .",
    "thus the components of the random vector @xmath44 tend to be closer to deterministic .",
    "they are still random since the training sequence length is not increasing with @xmath0 and the variance of the estimates @xmath30 does not converge to zero .",
    "therefore for each of the @xmath82 components of the vector @xmath44 the entropy is smaller than when @xmath71 .",
    "however as there are exponentially many components @xmath47 , on the whole , the entropy of @xmath44 ( and hence the expected compressed length of the system file ) still increase but at a lower rate than when @xmath71 .",
    "the mean of sysratio versus @xmath0 ( left ) , the standard deviation of sysratio versus @xmath0 ( right ) ]    next , we discuss the characteristics of the mistake subsequence @xmath37 .",
    "figure [ fig : algorithmic - complexity- ] shows the graph ( with @xmath76 ) of the mean of the estimated algorithmic complexity @xmath50 of @xmath37 versus the mean of the system ratio @xmath2 on the horizontal axis .",
    "the dashed lines are the upper and lower envelopes of the standard deviation from the mean .",
    "the arrow points at the value of @xmath5 that corresponds to @xmath1 ( the source model order ) .",
    "as can be seen , for low values of sysratio the spread @xmath50 is low .",
    "there is a sharp threshold at @xmath5 where the spread around the mean value of @xmath50 increases significantly .    estimated algorithmic complexity @xmath50 of the mistake subsequence @xmath37 versus the sysratio @xmath2 ]    next , figure [ fig : divergence  of ] displays the graph ( with @xmath76 ) of the mean of the divergence @xmath51 of the mistake subsequence @xmath37 versus the mean of the system ratio @xmath2 on the horizontal axis .",
    "the dashed lines are the upper and lower envelopes of the standard deviation from the mean .",
    "the arrow points at the value of @xmath5 that corresponds to @xmath1 ( the source model order ) .",
    "as can be seen , for low values of sysratio the spread of @xmath51 is low . as the result above for @xmath50",
    ", we see a threshold at @xmath5 where the standard deviation around the mean value of @xmath51 increases significantly .",
    "divergence @xmath51 of the mistake subsequence @xmath37 versus the sysratio @xmath2 ]",
    "the paper introduces the notion of sysratio @xmath2 which is a measure of information density of the learner s model .",
    "it is similar to the notion of rate of information transmission @xcite as it measures the ratio of the number of useful information bits contained in a file that describes the learner decision rule per bit of representation ( in the file ) .",
    "the results of this paper depict that this information density influences the level of randomness of the mistakes made by a learner .",
    "the sysratio @xmath2 is a proper measure of complexity of a learner decision rule .",
    "it is with respect to @xmath2 that the characteristics of the random mistake subsequence @xmath37 follow what the theory @xcite predicts .",
    "the higher the sysratio the more significant the deviation @xmath51 of @xmath37 compared to a pure bernouli random sequence .",
    "in addition , we have shown that the higher the sysratio the larger the possible fluctuations in the algorithmic complexity @xmath50 of @xmath37 . the interesting point is the sharp non - linearity in this relationship .",
    "we showed that there is a threshold @xmath5 at which the spread in values of @xmath50 and @xmath51 increases and it corresponds to the point where the learner s model becomes too simple and is incapable of predicting well .",
    "j.  ratsaby and i.  chaskalovic .",
    "random patterns and complexity in static structures . in _ d.a .",
    "karras et .",
    "( eds.),proc .",
    "intl conf . on artificial intelligence and pattern recognition ( aipr09 ) _ , pages 255261 .",
    "isrst , 2009 ."
  ],
  "abstract_text": [
    "<S> what is the relationship between the complexity of a learner and the randomness of his mistakes ? </S>",
    "<S> this question was posed in @xcite who showed that the more complex the learner the higher the possibility that his mistakes deviate from a true random sequence . in the current paper </S>",
    "<S> we report on an empirical investigation of this problem . </S>",
    "<S> we investigate two characteristics of randomness , the stochastic and algorithmic complexity of the binary sequence of mistakes . </S>",
    "<S> a learner with a markov model of order @xmath0 is trained on a finite binary sequence produced by a markov source of order @xmath1 and is tested on a different random sequence . as a measure of learner s complexity </S>",
    "<S> we define a quantity called the _ sysratio _ , denoted by @xmath2 , which is the ratio between the compressed and uncompressed lengths of the binary string whose @xmath3 bit represents the maximum _ a posteriori _ decision made at state @xmath4 of the learner s model . </S>",
    "<S> the quantity @xmath2 is a measure of information density . </S>",
    "<S> the main result of the paper shows that this ratio is crucial in answering the above posed question . </S>",
    "<S> the result indicates that there is a critical threshold @xmath5 such that when @xmath6 the sequence of mistakes possesses the following features : ( 1 ) _ _ low divergence @xmath7 from a random sequence , ( 2 ) low variance in algorithmic complexity . when @xmath8 , the characteristics of the mistake sequence changes sharply towards a _ _ </S>",
    "<S> high _ @xmath7 _ and high variance in algorithmic complexity .    </S>",
    "<S> department of electrical and electronics engineering , ariel university center , ariel 40700 , israel    ` ratsaby@ariel.ac.il ` </S>"
  ]
}