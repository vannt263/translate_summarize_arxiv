{
  "article_text": [
    "conventional methods for distributed network optimization are based on sub - gradient descent in either the primal or dual domains , see  @xcite . for a large class of problems , these techniques yield iterations that can be implemented in a distributed fashion by only using local information .",
    "their applicability , however , is limited by increasingly _",
    "slow _ convergence rates .",
    "second order newton methods  @xcite are known to overcome this limitation leading to improved convergence rates .",
    "unfortunately , computing _ exact _ newton directions based only on local information is challenging . specifically , to determine the newton direction ,",
    "the inverse of the dual hessian is needed . determining this inverse ,",
    "however , requires global information .",
    "consequently , authors in  @xcite proposed approximate algorithms for determining these newton iterates in a distributed fashion . accelerated dual descent ( add )  @xcite , for instance , exploits the fact that the dual hessian is the weighted laplacian of the network and performs a truncated neumann expansion of the inverse to determine a local approximate to the exact direction .",
    "add allows for a tradeoff between accurate hessian approximations and communication costs through the n - hop design , where increased n allows for more accurate inverse approximations arriving at increased cost , and lower values of n reduce accuracy but improve computational times .",
    "though successful , the effectiveness of these approaches highly depend on the accuracy of the truncated hessian inverse which is used to approximate the newton direction . as shown in section  [ sec : exp ] , the approximated iterate can resemble high variation to the real newton direction , decreasing the applicability of these techniques . exploiting the sparsity pattern of the dual hessian , in this paper",
    "we tackle the above problem and propose a newton method for network optimization that is both faster and more accurate . using recently - developed solvers for symmetric diagonally dominant ( sddm ) linear equations , we approximate the newton direction up - to any arbitrary precision @xmath0 .",
    "the solver is a distributed implementation of  @xcite constructing what is known as an inverse chain .",
    "we analyze the properties of the proposed algorithm and show that , similar to conventional newton methods , superlinear convergence within a neighborhood of the optimal value is attained .",
    "we finally demonstrate the effectiveness of the approach in a set of experiments on randomly generated networks .",
    "namely , we show that our method is capable of significantly outperforming state - of - the - art methods in both the convergence speeds and in the accuracy of approximating the newton direction .",
    "the remainder of the paper is organized as follows .",
    "section  [ sec : back ] draws upon background material needed for the remainder of the paper .",
    "section  [ sec : probdefs ] defines the network flow optimization problem targeted in this paper .",
    "section  [ sec : sddm ] details our proposed distributed solver for sddm linear systems .",
    "section  [ sec : newtonapprox ] introduces the approximate newton method and rigorously analyzes its theoretical guarantees .",
    "section  [ sec : exp ] presents the experimental results .",
    "finally , section  [ sec : con ] concludes pointing - out interesting directions for future research .",
    "to determine the newton direction , we need to solve a symmetric diagonally dominant system of linear equations , defined as : @xmath1 where @xmath2 is a symmetric diagonally dominant m - matrix ( sddm ) .",
    "namely , @xmath2 is symmetric positive definite with non - positive off diagonal elements , such that for all @xmath3 : @xmath4_{ii } \\ge -\\sum_{j=1 , j\\ne i}^{n}\\left[\\bm{m}_{0}\\right]_{ij}\\ ] ] the system of equations in  [ lin_sys ] can be interpreted as representing an undirected weighted graph , @xmath5 , with @xmath2 being its laplacian .",
    "namely , @xmath6 , with @xmath7 representing the set of nodes , @xmath8 denoting the edges , and @xmath9 representing the weighted graph adjacency .",
    "nodes @xmath10 and @xmath11 are connected with an edge @xmath12 iff @xmath13 , where : @xmath14_{ii } \\ \\ \\ \\text{(if $ i = j$ ) } , \\ \\ \\ \\ \\text{or } \\ \\ \\ \\",
    "\\bm{w}_{ij } = -\\left[\\bm{m}_{0}\\right]_{ij } , \\text{otherwise}.\\ ] ] following  @xcite , we seek @xmath15-approximate solutions to @xmath16 , being the exact solution of @xmath17 , defined as :    let @xmath18 be the solution of @xmath19 .",
    "a vector @xmath20 is called an @xmath21 approximate solution , if : @xmath22    the r - hop neighbourhood of node @xmath23 is defined as @xmath24 .",
    "we also make use of the diameter of a graph , @xmath5 , defined as @xmath25 .",
    "a matrix @xmath26 is said to have a sparsity pattern corresponding to the r - hop neighborhood if @xmath27 for all @xmath28 and for all @xmath29 such that @xmath30 .",
    "we will denote the spectral radius of a matrix @xmath31 by @xmath32 , where @xmath33 represents an eigenvalue of the matrix @xmath31 .",
    "furthermore , we will make use of the condition number , @xmath34 of a matrix @xmath31 defined as @xmath35 . in",
    "@xcite it is shown that the condition number of the graph laplacian is at most @xmath36 , where @xmath37 and @xmath38 represent the largest and the smallest edge weights in @xmath5 . finally , the condition number of a sub - matrix of the laplacian is at most @xmath39 , see  @xcite .      for determining the newton direction ,",
    "we propose a fast distributed solver for symmetric diagonally dominant linear equations .",
    "our approach is based on a distributed implementation of the parallel solver of spielman and peng  @xcite . before detailing the parallel solver ,",
    "however , we next provide basic notions and notations required .",
    "the standard splitting of a symmetric matrix @xmath2 is : @xmath40 here , @xmath41 is a diagonal matrix such that @xmath42_{ii } = \\left[\\bm{m}_{0}\\right]_{ii}$ ] for @xmath43 , and @xmath44 representing a non - negative symmetric matrix such that @xmath45_{ij } = -\\left[\\bm{m}_{0}\\right]_{ij}$ ] if @xmath46 , and @xmath47_{ii } = 0 $ ] .",
    "we also define the loewner ordering :    let @xmath48 be the space of @xmath49-symmetric matrices .",
    "the loewner ordering @xmath50 is a partial order on @xmath48 such that @xmath51 if and only if @xmath52 is positive semidefinite .",
    "finally , we define the `` @xmath53 '' operation used in the sequel to come as :    let @xmath54 and @xmath55 be positive semidefinite symmetric matrices",
    ". then @xmath56 if and only iff @xmath57 with @xmath58 meaning @xmath59 is positive semidefinite .",
    "based on the above definitions , the following lemma represents the basic characteristics of the @xmath53 operator :    @xcite[approx_lemma_facts ] let @xmath60 and , @xmath61 be symmetric positive semi definite matrices",
    ". then    1 .",
    "\\(1 ) if @xmath56 , then @xmath62 , ( 2 ) if @xmath56 and @xmath63 , then @xmath64 2 .",
    "\\(3 ) if @xmath56 and @xmath63 , then @xmath64 , ( 4 ) if @xmath65 and @xmath66 , then @xmath67 3 .",
    "\\(5 ) if @xmath54 , and @xmath55 are non singular and @xmath56 , then @xmath68 , ( 6 ) if @xmath56 and @xmath69 is a matrix , then @xmath70    the next lemma shows that good approximations of @xmath71 guarantee good approximated solutions of @xmath17 .",
    "[ lemma_approx_matrix_inverse ] let @xmath72 , and @xmath73 .",
    "then @xmath74 is @xmath75 approximate solution of @xmath17 .",
    "the proof can be found in the appendix .      [",
    "sec : parrallelsolver ] the parallel sddm solver proposed in  @xcite is a parallelized technique for solving the problem of section  [ sec : probdef ] . it makes use of inverse approximated chains ( see definition  [ def : invchain ] ) to determine @xmath74 and can be split in two steps . in the first step , denoted as algorithm  [ algo : inv ] , a `` crude '' approximation , @xmath76 , of @xmath77 is returned .",
    "@xmath76 is driven to the @xmath15-close solution , @xmath74 , using richardson preconditioning in algorithm  [ algo : inv2 ] .",
    "before we proceed , we start with the following two lemmas which enable the definition of inverse chain approximation .    @xcite[sddm_splitting_lemma ] if @xmath78 is an sddm matrix , with @xmath79 being positive diagonal , and @xmath31 denoting a non - negative symmetric matrix , then @xmath80 is also sddm .",
    "@xcite[approx_inverse_formulae_lemma ] let @xmath81 be an sddm matrix , where @xmath79 is positive diagonal and , @xmath31 a symmetric matrix",
    ". then @xmath82.\\end{aligned}\\ ] ]    given the results in lemmas [ sddm_splitting_lemma ] and [ approx_inverse_formulae_lemma ] , we now can consider inverse approximated chains of @xmath83 :    [ def : invchain ] let @xmath84 be a collection of sddm matrices such that @xmath85 , with @xmath86 a positive diagonal matrix , and @xmath87 denoting a non - negative symmetric matrix .",
    "then @xmath88 is an inverse approximated chain if there exists positive real numbers @xmath89 such that : ( 1 ) for @xmath90 : @xmath91 , ( 2 ) @xmath92 , and ( 3 ) @xmath93 .",
    "[ algo : crudeparallel ]    * input * : inverse approximated chain , @xmath94 , and @xmath95 being    * output * : the `` crude '' approximation , @xmath96 , of @xmath16 @xmath97 @xmath98 @xmath99 $ ] * return * @xmath100    the quality of the `` crude '' solution returned by algorithm  [ algo : inv ] is quantified in the following lemma :    @xcite[rude_alg_guarantee_lemma ] let @xmath101 be the inverse approximated chain and denote @xmath102 be the operator defined by @xmath103 , namely , @xmath104 .",
    "then @xmath105    algorithm  [ algo : inv ] returns a `` crude '' solution to @xmath106 . to obtain arbitrary close solutions , spielman _ et .",
    "@xcite introduced the _ preconditioned richardson iterative scheme _",
    ", summarized in algorithm  [ algo : inv2 ] . following their analysis",
    ", lemma  [ exact_alg_guarantee_lemma ] provides the iteration count needed by algorithm  [ algo : inv2 ] to arrive at @xmath74 .    * input * : inverse approximated chain @xmath94 , @xmath95 , and @xmath15 .",
    "* output * : @xmath15 close approximation , @xmath74 , of @xmath107 * initialize * : @xmath108 ; + @xmath109 ( i.e. , algorithm  [ algo : inv ] ) @xmath110 @xmath111 @xmath112 @xmath113 * return * @xmath74    @xcite[exact_alg_guarantee_lemma ] let @xmath114 be an inverse approximated chain such that @xmath115",
    ". then @xmath116 arrives at an @xmath15 close solution of @xmath16 in @xmath117 iterations .",
    "we consider a network represented by a directed graph @xmath118 with node set @xmath119 and edge set @xmath120 .",
    "the flow vector is denoted by @xmath121_{e\\in\\mathcal{e}}$ ] , with @xmath122 representing the flow on edge @xmath123 .",
    "the flow conservation conditions at nodes can be compactly represented as @xmath124 where @xmath31 is the @xmath125 node - edge incidence matrix of @xmath5 defined as    @xmath126    and the vector @xmath127 denotes the external source , i.e. , @xmath128 ( or @xmath129 ) indicates @xmath130 units of external flow enters ( or leaves ) node @xmath131 .",
    "a cost function @xmath132 is associated with each edge @xmath123 .",
    "namely , @xmath133 denotes the cost on edge @xmath123 as a function of the edge flow @xmath122 .",
    "we assume that the cost functions @xmath134 are strictly convex and twice differentiable .",
    "consequently , the minimum cost networks optimization problem can be written as @xmath135    our goal is to investigate newton type methods for solving the problem in  [ eq : optimall ] in a distributed fashion . before diving into these details , however , we next present basic ingredients needed for the remainder of the paper .",
    "the dual subgradient method optimizes the problem in equation  [ eq : optimall ] by descending in the dual domain .",
    "the lagrangian , @xmath136 is given by @xmath137    the dual function @xmath138 is then derived as @xmath139 hence , it can be clearly seen that the evaluation of the dual function @xmath138 decomposes into e one - dimensional optimization problems .",
    "we assume that each of these optimization problems have an optimal solution , which is unique by the strict convexity of the functions @xmath134 . denoting the solutions by @xmath140 and using the first order optimality conditions",
    ", it can be seen that for each edge , e , @xmath141 is given by @xmath142^{-1}\\left(\\lambda^{(i)}-\\lambda^{(j)}\\right),\\ ] ] where @xmath143 and @xmath144 denote the source and destining nodes of edge @xmath145 , respectively ( see  @xcite for details ) .",
    "therefore , for an edge @xmath123 , the evaluation of @xmath141 can be performed based on local information about the edge s cost function and the dual variables of the incident nodes , @xmath131 and @xmath29 .",
    "the dual problem is defined as @xmath146 . since",
    "the dual function is convex , the optimization problem can be solved using gradient descent according to @xmath147 with @xmath148 being the iteration index , and @xmath149 denoting the gradient of the dual function evaluated at @xmath150 .",
    "importantly , the computation of the gradient can be performed as @xmath151 , with @xmath152 being a vector composed of @xmath153 as determined by equation  [ eq : mapback ] .",
    "further , due to the sparsity pattern of the incidence matrix @xmath31 , the @xmath154 element , @xmath155 , of the gradient @xmath156 can be computed as @xmath157    clearly , the algorithm in equation  [ eq : gd ] can be implemented in a distributed fashion , where each node , @xmath131 , maintains information about its dual , @xmath158 , and primal , @xmath153 , iterates of the outgoing edges @xmath145 .",
    "gradient components can then be evaluated as per  [ eq : gddist ] using only local information .",
    "dual variables can then be updated using  [ eq : gd ] .",
    "given the updated dual variables , the primal variables can be computed using  [ eq : mapback ] .",
    "although the distributed implementation avoids the cost and fragility of collecting all information at centralized location , practical applicability of gradient descent is hindered by slow convergence rates .",
    "this motivates the consideration of newton methods discussed next .",
    "newton s method is a descent algorithm along a scaled version of the gradient .",
    "its iterates are typically given by @xmath159 with @xmath160 being the newton direction at iteration @xmath148 , and @xmath161 denoting the step size .",
    "the newton direction satisfies @xmath162 with @xmath163 being the hessian of the dual function at the current iteration @xmath148 .      here , we detail some assumptions needed by our approach .",
    "we also derive essential lemmas quantifying properties of the dual hessian .",
    "the graph , @xmath5 , is connected , non - bipartite and has algebraic connectivity lower bound by a constant @xmath164 .",
    "[ ass : two ] the cost functions , @xmath165 , in equation  [ eq : optimall ] are    1 .",
    "twice continuously differentiable satisfying @xmath166 with @xmath167 and @xmath168 are constants ; and 2 .",
    "lipschitz hessian invertible for all edges @xmath169 @xmath170    the following two lemmas  @xcite quantify essential properties of the dual hessian which we exploit through our algorithm to determine the approximate newton direction .",
    "[ lemma : crap ] the dual objective @xmath171 abides by the following two properties  @xcite :",
    "the dual hessian , @xmath172 , is a weighted laplacian of @xmath5 : @xmath173^{-1}\\bm{a}^{\\mathsf{t}}.\\ ] ] 2 .",
    "the dual hessian @xmath172 is lispshitz continuous with respect to the laplacian norm ( i.e. , @xmath174 ) where @xmath175 is the unweighted laplacian satisfying @xmath176 with @xmath31 being the incidence matrix of @xmath5 .",
    "namely , @xmath177 : @xmath178 with @xmath179 where @xmath180 and @xmath181 denote the largest and second smallest eigenvalues of the laplacian @xmath175",
    ".    see appendix .",
    "the following lemma follows from the above and is needed in the analysis later :    [ lemma : b ] if the dual hessian @xmath172 is lipschitz continuous with respect to the laplacian norm @xmath174 ( i.e. , lemma  [ lemma : crap ] ) , then for any @xmath182 and @xmath183 we have @xmath184    see appendix .    as detailed in  @xcite ,",
    "the exact computation of the inverse of the hessian needed for determining the newton direction can not be attained exactly in a distributed fashion .",
    "authors in  @xcite proposed approximation techniques for computing this direction .",
    "the effectiveness of these algorithms , however , highly depend on the accuracy of such an approximation . in this work ,",
    "we propose a distributed approximator for the newton direction capable of acquiring @xmath15-close solutions for any arbitrary @xmath15 .",
    "our results show that this new algorithm is capable of significantly surpassing others in literature where its performance accurately traces that of the standard centralized newton approach .",
    "next , we detail our distributed sdd solver being at the core of our approximator .",
    "we propose a distributed solver for sddm systems which can be used to determine an approximation to the newton direction up to any arbitrary @xmath0 ( see section  [ sec : newtonapprox ] ) .",
    "our method is based on a distributed implementation of the parallel solver of section  [ parallel : sddm ] .",
    "similar to  @xcite , we first introduce an approximate inverse chain which can be computed in a distributed fashion .",
    "this leads us to a distributed version of the `` crude '' solver ( i.e. , algorithm  [ algo : crudeparallel ] ) . contrary to  @xcite , however , we then generalize the `` crude '' distributed solver to acquire _ exact _ solutions to an sddm system .",
    "for a generic sddm system of linear equations , our main results for determining an @xmath15-close solution ( i.e. , @xmath185 ) is summarized by :    [ lemma : sddm ] for the system of equations represented by @xmath106 , there is a distributed algorithm that uses only r - hop information and computes the @xmath15-close solution , @xmath74 , in @xmath186 time steps , with @xmath187 being the condition number of @xmath2 , @xmath188 representing the upper bound on the size of the r - hop neighborhood , @xmath189 the maximal degree of @xmath5 , and @xmath190 $ ] being the precision parameter .",
    "analogous to  @xcite , we will develop and analyze two distributed solvers for sddm systems ( i.e. , `` crude '' r - hop solver and `` exact '' r - hop solver ) leading to the proof of the above lemma .      algorithm  [ algo : distrhop ] presents the `` crude '' r - hop solver for sddm systems .",
    "each node receives the @xmath191 row of @xmath2 , @xmath191 component , @xmath192_{k}$ ] of @xmath193 , the length of the inverse chain , @xmath194 , and the local communication bound is assumed to be in the order of powers of 2 , i.e. , @xmath195 . ] @xmath196 as inputs , and outputs the @xmath191 component of the `` rude '' approximation of @xmath16 .",
    "* part one : * @xmath197_{k1},\\ldots,[\\bm{a}_0\\bm{d}^{-1}_0]_{kn}\\ } = \\left\\{\\frac{[\\bm{a}_0]_{k1}}{[\\bm{d}_0]_{11}},\\ldots,\\frac{[\\bm{a}_0]_{kn}}{[\\bm{d}_0]_{nn}}\\right\\}$ ] , @xmath198_{k1},\\ldots,[\\bm{d}^{-1}_0\\bm{a}_0]_{kn}\\ } = \\{\\frac{[\\bm{a}_0]_{k1}}{[\\bm{d}_0]_{kk}},\\ldots,\\frac{[\\bm{a}_0]_{kn}}{[\\bm{d}_0]_{kk}}\\}$ ] @xmath199_{k1},\\ldots,[\\bm{c}_0]_{kn } = \\text{comp}_0\\left([\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn } , r\\right)$ ] , @xmath200_{k1},\\ldots,[\\bm{c}_1]_{kn}= \\text{comp}_1\\left([\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn } , r\\right)$ ] + * part two : * * if *  @xmath201 @xmath202_k = [ \\bm{a}_0\\bm{d}^{-1}_0\\bm{b}_{i-1}]_k$ ] @xmath203_k = [ \\bm{a}_0\\bm{d}^{-1}_0\\bm{u}^{(i-1)}_{j-1}]_k$ ] @xmath204_k = [ \\bm{b}_{i-1}]_k + [ \\bm{u}^{(i-1)}_{2^{i-1}}]_k$ ] * if * @xmath205 @xmath206 @xmath202_k = [ \\bm{c}_0\\bm{b}_{i-1}]_k$ ] @xmath203_k = [ \\bm{c}_0\\bm{u}^{(i-1)}_{j-1}]_k$ ] @xmath204_k",
    "= [ \\bm{b}_{i-1}]_k + [ \\bm{u}^{(i-1)}_{l_{i-1}}]_k$ ] + * part three : * @xmath207_k = \\sfrac{[\\bm{b}_d]_k}{[\\bm{d}_0]_{kk}}$ ] * if * @xmath208 @xmath209_k = [ \\bm{d}^{-1}_0\\bm{a}_0\\bm{x}_{i+1}]_k$ ] @xmath210_k = [ \\bm{d}^{-1}_0\\bm{a}_0\\bm{\\eta}^{(i+1)}_{j-1}]_k$ ] @xmath211_k = \\frac{1}{2}\\left[\\frac{[\\bm{b}_i]_k}{[\\bm{d}_{0}]_{kk } } + [ \\bm{x}_{i+1}]_k + [ \\bm{\\eta}^{i + 1}_{2^i}]_{k}\\right]$ ] * if * @xmath212 @xmath213 @xmath209_k = [ \\bm{c}_1\\bm{x}_{i+1}]_k$ ] @xmath210_k = [ \\bm{c}_1\\bm{\\eta}^{(i+1)}_{j-1}]_k$ ] @xmath211_k = \\frac{1}{2}\\left[\\frac{[\\bm{b}_i]_k}{[\\bm{d}_0]_{kk } }   + [ \\bm{x}_{i+1}]_k + [ \\bm{\\eta}^{i+1}_{l_i}]_k \\right]$ ] @xmath214_k = \\frac{1}{2}\\left[\\frac{[\\bm{b}_0]_k}{[\\bm{d}_0]_{kk } } + [ \\bm{x}_1]_k + [ \\bm{d}^{-1}_0\\bm{a}_0\\bm{x}_1]_k \\right]$ ] * return * @xmath214_k$ ]    @xmath215_{kj } = \\sum\\limits_{r:\\bm{v}_r\\in \\mathbb{n}_1(v_j)}\\frac{[\\bm{d}_0]_{rr}}{[\\bm{d}_0]_{jj}}[(\\bm{a}_0\\bm{d}^{-1}_0)^l]_{kr}[\\bm{a}_0\\bm{d}^{-1}_0]_{jr}$ ] * return * @xmath216_{k1},\\ldots,[(\\bm{a}_0\\bm{d}^{-1}_0)^{r}]_{kn } \\}$ ]    @xmath217_{kj } = \\sum\\limits_{r:\\bm{v}_r\\in \\mathbb{n}_1(\\bm{v}_j)}\\frac{[\\bm{d}_0]_{jj}}{[\\bm{d}_0]_{rr}}[(\\bm{d}^{-1}_0\\bm{a}_0)^l]_{kr}[\\bm{d}^{-1}_0\\bm{a}_0]_{jr}$ ] * return * @xmath218_{k1},\\ldots,[(\\bm{d}^{-1}_0\\bm{a}_0)^{r}]_{kn } \\}$ ]    * analysis of algorithm  [ algo : distrhop ] * the following lemma shows that @xmath219 computes the @xmath191 component of the `` crude '' approximation of @xmath16 and provides the algorithm s time complexity    [ r_hop_rude_lemma ] let @xmath220 be the standard splitting and let @xmath221 be the operator defined by @xmath219 , namely , @xmath222 .",
    "then , @xmath223 .",
    "@xmath219 requires @xmath224 , where @xmath225 , to arrive at @xmath76",
    ".    see appendix .",
    "next , we provide the exact r - hop solver .",
    "similar to @xmath219 , each node @xmath226 receives the @xmath191 row @xmath83 , @xmath227_k$ ] , @xmath194 , @xmath196 , and a precision parameter @xmath15 as inputs , and outputs the @xmath191 component of the @xmath15 close approximation of vector @xmath16 .    * initialize * : @xmath228_k = 0 $ ] , and @xmath229_k = \\text{rdistrsolve}(\\{[m_0]_{k1},\\ldots , [ m_0]_{kn}\\ } , [ b_0]_k , d , r)$ ] @xmath230_k = [ \\bm{d}_0]_{kk}[\\bm{y}_{t-1}]_k - \\sum_{j : \\bm{v}_j\\in \\mathbb{n}_{1}(\\bm{v}_k)}[\\bm{a}_{0}]_{kj}[\\bm{y}_{t-1}]_j$ ] @xmath231_k = \\text{rdistrsolve}(\\{[\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , [ \\bm{u}^{(1)}_{t}]_k , d , r)$ ] @xmath232_k = [ \\bm{y}_{t-1}]_k - [ \\bm{u}^{(2)}_{t}]_k + [ \\bm{\\chi}]_k$ ] * end for * * return * @xmath233_k = [ \\bm{y}_q]_k$ ]    * analysis of algorithm  [ algo : edistr ] : * the following lemma shows that @xmath234 computes the @xmath191 component of the @xmath15 close approximation to @xmath16 and provides the time complexity analysis .",
    "[ dist_exact_algorithm_guarantee_lemma ] let @xmath220 be the standard splitting .",
    "further , let @xmath235 .",
    "then algorithm  [ algo : edistr ] requires @xmath236 iterations to return the @xmath191 component of the @xmath15 close approximation to @xmath16 .",
    "see appendix .",
    "next , the following lemma provides the time complexity analysis of @xmath234 .",
    "[ time_complexity_of_distresolve ] let @xmath220 be the standard splitting and let @xmath237 , then @xmath234 requires @xmath238 time steps .",
    "moreover , for each node @xmath226 , @xmath234 only uses information from the r - hop neighbors .    see appendix .    the complexity of the proposed algorithms depend on the length of the inverse approximated chain , @xmath194 . here",
    ", we provide an analysis to determine the value of @xmath194 which guarantees @xmath239 in @xmath240 .",
    "these results are summarized the following lemma    [ eps_d_lemma ] let @xmath220 be the standard splitting and let @xmath241 denote the condition number of @xmath83 . consider the inverse approximated chain @xmath240 with length @xmath242{2}}{\\sqrt[3]{2 } - 1}\\right)\\kappa\\right)\\rceil$ ] , then @xmath243 , with @xmath235 .",
    "see appendix .    combining the above results",
    "finalizes the proof of lemma  [ lemma : sddm ] .",
    "the usage of this distributed solver to approximate the newton direction , as detailed in the next section , enables fast and accurate distributed newton methods capable of approximating centralized newton directions up to any arbitrary @xmath15 .",
    "our approach only requires r - hop communication for the distributed approximation of the newton direction .",
    "given the results of lemma  [ lemma : crap ] , we can determine the approximate newton direction by solving a system of linear equations represented by an sdd matrix according to section  [ sec : sddm ] , with @xmath244 .",
    "formally , we consider the following iteration scheme : @xmath245 with @xmath148 representing the iteration number , @xmath161 the step - size , and @xmath246 denoting the approximate newton direction . we determine @xmath246 by solving @xmath247 using algorithm  [ algo : edistr ] .",
    "it is easy to see that our approximation of the newton direction , @xmath246 , satisfies @xmath248 where @xmath249 approximates @xmath250 according to the routine of algorithm  [ algo : edistr ] .",
    "the accuracy of this approximation is quantified in the following lemma    [ lemma : bla ] let @xmath251 be the hessian of the dual function , then for any arbitrary @xmath0 we have @xmath252    see appendix .",
    "given such an accurate approximation , next we analyze the iteration scheme of our proposed method showing that similar to standard newton methods , we achieve superlinear convergence within a neighborhood of the optimal value .",
    "we start by analyzing the change in the laplacian norm of the gradient between two successive iterations    consider the following iteration scheme @xmath253 with @xmath254 $ ] , then , for any arbitrary @xmath255 , the laplacian norm of the gradient , @xmath256 , follows : @xmath257 with @xmath180 and @xmath181 being the largest and second smallest eigenvalues of @xmath175 , @xmath168 and @xmath167 denoting the upper and lower bounds on the dual s hessian , and @xmath258 is defined in lemma  [ lemma : b ]",
    ".    see appendix .    at this stage , we are ready to present the main results quantifying the convergence phases exhibited by our approach :    let @xmath167 , @xmath168 , @xmath259 be the constants defined in assumption  [ ass : two ] and lemma  [ lemma : crap ] , @xmath180 and @xmath181 representing the largest and second smallest eigenvalues of the normalized laplacian @xmath175 , @xmath260 the precision parameter for the sddm ( section  [ sec : sddm ] ) solver , and letting the optimal step - size parameter @xmath261 . then the proposed algorithm given by the @xmath262 exhibits the following three phases of convergence :    1 .",
    "* strict decreases phase : * while @xmath263 : @xmath264 2 .",
    "* quadratic decrease phase : * while @xmath265 : @xmath266 3 .",
    "* terminal phase : * when @xmath267 : @xmath268}||\\bm{g}_{k}||_{\\mathcal{l}},\\ ] ]    where @xmath269 and @xmath270 , with @xmath271}\\\\\\nonumber \\bm{\\zeta}&=\\frac{b(\\alpha^{*}\\gamma(1+\\epsilon))^{2}}{2\\mu_{2}^{2}(\\mathcal{l})}\\end{aligned}\\ ] ]    we will proof the above theorem by handling each of the cases separately .",
    "we start by considering the case when @xmath272 ( i.e. , * strict decrease phase * ) .",
    "we have : @xmath273 where the last steps holds since @xmath274 . noticing that @xmath275 ( see appendix ) , the only remaining step needed is to evaluate @xmath276 . knowing that @xmath277",
    ", we recognize @xmath278 where the last step follows from the fact that @xmath279 .",
    "therefore , we can write @xmath280||\\bm{g}_{k}||_{\\mathcal{l}}^{2}.\\ ] ] it is easy to see that @xmath281 minimizes the right - hand - side of the above equation . using @xmath282 gives the constant decrement in the dual function between two successive iterations as @xmath264 considering the case when @xmath283 ( i.e. , * quadratic decrease phase * ) , equation  [ eq : normg ] can be rewritten as @xmath284 with @xmath285 and @xmath286 defined as in equation  [ eq : aux ] .",
    "further , noticing that since @xmath287 then @xmath288 .",
    "consequently the quadratic decrease phase is finalized by @xmath289 finally , we handle the case where @xmath290 ( i.e. , * terminal phase * ) .",
    "since @xmath291 , it is easy to see that @xmath292}||\\bm{g}_{k}||_{\\mathcal{l}}.\\end{aligned}\\ ] ]    having proved the three convergence phases of our algorithm , we next analyze the number of iterations needed by each phase .",
    "these results are summarized in the following lemma :    consider the algorithm given by the following iteration protocol : @xmath293 .",
    "let @xmath294 be the initial value of the dual variable , and @xmath295 be the optimal value of the dual function .",
    "then , the number of iterations needed by each of the three phases satisfy :    1 .",
    "the * strict decrease phase * requires the following number iterations to achieve the quadratic phase : @xmath296^{-2},\\ ] ] where @xmath297\\frac{\\gamma^{2}}{\\gamma}$ ] .",
    "the * quadratic decrease phase * requires the following number of iterations to terminate : @xmath298\\right)}{\\log_{2}(r)}\\right],\\ ] ] where @xmath299 , with @xmath300 being the first iteration of the quadratic decrease phase .",
    "the radius of the * terminal phase * is characterized by : @xmath301}{e^{-\\epsilon^{2}\\gamma\\bm{\\delta}}}\\mu_{n}(\\mathcal{l})\\sqrt{\\mu_{2}(\\mathcal{l})}.\\ ] ]    see appendix .    given the above result",
    ", the total message complexity can then be derived as @xmath302 .",
    "we evaluated our approach on two randomly generated networks . the first consisted of 30 nodes and 70 edges , while the second contained 90 nodes with 200 edges .",
    "the edges were chosen uniformly at random .",
    "the flow vectors , @xmath303 , were chosen to place source and sink nodes @xmath304 away from each other .",
    "an @xmath15 of @xmath305 , a gradient threshold of @xmath306 , and an r - hop of 1 were provided to our sddm solver for determining the approximate newton direction .",
    "we compared the performance of our algorithm , referred to sddm - add hereafter , to add , standard gradient descent , and the exact newton method ( i.e. , centralized newton iterations ) .",
    "the values of the primal objective and feasibility were chosen as performance metric .",
    "figure  [ fig : resbenchmark ] shows these convergence metrics comparing sddm - add , to add  @xcite , standard gradient descent , and the exact newton method ( i.e. , centralized newton iteration ) . on relatively small networks , 30 nodes and 70 edges , our approach converges approximately an order of magnitude faster compared to both add and gradient descent as demonstrated in figures  [ fig : perfsm ] and  [ fig : perfcp ] .",
    "it is also clear that on such networks , sddm - add is capable of closely tracing the exact newton method where convergence to the optimal primal objective is achieved after @xmath307 iterations compared to @xmath308 for add and @xmath309 for gradient descent .    in the second set of experiments that goal was to evaluate the performance of sddm - add on large networks where both add and gradient descent underperform .",
    "results reported in figures  [ fig : trajsm ] and  [ fig : trajcp ] on the larger 90 nodes and 200 edges network clearly demonstrate the effectiveness of our approach .",
    "benefiting from the approximation accuracy of the newton direction , sddm - add is capable of significantly outperforming state - of - the - art methods .",
    "as shown in figure  [ fig : trajcp ] convergence to the optimal solution ( as computed by exact newton iterations ) is achieved after 3000 iterations , while add and gradient descent underperform by converging to a primal value of @xmath310 .",
    "in this paper we proposed a fast and accurate distributed newton method for network flow optimization problems .",
    "our approach utilizes the sparsity pattern of the dual hessian to approximate the newton direction using only local information .",
    "we achieve @xmath15-close approximations by proposing a novel distributed solver for symmetric diagonally dominant systems of linear equations involving m - matrices .",
    "our solver provides a distributed implementation of the algorithm of spielam and peng by considering an approximate inverse chain that can be computed in a distributed fashion .",
    "the proposed approximate newton method utilizes the distributed solver to obtain @xmath15-close approximations to the exact newton direction up - to any arbitrary @xmath0 .",
    "we further analyzed the properties of the resulting approximate algorithm showing that , similar to conventional newton methods , superlinear convergence within a neighborhood of the optimal value can be attained . finally , we demonstrated the effectiveness of our method in a set of experiments on randomly generated networks .",
    "results showed that on both small and large networks , our algorithm , outperforms state - of - the - art techniques in a variety of convergence metrics .",
    "possible extensions include applications to network utility maximization  @xcite , general wireless communication optimization problems  @xcite , and stochastic settings  @xcite .",
    "99    s. authuraliya and s. h. low , _ optimization flow control with newton - like algorithm _ , telecommunications systems * 15 * ( 200 ) , 345 - 358 .",
    "bertsekas , _ nonlinear programming _ , athena scientific , cambridge , massachusetts , 1999 .",
    "bertsekas , a. nedic , and a.e .",
    "ozdaglar , _ convex analysis and optimization _ , athena scientific , cambridge , massachusetts , 2003 .",
    "s. boyd and l. vandenberghe , _ convex optimization _ , cambridge university press , cambridge , uk , 2004 .",
    "a. jadbabaie , a. ozdaglar , and m. zargham , _ a distributed newton method for network optimization _ , proceedings of ieee cdc , 2009 .",
    "m. zargham , a. ribeiro , a. ozdaglar , and a. jadbabaie , _ accelerated dual descent for network optimization _ , proceedings of ieee , 2011 .",
    "e. wei , a. ozdaglar , and a. jadbabaie , _ a distributed newton method for network utility maximization _",
    ", lids technical report 2823 ( 2010 ) . j. sun and h. kuo , _ applying a newton method to strictly convex separable network quadratic programs _",
    ", siam journal of optimization , 8 , 1998 .",
    "r. tyrrell rockafellar , _ network flows and monotropic optimization _ , j. wiley & sons , inc . , 1984 .",
    "e. gafni and d. p. bertsekas , _ projected newton methods and optimization of multicommodity flows _ , ieee conference on decision and control ( cdc ) , orlando , fla . , dec .",
    "r. peng , and d. a. spielman , _ an efficient parallel solver for sdd linear systems _ , the 46th annual acm symposium on theory of computing2014 .",
    "a. nedic and a. ozdaglar , _ approximate primal solutions and rate analysis for dual subgradient methods _ , siam journal on optimization , forthcoming ( 2008 ) .",
    "s. low and d.e .",
    "lapsley , _ optimization flow control , i : basic algorithm and convergence , _ ieee / acm transactions on networking * 7 * ( 1999 ) , no . 6 , 861 - 874 .    a. ribeiro and g. b. giannakis , _ separation theorems of wireless networking , _ ieee transactions on information theory ( 2007 ) .",
    "a. ribeiro , _ ergodic stochastic optimization algorithms for wireless communication and networking , _ ieee transactions on signal processing ( 2009 ) .",
    "the complete proofs can be found at : https://db.tt/mbbw15zx"
  ],
  "abstract_text": [
    "<S> dual descent methods are commonly used to solve network flow optimization problems , since their implementation can be distributed over the network . </S>",
    "<S> these algorithms , however , often exhibit slow convergence rates . </S>",
    "<S> approximate newton methods which compute descent directions locally have been proposed as alternatives to accelerate the convergence rates of conventional dual descent . </S>",
    "<S> the effectiveness of these methods , is limited by the accuracy of such approximations . in this paper </S>",
    "<S> , we propose an efficient and accurate distributed second order method for network flow problems . </S>",
    "<S> the proposed approach utilizes the sparsity pattern of the dual hessian to approximate the the newton direction using a novel distributed solver for symmetric diagonally dominant linear equations . </S>",
    "<S> our solver is based on a distributed implementation of a recent parallel solver of spielman and peng ( 2014 ) . </S>",
    "<S> we analyze the properties of the proposed algorithm and show that , similar to conventional newton methods , superlinear convergence within a neighborhood of the optimal value is attained . </S>",
    "<S> we finally demonstrate the effectiveness of the approach in a set of experiments on randomly generated networks . </S>"
  ]
}