{
  "article_text": [
    "the image formation process is an inverse problem that can be modeled as the following linear system @xmath1 where @xmath2 is the non - negative observed data , @xmath3 represents an ideal , undistorted image to be recovered , @xmath4 is a typically ill - conditioned matrix describing the blurring effect , @xmath5 is a known non - negative background radiation and @xmath6 is the noise corrupting the data .",
    "a typical assumption for the matrix @xmath7 is that it has non - negative elements and each row and column has at least one positive entry . because of the ill - conditioning affecting the problem and the presence of noise on the measured data , a trivial approach that seeks the solution of is in general not successful ; thus , alternative strategies must be exploited .",
    "variational approaches to image restoration @xcite suggest to recover the unknown object through iterative schemes suited for the following constrained minimization problem @xmath8 where @xmath9 is a continuously differentiable convex function measuring the difference between the model and the data .",
    "the definition of the function @xmath9 depends on the noise type introduced by the acquisition system .",
    "particularly , in the case of additive white gaussian noise the cost function is characterized by a least squares distance of the form @xmath10 while , when the data are affected by poisson noise , the so - called kullback - leibler ( kl ) divergence is used : @xmath11 where we assume that @xmath12 and @xmath13 , @xmath14 . in both cases , taking into account also the assumptions on @xmath7 , we may observe that @xmath9 is non - negative , convex and coercive on the non - negative orthant , which means that problem has global solutions . moreover , if the equation @xmath15 has only the solution @xmath16 , then @xmath17 is strictly convex , while the same conclusion holds for @xmath18 if the additional condition @xmath19 , @xmath14 , is satisfied @xcite . in these settings",
    ", the strict convexity of @xmath9 implies that the solution of is unique .",
    "+ due to the ill - posedness of the image restoration problem , one is not interested in computing the minimum points of @xmath9 in or because the exact solution of does not provide a sensible estimate of the unknown image .",
    "for this reason , iterative minimization methods are usually exploited to obtain acceptable solutions by arresting the algorithm before convergence through some stopping criteria , as the classic morozov s discrepancy principle in the case of gaussian noise ( see e.g. @xcite ) or some recently proposed strategies for poisson data @xcite .",
    "+ another technique to tackle to this problem requires to exactly solve the following optimization problem @xmath20 where @xmath21 is a regularization term adding a priori information on the solution and @xmath22 is a positive parameter balancing the role of the two objective function components @xmath9 and @xmath21 .",
    "a frequently used function for the regularization term is a smooth approximation of the total variation , also known in the literature as _ hypersurface potential _ ( hs ) , defined as @xcite @xmath23 where the discrete gradient operator @xmath24 is set through the standard finite difference with periodic boundary conditions @xmath25 when @xmath26 and @xmath9 is one of the two considered cost functions , the objective function in is non - negative , strictly convex and coercive on the non - negative orthant @xcite .",
    "it follows that problem has a unique solution .",
    "+ both formulations of the imaging problem require an effective optimization method able to provide a meaningful solution in a reasonable time . among all possible choices ,",
    "first - order methods are particularly suited to deal with this kind of problems for several reasons .",
    "first , due to the large size of the images ( which becomes a crucial issue especially in 3d applications ) , the handling of the hessian matrix is an impractical task .",
    "then , first - order methods are used to quickly achieve solutions with low / medium accuracy , which is a general requirement in imaging problems .",
    "finally , when the optimization scheme is used as iterative regularization method to minimize the cost function , an excessively fast convergence makes the automatic choice of the stopping iteration a crucial issue , since a difference of few iterations from the one providing the best reconstruction can lead to substantial differences in the final images .",
    "+ in this paper we extend to the case of a general scaled gradient projection method @xcite a steplength selection rule recently proposed by fletcher @xcite in the unconstrained optimization framework and we test its effectiveness in image deblurring problems .",
    "this rule is based on the estimate of some eigenvalues of the hessian matrix which , for quadratic problems , can be achieved by means of a lanczos process applied to a certain number of consecutive gradients . since the scheme depends only on these stored gradients , it can be generalized to nonquadratic objective functions , showing very competitive results in several benchmark problems with respect to other first - order and quasi - newton methods .",
    "the extension to scaled gradient projection methods applied to non - negatively constrained problems requires a generalization of the matrix with the last gradients accounting for the presence of both the scaling matrix multiplying the gradient and the projection on the non - negative orthant .",
    "the resulting scheme consists in the storage of a set of scaled gradients ( instead of the usual ones ) in which some components of the gradients themselves are put equal to zero .",
    "our numerical experiments on the non - negative minimization of the ls distance and the kl divergence show that the proposed approach is able to compete with standard gradient methods and other recently proposed schemes , providing in some cases good reconstructions with a significantly lower number of iterations .",
    "+ the plan of the paper is the following : in section [ sec2 ] we recall the features of a scaled gradient projection method and , in particular , of the scaling matrix multiplying the gradient . in section [ sec3 ]",
    "we focus the analysis on the choice of the steplength parameter and we describe state - of - the - art strategies and our proposed rule . in section [ sec4 ] some numerical experiments on small quadratic programming ( qp ) and",
    "image deblurring least - squares problems are presented , while in section [ sec5 ] we address the image deblurring problem with data perturbed with poisson noise also by adding an edge - preserving regularization term in the objective function .",
    "some ideas on a possible generalization of the proposed rule to different constraints are provided in section [ sec6 ] , together with a numerical test on the rudin - osher - fatemi model @xcite .",
    "our conclusions are given in section [ sec7 ] .",
    "a general scaled gradient projection ( sgp ) method @xcite for the solution of @xmath27 with @xmath28 differentiable function , is an iterative algorithm whose @xmath29-th iteration is defined by @xmath30 where    * @xmath31 ; * @xmath32 is the gradient of the objective function at iteration @xmath33 ; * @xmath34 $ ] is a linesearch parameter ensuring a sufficient decrease of the objective function along the descent direction @xmath35 , e.g. by means of an armijo rule @xcite ; * @xmath36 is a positive steplength chosen in a fixed range @xmath37 $ ] , with @xmath38 ; * @xmath39 is a symmetric and positive definite scaling matrix with eigenvalues lying in a fixed positive interval @xmath40 $ ] ; * @xmath41 denotes the projection operator onto the non - negative orthant with respect to the norm induced by the matrix @xmath42 : @xmath43    the boundedness conditions on the steplengths and the eigenvalues of the scaling matrices are necessary to prove the convergence result for this method ( see ( * ? ? ?",
    "* theorem 2.1 ) ) , that we report for completeness .",
    "let @xmath44 and assume that the level set @xmath45 is bounded .",
    "every accumulation point of the sequence @xmath46 generated by the sgp algorithm is a stationary point of .",
    "when sgp is applied to the imaging minimization problem or , the coercivity of the objective function on the non - negative orthant assures that @xmath47 is bounded for any @xmath48 , therefore the sequence generated by sgp is bounded and admits limit points ; the uniqueness of the limit point is ensured when the objective function is strictly convex .",
    "+ in imaging applications , the scaling matrix @xmath39 is usually chosen according to the cost function @xmath9 and the regularization term @xmath21 . following the approach proposed in @xcite ,",
    "if @xmath49 and @xmath50 can be decomposed in the form @xmath51 with @xmath52 and @xmath53 , then a possible scaling matrix is given by @xmath54 where @xmath55 is the componentwise ratio between @xmath56 and @xmath57 .",
    "we remark that the choice of a diagonal scaling matrix is preferable since in this case the projection on the non - negative orthant is straightforward and does not require the solution of a further quadratic subproblem at each iteration .",
    "since in general the imaging matrix @xmath7 has non - negative entries , the gradients of the cost functions in and satisfy the decomposition in @xmath58 where @xmath59 is the vector with all entries equal to 1 . in a similar way , the negative gradient of the regularization term in can be written as in with @xcite @xmath60_{i , j } \\ !",
    "= &   \\frac{x_{i+1,j } + x_{i , j+1}}{\\sqrt{((\\mathcal{d}{\\boldsymbol{x}})_{i , j})_1 ^ 2 \\ ! + \\ !",
    "( ( \\mathcal{d}{\\boldsymbol{x}})_{i , j})_2 ^ 2+\\delta^2 } } \\ !                                + \\ !",
    "\\frac{x_{i , j-1}}{\\sqrt{((\\mathcal{d}{\\boldsymbol{x}})_{i , j-1})_1 ^ 2 \\ ! + \\ !",
    "( ( \\mathcal{d}{\\boldsymbol{x}})_{i , j-1})_2 ^ 2 \\ ! + \\ !",
    "\\delta^2 } } \\\\                                                       & + \\frac{x_{i-1,j}}{\\sqrt{((\\mathcal{d}{\\boldsymbol{x}})_{i-1,j})_1 ^ 2+((\\mathcal{d}{\\boldsymbol{x}})_{i-1,j})_2 ^ 2+\\delta^2}},\\\\ [ v_r^{hs}({\\boldsymbol{x}})]_{i , j } \\ !",
    "= &   \\frac{2x_{i , j}}{\\sqrt{((\\mathcal{d}{\\boldsymbol{x}})_{i , j})_1 ^ 2 \\ ! + \\ !",
    "( ( \\mathcal{d}{\\boldsymbol{x}})_{i , j})_2 ^ 2 \\ ! + \\ !",
    "\\delta^2 } } \\ !                                + \\ !",
    "\\frac{x_{i , j}}{\\sqrt{((\\mathcal{d}{\\boldsymbol{x}})_{i , j-1})_1 ^ 2 \\ ! + \\ ! ( ( \\mathcal{d}{\\boldsymbol{x}})_{i , j-1})_2 ^ 2 \\ ! + \\",
    "! \\delta^2 } } \\\\                                                       & + \\frac{x_{i , j}}{\\sqrt{((\\mathcal{d}{\\boldsymbol{x}})_{i-1,j})_1 ^ 2+((\\mathcal{d}{\\boldsymbol{x}})_{i-1,j})_2 ^ 2+\\delta^2}}.\\end{aligned}\\ ] ] the crucial task of speeding up the convergence of a scaled gradient projection method is generally assigned to the steplength parameter , which will be analyzed in the following section .",
    "once the scaling matrix has been fixed , the steplength parameter @xmath36 is chosen to encode some second order information to improve the converge rate of the scheme .",
    "possible choices are the two rules proposed by barzilai and borwein ( bb ) @xcite for nonscaled gradient methods and extended by bonettini et al @xcite to account for the presence of a scaling matrix @xmath39 .",
    "these rules arise from the approximation of the hessian @xmath61 with the diagonal matrix @xmath62 and by imposing the following quasi - newton properties on @xmath63 : @xmath64 where @xmath65 and @xmath66 .",
    "the resulting values become @xmath67 which reduce to the standard bb rules when @xmath39 is equal to the identity matrix @xmath68 for all @xmath69 ( in the following , we will denote by gp a nonscaled gradient projection method ) .",
    "many other steplength rules have been investigated in the last years ( see @xcite and references therein ) and interesting convergence rate improvements have been observed by exploiting alternating criteria of the two bb rules , as the adaptive barzilai - borwein ( abb ) method @xcite and its generalizations abb@xmath70 and abb@xmath71 provided by frassoldati et al @xcite",
    ". + the aim of this paper is to realize an accelerating strategy for the sgp method through the generalization of a steplength selection rule recently suggested by fletcher @xcite in the unconstrained optimization framework .",
    "for unconstrained minimization problems , theoretical considerations , confirmed by numerical experiments , showed the efficacy of this rule in improving the performances of first - order algorithms exploiting a single bb steplength rule .",
    "this analysis encouraged us to investigate the possibility of extending the fletcher s scheme to the case of constrained optimization in order to use this innovative idea for scaled gradient projection method of the type , particularly in image deblurring applications . the new approach proposed in @xcite",
    "consists of a limited memory scheme defining the steplengths as the inverse of special approximations of the eigenvalue of the hessian @xmath72 .",
    "let us consider a quadratic objective function @xmath73 , where @xmath7 is a symmetric and positive definite matrix .",
    "then the steepest descent method applied to the unconstrained quadratic programming problem @xmath74 assumes the form @xmath75 in particular , the following relation between the gradients holds true : @xmath76 if a limited number @xmath0 of back values of the gradient vectors @xmath77\\ ] ] is stored in memory and the @xmath78 matrix @xmath79 containing the reciprocals of the corresponding last @xmath0 steplengths is considered , @xmath80,\\ ] ] then equations for @xmath81 can be rearranged in the matrix form @xmath82\\gamma.\\ ] ] this equality can be used to rewrite the tridiagonal @xmath83 matrix @xmath84 provided by @xmath0 steps of the lanczos iterative process applied to the matrix @xmath7 with starting vector @xmath85 @xcite . in fact , given an integer @xmath86 , the lanczos process generates orthonormal vectors @xmath87 that define a basis for the krylov sequence @xmath88 and such that the matrix @xmath89 where @xmath90 $ ] , @xmath91 , is tridiagonal .",
    "taking into account equation and that the columns of @xmath92 are in the space generated by the above krylov sequence , we have @xmath93 , where @xmath94 is @xmath95 upper triangular and nonsingular , assuming @xmath92 is full - rank .",
    "it follows from that the tridiagonal matrix @xmath84 can be written as @xmath96\\gamma r^{-1}\\ ] ] and , by introducing the vector @xmath97 , that is the vector that solves the linear system @xmath98 , we obtain @xmath99\\gamma r^{-1}.\\ ] ] the eigenvalues of the tridiagonal matrix @xmath84 , called ritz values , are approximations of @xmath0 eigenvalues of @xmath7 @xcite and , since @xmath7 is the hessian matrix of the objective function @xmath28 , they give some second order information about problem . the steplength selection rule proposed by fletcher consists in exploiting the reciprocal of the @xmath0 ritz values as steplengths in the next @xmath0 iterations .",
    "we refer to @xcite for a detailed motivation of this steplength rule and we focus on the features crucial for the extension of the rule to nonquadratic objective functions and to constrained optimization problems .",
    "first of all we remark that allows one to obtain the matrix @xmath84 by simply exploiting the partially extended cholesky factorization @xmath100 = r^t [ r \\quad { \\boldsymbol{r}}],\\ ] ] without the explicit use of the matrices @xmath101 and @xmath7 .",
    "this is important both for the computational point of view and for the extension to nonquadratic functions . for a general objective function",
    ", @xmath84 is upper hessenberg and the ritz - like values are obtained by computing the eigenvalues of a symmetric and tridiagonal approximation @xmath102 of @xmath84 defined as @xmath103 where @xmath104 and @xmath105 denote the diagonal and the strictly lower triangular parts of a matrix .",
    "possible negative eigenvalues of the resulting matrix are discarded before using this set of steplengths for the next iterations .",
    "several numerical experiments @xcite , for both quadratic and nonquadratic test problems , demonstrate that this new steplength selection rule is able to improve the convergence rate of steepest descent methods with respect to other , often used , possibilities for choosing the steplength .",
    "+ motivated by these promising results and taking into account that the convergence for the scaled gradient projection method is guaranteed for every choice of the steplength in a bounded interval , we tried to exploit the fletcher s steplength selection rule in the algorithms used for constrained optimization . in the extension of the original scheme to the sgp method ,",
    "the main change is the definition of a new matrix @xmath106 that generalizes the matrix @xmath92 in . in particular",
    ", we have to consider two fundamental elements : the presence of the scaling matrix multiplying the gradient direction and the projection onto the feasible set . as concerns the former issue",
    ", we exploit the remark that each scaled gradient iteration can be viewed as a usual gradient iteration applied to a scaled objective function by means of a transformation of variables of the type @xmath107 @xcite , where the notation @xmath108 indicates the square root matrix of @xmath42 . this idea led us to store at each iteration the scaled gradient @xmath109 instead of @xmath110 .",
    "the non - negativity constraint is addressed by looking at the complementarity condition of the kkt optimality criteria @xcite , for which the components of the gradient related to inactive constraints in the solution have to vanish . to this aim",
    ", we emphasized the minimization over these components by storing the vectors @xmath111 whose @xmath112-th entry is given by @xmath113_j & { \\rm{if } } \\",
    "x^{(k)}_j > 0 .",
    "\\end{cases}\\ ] ] driven by the previous considerations , our implementation of fletcher s rule for the constrained case is based on the following choice for the matrix @xmath106 : @xmath114.\\ ] ] as concerns the computational cost of the steplength derivation , each group of @xmath0 iterations ( called _ sweep _ in @xcite ) requires the computation of the @xmath0 scaled gradients @xmath115 and the @xmath83 symmetric matrix @xmath116 , which can be performed with @xmath117 vector - vector products .",
    "since @xmath0 is typically a very small number ( between 3 and 5 ) , the cholesky factorization of @xmath116 and the solution of the linear system @xmath118 are straightforward .",
    "it is worth noting that the computation of either the bb1 or the bb2 steplength for @xmath0 iterations needs 3@xmath0 vector - vector products .",
    "therefore , if we assume for example @xmath119 , then both the generalization of the limited memory approach and each bb steplength can be computed in @xmath120 products , while the computational cost grows up to @xmath121 for any alternating strategy of the two bb rules .",
    "+ in the next sections we present the benefits that can be gained by using the steplength selection rule based on the ritz values adapted to the constrained optimization in the image reconstruction framework .",
    "in this section we report the results of several numerical experiments we carried out on constrained qp problems in order to validate the efficacy of the limited memory selection rule .",
    "first we show few tests on the minimization of a quadratic function of 20 variables , with the analysis of the behaviour of three steplengths when varying some features of the optimization problem . then we present realistic experiments of imaging problems with a comparison of several scaled and nonscaled gradient projection methods .",
    "all the numerical experiments have been performed by means of routines implemented by ourselves in matlab@xmath122 r2010a and run on a pc equipped with a 1.60 ghz intel core i7 in a windows 7 environment .",
    "the aim of this section is to investigate possible dependencies of the results provided by a ( s)gp method with different steplengths on the features of the quadratic problem to be addressed , as the distribution of the eigenvalues of the hessian matrix @xmath7 , the number of active constraints and the condition number .",
    "therefore , we built up some ad hoc tests to evaluate different selection rules for different choices of these parameters of the problem .",
    "in particular , we consider the minimization problem @xmath123 where :    * we chose a vector @xmath124 and we defined the matrix @xmath7 as @xmath125 , where @xmath101 is an orthogonal matrix obtained by a qr factorization of a random matrix ; * we defined randomly the set @xmath126 of @xmath127 active constraints ; * we defined the vector of lagrange multipliers @xmath128 by setting @xmath129 if @xmath130 and @xmath131 if @xmath132 . in a similar way , we defined the solution of the problem @xmath133 by setting @xmath134 if @xmath130 and @xmath135 random in @xmath136 if @xmath132 ; * we defined the vector @xmath137 .",
    "the generalization of the limited memory ( ritz ) steplength to the constrained case has been compared to the abb@xmath70 and bb1 values , where in the former case we used the generalized adaptive alternation rule proposed in @xcite .",
    "for all the three algorithms we exploited both a monotone and a nonmonotone linesearch @xcite to determine the parameter @xmath138 . in the latter case ,",
    "the sufficient decrease at each iteration is evaluated with respect to the maximum of the objective function on the last @xmath139 iterations . in the limited memory rule ,",
    "the number @xmath0 of back stored gradient has been set equal to 3 .",
    "following @xcite , we started by considering @xmath140 and we investigated possible choices of the scaling matrix for the minimization problem .",
    "the number of active constraints has been set equal to 8 .",
    "we remark that , since @xmath7 in our tests has also negative entries , the scaling matrix provided by the splitting of @xmath141 in section 2 is not applicable .",
    "possible scaling matrices are given by :    * the inverse of the diagonal of @xmath7 : @xmath142 , which for the quadratic case is equivalent to apply a nonscaled gradient projection method to a preconditioned version of the minimization problem ; * the scaling matrix proposed by coleman and li @xcite for interior trust region approaches applied to nonlinear minimization problems subject to box constraints : @xmath143 , where @xmath144 if @xmath145 and @xmath146 if @xmath147 ; * the current iteration : @xmath148 .",
    "the diagonal entries of all the scaling matrices have been projected in the range @xmath149 $ ] to guarantee the convergence of the schemes . in order to avoid the dependency of the analysis on the stopping criterion used , in table [ tabg1 ] we reported the number of iterations required by the different algorithms to reach a relative reconstruction error ( rre ) @xmath150 lower than prefixed thresholds ( e.g. , @xmath151 , @xmath152 , @xmath153 ) .",
    "the performances with the trivial scaling matrix @xmath154 are also reported .",
    "+    .numbers of iterations required by sgp equipped with the limited memory ( ritz ) , abb@xmath70 and bb1 steplengths to reach rres lower than @xmath151 , @xmath152 and @xmath153 for different scaling matrices ( see text ) .",
    "the results obtained with a monotone ( @xmath155 ) and nonmonotone ( @xmath139 ) linesearch are reported .",
    "the asterisk denotes the maximum number of iterations allowed .",
    "[ cols=\"^,^,^,^,^,^,^,^ \" , ]      and the minimum value @xmath156 provided by the different methods for the test problem g. ]    the results presented both in table [ tab4 ] and in figure [ f_fmin2 ] confirm the goodness of the suggested limited memory steplength selection scheme for a gradient projection method with respect to standard approaches also for a constrained optimization problem where the feasible set is different from the simple non - negative orthant .",
    "in this paper we considered a first - order method for the minimization of non - negatively constrained optimization problems arising in the image reconstruction field , and we introduced a new strategy for the steplength selection which generalizes a rule recently proposed in the unconstrained optimization framework .",
    "the steplength value is based on the storage of a limited number of consecutive objective function gradients and we showed how it can be extended to account for the presence of both a scaling matrix multiplying the gradient of the objective function and a non - negative constraint on the pixels of the unknown image .",
    "we first tested our rule in the minimization of a quadratic function with different features , and we showed that the limited memory steplength is extremely competitive with respect to state - of - the - art bb - like choices .",
    "similar conclusions can be drawn by the numerical experiments we carried out on image reconstruction problems where the measured images are affected by either gaussian or poisson noise .",
    "a final test on the rof model showed the potentiality of the proposed rule also in optimization problems with different constraints .",
    "+ thanks to the significant reduction of the iterations achievable by the proposed steplength , in our future work we will consider the application of our new scheme to real - world imaging problems , as the reconstruction of x - ray images of solar flares starting from the emitted radiation @xcite and the deblurring of conventional stimulated emission depletion ( sted ) microscopy images of sub - cellular structures in fixed cells @xcite .",
    "moreover , the proposed rule will be tested also within a sgp method where the sequence of scaling matrices converges to the identity , since in this case strong convergence results have been recently proved under mild convexity assumptions @xcite .",
    "this work has been partially supported by the italian spinner 2013 phd project `` high - complexity inverse problems in biomedical applications and social systems '' and by miur ( italian ministry for university and research ) , under the projects firb - futuro in ricerca 2012 , contract rbfr12m3ac , and prin 2012 , contract 2012mte38n .",
    "the italian gncs - indam ( gruppo nazionale per il calcolo scientifico - istituto nazionale di alta matematica ) is also acknowledged .",
    "40 acar , r. , vogel , c.r . :",
    "analysis of bounded variation penalty methods for ill - posed problems .",
    "inverse probl .",
    "10(6 ) , 12171229 ( 2004 ) bardsley , j.m . ,",
    "goldes , j. : regularization parameter selection methods for ill - posed poisson maximum likelihood estimation .",
    "inverse probl .",
    "25(9 ) , 095005 ( 2009 ) barzilai , j. , borwein , j.m . :",
    "two - point step size gradient methods .",
    "i m a j. numer .",
    "8(1 ) , 141148 ( 1988 ) bertero , m. , boccacci , p. , talenti , g. , zanella , r. , zanni , l. : a discrepancy principle for poisson data .",
    "inverse probl .",
    "26(10 ) , 105004 ( 2010 ) bertero , m. , lantri , h. , zanni , l. : iterative image reconstruction : a point of view . in : censor , y. , jiang , m. , louis , a.k .",
    "mathematical methods in biomedical imaging and intensity - modulated radiation therapy , pp .",
    "edizioni della normale , pisa ( 2008 ) bertsekas , d. : nonlinear programming .",
    "athena scientific , belmont ( 1999 ) bertsekas , d. : convex optimization theory .",
    "supplementary chapter 6 on convex optimization algorithms , 2 december 2013 edn .",
    "athena scientific , belmont ( 2009 ) birgin , e.g. , martinez , j.m .",
    ", raydan , m. : inexact spectral projected gradient methods on convex sets .",
    "i m a j. numer .",
    "23(4 ) , 539559 ( 2003 ) bonettini , s. , landi , g. , loli piccolomini , e. , zanni , l. : scaling techniques for gradient projection - type methods in astronomical image deblurring .",
    "j. comput .",
    "90(1 ) , 929 ( 2013 ) bonettini , s. , prato , m. : nonnegative image reconstruction from sparse fourier data : a new deconvolution algorithm .",
    "inverse probl .",
    "26(9 ) , 095001 ( 2010 ) bonettini , s. , prato , m. : accelerated gradient methods for the x - ray imaging of solar flares .",
    "inverse probl .",
    "30(5 ) , 055004 ( 2014 ) bonettini , s. , prato , m. : a new general framework for gradient projection methods .",
    "arxiv e - prints , 1406.6601 ( 2014 ) bonettini , s. , ruggiero , v. : an alternating extragradient method for total variation based image restoration from poisson data .",
    "inverse probl .",
    "27(9 ) , 095001 ( 2011 ) bonettini , s. , ruggiero , v. : on the convergence of primal - dual hybrid gradient algorithms for total variation image restoration . j. math .",
    "imaging vis .",
    "44(3 ) , 236253 ( 2012 ) bonettini , s. , zanella , r. , zanni , l. : a scaled gradient projection method for constrained image deblurring .",
    "inverse probl .",
    "25(1 ) , 015002 ( 2009 ) carlavan m. , blanc - fraud l. : regularizing parameter estimation for poisson noisy image restoration . international icst workshop on new computational methods for inverse problems , may 2011 , paris , france .",
    "chambolle , a. : an algorithm for total variation minimization and applications .",
    "imaging vis .",
    "20(12 ) , 8997 ( 2004 ) chambolle , a. , pock , t. : a first - order primal - dual algorithm for convex problems with applications to imaging . j. math .",
    "imaging vis .",
    "40(1 ) , 120145 ( 2011 ) coleman , t.f . , li , y. : an interior trust region approach for nonlinear minimization subject to bounds .",
    "siam j. optim .",
    "6(2 ) , 418445 ( 1996 ) cornelio , a. , porta , f. , prato , m. , zanni , l. : on the filtering effect of iterative regularization algorithms for discrete inverse problems .",
    "inverse probl .",
    "29(12 ) , 125013 ( 2013 ) dai , y.h . , yuan , y.x . : alternate minimization gradient method .",
    "i m a j. numer .",
    "23(3 ) , 377393 ( 2003 ) daube - witherspoon , m.e . ,",
    "muehllener , g. : an iterative image space reconstruction algorithm suitable for volume ect .",
    "ieee t. med .",
    "imaging 5(2 ) , 6166 ( 1986 ) de asmundis , r. , di serafino , d. , riccio , f. , toraldo , g. : on spectral properties of steepest descent methods .",
    "i m a j. numer .",
    "33(4 ) , 14161435 ( 2013 ) de asmundis , r. , di serafino , d. , hager , w.w . , toraldo , g. , zhang , h. : an efficient gradient method using the yuan steplength .",
    "59(3 ) , 541563 ( 2014 ) fletcher , r. : a limited memory steepest descent method . math . program .",
    "135(12 ) , 413436 ( 2012 ) frassoldati , g. , zanghirati , g. , zanni , l. : new adaptive stepsize selections in gradient methods . j. ind . manage",
    "4(2 ) , 299312 ( 2008 ) golub , g.h . , van loan , c.f . : matrix computations , 3rd edn .",
    "john hopkins university press , baltimore ( 1996 ) grippo , l. , lampariello , f. , lucidi , s. : a nonmonotone line search technique for newton s method .",
    "siam j. numer .",
    "23(4 ) , 707716 ( 1986 ) hansen , p.c . : rank - deficient and discrete ill - posed problems .",
    "siam , philadelphia ( 1997 ) hansen , p.c . ,",
    "nagy , j.g . ,",
    "oleary , d.p . : deblurring images : matrices , spectra and filtering .",
    "siam , philadelphia ( 2006 ) harmany , z.t .",
    ", marcia , r.f . , willett , r.m .",
    ": this is spiral - tap : sparse poisson intensity reconstruction algorithms  theory and practice . ieee t. image process .",
    "3(21 ) , 10841096 ( 2012 ) lantri , h. , roche , m. , aime , c. : penalized maximum likelihood image restoration with positivity constraints : multiplicative algorithms .",
    "inverse probl .",
    "18(5 ) , 13971419 ( 2002 ) lantri , h. , roche , m. , cuevas , o. , aime , c. : a general method to devise maximum likelihood signal restoration multiplicative algorithms with non - negativity constraints . signal process .",
    "81(5 ) , 945974 ( 2001 ) lucy , l. : an iterative technique for the rectification of observed distributions .",
    "j. 79(6 ) , 745754 ( 1974 ) nocedal , j. , wright , s.j . : numerical optimization , 2nd edn .",
    "springer , new york ( 2006 ) porta , f. , zanella , r. , zanghirati , g. , zanni , l. : limited - memory scaled gradient projection methods for real - time image deconvolution in microscopy . commun .",
    "nonlinear sci .",
    "21 , 112127 ( 2015 ) prato , m. , cavicchioli , r. , zanni , l. , boccacci , p. , bertero , m. : efficient deconvolution methods for astronomical imaging : algorithms and idl - gpu codes .",
    "539 , a133 ( 2012 ) prato , m. , la camera , a. , bonettini , s. , bertero , m. : a convergent blind deconvolution method for post - adaptive - optics astronomical imaging .",
    "inverse probl .",
    "29(6 ) , 065017 ( 2013 ) richardson , w.h . :",
    "bayesian based iterative method of image restoration .",
    "62(1 ) , 5559 ( 1972 ) rudin , l. , osher , s. , fatemi , e. : nonlinear total variation based noise removal algorithms .",
    "physica d 60(14 ) , 259268 ( 1992 ) ruggiero , v. , zanni , l. : a modified projection algorithm for large strictly - convex quadratic programs .",
    "j. optimiz .",
    "theory app .",
    "104(2 ) , 281299 ( 2000 ) setzer , s. , steidl , g. , teuber , t. : deblurring poissonian images by split bregman techniques",
    ". j. vis . commun .",
    "image r. 21(3 ) , 193199 ( 2010 ) vogel , c.r .",
    ": computational methods for inverse problems .",
    "siam , philadelphia ( 2002 ) yuan , y. : a new stepsize for the steepest descent method . j. comp .",
    "24 , 149156 ( 2006 ) zanella , r. , boccacci , p. , zanni , l. , bertero , m. : efficient gradient projection methods for edge - preserving removal of poisson noise .",
    "inverse probl .",
    "25(4 ) , 045010 ( 2009 ) zanella , r. , zanghirati , g. , cavicchioli , r. , zanni , l. , boccacci , p. , bertero , m. , vicidomini , g. : towards real - time image deconvolution : application to confocal and sted microscopy .",
    "rep . 3 , 2523 ( 2013 ) zhou , b. , gao , l. , dai , y.h .",
    ": gradient methods with adaptive step - sizes .",
    "35(1 ) , 6986 ( 2006 ) zhu , m. , wright , s.j . , chan , t.f . : duality - based algorithms for total - variation - regularized image restoration . comput",
    "47(3 ) , 377400 ( 2008 )"
  ],
  "abstract_text": [
    "<S> gradient methods are frequently used in large scale image deblurring problems since they avoid the onerous computation of the hessian matrix of the objective function . </S>",
    "<S> second order information is typically sought by a clever choice of the steplength parameter defining the descent direction , as in the case of the well - known barzilai and borwein rules . in a recent paper , </S>",
    "<S> a strategy for the steplength selection approximating the inverse of some eigenvalues of the hessian matrix has been proposed for gradient methods applied to unconstrained minimization problems . in the quadratic case , </S>",
    "<S> this approach is based on a lanczos process applied every @xmath0 iterations to the matrix of the gradients computed in the previous @xmath0 iterations , but the idea can be extended to a general objective function . in this paper </S>",
    "<S> we extend this rule to the case of scaled gradient projection methods applied to constrained minimization problems , and we test the effectiveness of the proposed strategy in image deblurring problems in both the presence and the absence of an explicit edge - preserving regularization term . </S>"
  ]
}