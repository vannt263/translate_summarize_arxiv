{
  "article_text": [
    "the ape research group@xcite has traditionally focused on the design and development of custom silicon , electronics and software optimized for lattice qcd ( lqcd ) .",
    "recent works in lqcd numerical application area @xcite have shown an increasing interest on clusters of commodity pc s .",
    "this is mainly due to two facts : good sustained performance of commodity processors on numerical applications and slowly emerging low latency , high bandwidth network interconnects .",
    "this paper describes apenet , a 3d network of point - to - point , low - latency , high - bandwidth links well suited for medium sized clusters running numerical applications .",
    "apenet is a 3d network of point - to - point links with toroidal boundary conditions .",
    "each processing element ( pe ) , in our case a cluster node , has 6 full - duplex communication channels ( @xmath0 , @xmath1 , @xmath2 , @xmath3 , @xmath4 , @xmath5 ) .",
    "data are transmitted in packets which are routed to the destination pe according to simple  and software overridable  rules .",
    "packet delivery is always guaranteed : trasmission is delayed until the receiver has enough room in its receive buffers .",
    "no external routing device is necessary : _ next - neighbour _ and longer distance communications are obtained efficently hopping until the destination pe is reached , without penalties for in - between pes .",
    "latency is kept to the minimum thanks to a lightweight low level protocol  just two 64bit words for the header and the footer ,  and to the cut - through architecture of the switching device . within 10 clock cycles from the arrival of the header ,",
    "the receiving channel starts forwarding the packet along its path , either toward local buffers  for packets intended for that very pe  or toward the proper trasmitting channel  for packets which hop away .",
    "      the building block of the apenet implementation is the apelink card , shown in fig .",
    "[ fig : foto ]    [ t ]     the apelink is a pci - x 133mhz 64bit card which uses an altera s stratix device , a last generation fpga , as a the network device controller , and six pairs of serializers / deserializers from national semiconductors as physical link interfaces .",
    "[ bht ]    the apelink card , see fig .",
    "[ fig : apelink ] , is composed of three major functional blocks . each block has its own clock domain and all data communications between these blocks are based on dual clock fifos , which guarantee robustness of the hardware itself .",
    "the first block is the pci - x interface , which handles the communication with the host pci - x bus ; the second block , called _ crossbar switch _ , controls the data flow among the pci - x channel and the remote communication _ links _ ; the third block implements six remote communication bi - directional _",
    "links_.      the main programming interface is a proprietary simple library ( ` apelib ` ) of c functions , including synchronous , asyncronous and basic collective functions .",
    "it relies on the apelink driver , a linux device driver fully multi processor - aware ( smp ) and supporting versions 2.4 and 2.6 of the linux kernel .",
    "we are developing both an mpi implementation based on lam - mpi and a network device driver , which allows simple ip protocol traffic to be routed on the apenet .",
    "the ` apelib ` is targeted for numerical application code and includes basic primitives such as @xmath6 , @xmath7 , @xmath8 , and some collective functions ( @xmath9 , @xmath10 ) .",
    "the @xmath8 primitive squeezes the best performances from our architecture , as it asymptotically exercises two channels at once , incrementing the aggregated bandwidth .",
    "in this section we report some preliminary low - level benchmark results , obtained on apelink early prototypes .    benchmarks were performed on some dual intel xeon pc s , with both serverworks gc - le and intel e7501 chipsets .",
    "the pc s are connected in a small ring topology .",
    "the apelink channel speed is currently kept at 100 mhz with a peak performance of 508 mb / s per link while the pci - x interface runs at 133mhz .",
    "the benchmark performs a `` ping - pong '' data transfer ( unidirectional and bi - directional ) between two adjacent pe . in the unidirectional test",
    ", one pe sends a message to a remote pe then blocks on receiving a response .",
    "the second pe receives the full message and sends back the same amount of data .",
    "half round - trip time , averaged on a number of iterations , is defined as the latency , i.e. the message transfer time .",
    "[ t ]     from the same test we have estimated the sustained bandwidth .",
    "the bi - directional test differs from the unidirectional one since both pes send data simultaneously using the @xmath8 function .",
    "[ htb ]     in fig .",
    "[ fig - latency ] we plot the latency for message sizes ranging from 16 to 16k bytes .",
    "the smallest message size is 16 as the minimum packet payload is a 128bit word .",
    "the estimated latency is @xmath11 and is constant up to 256 bytes size message . for 4096 bytes messages we measure @xmath12 which is quite good and pretty similar to commercial interconnects@xcite .",
    "[ fig - bandwidth ] shows the bandwidth plot with message sizes ranging from 16 bytes to 1 mb .",
    "the bi - directional zero - copy bandwidth saturates at 677 mb / s . at 1",
    "mb message size the uni - directional bandwidth is 470mb / s , roughly @xmath13 of the channel peak performance at 100mhz .",
    "the plot shows two pairs of curves : those marked _",
    "zero - copy _ refer to the use of pinned - down memory , suitable to be used for pci dma transfers . this way the overhead of expensive memory copy operations to / from dma memory buffers",
    "are avoided .",
    "non _ zero - copy _ data are reported only to simplify the discussion .",
    "the hardware design of the apelink card is completed and we are running tests on the final release of the board whose link channels run at full speed ( 133mhz ) .",
    "preliminary benchmarks have shown encouraging results , comparable with commercial network interconnects . the apelink software is currently in fast progress : current activities focus on a better low level driver a mpi implementation .",
    "the infn prototype apenet pc cluster , composed of 16 pc s equipped with apelink boards , is ready to be used on lqcd test codes .",
    "we have plans to expand it up to 64 pc s ( @xmath14 topology ) in the near future ."
  ],
  "abstract_text": [
    "<S> developed by the ape group , apenet is a new high speed , low latency , 3-dimensional interconnect architecture optimized for pc clusters running lqcd - like numerical applications . </S>",
    "<S> the hardware implementation is based on a single pci - x 133mhz network interface card hosting six independent bi - directional channels with a peak bandwidth of 676 mb / s each direction . </S>",
    "<S> we discuss preliminary benchmark results showing exciting performances similar or better than those found in high - end commercial network systems . </S>"
  ]
}