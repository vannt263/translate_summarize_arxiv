{
  "article_text": [
    "artificial neural networks , especially convolutional neural networks ( cnn ) @xcite have proven to be powerful , flexible models @xcite for a variety of complex tasks , particularly in computer vision @xcite . by composing sequences of simple operations ( layers ) into complex structures ( networks ) and then subsequently estimating all network parameters through gradient backpropagation @xcite , they are able to learn representations of complex phenomena , arguably , better than contemporary alternatives .    despite their widespread adoption and success , these complex networks present two core challenges that often make them seem like a black - art to work with  @xcite .",
    "first , they are indeed complex structures and hence resist theoretical analysis : the few known results @xcite do little to explain what is actually being learned or how to improve a certain network .",
    "second , these complex networks have millions of parameters and hence require a large amount of supervised data to learn , such as coco @xcite , imagenet @xcite or youtube-8 m @xcite which take many person - years to build .",
    "this need limits the artificial neural network extensibility to situations without large - scale data .",
    "although some efforts toward self - supervision  @xcite , which have potential to mitigate this issue , have begun , abundant supervision remains a core limitation .    on",
    "the contrary , sparse representation models  @xcite , like dictionary learning through elastic net problem @xcite , afford strong theoretical analysis , indeed often rigorous theoretical analysis resulting in analytical confidence when used .",
    "they are potentially better matched to computer vision , which has many inherently sparse problems .",
    "furthermore , due to their structure , they tend to need significantly less training data than artificial neural networks . and , although they have seen success in vision , both in generative @xcite discriminative @xcite forms , at the low- @xcite and high - levels @xcite , they have not been able to keep pace with cnns in large - scale or high - dimensional problems @xcite .",
    "this strong contrast between artificial networks and sparse representations is the foundation for our work .",
    "we seek to bridge between these two modeling approaches yielding a representation that remains as general as these two are , but has better scalability than classical sparse representation learning and , at the same time , requires less labeled data than modern artificial neural networks .    indeed , forging this bridge has been considered by others already .",
    "for example , greedy deep dictionary learning @xcite sequentially encodes the data through a series of sparse factorizations .",
    "while interesting , this approach is neither end - to - end , nor does it afford a supervised loss function , limiting its potential in many vision problems .",
    "convolutional sparse coding @xcite derives a convolution form of sparse representations , but it is not yet known how to embed these into artificial neural networks . on the other hand",
    ", rectified linear units @xcite induce a sparsity in activation , and have improved performance .",
    "however , they provide no means for tuning the level of sparsity . summarily , although there has been work in forging this bridge , no approach we are aware of successfully unifies sparse representation modeling and artificial neural networks with a controllable level of sparsity and a capability to move between supervised and unsupervised loss ( see section [ sec : related ] for more discussion ) .    in this paper , we take a step toward bridging these two disparate fields .",
    "the core novelty in our work is two network layers that do sparse representation learning , or dictionary learning , directly within the network .",
    "we call them sparse factorization layers because they indeed factorize the input into its sparse reconstruction coefficients , which are then passed onward to later layers in the network  sparse activation .",
    "we propose both a _ sparse factorization layer _ ( sf ) that learns the dictionary over all of its input terms , analogous to a fully connected layer , and a _ convolutional sparse factorization layer _",
    "( csf ) that slides the dictionary over the input analogous ( in spirit ) to a convolutional layer in a cnn .",
    "we show how to compute the back - propagation gradients for the parameters ( dictionaries ) of both layers , leveraging theoretical results on the local linearity of dictionaries @xcite . hence the two layers can be incorporated and trained directly within modern networks .",
    "furthermore , we propose a new semisupervised loss that leverages the generative nature of the sparse factorization layers .",
    "the * main contribution * of our paper is these two sparse factorization layers that can be plugged into modern convolutional neural networks , trained in an end - to - end manner , and facilitate tunable activation sparsity .",
    "a secondary contribution of the paper is a semisupervised loss function that combines the generative nature of the sparse factorization layers into the discriminative convolutional network .",
    "we demonstrate the empirical utility of the sf and csf layers as replacements for traditional cnn layers .",
    "it is beyond the scope of this paper to evaluate all of our stated goals of bridging these two disparate fields .",
    "so , we focus on the use of these two sparse factorization layers with limited data in comparison to a classical convolutional neural network . on the digit classification problem with mnist , comparable networks with sf and csf layers replacing their analogs in the original cnn perform on par with the cnn",
    ".    however , when we reduce the amount of training data to @xmath0 of the original size ( 10 samples per class ) , the performance of our models are @xmath1 and @xmath2 higher than the cnn , for the sparse factorization and convolutional sparse factorization layers , respectively .",
    "as expected the relative performance improvement of our sparse factorization layers decreases with respect to cnn performance as the amount training data increases .",
    "we also show marked improvement ( @xmath3 absolute accuracy ) in performance on one of the mnist variations dataset .",
    "section [ sec : experiments ] has full results .",
    "we first focus our discussion on methods seeking to bridge between artificial neural networks and sparse representation modeling .",
    "greedy deep dictionary learning @xcite involves a layered , generative architecture that seeks to encode inputs through a series of sparse factorizations , which are trained individually to reconstruct the input faithfully .",
    "it , however , has no mechanism for end - to - end training and is unsupervised , limiting its potential in various tasks like classification , given modern performance statistics .",
    "sparse autoencoders @xcite do the same using linear operations and sparsifying transforms in place of sparse factorization .",
    "although these models can be trained in an end - to - end manner , we are not aware of any work that incorporates them into supervised or semisupervised tasks . in contrast , the sf layer that we propose subsumes these capabilities , can be trained end - to - end and can be embedded in supervised , semisupervised and unsupervised networks .    recent work in _ truly _ convolutional sparse coding @xcite strives to produce a sparse representation of an image by encoding the image spatially as a whole , instead of in isolated patches , which has been the standard way of incorporating sparse representations into image analysis .",
    "this method is similar to our proposed convolutional sparse factorization layer : although they embed the sparse coding operation in the convolution , we are convolutional only in spirit , choosing to map the sparse factorization approach over every patch in the image .",
    "although this work has promise and could potentially even be used to extend the ideas in our paper , we are not aware of a result indicating that they can be incorporating into artificial neural networks , like our methods .",
    "lastly , hierarchical and multi - layered sparse coding of images @xcite has been performed both in a greedy fashion and end - to - end fashion .",
    "however , these typically involve heuristics and lack a principled calculus to embed them within the artificial neural network framework .",
    "they hence lack compatibility with traditional cnn components .",
    "in contrast , both of our new layers can be dropped - in to various artificial ( and convolutional ) neural networks and trained in the same architecture with back - propagation .",
    "[ [ sparsity - in - cnns ] ] sparsity in cnns + + + + + + + + + + + + + + + +    we relate our work to past methods that directly induce sparsity into artificial neural networks .",
    "various methods @xcite have indirectly induced sparsity in the parameters or activations by incorporating sparsity - inducing penalties into network loss functions .",
    "techniques like dropout @xcite and dropconnect @xcite artificially simulate activation sparsity during inference to prevent redundancy and improve performance .",
    "glorot et al .",
    "@xcite argue for the use of the rectified linear unit ( relu ) , a sparsifying activation function , to improve performance in cnns .",
    "this works primarily by limiting the sensitivity of the activations to small changes in the input , thereby forming a more robust and error - tolerant network .",
    "however , these methods either lack theoretical basis , offer no means for controlling the activations sparsity , and/or require that all nonzero activations are positive , unlike our work . the rest of the paper is organized as follows . in section [ sec : background ]",
    "we lay the notational and modeling foundation for the paper .",
    "section [ sec : sparsemain ] introduces sparse factorization and the sparse factorization network layers .",
    "section [ sec : experiments ] describes our experimental evaluation of our work , and finally , section [ sec : conclusion ] summarizes our work and discusses its limitations and future directions .",
    "[ [ notation ] ] notation + + + + + + + +    we have made an effort to maintain consistent notation throughout the paper , sometimes at the expense of deviating from the common notation in existing literature , especially that in dictionary learning . to that end , we use bold lower - case letters , such as @xmath4 and @xmath5 to represent tensors that are typically vectors  these are , e.g. , inputs and outputs of neural network layers or sparse representation ( coding ) coefficients . we use bold upper - case symbols , such as @xmath6 , to denote tensors that act as operators , such as weight matrices in neural networks and dictionaries in sparse representations ( these are generally the parameters we seek to learn ) .",
    "greek letters , such as @xmath7 , are used for scalars .",
    "[ [ convolutional - neural - networks ] ] convolutional neural networks + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    artificial neural networks are systems constructed by the composition of potentially many simple , differentiable operations .",
    "let @xmath8 denote the input to the network , which is generally a tensor of some order , such as an image .",
    "the first operation , or layer , denoted @xmath9 , takes input @xmath4 and parameters @xmath10 to produce an output , or activation , @xmath11 , which serves as input to the second layer .",
    "the process repeats until we reach the activation of the @xmath12th layer of the network , @xmath13 . for readability",
    ", we use the term activation to refer to the output of each layer in the network , not only those layers commonly called _ activation functions _ like the sigmoid or tanh layers .    despite the enormous flexibility of such models ,",
    "only a small variety of layers are commonly used ; most notably , _ linear transform layers _ ( @xmath14 , for vector @xmath4 and parameter ( weight ) matrix @xmath6 ) , _ convolution layers _ ( @xmath15 , for tensor @xmath4 , kernel bank @xmath6 , and offset @xmath16 ) , and parameter - free transforms like _ local pooling _ , _ rectification _ , and _ sigmoid _ activation .",
    "cnns are networks with one or more convolution layers .    for",
    "any such supervised network , a loss function @xmath17 compares the final activation to some supervision information @xmath18 about the input @xmath4 to measure , for example , classification correctness , or some other measure of output quality .",
    "the chain rule is then employed to calculate the gradient of this loss with respect to each intermediate activation : @xmath19 at each step of the backward computation ( the right column in ( [ eq : cnn_model ] ) ) , the gradient @xmath20 is also computed : @xmath21 with these calculated , gradient descent on the network parameters is used to shrink the overall loss .",
    "the @xmath22 denotes supervised loss ; we will later incorporate an unsupervised loss into the formulation and denote it by @xmath23 .",
    "it should be noted that networks are not limited to a single chain of operations , but we restrict our discussion to such for simplicity and without loss of generality .",
    "additionally , our use of the notation @xmath24 is purely symbolic , as the parameters @xmath6 may be a scalar , vector , matrix , or a collection thereof  or missing altogether .",
    "sparse representation models like dictionary learning  @xcite decompose , or factorize , a signal into the linear combination of just a few elements from an overcomplete basis , commonly called a dictionary .",
    "in contrast , common layers in neural networks , such as the linear transform or fully - connected layer , compose the output from the parameter matrix and the input . concretely , consider such a linear transform layer : @xmath25 for input vector @xmath4 and parameters @xmath6 .",
    "the ( forward ) process of sparse factorization is significantly different , involving the solution to an unconstrained optimization problem : @xmath26 where the @xmath27-norm is a sparsity - inducing regularizer , and the ( typically miniscule ) @xmath28 penalty on @xmath5 is for numerical stability of the optimization algorithm and the @xmath7 terms are scalar weights on them .",
    "this formulation is referred to as the _ elastic - net problem _ @xcite and is one variant of many sparse representation formulations ; our consideration here is general .",
    "careful inspection indicates the linear transform takes a dense combination of columns of the parameter matrix @xmath6 weighted by the input @xmath4 and outputs the results .",
    "however , the sparse representation instead computes the best fit of the input @xmath4 with respect to a sparse combination of columns of the parameter matrix @xmath6 and then returns the weights of this sparse combination .",
    "in essence , it factorizes @xmath4 as a function of @xmath6 leading to sparse activations ; hence the name of our methods .",
    "figure [ fig : visual_ltvsf ] provides an illustration of this comparison .",
    "sparse activations are fundamentally different than the common sparsity inducing regularization used in current neural network training ( see section [ sec : related ] for more discussion on this relationship ) .",
    "this difference leads to new neural network layers that are based in sparse representation modeling , yielding sparse activations , strong theoretical guarantees , and the promise of training with less labeled data .",
    "indeed , this work builds on the large body of work in sparse representation modeling and dictionary learning that has been prevalent in computer vision @xcite .",
    "denotes matrix multiplication . ]",
    "we next show how to create two novel artificial neural network layers based on this core idea of sparse factorization as well as the strong theoretical foundation on which it is based ( section [ sec : sparse : theory ] ) .",
    "examples of the two new layers in comparison to a baseline network based off of lenet @xcite is included in figure [ fig : networks ] .",
    "we also describe a new semi - supervised loss that leverages the generative nature of sparse representations to regularize the classification loss ( section [ sec : semisupervision ] ) . in section [ sec",
    ": experiments ] , we describe the results of our experiments using these novel sparse factorization layers , which demonstrate a large gain in performance for limited training data .          the sparse factorization ( sf ) layer directly implements this sparse representation idea as a new layer that can be plugged into any artificial neural network .",
    "parameter - for - parameter , the sf layer can replace a linear transform or fully - connected layer in current networks ( indeed , we do this in our experiments to make a direct comparison , section [ sec : experiments ] ) .",
    "forward computation is straightforward , as we will explain , and backward computation leads to the estimation of the parameter matrix ( the dictionary ) within the gradient descent optimization regime .",
    "concretely , a sf layer is parameterized by a dictionary @xmath29 , which we assume is known for the forward pass .",
    "the dictionary , or parameter matrix , represents the overcomplete basis into which we will factorize the input @xmath30 @xmath31 where the output activation @xmath32 is sparse for correctly chosen values of @xmath33 , due to the @xmath27-regularizer .",
    "again , this is an instance of the elastic - net problem and can be solved with least - angle regression ( lars )  @xcite , completing the forward pass of the layer .    during backpropagation",
    ", we need to compute the gradient of the loss , @xmath22 , with respect to the previous layer s activation @xmath34 and the parameter matrix @xmath35 using the transferred gradient @xmath36 from layer @xmath37 .",
    "this is possible only if the forward computation procedure is differentiable ; mairal et al .",
    "@xcite prove the differentiability of the optimization in @xmath38 under mild conditions and compute its gradients , albeit for the problem of task - specific dictionary learning .",
    "we present here the results of practical interest to the sf layer computation , with further details in section [ sec : sparse : theory ] .",
    "define @xmath39 to be the nonzero support of @xmath40 , i.e. , the indices of the coefficients that are nonzero , and define auxiliary variable @xmath41 to have @xmath42 = 0 $ ] where @xmath43 is the complement of @xmath39 and the @xmath44 $ ] notation means indexing or slicing the vector . then , let @xmath45 = ( { { \\mathbf}{p}}_i^{t}{{\\mathbf}{p}}_i+\\lambda_2\\mathbf{i})^{-1 } { \\nabla_{\\!\\ ! { { \\mathbf}{a}}_{i}[\\lambda ] } l_s }      \\label{eq : b}\\end{aligned}\\ ] ] where @xmath46 } l_s } = ( { \\nabla_{\\!\\ ! { { \\mathbf}{a}}_{i } } l_s})[\\lambda]$ ] is the back - propagated supervised loss from the higher layer indexed at only the nonzero support of @xmath40 via @xmath39 .",
    "the gradients of interest are computed directly with this auxiliary vector @xmath47 : @xmath48 the gradient @xmath49 is backpropagated to lower layers in the network , and the gradient on the parameters , @xmath50 is used to update the dictionary @xmath35 at this sparse factorization layer .",
    "note that after every gradient step , @xmath35 must be normalized so that each element has @xmath28-norm no greater than 1 ; without this , we could scale @xmath35 up and @xmath40 down by a large constant , reducing the regularization penalties arbitrarily without improving the quality of the sparse representation .",
    "sparse representations have had success in patch - based models @xcite .",
    "likewise , the convolution layer in artificial neural networks has achieved great success @xcite .",
    "the translation invariance induced by these approaches is particularly well - suited to image - based problems .",
    "we hence generalize the sf layer to operate convolutionally on image patches .",
    "this new layer , the _ convolutional sparse factorization _ ( csf ) layer , performs the factorization locally on rectangular input regions much like convolutional layers perform localized linear transforms .",
    "consider such a layer that takes in @xmath51 images as input and performs the sparse factorization on overlapping patches of size @xmath52 .",
    "let @xmath34 be an input image to the csf layer , and let @xmath53 be the layer dictionary .",
    "denote patch @xmath54 into image @xmath34 as @xmath55 the csf layer is defined as @xmath56 for each patch @xmath55 , a sparse factorization @xmath57 is computed in forward inference .",
    "these sparse factorization activation vectors are analogous to the localized patch response to a bank of kernels in traditional cnns .",
    "these are arranged into a @xmath58 output tensor  exactly the same size as a traditional convolution layer .",
    "however , note that while we call it a convolutional layer , it is convolutional only in spirit : at each patch location , it is solving a sparse factorization kernel and it uses the same parameter matrix @xmath35 everywhere . defined in this way ,",
    "the csf layer is a drop - in replacement for convolutional layers .    during backpropagation ,",
    "given the output gradients @xmath59 , @xmath60 is computed for each patch as in eq .",
    "[ eq : b ] .",
    "this allows us to compute patch gradients , @xmath61 and @xmath62 , for the local dictionary and the current layer activation , respectively adapting eqs .",
    "[ eq : sfgradpi ] and [ eq : sfgradaminus1 ] . due to the linearity of convolution",
    ", we simply sum these gradients over all patches to yield the layer gradients that are used to update the parameter ( the dictionary ) and passed down to the lower layer .      to incorporate the sparse factorization and convolutional sparse factorization layers within the end - to - end backpropagation - based training ,",
    "their respective forward operations need to be differentiable and we need to compute the analytical derivatives . to that end , we review the pertinent results of mairal et al .",
    "@xcite , which serve as the theoretical basis for our two new layers .",
    "they prove the differentiability of the sparse factorization operation that is used in both @xmath38 and @xmath63 , and outline an optimization framework that allows sparse coding to be used in a variety of tasks .",
    "the framework in @xcite contains three stages : a vector input @xmath64 is first multiplied by the linear transform matrix @xmath65 ; given dictionary @xmath66 , the sparse code of the resulting vector is computed through the elastic net , @xmath67 from eq .",
    "[ eq : sp_coding ] , and a function @xmath68 is applied to the sparse output , with parameters @xmath69 .",
    "mairal et al .",
    "@xcite considered only the cases where @xmath70 is linear or bilinear , though their derivation holds for any differentiable @xmath70 , which is critical for our work as will become clear below .",
    "this output , @xmath71 , is compared to supervision @xmath18 in a twice - differentiable loss function @xmath72 , and the task is defined by the minimization thereof . to this end",
    ", they prove that @xmath67 is differentiable for any input @xmath4 that has a probability distribution with compact support , which is expected to be satisfied in our operating conditions . given this proof ,",
    "they compute the gradients required to update @xmath73 and @xmath74 using a descent algorithm ; we have similarly relied on this proof to derive our gradients in eqs .",
    "[ eq : sfgradpi ] and [ eq : sfgradaminus1 ] .",
    "although originally developed for compressed sensing , we take a new view onto this framework : it is a shallow artificial neural network .",
    "we detail the forward pass of this shallow network below , using our established convention : @xmath75 relating this shallow network to the sf and csf layers in larger networks , we map the last process @xmath70 as the remainder of the network _ above _ the inserted sf and csf layers , noting that their proof holds for any differentiable @xmath70 .",
    "we drop the linear layer and replace it with whatever is _ below _ the inserted sf and csf layers without impacting the gradient of the sf or csf layers in any way .",
    "also , we have separated the derivation of the backpropagation gradient after splitting the operations into three steps , resulting trivially in eq [ eq : sfgradaminus1 ] .",
    "a fundamental difference between the sparse factorization layers and their traditional neural networks counterparts is the generative nature of sparse factorization .",
    "consider the extended optimization to include the parameters ( dictionary ) in addition to the sparse factorization ( code ) , @xmath76     \\nonumber\\\\      \\text{s.t .",
    "} & \\quad\\lvert { { \\mathbf}{p}}\\rvert_2   \\le 1 \\enspace , \\nonumber\\end{aligned}\\ ] ] where we use a underset - subscript @xmath77 to indicate sample index to avoid confusion over layer indices in earlier notation and add the @xmath28 regularization on the dictionary .",
    "we immediately notice this goal of a dictionary that facilitates high fidelity reconstruction of the training data .",
    "this goal is unsupervised , in contrast to the supervised goals commonly used to train convolutional neural networks .",
    "note the relation to auto - encoders @xcite , which are also generative ; however , their layer definitions follow the traditional linear transforms we have already discussed .",
    "of course , we do not directly solve this non - convex optimization , but solve it instead through backpropagation using the earlier derivations .",
    "however , we do incorporate this goal of seeking a dictionary that enables high fidelity reconstructions of our data , which is similar to multi - task regularization @xcite .",
    "we define the unsupervised loss that can be used to regularize the sf and csf layer parameter ( dictionary ) as @xmath78 where @xmath79 indicates the activation to the forward pass of sf or csf layer @xmath80 after solving the @xmath38 or @xmath63 , respectively .",
    "this loss can be summed over all samples in a batch .",
    "the gradient associated with this task , @xmath81 , is well known in the dictionary learning literature @xcite , though more sophisticated optimization methods are typically preferred for learning purely unsupervised dictionaries .",
    "the expression for @xmath82 is given by @xmath83 we can incorporate it into our gradient descent method by defining an effective semisupervised gradient , @xmath84 as a convex combination of the two : @xmath85 for @xmath86 .",
    "note that the sample or activation @xmath34 used to calculate the unsupervised loss need not be the same sample or activation used for the supervised loss , nor does it need any corresponding supervision @xmath18 , allowing for the use of weakly annotated samples .    finally , because we are layering the sparse factorization layers into larger neural network structures , we need to propagate the gradient from the unsupervised loss down past the dictionary learning for an end - to - end regime .",
    "this gradient , @xmath87 is transferred directly or as part of the @xmath88 gradient which is trivially derived .    in practice",
    ", we use a step - down schedule for the semisupervision parameter @xmath89 , training for several iterations with @xmath90 , then several more with 0.5 , 0.3 , and finally 0.0 .",
    "we observe that this improves performance when compared to purely supervised training this step - down schedule is similar to the one proposed in mairal et al .",
    "@xcite except that do not use the piecewise constant - inverse learning rate schedule , favoring instead stochastic gradient descent with a momentum of 0.9 .",
    "we evaluate our new layers on a classic vision problem allowing us to thoroughly inspect various aspects of the layers in comparison to a well - studied baseline network .",
    "although we do expect our new layers to positively impact other vision problems , especially those with limited training data , we restrict the scope of our study to classification at moderate scale .",
    "recall that figure [ fig : networks ] visualizes the three core networks that we will compare .",
    "our baseline model is a curtailed version of lenet-5 @xcite , comprising two convolution layers and two linear layers , and denoted lenet .",
    "our variants are as follows : csf replaces the first convolution layer with a csf layer ; sf replaces the first linear layer with a sf layer and removes the rectified linear unit layer ; and csf+sf makes both modifications .",
    "all other things about these networks are equivalent , including the total number of parameters .",
    "[ [ implementation ] ] implementation + + + + + + + + + + + + + +    all networks are implemented in matconvnet   @xcite and trained using stochastic gradient descent with momentum .",
    "forward computation in sf and csf layers is performed with the spams sparse modeling library  @xcite .",
    "please contact the authors for access to the source code .",
    "we first compare the csf and sf layers to traditional methods on the task of digit classification .",
    "table [ tab : results ] shows the networks classification accuracy on : mnist @xcite , a handwritten digit classification dataset ; mnist - rot , a modified mnist with each digit rotated randomly ; mnist - rand , which contains random noise in the background of every image ; and mnist - img , which superimposes the written digits on randomly selected image patches .",
    "the original mnist has 60000 training images and 10000 for testing and its variants have 12000 for training and 50000 for testing .",
    "we observe improved performance on most tasks through the use of the csf layer .",
    "this is intuitive especially for the mnist - rand and mnist - img datasets  on which it outperformed the baseline the most  given the rich history of natural - image denoising and patch reconstruction with sparse representation models  @xcite .",
    "the performance of the sf layer is more volatile .",
    "this could be due to the unpredictable structure of the network s intermediate activations , which may not readily admit a sparse representation , jeopardizing the stability of the sf layer s output .",
    "or it could be due to the constraint we imposed to maintain a consistent total number of parameters across the three models being compared ; the dictionary underlying the sf layer may not be _ wide _ enough .",
    ".classification accuracy scores on the digit classification datasets , in percent .",
    "boldface indicates the best score for each dataset . [",
    "cols=\">,^,^,^,^\",options=\"header \" , ]      to explore our original hypothesis that cnns with sparse factorization layers will perform comparatively better than traditional cnns in the face of limited supervised data , we also evaluate our layers performance on very small training data sets . by varying the number of training samples from 600 ( 1% of total training set ) to 100 ( 0.167% ) ,",
    "we observe how each network deals with severely limited available supervision . each data point in figure",
    "[ fig : trainsize ] is the mean of several trials of each method , where each trial is trained on a unique subset of the data .    the networks containing sparse factorization layers outperform the baseline on limited training data , with the accuracy gap widening as data becomes scarcer , suggesting they may be more resistant to overfitting .",
    "when supervision is most scarce ( 100 samples in this experiment ) , the sf and csf networks outperform the lenet network by @xmath1 and @xmath2 , respectively . surprisingly , the sf layer outperforms both the baseline and csf network despite underperforming them when trained on the full dataset , as shown in table [ tab : results ] .",
    "this may be an artifact of our nave network design , or an indication that the structure of the sparse factorization problem may be inherently amenable to learning in low - supervision settings .",
    "in this paper , we have presented two novel cnn layers based on sparse representational modeling in contrast to  and in tandem with ",
    "more traditional , compositional layers .",
    "the two layers , sparse factorization and convolutional sparse factorization layers , analogous to fully - connected and convolutional layers in traditional cnns , are similarly able to be dropped into existing networks .",
    "they are trained with end - to - end back - propagation as well , but they can also be trained with a semisupervised loss , given their origins in unsupervised learning .",
    "our experiments clearly demonstrate potential in networks with these layers to train strong networks with limited labeled data .",
    "much remains to be explored with the sf and csf layers .",
    "solving an optimization problem in every forward pass is expensive compared to traditional cnn units , and our implementations typically exhibited training speeds 10 to 40 times slower than with the pure cnn ( although they are implemented only on the multicore cpu and not gpu ) .",
    "we postulate that this can be offset by the use of pretraining ; since both factorization layers are capable of producing and propagating an unsupervised loss , they ( and layers prior to them ) can be trained in a wholly unsupervised fashion to initialize the network parameters .",
    "the factorization layers also present unique challenges in designing the network that we hope to address in future work , such as choosing the placement , width ( dictionary size ) , and sparsity hyperparameters for csf and sf layers , all of which we observed have a non - negligible effect ."
  ],
  "abstract_text": [
    "<S> whereas cnns have demonstrated immense progress in many vision problems , they suffer from a dependence on monumental amounts of labeled training data . on the other hand , dictionary learning does not scale to the size of problems that cnns can handle , despite being very effective at low - level vision tasks such as denoising and inpainting . </S>",
    "<S> recently , interest has grown in adapting dictionary learning methods for supervised tasks such as classification and inverse problems . </S>",
    "<S> we propose two new network layers that are based on dictionary learning : a sparse factorization layer and a convolutional sparse factorization layer , analogous to fully - connected and convolutional layers , respectively . </S>",
    "<S> using our derivations , these layers can be dropped in to existing cnns , trained together in an end - to - end fashion with back - propagation , and leverage semisupervision in ways classical cnns can not . </S>",
    "<S> we experimentally compare networks with these two new layers against a baseline cnn . </S>",
    "<S> our results demonstrate that networks with either of the sparse factorization layers are able to outperform classical cnns when supervised data are few . </S>",
    "<S> they also show performance improvements in certain tasks when compared to the cnn with no sparse factorization layers with the same exact number of parameters . </S>"
  ]
}