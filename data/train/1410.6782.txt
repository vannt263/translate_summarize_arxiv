{
  "article_text": [
    "we consider ranking and selection of systems based on the average performance of the alternatives .",
    "the particular set - up used here is motivated by problems from optimization under uncertainty .",
    "often in operations research as well as in technical applications the performance of solutions depends on some random influence like market conditions , material quality , or simply measurement errors . we shall call such random influences a random _ scenario_. usually , the aim is then to find a solution with minimal expected costs taken over all scenarios .",
    "we assume here that we can describe the random scenario mathematically by a random variable @xmath0 with a distribution @xmath1 .",
    "let @xmath2 denote the cost of solution @xmath3 if it has been applied to random scenario @xmath4 then the average costs are given by the expectation @xmath5 . except for particularly simple cases , we will not be able to calculate this expression analytically . instead , we have to estimate @xmath5 based on a sample @xmath6 where @xmath7 are observations of the random variable @xmath8 , usually produced by a stochastic simulation model on a computer .    with estimated costs , optimization can be performed by heuristic search methods like genetic algorithms or ant algorithms .",
    "typically , these methods take a relatively small set @xmath9 of solutions ( a ` population ' ) and try to improve the quality of @xmath10 iteratively .",
    "the improvement step usually includes a _ selection _ of the best or at least the most promising solutions from @xmath10 with respect to their expected costs @xmath11 .",
    "as @xmath12 can only be estimated , this selection will return sub - optimal solutions with a certain error probability .",
    "often , this probability can be made arbitrarily small by increasing the sample size @xmath13 , i.e.  the number of simulations .",
    "the aim of this paper is to find selection mechanisms for good solutions suitable for heuristic optimization that have error probabilities below a given bound and use few simulations only .",
    "most heuristic optimization procedures do not need the exact values of @xmath14 in order to perform a reasonable selection , instead it is sufficient to find the correct _ ranking _ @xmath15 or even only part of it . as it has been noted before ( see e.g. @xcite ) it is much easier ( needs less simulations ) to estimate the correct ranking than to estimate the correct values .",
    "methods of ranking and selection are therefore widely used in heuristic optimization under uncertainty , see e.g. @xcite for an overview .    in this paper",
    "we have a fixed set @xmath16 of solutions , the observed cost values are assumed to be normally distributed with unknown mean and unknown covariance matrix .",
    "we use a bayesian approach and estimate the ranks of the solutions in each iteration based on the present posterior means .",
    "roughly , the steps of our procedure to rank and select the solutions can be summarized as follows :    1 .",
    "initialization : observe all solutions for a fixed number @xmath17 of scenarios ( = simulations ) .",
    "2 .   determine the ranking and selection based on the posterior means given these observations .",
    "calculate a lower bound lb(pcs ) for the _ probability of correct selection _",
    "pcs 4 .   while lb(pcs ) @xmath18 do 1 .",
    "allocate a fixed _ budget _ of additional simulations to the solutions , 2 .",
    "recalculate the posterior means , the ranking and selection and lb(pcs ) based on the extended set of observations .",
    "5 .   return the last ranking and selection .",
    "input parameters are the allowed error probability @xmath19 , the simulation budget @xmath20 and the initial sample size @xmath17 .",
    "this is the standard framework of a ( bayesian ) computing budget allocation ( cba ) algorithm , see e.g.  in @xcite .",
    "our contributions concern the following points which we label for further reference :    depend : :    we allow for _ dependent observations _ , i.e.  we apply the solutions to    common random scenarios ( common random numbers , crn ) observing    @xmath21 , a vector of possibly    dependent , normally distributed values .",
    "target : :    we use a very _ general concept of a target _ for ranking and selection    that includes targets as median or span of means and still allows to    bound the probability of a correct selection in a simple bonferroni    fashion .",
    "alloc : :    most importantly , we introduce a new _ heuristic allocation scheme _ for    additional simulations based on the posterior distribution of the    means and the covariance matrix .",
    "this scheme reuses terms that have    been evaluated to calculate the lb(pcs ) in the iteration before and    therefore requires little additional effort . in the experiments    described below",
    ", our algorithm needed less simulations to guarantee    lb(pcs)@xmath22 than competitors from literature .",
    "let us discuss these issues in more detail .",
    "* depend * : classical ranking and selection ( r&s ) procedures evaluate the alternatives based on random scenarios drawn _ independently _ for each alternative , see e.g.  @xcite .",
    "only a few authors ( see e.g.  @xcite ) use _ dependent _ samples based on common random numbers ( crn ) for simulation .",
    "it is well known , that if the observations are _ positively correlated _ , it is more efficient to use common random numbers ( crn ) , i.e.  to compare the solutions on the same scenario ( see e.g. @xcite ) .",
    "positive correlation in our case roughly means that if a solution @xmath23 has , for a scenario @xmath24 costs @xmath2 that are above average , then costs @xmath25 will tend to be over average for all the other solutions @xmath26 also .",
    "in other words , if some scenario @xmath27 is relatively difficult ( costly ) for solution @xmath28 then @xmath29 will tend to be difficult for all solutions .",
    "similarly , a scenario that has small costs ( below average ) for one solution will tend to be an easy scenario for all solutions creating smaller costs for all of them .",
    "as this is the behaviour we would expect from a set of solutions at least after some iterations of improvement , it seems to be justified to expect positive correlation and to use crn specific r&s procedures for heuristic optimization .",
    "the numerical results below show that if the cost are positively correlated huge savings in simulation effort are indeed possible and if there is no positive correlation our approach is not worse than other procedures .    *",
    "target * : we fix a set @xmath30 of ranks and try to select ( only ) those solutions that have these ranks with respect to the posterior means ( and sort them if necessary ) .",
    "e.g.  if we have @xmath31 then we want to select the @xmath32 best solutions ( with minimal means ) .",
    "we describe the sets of possible correct selections in a way that uses a minimal set of pairwise comparisons of solutions , see @xcite for a similar approach .",
    "this allows to give a bonferroni type of lower bound to the posterior probability of correct selection that involve only one - dimensional @xmath33-distributions .",
    "the important and difficult part of this calculation is the determination of the posterior distributions in the presence of incomplete observations due to the possibly unequal allocation of the budget .",
    "* alloc * : the expressions forming the bonferroni lower bound for the pcs may be used to identify pairs of solutions for which the comparison has a large error probability and which need more data for a clear r&s decision .",
    "this results in a simple allocation rule that is simpler to calculate and turned out to be more efficient in our experiments than greedy ocba allocation rules as they are often used in literature ( see e.g.  @xcite ) .",
    "this paper is based on the doctoral thesis of one of the authors ( @xcite ) .",
    "it is organized along the three issues outlined above as follows .",
    "* depend * : in the first section we give the exact mathematical description of the sampling process and describe our bayesian model .",
    "details of the rather involved calculation of the posterior distribution with missing data are summarized in an appendix .",
    "* target * : the second section generalizes the concept of ranking and selection of solutions to the case of partial selections . *",
    "alloc * : in section 3 we introduce several schemes for allocating the simulation budget to solutions based on the posterior distributions .",
    "a precise definition of our complete ranking and selection algorithm is given in section 4 .",
    "we report on extensive empirical tests with this algorithm in section 5 where we compare it to a greedy type of ocba algorithm and to the standard @xmath34 procedure of @xcite .",
    "some conclusions are given in the final section .",
    "let @xmath35 denote the fixed set of solutions .",
    "@xmath36 is the @xmath32-th observation of solution @xmath37 , i.e.  the cost when @xmath38 is applied to the @xmath32-th random scenario .",
    "we do not consider cost functions and scenarios explicitely in the sequel .",
    "we collect the @xmath36 into a matrix - like object @xmath39 : a column in our data @xmath40 represents the observations of all solutions with a single scenario , whereas the rows represent all observations from a particular solution .",
    "@xmath40 need not be a complete matrix : due to possibly unequal allocation of simulations we may have a ragged right border , see below .",
    "as we use crn , missing values at the end of a row are not really ` missing ' but are rather ` not yet observed ' .",
    "simulating _ all _ of the solutions from @xmath41 with the @xmath32-th scenario therefore leads to a ( column ) vector of observations @xmath42 as it is often done in ranking and selection , we assume that @xmath43 are independent and identically @xmath44-distributed observations . here , @xmath45 denotes the @xmath46-dimensional normal distribution with mean @xmath47 and the positive definite @xmath48 covariance matrix @xmath49 .",
    "this model includes the case , where the @xmath50 solutions are simulated independently ( i.e.  with different scenarios ) , then @xmath51 is a diagonal matrix . @xmath52 and @xmath51 are assumed to be unknown .    our overall goal is to extract information about @xmath53 from the observations @xmath39 .",
    "typically , we want to identify solutions @xmath37 that have small mean values @xmath12 .",
    "we take a bayesian point of view , that is we assume that the unknown parameters @xmath53 and @xmath51 are themselves observations of random variables @xmath54 and @xmath55 having a prior distribution with density @xmath56 .",
    "we do not assume any specific prior knowledge about the parameters , therefore we shall use the so - called uninformed prior distribution ( see @xcite , @xcite ) with @xmath57 where @xmath58 means that the right hand side gives the density @xmath59 up to some multiplicative constant that does not depend on @xmath53 or @xmath51 .",
    "@xmath60 is a so - called hyperparameter that allows to control the degree of uncertainty about @xmath51 .      instead of simulating all solutions equally often",
    ", we iteratively allocate a given amount @xmath20 of simulations to the solutions , depending on the observations seen so far , see the detailed algorithm in section  [ sec : thealgorithm ] below . the result",
    "are samples that may contain different numbers of observations for different solutions .",
    "we use @xmath61 to denote the random ( row ) vector of the @xmath62 observations produced for the @xmath38-th solution , @xmath63 , and similarly @xmath64 for a specific sample .",
    "let @xmath65 be the vector of the @xmath46 present sample sizes , then the overall samples can be collected into a matrix - like scheme with possibly different row lengths @xmath62 : @xmath66 columns @xmath67 may be incomplete , as the @xmath32-th simulation ( scenario ) may not have been allocated to all solutions , see figure  [ fig : samplingscheme ] for an illustration .",
    "let @xmath68 denote the set of possible samples of that shape for a particular size vector @xmath69 .",
    "we shall now define the possible samples and allocation rules recursively . at first",
    "( in iteration @xmath70 ) , all solutions are simulated @xmath17 times where @xmath17 is a given fixed number .",
    "with @xmath71 we are therefore observing a ( @xmath72-matrix ) sample @xmath73 from @xmath74 next we apply an _ allocation rule _ @xmath75 @xmath76 that determines the numbers @xmath77 of additional simulations for solution @xmath63 in the first iteration",
    ". then the overall number of simulations seen so far is @xmath78 after these additional simulations have been performed we have a sample @xmath79 let @xmath80 be the set of all possible samples after the first iteration , i.e. @xmath81 where the operation @xmath82 concatenates elements row - wise , i.e. @xmath83 generally , after the @xmath84-th iteration we have a sample @xmath85 where @xmath86 gives the total number of simulations performed with each solution so far .",
    "then , an allocation rule @xmath87 is applied to determine the numbers of additional simulations in the @xmath88-st iteration .",
    "the set of possible samples after that iteration is @xmath89 note , that under crn the simulation of different iterations will in general not be independent as we may have to _ reuse _ scenarios ( random numbers ) for some solutions that have already been used in earlier iterations for other solutions .",
    "a new independent scenario is drawn for the simulation of some solution @xmath38 only if @xmath90 has been applied to all scenarios drawn before .",
    "see figure  [ fig : samplingscheme ] for a simple example .     and",
    "budget @xmath91 .",
    "the allocations of the second iteration are marked in gray , those of the third in black .",
    "the results in the ( complete ) columns @xmath92 are @xmath93-distributed . ,",
    "width=188 ]    the allocation schemes we use below depend on the posterior distribution of the mean @xmath54 given data @xmath94 which is determined in the next subsection .",
    "the likelihood function of incomplete samples as described in the last section are examined in great detail and generality in @xcite .",
    "our case is comparatively simple , as we have consecutive observations for each solution ( row in the sample ) and there are no ` holes ' , data may only be missing at the end of the sample .",
    "this is due to the crn scheme we are using : assume a solution @xmath95 has been simulated with the first @xmath32 scenarios resulting in @xmath96 .",
    "if no simulations are allocated to @xmath38 in the next iterations , but additional simulations are performed in later iterations , these have to use scenarios @xmath97 resulting in observations @xmath98 .",
    "also , the pattern of the missing data is a function only of sample values observed before , this is called a monotone and ignorable pattern ( @xcite ) .    there is no closed expression for the posterior density of @xmath54 given data @xmath99 if the covariance matrix @xmath51 of the underlying normal distribution is unknown .",
    "we therefore use a semi - bayesian approach and first assume that the covariance matrix is known .",
    "we determine the conditional distribution of @xmath54 given @xmath100 and @xmath101 exactly and then replace @xmath102 by its ( non - bayesian ) estimate @xmath103 .    to simplify notation ,",
    "let the set of solutions @xmath104 be ordered such that for the present data @xmath105 with @xmath106 we have @xmath107 this means that solution @xmath108 has been simulated with @xmath109 different independent scenarios . as we use crn , the other solutions @xmath110 have been observed with subsets of these scenarios , see fig .",
    "[ fig : monotonesample ] for a simple example .     and @xmath111 .",
    ", width=226 ]    we need projections of vectors and matrices to components corresponding to subsets of the solutions @xmath104 . for @xmath112 and @xmath113 let @xmath114}:= ( y_1 , \\ldots , y_{i-1})^t\\in { \\mathbb{r}}^{i-1},\\quad \\text { and } \\quad   { \\bm{y}}_{[\\le i]}:= ( y_1 , \\ldots , y_{i})^t\\in { \\mathbb{r}}^{i}.\\ ] ] similarly , for the @xmath115 matrix @xmath116 we define @xmath117}&:=&(\\sigma_{kl})_{k , l = 1 , \\ldots , i-1 } , \\qquad   { \\bm{\\sigma}}_{[i,<i]}:= ( \\sigma_{i1 } , \\ldots , \\sigma_{i , i-1})\\quad \\text { and } \\label{eq : sigmaproj}\\\\    \\beta_i & : = & \\sigma_{[i,<i]}\\big(\\sigma_{[<i]}\\big)^{-1 } \\label{eq : defbeta}.   \\end{aligned}\\ ] ] here , @xmath118 is a @xmath119-dimensional row vector .",
    "let @xmath120 be the sample mean of all @xmath62 observations @xmath121 available for solution @xmath122 .",
    "for @xmath123 we have @xmath124 therefore we may define the sample mean of solution @xmath38 restricted to its first @xmath125 observations @xmath126    [ fig : samplemeans ] } $ ] contains all @xmath127 sample means from the boxed values.,title=\"fig:\",width=226 ]    then @xmath128}:=({\\overline{\\bm{x}}}^{(n_i)}_{1 } , \\ldots,{\\overline{\\bm{x}}}^{(n_i)}_{i-1})^t $ ] is the @xmath119-dimensional column vector of sample means restricted to the first @xmath62 observations for solutions @xmath129 , that all have at least @xmath62 observations , see fig .",
    "[ fig : samplemeans ] .    the posterior distribution of @xmath54 given an incomplete observation @xmath130 with known prior covariance is given in the following theorem , its rather technical proof is sketched in the appendix .    [",
    "the : posterior ] let ( the columns ) @xmath131 be @xmath132 distributed with _ known _",
    "covariance matrix @xmath51 and unknown mean @xmath133 .",
    "assume that the variable @xmath54 for the mean has the uninformative prior @xmath134 .",
    "let the ( possibly incomplete ) data @xmath135 with @xmath136 for some @xmath137 , be given as described in section  [ subsec : iterativeallocation ] .",
    "then the posterior distribution of @xmath54 given @xmath100 is an @xmath46-dimensional normal distribution @xmath138 with mean @xmath139 where @xmath140 } - \\bar{{\\bm{x}}}_{[<i]}^{(n_i)})\\quad \\text { for } i=2 , \\ldots , l,\\ ] ] and covariance matrix @xmath141}$ ] where @xmath142}=\\frac{\\sigma_{11}}{n_1},\\qquad { \\bm{\\lambda}}_{[\\le i]}= \\left(\\begin{array}{c|l }        { \\bm{\\lambda}}_{[<i ] } & { \\bm{\\lambda}}_{[<i]}\\ \\beta_i^t \\\\[0.9ex]\\hline        \\rule[0.5ex]{0pt}{2ex}\\beta_i\\ { \\bm{\\lambda}}_{[<i ] } & \\frac{1}{n_i}(\\sigma_{ii}-\\beta_i{\\bm{\\sigma}}_{[<i]}\\beta_i^t ) + \\beta_i\\   { \\bm{\\lambda}}_{[<i]}\\ \\beta_i^t \\end{array}\\right)\\ ] ] for @xmath143 .    for the case of",
    "an _ unknown _ covariance matrix @xmath51 we have to replace @xmath144}$ ] and @xmath118 by their estimators @xmath145}({\\bm{x}})$ ] and @xmath146 . for the estimation of @xmath147}=\\big(\\sigma_{kl}\\big)_{1\\le k , l < i}$ ]",
    "we only consider solutions @xmath148 .",
    "these all have sample sizes @xmath149 we may therefore define for @xmath150 and @xmath151 the estimator @xmath152 and put @xmath153}= \\hat{{\\bm{\\sigma } } } _ { [ < i]}({\\bm{x}})&:=&\\big ( \\hat{\\sigma}^{(n_{i-1})}_{kl } \\big)_{k , l=1 , \\ldots , i-1 } \\label{eq : defsigest}\\\\ \\hat{{\\bm{\\sigma}}}_{[i,<i]}=\\hat{{\\bm{\\sigma}}}_{[i,<i]}({\\bm{x}})&:= & ( \\hat{\\sigma}^{(n_{i-1})}_{i1 } , \\ldots , \\hat{\\sigma}^{(n_{i-1})}_{i , i-1}).\\notag\\end{aligned}\\ ] ] note that we use the maximum - likelihood estimator for the covariances and that @xmath154}$ ] is not necessarily contained in @xmath155}$ ] as both estimators may use different sample sizes @xmath156 and @xmath62",
    ". we also have to estimate the @xmath118 and put @xmath157}(\\hat{{\\bm{\\sigma } } } _ { [ < i]})^{-1},\\ ] ] where we have to make sure that @xmath158 } $ ] is non singular . from @xcite it is known , that if the sample size @xmath156 of @xmath158}$ ] fulfills @xmath159 then @xmath158}$ ] is positive definite with probability one .",
    "this could be guaranteed if we require that the initial sample size @xmath160 is larger than the number of solutions @xmath161 as then @xmath162 .",
    "we may now plug these estimators into the definition of the posterior means and obtain @xmath163 } -    \\bar{{\\bm{x}}}_{[<i]}^{(n_i)}),\\quad \\hat{{\\bm{\\nu}}}({\\bm{x}}):=(\\hat{\\nu}_1 , \\ldots , \\hat{\\nu}_l).\\ ] ] to obtain an adequate estimator @xmath164 we first look at the case of _ complete _ observations , i.e.  with @xmath165 .",
    "then it is well - known ( see e.g. @xcite , 10.3 ) that with @xmath51 _ known _ and an uninformative prior distribution for the mean @xmath54 , the posterior distribution of @xmath54 would be normal with covariance matrix @xmath166 .",
    "if @xmath51 is _ unknown _ with the improper prior distribution as in the marginal posterior distribution of the mean @xmath54 in the complete observation case is an @xmath46-dimensional @xmath33-distribution with @xmath167 degrees of freedom , location parameter @xmath168 and scale matrix @xmath169 .",
    "here , the correction term @xmath170 involves the hyperparameter @xmath171 , the sample size and the dimension of the problem . for the present situation of incomplete observations and _ known _ covariance matrix",
    ", the posterior covariance @xmath172 has to be replaced by @xmath173 as in , reflecting the different sample sizes for each solution instead of the fixed size @xmath174 .",
    "then , in the case of _ unknown _ @xmath102 , it seems reasonable to approximate the posterior distribution of the mean @xmath54 by an @xmath46-dimensional @xmath175-distribution with @xmath167 degrees of freedom and location parameter @xmath176 .",
    "the scale matrix should be obtained from @xmath173 with the @xmath177 replaced by their adequate estimators . in particular , the correction terms @xmath170 from the complete observation case should be replaced on the @xmath38-th step of the recursion in by @xmath178 using the number @xmath38 of solutions considered in that step .",
    "we thus obtain @xmath179}=\\hat{{\\bm{\\lambda}}}_{[\\le 1]}({\\bm{x}})&:=\\frac{\\hat{\\sigma}^{(n_1)}_{11}}{n_1 - 1+\\nu_0},\\label{eq : lambdahat }   \\\\",
    "\\hat{{\\bm{\\lambda}}}_{[\\le i]}= \\hat{{\\bm{\\lambda}}}_{[\\le i]}({\\bm{x } } )    & : = \\left(\\begin{array}{c|l }        \\hat{{\\bm{\\lambda}}}_{[<i ] } & \\hat{{\\bm{\\lambda}}}_{[<i]}\\ \\hat{\\beta}_i^t \\\\[0.9ex]\\hline        \\rule[0.5ex]{0pt}{2ex}\\hat{\\beta}_i\\ \\hat{{\\bm{\\lambda}}}_{[<i ] } & \\frac{1}{n_i - i+\\nu_0}(\\hat{\\sigma}^{(n_i)}_{ii}-\\hat{\\beta}_i\\hat{{\\bm{\\sigma}}}_{[<i]}\\hat{\\beta}_i^t ) + \\hat{\\beta}_i\\   \\hat{{\\bm{\\lambda}}}_{[<i]}\\ \\hat{\\beta}_i^t \\end{array}\\right ) \\notag\\end{aligned}\\ ] ] for @xmath143 , and @xmath180}({\\bm{x}})$ ] .    in this paper ,",
    "the posterior distribution of the mean @xmath54 is used mainly to express the probability of a wrong selection , see below .",
    "in particular , we need the posterior probability of the event @xmath181 for @xmath182 and @xmath183 . from the properties of the multivariate @xmath33-distribution we may conclude that @xmath184\\ = \\",
    "{ \\text{\\textbf{p}}}[w_i - w_j\\le \\delta\\mid { \\bm{x}}={\\bm{x}}]\\\\    & \\approx \\",
    "g(\\delta;\\ \\min\\{n_i , n_j\\}-1,\\ ; \\hat{\\nu}_i-\\hat{\\nu}_j,\\ ;    \\hat{\\lambda}_{ii}+\\hat{\\lambda}_{jj}-2\\hat{\\lambda}_{ij})\\notag\\\\    & : = p_{ij}^\\delta\\notag\\end{aligned}\\ ] ]    where @xmath185 is the distribution function of the one - dimensional @xmath186-distribution with @xmath32 degrees of freedom , location parameter @xmath187 and scale parameter @xmath20 .",
    "this approximation will be used below .",
    "we start with a general definition of the ranking and selection schemes we are examining in this paper . it is a generalization of the approach in @xcite .",
    "we want to select solutions from the set @xmath188 according to their ranks under some performance measure , in our case the mean value .",
    "e.g.  we want to select the solution with the lowest mean value ( the best solution ) or the @xmath189 best solutions as it is often used e.g.  in ant algorithms .",
    "we restrict ourselves to such selections that can be determined using pairwise comparisons of solutions only .",
    "the idea is to formulate _ minimal _",
    "requirements for the correctness of a selection in terms of pairwise comparisons and then use to determine the error probability .",
    "this may allow a tighter error bound compared to a complete ranking of all solutions .",
    "we first introduce an abstract concept which is illustrated by an example below .    for a general ranking and selection scheme ( r&s - scheme )",
    "we first define a set of _ target ranks _ @xmath190 .",
    "this means that we want to select those solutions that have ranks from @xmath191 with respect to their mean values . as the mean",
    "values are assumed to be unknown , we use their available estimates @xmath192 instead .",
    "these are ordered in increasing order @xmath193 and the solutions with estimated ranks in @xmath191 are selected , i.e.  the solutions @xmath194 the aim is then to determine the error probability of the selection @xmath195 i.e.the posterior probability that a solution @xmath196 does not have a rank from @xmath197 .",
    "as we have continuous posterior distributions for which @xmath198 = 0\\ ] ] we may restrict ourselves to the @xmath46-dimensional euclidean space without equal values : @xmath199 let @xmath200 be the set of all @xmath46-dimensional vectors , in which the selected positions from @xmath201 have ranks @xmath191 .",
    "thus , @xmath202 is the set of all potential means for which the selection @xmath201 would be correct for target @xmath191 . recall that @xmath203 is the random variable indicating the unknown mean . with the help of @xmath202 we can therefore express the probability that @xmath201 is a correct selection for target @xmath191 simply as @xmath204 = { \\text{\\textbf{p } } } [ \\{\\text{rank}(w_l)\\mid l \\in b\\}=a \\mid { \\bm{x}}={\\bm{x}}].\\ ] ] for an easier access to this probability we assume that there is a binary _ relation _ @xmath205\\ \\cup\\ [ ( { \\mathcal{l}}-b)\\times b]\\ ] ] on pairs @xmath206 that relates selected positions @xmath196 to unselected positions @xmath207 such that @xmath202 has the following representation @xmath208 where as usual @xmath209 is true if @xmath210",
    ". thus @xmath211 provides a definition of a correct selection by pairwise comparison of certain pairs @xmath212 .",
    "note that @xmath201 and hence @xmath202 and @xmath211 all depend on the observation @xmath130 .",
    "the following examples explain these definitions .",
    "assume that holds .    1 .",
    "let @xmath213 this means we want to select the best solution . then with , the selection is @xmath214 as @xmath215 is the smallest posterior mean . with @xmath216",
    "we have from @xmath217 @xmath202 is the set of all potential mean value vectors where selection @xmath218 leads to the best ( minimal ) solution .",
    "2 .   if @xmath219 for some @xmath220 we want to select the @xmath189 best ( minimal ) solutions .",
    "then from we see that we must choose @xmath221 . as characterizing relation",
    "we obtain @xmath222 and @xmath223 3 .",
    "if , in addition , the @xmath189 best solutions have to be ranked among themselves then we would choose @xmath224 and @xmath225 this example includes complete ranking of the solution with @xmath226 and @xmath227 4 .",
    "if we want to select the median with @xmath228 we take as selection @xmath229 the median solution with respect to posterior means and as auxiliary sets @xmath230 and @xmath231 .",
    "then we use as characterizing relation @xmath232 and obtain @xmath233 as the set of vectors from @xmath234 where about half of the components is less than @xmath235 and the rest is larger , therefore @xmath235 is the median .",
    "5 .   as a final example let @xmath236 denote a target that could be used to describe the span of the mean values of the solution .",
    "then the selection should be @xmath237 from and @xmath238 and @xmath239    in the iterative procedure below , the posterior means @xmath240 have to be determined after each iteration based on the new observations .",
    "therefore , @xmath241 and @xmath202 have to be adapted in each iteration also .",
    "we now define the probability of correct selection ( pcs ) for a selection as in simply by @xmath242.\\ ] ] to determine pcs directly , one has to solve high dimensional integrals in order to arrive at the marginal distribution of the posterior distribution on the components of the selection @xmath201 .",
    "instead , we use the relation @xmath243 that defines the set @xmath202 to derive a simple lower bound of bonferroni type for the pcs as follows @xmath244\\notag\\\\ & = & { \\text{\\textbf{p}}}\\big [ w_i \\le w_j \\text { for all } 1\\le i , j\\le l \\text { with } \\rho_{ab}(i , j)\\mid { \\bm{x}}={\\bm{x}}\\big]\\notag\\\\ & = & 1- { \\text{\\textbf{p}}}\\big [   w_i > w_j \\text { for some } 1\\le i , j\\le l \\text { with } \\rho_{ab}(i , j)\\mid { \\bm{x}}={\\bm{x}}\\big]\\notag\\\\ & \\ge &   1- \\sum_{(i , j)\\in \\rho_{ab } } { \\text{\\textbf{p } } } [   w_i > w_j \\mid { \\bm{x}}={\\bm{x } } ] \\label{eq : bonferroni}\\\\ & = & 1- \\sum_{(i , j)\\in \\rho_{ab } } \\big(1-{\\text{\\textbf{p}}}[w_i - w_j\\le 0 \\mid { \\bm{x}}={\\bm{x}}]\\big ) . \\notag   \\end{aligned}\\ ] ] using we may approximate this lower bound by the following expression @xmath245    it is quite usual in ranking and selection to relax the definition of correctness of a selection . instead of strict comparisons of means",
    "@xmath246 for relevant pairs @xmath210 we are content if @xmath247 for some fixed _ indifference zone _ value @xmath248 .",
    "this allows to take the limited precision of our observations into account .",
    "formally , we then define @xmath249 and have , using @xmath250\\notag\\\\      & \\ge & 1- \\sum_{(i , j)\\in \\rho_{ab } } { \\text{\\textbf{p } } } [   w_i >",
    "w_j+\\delta { } \\mid { \\bm{x}}={\\bm{x } } ] \\notag \\\\      & \\approx & 1- \\negthickspace\\negthickspace \\sum_{(i , j)\\in \\rho_{ab } } \\negthickspace\\big(1-g ( \\delta { } ;      \\min\\{n_i , n_j\\}-1,\\ ; \\hat{\\nu}_i-\\hat{\\nu}_j,\\ ;       \\hat{\\lambda}_{ii}+\\hat{\\lambda}_{jj}-2\\hat{\\lambda}_{ij})\\big)\\label{eq : deflbdelta}\\\\      & = &   1- \\negthickspace\\negthickspace \\sum_{(i , j)\\in \\rho_{ab}}(1-p_{ij}^\\delta)\\       = : lb^\\delta({\\bm{x } } ) \\notag   \\end{aligned}\\ ] ]",
    "in each of the iterations described in section  [ subsec : iterativeallocation ] it has to be decided how many simulations @xmath251 should be performed for each solution @xmath37 .",
    "we first look at a modified version of the so - called optimal computing budget allocation ( ocba ) strategy introduced by chen ( see for example @xcite ) .",
    "ocba allocation strategies try to allocate a fixed simulation budget @xmath252 in such a way that the expected value of the @xmath253 is maximized .",
    "let @xmath254 with @xmath255 denote an allocation of the @xmath20 simulations to solutions @xmath256 . calculating the expected effect of additional simulations and to find @xmath257 that are strictly optimal with respect to that expectation is a difficult non - linear optimization problem , see e.g.@xcite .",
    "therefore , most versions of the ocba in the literature solve a substitute problem and try to maximize an approximation @xmath258 to the lower bound as in for given data @xmath130 and additional simulations @xmath259 . in this sense ,",
    "a myopically optimal allocation is the solution to the following optimization problem : @xmath260 in general , even this simplified problem can be solved exactly only for very small instances of @xmath261 and small simulation budgets @xmath262 . for larger instances ,",
    "heuristic solution approaches must be used , see e.g. @xcite .",
    "we adapt this approach to our environment with dependent observations .",
    "the difficulty lies in estimating the future covariances _",
    "after _ the additional simulations @xmath259 have been performed .",
    "we use the lower bound @xmath263 where the scale parameter @xmath264 is a weighted mean of the posterior covariances : @xmath265\\hat{\\lambda}_{ij}.\\end{aligned}\\ ] ]    here it is assumed that the posterior variance of @xmath266 after @xmath259 additional simulations can be estimated from the present posterior values @xmath267 and @xmath268 by entering the variances @xmath269 inversely proportional to the relative increases of the sample sizes and by scaling the covariance @xmath270 such that the influence of the sample sizes @xmath271 is proportional to the posterior variances @xmath272 and @xmath273 .    a greedy heuristic ( greedyocba ) to find good solutions to the problem",
    "may then be defined similar as in @xcite and @xcite .",
    "greedyocba first measures the benefit of assigning all @xmath20 additional simulations to a single solution @xmath274 : @xmath275+\\sum_{i\\in{\\mathcal{l}}:(i , l)\\in\\rho_{ab}}\\left[p^\\delta_{il}(0,b)-p^\\delta_{il}(0,0)\\right],\\end{aligned}\\ ] ] where the @xmath276 are defined in and @xmath277 is the @xmath278-th unit vector which has a @xmath108 at the @xmath279-th position and zeros elsewhere . then , greedyocba distributes the simulation budget proportional to the weights @xmath280 , i.e. @xmath281 a remaining budget @xmath282 is allocated according to the largest remainder method .",
    "this allocation strategy requires @xmath283 evaluations of the distribution function @xmath284 in total .",
    "hence its computational effort is about twice as high as the calculation of the bonferroni bound @xmath285 in .",
    "this complexity can be reduced if we do not try to estimate the future pcs but simply look at its present value , more precisely its components @xmath286 $ ] , defined in .",
    "@xmath287 describes the posterior probability of @xmath288-dominance of @xmath289 over @xmath290 .",
    "recall that for @xmath210 we have to make sure the @xmath291-dominance @xmath181 in order for the present selection @xmath201 to be correct with indifference parameter @xmath291 .",
    "hence a small value of @xmath292 for @xmath293 indicates that at least one of the solutions @xmath294 has insufficient data .",
    "we therefore define the weights @xmath295 then the budget is allocated proportionally to these weights as above . here",
    ", those solutions @xmath279 get a larger weight that are part of a pair @xmath296 that has a small dominance probability and where the partner of @xmath279 has a relatively small variance .",
    "this implies that @xmath279 has a relatively large variance and hence needs more simulations to increase the certainty of the dominance relation @xmath297 .",
    "we call this strategy dominance probability weighting ( @xmath298 ) . in empirical tests",
    "@xmath298 proved slightly better than greedyocba with respect to the number of simulations needed , at the same time it needs less computational effort as only the @xmath299 from the bonferroni bound are used .",
    "it turned out in the experiments that the performance could be improved further by stopping simulations for those solutions with high enough dominance probabilities .",
    "therefore we restricted the definition of @xmath300 in to those @xmath287 which are below @xmath301 where @xmath302 is the number of summands in the bonferroni bound .",
    "the allocation strategy @xmath303 therefore uses the weights @xmath304 and then allocates simulations as in .",
    "empirical results on greedyocba and @xmath303 are given in section  [ sec : computational - study ] below .",
    "we are now in the position to define our algorithm in detail .",
    "let the following items be given :    @xmath104 is the set of solutions , @xmath190 is the target set of ranks to be selected and possibly ranked , @xmath305 is the bound for the error probability , @xmath306 is the simulation budget for each iteration , @xmath307 is an initial sample size and @xmath291 is the indifference zone parameter .",
    "0.5 cm    * initialization * : :    :  observe @xmath308 for    @xmath309 i.e.  choose @xmath17 random    scenarios and apply all solutions to each of them .",
    "let    @xmath130 denote the result .",
    "+    determine the posterior means    @xmath310 as in , the    selection @xmath311 as in , relation @xmath312 as in and and the dominance probabilities    @xmath313 , as in .",
    "finally    calculate the lower bound @xmath314 for the    pcs as in . *",
    "while * : :    @xmath315**do * *    +    * apply an allocation rule @xmath316 to determine the number of    additional simulation runs    @xmath317 ,    * perform @xmath251 simulations with solution    @xmath37 , taking into account the common    random numbers scheme as described in    section  [ subsec : iterativeallocation ] , let @xmath130 be    the extended data including the new simulation results ,    * update the posterior means    @xmath176 , the selection    @xmath311 , the relation @xmath243 , the    dominance probabilities    @xmath313 , and the lower bound    @xmath314 .",
    ": :    * return * the present selection @xmath311 .    in @xcite it is shown that this algorithm terminates if at least one simulation is allocated to each solution in each iteration .",
    "we implemented the algorithm bayesrs and compared its efficiency to other r&s procedures .",
    "the main objectives of the study were :    * to compare different allocation strategies within the framework of bayesrs , here we use greedyocba , @xmath303 and equalallocation , the naive equal allocation of the budget to solutions , * to show the increasing efficiency of @xmath303 with increasingly positive correlation of the observations , * to compare bayesrs with allocation rule @xmath303 to another standard from literature , namely @xmath34 , a procedure designed to find the best solution , see @xcite , and * to investigate the behaviour of our approach in cases where there is no positive correlation .",
    "it seems that the ( approximately ) correct determination of the posterior distribution for missing values makes the allocation more precise , even if , in the case of negative correlation , common random numbers may increase the variance of the estimators used .",
    "we measured the performance by the average number of simulations each strategy needs with a common error bound @xmath19 .",
    "also we checked the empirical error probabilities or rather pcs . the tests were implemented using the free statistics software ` r ` . in @xcite further studies show the efficiency of these allocation strategies in the context of heuristic optimization methods like ant algorithms .",
    "as our methods require normally distributed observations , we generated them by a @xmath318-random generator for different values of @xmath53 and @xmath51 .",
    "some values were fixed : the number of solutions @xmath319 ( other values showed similar behaviour ) , the error probability @xmath320 and the indifference zone parameter @xmath321 .",
    "the initial sample size was @xmath322 , the budget to be allocated in each iteration was @xmath323 , and at least one simulation was allocated to each solution in each iteration .",
    "the parameter @xmath171 for the prior probability of the covariance matrix as in showed best results in our setup for @xmath324 and was therefore fixed to this value throughout our tests .",
    ".different values for @xmath325 were used in the @xmath53 -case `` ufc '' . [ cols=\"<,<\",options=\"header \" , ]     a _ basic scenario _ consists of the following four variables that are varied in the tests :    1 .",
    "we have examined three different * r&s - cases * : in `` best@xmath326 '' we want to select the best solution , i.e.  we use a target set @xmath327 ( see subsection  [ subsec : targetandselection ] ) , in `` best@xmath328 '' we want to select the better half of the solutions ( @xmath329 ) and in `` rank@xmath328 '' we also want to rank these ten best solutions .",
    "also three different * @xmath53-cases * have been used for the @xmath318-generator . in the case ``",
    "inc '' , @xmath53 is the increasing sequence @xmath330 . in the unfavourable case `` ufc '' , @xmath53 is adapted to the r&s - case chosen as described in table [ tab : mucase ] . in the case",
    "`` unif '' , @xmath46 values are drawn randomly from the interval @xmath331 $ ] with a minimal distance of at least @xmath332 and then sorted into increasing order .",
    "this is repeated @xmath333 times , so that 15 simulations with different random @xmath53 are performed . as we use a bayesian approach",
    ", the `` unif''-case seems to be the most natural to simulate the uninformative prior but the cases may differ remarkably depending on how well the random values are separated .",
    "3 .   as our method claims to work",
    "well when observations are positively correlated , we created covariance matrices @xmath334 for the @xmath318-generator with a given joint correlation @xmath335 @xmath336 . to do so ,",
    "variances @xmath337 were chosen uniformly distributed in the interval @xmath338 $ ] , then we put @xmath339 for @xmath340 . to examine the robustness of our procedure when there are negative correlations , we complemented the above construction for @xmath341 with @xmath342 resulting in a covariance matrix with alternating positive and negative entries . finally we used a random covariance matrix following a wishart distribution with @xmath46 degrees of freedom and a random scale matrix .",
    "these make nine different @xmath51**-cases**. 4 .   finally , we distinguish two * crn - case * : we apply the complex calculation for the posterior means and covariances based on the joint distribution as described above ( case `` iscrn '' ) or we simply use the marginal posterior distributions as if no crn had been used . in this case",
    "( `` nocrn '' ) the posterior means and variances are estimated by standard non - bayesian estimators .",
    "this would be correct in the uncorrelated case with @xmath343 , but it is a simplification in the other cases .",
    "this results in 162 different scenarios . for each scenario , @xmath344",
    "random covariance matrices @xmath51 were generated according to the @xmath51-case chosen and , if the @xmath345-case `` unif '' was used , also @xmath346 random vectors @xmath53 were generated .",
    "for each pair of @xmath347 , @xmath348 repetitions of the different allocation strategies were performed .",
    "we kept track of the random seeds so that all strategies used the same random observations .",
    "-case : comparison of the allocation strategies equalallocation , greedyocba and @xmath349 using the full posterior distribution of the unknown means.,width=340 ]      we first show the results for @xmath53-case `` ufc '' with crn - case `` iscrn '' . in figure",
    "[ fig : ufc_compfulldistrabs ] the mean number of simulations necessary to obtain a pcs @xmath351 with bayesrs is shown for the three allocation strategies equalallocation , greedyocba and @xmath349 .",
    "the @xmath352-axis shows the nine different @xmath353-cases repeated for each of the three r&s - cases `` best@xmath326 '' , `` best@xmath328 '' and `` rank@xmath328 '' . here",
    ", the @xmath102-case with the random covariance matrix is indicated by rnd. the number of simulations on the @xmath354-axis is the mean over the @xmath355 different covariance matrices , each with the given correlation , and the @xmath348 repetitions for each scenario .",
    "figure [ fig : ufc_compfulldistrabs ] clearly shows that the intelligent allocation rules are much more efficient than the simple equal allocation .",
    "also in all scenarios , our new allocation rule @xmath349 performed better than the classical greedyocba .",
    "this holds for all @xmath356-cases , including the ones involving negative correlations as well as the random covariance matrix .",
    "increasing the correlation from @xmath357 to @xmath358 reduced the necessary effort , for all strategies .",
    "figure [ fig : ufc_compmargdistrabs ] shows @xmath53-case `` ufc '' with crn - case `` nocrn '' .",
    "we use the same multi - normal observations as before , but this time the posterior distribution is based on the marginal distribution of the observation only , neglecting the possible dependence . in particular , the complicated definition of the posterior covariance matrix @xmath359 in is replaced by a diagonal matrix @xmath360 with diagonal @xmath361 as defined in .",
    "note that this is justified only in the @xmath356-case @xmath362 , the only case where the observation for different solutions are independent .",
    "-case : comparison of the allocation strategies equalallocation , greedyocba and @xmath363 . here , the posterior distribution is calculated using marginal distributions only.,width=340 ]    nevertheless , figure [ fig : ufc_compmargdistrabs ] shows a similar picture as figure [ fig : ufc_compfulldistrabs ] , @xmath349 is superior in all scenarios , but the absolute values of the number of simulations needed is larger .    in figure",
    "[ fig : ufc_compmargfullabs ] we therefore compare the crn - cases `` iscrn '' and `` nocrn '' for the best allocation strategy @xmath349 and @xmath53-case `` ufc '' .",
    "figure [ fig : ufc_compmargfullabs ] shows that it is indeed worth calculating the full posterior distribution in order to save simulations .",
    "-case : comparison of the two crn - cases for strategy @xmath363.,width=340 ]    finally , figure [ fig : ufc_emppcs ] shows the relative frequency of correct selections and rankings in the above experiments . as it can be seen , it is always well above @xmath364 , indicating that our setup is quite conservative .",
    "-case : the empirical frequency of correct selections in the above experiments.,width=340 ]      next we look at the results for @xmath53-case  inc  , the results are quite similar .",
    "figure [ fig : inc_compfulldistrabs ] shows the crn - case `` iscrn '' and again , @xmath349 performs best , though for the two first r&s - cases best@xmath326 and best@xmath328 , greedyocba has about the same performance .",
    "-case `` inc '' : comparison of the allocation strategies equalallocation , greedyocba and @xmath349 using the full posterior distribution of the unknown means.,width=340 ]    figure [ fig : inc_compfullmarginabs ] again compares the result for @xmath365 for the estimators based on full posterior distributions and the ones using only marginal information .",
    "the advantage of the full distribution is obvious .",
    "the empirical pcs was well above 95% in all cases .",
    "-case `` inc '' : comparison of the crn - cases for the strategy @xmath349.,width=340 ]      in the `` unif''-case , results are means over @xmath346 different @xmath53 drawn randomly from @xmath331^l$ ] , over @xmath355 random covariance matrices and @xmath366 repetitions .",
    "results vary considerably depending on the difficulty of the random @xmath53 , so that the mean number of simulations gives only a rough picture of the performance .",
    "figure [ fig : unif_compfulldistrabs ] shows that our new strategy @xmath349 is superior to the other strategies for positively correlated observations and with random covariance matrix , but it uses more simulations than greedyocba for @xmath367 . in figure",
    "[ fig : unif_emppcs ] , it can be seen , that @xmath368 and greedyocba have high empirical pcs except for one of the cases with @xmath367 .",
    "-case `` unif '' : comparison of the allocation strategies equalallocation , greedyocba and @xmath349 using the full posterior distribution of the unknown means.,width=340 ]    -case `` unif '' : the empirical pcs.,width=340 ]      in @xcite ( see also @xcite ) the sequential procedure @xmath34 is introduced .",
    "it is suitable to identify the best solution by iteratively excluding solutions that seem inferior to at least one of the other solutions .    in our tests , @xmath34 needed much more simulation runs than our procedure bayesrs with allocation @xmath368 , in particular in the @xmath53-case  ufc  , where @xmath34 was not able to find the best solution within our limit of 150000 simulations . to make results better comparable",
    ", we biased the setup in favor of @xmath34 . for @xmath34",
    ", we used a larger indifference zone parameter @xmath369 in the @xmath53-cases  ufc  and  inc  , and @xmath370 the actual minimal distance in @xmath53 for  unif-case , thus adapting the parameter to the step between best and second best solution .",
    "for @xmath371 , we used @xmath332 for all @xmath53-cases as before .",
    "figure [ fig : compdpwkn ] shows the mean number of simulations needed for the r&s - case best@xmath326 and the three @xmath53-cases . to make the figure better readable , we scaled down the results for @xmath34 in the  unif-case by a factor @xmath372 . in the @xmath53-case",
    " unif  , we performed only @xmath373 repetitions for @xmath374 covariance matrices and @xmath375 different @xmath53-values to save runtime .    with the relaxed indifference zone parameter @xmath369 ,",
    "@xmath34 was considerably faster than @xmath368 in the ",
    "ufc-case , but missed the empirical pcs of @xmath376 in most of these cases , as can be seen in figure [ fig : compdpwkn_pcs ] .",
    "hence , one may conclude that in most cases our new approach is superior to @xmath34 with respect to the number of simulations needed and empirical error probabilities observed .     and @xmath377 for the r&s - case best@xmath326 .",
    "here , @xmath34 has a larger indifference zone parameter @xmath291 for the first two cases and its ",
    "unif-values are shown multiplied by @xmath378 . ,",
    "width=340 ]     and @xmath377 for the r&s - case best@xmath326 . here , @xmath34 was below @xmath376 for most covariances in the @xmath53-cases",
    " ufc  and unif  .",
    ", width=340 ]",
    "in this paper we presented a new sequential bayesian r&s procedure with support for common random numbers .",
    "based on an approximation of the posterior distribution of the unknown mean and covariance , the simulation effort could be allocated to solutions in such a way that , empirically , a given pcs can be obtained with less simulations than are needed by other strategies .",
    "in our future work we will extend this concept to multivariate selection problems .",
    "essential parts of this problem were solved in @xcite .",
    "we need the following well - known facts about multivariate normal distributions , see e.g. @xcite or @xcite , sec .",
    "let @xmath379 be a random vector with distribution @xmath380 .",
    "let @xmath381 and @xmath382 be a partition of @xmath383 for some @xmath384 and define @xmath385 so that @xmath386 note that with the notation from ,   we may write @xmath387}$ ] and @xmath388}$ ] . then the marginal distributions of @xmath389 and @xmath390 are normal : @xmath391 moreover , the conditional distribution of @xmath390 given @xmath392 is a @xmath393-dimensional normal distribution @xmath394 with @xmath395 the converse does also hold : if @xmath396 are random variables such that @xmath397 has a @xmath398-distribution and the conditional distribution of @xmath399 given @xmath400 is as in , then the joint distribution of @xmath401 is @xmath402 .    in particular , if @xmath403 so that @xmath404 then we have @xmath405 @xmath406 and @xmath407 } & ( \\sigma_{1d } , \\ldots , \\sigma_{d-1,d})^t\\\\    ( \\sigma_{d1 } , \\ldots , \\sigma_{d , d-1 } ) & \\sigma_{dd } \\end{pmatrix } = \\begin{pmatrix }    { \\bm{\\sigma}}_{[<d ] } & { \\bm{\\sigma}}_{[d,<d]}^t\\\\    { \\bm{\\sigma}}_{[d,<d ] } & \\sigma_{d , d } \\end{pmatrix } , \\ ] ] where we have used the symmetry of the covariance matrix @xmath51 and the notation from .",
    "then from , the conditional distribution of @xmath408 given @xmath409 is a one - dimensional normal distribution with mean @xmath410}^{-1}\\big((y_1 , \\ldots , y_{d-1})^t-(\\mu_1 , \\ldots , \\mu_{d-1})^t\\big ) \\notag\\\\   & = \\mu_d + \\beta_d({\\bm{y}}_{[<d]}-{\\bm{\\mu}}_{[<d]})\\label{eq : defmutilde}\\end{aligned}\\ ] ] and variance ( independent of @xmath411 ) @xmath412}^{-1 } ( \\sigma_{d1 } , \\ldots , \\sigma_{d , d-1})^t \\label{eq : defsigmatilde}\\\\   & = & \\sigma_{dd } - \\beta_d { \\bm{\\sigma}}_{[<d ] } \\beta_d^t.\\notag   \\end{aligned}\\ ] ]        then the likelihood function of the unknown mean @xmath53 given data @xmath130 is @xmath417}-\\bar{{\\bm{x}}}_{[<i]}^{(n_i ) } ) , \\frac{\\tilde{\\sigma}_{(i-1)}}{n_i } ) \\ ] ] where @xmath418 is the one - dimensional normal density with mean @xmath187 and variance @xmath20 .    as we assume that the covariance matrix @xmath51 is known , the likelihood function for @xmath419 given the sample @xmath420 as in can be written as a product of independent blocks of normal densities of different dimensionality .",
    "let @xmath421 be the number of solutions with at least @xmath62 observations .",
    "@xmath422 is the height of the data blocks with at least @xmath62 observations , see fig .",
    "[ fig : datablocks ] .",
    "then with @xmath424 we have for the likelihood function @xmath425 } , { \\bm{\\sigma}}_{[\\le l_i]}\\big),\\ ] ] where @xmath426 is the density function of the @xmath279-dimensional normal distribution . here",
    ", the inner product describes the likelihood for the sample values within one of the blocks as marked in fig .",
    "[ fig : datablocks ] . note that may contain empty products ( defined to be @xmath108 ) if @xmath427 .",
    "now each of the @xmath422-dimensional densities in can be factorized into @xmath422 factors of one - dimensional conditional densities .",
    "we write @xmath428 for the density of the conditional distribution of @xmath383 given @xmath429 . for @xmath430",
    "let @xmath431 then using and we obtain @xmath432 } , { \\bm{\\sigma}}_{[\\le l_i]}\\big)\\notag \\\\    & = f_{x_{1k}}(x_{1k})\\cdot f_{x_{2k}|x_{1k}}(x_{2k}|x_{1k})\\cdot \\ldots\\label{eq : nfactorization } \\\\    & \\hspace*{3cm}\\cdot f_{x_{l_ik}|x_{1,k } , \\ldots , x_{l_i-1k}}(x_{l_ik}|x_{1k } , \\ldots , x_{l_i-1k})\\notag\\\\    & = \\phi_1(x_{1k};\\ \\mu_1,\\sigma_{11})\\cdot \\phi_1(x_{2k};\\ \\tilde{\\mu}_{(1)}(x_{1k}),\\tilde{\\sigma}_{(1 ) } ) \\cdot \\ldots \\notag\\\\    & \\hspace*{3 cm } \\cdot \\phi_1\\big(x_{l_ik};\\ \\tilde{\\mu}_{(l_i-1)}(x_{1k } , \\ldots , x_{l_i-1,k}),\\tilde{\\sigma}_{(l_i-1)}\\big)\\notag\\\\    & = \\phi_1(x_{1k};\\ \\mu_1,\\sigma_{11})\\cdot \\phi_1(x_{2k};\\    \\tilde{\\mu}_{(1)}({\\bm{x}}_{[<2,k]}),\\tilde{\\sigma}_{(1 ) } ) \\cdot \\ldots \\notag\\\\    & \\hspace * { 3 cm } \\cdot\\phi_1\\big(x_{l_ik};\\    \\tilde{\\mu}_{(l_i-1)}({\\bm{x}}_{[<l_i , k]}),\\tilde{\\sigma}_{(l_i-1)}\\big)\\notag   \\end{aligned}\\ ] ] where @xmath433}:=(x_{1k } , \\ldots , x_{i-1,k})^t .",
    "$ ] plugging into and reordering terms we get @xmath434}),\\tilde{\\sigma}_{(i-1)}).\\ ] ] dropping terms that do not depend on @xmath53 ( remember that @xmath51 was assumed to be known ) we obtain from straightforward calculations",
    "@xmath435}),\\tilde{\\sigma}_{(i-1)})&\\propto\\notag\\\\ \\phi_1(\\bar{{\\bm{x}}}_i;&\\ \\mu_i+\\beta_i(\\bar{{\\bm{x}}}_{[<i]}^{(n_i ) } - { \\bm{\\mu}}_{[<i ] } ) , \\tilde{\\sigma}_{(i-1)}/n_i)\\label{eq : prop2}\\end{aligned}\\ ] ] for @xmath436 . from , and we therefore obtain @xmath437}^{(n_i ) } - { \\bm{\\mu}}_{[<i ] } ) , \\tilde{\\sigma}_{(i-1)}/n_i ) \\notag\\\\ & = \\phi_1(\\mu_1;\\ \\bar{{\\bm{x}}}_{1 } , \\sigma_{11}/n_1 ) \\cdot \\prod_{i=2}^l \\phi_1(\\mu_i;\\ \\bar{{\\bm{x}}}_i + \\beta_i",
    "(   { \\bm{\\mu}}_{[<i]}-\\bar{{\\bm{x}}}_{[<i]}^{(n_i ) } ) , \\tilde{\\sigma}_{(i-1)}/n_i).\\label{eq : posteriorfactorized}\\end{aligned}\\ ] ]      moreover , from we see , that is the factorized representation of a @xmath46-dimensional normal density @xmath440 where the mean @xmath441 and covariance matrix @xmath173 are such that the operations in lead to the representation in . to find @xmath442 and @xmath173 we have to undo the operations from",
    "this is possible since we see from that @xmath443}-\\bar{{\\bm{x}}}_{[<i]}^{(n_i ) } ) , \\tilde{\\sigma}_{(i-1)}/n_i).\\label{eq : dichtewix }   \\end{aligned}\\ ] ] from this it is also obvious , that @xmath444},w_{[<i ] } } (   { \\bm{\\mu}}_{[<i]}\\mid { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]})\\end{aligned}\\ ] ] where @xmath445 and @xmath446}$ ] denotes all observations for solutions @xmath447 .",
    "this means that the conditional expressions do not depend on observations for solutions @xmath448 resp .",
    "therefore we can conclude for the means @xmath450 @xmath451\\ = \\ { \\text{\\textbf{e}}}[w_1 \\mid { \\bm{x}}_{1{\\,\\bm{\\cdot}\\,}}={\\bm{x}}_{1{\\,\\bm{\\cdot}\\ , } } ] = \\bar{{\\bm{x}}}_{1 } \\label{eq : nu1}\\\\   \\nu_i&= { \\text{\\textbf{e}}}[w_i \\mid { \\bm{x}}={\\bm{x}}]\\ = \\ { \\text{\\textbf{e}}}[w_i \\mid { \\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\ , } ] } ]   \\label{eq : nu2}\\\\   & =   { \\text{\\textbf{e}}}\\big[{\\text{\\textbf{e } } } [ w_i\\mid { \\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]},w_{[<i ] } ] \\",
    "\\big|\\ { \\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\   & = { \\text{\\textbf{e}}}\\big [ \\bar{{\\bm{x}}}_i + \\beta_i (   w_{[<i]}-\\bar{{\\bm{x}}}_{[<i]}^{(n_i ) } ) \\ \\big|\\    { \\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\ & = \\bar{{\\bm{x}}}_i + \\beta_i\\big({\\text{\\textbf{e}}}\\big[w_{[<i]}\\ \\big|\\   { \\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[\\le i,{\\,\\bm{\\cdot}\\,}]}\\big ] -\\bar{{\\bm{x}}}_{[<i]}^{(n_i)}\\big)\\notag\\\\   & = \\bar{{\\bm{x}}}_i + \\beta_i ( { \\bm{\\nu}}_{[<i ] } - \\bar{{\\bm{x}}}_{[<i]}^{(n_i)})\\qquad \\text { for } i=2 , \\ldots , l. \\notag   \\end{aligned}\\ ] ] for the @xmath452 posterior covariance matrix @xmath453=\\big({\\text{cov}}[w_k , w_l\\mid { \\bm{x}}={\\bm{x}}]\\big)_{k , l=1 , \\ldots , l}\\ ] ] we again put @xmath454 } : = \\big({\\text{cov}}[w_k , w_l\\mid { \\bm{x}}={\\bm{x}}]\\big)_{k , l=1 , \\ldots , i}.\\ ] ] then we have for @xmath455 that @xmath456 } $ ] equals @xmath457 \\\\        { \\bm{\\lambda}}_{[<i ] } & \\qquad\\vdots\\\\        & { \\text{cov}}[w_{i-1},w_i| { \\bm{x}}={\\bm{x}}]\\\\[0.9ex]\\hline        \\rule[0.6ex]{0pt}{2ex}{\\text{cov}}[w_i , w_1| { \\bm{x}}={\\bm{x}}]\\ \\ldots\\ { \\text{cov}}[w_i , w_{i-1}|        { \\bm{x}}={\\bm{x } } ] & { \\text{cov}}[w_i , w_i| { \\bm{x}}={\\bm{x } } ] \\end{array}\\right)\\ ] ] to determine the entries of this matrix , we use the following facts about the conditional covariance which are straightforward to prove .",
    "let @xmath458 and @xmath459 be random variables such that all integrals exist and let @xmath460 be a suitable function : @xmath461&= { \\text{\\textbf{e}}}[u\\cdot v\\mid z]-{\\text{\\textbf{e}}}[u\\mid z]\\cdot { \\text{\\textbf{e}}}[v\\mid z],\\notag\\\\{\\text{cov}}\\big[u , \\big(v+g(z)\\big)\\ \\big|\\ z\\big]&={\\text{cov}}[u , v\\mid z],\\notag \\\\   { \\text{cov}}[u , v\\mid z]&= { \\text{cov}}\\big[u,\\ { \\text{\\textbf{e}}}[v\\mid y , z]\\;\\big |\\ , z\\big ] \\ \\text { if $ u = g(y ) , $ } \\label{eq : covprop1}\\\\ { \\text{cov}}[u , u\\mid z]&= { \\bm{v}}[u\\mid z]\\label{eq : covprop2}\\\\ & =   { \\text{\\textbf{e}}}\\big[{\\bm{v}}[u\\mid z , y]\\;\\big |\\ , z\\big ] + { \\bm{v}}\\big[{\\text{\\textbf{e}}}[u\\mid z , y]\\,\\big |\\ ; z\\big].\\notag \\ ] ] we now have from @xmath462}&={\\text{cov}}[w_1,w_1\\mid { \\bm{x}}={\\bm{x}}]\\notag\\\\ & =   { \\bm{v}}[w_1\\mid { \\bm{x}}={\\bm{x}}]= \\frac{\\sigma_{11}}{n_1}={\\text{cov}}[w_1,w_1\\mid { \\bm{x}}_{1{\\,\\bm{\\cdot}\\,}}={\\bm{x}}_{1{\\,\\bm{\\cdot}\\ , } } ] \\label{eq : lambda1}\\end{aligned}\\ ] ] and similarly from and @xmath463 = { \\bm{v}}[w_i\\mid { \\bm{x}}={\\bm{x}}]\\notag\\\\ & = { \\text{\\textbf{e}}}\\big [ { \\bm{v } } [ w_i|{\\bm{x } } , w_{[<i ] } ] \\;\\big|\\ , { \\bm{x}}={\\bm{x}}\\big]+{\\bm{v}}\\big[{\\text{\\textbf{e}}}[w_i\\mid { \\bm{x } } , w_{[<i]}]\\;\\big|\\ , { \\bm{x}}={\\bm{x}}\\big]\\label{eq : covecke}\\\\ & = { \\text{\\textbf{e}}}\\big [ \\frac{\\tilde{\\sigma}_{(i-1)}}{n_i } \\;\\big|\\ , { \\bm{x}}={\\bm{x}}\\big]\\notag\\\\ & \\hspace * { 2cm}+{\\bm{v}}\\big[\\bar{x}_i+\\beta_i(w_{[<i]}-\\bar{x}_i^{(n_i ) } ) \\;\\big|\\ , { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\ & = \\frac{\\tilde{\\sigma}_{(i-1)}}{n_i } + { \\bm{v}}\\big[\\beta_i w_{[<i ] } \\;\\big|\\ , { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\ & = \\frac{1}{n_i}(\\sigma_{ii}-\\beta_i{\\bm{\\sigma}}_{[<i]}\\beta_i^t ) + \\beta_i\\;{\\text{cov}}\\big[w_{[<i]}\\mid { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]\\ ; \\beta_i^t\\notag\\\\ & = \\frac{1}{n_i}(\\sigma_{ii}-\\beta_i{\\bm{\\sigma}}_{[<i]}\\beta_i^t ) + \\beta_i \\ { \\bm{\\lambda}}_{[<i]}\\ \\beta_i^t\\notag.\\end{aligned}\\ ] ] finally , for @xmath464 we have using and @xmath465 =   { \\text{cov}}\\big[w_k , w_i\\ \\big|\\    { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]}\\label{eq : covspalte}\\\\    & = & { \\text{cov}}\\big[w_k,\\ { \\text{\\textbf{e}}}[w_i\\mid w_{[<i]},{\\bm{x}}_{[\\le i ] } ] \\",
    "\\big|\\   { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\    & = & { \\text{cov}}\\big[w_k,\\ \\bar{x}_i+\\beta_i(w_{[<i]}-\\bar{{\\bm{x}}}_{[<i]}^{(n_i)})\\ \\big|\\   { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\    & = & { \\text{cov}}\\big[w_k,\\ \\beta_i w_{[<i]}\\ \\big|\\   { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}\\big]\\notag\\\\    & = & \\beta_i \\big({\\text{cov}}[w_k , w_l\\ \\big|\\ { \\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}={\\bm{x}}_{[<i,{\\,\\bm{\\cdot}\\,}]}]\\big)^t_{l=1 , \\ldots , i-1}\\notag\\\\    & = &   \\beta_i\\ { \\bm{\\lambda}}_{[<i]}.\\notag\\end{aligned}\\ ] ] from  -   and we now obtain the recursion @xmath466}=\\frac{\\sigma_{11}}{n_1},\\qquad { \\bm{\\lambda}}_{[\\le i]}= \\left(\\begin{array}{c|l }        { \\bm{\\lambda}}_{[<i ] } & { \\bm{\\lambda}}_{[<i]}\\ \\beta_i^t \\\\[0.9ex]\\hline        \\rule[0.5ex]{0pt}{2ex}\\beta_i\\ { \\bm{\\lambda}}_{[<i ] } & \\frac{1}{n_i}(\\sigma_{ii}-\\beta_i{\\bm{\\sigma}}_{[<i]}\\beta_i^t ) + \\beta_i\\   { \\bm{\\lambda}}_{[<i]}\\ \\beta_i^t \\end{array}\\right)\\ ] ] for @xmath467 which concludes the proof of theorem [ the : posterior ] .",
    "jrgen branke , stephen  e. chick , and christian schmidt . . _ management science _ , 530 ( 12):0 19161932 , dec 2007 .",
    "doi : 10.1287/mnsc.1070.0721 .",
    "url http://mansci.journal.informs.org/cgi/doi/10.1287/mnsc.1070.0721 .",
    "chun - hung chen , hsiao - chang chen , and liyi dai . .",
    "in john  m. charnes , douglas  j. morrice , daniel  t. brunner , and james  j. swain , editors , _ proceedings of the 1996 winter simulation conference _ , pages 398405 .",
    "ieee , 1996 .",
    "isbn 0 - 7803 - 3383 - 7 .",
    "doi : 10.1109/wsc.1996.873307 .",
    "chun - hung chen , jianwu lin , enver ycesan , and stephen  e. chick . .",
    "_ discrete event dynamic systems _ , 100 ( 3):0 251270 , 2000 .",
    "doi : 10.1023/a:1008349927281 .",
    "url http://www.springerlink.com/index/p132u04nm122g459.pdf .",
    "chun - hung chen , donghai he , michael fu , and loo  hay lee . . _ informs journal on computing _ , 200 ( 4):0 579595 , sep 2008 .",
    "doi : 10.1287/ijoc.1080.0268 .",
    "url http://joc.journal.informs.org/cgi/doi/10.1287/ijoc.1080.0268 .",
    "hsiao - chang chen , liyi dai , chun - hung chen , and enver ycesan . .",
    "in sigrun andradottir , kevin  j. healy , david  h. withers , and barry  l. nelson , editors , _ proceedings of the 1997 winter simulation conference _ , pages 334341 .",
    "ieee , 1997 .",
    "isbn 0 - 7803 - 4278-x .",
    "doi : 10.1109/wsc.1997.640417 .",
    "stephen  e. chick and koichiro inoue . .",
    "_ management science _ , 470 ( 8):0 11331149 , aug 2001 .",
    "doi : 10.1287/mnsc.47.8.1133.10229 .",
    "url http://mansci.journal.informs.org/cgi/doi/10.1287/mnsc.47.8.1133.10229 .",
    "christian schmidt , jrgen branke , and stephen  e. chick . .",
    "in franz rothlauf and others , editors , _ applications of evolutionary computing _ , pages 752763 . 2006 .",
    "doi : 10.1007/11732242_73 .",
    "url http://www.springerlink.com/index/jk184206618h1248.pdf ."
  ],
  "abstract_text": [
    "<S> we want to select the best systems out of a given set of systems ( or rank them ) with respect to their expected performance . the systems allow random observations only and we assume that the joint observation of the systems has a multivariate normal distribution with unknown mean and covariance . </S>",
    "<S> we allow dependent marginal observations as they occur when common random numbers are used for the simulation of the systems . </S>",
    "<S> in particular , we focus on positively dependent observations as they might be expected in heuristic optimization where ` systems ' are different solutions to an optimization problem with common random inputs .    in each iteration , we allocate a fixed budget of simulation runs to the solutions . </S>",
    "<S> we use a bayesian setup and allocate the simulation effort according to the posterior covariances of the solutions until the ranking and selection decision is correct with a given high probability . here , the complex posterior distributions are approximated only but we give extensive empirical evidence that the observed error probabilities are well below the given bounds in most cases .    </S>",
    "<S> we also use a generalized scheme for the target of the ranking and selection that allows to bound the error probabilities with a bonferroni approach . </S>",
    "<S> our test results show that our procedure uses less simulations than comparable procedures from literature even in most of the cases where the observations are not positively correlated .    </S>",
    "<S> * keywords : * sequential ranking and selection , common random numbers , bayesian statistics , multiple testing , missing data </S>"
  ]
}