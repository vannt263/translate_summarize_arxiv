{
  "article_text": [
    "a major goal of current psychiatric neuroimaging research is to predict clinical outcomes on the basis of quantitative image data .",
    "many studies have focused on `` predicting '' current disease states from brain images [ e.g. , @xcite ] . while seemingly less difficult than accurate prediction of _ future _ outcomes , the goal of clinically useful imaging - based diagnosis has proved highly challenging [ @xcite ] .",
    "this paper addresses two important limitations of standard methods for using brain images to predict psychiatric outcomes :    ordinarily , the voxels ( volume units ) of the brain are treated as interchangeable predictors or `` features . ''",
    "accuracy might be improved by properly exploiting the spatial arrangement of the brain .",
    "in some cases brain images may prove successful for diagnostic classification , but only because the images are related to one or more scalar covariates that drive the association .",
    "this is a nonstandard form of confounding , and there seems to be no existing methodology for detecting it . in other words , little is known about how to assess whether image data offers `` added value '' for prediction , beyond what is available from nonimage data  which will typically be much simpler to acquire .    to address limitation ( i ) , we approach the general problem as one of regressing scalar responses on _ image predictors _ , which are viewed , as in @xcite and @xcite , as a challenging special case of functional predictors [ @xcite ] .",
    "the responses @xmath0 are assumed to be generated independently by the model @xmath1 here @xmath2 denotes an exponential family distribution with mean @xmath3 and scale parameter @xmath4 , along with a link function @xmath5 ; @xmath6 is an @xmath7-dimensional vector of ( scalar ) covariates , of which the first is the constant 1 ; @xmath8 is a functional predictor with domain @xmath9 or @xmath10 ; and the corresponding effect , the _ coefficient function _ or _ coefficient image _ @xmath11 , is the parameter of interest . the simplest special case is the linear model @xmath12 where the @xmath13 are independent and identically distributed errors with mean 0 and variance @xmath14 . when @xmath15 ( i.e. , no scalar covariates ) , model ( [ flm ] ) is the extension , from one - dimensional to multidimensional predictors , of the functional linear model that has been studied by @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and many others .    for the case of one",
    "- dimensional functional predictors , a popular way to take spatial information into account is to restrict @xmath16 to the span of a spline basis [ e.g. , @xcite ] .",
    "spline methods for two - dimensional predictors have been studied by @xcite and @xcite , and by @xcite , whose work was motivated by neuroimaging applications .",
    "some more recent work has considered neuroimaging applications with two- and three - dimensional predictors [ @xcite ] . in this paper , we propose a set of new approaches based on a wavelet representation of the coefficient image .",
    "the idea of transforming the images to the wavelet domain has previously appeared in the brain mapping literature , where it is customary to fit separate models at each voxel , with the image - derived quantity regressed on demographic or clinical variables of interest [ e.g. , @xcite ] .",
    "but for our objective of using entire images in a _",
    "single _ model to predict a scalar response , working in the wavelet domain has been mentioned as a natural idea [ @xcite ] but rarely if ever pursued , at least until the very recent work of @xcite . unlike spline bases , wavelet bases are designed for sparse representation and yield estimates that adapt to the features of the coefficient image .",
    "limitation ( ii ) was highlighted by the results of the recent adhd-200 global competition for automated diagnosis of attention deficit / hyperactivity disorder [ @xcite ] .",
    "teams were provided with functional magnetic resonance images from adhd subjects and controls on which to train diagnostic algorithms , and then applied these algorithms to predict diagnosis in a separate set of images .",
    "a team of biostatisticians from johns hopkins university , whose methods are described by @xcite , achieved the highest score for correct imaging - based classification and were declared the winners .",
    "but a team from the university of alberta , which discarded the images and used just four scalar predictors [ age , sex , handedness and iq ; see @xcite ] , attained a slightly higher classification score [ see @xcite for related discussion ] .    to address limitation ( ii ) , we test the effect of image predictors via a permutation - based approach originally proposed in the neuroimaging literature [ @xcite ] , which we extend to allow for scalar covariates .",
    "we also consider how to extend the traditional notion of confounding to settings with both scalar and image predictors .",
    "these ideas are illustrated using our wavelet methods , but are not specific to them ; rather , they are applicable with other approaches to functional or high - dimensional regression .",
    "our contributions can be summarized as follows : ( i ) we propose novel wavelet - domain methodology for regression with image predictors . while @xcite and @xcite have studied the wavelet - domain lasso for image predictors , we also propose and compare several other methods , and consider the generalized linear case and the role of scalar covariates .",
    "( ii ) we extend predictive performance - based hypothesis testing [ @xcite ] to the case where scalar confounders are present , providing a new way to assess the usefulness of image - based prediction .",
    "in section  [ wave ] we introduce wavelet bases , and motivate and outline a general template for scalar - on - image regression in the wavelet domain .",
    "section  [ twda ] describes three specific algorithms , which are evaluated in simulations in section  [ simsec ] .",
    "section  [ infsec ] considers hypothesis testing and confounding with image predictors . in section  [ appsec ] the proposed methods",
    "are applied to a portion of the adhd-200 data set , and the results point to a possible role of confounding in the competition s surprising result .",
    "section  [ discuss ] offers a concluding discussion .",
    "wavelet bases are a popular way to obtain a sparse representation for functional data , in particular , when the degree of smoothness exhibits local variation [ see @xcite for statistically - oriented treatments ] .",
    "a wavelet basis for @xmath17 is constructed from a scaling function ( or `` father wavelet '' ) @xmath4 and a wavelet function ( `` mother wavelet '' ) @xmath18 [ see figure  [ figdaub](a)(b ) ] , with the following properties :    * for each @xmath19 , the shifted and dilated functions @xmath20 form an orthonormal basis for @xmath21 , where @xmath22 is a nested sequence of subspaces whose union is a dense subspace of @xmath17 . * for each @xmath19 , @xmath23 form an orthonormal basis for a `` detail space '' @xmath24 satisfying @xmath25 .",
    "hence , for any integer @xmath26 , @xmath27 is a dense subspace of @xmath17 .     and wavelet function @xmath18 for 1d @xcite `` least - asymmetric '' wavelets with 10 vanishing moments .",
    "2d basis functions are formed from tensor products such as @xmath28 and @xmath29 . ]",
    "given appropriate boundary handling , such as modifying the scaling and wavelet functions to be periodic , one can likewise construct orthonormal wavelet bases for @xmath30 $ ] , of the form @xmath31 that is , @xmath32 scaling functions ( corresponding to the large - scale features of the data ) , @xmath32 wavelet functions at level @xmath33 , @xmath34 wavelet functions at level @xmath35 and so on , with higher wavelet levels capturing finer - scale details .",
    "this multiscale structure is what makes wavelet bases so useful for sparse representation of functions with varying degrees of smoothness .",
    "the wavelet decomposition level @xmath33 acts as a tuning parameter .",
    "a small @xmath33 implies that a small number ( @xmath32 ) of scaling functions are used to construct the macro features of the function , with most of the basis elements dedicated to providing detail at a variety of scales .",
    "a large @xmath33 allows for many more scaling functions , each at higher resolution , and thus fewer basis elements corresponding to detail .    in practice ,",
    "a function @xmath36 $ ] is observed at finitely many points , ordinarily taken to be the @xmath37 ( for some positive integer @xmath38 ) equally spaced points @xmath39 .",
    "( when the function is observed at a number of points that is not a power of 2 , one can insert zeroes before and after to attain the next highest power of  2 . ) the observed values can then be interpolated by the @xmath40-dimensional truncated basis @xmath41 the discrete wavelet transform ( dwt ) , implemented by the @xmath42 pyramid algorithm of @xcite , expands @xmath43 with respect to this basis . given a judicious choice of @xmath4 and @xmath18 , signals of varying smoothness can be well represented with a small number of coefficients . throughout this paper",
    "we use the compactly supported @xcite `` least - asymmetric '' wavelets with 10 vanishing moments , displayed in figure  [ figdaub ] .",
    "wavelet bases for two dimensions can be constructed by taking tensor products of the @xmath4 and @xmath18 functions .",
    "the two - dimensional scaling function is @xmath44 and there are three types of 2d wavelets : @xmath45 , @xmath46 and @xmath47 , roughly corresponding to `` horizontal , '' `` vertical '' and `` diagonal '' detail , respectively [ see figure  [ figdaub](c)(d ) ] .",
    "these functions are dilated and translated just as their 1d counterparts are .",
    "wavelet bases for 3d are constructed similarly .",
    "@xcite discuss alternative wavelet transforms for images that are not constructed as tensor products .",
    "henceforth , the functional predictor @xmath48 of ( [ gmuspec ] ) , ( [ flm ] ) will be replaced by the @xmath49th discretized image observation @xmath50^t$ ] , where @xmath51 are distinct spatial locations at which the function @xmath52 is measured . often , in practice , each image",
    "is given as a matrix or 3d array ; @xmath53 is then obtained by converting this into a vector . from now until section  [ x2glm ] we focus on the linear model ( [ flm ] ) , which can now be written in matrix form as @xmath54 here @xmath55 ; @xmath56 ; @xmath57 is the @xmath58 matrix with @xmath49th row @xmath59 ; @xmath60 is the @xmath61 matrix with @xmath49th row @xmath62 ; and @xmath63 is a similarly discretized version of the coefficient image @xmath64 .",
    "more precisely , for @xmath65 , @xmath66 , where the @xmath67 s are quadrature weights such that @xmath68 is a good approximation to the integral in ( [ flm ] ) ; but for image data , @xmath69 typically form an equally spaced grid , so these weights are taken as constant and hence ignored . with these definitions , ( [ flm ] ) is just the @xmath49th of the @xmath70 equations that make up the vector equation ( [ matrixform ] ) .    to simplify the notation , we shall use a single subscript and denote the wavelet basis functions for a given @xmath33 as @xmath71 .",
    "the wavelet representation of the @xmath49th observed image @xmath52 is @xmath72 , in which the _ wavelet coefficients _ are given by @xmath73 .",
    "the coefficient vector @xmath74 can be written as @xmath75 , where @xmath76 is an @xmath77 orthonormal matrix ( which is not formed explicitly when @xmath78 is computed by the dwt ) .",
    "similarly the discretized coefficient function @xmath79 can be represented in terms of its wavelet coefficients as @xmath80 , leading to the wavelet - domain version of model ( [ matrixform ] ) : @xmath81\\\\[-8pt]\\nonumber & = & { \\mathbf{t}}{\\bolds{\\delta}}+ \\tilde{{\\mathbf{x}}}\\tilde{{\\bolds{\\beta } } } + { \\bolds{\\varepsilon}},\\end{aligned}\\ ] ] where @xmath82 is the @xmath83 matrix with @xmath49th row @xmath84 .",
    "the key point is that the wavelet - domain form ( [ wdform ] ) is better suited than the original form ( [ matrixform ] ) for applying sparse techniques for high - dimensional regression  both because wavelet bases are designed for sparse representation of images [ @xcite ] and because the dwt approximately decorrelates or `` whitens '' data [ @xcite ] .",
    "we can thus formulate a `` meta - algorithm '' for scalar - on - image regression in the wavelet domain :    apply the dwt to the image predictors to transform model ( [ matrixform ] ) into model  ( [ wdform ] ) .",
    "use some high - dimensional regression methodology to derive a sparse estimate  @xmath85 .",
    "apply the inverse dwt to @xmath85 to obtain a coefficient image estimate @xmath86 for the original model ( [ matrixform ] ) .",
    "different choices for step 2 lead to specific algorithms , as described in the next section .",
    "the above general scheme can be extended to multiple image predictors [ cf .",
    "we note that this meta - algorithm has been applied before for 1d functional predictors [ @xcite ] and more for image predictors [ @xcite ] .",
    "past work on wavelet - domain classification , as opposed to regression [ e.g. , @xcite ] , may bear comparison to our proposed methods . @xcite develop wavelet - domain functional mixed models with images as _ responses_.",
    "the functional linear model ( [ flm ] ) is often fitted by assuming the coefficient function has a truncated functional principal component , or karhunen  love , representation @xmath87 , where @xmath7 is a positive integer and @xmath88 are the first @xmath7 eigenfunctions of the covariance operator associated with the predictor functions @xmath52 [ e.g. , @xcite ] .",
    "the eigenfunctions @xmath89 can be estimated by viewing the functional predictors as ( highly ) multivariate data , and applying ordinary principal component analysis to the predictor matrix @xmath60 .    here and in section  [ swdpls ]",
    ", we assume that @xmath60 has mean - centered columns , that is , @xmath90 .",
    "the approach of the previous paragraph then amounts to assuming @xmath91 for some @xmath92 , where @xmath93 is the singular value decomposition of @xmath60 , and @xmath94 comprises the leading @xmath7 columns of @xmath95 .",
    "hence , estimation reduces to choosing @xmath96 to minimize the principal component regression [ pcr ; @xcite ] criterion @xmath97 ( this is a slightly nonstandard pcr criterion , in that principal component reduction is applied only to @xmath60 but not to @xmath57 .",
    "a similar remark applies to the other criteria introduced below . )",
    "as shown by @xcite , pcr can be implemented more effectively by exploiting the functional character of the data . in the one - dimensional functional predictor case , this has usually meant forming smooth estimates of the eigenfunctions  as in the fpcr@xmath98 method of @xcite , which expands the eigenfunctions with respect to a @xmath99-spline basis [ cf .",
    "but for image predictors , local adaptivity  the ability to capture sharp features in some areas vs. a high degree of smoothness elsewhere  becomes particularly important .",
    "this motivates using a wavelet basis , rather than a spline basis , to represent the eigenfunctions , or , in other words , developing a wavelet - domain version of pcr as an instance of the meta - algorithm of section  [ metalg ] .",
    "a _ _ non__sparse wavelet - domain pcr estimate would minimize @xmath100 which is analogous to ( [ pcrcrit ] ) but based on the svd of @xmath101 rather than of @xmath60 .",
    "however , the advantage of working in the wavelet domain is to obtain a sparse coefficient estimate by replacing the pc weights @xmath102 with weights from a sparse version of pca .",
    "several penalty - based methods have been proposed for sparse pca [ e.g. , @xcite ] , but we opted for the approach of @xcite , which is simpler than the penalized methods and , unlike them , was developed with a view toward sparse wavelet representations of signals .",
    "@xcite propose to select the features or coordinates with highest variance , and apply pca only to these .",
    "the resulting sparse pcr criterion is @xmath103 here @xmath104 consists of the @xmath105 columns of @xmath82 having highest variance , and @xmath106 consists of the leading @xmath7 columns of @xmath107 , where @xmath108 is the svd of @xmath104 .",
    "the minimizer @xmath109 of ( [ spcrcrit ] ) can be obtained by simple least squares .",
    "the vector of wavelet coefficient estimates is then @xmath110 , and the coefficient image estimate @xmath111 is derived by the inverse dwt .      whereas pcr reduces dimension by regressing on the leading pcs of the predictors , partial least squares [ pls ; @xcite ] works by regressing on a set of components that are relevant to predicting the responses .",
    "a ( nonsparse ) wavelet - domain pls estimate [ cf .",
    "@xcite ] is derived by minimizing @xmath112 [ cf .",
    "( [ nspcrcrit ] ) ] , where the columns of @xmath113 are defined iteratively as follows [ @xcite ] :    * @xmath114 ; * for @xmath115 , @xmath116    once again , however , the point of working in the wavelet domain is to obtain a sparse estimate . to define sparse wavelet - domain pls , as with pcr , we could have used penalization to derive sparse pls components [ @xcite ] , but we instead opted to build on the aforementioned approach of @xcite to sparse pca .",
    "a natural pls analogue of that approach is to select those features @xmath117 whose covariance with @xmath118 has the greatest magnitude .",
    "this results in the sparse pls criterion @xmath119 here @xmath120 consists of the @xmath105 columns of @xmath101 having highest covariance with @xmath121 , and the columns of @xmath122 are defined analogously to those of @xmath113 in ( [ nsplscrit ] ) . as for pcr",
    ", the least - squares minimizer @xmath109 of ( [ splscrit ] ) leads directly to estimates of the wavelet coefficients @xmath123 and of the resulting coefficient image @xmath79 .",
    "our pls algorithm is a wavelet - domain counterpart of the spline - based functional pls procedure denoted by fpls@xmath98 in @xcite .",
    "we note that @xcite and @xcite have proposed more explicitly functional formulations of pls , based on covariance operators on function spaces .",
    "since wavelet bases are well suited for sparse representation of functions , recent work has considered combining them with sparsity - inducing penalties , both for semiparametric regression [ @xcite ] and for regression with functional or image predictors [ @xcite ] .",
    "the latter papers focused on @xmath124 penalization , also known as the lasso [ @xcite ] , in the wavelet domain .",
    "alternatives to the lasso include the scad penalty [ @xcite ] and the adaptive lasso [ @xcite ] .",
    "here we consider the elastic net ( en ) estimator for wavelet - domain model ( [ wdform ] ) , which minimizes @xmath125\\ ] ] over @xmath126 , for a regularization parameter @xmath127 and a mixing parameter @xmath128 $ ] which controls the relative strength of the @xmath124 and @xmath129 penalties on the coefficients [ @xcite ] .    in the original nomenclature of @xcite ,",
    "the minimizer of ( [ encrit ] ) is the `` nave '' en , whereas en is a rescaled version . since we shall make use of the generalized linear extension of en as implemented by @xcite , we follow these authors in omitting the rescaling step",
    "when @xmath130 , the @xmath124 penalty shrinks small coefficients to zero , leading to a sparse wavelet representation .",
    "the wavelet - domain lasso is obtained when @xmath131 .",
    "as explained by @xcite , given a group of important features that are highly correlated , the lasso tends to select just one , whereas en selects the entire group , which is often preferable  even in the wavelet domain , notwithstanding the `` whitening '' property of the discrete wavelet transform .",
    "all three of the above methods seek to represent the coefficient image @xmath16 sparsely , as a linear combination of a subset of the wavelet basis functions , but they deploy very different strategies to choose that subset .",
    "the @xmath124 penalty in the elastic net criterion ( [ encrit ] ) has the effect of shrinking small coefficients to zero .",
    "this can be interpreted as imposing a prior that favors a sparse estimate .",
    "the pcr criterion ( [ spcrcrit ] ) eliminates basis elements _ before _ performing regression , based on an implicit assumption that those basis elements with low variance in the data have little to contribute to the coefficient image .",
    "this assumption is broadly consistent , on the one hand , with the assumption of @xcite that such basis elements are merely capturing noise ; and , on the other hand , with the underlying assumption of pcr , namely , that the highest - variance principal components are most relevant in regression [ see @xcite for some relevant discussion ] .",
    "the pls criterion ( [ splscrit ] ) likewise lets the data determine which basis elements to include ; but here , instead of considering only the wavelet - transformed image data @xmath82 as in pcr , we define relevant components by iteratively maximizing covariance with the responses @xmath121 .",
    "the above three wavelet - domain algorithms can be straightforwardly extended from linear to generalized linear models ( glms ) of the form @xmath132={\\mathbf{t}}{\\bolds{\\delta}}+ \\tilde{{\\mathbf{x}}}\\tilde{{\\bolds{\\beta}}},\\ ] ] for a link function @xmath5 , generalizing ( [ wdform ] ) . for pcr ,",
    "one simply fits a glm , as opposed to a linear model , to the sparse pcs . for the elastic net , the ` glmnet ` algorithm of @xcite",
    "is available for the generalized linear case .",
    "pls is sometimes performed in an iteratively reweighted manner for glms [ @xcite ] , but in high - dimensional settings , such algorithms may require nontrivial modification [ e.g. , @xcite ] to avoid convergence . here",
    "we view pls as a generic approach to constructing relevant components , which may be employed beyond the linear regression setting [ e.g. , @xcite ] .",
    "thus , we construct pls components exactly as we would for a linear model , but then use these components to fit a glm .      for wavelet - domain pcr and pls , three tuning parameters must be selected : the resolution - level parameter @xmath33 ; the number @xmath105 of wavelet coefficients to retain [ i.e. , the number of columns of @xmath133 in ( [ spcrcrit ] ) or of @xmath120 in ( [ splscrit ] ) ] ; and the number @xmath7 of pcs or pls components .",
    "we generally fix @xmath134 , since we have found that resolution level to be generally either optimal or near - optimal as measured by cross - validation ( cv ) . for wavelet - domain",
    "elastic net , one must choose @xmath33 and the two penalty parameters @xmath135 and @xmath136 in ( [ encrit ] ) , but we again prefer to fix @xmath134 .",
    "these tuning parameters are chosen by repeated @xmath137-fold cv . in the @xmath138th of @xmath139 repetitions we divide the data points @xmath140 ( @xmath141 ) into @xmath137 equal - sized validation sets indexed by @xmath142 .",
    "we can then choose the tuning parameters to minimize the cv score @xmath143 where @xmath144 are the estimates that result when model ( [ wdglm ] ) is fitted ( by pcr , pls or en ) with the observations indexed by @xmath145 excluded , and @xmath146 is an appropriate loss function . for linear regression",
    "the standard loss function is the squared error @xmath147 .",
    "for the generalized linear case , following @xcite , we use the deviance @xmath148 as the loss function .",
    "specifically for logistic regression , unusually large summands can dominate criterion  ( [ rkcv ] ) . therefore , similarly to @xcite , we instead choose the tuning parameters by a robust cv score that takes the median rather than the mean over each set of @xmath137 validation sets : @xmath149",
    "to test the performance of our methods with realistic image predictors , we created a data set based on the positron emission tomography ( pet ) data previously studied by @xcite .",
    "that data set included axial slices from 33 amyloid beta maps , from which we extracted a square region of @xmath150 voxels . to generate a larger sample of @xmath151 images , we applied a procedure similar to that of @xcite :    we estimated the ( vectorized ) principal components ( eigenimages ) @xmath152 with corresponding eigenvalues @xmath153 .    for @xmath154 , we generated the @xmath49th simulated predictor image as @xmath155 , with the @xmath156 s simulated independently from the @xmath157 distribution .    in step 1 above we used the sparse pca method of @xcite , including the 492 wavelet coefficients having the highest variance .",
    "this number of wavelet coefficients was sufficient to capture 99.5% of the `` excess '' variance , in the sense of section  4.2 of @xcite .",
    "( left ) and @xmath158 ( right ) used in the simulation study . ]",
    "we used two different true coefficient images @xmath159 , which are shown in figure  [ figomega ] .",
    "the first image @xmath160 is similar to that used by @xcite .",
    "taking its domain to be @xmath161 ^ 2 $ ] , this coefficient image is given by @xmath162 , where @xmath163 are the densities of the bivariate normal distributions @xmath164\\quad\\mbox{and}\\quad n \\left [ \\pmatrix{20 \\cr 55 } , 10{\\mathbf{i}}_2 \\right],\\ ] ] respectively .",
    "the second image @xmath158 is a two - dimensional analogue of the `` bumps '' function used by @xcite , and many subsequent authors , to illustrate the properties of wavelets",
    ".    we then simulated continuous or binary outcomes @xmath0 with specified approximate values of the coefficient of determination @xmath165 , in the sense detailed in supplementary appendix  a.1 [ @xcite ] .",
    "we generated 100 sets of @xmath151 continuous outcomes and 100 sets of 500 binary outcomes , for each of the @xmath165 values @xmath166 .",
    "we compared the performance of the three wavelet - domain methods described in section  [ twda ] with three analogous `` voxel - domain '' methods , that is , sparse pcr , sparse pls and elastic net without transformation to the wavelet domain .",
    "the wavelet- and voxel - domain methods are denoted by wpcr , wpls and wnet and by vpcr , vpls and vnet , respectively .",
    "we also included the @xmath99-spline - based functional pcr method ( `` fpcr@xmath167 , '' or simply fpcr ) of @xcite ( @xcite ) .",
    "tuning parameter selection was as described in supplementary appendix  a.1 [ @xcite ] .",
    "performance was evaluated in terms of estimation error and prediction error .",
    "estimation error is defined by the scaled mean squared error ( mse ) @xmath168 , where @xmath169 are the true and estimated coefficient images .",
    "prediction error is defined using a separate set of outcomes @xmath170 , generated from the same conditional distribution as @xmath0 .",
    "we use the scaled mean squared prediction error @xmath171 as our criterion for linear regression and the mean of the deviances of @xmath170 for logistic regression .",
    "( left subfigure ) , and prediction error ( right subfigure ) in the simulation study . ]",
    "figure  [ simbox ] presents boxplots of the results . in general , all seven methods differ only slightly in prediction error .",
    "much greater differences are seen for estimation error . compared with the corresponding voxel - domain methods ,",
    "the estimation mse for wavelet methods is either roughly equal or clearly lower on average , and the variability of the mse is often much lower .",
    "the wavelet methods also markedly outperform @xmath99-spline - based fpcr .",
    "somewhat contrary to expectation , the superior performance of wavelet methods is not clearly more pronounced for @xmath158 than for  @xmath160 .",
    "while the wavelet - domain methods do not clearly attain lower estimation error than voxel - domain methods for logistic regression with @xmath172 , they do appear superior for the @xmath173 setting ( which seems more realistic ) and for linear regression .",
    "moreover , qualitatively , wavelet - domain modeling helps to capture the main features of the coefficient image .",
    "figure  [ show5 ] displays an example of the training - set estimates derived by wavelet - domain lasso versus ordinary lasso .",
    "the wavelet - domain estimates are clearly more similar to each other and to the true coefficient image than are the ordinary lasso estimates.=-1     from the comparative simulation study ( top left ) compared with five training - set coefficient function estimates ( for data simulated under @xmath174 setting ) based on wavelet - domain lasso ( other top panels ) and voxel - domain lasso ( bottom panels ) .",
    "the wavelet - based estimates are reasonably accurate , while each of the voxel - domain estimates has about 2025 scattered voxels with nonzero values .",
    "note the unequal scales . ]",
    "the wavelet - domain en appears to have a slight edge overall compared with pcr and pls . for this reason , and",
    "because wavelet en ( or at least its special case , the lasso ) are now somewhat established in the literature [ @xcite ] , the simulations and real - data analyses in the next two sections consider only wavelet - domain en .",
    "we now turn to what the referred to as limitation ( ii ) of predictive analyses in neuroimaging : the need for methodology to assess the predictive value of image data , in particular , when scalar covariates are present .",
    "consider testing the null hypothesis @xmath175 in the general model ( [ efspec ] ) , ( [ gmuspec ] ) , that is , testing the null parametric model @xmath176 versus the alternative ( [ gmuspec ] ) .",
    "informally , we are asking whether the images have predictive value beyond the information contained in the scalar predictors .",
    "we propose a permutation test procedure in which the cv criterion ( [ rkcv ] ) or ( [ robcv ] ) serves as the test statistic .",
    "if the true - data cv falls in the left tail of the distribution of permuted - data cv values , significance is declared .",
    "permutation techniques of this kind have previously appeared in the neuroimaging and machine learning literature [ @xcite ] .",
    "the way the permutation distribution is constructed depends on the null model under consideration .",
    "when @xmath15 in ( [ gmuspec ] ) ( no scalar covariates ) , one can simply permute the responses : that is , we repeatedly reorder the responses as @xmath177 for some permutation @xmath178 , refit the model , and record the cv value . for the linear model ( [ flm ] ) with scalar covariates ,",
    "a common approach is to permute the residuals from the null parametric model : that is , model ( [ flm ] ) is refitted repeatedly with the @xmath49th response of the form @xmath179 , where the hats refer to fitted values and residuals from the model @xmath180 . for some glms ,",
    "however , such pseudo - responses based on permuted residuals are not of the correct form ( e.g. , for logistic regression , they are not binary ) .",
    "one can instead form pseudo-_predictors _ , by regressing the predictor of interest on the nuisance covariates and permuting the residuals from this fit .",
    "in other words , we replace the design matrix @xmath181 with @xmath182,\\ ] ] where @xmath183 and @xmath184 is a permutation matrix .",
    "although a similar idea was proposed by @xcite for ( ordinary ) logistic regression , we have adopted it as our preferred permutation approach even for the linear case ; see supplementary appendix  b [ @xcite ] for further discussion .",
    "we conducted a simulation study , using the adhd-200 image data analyzed in section  [ appsec ] , to assess the type - i error rate and power of the permutation test procedure . here",
    "we focus on logistic regression ( see supplementary appendix  c [ @xcite ] , for linear regression results ) and the wavelet - domain lasso .",
    "we first considered the case without scalar covariates and generated binary responses @xmath185 , @xmath186 , where @xmath187 where @xmath188 is a constant used to adjust the base rate ( probability of event ) ; @xmath189 is the @xmath49th image ( expressed as a mean - zero vector ) ; @xmath79 is the true coefficient image shown in figure  [ psim](a ) ( similarly vectorized ) , multiplied by an appropriate constant to attain a specified value of @xmath165 ( see supplementary appendix  a [ @xcite ] , regarding the definition of @xmath165 ) . for each of the base rates 0.25 , 0.5 , 0.75 and each of the @xmath165 values 0.04 , 0.07 , 0.1 , 0.15 , 0.2 , 0.25 , 0.3",
    ", we simulated 200 response vectors to assess power to reject @xmath190 at the @xmath191 level , as well as 1000 response vectors with @xmath192 ( @xmath193 ) to assess the type - i error rate .",
    "next we considered testing the same null hypothesis for the model @xmath194 with a scalar covariate @xmath195 such that @xmath165 for the submodel @xmath196 is approximately 0.2 .",
    "we generated the same number of response vectors as above for each of the above @xmath165 values , but here @xmath165 refers to the partial @xmath165 adjusting for @xmath195 ( see supplementary appendix  a.2 [ @xcite ] ) .",
    "the results , displayed in figure  [ psim](b ) and ( c ) , indicate that the nominal error rate is approximately attained for both models . for a given @xmath197 ,",
    "the power is somewhat higher for model ( [ pseq ] ) than for model ( [ pseq2 ] ) , and highest for either model when the base rate is 0.5 .",
    "evidently , for base rates closer to 0 or 1 , the cv deviance under the null hypothesis tends to be lower , and thus a stronger signal is needed to reject the null .",
    "used in the power study : gray denotes 0 , black denotes 1 .  estimated probability of rejecting the null hypothesis @xmath198 as a function of @xmath165 , with 95% confidence intervals , for model ( [ pseq ] ) .",
    "same , for model ( [ pseq2 ] ) . ]",
    "basing a test of the hypothesis @xmath199 on the prediction performance of an estimation algorithm , rather than on an estimate of @xmath64 , is admittedly somewhat unconventional . in neuroimaging",
    "specifically , inference typically proceeds by fitting separate models at each voxel , and then applying some form of multiple testing correction [ @xcite ] .",
    "in the present setting of a single model that uses the entire image to predict a scalar response , it might be possible to assign values to individual voxels as in @xcite . in practice , however , predictive algorithms tend to produce rather unstable estimates , as a number of authors have acknowledged [ e.g. , @xcite ] .",
    "our hypothesis testing approach thus sets the more modest inferential goal of verifying that the coefficient image as a whole yields better - than - chance prediction.=-1      for ordinary , as opposed to functional , regression , confounding is said to occur when ( i ) @xmath200 appears predictive of @xmath118 , but this relationship can be attributed to a third variable @xmath201 such that ( ii ) @xmath201 is predictive of @xmath118 and ( iii ) @xmath201 is correlated with @xmath200 . for example , birth order ( @xmath200 ) is associated with the occurrence of down syndrome ( @xmath118 ) , but this is due to the effect of the confounding variable maternal age ( @xmath201 ) [ @xcite ] .    to extend the above definition to the case of a functional predictor @xmath202 ,",
    "suppose that ( i ) @xmath202 is ostensibly related to @xmath118 , in the sense that @xmath16 is not identically zero when model ( [ gmuspec ] ) includes no scalar covariates , but ( ii ) the scalar variable @xmath201 is also predictive of @xmath118 .",
    "a functional - predictor analogue of point ( iii ) is to suppose that @xmath201 is correlated with @xmath203 , where @xmath204 is an estimate obtained with @xmath201 excluded from model ( [ gmuspec ] ) . aside from this `` global '' analogue of ( iii ) , it may be useful to consider a `` local '' analogue which holds if @xmath201 is correlated with @xmath205 , specifically for @xmath206 such that @xmath207 ; but this is somewhat less straightforward to assess .",
    "we now apply the wavelet - domain elastic net to `` predicting '' adhd diagnosis using maps of fractional amplitude of low - frequency fluctuations ( falff ) [ @xcite ] from a portion of the adhd-200 sample referred to in the ( ) .",
    "falff is defined as the ratio of bold signal power spectrum within the 0.010.08  hz range to total over the entire range .",
    "@xcite reported altered levels of falff in a sample of children with adhd relative to controls , specifically in frontal regions .",
    "that study relied on the traditional analytic approach in neuroimaging , which regresses the imaged quantity ( in this case falff ) on diagnostic group , separately at each voxel .",
    "here we employed scalar - on - image logistic regression , which reverses the roles of response and predictor , to regress diagnostic group on falff images .",
    "our sample consisted of 333 individuals : 257 typically developing controls and 76 with combined - type adhd .",
    "the sample included 198 males and 135 females , with age range 720 ( see supplementary appendix  d [ @xcite ] , for further details ) .",
    "we chose the 2d slice for which the mean across voxels of the sd of falff was highest .",
    "this was the axial slice located at @xmath208 ( just dorsal to the corpus callosum ) in the coordinate space of the montreal neurological institute s mni152 template ( 4  mm resolution ) .",
    "we fitted two models .",
    "the first was @xmath209 where @xmath210 denotes the @xmath49th subject s falff image .",
    "the second model was @xmath211 where the vector @xmath6 includes the @xmath49th subject s age , sex , iq and mean fd , as well as a leading 1 for the intercept .",
    "figure  [ wnetfhat ] shows the coefficient images attained for model ( [ iom ] ) with each value of the mixing parameter @xmath135 .",
    "as expected , increasing values of @xmath135 lead to more - sparse estimates in the wavelet domain , and hence in the voxel domain .",
    "figure  [ wnetcv ] shows the cv deviance as a function of @xmath136 for @xmath212 , which had the lowest cv deviance overall , as well as for @xmath131 .    )",
    "applied to the adhd-200 data , using wavelet - domain elastic net with four different values of the mixing parameter @xmath135 . ]",
    "[ wnetfhat ]    /@xmath213 one approximate standard error , for the wavelet - domain elastic net models with @xmath214 . ]    [ wnetcv ]    the left subfigure of figure  [ ptr ] shows that the cv deviance lies in the left tail of the permutation distribution for model ( [ iom ] ) , indicating a significant effect of the falff image predictors ( @xmath215 ) .",
    "however , with the scalar covariate adjustment of model ( [ iom2 ] ) , this effect disappears .",
    "the next subsection examines more closely how the scalar covariates may be acting as confounders .    ) , but not in model ( [ iom2 ] ) , which adjusts for scalar covariates .",
    "when only younger individuals are included ( right ) , neither model shows a significant falff effect . ]    our test of model ( [ iom ] ) entailed 999 permuted - data fits with four candidate values of @xmath135 and 100 of @xmath136 , requiring 14.25 hours on an intel xeon e5 - 2670 processor running at 2.6 ghz . in practice , we recommend parallelizing the permutations via cluster computing to make the computation time more manageable . in addition , truncated sequential probability ratio tests [ @xcite ] could in some cases reduce computation time via early stopping .",
    "we also explored fitting model ( [ iom ] ) with the full 3d falff images as predictors ; see supplementary appendix  e [ @xcite ] .      as discussed in section  [ seccfd ] , the notion of confounding entails three elements ( see figure  [ cfd - diag ] ) . point ( i ) , an apparent effect of the image predictor falff on diagnosis , was established by the above permutation test result for model ( [ iom ] ) . to check point ( ii ) of the definition for each of the four scalar covariates under consideration",
    ", we performed an ordinary logistic regression with diagnosis ( 1@xmath216adhd , 0@xmath216control ) as response and the above four scalar predictors . in table",
    "[ plr ] ( at left ) , sex , age and iq are all seen to be significantly related to diagnosis .",
    "see also figure  [ sep ] , which compares the fitted probabilities from this ordinary logistic regression with those resulting from models ( [ iom ] ) and  ( [ iom2 ] ) .",
    "the scalar - covariates model is seen to separate the two groups ( black vs. gray dots ) quite well ; the image predictors increase the spread of the predicted probabilities without clearly improving the two groups separation . based on these results ,",
    "each of these three variables may be acting as a confounder .",
    ", outcome @xmath217 and confounder @xmath218 ( see section  [ seccfd ] ) , illustrated with respect to the adhd-200 data . ]",
    "@@ld2.15d1.5d2.15c@ & & + & & + & & & & + intercept & 3.90  ( 1.11 , 6.78 ) & 0.007 & & + sex  ( m  f ) & 1.26  ( 0.65 , 1.91 ) & 0.00008 & 0.14  ( 0.03,0.24)&0.011 + age & -0.20  ( -0.32 , -0.09 ) & 0.0005&-0.35  ( -0.44,-0.25 ) & @xmath219 + iq & -0.03  ( -0.05 , -0.01 ) & 0.003 & -0.09  ( -0.19,0.02)&0.10 + mean fd & -2.51  ( -8.80 , 3.56 ) & 0.42&-0.04  ( -0.15,0.07)&0.47 +    ) ; an ordinary logistic regression with the four scalar covariates ; and model ( [ iom2 ] ) , which includes both . also shown are the @xmath165 values , as defined in supplementary appendix  , for the three models . ]",
    "next we consider point ( iii ) , that is , the correlations of each scalar covariate with @xmath220 , where @xmath221 is the coefficient image estimate from the falff - only model ( [ iom ] ) or , equivalently , with the predicted logit probability of adhd from that model . the results , shown at right in table  [ plr ] , point to age and sex as the principal confounders .",
    "( here sex was treated as a binary variable , with 1 for male and 0 for female ; a @xmath201-test and a mann  whitney test yielded similar results . )",
    "`` local '' examination in the sense of section  [ seccfd ] reveals that the falff @xmath222 tends to be higher in males and in younger individuals for many voxels @xmath223 ; and such regions overlap considerably with those in which @xmath224 . in other words , the ostensible association between falff and adhd likely reflects the dependence of falff on age and sex , which in turn are related to adhd in our sample .",
    "further inspection revealed that , of the 67 individuals with age above 14.0 , only 8 had adhd , with maximum age 17.43whereas the controls had ages as high as 20.45 .",
    "this led us to suspect that these older individuals might be driving the confounding with age that results in a spurious effect of falff on diagnosis . to investigate this possibility",
    ", we repeated the analysis using only the 266 individuals of age 14.0 or lower . figure  [ ptr ] shows that in this subsample , the falff effect is no longer significant , even without adjusting for the scalar covariates .",
    "moreover , given how far the test statistic is from the left tail of the permutation distribution , it seems unlikely that the loss of significance is due merely to the lower sample size .    in general ,",
    "absent careful matching at the design stage , it would be advisable to match the two diagnostic groups optimally on a complete set of clearly relevant variables , via algorithms such as those described in @xcite .",
    "our aim here , however , was to show how a straightforward new notion of confounding for functional predictors can be used to identify a principal scalar confounder , whose impact can be removed by the crude device of simply truncating the age range .",
    "our analysis in section  [ appsec ] included only one imaging modality and only a subset of the individuals from the adhd-200 global competition database . at any rate ,",
    "our essentially negative result is consistent with the finding [ @xcite ] that diagnostic accuracy was optimized by basing prediction on scalar predictors , while ignoring the image data . in a blog comment on that outcome , cited both by @xcite and by @xcite , the neuroscientist russ poldrack suggested that `` any successful imaging - based decoding could have been relying upon correlates of those variables rather than truly decoding a correlate of the disease . ''",
    "stated a bit differently , the competing teams successes in using the image data to predict diagnosis may have been brought about by confounding .",
    "but there appear to have been few attempts , if any , to study systematically how confounding may give rise to spurious relationships between quantitative image data and clinical variables .",
    "similarly , analyses of the adhd-200 data , and related work on brain `` decoding , '' have devoted little attention to formally testing the contribution of imaging data to prediction of scalar responses [ but see @xcite ] .    as we have shown , these two interrelated issues ",
    "testing the effect of image predictors and investigating possible confounders  can be handled straightforwardly within our scalar - on - image regression framework .",
    "the permutation test procedure of section  [ statsig ] found a statistically significant relationship between falff images and adhd diagnosis , but this disappeared when four scalar covariates were adjusted for .",
    "further examination , in light of our extension of the notion of confounding to functional / image predictors in section  [ seccfd ] , pointed to age and sex as the key confounders .",
    "the adhd-200 project is one of a number of recent initiatives to make large samples of neuroimaging data publicly available [ @xcite ] .",
    "these initiatives have been a boon for statistical methodology development , but it must be borne in mind that even as neuroimaging sample sizes increase rapidly , they remain much smaller than the data dimension .",
    "no approach to scalar - on - image regression can completely escape the ensuing nonidentifiability of the coefficient image .",
    "we can , however , ( i ) put forth assumptions , likely to hold approximately in practice , that reduce the effective dimension of the coefficient image ; and ( ii ) employ multiple methods in the hope that these will converge upon similar coefficient image estimates , at least when the signal is sufficiently strong .    with these considerations in mind ,",
    "we have introduced three methods for scalar - on - image regression , each relying on a different set of assumptions to achieve reduction in the wavelet domain .",
    "implementations of these three methods , for 2d and 3d image data , are provided in the ` refund.wave ` package [ @xcite ] for r [ @xcite ] , available at .",
    "this new package , a spinoff of the ` refund ` package [ @xcite ] , relies on the ` wavethresh ` package [ @xcite ] for wavelet decomposition and reconstruction .",
    "as discussed in section  [ metalg ] , the three methods described here are merely three instances of a meta - algorithm for scalar - on - image regression .",
    "the`refund.wave ` package allows for straightforward incorporation of alternative penalties , and other extensions may allow for more refined wavelet - domain algorithms , which may improve the stability and reproducibility of the coefficient image estimates [ @xcite ] .",
    "for instance , in wavelet - based nonparametric regression , thresholding is often performed in a level - specific manner .",
    "analogously , it might be appropriate to modify criterion ( [ encrit ] ) so as to differentially penalize coefficients at different levels .",
    "one might also employ resampling techniques [ cf .",
    "@xcite ] to select those wavelet basis elements that are consistently predictive of the outcome .",
    "finally , wavelets whose domain is anatomically customized , such as the wavelets defined on the cortex by @xcite , offer a promising new way to confine the analysis to relevant portions of the brain.=-1",
    "the authors are grateful to the editor , karen kafadar , and to the associate editor and referees , whose feedback led to major improvements in the paper ; to adam ciarleglio , for contributions to software implementation ; to xavier castellanos , samuele cortese , cameron craddock , brett lullo , eva petkova , fabian scheipl and victor solo for helpful discussions about our methodology and its application ; to jeff goldsmith , lei huang and ciprian crainiceanu for sharing their insights as well as a preprint of @xcite ; and to the adhd-200 consortium ( ) and the neuro bureau ( ) for making the fmri data set publicly available .",
    "in addition to the funding sources listed on the first page , the first author thanks the national science foundation for its support of the statistical and applied mathematical sciences institute , whose summer 2013 program on neuroimaging data analysis provided a valuable opportunity to present part of this research .",
    "this work utilized computing resources at the high performance computing facility of the center for health informatics and bioinformatics at new york university langone medical center ."
  ],
  "abstract_text": [
    "<S> an increasingly important goal of psychiatry is the use of brain imaging data to develop predictive models . </S>",
    "<S> here we present two contributions to statistical methodology for this purpose . </S>",
    "<S> first , we propose and compare a set of wavelet - domain procedures for fitting generalized linear models with scalar responses and image predictors : sparse variants of principal component regression and of partial least squares , and the elastic net . </S>",
    "<S> second , we consider assessing the contribution of image predictors over and above available scalar predictors , in particular , via permutation tests and an extension of the idea of confounding to the case of functional or image predictors . using the proposed methods , we assess whether maps of a spontaneous brain activity measure , derived from functional magnetic resonance imaging , can meaningfully predict presence or absence of attention deficit / hyperactivity disorder ( adhd ) . </S>",
    "<S> our results shed light on the role of confounding in the surprising outcome of the recent adhd-200 global competition , which challenged researchers to develop algorithms for automated image - based diagnosis of the disorder .    </S>",
    "<S> ./style / arxiv - general.cfg    ,    ,    , </S>"
  ]
}