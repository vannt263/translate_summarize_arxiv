{
  "article_text": [
    "-5 mm    how can we understand intelligent behavior ?",
    "how can we design intelligent computers ?",
    "these are questions that have been discussed by scientists and the public at large for over 50 years . as mathematicians ,",
    "however , the question we want to ask is `` is there a _ mathematical _ theory underlying intelligence ? ''",
    "i believe the first mathematical attack on these issues was control theory , led by wiener and pontryagin .",
    "they were studying how to design a controller which drives a motor affecting the world and also sits in a feedback loop receiving measurements from the world about the effect of the motor action .",
    "the goal was to control the motor so that the world , as measured , did something specific , i.e.  move the tiller so that the boat stays on course .",
    "the main complication is that nothing is precisely predictable : the motor control is not exact , the world does unexpected things because of its complexities and the measurements you take of it are imprecise .",
    "all this led , in the simplest case , to a beautiful analysis known as the wiener - kalman - bucy filter ( to be described below ) .",
    "but control theory is basically a theory of the output side of intelligence with the measurements modeled in the simplest possible way : e.g.  linear functions of the state of the world system being controlled plus additive noise .",
    "the real input side of intelligence is perception in a much broader sense , the analysis of all the noisy incomplete signals which you can pick up from the world through natural or artificial senses .",
    "such signals typically display a mix of distinctive patterns which tend to repeat with many kinds of variations and which are confused by noisy distortions and extraneous clutter .",
    "the interesting and important structure of the world is thus coded in these signals , using a code which is complex but not perversely so .",
    "-5 mm    the first serious attack on problems of perception was the attempt to recognize speech which was launched by the us defense agency arpa in 1970 .",
    "at this point , there were two competing ideas of what was the right formalism for combining the various clues and features which the raw speech yielded .",
    "the first was to use logic or , more precisely , a set of ` production rules ' to augment a growing database of true propositions about the situation at hand .",
    "this was often organized in a ` blackboard ' , a two - dimensional buffer with the time of the asserted proposition plotted along the @xmath0-axis and the level of abstraction ( i.e.  signal  phone  phoneme  syllable  word  sentence ) along the @xmath1-axis .",
    "the second was to use statistics , that is , to compute probabilities and conditional probabilities of various possible events ( like the identity of the phoneme being pronounced at some instant ) .",
    "these statistics were computed by what was called the ` forward - backward ' algorithm , making 2 passes in time , before the final verdict about the most probable translation of the speech into words was found .",
    "this issue of logic vs.  statistics in the modeling of thought has a long history going back to aristotle about which i have written in [ m ] .",
    "i think it is fair to say that statistics won .",
    "people in speech were convinced in the 1970 s , artificial intelligence researchers converted during the 1980 s as expert systems needed statistics so clearly ( see pearl s influential book [ p ] ) , but vision researchers were not converted until the 1990 s when computers became powerful enough to handle the much larger datasets and algorithms needed for dealing with 2d images .",
    "the biggest reason why it is hard to accept that statistics underlies all our mental processes  perception , thinking and acting  is that we are not consciously aware of 99% of the ambiguities with which we deal every second .",
    "what philosophers call the ` raw qualia ' , the actual sensations received , do not make it to consciousness ; what we are conscious of is a precise unambiguous enhancement of the sensory signal in which our expectations and our memories have been drawn upon to label and complete each element of the percept .",
    "a very good example of this comes from the psychophysical experiments of warren & warren [ w ] in 1970 : they modified recorded speech by replacing a single phoneme in a sentence by a noise and played this to subjects . remarkably ,",
    "the subjects did _ not _ perceive that a phoneme was missing but believed they had heard the one phoneme which made the sentence semantically consistent :    l|l actual sound & perceived words + the ? ",
    "eel is on the shoe & the _ h_eel is on the shoe + the ? ",
    "eel is on the car & the _ wh_eel is on the car + the ? ",
    "eel is on the table & the _ m_eal is on the table + the ? ",
    "eel is on the orange & the _ p_eel is on the orange    two things should be noted .",
    "firstly , this showed clearly that the actual auditory signal did not reach consciousness .",
    "secondly , the choice of percept was a matter of probability , not certainty .",
    "that is , one might find some odd shoe with a wheel on it , a car with a meal on it , a table with a peel on it , etc .",
    "but the words which popped into consciousness were the most likely .",
    "an example from vision of a simple image , whose contents require major statistical reasoning to reconstruct , is shown in figure  [ fig : oldman ] .",
    "it is important to clarify the role of probability in this approach .",
    "the uncertainty in a given situation need not be caused by observations of the world being truly unpredictable as in quantum mechanics or even effectively so as in chaotic phenomena .",
    "it is rather a matter of efficiency : in order to understand a sentence being spoken , we do not need to know all the things which affect the sound such as the exact acoustics of the room in which we are listening , nor are we even able to know other factors like the state of mind of the person we are listening to . in other words , we always have incomplete data about a situation .",
    "a vast number of physical and mental processes are going on around us , some germane to the meaning of the signal , some extraneous and just cluttering up the environment . in this ` blooming , buzzing ' world , as william james called it , we need to extract information and the best way to do it , apparently , is to make a stochastic model in which all the irrelevent events are given a simplified probability distribution .",
    "this is not unlike the stochastic approach to navier - stokes , where one seeks to replace turbulence or random molecular effects on small scales by stochastic perturbations .",
    "-5 mm    having accepted that we need to use probabilities to combine bits and pieces of evidence , what is the mathematical set up for this ?",
    "we need the following ingredients : a ) a set of random variables , some of which describe the observed signal and some the ` hidden ' variables describing the events and objects in the world which are causing this signal , b ) a class of stochastic models which allow one to express the variability of the world and the noise present in the signals and c ) specific parameters for the one stochastic model in this class which best describes the class of signals we are trying to decode now .",
    "more formally , we shall assume we have a set @xmath2 of observed and hidden random variables , which may have real values or discrete values in some finite or countable sets , we have a set @xmath3 of parameters and we have a class of probability models @xmath4 on the @xmath0 s for each set of values of the @xmath5 s .",
    "the crafting or learning of this model may be called the first problem in the mathematical theory of perception .",
    "it is usual to factor these probability distributions : @xmath6 where the first factor , describing the likelihood of the observations from the hidden variables , is called the _ imaging model _ and the second , giving probabilities on the hidden variables , is called the _",
    "prior_. in the full bayesian setting , one has an even stronger prior , a full probability model @xmath7 , including the parameters .",
    "the second problem of perception is that we need to estimate the values of the parameters @xmath5 which give the best stochastic model of this aspect of the world .",
    "this often means that you have some set of measurements @xmath8 and seek the value of @xmath5 which maximizes their likelihood @xmath9 .",
    "if the hidden variables as well as the observations are known , this is called supervised learning ; if the hidden variables are not known , then it is unsupervised and one may maximize , for instance , @xmath10 .",
    "if one has a prior on the @xmath5 s too , one can also estimate them from the mean or mode of the full posterior @xmath11 .",
    "usually a more challenging problem is how many parameters @xmath5 to include . at one extreme",
    ", there are simple ` off - the - shelf ' models with very few parameters and , at the other extreme , there are fully non - parametric models with infinitely many parameters . here",
    "the central issue is how much data one has : for any set of data , models with too few parameters distort the information the data contains and models with too many overfit the accidents of this data set .",
    "this is called the _ bias - variance dilemma_. there are two main approaches to this issue .",
    "one is cross - validation : hold back parts of the data , train the model to have maximal likelihood on the training set and test it by checking the likelihood of the held out data .",
    "there is also a beautiful theoretical analysis of the problem due principally to vapnik [ v ] and involving the _ vc dimension _ of the models  the size of the largest set of data which can be split in all possible ways into more and less likely parts by different choices of @xmath5 .",
    "as grenander has emphasized , a very useful test for a class of models is to synthesize from it , i.e.  choose random samples according to this probability measure and to see how well they resemble the signals we are accustomed to observing in the world .",
    "this is a stringent test as signals in the world usually express layers and layers of structure and the model tries to describe only a few of these .    the third problem of perception is using this machinary to actually perceive : we assume we have measured specific values @xmath12 and want to infer the values of the hidden variables @xmath13 in this situation . given these observations , by bayes rule ,",
    "the hidden variables are distributed by the so - called _ posterior _ distribution : @xmath14 one may then want to estimate the mode of the posterior , the most likely value of @xmath13 . or one may want to estimate the mean of some functions @xmath15 of the hidden variables . or , if the posterior is often multi - modal and some evidence is expected to available later , one usually wants a more complete description or approximation to the full posterior distribution .",
    "-5 mm    a convenient way to introduce the ideas of pattern theory is to outline the simple hidden markov model method in speech recognition to illustrate many of the ideas and problems which occur almost everywhere . here",
    "the observed random variables are the values of the sound signal @xmath16 , a pressure wave in air .",
    "the hidden random variables are the states of the speaker s mouth and throat and the identity of the phonemes being spoken at each instant .",
    "usually this is simplified , replacing the signal by samples @xmath17 and taking for hidden variables a sequence @xmath18 whose values indicate which phone in which phoneme is being pronounced at time @xmath19 .",
    "the stochastic model used is : @xmath20 i.e.  the @xmath21 form a markov chain and each @xmath22 depends only on @xmath18 .",
    "this is expressed by the graph : @xmath23 in which each variable corresponds to a vertex and the graphical markov property holds : if 2 vertices @xmath24 in the graph are separated by a subset @xmath25 of vertices , then the variables associated to @xmath26 and @xmath27 are conditionally independent if we fix the variables associated to @xmath25 .    this simple model works moderately well to decode speech because of the linear nature of the graph , which allows the ideas of dynamic programming to be used to solve for the marginal distributions and the modes of the hidden variables , given any observations @xmath28 .",
    "this is expressed simply in the recursive formulas : @xmath29 note that if each @xmath18 can take @xmath30 values , the complexity of each time step is @xmath31 .    in any model ,",
    "if you can calculate the conditional probabilities of the hidden variables and if the model is of exponential type , i.e.@xmath32 then there is also an efficient method of optimizing the parameters @xmath5 .",
    "this is called the _ em algorithm _ and , because it holds for hmm s , it is one of the key reasons for the early successes of the stochastic approach to speech recognition .",
    "for instance , a markov chain @xmath21 is an exponential model if we let the @xmath5 s be @xmath33 and write the chain probabilities as : @xmath34 the fundamental result on exponential models is that the @xmath5 s are determined by the expectations @xmath35 and that any set of expectations @xmath36 that can be achieved in some probability model ( with all probabilities non - zero ) , is also achieved in an exponential model .",
    "-5 mm    in this model , the observations @xmath22 are naturally continuous random variables , like all primary measurements of the physical world . but the hidden variables are discrete : the set of phonemes , although somewhat variable from language to language , is always a small discrete set .",
    "this combination of discrete and continuous is characteristic of perception .",
    "it is certainly a psychophysical reality : for example experiments show that our perceptions lock onto one or another phoneme , resisting ambiguity ( see [ l ] , ch.8 , esp .",
    "but it shows itself more objectively in the low - level statistics of natural signals .",
    "take almost any class of continuous real - valued signals @xmath16 generated by the world and compile a histogram of their changes @xmath37 over some fixed time interval @xmath38 .",
    "this empirical distribution will very likely have kurtosis ( @xmath39 ) greater than 3 , the kurtosis of any gaussian distribution !",
    "this means that , compared to a gaussian distribution with the same mean and standard deviation , @xmath0 has higher probability of being quite small or quite large but a lower probability of being average .",
    "thus , compared to brownian motion , @xmath16 tends to move relatively little most of the time but to make quite large moves sometimes .",
    "this can be made precise by the theory of stochastic processes with iid increments , a natural first approximation to any stationary markov process .",
    "the theory of such processes says that ( a ) their increments always have kurtosis at least 3 , ( b ) if it equals 3 the process is brownian and ( c ) if it is greater , samples from the process almost surely have discontinuities . at the risk of over - simplfying",
    ", we can say _",
    "kurtosis @xmath40 3 is nature s universal signal of the presence of discrete events / objects in continuous space - time_.    a classic example of this are stock market prices .",
    "their changes ( or better , changes in log(price ) ) have a highly non - gaussian distribution with polynomial tails . in speech , the changes in the log(power ) of the windowed fourier transform show the same phenomenon , confirming that @xmath16 can not be decently modeled by colored gaussian noise .",
    "-5 mm    applying hmm s in realistic settings , it usually happens that @xmath30 is too large for an exhaustive search of complexity @xmath31 or that the @xmath18 are real valued and , when adequately sampled , again @xmath30 is too large .",
    "there is one other situation in which the hmm - style approach works easily  the _ kalman filter_. in kalman s setting , each variable @xmath18 and @xmath22 is real vector - valued instead of being discrete and @xmath41 and @xmath42 are _ gaussian distributions with fixed covariances and means depending linearly on the conditioning variable_. it is then easy to derive recursive update formulas , similar to those above , for the conditional distributions on each @xmath18 , given the past data @xmath43 .    but usually , in the real - valued variable setting , the @xmath44 s are more complex than gaussian distributions .",
    "an example is the tracking problem in vision : the position and velocity @xmath18 of some specific moving object at time @xmath45 is to be inferred from a movie @xmath46 , in which the object s location is confused by clutter and noise .",
    "it is clear that the search for the optimal reconstruction @xmath18 must be pruned or approximated .",
    "a dramatic breakthrough in this and other complex situations has been to adapt the hmm / kalman ideas by using weak approximations to the marginals @xmath47 by a finite set of samples , an idea called _ particle filtering _ :",
    "@xmath48 this idea was proposed originally by gordon , salmond and smith [ g - s - s ] and is developed at length in the recent survey [ d - f - g ] .",
    "an example with explicit estimates of the posterior from the work of isard and blake [ i - b ] is shown in figure 2 .",
    "they follow the version known as bootstrap particle filtering in which , for each @xmath49 , @xmath30 samples @xmath50 are drawn with replacement from the weak approximation above , each sample is propagated randomly to a new sample @xmath51 at time @xmath52 using the prior @xmath53 and these are reweighted proportional to @xmath54 .",
    "-5 mm    a more serious problem with the hmm approach is that the markov assumption is never really valid and it may be much too crude an approximation . consider speech recognition .",
    "the finite lexicon of words clearly constrains the expected phoneme sequences , i.e.  if @xmath18 are the phonemes , then @xmath55 depends on the current word(s ) containing these phonemes , i.e.  on a short but variable part of the preceding string @xmath56 of phonemes . to fix this",
    ", we could let @xmath18 be a pair consisting of a word and a specific phoneme in this word ; then @xmath55 would have two quite different values depending on whether @xmath57 was the last phoneme in the word or not . within a word",
    ", the chain needs only to take into account the variability with which the word can be pronounced . at word boundaries",
    ", it should use the conditional probabilities of word pairs .",
    "this builds much more of the patterns of the language into the model .",
    "why stop here ?",
    "state - of - the - art speech recognizers go further and let @xmath18 be a pair of consecutive words plus a triphone in the second word ( or bridging the first and second word ) whose middle phoneme is being pronounced at time @xmath45 .",
    "then the transition probabilities in the hmm involve the statistics of ` trigrams ' , consecutive word triples in the language .",
    "but grammar tells us that words sequences are also structured into phrases and clauses of variable length forming a parse tree .",
    "these clearly affect the statistics .",
    "semantics tells us that words sequences are further constrained by semantic plausibility ( ` sky ' is more probable as the word following ` blue ' than ` cry ' ) and pragmatics tells us that sentences are part of human communications which further constrain probable word sequences .",
    "all these effects make it clear that certain parts of the signal should be grouped together into units on a higher level and given labels which determine how likely they are to follow each other or combine in any way .",
    "this is the essence of grammar : higher order random variables are needed whose values are subsets of the low order random variables .",
    "the simplest class of stochastic models which incorporate variable length random substrings of the phoneme sequence are _ probabilistic context free grammars _ or pcfg s .",
    "mathematically , they are a particular type of random branching tree .    * definition * _ a is a stochastic model in which the random variables are ( a ) a sequence of rooted trees @xmath58 , ( b ) a linearly ordered sequence of observations @xmath22 and a 1:1 correspondence between the observations @xmath22 and the leaves of the whole forest of trees such that the children of any vertex of any tree form an interval @xmath59 in time and ( c ) a set of labels @xmath60 for each vertex .",
    "the probability model is given by conditional probabilities @xmath61 for the labels of each child of each vertex is the name of the ` production rule ' with this vertex as its head , esp .",
    "it fixes the arity of the vertex .",
    "we are doing it this way to simplify the markov property . ] and @xmath62 for the observations , conditional on the label of the corresponding leaf .",
    "_    see figure 3 for an example .",
    "this has a markov property if we define the ` extended ' state @xmath63 at leaf @xmath49 to be not only the label @xmath18 at this leaf but the whole sequence of labels on the path from this leaf to the root of the tree in which this leaf lies .",
    "conditional on this state , the past and the future are independent .",
    "this is a mathematically elegant and satisfying theory : unfortunately , it also fails , or rather explodes because , in carrying it out , the set of labels gets bigger and bigger .",
    "for instance , it is not enough to have a label for noun phrase which expands into an adjective plus a noun .",
    "the adjective and noun must agree in number and ( in many languages ) gender , a constraint that must be carried from the adjective to the noun ( which need not be adjacent ) via the label of the parent .",
    "so we need 4 labels , all combinations of singular / plural masculine / feminine noun phrases .",
    "and semantic constraints , such as pr(`blue sky ' ) @xmath40 pr(`blue cry ' ) , would seem to require even more labels like ` colorable noun phrases ' .",
    "rather than letting the label set explode , it is better to consider a bigger class of grammars , which express these relations more succinctly but which are not so easily converted into hmm s : _ unification grammars _ [ sh ] or _ compositional grammars _ [ b - g - p ] .",
    "the need for grammars of this type is especially clear when we look at formalisms for expressing the grouping laws in vision : see figure 3 .",
    "the further development of stochastic compositional grammars , both in language and vision , is one of the main challenges today .",
    "-5 mm    the theory of hmm s deals with one - dimensional signals .",
    "but images , the signals occurring in vision , are usually two - dimensional  or three - dimensional for mr scans and movies ( 3 space dimensions and 2 space plus 1 time dimension ) , even four - dimensional for echo cardiograms . on the other hand ,",
    "the parse tree is a more abstract graphical structure and other ` signals ' , like medical data gathered about a particular patient , are structured in complex ways ( e.g.  a set of blood tests , a medical history ) .",
    "this leads to the basic insight of grenander s pattern theory [ g ] : that the variables describing the structures in the world are typically related in a graphical fashion , edges connecting variables which have direct bearing on each other .",
    "finding the right graph or class of graphs is a crucial step in setting up a satisfactory model for any type of patterns .",
    "thus the applications , as well as the mathematical desire to find the most general setting for this theory , lead to the idea of replacing a simple chain of variables by a set of variables with a more general graphical structure .",
    "the general concept we need is that of a markov random field :    * definition * _ a is a graph @xmath64 , a set of random variables @xmath65 , one for each vertex , and a joint probability distribution on these variables of the form : @xmath66 where @xmath67 ranges over the cliques ( fully connected subsets ) of the graph , @xmath68 are any functions and @xmath69 a constant . if the variables @xmath60 are real - valued for @xmath70 , we make this into a probability density , multiplying by @xmath71 .",
    "moreover , we can put each model in a family by introducing a temperature @xmath72 and defining : _",
    "@xmath73    these are also called _",
    "gibbs models _ in statistical mechanics ( where the @xmath68 are called _ energies _ ) and _ graphical models _ in learning theory and , like markov chains , are characterized by their conditional independence properties .",
    "this characterization , called the hammersley - clifford theorem , is that if two vertices @xmath74 are separated by a subset @xmath75 ( all paths in @xmath76 from @xmath26 to @xmath27 must include some vertex in @xmath25 ) , then @xmath77 and @xmath78 are conditionally independent given @xmath79 .",
    "the equivalence of these independence properties , plus the requirement that all probabilities be positive , with the simple explicit formula for the joint probabilities makes it very convincing that mrf s are a natural class of stochastic models .",
    "-5 mm    this class of models is very expressive and many types of patterns which occur in the signals of nature can be captured by this sort of stochastic model . a basic example is the ising model and its application to the image segmentation problem . in the simplest form ,",
    "we take the graph @xmath76 to be a square @xmath80 grid with two layers , with observable random variables @xmath81 associated to the top layer and hidden random variables @xmath82 associated to the bottom layer .",
    "we connect by edges each @xmath83 vertex to the @xmath84 vertex above it and to its 4 neighbors @xmath85 in the @xmath0-grid ( except when the neighbor is off the grid ) and no others .",
    "the cliques are just the pairs of vertices connected by edges . finally , we take for energies : @xmath86 the modes of the posteriors @xmath87 are quite subtle : @xmath0 s at adjacent vertices try to be equal but they also seek to have the same sign as the correponding @xmath88 .",
    "if @xmath88 has rapid positive and negative swings , these are in conflict .",
    "hence the more probable values of @xmath0 will align with the larger areas where @xmath88 is consistently of one sign .",
    "this can be used to model a basic problem in vision : the _ segmentation problem_. the vision problem is to decompose the domain of an image @xmath1 into parts where distinct objects are seen .",
    "for example , the oldman image might be decomposed into 6 parts : his body , his head , his cap , the bench , the wall behind him and the sky .",
    "the decomposition is to be based on the idea that the image will tend to either slowly varying or to be statistically stationary at points on one object , but to change abruptly at the edges of objects . as proposed in [ g - g ] , the ising model can be used to treat the case where the image has 2 parts , one lighter and one darker , so that at the mode of the posterior the hidden variables @xmath0 will be @xmath89 on one part , @xmath90 on the other .",
    "an example is shown in figure 4 .",
    "this approach makes a beautiful link between statistical mechanics and perception , in which the process of finding global patterns in a signal is like forming large scale structures in a physical material as the temperature cools through a phase transition .",
    "+    more complex models of this sort have been used extensively in image analysis , for texture segmentation , for finding disparity in stereo vision , for finding optic flow in moving images and for finding other kinds of groupings .",
    "we want to give one example of the expressivity of these models which is quite instructive .",
    "we saw above that exponential models can be crafted to reproduce some set of observed expectations but we also saw that scalar statistics from natural signals typically have high kurtosis , i.e.  significant outliers , so that their whole distribution and not just their mean needs to be captured in the model .",
    "putting these 2 facts together suggests that we seek exponential models which duplicate the whole distribution of some important statistics @xmath91 .",
    "this can be done using as parameters not just unknown constants but unknown functions : @xmath92 if @xmath93 depends only the variables @xmath94 , for some clique @xmath95 , this is a mrf , whose energies have unknown functions in them .",
    "an example of this fitting is shown in figure 5 .",
    "-5 mm    however , a problem with mrf models is that the dynamic programming style algorithm used in speech and one - dimensional models to find the posterior mode has no analog in 2d .",
    "one strategy for dealing with this , which goes back to metropolis , is to imitate physics and introduce an artifical dynamics into the state space whose equilibrium is the gibbs distribution .",
    "this dynamics is called a _",
    "monte carlo markov chain _",
    "( mcmc ) and is how the panels in figure 4 were generated .",
    "letting the temperature converge to zero , we get _ simulated annealing _ ( see [ g - g ] ) and , if we do it slowly enough , will find the mode of the mrf model .",
    "although slow , this can be speeded up by biasing the dynamics ( called _ importance sampling _",
    " see [ t - z ] for a state - of - the - art implementation with many improvements ) and is an important tool .",
    "recently , however , another idea due to weiss and collaborators ( see [ y - f - w ] ) and linked to statistical mechanics has been found to give new and remarkably effective algorithms for finding these modes . from an algorithmic standpoint ,",
    "the idea is to use the natural generalization of dynamic programming , called _ bayesian belief propagation _ ( bbp ) , which computes the marginals and modes correctly whenever the graph is a tree and just use it anyway on an arbitrary graph @xmath76 !",
    "mathematically , it amounts to working on the universal covering graph @xmath96 , which is a tree , hence much simpler , instead of @xmath76 . in statistical mechanics",
    ", this idea is called the _ bethe approximation _ , introduced by him in the 30 s .",
    "to explain the idea , start with the _ mean field approximation_. the mean field idea is to find the best approximation of the mrf @xmath44 by a probability distribution in which the variables @xmath60 are all independent .",
    "this is formulated as the distribution @xmath97 which minimizes the kullback - liebler divergence @xmath98 . unlike computing the true marginals of @xmath44 on each @xmath60 which is very hard ,",
    "this approximation can be found by solving iteratively a coupled set of non - linear equations for the @xmath99 .",
    "but the assumption of independence is much too restrictive .",
    "the idea of bethe is instead to approximate @xmath44 by a @xmath100-invariant distribution on @xmath96 .",
    "such distributions are easy to describe : note that a markov random field on a _ tree _ is uniquely determined by its marginals @xmath101 for each edge @xmath102 and , conversely , if we are given a compatible set of distributions @xmath103 for each edge ( in the sense that , for all edges @xmath104 abutting a vertex @xmath105 , the marginals of @xmath106 give distributions on @xmath105 independent of @xmath49 ) , they define an mrf on @xmath76 .",
    "so if we start with a markov random field on any @xmath76 , we get a @xmath100-invariant markov random field on @xmath96 by making duplicate copies for each random variable @xmath107 for each @xmath108 over @xmath105 and lifting the edge marginals .",
    "but more generally , if we have any compatible set of probability distributions @xmath109 on @xmath76 , we also get a @xmath100-invariant mrf on @xmath96 .",
    "then the bethe approximation is that family @xmath110 which minimizes @xmath111 . as in the mean field case",
    ", there is a natural iterative method of solving for this minimum , which turns out , remarkably , to be identical to the generalization of bbp to general graphs @xmath76 .",
    "this approach has proved effective in some cases at finding best segmentations of images via the mode of a two - dimensional mrf .",
    "other interesting ideas have been proposed for solving the segmentation problem which we do not have time to sketch : region growing , see esp .",
    "[ z - y ] ) , using the eigenfunctions of the graph - theoretic laplacian , see [ s - m ] , and multi - scale algorithms , see [ p - b ] and [ s - b - b ] .",
    "-5 mm    although signals as we measure them are always sampled discretely , in the world itself signals are functions on the continua , time or space or both together . in some situations , a much richer mathematical theory emerges by replacing a countable collection of random variables by random processes and asking whether we can find good stochastic models for these continuous signals .",
    "i want to conclude this talk by mentioning three instances where some interesting analysis has arisen when passing to the continuum limit and going into some detail on two . we will not worry about algorithmic issues for these models .",
    "-5 mm    this is the area where the most work has been done , both because of its links with other areas of analysis and because it is one of the central problems of image processing . you observe a degraded image @xmath112 as a function of continuous variables and seek to restore it , removing simultaneously noise and blur . in the discrete",
    "setting , the ising model or variants thereof discussed above can be applied for this .",
    "there are two closely related ways to pass to the continuous limit and reformulate this as a problem in analysis .",
    "as both drop the stochastic interpretation and have excellent treatments in the literature , we only mention briefly one of a family of variants of each approach :    _ optimal piecewise smooth approximation of i via a variational problem _ : @xmath113 where @xmath114 , the improved image , has discontinuities along the set of ` edge ' curves @xmath115 .",
    "this approach is due to the author and shah and has been extensively pursued by the schools of degiorgi and morel .",
    "see [ m - s ] .",
    "it is remarkable that it is still unknown whether the minima to this functional are well behaved , e.g.  whether @xmath115 has a finite number of components .",
    "stochastic variants of this approach should exist .",
    "_ non - linear diffusion of i _ : @xmath116 where @xmath114 at some future time is the enhancement .",
    "this approach started with the work of perona and malik and has been extensively pursued by osher and his coworkers .",
    "see [ gu - m ] .",
    "it can be interpreted as gradient descent for a variant of the previous variational problem .",
    "-5 mm    one of the earliest discoveries about the statistics of images @xmath117 was that their power spectra tend to obey power laws @xmath118 where @xmath119 varies somewhat from image to image but clusters around the value 2 .",
    "this has a very provocative interpretation : this power law is implied by self - similarity ! in the language of lattice field theory , if @xmath120 is a random lattice field and @xmath121 is the block averaged field @xmath122 then we say the field is a renormalization fixed point if the distributions of @xmath117 and of @xmath121 are the same .",
    "the hypothesis that natural images of the world , treated as a single large database , have renormalization invariant statistics has received remarkable confirmation from many quite distinct tests .",
    "why does this hold ?",
    "it certainly is nt true for auditory or tactile signals .",
    "i think there is one major and one minor reason for it .",
    "the major one is that the world is viewed from a random viewpoint , so one can move closer or farther from any scene .",
    "to first approximation , this scales the image ( though not exactly because nearer objects scale faster than distant ones ) .",
    "the minor one is that most objects are opaque but have , by and large , parts or patterns on them and , in turn , belong to clusters of larger things .",
    "this observation may be formulated as saying the world is not merely made up of objects but it is cluttered with them .    the natural setting for",
    "scale invariance is pass to the limit and model images as random functions @xmath112 of two real variables .",
    "then the hypothesis is that a suitable function space supports a probability measure which is invariant under both translations and scalings @xmath123 , whose samples are ` natural images ' .",
    "this hypothesis encounters , however , an infra - red and an ultra - violet catastrophe : + a ) the infra - red one is caused by larger and larger scale effects giving bigger and bigger positive and negative swings to a local value of @xmath117 . but these large scale effects are very low - frequency and this is solved by considering @xmath117 to be defined only modulo an unknown constant , i.e.  it is a sample from a measure on a function space mod constants .",
    "+ b ) the ultra - violet one is worse : there are more and more local oscillations of the signals at finer and finer scales and this contradicts lusin s theorem that an integrable function is continuous outside sets of arbitrarily small measure .",
    "in fact , it is a theorem that _ there is no translation and scale invariant probability measure on the space of locally integrable functions mod constants_. this can be avoided by allowing images to be generalized functions .",
    "in fact , the support can be as small as the intersection of all negative sobolev spaces @xmath124 .",
    "to summarize what a good statistical theory of natural images should explain , we have scale - invariance as just described , kurtosis greater than 3 as described in section 2.1 and finally the right local properties :    * hypothesis i * : :    a theory of images is a translation and scale invariant probability    measure on the space of generalized functions @xmath112 mod    constants .",
    "* hypothesis ii * : :    for any filter @xmath125 with mean 0 , the marginal statistics of    @xmath126 have kurtosis greater than 3 . *",
    "hypothesis iii * : :    the local statistics of images reflect the preferred local geometries ,    esp .",
    "images of straight edges , but also curved edges , corners , bars ,    ` t - junctions ' and ` blobs ' as well as images without geometry , blank    ` blue sky ' patches .",
    "hypothesis iii is roughly the existence of what marr , thinking globally of the image called the _ primal sketch _ and what julesz , thinking locally of the elements of texture , referred to as _ textons_. by scale invariance , the local and global image should have the same elements .    to quantify hypothesis iii ,",
    "what is needed is a major effort at data mining .",
    "specifically , the natural approach seems to be to take a small filter bank of zero mean local filters @xmath127 , a large data base of natural images @xmath128 leading to the sample of points in @xmath129 given by @xmath130 for all @xmath131 .",
    "one seeks a good non - parametric fit to this dataset . but",
    "hypothesis iii shows that this distribution will not be simple .",
    "for example lee et al [ l - p - m ] have taken @xmath132 , @xmath133 a basis of zero mean filters with fixed @xmath134 support .",
    "they then make a linear tranformation in @xmath135 normalizing the covariance of the data to @xmath136 ( ` whitening ' the data ) , and to investigate the outliers , map the data with norms in the upper 20% to @xmath137 by dividing by the norm .",
    "the analysis reveals that the resulting data has asymptotic infinite density along a non - linear surface in @xmath137 ! this surface is constructed by starting with an ideal image , black and white on the two sides of a straight edge and forming a @xmath134 discrete image patch by integrating this ideal image over a tic - tac - toe board of square pixels .",
    "as the angle of the edge and the offset of the pixels to the edge vary , the resulting patches form this surface .",
    "this is the most concrete piece of evidence showing the complexity of local image statistics .    are there models for these three hypotheses ?",
    "we can satisfy the first hypothesis by the unique scale - invariant gaussian model , called the free field by physicists  but its samples look like clouds and its marginals have kurtosis 3 , so neither the second nor third hypothesis is satisfied .",
    "the next best approximation seems to be to use infinitely divisible measures , such as the model constructed by the author and b.gidas [ m - g ] , which we call _ random wavelet expansions _ : @xmath138 where @xmath139 is a poisson process in @xmath140 and @xmath141 are samples from an auxiliary levi measure , playing the role of individual random wavelet primitives .",
    "but this model is based on adding primitives , as in a world of transparent objects , which causes the probability density functions of its marginal filter statistics to be smooth at 0 instead of having peaks there , i.e.  the model does not produce enough ` blue sky ' patches with very low constrast .",
    "a better approach are the random collage models , called _",
    "dead leaves models _ by the french school : see [ l - m - h ] . here",
    "the @xmath141 are assumed to have bounded support , the terms have a random depth and , instead of being simply added , each term occludes anything behind it with respect to depth .",
    "this means @xmath112 equals the one @xmath141 which is in front of all the others whose support contains @xmath142 .",
    "this theory has major troubles with both infra - red and ultra - violet limits but it does provide the best approximation to date of the empirical statistics of images .",
    "it introduces explicitly the hidden variables describing the discrete objects in the image and allows one to model their preferred geometries .",
    "crafting models of this type is not simply mathematically satisfying .",
    "it is central to the main application of computer vision : object recognition .",
    "when an object of interest is obscured in a cluttered badly lit scene , one needs a @xmath44-value for the hypothesis test  is this fragment of stuff part of the sought - for object or an accidental conjunction of things occurring in generic images ? to get this @xmath44-value , one needs a null hypothesis , a theory of generic images .",
    "-5 mm    as we have seen in the last section , modeling images leads to objects and these objects have shape  so we need stochastic models of shape , the ultimate non - linear sort of thing .",
    "again it is natural to consider this in the continuum limit and consider a @xmath49-dimensional shape to be a subset of @xmath129 , e.g.  a connected open subset with nice boundary @xmath115 .",
    "it is very common in multiple images of objects like faces , animals , clothes , organs in your body , to find not identical shapes but warped versions .",
    "how is this to be modeled ?",
    "one can follow the ideas of the previous section and take a highly empirical approach , gathering huge databases of faces or kidneys .",
    "this is probably the road to the best pattern recognition in the long run . but",
    "another principle that grenander has always emphasized is to take advantage of the group of symmetries of the situation  in this case , the group of all diffeomorphisms of @xmath129 .",
    "he and miller and collaborators ( see [ gr - m ] ) were led to rediscover the point of view of arnold which we next describe .",
    "let @xmath143 and @xmath144 be the volume - preserving subgroup .",
    "we want to bypass issues of the exact degree of differentiability of these diffeomorphisms , but consider @xmath145 and @xmath144 as infinite dimensional riemannian manifolds .",
    "let @xmath146 be a path in @xmath144 and define its length by : @xmath147 this length is nothing but the _ right_-invariant riemannian",
    "metric : @xmath148 arnold s beautiful theorem is :    * theorem * _ geodesics in @xmath144 are solutions of euler s equation : _",
    "@xmath149 this result suggests using geodesics on suitable infinite dimensional manifolds to model optimal warps between similar shapes in images and using diffusion on these manifolds to craft stochastic models .",
    "but we need to get rid of the volume - preserving restriction .",
    "the weak metric used by arnold no longer works on the full @xmath145 and in [ c - r - m ] , christensen et al introduced : @xmath150 where @xmath105 is any vector field and @xmath151 is a fixed positive self - adjoint differential operator e.g.  @xmath152 then a path @xmath153 in @xmath76 has both a velocity : @xmath154 and a _ momentum : _",
    "@xmath155 ( so @xmath156 , @xmath157 the green s function of @xmath151 ) .",
    "what is important here is that the momentum @xmath158 can be a generalized function , even when @xmath159 is smooth .",
    "the generalization of arnold s theorem , first derived by vishik , states that geodesics are : @xmath160 this equation is a new kind of regularized compressible euler equation , called by marsden the template matching equation ( tme ) .",
    "the left hand side is the derivative along the flow of the momentum , as a measure , and the right hand side is the force term .",
    "a wonderful fact about this equation is that by making the momentum singular , we get very nice equations for geodesics on the @xmath145-homogeneous spaces : + ( a ) @xmath161 set of all @xmath30-tuples of distinct points in @xmath162 and + ( b ) @xmath163 set of all images of the unit ball under a diffeomorphism . + in the first case , we have @xmath164 where @xmath165 is the stabilizer of a specific set @xmath166 of @xmath30 distinct points . to get geodesics on @xmath167",
    ", we look for ` particle solutions of the tme ' , i.e.@xmath168 where @xmath169 is a path in @xmath167 the geodesics on @xmath145 , which are perpendicular to all cosets @xmath170 , are then the geodesics on @xmath167 for the quotient metric : @xmath171 where @xmath172 . for these we get the interesting hamiltonian ode : @xmath173 which makes points traveling in the same direction attract each other and points going in opposite directions repel each other . this space leads to a non - linear version of the theory of landmark points and shape statistics of kendall [ sm ] and has been developed by younes [ yo ] .",
    "a similar treatment can be made for the space of shapes @xmath174 , where @xmath175 is the stabilizer of the unit sphere .",
    "geodesics on @xmath176 come from solutions of the tme for which @xmath177 is supported on the boundary of the shape and perpendicular to it .",
    "even though the first of these spaces @xmath178 might seem to be quite a simple space , it seems to have a remarkable global geometry reflecting the many perceptual distinctions which we make when we recognize a similar shapes , e.g.  a cell decomposition reflecting the different possible graphs which can occur as the ` medial axis ' of the shape .",
    "this is an area in which i anticipate interesting results .",
    "we can also use these riemannian structures to define brownian motion on @xmath179 and @xmath167 ( see [ d - g - m ] , [ yi ] ) . putting a random stopping time on this walk ,",
    "we get probability measures on these spaces . to make the ideas more concrete ,",
    "in figure 6 we show a simulation of the random walk on @xmath178 .",
    "-5 mm    the patterns which occur in nature s sensory signals are complex but allow mathematical modeling .",
    "their study has gone through several phases . at first , ` off - the - shelf ' classical models ( e.g.  linear gaussian models ) were adopted based only on intuition about the variability of the signals .",
    "now , however , two things are happening : computers are large enough to allow massive data gathering to support fully non - parametric models .",
    "and the issues raised by these models are driving the study of new areas of mathematics and the development of new algorithms for working with these models .",
    "applications like general purpose speech recognizers and computer driven vehicles are likely in the foreseeable future .",
    "perhaps the ultimate dream is a fully unsupervised learning machine which is given only signals from the world and which finds their statistically significant patterns with no assistance : something like a baby in its first 6 months ."
  ],
  "abstract_text": [
    "<S> is there a mathematical theory underlying intelligence ? </S>",
    "<S> control theory addresses the output side , motor control , but the work of the last 30 years has made clear that perception is a matter of bayesian statistical inference , based on stochastic models of the signals delivered by our senses and the structures in the world producing them </S>",
    "<S> . we will start by sketching the simplest such model , the hidden markov model for speech , and then go on illustrate the complications , mathematical issues and challenges that this has led to .    </S>",
    "<S> 4.5 mm    * keywords and phrases : * perception , speech , vision , bayesian , statistics , inference , markov . </S>"
  ]
}