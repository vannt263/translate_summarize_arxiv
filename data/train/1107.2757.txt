{
  "article_text": [
    "we consider a lossless data compression scheme that builds upon the the _ number partitioning _ problem and the closely related problem of _ subset sums _ : given a set of integers , @xmath0 , @xmath1 , @xmath2 , the number partitioning problem is the problem of finding a subset @xmath3 , such that the sums of @xmath4 over @xmath5 would be as balanced as possible with the sum over the remaining @xmath4 .",
    "more precisely , the goal is to find a subset @xmath5 such that @xmath6 would be minimum , or equivalently , to find a binary vector @xmath7 such that @xmath8 would be minimum .",
    "perfect partitioning means that this expression is exactly equal to zero .",
    "the problem of finding an optimum partition is np ",
    "complete @xcite , @xcite and it has a fairly long history ( see , e.g. , ( * ? ? ? * section 9.2 ) , ( * ? ? ? * chapter 7 ) and many references therein ) . for the case",
    "where @xmath4 are drawn independently at random , some rigorous results have been obtained using methods of statistical mechanics , see , e.g , @xcite , @xcite , @xcite , @xcite .",
    "it has been shown ( see also @xcite , @xcite , @xcite ) that for a randomly selected vector @xmath9 , and for @xmath10 ( @xmath11 , constant ) , there is a phase transition at @xmath12 .",
    "for @xmath13 , there are exponentially many solutions ( @xmath14vectors ) to the number partitioning problem .",
    "more precisely , there are exponentially about @xmath15 many solutions on the average . however , for @xmath16 , the probability that there exists even one solution decays exponentially .    in the related problem of subset sums",
    ", the scope is extended to the evaluation of the total number @xmath17 of binary vectors @xmath18 such that @xmath19 , for any given value of @xmath20 in the appropriate range , not only @xmath21 .",
    "sasamoto @xcite proposed a data compression scheme based on subset sums in its constrained version , that is , the one where binary vectors are sought only among those which have a given _ composition _ , namely , given numbers @xmath22 ( @xmath23 ) and @xmath24 ( @xmath25 ) of occurrences of @xmath26 and @xmath27 , respectively , or equivalently , a given value of @xmath28 .. ] in particular , in view of the above described results concerning phase transitions , sasamoto s insight was that for @xmath29 above a certain threshold , the mapping between the set of binary vectors @xmath18 of a given composition to the sums @xmath30 must be essentially one  to - one for a typical realization of @xmath9 .",
    "this has lead him to propose a lossless data compression scheme that is based on encoding a binary string @xmath31 , with a composition of @xmath22 and @xmath24 , using a binary representation of @xmath32 plus a relatively small overhead ( of @xmath33 bits ) for specifying the composition of @xmath31 , or equivalently , the value of @xmath34 .",
    "sasamoto argued that the threshold of reliable decoding occurs at @xmath35 , where @xmath36 is entropy of the binary information source that emits sequences with the aforementioned composition ( within some small tolerance ) with high probability , and so by taking @xmath37 ( @xmath38 , arbitrarily small ) and using the fact that the range of possible values of @xmath32 does not exceed @xmath39 , one may encode @xmath32 using @xmath40+\\log n$ ] bits , and thereby essentially achieve the entropy of the information source . while this coding scheme is not very attractive from the practical point of view , the interesting point here is the relationship between the phase transition of the subset problem and the abrupt passage between ambiguous and non  ambiguous decoding as @xmath29 crosses the entropy , in agreement with shannon s fundamental coding theorems @xcite .",
    "sasamoto s approach was to analyze the number @xmath41 of configurations @xmath18 with @xmath42 and @xmath43 , where @xmath20 and @xmath44 are the values pertaining to the source sequence @xmath45 that was actually compressed .",
    "he argued that for a typical realization of @xmath4 , the behavior is as follows : for @xmath46 , @xmath41 is exponentially large and so the decoding of @xmath47 , based on @xmath20 and @xmath44 , is ambiguous , but for @xmath48 , the expectation of @xmath41 is exponentially small , and so the decoding is reliable with high probability . in order to assess the number of solutions to the two simultaneous equations @xmath42 and @xmath43",
    ", he applied the saddle point method ( see also @xcite , @xcite ) .",
    "in particular , he first defined a partition function of a hamiltonian defined by a linear combination of @xmath32 and @xmath49 , and then used the integral representation of the inverse transform of this partition function , that yields @xmath41 .",
    "this integral in turn was approximated using the saddle point method .",
    "the analysis in @xcite , which relies on the analysis in @xcite , raises two technical concerns , however .",
    "the first is about the validity of the saddle point method in this situation : while the saddle point method is perfectly rigorous under the asymptotic regime where @xmath50 while @xmath51 is kept fixed , its validity becomes rather questionable is a function of @xmath52 ( see the ending paragraph of section 3 on page 9559 and pp .",
    "95639564 therein ) . ] in a regime where @xmath51 grows with @xmath52 , especially when the growth rate of @xmath51 is as fast as exponential .",
    "the authors of @xcite realize that the resulting approximation is definitely not valid when @xmath16 , which yields @xmath53 .",
    "the point , however , is that it is not quite clear whether this approximation is reliable even when @xmath54 .",
    "the fact that the resulting approximation below @xmath12 does not lead to an obvious absurd is not enough to guarantee that the approximation is reliable .",
    "the second concern is that there is a difference between calculating the expectation of @xmath41 when @xmath20 and @xmath44 are fixed and deterministic , and calculating the expectation of @xmath41 when @xmath55 and @xmath56 , because the latter is a _",
    "random variable_. the former quantity is what sasamoto calculated and the latter is actually the relevant quantity for analyzing the data compression scheme .",
    "when computing the expectation of @xmath57 , the randomness of @xmath58 is induced by the same set of random variables @xmath4 that generate also the values of @xmath32 pertaining to all other binary vectors @xmath18 . in other words , in this calculation both the function @xmath59 and its first argument @xmath58 fluctuate together , depending on @xmath4 .",
    "indeed , for one thing , @xmath57 ( and hence also its expectation ) must always be at least as large as unity ( by construction ) , whereas the expectation of @xmath41 for fixed @xmath20 and @xmath44 is shown in @xcite to decay exponentially to zero for @xmath29 above the threshold .",
    "this is clearly a contradiction .    in this work ,",
    "we first propose a rigorous approach to evaluate the expectation of @xmath60 , which is valid for every @xmath61 .",
    "our starting point is ( or can be interpreted as ) essentially the same inverse transform integral of the above  mentioned partition function ( but with a slight modification to account for the above discussed replacement of a fixed @xmath20 by @xmath58 ) .",
    "however , unlike in @xcite and @xcite , we avoid the use of the saddle point method in the evaluation of this integral and we propose a more refined analysis instead .",
    "the final result of this analysis is similar to that of @xcite for @xmath29 below the threshold , but it is not quite identical above the threshold : we show that when the relative frequencies of @xmath62 and @xmath63 in @xmath47 are @xmath64 and @xmath65 , respectively , and @xmath10 , @xmath66},\\ ] ] where @xmath67 denotes expectation w.r.t .",
    "the randomness of @xmath4 and @xmath68 means equality in the exponential order sense ( @xmath69 means that @xmath70 as @xmath71 ) .",
    "thus , indeed there is a phase transition at @xmath35 : for @xmath46 , there are exponentially many source vectors that are mapped to the same value of @xmath58 on the average , but for @xmath48 the expected number of additional source vectors ( other than @xmath47 ) vanishes . since @xmath57 is an integer  valued random variable , this also means ( by the chebychev inequality ) that @xmath72 also vanishes for @xmath73 .",
    "while the final conclusions of our analysis are essentially the same as in @xcite ( for @xmath46 ) , the message in this paper is three  fold : the first message is that it is not necessary to resort to the saddle point method in this case and it is possible to make the analysis rigorous as we show .",
    "the second message is that our analysis extends easily to more general situations , like larger source alphabets , availability of side information at the decoder ( a.k.a .",
    "slepian  wolf encoding ( * ? ? ?",
    "* section 15.8 ) , @xcite ) , and so on .",
    "the third message is that this analysis method can be used also to handle non ",
    "trivial alternative coding schemes , like a scheme based on the unconstrained subset sum problem .",
    "it turns out that for such a scheme , the phase transition occurs at a critical value of @xmath29 which is different from the entropy @xmath74 , and we provide an explicit expression , which is not trivial .",
    "consider first the problem of counting the number of solutions @xmath18 to the two simultaneous equations @xmath75 denoting the kronecker delta function by @xmath76 and @xmath77 by @xmath78 , we have @xmath79\\right\\}\\times\\nonumber\\\\ & & \\exp\\left\\{i\\theta\\left(n_+-n_--\\sum_j\\sigma_j)\\right)\\right\\}\\nonumber\\\\ & = & \\int_{-\\pi}^{+\\pi}\\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega\\mbox{d}\\theta } { ( 2\\pi)^2}e^{i\\theta(n_+-n_-)}\\times\\nonumber\\\\ & & \\prod_{j=1}^n\\sum_{\\sigma_j=-1}^{+1}\\exp\\{i\\omega a_j(\\hat{\\sigma}_j-\\sigma_j)-i\\theta\\sigma_j\\}\\nonumber\\\\ & = & \\int_{-\\pi}^{+\\pi}\\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega\\mbox{d}\\theta}{(2\\pi)^2}e^{i\\theta(n_+-n_- ) } \\prod_{j=1}^n\\left[e^{-i\\theta\\hat{\\sigma}_j}+e^{i(\\theta+2\\omega a_j)\\hat{\\sigma}_j}\\right]\\nonumber\\\\ & = & \\int_{-\\pi}^{+\\pi}\\int_{-\\pi}^{+\\pi } \\frac{\\mbox{d}\\omega\\mbox{d}\\theta}{(2\\pi)^2}e^{i\\theta(n_+-n_- ) } \\prod_{j:~\\hat{\\sigma}_j=+1}\\left[e^{-i\\theta}+e^{i(\\theta+2\\omega a_j)}\\right]\\times\\nonumber\\\\ & & \\prod_{j:~\\hat{\\sigma}_j=-1}\\left[e^{i\\theta}+e^{-i(\\theta+2\\omega a_j)}\\right]\\nonumber\\\\ & = & \\int_{-\\pi}^{+\\pi}\\int_{-\\pi}^{+\\pi } \\frac{\\mbox{d}\\omega\\mbox{d}\\theta}{(2\\pi)^2}\\cdot \\prod_{j:~\\hat{\\sigma}_j=+1}\\left[1+e^{2i(\\theta+\\omega a_j)}\\right]\\times\\nonumber\\\\ & & \\prod_{j:~\\hat{\\sigma}_j=-1}\\left[1+e^{-2i(\\theta+\\omega a_j)}\\right].\\end{aligned}\\ ] ] taking now the expectation over @xmath4 , and denoting @xmath80 we have : @xmath81^{n_+}\\times\\nonumber\\\\ & & \\left[1+e^{-2i\\theta}\\phi(-\\omega)\\right]^{n_-}\\nonumber\\\\ & = & 1+\\sum_n\\sum_k\\left(\\begin{array}{cc } n_+ \\\\",
    "n\\end{array}\\right ) \\left(\\begin{array}{cc } n_- \\\\ k\\end{array}\\right)\\times\\nonumber\\\\ & & \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi}\\phi^n(\\omega)\\phi^k(-\\omega ) \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\theta}{2\\pi}e^{2i\\theta(n - k)}\\nonumber\\\\ & = & 1+\\sum_{n=1}^{\\min\\{n_+,n_-\\}}\\left(\\begin{array}{cc } n_+ \\\\",
    "n\\end{array}\\right ) \\left(\\begin{array}{cc } n_- \\\\ n\\end{array}\\right)\\times\\nonumber\\\\ & & \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi}[\\phi(\\omega)\\phi(-\\omega)]^n\\nonumber\\\\ & = & 1+\\sum_{n=1}^{\\min\\{n_+,n_-\\}}l^{-2n}\\left(\\begin{array}{cc } n_+ \\\\",
    "n\\end{array}\\right ) \\left(\\begin{array}{cc } n_- \\\\ n\\end{array}\\right)\\sum_{s = n}^{nl}(\\lambda_s^n)^2 , \\label{longchain1}\\end{aligned}\\ ] ] where the summation over @xmath82 and @xmath83 in the second line is over @xmath84 and where @xmath85 is the number of vectors @xmath86 with @xmath87 .",
    "the last line of eq .",
    "( [ longchain1 ] ) has a simple interpretation : let @xmath88 , @xmath89 , @xmath90 , and @xmath91 .",
    "obviously , @xmath92 if and only if @xmath93 also , for every @xmath31 with the same composition as @xmath47 , @xmath94 . let then @xmath95 .",
    "every @xmath31 with the same composition as @xmath47 corresponds to a choice of particular subsets ( @xmath96 and @xmath97 , both of size @xmath82 ) of @xmath98 and @xmath99 , respectively . for a given @xmath82 ,",
    "the number of combinations of these subsets is the product of the binomial coefficients in the last line of ( [ longchain1 ] ) . for each such combination ,",
    "the probability of the event ( [ event ] ) is @xmath100 .",
    "the last line of eq.([longchain1 ] ) exhausts the product of this probability by the number of combinations for all possible values of @xmath82 .",
    "so far our analysis has been exact .",
    "we now need an evaluation of the exponential order of @xmath85 , where @xmath101 scales like @xmath102 , i.e. , @xmath103 , @xmath104 , and we expect the behavior to be symmetric in @xmath105 about the point @xmath106 .",
    "so it is enough to cover the range @xmath107 .",
    "the event @xmath87 is obviously equivalent to the event @xmath108 , where @xmath109 .",
    "the number of points @xmath110 with @xmath111 is exactly the same as number of points @xmath112 which satisfy @xmath113 , which is with excellent approximation given by @xmath114 times the volume of the region within the unit cube @xmath115^{n-1}$ ] of continuous valued @xmath116vectors @xmath117 that satisfy @xmath118 ( see appendix a for more details ) .",
    "this volume in turn has an exact formula ( see , e.g. , ( * ? ? ?",
    "* theorems 1,4 ) ) , which is given in the form of a sum of expressions with alternating signs , but it is not trivial to assess the exponential order of this formula in a compact manner .",
    "this result is actually quite expected .",
    "the critical rate @xmath148 can not be strictly larger than @xmath74 because , as said , the total number of sequences with composition @xmath149 is exponentially @xmath144 : had @xmath148 been larger than @xmath74 we would have obtained that @xmath150 grows with an exponential rate which is faster than @xmath74 ( at least when @xmath51 is subexponential ) , which is impossible . on the other hand , @xmath148 can not be strictly smaller than @xmath74 , because then it would mean that sasmoto s coding scheme achieves a compression ratio that is better than the entropy .",
    "thus , @xmath148 must be equal to @xmath74 .",
    "the above derivation extends straightforwardly in several directions ( one at a time or simultaneously ) : +    _ 1 .",
    "lossless source coding in the presence of correlated side information at the decoder .",
    "_ consider the case where the decoder has access to a side information sequence @xmath151 , which is correlated to the source sequence according to a given joint distribution @xmath152 , and the two sequences are i.i.d .  over time , i.e. , @xmath153 it is well known ( see , e.g. , ( * ? ? ?",
    "* section 15.8 ) ) that in this case , the best achievable compression ratio is given by the conditional entropy of the source given the side information , even if the encoder does not have access to the side information ( slepian  wolf coding @xcite ) .",
    "here we propose an alternative way to achieve this optimum compression ratio based on subset sum encoding : the encoder works essentially in the same manner as before .",
    "the decoder seeks solutions to the equation @xmath154 only within the set of @xmath31vectors whose joint empirical distribution together with the side information sequence is close to the joint distribution @xmath152 ( within some small tolerance ) . in this case",
    ", an analysis similar to the above , reveals that the critical rate is given by the conditional entropy of the source given the side information .",
    "multiple subset sums . _ instead of one set of random variables @xmath155 , randomly drawn in @xmath156 , consider an array of random variables @xmath157 , @xmath2 , @xmath158 , all statistically independent , where each @xmath159 is uniformly distributed in @xmath160 , where @xmath161 , @xmath162",
    ". the encoder encodes @xmath31 into @xmath163 , where each @xmath164 is represented by @xmath165 bits .",
    "the decoder reconstructs @xmath31 as the first vector whose encoding agrees with the given compressed input @xmath163 .",
    "it is easy to show that the decoding is unambiguous with high probability iff @xmath166 .",
    "thus , here the phase transition occurs at the whole hyperplane @xmath167 .",
    "general finite alphabets . _",
    "another natural direction of extending the above result is from the case of a binary source alphabet to a general finite alphabet which , without loss of generality , will be assumed to be @xmath168 .",
    "a simple strategy is to decompose the problem into @xmath169 binary encoding problems , and in each one of them we can rely directly on the binary code construction .",
    "let the input source string @xmath31 have @xmath170 occurrences of @xmath171 , @xmath172 ( of course , @xmath173 ) .",
    "now , for each @xmath174 , let us select independently at random @xmath175 , each one drawn under the uniform distribution over the integers @xmath176 , where @xmath177 and @xmath178 will be specified shortly .",
    "the resulting random selections are revealed to the encoder and the decoder .",
    "the encoder works as follows : given the source vector @xmath31 , we first encode the numbers @xmath179 as overhead ( just like the transmission of @xmath180 in the binary case ) .",
    "next , for each @xmath181 , we calculate @xmath182 and transmit its value using @xmath183 bits .",
    "the role of each @xmath184 is to represent the information pertaining to all locations where @xmath185 .",
    "based on the results of the binary alphabet case , in order to decode @xmath186 unambiguously , @xmath187 should be at least as large as @xmath188 ( think of encoding the binary sequences @xmath189 , where @xmath190 is the indicator function ) .",
    "for the next stage , the task is to fill in @xmath191 out of the remaining @xmath192 locations by @xmath193 , and so we have reduced the problem to that of encoding the binary sequence @xmath194 of length @xmath192 . by the same reasoning then , to decode @xmath195 reliably , @xmath196 should be at least as large as @xmath197 .",
    "this procedure continues until @xmath198 , where reliable decoding of @xmath199 requires @xmath200 .",
    "the overall coding rate ( neglecting the overhead ) is then @xmath201 which is easily shown ( using the chain rule of the entropy ) to be identical to the entropy of the source @xmath202",
    "returning to the binary case , suppose next that we wish to make the mapping from @xmath31 to @xmath32 essentially one  to  one over the _",
    "entire _ source vector space @xmath203 , and then there would be no need to specify the composition of @xmath47 to the decoder .",
    "this corresponds to the unconstrained subset sum problem .",
    "how large should @xmath29 be now ? here , the analysis is similar but somewhat more involved .",
    "the point in presenting the analysis for this case is not quite motivated by the usefulness of the data compression scheme itself , but more about demonstrating the applicability of the analysis method .",
    "this time , the derivation is as follows : @xmath204\\nonumber\\\\ & = & \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi } \\prod_{j=1}^n\\left(1+e^{i2\\omega a_j{{\\hat{\\sigma}}}_j}\\right).\\end{aligned}\\ ] ] taking now the expectation w.r.t .",
    "the randomness of @xmath205 , we readily obtain @xmath206^{n_+}\\cdot\\left[1+\\phi^*(\\omega)\\right]^{n_-}.\\ ] ] assuming that the binary vector @xmath207 is governed by a binary memoryless source with @xmath208 , we next take an ensemble average w.r.t .  the randomness of @xmath209 , and obtain @xmath210^k \\cdot\\left[q(1+\\phi^*(\\omega))\\right]^{n - k}\\nonumber\\\\ & = & \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi } \\left[1+p\\phi(\\omega)+q\\phi^*(\\omega)\\right]^n\\nonumber\\\\ & = & 1+\\sum_{k=1}^n\\left(\\begin{array}{cc}n \\\\",
    "k\\end{array}\\right)\\cdot \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi}\\left[p\\phi(\\omega)+q\\phi^*(\\omega)\\right]^k\\nonumber\\\\ & = & 1+\\sum_{k=1}^n\\left(\\begin{array}{cc}n \\\\",
    "k\\end{array}\\right)\\cdot l^{-k}\\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi } \\left[p\\sum_{\\ell=1}^{l}e^{i2\\omega\\ell}+q\\sum_{\\ell=1}^{l}e^{-i2\\omega\\ell}\\right]^k\\nonumber\\\\ & = & 1+\\sum_{k=1}^n\\left(\\begin{array}{cc}n",
    "\\\\ k\\end{array}\\right)\\cdot l^{-k } \\sum_{r=0}^k\\left(\\begin{array}{cc}k \\\\ r\\end{array}\\right)p^rq^{k - r}\\times\\nonumber\\\\ & & \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi } \\left(\\sum_{\\ell=1}^{l}e^{i2\\omega\\ell}\\right)^r\\cdot \\left(\\sum_{\\ell=1}^{l}e^{-i2\\omega\\ell}\\right)^{k - r}\\nonumber\\\\ & = & 1+\\sum_{k=1}^n\\left(\\begin{array}{cc}n \\\\",
    "k\\end{array}\\right)\\cdot l^{-k } \\sum_{r=0}^k\\left(\\begin{array}{cc}k",
    "\\\\ r\\end{array}\\right)p^rq^{k - r}\\times\\nonumber\\\\ & & \\int_{-\\pi}^{+\\pi}\\frac{\\mbox{d}\\omega}{2\\pi } \\left(\\sum_{\\ell = r}^{rl}\\lambda_{\\ell}^re^{i2\\omega\\ell}\\right)\\cdot \\left(\\sum_{\\ell = k - r}^{(k - r)l}\\lambda_{\\ell}^{k - r}e^{-i2\\omega\\ell}\\right)\\nonumber\\\\ & = & 1+\\sum_{k=1}^n\\left(\\begin{array}{cc}n \\\\",
    "k\\end{array}\\right)\\cdot l^{-k } \\sum_{r=0}^k\\left(\\begin{array}{cc}k \\\\",
    "r\\end{array}\\right)p^rq^{k - r}\\times\\nonumber\\\\ & & \\sum_{\\ell=\\max\\{r , k - r\\}}^{l\\min\\{r , k - r\\ } } \\lambda_{\\ell}^r\\lambda_{\\ell}^{k - r}.\\end{aligned}\\ ] ] now , similarly as before @xmath211 where @xmath212,\\ ] ] where @xmath213 . denoting @xmath214 and substituting this into eq .",
    "( [ longchain2 ] ) , we obtain @xmath215\\right\\}\\nonumber\\\\ & = & \\frac{1}{l}\\cdot\\exp\\left\\{n\\max_\\alpha[h(\\alpha)- \\alpha\\min_\\beta(d(\\beta\\|p)+\\psi(\\beta))]\\right\\}\\nonumber\\\\ & = & \\frac{1}{l}\\cdot\\exp\\left\\{n\\max_\\alpha[h(\\alpha)- \\alpha\\xi(p)]\\right\\}\\nonumber\\\\ & = & \\frac{1}{l}\\cdot 2^{n\\log_2[1+e^{-\\xi(p)}]},\\end{aligned}\\ ] ] where we have defined @xmath216 and @xmath217.\\ ] ] again , if @xmath10 , then the phase transition is now at @xmath218 , where @xmath219.\\ ] ]    note the special care should be exercised in the extreme cases where @xmath220 and @xmath221 .",
    "it is easy to see that in these cases @xmath222 for all @xmath223 , except @xmath224 but when @xmath225 and @xmath226 , @xmath227 , thus the sum @xmath228 is infinite for every @xmath229 $ ] .",
    "the choice @xmath230 is actually not allowed in the out  most maximization over @xmath231 since the sum over @xmath83 begins with @xmath232 .",
    "thus , for @xmath220 and @xmath221 , we have @xmath233 , which means that @xmath234 , as expected .",
    "obviously , @xmath148 can not be smaller than the entropy of the source . in general , @xmath148 is larger than the entropy , but the coding rate of the corresponding data compression scheme need not be as large as @xmath148 . by applying variable  rate entropy coding to @xmath32 , taking advantage of the non  uniform distribution of this random variable , one can compress it at a rate very close to the entropy of the source . while in this case , @xmath148 no longer has the meaning of coding rate , it does another meaning , which is related to the storage requirement for saving the numbers @xmath4 .",
    "note that for @xmath12 , it is easy to specify a particular set of integers @xmath4 that yields a one  to ",
    "one mapping from @xmath31 to @xmath32 : by setting @xmath235 , @xmath32 becomes the standard binary representation of @xmath31 ( up to a fixed shift ) .",
    "the above result tells us that we can manage with less storage since @xmath148 is in general less than 1 , except the case @xmath236 , where @xmath237 ( see also ( * ? ? ?",
    "* proposition 7.6 and exercise 7.8 ) ) .",
    "useful discussions with alexander vardy and with tuvi etzion , concerning the evaluation of @xmath85 ( which appears first in eq .",
    "( [ longchain1 ] ) ) , are acknowledged with thanks . in particular , a.  vardy has drawn my attention to ref .",
    "@xcite in this context and pointed out its relevance .",
    "given a lattice and given a certain region in space , the number of lattice sites in that region is approximately given by the volume of the region divided by the volume of a single voronoi cell pertaining to that lattice .",
    "this approximation improves as the ratio between these two volumes becomes very large .",
    "more precisely , there is a slight correction due to the fact that some of these voronoi cells may not be entirely included in the region in question . to obtain upper and lower bounds on the number of lattice points ,",
    "one may slightly expand ( for an upper bound ) or shrink ( for a lower bound ) the given region by an amount that guarantees full inclusion ( for an upper lower bound ) or full exclusion ( for a lower bound ) of the partially included voronoi cells that are near the boundaries . in our case",
    ", we have a cubic lattice in @xmath116 dimensions with spacings of @xmath238 in each dimension , so the volume of a voronoi cell is @xmath239 . by replacing @xmath240 with @xmath241",
    ", one can obtain the aforementioned upper and lower bounds , but the corrections of @xmath242 to @xmath240 and to @xmath243 have negligible effects in the exponential scale since @xmath51 grows exponentially with @xmath82 and hence @xmath244 vanishes in the limit .",
    "the modulus of the integrand depends solely on the real part of the exponent of the integrand , namely , on @xmath245 .",
    "now , consider an arbitrary complex number @xmath246 .",
    "then obviously , @xmath247 thus , we have to show that @xmath248 is maximized at @xmath249 , and only at @xmath249 , for all @xmath250 .",
    "this would be equivalent to the assertion that @xmath251 with equality if and only if @xmath249 , since the left  hand side is obtained from the right  hand side by setting @xmath249 .    in order to prove eq .",
    "( [ needtobeproved ] ) , we begin from the obvious inequality @xmath252 , which holds for all @xmath253 . integrating both sides of this inequality from @xmath254 to @xmath255 ( @xmath256 ) , we obtain @xmath257 with equality if and only if @xmath249 , or equivalently , @xmath258 which now holds for all @xmath255 since the left  hand side is an even function . similarly , beginning from the inequality @xmath259 and integrating both sides twice , we get @xmath260 , or equivalently @xmath261 which when combined with ( [ omegaineq ] ) yields @xmath262 or equivalently , @xmath263 which is @xmath264 adding @xmath265 to both sides of this inequality and rearranging terms , one obtains @xmath266\\le ( r^2+\\omega^2)(1-e^{-r})^2,\\ ] ] which is equivalent to eq .",
    "( [ needtobeproved ] ) ."
  ],
  "abstract_text": [
    "<S> we propose a rigorous analysis approach for the subset sum problem in the context of lossless data compression , where the phase transition of the subset sum problem is directly related to the passage between ambiguous and non  ambiguous decompression , for a compression scheme that is based on specifying the sequence composition . </S>",
    "<S> the proposed analysis lends itself to straightforward extensions in several directions of interest , including non  binary alphabets , incorporation of side information at the decoder ( slepian  wolf coding ) , and coding schemes based on multiple subset sums . </S>",
    "<S> it is also demonstrated that the proposed technique can be used to analyze the critical behavior in a more involved situation where the sequence composition is not specified by the encoder .    : number partitioning , integer partitioning , subset sum , data compression , source coding , entropy , phase transitions . </S>"
  ]
}