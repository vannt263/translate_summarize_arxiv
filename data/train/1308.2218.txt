{
  "article_text": [
    "the method of random projections has become popular for large - scale machine learning applications such as classification , regression , matrix factorization , singular value decomposition , near neighbor search , bio - informatics , and more  @xcite . in this paper",
    ", we study a number of simple and effective schemes for * coding * the projected data , with the focus on similarity estimation and training linear classifiers  @xcite .",
    "we will closely compare our method with the influential prior coding scheme in  @xcite .",
    "+ consider two high - dimensional vectors , @xmath0 .",
    "the idea is to multiply them with a random normal projection matrix @xmath1 ( where @xmath2 ) , to generate two ( much ) shorter vectors @xmath3 : @xmath4 in real applications , the dataset will consist of a large number of vectors ( not just two ) . without loss of generality , we use one pair of data vectors ( @xmath5 ) to demonstrate our results .",
    "+ in this study , for convenience , we assume that the marginal euclidian norms of the original data vectors , i.e. , @xmath6 , are known .",
    "this assumption is reasonable in practice  @xcite .",
    "for example , the input data for feeding to a support vector machine ( svm ) are usually normalized , i.e. , @xmath7 .",
    "computing the marginal norms for the entire dataset only requires one linear scan of the data , which is anyway needed during data collection / processing . without loss of generality , we assume @xmath7 in this paper . the joint distribution of @xmath8 is hence a bi - variant normal : @xmath9 \\sim n \\left ( \\left[\\begin{array}{c}0\\\\ 0\\end{array } \\right],\\ \\left[\\begin{array}{cc}1 & \\rho\\\\ \\rho & 1 \\end{array } \\right ] \\right ) , \\text { i.i.d.}\\hspace{0.25 in } j = 1 , 2 , ... , k.\\end{aligned}\\ ] ] where @xmath10 ( assuming @xmath7 ) . for convenience and brevity ,",
    "we also restrict our attention to @xmath11 , which is a common scenario in practice . throughout the paper",
    ", we adopt the conventional notation for the standard normal pdf @xmath12 and cdf @xmath13 : @xmath14      our first proposal is perhaps the most intuitive scheme , based on a simple uniform quantization : @xmath15 where @xmath16 is the bin width and @xmath17 is the standard floor operation , i.e. , @xmath18 is the largest integer which is smaller than or equal to @xmath19 . for example , @xmath20 , @xmath21 , @xmath22 .",
    "later in the paper we will also use the standard ceiling operation @xmath23 .",
    "we show that the collision probability @xmath24 is a monotonically increasing function of the similarity @xmath25 , making ( [ eqn_hw ] ) a suitable coding scheme for similarity estimation and near neighbor search .",
    "the potential benefits of coding with a small number of bits arise because the ( uncoded ) projected data , @xmath26 and @xmath27 , being real - valued numbers , are neither convenient / economical for storage and transmission , nor well - suited for indexing .",
    "+ since the original data are assumed to be normalized , i.e. , @xmath7 , the marginal distribution of @xmath28 ( and @xmath29 ) is the standard normal , which decays rapidly at the tail , e.g. , @xmath30 , @xmath31 .",
    "if we use @xmath32 as cutoff , i.e. , values with absolute value greater than 6 are just treated as @xmath33 and 6 , then the number of bits needed to represent the bin the value lies in is @xmath34 . in particular ,",
    "if we choose the bin width @xmath35 , we can just record the sign of the outcome ( i.e. , a one - bit scheme ) .",
    "in general , the optimum choice of @xmath36 depends on the similarity @xmath25 and the task . in this paper",
    "we focus on the task of similarity estimation ( of @xmath25 ) and we will provide the optimum @xmath36 values for all similarity levels .",
    "interestingly , using our uniform quantization scheme , we find in a certain range the optimum @xmath36 values are quite large , and in particular are larger than 6 .",
    "+ we can build * linear classifier * ( e.g. , linear svm ) using coded random projections .",
    "for example , assume the projected values are within @xmath37 .",
    "if @xmath38 , then the code values output by @xmath39 will be within the set @xmath40 .",
    "this means we can represent a projected value using a vector of length 6 ( with exactly one 1 ) and the total length of the new feature vector ( fed to a linear svm ) will be @xmath41 .",
    "see more details in section  [ sec_svm ] .",
    "this trick was also recently used for linear learning with binary data based on _ b - bit minwise hashing _  @xcite .",
    "of course , we can also use the method to speed up kernel evaluations for kernel svm with high - dimensional data .    * near neighbor search * is a basic problem studied since the early days of modern computing  @xcite with applications throughout computer science .",
    "the use of coded projection data for near neighbor search is closely related to _ locality sensitive hashing ( lsh ) _  @xcite .",
    "for example , using @xmath42 projections and a bin width @xmath36 , we can naturally build a hash table with @xmath43 buckets .",
    "we map every data vector in the dataset to one of the buckets . for a query data vector ,",
    "we search for similar data vectors in the same bucket .",
    "because the concept of lsh is well - known , we do not elaborate on the details .",
    "compared to  @xcite , our proposed coding scheme has better performance for near neighbor search ; the analysis will be reported in a separate technical report .",
    "this paper focuses on similarity estimation .",
    "@xcite proposed the following well - known coding scheme , which uses windows and a random offset : @xmath44 where @xmath45 .",
    "@xcite showed that the collision probability can be written as @xmath46 where @xmath47 is the euclidean distance between @xmath48 and @xmath49 .",
    "the difference between ( [ eqn_hwq ] ) and our proposal ( [ eqn_hw ] ) is that we do not use the additional randomization with @xmath50 ( i.e. , the offset ) . by comparing them closely",
    ", we will demonstrate the following advantages of our scheme :    1 .",
    "operationally , our scheme @xmath51 is simpler than @xmath52 .",
    "2 .   with a fixed @xmath36 ,",
    "our scheme @xmath51 is always more accurate than @xmath52 , often significantly so .",
    "3 .   for each coding scheme",
    ", we can separately find the optimum bin width @xmath36 .",
    "we will show that the optimized @xmath39 is also more accurate than optimized @xmath52 , often significantly so .",
    "4 .   for a wide range of @xmath25 values ( e.g. , @xmath53 ) ,",
    "the optimum @xmath36 values for our scheme @xmath39 are relatively large ( e.g. , @xmath54 ) , while for the existing scheme @xmath52 , the optimum @xmath36 values are small ( e.g. , about @xmath55 ) .",
    "this means @xmath39 requires a smaller number of bits than @xmath52 .    in summary ,",
    "uniform quantization is simpler , more accurate , and uses fewer bits than the influential prior work  @xcite which uses the window with the random offset .      in section  [ sec_hw ]",
    ", we analyze the collision probability for the uniform quantization scheme and then compare it with the collision probability of the well - known prior work  @xcite which uses an additional random offset . because the collision probabilities are monotone functions of the similarity @xmath25 , we can always estimate @xmath25 from the observed ( empirical ) collision probabilities . in section  [ sec_compare_hwq ]",
    ", we theoretically compare the estimation variances of these two schemes and conclude that the random offset step in  @xcite is not needed . + in section  [ sec_hw2 ] , we develop a 2-bit non - unform coding scheme and demonstrate that its performance largely matches the performance of the uniform quantization scheme ( which requires storing more bits ) .",
    "interestingly , for certain range of the similarity @xmath25 , we observe that only one bit is needed .",
    "thus , section  [ sec_h1 ] is devoted to comparing the 1-bit scheme with our proposed methods .",
    "the comparisons show that the 1-bit scheme does not perform as well when the similarity @xmath25 is high ( which is often the case applications are interested in ) . in section  [ sec_svm ] ,",
    "we provide a set of experiments on training linear svm using all the coding schemes we have studied .",
    "the experimental results basically confirm the variance analysis .",
    "section  [ sec_future ] presents several directions for related future research .",
    "finally , section  [ sec_conclusion ] concludes the paper .",
    "to use our coding scheme @xmath39 ( [ eqn_hw ] ) , we need to evaluate @xmath56 , the collision probability . from practitioners perspective , as long as @xmath57 is a monotonically increasing function of the similarity @xmath25 , it is a suitable coding scheme . in other words",
    ", it does not matter whether @xmath57 has a closed - form expression , as long as we can demonstrate its advantage over the alternative  @xcite , whose collision probability is denoted by @xmath58 .",
    "note that @xmath58 can be expressed in a closed - form in terms of the standard @xmath59 and @xmath60 functions : @xmath61 recall @xmath62 is the euclidean distance @xmath63 .",
    "it is clear that @xmath64 as @xmath65 .",
    "+ the following lemma  [ lem_pst ] will help derive the collision probability @xmath57 ( in theorem  [ thm_pw ] ) .",
    "[ lem_pst ] assume @xmath66 \\sim n \\left ( \\left[\\begin{array}{c}0\\\\ 0\\end{array } \\right],\\ \\left[\\begin{array}{cc}1 & \\rho\\\\ \\rho & 1 \\end{array } \\right ] \\right)$ ] , @xmath11 .",
    "then @xmath67,\\ y\\in[s , t]\\right ) = \\int_{s}^{t}\\phi(z)\\left[\\phi\\left(\\frac{t-\\rho z}{\\sqrt{1-\\rho^2}}\\right)- \\phi\\left(\\frac{s-\\rho z}{\\sqrt{1-\\rho^2}}\\right)\\right]dz\\\\ & \\frac{\\partial q_{s , t}(\\rho)}{\\partial \\rho } = \\frac{1}{2\\pi}\\frac{1}{(1-\\rho^2)^{1/2}}\\left(e^{-\\frac{t^2}{(1+\\rho)}}+e^{-\\frac{s^2}{(1+\\rho ) } } -2e^{-\\frac{t^2+s^2 - 2st\\rho}{2(1-\\rho^2)}}\\right )   \\geq 0\\end{aligned}\\ ] ] * proof : *  see appendix  [ app_lem_pst].@xmath68 +    [ thm_pw ] the collision probability of the coding scheme @xmath39 defined in ( [ eqn_hw ] ) is @xmath69dz\\end{aligned}\\ ] ] which is a monotonically increasing function of @xmath25 .",
    "in particular , when @xmath70 , we have @xmath71 ^ 2\\end{aligned}\\ ] ] * proof : *  the proof follows from lemma  [ lem_pst ] by using @xmath72 and @xmath73 , @xmath74 .",
    "@xmath68 +    figure  [ fig_pwq ] plots both @xmath57 and @xmath58 for selected @xmath25 values .",
    "the difference between @xmath57 and @xmath58 becomes apparent after about @xmath75 .",
    "for example , when @xmath76 , @xmath77 quickly approaches the limit 0.5 while @xmath58 keeps increasing ( to 1 ) as @xmath36 increases . intuitively , the fact that @xmath78 when @xmath76 , is undesirable because it means two orthogonal vectors will have the same coded value .",
    "thus , it is not surprising that our proposed scheme @xmath39 will have better performance than @xmath52 .",
    "we will analyze their theoretical variances to provide precise comparisons .",
    "in both schemes ( corresponding to @xmath39 and @xmath52 ) , the collision probabilities @xmath57 and @xmath58 are monotonically increasing functions of the similarity @xmath25 . since there is a one - to - one mapping between @xmath25 and @xmath57 , we can tabulate @xmath57 for each @xmath25 ( for example , at a precision of @xmath79 ) . from @xmath42 independent projections , we can compute the empirical @xmath80 and @xmath81 and find the estimates , denoted by @xmath82 and @xmath83 , respectively , from the tables . in this section , we compare the estimation variances for these two estimators , to demonstrate advantage of the proposed coding scheme @xmath39 .",
    "theorem  [ thm_vwq ] provides the variance of @xmath51 , for estimating @xmath25 from @xmath42 random projections .",
    "[ thm_vwq ] @xmath84 * proof : *  see appendix  [ app_thm_vwq ] .",
    "@xmath68 +    figure  [ fig_vwq ] plots the variance factor @xmath85 defined in ( [ eqn_vwq ] ) without the @xmath86 term .",
    "( recall @xmath62 . )",
    "the minimum is 7.6797 ( keeping four digits ) , attained at @xmath87 .",
    "the plot also suggests that the performance of this popular scheme can be sensitive to the choice of the bin width @xmath36 .",
    "this is a practical disadvantage . since we do not know @xmath25 ( or @xmath88 ) in advance and we must specify @xmath36 in advance , the performance of this scheme might be unsatisfactory , as one can not really find one `` optimum '' @xmath36 for all pairs in a dataset .    in comparison , our proposed scheme has smaller variance and is not as sensitive to the choice of @xmath36 .",
    "[ thm_vw ] @xmath89 ^ 2}\\end{aligned}\\ ] ] in particular , when @xmath76 , we have @xmath90\\left[\\frac{1/2-\\sum_{i=0}^\\infty \\left(\\phi((i+1)w ) - \\phi(iw)\\right)^2}{\\sum_{i=0}^\\infty \\left(\\phi((i+1)w ) - \\phi(iw)\\right)^2}\\right]\\end{aligned}\\ ] ] * proof : *  see appendix  [ app_thm_vw ] . @xmath68",
    "* remark : *  at @xmath91 , the minimum is @xmath92 attained at @xmath93 , as shown in figure  [ fig_vw0 ] . note that when @xmath65 , we have @xmath94 and @xmath95 , and hence @xmath96\\left[\\frac{1/2 - 1/4}{1/(2\\pi)}\\right ] = \\frac{\\pi^2}{4}$ ] . in comparison ,",
    "theorem  [ thm_vwq ] says that when @xmath76 ( i.e. , @xmath97 ) we have @xmath98 , which is significantly larger than @xmath99 .    , as @xmath65.,width=288 ]    to compare the variances of the two estimators , @xmath100 and @xmath101 , we compare their leading constants , @xmath102 and @xmath85 .",
    "figure  [ fig_vw ] plots the @xmath102 and @xmath85 at selected @xmath25 values , verifying that ( i ) the variance of the proposed scheme ( [ eqn_hw ] ) can be significantly lower than the existing scheme ( [ eqn_hwq ] ) ; and ( ii ) the performance of the proposed scheme is not as sensitive to the choice of @xmath36 ( e.g. , when @xmath75 ) .",
    "it is also informative to compare @xmath103 and @xmath85 at their `` optimum '' @xmath36 values ( for fixed @xmath25 ) .",
    "note that @xmath102 is not sensitive to @xmath36 once @xmath104 .",
    "the left panel of figure  [ fig_vwopt ] plots the best values for @xmath102 and @xmath85 , confirming that @xmath102 is significantly lower than @xmath85 at smaller @xmath25 values ( e.g. , @xmath53 ) .",
    "+    the right panel of figure  [ fig_vwopt ] plots the optimum @xmath36 values ( for fixed @xmath25 ) . around @xmath105 ,",
    "the optimum @xmath36 for @xmath102 becomes significantly larger than 6 and may not be reliably evaluated . from the remark for theorem  [ thm_vw ] , we know that at @xmath76 the optimum @xmath36 grows to @xmath106 .",
    "thus , we can conclude that if @xmath53 , it suffices to implement our coding scheme using just 1 bit ( i.e. , signs of the projected data ) . in comparison , for the existing scheme @xmath52",
    ", the optimum @xmath36 varies much slower .",
    "even at @xmath76 , the optimum @xmath36 is around 2 .",
    "this means @xmath52 will always need to use more bits than @xmath39 , to code the projected data .",
    "this is another advantage of our proposed scheme .",
    "+ in practice , we do not know @xmath25 in advance and we often care about data vector pairs of high similarities . when @xmath107 , figure  [ fig_vw ] and figure  [ fig_vwopt ] illustrate that we might want to choose small @xmath36 values ( e.g. , @xmath108 ) .",
    "however , using a small @xmath36 value will hurt the performance in pairs of the low similarities .",
    "this dilemma motivates us to develop non - uniform coding schemes .",
    "if we quantize the projected data according to the four regions @xmath109 , we obtain a 2-bit non - uniform scheme . at the risk of abusing notation , we name this scheme `` @xmath110 '' , not to be confused with the name of the existing scheme @xmath52 .    according to lemma  [ lem_pst ] , @xmath110 is also a valid coding scheme .",
    "we can theoretically compute the collision probability , denoted by @xmath111 , which is again a monotonically increasing function of the similarity @xmath25 . with @xmath42 projections",
    ", we can estimate @xmath25 from the empirical observation of @xmath111 and we denote this estimator by @xmath112 .",
    "theorem  [ thm_hw2 ] provides the expressions for @xmath111 and @xmath113 .",
    "[ thm_hw2 ] @xmath114 @xmath115 ^ 2}\\end{aligned}\\ ] ] * proof * :  see appendix  [ app_thm_hw2].@xmath68 +    figure  [ fig_pw2 ] plots @xmath111 ( together with @xmath57 ) for selected @xmath25 values .",
    "when @xmath116 , @xmath111 and @xmath77 largely overlap . for small @xmath36 , the two probabilities behave very differently , as expected .",
    "note @xmath111 has the same value at @xmath117 and @xmath118 , and in fact , when @xmath117 or @xmath118 , we just need one bit ( i.e. , the signs ) . note that @xmath111 and @xmath77 differ significantly at small @xmath36 .",
    "will this be beneficial ?",
    "the answer again depends on @xmath25 .",
    "figure  [ fig_vw2 ] plots both @xmath119 and @xmath102 at selected @xmath25 values , to compare their variances . for @xmath120 ,",
    "the variance of the estimator using the 2-bit scheme @xmath110 is significantly lower than that of @xmath39 .",
    "however , when @xmath25 is high , @xmath119 might be somewhat higher than @xmath103 .",
    "this means that , in general , we expect the performance of @xmath110 will be similar to @xmath39 . when applications mainly care about highly similar data pairs , we expect @xmath39 will have ( slightly ) better performance ( at the cost of more bits ) .",
    "finally , figure  [ fig_vw2opt ] presents the smallest @xmath119 values and the optimum @xmath36 values at which the smallest @xmath119 are attained .",
    "this plot verifies that @xmath39 and @xmath110 should perform very similarly , although @xmath51 will have better performance at high @xmath25 .",
    "also , for a wide range , e.g. , @xmath121 $ ] , it is preferable to implement @xmath110 using just 1 bit because the optimum @xmath36 values are large .",
    "when @xmath123 , it is sufficient to implement @xmath39 or @xmath110 using just one bit , because the normal probability density decays very rapidly : @xmath31 . note that we need to consider a very small tail probability because there are many data pairs in a large dataset , not just one pair . with the 1-bit scheme , we simply code the projected data by recording their signs .",
    "we denote this scheme by @xmath122 , and the corresponding collision probability by @xmath124 , and the corresponding estimator by @xmath125 .",
    "+ from theorem  [ thm_hw2 ] , by setting @xmath117 ( or equivalently @xmath118 ) , we can directly infer @xmath126 @xmath127    this collision probability is widely known  @xcite .",
    "the work of  @xcite also popularized the use 1-bit coding .",
    "the variance was analyzed and compared with a maximum likelihood estimator in  @xcite .",
    "+ figure  [ fig_varratiov1vwvw2 ] and figure  [ fig_varratiowv1vwvw2 ] plot the ratios of the variances : @xmath128 and @xmath129 , to illustrate how much we lose in accuracy by using only one bit .",
    "note @xmath130 is not related to the bin width @xmath36 while @xmath100 and @xmath113 are functions of @xmath36 . in figure",
    "[ fig_varratiov1vwvw2 ] , we plot the maximum values of the ratios , i.e. , we use the smallest @xmath100 and @xmath113 at each @xmath25 .",
    "the ratios demonstrate that potentially both @xmath51 and @xmath110 could substantially outperform @xmath122 , the 1-bit scheme .",
    "note that in figure  [ fig_varratiov1vwvw2 ] , we plot @xmath131 in the horizontal axis with log - scale , so that the high similarity region can be visualized better . in practice , many applications are often more interested in the high similarity region , for example , duplicate detections .    in practice",
    ", we must pre - specify the quantization bin width @xmath36 in advance . thus",
    ", the improvement of @xmath39 and @xmath110 over the 1-bit scheme @xmath122 will not be as drastic as shown in figure  [ fig_varratiov1vwvw2 ] . for more realistic comparisons ,",
    "figure  [ fig_varratiowv1vwvw2 ] plots @xmath128 and @xmath129 , for fixed @xmath36 values .",
    "this figure advocates the recommendation of the 2-bit coding scheme @xmath110 :",
    "1 .   in the high similarity region",
    ", @xmath110 significantly outperforms @xmath122 .",
    "the improvement drops as @xmath36 becomes larger ( e.g. , @xmath116 ) .",
    "@xmath51 also works well , in fact better than @xmath110 when @xmath36 is small .",
    "2 .   in the low similarity region , @xmath110 still outperforms @xmath122 unless @xmath25 is very low and @xmath36 is not small .",
    "note that the performance of @xmath51 is noticeably worse than @xmath110 and @xmath122 when @xmath25 is low .",
    "+    thus , we believe the 2-bit scheme @xmath110 with @xmath36 around 0.75 provides an overall good compromise .",
    "in fact , this is consistent with our observation in the svm experiments in section  [ sec_svm ] .",
    "+    * can we simply use the 1-bit scheme ? * when @xmath132 , in the high similarity region , the variance ratio @xmath129 is between 2 and 3 . note that , per projected data value , the 1-bit scheme requires 1 bit but the 2-bit scheme needs 2 bits . in a sense ,",
    "the performance of @xmath110 and @xmath122 is actually similar in terms of the total number bits to store the ( coded ) projected data , according the analysis in this paper .    for similarity estimation ,",
    "we believe it is preferable to use the 2-bit scheme , for the following reasons :    * the processing cost of the 2-bit scheme would be lower .",
    "if we use @xmath42 projections for the 1-bit scheme and @xmath133 projections for the 2-bit scheme , although they have the same storage cost , the processing cost of @xmath110 for generating the projections would be only 1/2 of @xmath134 .",
    "for very high - dimensional data , the processing cost can be substantial . *",
    "as we will show in section  [ sec_svm ] , when we train a linear classifier ( e.g. , using liblinear ) , we need to expand the projected data into a binary vector with exact @xmath42 1 s if we use @xmath42 projections for both @xmath122 and @xmath110 . for this application",
    ", we observe the training time is mainly determined by the number of nonzero entries and the quality of the input data . even with the same @xmath42 , we observe the training speed on the input data generated by @xmath110 is often slightly faster than using the data generated by @xmath122 . * in this study , we restrict our attention to linear estimators ( which can be written as inner products ) by simply using the ( overall ) collision probability , e.g. , @xmath135 .",
    "there is significant room for improvement by using more refined estimators .",
    "for example , we can treat this problem as a contingency table whose cell probabilities are functions of the similarity @xmath25 and hence we can estimate @xmath25 by solving a maximum likelihood equation .",
    "such an estimator is still useful for many applications ( e.g. , nonlinear kernel svm ) .",
    "we will report that work separately , to maintain the simplicity of this paper .",
    "note that quantization is a non - reversible process .",
    "once we quantize the data by the 1-bit scheme , there is no hope of recovering any information other than the signs .",
    "our work provides the necessary theoretical justifications for making practical choices of the coding schemes .",
    "we conduct experiments with random projections for training ( @xmath136-regularized ) linear svm ( e.g. , liblinear  @xcite ) on three high - dimensional datasets : _ arcene , farm , url _ , which are available from the uci repository .",
    "the original _ url _ dataset has about 2.4 million examples ( collected in 120 days ) in 3231961 dimensions .",
    "we only used the data from the first day , with 10000 examples for training and 10000 for testing .",
    "the _ farm _",
    "dataset has 2059 training and 2084 testing examples in 54877 dimensions .",
    "the _ arcene _",
    "dataset contains 100 training and 100 testing examples in 10000 dimensions .",
    "+ we implement the four coding schemes studied in this paper : @xmath52 , @xmath39 , @xmath110 , and @xmath134 .",
    "recall @xmath52  @xcite was based on uniform quantization plus a random offset , with bin width @xmath36 . here",
    ", we first illustrate exactly how we utilize the coded data for training linear svm .",
    "suppose we use @xmath110 and @xmath132 .",
    "we can code an original projected value @xmath137 into a vector of length 4 ( i.e. , 2-bit ) : @xmath138 , \\hspace{0.5 in } x\\in[-0.75\\ 0 ) \\rightarrow [ 0\\ 1\\ 0\\ 0 ] , \\\\\\notag & x\\in[0\\ 0.75 ) \\rightarrow [ 0\\ 0\\ 1\\ 0 ] , \\hspace{0.9 in } x\\in[0.75\\ \\infty ) \\rightarrow [ 0\\ 0\\ 0\\ 1 ] \\end{aligned}\\ ] ] this way , with @xmath42 projections , for each feature vector , we obtain a new vector of length @xmath139 with exactly @xmath42 1 s . this new vector",
    "is then fed to a solver such as liblinear .",
    "recently , this strategy was adopted for linear learning with binary data based on _ b - bit minwise hashing _  @xcite .    similarly ,",
    "when using @xmath122 , the dimension of the new vector is @xmath140 with exactly @xmath42 1 s . for @xmath39 and @xmath52",
    ", we must specify a cutoff value such as 6 otherwise they are `` infinite precision '' schemes . practically speaking , because the normal density decays very rapidly at the tail ( e.g. , @xmath141 ) , we essentially do not suffer from information loss if we choose a large enough cutoff such as 6 .",
    "+ figure  [ fig_urlsvmaccq ] reports the test accuracies on the _ url _ data , for comparing @xmath52 with @xmath39 .",
    "the results basically confirm our analysis of the estimation variances . for small bin width @xmath36",
    ", the two schemes perform very similarly .",
    "however , when using a relatively large @xmath36 , the scheme @xmath52 suffers from noticeable reduction of classification accuracies .",
    "the experimental results on the other two datasets demonstrate the same phenomenon .",
    "this experiment confirms that the step of random offset in @xmath52 is not needed , at least for similarity estimation and training linear classifiers .",
    "there is one tuning parameter @xmath142 in linear svm .",
    "figure  [ fig_urlsvmaccq ] reports the accuracies for a wide range of @xmath142 values , from @xmath79 to @xmath143 .",
    "before we feed the data to liblinear , we always normalize them to have unit norm ( which is a recommended practice ) .",
    "our experience is that , with normalized input data , the best accuracies are often attained around @xmath144 , as verified in figure  [ fig_urlsvmaccq ] . for other figures in this section ,",
    "we will only report @xmath142 from @xmath79 to 10 .",
    "figure  [ fig_urlsvmacc ] reports the test classification accuracies ( averaged over 20 repetitions ) for the _ url _ dataset . when @xmath145 , both @xmath39 and @xmath110 produce similar results as using the original projected data .",
    "the 1-bit scheme @xmath122 is obviously less competitive .",
    "we provide similar plots ( figure  [ fig_farmsvmacc ] ) for the _ farm _ dataset .",
    "we summarize the experiments in figure  [ fig_svmmaxacc ] for all three datasets .",
    "the upper panels report , for each @xmath42 , the best ( highest ) test classification accuracies among all @xmath142 values and @xmath36 values ( for @xmath110 and @xmath51 ) .",
    "the results show a clear trend : ( i ) the 1-bit ( @xmath122 ) scheme produces noticeably lower accuracies compared to others ; ( ii ) the performances of @xmath110 and @xmath39 are quite similar .",
    "the bottom panels of figure  [ fig_svmmaxacc ] report the @xmath36 values at which the best accuracies were attained .",
    "for @xmath110 , the optimum @xmath36 values are often close to 1 .",
    "one interesting observation is that for the _ farm _ dataset , using the coded data ( by @xmath39 or @xmath110 ) can actually produce better accuracy than using the original ( uncoded ) data , when @xmath42 is not large .",
    "this phenomenon may not be too surprising because quantization may be also viewed as some form of regularization and in some cases may help boost the performance .",
    "this paper only studies linear estimators , which can be written as inner products .",
    "linear estimators are extremely useful because they allow highly efficient implementation of linear classifiers ( e.g. , linear svm ) and near neighbor search methods using hash tables . for applications that allow",
    "* nonlinear estimators * ( e.g. , nonlinear kernel svm ) , we can substantially improve linear estimators by solving nonlinear mle ( maximum likelihood ) equations .",
    "the analysis will be reported separately .",
    "our work is , to an extent , inspired by the recent work on @xmath146-bit minwise hashing  @xcite , which also proposed a coding scheme for minwise hashing and applied it to learning applications where the data are binary and sparse .",
    "our work is for general data types , as opposed to binary , sparse data .",
    "we expect coding methods will also prove valuable for other variations of random projections , including the count - min sketch @xcite and related variants  @xcite and very sparse random projections  @xcite . another potentially interesting future direction is to develop refined coding schemes for improving _ sign stable projections _  @xcite ( which are useful for @xmath147 similarity estimation , a popular similarity measure in computer vision and nlp ) .",
    "the method of random projections has become a standard algorithmic approach for computing distances or correlations in massive , high - dimensional datasets .",
    "a compact representation ( coding ) of the projected data is crucial for efficient transmission , retrieval , and energy consumption .",
    "we have compared a simple scheme based on uniform quantization with the influential coding scheme using windows with a random offset  @xcite ; our scheme appears operationally simpler , more accurate , not as sensitive to parameters ( e.g. , the widow / bin width @xmath36 ) , and uses fewer bits .",
    "we furthermore develop a 2-bit non - uniform coding scheme which performs similarly to uniform quantization .",
    "our experiments with linear svm on several real - world high - dimensional datasets confirm the efficacy of the two proposed coding schemes .",
    "based on the theoretical analysis and empirical evidence , we recommend the use of the 2-bit non - uniform coding scheme with the first bin width @xmath148 , especially when the target similarity level is high .",
    "the joint density function of @xmath149 is @xmath150 . in this paper",
    "we focus on @xmath11 .",
    "we use the usual notation for standard normal pdf and cdf : @xmath151 , @xmath152 .",
    "the probability @xmath153 can be simplified to be @xmath154dx\\end{aligned}\\ ] ] next we evaluate its derivative @xmath155 .",
    "@xmath156 note that @xmath157 and @xmath158   \\end{aligned}\\ ] ] and @xmath159   \\end{aligned}\\ ] ]    combining the results , we obtain @xmath160 -\\frac{1}{2\\pi}\\frac{1}{(1-\\rho^2)^{1/2}}e^{-s^2/2}\\left [ -e^{-\\frac{s^2(1-\\rho)}{2(1+\\rho)}}+e^{-\\frac{\\left(t - s\\rho\\right)^2}{2(1-\\rho^2 ) } } \\right]\\\\\\notag = & \\frac{1}{2\\pi}\\frac{1}{(1-\\rho^2)^{1/2}}\\left(e^{-\\frac{t^2}{(1+\\rho ) } } -e^{-\\frac{t^2+s^2 - 2st\\rho}{2(1-\\rho^2 ) } } + e^{-\\frac{s^2}{(1+\\rho ) } } -e^{-\\frac{t^2+s^2 - 2st\\rho}{2(1-\\rho^2 ) } } \\right)\\\\\\notag = & \\frac{1}{2\\pi}\\frac{1}{(1-\\rho^2)^{1/2}}\\left(e^{-\\frac{t^2}{(1+\\rho)}}+e^{-\\frac{s^2}{(1+\\rho ) } } -2e^{-\\frac{t^2+s^2 - 2st\\rho}{2(1-\\rho^2)}}\\right)\\\\\\notag = & \\frac{1}{2\\pi}\\frac{1}{(1-\\rho^2)^{1/2}}\\left[\\left(e^{-\\frac{t^2}{2(1+\\rho)}}-e^{-\\frac{s^2}{2(1+\\rho ) } } \\right)^2 + 2e^{-\\frac{t^2+s^2}{2(1+\\rho)}}-2e^{-\\frac{t^2+s^2 - 2st\\rho}{2(1-\\rho^2)}}\\right]\\\\\\notag   \\geq & 0\\end{aligned}\\ ] ] the last inequality holds because @xmath161-\\left[-\\frac{t^2+s^2 - 2st\\rho}{2(1-\\rho^2)}\\right]\\\\\\notag = & \\frac{1}{2(1-\\rho^2)}\\left[-(t^2+s^2)(1-\\rho ) + t^2+s^2 - 2st\\rho\\right]\\\\\\notag = & \\frac{\\rho}{2(1-\\rho^2)}\\left[s - t\\right]^2\\\\\\notag \\geq & 0\\end{aligned}\\ ] ]    this completes the proof .",
    "from the collision probability , @xmath162 , we can estimate @xmath88 ( and @xmath25 ) .",
    "recall @xmath62 .",
    "we denote the estimator by @xmath163 ( and @xmath83 ) , from the empirical probability @xmath81 , which is estimated without bias from @xmath42 projections .",
    "note that @xmath164 for a nonlinear function @xmath165 . as @xmath166 ,",
    "the estimator @xmath163 is asymptotically unbiased .",
    "the variance can be determined by the `` delta '' method : @xmath167 ^ 2 + o\\left(\\frac{1}{k^2}\\right )   = \\frac{1}{k}p_{w , q}\\left(1-{p}_{w , q}\\right ) \\left[g^\\prime\\left(p_{w , q}\\right)\\right]^2 + o\\left(\\frac{1}{k^2}\\right)\\end{aligned}\\ ] ] since @xmath168 we have @xmath169 and @xmath170 because @xmath171 , we know that @xmath172 where @xmath173 this completes the proof .",
    "this proof is similar to the proof of theorem  [ thm_vwq ] . to evaluate the asymptotic variance",
    ", we need to compute @xmath174 : @xmath175 thus , @xmath176 ^ 2}\\end{aligned}\\ ] ] next , we consider the special case with @xmath177 . @xmath178",
    "@xmath179 combining the results , we obtain @xmath180",
    "@xmath181dx   + 2\\int_{w}^{\\infty}\\phi(x)\\left[1- \\phi\\left(\\frac{w-\\rho x}{\\sqrt{1-\\rho^2}}\\right)\\right]dx\\\\\\notag   = & 2\\int_{0}^{w}\\phi(x)\\left[\\phi\\left(\\frac{w-\\rho x}{\\sqrt{1-\\rho^2}}\\right)- 1+\\phi\\left(\\frac{\\rho x}{\\sqrt{1-\\rho^2}}\\right)\\right]dx   + 2\\int_{w}^{\\infty}\\phi(x)\\left[1- \\phi\\left(\\frac{w-\\rho x}{\\sqrt{1-\\rho^2}}\\right)\\right]dx\\\\\\notag    = & 4\\int_{0}^{w}\\phi(x)\\left[\\phi\\left(\\frac{w-\\rho x}{\\sqrt{1-\\rho^2}}\\right)- 1\\right]dx+2\\int_{0}^{w}\\phi(x)\\phi\\left(\\frac{\\rho x}{\\sqrt{1-\\rho^2}}\\right)dx   + 2\\int_{0}^{\\infty}\\phi(x)\\left[1- \\phi\\left(\\frac{w-\\rho x}{\\sqrt{1-\\rho^2}}\\right)\\right]dx\\\\\\notag    = & -4\\int_{0}^{w}\\phi(x)\\phi\\left(\\frac{-w+\\rho x}{\\sqrt{1-\\rho^2}}\\right)dx+2\\int_{0}^{w}\\phi(x)\\phi\\left(\\frac{\\rho x}{\\sqrt{1-\\rho^2}}\\right)dx   + 2\\int_{0}^{\\infty}\\phi(x)\\left[1- \\phi\\left(\\frac{w-\\rho x}{\\sqrt{1-\\rho^2}}\\right)\\right]dx\\\\\\notag   = & 1 - \\frac{1}{\\pi}\\cos^{-1}\\rho - 4\\int_{0}^{w}\\phi(x)\\phi\\left(\\frac{-w+\\rho x}{\\sqrt{1-\\rho^2}}\\right)dx\\end{aligned}\\ ] ]    we need to show @xmath182dx = \\frac{1}{2}-\\frac{1}{2\\pi}\\cos^{-1}\\rho\\end{aligned}\\ ] ] because @xmath183 we know @xmath184 also , @xmath185 thus , combining the results , we obtain @xmath186 where @xmath187 ^ 2}\\end{aligned}\\ ] ] this completes the proof ."
  ],
  "abstract_text": [
    "<S> the method of random projections has become very popular for large - scale applications in statistical learning , information retrieval , bio - informatics and other applications . using a well - designed * coding * scheme for the projected data , which determines the number of bits needed for each projected value and how to allocate these bits , can significantly improve the effectiveness of the algorithm , in storage cost as well as computational speed . in this paper </S>",
    "<S> , we study a number of simple coding schemes , focusing on the task of similarity estimation and on an application to training linear classifiers . </S>",
    "<S> we demonstrate that * uniform quantization * outperforms the standard existing influential method  @xcite . </S>",
    "<S> indeed , we argue that in many cases coding with just a small number of bits suffices . </S>",
    "<S> furthermore , we also develop a * non - uniform 2-bit * coding scheme that generally performs well in practice , as confirmed by our experiments on training linear support vector machines ( svm ) . </S>"
  ]
}