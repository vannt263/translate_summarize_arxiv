{
  "article_text": [
    "models have become widely adopted as means to implement many machine learning algorithms and represent the state - of - the - art for many image and speech recognition applications @xcite . as the application space for evolves beyond workstations and data centres towards low - power mobile and embedded platforms , so too must their design methodologies .",
    "mobile voice recognition systems , such as apple s _ siri _ , currently remain too computationally demanding to execute locally on a handset . instead",
    ", such applications are processed remotely and , depending on network conditions , are subject to variations in performance and delay @xcite .",
    "are also finding application in other emerging areas , such as autonomous vehicle localization and control , where meeting power and cost requirements is paramount @xcite .    , which replace manually engineered computer algorithms ,",
    "must be trained instead of programmed .",
    "this training involves an optimization process where network weights are adjusted with the objective of minimizing the output error .",
    "these adjustments often involve a variation of the method @xcite .",
    "while these training methods have been automated , much of the design process and choice of network hyper - parameters ( e.g. , number of hidden layers , nodes per layer , or choice of activation functions ) has been historically relegated to manual optimization .",
    "this relies on human intuition and expert knowledge of the target application in conjunction with extensive trial and error @xcite .",
    "this process is difficult , considering the vast network hyper - parameter space which includes : the number of convolutional or hidden layers , the quantity of nodes in each layer , the type of nonlinear activation functions , and many others which depend on the system in - hand .",
    "in addition , the problem with manual hyper - parameter tuning is that there is no guarantee that the process will result in optimal configurations .",
    "not only does the diversity of possible hyper - parameters create extremely large design spaces , but time intensive training phases on comprehensive data sets must also be performed prior to evaluating candidate solutions .",
    "this significant computational overhead renders exhaustive searches intractable , and necessitates the use of automated tools to intelligently explore the solution space while limiting the number of candidate models that must be trained and evaluated .    with the proliferation of machine learning on embedded and mobile devices",
    ", application designers must now deal with stringent power and cost requirements @xcite .",
    "these added constraints transform hyper - parameter design into a multi - objective optimization problem where no single optimal solution exists . instead , the set of points which are not dominated by any other solution forms a pareto - optimal front .",
    "simply put , this set includes all solutions for which no other is objectively superior in all criteria .",
    "formally , a solution vector @xmath1 ( where @xmath2 and @xmath3 are two objectives for optimization ) is said to dominate another point @xmath4 if @xmath5 and @xmath6 , or @xmath7 and @xmath8 ; the set of points which are not dominated by any other solution constitutes the pareto - optimal front @xcite .",
    "this paper presents an automated method that effectively trains a neural network to design other neural networks , optimizing for both performance and implementation cost .",
    "this meta - heuristic exploits machine learning to predict the performance of candidate solutions ( modelling the response surface ) ; the algorithm learns which points to explore and avoids the lengthy computations involved in evaluating solutions which are predicted to be unfit .",
    "this leveraging of techniques dramatically reduces the proposed algorithm run - time , which can ultimately result in the reduction of product design time , application time - to - market , and overall costs .      while there are many different models , the is a well - known form , which rose in popularity with the advent of the back - propagation training algorithm @xcite",
    ". characterized by a series of cascaded _ hidden _",
    "layers , have a single feed - forward path from the input to output layers .",
    "layers are also fully - connected ; each node has a connection to each and every node in adjacent layers .",
    "when illustrated as a directed graph ( shown in figure  [ fig : mlp ] ) , graph connections are representative of signals being multiplied by corresponding weights , and graph nodes the summation of all inputs followed by non - linear activation functions .",
    "the evaluation of such elements , commonly or sigmoid functions , is comparatively simple .",
    "therefore , multiply - accumulate operations and memory accesses remain the dominant tasks in terms of cost @xcite .    while structurally simple , determining an optimal configuration is a difficult task due to the large number of parameters inherent to even the simplest designs . for example , given the simple shown in figure  [ fig : mlp1 ] , with @xmath9 inputs , a single hidden layer with @xmath10 nodes , and @xmath11 nodes in the output layer , @xmath12 operations must be evaluated at each inference pass and an equal number of weights must be accessed in memory .",
    "starting from this initial configuration , a designer may then choose to include a second hidden layer , as illustrated in figure  [ fig : mlp2 ] ; the first will then act as a feature extractor and the newly added layer will then process only the limited number of features generated by the first .",
    "this alteration allows for the reduction in dimension of the first hidden layer , which reduces the total number of connections to the input layer .",
    "however , this also increases the network depth and results in a cost penalty ( in terms of requiring additional memory accesses and arithmetic operations associated with the newly added layer ) .",
    "the key problem demonstrated is that even for these two simple configurations , there is no systematic way to determine which design yields superior performance without having trained and evaluated both .",
    "even when the designer has _ a priori _ knowledge of the application , determining the optimal hyper - parameters is non - intuitive , especially for deep networks .",
    "a concrete example of the described problem would be the design of an embedded system to recognize handwritten numerical characters ( such as those contained in the dataset ) .",
    "if the implementation goal is throughput of categorized digits , and a penalty is incurred when a character is misclassified ( perhaps requiring manual intervention ) , then a smaller network that requires fewer clock cycles to evaluate may still result in an overall throughput exceeding those of more accurate alternatives .",
    "in such a scenario , the engineer would require knowledge of the cost and performance of all pareto - optimal solutions in order to meet all requirements with the lowest implementation costs .",
    "even more complex structures are those of , which have demonstrated state - of - the - art results in image recognition @xcite .",
    "the use of convolutional layers further complicates the design process as they introduce a separation between the memory and processing requirements .",
    "illustrated in figure  [ fig : cnn_graph ] , convolutional layers are composed of trainable filters ( five _ 3-by-3 _ kernels are shown ) .",
    "have the advantage of reduced memory requirements due to each convolutional filter reusing the same kernel weights for all input values .",
    "however , this is at the expense of increased processing demands , the result of each convolutional filter requiring @xmath13 multiplication operations for each input value ( where the convolutional kernels are sized _ n - by - n _ ) . in a , each filter produces a processed copy of the input data ( as illustrated in figure  [ fig : cnn_graph ] ) ; and without any form of down - sampling , this greatly increases the computational complexity of the following layers .",
    "an example of a possible efficient down - sampling method is the inclusion of max - pooling ( or mean - pooling ) after convolutional layers @xcite .",
    "illustrated in figure  [ fig : cnn_graph ] , max - pooling refers to partitioning the filtered images into non - overlapping _ k - by - k _ regions , with each outputting the maximum pixel value within ( alternatively , mean - pooling would output the mean value ) .",
    "all of these additional parameters further increase the design space dimensionality , forcing a designer to not only choose how many filters to use in each layer , but also the kernel and pooling sizes .",
    "this greatly affects both the performance and computational complexity of the resulting networks .",
    "the key contributions of this work are presented as follows :    * we introduce a method , which employs techniques to predict classification accuracy , to automate the design of hyper - parameters .",
    "this method is then validated with and designs targeting the -10 and image recognition datasets @xcite .",
    "* we demonstrate that multi - objective hyper - parameter optimization can successfully be used as a method to reduce implementation cost ( computational complexity ) .",
    "this work exists at the intersection of two fields : automated hyper - parameter optimization , and reduction of computational complexity . to our knowledge",
    ", this work is the first that has applied automated hyper - parameter optimization as a method of reducing computational complexity .",
    "the design of neural network hyper - parameters has long been considered to be unwieldy , unintuitive , and as a consequence , ideal for automated hyper - parameter optimization techniques such as in @xcite , @xcite , @xcite , and @xcite .",
    "however , these works have been developed with the sole purpose of optimizing performance , with little regard to the resulting computational resource requirements .",
    "two types of sequential hyper - parameter optimization algorithms were presented in @xcite ; in both cases experimental results compared positively to human designed alternatives .",
    "these findings were echoed in @xcite , where it was demonstrated that a random search of a large design space , which contains areas that may be considered less promising , can at times exceed the performance of manually tuned hyper - parameters .",
    "positive results were also presented in @xcite where similar algorithms were extended using a method to extrapolate the shape of learning curves during training , so as to evaluate fewer epochs for unfit solutions and reduce design time . since parameters which perform favourably for",
    "a large network may not be optimal for a smaller alternative , and the most important hyper - parameters ( with the greatest impact on resulting performance ) may vary for different data sets , the need for multi - objective optimization ( especially where low power platforms are targeted ) is clear @xcite .",
    "additionally , in @xcite an automated method was applied to the multi - objective optimization problem of application - specific design , a field which also consists of high - dimensionality solution spaces .",
    "it was demonstrated that through the presented method could efficiently identify pareto - optimal architectures .",
    "there also exists a body of work attempting to reduce the computational complexity of models through weight quantization or the removal of extraneous node connections ( pruning ) .",
    "the research in @xcite and @xcite are examples of two methods that construct networks which reduce the need for multiplications , or modify already trained networks in order to minimize the number of non - zero weights .    in @xcite , the authors attempted to reduce the computational resources required when evaluating fully - connected , as well as convolutional , networks through representing all weights as binary values of @xmath14 or @xmath15 .",
    "doing so reduces the number of multiplication operations performed each forward pass ; however the requirement to store floating - point weights during training remains . the work in @xcite also compared trained binary weighted networks to traditional equivalents with equal layer dimensions and similar performance was demonstrated .",
    "however , @xcite only considered very large network dimensions ; further benefits may be obtained from smaller optimized architectures .    instead of restricting weights to specific values , in @xcite a pruning method",
    "was presented in which a network can be trained while reducing the number of non - zero weights .",
    "the resulting compressed networks have lower bandwidth requirements and require fewer multiplications due to most weights being zero .",
    "the results in @xcite demonstrated up to 70% reductions in the numbers of floating - point operations required for various networks , with little to no reduction in performance .",
    "however , such pruning methods are not mutually exclusive to the use of tools and could very well be implemented in conjunction with the presented methodology in order to compress an already optimized network configuration .",
    "this work presents a approach that searches for pareto - optimal hyper - parameter configurations and has been applied to both and topologies .",
    "the design space is confined to : the numbers of and convolutional layers , the number of nodes or filters in each layer , the convolution kernel sizes , the max - pooling sizes , the type of activation function , and network training rate .",
    "these degrees of freedom constitute vast design spaces and all strongly influence the resulting networks cost and performance",
    ".    for design spaces of such size , performing an exhaustive search is intractable ( designs with over @xmath16 to @xmath0 possible solutions are not uncommon ) , therefore we propose to model the response surface using an for regression where the set of explored solution points is used as a training set .",
    "the presented meta - heuristic is then used to predict the performance of candidate networks ; only points which are expected not to be pareto - dominated are explored .",
    "a flowchart describing the implementation is shown in figure  [ alg : dse ] .",
    "the general steps performed during the design space exploration can be broken down into :    1 .",
    "sample the next candidate point from a gaussian distribution centred around the previously explored solution ( or sample a random point for the first iteration ) .",
    "2 .   predict the candidate solution performance using the neural network , and calculate the cost as : + @xmath17 3 .   compare the predicted results to the current pareto - optimal front .",
    "if the candidate is predicted to be pareto - dominated , it is accepted with probability @xmath18 , otherwise it is accepted with probability @xmath19 .",
    "if rejected , the previously explored solution is rolled back and the algorithm returns to step 1 .",
    "if accepted , the candidate model is trained , tested , and the evaluated results are added to the training set of the ( which is then retrained ) .",
    "finally , if the training set size exceeds the maximum number of desired iterations , the process ends .",
    "otherwise , the algorithm returns to step 1 and a new solution is sampled .",
    "the sampling strategy proposed is an adaptation of the metropolis - hastings algorithm . in each iteration",
    "a new candidate is sampled from a gaussian distribution centred around the previously explored solution point . performing this",
    "random walk limits the number of samples chosen from areas of the design space that are known to contain unfit solutions , thereby reducing wasted exploration effort .",
    "however , exploring an inferior solution may eventually lead to those of superior performance , therefore the probability of accepting such a solution ( @xmath18 ) must remain greater than zero ; this also ensures that the training set for the remains varied .",
    "all experimental results in section  [ sec : experimental_results ] were obtained with @xmath20 .",
    "we choose to model the response surface using a model with an input set representative of network hyper - parameters and a single output trained to predict the error of corresponding networks .",
    "this is composed of two hidden layers and a linear output layer .",
    "experimental results demonstrated that sizing the hidden layers with @xmath21 to @xmath22 the number of input nodes provided the best performance .",
    "the network inputs are formed as arrays characterizing all explored dimensions .",
    "integer input parameters ( such as number of nodes in a hidden layer , or size of the convolutional kernels ) are scaled by the maximum possible value of the respective parameter , resulting in normalized variables between 0 and 1 . for each parameter that represents a choice where the options have no numerical relation to each other ( such as whether or sigmoid functions are used ) an input mode is added and the node that represents the chosen option is given an input value of 1 , all others @xmath15 .",
    "for example , a solution with two hidden layers with 20 nodes each ( assuming a maximum of 100 ) , using ( with the other option being sigmoid functions ) and with a learning rate of 0.5 would be presented as input values : @xmath23 $ ] .",
    "the model was trained using , where 100 training epochs were performed on the set of explored solutions each time the next is evaluated ( and in turn , added to the training set ) .",
    "the learning rate was kept constant , with a value of 0.1 , in order to train the network quickly during early exploration , when the set of evaluated solutions is limited .",
    "we evaluated our strategy on applications targeting the standard and -10 image recognition datasets . in the case of designing models targeting the simpler problem ,",
    "the results generated by the algorithm were compared to the true pareto - optimal front obtained from an exhaustive search performed on a constrained solution space .",
    "such a limitation of the design space was required in order to render the exhaustive search tractable .",
    "the algorithm was also evaluated on much larger design spaces for both and models targeting as well as an even larger one for the design of targeting -10 . in all cases only a few iterations were required in order to converge to approximated pareto - optimal fronts .    in order to perform the ,",
    "all models were trained and tested using the _ theano _ framework in order to take advantage of optimizations @xcite .",
    "this allowed the entire and evaluation to be performed using a nvidia _",
    "tesla k20c _ . upon completion ,",
    "all explored network data and simulation results were stored to disk , allowing future design re - use .",
    "to model cost , we assumed normalized memory access and multiply - accumulate operation costs .",
    "these can be quantified at either the software or hardware levels ( depending on the target application ) . in both cases",
    "the costs ( such as power , area , or time ) are implementation specific and the exact values used are transparent to the algorithm .",
    "normalized costs assumed for all the experimental results are shown in table  [ table : costs ] , which are based on the energy costs for 32-bit floating - point operations from @xcite .",
    "m0.275 < m0.275 < m0.275 < operation & energy cost from @xcite & normalized cost + addition & @xmath24 & n / a + multiplication & @xmath25 & n / a + multiply - accumulate & @xmath26 & @xmath27 + dram memory access & @xmath28 & @xmath29 +    [ table : costs ]    we experimentally validated the algorithm on two separate image recognition databases ; future work is planned to address a broader range of benchmarks .",
    "the first , , contains @xmath30 pixel grayscale images of handwritten numeric characters ( from 0 to 9 ) .",
    "the second , -10 , is a much more complicated dataset composed of @xmath31 rgb colour images , each belonging to one of ten object categories .",
    "the network hyper - parameters composing the design spaces explored are outlined in table  [ table : dse_parameters ] .",
    "in addition , several parameters were kept constant for all experiments : output layers were composed of _ softmax _ units ( with _ jth _ output defined as @xmath32 for a layer with @xmath33 nodes ) and all network training was performed with categorical cross - entropy @xmath34\\right)$ ] as loss function @xcite .",
    "finally , for all except the reduced design problem , batch normalization was included after each network layer in order to smooth the response surfaces @xcite .",
    "[ table : dse_parameters ]      in order to evaluate the efficiency with which the method approximates the true pareto - optimal front , we first compare experimental results to those of an exhaustive search targeting the design of models for the dataset . in order to make an exhaustive search tractable",
    ", we limited the design space to only the values outlined in the first section of table  [ table : dse_parameters ] .",
    "this resulted in a moderate design space of @xmath35 solutions , all of which were trained and tested .",
    "the results of executing the algorithm for 200 iterations ( each iteration represents a design training , evaluation , and model update pass ) are plotted alongside those of the exhaustive search in figure  [ fig : mnist_exhaustive ] .",
    "these results demonstrate that the true pareto - optimal front is very closely approximated by the presented method , while requiring very few solutions to be evaluated .",
    "however , it should be noted that is by nature non - deterministic and training the same network twice may yield different performances .",
    "consequently , the generated results ( shown in figure  [ fig : mnist_exhaustive ] ) dominate those of the exhaustive search at several points .",
    "this figure also demonstrates that the majority of solutions evaluated by the algorithm remain within a close vicinity of the true pareto - front , successfully avoiding pareto - dominated areas .",
    "since the true pareto - optimal front is known for this restricted case , we use as the metric of evaluation ; quantifying how closely the approximated set differs from the exact @xcite .",
    "is defined in eq .  , where @xmath36 and @xmath37 are the approximate and exact pareto - optimal fronts , and the @xmath38 function represents the normalized distance between solutions @xmath39 and @xmath40 .",
    "@xmath41    the evolution of the approximated pareto - optimal set discovered by the algorithm , as a function of iterations completed is plotted in figure  [ fig : mnist_mlp_reduced_results ] .",
    "evident from this figure is that the optimal front obtained from the algorithm progressively approaches the true pareto - optimal , while evaluating only a comparatively small number of iterations .",
    "the proposed method identifies the optimal solutions with high accuracy , achieving low values of 7.1% after completing only 50 iterations , 5.0% after 100 , and 3.6% after 200 .",
    "the results in table  [ table : adrs ] also demonstrate fast convergence , with the largest changes occurring over the first 30 iterations .",
    "further execution yields more gradual changes as the approximated pareto - front asymptotically approaches the exact . decreasing design time ( both in terms of computation and man - hours )",
    ", these results also demonstrate that by exploring less than 1% of the design space , the method closely predict which hyper - parameters are required for optimal solutions . in addition , the generated set of pareto - optimal configurations also exposes the trade - offs application designers may want to make for cost - constrained systems , allowing for more informed design choices to be made .",
    "< m0.225 < m0.2 < iterations & of explored set & execution time + 10 & 30% & 2.0 min + 20 & 21% & 4.1 min + 30 & 8.4% & 6.4 min + 50 & 7.1% & 11.2 min + 70 & 6.1% & 16.4 min + 100 & 5.0% & 25.4 min + 150 & 4.5% & 45.2 min + 200 & 3.6% & 70.1 min +    [ table : adrs ]      in order to evaluate performance of the heuristic method for much larger designs , the algorithm was run on the remaining spaces described in table  [ table : dse_parameters ] for both and design problems . the total execution times ( on an intel _",
    "xeon e5 - 1603 _ with 8 gb of and a nvidia _ tesla k20c _ ) for the design examples are listed in table  [ table : runtime ] and the corresponding pareto - optimal results ( plotted as functions of the number of iterations completed ) are shown in figure  [ fig : experimental_results ] . in these plots , the colour scheme presents solutions with low error in blue , and low cost in red .",
    "even though the outputs can not be directly compared to the true results from an exhaustive search ( the -10 example design space exceeds @xmath16 solutions ) , the trends discussed in section  [ sec : exhaustive ] are mirrored by all plots in figure  [ fig : experimental_results ] .",
    "m0.27 < m0.29 < m0.29 < target & algorithm execution time & mean solution evaluation time + ( ) & 3.6 h & 30 s + ( ) & 18 h & 2 min + -10 ( ) & 66 h & 8 min +    [ table : runtime ]    because of the intractable nature of such exhaustive searches , the generated results are not expected to always predict the true pareto - optimal fronts . instead",
    ", automated exploration must only match , or exceed , the fitness of manually designed networks in order to provide cost reductions . in comparison with manually designed architectures in literature ,",
    "the pareto - optimal results in figure  [ fig : mnist_cnn_results ] include points that offer equivalent performance to the designs in @xcite and @xcite , with implementation costs as low as 25% of their manually designed counterparts ( when weighted with the same cost model detailed in table  [ table : costs ] ) .",
    "this demonstrates that the proposed method can design with vastly reduced cost requirements and same levels of performance when compared to manual design approaches .",
    "furthermore , the pareto - optimal results can also find data points with substantial cost savings penalized only by a small decrease in performance . given that increasing accuracy by only 0.01%",
    "requires a doubling in implementation cost ( for designs shown in figure  [ fig : mnist_cnn_results ] ) , these additional points are invaluable for application platforms with extremely stringent cost budgets .      in order to validate the assumption that a neural network can be trained through regression to model the response surface with sufficient accuracy , the prediction error is plotted in figure  [ fig : rsm_error ]",
    "this graph plots the absolute value of the error ( % difference between the predicted and the evaluated performance of each explored solution ) for each of the 500 algorithm iterations performed during the design example ( with results in figure  [ fig : mnist_cnn_results ] ) .",
    "the narrow error spikes , which are expected , occur at points where the algorithm encounters previously unexplored areas . as these solutions are added to the training set , the prediction error decreases as the response surface model",
    "is updated , and the spikes begin to occur less frequently . outside of these sparse peaks ,",
    "the prediction accuracy is exceptionally high ; the mean error over the last 95 iterations ( all points after the last spike ) is only 0.35% .",
    "a method to automate the multi - objective optimization of neural network hyper - parameters , reducing both algorithm error and computational complexity , was presented . when compared to the results of an exhaustive search on a restricted design space targeting the dataset , the presented method was shown to dramatically reduce computation time required for convergence to a pareto - optimal solution set .",
    "a low of 5% was achieved after only 100 iterations ; in practice , fewer solution evaluations may be required , with corresponding execution times less than those in table  [ table : runtime ] .",
    "furthermore , scalability of the method was demonstrated on larger design spaces for both and models targeting the -10 dataset as well .",
    "even when evaluated on massive design spaces , the presented method was found to still efficiently converge to a diverse pareto - optimal front .",
    "not only was the automation of hyper - parameter design process demonstrated to be both feasible and time efficient , but when compared to manually designed networks from literature , the automated technique produced results with near identical performance while reducing the associated costs by a factor of @xmath42 .",
    "as applications for make further inroads in mobile and embedded market segments , the need to reduce time - to - market and costs will necessitate the use of such automated design methods .",
    "this work was supported in part by a scholarship from the , as well as equipment donations from nvidia corporation .",
    "m.  courbariaux , y.  bengio , and j .-",
    "binaryconnect : training deep neural networks with binary weights during propagations . in _ advances in neural information processing systems 28",
    "_ , pages 31233131 .",
    "curran associates , inc . , 2015 .",
    "t.  domhan , j.  t. springenberg , and f.  hutter .",
    "speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves . in _ proc .",
    "of the twenty - fourth int .",
    "joint conf . on artificial intelligence ,",
    "ijcai _ , pages 34603468 , july 2015 .",
    "s.  ishibushi et  al .",
    "statistical localization exploiting convolutional neural network for an autonomous vehicle . in _ industrial electronics society , iecon 2015 - 41st annual conf . of the ieee _ , pages 13691375 , nov 2015 .",
    "a.  krizhevsky , i.  sutskever , and g.  e. hinton .",
    "classification with deep convolutional neural networks . in _ advances in neural information processing systems 25 _ , pages 10971105 .",
    "curran associates , inc . , 2012 .",
    "t.  okabe , y.  jin , and b.  sendhoff .",
    "a critical survey of performance indices for multi - objective optimisation . in _ evolutionary computation , 2003 .",
    "the 2003 congress on _ , volume  2 , pages 878885 , dec 2003 .",
    "s.  r. young et  al . optimizing deep learning hyper - parameters through an evolutionary algorithm . in _ proc . of the workshop on machine learning in high - performance computing environments",
    "_ , mlhpc 15 , pages 4:14:5 , new york , ny , usa , 2015 ."
  ],
  "abstract_text": [
    "<S> artificial neural networks have gone through a recent rise in popularity , achieving state - of - the - art results in various fields , including image classification , speech recognition , and automated control . </S>",
    "<S> both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper - parameters ( e.g. , number of hidden layers , nodes per layer , or choice of activation functions ) , which have traditionally been optimized manually . with machine learning penetrating low - power mobile and embedded areas , </S>",
    "<S> the need to optimize not only for performance ( accuracy ) , but also for implementation complexity , becomes paramount . in this work </S>",
    "<S> , we present a multi - objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling . </S>",
    "<S> given spaces which can easily exceed @xmath0 solutions , manually designing a near - optimal architecture is unlikely as opportunities to reduce network complexity , while maintaining performance , may be overlooked . </S>",
    "<S> this problem is exacerbated by the fact that hyper - parameters which perform well on specific datasets may yield sub - par results on others , and must therefore be designed on a per - application basis . in our work </S>",
    "<S> , machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks . </S>",
    "<S> the method is evaluated on the and -10 image datasets , optimizing for both recognition accuracy and computational complexity . </S>",
    "<S> experimental results demonstrate that the proposed method can closely approximate the pareto - optimal front , while only exploring a small fraction of the design space . </S>"
  ]
}