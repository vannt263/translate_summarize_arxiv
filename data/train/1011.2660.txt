{
  "article_text": [
    "kernel techniques are now a standard tool of statistical practice and kernel versions of many methods of classical multivariate statistics have now been created .",
    "a few important examples can be found in @xcite ( see the description of kernel pca , pages 4145 ) and @xcite ( for kernel ica ) , for instance .",
    "there are several ways to describe kernel methods , but one of them is to think of them as classical multivariate techniques using generalized notions of inner - product . a basic input in these techniques is a kernel matrix , that is , an inner - product ( or gram ) matrix , for generalized inner - products . if our vectors of observations are @xmath1 , the kernel matrices studied in this paper have @xmath2 entry @xmath3 or @xmath4 , for a certain @xmath5 .",
    "popular examples include the gaussian kernel [ entries @xmath6 , the sigmoid kernel [ entries @xmath7 and polynomial kernels [ entries @xmath8 .",
    "we refer to @xcite for more examples .",
    "as explained in , for instance , @xcite , kernel techniques allow practitioners to essentially do multivariate analysis in infinite - dimensional spaces , by embedding the data in a infinite - dimensional space through the use of the kernel .",
    "a nice numerical feature is that the embedding need not be specified , and all computations can be made using the finite - dimensional kernel matrix .",
    "kernel techniques also allow users to do certain forms of nonlinear data analysis and dimensionality reduction , which is naturally very desirable . @xcite and @xcite are two interesting relatively recent papers concerned broadly speaking with the same types of inferential questions we have in mind and investigate in this paper , though the settings of these papers is quite different from the one we will work under .",
    "kernel matrices and the closely related laplacian matrices also play a central role in manifold learning [ see , e.g. , @xcite and @xcite for an overview of various techniques ] . in `` classical '' statistics , they have been a mainstay of spatial statistics and geostatistics in particular [ see @xcite ] .    in geostatistical applications , it is clear that the dimension of the data is at most  3 . also , in applications of kernel techniques and manifold learning , it is often assumed that the data live on a low - dimensional manifold or structure , the kernel approach allowing us to somehow recover ( at least partially ) this information .",
    "consequently , most theoretical analyses of kernel matrices and kernel or manifold learning techniques have focused on situations where the data is assumed to live on such a low - dimensional structure . in particular , it is often the case that asymptotics are studied under the assumption that the data is i.i.d . from a fixed distribution ",
    "independent of the number of points .",
    "some remarkable results have been obtained in this setting [ see @xcite and also @xcite ] .",
    "let us give a brief overview of such results .",
    "in @xcite , the authors prove that if @xmath9 are i.i.d . with distribution @xmath10 , under regularity conditions on the kernel @xmath11 , the @xmath12th largest eigenvalue of the kernel matrix @xmath13 , with entries @xmath14 converges to the @xmath12th largest eigenvalue of the operator @xmath15 defined as @xmath16 in this important paper , the authors were also able to obtain fluctuation behavior for these eigenvalues , under certain technical conditions [ see theorem 5.1 in @xcite ] .",
    "similar first - order convergence results were obtained , at a heuristic level but through interesting arguments , in @xcite .",
    "these results gave theoretical confirmation to practitioners intuition and heuristics that the kernel matrix could be used as a good proxy for the operator @xmath15 on @xmath17 , and hence kernel techniques could be explained and justified through the spectral properties of this operator .",
    "to statisticians well versed in the theory of random matrices , this set of results appears to be similar to results for low - dimensional covariance matrices stating that when the dimension of the data is fixed and the number of observations goes to infinity , the sample covariance matrix is a spectrally consistent estimator of the population covariance matrix [ see , e.g. , @xcite ] .",
    "however , it is well known [ see , e.g. , @xcite , @xcite , @xcite ] that this is not the case when the dimension of the data , @xmath18 , changes with @xmath19 , the number of observations , and in particular when asymptotics are studied under the assumption that @xmath20 has a finite limit .",
    "we refer to the asymptotic setting where @xmath18 and @xmath19 both tend to infinity as the `` high - dimensional '' setting .",
    "we note that given that more and more datasets have observations that are high dimensional , and kernel techniques are used on some of them [ see @xcite ] , it is natural to study kernel random matrices in the high - dimensional setting .",
    "another important reason to study this type of asymptotics is that by keeping track of the effect of the dimension of the data , @xmath18 , and of other parameters of the problem on the results , they might help us give more accurate prediction about the finite - dimensional behavior of certain statistics than the classical `` small @xmath18 , large @xmath19 '' asymptotics .",
    "an example of this phenomenon can be found in the paper @xcite where it turned out in simulation that some of the doubly asymptotic results concerning fluctuation behavior of the largest eigenvalue of a wishart matrix with identity covariance are quite accurate for @xmath18 and @xmath19 as small as 5 or 10 , at least in the right tail of the distribution .",
    "[ we refer the interested reader to @xcite for more details on the specific example we just described . ]",
    "hence , it is also potentially practically important to carry out these theoretical studies for they can be informative even for finite - dimensional considerations .    the properties of kernel random matrices under classical random matrix assumptions",
    "have been studied by the author in the recent @xcite .",
    "it was shown there that when the data is high dimensional , for instance @xmath21 , and the operator norm of @xmath22 is , for example , bounded , kernel random matrices essentially act like standard gram/``covariance matrices , '' up to recentering and rescaling , which depend only on @xmath5 .",
    "naturally , a certain scaling is needed to make the problem nondegenerate , and the results we just stated hold , for instance , when @xmath23 , for otherwise the kernel matrix is in general degenerate .",
    "we refer to @xcite for more details and discussions of the relevance of these results in practice . in limited simulations",
    ", we found that the theory agreed with the numerics even when @xmath18 was of the order of several 10 s and @xmath20 was not `` too small '' ( e.g. , @xmath24 ) .",
    "these results came as somewhat of a surprise and seemed to contradict the intuition and numerous positive practical results that have been obtained , since they suggested that the kernel matrices we considered were just a ( centered and scaled ) version of the matrix @xmath25 .",
    "however , it should be noted that the assumptions implied that the data was truly high dimensional .",
    "so an interesting middle ground , from modeling , theoretical and practical points of view is the following : what happens if the data does not live exactly on a fixed - dimensional manifold , but lives `` nearby ? '' in other words , the data is now sampled from a `` noisy '' version of the manifold .",
    "this is the question we study in this paper .",
    "we assume now that the data points @xmath26 we observe are of the form @xmath27 where @xmath28 is the `` signal '' part of the observations ( and live , for instance , on a low - dimensional manifold , e.g. , a three - dimensional sphere ) and @xmath29 is the noise part of the observations ( and is , e.g. , multivariate gaussian in dimension @xmath18 , where @xmath18 might be 100 ) .",
    "we think this is interesting from a practical standpoint because the assumption that the data is exactly on a manifold is perhaps a bit optimistic and the `` noisy manifold '' version is perhaps more in line with what statisticians expect to encounter in practice ( there is a clear analogy with linear regression here ) . from a theoretical standpoint ,",
    "such a model allows us to bridge the two extremes between truly low - dimensional data and fully high - dimensional data . from a modeling standpoint",
    ", we propose to scale the noise so that its norm stays bounded ( or does not grow too fast ) in the asymptotics .",
    "that way , the `` signal '' part of the data is likely to be affected but not totally drowned by the noise .",
    "it is important to note , however , that the noise is not `` small '' in any sense of the word ",
    "it is of a size comparable with that of the signal .    in the case of spherical noise",
    "( see below for details but note that the gaussian distribution falls into this category ) our results say that , to first - order , the kernel matrix computed from information@xmath0noise data behaves like a kernel matrix computed from the `` signal '' part of the data , but , we might have to use a different kernel than the one we started with .",
    "this other kernel is quite explicit . in the case of dot - product kernel matrices [ i.e. , @xmath30",
    ", the original kernel can be used ( under certain assumptions)so , to first - order , the noise part has no effect on the spectral properties of the kernel matrix .",
    "the results are different when looking at euclidean distance kernels [ i.e. , @xmath31 where the effect of the noise is basically to change the kernel that is used .",
    "this is in any case a quite positive result in that it says that the whole body of work concerning the behavior of kernel random matrices with low - dimensional input data can be used to also study the `` information@xmath0noise '' case  the only change being a change of kernels .",
    "the case of elliptical noise is more complicated .",
    "the dot - product kernels results still have the same interpretation .",
    "but the euclidean distance kernels results are not as easy to interpret .",
    "before we start , we set some notation . we use @xmath32 to denote the frobenius norm of the matrix @xmath13 [ so @xmath33 and @xmath34 to denote its operator norm , that is , its largest singular value .",
    "we also use @xmath35 to denote the euclidean norm of the vector @xmath36 .",
    "@xmath37 is shorthand for @xmath38 . unless otherwise noted , functions that are said to be lipschitz are lipschitz with respect to euclidean norm .",
    "we split our results into two parts , according to distributional assumptions on the noise .",
    "one deals with the gaussian - like case , which allows us to give a simple proof of the results .",
    "the second part is about the case where the noise has a distribution that satisfies certain concentration and ellipticity properties .",
    "this is more general and brings the geometry of the problem forward .",
    "it also allows us to study the robustness ( and lack thereof ) of the results to the sphericity of the noise , an assumption that is implicit in the high - dimensional gaussian ( and gaussian - like ) case .",
    "we draw some practical conclusions from our results for the case of spherical noise in section [ subsec : practicalconsequences ] .",
    "we first study a setting where the noise is drawn according to a distribution that is similar to a gaussian , but slightly more general .",
    "[ thm : infoplusnoisegaussiancase ] suppose we observe data @xmath1 in @xmath39 , with @xmath40 where @xmath41 where the @xmath18-dimensional vector @xmath42 has i.i.d .",
    "entries with mean  0 , variance 1 , and fourth moment @xmath43 , and @xmath44 .",
    "we assume that there exists a deterministic vector @xmath45 and a real @xmath46 , possibly dependent on @xmath19 , such that @xmath47 . also , @xmath43 might change with @xmath19 but is assumed to remain bounded .",
    "@xmath48 are i.i.d . , and we also assume that @xmath49 and @xmath48 are independent .",
    "we consider the random matrices @xmath50 with @xmath2 entry @xmath51 where @xmath52 let us call @xmath53 .",
    "let @xmath54 be the matrix with @xmath2th entry @xmath55 assuming only that @xmath43 is bounded uniformly in @xmath19 , we have , for a constant @xmath56 independent of @xmath19 , @xmath18 and @xmath22 , @xmath57 .\\ ] ]    we place ourselves in the high - dimensional setting where @xmath19 and @xmath18 tend to infinity .",
    "we assume that @xmath58 , as @xmath18 tends to infinity .    under these assumptions , for any fixed @xmath59 and @xmath46",
    ", @xmath60    if we further assume that @xmath61 remains , for instance , bounded , the same result holds if we replace the diagonal of @xmath62 by @xmath63 , because @xmath64 and therefore @xmath65 .",
    "the approximating matrix we then get is the matrix with @xmath2th entry @xmath66 , where @xmath67 , that is , a `` pure signal '' matrix involving a different kernel from the one with which we started .",
    "we note that there is a potential measurability issue that we address in the proof .",
    "our theorem really means that we can find a random variable that dominates the `` random element '' @xmath68 and goes to 0 in probability .",
    "( this measurability issue could also be addressed through separability arguments but outer - probability statements suffice for our purposes in this paper . )",
    "a subcase of our result is the case of gaussian noise : then @xmath42 is @xmath69 and our result naturally applies .",
    "we also note that @xmath70 can change with @xmath19 .",
    "the class of functions we consider is fixed in the last statement of the theorem but if we were to look at a sequence of kernels we could pick a different function in the class @xmath71 for each @xmath19 [ the proof also applies to matrices with entries @xmath72 , where the functions considered also depend on @xmath2 , but we present the results with a function @xmath5 common to all entries ] .",
    "it should also be noted that the proof technique allows us to deal with classes of functions that vary with @xmath19 : we could have a varying @xmath73 .",
    "as ( [ eq : explicitcontrolerrorgaussianlikecase ] ) makes clear , the approximation result will hold as soon as the right - hand side of ( [ eq : explicitcontrolerrorgaussianlikecase ] ) goes to 0 asymptotically , that is , @xmath74 .",
    "finally , we work here with uniformly lipschitz functions .",
    "the proof technique carries over to other classes , such as certain classes of hlder functions , but the bounds would be different .",
    "proof of theorem [ thm : infoplusnoisegaussiancase ] the strategy is to use the same entry - wise expansion approach that was used in @xcite .",
    "to do so , we remark that @xmath75 remains essentially constant [ across @xmath2 ] in the setting we are considering  this is a consequence of the `` spherical '' nature of high - dimensional gaussian distributions .",
    "we can therefore try to approximate @xmath77 by @xmath78 and all we need to do is to show that the remainder is small .",
    "we also note that if , as we assume , @xmath79 , then @xmath80 , since @xmath81 .",
    "- _ work conditional on _ @xmath82 , _ for _ @xmath83 .",
    "we clearly have @xmath84 let us study the various parts of this expansion .",
    "conditional on @xmath85 , if we call @xmath86 , we see easily that @xmath87 and @xmath88 note that @xmath89 , which we denote @xmath90 , has i.i.d .",
    "entries , with mean 0 , variance  @xmath91 and fourth moment @xmath92 .",
    "we call @xmath93 and @xmath94    with this notation , we have @xmath95    therefore , for any function @xmath5 in @xmath96 , @xmath97 and hence , @xmath98 ^ 2 \\leq2 c_0(n)^2 [ \\beta_{i ,",
    "j}^2 + 4\\alpha_{i , j}^2 ] .\\ ] ] we naturally also have @xmath99 ^ 2 \\leq2 c_0(n)^2 [ \\beta _ { i , j}^2 + 4\\alpha_{i , j}^2 ] .\\ ] ] so we have found a random variable @xmath100 $ ] that dominates the random element @xmath101 ^ 2 $ ] .",
    "one might be concerned about the measurability of @xmath102but by using outer expectations [ see @xcite , page 258 ] , we can completely bypass this potential problem . in",
    "what follows , we denote by @xmath103 an outer expectation .",
    "( though this technical point does not shed further light on the problem , it naturally needs to be addressed . )    hence , @xmath104    let us focus on @xmath105 for a moment .",
    "let us call @xmath106 .",
    "we first note that @xmath107 .",
    "in particular , @xmath108 so @xmath109 .",
    "therefore , @xmath110 .",
    "now recall the results found , for instance , in lemma a-1 in @xcite : if the vector @xmath111 has i.i.d .",
    "entries with mean 0 , variance @xmath112 and fourth moment @xmath113 , and if @xmath13 is a symmetric matrix , @xmath114 where @xmath115 is the hadamard product of @xmath13 with itself , that is , the entrywise product of two matrices .    applying this result in our setting [ i.e. , using the moments ( given above ) of @xmath90 , which has i.i.d .",
    "entries , in the previous formula ] gives @xmath116 it is easy to see that @xmath117 , since @xmath118 and @xmath119",
    ". therefore , @xmath120 we note that under our assumptions on @xmath121 and the fact that @xmath43 remains bounded in @xmath19 ( and therefore @xmath18 ) , this term will go to 0 as @xmath122 .    on the other hand , because @xmath123 , and because @xmath124 and @xmath125 , we have @xmath126    hence , we have for @xmath56 a constant independent of @xmath22 , @xmath18 and @xmath19 , @xmath127 .\\end{aligned}\\ ] ] this inequality allows us to conclude that , for another constant @xmath56 , @xmath128 , \\ ] ] since clearly , @xmath129    under the assumption that @xmath130 exists and is less than @xmath131 , we finally conclude that @xmath132,\\ ] ] and ( [ eq : explicitcontrolerrorgaussianlikecase ] ) is shown .    therefore , under our assumptions , @xmath133 hence , when @xmath19 and @xmath18 tend to @xmath134 , @xmath135 as announced in the theorem .",
    "the proof of theorem [ thm : infoplusnoisegaussiancase ] makes clear that the heart of our argument is geometric : we exploit the fact that @xmath136 is essentially constant across pairs @xmath2 .",
    "it is therefore natural to try to extend the theorem to more general assumptions about the noise distribution than the gaussian - like one we worked under previously .",
    "it is also important to understand the impact of the implicit geometric assumptions ( i.e. , sphericity of the noise ) that are made and in particular the robustness of our results against these geometric assumptions .",
    "we extend the results in two directions .",
    "first , we investigate the generalization of our gaussian - like results to the setting of euclidean - distance kernel random matrices , when the noise is distributed according to a distribution satisfying a concentration inequality multiplied by a random variable , that is , a generalization of elliptical distributions .",
    "this allows us to show that the gaussian - like results of theorem [ thm : infoplusnoisegaussiancase ] essentially hold under much weaker assumptions on the noise distribution , as long as the gaussian geometry ( i.e. , a spherical geometry ) is preserved ( see corollary [ coro : sphericalnoisekernels ] ) .",
    "the results of theorem [ thm : infoplusnoiseconccase ] show that breaking the gaussian geometry results in quite different approximation results .",
    "we also discuss in theorem [ thm : infoplusnoiseconccasedotproductskernels ] the situation of inner - product kernel random matrices under the same `` generalized elliptical '' assumptions on the noise .",
    "we have the following theorem .",
    "[ thm : infoplusnoiseconccase ] suppose we observe data @xmath137 in @xmath39 , with @xmath138 we place ourselves in the high - dimensional setting where @xmath19 and @xmath18 tend to infinity .",
    "we assume that @xmath44 .",
    "@xmath48 are i.i.d . with @xmath139 , and we also assume that @xmath140 and @xmath48 are independent .",
    "@xmath141 are random variables independent of @xmath48 .",
    "we now assume that the distribution of @xmath29 is such that , for any 1-lipschitz function @xmath142 , if @xmath143 , @xmath144 where for simplicity we assume that @xmath145 , @xmath56 and @xmath146 are independent of @xmath18 .",
    "we call @xmath147 and assume that @xmath61 stays bounded as @xmath122 .",
    "we assume that @xmath148 , @xmath149 $ ] , where @xmath150 and @xmath151 are deterministic sequences depending on @xmath18 .",
    "we assume without loss of generality that @xmath152 .",
    "calling @xmath153 , we assume that there exists @xmath154 such that @xmath155 and @xmath156 such that @xmath157    then we have @xmath158 \\bigr|\\rightarrow0\\qquad \\mbox{in probability}.\\ ] ]    we call @xmath159 , and suppose we pick @xmath160 such that @xmath161 .",
    "( note that @xmath162 is always a possibility . )",
    "we call , for @xmath163 given , @xmath164 $ ] , and @xmath165 we consider the random matrices @xmath50 with @xmath2 entry",
    "@xmath166    let us call @xmath54 the matrix with @xmath2th entry @xmath167 we have , for any given @xmath46 and @xmath163 ,",
    "@xmath168    we have the following corollary in the case of `` spherical '' noise , which is a generalization of the gaussian - like case considered in theorem [ thm : infoplusnoisegaussiancase ] .",
    "[ coro : sphericalnoisekernels ] suppose we observe data @xmath1 in @xmath39 , with @xmath40 where @xmath28 and @xmath29 satisfy the same assumptions as in theorem [ thm : infoplusnoiseconccase ] [ with @xmath169",
    ". then the results of theorem [ thm : infoplusnoiseconccase ] apply with @xmath170\\ ] ] and @xmath171    as in theorem [ thm : infoplusnoisegaussiancase ] , we deal with potential measurability issues concerning the @xmath172 in the proof .",
    "our theorem is really that we can find a random variable that goes to 0 with probability 1 and dominates the random element @xmath173an outer - probability statement .",
    "this theorem generalizes theorem [ thm : infoplusnoisegaussiancase ] in two ways .",
    "the `` spherical '' case , detailed in corollary [ coro : sphericalnoisekernels ] , is a more general version of theorem [ thm : infoplusnoisegaussiancase ] limited to gaussian noise .",
    "this is because the gaussian setting corresponds to @xmath174 and @xmath175 .",
    "however , assuming `` only '' concentration inequalities allows us to handle much more complicated structures for the noise distribution .",
    "some examples are given below .",
    "we also note that if the @xmath28 s ( i.e. , the signal part of the @xmath9 s ) are sampled , for instance , from a fixed manifold of finite euclidean diameter , the conditions on @xmath176 are automatically satisfied , with @xmath154 being the euclidean diameter of the corresponding manifold .",
    "another generalization is `` geometric '' : by allowing @xmath141 to vary with @xmath177 , we move away from the spherical geometry of high - dimensional gaussian vectors ( and generalizations ) , to a more `` elliptical '' setting . hence , our results show clearly the potential limitations and the structural assumptions that are made when one assumes gaussianity of the noise .",
    "theorem [ thm : infoplusnoiseconccase ] and corollary [ coro : sphericalnoisekernels ] show that the gaussian - like results of theorem [ thm : infoplusnoisegaussiancase ] are not robust against a change in the geometry of the noise .",
    "we note however that if @xmath141 is independent of @xmath29 and @xmath178 , @xmath179 , so all the noise models have the same covariance but they may yield different approximating matrices and hence different spectral behavior for our information@xmath0noise models .",
    "however , the spherical results have the advantage of having simple interpretations . in the setting of corollary [ coro : sphericalnoisekernels ] , if we assume that @xmath180 and @xmath181 are uniformly bounded ( in @xmath19 ) over the class of functions we consider , we can replace the diagonal of @xmath62 by @xmath63 and have the same approximation results . then the `` new '' @xmath62 is a kernel matrix computed from the signal part of the data with the new kernel @xmath182 .    to make our result more concrete , we give a few examples of distributions for which the concentration assumptions on @xmath29 are satisfied :    * gaussian random variables , for which we have @xmath183 .",
    "we refer to ledoux [ ( @xcite ) , theorem 2.7 ] for a justification of this claim .",
    "* vectors of the type @xmath184 where @xmath36 is uniformly distributed on the unit ( @xmath185-)sphere in dimension @xmath18 .",
    "theorem 2.3 in @xcite shows that our assumptions are satisfied , with @xmath186 , after noticing that a 1-lipschitz function with respect to euclidean norm is also 1-lipschitz with respect to the geodesic distance on the sphere .",
    "* vectors @xmath187 , with @xmath36 uniformly distributed on the unit ( @xmath185-)sphere in @xmath39 and with @xmath188 having bounded operator norm .",
    "* vectors of the type @xmath189 , @xmath190 , where @xmath36 is uniformly distributed in the unit @xmath191 ball or sphere in @xmath39 .",
    "( see ledoux [ ( @xcite ) , theorem 4.21 ] which refers to @xcite as the source of the theorem . ) in this case , @xmath145 depends only on @xmath146 .",
    "* vectors with log - concave density of the type @xmath192 , with the hessian of @xmath193 satisfying , for all @xmath194 , @xmath195 , where @xmath196 is the real that appears in our assumptions .",
    "see ledoux [ ( @xcite ) , theorem 2.7 ] for a justification .",
    "* vectors @xmath36 distributed according to a ( centered ) gaussian copula , with corresponding correlation matrix , @xmath197 , having @xmath198 bounded .",
    "we refer to @xcite for a justification of the fact that our assumptions are satisfied .",
    "[ if @xmath199 has a gaussian copula distribution , then its @xmath177th entry satisfy @xmath200 , where @xmath201 is multivariate normal with covariance matrix @xmath197 , @xmath197 being a correlation matrix , that is , its diagonal is 1 . here",
    "@xmath202 is the cumulative distribution function of a standard normal distribution .",
    "taking @xmath203 gives a centered gaussian copula .",
    "] this last example is intended to show that the result can handle quite complicated and nonlinear noise structure .",
    "we note that to justify that the assumptions of the theorem are satisfied , it is enough to be able to show concentration around the mean or the median , as proposition  1.8 in @xcite makes clear .",
    "the reader might feel that the assumptions concerning the boundedness of the @xmath141 s will be limiting in practice .",
    "we note that the same proof essentially goes through if we just require that @xmath204 s belong to the interval @xmath205 $ ] with probability going to 1 , but this requires a little bit more conditioning and we leave the details , which are not difficult , to the interested reader .",
    "so for instance , if we had a tail condition on @xmath204 , we could bound latexmath:[$\\max",
    "so this boundedness condition is here just to make the exposition simpler and is not particularly limiting in our opinion .",
    "on the other hand , we note that our conditions allow dependence in the @xmath141 s and are therefore rather weak requirements .    finally , the theorem as stated is for a fixed @xmath131 , though the class of functions we are considering might vary with @xmath19 and @xmath18 through the influence of @xmath207 .",
    "the proof makes clear that @xmath131 could also vary with @xmath19 and @xmath18 .",
    "we discuss in more details the necessary adjustments after the proof .",
    "proof of theorem [ thm : infoplusnoiseconccase ] we use the notation @xmath208 and @xmath209 to denote probability conditional on @xmath85 .",
    "we call @xmath210 .",
    "let us also call @xmath211 ; similarly , @xmath212 denotes probability conditional on @xmath213 .",
    "we call @xmath214",
    ". we will start by working conditionally on @xmath213 and eventually decondition our results .",
    "we assume from now on that the @xmath213 we work with is such that @xmath215 .",
    "note that @xmath216 by assumption and also @xmath217 .",
    "the main idea now is that , in a strong sense , @xmath218 where @xmath219 . to show this formally , we write @xmath220=2\\alpha_{i , j}+\\beta_{i , j } , \\ ] ] where @xmath221 and @xmath222    our aim is to show that , as @xmath19 and @xmath18 tend to infinity , @xmath223    - _ on _ @xmath224 .",
    "note that if @xmath225 , @xmath226 .",
    "clearly , @xmath227 since we assumed that @xmath228 , we see that the function @xmath229 is lipschitz ( with respect to euclidean norm ) , with lipschitz constant smaller than @xmath230 , when @xmath231 is in @xmath232 . also , since @xmath139 , @xmath233 , where the expectation is conditional on @xmath213 .",
    "hence , our concentration assumptions on @xmath29 imply that @xmath234\\bigr)^b\\bigr ) .\\ ] ] therefore , if we use a simple union bound , we get @xmath235\\bigr)^b\\bigr ) .\\ ] ] in particular , if we pick , for @xmath156 , @xmath236 , we see that @xmath237\\bigr)^b\\bigr)\\\\ & = & 2c \\exp ( -2(\\log n)^{\\varepsilon})\\rightarrow0.\\end{aligned}\\ ] ]    since @xmath238 and since the latter goes to 0 , we have , unconditionally , @xmath239    - _ on _ @xmath240 .",
    "we see that if @xmath241 and @xmath242 are vectors in @xmath39 , the map @xmath243 is @xmath244 , by the triangle inequality .",
    "therefore , using propositions 1.11 and 1.7 in @xcite [ and using the fact that @xmath245 as @xmath246 and @xmath247 is continuous when using the latter ] , we conclude that @xmath248 if now @xmath249 , and if @xmath250 , @xmath251 where @xmath15 is a constant which does not depend on @xmath213 .",
    "so we conclude that unconditionally , if @xmath252 note also that under our assumptions , @xmath253 .",
    "recall that we aim to show that @xmath254 let us first work on @xmath255 using the fact that @xmath256 , and therefore , @xmath257 if we choose @xmath258 and @xmath259 , we see that the previous equation becomes @xmath260 therefore , if we can show that @xmath261 goes to 0 in probability , we will have @xmath262 in probability . using the concentration result given in ( [ eq : conditionalconcentration ] ) , in connection with proposition 1.9 in @xcite and",
    "a slight modification explained in @xcite , we have @xmath263\\\\[-8pt ] & \\leq & \\frac{r^2_{\\infty } ( p)}{p } \\frac{32 c}{b(c_0)^{2/b}}\\gamma(2/b)=r^2_{\\infty}(p)\\frac { \\kappa_b}{p } .\\nonumber\\end{aligned}\\ ] ] using our assumption that @xmath61 remains bounded , we see that @xmath264 therefore , for some @xmath15 independent of @xmath18 , @xmath265 with probability going to 1",
    ". our assumptions also guarantee that @xmath266 , so we conclude that , for a constant @xmath15 independent of @xmath18 , @xmath267 using ( [ eq : controlgammaijsquare ] ) , we have the deterministic inequality @xmath268 so we can finally conclude that with high probability @xmath269 putting all these elements together , we see that when @xmath270 we can find a constant @xmath15 such that @xmath271 in other words , @xmath272 \\bigr|>ku_p \\bigr)\\rightarrow0 .\\ ] ] this establishes ( a strong form of ) the first part of the theorem , that is , ( [ eq : interpointdistanceellipcase ] ) .",
    "- _ second part of the theorem _ [ _ equation _ ( [ eq : mainresconcellipcase ] ) ] . to get to the second part",
    ", we recall that , assuming that @xmath5 is @xmath131-lipschitz on an interval containing @xmath273 , we have @xmath274    let us define , for @xmath163 given , the event @xmath275\\ } , \\ ] ] and the random element @xmath276    when @xmath277 is true , all the pairs @xmath278 are in @xmath207 : the part concerning @xmath279 is obvious , and the one concerning @xmath280 comes from the definition of @xmath277 .",
    "so when @xmath277 is true , we also have @xmath281 let us now consider the random variable @xmath282 such that @xmath283 on @xmath277 and @xmath134 otherwise , so @xmath284 .",
    "our remark above shows that @xmath285    now , we see from our assumptions about @xmath286 , ( [ eq : controlmaxerrorgeomapproxentrywise ] ) and the fact that @xmath287 , that for any @xmath163 , @xmath288 .",
    "so we have @xmath289 also , @xmath290 with probability tending to 1 , so we can conclude that @xmath291 hence , we also have @xmath292 where this statement might have to be understood in terms of outerprobabilities  hence the @xmath293 instead of @xmath10 .",
    "[ see @xcite , page 258 . in plain english",
    ", we have found a random variable , latexmath:[${\\tau_n \\max_{i\\neq j } } |2\\alpha_{i , j}+\\beta _ { i , j }    which is larger than the random element @xmath102 . ]    in other respects , we have , for all @xmath295 , @xmath296 since @xmath297 therefore , @xmath298 where once again this statement may have to be understood in terms of outer probabilities .",
    "the result stated in ( [ eq : mainresconcellipcase ] ) is proved .",
    "we mentioned before the proof the possibility that we might let @xmath131 vary with @xmath19 and @xmath18 and still get a good approximation result .",
    "this can be done by looking at ( [ eq : keyeqcontrolsuperrorellipconccaseproof ] ) above : @xmath102 is less than @xmath299 with high probability , so when @xmath300 the main approximation result of theorem [ thm : infoplusnoiseconccase ] holds , for a @xmath131 and therefore a class of functions , that vary with @xmath19 ( and @xmath18 ) .",
    "we now turn our attention to kernel matrices of the form @xmath301 which are also of interest in practice . in that setting , we are able to obtain results similar in flavor to theorem [ thm : infoplusnoiseconccase ] , with slight modifications on the assumptions we make about @xmath5 .",
    "[ thm : infoplusnoiseconccasedotproductskernels ] suppose we observe data @xmath302 in @xmath39 , with @xmath138 we place ourselves in the high - dimensional setting where @xmath19 and @xmath18 tend to infinity .",
    "we assume that @xmath44 .",
    "@xmath48 are i.i.d . with @xmath139 , and we also assume that @xmath303 and @xmath48 are independent .    @xmath304 are assumed to be independent of @xmath48 .",
    "we also assume that we can find a deterministic sequence @xmath305 such that @xmath306 and @xmath152 .",
    "we assume that the distribution of @xmath29 is such for any 1-lipschitz function @xmath142 ( with respect to euclidean norm ) , if @xmath143 , @xmath144 where for simplicity we assume that @xmath145 , @xmath56 and @xmath146 are independent of @xmath18 .",
    "we call @xmath147 and assume that @xmath61 stays bounded as @xmath122 .",
    "we call @xmath307 , and @xmath154 a real such that @xmath308 .",
    "we assume that there exists @xmath156 such that @xmath309    we then have @xmath310    we call @xmath311 $ ] and @xmath312    we then consider the random matrices @xmath50 with @xmath2 entry",
    "@xmath313    let us call @xmath62 the matrix with @xmath2th entry @xmath314 we have , for any @xmath46 and @xmath163 , @xmath315    we note that under our assumptions , we also have @xmath316 , with high probability , and uniformly in @xmath5 in @xmath317 . therefore , when @xmath318 , the result is also valid if we replace the diagonal of @xmath54 by @xmath319in which case the new approximating matrix is the kernel matrix computed from the signal part of the data . furthermore , the same argument shows that we get a valid operator norm approximation of @xmath13 by this `` pure signal '' matrix as soon as @xmath320 tends to 0",
    ".    the same measurability issues as in the previous theorems might arise here and the statement should be understood as before : we can find a random variable going to 0 in probability that is larger than the random element @xmath321 .    finally , let us note that once again the theorem is stated for a fixed @xmath131 [ and hence for an essentially fixed ( with @xmath19 ) class of functions , though some changes in this class might come from varying @xmath322 , but the proof allows us to deal with a varying @xmath323 .",
    "the adjustments are very similar to the ones we discussed after the proof of theorem [ thm : infoplusnoiseconccase ] and we leave them to the interested reader .",
    "proof of theorem [ thm : infoplusnoiseconccasedotproductskernels ] the proof is quite similar to that of theorem [ thm : infoplusnoiseconccase ] , so we mostly outline the differences and use the same notation as before .",
    "we now have to focus on @xmath324 the analysis of @xmath325 is entirely similar to our analysis of @xmath326 in the proof of theorem [ thm : infoplusnoiseconccase ] .",
    "the key remark now is that as function of @xmath29 , when @xmath327 , it is , with the new definition of @xmath154 , @xmath328-lipschitz with respect to euclidean norm .",
    "so we immediately have , with the new definition of @xmath154 : if @xmath329 , and @xmath330 , for some @xmath331 which does not depend on @xmath213 , @xmath332 now , since @xmath333 , we conclude as before that @xmath334    on the other hand , using the fact that @xmath335 , and analyzing the concentration properties of @xmath336 in the same way as we did those of @xmath337 , we conclude that if @xmath338 , we can find a constant @xmath15 such that @xmath339 and @xmath340 similar arguments , relying on the fact that is obviously 1-lipschitz with respect to euclidean norm , also lead to the fact that @xmath341 therefore , we can find @xmath15 , greater than 1 without loss of generality , such that @xmath342 we can therefore conclude that @xmath343    if @xmath344 , then both @xmath345 and @xmath346 tend to 0 . therefore , under our assumptions , @xmath347 so we have shown the first assertion of the theorem .",
    "the final step of the proof is now clear : we have , for all @xmath2 , @xmath348 when for all @xmath2 , @xmath349 and @xmath350 are in @xmath351 .",
    "this event happens with probability going to 1 under our assumptions .",
    "so following the same approach as before and dealing with measurability in the same way , we have , with probability going to 1 , @xmath352 so we conclude that @xmath353 from this statement , we get in the same manner as before , @xmath354    as before , the equations above show that if @xmath355 , the same approximation result holds , now with a varying @xmath323 .",
    "our aim in giving approximation results is naturally to use existing knowledge concerning the approximating matrix to reach conclusions concerning the information@xmath0noise kernel matrices that are of interest here . in particular",
    ", we have in mind situations where the `` signal '' part of the data , that is , what we called @xmath286 in the theorems , and @xmath5 [ or @xmath356 , with @xmath61 being as defined in theorems [ thm : infoplusnoisegaussiancase ] or [ thm : infoplusnoiseconccase ] ] are such that the assumptions of theorems 3.1 or 5.1 in @xcite are satisfied , in which case we can approximate the eigenvalues of @xmath62 by those of the corresponding operator in @xmath17 . in this setting the matrix @xmath357 , which is normalized so its entries are of order @xmath358 has a nondegenerate limit , which is why we considered for our kernel matrices the normalization @xmath359 .",
    "[ this normalization by @xmath358 makes our proofs considerably simpler than the ones given in @xcite . ]",
    "another potentially interesting application is the case where the signal part of the data is sampled i.i.d . from a manifold with bounded euclidean diameter , in which case our results are clearly applicable .",
    "the practical interest of the theorems we obtained above lie in the fact that the frobenius norm is larger than the operator norm , and therefore all of our results also hold in operator norm .",
    "now we recall the discussion in el karoui [ ( @xcite ) , section 3.3 ] , where we explained that consistency in operator norm implies consistency of eigenvalues and consistency of eigenspaces corresponding to separated eigenvalues [ as consequences of weyl s inequality and the davis  kahane @xmath360 theorem ",
    "see @xcite and @xcite ] .",
    "theorems [ thm : infoplusnoisegaussiancase ] , [ thm : infoplusnoiseconccase ] , [ thm : infoplusnoiseconccasedotproductskernels ] therefore imply that under the assumptions stated there , the spectral properties of the matrix @xmath13 can be deduced from those of the matrix @xmath62 . in particular , for techniques such as kernel pca",
    ", we expect , when it is a reasonable idea to use that technique , that @xmath13 will have some separated eigenvalues , that is , a few will be large and there will be a gap in the spectrum . in that setting , it is enough to understand @xmath62 , which corresponds , if @xmath361 , to a pure signal matrix , with a possibly slightly different kernel , to have a theoretical understanding of the properties of the technique .",
    "for instance , if @xmath361 , if the assumptions underlying the first - order results of @xcite are satisfied for @xmath62 , the ( first - order ) spectral properties of @xmath13 are the same as those of @xmath62 , and hence of the corresponding operator in @xmath17 .",
    "our analysis reveals a very interesting feature of the gaussian kernel , that is , the case where @xmath362 , for some @xmath363 : when theorem [ thm : infoplusnoisegaussiancase ] or corollary [ coro : sphericalnoisekernels ] ( i.e. , theorem [ thm : infoplusnoiseconccase ] with @xmath361 ) apply , the eigenspaces corresponding to separated eigenvalues of the signal@xmath0noise kernel matrix converge to those of the pure signal matrix .",
    "this is simply due to the fact that in that setting , if @xmath364 is the matrix such that @xmath365 a rescaled version of the `` pure signal '' matrix @xmath176 with @xmath2th entry @xmath366 , we have @xmath367 this latter statement is a simple consequence of the fact that @xmath368 is a diagonal matrix with entries @xmath369 on the diagonal , and therefore its operator norm goes to 0 .",
    "on the other hand , @xmath364 clearly has the same eigenvectors as the pure signal matrix @xmath176 .",
    "hence , because the eigenspaces of @xmath62 are consistent for the eigenspaces of @xmath364 corresponding to separated eigenvalues , they are also consistent for those of @xmath176 .",
    "( we note that our results are actually stronger and allow us to deal with a collection of matrices with varying @xmath370 and not a single @xmath370 , as we just discussed .",
    "this is because we can deal with approximations over a collection of functions in all our theorems . )    because of the practical importance of eigenspaces in techniques such as kernel pca , these remarks can be seen as giving a theoretical justification for the use of the gaussian kernel over other kernels in the situations where we think we might be in an information@xmath0noise setting , and the noise is spherical .    on the other hand",
    ", @xmath364 underestimates the large eigenvalues of @xmath176 because @xmath371 , and obviously @xmath372 . using weyl s inequality",
    "[ see @xcite ] , we have , if we denote by @xmath373 is the @xmath177th eigenvalue of the symmetric matrix @xmath13 , @xmath374 since the right - hand side goes to 0 asymptotically , the eigenvalues of @xmath176 ( the `` pure signal '' matrix ) that stay asymptotically bounded away from 0 are underestimated by the corresponding eigenvalues of @xmath62 .",
    "when the noise is elliptical , that is , @xmath141 s are not all equal to 1 , the `` new '' matrix @xmath364 we have to deal with has entries @xmath375 so it can be written in matrix form @xmath376 where @xmath377 is a diagonal matrix with @xmath378 . by the same arguments as above , @xmath379 in probability , but now @xmath364 does not have the same eigenvectors as the pure signal matrix @xmath176 .",
    "so in this elliptical setting if we were to do kernel analysis on @xmath13 , we would not be recovering the eigenspaces of the pure signal matrix @xmath176 .",
    "in various parts of statistics and machine learning , it has been argued that laplacian matrices should be used instead of kernel matrices .",
    "see , for instance , the very interesting @xcite , where various spectral properties of laplacian matrices have been studied , under a `` pure '' signal assumption in our terminology .",
    "for instance , it is assumed that the data is sampled from a fixed - dimensional manifold . in light of the theoretical and practical success of these methods ,",
    "it is natural to ask what happens in the information@xmath0noise case .",
    "there are several definitions of laplacian matrices .",
    "a popular one [ see , e.g. , the work of @xcite , among other publications ] , is derived from kernel matrices : given @xmath13 a kernel matrix , the laplacian matrix is defined as @xmath380    when our theorems [ thm : infoplusnoiseconccase ] or [ thm : infoplusnoiseconccasedotproductskernels ] apply , we have seen that , for relevant classes of functions @xmath381 , @xmath382 in probability .",
    "let us now focus on the case of a single function @xmath5 . if we call @xmath383 the laplacian matrix corresponding to @xmath384 , we have @xmath385    we conclude that @xmath386 in probability",
    "; we can therefore deduce that the spectral properties of the laplacian matrix @xmath387 from those of @xmath383 , which , when @xmath388 , is a `` pure signal '' matrix , where we have slightly adjusted the kernel . here again , the gaussian kernel plays a special role , since when we use a gaussian kernel , @xmath383 is a scaled version of the laplacian matrix computed from the signal part of the data .",
    "finally , other versions of the laplacian are also used in practice . in particular , a `` normalized '' version",
    "is sometimes advocated , and computed as @xmath389 , if @xmath377 is the diagonal of the matrix @xmath387 defined above .",
    "we have just seen that @xmath390 in probability and @xmath391 in probability .",
    "therefore , if the entries of @xmath392 are bounded away from 0 with probability going to  1 , we conclude that @xmath393 stays bounded with high probability and @xmath394 so once again , understanding the spectral properties of @xmath395 essentially boils down to understanding those of @xmath396 , which is , in the spherical setting where @xmath361 , a `` pure signal '' matrix . in the case of the gaussian kernel , @xmath396 is equal to the normalized laplacian matrix computed from the `` pure signal '' data @xmath286 .",
    "[ [ the - question - of - centering ] ] the question of centering + + + + + + + + + + + + + + + + + + + + + + + + +    in practice , it is often the case that one works with centered versions of kernel matrices : either the row sums , the column sums or both are made to be equal zero .",
    "these centering operations amount to multiplying ( resp .",
    ", on the right , left or both ) our original kernel matrix by the matrix @xmath397 , where @xmath398 is the @xmath19-dimensional vector whose entries are all equal to 1 .",
    "this matrix has operator norm 1 , so when @xmath62 is such that @xmath399 , the same is true for @xmath400 and @xmath401 , where @xmath45 and @xmath146 are either 0 or 1 .",
    "this shows that our approximations are therefore also informative when working with centered kernel matrices .",
    "our results aim to bridge the gap in the existing literature between the study of kernel random matrices in the presence of pure low - dimensional signal data [ see , e.g. , @xcite ] and the case of truly high - dimensional data [ see @xcite ] .",
    "our study of information@xmath0noise kernel random matrices shows that , to first order , kernel random matrices are somewhat `` spectrally robust '' to the corruption of signal by additive high dimensional and spherical noise ( whose norm is controlled ) . in particular , they tend to behave much more like a kernel matrix computed from a low - dimensional signal than one computed from high - dimensional data .",
    "some noteworthy results include the fact that dot - product kernel random matrices are , under reasonable assumptions on the kernel and the `` signal distribution '' spectrally robust for both eigenvalues and eigenvectors .",
    "the gaussian kernel also yields spectrally robust matrices at the level of eigenvectors , when the noise is spherical .",
    "however , it will underestimate separated eigenvalues of the gaussian kernel matrix corresponding to the signal part of the data .    on the other hand ,",
    "euclidean distance kernel random matrices are not , in general , robust to the presence of additive noise . as our results show , under reasonably minimal assumptions on both the noise , the kernel and the signal distribution , a euclidean distance kernel random matrix computed from additively corrupted data behaves like another euclidean distance kernel matrix computed from another kernel : in the case of spherical noise , it is a shifted version of @xmath5 , the shift being twice the norm of the noise . for spherical noise ,",
    "this is bound to create ( except for the gaussian kernel ) potentially serious inconsistencies in both estimators of eigenvalues and eigenvectors , because the eigenproperties of the kernel matrix corresponding to the function @xmath402 are in general different from that of the kernel matrix corresponding to the function @xmath5 .",
    "the same remarks apply to the case of elliptical noise , where the change of kernel is not deterministic and even more complicated to describe and interpret .",
    "our study also highlights the importance of the implicit geometric assumptions that are made about the noise .",
    "in particular , the results are qualitatively different if the noise is spherical ( e.g. , multivariate gaussian ) or elliptical ( e.g. , multivariate  @xmath403 ) .",
    "interpretation is more complicated in the elliptical case and a number of nice properties ( e.g. , robustness or consistency ) which hold for spherical noise do not hold for elliptical noise .",
    "we note that our study suggests that simple practical ( and entrywise ) corrections could be used to go from the `` signal@xmath0noise '' situation to an approximation of the `` pure signal '' situation . those would naturally depend on the noise geometry and what information practitioners have about it .",
    "our results can therefore be seen as highlighting ( from a theoretical point of view ) the strength and limitations of techniques which rely on kernel random matrices as a primary element in a data analysis .",
    "we hope they shed light on an interesting issue and will help refine our understanding of the behavior of kernel techniques and related methodologies for high - dimensional input data .",
    "the author would like to thank peter bickel for suggesting that he consider the problem studied here and in general for many enlightening discussions about topics in high - dimensional statistics .",
    "he would also like to thank an anonymous referee and associate editors for raising interesting questions which contributed to a strengthening of the results presented in the paper .",
    "belkin , m. and niyogi , p. ( 2003 ) .",
    "laplacian eigenmaps for dimensionality reduction and data representation .",
    "_ neural comput . _ * 15 * 13731396 .",
    "available at http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780317 .",
    "zwald , l. , bousquet , o. and blanchard , g. ( 2004 ) .",
    "statistical properties of kernel principal component analysis . in _ learning theory_. _ lecture notes in computer science _ * 3120 * 594608 .",
    "springer , berlin ."
  ],
  "abstract_text": [
    "<S> kernel random matrices have attracted a lot of interest in recent years , from both practical and theoretical standpoints . </S>",
    "<S> most of the theoretical work so far has focused on the case were the data is sampled from a low - dimensional structure . </S>",
    "<S> very recently , the first results concerning kernel random matrices with high - dimensional input data were obtained , in a setting where the data was sampled from a genuinely high - dimensional structure  similar to standard assumptions in random matrix theory .    in this paper </S>",
    "<S> , we consider the case where the data is of the type `` information@xmath0noise . '' in other words , each observation is the sum of two independent elements : one sampled from a `` low - dimensional '' structure , the signal part of the data , the other being high - dimensional noise , normalized to not overwhelm but still affect the signal . </S>",
    "<S> we consider two types of noise , spherical and elliptical .    in the spherical setting , </S>",
    "<S> we show that the spectral properties of kernel random matrices can be understood from a new kernel matrix , computed only from the signal part of the data , but using ( in general ) a slightly different kernel . </S>",
    "<S> the gaussian kernel has some special properties in this setting .    the elliptical setting , which is important from a robustness standpoint , is less prone to easy interpretation .    .    </S>"
  ]
}