{
  "article_text": [
    "the complexity of classification models is usually highly related , and typically linear w.r.t the number of possible categories denoted @xmath0 . when facing problems with a very large number of classes , like text classification in large ontologies , object recognition or word prediction in deep learning language models , this becomes a critical point making classification methods inefficient in term of inference complexity .",
    "there is thus a need to develop new methods able to predict in large output spaces at a low cost .",
    "several methods have been recently developed for reducing the classification speed .",
    "they are based on the idea of using a _ structure _ that organizes the possible outputs , and allows one to reduce the inference complexity .",
    "two main families of approaches have been proposed : ( i ) error - correcting codes approaches @xcite that associate a short code to each category ; the classification becomes a code prediction problem which can be achieved faster by predicting each element of the code .",
    "( ii ) the second family is hierarchical methods @xcite where the possible categories are leaves of a tree . in that case , an output is predicted by choosing a path in the tree like in decision trees .",
    "these two methods typically involve a prediction complexity of @xmath1 allowing a great speed - up .",
    "but these approaches suffer from one major drawback : the structure used for prediction ( error correcting codes , or tree ) is usually built during a preliminary step - before learning the classifier - following hand - made heuristics , typically by using clustering algorithms or huffman codes @xcite .",
    "the problem is that the quality of the obtained structure greatly influences the quality of the final classifier , and this step is thus a critical step .    in this paper",
    ", we propose the _ reinforced decision tree ( rdt ) _ model which is able to simultaneously learn how to organize categories in a hierarchy and learn the corresponding classifier in a single step .",
    "the obtained system is a high speed and efficient predictive model where the category structure has been fitted for the particular task to solve , while it is built _ by hand _ in existing approaches .",
    "the main idea of rdt is to consider the classification problem as a * sequential decision process * where a learned policy guides any input @xmath2 in a tree structure from the root to one leaf .",
    "the learning algorithm is inspired from policy - gradient methods @xcite which allows us to act on both the way an input @xmath2 falls into the tree , but also on the categories associated with the leaves of this tree .",
    "the difference with the classical reinforcement learning context is that the feedback provided to the system is a derivable loss function as proposed in @xcite which gives more information than a reward signal , and allows fast learning of the parameters .",
    "the contributions are :    * a novel model able to simultaneously discover a relevant hierarchy of categories and the associated classification model in one step . * a gradient - based learning method based on injecting a derivable loss function in policy gradient algorithms . * a first set of preliminary experiments over toy datasets allowing to better understand the properties of such an approach .",
    "we consider the multi - class classification problem where each input @xmath3 has to be associated with one of the @xmath0 possible categories .",
    "let us denote @xmath4 the label of @xmath2 , @xmath5 such that @xmath6 if @xmath2 belongs to class @xmath7 and @xmath8 elsewhere .",
    "we will denote @xmath9 the set of @xmath10 training examples .      the _ reinforced decision tree ( rdt ) _ architecture shares common points with decision trees .",
    "let us denote @xmath11 such a tree with parameters @xmath12 and @xmath13 that will be defined in the later :    * the tree is composed of a set of nodes denoted @xmath14 where @xmath15 is the number of nodes of the tree * @xmath16 is the root of the tree .",
    "* @xmath17 means that node @xmath18 is the parent of @xmath19 .",
    "each node but @xmath16 has only one parent , @xmath16 has no parent since it is the root of the tree . *",
    "@xmath20 if and only if @xmath19 is a leaf of the tree .",
    "note that we do not have any constraint concerning the number of leaves or the topology of the tree .",
    "each node of the tree is associated with its own set of parameters :    * a node @xmath19 is associated with a set of parameters denoted @xmath21 if @xmath19 is an internal node i.e @xmath22 . * a node @xmath19 is associated with a set of parameters denoted @xmath23 if @xmath19 is a leaf of the tree .",
    "parameters @xmath24 are the parameters of the policy that will guide an input @xmath2 from the root node to one of the leaf of the tree .",
    "parameters @xmath25 correspond to the prediction that will be produced when an input reached the leaf @xmath19 .",
    "our architecture is thus very close to classical decision trees , the * major difference * being that the prediction associated with each leaf is a set of parameters that will be learned during the training process , allowing the model to choose how to match the leaves of the tree with the categories .",
    "the model will thus be able to simultaneously learn which path an input has to follow based on the @xmath21 parameters , but also how to organize the categories in the tree based on the @xmath25 parameters .",
    "an example of rdt is given in figure [ fig : rdt ] .",
    "let us denote @xmath26 a trajectory , @xmath27 where @xmath28 and @xmath29 is the index of the i - th node of the trajectory .",
    "@xmath26 is thus a sequence of nodes where @xmath30 , @xmath31 and @xmath32 .",
    "each internal node @xmath19 is associated with a function ( or policy ) @xmath33 which role is to compute the probability that a given input @xmath2 will fall in one of the children of @xmath19 .",
    "@xmath33 is defined as : @xmath34 with @xmath35 if @xmath36 . in other words , @xmath37 is the probability that @xmath2 moves from @xmath19 to @xmath38 . note that the different between rdt and dt is that the decision taken at each node is stochastic and not deterministic which will allow us to use gradient - based learning algorithms .",
    "the probability of a trajectory @xmath27 given an input @xmath2 can be written as : @xmath39    once a trajectory has been sampled , the prediction produced by the model depends on the leaf @xmath40 reached by @xmath2 .",
    "the model directly outputs @xmath41 as a prediction , @xmath41 being a vector in @xmath42 ( one score per class ) as explained before .",
    "note that the model produces one score for each possible category , but the inference complexity of this step is @xmath43 since it just corresponds to returning the value @xmath41 .    the complete inference process is described in algorithm [ algo ] :    1 .   sample a trajectory @xmath27 given @xmath2 , by sequentially using the policies @xmath44 2 .   returns the predicted output @xmath41      [ algo ]    the goal of the learning procedure is to simultaneously learn both the _ policy functions _ @xmath33 and the _ output parameters _",
    "@xmath25 in order to minimize a given learning loss denoted @xmath45 which corresponds here to a classification loss ( e.g square loss or hinge loss ) . ]",
    "our learning algorithm is based on an extension of * policy gradient techniques * inspired from the reinforcement learning literature and similar to @xcite .",
    "more precisely , our learning method is close to the methods proposed in @xcite with the difference that , instead of considering a reward signal which is usual in reinforcement learning , we consider a loss function @xmath45 .",
    "this function computes the quality of the system , providing a richer feedback information than simple rewards since it can be derivated , and thus gives the direction in which parameters have to be updated .",
    "the performance of our system is denoted @xmath46 : @xmath47\\ ] ] where @xmath48 is the prediction made following trajectory @xmath26 - i.e the sequence of nodes chosen by the @xmath49-functions .",
    "the optimization of @xmath50 can be made by gradient - descent techniques and we need to compute the gradient of @xmath50 : @xmath51    this gradient can be simplified such that : @xmath52    using the monte carlo approximation of this expectation by taking @xmath53 trail histories over the @xmath10 training examples , and given that @xmath54 , we obtain : @xmath55 % \\nabla_{\\theta,\\alpha}log p_\\alpha(h | x_i ) = \\nabla_{\\theta,\\alpha } \\sum\\limits_{i=1}^{t-1 } log \\pi_{\\theta_{(i)}}(x , n_{(i+1)})\\ ] ]    intuitively , the gradient is composed of two terms :    * the first term aims at penalizing trajectories with high loss - and thus encouraging to find trajectories with low loss .",
    "the first term only acts on the @xmath12 parameters , modifying the probabilities computed at the internal nodes levels . *",
    "the second term is the gradient computed over the final loss and concerns the @xmath13 values corresponding to the leaf node where the input @xmath2 arrives at the end of the process .",
    "it thus changes the @xmath13 used for prediction in order to capture the category of @xmath2 .",
    "this gradient term is responsible about how to allocate the categories in the leaves of the tree .",
    "the learning algorithm in its stochastic gradient variant is described in algorithm [ algo ] .",
    "[ [ inference - complexity ] ] inference complexity : + + + + + + + + + + + + + + + + + + + + + +    the complexity of the inference process is linear with the depth of the tree . typically , in a multi - class classification problem , the depth of the tree will be proportional to @xmath56 , resulting in a very high speed inference process similar to the one obtained for example using hierarchical softmax modules .    [ [ learning - complexity ] ] learning complexity : + + + + + + + + + + + + + + + + + + + + +    the policy gradient algorithm developed in this paper is an iterative gradient - based method .",
    "each learning iteration complexity is @xmath57 but the number of needed iterations is not known .",
    "moreover , the optimization problem is clearly not a convex problem , and the system can be stuck in a local minimum . as explained in section [ exp ] , a way to avoid problematic local minimum is to choose a number of leaves which is higher than the number of categories , giving more freedom degrees to the model .",
    "[ [ using - rdt - for - complex - problems ] ] using rdt for complex problems + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    different functions topologies can be used for @xmath49 . in the following",
    "we have used simple linear functions , but more sophisticated ones can be tested like neural networks .",
    "moreover , in our model , there is no constraints upon the @xmath13 parameters , nor about the loss function @xmath45 which only has to be derivable .",
    "it thus means that our model can also be used for other tasks like multivariate regression , or even for producing continuous outputs at a low price . in that case ,",
    "rdts act as discretization processes where the objective of the task and the discretization are made simultaneously .",
    "in this section , we provide a set of experiments that have been made on toy - datasets to better understand the ability of rdt to perform good predictions and to discover a relevant hierarchy .",
    "our model has been compared to the same model but where the categories associated with the leaves ( the @xmath25 values ) have been chosen randomly , each leaf being associated to one possible category - i.e each vector @xmath25 is full of @xmath58 with one @xmath59-value at a random position - this model is called _",
    "random tree_. in that case , the @xmath13-parameters are not updated during the gradient descent .",
    "hyper - parameters ( learning rate and number of iterations ) have been tuned by cross - validation , and the results have been averaged on five different runs .",
    "we also made comparisons with a linear svm ( one versus all ) and decision trees .",
    "these preliminary experiments aim at showing the ability of our integrated approach to learn how to associate categories to leaves of the tree .",
    "we consider * simple 2d datasets * composed of categories sampled following a gaussian distribution .",
    "each category is composed of 100 vectors ( 50 for train , 50 for test ) sampled following @xmath60 where @xmath61 has been uniformly sampled between @xmath62 and @xmath63 and @xmath64 has been also sampled - see figure [ res2]-right for the dataset with @xmath65 categories and [ res3]-right for the dataset with @xmath66 categories .",
    "two preliminary conclusions can be drawn from the presented results : first , one can see that , when considering a particular architecture , the rdt is able to determine how to allocate the categories in the leaves of the tree : the performance of rdt w.r.t random trees where categories have been randomly sampled in the leaves is clearly better .",
    "( ii ) the second conclusion is that , when considering a problem with @xmath0 categories , a good option is to build a tree with @xmath67 leaves since it gives more freedom degrees to the model which will more easily find how to allocate the categories in the leaves . note that , for the problem with 32 categories , a 84% accuracy is obtained when using a tree of depth 6 : only 6 binary classifiers are used for predicting the category . in comparison to a linear svm , where the inference complexity is higher than ours ( @xmath68 ) our approach performs better .",
    "this is mainly due to the ability of our model to learn non - linear decision frontiers . at last ,",
    "when considering decision trees , one can see that they are sometimes equivalent to rdt .",
    "we think that this is mainly due to the small dimension of the input space , and the small number of examples for which decision trees are well adapted .",
    "is the width of the tree ( i.e the number of children per node ) , @xmath69 is the depth and @xmath70 is the resulting number of leaves . ]     is the width of the tree ( i.e the number of children per node ) , @xmath69 is the depth and @xmath70 is the resulting number of leaves . ]",
    "in multi - class classification problems , the classical approach is to train one - versus - all classifiers .",
    "it is one of the most efficient technique @xcite even with a very large number of classes , but the inference complexity is linear w.r.t the number of possible categories resulting in low - speed prediction algorithms .",
    "hierarchical models have been proposed to reduce this complexity .",
    "they have been developed for two cases : ( i ) a first one where a hierarchy of category is already known ; in that case , the hierarchy of classifiers is mapped on the hierarchy of classes .",
    "( ii ) a second approach closer to ours consists in automatically building a hierarchy from the training set .",
    "this is usually done in a preliminary step by using for example clustering techniques like spectral clustering on the confusion matrix @xcite , using probabilistic label tree @xcite or even partitioning optimization @xcite .",
    "facing these approaches , rdt has the advantage to learn the hierarchy and the classifier in an integrated step only guided by a unique loss function .",
    "the closest work is perhaps @xcite which discovers the hierarchy using online learning algorithms , the construction of the tree being made during learning .",
    "other families of methods have been proposed like error - correcting codes @xcite , sparse coding @xcite or even using representation learning techniques , representations of categories being obtained by unsupervised models @xcite .    at last ,",
    "the use of sequential learning models , inspired by reinforcement learning , in the context of classification or regression has been explored recently for different applications like features selection @xcite or image classification @xcite .",
    "our model belongs to this family of approaches .",
    "we have presented _ reinforced decision trees _ which is a learning model able to simultaneously learn how to allocate categories in a hierarchy and how to classify inputs .",
    "rdts are sequential decision models where the prediction over one input is made using @xmath1 classifiers , making this method suitable for problems with large number of categories .",
    "moreover , the method can be easily adapted to any learning problem like regression or ranking , by changing the loss function .",
    "rdts are learned by using a policy gradient - inspired methods .",
    "preliminary results show the effectiveness of this approach .",
    "future work mainly involves real - world experimentation , but also extension of this model to continuous outputs problems ."
  ],
  "abstract_text": [
    "<S> in order to speed - up classification models when facing a large number of categories , one usual approach consists in organizing the categories in a particular structure , this structure being then used as a way to speed - up the prediction computation . </S>",
    "<S> this is for example the case when using error - correcting codes or even hierarchies of categories . </S>",
    "<S> but in the majority of approaches , this structure is chosen _ by hand _ , or during a preliminary step , and not integrated in the learning process . </S>",
    "<S> we propose a new model called reinforced decision tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure . </S>",
    "<S> this approach keeps the advantages of existing techniques ( low inference complexity ) but allows one to build efficient classifiers in one learning step . </S>",
    "<S> the learning algorithm is inspired by reinforcement learning and policy - gradient techniques which allows us to integrate the two steps ( building the tree , and learning the classifier ) in one single algorithm .    </S>",
    "<S> reinforcement learning , machine learning , policy gradient , decision trees , classification </S>"
  ]
}