{
  "article_text": [
    "the concept of entropy plays a central role in information theory , and has found a wide array of uses in other disciplines , including statistics , probability and combinatorics .",
    "the _ ( differential ) entropy _ of a random vector @xmath6 with density function @xmath7 is defined as @xmath8 where @xmath9 .",
    "it represents the average information content of an observation , and is usually thought of as a measure of unpredictability .    in statistical contexts , it is often the estimation of entropy that is of primary interest , for instance in goodness - of - fit tests of normality @xcite or uniformity @xcite , tests of independence @xcite , independent component analysis @xcite and feature selection in classification @xcite .",
    "see , for example , @xcite and @xcite for other applications and an overview of nonparametric techniques , which include methods based on sample spacings ( in the univariate case ) , histograms and kernel density estimates , among others .",
    "the estimator of @xcite is particularly attractive as a starting point , both because it generalises easily to multivariate cases , and because , since it only relies on the evaluation of @xmath0th - nearest neighbour distances , it is straightforward to compute .    to introduce this estimator ,",
    "let @xmath10 be independent random vectors with density @xmath7 on @xmath2 .",
    "write @xmath11 for the euclidean norm on @xmath2 , and for @xmath12 , let @xmath13 denote a permutation of @xmath14 such that @xmath15 .",
    "for conciseness , we let @xmath16 denote the distance between @xmath17 and the @xmath0th nearest neighbour of @xmath17 .",
    " leonenko estimator of the entropy @xmath18 is given by @xmath19 where @xmath20 denotes the volume of the unit @xmath21-dimensional euclidean ball and where @xmath22 denotes the digamma function .",
    "in fact , this is a generalisation of the estimator originally proposed by @xcite , which was defined for @xmath23 . for integers",
    "@xmath0 we have @xmath24 where @xmath25 is the euler  mascheroni constant , so that @xmath26 as @xmath27 .",
    "this estimator can be regarded as an attempt to mimic the ` oracle ' estimator @xmath28 , based on a @xmath0-nearest neighbour density estimate that relies on the approximation @xmath29 the initial purpose of this paper is to provide a detailed description of the theoretical properties of the kozachenko  leonenko estimator .",
    "in particular , when @xmath3 , we show that under a wide range of choices of @xmath0 ( which must diverge to infinity with the sample size ) and under regularity conditions , the estimator satisfies @xmath30 this immediately implies that in these settings , @xmath31 is efficient in the sense that @xmath32 the fact that the asymptotic variance is the best attainable follows from , e.g. , @xcite .",
    "when @xmath33 , we show that   no longer holds but the kozachenko ",
    "leonenko estimator is still root-@xmath1 consistent provided @xmath0 is bounded .",
    "moreover , when @xmath34 , it turns out a non - trivial bias means that the rate of convergence is slower than @xmath35 in general , regardless of the choice of @xmath0 .",
    "these results motivate our second main contribution , namely the proposal of a new entropy estimator , formed as a weighted average of kozachenko  leonenko estimators for different values of @xmath0 .",
    "we show that it is possible to choose the weights in such a way as to cancel the dominant bias terms , thereby yielding an efficient estimator in arbitrary dimensions , given sufficient smoothness .",
    "there have been several previous studies of the kozachenko ",
    "leonenko estimator , but results on the rate of convergence have until now confined either to the case @xmath23 or ( very recently ) the case where @xmath0 is fixed as @xmath1 diverges .",
    "the original @xcite paper proved consistency of the estimate under mild conditions in the case @xmath23 .",
    "@xcite proved that the mean squared error of a truncated version of the estimator is @xmath36 when @xmath23 and @xmath37 under a condition that is almost equivalent to an exponential tail ; @xcite showed that the bias vanishes asymptotically while the variance is @xmath36 when @xmath23 and @xmath7 is compactly supported and bounded away from zero on its support . very recently , in independent work and under regularity conditions , @xcite derived the asymptotic normality of the estimator when @xmath23 , confirming the suboptimal asymptotic variance in this case .",
    "previous works on the general @xmath0 case include @xcite , where heuristic arguments were presented to suggest the estimator is consistent for general @xmath21 and general fixed @xmath0 and has variance of order @xmath38 for @xmath37 and general fixed @xmath0 . the only work of which we are aware in which @xmath0 is allowed to diverge with @xmath1 is @xcite , where the estimator was shown to be consistent for general @xmath21 .",
    "other very recent contributions include @xcite and @xcite .    in this paper , we substantially weaken regularity conditions compared with these previous works , and show that with an appropriate choice of @xmath0 , efficient entropy estimators can be obtained in arbitrary dimensions , even in cases where the support of the true density is the whole of @xmath2 .",
    "such settings present significant new challenges and lead to different behaviour compared with more commonly - studied situations where the underlying density is compactly supported and bounded away from zero on its support . to gain intuition , consider the following second - order taylor expansion of @xmath39 around a density estimator @xmath40 : @xmath41 when @xmath7 is bounded away from zero on its support",
    ", one can estimate the ( smaller order ) second term on the right - hand side , thereby obtaining efficient estimators of entropy in higher dimensions @xcite ; however , when @xmath7 is not bounded away from zero on its support such procedures are no longer effective . to the best of our knowledge ,",
    "therefore , this is the first time that a nonparametric entropy estimator has been shown to be efficient in multivariate settings for densities having unbounded support .",
    "( we remark that when @xmath37 , the histogram estimator of @xcite is known to be efficient under fairly strong tail conditions . )",
    "although the focus of this work is on efficient estimation , our results have other methodological implications : first , they suggest that prewhitening the data , i.e.  replacing @xmath17 with @xmath42 before computing the estimator , where @xmath43 denotes the sample covariance matrix , can yield substantial bias reduction .",
    "second , they yield asymptotically valid confidence intervals and hypothesis tests in a straightforward manner .",
    "the outline of the rest of the paper is as follows . in section",
    "[ sec : bias ] , we give our main results on the bias of kozachenko  leonenko estimators , and present examples to understand the strength of the conditions .",
    "we also discuss prewhitening the estimators to improve performance .",
    "section  [ sec : var ] is devoted to the study of the variance of the estimators and their asymptotic normality . in section  [ sec : weighted ] , we introduce our weighted kozachenko ",
    "leonenko estimators , and show both how to choose the weights to achieve efficiency , and how to construct confidence intervals for entropy .",
    "proofs of main results are presented in section  [ sec : proof ] with auxiliary material and detailed bounds for various error terms deferred to the appendix .",
    "we conclude the introduction with some notation used throughout the paper . for @xmath44 and @xmath45 , let @xmath46 be the closed euclidean ball of radius @xmath47 about @xmath48 .",
    "we write @xmath49 , @xmath50 and @xmath51 for the operator norm , frobenius norm and determinant , respectively , of @xmath52 . for an @xmath53 dimensional array @xmath54",
    "we write @xmath55 . for a smooth function @xmath56",
    ", we write @xmath57 and @xmath58 for its first , second , and @xmath59 derivatives respectively , @xmath60 for its laplacian , and @xmath61 for its uniform norm . we write @xmath62 to mean that there exists @xmath63 , depending only on @xmath21 and @xmath7 , such that @xmath64 .",
    "in this section we study the bias of the kozachenko  leonenko estimator  . for @xmath65 , we write @xmath66 so that @xmath67 to gain intuition about the bias , we introduce for @xmath44 and @xmath68 , the sequence of distribution functions @xmath69 where @xmath70 further , for @xmath68 , define the limiting distribution function @xmath71 where @xmath72 . that this is the limit distribution for each fixed @xmath0 follows from a poisson approximation to the binomial distribution .",
    "we therefore expect that @xmath73 although we do not explicitly use this approximation in our asymptotic analysis of the bias , it motivates much of our development .",
    "it also explains the reason for using @xmath74 in the definition of @xmath31 , rather than simply @xmath0 .",
    "we will work with the following conditions :    * ( a1)*(@xmath75 ) : :    @xmath7 is bounded , and writing    @xmath76 and    @xmath77 , we have that @xmath7 is    @xmath78 times continuously differentiable and there exist    @xmath79 and a borel measurable function @xmath80    such that for each @xmath81 and    @xmath82 , @xmath83    and    @xmath84    as @xmath85 , for each    @xmath86 . *",
    "( a2)*(@xmath87 ) : :    @xmath88 .",
    "condition  * ( a1)*(@xmath75 ) is discussed in section  [ sec : assumptions ] below .",
    "we are now in a position to state our main result on the bias of the kozachenko ",
    "leonenko estimator .",
    "[ biasthm ] suppose * ( a1)*(@xmath75 ) and * ( a2)*(@xmath87 ) hold for some @xmath89 .",
    "let @xmath90 denote any deterministic sequence of positive integers with @xmath91 as @xmath92 for some @xmath86 . then    *",
    "if @xmath93 and @xmath94 , we have @xmath95 uniformly for @xmath96 as @xmath92 . * if @xmath97 or @xmath98 with either @xmath99 or @xmath100 $ ] , then for every @xmath101 we have @xmath102 uniformly for @xmath96 , as @xmath92 .",
    "it is important to note that the conditions * ( a1)*(@xmath75 ) with @xmath103 together with * ( a2)*(@xmath87 ) for some @xmath94 ensure that the integral in part  ( i ) is finite when @xmath98 ; cf .",
    "proposition  [ prop : moment ] in section  [ sec : biasproof ] .",
    "the following corollary , which is a straightforward extension of theorem  [ biasthm](i ) , simplifies the expression for the dominant contribution to the bias when @xmath27 as @xmath92 .",
    "[ cor : easycor ] assume the conditions of theorem  [ biasthm](i ) , and let @xmath104 denote any deterministic sequence of positive integers with @xmath105 and @xmath106 as @xmath92 .",
    "then @xmath107 uniformly for @xmath108 as @xmath92 .",
    "the proof of theorem  [ biasthm ] is given in section  [ sec : biasproof ] , but we present here some of the main ideas .",
    "first , note that @xmath109 where @xmath110 denotes the density of a @xmath111 random variable at @xmath112 , with @xmath113 .",
    "for @xmath114 and @xmath115 , define @xmath116 . since @xmath117 is a continuous , non - decreasing function of @xmath47",
    ", we can define a left - continuous inverse for @xmath112 by @xmath118 so that @xmath119 if and only if @xmath120 .",
    "we use the approximation @xmath121 for small @xmath122 , which is formalised in lemma  [ lemma : hxinvbounds](ii ) in section  [ sec : biasproof ] .",
    "the proof of theorem  [ biasthm](i ) can be seen as justifying the use of the above approximation in the following : @xmath123 note that @xmath124 , which is negligible compared with the main @xmath125 contribution . in case ( ii ) , the integral is not necessarily finite , so the approximation above is not as helpful , though the proof of the theorem is similar in the two cases .",
    "these heuristics make clear that the function @xmath126 plays a key role in understanding the bias .",
    "this function is in general complicated , though some understanding can be gained from the following simple example , where it can be evaluated explicitly , leading to an exact expression for the bias , even though * ( a1)*(@xmath75 ) does not hold for any @xmath127 .",
    "consider the uniform distribution , @xmath128 $ ] .",
    "for @xmath129 , we have @xmath130 it therefore follows that @xmath131      condition * ( a1)*(@xmath75 ) imposes smoothness and tail constraints on the unknown density  @xmath7 . in particular",
    ", it is reminiscent of more standard hlder smoothness conditions , though it also requires that the partial derivatives of the density vary less where @xmath7 is small . roughly speaking",
    ", it also means that the partial derivatives of the density decay about as fast as the density itself in the tails of the distribution .",
    "moreover , it can be shown that if * ( a1)*(@xmath75 ) holds for a density @xmath132 then it also holds for any density from the location - scale family : @xmath133    in fact , * ( a1)*(@xmath75 ) is satisfied by many standard families of distributions , as indicated by the following examples .",
    "[ eg : normal ] consider the standard @xmath21-dimensional normal distribution , with density function @xmath134 then , for any @xmath135 , @xmath136 for some polynomial @xmath137 of degree @xmath138 with positive coefficients .",
    "moreover , @xmath139 hence , @xmath140 since @xmath141 only when @xmath142 , and since @xmath143 as @xmath144 for all @xmath145 , we may therefore use taylor s theorem and the equivalence of norms on finite - dimensional vector spaces to see that * ( a1)*(@xmath75 ) is satisfied for any @xmath127 with @xmath146 and @xmath147 .",
    "[ eg : t ] consider the standard @xmath21-dimensional @xmath138-distribution with @xmath148 degrees of freedom , with density function @xmath149 in this case , @xmath150 where @xmath137 is a polynomial of degree @xmath138 with positive coefficients .",
    "moreover , @xmath151 it may also be shown that @xmath152 so we may use taylor s theorem to show that * ( a1)*(@xmath75 ) is satisfied for any @xmath127 with @xmath146 and @xmath80 taken be a sufficiently large constant ( which may depend on @xmath153 and @xmath21 ) .",
    "although * ( a1)*(@xmath75 ) is rather general for densities with @xmath154 for all @xmath44 , a class of such densities for which the condition does not hold is those of the form @xmath155 for large @xmath156 , which have highly oscillatory behaviour in the tails . on the other hand , * ( a1)*(@xmath75 )",
    "apparently precludes points @xmath157 with @xmath158 . to provide some guarantees on the performance of kozachenko  leonenko estimators in settings where the density vanishes at certain points",
    ", we now give a very general condition under which our approach to studying the bias can be applied .",
    "[ prop : weakcond ] assume that @xmath7 is bounded , that * ( a2)*(@xmath87 ) holds for some @xmath159 , and let @xmath160 be as in theorem  [ biasthm ] .",
    "let @xmath161 , let @xmath162 , and assume further that there exists @xmath163 such that the function on @xmath164 given by @xmath165 is real - valued .",
    "suppose that @xmath166 is such that @xmath167 as @xmath92 , where @xmath168",
    ". then writing @xmath169 , we have for every @xmath101 that @xmath170 uniformly for @xmath96 .    to aid interpretation of proposition  [ prop : weakcond ] , we first remark that if  * ( a1)(@xmath75 ) * holds , then so does  , with @xmath171 , where @xmath172 is defined in   below .",
    "we can then obtain explicit bounds on the terms in  , which enable us to recover theorem  [ biasthm](ii ) .    for @xmath173 ,",
    "consider the density @xmath174 of the @xmath175 distribution .",
    "then for any @xmath176 , we may take @xmath177\\ ] ] to deduce from proposition  [ prop : weakcond ] that for every @xmath101 , @xmath178    similar calculations show that the bias is of the same order for @xmath111 distributions with @xmath179 .",
    "finally , we show that we can even obtain a small bias in settings where all derivatives of @xmath7 are zero at a point @xmath180 where @xmath158 .",
    "let @xmath181 .",
    "then for any @xmath176 , we may take @xmath182\\ ] ] to deduce from proposition  [ prop : weakcond ] once again that @xmath183 for every @xmath101 .",
    "so far , we have assumed that the kozachenko ",
    "leonenko estimator @xmath31 is applied directly to @xmath184 ; when @xmath98 and our regularity conditions hold , theorem  [ biasthm](i ) then yields that the dominant asymptotic contribution to the bias of @xmath31 is @xmath185 in this section , we consider the possibility of an alternative strategy , namely to make a transformation to the observations , apply the kozachenko ",
    "leonenko estimator to the transformed data , and then correct for the transformation in an appropriate way . recall that for any @xmath186 with @xmath187 , we have @xmath188 this motivates us to consider transformed estimators defined as follows : set @xmath189 , and let @xmath190 if @xmath191 is either orthogonal , or a positive scalar multiple of the identity , then @xmath192 and we have gained nothing , so we should look for transformations that are not of this type .",
    "in fact , under the conditions of theorem  [ biasthm](i ) , we see that the dominant asymptotic contribution to the bias of @xmath193 is @xmath194 in general , it is hard to compare the magnitudes of these two expressions ( [ eq : originalbias ] ) and ( [ eq : lineartrans ] ) .",
    "however , in certain specific cases , an explicit comparison is possible .",
    "for instance , suppose that we can write @xmath195 for some positive definite @xmath196 , where either @xmath197 for some univariate density @xmath198 , or can be written in the form @xmath199 for some @xmath200 and @xmath201 .",
    "then , as shown in propositions  [ prop : product ] and  [ prop : spherical ] in appendix  [ appendix : conds ] , we have @xmath202 from the expressions above , we see that the linear transformation strategy results in reduced asymptotic bias whenever @xmath203 . in fact , by the am  gm inequality ,",
    "@xmath204 is minimised when the eigenvalues of @xmath205 are equal .",
    "one option for a practical data - driven approach is to ` prewhiten ' the data by setting @xmath206 , where @xmath207 is the sample covariance matrix and @xmath208 is the sample mean .    in order to investigate the empirical performance of the prewhitened estimator @xmath193 with @xmath206 , we generated samples of size @xmath209 from the distributions @xmath210 and @xmath211 , where @xmath212 figures [ fig : prew1 ] and [ fig : prew2 ] present the mean squared errors of both the prewhitened and orignal kozachenko  leonenko estimator based on @xmath213 repetitions , suggesting that the prewhitened estimator can yield significant improvements in performance for appropriate choices of @xmath0 .     ]     ]",
    "we now study the asymptotic variance of kozachenko  leonenko estimators under the assumption that the tuning parameter @xmath0 is diverging with @xmath1 ( which is necessary for efficiency ) .",
    "[ varthm ] assume * ( a1)*(2 ) and that * ( a2)*(@xmath87 ) holds for some @xmath214 .",
    "let @xmath215 and @xmath216 denote any two deterministic sequences of positive integers with @xmath217 , with @xmath218 and with @xmath219 , where @xmath220 then @xmath221 as @xmath92 , uniformly for @xmath222 .",
    "the proof of this theorem is lengthy , and involves many delicate error bounds , so we outline the main ideas here .",
    "first , we argue that @xmath223 where we hope to exploit the fact that @xmath224 .",
    "the main difficulties in the argument are caused by the fact that handling the covariance above requires us to study the joint distribution of @xmath225 , and this is complicated by the fact that @xmath226 may be one of the @xmath0 nearest neighbours of @xmath227 or vice versa , and more generally , @xmath227 and @xmath226 may have some of their @xmath0 nearest neighbours in common .",
    "dealing carefully with the different possible events requires us to consider separately the cases where @xmath228 is small and large , as well as the proximity of @xmath226 to @xmath227 .",
    "finally , however , we can apply a normal approximation to the relevant multinomial distribution ( which requires that @xmath27 ) to deduce the result .",
    "we remark that under stronger conditions on @xmath0 , it should also be possible to derive the same conclusion about the asymptotic variance of @xmath31 while only assuming similar conditions on the density to those required in proposition  [ prop : weakcond ] , but we do not pursue this here .    a straightforward consequence of theorem  [ varthm ] is the asymptotic normality of kozachenko  leonenko estimators .",
    "[ clt ] assume that @xmath3 and the conditions of theorem [ varthm ] are satisfied .",
    "if @xmath229 , we additionally assume that @xmath230",
    ". then @xmath231 and @xmath232 as @xmath92 , uniformly for @xmath233 .",
    "as was mentioned in the introduction , the asymptotic variance in theorem  [ clt ] is best possible .",
    "although the results of sections  [ sec : bias ] and  [ sec : var ] reveal that the estimators @xmath31 can not be efficient when @xmath234 , the insights provided motivate us to propose and analyse alternative estimators of @xmath18 that can be written as weighted averages of kozachenko  leonenko entropy estimators . in this section we show that , for carefully chosen weights , they have reduced asymptotic bias while maintaining the same asymptotic variance as kozachenko  leonenko estimators , and as such can achieve efficiency in higher dimensions . for @xmath235 such that @xmath236 , we define the weighted estimator @xmath237 where @xmath238 . then , under the conditions of theorem  [ biasthm](i ) , we will show that for appropriate choices of @xmath239 and for every @xmath101 , @xmath240 as @xmath92 .",
    "this facilitates a choice of @xmath0 and @xmath241 such that @xmath242 , and results in an estimator that has bias @xmath243 for @xmath244 and , under the stronger assumption that @xmath245 , for @xmath246 . in propostion  [ weightedbiasprop ] in section  [ sec : weightedproof ] we will see that , under stronger assumptions on the smoothness of @xmath7 and for @xmath0 diverging to infinity , the bias of @xmath31 can be expanded in powers of @xmath125 .",
    "it then turns out that the weights @xmath239 we should consider are elements of the following set , where the restrictions on the support of @xmath239 are convenient for our analysis .",
    "let @xmath247 and let @xmath248 for @xmath249 .",
    "a sufficient condition for @xmath250 is that the @xmath251 matrix @xmath252 , with @xmath253 entry @xmath254 is invertible , as then @xmath255 gives the non - zero entries of a vector @xmath256 , where @xmath257 .",
    "now define @xmath258 to have @xmath253 entry @xmath259 .",
    "since @xmath260 as @xmath261 for @xmath262 , we have @xmath263 as @xmath27 .",
    "now , @xmath191 is a vandermonde matrix ( depending only on @xmath21 ) and as such has determinant @xmath264 hence , by the continuity of the determinant and eigenvalues of a matrix , we have that there exists @xmath265 such that , for @xmath266 , the matrix @xmath252 is invertible and @xmath267 where @xmath268 denotes the eigenvalue of a matrix with smallest absolute value .",
    "thus , for each @xmath266 , there exists @xmath256 satisfying @xmath269 .",
    "the following theorem establishes the efficiency of @xmath270 for arbitrary @xmath21 .",
    "the main ideas of the proof are provided in section  [ sec : weightedproof ] , with details deferred to appendix  [ appendix : weighted ] .",
    "[ weightedclt ] assume * ( a1)*(@xmath75 ) , * ( a2)*(@xmath87 ) , and that the conditions on @xmath271 of theorem  [ varthm ] hold .",
    "assume further that @xmath272 for large @xmath1 and that @xmath273 .",
    "* if @xmath274 , @xmath275 and @xmath276 , then @xmath277 uniformly for @xmath233 .",
    "* if @xmath234 , @xmath278 $ ] and @xmath275 , then @xmath279 uniformly for @xmath233 .",
    "theorem  [ weightedclt ] generalises theorem  [ clt ] , since we can simply that @xmath280 .",
    "we remark that the level of smoothness we require for efficiency in theorem  [ weightedclt](i ) , namely @xmath274 , is more than is needed for the two - stage estimator of @xcite in the case where @xmath7 is compactly supported and bounded away from zero on its support , where @xmath281 suffices . as alluded to in the introduction",
    ", the fact that the function @xmath282 is non - differentiable at @xmath283 means that the entropy functional is no longer smooth when @xmath7 has full support , so the arguments of @xcite can no longer be applied and very different behaviour may occur @xcite .",
    "theorem [ weightedclt](i ) of the previous subsection can be used to construct asymptotic confidence intervals for @xmath39 based on @xmath270 in the standard way , given a consistent estimator @xmath284 of @xmath285 ; cf .",
    "also @xcite . since @xmath286 , it is sufficient to find a consistent estimator of @xmath287 , since we may simply use @xmath288 to estimate @xmath289 .",
    "for this purpose , it is natural to consider the estimator @xmath290 an alternative , motivated by   in the proof of theorem  [ varthm ] , is to define @xmath291 ; as shown in proposition  [ consistency ] below , these estimators have the same limiting behaviour .    [ consistency ]",
    "assume that the conditions of theorem  [ varthm ] hold .",
    "then @xmath292 uniformly for @xmath233 .",
    "the same conclusion holds for @xmath293 .",
    "the following corollary is immediate .",
    "[ cor : conf ] assume that the conditions of theorem  [ weightedclt](i ) hold and set @xmath294 .",
    "for any @xmath295 , let @xmath296 denote the @xmath297th quantile of the standard normal distribution .",
    "then @xmath298 as @xmath92 , uniformly for @xmath233 .",
    "corollary  [ cor : conf ] guarantees that @xmath299 provides an asymptotically valid confidence interval for @xmath39 , of asymptotically minimal width .",
    "the proof of theorem  [ biasthm ] relies on the following two auxiliary results , whose proofs are given in appendix  [ appendix : auxiliary ] .",
    "[ prop : moment ] let @xmath7 be a bounded density satisfying * ( a2)*(@xmath87 ) and let @xmath300 be a measurable function with @xmath301 as @xmath85 for every @xmath101 .",
    "then @xmath302 for all @xmath303 $ ] .",
    "recall the definition of @xmath126 in  .",
    "the first part of lemma  [ lemma : hxinvbounds ] below provides crude but general bounds ; the second gives much sharper bounds in a more restricted region .",
    "[ lemma : hxinvbounds ]    a.   assume that @xmath6 has a bounded density @xmath7 and that * ( a2)*(@xmath87 ) holds .",
    "then for every @xmath112 and @xmath114 , @xmath304 b.   assume * ( a1)*(@xmath75 ) , and let @xmath305 , @xmath166 be such that @xmath306 where @xmath80 is taken from * ( a1)*(@xmath75 ) .",
    "then there exists @xmath63 such that for all @xmath1 sufficiently large , @xmath307 and @xmath308 , we have @xmath309 where @xmath310 and @xmath311 if @xmath75 is an even integer .",
    "moreover , if @xmath103 , then @xmath312    we are now in a position to prove theorem  [ biasthm ] .",
    "\\(i ) define @xmath313 where @xmath80 is as in assumption * ( a1)*(@xmath75 ) , let @xmath314 and let @xmath315 . recall that @xmath316 and let @xmath317 the proof is based on the fact that @xmath318 and lemma  [ lemma : hxinvbounds](ii ) , which allow us to make the transformation @xmath319 . writing @xmath320 for remainder terms to be bounded at the end of the proof , we can write @xmath321 proposition  [ prop : moment ] tells us that @xmath322 uniformly for @xmath96 as @xmath92 . noting that @xmath323",
    ", it now remains to bound @xmath324 . henceforth , to save repetition ,",
    "we adopt without further mention the convention that whenever an error term inside @xmath325 or @xmath326 depends on @xmath0 , this error is uniform for @xmath96 ; thus @xmath327 as @xmath92 means @xmath328 as @xmath92 .    _",
    "to bound @xmath329_. first note that for any @xmath330 , @xmath331 where the conclusion follows from hlder s inequality as in the proof of proposition  [ prop : moment ] in appendix  [ appendix : auxiliary ] .",
    "now , from lemma  [ lemma : hxinvbounds](i ) , @xmath332 moreover , @xmath333 when @xmath334 , so we deduce that for each @xmath101 , @xmath335 as @xmath92 . since @xmath336 when @xmath94 , this bound on @xmath329 is sufficiently tight .    _ to bound @xmath337 . _ for random variables @xmath338 and @xmath339 we have that for every @xmath86 @xmath340 where the final equality follows from standard bounds on the left - hand tail of the binomial distribution ( see , e.g.  @xcite , equation  ( 6 ) , page 440 ) .",
    "we therefore deduce from   and the cauchy ",
    "schwarz inequality that for each @xmath145 , @xmath341 ( with further work , it can in fact be shown that @xmath342 , but this is not necessary for our overall conclusion . )    _ to bound @xmath343 .",
    "_ we can write @xmath344 \\mathrm{b}_{k , n - k}(s ) \\ , ds \\,dx \\\\      & = : r_{31}+r_{32},\\end{aligned}\\ ] ] say .",
    "now , note that @xmath345 } \\sup_{x \\in \\mathcal{x}_n } \\frac{g_*^{d\\beta}(x)s}{f(x ) } \\rightarrow 0.\\ ] ] it follows by lemma  [ lemma : hxinvbounds](ii ) that there exists @xmath346 such that for @xmath347 , @xmath96 , @xmath348 and @xmath308 , taking @xmath349 , @xmath350 thus , for @xmath347 and @xmath96 , using the fact that @xmath351 for @xmath352 , @xmath353 \\mathrm{b}_{k , n - k}(s ) \\,ds \\,dx \\\\      & \\leq \\frac { \\gamma(k+4/d ) \\gamma(n)}{2(d+2)^{2}v_d^{4/d } \\gamma(k ) \\gamma(n+4/d ) \\delta_n^{2/d } } \\int_{\\mathcal{x}_n } g_*(x)^2 f(x)^{1 - 2/d } \\ , dx \\\\      & \\hspace{3 cm } + \\frac{2c^{2 } \\gamma(k+(4 + 2 \\eta)/d ) \\gamma(n ) } { \\gamma(k ) \\gamma(n+(4 + 2 \\eta)/d ) \\delta_n^{(2 + 2 \\eta)/d } } \\int_{\\mathcal{x}_n } g_*(x)^2 f(x)^{1 - 2/d } \\,dx.\\end{aligned}\\ ] ] on the other hand , we also have for @xmath347 and @xmath96 that @xmath354 it follows by proposition  [ prop : moment ] and the fact that @xmath355 that @xmath356    _ to bound @xmath357_. consider the random variable @xmath338 .",
    "then , using we conclude that for every @xmath101 , @xmath358    _ to bound @xmath359 .",
    "_ it follows from proposition  [ prop : moment ] and the fact that @xmath360 as @xmath92 that for each @xmath101 , @xmath361    _ to bound @xmath362_. by stirling s formula , @xmath363 as required .",
    "\\(ii ) the bias calculation in this setting is very similar , but we approximate @xmath364 simply by @xmath365 .",
    "writing @xmath366 for the modified error terms , we obtain @xmath367 here , @xmath368 , and @xmath369 , for every @xmath101 in both cases . on the other hand , @xmath370 for every @xmath101 .",
    "similarly , @xmath371 finally , @xmath372 for every @xmath101 . this concludes the proof .      to deal with the integrals over @xmath373",
    ", we first observe that by  , for every @xmath101 , @xmath374 moreover , @xmath375 for every @xmath145 . now a simpler form of lemma  [ lemma : hxinvbounds](ii ) states that there exists @xmath376 such that for @xmath377 and @xmath308 , we have @xmath378 } \\bigl|v_df(x)h_x^{-1}(s)^d - s\\bigr| \\leq \\frac{2dv_d^{-\\tilde{\\beta}/d}}{d+\\tilde{\\beta}}s^{1+\\tilde{\\beta}/d}\\frac{c_{n,\\tilde{\\beta}}(x)}{f(x)^{1+\\tilde{\\beta}/d}}.\\ ] ] it follows from  ,  ,   and an almost identical argument to that leading to   that for every @xmath101 , @xmath379 as required .",
    "since this proof is long , we focus here on the main argument , and defer proofs of bounds on the many error terms to appendix  [ appendix : varproof ] .",
    "we use the same notation as in the proof of theorem  [ biasthm ] , except that we change the definition of @xmath172 so that @xmath380 .",
    "we write @xmath381 for this newly - defined @xmath172 .",
    "similar to the proof of theorem  [ biasthm ] , all error terms inside @xmath325 and @xmath326 that depend on @xmath0 are uniform for @xmath233 . first note that @xmath382 our first claim is that for every @xmath101 , @xmath383\\ ] ] as @xmath92 .",
    "the proof of this claim uses similar methods to those in the proof of theorem  [ biasthm ] .",
    "in particular , writing @xmath384 for remainder terms to be bounded later , we have @xmath385 \\,dx + \\sum_{i=1}^4 s_i \\nonumber \\\\      & = \\int_\\mathcal{x } f(x ) \\log^2 f(x ) \\,dx + \\sum_{i=1}^5 s_i + \\frac{1}{k}\\{1+o(1)\\ } , \\end{aligned}\\ ] ] as @xmath92 . in appendix",
    "[ appendix : s ] , we show that for every @xmath101 , @xmath386\\ ] ] as @xmath92 .",
    "the next step of our proof consists of showing that for every @xmath101 , @xmath387 as @xmath92 .",
    "define @xmath388 so that @xmath389 writing @xmath390 , we therefore have that @xmath391 to deal with the first term in  , we make the substitution @xmath392 and let @xmath393 . writing @xmath394 for remainder terms to be bounded later , for every @xmath101 , @xmath395 in appendix  [ appendix : t ] , we show that for every @xmath101 , @xmath396 as @xmath92 .",
    "we now deal with the second term in  . by similar arguments to those used previously , for every @xmath101 , @xmath397 in appendix  [ appendix : u ]",
    ", we show that for every @xmath101 , @xmath398.we also require some further notation .",
    "let @xmath399 denote the conditional distribution function of @xmath225 given @xmath400 .",
    "let @xmath401 , and let @xmath402 so that @xmath403 and @xmath404 for every @xmath101 .",
    "for pairs @xmath405 with @xmath406 and @xmath407 , let @xmath408 , and write @xmath409 write @xmath410 with @xmath411 for @xmath412 , let @xmath413 denote the distribution function of a @xmath414 random vector at @xmath415 , and let @xmath416 denote the standard univariate normal distribution function .",
    "writing @xmath417 for remainder terms to be bounded later , we have @xmath418 the proof is completed by showing in appendix  [ appendix : w ] that for every @xmath101 , @xmath419 as @xmath92 .",
    "recall from the introduction that @xmath420 by the central limit theorem applied to @xmath421 , for the first part it suffices by slutsky s theorem to show that @xmath422 as @xmath92 . by theorem  [ biasthm ] , under the conditions imposed , we know that @xmath423 as @xmath92 . moreover , from   in the proof of theorem  [ varthm ] , we have that @xmath424 as @xmath92 .",
    "hence @xmath425 as @xmath92 .",
    "now by similar , but simpler , methods to those employed in the proof of theorem  [ biasthm ] , we have that for all @xmath145 , @xmath426 thus , @xmath427 as @xmath92 and the first result follows .",
    "the second part is an immediate consequence of   and  , together with an application of the cauchy ",
    "schwarz inequality .",
    "the first step towards proving theorem  [ weightedclt ] is to gain a higher - order understanding of the bias of kozachenko  leonenko estimators , which is provided by the following proposition . the proof is given in appendix  [ appendix : weighted ] .",
    "[ weightedbiasprop ] suppose that * ( a1)*(@xmath75 ) and * ( a2)*(@xmath87 ) hold for some @xmath428 .",
    "let @xmath90 denote any deterministic sequence of positive integers with @xmath429 as @xmath92 for some @xmath430 .",
    "then there exist @xmath431 , depending only on @xmath7 and @xmath21 , such that for each @xmath101 , @xmath432 as @xmath92 , uniformly for @xmath96 , where @xmath433 if @xmath434 , and @xmath435 if @xmath75 is an even integer",
    ".    note that in the setting of theorem  [ biasthm](i ) we have @xmath436 proposition  [ weightedbiasprop ] provides justification for our choice of @xmath437 .",
    "the following result on the variance of @xmath270 is proved similarly to theorem  [ varthm ] , and the proof is given in appendix  [ appendix : weighted ] .",
    "[ weightedvarprop ] assume that the conditions of theorem  [ varthm ] hold . for @xmath1 sufficiently large , find @xmath438 such that @xmath439 .",
    "then @xmath440 as @xmath92 , uniformly for @xmath233 .",
    "we are now in a position to prove our main result on the theoretical properties of our weighted kozachenko  leonenko estimators .",
    "\\(i ) by proposition  [ weightedbiasprop ] and the fact that @xmath441",
    ", we have @xmath442 under our conditions on @xmath443 . by proposition  [ weightedvarprop ]",
    "we have @xmath444 .",
    "the results therefore follow by very similar arguments to those presented in theorem  [ clt ] .",
    "\\(ii ) by proposition  [ weightedbiasprop ] again , we have @xmath445 similarly , by proposition  [ weightedvarprop ] , @xmath446 the result follows by an application of chebychev s inequality .",
    "similar to  , we may write @xmath447 thus , mimicking arguments in the proofs of theorems  [ biasthm ] and  [ varthm ] , for any @xmath448 , @xmath449 as @xmath92 . hence , by the cauchy ",
    "schwarz inequality , @xmath450 \\rightarrow \\mathbb{e}\\{\\log^m f(x_1)\\}\\end{aligned}\\ ] ] as @xmath92 .",
    "thus @xmath451 . similarly , from   and cauchy  schwarz , @xmath452 as @xmath92 .",
    "we conclude that @xmath453 .",
    "the fact that @xmath454 is also asymptotically unbiased can be deduced from   ( and the subsequent bounds on the remainder terms ) in the proof of theorem [ varthm ] .",
    "the first conclusion follows , and the second can be deduced from the first , together with the fact that @xmath455 as @xmath27 .    * acknowledgements : * the second author is grateful to sebastian nowozin for introducing him to this problem , and to grard biau for helpful discussions",
    ".    99    beirlant , j. , dudewicz , e.  j. , gyrfi , l. , and van der meulen , e.  c. ( 1997 ) nonparametric entropy estimation : an overview .",
    "_ , * 6 * , 1739 .",
    "biau , g. and devroye , l. ( 2015 ) _ lectures on the nearest neighbor method_. springer , new york .",
    "cai , t. t. and low , m. g. ( 2011 ) testing composite hypotheses , hermite polynomials and optimal estimation of a nonsmooth functional .",
    "_ , * 39 * , 10121041 .",
    "cressie , n. ( 1976 ) on the logarithms of high - order spacings .",
    "_ biometrika _ , * 63 * , 343355 .",
    "delattre , s. and fournier , n. ( 2016 ) on the kozachenko ",
    "leonenko entropy estimator .",
    "available at ` arxiv:1602.07440 ` .",
    "gao , w. , oh , s. and viswanath , p. ( 2016 ) demystifying fixed @xmath0-nearest neighbor information estimators .",
    "available at ` arxiv:1604.03006 ` .",
    "goria , m.  n. , leonenko , n.  n. , mergel , v.  v. and novi inverardi , p.  l. ( 2005 ) a new class of random vector entropy estimators and its applications in testing statistical hypotheses . _ j. nonparametr .",
    "_ , * 17 * , 277297 .",
    "gtze , f. ( 1991 ) on the rate of convergence in the multivariate clt .",
    "_ , * 19 * , 724739",
    ".    hall , p. and morton , s.  c. ( 1993 ) on the estimation of entropy .",
    "_ ann . inst .",
    "_ , * 45 * , 6988 .",
    "ibragimov , i. a. and khasminskii , r. z. ( 1991 ) asymptotically normal families of distributions and efficient estimation .",
    "statist . _ ,",
    "* 19 * , 16811724 .",
    "kozachenko , l.  f. and leonenko , n.  n. ( 1987 ) sample estimate of the entropy of a random vector .",
    "_ , * 23 * , 95101 .",
    "kwak , n. and choi , c. ( 2002 ) input feature selection by mutual information based on parzen window .",
    "_ ieee trans . pattern anal .",
    "_ , * 24 * , 16671671 .",
    "laurent , b. ( 1996 ) efficient estimation of integral functionals of a density .",
    "_ , * 24 * , 659681 .",
    "lepski , o. , nemirovski , a. and spokoiny , v. ( 1999 ) on estimation of the @xmath456 norm of a regression function .",
    "fields _ , * 113 * , 221253 .",
    "levit , b. ya .",
    "( 1978 ) asymptotically efficient estimation of nonlinear functionals .",
    "_ , * 14 * , 204209 .",
    "miller , e.  g. and fisher , j.  w. ( 2003 ) ica using spacings estimates of entropy .",
    "_ j. mach .",
    "_ , * 4 * , 12711295 .",
    "mnatsakanov , r.  m. , misra , n. , li , s. and harner , e.  j. ( 2008 ) @xmath457-nearest neighbor estimators of entropy .",
    "methods statist . _ , * 17 * , 261277 .",
    "paninski , l. ( 2003 ) estimation of entropy and mutual information .",
    "_ neural comput .",
    "_ , * 15 * , 11911253 .",
    "shorack , g.  r. and wellner , j.  a. ( 2009 ) _ empirical processes with applications to statistics_. .",
    "singh , h. , misra , n. , hnizdo , v. , fedorowicz , a. and demchuk , e. ( 2003 ) nearest neighbor estimates of entropy .",
    "_ , * 23 * , 301321 .",
    "singh , s. and pczos , b. ( 2016 ) analysis of @xmath0 nearest neighbor distances with application to entropy estimation .",
    "available at ` arxiv:1603.08578 ` .",
    "tsybakov , a.  b. and van der meulen , e.  c. ( 1996 ) root-@xmath1 consistent estimators of entropy for densities with unbounded support . _",
    "_ , * 23 * , 7583 .",
    "vasicek , o. ( 1976 ) a test for normality based on sample entropy .",
    "_ j. r. stat .",
    "_ , * 38 * , 5459 .",
    "fix @xmath303 $ ] , and let @xmath458 .",
    "we first claim that given any @xmath101 , there exists @xmath459 such that @xmath460 . to see this , observe that there exists @xmath461 $ ] such that @xmath462 for @xmath463 .",
    "but then @xmath464 } \\delta^\\epsilon \\sup_{x : f(x ) \\geq \\delta } g(x ) \\leq \\max\\bigl\\{1,c^\\epsilon \\sup_{x : f(x ) \\geq \\delta_0 } g(x)\\bigr\\ } \\leq c^\\epsilon \\delta_0^{-\\epsilon}.\\ ] ] hence , defining @xmath465 , we have @xmath466 which establishes our first claim .",
    "now choose @xmath467 and let @xmath468 .",
    "then , by hlder s inequality , @xmath469 since @xmath470 .",
    "\\(i ) the lower bound is immediate from the fact that @xmath471 for any @xmath45 . for the upper bound ,",
    "observe that by markov s inequality , for any @xmath45 , @xmath472 the result follows on substituting @xmath473 for @xmath112 .",
    "\\(ii ) we first prove this result in the case @xmath474 $ ] , giving the stated form of @xmath475 .",
    "let @xmath476 , and let @xmath477 .",
    "now , by * ( a1)*(@xmath75 ) and the mean value theorem , we have for @xmath478 that @xmath479 it is convenient to write @xmath480 then , provided @xmath481 $ ] , we have @xmath482 now , by our hypothesis , we know that @xmath483 as @xmath92 .",
    "hence there exists @xmath346 such that for all @xmath347 , all @xmath307 and all @xmath308 , we have @xmath484 moreover , there exists @xmath485 such that for all @xmath486 , all @xmath307 and all @xmath308 we have @xmath487 finally , we can choose @xmath488 such that @xmath489 and such that @xmath490 for @xmath491 .",
    "it follows that for @xmath492 , @xmath307 and @xmath308 , we have that @xmath481 $ ] and @xmath493   \\geq s.\\end{aligned}\\ ] ] the lower bound is proved by very similar calculations , and the result for the case @xmath474 $ ] follows .",
    "the general case can be proved using very similar arguments , and is omitted for brevity .",
    "let @xmath7 be a density of the form @xmath195 for some positive definite @xmath196 and density @xmath132 , and let @xmath494 .",
    "suppose further that @xmath495 for some invertible @xmath52 .",
    "then @xmath496 thus , provided @xmath497 , we have @xmath498 this is proportional to @xmath499 as a function of @xmath500 if there exists @xmath501 such that @xmath502 where @xmath503 is the kronecker delta . in the next two propositions , we provide two simple , but reasonably general , conditions under which   holds .",
    "[ prop : product ] suppose that @xmath98 and that @xmath132 satisfies * ( a1)*(@xmath75 ) and * ( a2)*(@xmath87 ) for some @xmath504 and @xmath94 .",
    "suppose further that @xmath505 for some density @xmath198 on @xmath506",
    ". then holds .    for @xmath507",
    ", we have @xmath508 we may integrate by parts to write , for any @xmath509 , @xmath510 since @xmath132 satisfies * ( a1)*(@xmath75 ) for @xmath504 , it must be the case that @xmath198 has a bounded second derivative and hence @xmath198 must satisfy @xmath511 as @xmath512 .",
    "hence , taking @xmath513 and @xmath514 in we see that @xmath515 when @xmath516 , we find that @xmath517 is independent of @xmath518 , as required .    [",
    "prop : spherical ] suppose that @xmath98 and that @xmath132 satisfies * ( a1)*(@xmath75 ) and * ( a2)*(@xmath87 ) for some @xmath504 and @xmath94 .",
    "suppose further that @xmath519 for some function @xmath201 and some @xmath200 , where @xmath520",
    ". then ( [ d2int ] ) holds .    for @xmath521 ,",
    "@xmath507 and @xmath522 , @xmath523 moreover , for @xmath524 , @xmath525 since this mixed second partial derivative is an odd function of both @xmath526 and @xmath527 when @xmath507 , it follows that @xmath515 finally , when @xmath516 , @xmath517 is independent of @xmath518 , so the claim follows .",
    "_ to bound @xmath532 .",
    "_ we have @xmath533 it therefore follows from lemma  [ lemma : hxinvbounds](ii ) that for every @xmath101 , latexmath:[\\ ] ] by similar means we can establish the same bound for the approximation of @xmath630 by @xmath685 .    to conclude the proof , we write @xmath759 , where @xmath760 and deal with these two regions separately . by slepian s inequality , @xmath761 for all @xmath762 and @xmath138 , so by  , for every @xmath145 , @xmath763 by lemma  [ lemma : hxinvbounds](ii ) we have , uniformly in @xmath764 , @xmath765 with similar bounds holding for @xmath679 and @xmath766 .",
    "we therefore have that for each @xmath101 , @xmath767 a corresponding lower bound of the same order for the left - hand side of   follows from   and the fact that @xmath768 uniformly in @xmath412 .",
    "it now follows from  , , and that for each @xmath145 @xmath769 as required .",
    "the proof of the proposition is very similar to the proof of theorem  [ biasthm ] , with the main difference being in bounding the error corresponding to @xmath343 . in this case",
    "we just need to bound the error in a higher - order taylor expansion of @xmath770 this can be done using lemma  [ lemma : hxinvbounds](ii ) ; we omit the details for brevity .",
    "we first study the diagonal terms in the standard expansion of the variance of @xmath270 , and claim that @xmath771 in the proof of theorem  [ varthm ] we studied the terms with @xmath772 and showed that , for @xmath518 with @xmath773 , @xmath774 as @xmath92 . for @xmath775 , using similar arguments to those used in the proof of theorem  [ biasthm ] , we have that @xmath776 as @xmath92 .",
    "now   follows on noting that @xmath777 and each @xmath239 has at most @xmath21 non - zero entries .",
    "we now study the off - diagonal terms in the expansion of the variance of @xmath270 , and claim that @xmath778 as @xmath92 . in light of",
    ", it is sufficient to show that @xmath779 as @xmath92 .",
    "we suppose here that @xmath780 and @xmath781 ; the @xmath772 case is dealt with in the proof of theorem  [ varthm ] . we broadly follow the corresponding part of the proof of theorem  [ varthm ] , though we require some new ( similar ) notation .",
    "let @xmath782 denote the conditional distribution function of @xmath783 given @xmath400 and let @xmath784 denote the conditional distribution function of @xmath785 given @xmath786 .",
    "let @xmath787 let @xmath788 , and let @xmath789 for pairs @xmath405 with @xmath790 and @xmath791 , let @xmath792 and write @xmath793 also write @xmath794 where @xmath795 .",
    "writing @xmath796 for remainder terms to bounded later , we have @xmath797 as @xmath92 .",
    "the final equality follows by the fact that , for borel measurable sets @xmath798 , @xmath799      _ to bound @xmath802 _ : similar to our work used to bound @xmath609 , we may show that @xmath803 as @xmath92 , for fixed @xmath804 . also , @xmath805 as @xmath92 . using these facts and very similar arguments to those used to bound @xmath609 we have @xmath806      _ to bound @xmath809 _ : let @xmath810 , where @xmath811 .",
    "further , let @xmath812 then , as in  , we have @xmath813 we now proceed to approximate @xmath814 by @xmath815 and @xmath816 by @xmath817 .",
    "this is rather similar to the corresponding approximation in the bounds on @xmath673 , so we only present the main differences .",
    "first , let @xmath818 we also define @xmath819 and set @xmath820 .",
    "our aim now is to provide a bound on @xmath821 . by the smoothness of the function @xmath822 and * ( a1)*(2 )",
    "we have that @xmath823 for @xmath824 . from this and similar bounds to  , we find that @xmath825",
    ". we therefore have @xmath826 which is as in the @xmath827 case except with the factor of @xmath828 missing .",
    "note now that @xmath829 using  , similar bounds to   and the same arguments as leading up to , @xmath830 where @xmath831 and @xmath832 now let @xmath833 and @xmath834 . similarly to  , we have @xmath835 similarly to the arguments leading up to  , it follows that @xmath836 where the powers on the log factors are smaller because of the absence of the factor of @xmath837 in  .",
    "the remainder of the work required to bound @xmath809 is very similar to the work done from   to  , using also  .",
    "we then have @xmath838 as required ."
  ],
  "abstract_text": [
    "<S> many statistical procedures , including goodness - of - fit tests and methods for independent component analysis , rely critically on the estimation of the entropy of a distribution . in this paper , </S>",
    "<S> we seek entropy estimators that are efficient in the sense of achieving the local asymptotic minimax lower bound . to this end </S>",
    "<S> , we initially study a generalisation of the estimator originally proposed by @xcite , based on the @xmath0-nearest neighbour distances of a sample of @xmath1 independent and identically distributed random vectors in @xmath2 . </S>",
    "<S> when @xmath3 and provided @xmath4 ( as well as other regularity conditions ) , we show that the estimator is efficient ; on the other hand , when @xmath5 , a non - trivial bias precludes its efficiency regardless of the choice of @xmath0 . </S>",
    "<S> this motivates us to consider a new entropy estimator , formed as a weighted average of kozachenko  leonenko estimators for different values of @xmath0 . </S>",
    "<S> a careful choice of weights enables us to obtain an efficient estimator in arbitrary dimensions , given sufficient smoothness . </S>",
    "<S> in addition to the new estimator proposed and theoretical understanding provided , our results also have other methodological implications ; in particular , they motivate the prewhitening of the data before applying the estimator and facilitate the construction of asymptotically valid confidence intervals of asymptotically minimal width . </S>"
  ]
}