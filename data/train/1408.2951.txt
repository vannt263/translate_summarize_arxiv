{
  "article_text": [
    "suppose that we have a matrix observation @xmath0 .",
    "here we use the notation of @xcite for matrix - variate normal distributions : @xmath1 indicates that @xmath2 has a probability density @xmath3 with respect to the lebesgue measure on @xmath4 , where @xmath5 , @xmath6 is the mean matrix , @xmath7 is the covariance matrix for rows , and @xmath8 is the covariance matrix for columns . here , we indicate the size of a matrix @xmath9 by writing @xmath10 .",
    "the vectorization @xmath11 of @xmath12 satisfies @xmath13 where @xmath14 denotes the kronecker product , see @xcite . here , the vectorization of @xmath10 is the @xmath15 vector defined by @xmath16 and the kronecker product @xmath17 of two matrices @xmath18 and @xmath19 is the @xmath20 matrix defined by @xmath21 matrix - variate normal distributions appear naturally when we consider linear regression problems that have several dependent variables ( multivariate linear regression ) .",
    "we consider the prediction of @xmath22 by a predictive density @xmath23 .",
    "we evaluate predictive densities by the kullback ",
    "leibler divergence @xmath24 as a loss function",
    ". the kullback  leibler risk function of a predictive density @xmath23 is @xmath25 \\nonumber \\\\      & = \\int \\int p(y|m ) \\widetilde{p}(\\widetilde{y } | m ) \\log \\frac{\\widetilde{p}(\\widetilde{y } | m)}{\\hat{p } ( \\widetilde{y } | y ) } { \\rm d } y { \\rm d } \\widetilde{y}.\\end{aligned}\\ ] ] we consider bayesian predictive densities with a prior @xmath26 : @xmath27    in the following , we assume that @xmath28 .",
    "when @xmath29 , this problem reduces to the prediction of vector - variate normal model @xmath30 .",
    "this problem has been studied by several authors under the assumption that @xmath31 .",
    "first , the cases where @xmath32 is proportional to @xmath33 were investigated .",
    "@xcite gave the analytical form of bayesian predictive densities based on the stein prior @xmath34 and proved that bayesian predictive densities based on the stein prior dominate those based on the jeffreys prior under the kullback  leibler risk .",
    "since the stein prior puts more weight near the origin , the amount of risk reduction is large when the true value of @xmath35 is near the origin .",
    "@xcite generalized this result and proved that bayesian predictive densities based on superharmonic priors dominate those based on the jeffreys prior under the kullback  leibler risk .",
    "next , @xcite and @xcite considered the cases where @xmath32 is not necessarily proportional to @xmath33 .",
    "bayesian predictive densities based on superharmonic priors dominate those based on the uniform prior under the kullback  leibler risk also in this general situation . @xcite and @xcite applied their results to linear regression problems .",
    "now , since matrix - variate normal distributions are special cases of vector - variate normal distributions as in , the above results also apply to the matrix - variate normal distributions . in this paper , we propose superharmonic priors that shrink the singular values of the mean matrix parameters @xmath36 . from the results of @xcite , bayes estimators based on the introduced priors dominate the maximum likelihood estimator and are thus minimax . furthermore , bayesian predictive densities based on the introduced priors dominate those based on the uniform prior and are thus minimax .",
    "the amount of risk reduction is large when the true value of the mean matrix parameter @xmath36 has smaller singular values .",
    "therefore , introduced priors work particularly well when the true value of the mean matrix parameter @xmath36 is low rank , since low rank matrices have sparse singular values .",
    "we apply the introduced priors to the multivariate linear regression problems , where we can reasonably expect the low rank assumption to be satisfied from the theory of reduced rank regression @xcite .",
    "finally , we provide some numerical results .",
    "this paper is organized as follows . in section 2",
    ", we propose singular value shrinkage priors , prove that they are superharmonic and provide the analytical form of bayesian predictive densities based on them . in section 3",
    ", we apply singular value shrinkage priors to multivariate linear regression problems and discuss the relationship with reduced - rank regression . in section 4 ,",
    "some numerical results are provided .",
    "concluding remarks are given in section 5 .",
    "the idea of singular value shrinkage was utilized by @xcite and @xcite . here",
    "we review their work . in this subsection , we assume @xmath37 .",
    "let @xmath38 @xmath39 and @xmath40 be the singular value decomposition of a matrix @xmath41 , where @xmath42 is the zero matrix of size @xmath43 , and @xmath44 are the singular values of @xmath41 .",
    "similarly , let @xmath45 @xmath46 and @xmath47 be the singular value decomposition of an estimator @xmath48 of @xmath36 , where @xmath49 are the singular values of @xmath48 .",
    "@xcite proposed @xmath50 as an empirical bayes estimator , where @xmath51 .",
    "they proved that @xmath52 is minimax and dominates the maximum likelihood estimator under the frobenius loss @xmath53 @xcite noticed that @xmath52 can be represented in singular value decomposition form as @xmath54 @xmath55 therefore , @xmath52 shrinks the singular values of the observation @xmath41 . when @xmath29 , @xmath52 coincides with the james - stein estimator .",
    "we note that @xmath52 is not a bayes estimator . in this study",
    ", we develop priors shrinking the singular values .",
    "the bayes estimator based on the introduced prior has properties similar to those of @xmath52 .",
    "this is an extension of the relationship between the james - stein estimator and the stein prior .",
    "we consider a prior defined by @xmath56 note that we assume @xmath28 . from the relation @xmath57 where @xmath58 denotes the @xmath59th singular value of @xmath36 , we obtain @xmath60 therefore , this prior puts more weight on matrices with smaller singular values .",
    "when @xmath29 , @xmath61 coincides with the stein prior @xmath34 .",
    "we call @xmath61 the singular value shrinkage prior in the following .",
    "we provide a proof of superharmonicity of the singular value shrinkage prior @xmath61 .",
    "an extended real - valued function @xmath62 is said to be superharmonic if it satisfies the following properties",
    "@xcite :    1 .",
    "@xmath63 is lower semicontinuous .",
    "2 .   @xmath64 .",
    "3 .   @xmath65 for every @xmath66 and @xmath67 , where @xmath68 denotes the surface area of the unit sphere in @xmath69 and @xmath70 denotes the sphere with center @xmath71 and radius @xmath72 .",
    "if @xmath63 is a @xmath73 function , then @xmath63 is superharmonic if and only if @xmath74 holds for every @xmath71 from lemma 3.3.4 of @xcite .",
    "we define a function @xmath75 to be superharmonic when @xmath76 is superharmonic .",
    "[ th_pisvs_superharmonic ] the prior density @xmath61 is superharmonic .",
    "first we prove that @xmath77 is superharmonic for every @xmath78 .",
    "we write the @xmath79th entry of a matrix @xmath41 by @xmath80 and the @xmath79th entry of @xmath81 by @xmath82 .",
    "let @xmath83 so that @xmath84 where the subscripts @xmath85 , @xmath86 , @xmath87 run from @xmath88 to @xmath89 and the subscripts @xmath59 , @xmath90 , @xmath87 run from @xmath88 to @xmath91 , @xmath92 is 1 when @xmath93 and 0 otherwise , and the @xmath79th entry of a matrix @xmath41 is denoted by @xmath94 . from the definition ,",
    "@xmath95    by using @xmath96 and @xmath97 , we obtain @xmath98 where @xmath99 is the @xmath100th entry of the inverse matrix of @xmath101 .",
    "therefore , @xmath102 here , we used @xmath103 which can be derived by differentiating the equation @xmath104 .",
    "we have @xmath105 where @xmath106 and @xmath107 by using @xmath108 , we obtain @xmath109 thus , @xmath110 on the other hand , by @xmath111 , @xmath112 hence , noting that @xmath113 , we obtain @xmath114    thus , from @xmath115 and @xmath116 , @xmath117 therefore , @xmath118 is superharmonic for every @xmath78 .",
    "now , let @xmath119 then , @xmath120 is superharmonic by @xmath121 and @xmath122 , since @xmath123 where @xmath124 denotes the @xmath59th eigenvalue of @xmath125 . also , @xmath126 for every @xmath36 .",
    "therefore , by theorem 3.4.8 of @xcite , @xmath61 is superharmonic .    from this proof , we immediately obtain the following theorem .",
    "the prior @xmath61 satisfies @xmath127 if the rank of @xmath36 is full .",
    "consider @xmath128 in @xmath116 .",
    "therefore , the superharmonicity of @xmath61 is strongly concentrated in the same way as the laplacian of the stein prior becomes a dirac delta function .",
    "another interesting point about this prior is that @xmath61 is superharmonic column - wise : @xmath61 is superharmonic as a function of the @xmath59th column of @xmath36 for every @xmath36 ( @xmath129 ) .",
    "we provide another proof of theorem 2 in the appendix .",
    "we prove that bayesian predictive densities and bayes estimators based on the singular value shrinkage priors are minimax .",
    "when @xmath130 , we define the marginal distribution of @xmath41 with prior @xmath26 by @xmath131 when @xmath132 , we denote the marginal distribution by @xmath133 .",
    "[ th_m_superharmonic ] if @xmath134 for every @xmath41 and @xmath135 is superharmonic , then @xmath136 is superharmonic .",
    "let @xmath137 then , putting @xmath138 , @xmath139 now , for every @xmath66 and @xmath67 , @xmath140 here , the second equation follows from fubini s theorem and the third inequality follows from the superharmonicity of @xmath135 .",
    "therefore , @xmath136 is superharmonic .    in terms of estimation of @xmath36 from @xmath141 ,",
    "the following result @xcite is known .",
    "[ th_stein ] @xcite if @xmath142 satisfies @xmath143 , then bayes estimator @xmath144 with prior @xmath135 is minimax under the frobenius loss .",
    "furthermore , bayes estimator with prior @xmath135 dominates the maximum likelihood estimator unless @xmath135 is the uniform prior .",
    "lemma [ th_stein ] ( @xcite ) is obtained from the expression @xmath145 - { \\rm e}_{m } \\left [ \\| \\hat{m}^{\\pi } - m \\|_{{\\rm",
    "f}}^2 \\right ] \\notag \\\\ = & { \\rm e}_{m } \\left [ \\| \\nabla \\log m_{\\pi } ( x ) \\|^2 - 2 \\frac{\\delta m_{\\pi } ( x)}{m_{\\pi } ( x ) } \\right ] .",
    "\\label{est_risk_diff}\\end{aligned}\\ ] ] of the risk difference between the maximum likelihood estimator and bayes estimator with prior @xmath135 .",
    "in terms of prediction , we obtain the following result . here",
    ", we consider the case where @xmath146 is proportional to @xmath147 .",
    "this assumption corresponds to the setting of @xcite and @xcite .",
    "the general covariance case is considered in section 2.5 .",
    "we assume @xmath148 without loss of generality .",
    "let @xmath149 we write bayesian predictive density based on the uniform prior @xmath150 by @xmath151 .",
    "[ th_george ] if @xmath135 is superharmonic , then @xmath152 is minimax under the kullback  leibler risk @xmath153 .",
    "furthermore , @xmath152 dominates @xmath151 unless @xmath135 is the uniform prior .    from lemma 2 of @xcite ,",
    "the difference of @xmath153 is @xmath154 where @xmath155 denotes the expectation with respect to @xmath156 .",
    "it can be rewritten as @xmath157 since @xmath135 is superharmonic , from lemma 3.4.4 of @xcite and @xmath158 , @xmath159 thus , the first term of the right hand side of @xmath160 is nonnegative .",
    "also , since @xmath142 is superharmonic from lemma @xmath161 and logarithm of a superharmonic function is superharmonic , @xmath162 is superharmonic .",
    "then , from lemma 3.4.4 of @xcite and @xmath158 , the second term of the right hand side of @xmath160 is nonnegative .",
    "therefore , @xmath160 is nonnegative .",
    "proposition @xmath163 is without an assumption concerning twice differentiablility of @xmath135 and is a slight generalization of the results in @xcite .    from theorem @xmath164 and lemma @xmath161",
    ", @xmath133 is superharmonic . also , from @xmath165 in the next subsection , @xmath133 is a @xmath73 function .",
    "therefore , the singular value shrinkage prior @xmath61 satisfies the conditions of lemma @xmath166 and proposition @xmath163 .    by combining the results above ,",
    "we obtain the following .    1 .",
    "the bayes estimator based on the prior @xmath61 is minimax and dominates the maximum likelihood estimator under the frobenius risk .",
    "the bayesian predictive density @xmath167 based on the prior @xmath61 is minimax and dominates @xmath151 under the kullback  leibler risk @xmath153 .",
    "since @xmath61 shrinks the singular values for each , the risk reduction of @xmath61 is large when the true value of the parameter @xmath36 has small singular values .",
    "a remarkable point is that @xmath61 works well even when only part of the singular values are small . in particular",
    ", @xmath61 works stably well in the low rank case , which indicates the sparsity of singular values .",
    "we confirm these facts by numerical experiments in the next section .",
    "we provide the analytical form of bayesian predictive densities based on the singular value shrinkage priors . here",
    ", we consider the case where @xmath146 is proportional to @xmath147 , and assume @xmath168 without loss of generality .",
    "the bayesian predictive density based on the singular value shrinkage prior is @xmath169 here , @xmath170 and @xmath171 is the hypergeometric function of matrix argument @xcite defined by @xmath172 where @xmath173 are arbitrary complex numbers , @xmath174 is a @xmath175 complex symmetric matrix , @xmath176 denotes summation over all partitions @xmath177 of @xmath178 , @xmath179 is the generalized pochhammer symbol , and @xmath180 denotes the zonal polynomial .",
    "we represent the bayesian predictive density as @xmath181 where @xmath142 denotes the marginal distribution with prior @xmath135 . here , @xmath182 does not depend on @xmath135 , because @xmath183 is the sufficient statistic for @xmath36 .",
    "therefore , we obtain @xmath184 next , we calculate @xmath185 and @xmath186 . from the definition , @xmath185 is @xmath187 now we interpret @xmath188 as the probability density of @xmath189",
    ". then @xmath185 is viewed as the expectation of @xmath26 . from theorem 3.5.1 in @xcite",
    ", @xmath190 has a noncentral wishart distribution : @xmath191 therefore , @xmath192.\\ ] ] by using theorem 3.5.6 in @xcite , we obtain @xmath193      \\notag \\\\      & = \\frac{2^{-\\frac{m(n - m-1)}{2 } } \\gamma_m \\left ( \\frac{m+1}{2 } \\right ) }      { \\gamma_m \\left ( \\frac{n}{2 }",
    "\\right ) } { \\rm etr } \\left ( -\\frac{1}{2 } y^{\\top } y \\right )      { } _ 1 f_1 \\left ( \\frac{m+1}{2 } ; \\frac{n}{2 } ; \\frac{1}{2 } y^{\\top } y \\right).\\end{aligned}\\ ] ] using this , we obtain @xmath194 similarly , we obtain @xmath195    substituting @xmath196 , @xmath165 , and @xmath197 to @xmath198 , we obtain the theorem .",
    "thus far , we assumed that the covariance matrices of observation data @xmath199 and @xmath33 and of future data @xmath200 and @xmath32 are the same .",
    "however , these two covariance structures are generally different when we consider the regression problem @xcite .",
    "we extend the results for this general situation to matrix - variate case .",
    "we define @xmath201 @xmath202 and write the diagonalization of @xmath203 as @xmath204 where @xmath205 is an orthogonal matrix and @xmath206 is a diagonal matrix .",
    "let @xmath207 from theorem 3.2 of @xcite , we obtain the following theorem .",
    "[ th_koba ] if @xmath208 is superharmonic as a function of @xmath209 , @xmath152 dominates @xmath151 under @xmath153    we can construct a prior that satisfies the condition of theorem @xmath210 by using the singular value shrinkage prior : @xmath211 . \\label{prior_koba}\\ ] ] the analytical form of bayesian predictive densities based on prior @xmath212 is not yet obtained .",
    "we conjecture that some extension of generalized laguerre polynomial of matrix argument is necessary to solve this problem .",
    "in this section , we apply the results in the previous section to the multivariate linear regression problems .    in some areas , such as econometrics and chemometrics , linear regression problems often arise , wherein we have several dependent variables @xcite .",
    "such problems are called multivariate linear regression .",
    "namely , @xmath213 where @xmath214 is a matrix of explanatory variables , @xmath215 is a matrix of response variables and @xmath216 is a matrix of regression coefficients .",
    "we assume that @xmath217 is known and @xmath218 in the following .",
    "let @xmath219 and @xmath220 then , we can reduce @xmath221 to @xmath222 @xmath223 therefore , we can formulate multivariate linear regression to the form of prediction as @xmath222 @xmath224 thus we can reduce the multivariate linear regression problem to the matrix - variate normal model .",
    "therefore , letting @xmath225 , @xmath226 and @xmath227 , bayesian predictive densities based on the prior @xmath212 dominate those based on the jeffreys prior .",
    "when constructing the predictive density of @xmath228 , we construct bayesian predictive densities for @xmath229 based on prior @xmath212 , and for @xmath230 we use the density of @xmath231 directly .    for the multivariate linear regression problem , it is useful to assume that the regression coefficient matrix @xmath232 is low rank .",
    "this method is called reduced - rank regression @xcite .",
    "the rank of a matrix equals the number of non - zero singular values , so a low rank matrix has sparse singular values .",
    "therefore , bayesian predictive densities based on the singular value shrinkage prior @xmath212 work particularly well in such situations .",
    "also , they are minimax , whereas reduce - rank method is not minimax",
    ".    we can also apply this result to time series analysis .",
    "reduced - rank methods for time series analysis are discussed in @xcite .",
    "one typical example is the ar model for multivariate time series : @xmath233 often we can assume that @xmath234is reduced - rank . therefore we obtain a better prediction of future variables by using the singular value shrinkage priors .",
    "in this section , we show several experimental results on the bayesian inference with singular value shrinkage priors .",
    "we used two methods to calculate the hypergeometric function of matrix argument in @xmath235 .",
    "first is an algorithm by @xcite that exploits the combinatorial properties of the jack function .",
    "second is the holonomic gradient method of @xcite , wherein we numerically solve a pde that @xmath236 satisfies .",
    "while the first method does not wok well for matrix arguments with large singular values , the second method can in principle be used for any matrix arguments .",
    "we compare the singular value shrinkage prior to the jeffreys prior and the stein prior . here , the jeffreys prior coincides with the uniform prior and the stein prior is @xmath237 where @xmath238 denotes the frobenius norm of @xmath36 .",
    "we note that @xmath239 where @xmath58 is the @xmath59th singular value of @xmath36 , holds .",
    "first , we investigate the performance of bayes estimators based on the singular value shrinkage priors .",
    "we comment on the calculation method . from @xcite , bayes estimator for the mean matrix parameter with prior",
    "@xmath26 is @xmath240 where @xmath241 is the marginal distribution of @xmath41 with prior @xmath26 .",
    "when @xmath135 is the singular value shrinkage prior @xmath61 , by @xmath165 , @xmath242 in the numerical experiment , we approximated the partial differentiation of @xmath243 by finite difference : @xmath244 where @xmath245 is a @xmath246 matrix whose @xmath79th element is 1 and other elements are 0 .",
    "we put @xmath247 .    in the following ,",
    "we compare the risk functions of the bayes estimator with the jeffreys prior ( it coincides with the maximum likelihood estimator ) , the bayes estimator with the stein prior , and the bayes estimator with the singular value shrinkage prior .",
    "we sampled @xmath41 @xmath248 times and approximated the risk by the sample mean of loss ( square of the frobenius norm of the difference between estimate and the true parameter ) .",
    "figure @xmath249 shows the risk functions for @xmath250 , @xmath251 and @xmath252 .",
    "the singular value shrinkage prior performs better than the jeffreys prior , and the amount of risk reduction increases as @xmath253 decreases . on the other hand ,",
    "the stein prior does not perform well because the frobenius norm of @xmath254 is not small , even when @xmath255 .",
    "figure @xmath256 shows the risk functions for @xmath250 , @xmath251 and @xmath255 .",
    "though the stein prior performs best when @xmath257 is small , the risk of the stein prior becomes almost the same as the maximum likelihood estimator as @xmath257 increases . on the other hand",
    ", the singular value shrinkage prior performs stably better than the maximum likelihood estimator regardless of the value of @xmath257 .",
    "this is because the singular value shrinkage prior shrinks @xmath257 and @xmath253 for each , while the stein prior shrinks @xmath258 , the frobenius norm .    ,",
    "@xmath251 and @xmath252.,width=302 ]    , @xmath251 and @xmath255.,width=302 ]    figure @xmath259 shows the risk functions for @xmath260 , @xmath261 and @xmath262 , @xmath263 and figure @xmath264 shows the risk functions for @xmath260 , @xmath261 and @xmath265 .",
    "the performance of the singular value shrinkage prior is qualitatively the same as when @xmath250 , @xmath251 .",
    "figure @xmath264 indicates that this prior works stably well for low rank matrices .    , @xmath261 and @xmath266.,width=302 ]    , @xmath261 and @xmath265.,width=302 ]    next , we compare the risk of bayesian predictive densities based on the jeffreys prior , the stein prior and singular value shrinkage prior .",
    "we sampled @xmath41 @xmath248 times and approximated the risk by the sample mean of the kullback  leibler loss .",
    "figure @xmath267-@xmath268 shows the risk functions for the same settings in figure @xmath249-@xmath264 .",
    "the performance of singular value shrinkage prior is qualitatively the same as in the estimation .    ,",
    "@xmath251 and @xmath252.,width=302 ]    , @xmath251 and @xmath255.,width=302 ]    , @xmath261 and @xmath266.,width=302 ]    , @xmath261 and @xmath265.,width=302 ]",
    "in this paper , we developed singular value shrinkage priors for the mean matrix parameters of the matrix - variate normal model .",
    "we proved the superharmonicity of the introduced prior . from the previous studies about the relationship between the superharmonicity of a prior and the minimaxity of the bayes estimator and the bayesian predictive density , bayes estimators and bayesian predictive densities based on",
    "the introduced prior are minimax and also dominate those based on the jeffreys prior .",
    "we provided the analytical form of bayesian predictive densities based on the singular value shrinkage prior , using the classical results in multivariate analysis .",
    "next , we applied the introduced prior to multivariate regression problems .",
    "here we considered priors depending on the future samples . in multivariate regression problems ,",
    "the matrix of regression coefficients is often assumed to be low rank .",
    "singular value shrinkage prior works stably well in such situations .",
    "finally , numerical results showed the effectiveness of singular value shrinkage priors .",
    "we present several problems that require further study .",
    "we obtained the analytical form of bayesian predictive densities based on singular value shrinkage priors for the case where @xmath146 is proportional to @xmath147 .",
    "the calculation for the general covariance case is a future problem .",
    "this is indispensable to obtain bayesian predictive densities in the multivariate linear regression problems .",
    "we conjecture that some extension of generalized laguerre polynomial of matrix argument is necessary to solve this problem .",
    "we extended the bayesian shrinkage prediction method for vector - variate normal distributions of @xcite and @xcite to matrix - variate case .",
    "then , we can consider further extensions to tensor - variate case .",
    "recently , tensor data has gained popularity @xcite .",
    "however , there are several technical problems with this extension .",
    "first , we have difficulties in defining singular values for tensors .",
    "although @xcite provides one definition of singular values for tensors , it does not seem to be a definitive answer .",
    "second , since it is uncommon that all entries of tensor data can be observed , we should consider prediction with observing only limited entries . finally , the method of regression with tensor is now emerging in areas such as brain signal processing .",
    "we expect that the singular value shrinkage prior works well in this problem as well as in multivariate linear regression .",
    "here , we introduce a coordinate system on the space of matrices that is useful for treating singular values .",
    "we put the singular value decomposition of a matrix @xmath41 as @xmath269 @xmath39 @xmath270 where @xmath42 is the zero matrix of size @xmath43 and @xmath44 are the singular values of @xmath41 .    since any orthogonal matrix can be represented as the exponential of an anti - symmetric matrix , there exist matrices @xmath254 @xmath271 and @xmath232 @xmath272 such that @xmath273 @xmath274 we put the entries of @xmath254 and @xmath232 as @xmath275 @xmath276    then we can use @xmath277 as a local coordinate , where @xmath278 represents the entries of @xmath254 , @xmath279 represents the singular values of @xmath41 , and @xmath280 represents the entries of the first to @xmath89th row of @xmath232 .",
    "it is proved as follows .",
    "let @xmath281 be the matrix composed of the first to @xmath89th row of @xmath282 and @xmath32 be the matrix composed of the first to @xmath89th column of @xmath33 .",
    "thus , @xmath283 and @xmath284 then , @xmath285 can be rewritten as @xmath286 now , in the neighborhood of @xmath287 , the first to @xmath89th row of @xmath282 depend only on the first to @xmath89th row of @xmath232 , since @xmath288 therefore , @xmath277 can be used as a local coordinate system .",
    "the metric tensor does not depend on @xmath254 and @xmath232 from invariance .",
    "therefore we calculate the metric tensor on the neighborhood of @xmath289 , @xmath287 . from @xmath290",
    ", we have @xmath291 putting @xmath289 , @xmath287 , we get @xmath292 then , the components of @xmath293 are obtained as follows : @xmath294 therefore , @xmath295 and it follows that @xmath296 in the matrix form , the metric tensor can be written as @xmath297 where @xmath298 is the @xmath89-dimensional identity matrix .          from @xmath309",
    "we can determine whether a function @xmath310 is superharmonic or not .",
    "for example , the laplacian of @xmath61 is @xmath311 this method of determining the superharmonicity of a prior is useful for various forms of priors that can be represented with singular values",
    ". we can construct priors with different strengths of shrinkage for each singular value , and determine whether it is superharmonic .",
    "we can infer the explicit form of the prior @xmath61 naturally as follows .",
    "first , we note the relationship between the james - stein estimator and the stein prior .",
    "the james - stein estimator is @xmath312 and the stein prior is @xmath313 here , we notice that @xmath314 appears in common .",
    "now , the shrinkage estimator of @xcite is @xmath315 which can be represented in singular value coordinate as @xmath316 then , from the analogy above , we can infer the prior form of @xmath317 where the prior for @xmath318 is uniform in the same way as the stein prior depends only on the norm and not on the angle ."
  ],
  "abstract_text": [
    "<S> we develop singular value shrinkage priors for the mean matrix parameters in the matrix - variate normal model with known covariance matrices . </S>",
    "<S> introduced priors are superharmonic and put more weight on matrices with smaller singular values . </S>",
    "<S> they are a natural generalization of the stein prior . </S>",
    "<S> bayes estimators and bayesian predictive densities based on introduced priors are minimax and dominate those based on the uniform prior in finite samples . </S>",
    "<S> the risk reduction is large when the true value of the parameter has small singular values . </S>",
    "<S> in particular , introduced priors perform stably well in the low rank case . </S>",
    "<S> we apply this result to multivariate linear regression problems by considering priors depending on the future samples . </S>",
    "<S> introduced priors are compatible with reduced - rank regression . </S>"
  ]
}