{
  "article_text": [
    "approximate bayesian computation ( abc ) now plays an important role in performing ( approximate ) bayesian inference for the parameter of a proposed statistical model ( called the generative model here ) that has an intractable likelihood . despite the intense attention abc",
    "has recently received , the approach still suffers from several drawbacks .",
    "an obvious disadvantage is the usual necessity to reduce the data to a low dimensional summary statistic .",
    "this leads to a loss of information that is difficult to quantify .",
    "the second , often less severe but sometimes related , drawback is the computational challenge of achieving stringent matching between the observed and simulated summary statistics .    in situations",
    "where an alternative parametric model ( referred to as an auxiliary model ) can be formulated that has a tractable likelihood , the methodology known as indirect inference ( ii ) ( see , e.g. , gourieroux , monfort and renault , @xcite and @xcite ) is applicable .",
    "ii has been thoroughly examined in the classical framework .",
    "most methods differ in the way that observed and simulated data are compared via the auxiliary model .",
    "we expand on this later in the article . for the moment",
    ", we note that some key references are @xcite , @xcite and @xcite .",
    "however , ii has been far less studied in the bayesian paradigm .",
    "@xcite developed an abc approach that uses ii to obtain summary statistics .",
    "in particular , the estimated parameter of the auxiliary model fitted to the data becomes the observed summary statistic .",
    "we adopt a similar naming convention to @xcite and refer to this method as abc ip where `` i '' stands for `` indirect '' and `` p '' stands for `` parameter '' . @xcite also list another method , abc il ( where `` l '' stands for `` likelihood '' ) , which is essentially an abc version of @xcite .",
    "this approach follows abc ip in the sense that the parameter estimate of the auxiliary model is again the summary statistic . however , the abc discrepancy is based on the auxiliary likelihood , rather than a direct comparison of the auxiliary parameters .",
    "@xcite advocate a slightly different approach to abc with ii , which is effectively an abc version of the classical approach in @xcite . here ,",
    "@xcite use the score vector based on the auxiliary model as the summary statistic , which is referred to as abc is ( here `` s '' stands for `` score '' ) .",
    "the parameter value used in the score is given by the mle of the auxiliary model fitted to the observed data .",
    "this approach can be far cheaper from a computational point of view since it avoids an expensive fitting of the auxiliary model to each dataset simulated from the generative model required in abc ip and abc il .    throughout the paper ,",
    "the collection of approaches that use the parametric auxiliary model to form summary statistics is referred to as abc ii methods .",
    "an advantage of this approach over more traditional summary statistics is that some insight can be gained on the potential utility of the ii summary statistic prior to the abc analysis .",
    "additionally , if the auxiliary model is parsimonious , then the summary statistic can be low - dimensional .",
    "@xcite ( see also @xcite ) suggest an alternative approach for combining ii with bayesian inference .",
    "this method has similar steps to abc ip and abc il but essentially uses the likelihood of the auxiliary model as a replacement to the intractable generative model likelihood .",
    "we note here that this is a fundamentally different approach as it is not a standard abc method .",
    "in particular , there is no comparison of summary statistics and no need to choose an abc tolerance . here , we refer to this method as parametric bayesian indirect likelihood ( pbil ) .",
    "the focus of this paper is the application of a parametric auxiliary model for the full data , which we refer to as pdbil ( where `` d '' stands for `` data '' ) .",
    "however , the ideas in this paper are equally applicable if a parametric model is applied at the summary statistic ( not necessarily obtained using abc ii techniques ) level ( i.e. , some data reduction technique has been applied ; see @xcite , for a review ) .",
    "this is referred to as psbil ( where `` s '' stands for `` summary statistic '' ) .",
    "we show that the bayesian version of the synthetic likelihood method of @xcite is a psbil method . in the paper",
    ", we refer to the collection of abc ii and pbil approaches as pbii methods ( `` bayesian '' `` indirect '' `` inference '' using a `` parametric '' auxiliary model ) .    in the process of reviewing these pbii methods , we create a novel framework called bayesian indirect likelihood ( bil ) which encompasses pbii as well as abc methods generally .",
    "in particular , if a specific nonparametric auxiliary model is selected ( npbil ) instead of a parametric one ( pbil ) , then the general abc method is recovered .",
    "a nonparametric kernel can be applied either at the full data ( npdbil ) or summary statistic ( npsbil ) level .",
    "the abc ii approaches are thus a special type of npsbil method where the summary statistic is formed on the basis of a parametric auxiliary model .",
    "this framework is shown in figure  [ fig : bil ] , which also highlights the methods that this paper addresses .",
    "this article does not develop any new algorithms for pbii .",
    "however , this paper does make several interesting and useful contributions .",
    "firstly , we explore the pdbil method in more detail theoretically , and recognise that it is fundamentally different to abc ii .",
    "the behaviour of this method is also substantially different .",
    "a technique sometimes applied with classical ii methods is to increase the simulated dataset size beyond that of the observed data in order to reduce the variability of the estimated quantities under the auxiliary model ( see , e.g. , @xcite ; @xcite ; @xcite ) .",
    "we demonstrate that pdbil and abc ii behave differently for increasing size of the simulated data .",
    "our second contribution is to compare the assumptions required for each pbii approach .",
    "our theoretical and empirical results that the pbil method will provide good approximations if the auxiliary model is sufficiently flexible to estimate the likelihood of the true model well based on parameter values within regions of posterior probability .",
    "abc ii methods rely on the parameter estimate or the score of the auxiliary model to provide a near - sufficient summary statistic .",
    "finally , our creation of the general bil framework provides a clear way to see the connections between pbii and other methods .",
    "the paper is organised as follows . in section  [ sec : notation ] , the notation used throughout the paper is defined .",
    "the abc ii methods are reviewed in section  [ sec : abc ii ] .",
    "the pbil approach is presented in section  [ sec : bil ] .",
    "the theoretical developments in this section , which offer additional into the pbil approximation , are new .",
    "in addition , this section demonstrates how the synthetic likelihood approach of @xcite is a pbil method on the summary statistic level .",
    "section  [ sec : abcasbil ] shows how abc can be recovered as a bil method via a nonparametric choice of the auxiliary model .",
    "section  [ sec : comparebii ] provides a comparison between abc ii and pdbil .",
    "the contributions of this article are demonstrated on examples with varying complexity in section  [ sec : results ] .",
    "the highlights of this section include improved approximate inferences for quantile distributions and a multivariate markov jump process explaining the evolution of parasites within a host .",
    "the article concludes with a discussion in section  [ sec : discussion ] .",
    "consider an observed dataset @xmath0 taking values in @xmath1 of dimension @xmath2 assumed to have arisen from a generative model with an intractable likelihood @xmath3 , where @xmath4 is the parameter of this model .",
    "intractability here refers to the inability to compute @xmath3 pointwise as a function of @xmath5 .",
    "we assume that there is a second statistical model that has a tractable likelihood function .",
    "we denote the likelihood of this auxiliary model by @xmath6 , where @xmath7 denotes the parameter of this auxiliary model",
    ". there does not necessarily need to be any obvious connection between @xmath5 and @xmath8",
    ". the auxiliary model could be purely a data analytic model that does not offer any mechanistic explanation of how the observed data arose .",
    "the parameter estimate of the auxiliary model when fitted to the observed data is given by @xmath9 . assuming a prior distribution on the parameters of the generative model , @xmath10 ,",
    "our interest is in sampling from the posterior distribution , @xmath11 , or some approximation thereof .",
    "we denote data simulated from the model as @xmath12 . in this paper",
    ", we also consider the effect of using @xmath13 independent replicates of data simulated from the model , which we denote @xmath14 and define @xmath15 .",
    "therefore the total size of @xmath16 is @xmath17 .",
    "note that this could also relate to a stationary time series simulation of length @xmath17 ( see , e.g. , @xcite ) . in the case of a stationary time",
    "series or independent and identically distributed ( i.i.d . ) data it is not a requirement for the simulated dataset size to be a multiple of the observed data size .",
    "however , for the sake of simplicity we restrict @xmath13 to be a positive integer .    since the likelihood of the auxiliary model is tractable , we can potentially consider richly parameterised statistical models to capture the essential features of the data .",
    "we assume throughout the paper that the dimensionality of the auxiliary model parameter is at least as large as the dimensionality of the generative model parameter , that is , @xmath18 .",
    "@xcite note that it still may be possible to obtain useful estimates of a subset of the parameters when this assumption does not hold .",
    "we do not consider this here .",
    "abc is widely becoming a standard tool for performing ( approximate ) bayesian inference on statistical models with computationally intractable likelihood evaluations but where simulation is straightforward .",
    "abc analyses set @xmath19 so that the simulated dataset is the same size as the observed . however , for the purposes of this paper , we relax this commonly applied restriction for the moment .",
    "set @xmath20 simulate @xmath21 draw @xmath22 simulate @xmath23 compute @xmath24 @xmath25 and @xmath26 @xmath27 and @xmath28    in abc , a summary statistic is defined by a collection of functions @xmath29 , for each @xmath30 .",
    "henceforth , the subscript @xmath13 is omitted to ease presentation .",
    "proposed parameter values that produce a simulated summary statistic , @xmath31 , `` close '' to the observed summary statistic , @xmath32 , are given more weight . here , we define `` close '' by the discrepancy function @xmath33 and a kernel weighting function , @xmath34 , where @xmath35 is a bandwidth referred to as the abc tolerance .",
    "the abc target distribution is given by @xmath36 where @xmath37 is referred to here as the abc likelihood .",
    "it can be shown that if @xmath38 is a sufficient statistic and @xmath19 then @xmath39 as @xmath40 @xcite .",
    "unfortunately , abc can not be trusted when the value of @xmath13 is increased in the sense that the target @xmath41 can move further away from @xmath11 . in the simple example in appendix",
    "a of the supplemental article @xcite , the posterior distribution for a univariate @xmath42 converges to a point mass centred on the single observation @xmath43 as @xmath44 and @xmath40 ( see also @xcite , pages 2829 , for a similar example ) .",
    "we also verify this behaviour empirically on a toy example .",
    "this suggests that whilst it is tempting to increase the simulated dataset size to reduce the variability of the simulated summary statistic , such an approach is fraught with danger .",
    "standard abc procedures can make use of @xmath13 simulated datasets but in a different way . here",
    "the abc likelihood is estimated via @xmath45 .",
    "however , we note that this is an unbiased estimate ( regardless of @xmath13 ) of a more standard abc likelihood , and thus this process does not alter the abc approximation @xcite . therefore ,",
    "for the remaining presentation in this section we set @xmath19 .",
    "however , increasing @xmath13 may improve the performance of abc algorithms and allow a smaller value of @xmath35 for the same computational cost .",
    "we do not investigate this here .",
    "how the parameter value is proposed depends on the chosen abc algorithm . here , we use markov chain monte carlo ( mcmc abc , @xcite ) with the proposal distribution @xmath46 chosen carefully , and sometimes based on previous abc analyses .",
    "the approach is shown in algorithm  [ alg : mcmc ] for completeness . in the algorithm",
    ", @xmath47 is the number of iterations .",
    "we choose @xmath20 , the initial value of the chain , so that it is well supported by the target distribution .",
    "this initial value may come from a previous analysis or by performing some preliminary runs .",
    "this algorithm is mainly used for simplicity , although it is important to note that other abc algorithms could be applied .",
    "one difficult aspect in abc is that often some form of data reduction is required , to avoid the curse of dimensionality associated with comparing all the data at once @xcite .",
    "this choice of summary statistic is therefore crucial for a good approximation , even when @xmath35 can be reduced to practically 0 . in some applications ,",
    "it is possible to propose another model that provides a good description of the data .",
    "the summary statistic used in abc can be formulated based on this auxiliary model .",
    "these approaches are summarised below .",
    "@xcite suggest using the parameter estimate of the auxiliary model as the summary statistic to use in abc .",
    "data are simulated from the generative model based on a proposed parameter value , then the auxiliary model parameter is estimated based on this simulated data .",
    "the way the auxiliary parameter is estimated provides a mapping between the generative model and the auxiliary model parameters .",
    "the abc algorithm uses a noisy mapping between @xmath5 and @xmath8 through the simulated data , @xmath48 , generated on the basis of @xmath5 , @xmath49 . for this purpose ( explained below )",
    ", we use the maximum likelihood estimate ( mle ) of the auxiliary model @xmath50 where @xmath51 .",
    "abc ip relies on the following assumption :    [ ass : abcip ] the estimator of the auxiliary parameter , @xmath49 , is unique for all @xmath5 with positive prior support .",
    "it is important to note that abc ip ( as well as other abc approaches below ) use @xmath19 so the approximation quality of the method can depend on the statistical efficiency of the estimator @xmath49 based on this finite sample .",
    "additionally , the mle is asymptotically sufficient ( @xcite , page  307 ) . for this reason , we advocate the use of the mle in general as it is typically more efficient than other estimators like sample moments ( for the auxiliary model ) .",
    "sample moments may be computationally easier to obtain , but are likely to result in a poorer abc approximation if the statistical efficiency is lower than the mle .",
    "we note that the optimal choice of auxiliary estimator ( trading off between computational effort and statistical efficiency ) may be problem dependent .",
    "an additional complication is that the auxiliary model is fitted based on data generated from a different model , @xmath51 .",
    "therefore , the efficiency of @xmath52 should be based on @xmath53 not @xmath54 ( see , e.g. , @xcite ) .",
    "this can be investigated by simulation .    in section  [ subsec : quantile_example ]",
    ", we provide an example where the auxiliary model does not satisfy assumption  [ ass : abcip ] , creating difficulties for abc ip .",
    "the abc ii methods ( and the pdbil method ) that follow do not necessarily require unique auxiliary parameter estimates .",
    "an advantage of the ii approach to obtaining summary statistics is that the summary statistic will likely be useful if the auxiliary model fits the observed data ( see section  [ subsec : abciisummary ] for more discussion ) . in the case of abc ip , the ( approximate ) covariance matrix of the auxiliary parameter estimate based on the observed data",
    "can be estimated by the inverse of the observed information matrix [ we denote this information matrix by @xmath55 .",
    "intuitively , we expect this discrepancy function to be more efficient than a euclidean distance , as it can take into account the variability of summary statistics and the correlations between summary statistics . denoting the observed summary statistic as @xmath9 and the simulated summary statistic as @xmath56 ( dropping @xmath5 for notational convenience )",
    ", we use the following discrepancy for abc ip : @xmath57 it is important to note that this is essentially an abc version of the classical approach in @xcite . a more appropriate weighting matrix may involve considering the variance of @xmath52 when the data are generated under an alternative model , @xmath51 ( @xcite provides a result , the so - called sandwich estimator ) .",
    "@xcite also describe an approach that uses the auxiliary likelihood to set up an abc discrepancy . here , the abc discrepancy is @xmath58 this is effectively an abc version of the classical approach of @xcite .",
    "we note that @xmath59 will remain unchanged throughout the algorithm and provides an upperbound for values of @xmath60 obtained for every simulated dataset .",
    "abc il uses the same summary statistic as abc ip but uses a discrepancy based on the likelihood rather than the mahalanobis distance . note that the discrepancy function for abc ip appears in the second - order taylor series approximation of @xmath61 about @xmath62 ( @xcite , page  126 ) assuming standard regularity conditions for @xmath59 and @xmath9 .",
    "the abc tolerance could be viewed here as a certain cut - off value of the auxiliary log - likelihood .",
    "the abc il approach relies on the following assumption .",
    "[ ass : abcip_likelihood ] the auxiliary likelihood evaluated at the auxiliary estimate , @xmath63 , is unique for all @xmath5 with positive prior support .",
    "we note that this assumption can still be satisfied even when the auxiliary model does not have a unique mle ( see section  [ subsec : quantile_example ] for an example ) .",
    "the abc ip and abc il methods use parameter estimates of the auxiliary model as summary statistics and can thus be expensive as it can involve a numerical optimisation every time data is simulated from the generative model .",
    "the next approach to obtaining summary statistics from ii avoids this optimisation step .",
    "@xcite advocate the use of the score vector of the auxiliary model evaluated at the auxiliary mle , @xmath9 , as the summary statistic .",
    "we denote the score vector of the auxiliary model as @xmath64 where @xmath65 .",
    "each component of the summary statistic involving the observed data and the mle , @xmath66 , is assumed to be numerically 0 under standard regularity assumptions ( see below , assumption  [ ass : abcis ] ) .",
    "thus , the search is for parameter values of the generative model that lead to simulated data , @xmath48 , that produces a score close to @xmath67 . noting that the approximate covariance matrix of the observed score is given by the observed information matrix @xmath68",
    ", the following abc discrepancy is obtained for abc is @xmath69 this is essentially an abc version of @xcite .",
    "this approach is fast relative to abc ip when the mle of the auxiliary model is not analytic whilst the score is analytic since no numerical optimisation is required every time data are simulated from the generative model . of course",
    ", it may be necessary to estimate the score numerically , which would add another layer of approximation and may be slower . in the examples of this paper , we are able to obtain the score analytically .",
    "abc is relies on the following assumptions .",
    "[ ass : abcis ] the mle of the auxiliary model fitted to the observed data , @xmath9 , is an interior point of the parameter space of @xmath8 and @xmath68 is positive definite .",
    "the log - likelihood of the auxiliary model , @xmath70 , is differentiable and the score , @xmath71 , is unique for any @xmath48 that may be drawn according to any @xmath5 that has positive prior support .",
    "we note that assumption  [ ass : abcis ] is generally weaker than assumption  [ ass : abcip ] ( abc ip ) , since it may still hold even if the mle of the auxiliary model is not unique .      only models in the exponential family possess a minimal sufficient statistic with dimension equal to that of @xmath72 . for other models , under suitable conditions , the pitman  koopman ",
    "darmois theorem states that the dimension of any sufficient statistic increases with the sample size . for many complex models , such as those considered in the abc setting",
    ", the minimal sufficient statistic will be the full dataset ( or the full set of order statistics if the data are i.i.d . ) . the summary statistic produced by abc ii",
    "will always have dimension @xmath73 , and thus will not produce sufficient statistics in general ( this argument of course carries over to any abc method that uses some data reduction technique ; see @xcite , for a review ) .",
    "intuitively , our suggestion is that the summary statistic produced by abc ii should carry most of the information contained in the observed data provided that the auxiliary model provides a good description of the data .",
    "unfortunately , this is difficult to verify since it is usually not possible to quantify the amount of information lost in data reduction . despite this , by conducting goodness - of - fit tests and/or residual analysis on the auxiliary model fit to the data will at least provide some guidance on the usefulness of the summary statistic produced by abc ii .",
    "this is in contrast to the more traditional approach of summarising based on simple functions of the data ( e.g. , @xcite ) , whose utility is difficult to assess prior to running an abc analysis without performing an expensive simulation study .",
    "furthermore , abc ii methods provide natural discrepancy functions between summary statistics as shown above . selecting the discrepancy function and determining appropriate weighting of the summary statistics in traditional abc can be problematic .",
    "it is well known that the choice of summary statistic in abc involves a compromise between and dimensionality @xcite .",
    "a low - dimensional and near - sufficient summary statistic represents an optimal trade - off .",
    "another advantage of abc ii over usual abc is the dimensionality of the abc ii summary statistic can be controlled by selecting parsimonious auxiliary models and using standard model choice techniques to choose between a set of possible auxiliary models [ e.g. , the akaike information criterion ( aic ) and the bayesian information criterion ( bic ) ] .",
    "@xcite and @xcite propose a method that has similar steps to abc ip and abc il but is theoretically quite different , as we show below .",
    "after data are simulated from the generative model , the auxiliary parameter is estimated . this auxiliary estimate",
    "is then passed into the auxiliary likelihood of the observed data .",
    "this likelihood is then treated in the usual way and fed into a bayesian algorithm , for example , mcmc .",
    "one first defines a collection of functions @xmath74 .",
    "the artificial likelihood is then defined as follows : @xmath75 and the target distribution of this approach is given by @xmath76 where the subscripts @xmath77 and @xmath13 denote the dependence of the target on the auxiliary model choice and the number of replicate simulated datasets , respectively .",
    "this approach is effectively a bayesian version of the simulated quasi - maximum likelihood approach of @xcite .",
    "@xcite proposes to maximise @xmath78 with respect to @xmath5 .",
    "instead of applying this as an abc discrepancy as in the abc il method above , @xcite and @xcite treat this auxiliary likelihood as a replacement to the likelihood of the generative model in the same way that @xcite does .",
    "it is important to note that this approach does not perform a comparison of summary statistics , and hence there is no need to choose an abc tolerance .",
    "thus , it is not a standard abc algorithm .",
    "we refer to this approach simply as pdbil , since we apply a `` parametric '' auxiliary model for the full `` data '' .",
    "when the full data have been summarised as a summary statistic , @xmath32 , an alternative approach is to apply a parametric auxiliary model for the summary statistic likelihood , @xmath79 ( see section  [ subsec : psbil ] for more details ) . in figure  [ fig : bil ] , these methods fall under the pbil class . within this class ,",
    "the focus of this paper is on the pdbil method .",
    "the theoretical aspects of this approach are yet to be investigated in the literature .",
    "some clues are offered in @xcite and @xcite , but we formalise and extend the theory here . the subscript @xmath13 is used to denote that this target remains an approximation to the true posterior distribution and that the approximate target may change with @xmath13 ( we show below that in general the target does depend on @xmath13 ) . however , because it is not an abc algorithm , it is unclear how pdbil behaves as @xmath13 increases . @xcite use a very large simulation size ( @xmath80 ) , without a theoretical investigation .    with @xmath0 fixed",
    ", we consider a potential limiting likelihood @xmath81 with associated posterior @xmath82 note that the parameter of @xmath78 is @xmath83 but the parameter of @xmath81 is @xmath84 .",
    "we emphasise that the results below all assume that @xmath0 is a fixed value in @xmath1 .",
    "to ease presentation , we define the random variable @xmath85 where @xmath86 is a sequence of i.i.d .",
    "random variables distributed according to @xmath87 , and we can write @xmath88,\\ ] ] where @xmath89 is expectation with respect to the distribution of @xmath90 .",
    "the results below provide sufficient conditions under which , as @xmath91 , @xmath92 where @xmath93 is some function of @xmath5 whose posterior expectation is of interest .",
    "this does not assume that @xmath94 , which in general will not be the case . a useful tool to allow us to answer both questions is provided by @xcite .",
    "[ thm : billingsley ] if @xmath95 is a sequence of uniformly integrable random variables and @xmath95 converges in distribution to @xmath96 then @xmath96 is integrable and @xmath97 .",
    "a simple sufficient condition for uniform integrability is that for some @xmath98 , @xmath99    [ res : conv_posterior_and_expectation ] assume that @xmath100 as @xmath101 for all @xmath5 with positive prior support , @xmath102 and @xmath103 .",
    "then @xmath104 furthermore , if @xmath105 is a continuous function satisfying @xmath106 for some @xmath98 then @xmath107    the first part follows from the fact that the numerator of @xmath108 converges pointwise and the denominator is positive and converges by the bounded convergence theorem . for the second part , if for each @xmath109 , @xmath110 is distributed according to @xmath111 and @xmath5 is distributed according to @xmath112 then @xmath110 converges to @xmath5 in distribution as @xmath101 by scheff s lemma @xcite .",
    "since @xmath93 is continuous , @xmath113 converges in distribution to @xmath114 as @xmath101 by the continuous mapping theorem and we conclude by application of theorem  [ thm : billingsley ] .    a simple condition for @xmath115 as @xmath101 to hold is provided by the following result .",
    "[ res : conv_likelihood ] assume that @xmath116 converges in probability to @xmath117 as @xmath101 .",
    "if @xmath118<\\infty\\ ] ] for some @xmath98 then @xmath115 as @xmath119 .    the result follows by application of theorem  [ thm : billingsley ] .    although the results above hold under conditions on the fixed , observed data @xmath0 , they will often hold for a range of possible values of @xmath0 .",
    "the function @xmath120 is often referred to as the mapping or binding function in the ii literature . in @xcite",
    ", it is assumed that this function is 11 but the results above demonstrate that this is not a necessary condition for result  [ res : conv_likelihood ] to hold .",
    "the following example where the auxiliary model is a mixture model demonstrates this principle .",
    "assume that the true model is @xmath121 while the auxiliary model is a mixture of normals , @xmath122 so that @xmath123 .",
    "assuming an infinite sample from the true model , there are an infinite number of mles of the auxiliary model ; @xmath124 where @xmath125 , @xmath126 where @xmath127 or @xmath128 where @xmath129 .",
    "all of these possible mappings produce the same value of the auxiliary likelihood , which coincides with the value of the generative likelihood .",
    "it is straightforward under the assumptions of results  [ res : conv_posterior_and_expectation][res : conv_likelihood ] to show that pdbil will target the true posterior as @xmath44 if the true model is contained within the auxiliary model .",
    "when the generative model is a special case of the auxiliary model , using the notation of @xcite , the auxiliary and generative parameter can be written as @xmath130 and @xmath131 where @xmath132 and @xmath133 denote `` extended '' and `` reduced '' respectively and @xmath134 is fixed .",
    "the proof of this result is straightforward .",
    "it involves demonstrating that @xmath135 is consistent also for the parameter of the generative ( reduced ) model . therefore , when @xmath91 the generative and auxiliary likelihoods will coincide .",
    "this theoretical result can not typically be realised in practice since a model which incorporates an intractable model as a special case is likely to also be intractable . however , it does suggest that the auxiliary model be chosen to be adequately flexible to give a good approximation of the generative likelihood for @xmath5 values with positive prior support . in practice",
    ", our empirical evidence indicates that it is only necessary for the auxiliary likelihood to mimic the behaviour of the generative likelihood for the values of @xmath5 with nonnegligible posterior support and for the auxiliary likelihood to be negligible in regions of the parameter space with little posterior support .",
    "if this is not the case , it is likely that the pdbil method will lead to poor approximations ( as we demonstrate in section  [ subsec : toy_example ] ) .",
    "if the binding function , @xmath120 , were known , the pdbil method would proceed straightforwardly .",
    "since it will not be available in practice , it can be estimated indirectly through the simulated data @xmath16 . from the above ,",
    "it is desirable for the pdbil method if the auxiliary likelihood is as close as possible to the true likelihood .",
    "@xcite show , for a particular choice of the auxiliary model , that choosing the mle for @xmath135 minimises the kullback ",
    "leibler divergence between the generative and auxiliary likelihoods .",
    "furthermore , this choice will often lead to results  [ res : conv_posterior_and_expectation][res : conv_likelihood ] holding .",
    "therefore , we advocate the use of the mle with this method .",
    "when the mle of the auxiliary model is used , @xcite provides an expression which defines @xmath120 [ see equation ( 25 ) of @xcite ] .",
    "set @xmath20 simulate @xmath136 compute @xmath137    draw @xmath22 simulate @xmath138 compute @xmath139 compute @xmath140 @xmath25 @xmath141 @xmath27 @xmath142    it remains to be seen what the target of @xmath143 is relative to @xmath144 . from an intuitive perspective",
    ", increasing @xmath13 leads to a more precise determination of the mapping @xmath120 , and thus should lead to a better approximation .",
    "a more theoretical argument is as follows .",
    "the approximations will coincide with each other provided @xmath145 = p_a({\\mathbf{y}}|{\\boldsymbol{\\phi}}({\\boldsymbol{\\theta}}))$ ] @xcite .",
    "unfortunately , even if one uses an unbiased estimator for the auxiliary parameter , that is @xmath146 = { \\boldsymbol{\\phi}}({\\boldsymbol{\\theta}})$ ] for any value of @xmath13 , this result will still rarely hold in general .",
    "the likelihood can be viewed here as a nonlinear function of the auxiliary parameter estimate , and so @xmath147 \\neq p_a({\\mathbf{y}}|{\\boldsymbol{\\phi}}({\\boldsymbol{\\theta}}))$ ] in general .",
    "our empirical evidence ( see section  [ sec : results ] ) suggests that @xmath143 typically becomes less precise relative to @xmath144 as the likelihood estimate becomes more noisy , that is , when @xmath13 is reduced .",
    "the message of results  [ res : conv_posterior_and_expectation][res : conv_likelihood ] is that , provided the auxiliary model is suitably chosen , a better approximation can be anticipated by taking @xmath13 as large as possible , which has the effect of reducing the bias of @xmath148 .      as an example",
    ", mcmc can be used to sample from the pdbil target ( referred to here as mcmc pdbil , see @xcite ) .",
    "this approach is presented in algorithm  [ alg : mcmc - bil ] .    as results  [ res : conv_posterior_and_expectation][res : conv_likelihood ] suggest ,",
    "the aim with pdbil is to select @xmath13 as large as possible .",
    "we demonstrate in the examples that it is desirable to consider values of @xmath13 greater than one due to the improved statistical efficiency of mcmc pdbil ( and potentially other algorithms that implement pdbil ) when increasing @xmath13 .",
    "of course , the method will become computationally infeasible for very large @xmath13 .",
    "@xcite proposes a method called synthetic likelihood when it is convenient to perform inference on the basis of a set of summary statistics rather than the full data .",
    "considering bayesian inference , the target distribution when the data have been reduced to a summary statistic is given by @xmath149 the major issue with this construction is that there is no analytical form for the likelihood function of the summary statistic , @xmath79 .",
    "@xcite overcomes this by applying , based on the terminology in this paper , a parametric auxiliary model for this probability distribution , @xmath150 . in our framework , an approach that applies a parametric auxiliary model to form the likelihood of the summary statistic rather than the likelihood of the full data ( as is presented in section  [ subsec : bil_target ] ) is referred to here as psbil ( where `` s '' denotes `` summary statistic '' )",
    ". therefore the bayesian version of @xcite is a psbil approach .",
    "for the auxiliary likelihood , @xmath150 , @xcite considers using the likelihood of a multivariate normal distribution , @xmath151 , where @xmath152 is the auxiliary parameter . as is the case with pdbil , the binding function @xmath120 is rarely known but can be estimated via simulated data from the generative model for a particular value of @xmath5 . using our notation",
    ", we obtain the following dataset from the true model of the summary statistic , @xmath153 .",
    "this represents @xmath13 i.i.d .",
    "observations from @xmath154 .",
    "an advantage of selecting such a simple auxiliary model is that the mle has the analytic form @xmath155 where the superscript @xmath47 denotes transpose .",
    "the auxiliary likelihood used is then based on @xmath156 .",
    "our results indicate that the target distribution of this method will depend on @xmath13 , and , if the auxiliary model for the summary statistic likelihood is reasonable , better approximations of @xmath157 are likely to be obtained for large @xmath13 .",
    "an alternative and perhaps natural candidate for @xmath158 is to use a kernel density estimate based on the samples @xmath16 .",
    "this corresponds to choosing @xmath159 and we define @xmath160 @xcite , where @xmath35 is the bandwidth parameter .",
    "we have then @xmath161 and this is exactly the form of the standard abc likelihood . in addition , @xmath13 does not affect the likelihood ( although it may help computationally in some algorithms ) and @xmath35 controls the level of approximation . here , we see that this is an estimate of the abc likelihood where the comparison is made between the full datasets . here",
    ", we obtain the npdbil approach as presented in figure  [ fig : bil ] ( where `` d '' corresponds to full `` data '' ) .",
    "alternatively , a nonparametric density estimate of the auxiliary model of the summary statistic likelihood , @xmath162 , could be applied . using a similar procedure to above",
    ", we obtain @xmath163 .",
    "we refer to this in figure  [ fig : bil ] as npsbil ( npbil based on a `` summary statistic '' ) .",
    "this is the approach adopted by @xcite , however , their focus is on point estimation ( posterior mean ) .",
    "unfortunately , @xcite refer to their bayesian estimator as bil , however , under our framework bil is a more general class of methods . of course",
    ", if the summary statistic is derived from some parametric auxiliary model , then the abc ii class of method is recovered as an npsbil method .",
    "the reader is again referred to figure  [ fig : bil ] to see the connection between these methods .    by selecting a parametric model for the auxiliary likelihood ( pbil )",
    ", we can potentially overcome the curse of dimensionality associated with the nonparametric aspect of abc .",
    "this requires further research . of course , finding a suitable parametric auxiliary model may be challenging in practice .",
    "there are a few remarks to be made about the above results in relation to theoretical comparisons between abc ii and pdbil .",
    "[ rem : bil_increasing_n ] under suitable conditions better approximations with pdbil are obtained by increasing @xmath13 .",
    "this is in stark contrast with abc ii , which can not be trusted for @xmath164 .",
    "[ rem : abc_never_correct ] in the case where the true model is a special case of the auxiliary model , the pdbil method will be exact in the limit as @xmath165 .",
    "in contrast , in this ideal situation , abc ii still does not produce sufficient statistics ( see the dimensionality argument in section  [ subsec : abciisummary ] ) and will not target the true posterior in the limit as @xmath40 .",
    "an example is where the true model is a t - distribution with location , scale and degrees of freedom of @xmath166 , @xmath167 and 1 , respectively .",
    "the auxiliary model is a more general t - distribution with degrees of freedom  @xmath168 . in this case , the pdbil method is exact in the limit as @xmath44 as the true model is incorporated within the auxiliary model .",
    "unfortunately , abc ii does not produce a sufficient statistic as the summary statistic will be of dimension three whilst it is known for this model the minimal sufficient statistic consists of all the order statistics . of course ,",
    "finding an auxiliary model that satisfies this condition in practice will rarely be feasible .    [ rem : bil_never_correct ] even if the auxiliary parameter estimate or score happen to be a sufficient statistic for the generative model , pdbil still will not generally target the true posterior , as the auxiliary and generative likelihoods will still not match up .",
    "in this situation , the abc ii approaches will enjoy convergence to the true posterior as @xmath40 whilst pdbil will not converge to the true posterior as @xmath44 .",
    "however , sufficient statistics are rarely achieved in practice .",
    "remarks  [ rem : abc_never_correct ] and  [ rem : bil_never_correct ] demonstrate that pbii methods generally will not ( and rarely will ) target the true posterior distribution asymptotically .",
    "this is generally the case for other techniques in the literature for dealing with models that have intractable likelihood functions .",
    "there are some exceptions to this .",
    "for example , exact techniques are available for so - called doubly intractable models when perfect simulation from the generative model is possible ( e.g. , @xcite ; @xcite ) .",
    "furthermore , so - called pseudo - marginal methods @xcite are applicable when a positive and unbiased estimator of the likelihood is available and is a current area of research . despite not being exact ,",
    "we demonstrate that pbii methods can produce quite good approximations in some applications .",
    "the characteristics of a good auxiliary model differ between the abc ii and pdbil methods . in the context of abc ii , we simply require a good summarisation of the data , that is , a low - dimensional summary statistic that hopefully carries most of the information in the observed data . therefore , we feel that it is useful if the auxiliary model in this context provides a good fit to the data and is parsimonious , so that the essential features of the data are described well and as succinctly as possible .",
    "this is independent of the process for selecting a generative model .",
    "therefore , the same auxiliary model should be used regardless of which generative model is fitted to the data . for pdbil , we require a flexible auxiliary model that can mimic the behaviour of the generative model for different values of @xmath5 within the posterior support . here , it is not necessary for the auxiliary model to provide a good fit to the data considering the fact that the generative model being proposed might be mis - specified . the auxiliary model chosen for pdbil may alter depending on the generative model being proposed . in our examples ,",
    "the generative model is either known or provides a good fit to the data . in such cases , it would not be uncommon to choose the same auxiliary model for the abc ii and pdbil methods .",
    "the conditions required for pdbil to produce exact results are very strong and finding an auxiliary model that is sufficiently flexible so that the auxiliary likelihood can mimic the generative likelihood could be difficult in practice . in some applications , an auxiliary model that is a simplified version of the generative model may be specified where the parameter of each model has the same interpretation .",
    "for example , the auxiliary model for a continuous time markov jump process may be its corresponding linear noise approximation .",
    "in such situations , the pdbil method is unlikely to perform well whilst it remains possible that such an approximate model could produce summary statistics for abc even though the auxiliary model would not fit the data well .",
    "@xcite show that ii can work well in the classical framework when the auxiliary model is a simplified version of the generative model .",
    "further research is required in the bayesian setting .",
    "an additional advantage of the abc ii approach over pdbil is the extra flexibility of being able to accommodate additional summary statistics that do not involve an auxiliary model , since this method belongs in the more general npsbil class ( see @xcite , for an example where the summary statistic is a combination of auxiliary parameter estimates and other summary statistics ) . @xcite and @xcite consider ii applications in a classical framework where the comparison of observed and simulated data is made on the basis of both an auxiliary model and supplementary summary statistics .",
    "in this example , we consider a simple model so that exact bayesian inferences are trivially obtained . our intention here is to investigate the theoretical considerations in section  [ sec : bil ] . in particular , we show that when the auxiliary model is reasonable , pdbil produces better approximations as the size of simulated datasets goes beyond that of the observed data and as a useful by - product increases the acceptance probability of the mcmc moves .",
    "we also demonstrate empirically that unfortunately abc approaches ( including those using ii to obtain summary statistics ) do not possess this same desirable property as @xmath13 is increased . additionally , we investigate the output of pdbil when the auxiliary model is poorly chosen .",
    "[ fig : toy ]    here , the data are @xmath169 independent draws from a poisson distribution with a mean of @xmath170 , @xmath171 . the prior is @xmath172 ( where @xmath173 and @xmath174 ) , which results in a @xmath175 posterior .",
    "for such a relatively large value for the mean of the poisson distribution , a normal approximation with mean , @xmath166 , and variance , @xmath176 , will be reasonable .",
    "we use this normal distribution as the auxiliary model . here , the auxiliary likelihood , mle and score are trivial to compute .",
    "the anderson  darling test for normality produced a @xmath177-value of about 0.576 , which indicates no evidence against the assumption that the normal auxiliary model provides a good description of the data .",
    "the summary statistic based on this auxiliary model includes the sample mean , @xmath178 , which is a sufficient statistic for the generative model .",
    "thus , the abc ii approaches can be expected to produce essentially exact inferences ( excluding monte carlo error ) as long as the abc tolerance is low enough .",
    "as demonstrated in figure  [ fig : toy ] , this is the case .",
    "such sufficiency is not usually achieved in practice .",
    "however , it can be seen that the abc posterior is grossly over - precise when the size of the simulated datasets is increased to 1000 ( i.e. , @xmath179 ) .    in the limit as @xmath44 ,",
    "the pdbil method boils down to a @xmath180 distribution approximating a @xmath181 distribution .",
    "the central limit theorem states that the normal approximation improves as @xmath182 increases . since @xmath170",
    ", pdbil can never target the true posterior .",
    "the pdbil target distribution ( for @xmath44 ) is proportional to @xmath183 while the true posterior is proportional to @xmath184 .",
    "figure  [ fig : toy](d ) demonstrates a small amount of bias for the pdbil method ( this is an illustration of remark  [ rem : bil_never_correct ] ) .",
    "figure  [ fig : toy](d ) presents the results for the pdbil method based on simulated dataset sizes of @xmath19 and @xmath179 ( results for @xmath185 and @xmath186 are even closer to the true posterior but are not shown on the figure ) .",
    "it is evident from the figure that a more precise posterior is achieved when using larger simulated datasets , without necessarily over - shooting the true posterior . additionally , there was an increase in the mcmc acceptance rate as @xmath13 increased . for the @xmath13 values investigated here ,",
    "the acceptance rates were roughly 46% , 67% , 72% and 73% for increasing @xmath13 .",
    "these acceptance rates are very high , especially relative to abc algorithms which generally suffer from quite low acceptance probabilities .",
    "this example demonstrates that better inferences using pdbil can be obtained by increasing the size of the simulated dataset beyond that of the observed .",
    "unfortunately , abc inferences that use a simulated data size larger than that of the observed data can not be trusted in the same way ( see remark  [ rem : bil_increasing_n ] ) .",
    "the reason for improved inferences from pdbil as @xmath13 is increased is apparent from figure  [ fig : toy_loglike ] . here",
    ", it can be seen from increasing @xmath13 the log - likelihoods of the generative and auxiliary models are becoming more correlated with the slope of the relationship becoming approximately one .",
    "figure  [ fig : compare_true_aux](a ) shows the true @xmath187 and auxiliary @xmath180 log - likelihood values for @xmath188 within the 99% highest prior density region .",
    "the vertical lines indicate the bounds of a 99% credible interval based on the true posterior .",
    "it can be seen that the auxiliary log - likelihood is a poor approximation to the true log - likelihood in regions with negligible posterior support .",
    "this is even the case for larger @xmath182 values where it would be expected that a normal approximation would be more appropriate .",
    "however , the normal approximation will perform relatively poorly in the tails of the distribution .",
    "it is evident that the auxiliary likelihood acts as a useful replacement likelihood in the region of high posterior support [ see figure  [ fig : compare_true_aux](b ) ] , and this is enough to result in a good approximation of the true posterior for large @xmath13 .        finally , we investigate the output from pdbil when the auxiliary model is chosen poorly .",
    "figure  [ fig : compare_true_aux](c ) shows the results for when the auxiliary model is @xmath189 , where @xmath190 is fixed . here , the pdbil posterior as @xmath44 is proportional to @xmath191 . here",
    ", we consider @xmath192 ( over - dispersed ) and @xmath193 ( under - dispersed ) .",
    "the results show over - precise and conservative results in the under - dispersed and over - dispersed case , respectively .",
    "the under - dispersed and over - dispersed auxiliary models have thinner and fatter tails , respectively , than the likelihood of the generative model in the parameter space well supported by the posterior distribution ( see figure  [ fig : compare_true_aux ] ) . in these cases ,",
    "the auxiliary model is not providing a useful replacement likelihood . just by chance abc ii",
    "is still exact here as @xmath40 since the parameter estimate for @xmath166 is a sufficient statistic for @xmath182 .",
    "quantile distributions ( or functions ) represent a class of distributions that are defined in terms of their quantile function .",
    "such functions can be formulated to create more flexible distributions than other standard distributions . in this example , the focus is on the @xmath194-and-@xmath195 distribution described in , for example , @xcite ( the reader is also referred to the references therein ) .",
    "this quantile function , which can also be interpreted as a transformation of a standard normal random variate , has the following form : @xmath196 \\\\[-8pt ] & & \\phantom{a + } { } \\cdot\\bigl(1 + z(p)^2\\bigr)^kz(p).\\nonumber\\end{aligned}\\ ] ] here , @xmath177 denotes the quantile of interest while @xmath197 represents the quantile function of the standard normal distribution .",
    "the model parameter is @xmath198 , though common practice is to fix @xmath199 at 0.8 , which we do here ( see @xcite , for a justification ) .",
    "the likelihood function can be computed numerically , although this is more expensive than model simulation which is cheaply implemented for quantile distributions via the inversion method .",
    "full likelihood - based inference is more expensive than the simulation - based approaches for the relatively large dataset considered here .",
    "the observed dataset consists of 10,000 independent draws from the @xmath194-and-@xmath195 distribution with @xmath200 , @xmath201 , @xmath202 , @xmath203 and @xmath204 ( same as considered in @xcite ) .",
    "a nonparametric estimate of the probability density function based on these samples is shown in figure  [ fig : gandk_data ] .",
    "the data exhibit significant skewness and kurtosis .",
    "we use a three component normal mixture model with 8 parameters as the auxiliary model .",
    "a mixture model is a suitable choice for an auxiliary distribution since it can be made arbitrarily flexible whilst maintaining a tractable likelihood function .",
    "therefore , auxiliary mles are computationally easy to obtain [ here we use the expectation - maximisation ( em ) algorithm ] and the subsequent likelihood can be evaluated cheaply . on the other hand , mixture models",
    "can he highly irregular and the mle is not consistent in general .",
    "the invariance of the likelihood to a re - labelling of the components causes an immediate issue for abc ip , which requires a unique auxiliary parameter estimate . in an attempt to overcome this , we post - process",
    "the mixture model parameter estimates generated throughout the abc ip algorithm by ordering them based on the component means .",
    "since pdbil and abc il use the likelihood of the auxiliary model , they more naturally overcome the label switching issue .",
    "however , the mixture model can give other numerical issues such as those resulting from infinite likelihoods .",
    "this would create serious issues for methods that use the auxiliary likelihood ( the auxiliary likelihood would not be unique ) . from investigations on the dataset here",
    ", it appears that the likelihood is well behaved and that the modes in the likelihood correspond only to re - labelling of components .",
    "therefore , we proceed with abc il and pdbil with caution .",
    "the abc is method , based on the score vector , appeared to not have any difficulties accommodating the auxiliary mixture model .    from figures  [ fig : gandk_data](a ) and  [ fig : gandk_data](b )",
    ", it can be seen that there is a correspondence between both the densities and the cumulative distribution functions of the mixture model and the data .",
    "however , we performed a hypothesis test to assess the goodness - of - fit of the three component mixture model with a parameter given by the mle .",
    "the test - statistic was the kolmogorov ",
    "smirnov statistic that computes the maximum absolute difference between the theoretical and empirical c.d.f.s . to avoid any distributional assumption about this test - statistic , we simulated 10,000 values of this statistic under the assumption that the mixture model is correct .",
    "we found that the observed test - statistic was exceeded 0.25% of the time , indicating strong evidence against the mixture providing a good fit to the data .",
    "figure  [ fig : gandk_data](b ) shows the differences between the empirical and theoretical c.d.f.s .",
    "however , from figure  [ fig : gandk_data](a ) it is evident that the mixture model can explain several features of the true model , and since the dataset size is large there is a high probability of detecting a difference . our results below show that we are able to obtain quite accurate posterior distributions with the pbii methods despite the lack of fit suggested by the hypothesis test . in appendix b of the supplemental article @xcite , we present results from using a four component mixture model .",
    "unfortunately we found this was substantially more expensive to apply and resulted in some numerical problems .",
    "the proposal distribution in the mcmc for the pdbil algorithm was guided using the results in @xcite , who analysed the same data via a traditional abc approach that used robust measures of location , scale , skewness and kurtosis as the summary statistics .",
    "pdbil was run using @xmath13 values of 1 , 2 , 4 , 10 , 20 and 50 for a number of iterations given by 1 million , 500,000 , 500,000 , 200,000 , 100,000 and 75,000 , respectively .",
    "the mcmc acceptance probabilities obtained were about 2.8% , 5% , 7.1% , 13.1% , 18.5% and 20.8% , respectively .",
    "the average effective sample size ( ess , averaged over the four parameters ) divided by the computing time ( in hours ) were roughly 63 , 127 , 106 , 124 , 70 and 41 , respectively .",
    "this demonstrates how pdbil is still feasible as @xmath13 increases to a certain point .",
    "however , for very large @xmath13 the computation becomes unmanageable .",
    "figure  [ fig : gandk_large ] shows the results for @xmath19 , @xmath179 and @xmath205 ( the results for @xmath206 and @xmath205 were quite similar ) . a very time consuming exact mcmc algorithm",
    "was run for 10,000 iterations to obtain a gold - standard ( producing an average ess per hour of 6 ) .",
    "the results show an increase in precision of the pdbil posteriors as @xmath13 increases .",
    "the results for @xmath207 and @xmath208 are very accurate , while the pdbil posteriors for @xmath194 and @xmath195 show some bias ( also with a loss of precision for @xmath194 ) .",
    "abc ip and abc il were run for 1 million iterations with a tolerance tuned to achieve an acceptance rate of about 1% . due to the abc",
    "is method being so much faster than the other pbii approaches , we aimed for a relatively lower abc tolerance and ran the algorithm for more iterations . more specifically , 7 million iterations were used and the abc tolerance chosen resulted in an acceptance rate of 0.3% .",
    "we also applied a regression adjustment to the ( appropriately thinned ) abc ii samples using the approach of @xcite in an attempt to eliminate the effect of the abc tolerance . in order to apply regression adjustment for abc il ,",
    "the same post - processing procedure used for abc ip was required . without regression adjustment",
    "[ see figure  2 of the supplemental @xcite ] , the abc is method gave slightly better results than other abc ii methods , which could be due to the ability of abc is getting to a lower abc tolerance .",
    "the unadjusted abc il results were also slightly better than the unadjusted abc ip results .",
    "abc is produced an average ess per hour of 90 while the corresponding number was 50 and 30 for abc ip and abc il , respectively , showing that the abc is method required less time to produce a better approximation .",
    "regression adjustment offered improvement to all the abc ii methods .",
    "we compared the pbii approaches with the abc results of @xcite .",
    "it should be noted that we applied a local regression adjustment to the abc results here as we found some improvement for the parameters @xmath207 and @xmath194 ( results were very similar for @xmath208 and @xmath195 relative to those obtained in @xcite ) .",
    "the results are shown in figure  [ fig : gandk_compare ] .",
    "overall , the pbii results present a marked improvement over the abc analysis of @xcite , with @xmath194 seemingly the most difficult parameter to estimate .",
    "the abc ii methods with regression adjustment produced very similar results .",
    "taking into account accuracy and computational efficiency , abc is with regression adjustment is probably the preferred method .",
    "when a four component auxiliary model was used [ appendix b of the supplemental article @xcite ] , the abc ii methods with regression adjustment produced similar results and outperformed pdbil in terms of accuracy .",
    "further , the abc is approach was able to avoid the heavy computation associated with fitting the four component mixture model at every iteration , and thus also avoided other numerical issues such as the em algorithm converging to potentially local optima .",
    "the abc ii regression adjustment results showed some improvement for @xmath194 and @xmath195 when going from the three to four component mixture model .",
    "drovandi , pettitt andfaddy ( @xcite ) developed an abc ip approach to estimate the parameters of a stochastic model of macroparasite population evolution developed by @xcite ( see also @xcite ) .",
    "data was collected independently on 212 cats , who were initially injected with a certain number of juvenile _ brugia pahangi _ parasites . some time after each cat was sacrificed ,",
    "the number of parasites that had reached maturity were counted and recorded ( see @xcite ) .",
    "@xcite discovered that a beta - binomial model provided a good description of the data .",
    "below provides a brief review of the generative and auxiliary models , with the reader referred to @xcite for more details .    at time @xmath209 , any host is described by three random variables @xmath210 , where @xmath211 is the number of mature parasites , @xmath212 is the number of larvae and @xmath213 is a discrete version of the host s immunity .",
    "it is assumed that each larva matures at a constant rate of @xmath214 per day .",
    "larvae die at a rate @xmath215 per larva where @xmath216 represents the rate at which natural death of larvae occurs and @xmath217 is a rate parameter that describes additional death of larvae due to the immune response of the host .",
    "the acquisition of immunity is assumed to be dependent only on the number of larvae and occurs at rate @xmath218 , and a host loses immunity at a rate @xmath219 per unit of immunity .",
    "mature parasites die at a rate of @xmath220 adults per day .",
    "parameters @xmath214 and @xmath220 have been previously estimated at 0.04 @xcite and 0.0015 @xcite , respectively .",
    "the data were modelled via a continuous time discrete trivariate markov process . given current values of the states at time @xmath209 , @xmath221 , @xmath222 , @xmath223 , and a small time increment @xmath224 the transition probabilities at time @xmath225 are given by @xmath226 and the probability of remaining in the same state is one minus the sum of the above probabilities . only the final mature count is observed whilst the immunity and larvae counts are unobserved throughout the process .",
    "moreover , the immune response variable @xmath213 is unbounded .",
    "data generative likelihood - based approaches appear infeasible due to computational issues ( see @xcite ) .",
    "simulation is straightforward via the algorithm of @xcite .",
    "the prior distributions are : @xmath227 , @xmath228 , @xmath229 and @xmath230 .    here",
    ", we denote the observed data as @xmath231 where @xmath232 is the mature count for the @xmath233th host .",
    "covariates for the @xmath233th host are given by @xmath234 ( initial larvae count ) and @xmath235 ( sacrifice time ) .",
    "for the auxiliary model , @xcite capture the overdispersion via a beta - binomial regression model and take into account the effect that @xmath235 and @xmath234 have on @xmath232 .",
    "denote @xmath236 and @xmath237 as the beta - binomial parameters for the @xmath233th host .",
    "more specifically , the @xmath233th observation has the following probability distribution : @xmath238 where @xmath239 denotes the beta function .",
    "consider a re - parameterisation in terms of a proportion , @xmath240 , and over - dispersion , @xmath241 , parameter .",
    "the auxiliary model relates these parameters to the covariates via @xmath242 where @xmath243 and @xmath244 hence , the auxiliary model has the parameter @xmath245 while the generative model has the parameter @xmath246 .        using the approach outlined in appendix c of the supplemental article ( @xcite,@xcite ) , we obtained goodness - of - fit @xmath177-values of 0.37 and 0.47 , indicating no evidence against the beta - binomial model providing a good description of the data .",
    "@xcite use the aic to select this auxiliary model over competing auxiliary models .      for validation of the pbii methods for this example , data was simulated using the same experimental design as the observed data based on the parameter configuration estimated by @xcite ; @xmath247 , @xmath248 , @xmath249 and @xmath250 .",
    "we found that the pbii methods were able to recover the parameters @xmath168 and @xmath216 well , @xmath219 was determined less precisely and @xmath217 was not recovered .",
    "the data are not particularly informative about @xmath219 and @xmath217 ( see @xcite , for more discussion ) .",
    "the abc is gave the most precise posterior distributions for @xmath168 and @xmath251 out of the pbii methods .",
    "for full details on the analysis of this simulated data , see appendix c of the supplemental article @xcite .      here",
    ", we used the abc ip results of @xcite to form an mcmc proposal distribution . the pdbil method with @xmath19 , @xmath206 and @xmath205 was run for 1 million , 100,000 and 50,000 iterations , respectively .",
    "acceptance probabilities of roughly 1.4% , 23.5% and 28.2% , respectively , were obtained .",
    "the average ess per hour was 37 , 79 and 58 , respectively .",
    "the substantial increase in acceptance probability allowed us to use fewer iterations .",
    "the results are shown in figure  [ fig : cat_bil ] .",
    "the figures suggest that we are not able to gain any additional information from the data for the parameter @xmath252 from the pdbil approach by increasing @xmath13 .",
    "however , an increase in precision is obtained for @xmath251 as @xmath13 is increased .",
    "the posteriors are shifted slightly for the other two parameters , however , they are still largely uninformative , although the posterior for @xmath219 for large @xmath13 may indicate some preference for smaller values of @xmath219 .",
    "we now compare the results of pdbil with abc .",
    "abc ip and abc il mcmc algorithms were all run for 1 million iterations .",
    "the abc ip and abc il tolerances were chosen so that the acceptance rate was about 1.5% . due to the increased computational efficiency of abc",
    "is , we ran this algorithm for 20 million iterations and tuned the tolerance to obtain an acceptance rate of about 0.1% .",
    "abc ip and abc il used about 15 hours of computing time while abc is only required 11 hours even though 20 times more iterations were run .",
    "[ figsub : cat_ii_compare_nu_reg][figsub : cat_ii_compare_mul_reg ]    the estimated posterior densities ( after appropriate thinning ) for the different approaches are presented in figure  [ fig : cat_ii_compare ] . in general , the data are not informative about the @xmath219 and @xmath217 parameters , so we turn our focus to the parameters @xmath168 and @xmath251 .",
    "we note that it is difficult to compare the approximations without having available a gold standard .",
    "it can be seen that pdbil produces the most precise inferences for the parameters @xmath168 and @xmath251 . despite being able to reduce the abc tolerance",
    ", the abc is method appears to be the least precise .",
    "this is in contrast to the results for the simulated data in appendix c of the supplemental article @xcite , where abc is produced the most precise results .",
    "regression adjustment was also applied to the abc ii methods in an attempt to reduce the effect of the abc tolerance .",
    "these adjustments were applied individually to @xmath253 and @xmath254 [ see appendix c of the supplemental article ( drovandi , pettitt and lee , @xcite ) ] and the results are shown in figure  [ fig : cat_ii_compare_reg ] . the regression adjustment does increase the precision of the abc ii posteriors . the regression adjustment appears to shift the modes of the abc ii results slightly for @xmath168 . for @xmath251 , the regression adjustment brings the abc ii results closer to that obtained by pdbil for @xmath205 .",
    "this paper has provided an extensive comparison of pbii methods , from theoretical , practical and empirical perspectives .",
    "we discovered that the pdbil method of @xcite is fundamentally different to abc ii approaches developed in the literature .",
    "more specifically , we showed that pdbil can produce better approximations by increasing the size of the simulated datasets as long as the auxiliary model provides a useful replacement likelihood for the generative likelihood for a variety of @xmath5 values .",
    "in contrast , abc methods ( including those that use ii to form the summary statistic ) should simulate datasets the same size as the observed .",
    "the pdbil method has the additional advantage of not having to determine an appropriate abc tolerance .",
    "furthermore , we found that increasing the size of the simulated dataset beyond that of the observed does not necessarily make computation infeasible due to the increase in statistical efficiency .",
    "however , it is of interest to determine the size of the simulated dataset upon which negligible improvement will be obtained .",
    "this requires further research .",
    "we have also established that bil is a rather flexible framework since the synthetic likelihood approach of @xcite is a pbil method that applies a parametric auxiliary likelihood to the summary statistic likelihood while abc can be recovered by selecting a specific nonparametric auxiliary model .",
    "our focus in this paper has been on the pbil method where a parametric auxiliary model is proposed for the full data likelihood .",
    "however , the ideas in this paper may carry over to when the auxiliary model is applied to a summary statistic likelihood as in @xcite .",
    "for the pbil method to have some chance of a good approximation to the true posterior for the specified generative model , it is important that the auxiliary model is able to well fit data simulated from the generative model for parameter values within nonnegligible posterior regions , at least in the majority of simulations .",
    "it would be possible to perform a goodness - of - fit test on the auxiliary model for every dataset generated from the proposed model during the mcmc pbil algorithm in order to assess the usefulness of the auxiliary model in the context of the pbil method .",
    "this is the subject of further research .    in this paper",
    ", we have not addressed the issue of which abc ii method provides the best approximation .",
    "abc is is much faster ( when the auxiliary score vector is analytic ) and requires only weak assumptions , but did not always outperform the other abc ii methods in the examples considered in this paper .",
    "the abc ip and abc il methods differ only in their discrepancy function and it is not clear if one discrepancy function dominates the other across applications .",
    "furthermore , it remains unknown if the auxiliary parameter estimate or auxiliary score carries the most information in the observed data .",
    "it could be that the optimal choice of abc ii approach is problem dependent . until further research is conducted",
    ", we suggest trying all three methods ( assuming that abc ip and abc il are computationally feasible ) .",
    "one approach to speed up abc ip and abc il might be to start with a computationally simple but consistent estimator ( e.g. the method of moments ) and apply one iteration of a newton  raphson method to produce an asymptotically efficient estimator ( @xcite , page  308 ) in a timely manner .    from a practical perspective",
    ", these methods have led to improved approximate analyses for two substantive problems compared with that obtained in @xcite and @xcite . across applications considered in this paper ,",
    "abc is was the most computationally efficient and led to good posterior approximations .",
    "overall , pdbil avoids having to choose an abc discrepancy function and the abc tolerance . if an auxiliary model can be proposed that satisfies a rather strong condition ,",
    "more precise inferences can be obtained by taking @xmath13 large , which we showed is still computationally feasible with mcmc pdbil up to a point .",
    "however , abc ii appears to provide a more general framework for pbii problems , due to the extra flexibility of being able to incorporate additional summary statistics outside the set formed from the auxiliary model and potentially providing better approximations when the auxiliary model is a simplified version of the generative model .",
    "it is this extra flexibility that may see abc ii as the method of choice as ever - increasingly complex applications are encountered .",
    "the first two authors were supported by the australian research council discovery projectdp110100159 .",
    "the authors would like to express their gratitude to the referees and editorial team associated with this paper .",
    "the comments and suggestions provided have led to a substantially improved paper .",
    "the authors would like to thank edwin michael and david denham for access to the macroparasite data .",
    "the first author is grateful to ewan cameron , richard everitt , dennis prangle , andy wood and christian robert for useful discussions about this work .",
    "the first author is also grateful to warwick university ( where this work was partially completed ) for providing some funding for a three month visit ."
  ],
  "abstract_text": [
    "<S> indirect inference ( ii ) is a methodology for estimating the parameters of an intractable ( generative ) model on the basis of an alternative parametric ( auxiliary ) model that is both analytically and computationally easier to deal with . </S>",
    "<S> such an approach has been well explored in the classical literature but has received substantially less attention in the bayesian paradigm . </S>",
    "<S> the purpose of this paper is to compare and contrast a collection of what we call parametric bayesian indirect inference ( pbii ) methods . </S>",
    "<S> one class of pbii methods uses approximate bayesian computation ( referred to here as abc ii ) where the summary statistic is formed on the basis of the auxiliary model , using ideas from ii . </S>",
    "<S> another approach proposed in the literature , referred to here as parametric bayesian indirect likelihood ( pbil ) , uses the auxiliary likelihood as a replacement to the intractable likelihood . </S>",
    "<S> we show that pbil is a fundamentally different approach to abc ii . </S>",
    "<S> we devise new theoretical results for pbil to give extra insights into its behaviour and also its differences with abc ii . </S>",
    "<S> furthermore , we examine in more detail the assumptions required to use each pbii method . </S>",
    "<S> the results , insights and comparisons developed in this paper are illustrated on simple examples and two other substantive applications . </S>",
    "<S> the first of the substantive examples involves performing inference for complex quantile distributions based on simulated data while the second is for estimating the parameters of a trivariate stochastic process describing the evolution of macroparasites within a host based on real data . </S>",
    "<S> we create a novel framework called bayesian indirect likelihood ( bil ) that encompasses pbii as well as general abc methods so that the connections between the methods can be established .    , </S>"
  ]
}