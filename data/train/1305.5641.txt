{
  "article_text": [
    "photometric redshifts ( hereinafter photo - z ) provide an estimate of the redshift of sources obtained using photometry instead of spectroscopy .",
    "they are in fact driven by : ( i ) the shape of the broadband continuum of the object s spectroscopic emission , and ( ii ) by a limited number of strong spectral features ( i.e. the one at 4000  , the ly@xmath9 forest and the lyman limit ) , which are still recognizable after the integration of the spectral energy distribution ( sed ) sampled by the filter s transmission function .    at the price of lower accuracy ,",
    "photo - z offer several advantages with respect to their spectroscopic counterparts : ( i ) being derived from intermediate / broad band imaging , photo - z are much more effective in terms of observing time ; ( ii ) they may allow to probe objects much fainter than the spectroscopic flux limit and ( iii ) under specific conditions , they allow to correct some biases , such as those encountered at high redshift where , as it has been noticed @xcite , spectroscopy is pushed to its limits both by the low signal - to - noise ratio ( snr ) in the spectra and by the fact that , in many cases , even when a good signal - to - noise ratio is achieved , the lack of features in the observed spectral range may undermine the estimation of a trustworthy redshift @xcite .",
    "the latter aspect becomes crucial when photometric redshifts methods are applied to quasars ( qso ) and , in particular to the construction and characterization of the large , complete samples which are required by modern cosmology .",
    "in fact , quasar samples have always been , and still are , constructed either by compiling lists of more or less serendipitous discoveries obtained with different techniques and selection criteria @xcite , or via a two - step process where the first one consists in the identification of qso candidates from multi - wavelength surveys , and the second requires the spectroscopic validation of the candidates . in practice , due to the large amount of observing time required by spectroscopy , the latter step is usually optimized by applying the spectroscopic validation procedure just to a more or less significant subsample of the candidates , and then by extrapolating the resulting statistics to the whole sample .",
    "modern surveys are usually so deep and extensive that the number of candidates rapidly becomes too large to be handled with the latter approach . on the other hand , modern multi - wavelength",
    "digital surveys also provide such a wealth of information ( multi - band high accuracy photometry ) that it becomes feasible to approximate the sed of objects over a quite large range of redshifts @xcite , thus minimizing the need for spectroscopic follow - up .    in the last few years it has in fact been demonstrated that , after having provided an accurate enough photometry and significant wavelength coverage , it is possible to obtain samples of photometrically selected quasars matching the low contamination and high completeness @xcite required by many fields of modern cosmology .",
    "the relevance of these photometric samples will increase more and more in the near future , when the new generation of deeper and more accurate surveys will allow to access larger and more complete samples of qsos .",
    "these _ photometric _ samples are in fact already being used for a variety of applications such as the measurement of the integrated sachs ",
    "wolfe effect @xcite , the cosmic magnification bias @xcite , the clustering of quasars on large @xcite and small @xcite scales , to quote just a few .",
    "since both candidate selection and photometry redshift estimates are performed on the same data ( colors in many bands ) , it is also apparent that for the same samples , photometric data alone should carry enough information to characterize in an almost univocal way the sed and therefore also to derive accurate estimates of photometric redshifts @xcite .",
    "it goes without saying that the utility of the photometric samples goes hand in hand with the development of photo - z methods capable to provide accurate enough estimates of the redshifts .    in this paper",
    "we use a new empirical method , named multi layer perceptron with quasi newton algorithm or mlpqna , and apply it to the evaluation of photometric redshift of quasars . in section [ qso :",
    "sec : thedata ] we discuss the datasets used for the experiments and in section [ qso : sec : method ] we present both a detailed description of the mlpqna method and the statistical indicators used throughout the paper .",
    "we wish to stress that the lack of a common agreement on such indicators is among the main obstacles in comparing the performances of different methods . in section [ qso :",
    "sec : experiments ] we describe the experiments performed in order to select the best combination of input parameters , bands and network topology .",
    "the results of these experiments are summarized and discussed in section [ qso : sec : discussion ] , where we also present the final performances of the best experiments .",
    "finally , we compare our results with those available in literature and draw some general conclusions .",
    "a short appendix provides the reader with the math behind the quasi newton algorithm .",
    "the sample of quasars , used in the experiments described in this paper , is based on the spectroscopically selected quasars from the sdss - dr7 database ( table _ star _ of the sdss database ) . according to the spectroscopic classification index ( _ index sp _ or _ specclass _ ) provided in the sdss - dr7 release @xcite , we selected quasars , for which a reliable measure of the spectroscopic redshifts ( with _ zconf _ @xmath10 0.90 ) is available .",
    "we then cross - matched the sdss quasars sample identified as point sources with clean measured photometry in all filters ( _ ugriz _ ) , with the latest versions of the datasets from : galex  @xcite , ukidss  @xcite and wise  @xcite .",
    "these three surveys observed large fractions of the sky in the ultraviolet , near infrared and middle infrared spectral intervals , respectively .",
    "after the cross matching we obtained a series of multi - band catalogues , defined as it follows .",
    "* sdss - ( dr7 ) *  @xcite has observed @xmath11 deg@xmath12 of the sky in 5 bands ( _ ugriz _ ) covering the [ 3551 , 8931 ] wavelengths range .",
    "photometric sdss observations reach the limiting magnitude of 22.2 in the _ r _ band ( @xmath13 completeness for point sources ; @xcite ) .",
    "* galex - ( dr6/7 ) *  @xcite is a 2-band survey ( _ nuv , fuv _ for near and far ultraviolet respectively ) covering the [ 1300,3000 ] wavelength interval .",
    "the galex photometric survey has observed the whole sky to the near ultraviolet limiting magnitude @xmath14 .",
    "* ukidss - ( dr9 ) *  @xcite has been designed to be the sdss infrared counterpart and covers @xmath157000 deg@xmath12 of the sky in the _ yjhk _ near - infrared bands covering the @xmath15 0.9 to 2.4 @xmath16 m spectral range down to the limiting magnitude _",
    "_ k__=18.3 .",
    "the large area survey ( las ) has imaged @xmath17 deg@xmath12 ( overlapping with the sdss ) , with the additional _ y _ band down to the limiting magnitude of 20.5 .    the * wise * mission  @xcite has observed the entire sky in the mid - infrared spectral interval at 3.4 , 4.6 , 12 , and 22 @xmath16 m with an angular resolution of 6.1@xmath18 , 6.4@xmath18 , 6.5@xmath18 and 12.0@xmath18 in the four bands , achieving 5@xmath19 point source sensitivities of 0.08 , 0.11 , 1 and 6 mjy in unconfused regions on the ecliptic , respectively . the astrometric accuracy of wise is @xmath20 , and 1.4@xmath18 for the four wise bands , respectively .",
    "the transmission curves of all filters related with the four surveys are shown in fig .",
    "[ qso : fig : transmission_curves ] .",
    "all these surveys present a large common overlap region and overall good astrometry with comparable astrometric accuracy . in order to cross - match the catalogues we used a maximum radius @xmath21 to associate the optical quasars to counterparts in each of the three catalogs .",
    "afterwards we rejected all sources containing one or more missing data in any of their photometric parameters . in this case with the term _ missing data _ we mean undefined numerical values underlying either not detected or contaminated magnitude measurements .",
    "this last step is crucial in empirical methods since the presence of missing data might affect their generalization capabilities @xcite .",
    "the resulting number of objects in the datasets used for the experiments are :    * sdss : @xmath22 ; * sdss @xmath23 galex : @xmath24 ; * sdss @xmath23 ukidss : @xmath25 ; * sdss @xmath23 galex @xmath23 ukidss : @xmath26 ; * sdss @xmath23 galex @xmath23 ukidss @xmath23 wise : @xmath27 ;    an additional dataset was produced by decimating the final _ four - surveys _ cross - matched catalogue .",
    "this dataset was used to perform the preliminary feature - selection or _ pruning _ phase ( see sec .",
    "[ qso : featureselection ] ) and consisted of @xmath28 objects , each observed in 15 bands ( 4 ukidss , 2 galex , 5 sdss and 4 wise ) and with accurate spectroscopic redshift estimates .",
    "the decimation was needed to reduce the computational time needed to perform the large number of experiments described in what follows .",
    "for some bands there were multiple measurements ( i.e. magnitude measured accordingly to different definitions ) and therefore we are left with a total of 43 different features .    finally , in producing training and test sets we made sure that they had compatible spectroscopic redshifts distributions ( see fig .",
    "[ qso : fig : zspec_histograms ] ) .",
    "this section is dedicated to the description of the machine learning method used for the experiments .",
    "all mathematical details are reported in the appendix .      from a technical point of view ,",
    "the mlpqna method , is a multi layer perceptron ( mlp ; @xcite ) neural network trained by a learning rule based on the quasi newton algorithm ( qna ) ; in other words and as it is synthesized in the acronym , mlpqna differs from more traditional mlp s implementations in the way the optimal solution of the regression problem is found . in previous papers ,",
    "most of the characteristics of the method have been described in the contexts of both classification @xcite and regression @xcite .    according to @xcite , feed forward neural networks ( in their various implementations ) provide a general framework for representing non linear functional mappings between a set of input variables ( also called features ) and a set of output variables ( the targets ) .",
    "the training of a neural network can be in fact seen as the search for the function which minimizes the errors of the predicted values with respect to the true values available for a small but significant subsample of objects in the same parameter space .",
    "this subset is also called _ training set _ or _ knowledge base _ ( kb ) .",
    "the final performances of a specific neural network ( nn ) depend on many factors , such as topology , the way the minimum of the error function is searched and found , the way errors are computed , as well as the intrinsic quality of training data .",
    "the formal description of a feed - forward neural network with two computational layers is given in the eq .",
    "[ qso : eq0 ] :    @xmath29    equation [ qso : eq0 ] can be better understood by using a graph like the one shown in fig .",
    "[ qso : mlp ] .",
    "the input layer @xmath30 is made of a number of neurons ( also known as perceptrons ) equal to the number of input variables @xmath31 ; the output layer , on the other hand , will have as many neurons as the output variables @xmath32 .    in the general case",
    ", the network may have an arbitrary number of hidden layers ( in the depicted case there is just one hidden layer with three neurons ) , each of one can be formed by an arbitrary number of neurons @xmath33 . in a",
    "fully connected feed - forward network each node of a layer is connected to all the nodes in the adjacent layers .",
    "each connection is represented by an adaptive weight @xmath34 which can be regarded as the strength of the synaptic connection between neurons @xmath35 and @xmath36 , while the response of each perceptron to the inputs is represented by a non - linear function @xmath37 , referred to as the _ activation function_. all the above characteristics , the topology of the network and the weight matrix of its connections , define a specific implementation and are usually called _",
    "model_.    the model , however ,",
    "is only part of the story .",
    "in fact , in order to find the model that best fits the data in a specific problem , one has to provide the network with a set of examples , _ i d est _ of objects for which the final output is known by independent means .",
    "these data , already defined as _ training set _ or _ knowledge base _ , are used by the network to find the optimal model .    in our implementation",
    "we choose as learning rule the qna , which differs from the newton algorithm in how the hessian of the error function is computed .",
    "newtonian models are variable metric methods used to find local maxima and minima of functions @xcite and , in the case of mlps , they can be used to find the stationary ( i.e. the zero gradient ) point of the learning function .",
    "the complete mathematical description of the mlp with qna model is reported in the appendix [ qso : appendixa ] .",
    "the model has been made available to the community through the data mining & exploration web application resource ( dameware ; @xcite ) .      in this work",
    "we use our implementation of the qna based on the limited - memory bfgs ( l - bfgs ; @xcite ) , where bfgs is the acronym composed of the names of the four inventors @xcite .",
    "summarising , the algorithm for mlp with qna is the following .",
    "let us consider a generic mlp with @xmath38 being the weight vector at time @xmath39 .    1 .",
    "initialize all weights @xmath40 with small random values ( typically normalized in @xmath41 $ ] ) , set the constant error tolerance @xmath42 and @xmath43 ; 2 .",
    "present to the network all training set and calculate @xmath44 as the error function for the current weight configuration ; 3 .   if @xmath43 then @xmath45 4 .   else",
    "@xmath46 , where @xmath47 and @xmath48 ; 5 .",
    "calculate @xmath49 , where @xmath9 is obtained by line search equation ( see eq .",
    "[ qso : eq6 ] in the appendix ) ; 6 .",
    "calculate @xmath50 and @xmath51 for the next iteration , as reported in eq .",
    "[ qso : eq19 ] ; 7 .   if @xmath52 then @xmath53 and goto ( ii ) , else stop .",
    "as it is known , all _ line search _ methods , being based on techniques which search for the minimum error by exploring the error function surface , are likely to get stuck in a local minimum and many solutions to this problem have been proposed @xcite . in order to optimize",
    "the convergence of the gradient descent analysis ( gda , see appendix ) , newton s method uses the information on the second - order derivatives . by having information on the second derivatives ,",
    "qna is more effective in avoiding local minima of the error function and more accurate in the error function trend follow - up , thus revealing a _ natural _ capability to find the absolute minimum error of the optimization problem @xcite .    in the l - bfgs version of the algorithm ,",
    "in the case of high dimensionality ( i.e. input data with many parameters ) , the amount of memory required to store the hessian is too large , along with the machine time required to process it .",
    "therefore , instead of using a complete number of gradient values to generate the hessian , we can use a smaller number of values to approximate it .    by the way ,",
    "if the convergence slows down , performances may even increase . a statement which only a first sight might seem paradoxical but , while the convergence is measured by the number of iterations , the performances depend on the number of processor s time units spent to calculate the result .",
    "related to the computational cost there is also the strategy adopted in terms of stopping criteria for the method .",
    "as it is known , the process of adjusting the weights based on the gradients is repeated until a minimum is reached . in practice",
    ", one has to decide the stopping condition of the algorithm . among the possible criteria ,",
    "the algorithm could be terminated when : ( i ) the hessian approximation error becomes sufficiently small ( by definition the gradient will be zero at a minimum ) ; ( ii ) the maximum number of iterations is reached , in terms of a fixed threshold ; ( iii ) based on the cross validation .",
    "the cross validation can be used to monitor generalization performance during training and to terminate the algorithm when there is no more improvement .",
    "statistically significant results come out by trying multiple independent data partitions and then averaging the performances .",
    "there are several variants of cross validation methods @xcite .",
    "we , in particular , have chosen the k - fold cross validation , particularly suited in presence of a scarcity of known data samples @xcite .",
    "the mechanism , also known as _ leave - one - out _ , is quite simple , since it consists in dividing the training set of @xmath54 samples into @xmath35 subsets ( @xmath55 ) .",
    "the model is then trained on @xmath56 subsets and validated by testing it on the left out subset .",
    "this procedure is then iterated leaving out each time a different subset for validation and its mean squared error is averaged on all cycles .    for what the mlp topology is concerned ,",
    "a significant contribution came from the seminal paper by bengio & lecun @xcite .",
    "they in fact re - analysed the implications of the haykin pseudo - theorem @xcite , proving that complex problems , in which the mapping function is highly non linear and the local density of data in the parameter space is very variable , are better matched by _",
    "deep _ networks with more than one hidden computational layer .      in order to evaluate and reciprocally compare the experiments described in the next section we adopted the following definitions : + @xmath57    @xmath58 ^ 2 } } } { n } } \\label{qso : eq22}\\ ] ]    @xmath59    @xmath60    @xmath61    where @xmath19 is the standard deviation",
    ", mad is the median absolute deviation , nmad the normalized mad and rms is the root mean square .",
    "the term @xmath62 in all above expressions may be either @xmath63 defined as : + @xmath64    or the normalized residuals @xmath0 defined as : + @xmath65",
    "our approach is based on machine learning methods and therefore , it needs to be as automatic as possible , in order to optimize the decisional support to the user ( in this case the astronomer ) .",
    "therefore , the results of the individual experiments as well as their comparison with others , have to be evaluated in a consistent and objective manner through an homogeneous set of statistical indicators .",
    "in what follows we shall discuss the general experiment workflow and the outcome of the experiment phases .      for machine learning supervised methods",
    "it is common practice to use the available kb to obtain at least three disjoint subsets for every experiment : one ( training set ) for training purposes , i.e. to train the method in order to acquire the hidden correlation among the input features , which is needed to perform the regression ; the second one ( validation set ) to check the training , in particular against loss of generalization capabilities ( a phenomenon also known as overfitting ) ; and the third one ( test set ) to evaluate the overall performances of the model . as a rule of thumb in case of machine learning methods",
    ", these sets should be populated with respectively 60% , 20% and 20% of the objects in the kb @xcite . in our case , however , we reduced the training+validation data amount ( from 80% to 60% ) , driven by the past experience with the very accurate regression capabilities of the model also in case of smaller knowledge bases @xcite , obtaining implicitly the possibility to verify its prediction performance on a larger test set , as well as a faster execution of the training phase .",
    "furthermore , in order to ensure a proper coverage of the kb in the parameter space ( ps ) , the data objects were indeed divided up among the three datasets by random extraction , and usually this process is iterated several times to minimize the possible biases induced by fluctuations in the coverage of the ps , namely small differences in the redshift distribution of training and test samples used in the experiments .",
    "the first two criteria used to decide the stopping condition of the algorithm , as mentioned at the end of sec .",
    "[ qso : sec : mlpqna ] , are mainly sensitive to the choice of specific parameters and may lead to poor results if the parameters are improperly set .",
    "the cross validation does not suffer of such drawback ; it can avoid overfitting the data and is able to improve the generalization performance of the model .",
    "however , if compared to the traditional training procedures , the cross validation is much more computationally expensive .",
    "therefore , by exploiting the cross validation technique ( see sec .",
    "[ qso : sec : mlpqna ] ) , training and validation were indeed performed together using @xmath66 of the objects as a training + validation set , and the remaining @xmath67 as test set .",
    "the automatized process of the cross - validation was done by performing ten different training runs with the following procedure : ( i ) splitting of the training / validation set into ten random subsets , each one composed by 10% of the dataset ; ( ii ) at each training run we applied the 90% of the dataset and the excluded 10% for validation .    as remarked in sec .",
    "[ qso : sec : mlpqna ] , the k - fold cross validation is able to avoid overfitting on the training set @xcite , with an increase of the execution time estimable around @xmath56 times the total number of runs @xcite .    in terms of the internal parameter setup of the mlpqna",
    ", we need to consider the following topological parameters : +    * input layer : a variable number of neurons , corresponding to the pruned number of survey parameters used in all experiments , up to a maximum number of 43 nodes ( all available features ) ; * neurons on the first hidden layer : a variable number of hidden neurons , depending on the number @xmath54 of input neurons ( features in the dataset ) , equal to @xmath68 as rule of thumb ; * neurons on the second hidden layer : a variable number of hidden neurons , ranging from 0 ( to ignore the second layer ) to @xmath69 ; * output layer : one neuron , returning the reconstructed photo - z value .    for the qna learning rule , we heuristically fixed the following values as best parameters for the final experiments : +    * step : @xmath70 ( one of the two stopping criteria .",
    "the algorithm stops if the approximation error step size is less than this value . a step value equal to zero means to use the parameter maxit as the unique stopping criterion ) ; * res : @xmath71 ( number of restarts of hessian approximation from random positions , performed at each iteration ) ; * dec : @xmath72 ( regularization factor for weight decay .",
    "the term @xmath73 is added to the error function , where @xmath74 is the total number of weights in the network , directly depending on the total number of neurons inside . when properly chosen , the generalization performances of the network are highly improved ) ; * maxit : @xmath75 ( max number of iterations of hessian approximation . if zero the step parameter is used as stopping criterion ) ; * cv ( k ) : @xmath76 ( k - fold cross validation , with @xmath77 ) ; * error evaluation : mean square error ( between target and network output ) .    with these parameters",
    ", we obtained the statistical results reported in sec .",
    "[ qso : complexityselection ] .",
    "as it is known , supervised machine learning models are powerful methods for learning the hidden correlation between input and output features from training data .",
    "of course , their generalization and prediction capabilities strongly depend on : the intrinsic quality of data ( signal - to - noise ratio ) , the level of correlation among different features ; the amount of missing data present in the dataset @xcite .",
    "it is obvious that some , possibly many , of the @xmath78 parameters listed in tab .",
    "[ qso : tab : features ] may not be independent and that their number needs to be reduced in order to speed up the computation ( which scales badly with the number of features ) . this is a common problem in data mining and there is a wide literature on how to optimize the selection of features which are most relevant for a given purpose @xcite .",
    "this process is usually called _ feature selection _ or _ pruning _ , and consists in finding a compromise between the number of features ( and therefore the computational time ) and the required accuracy of the final results . in order to do so , we extracted from the main catalogue several subsets containing different groups of variables ( features ) .",
    "each one of these subsets was then analyzed separately in specific runs of the method ( runs which in the data mining community are usually called experiments ) , in order to allow the comparison and evaluation .",
    "we wish to stress that our main concern was not only to disentangle which bands carry the most information but also , for a given band , which type of measurements ( e.g. point spread function , petrosian or isophotal magnitude ) are more effective .",
    "we performed a series of regression experiments to evaluate the performances obtained by the pruning of photometric quantities on the small dataset described in sec .",
    "[ qso : sec : thedata ] .",
    "the pruning experiments consisted into several combinations of surveys and their features : +    * a _ full _ features experiment to be used as a benchmark for all the other experiments ; * some _ service _",
    "experiments used to select the best combination of input features in order to eliminate redundancies in the flux measurements ( i.e. , petrosian magnitudes against isophotal magnitudes ) ; * _ three - survey _ experiments for all possible combinations of three ( out of four ) surveys ; * _ two - survey _ experiments with all possible combinations of two ( out of four ) surveys ; * _ single - survey _ experiments .",
    "the output of the experiments consisted of lists of photometric redshift estimates for all objects in the kb .",
    "all pruning experiments were performed using @xmath79 objects in the training set and @xmath80 in the test set . in tab .",
    "[ qso : tab : experiments ] , we list the outcome of the experiments for the feature selection .",
    "both @xmath81 and @xmath82 were computed using the objects in the test set alone .",
    "as it can be seen , among the various types of magnitudes available for galex and ukidss , the best combination is obtained using the isophotal magnitudes for galex and the calibrated magnitudes ( @xmath83 ) for ukidss .    therefore at the end of the pruning phase the best combination of features turned out to be : the five sdss @xmath84 , the two isophotal magnitudes of galex , the four @xmath83 for ukidss and the four magnitudes for wise .",
    "once the most significant features had been identified , we had to check which types of flux combinations were more effective , in terms of magnitudes or related colors .",
    "experiments were performed on all five cross - matched datasets listed in section [ qso : sec : thedata ] .",
    "as it could be expected , the optimal combination turned out to be always the mixed one , i.e the one including colors and one reference magnitude for each of the included surveys ( r for sdss , nuv for galex , k for ukidss and w4 for wise ) . from the data mining point of view",
    "this is rather surprising since the amount of information should not change by applying linear combinations between features .",
    "but from the physical point of view this can be easily understood by noticing that even though colors are derived as a subtraction of magnitudes , the content of information is quite different , since an ordering relationship is implicitly assumed , thus increasing the amount of information in the final output ( gradients instead of fluxes ) .",
    "the additional reference magnitude instead removes the degeneracy in the luminosity class for a specific galaxy type .",
    "the final check was about the hierarchical complexity of the network in terms of number of internal layers , whether _ shallow _ or _ deep _ according the definitions in @xcite , where _ deep _ is referred to a feed - forward network with more than one hidden layer . the above quoted cross - matched datasets were therefore processed through both a three - layers ( input + hidden + output ) and a four - layers ( input + 2 hidden layers + output ) network . in all cases the four - layers network performed significantly better , thus confirming the performance enhancement with _ deep _ networks in case of a particularly complex non - linear regression cases , i.e. in case of a highly multi - variate distributions of the input parameter space .",
    "the experiments with best results have been obtained using a four - layers network , trained on the mixed ( colors + reference magnitudes ) datasets and their statistics are reported in tables [ qso : tab : comparison1 ] , [ qso : tab : comparison2 ] , [ qso : tab : compoutliers ] and [ qso : tab : catastrop ] .",
    "in 2002 we begun to explore the usage of mlp s for the evaluation of photo - z both for _ normal _ galaxies and quasars @xcite .",
    "several years later , @xcite used a combination of two mlp s to correct for the degeneracy introduced by the inhomogeneities in the knowledge base .",
    "then @xcite demonstrated that the subtleties in the mapping function could be more easily captured using the so - called _ wge ( weak gated experts ) _ method , a hierarchical combination of mlp s each specialized in a specific partition of the parameter space , whose individual outputs were combined by an additional mlp .    furthermore , @xcite published a seminal paper which somehow has disproved the haykin - pseudo theorem @xcite , pointing out that problems with a large amount of distribution irregularities in the parameter space , are better treated by what they defined as _ deep _ networks , i.e. networks with more than one computational ( hidden ) layer . in this paper",
    "we exploited @xcite findings , by using the supervised machine learning based method mlpqna to evaluate photometric redshifts of quasars using multi - band data obtained from the cross - matching of the galex , sdss , ukidss and wise surveys .    in the tables",
    "[ qso : tab : comparison1 ] , [ qso : tab : comparison2 ] and [ qso : tab : compoutliers ] we compare our best results to those presented by other authors @xcite , in terms of an homogeneous set of statistical indicators , defined in sec .",
    "[ qso : sec : stat ] .",
    "unfortunately , the whole set of indicators was not available for all bibliographical sources and in several cases we could only use a few quantities .",
    "results are listed according to the combinations of surveys used in the experiment .    the best experiment , which makes use of a selected combination of parameters drawn from the four cross - matched surveys , leads to a @xmath85 and a median absolute deviation @xmath4 . the fraction of catastrophic outliers , i.e. of objects with photo - z deviating more than @xmath6 from the spectroscopic value is @xmath7 , leading to a @xmath86 after their removal ( as reported in tab .  [ qso : tab : catastrop ] ) .",
    "the larger the number of surveys ( bands ) used , the more accurate are the results .",
    "this result , which might seem evident , is not obvious at all , since the higher amount of information carried by the additional bands implies also a strong decrease in the number of objects which are contained in the training set and should therefore cause a decrease in the generalization performances of the network .",
    "this result , together with the fact that mlpqna performs well also with small kb s @xcite , seems particularly interesting , since it has far reaching implications on ongoing and future surveys : a better photometric coverage is much more relevant than an increase of spectroscopic templates in the kb .    concerning the performance evaluation in terms of photometric redshift reconstruction , all statistical results reported throughout this paper",
    "are referred to test data sets only .",
    "in fact , it is good practice to evaluate the results on data ( i.e. the test set ) which have never been presented to the network during the training and/or validation phases . the usage of _ test plus training _ data might introduce an obvious positive systematic bias which could mask reality .",
    "more in general , empirical methods , such as mlpqna , have the advantage that the training set is made up of real sky objects .",
    "hence they do not suffer from the uncertainty of having accurate templates . in this sense",
    "any empirical method intrinsically includes effects such as the filter band - pass and flux calibrations .",
    "in fact , as deeply discussed by @xcite , one of the main drawbacks of these methods is the difficulty in extrapolating to regions of the input parameter space that are not well sampled by the training data .",
    "therefore the efficiency of empirical methods degrades for objects at fainter magnitudes than those included in the training set , as this would require an extrapolation capability on data having properties , such as redshift and photometry , not included in the learned sample .",
    "in fact , another strong requirement of such methods is that the training set must be large enough to cover properly the parameter space in terms of colors , magnitudes , object types and redshift . in this case",
    "the calibrations and corresponding uncertainties are well known and only limited extrapolations beyond the observed locus in color - magnitude space are required . in conclusion , under the conditions described above about the consistency of the training set , a realistic way to measure photometric uncertainties is to compare the photometric redshifts estimation with spectroscopic measures in the test samples .",
    "as it can be seen in the tables [ qso : tab : comparison1 ] , [ qso : tab : comparison2 ] and [ qso : tab : compoutliers ] , in all cases mlpqna obtains very relevant results . only in the sdss+galex case , the non - normalized quantities ( i.e. those referred to the error @xmath87 ) show a substantial agreement between our results and those by @xcite .",
    "the better performances of mlpqna in the normalized indicators ( i.e. those referred to the error @xmath88 ) , is a consequence of the better performances of the mlpqna method in terms of fraction of catastrophic outliers .    we wish to stress that both our four - layers mlpqna and the _ wge _ method discussed in @xcite take advantage of a substantial improvement in complexity with respect to the traditional three - layers mlp networks used in the literature , and therefore deal better with the complexity of the multi - color parameter space .",
    "average statistical indicators such as bias and standard deviation , however , provide only part of the information which allows to correctly evaluate the performances of a method and , for instance , they provide only very little evidence of the systematic trends which are observed as a sudden increase in the residuals spread over specific regions of the redshift space @xcite . in the worst cases , these regions correspond to degeneracies in the parameter space and , as it could be expected , the relevance of such degeneracies decreases for increasing number of bands .    for what the analysis of the catastrophic outliers is concerned , according to @xcite , the parameter @xmath89 enables the identification of outliers in photometric redshifts derived through sed fitting methods ( usually evaluated through numerical simulations based on mock catalogues ) .",
    "in fact , in the hypothesis that the redshift error @xmath90 is gaussian , the catastrophic redshift error limit would be constrained by the width of the redshift probability distribution , corresponding to the @xmath13 confidence interval , i.e. with @xmath91 . in our case , however , photo - z are empirical , i.e. not based on any specific fitting model and it is preferable to use the standard deviation value @xmath92 derived from the photometric cross matched samples , although it could overestimate the theoretical gaussian @xmath19 , due to the residual spectroscopic uncertainty as well as to the method training error .",
    "therefore , we consider as catastrophic outliers the objects with @xmath93 .",
    "it is also important to notice that for empirical methods it is useful to analyze the correlation between the @xmath94 and the standard deviation @xmath95 calculated on the data sample for which @xmath96 .",
    "in fact , the quantity @xmath97 would be comparable to the value of the @xmath98 .    as it is shown in tab .",
    "[ qso : tab : catastrop ] , in our data the @xmath95 is always slightly larger than the corresponding @xmath99 , which is exactly what is expected due to the overestimate induced by the above considerations ( see also fig .",
    "[ qso : fig : histogram ] ) .",
    "finally , we would like to stress that the difficulties encountered by us and by other teams in comparing different methods , especially in light of the crucial role that photo - z play in the scientific exploitation of present and future large surveys ( cf .",
    "@xcite , @xcite , @xcite ) , confirm that it would be desirable to re - propose an upgraded version of the extremely useful phat contest ( @xcite , @xcite ) , which allowed a direct , effective and non ambiguous comparison of different methods applied on the same datasets and through the same set of statistical indicators .",
    "this new contest should be applied to a much larger dataset , with a more practical selection of photometric bands , and should take into account also other parameters such as scalability and robustness of the algorithms , as well as the degeneracy characterization .      the authors would like to thank the anonymous referee for the comments and suggestions which helped us to improve the paper . the authors wish to thank the whole dameware team , for the many useful discussions .",
    "+ the authors also wish to thank the financial support of project f.a.r.o .",
    ", @xmath100 call by the university federico ii of naples , and of the prin - miur 2011 for euclid mission .",
    "+ one of us ( gl ) wishes to thank prof g.s .",
    "djorgovski and the whole department of astronomy at the california institute of technology in pasadena , for hospitality .",
    "+ am and mb wish to thank the financial support of prin - inaf 2010 , _ architecture and tomography of galaxy clusters_. + r. da . acknowledges the financial support of the us virtual astronomical observatory , which is sponsored by the national science foundation and the national aeronautics and space administration",
    ". +    ccll galex & nuv , fuv & mag , mag_iso & near and far uv total and isophotal mags + & & mag_aper_1 mag_aper_2 mag_aper_3 & phot . through 3 , 4.5 and 7.5 arcsec apertures",
    "+ & & mag_auto and kron_radius & magnitudes and kron radius in units of a or b + sdss & u , g , r , i , z & psfmag & psf fitting magnitude in the u g , r , i , z bands .",
    "+ ukidss & y , j , h , k & psfmag & psf fitting magnitude in @xmath101 bands + & & apermag3 , apermag4 , apermag6 & aperture photometry through 2 , 2.8 & 5.7@xmath18 + & & & circular aperture in each band + & & hallmag , petromag & calibrated magnitude within circular + & & & aperture r_hall and petrosian magnitude + & & & in @xmath101 bands + wise & w1 , w2 , w3 , w4 & w1mpro , w2mpro , w3mpro , w4mpro & w1 : 3.4 @xmath102 and 6.1@xmath18 angular resolution ; + & & & w2 : 4.6 @xmath102 and 6.4@xmath18 angular resolution ; + & & & w3 : 12 @xmath102 and 6.5@xmath18 angular resolution ; + & & & w4 : 22 @xmath102 and 12@xmath18 angular resolution .",
    "+ & & & magnitudes measured with profile - fitting photometry + & & & at the 95% level .",
    "brightness upper limit if the flux + & & & measurement has snr@xmath103 + sdss & - & @xmath104 & spectroscopic redshift +    cccccc x & x & x & x & 0.0033 & 0.174 + x@xmath105 & x & x@xmath106 & x & -0.0001 & 0.152 + x@xmath107 & x & x@xmath106 & x & -0.0016 & 0.165 + x@xmath108 & x & x@xmath106 & x & 0.0054 & 0.151 + x@xmath109 & x & x@xmath106 & x & -0.0026 & 0.151 + x@xmath110 & x & x@xmath106 & x & -0.0008 & 0.152 + x@xmath111 & x & x@xmath106 & x & 0.0041 & 0.163 + x@xmath112 & x & x@xmath106 & x & -0.0033 & 0.155 + & & x@xmath113 & & -0.0091 & 0.299 + & & x@xmath114 & & 0.0111 & 0.465 + & & x@xmath106 & & -0.0081 & 0.294 +    cccccc x@xmath109 & x & x@xmath106 & x & -0.0026 & 0.151 + x@xmath109 & x & x@xmath106 & & -0.0046 & 0.152 + x@xmath109 & x & & x & 0.0025 & 0.162 + & x & x@xmath106 & x & -0.0032 & 0.179 + x@xmath109 & & x@xmath106 & x & 0.0110 & 0.203 + & & x@xmath106 & x & 0.0045 & 0.236 + x@xmath109 & & & x & 0.0175 & 0.288 + & x & x@xmath106 & & -0.0027 & 0.210 + & x & & x & -0.0039 & 0.197 + x@xmath109 & x & & & -0.0055 & 0.240 + x@xmath109 & & x@xmath106 & & 0.0133 & 0.238 + & & & x & 0.0165 & 0.297 + & x & & & -0.0162 & 0.338 + x@xmath105 & & & & 0.0550 & 0.419 + & & x@xmath106 & & -0.0081 & 0.294 +    ccccc mlpqna & 0.007 & 0.25 & 0.102 & 0.26 + bovy et al .",
    "& - & 0.46 & - & - + laurino et al . &",
    "0.210 & 0.28 & 0.110 & 0.35 + ball et al .",
    "& - & 0.35 & - & - + richards et al . & - & 0.52 & - & - + mlpqna & 0.003 & 0.21 & 0.060 & 0.22 + bovy et al .",
    "& - & 0.26 & - & - + laurino et al .",
    "& 0.13 & 0.21 & 0.061 & 0.25 + ball et al . & - & 0.23 & - & - + richards et al . & - & 0.37 & - & - + mlpqna & 0.001 & 0.25 & 0.066 & 0.26 + bovy et al . & - & 0.28 & - & - + mlpqna & 0.0009 & 0.18 & 0.043 & 0.19 + bovy et al . & - & 0.21 & - & - + mlpqna & 0.002 & 0.15 & 0.040 & 0.15 +    cccccc mlpqna & 0.032 & 0.15 & 0.039 & 0.17 & 0.058 + laurino et al . &",
    "0.095 & 0.16 & 0.041 & 0.19 & - + ball et al . & 0.095 & 0.18 & - & - & - + richards et al . &",
    "0.115 & 0.28 & - & - & - + mlpqna & 0.012 & 0.11 & 0.029 & 0.11 & 0.043 + laurino et al . &",
    "0.058 & 0.29 & 0.029 & 0.11 & - + ball et al . &",
    "0.06 & 0.12 & - & - & - + richards et al .",
    "& 0.071 & 0.18 & - & - & - + mlpqna & 0.008 & 0.11 & 0.027 & 0.11 & 0.040 + mlpqna & 0.005 & 0.087 & 0.022 & 0.088 & 0.032 + mlpqna & 0.004 & 0.069 & 0.020 & 0.069 & 0.029 +    ccccc & @xmath115 & @xmath116 & @xmath117 & @xmath118 + mlpqna & 7.68 & 0.38 & 6.53 & 1.24 + bovy et al .",
    "& & 0.51 + mlpqna & 4.88 & 1.61 & 4.57 & 1.37 + bovy et al . & & 1.86 + mlpqna & 4.00 & 1.73 & 3.82 & 1.38 + bovy et al . & & 1.92 + mlpqna & 2.86 & 1.47 & 3.05 & 0.23 + bovy et al . & & 1.13 + mlpqna & 2.57 & 0.87 & 2.88 & 0.91 +    lccccc sdss & 41431 & 0.15 & 6.53 & 0.062 & 0.058 + sdss + galex & 17876 & 0.11 & 4.57 & 0.045 & 0.043 + sdss+ukidss & 12438 & 0.11 & 3.82 & 0.041 & 0.040 + sdss+galex+ukidss & 5836 & 0.087 & 3.05 & 0.040 & 0.032 + sdss+galex+ukidss+wise & 5716 & 0.069 & 2.88 & 0.035 & 0.029 +    , title=\"fig : \" ] +    ]    ]     distributions for all five cross - matched test data sets .",
    "lines are referred to , respectively , sdss ( gray ) , sdss+galex ( blue ) , sdss+ukidss ( green ) , sdss+galex+ukidss ( red ) and sdss+galex+ukidss+wise ( black).[qso : fig : histogram ] ]     vs @xmath119 ) ; ( a ) sdss , ( b ) sdss+galex , ( c ) sdss+ukidss , ( d ) sdss+galex+ukidss and ( e ) sdss+galex+ukidss+wise .",
    "all diagrams refer to results on test sets .",
    "gray points are catastrophic outliers ( defined in tab .",
    "[ qso : tab : compoutliers ] ) .",
    "red line is the dot - to - dot straight line passing through photometric and spectroscopic redshift limits in the available knowledge base.[qso : fig : scattertestset ] ]",
    "most newton methods use the hessian of the function to find the stationary point of a quadratic form .",
    "it needs to be stressed , however , that the hessian of a function is not always available and in many cases it is far too complex to be computed in an analytical way .",
    "more often it is easier to compute the function gradient which can be used to approximate the hessian via @xmath54 consequent gradient calculations . in order to better understand why qna are so powerful , it is convenient to start from the classical and quite common gradient descent algorithm ( gda ) used for back propagation @xcite . in gda ,",
    "the direction of each updating step for the mlp weights is derived from the error descent gradient , while the length of the step is determined from the learning rate . in case of particularly complex problems",
    "this method is inaccurate and ineffective and therefore may get stuck in local minima .",
    "a more effective approach is to move towards the negative direction of the gradient ( _ line search direction _ ) not by a fixed step , but by moving towards the minimum of the function along that direction .",
    "this can be achieved by first deriving the descent gradient and then by analyzing it with the variation of the learning rate @xcite .",
    "let us suppose that at step @xmath120 , the current weight vector is @xmath38 , and let us consider a search direction @xmath121 . if we select the parameter @xmath122 in order to minimize @xmath123 , the new weight vector can be expressed as :        and the problem of _ line search _ becomes a 1-dimensional minimization problem which can be solved in many different ways .",
    "simple variants are : i ) to move @xmath125 by varying @xmath122 by small intervals , then evaluate the error function at each new position , and stop when the error begins to increase , or ii ) to use the parabolic search for a minimum and compute the parabolic curve crossing pre - defined learning rate points .",
    "the minimum @xmath126 of the parabolic curve is a good approximation of the minimum of @xmath125 and it can be derived by means of the parabolic curve which crosses the fixed points with the lowest error values .",
    "another approach makes instead use of _ trust region _ based strategies which minimize the error function , by iteratively growing or contracting the region of the function by adjusting a quadratic model function which best approximates the error function . in this sense",
    "this technique can be considered as a dual to line search , since it tries to find the best size of the region by fixing the step size ( while the line search strategy always chooses the step direction before selecting the step size ) , @xcite .",
    "all these approaches , however , rely on the assumption that the optimal search direction is given at each step by the negative gradient : an assumption which not only is not always true , but can also lead to serious wrong convergence .",
    "in fact , if the minimization is done along the negative gradient direction , the subsequent search direction ( the new gradient ) will be orthogonal to the previous one : in fact , note that when the line search founds the minimum , it is : @xmath127 and hence , @xmath128 where @xmath129 .",
    "the iteration of the process therefore leads to oscillations of the error function which slow down the convergence process .",
    "the method implemented here relies on selecting other directions so that the gradient component , parallel to the previous search direction , would remain unchanged at each step .",
    "suppose that you have already minimized with respect to the direction @xmath130 starting from the point @xmath38 and reaching the point @xmath131 , where eq .",
    "[ qso : eq3 ] becomes : @xmath132 by choosing @xmath133 so to preserve the gradient component parallel to @xmath130 equal to zero , it is possible to build a sequence of directions @xmath126 in such a way that each direction is conjugated to the previous one on the dimension @xmath134 of the search space ( this is known as conjugate gradients method ; @xcite ) . in presence of a squared error function ,",
    "the update weights algorithm is : @xmath135 with : @xmath136    furthermore , @xmath126 can be obtained for the first time via the negative gradient and in the subsequent iterations , as a linear combination of the current gradient and of the previous search directions : @xmath137 with : @xmath138    this algorithm finds the minimum of a square error function at most in @xmath134 steps but at the price of a high computational cost , since in order to determine the values of @xmath9 and @xmath139 , it makes use of that _ hessian matrix h _ which , as we already mentioned , is very demanding in terms of computing time .",
    "a fact which puts serious constraints on the application of this family of methods to medium / large data sets .",
    "excellent approximations for the coefficients @xmath9 and @xmath139 can , however , be obtained from analytical expressions that do not use the hessian matrix explicitly . for instance , @xmath139 can be calculated through any one of the following expressions ( respectively @xcite ) :          these expressions are all equivalent if the error function is square - typed , otherwise they assume different values . typically the polak - ribiere equation obtains better results because , if the algorithm is slow and subsequent gradients are quite alike between them , its equation produces values of @xmath139 such that the search direction tends to assume the negative gradient direction @xcite .    concerning the parameter @xmath9 , its value can be obtained by using the line search method directly .",
    "the method of conjugate gradients reduces the number of steps to minimize the error up to a maximum of @xmath134 because there could be almost @xmath134 conjugate directions in a @xmath134-dimensional space . in practice",
    "however , the algorithm is slower because , during the learning process , the property _ conjugate _ of the search directions tends to deteriorate .",
    "it is useful , to avoid the deterioration , to restart the algorithm after @xmath134 steps , by resetting the search direction with the negative gradient direction .    by using a local square approximation of the error function",
    ", we can obtain an expression for the minimum position .",
    "the gradient in every point @xmath143 is in fact given by : @xmath144 where @xmath145 corresponds to the minimum of the error function , which satisfies the condition : @xmath146    the vector @xmath147 is known as newton direction and it is the base for a variety of optimization strategies , such as for instance the qna , which instead of calculating the @xmath148 matrix and then its inverse , uses a series of intermediate steps of lower computational cost to generate a sequence of matrices which are more and more accurate approximations of @xmath149 . from the newton formula ( eq . [ qso : eq13 ] ) we note that the weight vectors on steps @xmath120 and @xmath150 are correlated to the correspondent gradients by the formula : @xmath151 which is known as _ quasi newton condition_. the approximation @xmath152 is therefore built in order to satisfy this condition .",
    "the formula for @xmath152 is : @xmath153 where the vectors are : @xmath154    using the identity matrix to initialize",
    "the procedure is equivalent to consider , step by step , the direction of the negative gradient while , at each next step , the direction @xmath155 is for sure a descent direction .",
    "the above expression could carry the search out of the interval of validity for the squared approximation .",
    "the solution is hence to use the _ line search _ to found the minimum of function along the search direction . by using such system , the weight updating expression ( eq .",
    "[ qso : eq5 ] ) can be formulated as follows :",
    "@xmath156 where @xmath9 is obtained by the _ line search_.    one of the main advantages of qna , compared with conjugate gradients , is that the _ line search _ does not require the calculation of @xmath9 with a high precision , because it is not a critical parameter .",
    "unfortunately , however , again , it requires a large amount of memory to calculate the matrix @xmath152 ( @xmath157 ) , for large @xmath134 .",
    "one way to reduce the required memory is to replace at each step the matrix @xmath152 with a unitary matrix . with such replacement and after multiplying by @xmath37 ( the current gradient ) , we obtain : @xmath158      abazajian , k.n . et al .",
    "2009 , apjs , 182 , 2 , 543 - 558 aihara , h. , et al .",
    ", 2011 , apjs , 193 , 29 ball , n. m. et al .",
    "2008 , apj , 683 , 1 , 12 - 21 baum , w. a. , 1962 , in problems of extra - galactic research , proceedings from iau symposium no .",
    "15 , edited by mcvittie , g. c. , 390 baum , e. , and wilczek , f. , 1988 , supervised learning of probability distributions by neural networks .",
    "neural information processing systems , anderson , d.z . ed .",
    ", american institute of physics , new york , pp .",
    "52 - 61 bengio , y. , & lecun , j. , 2007 , in large - scale kernel machines . mit press .",
    "bishop , c. m. , 2006 , pattern recognition and machine learning .",
    "springer isbn 0 - 387 - 31073 - 8 .",
    "bovy , j. , et al .",
    "; astrophysical journal , 749 , 41 .",
    "brescia , m. , cavuoti , s. , paolillo , m. , longo , g. , puzia , t. , 2012a , monthly notices of the royal astronomical society , volume 421 , issue 2 , pp .",
    "1155 - 1165 .",
    "brescia , m. , 2012 , new trends in e - science : machine learning and knowledge discovery in databases . in horizons in computer science research , thomas s. clary ( eds . ) , series horizons in computer science vol . 7 , nova science publishers , isbn : 978 - 1 - 61942 - 774 - 7 .",
    "broyden , c. g. , 1970 , journ . of the inst . of math . and its appl . , 6 , 76 budavari , t. , et al . , 2001 , aj , 122 , 1163 byrd , r.h . ,",
    "nocedal , j. , schnabel , r.b . , 1994 , mathematical programming , 63 , 4 , pp .",
    "129 - 156 cavuoti , s. , brescia , m. , longo , g. , mercurio , a. , 2012a , a&a , vol .",
    "546 , a13 , pp .",
    "1 - 8 arxiv:1206.0876 .",
    "cavuoti , s. , brescia , m. , longo , g. , garofalo , m. , nocella , a. , 2012b , science - image in action , world scientific publishing , pp .",
    "241 - 247 celis , m. , dennis , j.  e. , tapia , r.  a. , 1985 , numerical optimization , p. boggs , r. byrd and r. schnabel ( eds . ) , siam , philadelphia usa , pp .",
    "chambers , k.c . , 2011 , status and early science from the ps1 science mission , american astronomical society .",
    "bulletin of the american astronomical society , vol .",
    "collister , a.a . , & lahav , o. , 2004 , pasp , vol .",
    "116 , issue 818 , 345 - 351 connolly , a. j. , csabai , i. , szalay , a. s. , koo , d. c. , kron , r. g. , & munn , j. a. , 1995 , aj , 110 , 2655 dabrusco , r. , et al .",
    "2009 , mnras , 396 , 1 , pp .",
    "223 - 262 dabrusco , r. , et al .",
    "2007 , apj , 663 , 2 , pp .",
    "752 - 764 davidon , w.c . , 1968 ,",
    "j. 10 , 406 the dark energy survey collaboration , 2005 , the dark energy survey , white paper submitted to the dark energy task force , 42 pages , arxiv:0510346 fernandez - soto , a. , lanzetta , k.m . ,",
    "chen , h.w .",
    ", pascarelle , s.m . ,",
    "yahata , n. , 2001 , apj supplement series , 2001 , 135 , pp .",
    "41 - 61 ( 39 ) fletcher , r. , 1970 , computer journal 13 : 317 fletcher , r. , reeves , c. m. , 1964 , function minimization by conjugate gradients .",
    "j. 7 , 2 , 149 - 154 .",
    "mr 0187375 floudas , c.a . , & jongen , h.  t. , 2005 , journal of global optimization , vol .",
    "32 , number 3 , 409 - 415 fu , limin . , 1994 , neural networks in computer intelligence .",
    "munson and l. goldberg ( eds . ) , mcgraw - hill ny geisser , s. , 1975 , journal of the american statistical association , 70 ( 350 ) , 320 - 328 .",
    "giannantonio , t. , et al . , 2006 , phys .",
    "d , 74 , 063520 giannantonio , t. , scranton , r. , crittenden , r. g. , nichol , r. c. , boughn , s. p. , myers , d. , & richards , g. t. , 2008 , phys .",
    "d , 77 , 123520 goldfarb , d. , 1970 , mathematics of computation , 24 , 23 golub , g.  h. , & ye , q. , 1999 , siam journal of scientific computation , vol .",
    "1305 - 1320 guyon , i. , elisseeff , a. , 2003 , an introduction to variable and feature selection , journal of machine learning research , vol .",
    "1157 - 1182 guyon , i. , elisseeff , a. , 2006 , in feature extraction , foundations and applications , guyon , i. ; gunn , s. ; nikravesh , m. ; zadeh , l. a. editors ; series : studies in fuzziness and soft computing , springer , vol .",
    "207 haykin , simon , 1998 .",
    "neural networks : a comprehensive foundation , vol .",
    "2 , prentice hall hennawi , j. f. , et al . , 2006 , aj , 131 , 1 hestenes , m. r. , stiefel , e. : methods of conjugate gradients for solving linear systems .",
    "standards 49 ( 1952 ) , 6 , 409 - 439 .",
    "mr 0060307 hildebrandt , h. , et al .",
    ", 2010 , phat : photo - z accuracy testing , astronomy and astrophysics , vol .",
    "523 , 21 pp .",
    "kearns , m. , 1996 , a bound on the error of cross validation using the approximation and estimation rates , with consequences for training - test split , neural information processing 8 , d.s .",
    "touretzky , m , c . mozer and m.e .",
    "hasselmo ( eds . ) , morgan kaufmann , pp .",
    "183 - 189 lanzetta , k.m . ,",
    "yahil , a. , fernandez - soto , a. , 1998 , astronomical journal , 116 , pp .",
    "1066 - 1073 ( 18 ) laurino , o. , dabrusco , r. , longo , g. , riccio , g. , 2011 , mnras , 418 , 4 , pp .",
    "2165 - 2195 lawrence , a. , et al . , 2007 , mnras , 379 , 1599 lindeberg , t. , 1998 , feature detection with automatic scale selection , international journal of computer vision 30 ( 2 ) , pp 77 - 116 marlin , b.m . , 2008 , missing data problems in machine learning , library and archives : canada .",
    "martin , d.  c. , et al . , 2005 ,",
    "apj , 619 , l1 myers , a. d. , brunner , r. j. , richards , g. t. , nichol , r. c. , schneider , d. p. , vanden berk , d. e. , scranton , r. , gray , a. g. , & brinkmann , jon , 2006 , apj , 638 , 622 mobasher , b. , capak , p. , scoville , n. z. , dahlen , t. , salvato , m. , aussel , h. , thompson , d. j. , feldmann , r. , tasca , l. , le fevre , o. , lilly , s. , carollo , c. m. , kartaltepe , j. s. , mccracken , h. , mould , j. , renzini , a. , sanders , d. b. , shopbell , p. l. , taniguchi , y. , ajiki , m. , shioya , y. , contini , t. , giavalisco , m. , ilbert , o. , iovino , a. , le brun , v. , mainieri , v. , mignoli , m. , scodeggio , m. , 2007 . the astrophysical journal supplement series , volume 172 , issue 1 , pp .",
    "117 - 131 myers , a. d. , brunner , r. j. , nichol , r. c. , richards , g. t. , schneider , d. p. & bahcall , n. a. , 2007a , apj , 658 , 85 myers , a. d. , brunner , r. j. , nichol , r. c. , richards , g. t. , schneider , d. p. & bahcall , n. a. , 2007b , apj , 658 , 99 polak , e. , ribiere , g. , 1969 , note sur la convergence de methodes des directions conjugees .",
    "rech . oper .",
    "16-r1 , 35 - 43 .",
    "mr 0255025 refregier , a. , et al . ,",
    "2010 , euclid imaging consortium science book , arxiv:1001.0061 richards , g. t. , et al .",
    ", 2001a , aj , 121 , 2308 richards , g. t. , et al .",
    ", 2001b , aj , 122 , 1151 richards , g. t. , et al . , 2002 ,",
    "aj , 123 , 2945 richards , g. t. et al .",
    "2009 , apjs , 180 , 67 - 83 ripley , b.d . , 1996 ,",
    "pattern recognition and neural networks , cambridge university press rubinstein , r.y . ,",
    "kroese , d.p . , 2004 ,",
    "the cross - entropy method : a unified approach to combinatorial optimization , monte - carlo simulation and machine learning .",
    "springer - verlag , new york ny schneider , d. p. , richards , g. t. , hall , p. b. , strauss , m. a. , anderson , s. f. , boroson , t. a. , ross , n. p. , shen , y. , brandt , w. n. , fan , x. , inada , n. , jester , s. , knapp , g. r. , krawczyk , c. m. , thakar , a. r. , vanden berk , d. e. , voges , w. , yanny , b. , york , d. g. , bahcall , n. a. , bizyaev , d. , blanton , m. r. , brewington , h. , brinkmann , j. , eisenstein , d. , frieman , j. a. , fukugita , m. , gray , j. , gunn , j. e. , hibon , p. , ivezic , z. , kent , s. m. , kron , r. g. , lee , m. g. , lupton , r. h. , malanushenko , e. , malanushenko , v. , oravetz , d. , pan , k. , pier , j. r. , price , t. n. , saxe , d. h. , schlegel , d. j. , simmons , a. , snedden , s. a. , subbarao , m. u. , szalay , a. s. , weinberg , d. h. , scranton , r. , et al .",
    ", 2010 , aj , vol . 139 , issue 6 , article i d .",
    "2360 scranton , r. , et al .",
    ", 2005 , apj , 633 , 589 shanno , d.  f. , 1990 , recent advances in numerical techniques for large - scale optimization , neural networks for control , mit press , cambridge ma .",
    "shanno , d. f. , 1970 , math .",
    ". 24 : 647 sylvain , a. , & celisse , a. , 2010 , a survey of cross - validation procedures for model selection .",
    "statistics surveys , 4 , 40 - 79 .",
    "doi : 10.1214/09-ss054 .",
    "r. tagliaferri , g. longo , s. andreon , s. capozziello , c. donalek , and g. giordano , 2002 , neural networks and photometric redshifts , astro - ph/0203445 .",
    "vetterling , t. , flannery , b.  p. , 1992",
    ", conjugate gradients methods in multidimensions .",
    "numerical recipes in c - the art of scientific computing , w. h. press and s. a. teukolsky ( eds . ) , cambridge university press ; 2nd edition . veron - cetty , m .-",
    "p . , veron , p. , 2000 , a catalogue of quasars and active nucleai .",
    "eso scientific report , n. 19 .",
    "wright , e.  l. , et al .",
    ", 2010 , aj , 140 , 1868 wolf , c. , et al . , 2004 , a&a , 421 , 913"
  ],
  "abstract_text": [
    "<S> mlpqna stands for multi layer perceptron with quasi newton algorithm and it is a machine learning method which can be used to cope with regression and classification problems on complex and massive data sets . in this paper </S>",
    "<S> we give the formal description of the method and present the results of its application to the evaluation of photometric redshifts for quasars . </S>",
    "<S> the data set used for the experiment was obtained by merging four different surveys ( sdss , galex , ukidss and wise ) , thus covering a wide range of wavelengths from the uv to the mid - infrared . </S>",
    "<S> the method is able i ) to achieve a very high accuracy ; ii ) to drastically reduce the number of outliers and catastrophic objects ; iii ) to discriminate among parameters ( or features ) on the basis of their significance , so that the number of features used for training and analysis can be optimized in order to reduce both the computational demands and the effects of degeneracy . </S>",
    "<S> the best experiment , which makes use of a selected combination of parameters drawn from the four surveys , leads , in terms of @xmath0 ( i.e. @xmath1 ) , to an average of @xmath2 , a standard deviation @xmath3 and a median absolute deviation @xmath4 over the whole redshift range ( i.e. @xmath5 ) , defined by the 4-survey cross - matched spectroscopic sample . </S>",
    "<S> the fraction of catastrophic outliers , i.e. of objects with photo - z deviating more than @xmath6 from the spectroscopic value is @xmath7 , leading to a @xmath8 after their removal , over the same redshift range . </S>",
    "<S> the method is made available to the community through the dameware web application . </S>"
  ]
}