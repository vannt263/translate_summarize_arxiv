{
  "article_text": [
    "we first analyse the evolution of the performance in learning more rules in parallel .",
    "surprisingly , this exhibits a percentage of success for each rule that does not increase monotonically with the number of trials , as for single rule learning @xcite ( fig.1 ) . in the case of learning in parallel",
    "the or and and rules the non - monotonicity is dramatic : already after few applications of both rules in parallel the network recovers the right answer to or ( blue circles ) with a performance of about 100% , however it can not provide the right answer to and ( red triangles ) and is therefore unable to learn both rules ( black line ) .",
    "after about 500 trials , the network starts recovering the right answer to and , however this leads to a sharp decrease in the 100% or performance , down to about 50% performance for both rules , corresponding to a random 0/1 answer .",
    "after further applications , the network improves again its performance for each individual rule , which implies that it has to partially forget the right answer to or in order to learn the and rule . a good performance of 90% in learning both rules",
    "is reached only after a longer training time .",
    "the above observation suggests that some rules ( or in the previous case ) are `` easier '' to learn and will drive the initial performance",
    ". however the network needs a longer training time to organize the synaptic connections able to provide the right answer to both rules .",
    "this effect is also observed for three - rule - learning , where oscillations are less striking but well beyond statistical fluctuations . in this case",
    "the easiest rule to learn is the random rule ( non - monotonicity shown in the inset ) , whereas and and xor learning exhibit similar performances . in both two and three - rule learning ,",
    "the performance in the whole task is always monotonic .",
    "this suggests that the final response in complex learning results from the alternation of learning and forgetting , where partially `` unlearning '' is functional to the improvement in the overall performance .",
    "we then question the dependence of the multi - task learning performance on the percentage of inhibitory synapses @xmath0 in the network .",
    "surprisingly , the success rate does not show a monotonic trend with @xmath0 ( fig.2 ) .",
    "in particular the asymptotic value of the performance strongly depends on @xmath0 and exhibits a maximum for @xmath1 .",
    "increasing the network size improves the asymptotic performance , as in single rule learning @xcite , but does not affect the non - monotonic behaviour .",
    "interestingly , the initial configuration of synaptic strengths does not affect the performance results .",
    "in particular , even starting from a uniform distribution of strengths for all neurons , at the end of the learning routine inhibitory neurons are on average stronger than excitatory ones and , for @xmath2 , the average strength ratio is close to typical balanced networks @xcite .",
    "therefore , the plastic adaptation regulates the relative strength and hinders the creation of excessively strong inhibitory synapses .    to better understand this result",
    ", we first analyse the structure of the union of all paths of active synapses connecting input and output neurons which provide the right answer to all rules .",
    "this constitutes the backbone supporting information transmission .",
    "each backbone in fig.3 shows the temporal sequence of firing neurons , where neurons active simultaneously lie on the same horizontal line and the temporal evolution describes how firings propagate in time from input neurons ( green ) at @xmath3 to the output neuron ( black ) .",
    "the lines connect the pre - synaptic firing neuron with the post - synaptic stimulated neuron drawn at its own firing time .",
    "therefore , a one - time - step line indicates a strong temporal correlation between successive firings , whereas long temporal connections evidence a temporal delay between the pre and post - synaptic neuron firings . the last time step contains always the output neuron , that can be also reached at earlier times , namely more than once during an avalanche .",
    "fig.3 shows that for a purely excitatory network and for @xmath4 , the backbone has a simpler structure involving few neurons , whose firings are mostly successive in time .",
    "conversely , for @xmath5 the structure becomes larger and with an intricate network of firings .",
    "indeed the size of the backbone averaged over many configurations exhibits a maximum value for this fraction of inhibitory synapses ( suppl .",
    "moreover , the average shortest path connecting input and output neurons within the backbone exhibits an opposite trend for different @xmath0 , namely a minimum value for @xmath1 ( suppl .",
    "fig.4 ) . furthermore , for these values of @xmath0 the multiplicity of independent paths through each neuron ( suppl .",
    "fig.5 ) varies over a wider range , indicating the emergence of alternative paths , enhancing cooperative effects under the combined action of excitatory and inhibitory synapses . at the same time , within this complex backbone a preferential path for information transmission is identified under the combined action of excitatory and inhibitory synapses .",
    "the fraction @xmath2 thus optimizes the structural features .",
    "recent experimental results have suggested that neuronal systems in a resting state can be viewed as systems acting close to a critical state where activity does not have a characteristic size .",
    "we question if the same behaviour is observed for the response to an external stimulus .",
    "we measure the size of neuronal avalanches as the number of neurons active when the system provides the right answer to each step of all rules . for purely excitatory networks learning",
    "is achieved only if all neurons are involved in the avalanche , i.e. , the distribution is peaked at @xmath6 ( vertical line fig.4 ) . however , this does not imply that all neurons belong to the backbone , because many neurons are activated in an inefficient attempt to reach the output neuron .",
    "conversely , for increasing @xmath0 , activity is progressively confined , namely the peak height decreases and the distribution extends towards smaller sizes .",
    "this implies that the system can recover the right answer by involving a limited number of neurons ( fig.4 ) and therefore by a more efficient employment of resources .",
    "results suggest that purely excitatory networks , able to exhibit avalanches without a characteristic size in spontaneous activity , can not provide the right answer to a stimulus unless the entire system is involved .",
    "this observation is particularly striking in the case of a right answer `` zero '' . in the following",
    ", we will provide an understanding of these optimal learning conditions .",
    "an avalanche of size @xmath7 represents a configuration able to learn the rule where @xmath7 neurons are active and @xmath8 are inactive . in the framework of spin models ,",
    "this avalanche is therefore analogous to a configuration with @xmath7 up spins and @xmath8 down spins .",
    "the probability @xmath9 to obtain an @xmath7-avalanche , regardless of the individual firing neurons , is the probability to observe such configuration . here",
    "@xmath0 tunes the degree of structural disorder in the system and therefore plays the role of temperature in thermal systems .",
    "we then define the entropy associated to the learning dynamics as @xmath10 which quantifies the variability in the response provided by the system .",
    "the entropy @xmath11 is equal to zero for @xmath12 , as for thermal systems at zero temperature , and progressively increases with @xmath0 ( fig.5 ) . in a learning routine",
    "the network must be able to trigger and sustain the activity in response to the external stimulus , which is , for instance , impossible is a purely inhibitory network . as a consequence , the level of excitability of the whole network must be adequate to tackle the task .",
    "we quantify the level of excitability of the network by evaluating the average synaptic strength @xmath13 , where @xmath14 is positive ( negative ) for excitatory ( inhibitory ) synapses .",
    "the excitability @xmath15 is a decreasing function of @xmath0 and tends to zero for @xmath16 ( fig5 ) . in order to combine the above ingredients , we propose a novel functional @xmath17 which quantifies the balance between variability , both in the structure and in the response , and the excitability of the system",
    "this definition is reminiscent of the free energy in statistical physics , with the additional requirement that the system can not learn in a state with zero excitability . as the free energy , @xmath18",
    "is composed of an energetic and an entropic term and vanishes in the extreme cases of absence of variability or excitability , assuring that both features are required for learning .",
    "this functional is a non - monotonic function of @xmath0 and exhibits a maximum for @xmath2 ( fig.5 ) , percentage that leads to optimal learning by the ability of the system to react to an external stimulus combined to the possibility of tuning the response saving resources .",
    "real brains are able to perform more tasks in parallel . we investigate the multiple task learning performance of a neuronal network able to reproduce the critical behaviour of spontaneous activity .",
    "networks undergo the teaching procedure of two or more rules in parallel , via a numerical procedure ( different inputs and the same output for all rules ) which allows , at the same time , to introduce neuronal firing interference and to monitor separately the performance to each rule .",
    "the network starts by learning the `` easiest '' rule , the one which requires less plastic adaptation . under this point of view , or is the easiest rule to learn , whereas xor requires a longer training .",
    "even if the first rule is learned , further applications stimulate the system to accomplish the entire task . the network is then obliged to redefine the connectivity network , even if it was successful in providing the right answer to the first rule .",
    "this operation implies that the system partially forgets the previous right answer , down to a level of performance where the answer is compatible to a random outcome . only after this partial forgetting the system is able to identify a synaptic structure providing the right answer to all rules .",
    "therefore , learning and forgetting appear to be the two , apparently opposite , mechanisms that must coexist to realize multi - task - learning .",
    "current evidence from functional magnetic resonance imaging ( fmri ) experiments @xcite and eeg data @xcite shows that a greater brain signal variability indicates a more sophisticated neural system , able to explore multiple functional states .",
    "signal variability also reflects a greater coherence between regions and a natural balance between excitation and inhibition originating the inherently variable response in neural functions .",
    "furthermore , the observation that older adults exhibit less variability reflecting less network complexity and integration , suggests that variability can be a proxy for optimal systems . in our study",
    ", we quantify variability in the response by the shannon entropy associated to neuronal activity in learning , combined to the structural disorder measured by the percentage of inhibitory synapses in the system .",
    "the quantity @xmath19 is therefore an entropic term measuring the level of variability , far from being just noise . at the same time",
    ", the cognitive performance must rely on the capability of the system to react to an external stimulation .",
    "this feature can be interpreted as an energetic term , which is maximum for purely excitatory networks , where learning always triggers an all - encompassing activity , involving also unnecessary resources .",
    "therefore , it is beneficial for brains performing complex tasks to realize a balance between the entropic and the energetic features .",
    "our study shows that multi - task learning is optimized not for a structurally balanced network with 50% inhibitory synapses , but for @xmath20 inhibitory synapses , value measured in mammalian brains .",
    "this percentage allows the desired balance between excitability and variability in the response . to better understand why this particular value complies with the two main requirements for learning , excitability and variability , it is useful to recall the structure of the backbone of paths carved by the plastic adaptation process",
    "indeed , the backbone results in a more complex structure of firing connections for @xmath20 inhibitory synapses , than for a structurally balanced network : the number of neurons involved usefully in the process increases .",
    "indeed , a purely excitatory network involves all neurons in successfully performing a task but the majority of them does not operate to convey the information from input to output , i.e. the backbone is quite bare . at the same time",
    "the multiplicity of paths going through each neuron varies over a range which is maximum for @xmath5 .",
    "this does not simply imply longer paths , since the average length of the shortest path connecting input and output is minimum for @xmath5 .",
    "also this result is striking since it would be more reasonable to expect shorter paths for a higher percentage of inhibitory synapses , corresponding to a multiplicity range an order of magnitude smaller .",
    "results suggest that a complex backbone , with a wide range of path multiplicity and therefore the coexistence of a larger number of paths , including very short ones , is the optimal firing structure for learning .",
    "this observation confirms that learning is a truly collective process , where a large number of units with intricate connections participate to the dynamics .",
    "an excess of excitatory or inhibitory synapses , with respect to this optimal value , would hinder the emergence of this complex structure and therefore limit the learning performance of the system .",
    "it would be interesting to extend the present study to other networks , as for instance modular networks , and verify , as found for spontaneous activity , if the particular network topology affects the multi - task learning behaviour .",
    "* neuronal network . *",
    "we consider @xmath21 neurons placed at random in a two - dimensional space and characterized by a potential @xmath22 .",
    "each neuron can be either excitatory or inhibitory , according to dale s law , with a random out - going connectivity degree , @xmath23 , assigned according to the experimental distribution of functionality networks @xcite , @xmath24 with @xmath25 $ ] .",
    "this distribution implies that the network does not exhibit a characteristic connectivity degree . on the contrary ,",
    "few neurons are highly connected and act as hubs with respect to information transmission , whereas the majority of neurons are connected to few other neurons .",
    "connections are established according to a distance dependent probability , @xmath26 , where @xmath27 is their spatial distance and @xmath28 the connectivity spatial range @xcite .",
    "once the outgoing connections are chosen , we evaluate the in - degree of each neuron @xmath29 .",
    "the initial synaptic strengths are randomly drawn from a uniform distribution , @xmath30 $ ] , where @xmath31 .",
    "since the neuronal connectivity degree is power law distributed , the level of inhibition is expressed in terms of @xmath0 , the fraction of inhibitory synapses in the network , with inhibitory neurons highly connected ( @xmath32 ) @xcite .    a neuron @xmath33 fires as soon",
    "as its potential is above a fixed threshold , @xmath34 , changing the membrane potential of post - synaptic neurons proportionally to @xmath14 @xcite @xmath35 where @xmath36 is the in - degree of neuron @xmath37 , the sum is extended to all out - going connections of @xmath33 and the plus or minus sign is for excitatory or inhibitory synapses , respectively .",
    "the factor @xmath38 makes the potential variation of neuron @xmath37 , induced by neuron @xmath33 , independent of the connectivity of both neurons , whereas the factor @xmath39 normalizes the synaptic strength values for each neuron .",
    "we note that the unit time step represents the time unit for propagation from one neuron to the connected ones , which in real systems could be of the order of 10ms . after firing ,",
    "a neuron is set to a zero resting potential and in a refractory state lasting @xmath40 time step .",
    "the initial neuron potentials are uniformly distributed between @xmath41 and @xmath42 .",
    "a small random fraction @xmath43 of neurons is fixed at zero potential and act as boundary sites or sinks for the charge .    * learning routine . * for each rule we choose at random two input neurons and a unique output neuron , under the condition that they are not boundary sites and they are mutually separated on the network by @xmath44 nodes .",
    "@xmath44 represents the minimum distance between input and output neurons , which can also be joined by much longer paths . in the standard multi - layer perceptron framework",
    ", @xmath44 would play the role of the number of hidden layers .",
    "assigning the same output neuron to all rules enables , at the same time , to discriminate between different rules and to have interference among the different paths carrying information .",
    "we test the ability of the network to learn in parallel different boolean rules : and , or , xor and a rule ran , which associates a random output to all possible binary states at two inputs . for each rule",
    "the binary value 1 is identified with the neuron firing , i.e. , @xmath45 at any time during the avalanche propagation .",
    "conversely , the binary state 0 corresponds to the neuron which has been depolarized but fails to reach the firing threshold during the entire avalanche .",
    "once the input sites are stimulated , their activity may bring to threshold other neurons and therefore lead to an avalanche of firings .",
    "we impose no restriction on the number of firing neurons in the propagation and let the avalanche evolve to its end . if the avalanche during the propagation did not reach the output neuron , we consider that the system was in a state unable to respond to the given stimulus , and as a consequence to learn .",
    "we therefore increase uniformly the potential of all neurons by a small quantity , @xmath46 , until the configuration reaches a state where the output neuron is first perturbed .",
    "we then compare the state of the output neuron with the desired output .",
    "a single learning step requires the application of the entire sequence of states for each rule , letting the activity propagate till the end and then monitoring the state of the output neuron .",
    "if the answer is right for all three entries of a rule , this has been learned .",
    "the routine proceeds until the system learns all rules .",
    "* plastic adaptation .",
    "* plastic adaptation obeys a non - uniform generalization @xcite of the negative feedback algorithm @xcite : if the output neuron is in the correct state according to the rule , we keep the value of synaptic strengths .",
    "conversely , if the response is wrong , we modify the strengths of those synapses active during the avalanche @xcite by @xmath47 , where @xmath48 is the chemical distance of each presynaptic neuron from the output neuron and @xmath49 represents the strength of synaptic adaptation . here",
    "@xmath49 represents the ensemble of all possible physiological factors influencing synaptic plasticity .",
    "therefore synapses can be either strengthened or weakened depending on the mistake : if the output neuron fails to be in a firing state , we strengthen the synapses , conversely strengths are weakened if the right answer 0 is missed . once the strength becomes smaller than a threshold , @xmath50 , the synapse is pruned .",
    "this ingredient is very important as since decades the crucial role of selective weakening and elimination of unneeded connections in adult learning has been recognized @xcite .",
    "the synapses involved in the signal propagation and responsible for the wrong answer , are therefore not adapted uniformly but inversely proportional to the chemical distance from the output site .",
    "namely , synapses directly connected to the output neuron receive the strongest adaptation @xmath51 .",
    "this rule dependence on @xmath48 models the feedback to the wrong answer triggered in the region of the output neuron and propagating backward towards the input neurons . in the brain",
    "this mechanism is regulated by the release of messenger molecules , as some hormones ( dopamine suppressing ltd @xcite or adrenal hormones enhancing ltd @xcite ) or nitric oxide @xcite .",
    "three different procedures for plastic adaptations are tested : homeostatic , uniform and restricted . in homeostatic",
    "plasticity the adaptation of excitatory and inhibitory synapses have opposite signs to realize conservation of the average strength @xcite . in the uniform case",
    "all active synapses undergo the same adaptation , whereas in the restricted case only excitatory synapses are modified .",
    "f. lombardi , f. , herrmann , h.j .",
    ", perrone - capano , c. , plenz , c. & de arcangelis , l. balance between excitation and inhibition controls the temporal organization of neuronal avalanches .",
    "lett . _ * 108 * , 228703 ( 2012 ) .",
    "coussens , c.m . ,",
    "kerr , d.s . & abraham , w.c .",
    "glucocorticoid receptor activation lowers the threshold for nmda - receptor - dependent homosynaptic long - term depression in the hippocampus through activation of voltage - dependent calcium channels . _",
    "j. neurophysiol . _",
    "* 78 * , 1 ( 1997 ) .",
    "reyes - harde , m. , empson , r. , potter , b.v.l .",
    ", galione , a. & stanton , p.k .",
    "evidence of a role for cyclic adp - ribose in long - term synaptic depression in hippocampus .",
    "usa _ * 96 * , 4061 ( 1999 ) ."
  ],
  "abstract_text": [
    "<S> performing more tasks in parallel is a typical feature of complex brains . </S>",
    "<S> these are characterized by the coexistence of excitatory and inhibitory synapses , whose percentage in mammals is measured to have a typical value of 20 - 30% . here </S>",
    "<S> we investigate parallel learning of more boolean rules in neuronal networks . </S>",
    "<S> we find that multi - task learning results from the alternation of learning and forgetting of the individual rules . </S>",
    "<S> interestingly , a fraction of 30% inhibitory synapses optimizes the overall performance , carving a complex backbone supporting information transmission with a minimal shortest path length . </S>",
    "<S> we show that 30% inhibitory synapses is the percentage maximizing the learning performance since it guarantees , at the same time , the network excitability necessary to express the response and the variability required to confine the employment of resources .    </S>",
    "<S> brain functions , such as learning and memory , operate through coordinated neuronal activations in which highly connected neurons ( hubs ) have an orchestrating role . </S>",
    "<S> synaptic plasticity @xcite regulates the balance of excitation and inhibition shaping cortical networks into a complex scale - free functional structure , where hubs are found to be inhibitory neurons @xcite . </S>",
    "<S> experimental observations suggest that in mammalian brains the fraction of inhibitory synapses is close to 20 - 30% , however this value is not justified by any theoretical argument . </S>",
    "<S> evoked activity in the mammalian cortex exhibits a large variability to a repeated stimulus , which is expression of the fluctuations in ongoing activity when the stimulus is applied @xcite . </S>",
    "<S> spontaneous activity therefore represents the fundamental support on which neuronal systems develop learning skills . in recent years </S>",
    "<S> , a novel mode of spontaneous activity has been detected , neuronal avalanches . </S>",
    "<S> these are bursts of firing neurons measured both in vitro and in vivo , whose size and duration distributions show a robust power law behaviour @xcite . in this framework , learning can be interpreted as a phenomenon occurring in a critical state , where the ongoing activity does not exhibit a characteristic size . </S>",
    "<S> recently , learning of single boolean rules has been investigated @xcite in a neuronal network model able to fully reproduce the scaling properties and the temporal organization of neuronal avalanches @xcite . </S>",
    "<S> the model , without any reinforcement learning @xcite or error back propagation @xcite , is able to learn even complex rules : the learning performance increases monotonically with the number of times a rule is applied and all rules can be learned ( and remembered ) provided that the plastic adaptation is sufficiently slow . </S>",
    "<S> the performance rate and the learning time exhibit universal scaling features , independent of the particular rule and , surprisingly , the percentage of inhibitory synapses in the network . </S>",
    "<S> multi - task learning requires a more complex organization of neuronal activity where inhibitory synapses may indeed play an important role since competing rules have to establish a synaptic backbone for information transmission under interference effects , not present in single rule learning . </S>",
    "<S> synaptic plasticity is the key process sculpting this backbone . </S>",
    "<S> recently , interesting homeostatic features have been detected in plasticity consisting in balanced depression and potentiation of excitatory and inhibitory synapses aiming at the conservation of the total synaptic strength @xcite .    </S>",
    "<S> here we study the parallel learning of boolean rules with two inputs ( and , or , xor and ran , a rule associating to each input configuration a random output ) on a neuronal network able to reproduce the statistical behaviour of spontaneous activity . a firing ( non - firing ) neuron corresponds to the boolean variable 1 ( 0 ) ( see methods ) . </S>",
    "<S> the response of the network to the successive applications of each rule is monitored for several trials , where plastic adaptation is performed according to a non - uniform negative feedback algorithm if the system gives a wrong answer ( see methods ) . </S>",
    "<S> we demonstrate that the 30% fraction of inhibitory synapses , measured in mammalian brains , optimizes the network s performance . </S>",
    "<S> we provide an understanding of this behaviour in terms of the structural features of the cluster of paths supporting information transmission and the activity dynamical properties .    in the following we will investigate the learning performance of the model as function of the parameter @xmath0 . </S>",
    "<S> the role of other parameters has been addressed in previous works @xcite and found to be irrelevant for the scaling behaviour of avalanche activity . </S>",
    "<S> we first verify that for each rule the choice of hubs inhibitory neurons and homeostatic plastic adaptation optimize the performance in both single and parallel learning ( see suppl . </S>",
    "<S> fig.1,2 ) . on this basis </S>",
    "<S> we adopt these two ingredients in our procedure . </S>",
    "<S> moreover , we verify that also for parallel learning the performance improves for increasing average connectivity in the network and for decreasing distance between input and output neurons . </S>",
    "<S> most importantly , slow plastic adaptation improves the performance also in multi - task learning and curves corresponding to different plastic responses collapse onto a universal function by appropriately rescaling the axes in terms of the plastic adaptation strength ( suppl . </S>",
    "<S> fig.3 ) . </S>"
  ]
}