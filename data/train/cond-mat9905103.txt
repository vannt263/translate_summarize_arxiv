{
  "article_text": [
    "in december 1998 , in the aftermath of el nino and its companion , la nina , the weather in the roanoke , virginia , area was unusually mild .",
    "weather data have been collected here since 1934 , and record highs and lows for any particular day are known . as part of the daily weather forecasts ,",
    "local tv stations report these record values and compare them to the highest and lowest temperature values of the day .",
    "remarkably , during the _ nine _ days from november 29 to december 7 , 1998 , the previous record highs were _ broken five times and tied once_!@xcite one might wonder , as we did , how frequently such a series of records could possibly occur .",
    "when only few weather data are available , such as in the early years of record keeping , it is obviously quite easy to experience new extremes .",
    "however , as the data sets become larger , the record highs ( and lows ) are pushed to higher ( lower ) values , so that the setting of a new record becomes a far less frequent event .",
    "thus , we began exploring questions such as : how probable was it to set a new record in 1998 , 64 years after the records began",
    "? how do record highs increase with time ? how long do records typically survive before they are broken ?    not surprisingly , similar questions have been posed before .",
    "it appears that the earliest studies are due to n.  bernoulli who analyzed life expectancies in 1709 @xcite .",
    "later , flood control and structural safety issues were considered , to name just a few of the numerous applications .",
    "beginning in the 1920 s , the mathematical techniques were developed and the study of records , or extremes , became known as `` extreme value statistics,''@xcite which is still an active area of research . in this paper , we provide a pedagogical introduction to some of the most basic results .",
    "we begin in section ii with the simplest model for records and a concise statement of the problem .",
    "there is no attempt to address real records . as a result of complex physical processes ,",
    "the statistics of real weather records is undoubtedly far more intricate than that generated by simple random numbers .",
    "the quotation marks in the title should remind the reader that actual weather records are _ not _ the subject of this article .",
    "section iii is devoted to the full distribution function for the model records , their averages , and standard deviations .",
    "next , we discuss the record lifetimes and derive the associated distribution in section iv .",
    "although we focus on `` record highs , '' a completely analogous line of reasoning can be pursued for `` record lows.''@xcite in the final section , we turn to more general questions and conclude by listing a number of problems which , to the best of our knowledge , are still unsolved . some technical details are provided in the appendix .",
    "our much simplified model for the physical data ( temperatures , water levels , etc . ) is based on a probability density @xmath0 for a real variable @xmath1 .",
    "for example , if we are considering temperatures , @xmath2 might be the probability that the temperature at noon , on a specific day of the year , takes a value between @xmath1 and @xmath3 .",
    "the `` data '' in our simplified model are just a sequence of random numbers : @xmath4 in keeping with the language of weather records , we refer to the ( integer ) label of these numbers as `` time '' ( or `` year '' ) : @xmath5 each of the @xmath1 s is drawn from the _ same _ _ distribution _ @xmath0 .",
    "thus , our data form a set of independent , identically distributed random variables .",
    "this condition is probably the most serious shortcoming when applied to physical reality , where major correlations or variations can be expected .",
    "for example , it precludes `` global warming , '' a situation in which the underlying distribution @xmath0 itself is a function of time .    before discussing the sequence and the records ,",
    "let us provide some details about the distribution @xmath0 .",
    "it may be defined over the entire real axis or restricted to an interval , that is , it may have infinite or finite support . for simplicity , we require it to be _ continuous and positive _ , which excludes , for example , dice throwing . as a result , we can ignore the possibility of ties . of course , it must be properly _ normalized _ , that is , @xmath6 . here and in the following ,",
    "any integral limits that are not explicitly specified are to be taken as the appropriate ( upper and/or lower ) bounds of the support .",
    "because the distribution @xmath0 reflects how we model our data set , we refer to it as the `` model distribution , '' or simply `` the model . ''",
    "in particular , we will investigate to what extent our results for record distributions and lifetimes depend on the underlying model .",
    "for later reference , we also introduce a second distribution , simply related to @xmath0 : @xmath7 clearly , @xmath8 is the probability that a random number , drawn from the distribution @xmath0 , will not exceed @xmath9 .",
    "thus , @xmath8 is a monotonic function , varying from 0 to 1 .",
    "the reader familiar with random numbers on a computer will recognize that the inverse of this function , @xmath10 , is just the operation to generate random numbers for an arbitrary density @xmath0 from the uniform distribution on the interval @xmath11 $ ] .",
    "following galambos@xcite , we call @xmath8 the `` common distribution function . ''",
    "returning to the sequence ( [ sequence ] ) , we define the `` record '' @xmath12 as the _ largest _ number in a string of length @xmath13 : @xmath14 where we have included @xmath15 as the ( arbitrary ) initial value . after generating the next new number @xmath16 , we determine whether the record has been broken , according to @xmath17    this procedure is continued until we have obtained a sequence of length @xmath18 .",
    "of course , the records @xmath12 themselves are stochastic quantities .",
    "so , we can define a density for @xmath9 : @xmath19 so that the probability for finding the record to lie between @xmath9 and @xmath20 is just @xmath21 .",
    "our goal is to determine , given the underlying model distribution @xmath0 , this @xmath22 or , at a simpler level , the average record :    @xmath23    before delving into the analytic aspects , let us consider computer simulations of these records . to study their statistics , we generate a large number , @xmath24 , of sequences ( modeling , for example , weather data from @xmath24 cities ) : @xmath25 .",
    "based on this ensemble of sequences , we can define the average record as a function of time @xmath26 as an ilustration , following are _ two _ ( @xmath27 ) sequences of _ ten _",
    "( @xmath28 ) random numbers generated by rand ( ) in ms - excel ( quoted to 4 digits for simplicity ) :    @xmath29    @xmath30 and the associated sequences of records :    @xmath31    @xmath32 the _ average _ record is , therefore ,    @xmath33 since rand ( ) outputs a random number uniformly distributed between 0 and 1 , it is hardly surpirsing to see that the average record bumps up against 1 here . to get a good grasp of this process , the reader should attempt , say , 100 sequences of @xmath34 on a spreadsheet . on a typical modern pc ,",
    "it takes only `` the blink of an eye '' , literally !    for the computer simulations presented in this article , we used a high quality random generator ( ran2 from _ numerical recipies_@xcite ) and averaged over @xmath35 sequences of length @xmath36 .",
    "this procedure requires just a few minutes running linux on a 450 mhz pentium ii , and produces excellent statistics .",
    "in fact , with @xmath37 sequences , it is possible to generate a reasonable picture of the whole probability distribution for the records",
    ". a theoretical approach to the averages will be our next task .",
    "our goal in this section is to find an analytic form for the probability density @xmath22 .",
    "we will do this recursively , by assuming that we know the value of the record , say @xmath38 , at time @xmath39 .",
    "then , we seek the _ conditional _ probability , @xmath40 , that the record , at time @xmath13 , has the value @xmath9 , provided it had the value @xmath38 at time @xmath39 .",
    "fortunately , it is very easy to write down this quantity !",
    "clearly , it vanishes for @xmath41 , because the new record can not be smaller than the old one .",
    "this leaves us with two possibilities : either the old record stays the same so that @xmath42 , or it is broken , resulting in the new value @xmath43 .",
    "the former is the case if the new random number , @xmath44 , does not exceed @xmath38 .",
    "reviewing eq .",
    "( [ q - def ] ) , we see that this case occurs with probability @xmath8 .",
    "in contrast , if @xmath44 exceeds @xmath38 , then it sets the new record @xmath9 .",
    "this latter case occurs with probability @xmath45 .",
    "summarizing , the conditional probability is    @xmath46    the heavyside step function @xmath47 i unity if its argument is positive and zero otherwise .",
    "its derivative is just the dirac delta function @xmath48 , so that the two terms in eq .",
    "( [ conditionalp ] ) can be combined into @xmath49 \\ .",
    "\\label{condp}\\ ] ] because we have exhausted all possibilities for @xmath9 , the conditional probability must be normalized with respect to an integration over @xmath9 . this condition is easily checked : because @xmath50 is a total derivative , its integral is just @xmath51 evaluated at the limits .",
    "so , we have @xmath52 .    from the _",
    "conditional _ probability , it is a simple step to arrive at our main target : the record probability density ( the `` record '' distribution ) @xmath22 : @xmath53 substituting the explicit form ( [ condp ] ) for the conditional probability , we obtain a recursion relation for @xmath50 : @xmath54 p(r',t-1 ) \\nonumber \\\\ & = & \\frac \\partial { \\partial r } \\ !",
    "\\left [ q(r ) \\ !",
    "! dr'\\,\\,\\theta ( r - r')\\right ] \\ !",
    "p(r',t-1 ) \\nonumber \\\\ & = & \\frac \\partial { \\partial r}q(r ) \\ !",
    "dr'\\,\\,p(r',t-1 ) .",
    "\\label{p_rec}\\end{aligned}\\ ] ] clearly , this form is still slightly unwieldy , due to the integration .",
    "let us define the `` barrier '' distribution @xmath55 to be the probability that at time @xmath13 the record is at @xmath9 or lower : @xmath56 so that @xmath57 from eqs . ( [ p_rec ] - [ dqdr ] ) , the function @xmath58 satisfies a very simple recursion equation , namely , @xmath59 equation ( [ q_rec ] ) has a simple interpretation .",
    "recall that @xmath8 is the probability that the next random number is less or equal to @xmath9 , and @xmath55 is the probability that , at time @xmath13 , the record has _ not exceeded _",
    "the product of the two gives the probability that the record remains unbroken after the next time step .    the recursion relation ( [ q_rec ] )",
    "is easily solved : @xmath60^t q(r,0)=\\left [ q(r)\\right]^t   \\label{q = q^t}\\ ] ] because @xmath61 for all @xmath62 .",
    "we can deduce two important properties from this general solution for this barrier distribution @xmath58 . because @xmath8 is a monotonic function ,",
    "so is @xmath63 .",
    "this behavior of @xmath63 may be interpreted as at any given time , it is more difficult to go over a higher barrier ( break a record ) . on the other hand , because @xmath64 for any fixed @xmath65 @xmath58 _ decreases _ with @xmath13 which implies that any given record can be broken , provided we wait long enough ! we emphasize that these `` sensible '' results are completely independent of the details of the underlying distribution @xmath0 .    from ( [ q = q^t ] ) , the record distribution follows : @xmath66 ^{t-1}\\frac{\\partial q(r)}{\\partial r } = t\\left [ q(r)\\right ] .",
    "^{t-1}p(r ) \\label{p_simple}\\ ] ] once @xmath22 is known , we can compute average values for the records , @xmath67 , through the defintion ( [ averager ] ) , as well as the standard deviations and all higher moments . for later reference",
    ", we provide a simplified representation of the integral in eq.([averager ] ) . substituting ( [ p_simple ] ) into ( [ averager ] )",
    ", we have @xmath68 ^{t-1}p(r)$ ] .",
    "next , recall that @xmath8 is monotonic , so that it can be inverted uniquely to give the function @xmath10 .",
    "now , change the integration variable from @xmath9 to @xmath69 , using @xmath70 .",
    "the result is @xmath71 note that the limits of integration are now _ unique _ , that is , independent of the underlying model !    in the next section , we will illustrate the characteristics of @xmath22 and @xmath67 with two simple , explicitly solvable distributions @xmath72 .",
    "let us illustrate our results by considering two particularly simple cases : a flat distribution @xmath73 and a pure exponential @xmath74 these distributions differ significantly in that the former has an _ upper bound _ for the allowed values of the data . as a result",
    ", the possible record values are also bounded ! in contrast , the latter distribution extends to infinity , setting no limits on the possible records .",
    "studying these two simple distributions will lead us to find rather different , but hopefully generic behavior for bounded versus unbounded models .    the flat distribution ( [ p - flat ] ) corresponds to data whose values are equally probable over a given interval .",
    "( [ q = q^t ] ) , we find @xmath75 ( for @xmath76 ) and @xmath60 ^t = r^t \\label{q - flat}\\ ] ] so that @xmath77 both results are easily interpreted .",
    "the barrier distribution @xmath55 displays explicitly the general properties discussed above : increasing with @xmath9 at fixed @xmath13 and decreasing with @xmath13 at fixed @xmath78 .",
    "in contrast , the record distribution @xmath22 displays a _",
    "maximum in _",
    "@xmath13 for a fixed @xmath9 . for early times , the probability to find the record at @xmath9 is low because this value has not yet been reached , whereas , for late times , it has already been exceeded !",
    "we can also study @xmath22 as a function of @xmath9 for a fixed @xmath13 .",
    "the maximum value always occurs at @xmath79 , increasing linearly with time .",
    "if we apply the normalization condition @xmath80 , the width of the peak must narrow with time . in other words , the late time records are `` piled up '' just below the highest allowed value ( unity in this case ) .",
    "let us investigate how this feature is reflected in the _ average _ record .",
    "( [ averager ] ) , we find easily @xmath81 as expected , @xmath67 increases monotonically as a function of time , reaching its upper bound at @xmath82 : _",
    "t r(t)=1 of course , its _ rate _ of increase must vanish in this limit . in this case",
    ", the asymptotic is @xmath83 .",
    "this behavior is illustrated in fig .",
    "1a , which shows that the exact result , eq .",
    "( [ avr - flat ] ) is in excellent agreement with monte carlo data averaged over @xmath84 sequences with 1000 entries each .",
    "several of these properties are generic in the following sense .",
    "if the underlying distribution @xmath72 has an upper bound , that is , @xmath85 for @xmath86 , then @xmath87 . not surprisingly , the behavior of @xmath72 near @xmath88 will dictate the asymptotics .",
    "for example , if we assume @xmath89 , so that @xmath90 for @xmath1 , then @xmath91 , and we can show that @xmath92 , which is a generalization of the flat case .",
    "we next turn to the purely exponential distribution ( [ p - exp ] ) and its associated @xmath93 . the two characteristic distribution functions are @xmath94^t \\label{q - exp}\\ ] ] and @xmath95^{t-1}\\ .",
    "\\label{p - exp}\\ ] ] many properties are qualitatively similar to the flat case .",
    "one difference is that the position , @xmath96 , of the maximum in @xmath22 increases with time indefinitely : @xmath97 once again , this behavior is reflected in the average record .",
    "( [ rtoq ] ) and deferring details to appendix a , we obtain @xmath98 which is obviously a monotonically increasing function of time . to exhibit the asymptotic behavior for large times",
    ", we can write eq .",
    "( [ avr - exp ] ) in more compact notation : r(t)= ( t+1 ) - ( 1 ) , where @xmath99 is the digamma function with known asymptotic form.@xcite as a result , @xmath100 where @xmath101 .",
    "note that the _ rate _ of increase also vanishes with time , but with a slower decay , @xmath102 . in fig .",
    "1b , we show the comparison of the asymptotic result ( [ avr - exp - as ] ) with monte carlo data .",
    "the agreement is clearly excellent .",
    "as before , we can consider the behavior of the average record for more general unbounded distributions @xmath0 . here",
    ", the argument rests on a saddle - point approximation for @xmath22 around its maximum value , @xmath96 .",
    "asymptotically , the average can be approximated by the position of the maximum . because @xmath96 becomes very large for late times",
    ", the asymptotics should be controlled by the behavior of @xmath0 for large @xmath1 .",
    "some examples of possible asymptotic behaviors of unbounded @xmath72 s and their associated saddle points @xmath96 are    1 .   a power law @xmath103 ( with @xmath104 to ensure that @xmath105 exists ) , resulting in @xmath106 ; 2 .",
    "an exponential @xmath107 , giving @xmath108 ; 3 .",
    "a gaussian @xmath109for which @xmath110 .",
    "we invite the reader to carry out simulations for , say , @xmath111\\ ] ] and check the results against the predictions in item 1 .",
    "we would also like to caution the reader that the approach to asymptopia for the gaussian is _ extremely _ slow .",
    "in particular , we find that this regime lies beyond @xmath112 .",
    "next , we turn to a key question in the study of floods or earthquakes .",
    "what is the typical time span between two large events ? at the practical level , how much time do we have to construct dams or to repair levees before the next record - breaking flood ?",
    "of course , our study will not provide an _ exact time span _ until the next disaster ; it will only give an _ estimate _ of how long a record might survive .",
    "we refer to the time span between the setting of a record and its subsequent breaking as its `` lifetime '' ( or `` record time''@xcite ) . in the following ,",
    "we investigate the distribution of these record lifetimes .",
    "specifically , we will consider data sequences with @xmath18 entries .",
    "for each sequence , we will identify the associated records and their lifetimes . by analyzing a large number of data sequences",
    ", we can compile a histogram , @xmath113 , of how likely a record would survive for a time span @xmath114 .",
    "let us introduce a simple representation of all possible histories of records : a tree - like structure .",
    "we begin by generating a data sequence @xmath115 . because we only need to know _ where _ the records occur in this sequence , we can associate this sequence with the following binary string .",
    "if @xmath116 is a new record , replace it by the letter @xmath9 ( `` record '' ) ; otherwise , replace it by @xmath117 ( `` lower '' ) .",
    "note that , had we used a discrete underlying distribution @xmath72 , then we would have to consider the complication of ties . as an example",
    ", the sequence @xmath118 is replaced by @xmath119 . by convention ,",
    "the first entry is always @xmath9 . if a record is established at time @xmath13 and broken at time @xmath120 , then that record s lifetime is defined to be @xmath114 .",
    "clearly , the binary string is much simpler than the original data sequence , but it contains enough information about record lifetimes for us to predict the distribution @xmath113 .    the binary strings are easily visualized via a tree structure .",
    "starting from a single vertex ( the `` ancestor '' ) @xmath9 on the first line ( @xmath121 ) , time runs downwards . at time @xmath122",
    "( the second line ) , our string has two possible entries : @xmath9 or @xmath117 . to represent these , we draw two branches from the first line to the second : one to the right ( labeled @xmath9 ) , and one towards the left ( labeled @xmath117 ) .",
    "each of these branches ends in a new vertex .",
    "these branch again , giving us four vertices in total on the third ( @xmath123 ) line . continuing this procedure to the @xmath18th level",
    ", we find @xmath124 vertices there . as an illustration ,",
    "the @xmath125 tree is shown in fig .",
    "clearly , all possible binary strings with 4 elements appear in this tree , each associated with exactly one vertex on the @xmath126 line .",
    "the record lifetimes associated with a given string are now easily identified : following an @xmath117 branch means that the current record survives , while choosing the @xmath9 branch implies that a new record is set .",
    "thus , each vertex can be labeled with the set of record lifetimes @xmath127 leading to it .",
    "2b shows the `` tree of lifetimes '' for the @xmath125 case shown in fig . 2a .",
    "note that the number of entries in these sets varies for different strings .",
    "for example , the far right string in fig .",
    "2a , where every record is broken at the next time step , gives rise to @xmath128 , while the far left string corresponds to @xmath129 : the record is set at @xmath121 and survives the next three time steps .",
    "a simple recursion relation emerges . from a particular entry at time @xmath13 to the two `` daughters '' at time @xmath130 , the set @xmath131 branches into two : @xmath132 and @xmath133 .",
    "moreover , because any distribution @xmath0 will generate the same set of binary strings , it is obvious that the associated lifetimes @xmath134 are completely _ independent _ of the underlying model !      to complete the construction of the histogram , we need to find the _ string _ _ probability _ , that is , the probability that a specific string will appear . at the @xmath18th level , each string is associated with a specific vertex , which is uniquely labeled by the set @xmath135 .",
    "so , let us denote this probability by @xmath136 . at the top level ( @xmath121 ) , this probability is obviously trivial : @xmath137 .",
    "for example , the year record keeping starts , any temperature will be a `` record '' in the second `` year '' ( @xmath122 ) , there are two vertices : @xmath138 and @xmath139 , corresponding to the first record surviving or being broken , respectively . in terms of the original data sequence @xmath140 , these possibilities are given by @xmath141 or @xmath142 .",
    "let us focus first on the former case for which the probability , @xmath143 , is just @xmath144 . at first sight , this expression seems to depend on the model distribution @xmath0 .",
    "however , recalling eq .",
    "( [ rtoq ] ) , we can transform the integration variables to @xmath145 where we have exploited the monotonicity of @xmath146 to replace @xmath147 by @xmath148 .",
    "not only is this integral trivial to compute , it is also _ manifestly independent _ of the underlying model ! in a similar manner , we can compute explicitly that the probability of the latter case , @xmath149 , is also 1/2 .",
    "let us illustrate this process for the next year ( @xmath123 ) .",
    "for the four possible histories , different combinations of @xmath150-functions appear .",
    "explicitly , their associated probabilities are : @xmath151 in fig .",
    "2c , we show all the probabilities in the @xmath125 tree .",
    "note that the sum of all the @xmath152 s at each level is indeed unity ; the probability of having _ any _ history after @xmath18 years must be 1 .    in appendix b",
    ", we show that the general result for an arbitrary string of any length @xmath18 is @xmath153 note that the last factor is actually just @xmath18 .    another somewhat counter - intuitive result concerns @xmath154 , the probability for the _",
    "last record to survive _ @xmath114 steps ( regardless of what happened earlier ) .",
    "note that the `` last record '' is also the `` highest record , '' since the last record is necessarily larger than all previous records !",
    "now , to say that this record survives @xmath114 steps means that the values in the rest of the string ( @xmath155 of them ) are lower .",
    "therefore , in a string of @xmath18 steps , the last record must have occurred at the @xmath156-th step .",
    "thus , to find @xmath154 , we ask : what is the probability for the highest record to show up at the @xmath157-th step ?",
    "we might guess that the highest record is unlikely to occur near the beginning of the string , because there are many chances for it to be broken later . on the other hand ,",
    "if we use the language of athletics , we might conclude that the record facing the last athlete may be quite high ( given that `` many guys have gone before '' ) and breaking the record may not be easy .",
    "so , perhaps @xmath154 should be peaked in the middle ?",
    "the surprise is that @xmath154 is _ independent of _",
    "@xmath114 ( or equivalently , @xmath157 ) ! in other words , the highestl record _ may occur at any step with equal probability _ !",
    "the proof may be found in appendix c.      lastly , we turn to the ( unnormalized ) lifetime distribution , @xmath113 .",
    "we again llustrate by explicit calculations of the @xmath125 case and relegate the general discussion to appendix d. combining the string probabilities with the lifetimes , we obtain : @xmath158 focusing on a particular binary string , we recall that the associated record lifetimes @xmath159 as well as the string probability are entirely model - independent . as a result ,",
    "the lifetime distribution itself is _ universal _ , that is , its form does not depend on the underlying model distribution @xmath0 .",
    "moreover , the result for @xmath160 suggests a surprisingly simple form for the lifetime histogram , namely , @xmath161 the general proof can be found in appendix d. remarkably , @xmath162 is not only universal , but also is independent of @xmath18 , that is , the length of the original data sets !",
    "so , the probability for a record to survive , for example , 5 time steps is the same , no matter how much data we accumulate . phrased differently , this result is not surprising at all : because @xmath113 is also , roughly speaking , the probability for the next huge disaster to strike after @xmath114 time steps , it would be very disturbing if that probability depended on the length of our data sets : that would imply that we could avoid or court disaster by just continuing to take data . clearly , such a result would be nonsensical .",
    "nobody would believe that , by observing the weather , we can change it !    although the lack of an @xmath18-dependence can be understood in this way , the universality with respect to the underlying model @xmath0 is a much stronger statement : as long as _ all _ underlying data arise from the _ same _ distribution , the functional _",
    "form _ of this distribution is completely irrelevant .",
    "flat , exponential or gaussian @xmath72 s all generate identical lifetime histograms . in fig .",
    "3 , we show monte carlo data for the lifetime histograms of flat and exponential @xmath0",
    ". they are indistinguishable , apart from statistical fluctuations !",
    "the agreement with the theoretically expected @xmath163 is again excellent .",
    "we have presented a pedagogical introduction to the statistics of extremes . though we motivated the discussion by reference to weather records , the model studied here is much simplified . generating a sequence of @xmath18 random numbers , all selected from the _ same _ underlying distribution @xmath0 ,",
    "we keep track of the ones which are , say , higher than all the preceding numbers  `` record highs . '' studying the statistics of such strings , we identify several universal features , that is , properties which are independent of the underlying distribution @xmath72 .",
    "in particular , we ask how long a record survives before being broken , that is , we focus on the lifetimes of records and compile a histogram .",
    "not only is that histogram universal , but it is also exceedingly simple : records lasting @xmath114 steps occur with a decreasing frequency of @xmath164 .",
    "we also inquire into some details of these strings . as far as records are concerned ,",
    "the sequence of numbers can be reduced to a binary string of @xmath9 s and @xmath117 s . at each step , we only keep track of whether the record is broken @xmath165 or not @xmath166 .",
    "a `` tree '' representing all possible sequences can be drawn which displays whether a record is broken or not at each step .",
    "it is natural to ask , what is the probability that records will be broken at specific steps along the string ?",
    "the answer , given by eq .",
    "( [ p(r ) ] ) or ( [ p_n ] ) , is also universal .",
    "perhaps most surprising is the answer to the question , what is the probability that the overall record will occur at the @xmath114th step ?",
    "the answer is _ independent _ of @xmath114 .",
    "the overall record occurs at any step with _",
    "equal probability_.    another interesting question is the following . averaged over many sequences , how does the overall record `` inch up '' with time ?",
    "although the answer is not completely universal , the average behavior falls into one of several classes .",
    "the simplest is due to a @xmath0 which is bounded .",
    "then , not surprisingly , the average just runs into this bound . for unbounded @xmath72 s ,",
    "the asymptotic behavior is mostly dictated by the `` tail '' of the distribution . in this sense",
    ", there is a limited form of universality , that is , properties are independent of the details of the rest of @xmath0 . beyond the average record , we also studied the probability density , @xmath22 , for finding the record at @xmath9 after @xmath13 steps . like the central limit theorem",
    ", there is universality for large values of @xmath13 , provided an appropriate time - dependent rescaling of @xmath167 is included .",
    "however , there are _ three _ limiting distributions , as well as the possibility of _ no _ limiting distribution .",
    "these fascinating properties lie outside the scope of this article .",
    "we refer the interested reader to the book by galambos for example.@xcite    looking beyond our simple model for `` weather '' records , we may inquire about a natural generalization  a model for `` global warming . '' here , we let the underlying distribution _ drift upwards _ with time . the simplest is a uniform drift , that is , letting the model distribution at time step @xmath13 , @xmath168 , assume the form @xmath169 with @xmath170 .",
    "although numerous results still hold , there are also significant differences . obviously , the average record here must increase linearly . probably the most significant difference between the simple model for `` weather '' records and",
    "this `` global warming '' case is that the limiting distribution for @xmath22 is expected to assume the form @xmath171 , with no time - dependent rescaling .",
    "we are not aware of any general results for such drifting @xmath72 s .",
    "certainly , these expectations are confirmed for a _ flat , drifting _",
    "for large times , we find @xmath172 , with q(r , t)=q^*(r - t ) , where q^*()=^k 1-k1-(k-1 ) .",
    "to arrive at these results , we relied on a generalized version of ( [ q = q^t ] ) : @xmath173 , where @xmath174 .",
    "clearly , these general forms are applicable for _ any _ time - dependent @xmath72 . for example",
    ", we could consider a sequence composed of the _ sum _ of all random numbers generated before .",
    "if the random numbers are distributed such that both positive and negative values are present , we may think of the sequence as the value of a stock , making gains and losses from day to day .",
    "the variations are limited only by the imagination . unlike the simple model presented above",
    ", there are very few known results for the distributions of the record lifetimes in general .",
    "needless to say , exploring the universality classes will be a task both daunting and rewarding .            to obtain the desired result ( [ avr - exp ] ) , we start from ( [ rtoq ] ) and the inverse of ( [ q - exp ] ) .",
    "@xmath175 q^{t-1 } & = & t \\",
    "\\left [ \\sum_{n=1}^\\infty q^n",
    "/ n\\,\\right ] q^{t-1 } \\nonumber \\\\ & = & \\sum_{n=1}^\\infty \\frac { t}{n\\left ( n+t\\right)}=\\sum_{n=1}^\\infty \\left [   \\frac { 1}{n}-\\frac { 1}{n+t}\\right ] \\label{avr - exp - ap } \\\\ & = & \\sum_{k=1}^t\\frac { 1}{k } \\nonumber\\end{aligned}\\ ] ] those who like a bit more mathematical rigor may consider @xmath176 , which justifies the exchange of the integral and sum .      we provide some details for computing the general case @xmath177 .",
    "first , let us introduce an alternate way of labelling the vertices .",
    "instead of the record lifetimes @xmath178 , we keep track of the times @xmath179 when records are broken : that is , @xmath180 denotes the position of the @xmath181th @xmath9 along the string .",
    "so , we may also denote the general string probability by @xmath182 . by convention @xmath183 , while @xmath184 , @xmath185 , etc : @xmath186 where the last line comes from the length of the string : @xmath187 .    to obtain @xmath50 for an arbitrary string , we start with p_n ( r_1,r_2,  r_k ) = dq_1  dq_n   .",
    "to say that the first record @xmath188 associated with @xmath189 has a lifetime of @xmath190 means that the next @xmath191 @xmath9 s ( and their associated @xmath69 s ) are lower .",
    "therefore , the first @xmath191 factors in the integrand are @xmath192 so that the integration over the variables @xmath193 can be performed trivially , with the result @xmath194 .",
    "now , the next record is @xmath195 associated with @xmath196 , so that the next factor in the integrand must be @xmath197 .",
    "there will be no more appearances of @xmath189 in the rest of the integrand .",
    "so , the integral over @xmath189 ( up to @xmath196 ) can be performed , giving @xmath198 .",
    "note that @xmath199 so that this result can be written simply as @xmath200 .    in a similar way",
    ", we integrate over the @xmath201 variables @xmath202 up to @xmath196 , arriving at @xmath203 or @xmath204 .",
    "performing the integral over @xmath196 gives @xmath205 .",
    "this process can be carried on until the last integration ( over @xmath206 , up to 1 ) .",
    "the integrand here consists of the result from the integrals over the previous variables @xmath207 as well as the @xmath69 s from the rest of the sequence : @xmath208 . with this additional factor of @xmath209 ,",
    "the last integral provides a factor of @xmath18 .",
    "the final result is @xmath210 rewriting the @xmath211 s in terms of the @xmath212 s , we have p_n ( \\ { } ) = , where the last factor is precisely @xmath18 , because the sum of the lifetimes of all records is just the length of the string .",
    "one interesting property of these @xmath50 s follows from the fact that , given a particular string of length @xmath18 , we can `` generate '' two strings of length @xmath213 , by concatenating either an @xmath9 or an @xmath117 at the end . in the former case , @xmath214 stays the same while @xmath215 . in the latter case , the value of @xmath214 increases by 1 . from eq .",
    "( [ p_n ] ) , we obtain the following `` recursion '' relations : @xmath216 not surprisingly , @xmath217    to illustrate , consider the first pair of entries at the 4@xmath218 level in fig .",
    "2 and their relationship to the first entry at the 3@xmath219 level ( @xmath220 ) . from fig 2c , we read off @xmath221 , @xmath222 , and @xmath223 .",
    "these quantities satisfy eqs .",
    "( [ recur1],[recur2 ] ) .    [ [ probability - for - strings - where - the - last - record - survived - m - steps ] ] probability for strings where the last record survived @xmath114 steps ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    an easy way to arrive at the lifetime histogram @xmath163 is through the probability for the last record to survive @xmath114 steps , regardless of what happened earlier .",
    "let us define this quantity as @xmath224 .",
    "to be precise , it is @xmath225 where @xmath48 is the kronecker delta ( @xmath48 is unity if its argument vanishes and zero otherwise ) .",
    "be careful with this sum notation : it stands for summing over @xmath226 as well , because @xmath226 is the _ number of records _ in the string .",
    "now , because the `` last '' record is also the overall record , ( that is , all other @xmath9 s are lower ) , it is easy to compute @xmath227 . because we are summing over all possible ways that the records before this step ( @xmath228 ) are broken ,",
    "we may write @xmath229    as remarked in the main text , this result is not intuitively obvious at all . intuition might lead us to an @xmath227 with a maximum in the middle of the string , but eq .",
    "( [ s=1/n ] ) tells us that the distribution is entirely flat .",
    "note that @xmath113 is not a real probability in the sense that @xmath230 .",
    "the normalization condition here is _ \\ { } p_n ( \\ { } ) = 1 , but the histogram is defined by t_n(m)=_\\ { } p_n ( \\ { } ) _ i=1^k ( _ i - m ) .",
    "again , we caution the reader that the @xmath231 here involves a sum over @xmath226 as well .",
    "summing over @xmath114 produces @xmath232 .",
    "now , given a string of @xmath18 random numbers , the only way for a record lifetime to be @xmath18 is that the first record survives .",
    "thus , @xmath233 is precisely @xmath234 , so that @xmath235 our goal reduces to proving t_n+1(m)=t_n(m)1m <",
    "there are two types of contributions to @xmath113 .",
    "one is from all the `` interior '' records : @xmath236 that is , @xmath237 with @xmath238 .",
    "the other piece , from the last record alone , is precisely @xmath227 . for @xmath239 ,",
    "let us start with @xmath240 and go from @xmath18 to @xmath213 , by looking at @xmath241 where the @xmath226 s appearing here are still those associated with @xmath240 ( that is , the last record may be labeled by @xmath242 ) . because the sum over @xmath214 and @xmath243 can be performed without the @xmath244 s , eqs .",
    "( [ recur1 ] ) and ( [ recur2 ] ) can be used to obtain the following recursion relation _ \\ { } p_n+1 ( \\ { } ) _ i=1^k-1 ( _ i - m ) = _ \\ { } p_n ( \\ { } ) _ i=1^k-1 ( _ i - m ) . but",
    "the quantity ( [ spd ] ) is not the only contribution to @xmath239 , because concatenating an @xmath9 to an @xmath18-string will produce an `` interior '' record for the @xmath245-string .",
    "focusing on a specific @xmath114 , this extra bit is just _",
    "\\ { } p_n+1 ( _ 1,_2,  ,_k,_k+1=1 ) ( _ k - m ) , which is _ \\ { } p_n ( _ 1,_2,  ,_k ) ( _ k - m ) = s_n(m ) .",
    "so , we conclude t_n+1(m)=t_n(m)+ . with the help of this equation",
    ", we have @xmath246 but , according to eq .",
    "( [ s=1/n ] ) , s_n+1(m)=s_n(m ) so that t_n+1(m)=t_n(m)+s_n(m)=t_n(m ) .",
    "thus , using ( [ tnn ] ) , we arrive at @xmath247 for any @xmath248 .",
    "see for example , e. j. gumbel , _ the statistics of extremes _ ( columbia university press , new york 1958 ) ; j. galambos , _ the asymptotic theory of extreme order statistics _",
    "( wiley , new york 1978 ) .",
    "gumbel gives a short historical summary in his book ."
  ],
  "abstract_text": [
    "<S> we present a simple , pedagogical introduction to the statistics of extreme values . </S>",
    "<S> motivated by a string of record high temperatures in december 1998 , we consider the distribution , averages and lifetimes for a simplified model of such `` records . </S>",
    "<S> '' our `` data '' are sequences of independent random numbers all of which are generated from the same probability distribution . </S>",
    "<S> a remarkable universality emerges : a number of results , including the lifetime histogram , are universal , that is , independent of the underlying distribution .    </S>"
  ]
}