{
  "article_text": [
    "bayesian inference for the stochastic volatility model has been extensively studied . in this paper , we focus on comparisons with the method of kastner and fruwirth - schnatter ( 2014 ) .",
    "this state - of - the - art method combines the method of kim et al .",
    "( 1998 ) with the asis ( ancillary sufficiency interweaving strategy ) technique of yu and meng ( 2011 ) .",
    "kastner and fruwirth - schnatter s method consists of two parts .",
    "the first is an update of the latent variables @xmath11 and the second is a joint update of @xmath7 and the latent variables @xmath11 .",
    "we improve this method here by saving and re - using sufficient statistics to do multiple parameter updates at little additional computational cost .",
    "the non - linear relationship between the latent and the observation process prohibits the direct use of kalman filters for sampling the latent variables @xmath3 .",
    "kim et al .",
    "( 1998 ) introduced an approximation to the stochastic volatility model that allows using kalman filters to draw samples of @xmath3 which can be later reweighed to give samples from their exact posterior distribution .",
    "this approximation proceeds as follows .",
    "first , the observation process for the stochastic volatility model is written in the form @xmath13 where @xmath14 has a @xmath15 distribution .",
    "next , the distribution of @xmath14 is approximated by a ten - component mixture of gaussians with mixture weights @xmath16 , means @xmath17 and variances @xmath18 .",
    "the values of these mixture weights , means and variances can be found in omori ( 2007 ) . at each step of the sampler , at each time @xmath19 , a single component of the mixture is chosen to approximate the distribution of @xmath14 by drawing a mixture component indicator @xmath20 with probabilities proportional to @xmath21    conditional on @xmath22 , the observation process is now linear and gaussian , as is the latent process : @xmath23    kalman filtering followed by a backward sampling pass can now be used to sample a latent sequence @xmath11 . for a description of this sampling procedure ,",
    "see petris et al .",
    "( 2009 ) .",
    "the mis - specification of the model due to the approximation can be corrected using importance weights @xmath24 where @xmath25 is the @xmath26 density , @xmath27 is the @xmath28 density and the index @xmath29 refers to a draw .",
    "posterior expectations of functions of @xmath7 can then be computed as @xmath30 , with @xmath31 the draws .",
    "we note that , when doing any other updates affecting @xmath11 in combination with this approximate scheme , we need to continue to use the same mixture of gaussians approximation to the observation process . if an update drawing from an approximate distribution is combined with an update drawing from an exact distribution , neither update will draw samples from their target distribution , since neither update has a chance to reach equilibrium before the other update disturbs things",
    ". we would then be unable to compute the correct importance weights to estimate posterior expectations of functions of @xmath7 .",
    "asis methods ( yu and meng ( 2011 ) ) are based on the idea of interweaving two parametrizations . for the stochastic volatility model ,",
    "these are the so - called non - centered ( nc ) and centered ( c ) parametrizations .",
    "the nc parametrization is the one in which the stochastic volatility model was originally presented above .",
    "the c parametrization for the stochastic volatility model is @xmath32 the mixture of gaussians approximation for c is the same as for nc .",
    "kastner and fruwirth - schnatter ( 2014 ) propose two new sampling schemes , gis - c and gis - nc , in which they interweave these two parametrizations , using either the nc or c parameterization as the baseline .",
    "the authors report a negiligible performance difference between using nc or c as the baseline . for the purposes of our comparisons ,",
    "we use the method with nc as the baseline , gis - nc , which proceeds as follows .    1 .",
    "draw @xmath11 given @xmath33 using the linear gaussian approximation update ( nc ) 2 .   draw @xmath7 given @xmath34 using a metropolis update ( nc ) 3 .",
    "move to c by setting @xmath35 4 .",
    "draw @xmath7 given @xmath36 using a metropolis update ( c ) 5 .",
    "move back to nc by setting @xmath37 6 .",
    "redraw the mixture component indicators @xmath38 given @xmath39 .",
    "theorem 4 of yu and meng ( 2011 ) establishes a link between asis and the px - da ( parameter expansion - data augmentation ) method of liu and wu ( 1999 ) . in the case of the stochastic volatility model",
    ", this means that we can view the asis scheme for updating @xmath11 and @xmath7 as a combination of two updates , both done in the nc parametrization .",
    "the first of these draws new values for @xmath7 conditional on @xmath11 .",
    "the second draws new values for both @xmath11 and @xmath7 , such that when we propose to update @xmath40 to @xmath41 and @xmath42 to @xmath43 , we also propose to update the sequence @xmath11 to @xmath44 . for this second update",
    ", the metropolis acceptance probability needs to be multiplied by a jacobian factor @xmath45 to account for scaling @xmath42 .",
    "a joint translation update for @xmath40 and @xmath11 has been previously considered by liu and sabatti ( 2000 ) and successfully applied to to stochastic volatility model .",
    "scale updates are considered by liu and sabatti ( 2000 ) as well , though they do not apply them to the stochastic volatility model .",
    "the view of asis updates as joint updates to @xmath7 and @xmath11 makes it easier to see why asis updates improve efficiency . at first glance",
    ", they look like they only update the parameters , but they actually end up proposing to change both @xmath7 and @xmath11 in a way that preserves dependence between them .",
    "this means that moves proposed in asis updates are more likely to end up in a region of high posterior density , and so be accepted .",
    "kastner and fruwirth - schnatter ( 2014 ) do a single metropolis update of the parameters for every update of the latent sequence .",
    "however , we note that given values for the mixture indices @xmath38 , @xmath46 and @xmath11 , low - dimensional sufficient statistics exist for all parameters in the centered parametrization . in the non - centered parametrization , given @xmath38 , @xmath46 and @xmath11 , low - dimensional sufficient statistics exist for @xmath47 .",
    "we propose doing multiple metropolis updates given saved values of these sufficient statistics ( for all parameters in the case of c and for @xmath47 in the case of nc ) .",
    "this allows us to reach equilibrium given a fixed latent sequence at little computational cost since additional updates have small cost , not dependent on @xmath48 .",
    "also , this eliminates the need to construct complex proposal schemes , since with these repeated samples the algorithm becomes less sensitive to the particular choice of proposal density .",
    "the sufficient statistics in the case of nc are @xmath49    with the log - likelihood of @xmath47 as a function of the sufficient statistics being @xmath50    in the case of c the sufficient statistics are @xmath51    with the log - likelihood as a function of the sufficient statistics being @xmath52    the details of the derivations are given in the appendix .",
    "the general framework underlying ensemble mcmc methods was introduced by neal ( 2010 ) . an ensemble mcmc method using embedded hmms for parameter inference in non - linear , non - gaussian state space models was introduced by shestopaloff and neal ( 2013 ) .",
    "we briefly review ensemble methods for non - linear , non - gaussian state space models here .",
    "ensemble mcmc builds on the idea of mcmc using a temporary mapping .",
    "suppose we are interested in sampling from a distribution with density @xmath53 on @xmath54 .",
    "we can do this by constructing a markov chain with transition kernel @xmath55 with invariant distribution @xmath56 .",
    "the temporary mapping strategy takes @xmath57 to be a composition of three stochastic mappings .",
    "the first mapping , @xmath58 , takes @xmath59 to an element @xmath60 of some other space @xmath61 .",
    "the second , @xmath62 , updates @xmath60 to @xmath63 .",
    "the last , @xmath64 , takes @xmath63 back to some @xmath65 .",
    "the idea behind this strategy is that doing updates in an intermediate space @xmath61 may allow us to make larger changes to @xmath59 , as opposed to doing updates directly in @xmath54 .    in the ensemble method , the space @xmath61 is taken to be the @xmath12-fold cartesian product of @xmath54 .",
    "first , @xmath59 mapped to an ensemble @xmath66 , with the current value @xmath59 assigned to @xmath67 , with @xmath68 chosen uniformly at random .",
    "the remaining elements @xmath69 for @xmath70 are chosen from their conditional distribution under an ensemble base measure @xmath71 , given that @xmath72 .",
    "the marginal density of an ensemble element @xmath67 in the ensemble base measure @xmath71 is denoted by @xmath73 .",
    "next , @xmath60 is updated to @xmath63 using any update that leaves invariant the _ ensemble density _ @xmath74 finally , a new value @xmath75 is chosen by selecting an element @xmath67 from the ensemble with probabilities proportional to @xmath76 .",
    "the benefit of doing , say metropolis , updates in the space of ensembles is that a proposed move is more likely to be accepted , since for the ensemble density to be large it is enough that the proposed ensemble contains at least some elements with high density under @xmath56 .    in shestopaloff and neal ( 2013 ) , we consider an ensemble over latent state sequences @xmath11 .",
    "specifically , the current state , @xmath77 , consisting of the latent states @xmath11 and the parameters @xmath7 is mapped to an ensemble @xmath78 where the ensemble contains all distinct sequences @xmath79 passing through a collection of pool states chosen at each time @xmath19 . the ensemble is then updated to @xmath80 using a metropolis update that leaves @xmath81 invariant . at this step , only @xmath7 is changed .",
    "we then map back to a new @xmath82 , where @xmath83 is now potentially different from the original @xmath11 .",
    "we show that this method considerably improves sampling efficiency in the ricker model of population dynamics .    as in the original neal ( 2010 ) paper",
    ", we emphasize here that applications of ensemble methods are worth investigating when the density at each of the @xmath12 elements of an ensemble can be computed in less time than it takes to do @xmath12 separate density evaluations .",
    "for the stochastic volatility model , this is possible for ensembles over latent state sequences , and over the parameters @xmath40 and @xmath84 . in this paper",
    ", we will only consider joint ensembles over @xmath11 and over @xmath42 .",
    "since we will use @xmath85 in the mcmc state , we will refer to ensembles over @xmath86 below .",
    "we propose two ensemble mcmc sampling schemes for the stochastic volatility model .",
    "the first , ens1 , updates the latent sequence , @xmath11 , and @xmath86 by mapping to an ensemble composed of latent sequences @xmath11 and values of @xmath86 , then immediately mapping back to new values of @xmath11 and @xmath86 . the second ,",
    "ens2 , maps to an ensemble of latent state sequences @xmath11 and values of @xmath86 , like ens1 , then updates @xmath47 using an ensemble density summing over @xmath11 and @xmath86 , and finally maps back to new values of @xmath11 and @xmath86 .    for both ens1 and ens2",
    ", we first create a pool of @xmath86 values with @xmath87 elements , and at each time , @xmath19 , a pool of values for the latent state @xmath3 , with @xmath88 elements . the current value of @xmath86",
    "is assigned to the pool element @xmath89}$ ] and for each time @xmath19 , the current @xmath3 is assigned to the pool element @xmath90}$ ] .",
    "( since the pool states are drawn independently , we do nt need to randomly assign an index to the current @xmath86 and the current @xmath3 s in their pools . )",
    "the remaining pool elements are drawn independently from some distribution having positive probability for all possible values of @xmath3 and @xmath86 , say @xmath91 for @xmath3 and @xmath92 for @xmath86 .",
    "the total number of ensemble elements that we can construct using the pools over @xmath3 and over @xmath86 is @xmath93 .",
    "naively evaluating the ensemble density presents an enormous computational burden for @xmath94 , taking time on the order of @xmath93 . by using the forward algorithm , together with a `` caching '' technique",
    ", we can evaluate the ensemble density much more efficiently , in time on the order of @xmath95 .",
    "the forward algorithm is used to efficiently evaluate the densities for the ensemble over the @xmath3 .",
    "the caching technique is used to efficiently evaluate the densities for the ensemble over @xmath86 , which gives us a substantial constant factor speed - up in terms of computation time .    in detail",
    ", we do the following .",
    "let @xmath96 be the initial state distribution , @xmath97 the transition density for the latent process and @xmath98 the observation probabilities .",
    "we begin by computing and storing the initial latent state probabilities  which do not depend on @xmath86  for each pool state @xmath99}$ ] at time @xmath100 .",
    "@xmath101 } ) , \\ldots , p(x_{1}^{[l_{x}]}))\\end{aligned}\\ ] ]    for each @xmath102}$ ] in the pool and each pool state @xmath99}$ ] we then compute and store the initial forward probabilities @xmath103}(x_{1}^{[k ] } | \\eta^{[l ] } ) & = & p(x_{1}^{[k]})\\frac{p(y_{1}|x_{1}^{[k ] } , \\eta^{[l]})}{\\kappa_{1}(x_{1}^{[k]})}\\end{aligned}\\ ] ] then , for @xmath104 , we similarly compute and store the matrix of transition probabilities @xmath105}|x_{i-1}^{[1 ] } ) & \\ldots & p(x_{i}^{[l_{x}]}|x_{i-1}^{[1 ] } ) \\\\ \\vdots & \\ddots & \\vdots \\\\ p(x_{i}^{[1]}|x_{i-1}^{[l_{x } ] } ) & \\ldots & p(x_{i}^{[l_{x}]}|x_{i-1}^{[l_{x } ] } ) \\\\ \\end{pmatrix}\\ ] ] where @xmath106 } | x_{i-1}^{[k_{2 } ] } ) \\propto \\exp(-(x_{i}^{[k_{1 } ] } - \\phi x_{i-1}^{[k_{2}]})^{2}/2)\\end{aligned}\\ ] ] are transition probabilities between pool states @xmath107}$ ] and @xmath108}$ ] for @xmath109 .",
    "we then use the stored values of the transition probabilities @xmath110 to efficiently compute the vector of forward probabilities for all values of @xmath102}$ ] in the pool @xmath111}(x_{i } | \\eta^{[l ] } ) & = & \\frac{p(y_{i}|x_{i } , \\eta^{[l]})}{\\kappa_{i}(x_{i})}\\sum_{k=1}^{l_{x } } p(x_{i } | x_{i-1}^{[k ] } ) \\alpha_{i-1}^{[l]}(x_{i-1}^{[k ] } | \\eta^{[l ] } ) , \\quad i = 1 , \\ldots , n\\end{aligned}\\ ] ] with @xmath112 } , \\ldots , x_{i}^{[l_{x}]}\\}$ ] .    at each time @xmath19",
    ", we divide the forward probabilities @xmath113}(x_{i})$ ] by @xmath114 } = \\sum_{k=1}^{l_{x}}\\alpha_{i}^{[l]}(x_{i } | \\eta^{[l]})$ ] , storing the @xmath114}$ ] values and using the normalized @xmath113}$ ] s in the next step of the recursion .",
    "this is needed to prevent underflow and for ensemble density computations . in the case of all the forward probabilities summing to @xmath115",
    ", we set the forward probabilities at all subsequent times to @xmath115 .",
    "note that we wo nt get underflows for all values of @xmath102}$ ] , since we are guaranteed to have a log - likelihood that is not @xmath116 for the current value of @xmath86 in the mcmc state .    for each @xmath102}$",
    "] , the ensemble density can then be computed as @xmath117 } = \\prod_{i=1}^{n } c_{i}^{[l]}\\end{aligned}\\ ] ] to avoid overflow or underflow , we work with the logarithm of @xmath118}$ ] .",
    "even with caching , computing the forward probabilities for each @xmath102}$ ] in the pool is still an order @xmath119 operation since we multiply the vector of forward probabilities from the previous step by the transition matrix .",
    "however , if we do not cache and re - use the transition probabilities @xmath110 when computing the forward probabilities for each value of @xmath102}$ ] in the pool , the computation of the ensemble densities @xmath118}$ ] , for all @xmath120 , would be about @xmath121 times slower .",
    "this is because computing forward probabilities for a value of @xmath86 given saved transition probabilities only involves multiplications and additions , and not exponentiations , which are comparatively more expensive .    in ens1 , after mapping to the ensemble , we immediately sample new values of @xmath86 and @xmath11 from the ensemble .",
    "we first sample a @xmath102}$ ] from the marginal ensemble distribution , with probabilities proportional to @xmath118}$ ] .",
    "after we have sampled an @xmath102}$ ] , we sample a latent sequence @xmath11 conditional on @xmath102}$ ] , using a stochastic backwards recursion .",
    "the stochastic backwards recursion first samples a state @xmath122 from the pool at time @xmath48 with probabilities proportional to @xmath123}(x_{n } | \\eta^{[l]})$ ] .",
    "then , given the sampled value of @xmath3 , we sample @xmath124 from the pool at time @xmath125 with probabilities proportional to @xmath126}(x_{i-1 } | \\eta^{[l]})$ ] , going back to time @xmath100 .    in the terminology of shestopaloff and neal ( 2013 )",
    "this is a `` single sequence '' update combined with an ensemble update for @xmath86 ( which is a `` fast '' variable in the terminology of neal ( 2010 ) since recomputation of the likelihood function after changes to this variable is fast given the saved transition probabilities ) .    in ens2 , before mapping back to a new @xmath86 and a new @xmath11 as in ens1 , we perform a metropolis update for @xmath47 using the ensemble density summing over all @xmath102}$ ] and all latent sequences in the ensemble , @xmath127}$ ] .",
    "this approximates updating @xmath47 using the posterior density of @xmath7 with @xmath11 and @xmath86 integrated out , when the number of pool states is large .",
    "the update nevertheless leaves the correct distribution exactly invariant , even if the number of pool states is not large .",
    "a good choice for the pool distribution is crucial for the efficient performance of the ensemble mcmc method .    for a pool distribution for @xmath3 ,",
    "a good candidate is the stationary distribution of @xmath3 in the ar(1 ) latent process , which is @xmath128 .",
    "the question here is how to choose @xmath47 . for ens1 , which does not change @xmath47",
    ", we can simply use the current value of @xmath47 from the mcmc state , call it @xmath129 and draw pool states from @xmath130 for some scaling factor @xmath40 .",
    "typically , we would choose @xmath131 in order to ensure that for different values of @xmath47 , we produce pool states that cover the region where @xmath3 has high probability density .",
    "we can not use this pool selection scheme for ens2 because the reverse transition after a change in @xmath47 would use different pool states , undermining the proof via reversibility that the ensemble transitions leave the posterior distribution invariant .",
    "however , we can choose pool states that depend on both the current and the proposed values of @xmath47 , say @xmath47 and @xmath132 , in a symmetric fashion .",
    "for example , we can propose a value @xmath132 , and draw the pool states from @xmath133 where @xmath134 is the average of @xmath47 and @xmath132 .",
    "the validity of this scheme can be seen by considering @xmath132 to be an additional variable in the model ; proposing to update @xmath47 to @xmath132 can then be viewed as proposing to swap @xmath47 and @xmath132 within the mcmc state .",
    "we choose pool states for @xmath86 by sampling them from the model prior .",
    "alternative schemes are possible , but we do not consider them here .",
    "for example , it is possible to draw local pool states for @xmath86 which stay close to the current value of @xmath86 by running a markov chain with some desired stationary distribution @xmath135 steps forwards and @xmath136 steps backwards , starting at the current value of @xmath86 .",
    "for details , see neal ( 2003 ) .    in our earlier work ( shestopaloff and neal ( 2013 ) ) , one recommendation we made was to consider pool states that depend on the observed data @xmath4 at a given point , constructing a `` pseudo - posterior '' for @xmath3 using data observed at time @xmath19 or in a small neighbourhood around @xmath19 . for the ensemble",
    "updates ens1 and ens2 presented here , we can not use this approach , as we would then need to make the pool states also depend on the current values of @xmath40 and @xmath86 , the latter of which is affected by the update .",
    "we could switch to the centered parametrization to avoid this problem , but that would prevent us from making @xmath86 a fast variable .",
    "the goal of our computational experiments is to determine how well the introduced variants of the ensemble method compare to our improved version of the kastner and fruwirth - schnatter ( 2014 ) method .",
    "we are also interested in understanding when using a full ensemble update is helpful or not .",
    "we use a series simulated from the stochastic volatility model with parameters @xmath137 with @xmath138 .",
    "a plot of the data is presented in figure [ fig : data ] .",
    "0.48        0.48     we use the following priors for the model parameters . @xmath139 \\\\ \\sigma^{2 } & \\sim & \\textnormal{inverse - gamma}(2.5 , 0.075)\\end{aligned}\\ ] ] we use the parametrization in which the inverse - gamma@xmath140 has probability density @xmath141 for @xmath142 the @xmath143 and @xmath144 quantiles of this distribution are approximately + @xmath145 .    in the mcmc state , we transform @xmath47 and @xmath84 to @xmath146 with the priors transformed correspondingly .",
    "we compare three sampling schemes  the kastner and fruwirth - schnatter ( kf ) method , and our two ensemble schemes , ens1 , in which we map to an ensemble of @xmath86 and @xmath11 values and immediately map back , and ens2 , in which we additionally update @xmath147 with an ensemble update before mapping back .",
    "we combine the ensemble scheme with the computationally cheap asis metropolis updates .",
    "it is sensible to add cheap updates to a sampling scheme if they are available .",
    "note that the asis ( or translation and scale ) updates we use in this paper are generally applicable to location - scale models and are not restricted by the linear and gaussian assumption .",
    "pilot runs showed that @xmath148 updates appears to be the point at which we start to get diminishing returns from using more metropolis updates ( given the sufficient statistics ) in the kf scheme .",
    "this is the number of metropolis updates we use with the ensemble schemes as well .",
    "the kf scheme updates the state as follows :    1 .",
    "update @xmath11 , using the kalman filter - based update , using the current mixture indicators @xmath38 .",
    "2 .   update the parameters using the mixture approximation to the observation density .",
    "this step consists of @xmath148 metropolis updates to @xmath47 given the sufficient statistics for nc , followed by one joint update of @xmath40 and @xmath86 .",
    "3 .   change to the c parametrization .",
    "4 .   update all three parameters simultaneously using @xmath148 metropolis updates , given the sufficient statistics for c. note that this update does not depend on the observation density and is therefore exact .",
    "update the mixture indicators @xmath38 .",
    "the ens1 scheme proceeds as follows :    1 .",
    "map to an ensemble of @xmath86 and @xmath11 .",
    "2 .   map back to a new value of @xmath86 and @xmath11 .",
    "3 .   do steps 2 ) - 4 ) as for kf , but with the exact observation density .",
    "the ens2 scheme proceeds as follows :    1 .",
    "map to an ensemble of @xmath86 and @xmath11 .",
    "2 .   update @xmath147 using an ensemble metropolis update .",
    "3 .   map back to a new value of @xmath86 and @xmath11 .",
    "4 .   do steps 2 ) - 4 ) as for kf , but with the exact observation density .",
    "the metropolis updates use a normal proposal density centered at the current parameter values .",
    "proposal standard deviations for the metropolis updates in nc were set to estimated marginal posterior standard deviations , and to half of that in c. this is because in c , we update all three parameters at once , whereas in nc we update @xmath40 and @xmath86 jointly and @xmath47 separately .",
    "the marginal posterior standard deviations were estimated using a pilot run of the ens2 method .",
    "the tuning settings for the metropolis updates are presented in table [ table : metstat ] .    for ensemble updates of @xmath147",
    ", we also use a normal proposal density centered at the current value of @xmath147 , with a proposal standard deviation of @xmath100 , which is double the estimated marginal posterior standard deviation of @xmath147 .",
    "the pool states over @xmath3 are selected from the stationary distribution of the ar(1 ) latent process , with standard deviation @xmath149 for the ens1 scheme and @xmath150 for the ens2 scheme .",
    "we used the prior density of @xmath86 to select pool states for @xmath86 .     &",
    "@xmath40 & @xmath147 & @xmath86 & for @xmath147 ( nc ) & for @xmath151 ( nc ) & @xmath40 & @xmath147 & @xmath86 & for @xmath152 + kf & & & & & 0.12 & & & & + ens1 & & & & & & & & & + ens2 & & & & & & & & & +    for each method , we started the samplers from @xmath153 randomly chosen points .",
    "parameters were initalized to their prior means ( which were @xmath115 for @xmath40 , @xmath154 for @xmath147 and @xmath155 for @xmath86 ) , and each @xmath156 , was initialized independently to a value randomly drawn from the stationary distribution of the ar(1 ) latent process , given @xmath147 set to the prior mean . for the kf updates , the mixture indicators @xmath38 where all initialized to @xmath153 s ,",
    "this corresponds to the mixture component whose median matches the median of the @xmath15 distribution most closely .",
    "all methods were run for approximately the same amount of computational time .      before comparing the performance of the methods",
    ", we verified that the methods give the same answer up to expected variation by looking at the @xmath157 confidence intervals each produced for the posterior means of the parameters .",
    "these confidence intervals were obtained from the standard error of the average posterior mean estimate over the five runs .",
    "the kf estimates were adjusted using the importance weights that compensate for the use of the approximate observation distribution .",
    "no significant disagreement between the answers from the different methods was apparent .",
    "we then evaluated the performance of each method using estimates of autocorrelation time , which measures how many mcmc draws are needed to obtain the equivalent of one independent draw .",
    "to estimate autocorrelation time , we first estimated autocovariances for each of the five runs , discarding the first @xmath158 of the run as burn - in , and plugging in the overall mean of the five runs into the autocovariance estimates .",
    "( this allows us to detect if the different runs for each method are exploring different regions of the parameter / latent variable space ) .",
    "we then averaged the resulting autocovariance estimates and used this average to get autocorrelation estimates @xmath159 .",
    "finally , autocorrelation time was estimated as @xmath160 , with @xmath12 chosen to be the point beyond which the @xmath161 become approximately @xmath115 .",
    "all autocovariances were estimated using the fast fourier transform for computational efficiency .",
    "the results are presented in tables @xmath162 and @xmath163 .",
    "the timings for each sampler represent an average over @xmath164 iteratons ( each iteration consisting of the entire sequence of updates ) , with the samplers started from a point taken after the sampler converged to equilibrium .",
    "the program was written in matlab and run on a linux system with an intel xeon x5680 3.33 ghz cpu . for a fair comparison , we multiply estimated autocorrelation times by the time it takes to do one iteration and compare these estimates .",
    "& & & & @xmath40 & @xmath147 & @xmath86 & @xmath40 & @xmath147 & @xmath86 + & 1 & 195000 & 0.11 & 2.6 & 99 & 160 & 0.29 & 11 & 18 + & 10 & 180000 & 0.12 & 2.7 & 95 & 150 & 0.32 & 11 & 18 + & 30 & 155000 & 0.14 & 2.6 & 81 & 130 & 0.36 & 11 & 18 + & 50 & 140000 & 0.16 & 2.3 & 91 & 140 & 0.37 & 15 & 22 + & 1 & 155000 & 0.14 & 2.4 & 35 & 71 & 0.34 & 4.9 & 9.9 + & 10 & 135000 & 0.16 & 2.2 & 18 & 26 & 0.35 & 2.9 & 4.2 + & 30 & 110000 & 0.20 & 2.3 & 19 & 26 & 0.46 & 3.8 & 5.2 + & 50 & 65000 & 0.33 & 1.9 & 16 & 24 & 0.63 & 5.3 & 7.9 + & 1 & 115000 & 0.19 & 1.9 & 34 & 68 & 0.36 & 6.5 & 13 + & 10 & 95000 & 0.23 & 1.9 & 11 & 17 & 0.44 & 2.5 & 3.9 + & 30 & 55000 & 0.38 & 2.2 & 8.9 & 12 & 0.84 & 3.4 & 4.6 + & 50 & 55000 & 0.39 & 1.9 & 11 & 14 & 0.74 & 4.3 & 5.5 + & 1 & 85000 & 0.25 & 2.2 & 33 & 67 & 0.55 & 8.3 & 17 + & 10 & 60000 & 0.38 & 1.9 & 8.3 & 11 & 0.72 & 3.2 & 4.2 + & 30 & 50000 & 0.42 & 1.8 & 8.4 & 11 & 0.76 & 3.5 & 4.6 + & 50 & 45000 & 0.48 & 1.9 & 9.1 & 12 & 0.91 & 4.4 & 5.8 +     & & & & & @xmath40 & @xmath47 & @xmath86 & @xmath40 & @xmath147 & @xmath86 + & 1 & 0.32 & 110000 & 0.20 & 2.5 & 100 & 170 & 0.5 & 20 & 34 + & 10 & 0.32 & 95000 & 0.23 & 2.4 & 91 & 140 & 0.55 & 21 & 32 + & 30 & 0.32 & 80000 & 0.27 & 2.5 & 97 & 150 & 0.68 & 26 & 41 + & 50 & 0.32 & 70000 & 0.30 & 2.7 & 90 & 140 & 0.81 & 27 & 42 + & 1 & 0.33 & 80000 & 0.26 & 2.3 & 34 & 68 & 0.6 & 8.8 & 18 + & 10 & 0.33 & 70000 & 0.31 & 2.3 & 18 & 26 & 0.71 & 5.6 & 8.1 + & 30 & 0.33 & 55000 & 0.39 & 2 & 18 & 27 & 0.78 & 7 & 11 + & 50 & 0.34 & 35000 & 0.61 & 2.4 & 12 & 19 & 1.5 & 7.3 & 12 + & 1 & 0.34 & 60000 & 0.36 & 1.7 & 33 & 69 & 0.61 & 12 & 25 + & 10 & 0.35 & 50000 & 0.44 & 2.1 & 10 & 15 & 0.92 & 4.4 & 6.6 + & 30 & 0.34 & 30000 & 0.71 & 1.8 & 10 & 15 & 1.3 & 7.1 & 11 + & 50 & 0.34 & 25000 & 0.81 & 1.8 & 12 & 17 & 1.5 & 9.7 & 14 + & 1 & 0.34 & 45000 & 0.49 & 2.2 & 29 & 61 & 1.1 & 14 & 30 + & 10 & 0.35 & 30000 & 0.72 & 1.6 & 7.3 & 11 & 1.2 & 5.3 & 7.9 + & 30 & 0.36 & 25000 & 0.86 & 1.6 & 7.3 & 9.3 & 1.4 & 6.3 & 8 + & 50 & 0.36 & 25000 & 0.96 & 1.8 & 5.9 & 7.8 & 1.7 & 5.7 & 7.5 +    we ran the kf method for @xmath165 iterations , with estimated autocorrelation times using the original ( unweighed ) sequence for @xmath152 of @xmath166 , which after adjusting by computation time of @xmath167 seconds per iteration are @xmath168 .",
    "it follows that the ens1 method with @xmath169 set to @xmath170 and @xmath87 set to @xmath121 is better than the kf method by a factor of about @xmath0 for the parameter @xmath86 . for ens2 , the same settings @xmath171 and @xmath172 appears to give the best results , with ens2 worse by a factor of about @xmath173 than ens1 for sampling @xmath86 .",
    "we also see that the ens1 and ens2 methods are nt too sensitive to the particular tuning parameters , so long at there is a sufficient number of ensemble elements both for @xmath3 and for @xmath86 .",
    "the results show that using a small ensemble ( @xmath121 or so pool states ) over @xmath86 is particularly helpful .",
    "one reason for this improvement is the ability to use the caching technique to make these updates computationally cheap .",
    "a more basic reason is that updates of @xmath86 consider the entire collection of latent sequences , which allows us to make large changes to @xmath86 , compared to the metropolis updates .",
    "even though the ens2 method in this case is outperformed by the ens1 method , we have only applied it to one data set and there is much room for further tuning and improvement of the methods .",
    "a possible explanation for the lack of substantial performance gain with the ensemble method is that conditional on a single sequence , the distribution of @xmath47 has standard deviation comparable to its marginal standard deviation , which means that we ca nt move too much further with an ensemble update than we do with our metropolis updates .",
    "an indication of this comes from the acceptance rate for ensemble updates of @xmath147 in ens2 , which we can see is nt improved by much as more pool states are added .",
    "parameter estimates for the best performing kf , ens1 and ens2 settings are presented in table [ table : est ] .",
    "these estimates were obtained by averaging samples from all @xmath153 runs with @xmath158 of the sample discarded as burn - in .",
    "we see that the differences between the standard errors are in approximate agreement with the differences in autocorrelation times for the different methods .",
    ".estimates of posterior means , with standard errors of posterior means shown in brackets . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "we found that noticeable performance gains can be obtained by using ensemble mcmc based sampling methods for the stochastic volatility model . it may be possible to obtain even larger gains on different data sets , and with even better tuning . in particular , it is possible that the method of updating @xmath47 with an ensemble , or some variation of it , actually performs better than a single sequence method in some other instance .",
    "the method of kastner and fruwirth - schnatter ( 2014 ) relies on the assumption that the state process is linear and gaussian , which enables efficient state sequence sampling using kalman filters",
    ". the method would not be applicable if this was not the case .",
    "however , the ensemble method could still be applied to this case as well .",
    "it would be of interest to investigate the performance of ensemble methods for stochastic volatility models with different noise structures for the latent process .",
    "it would also be interesting to compare the performance of the ensemble mcmc method with the pmcmc - based methods of andrieu et .",
    "al ( 2010 ) and also to see whether techniques used to improve pmcmc methods can be used to improve ensemble methods and vice versa .",
    "multivariate versions of stochastic volatility models , for example those considered in scharth and kohn ( 2013 ) are another class of models for which inference is difficult , and that it would be interesting to apply the ensemble mcmc method to .",
    "we have done preliminary experiments applying ensemble methods to multivariate stochastic volatility models , with promising results . for these models , even though the latent process is linear and gaussian , due to a non - constant covariance matrix the observation process does not have a simple and precise mixture of gaussians approximation .",
    "this research was supported by the natural sciences and engineering research council of canada .",
    "a.  s.  is in part funded by an nserc postgraduate scholarship .",
    "r.  n.  holds a canada research chair in statistics and machine learning .",
    "0.2 in    andrieu , c. , doucet , a. and holenstein , r. ( 2010 ) .",
    "`` particle markov chain monte carlo methods '' , _ journal of the royal statistical society b _ , vol .",
    "72 , pp .",
    "269 - 342 .",
    "kastner , g. and fruhwirth - schnatter , s. ( 2014 ) .",
    "`` ancillarity - sufficiency interweaving strategy ( asis ) for boosting mcmc estimation of stochastic volatility models '' , _ computational statistics & data analysis _",
    "76 , pp .  408 - 423 .",
    "kim , s. , shephard , n. and chib , s. ( 1998 ) .",
    "`` stochastic volatility : likelihood inference and comparison with arch models '' , _ review of economic studies_. vol .",
    "65 , pp .  361 - 393 .",
    "lindsten , f. and schon , t. b. ( 2013 ) .",
    "`` backward simulation methods for monte carlo statistical inference '' , _ foundations and trends in machine learning_. vol .",
    "6(1 ) , pp .  1 - 143 .",
    "liu , j.s . and sabatti , c. ( 2000 ) .",
    "`` generalized gibbs sampler and multigrid monte carlo for bayesian computation '' , _ biometrika _ , vol .",
    "87 , pp .",
    "353 - 369 .",
    "liu , j.s . and wu , y.n .",
    "`` parameter expansion for data augmentation '' , _ journal of the american statistical association _ vol .",
    "94 , pp .",
    "1264 - 1274 .",
    "neal , r. m. ( 2003 ) . `` markov chain sampling for non - linear state space models using embedded hidden markov models '' , technical report no . 0304 , department of statistics , university of toronto , http://arxiv.org/abs/math/0305039 .",
    "neal , r. m. , beal , m. j. , and roweis , s. t. ( 2004 ) .",
    "`` inferring state sequences for non - linear systems with embedded hidden markov models '' , in s. thrun , et al ( editors ) , _ advances in neural information processing systems 16 _ , mit press .",
    "neal , r. m. ( 2010 ) .",
    "`` mcmc using ensembles of states for problems with fast and slow variables such as gaussian process regression '' , technical report no . 1011 , department of statistics , university of toronto , http://arxiv.org/abs/1101.0387 .",
    "omori , y. , chib , s. , shephard , n. and nakajima , j. ( 2007 ) .",
    " stochastic volatility model with leverage : fast and efficient likelihood inference  , _ journal of econometrics _ ,",
    "vol .  140 - 2 , pp .  425 - 449 .",
    "petris , g. , petrone , s. and campagnoli , p. ( 2009 ) . _ dynamic linear models with r _ , springer : new york .",
    "scharth , m. and kohn , r. ( 2013 ) .",
    "`` particle efficient importance sampling '' , arxiv preprint 1309.6745v1 .",
    "shestopaloff , a. y. and neal , r. m. ( 2013 ) .",
    "`` mcmc for non - linear state space models using ensembles of latent sequences '' , technical report , http://arxiv.org/abs/1305.0320 .",
    "yu , y. and meng , x. ( 2011 ) .",
    "`` to center or not to center , that is not the question : an ancillarity - sufficiency interweaving strategy ( asis ) for boosting mcmc efficiency '' , _ journal of computational and graphical statistics _ , vol .",
    "20 ( 2011 ) , pp .",
    "531 - 570 .",
    "here , we derive the sufficient statistics for the stochastic volatility model in the two parametrizations and the likelihoods in terms of sufficient statistics ."
  ],
  "abstract_text": [
    "<S> in this paper , we introduce efficient ensemble markov chain monte carlo ( mcmc ) sampling methods for bayesian computations in the univariate stochastic volatility model . </S>",
    "<S> we compare the performance of our ensemble mcmc methods with an improved version of a recent sampler of kastner and fruwirth - schnatter ( 2014 ) . </S>",
    "<S> we show that ensemble samplers are more efficient than this state of the art sampler by a factor of about @xmath0 , on a data set simulated from the stochastic volatility model . </S>",
    "<S> this performance gain is achieved without the ensemble mcmc sampler relying on the assumption that the latent process is linear and gaussian , unlike the sampler of kastner and fruwirth - schnatter .    </S>",
    "<S> the stochastic volatility model is a widely - used example of a state space model with non - linear or non - gaussian transition or observation distributions . </S>",
    "<S> it models observed log - returns @xmath1 of a financial time series with time - varying volatility , as follows : @xmath2 here , the latent process @xmath3 determines the unobserved log - volatility of @xmath4 . </S>",
    "<S> because the relation of the observations to the latent state is not linear and gaussian , this model can not be directly handled by efficient methods based on the kalman filter .    in a bayesian approach to this problem </S>",
    "<S> , we estimate the unknown parameters @xmath5 by sampling from their marginal posterior distribution @xmath6 . </S>",
    "<S> this distribution can not be written down in closed form . </S>",
    "<S> we can , however , write down the joint posterior of @xmath7 and the log - volatilities @xmath8 , @xmath9 and draw samples of @xmath10 from it . </S>",
    "<S> discarding the @xmath11 coordinates in each draw will give us a sample from the marginal posterior distribution of @xmath7 .    </S>",
    "<S> to sample from the posterior distribution of the stochastic volatility model , we develop two new mcmc samplers within the framework of ensemble mcmc , introduced by neal ( 2010 ) . </S>",
    "<S> the key idea underlying ensemble mcmc is to simultaneously look at a collection of points ( an `` ensemble '' ) in the space we are sampling from , with the @xmath12 ensemble elements chosen in such a way that the density of interest can be simultaneously evaluated at all of the ensemble elements in less time than it would take to evaluate the density at all @xmath12 points separately .    </S>",
    "<S> previously , shestopaloff and neal ( 2013 ) developed an ensemble mcmc sampler for non - linear , non - gaussian state space models , with ensembles over latent state sequences , using the embedded hmm ( hidden markov model ) technique of neal ( 2003 ) , neal et al . </S>",
    "<S> ( 2004 ) . </S>",
    "<S> this ensemble mcmc sampler was used for bayesian inference in a population dynamics model and shown to be more efficient than methods which only look at a single sequence at a time . in this paper </S>",
    "<S> we consider ensemble mcmc samplers that look not only at ensembles over latent state sequences as in shestopaloff and neal ( 2013 ) but also over a subset of the parameters . </S>",
    "<S> we see how well both of these methods work for the widely - used stochastic volatility model . </S>"
  ]
}