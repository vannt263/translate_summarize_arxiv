{
  "article_text": [
    "link discovery frameworks are of utmost importance during the creation of linked data  @xcite .",
    "this is due to their being the key towards the implementation of the fourth linked data principle , i.e. , the provision of links between datasets .",
    "two main challenges need to be addressed by link discovery frameworks  @xcite .",
    "first , they need to address the _ accuracy challenge _ ,",
    "i.e. , they need to generate correct links .",
    "a plethora of approaches have been developed for this purpose and contain algorithms ranging from genetic programming to probabilistic models .",
    "in addition to addressing the need for accurate links , link discovery frameworks need to address the _ challenge of time efficiency_. this challenge comes about because of the mere size of knowledge bases that need to be linked . in particular ,",
    "large knowledge bases such as linkedtcga  @xcite contain more than 20 billion triples .",
    "one of the approaches to improving the scalability of link discovery frameworks is to use planning algorithms in a manner akin ( but not equivalent to ) their use in databases  @xcite . in general , planners rely on cost functions to estimate the runtime of particular portions of link specifications .",
    "so far , it has been assumed that this cost function is linear in the parameters of the planning , i.e. , in the size of the datasets and the similarity threshold .",
    "however , this assumption has never been verified . in this paper , we address exactly this research gap and _ study how well other models for runtime approximation perform_. in particular , we study linear , exponential and mixed models for runtime estimation .",
    "the contributions of this paper are thus as follows :    * we present three different models for runtime approximation in planning for link discovery .",
    "* we compare these models on six different datasets and study how well they can approximate runtimes of specifications as well as with respect to how well they generalize across datasets . *",
    "we integrate the models with the helios planner for link discovery as described in  @xcite and compare their performance using 400 specifications .",
    "the rest of the paper is structured as follows : in section  [ sec : preliminaries ] , we present the concept and notations necessary to understand this work . the subsequent section , section  [ sec : approach ] , presents the runtime approximation problem and how it can be addressed by different models .",
    "we then delve into a thorough evaluation of these models in section  [ sec : evaluation ] and compare the expected runtimes generated by the models at hand with the real runtimes of the link discovery framework .",
    "we also study the transferability of the results we achieve and their performance when planning whole link specifications .",
    "finally , we recapitulate our results and conclude .",
    "in this section , we present the necessary concepts and notations to understand the rest of the paper .",
    "we begin by giving a description of a knowledge base @xmath0 and link discovery ( ld ) , we continue by providing a formal definition of a link specification ( ls ) and its semantics and we finish our preliminary section with an explanatory presentation of a plan , its components and its relation to a ls .",
    "[ [ knowledge - base . ] ] knowledge base .",
    "+ + + + + + + + + + + + + + +    a knowledge base @xmath0 is a set of triples @xmath1 , where @xmath2 is the set of all rdf resources , @xmath3 is the set of all rdf properties , @xmath4 is the set of all rdf blank nodes and @xmath5 is the set of all literals .",
    "[ [ link - discovery . ] ] link discovery .",
    "+ + + + + + + + + + + + + + +    given two ( not necessarily distinct ) sets of rdf resources @xmath6 and @xmath7 and a relation @xmath8 ( e.g , ` directorof ` , ` owl : sameas ` ) , the main goal of ld is to discover the set ( _ mapping _ ) @xmath9 .",
    "given that this task can be very tedious ( especially when @xmath6 and @xmath7 are large ) , ld frameworks are commonly used to achieve this computation .",
    "[ [ link - specification . ] ] link specification .",
    "+ + + + + + + + + + + + + + + + + + +    declarative ld frameworks use link specifications ( lss ) to describe the conditions for which @xmath10 holds for a pair @xmath11 .",
    "a ls consists of two basic components :    * _ similarity measures _ which allow the comparison of property values of resources found in the input data sets @xmath6 and @xmath7 .",
    "we define an _ atomic similarity measure _ @xmath12 as a function @xmath13 $ ] .",
    "we write @xmath14 to signify the similarity of @xmath15 and @xmath16 w.r.t .",
    "their properties @xmath17 resp . @xmath18 . *",
    "_ operators _ @xmath19 that allow the combination of two _",
    "similarity measures_.    an atomic ls consists of one similarity measure and has the form @xmath20 where @xmath21 $ ] .",
    "a complex ls @xmath22 consists of two ls , @xmath23 and @xmath24 .",
    "we call @xmath23 the _ left sub - specification _ and @xmath24 the _ right sub - specification _ of @xmath25 .",
    "we denote the semantics ( i.e. , the results of a ls for given sets of resources @xmath6 and @xmath7 ) of a ls @xmath25 as @xmath26}$ ] and call it a mapping .",
    "we begin by assuming the natural semantics of the combinations of measures .",
    "filters are pairs @xmath27 , where ( 1 ) @xmath28 is either empty ( denoted @xmath29 ) or a combination of similarity measures by means of specification operators and ( 2 ) @xmath30 is a threshold .",
    "note that an atomic specification can be regarded as a filter @xmath31 with @xmath32 = s \\times t$ ] .",
    "we will thus use the same graphical representation for filters and atomic specifications .",
    "we call @xmath27 the _ filter of @xmath25 _ and denote it with @xmath33 . for our example @xmath25 in fig .",
    "[ fig : specexample ] , @xmath34 .",
    "we denote the _ operator of a ls _ @xmath25 with @xmath35 . for @xmath36 , @xmath37 .",
    "the operator of the ls shown in our example is @xmath38 .",
    "the semantics of lss are then as shown in table  [ tab : semantics ] .",
    "[ [ execution - plan . ] ] execution plan .",
    "+ + + + + + + + + + + + + + +    to compute the mapping @xmath26}$ ] ( which corresponds to the output of @xmath25 for a given pair ( @xmath39 ) ) , ld frameworks implement ( at least partly ) a generic architecture consisting of a rewriter ( optional ) , a planner ( optional ) and an execution engine ( necessary ) .",
    "the _ rewriter _ performs algebraic operations to transform the input ls @xmath25 into a ls @xmath40 ( with @xmath26 } = { [ [ l]]}'$ ] ) that is potentially faster to execute .",
    "the most common planner is the _ canonical planner _ ( dubbed canonical ) , which simply traverses @xmath25 in post - order and has its results computed in that order by the execution engine . for the ls shown in fig .",
    "[ fig : specexample ] , the execution plan returned by canonicalwould thus foresee to first compute the mapping @xmath41 $ ] of pairs of resources whose property ` title ` has a cosine similarity greater or equal to @xmath42 .",
    "the computation of @xmath43 $ ] would follow .",
    "step 3 would be to compute @xmath44 while abiding by the semantics described in table  [ tab : semantics ] .",
    "step 4 would be to obtain @xmath45 by filtering the results and keeping only the pairs that have a similarity above 0.5 .",
    "step 5 would be @xmath46 $ ] and step 6 would be to compute @xmath47 . finally , step 7 would be to filter out the pairs of links in @xmath48 that have a similarity less than 0.8 . given that there is a 1 - 1 correspondence between ls and the plan generated by the canonical planner , we will reuse the representation of ls devised above for plans .",
    "the sequence of steps for such a plan is then to be understood as the sequence of steps that would be derived by canonicalfor the ls displayed .",
    ".semantics of link specifications [ cols=\"^,<\",options=\"header \" , ]     ( filter1)[]@xmath49 ; ( intersection ) [ right = of filter1,xshift=0.8 cm ] @xmath38 edge [ - > ] ( filter1 ) ; ( label3)[right = of intersection.west,anchor=west,xshift=2.2cm ] @xmath50 edge [ - > ] ( intersection ) ; ( filter2)[below = of intersection]@xmath51edge [ - > ] ( intersection ) ; ( intersection2)[right = of filter2.west,xshift=.8cm ] @xmath52 edge [ - > ] ( filter2 ) ; ( label2)[right = of intersection2.west,xshift=.4cm ] @xmath53edge [ - > ] ( intersection2 ) ; ( label1)[below = of label2.west,anchor=west]@xmath54edge [ - > ] ( intersection2 ) ;",
    "in general , planners aims to estimate the cost of the leaves of a plan , i.e. , the runtime of atomic link specifications .",
    "so far , linear models @xcite have been used for this purpose but the appropriateness of other models has never been evaluated .",
    "hence , in this work , we compare non - linear models with linear models to approximate the runtime of of atomic link specifications . like in previous works",
    ", we follow a _ sampling - based approach_. first , given a particular similarity measure @xmath55 ( e.g. , levenshtein ) and an implementation of the said measure ( e.g. , _ ed - join _",
    "@xcite ) , we begin by collecting sample of runtimes for a given measure with varying values of @xmath56 , @xmath57 and @xmath58 . and @xmath7 but found that they do not affect the models we considered .",
    "an exploration of other parameters remains future work .",
    "] these samples can be regarded as the output of a function that can predict the runtime of the implementation of @xmath55 for which we were given samples .",
    "the major question that is to be answered is hence _",
    "what is the shape of the runtime evaluation function _ ?",
    "we tried fitting functions of different shapes to the previously measured runtimes in order to compare their performance when planning the execution of link specifications .",
    "formally , these functions are mappings @xmath59 \\mapsto \\mathbb{r}$ ] , whose value at @xmath60 is an approximation of the runtime for the link specification with these parameters .",
    "if @xmath61 are the measured runtimes for the parameters @xmath62 , @xmath63 and @xmath64 , then we constrain the mapping @xmath65 to be a local minimum of the l2-loss : @xmath66 writing @xmath67 .    within this paper",
    ", we consider the following parametrized families of functions : @xmath68 the parameters are then determined by @xmath69 for some local minimum . in the case of @xmath70 and @xmath71",
    "this problem is linear in nature and we solved it using the pseudo - inverse of the associated vandermonde matrix .",
    "for @xmath72 we used the levenberg - marquardt algorithm @xcite for nonlinear least squares problems , using @xmath73 as initial guess for all parameters .",
    "we chose @xmath70 as the baseline linear fit .",
    "@xmath71 is the standard log - linear fit , except for the @xmath74 term .",
    "we included this term during a grid search for polynomials to perform a log - polynomial fit .",
    "higher orders of @xmath56 or @xmath57 or @xmath58 did not contribute to a better fit .",
    "@xmath72 can be interpreted as an interpolation of @xmath70 and @xmath71 with a constant offset @xmath75 .    to exemplify our approach for @xmath71 , assume we have measured @xmath76 @xmath77 and @xmath78 . inserting into eq .",
    "( 1 ) and taking the logarithm , one arrives at the optimization problem @xmath79 the solution to this least squares problem also is the unique solution of its normal equations : @xmath80 @xmath81 by multiplying and inverting matrices , we arrive at the linear equation @xmath82 where @xmath83 denotes the moore - penrose pseudo inverse of @xmath84 @xcite . multiplying the matrices , we arrive at @xmath85",
    "thus we have found the coefficients of the fit function .",
    "we evaluated the three runtime estimation models using six data sets .",
    "the first three are the benchmark data sets for ld dubbed amazon - google products , dblp - acm and dblp - scholar described in @xcite .",
    "we also created two larger additional data sets ( movies and villages , see table  [ tab : datasetsproperties ] ) from the data sets dbpedia , linkedgeodata and linkedmdb .",
    "the sixth dataset was the set of all english labels from dbpedia 2014 .",
    "table  [ tab : datasetsproperties ] describes the characteristics of the datasets and presents the properties used when linking the retrieved resources for the first four datasets .",
    "the mapping properties were provided to the link discovery algorithms underlying our results .",
    "each of our experiments consisted of two phases : during the _ training _ phase , we trained each of the models independently . for each model , we computed the set of coefficients for each of the approximation models that minimized the root mean squared error ( rmse ) on the training data provided .",
    "the aim of the subsequent _ test _ phase was to evaluate the accuracy of the runtime estimation provided by each model and the performance of the currently best ld planner , helios@xcite , when it relied of each of the three models for runtime approximations . throughout our experiments",
    ", we used the algorithms _ ed - join _",
    "@xcite ( which implements the levenshtein string distance ) and _ ppjoin+ _ @xcite ( which implements the jaccard , overlap , cosine and trigrams string similarity measures ) to execute atomics specifications . as thresholds @xmath58 we used random values between @xmath86 and @xmath73 .",
    "the aim of our evaluation was to answer the following set of questions regarding the performance of the three models _ exp _ , _ linear _ and _ mixed _ :    * @xmath87 : how do our models fit each class separately ? to answer this question , we began by splitting the source and target data of each of our datasets into two non - overlapping parts of equal size .",
    "we used the first half of each source and each target for training and the second half for testing . * * _ training _",
    ": we trained the three models on each dataset . for each model , dataset and mapper ,",
    "we a ) selected 15 source and 15 target random samples of random sizes from the first half of a dataset ( amazon - google products , dblp - acm , dblp - scholar , movies and villages ) and b ) compared each source sample with each target sample 3 times .",
    "note that we used the same samples across all models for the sake of fairness .",
    "overall , we ran 675 training experiments to train each model on each dataset . * * _ testing _ : to test the accuracy of each model , we ran the corresponding algorithm ( _ ed - join _ and _ ppjoin+ _ ) with a random threshold between @xmath86 and @xmath73 and recorded the real runtime of the approach and the runtimes predicted by our three models .",
    "each approach was executed 100 times against the whole of the second half of the same dataset .",
    "* @xmath88 : how do our models generalize across classes , i.e. , can a model trained on data from one class be used to predict runtimes accurately on another class ? * * _ training _ : we trained each model in the same manner as for @xmath87 on exactly the same five datasets with the sole difference that the samples were selected randomly from the whole dataset . *",
    "* _ testing _ : like in the previous series of experiments , we ran _ ed - join _ and _ ppjoin+ _ with a random threshold between @xmath86 and @xmath73 .",
    "each of the algorithms was executed 100 times against the remaining four datasets .",
    "* @xmath89 : how do our models perform when trained on a large dataset ? * * _ training _",
    ": we trained in the same fashion as to answer @xmath87 with the sole differences that ( 1 ) we used 15 source and 15 target random samples of various sizes between @xmath90 and @xmath91 from ( 2 ) the english labels of dbpedia to train our model . * * _ testing _ : we learned 100 lss for the amazon - gp , dblp - acm , movies and villages datasets using the unsupervised version of the eagle algorithm @xcite .",
    "we chose this algorithm because it was shown to generate meaningful specifications that return high - quality links in previous works .",
    "for each dataset , we ran the set of 100 specifications learned by eagle on the given dataset by using each of the models during the execution in combination with the helios planning algorithm @xcite , which was shown to outperforms the canonical planner w.r.t .",
    "runtime while producing exactly the same results .    throughout our experiments",
    ", we configured eagleby setting the number of generations and population size to 20 , mutation and crossover rates were set to 0.6 .",
    "all experiments for all implementations were carried out on the same 20-core linux server running _ openjdk _ 64-bit server @xmath92 on ubuntu 14.04.4 lts on intel(r ) xeon(r ) cpu e5 - 2650 v3 processors clocked at 2.30ghz .",
    "train _ experiment and each _ test _ experiment for @xmath89 was repeated three times . as evaluation measure , we computed root mean square error ( _ rmse _ ) between the _ expected _ runtime and the average _ execution _ runtime required to run each ls .",
    "we report all three numbers for each model and dataset .",
    "to address @xmath87 , we evaluated the performance of our models when trained and tested on the same class .",
    "we present the results of this series of experiments in table  [ tab : exp1 ] . for _ ppjoin+ _",
    "( in particular the _ trigrams _ measure ) , the _ mixed _ model achieved the lowest error when tested upon amazon - gp and dblp - scholar , whereas the _ linear _ model was able to approximate the expected runtime with higher accuracy on the movies and villages datasets . on average , _ linear _ model was able to achieve a lower _ rmse _ compared to the other two models .",
    "for the _ ed - join _ , the _ mixed _ model outperformed _ linear _ and _ exp _ in the majority of datasets ( dblp - scholar , movies and villages ) and obtained the lowest _ rmse _ on average .",
    "as we observe in table  [ tab : exp1 ] , for both measures , the _ exp _ model retrieved the highest error on average and is thus the model less suitable for runtime approximations .",
    "especially , for the _ ed - join _ , _ exp _ had the worst performance in four out of the five datasets and retrieved the highest _ rmse _ among the different test datasets for villages .",
    "this clearly answers our first questions : the _ linear _ and _ mixed _ approximation models are able achieve the smallest error when trained on the class on which they are tested .",
    "to continue with @xmath88 , we conducted a set of experiments in order to observe how well each model could generalize among the different classes included in our evaluation data .",
    "tables  [ tab : exp2acm ] ,  [ tab : exp2amazon ] ,  [ tab : exp2scholar ] ,  [ tab : exp2movies ] and  [ tab : exp2villages ] present the results of training on one dataset and testing the resulting models on the set of the remaining classes .",
    "the highest _ rmse _ error was achieved when both measures were tested using the _ exp _ model in all datasets but villages .",
    "however , table  [ tab : exp2villages ] shows that the fitting error when trained on villages is relatively low among all three models .",
    "additionally , we observe that the _ exp _ model s _ rmse _ increased exponentially as the quantity of the training data decreased , which constitutes this model as inadequate and unreliable for runtime approximations . by observing tables  [ tab : exp2acm ] and  [ tab : exp2scholar ]",
    ", we see that the _ rmse _ of the _ exp _ model increased by 38 orders of magnitude for _ ed - join_.    for both measures , the _ linear _ model outperformed the other two models on average when trained on the amazon - gp , dblp - acm and dblp - scholar datasets and achieved the lowest _ rmse _ when trained on movies for _ ed - join _ compared to _ exp _ and _ mixed_. both _ linear _ and _ mixed _ achieved minuscule approximation errors compared to _ exp _ , but _",
    "linear _ was able to produce at least 35% less _ rmse _ compared to _",
    "mixed_. therefore , we can answer @xmath88 by stating that the _ linear _ model is the most suitable and sufficient model that can generalize among different classes .    for our last question ,",
    "we tested the performance of the different models when trained on a bigger and more diverse dataset .",
    "table  [ tab : exp3 ] shows the results of our evaluation , where each model was trained on dbpedia english labels and tested on the the four evaluation datasets .",
    "the _ linear _ model error was 1 order of magnitude less than the _ rmse _ obtained by _ exp _ and 3 orders of magnitude less compared to the _ mixed _ error . in all four datasets , the",
    "model produced the highest _",
    "rmse_. for the villages dataset , the _ mixed _",
    "model s error was 1916 and 214 times higher compared to _ linear _ and _ exp _ resp .",
    "[ fig : exp ] and  [ fig : linear ] present the plans produces by heliosfor the ls ` minus(and(levenshtein(x.description , y.description)|0.5045,trigrams ( x.title , y.name)|0.4871)|0.2925,or(levenshtein(x.description,y.descri ption)|0.5045,trigrams(x.title , y.name)|0.4871)| 0.2925)>=0.2925 ` of the amazon - gp dataset , if the planner used the _ exp _ model and the _ linear _ or the _ mixed _ model resp .",
    "for the child ls ` and(levenshtein(x.description , y.descri ption)|0.5045,trigrams(x.title , y.name)|0.4871)|0.2925 ` , the _ linear _ and the _ mixed _ model chose to execute only ` trigrams(x.title , y.name)|0.4871 ) ` and use the other child as a filter .",
    "moreover , the plan retrieved by using the _ exp _ model for runtime approximations aims to execute both children lss , which results into an overhead in the execution of the ls .",
    "it is obvious that the _ linear _ model achieved by far the lowest _ rmse _ on average compared to the other two models , which concludes the answer to @xmath89 .",
    "( filter)[]@xmath93 ; ( minus)[right = of filter , xshift=0.7cm]@xmath94 edge [ - > ] ( filter ) ; ( filter1)[above = of minus.west,xshift=2.6cm ] @xmath93 edge [ - > ] ( minus ) ; ( intersection)[right = of filter1,xshift=0.7cm , yshift=0.8cm]@xmath52 edge [ - > ] ( filter1 ) ; ( label1)[above = of intersection.west,xshift=3.8cm]@xmath95 edge [ - > ] ( intersection ) ; ( label2)[below = of intersection.west,xshift=3.8cm]@xmath96 edge [ - > ] ( intersection ) ; ( filter2)[below = of minus.east,xshift=1.9cm ] @xmath93 edge [ - > ] ( minus ) ; ( union)[right = of filter2,xshift=0.7cm , yshift=-0.8cm]@xmath38 edge [ - > ] ( filter2 ) ; ( label3)[above = of union.west,xshift=3.8cm]@xmath95 edge [ - > ] ( union ) ; ( label4)[below = of union.west,xshift=3.8cm]@xmath96 edge [ - > ] ( union ) ;    ( filter0)[]@xmath93 ; ( minus)[below = of filter0,]@xmath94 edge [ - > ] ( filter ) ;    ( filter)[below = of minus]@xmath93 edge [ - > ] ( minus ) ;    ( filterintersection)[below = of filter]@xmath96 edge [ - > ] ( filter ) ; ( filter1)[below = of filterintersection ] @xmath95 edge [ - > ] ( filterintersection ) ;    ( filter2)[right = of minus.east ] @xmath93 edge [ - > ] ( minus ) ; ( union)[right = of filter2,xshift=0.7cm]@xmath38 edge [ - > ] ( filter2 ) ; ( label3)[above = of union.west,xshift=0.3cm]@xmath95 edge [ - > ] ( union ) ; ( label4)[below = of union.west,xshift=1.6cm,yshift=0.3cm]@xmath96 edge [ - > ] ( union ) ;",
    "the task of efficient query execution in database systems is similar to the task of execution optimization using runtime approximations in ld frameworks .",
    "efficient and scalable data management has been of central importance in database systems @xcite . over the past few years",
    ", there has been an extensive work on query optimization in databases that is based on statistical information about relations and intermediate results @xcite . the author of @xcite",
    "gives an analytic overview regarding the procedure of query optimization and the different approaches used at each step of the process .",
    "a novel approach in this field was presented by @xcite , in which the proposed approach introduced the concept of parametric query optimization . in this work",
    ", the authors provided the necessary formalization of the aforementioned concept and conducted a set of experiments using the buffer size as parameter . in order to minimize the total cost of generating all possible alternative execution plans",
    ", they used a set of randomized algorithms . on a similar manner ,",
    "the authors of @xcite introduced the idea of multi - objective parametric query optimization ( mpq ) , where the cost of plan is associated with multiple cost functions and each cost function is associated with various parameters .",
    "their experimental results showed however that the mpq method performs an exhaustive search of the solution space which addresses this approach computationally inefficient .",
    "another set of approaches in the field of query optimization have focused on creating dynamic execution plans .",
    "dynamic planning is based on the idea that the execution engine of a framework knows more than the planner itself .",
    "therefore , information generated by the execution engine is used to re - evaluate the plans generated by the optimizer .",
    "there has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays @xcite , dynamic planning in compile - time @xcite , adaptive query operators @xcite and re - ordering of operators @xcite .",
    "moreover , the problem addressed in this work focus on identifying scalable and time - efficient solutions towards ld .",
    "a large number of frameworks were developed to assist this issue , such as silk @xcite , limes@xcite , knofuss @xcite and zhishi.links @xcite .",
    "silk and knofuss implement blocking approaches in order to achieve efficient linking between resources .",
    "silk framework incorporates a rough index pre - match , whereas knofuss blocking technique is highly influenced from databases systems techniques . to this end , the only ld framework that provides both theoretical and practical guarantees towards scalable and accurate ld is limes .",
    "as we mentioned throughout this work , limesexecution strategy incorporates the heliosplanner  @xcite which is ( to the best of our knowledge ) the first execution optimizer in ld .",
    "heliosis able to provide accurate runtime approximations , which we have extended in this work , and is able to find the least costly execution plan for a ls , consuming a minute portion of the overall execution runtime .",
    "in this paper , we studied approximation functions that allow predicting the runtime of link specifications .",
    "we showed that on average , linear models are indeed the approach to chose to this end as they seem to overfit the least .",
    "still , mixed models also perform in a satisfactory manner .",
    "exponential models either fit very well or not at all and are thus not to be used . in future work , we will study further models for the evaluation of runtime and improve upon existing planning mechanisms for the declarative ld . in particular , we will consider other features when approximation runtimes , e.g. , the distribution of characters in the strings to compare ."
  ],
  "abstract_text": [
    "<S> time - efficient link discovery is of central importance to implement the vision of the semantic web . </S>",
    "<S> some of the most rapid link discovery approaches rely internally on planning to execute link specifications . in newer works , </S>",
    "<S> linear models have been used to estimate the runtime the fastest planners . </S>",
    "<S> however , no other category of models has been studied for this purpose so far . in this paper </S>",
    "<S> , we study non - linear runtime estimation functions for runtime estimation . </S>",
    "<S> in particular , we study exponential and mixed models for the estimation of the runtimes of planners . to this end </S>",
    "<S> , we evaluate three different models for runtime on six datasets using 400 link specifications . </S>",
    "<S> we show that exponential and mixed models achieve better fits when trained but are only to be preferred in some cases . </S>",
    "<S> our evaluation also shows that the use of better runtime approximation models has a positive impact on the overall execution of link specifications . </S>"
  ]
}