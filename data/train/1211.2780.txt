{
  "article_text": [
    "functional data analysis is a branch of statistics that has been studied frequently and developed over recent years .",
    "this type of data appears in many practical situations such as continuous phenomena .",
    "thus , the possible application fields that favor the use of functional data are broad and include the following : climatology , economics , linguistics , medicine , and so on .",
    "because the pioneering work of authors such as @xcite and @xcite , many developments have been investigated to build theories and methods regarding functional data ; for instance , how can the mean or the variance of functional data be defined ?",
    "what type of model can be considered with functional data ?",
    "these papers also emphasize the drawback of merely using multivariate methods with this type of data ; rather , they suggest considering these data as objects that belong to a specific functional space . the monographs of @xcite provide and overview of both the theoretical and practical aspects of functional data analysis .",
    "regression is one of the most studied functional models . in this model ,",
    "the variable of interest @xmath0 is real and the covariate @xmath1 belongs to a functional space @xmath2 endowed with a semi - norm @xmath3 .",
    "thus , the regression model is written @xmath4    where @xmath5 is an operator and @xmath6 is the random error term .",
    "much research has investigated this model when the operator @xmath7 is supposed to be linear , which has contributed to the popularity of the so - called functional linear model . in this linear context , the operator @xmath7 is written as @xmath8 where @xmath9 denotes the inner product of the space @xmath2 and @xmath10 belongs to @xmath2 .",
    "thus , the goal is to estimate the unknown function @xmath10 .",
    "we refer the reader to @xcite and @xcite for different methods of estimating @xmath10 .",
    "another way to approach the ( [ regression ] ) model is to think in a nonparametric way .",
    "many authors have also investigated this direction .",
    "recent advances on the topic have been the subject of a bibliographical review in @xcite and monographs by @xcite , @xcite , thereby providing the theoretical and practical properties of a kernel estimator of the operator @xmath7 .",
    "specifically , if @xmath11 is a sample of independent and identically distributed couples with the same law as @xmath12 , then , this kernel estimator is defined for all @xmath13 by    @xmath14    where @xmath15 is a kernel and @xmath16 is a bandwidth . @xcite",
    "considered the asymptotic normality of in the dependent case , while @xcite obtained almost sure convergence .",
    "this nonparametric regression estimator raises several problems because the choice of the semi - norm @xmath17 of the space @xmath2 and the choice of the bandwidth ,  . with regard to bandwidth , many solutions have been considered when the covariate is real ( e.g. , cross validation ) .",
    "recently , @xcite studied an estimator using a sequence of bandwidths in the multivariate setting that allowed the computation of this estimator in a recursive way thereby generalizing previous research ( @xcite and @xcite ) .",
    "this estimator has acceptable theoretical properties ( from the point of view of the mean square error and almost sure convergence ) .",
    "it is also of practical interest : for instance , it presents a computational time gain when researchers want to predict new values of the variable of interest when new observations appear .",
    "this case is not true for the basic kernel estimator , which must be computed again using the whole sample .",
    "the purpose of the current study is to adapt the recursive estimator studied in @xcite to a case in which the covariate is functional .",
    "the remainder of the paper is organized as follows .",
    "section 2 defines the recursive estimator of the operator @xmath7 when the covariate @xmath1 is functional and presents the asymptotic properties of this estimator .",
    "section 3 evaluates the performance of our estimator using a simulation study and a real dataset .",
    "finally , the proofs of the theoretical results are presented in section 4 .",
    "let @xmath18 be a pair of random variables defined in @xmath19 with values on @xmath20 where @xmath21 is a banach space endowed with a semi - norm @xmath3 .",
    "assume that @xmath22 is a sample of @xmath23 random variables independent and identically distributed , having the same distribution as @xmath18 .",
    "the model is then rewritten as @xmath24 where for any @xmath25 , @xmath26 is a random variable such that @xmath27 and @xmath28 + nonparametric regression aims to estimate the functional @xmath29 for @xmath30 . to this end , let us consider the family of recursive estimators indexed by a parameter @xmath31,$ ] and defined by @xmath32}({\\chi } ) : = \\frac{\\sum\\limits_{i=1}^n\\frac{y_i}{f(h_i)^{\\ell}}k\\left ( \\frac{\\|{\\chi}-{\\cal x}_i\\|}{h_i}\\right ) } { \\sum\\limits_{i=1}^n\\frac{1}{f(h_i)^{\\ell}}k\\left ( \\frac{\\|{\\chi}-{\\cal x}_i\\|}{h_i}\\right)},\\ ] ] where @xmath15 is a kernel , @xmath33 a sequence of bandwidths and @xmath34 the cumulative distribution function of the random variable @xmath35 .",
    "our family of estimators is a recursive modification of the estimate defined in and can be computed recursively by @xmath36}(\\chi)=\\frac{\\left[\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right]\\varphi_n^{[\\ell]}(\\chi)+\\left[\\sum\\limits_{i=1}^{n+1}f(h_i)^{1-\\ell}\\right]y_{n+1}k_{n+1}^{[\\ell]}\\left(\\|\\chi-{\\cal x}_{n+1 } \\|\\right)}{\\left[\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right]f_n^{[\\ell]}(\\chi)+\\left[\\sum\\limits_{i=1}^{n+1}f(h_i)^{1-\\ell}\\right]k_{n+1}^{[\\ell]}\\left(\\|\\chi-{\\cal x}_{n+1}\\| \\right)},\\ ] ] with    @xmath37}({\\chi})\\label{phin}=\\frac{\\sum\\limits_{i=1}^n\\frac{y_i}{f(h_i)^{\\ell}}k\\left ( \\frac{\\|{\\chi}-{\\cal x}_i\\|}{h_i}\\right)}{\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell } } , ~ f_n^{[\\ell]}({\\chi})\\label{fn}=\\frac{\\sum\\limits_{i=1}^n\\frac{1}{f(h_i)^{\\ell}}k\\left ( \\frac{\\|{\\chi}-{\\cal x}_i\\|}{h_i}\\right)}{\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}},\\end{aligned}\\ ] ]    and @xmath38}(\\cdot):=\\frac{1}{f(h_i)^{\\ell}\\sum\\limits_{j=1}^if(h_j)^{1-\\ell}}k\\left(\\frac{\\cdot}{h_i}\\right)$ ] . + more precisely , @xmath39}({\\chi})$ ] is the adaption to the functional model of the finite - dimensional recursive family of estimators introduced by @xcite , which includes the well - known recursive ( @xmath40 ) and semi recursive ( @xmath41 ) estimators .",
    "the recursive property of this estimator class is clearly useful in sequential investigations and for large sample sizes because the addition of a new observation means that the non - recursive estimators must be recomputed .",
    "in addition , we need to store extensive data to calculate them . + we assume that the following assumptions hold :    h1 : :    the operators @xmath42 and    @xmath43 are continuous on a neighborhood    of @xmath44 , and @xmath45",
    ". moreover , the    function    @xmath46 $ ]    is assumed to be derivable at @xmath47 .",
    "h2 : :    @xmath15 is nonnegative bounded kernel with support on the    compact @xmath48 $ ] such that    @xmath49}k(t)>0 $ ] .",
    "h3 : :    for any    @xmath50 , \\tau_h(s):=\\frac{f(hs)}{f(h)}\\rightarrow\\tau_0(s)<\\infty$ ]    as @xmath51 .",
    "h4 : :    ( i ) ; ;      @xmath52 and      @xmath53^{1-\\ell}\\rightarrow\\alpha_{[\\ell]}<\\infty$ ]      as @xmath54 .",
    "( ii ) ; ;      @xmath55 ,      @xmath56^r\\rightarrow\\beta_{[r]}<\\infty,$ ]      as @xmath54 .",
    "assumptions @xmath57 , @xmath58 and the first part of @xmath59 are common in nonparametric regression .",
    "they have been used by @xcite and are the same as those classically used in finite - dimensional settings .",
    "the conditions @xmath60}<\\infty$ ] and @xmath61 are particular to the recursive problem and the same as those used in finite - dimensional cases .",
    "note that @xmath34 plays a crucial role in our calculus : its limit at zero , and a fixed @xmath62 is known as ` small ball probability ' . before announcing our results , let us provide typical examples of bandwidths and small ball probabilities that satisfy @xmath63 and @xmath59 ( see @xcite for more details ) .",
    "+ if @xmath64 is a fractal ( or geometric ) process , then the small ball probabilities are of the form @xmath65 where @xmath66 and @xmath67 are positive constants , and @xmath3 might be a supremum norm , an @xmath68 norm or a besov norm .",
    "the choice of bandwidth @xmath69 with @xmath70 implies @xmath71 , @xmath72 .",
    "thus , @xmath63 and @xmath59 hold when @xmath73 .",
    "in fact , assumption @xmath63 and the first part of @xmath59 are clearly non - restrictive because they are the same as those used in the non - recursive case . with regard to @xmath74 ,",
    "if @xmath75 , then @xmath76 .",
    "thus , the condition is satisfied when @xmath77}=\\frac{1}{1-\\delta\\kappa r}$ ] . the same argument is also valid for @xmath78 , if @xmath79 .",
    "let us introduce the following notations from @xcite : @xmath80 we can establish the asymptotic mean square error of our recursive estimate .    [ bias_var ] under the assumptions @xmath81 , we have @xmath82}({\\chi})\\right]-r(\\chi)&=&\\varphi'(0)\\frac{\\alpha_{[\\ell]}}{\\beta_{[1-\\ell]}}\\frac{m_0}{m_1}h_n[1+o(1)]+o\\left[\\frac{1}{nf(h_n)}\\right ] , \\\\",
    "var\\left[r_n^{[\\ell]}({\\chi})\\right]&= & \\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\frac{m_2}{m_1 ^ 2}\\sigma_\\varepsilon^2(\\chi)\\frac{1}{nf(h_n)}\\left[1+o(1)\\right].\\end{aligned}\\ ] ]    theorem [ bias_var ] is an extension of @xcite result to the class of recursive estimators . using a bias - variance representation and an additional condition , the asymptotic mean square error of our estimators",
    "is established in the following result :    [ mse ] assume that the assumptions of theorem [ bias_var ] hold .",
    "if there exists a constant @xmath83 such that @xmath84 as @xmath85 then @xmath86}({\\chi})-r(\\chi)\\right)^2\\right]&=&\\left[\\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\frac{m_2\\sigma_\\varepsilon^2(\\chi)}{m_1 ^ 2}+\\frac{c\\alpha_{[\\ell]}^2}{\\beta_{[1-\\ell]}^2}\\frac{\\varphi'(0)^2m_0 ^ 2}{m_1 ^ 2}\\right].\\end{aligned}\\ ] ] in particular , if @xmath87 is fractal ( or geometric process ) with @xmath88 then the choice @xmath89 , @xmath90 , implies that @xmath91}({\\chi})-r(\\chi)\\right)^2\\right]&=&\\left[\\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\frac{m_2\\sigma_\\varepsilon^2(\\chi)}{c'_\\chi a^\\kappa m_1 ^ 2}+\\frac{\\alpha_{[\\ell]}^2}{\\beta_{[1-\\ell]}^2}\\frac{\\varphi'(0)^2m_0 ^ 2a^2}{m_1 ^ 2}\\right].\\end{aligned}\\ ] ]    @xcite established a similar result for the nadaraya - watson estimator using the finite - dimensional setting and continuous time processes .",
    "+ to obtain the almost sure convergence rate of our estimator , we assume that the following additional assumptions hold .",
    "h5 : :    there exist @xmath92 and @xmath93 such that    @xmath94<\\infty.$ ] h6 : :    @xmath95 .",
    "assumption @xmath96 is clearly met if @xmath0 is bounded , which implies that @xmath97 , \\forall   p \\geq1,n \\geq 2.\\end{aligned}\\ ] ] in fact , if we set @xmath98 , then one is able to write : @xmath99 because the function @xmath100 for all @xmath101 is concave down on set @xmath102 \\max\\{1,\\exp(\\frac{p}{\\mu } -1)\\ } , + \\infty[$ ] , jensen s inequality ( via assumption @xmath96 ) implies that @xmath103^{p/\\mu}\\\\    \\leq & \\left[\\ln \\sum\\limits_{i=1}^n{\\text{e}}\\exp\\left(\\lambda|y_i|^\\mu\\right)\\right]^{p/\\mu}=o[(\\ln n)^{p/\\mu } ] , \\end{aligned}\\ ] ] and ( [ moment - expo ] ) follows .",
    "an example of a sequence of random variables , @xmath104 satisfying @xmath96 ( and then ( [ moment - expo ] ) ) is the standard gaussian distribution , with @xmath105 and @xmath106 .",
    "@xcite used relation ( [ moment - expo ] ) in the multivariate framework to establish the optimal quadratic error of the nadaraya - watson estimator .",
    "assumption @xmath107 is satisfied when @xmath87 is a fractal or non - smooth , whereas condition @xmath108 is not necessary when @xmath109 .",
    "+ we can write the following theorem for our estimator of the regression operator .",
    "[ cv_ps_rnl ] assume that @xmath110 hold .",
    "if @xmath111 , then @xmath112^{1/2}\\left[r_n^{[\\ell]}(\\chi)-r(\\chi)\\right]=\\frac{\\left[2\\beta_{[1 - 2\\ell]}\\sigma^2_\\varepsilon(\\chi)m_2\\right]^{1/2}}{\\beta_{[1-\\ell]}m_1 } ~ a.s.\\ ] ]    the choices of bandwidths and small ball probabilities previously provided are typical examples that satisfy the condition @xmath113 case @xmath41 of theorem [ cv_ps_rnl ] is an extension to the functional setting of @xcite result concerning the almost sure convergence of devroye - wagner s estimator .",
    "note , that in the non - recursive framework , the rate of convergence obtained is of the form @xmath114^{1/2}$ ] ( see lemma 6.3 in @xcite ) . unlike the non - recursive case",
    ", the rate of convergence of the recursive estimators are obtained with exact upper bounds . + to obtain asymptotic normality , we make the following additional assumption , which was clearly verified by the choices of bandwidths and the small ball probabilities above .",
    "h7 : :    for any @xmath115 ,    @xmath116 .",
    "[ normality ] assume that @xmath117 and @xmath118 hold .",
    "if there exists @xmath119 such that @xmath120 , then @xmath121}(\\chi)-r(\\chi)\\right)\\stackrel{\\mathcal{d}}{\\rightarrow}\\mathcal{n}\\left(c\\frac{\\alpha_{[\\ell]}}{\\beta_{[1-\\ell]}}\\frac{m_0}{m_1}\\varphi'(0),\\ \\ \\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\frac{m_2}{m_1 ^ 2}\\sigma_\\varepsilon^2(\\chi)\\right).\\end{aligned}\\ ] ]    note that the choices of bandwidths and small ball probabilities above imply that @xmath122}}/{\\beta_{[1-\\ell]}^2}<1.$ ] thus , the recursive estimators are more efficient than classical estimators in this case in the sense that their asymptotic variance is small for a given @xmath123 and @xmath124 .",
    "+ because the result of theorem [ normality ] depends on unknown quantities , we derived a usable asymptotic distribution for the following corollary of case @xmath40 .",
    "[ normality2 ] assume that @xmath117 and @xmath118 hold . if @xmath125 and @xmath126 , then for any consistent estimators @xmath127 and @xmath128 of @xmath129 and @xmath130 respectively , we have @xmath131}\\widehat{m_1}^2}{\\widehat{m}_2\\widehat\\sigma_\\varepsilon^2(\\chi)}}\\left(r_n^{[0]}(\\chi)-r(\\chi)\\right)\\stackrel{\\mathcal{d}}{\\rightarrow}\\mathcal{n}\\left(0,1\\right),\\end{aligned}\\ ] ] where @xmath132 and @xmath133}$ ] are the empirical counterparts of @xmath34 and @xmath134}$ ] defined by @xmath135}=\\displaystyle\\frac{1}{n}\\sum\\limits_{i=1}^n\\frac{\\widehat f(h_i)}{\\widehat f(h_n)}.\\end{aligned}\\ ] ]    corollary [ normality2 ] is similar to the result obtained by @xcite in the non - recursive case .",
    "the assumptions used to establish this result are fulfilled by the choices of bandwidths and small ball probabilities above .",
    "if we consider the uniform kernel @xmath136}(\\cdot)$ ] , then the asymptotic distribution in corollary [ normality2 ] can be rewritten as @xmath137}}{\\widehat\\sigma_\\varepsilon^2}}\\left(r_n^{[0]}(\\chi)-r(\\chi)\\right)\\stackrel{\\mathcal{d}}{\\rightarrow}\\mathcal{n}\\left(0,1\\right).\\end{aligned}\\ ] ] according to corollary [ normality2 ] , the asymptotic confidence band of @xmath138 with level @xmath139 is given by @xmath140}(\\chi ) - z_{1-\\alpha/2 } \\sqrt{n\\widehat{f}(h_n)}\\sqrt{\\frac{\\widehat\\beta_{[1]}\\widehat{m_1}^2}{\\widehat{m}_2\\widehat\\sigma_\\varepsilon^2(\\chi ) } } ; r_n^{[0]}(\\chi ) + z_{1-\\alpha/2 } \\sqrt{n\\widehat{f}(h_n)}\\sqrt{\\frac{\\widehat\\beta_{[1]}\\widehat{m_1}^2}{\\widehat{m}_2\\widehat\\sigma_\\varepsilon^2(\\chi ) } } \\right],\\ ] ] where @xmath141 is the quantile of order @xmath142 of the standard normal distribution .",
    "to observe the behavior of our recursive estimator in practice , this section considers a simulation study .",
    "we simulated our data in the following way : the curves @xmath143 , and @xmath144 are standard brownian motions on @xmath48 $ ] , with @xmath145 .",
    "each curve is discretized into @xmath146 equidistant points on @xmath48 $ ] .",
    "the operator @xmath7 is defined by @xmath147 .",
    "the error @xmath6 is simulated as a gaussian random variable with mean @xmath148 and standard deviation @xmath149 .",
    "the simulations were repeated @xmath150 times to compute the prediction errors for a new curve @xmath151 , which was also simulated as a standard brownian motion on @xmath48 $ ] .",
    "+ in this functional context , the estimator depends on the choice of many parameters : the semi - norm @xmath17 of the functional space @xmath2 , the sequence of bandwidths @xmath152 , the kernel @xmath15 , the parameter @xmath153 and the distribution function @xmath34 in case @xmath154 . because the choice of kernel @xmath15 is not crucial",
    ", we used the quadratic kernel defined by @xmath155 } ( u)$ ] for all @xmath156 , which behaves correctly in practice and is easy to implement .",
    "we estimated the distribution function @xmath157 using the empirical distribution function , which is uniformly convergent .      in this simulation ,",
    "the semi - norm was based on the principal components analysis of the curves that retained @xmath158 principal components ( see@xcite for a description of this semi - norm ) , whereas @xmath153 is fixed and equal to @xmath148 .",
    "we show below that this parameter @xmath153 has a negligible influence on the behavior of the estimator . + we chose to take a sequence of bandwidths @xmath159 , @xmath160 with @xmath161 and @xmath162 .",
    "+ at the same time , we also computed the estimator introduced by @xcite .",
    "following @xcite , we introduced an automatic selection of the bandwidth with a cross validation procedure .",
    "we used this procedure for the estimator of @xcite . for the recursive estimator",
    ", we denoted @xmath163 with @xmath161 and @xmath164 , and we considered the cross validation criterion @xmath165,[-i ] } ( \\mathcal{x}_{i } ) \\right)^{2},\\ ] ] where @xmath166,[-i]}$ ] represents the recursive estimator of @xmath7 using the @xmath167-sample that corresponds to the initial sample without the @xmath168 observation @xmath169 for @xmath170 .",
    "then , we selected the values @xmath171 , @xmath172 of @xmath173 and @xmath174 that minimized @xmath175 .",
    "our estimator was @xmath166}$ ] using these selected values of @xmath173 and @xmath174 .",
    "+ table 1 presents the mean and standard deviations of the prediction error over @xmath150 simulations for the optimal values of @xmath173 and @xmath174 with respect to the @xmath176 criterion ( these optimal values were @xmath177 and @xmath178 for our estimator ) .",
    "specifically , denoting @xmath179 } = r_{n}^{[\\ell],[j ] } ( \\chi^{[j]})$ ] the predicted value at the @xmath180 iteration of the simulation ( @xmath181 ) for a new curve @xmath182}$ ] , we provided the mean ( @xmath183 ) and the standard deviations of the quantities @xmath184 } - y^{[j ] } \\right)^{2}$ ] .",
    "the errors were computed for our estimator ( label ( 1 ) in the table ) and the estimator from @xcite ( label ( 2 ) in the table ) , both of which were adapted using the @xcite procedure . from these results , we observed that the estimator from @xcite is slightly better than our estimator for the @xmath183 criterion . as we will observe in ( see subsection [ computime ] )",
    ", the advantage of our estimator is its computational time .",
    "this section also examines the behavior of the prediction errors when the sample size increases .",
    "we used @xmath185 , @xmath186 and @xmath187 ; as expected , the errors decreased when the sample size increased .",
    ".mean and standard deviation of the square prediction error computed via @xmath150 simulations for different values of @xmath23 with the optimal values of bandwidth provided by @xmath171 and @xmath172 [ cols=\"^,^,^,^\",options=\"header \" , ]     o true and predicted temperature curves for 2011 .",
    "the solid line denotes the true curve .",
    "the dashed lines represent the predicted curve and the @xmath188 confidence intervals with the recursive estimator.,width=453,height=302 ]      this subsection highlights the performances of our estimator with regard to computational time gain .",
    "the dataset we used consisted of information collected by oramip , a french organization that studies air quality .",
    "we disposed of a sample of @xmath189 daily ozone pollution measurements curves ( in @xmath190g / m@xmath191 ) .",
    "the variable @xmath192 of interest was the daily maximum of ozone .",
    "the ozone curve the day before ( from @xmath193 pm to @xmath194 pm the day after ) was used as a functional explicative variable @xmath195 .",
    "specifically , each @xmath196 was observed at p = 24 equidistant points that corresponded to hourly measurements .",
    "the sample was divided into a learning sample of @xmath197 and a test sample of @xmath198 .",
    "the ozone curve from the learning sample is plotted in figure 4 .",
    "every value of the test sample was predicted using our method and the method from @xcite .",
    "the learning sample and the curves of the test sample were predicted as if they had arrived in real time .",
    "again , the estimator from @xcite showed its advantage over our estimator with regard to mean prediction error ( @xmath199 vs. @xmath200 ) .",
    "however , our estimator performed best when predicting several values and using recursivity , especially when many values need to be predicted ( which was the case here , given that we predicted @xmath198 values ) . the computational time for our estimator ( in seconds ) was @xmath201 , whereas it was @xmath202 for the ferraty and vieu method ; however , we are conscious that this advantage was relative in this case because we dealt with daily measurements . in any case , the choice is up to the user : a better prediction error or a faster computational time .",
    "throughout the proofs , we denote @xmath203 a sequence of real numbers going to zero as @xmath204 goes to @xmath205 . the kernel estimate @xmath39}$ ]",
    "writes @xmath206}(\\chi)&=&\\frac{\\varphi_n^{[\\ell]}({\\chi})}{f_n^{[\\ell]}({\\chi})},\\end{aligned}\\ ] ] where @xmath207}$ ] and @xmath208}$ ] are defined in ( [ phin ] ) .      to prove the first assertion of theorem [ bias_var ] , let us use the following decomposition @xmath209}({\\chi})\\right]&=&\\frac{\\text{e}\\left[\\varphi_n^{[\\ell]}({\\chi})\\right]}{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]}-\\frac{\\text{e}\\left\\{\\varphi_n^{[\\ell]}({\\chi})\\left[f_n^{[\\ell]}({\\chi})-\\text{e}f_n^{[\\ell]}({\\chi})\\right]\\right\\}}{\\left\\{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]\\right\\}^2}\\\\ & & + \\frac{\\text{e}\\left\\{r_n^{[\\ell]}({\\chi})\\left[f_n^{[\\ell]}({\\chi})-\\text{e}f_n^{[\\ell]}({\\chi})\\right]^2\\right\\}}{\\left\\{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]\\right\\}^2}.\\end{aligned}\\ ] ]",
    "the first part of theorem [ bias_var ] is then a direct consequence of lemmas [ biais1]-[var ] below .",
    "[ biais1 ] under assumptions * h1 * -*h4 * , we have @xmath210}({\\chi})\\right]}{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]}-r(\\chi)=h_n\\varphi'(0)\\frac{\\alpha_{[\\ell]}}{\\beta_{[1-\\ell]}}\\frac{m_0}{m_1}\\left[1+o(1)\\right].\\end{aligned}\\ ] ]    [ termesa ] under assumptions * h1 * -*h4 * , we have @xmath211}({\\chi})\\left[f_n^{[\\ell]}({\\chi})-\\text{e}f_n^{[\\ell]}({\\chi})\\right]\\right\\}&= & o\\left[\\frac{1}{nf(h_n)}\\right],\\\\ \\text{e}\\left\\{r_n^{[\\ell]}({\\chi})\\left[f_n^{[\\ell]}({\\chi})-\\text{e}f_n^{[\\ell]}({\\chi})\\right]^2\\right\\}&=&o\\left[\\frac{1}{nf(h_n)}\\right].\\end{aligned}\\ ] ]    [ biais2 ] under assumptions * h1 * -*h4 * , we have @xmath212}({\\chi})\\right)=m_1\\left[1+o(1)\\right ] & \\text { and } & \\text{e}\\left(\\varphi_n^{[\\ell]}({\\chi})\\right)=r(\\chi)m_1\\left[1+o(1)\\right].\\end{aligned}\\ ] ]    the study of the variance term in theorem [ bias_var ] is based on the following decomposition which can be found in @xcite .",
    "@xmath213}(\\chi)\\right]&= & \\frac{\\text{var}\\left[\\varphi_n^{[\\ell]}({\\chi})\\right]}{\\left\\{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]\\right\\}^2}-4\\frac{{\\text{e}}\\left[\\varphi_n^{[\\ell]}({\\chi})\\right]\\text{cov}\\left[f_n^{[\\ell]}({\\chi}),\\varphi_n^{[\\ell]}({\\chi})\\right]}{\\left\\{{\\text{e}}\\left[f_n^{[\\ell]}({\\chi})\\right]\\right\\}^3 } \\nonumber\\\\\\mbox { } & & + 3\\text{var}\\left[f_n^{[\\ell]}({\\chi})\\right]\\frac{\\left\\{{\\text{e}}\\left[\\varphi_n^{[\\ell]}({\\chi})\\right]\\right\\}^2}{\\left\\{{\\text{e}}\\left[f_n^{[\\ell]}({\\chi})\\right]\\right\\}^4}+o\\left[\\frac{1}{nf(h_n)}\\right].\\end{aligned}\\ ] ] the second assertion of theorem [ bias_var ] follows from and lemma [ var ] below .",
    "@xmath214    [ var ] under assumptions * h1 * -*h4 * , we have @xmath215}({\\chi})\\right]&=&\\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}m_2\\frac{1}{nf(h_n)}\\left[1+o(1)\\right].\\\\ \\text{var}\\left[\\varphi_n^{[\\ell]}({\\chi})\\right]&=&\\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\left[r^2(\\chi)+\\sigma_\\epsilon^2(\\chi)\\right]m_2\\frac{1}{nf(h_n)}\\left[1+o(1)\\right].\\\\ \\text{cov}\\left[f_n^{[\\ell]}({\\chi}),\\varphi_n^{[\\ell]}({\\chi})\\right]&=&\\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}r(\\chi)m_2\\frac{1}{nf(h_n)}\\left[1+o(1)\\right].\\end{aligned}\\ ] ]        observe that @xmath210}({\\chi})\\right]}{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]}-r(\\chi)&=&\\dfrac{\\sum\\limits_{i=1}^n\\frac{1}{f(h_i)^\\ell}\\text{e}\\left[\\left(y_i - r(\\chi)\\right)k\\left(\\frac{\\|\\chi-{\\cal x}_i\\|}{h_i}\\right)\\right]}{\\sum\\limits_{i=1}^n\\frac{1}{f(h_i)^\\ell}\\text{e}\\left[k\\left(\\frac{\\|\\chi-{\\cal x}_i\\|}{h_i}\\right)\\right]}.\\end{aligned}\\ ] ] obviously @xmath216&=&{\\text{e}}\\left[\\left(r({{\\cal x}})-r(\\chi)\\right)k\\left(\\frac{\\|{{\\cal x}}-\\chi\\|}{h_i}\\right)\\right]\\\\&= & { \\text{e}}\\left[\\varphi\\left(\\|{{\\cal x}}-\\chi\\|\\right)k\\left(\\frac{\\|{{\\cal x}}-\\chi\\|}{h_i}\\right)\\right]\\\\&= & \\int_0 ^ 1\\varphi(h_it)k(t)d{\\text{p}}^{\\|{{\\cal x}}-\\chi\\|/h_i}(t).\\end{aligned}\\ ] ] therefore , a taylor s expansion of @xmath217 around 0 ensures that @xmath216&=&h_i\\varphi'(0)\\int_0 ^ 1 tk(t)d{\\text{p}}^{\\|{{\\cal x}}-\\chi\\|/h_i}(t)+o(h_i).\\end{aligned}\\ ] ] from the proof of lemma 2 in @xcite , * h2 * and fubini s theorem imply that @xmath218,\\end{aligned}\\ ] ] and @xmath219.\\end{aligned}\\ ] ] also , combining and , we have @xmath210}({\\chi})\\right]}{\\text{e}\\left[f_n^{[\\ell]}({\\chi})\\right]}-r(\\chi)&=&\\frac{\\sum\\limits_{i=1}^nh_if(h_i)^{1-\\ell}\\left\\{\\varphi'(0)\\left[k(1)-\\int_0 ^ 1(sk(s))'\\tau_{h_i}(s)ds\\right]+\\gamma_i\\right\\}}{\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\left[k(1)-\\int_0 ^ 1k'(s)\\tau_{h_i}(s)ds\\right]}\\\\&:=&\\frac{d_1}{d_2}.\\end{aligned}\\ ] ] finally , by virtue of * h3 * we get from toeplitz s lemma ( see @xcite ) that @xmath220}\\varphi'(0 ) m_0[1+o(1 ) ] , \\ \\",
    "\\frac{d_2}{n{f(h_n)}^{1-\\ell}}=\\beta_{[1-\\ell]}m_1[1+o(1)],\\end{aligned}\\ ] ] and lemma [ biais1 ] follows .",
    "@xmath214      equality allows to write @xmath221}({\\chi})\\right]&=&\\frac{1}{\\sum\\limits_{i=1}^n{f(h_i)}^{1-\\ell}}\\sum\\limits_{i=1}^n\\frac{1}{f(h_i)^\\ell}{\\text{e}}\\left[k\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right]\\\\&= &   \\frac{\\sum\\limits_{i=1}^n \\frac{f(h_i)^{1-\\ell}}{nf(h_n)^{1-\\ell}}\\left[k(1)-\\int_0 ^ 1k'(s)\\tau_{h_i}(s)ds\\right]}{\\displaystyle b_{n,1-\\ell}}=m_1[1+o(1)],\\end{aligned}\\ ] ] where the last equality follows from assumptions * h3 * , * h4 * and toeplitz s lemma .",
    "now , conditioning on @xmath87 , we have @xmath222&=&{\\text{e}}\\left\\{\\left[r({{\\cal x}})-r(\\chi)+r(\\chi)\\right]k\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right\\}=:a_i+b_i,\\end{aligned}\\ ] ] where @xmath223k\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right\\ } \\leq \\sup_{\\chi'\\in \\mathcal{b}(\\chi , h_i)}\\left|r(\\chi')-r(\\chi)\\right| { \\text{e}}k\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right),\\ ] ] and @xmath224 since @xmath7 is continuous ( * h1 * ) , then @xmath225= \\left[r(\\chi)+\\gamma_i\\right]{\\text{e}}k\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)=f(h_i)m_1\\left[r(\\chi)+\\gamma_i\\right].\\end{aligned}\\ ] ] we deduce from , with the help of assumptions * h3 * and * h4 * , and applying again toeplitz s lemma , that @xmath226}({\\chi})\\right]&= & \\frac{1}{\\sum\\limits_{i=1}^n{f(h_i)}^{1-\\ell}}\\sum_{i=1}^n\\frac{1}{f(h_i)^\\ell}{\\text{e}}\\left[y_ik\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right ] = r(\\chi)m_1\\left[1+o(1)\\right],\\end{aligned}\\ ] ] that proves lemma [ biais2 ] .",
    "@xmath214      first , notice that as in , we have @xmath227&=&f(h_i)\\left[k^2(1)-\\int_0 ^ 1(k^2)'(s)\\tau_{h_i}(s)ds\\right].\\end{aligned}\\ ] ] relation and assumption * h3 * ensure that @xmath228&=&o\\left[f(h_i)^2\\right],\\end{aligned}\\ ] ] thus @xmath229=m_2f(h_i)\\left[1+\\gamma_i\\right].\\end{aligned}\\ ] ] it follows that @xmath215}({\\chi})\\right]&= & \\frac{1}{\\left(\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right)^2}\\sum_{i=1}^nf(h_i)^{1 - 2\\ell}m_2\\left[1+\\gamma_i\\right]\\\\ & = & \\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\frac{1}{nf(h_n)}m_2\\left[1+o(1)\\right],\\end{aligned}\\ ] ] and the first step of lemma [ var ] follows . in a similar manner ,",
    "for the second step , let us write @xmath230}({\\chi})\\right]&=&\\frac{1}{\\left(\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right)^2}\\sum_{i=1}^nf(h_i)^{-2\\ell}\\text{var}\\left[y_ik\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right].\\end{aligned}\\ ] ] next , one obtains by conditioning on @xmath87 , @xmath231&= & { \\text{e}}\\left[r^2({{\\cal x}})k^2\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right]+{\\text{e}}\\left[\\sigma_\\varepsilon^2({{\\cal x}})k^2\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right].\\end{aligned}\\ ] ] assumption * h1 * and ensure that @xmath232&=&\\left[r^2(\\chi)+\\sigma_\\varepsilon^2(\\chi)\\right]{\\text{e}}\\left[k^2\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right]\\left[1+\\gamma_i\\right]\\\\&= & \\left[r^2(\\chi)+\\sigma_\\varepsilon^2(\\chi)\\right]m_2f(h_i)\\left[1+\\gamma_i\\right],\\end{aligned}\\ ] ] and then from toeplitz s lemma , with * h3 * and * h4 * , it follows that @xmath230}({\\chi})\\right]&=&\\frac{1}{\\left(\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right)^2}\\sum_{i=1}^nf(h_i)^{1 - 2\\ell}\\left[r^2(\\chi)+\\sigma_\\varepsilon^2(\\chi)\\right]m_2\\left[1+\\gamma_i\\right ] \\\\&=&\\frac{\\beta_{[1 - 2\\ell]}}{\\beta_{[1-\\ell]}^2}\\left[r^2(\\chi)+\\sigma_\\varepsilon^2(\\chi)\\right]m_2\\frac{1}{nf(h_n)}\\left[1+o(1)\\right],\\end{aligned}\\ ] ] which proves the second assertion of lemma [ var ] .",
    "the covariance term writes @xmath233}({\\chi}),\\varphi_n^{[\\ell]}({\\chi})\\right]=&\\frac{1}{\\left(\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right)^2}\\bigg\\{{\\text{e}}\\left[\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n\\frac{y_ik\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)k\\left(\\frac{\\|\\chi-{{\\cal x}}_j\\|}{h_j}\\right)}{f(h_i)^{\\ell}f(h_j)^{\\ell}}\\right]\\\\   -&\\sum\\limits_{i=1}^n\\frac{{\\text{e}}\\left[y_ik\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right]}{f(h_i)^{\\ell}}\\sum\\limits_{j=1}^n\\frac{{\\text{e}}k\\left(\\frac{\\|\\chi-{{\\cal x}}_j\\|}{h_j}\\right)}{f(h_j)^{\\ell}}\\bigg\\}\\\\ = & \\frac{1}{\\left(\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right)^2}\\sum\\limits_{i=1}^n\\frac{{\\text{e}}\\left[y_ik^2\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right]}{f(h_i)^{2\\ell}}\\\\   -&\\frac{1}{\\left(\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}\\right)^2}\\sum\\limits_{i=1}^n\\frac{{\\text{e}}\\left[y_ik\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)\\right]{\\text{e}}k\\left(\\frac{\\|\\chi-{{\\cal x}}_i\\|}{h_i}\\right)}{f(h_i)^{2\\ell}}:=i - ii . \\end{array}\\ ] ] notice that by and , one may write @xmath234=o\\left(\\frac{1}{n f(h_n)}\\right).\\end{aligned}\\ ] ] next , from assumption * h1 * and conditioning on @xmath87 , we have @xmath235= m_2f(h_i)\\left[r(\\chi)+\\gamma_i\\right].\\end{aligned}\\ ] ] it follows that @xmath236,\\end{aligned}\\ ] ] and the third assertion of lemma [ var ] follows again by applying toeplitz s lemma.@xmath214          let us consider the following decomposition @xmath237}(\\chi)-r(\\chi)=\\frac{\\tilde{\\varphi}_n^{[\\ell]}(\\chi)-r(\\chi)f_n^{[\\ell]}(\\chi)}{f_n^{[\\ell]}(\\chi)}+\\frac{\\varphi_n^{[\\ell]}(\\chi)- \\tilde{\\varphi}_n^{[\\ell]}(\\chi)}{f_n^{[\\ell]}(\\chi)},\\end{aligned}\\ ] ] where @xmath238}(\\chi)$ ] is a truncated version of @xmath207}(\\chi)$ ] defined by @xmath239}({\\chi})=\\frac{1}{\\sum\\limits_{i=1}^nf(h_i)^{1-\\ell}}\\sum\\limits_{i=1}^n\\dfrac{y_i}{f(h_i)^{\\ell}}\\mathds{1}_{\\left\\{\\left|y_i\\right|\\leq b_n\\right\\}}k\\left ( \\dfrac{\\|{\\chi}-{\\cal x}_i\\|}{h_i}\\right),\\ ] ] @xmath240 being a sequence of real numbers which goes to @xmath241 as @xmath242 next , for any @xmath243 , we have for the residual term of ( [ egalite1 ] ) @xmath244}(\\chi)-\\tilde{\\varphi}_n^{[\\ell]}(\\chi)\\right|>\\varepsilon\\left[\\frac { \\ln\\ln n } { nf(h_n)}\\right]^\\frac{1}{2}\\right\\ }   \\leq p\\left(\\bigcup\\limits_{i=1}^n\\left\\{|y_i|>b_n\\right\\}\\right )    \\leq\\text{e}\\left[e^{\\lambda|y|^\\mu   } \\right]n^{1-\\lambda\\delta},\\ ] ] where the last inequality follows by setting @xmath245 with the help of markov s inequality .",
    "assumption @xmath96 ensures that for any @xmath243 , @xmath246}(\\chi)-\\tilde{\\varphi}_n^{[\\ell]}(\\chi)\\right|>\\varepsilon\\left[\\frac { \\ln\\ln n } { nf(h_n)}\\right]^\\frac{1}{2}\\right\\}<\\infty \\text { if   $ \\delta>\\frac{2}{\\lambda}$ } , \\ ] ] which together with the borel - cantelli lemma imply that @xmath247^{1/2}\\left|\\varphi_n^{[\\ell]}(\\chi)-\\tilde{\\varphi}_n^{[\\ell]}(\\chi)\\right|\\rightarrow 0 \\text { a.s , } \\text { as   } n\\rightarrow\\infty .",
    "\\end{aligned}\\ ] ] the main term in writes @xmath248}(\\chi)-r(\\chi)f_n^{[\\ell]}(\\chi ) & = & \\left\\{\\tilde{\\varphi}_n^{[\\ell]}(\\chi)-r(\\chi)f_n^{[\\ell]}(\\chi)-\\text{e}\\left[\\tilde { \\varphi}_n^{[\\ell]}(\\chi)-r(\\chi)f_n^{[\\ell]}(\\chi)\\right]\\right\\}\\nonumber\\\\ * & & + \\left\\ { \\text{e}\\left[\\tilde{\\varphi}_n^{[\\ell]}(\\chi)-r(\\chi)f_n^{[\\ell]}(\\chi)\\right]\\right\\}:= n_1+n_2.\\end{aligned}\\ ] ] theorem [ cv_ps_rnl ] will be proved if lemmas [ lem1 ] and [ lem3 ] below are established . indeed ,",
    "from lemma [ biais2 ] we have @xmath249}({\\chi})\\right)=m_1\\left[1+o(1)\\right]$ ] and it can be shown ( following the same lines of the proof of lemma [ lem1 ] ) that @xmath250}(\\chi)-{\\text{e}}f_n^{[\\ell]}(\\chi)=o\\left(\\sqrt{\\frac{\\ln\\ln n}{nf(h_n)}}\\right ) \\text{a.s.}\\ ] ] @xmath214            let us set @xmath254 & \\text{and } & z_{n , i}= w_{n , i}-\\text{e}w_{n , i},\\end{aligned}\\ ] ] and define @xmath255 observe that @xmath256 ^ 2\\right)\\nonumber\\\\ \\mbox { } & & + \\text{e}\\left(k^2\\left(\\frac{\\|\\chi-{\\cal x}\\|}{h_i } \\right)y\\left [ 2r(\\chi)-y\\right]\\mathds{1}_{\\left\\lbrace \\mid y\\mid > b_n \\right\\rbrace}\\right)\\bigg\\}\\nonumber\\\\ \\mbox { } & & -\\sum\\limits_{i=1}^{n}f(h_i)^{-2\\ell}\\text{e}^2\\left(k\\left(\\frac{\\|\\chi-{\\cal x}\\|}{h_i } \\right)\\left [ y\\mathds{1}_{\\left\\{|y|\\leq b_n\\right\\}}-r(\\chi)\\right]\\right)\\nonumber\\\\&:=&a_1+a_2-a_3.\\end{aligned}\\ ] ] @xmath257 writes @xmath258\\right\\}\\\\ & = & \\sum\\limits_{i=1}^{n}\\frac{\\sigma^2_\\varepsilon(\\chi ) \\text{e}k^2\\left(\\frac{\\|\\chi-{\\cal x}\\|}{h_i } \\right)}{f(h_i)^{2\\ell } } + \\frac{\\text{e}\\left[k^2\\left(\\frac{\\|\\chi-{\\cal x}\\|}{h_i } \\right)\\{\\sigma^2_\\varepsilon({\\cal x})-\\sigma^2_\\varepsilon(\\chi)\\ } \\right]}{f(h_i)^{2\\ell}}\\nonumber\\\\&:=&a_{11}+a_{12}.\\label{egalite2}\\end{aligned}\\ ] ] from @xmath58 , by applying fubini s theorem , we have @xmath259.\\end{aligned}\\ ] ] applying again toeplitz s lemma , @xmath63 and @xmath59 allow to get @xmath260}\\sigma^2_\\varepsilon(\\chi)m_2 , \\",
    "\\ \\text{as } \\",
    "\\   n\\rightarrow+\\infty.\\ ] ] the second term of the decomposition of @xmath257 is bounded using as @xmath261 , \\end{aligned}\\ ] ] while the continuity of @xmath262 ( * h1 * ) with toeplitz s lemma ensure that @xmath263 now , let us study the term @xmath264 appearing in the decomposition of @xmath265 . using cauchy - schwartz s inequality , and denoting @xmath266 } k(t)$ ] ,",
    "we get @xmath267 ^ 2\\right)p\\left(|y|>b_n\\right)\\right\\}^\\frac{1}{2}\\\\ & \\leq&3q_n\\|k\\|_\\infty^2 \\sum\\limits_{i=1}^{n}f(h_i)^{-2\\ell},\\end{aligned}\\ ] ] where @xmath268 + we deduce from @xmath59 and @xmath96 , with the choice @xmath269 , that @xmath270\\rightarrow 0 , \\text { as } n\\rightarrow+\\infty \\text { with } \\delta>\\frac{2}{\\lambda}.\\ ] ] the last term @xmath271 is bounded @xmath272\\rightarrow 0 , \\text { as   } n\\rightarrow+\\infty .\\ ] ] relations ( [ a11 ] ) , ( [ a12 ] ) , ( [ a2 ] ) and ( [ a3 ] ) can be used to derive the following equivalence @xmath273}\\sigma^2_\\varepsilon(\\chi)m_2 ,   \\text {   as } n\\rightarrow+\\infty.\\ ] ] next , since @xmath274 as @xmath275 then the first part of * h6 * implies that @xmath276\\left\\{\\ln\\ln\\left[nf(h_n)^{1 - 2\\ell}\\right]\\right\\}^{2(\\alpha+1 ) } } \\rightarrow\\infty , \\ \\",
    "\\text { as } n \\to + \\infty.\\end{aligned}\\ ] ] setting @xmath277 , there exists @xmath278 such that for any @xmath279 , we have @xmath280\\left\\{\\ln\\ln\\left[if(h_i)^{1 - 2\\ell}\\right]\\right\\}^{2(\\alpha+1 ) } }   > \\frac{2\\left\\| k\\right\\|_\\infty^2\\max\\left\\{|r(\\chi)|^2 , ( \\delta\\ln i)^\\frac{2}{\\mu}\\right\\}}{f(h_i)^{2\\ell}}\\geq z_{n , i}^{2}.   \\end{aligned}\\ ] ] hence , the event @xmath281\\left\\{\\ln\\ln\\left[if(h_i)^{1 - 2\\ell}\\right]\\right\\}^{2(\\alpha+1)}}\\right\\}}$ ] is empty for @xmath282 we deduce from ( [ vn ] ) that @xmath283 let @xmath284 be a random function defined on @xmath285 such that for any @xmath286 using theorem 3.1 in @xcite , there exists a brownian motion @xmath287 such that @xmath288 a.s . , \\ \\",
    "\\text{as } t \\to \\infty , \\text { for any } t\\in[v_n , v_{n+1}[.\\ ] ] it follows that @xmath289=1 \\ \\ a.s.\\end{aligned}\\ ] ] then we have @xmath290 by virtue of the definition of @xmath284 and the fact that @xmath291 as @xmath292 . from",
    ", we have @xmath293\\right\\}^{1/2}b_{n,1-\\ell } } { \\left(2v_n\\ln\\ln v_n\\right)^\\frac{1}{2}}&= & \\frac{\\beta_{[1-\\ell]}}{\\left\\{2\\beta_{[1 - 2\\ell]}\\sigma^2_\\varepsilon(\\chi)m_2\\right\\}^{1/2}}.\\end{aligned}\\ ] ] lemma [ lem1 ] follows from the last convergence and the fact that @xmath294 , with the help of ( [ cv1]).@xmath214      we have @xmath295\\nonumber\\\\\\mbox{}&&-{\\text{e}}\\left[k\\left(\\frac{\\|\\chi-{\\cal x}\\|}{h_i } \\right)y{\\mathds{1}}_{\\{|y|>b_n\\}}\\right]\\bigg\\}:= a+b.\\end{aligned}\\ ] ] as in the proof of lemma [ biais1 ] , we can write @xmath296}}{\\beta_{[1-\\ell]}}\\varphi'(0)m_0\\left[1+o(1)\\right],\\end{aligned}\\ ] ] and then , @xmath297^{1/2}a&=&\\left[\\frac{nf(h_n)}{\\ln\\ln n}\\right]^{1/2}h_n\\frac{\\alpha_{[\\ell]}}{\\beta_{[1-\\ell]}}\\varphi'(0)m_0\\left[1+o(1)\\right]=o(1),\\end{aligned}\\ ] ] where the last equality follows from the condition @xmath298 .",
    "for the second term of the right - hand - side in , using cauchy - schwartz s inequality and the boundedness of the kernel @xmath15 , we get latexmath:[\\ ] ] by virtue of * h4(ii ) * and . @xmath214    99 ahmad , i. and lin , p.e .",
    "nonparametric sequential estimation of a multiple regression function",
    "_ , * 17 * , 63 - 75 .",
    "amiri , a. ( 2012 ) .",
    "recursive regression estimators with application to nonparametric prediction .",
    "_ j. nonparametr .",
    "* 24 * ( 1 ) , 169 - 186 .",
    "besse , p. , cardot , h. and ferraty , f. ( 1997 ) .",
    "simultaneous nonparametric regressions of unbalanced longitudinal data .",
    "data anal .",
    "_ , * 24 * , 255 - 270 .",
    "bosq , d. , and cheze - payaud , n. ( 1999 ) . optimal asymptotic quadratic error of nonparametric regression function estimates for a continuous - time process from sampled - data . _ statistics _ , * 32 * ( 3 ) , 229247 .",
    "cardot , h. , ferraty , f. and sarda , p. ( 2003 ) .",
    "splines estimators for the functional linear model .",
    "_ statistica sinica _ , * 13 * , 571 - 591 .",
    "collomb , g. ( 1976 ) .",
    "estimation nonparamtrique de la rgression par la mthode du noyau .",
    "_ unpublished phd thesis _",
    ", universit paul sabatier , toulouse , france .",
    "crambes , c. , kneip , a. and sarda , p. ( 2009 ) . smoothing splines estimators for functional linear regression , _ ann .",
    "_ , * 37 * , 35 - 72 .",
    "devroye , l. , ( 1977 ) . a uniform bound for the deviation of empirical distribution functions .",
    "_ j. mult .",
    "_ , * 7 * , 594 - 597 .",
    "devroye , l. and wagner , t.j .",
    "distribution - free consistency results in nonparametric discrimination and regression function estimation .",
    "_ , * 8 * , 231 - 239 .",
    "ferraty , f. mas , a. and vieu , p. ( 2007 ) .",
    "nonparametric regression on functional data : inference and practical aspects .",
    "n. z. j. stat . ,",
    "_ * 49 * ( 3 ) , 267286 .",
    "ferraty , f. and romain , y. ( 2010 ) .",
    "handbook on functional data analysis and related fields . _ oxford university press _ , oxford .",
    "ferraty , f. and vieu , p. ( 2006 ) .",
    "nonparametric modelling for functional data .",
    "methods , theory , applications and implementations . _",
    "springer - verlag _ , london .",
    "frank , i.e. and friedman , j.h .",
    "( 1993 ) . a statistical view of some chemometrics regression tools . _ technometrics _ , * 35 * , 109135 .",
    "gonzlez manteiga , w. and vieu , p. ( 2007 ) .",
    "statistics for functional data .",
    "stat . and data anal .",
    "_ , * 51 * , 4788 - 4792 .",
    "jain , c. , jogdeo , k. and stout , w. ( 1975 ) .",
    "upper and lower functions for martingales and mixing processes , _ ann .",
    "probab . _ * 3 * ( 1 ) , 119145 .",
    "ling , n. and wu , y. ( 2012 ) .",
    "consistency of modified kernel regression estimation for functional data , _ statistics _ * 46 * ( 2 ) , 149158 .",
    "masry , e. ( 1986 ) .",
    "recursive probability density estimation for weakly dependent stationary processes , _ ieee trans .",
    "inform . theory _",
    "* 32 * ( 2 ) , 254267 .",
    "masry , e. ( 2005 ) .",
    "nonparametric regression estimation for dependent functional data : asymptotic normality , _ stochastic process .",
    "appl . _ * 115 * 155177 .",
    "rachdi , m. and vieu , p. ( 2007 ) .",
    "nonparametric regression for functional data : automatic smoothing parameter selection . _",
    "j. statist .",
    "inference _ , * 137 * , 2784 - 2801 .",
    "ramsay , j.o . and dalzell , c.j .",
    "some tools for functional data analysis ( with discussion ) , _ j. roy .",
    "b _ , * 53 * , 539 - 572 . ramsay , j.o . and silverman , b.w .",
    "applied functional data analysis . _",
    "springer - verlag _ , new - york .",
    "ramsay , j.o . and silverman , b.w .",
    "_ functional data analysis ( @xmath336 ed.)_. springer - verlag , new - york .",
    "roussas , g. g.(1992 ) .",
    "exact rates of almost sure convergence of a recursive kernel estimate of a probability densiy function : application to regression and hazard rate estimation , _ j. nonparametr .",
    "_ * 1 * ( 3 ) , 171 195 ."
  ],
  "abstract_text": [
    "<S> the main purpose is to estimate the regression function of a real random variable with functional explanatory variable by using a recursive nonparametric kernel approach . </S>",
    "<S> the mean square error and the almost sure convergence of a family of recursive kernel estimates of the regression function are derived . </S>",
    "<S> these results are established with rates and precise evaluation of the constant terms . </S>",
    "<S> also , a central limit theorem for this class of estimators is established . </S>",
    "<S> the method is evaluated on simulations and real data set studies .    </S>",
    "<S> _ keywords  : _ functional data , recursive kernel estimators , regression function , quadratic error , almost sure convergence , asymptotic normality . + _ </S>",
    "<S> msc  : _ </S>",
    "<S> 62g05 ,  62g07 ,  62g08 . </S>"
  ]
}