{
  "article_text": [
    "restricted boltzmann machines ( rbms ) @xcite have recently attracted significant interest due to their versatility in a variety of unsupervised and supervised learning tasks @xcite , and in building deep architectures @xcite .",
    "a rbm is a bipartite undirected model that captures the generative process in which a data vector is generated from a binary hidden vector .",
    "the bipartite architecture enables very fast data encoding and sampling - based inference ; and together with recent advances in learning procedures , we can now process massive data with large models @xcite .",
    "this paper presents our contributions in developing rbm specifications as well as learning and inference procedures for multivariate ordinal data .",
    "this extends and consolidates the reach of rbms to a wide range of user - generated domains - social responses , recommender systems , product / paper reviews , and expert assessments of health and ecosystems indicators .",
    "ordinal variables are qualitative in nature  the absolute numerical assignments are not important but the relative order is .",
    "this renders numerical transforms and real - valued treatments inadequate .",
    "current rbm - based treatments , on the other hand , ignore the ordinal nature and treat data as unordered categories @xcite . while convenient , this has several drawbacks : first , order information is not utilised , leading to more parameters than necessary - each category needs parameters .",
    "second , since categories are considered independently , it is less interpretable in terms of how ordinal levels are generated .",
    "better modelling should account for the ordinal generation process .    adapting the classic idea from @xcite , we assume that each ordinal variable is generated by an underlying latent utility , along with a threshold per ordinal level .",
    "as soon as the utility passes the threshold , its corresponding level is selected . as a result",
    ", this process would implicitly encode the order .",
    "our main contribution here is a novel rbm architecture that accounts for multivariate , ordinal data .",
    "more specifically , we further assume that the latent utilities are gaussian variables connected to a set of binary hidden factors ( i.e. , together they form a gaussian rbm @xcite ) .",
    "this offers many advantages over the standard approach that imposes a fully connected gaussian random field over utilities @xcite : first , utilities are seen as being generated from a set of binary factors , which in many cases represent the user s hidden profile .",
    "second , utilities are decoupled given the hidden factors , making parallel sampling easier . and",
    "third , the posteriors of binary factors can be estimated from the ordinal observations , facilitating dimensionality reduction and visualisation .",
    "we term our model cumulative rbm ( @xmath0 ) .",
    "this new model behaves differently from standard gaussian rbms since utilities are never observed in full .",
    "rather , when an ordinal level of an input variable is observed , it poses an _ interval constraint _ over the corresponding utility .",
    "the distribution over the utilities now becomes a _ truncated _ multivariate gaussian .",
    "this also has another consequence during learning : while in standard rbms we need to sample for the _ free - phase _ only ( e.g. , see @xcite ) , now we also need to sample for the _ clamped - phase .",
    "_ as a result , we introduce a double persistent contrastive divergence ( pcd ) learning procedure , as opposed to the single pcd in @xcite .",
    "the second contribution is in advancing these ordinal rbms from modelling i.i.d .",
    "vectors to modelling matrices of correlated entries .",
    "these ordinal matrices are popular in multiuser - generated assessments : each user would typically judge a number of items producing a user - specific data vector where _ intra - vector _ entries are inherently correlated .",
    "since user s choices are influenced by their peers , these _ inter - vector _ entries are no longer independent .",
    "the idea is borrowed from a recent work in @xcite which models both the user - specific and item - specific processes .",
    "more specifically , an ordinal entry is assumed to be jointly generated from user - specific latent factors and item - specific latent factors .",
    "this departs significantly from the standard rbm architecture : we no longer map from a visible vector to an hidden vector but rather map from a visible matrix to two hidden matrices .    in experiments ,",
    "we demonstrate that our proposed @xmath0 is capable of capturing the latent profile of citizens around the world .",
    "our model is also competitive against state - of - the - art collaborative filtering methods on large - scale public datasets .",
    "we start with the rbm structure for ordinal vectors in section  [ sec : crbm - for - vectors ] , and end with the general structure for ordinal matrices in section  [ sec : cbm - for - matrix ] .",
    "section  [ sec : experiments ] presents experiments validating our ordinal rbms in modelling citizen s opinions worldwide and in collaborative filtering .",
    "section  [ sec : related - work ] discusses related work , which is then followed by the conclusions .",
    "filled nodes represent observed ordinal variables , shaded nodes are gaussian utilities , and empty nodes represent binary hidden factors .",
    "[ fig : model - architecture.],scaledwidth=30.0% ]    denote by @xmath1 the set of ordinal observations .",
    "for ease of presentation we assume for the moment that observations are homogeneous , i.e. , observations are drawn from the same discrete ordered category set @xmath2 where @xmath3 denotes the order in some sense .",
    "we further assume that each ordinal @xmath4 is solely generated from an underlying latent _ utility _ @xmath5 as follows @xcite @xmath6 & l=1\\\\ \\mathbb{i}\\left[\\theta_{i(l-1)}<u_{i}\\le\\theta_{il}\\right ] & 1<l\\le l-1\\\\ \\mathbb{i}\\left[\\theta_{i(l-1)}<u_{i}<\\infty\\right ] & l = l \\end{cases}\\label{eq : ordinal - model}\\ ] ] where @xmath7 are threshold parameters . in words , we choose an ordered category on the basis of the interval to which the underlying utility belongs .",
    "the utilities are connected with a set of hidden _ binary factors _",
    "@xmath8 so that the two layers of @xmath9 form a undirectional bipartite graph of restricted boltzmann machines ( rbms ) @xcite .",
    "binary factors can be considered as the hidden features that govern the generation of the observed ordinal data .",
    "thus the generative story is : we start from the binary factors to generate utilities , which , in turn , generate ordinal observations .",
    "see , for example , fig .",
    "[ fig : model - architecture . ] for a graphical representation of the model .",
    "let @xmath10 be the model potential function , which can be factorised as a result of the bipartite structure as follows @xmath11\\left[\\prod_{i , k}\\psi_{ik}(\\x_{i},h_{k})\\right]\\left[\\prod_{k}\\phi_{k}(h_{k})\\right]\\ ] ] where @xmath12 and @xmath13 are local potential functions .",
    "the model joint distribution is defined as    @xmath14    where @xmath15 is the normalising constant .",
    "we assume the utility layer and the binary factor layer form a gaussian rbm @xcite .",
    "this translates into the local potential functions as follows    @xmath16    where @xmath17 is the standard deviation of the @xmath18-th utility , @xmath19 are free parameters for @xmath20 and @xmath21 .    the ordinal assumption in eq .",
    "( [ eq : ordinal - model ] ) introduces _ hard constraints _ that we do not see in standard gaussian rbms .",
    "whenever an ordered category @xmath22 is observed , the corresponding utility is automatically _ truncated _ , i.e. , @xmath23 , where @xmath24 is the new domain of @xmath25 defined by @xmath4 as in eq .",
    "( [ eq : ordinal - model ] ) .",
    "in particular , the utility is truncated from above if the ordinal level is the lowest , from below if the level is the largest , and from both sides otherwise .",
    "for example , the conditional distribution of the _ _ latent utility @xmath26 is a truncated gaussian    @xmath27\\mathcal{n}\\left(\\x_{i};\\mu_{i}(\\hb),\\sigma_{i}\\right)\\label{eq : cond - utility - gauss}\\end{aligned}\\ ] ]    where @xmath28 is the normal density distribution of mean @xmath29 and standard deviation @xmath17 .",
    "the mean @xmath29 is computed as @xmath30    as a generative model , we can estimate the probability that an ordinal level is being generated from hidden factors @xmath31 as follows @xmath32 where @xmath33 , and @xmath34 is the cumulative distribution function of the gaussian .",
    "given this property , we term our model by cumulative restricted boltzmann machine ( @xmath0 ) .    finally , the thresholds are parameterised so that the lowest threshold is fixed to a constant @xmath35 and the higher thresholds are spaced as @xmath36 with free parameter @xmath37 for @xmath38 .",
    "often we are interested in the posterior of factors @xmath39 as it can be considered as a summary of the data @xmath40 .",
    "the nice thing is that it is now numerical and can be used for other tasks such as clustering , visualisation and prediction .    like standard rbms ,",
    "the factor posteriors given the utilities are conditionally independent and assume the form of logistic units    @xmath41    however , since the utilities are themselves hidden , the posteriors given only the ordinal observations are not independent : @xmath42 where @xmath43 and @xmath44 is the domain of the utility constrained by @xmath40 ( see eq .",
    "( [ eq : ordinal - model ] ) ) . here",
    "we describe two approximation methods , namely markov chain monte carlo ( mcmc ) and variational method ( mean - field ) .",
    "[ [ mcmc . ] ] mcmc .",
    "+ + + + +    we can exploit the bipartite structure of the rbm to run layer - wise gibbs sampling : sample the truncated utilities in parallel using eq .",
    "( [ eq : cond - utility - gauss ] ) and the binary factors using eq .",
    "( [ eq : posterior ] ) . finally , the posteriors are estimated as @xmath45 for @xmath46 samples .",
    "[ [ variational - method . ] ] variational method .",
    "+ + + + + + + + + + + + + + + + + + +    we make the approximation @xmath47 minimising the kullback - leibler divergence between @xmath48 and its approximation leads the following recursive update @xmath49\\mathcal{n}\\left(\\x_{i};\\hat{\\mu}{}_{i}(\\hb^{(t)}),\\sigma_{i}\\right)\\end{aligned}\\ ] ] where @xmath50 is the update index of the recursion , @xmath51 is the mean of utility @xmath25 with respect to @xmath52 , @xmath53 is the normalising constant , and @xmath54 . finally , we obtain @xmath55 .",
    "an important task is _ prediction _ of the ordinal level of an unseen variable given the other seen variables , where we need to estimate the following predictive distribution @xmath56 unfortunately , now @xmath57 are coupled due to the integration over @xmath58 making the evaluation intractable , and thus approximation is needed .",
    "for simplicity , we assume that the seen data @xmath40 is informative enough so that @xmath59 . thus we can rewrite eq .",
    "( [ eq : prediction ] ) as    @xmath60    now we make further approximations to deal with the exponential sum over @xmath31 .",
    "[ [ mcmc.-1 ] ] mcmc .",
    "+ + + + +    given the sampling from @xmath61 described in section  [ sub : factor - posteriors ] , we obtain    @xmath62    where @xmath46 is the sample size , and @xmath63 is computed using eq .",
    "( [ eq : ordinal - gen - given - rep ] ) .",
    "[ [ variational - method.-1 ] ] variational method .",
    "+ + + + + + + + + + + + + + + + + + +    the idea is similar to mean - field described in section  [ sub : factor - posteriors ] . in particular , we estimate @xmath64 using either mcmc sampling or mean - field update . the predictive distribution is approximated as @xmath65 where @xmath66 .",
    "the computation is identical to that of eq .",
    "( [ eq : ordinal - gen - given - rep ] ) if we replace @xmath67 ( binary ) by @xmath68 ( real - valued ) .",
    "learning is based on maximising the data log - likelihood @xmath69 where @xmath70 is defined in eq .",
    "( [ eq : model - def ] ) and @xmath71 .",
    "note that @xmath72 includes @xmath73 as a special case when the domain @xmath74 is the whole real space @xmath75 .",
    "recall that the model belongs to the exponential family in that we can rewrite the potential function as    @xmath76 where @xmath77 is a sufficient statistic , and @xmath78 is its associated parameter . now",
    "the gradient of the log - likelihood has the standard form of difference of expected sufficient statistics ( ess ) @xmath79 where @xmath80 is a truncated gaussian rbm and @xmath81 is the standard gaussian rbm .",
    "put in common rbm - terms , there are two learning phases : the _ clamped phase _ in which we estimate the ess w.r.t .",
    "the empirical distribution @xmath82 , and the _ free phase _ in which we compute the ess w.r.t .",
    "model distribution @xmath81 .",
    "the literature offers efficient stochastic gradient procedures to learn parameters , in which the method of @xcite and its variants  the contrastive divergence of @xcite and its persistent version of @xcite  are highly effective in large - scale settings .",
    "the strategy is to update parameters after short markov chains .",
    "typically only the free phase requires the mcmc approximation . in our setting , on the other hand , both the clamped phase and the free phase require approximation .",
    "since it is possible to integrate over utilities when the binary factors are known , it is tempting so sample only the binary factors in the rao - blackwellisation fashion .",
    "however , here we take the advantage of the bipartite structure of the underlying rbm : the layer - wise sampling is efficient and much simpler .",
    "once the hidden factor samples are obtained , we integrate over utilities for better numerical stability .",
    "the esses are the averaged over all factor samples .    for the clamped phase ,",
    "we maintain one markov chain per data instance . for memory efficiency",
    ", only the binary factor samples are stored between update steps . for the free phase ,",
    "there are two strategies :    * _ contrastive _ _ chains _ : one short chain is needed per data instance , but initialised from the clamped chain . that is , we discard those chains after each update . *",
    "_ persistent chains _ : free - phase chains are maintained during the course of learning , independent of the clamp - phase chains . if every data instance has the same dimensions ( which they do not , in the case of missing data ) , we need to maintain a moderate number of chains ( e.g. , @xmath83 ) . otherwise , we need one chain per data instance .    at each step , we collect a small number of samples and estimate the approximate distributions @xmath84 and @xmath85 .",
    "the parameters are updated according to the stochastic gradient ascent rule @xmath86 where @xmath87 is the learning rate .",
    "thresholds appear only in the computation of @xmath72 as they define the utility domain @xmath74 .",
    "let @xmath88 be the upper boundary of @xmath74 , and @xmath89 the lower boundary .",
    "the gradient of the log - likelihood w.r.t .",
    "boundaries reads @xmath90 recall from section  [ sub : model - structure - and - param ] that the boundaries @xmath91 and @xmath92 are the lower - threshold @xmath93 and the upper - threshold @xmath94 , respectively , where @xmath95 . using the chain rule , we would derive the derivatives w.r.t . to @xmath96 .",
    "we now consider the case where ordinal variables do not share the same ordinal scales , that is , we have a separate ordered set @xmath97 for each variable @xmath18 .",
    "this requires only slight change from the homogeneous case , e.g. , by learning separate set of thresholds for each variable .",
    "often the data has the matrix form , i.e. , a list of column vectors and we often assume columns as independent .",
    "however , this assumption is too strong in many applications .",
    "for example , in collaborative filtering where each user plays the role of a column , and each item the role of a row , a user s choice can be influenced by other users choices ( e.g. , due to the popularity of a particular item ) , then columns are correlated .",
    "second , it is also natural to switch the roles of the users and items and this clearly destroys the i.i.d assumption over the columns .",
    "thus , it is more precise to assume that an observation is _ jointly _ generated by both the row - wise and column - wise processes @xcite .",
    "in particular , let @xmath98 be the index of the data instance , each observation @xmath99 is generated from an utility @xmath100 . each data instance ( column ) @xmath98 is represented by a vector of binary hidden factors @xmath101 and each item ( row ) @xmath18 is represented by a vector of binary hidden factors @xmath102 . since our data matrix is usually incomplete ,",
    "let us denote by @xmath103 the incidence matrix where @xmath104 if the cell @xmath105 is observed , and @xmath106 otherwise .",
    "there is a single model for the whole incomplete data matrix .",
    "every observed entry @xmath105 is connected with two sets of hidden factors @xmath107 and @xmath108 .",
    "consequently , there are @xmath109 binary factor units in the entire model .",
    "let @xmath110 denote all latent variables and @xmath111 all visible ordinal variables .",
    "the matrix - variate model distribution has the usual form @xmath112 where @xmath113 is the normalising constant and @xmath114 is the product of all local potentials .",
    "more specifically , @xmath115\\left[\\prod_{i , s}\\phi_{s}(g_{is})\\right]\\end{aligned}\\ ] ] where @xmath116 are the same as those defined in eq .",
    "( [ eq : poten - params ] ) , respectively , and @xmath117    the ordinal model @xmath118 is similar to that defined in eq .",
    "( [ eq : ordinal - model ] ) except for the thresholds , which are now functions of both the data instance and the item , that is @xmath119 and @xmath120 for @xmath38 .",
    "it is easy to see that conditioned on the utilities , the posteriors of the binary factors are still factorisable .",
    "likewise , given the factors , the utilities are univariate gaussian    @xmath121p(u_{di}\\mid\\hb_{d},\\gb_{i})\\end{aligned}\\ ] ]    where @xmath122 is the domain defined by the thresholds at the level @xmath123 , and the mean structure is @xmath124    previous inference tricks can be re - used by noting that for each column ( i.e. , data instance ) , we still enjoy the gaussian rbm when conditioned on other columns__. _ _ the same holds for rows _ _",
    "( i.e. , items)_. _      although it is possible to explore the space of the whole model using gibbs sampling and use the short mcmc chains as before , here we resort to structured mean - field methods to exploit the modularity in the model structure .",
    "the general idea is to alternate between the column - wise and the row - wise conditional processes :    * in the _ column - wise _ process , we estimate item - specific factor posteriors @xmath125 , where @xmath126 and use them _ as if _ the item - specific factors @xmath127 are given . for example , the mean structure in eq .",
    "( [ eq : mean - structure - matrix ] ) now has the following form @xmath128 which is essentially the mean structure in eq .",
    "( [ eq : mean - structure ] ) when @xmath129 is absorbed into @xmath130 . conditioned on the estimated posteriors ,",
    "the data likelihood is now factorisable @xmath131 , where @xmath132 denotes the observations of the @xmath98-th data instance . * similarly , in the _ row - wise _ process we estimate data - specific posteriors @xmath133 , where @xmath134 and use them _ as if _ the data - specific factors @xmath135 are given .",
    "the data likelihood has the form @xmath136 , where @xmath137 denotes the observations of the @xmath18-th item .    at each step",
    ", we then improve the conditional data likelihood using the gradient technique described in section  [ sub : stochastic - gradient - learning ] , e.g. , by running through the whole data once .",
    "the structured mean - fields technique requires the estimation of the factor posteriors . to reduce computation",
    ", we propose to treat the trajectory of the factor posteriors during learning as a _",
    "stochastic process_. this suggests a simple smoothing method , e.g. , at step @xmath50 :    @xmath138 where @xmath139 is the _ smoothing factor , _ and @xmath140 is a utility sample in the _ clamped phase_. this effectively imposes an exponential decay to previous samples .",
    "the estimation of @xmath141 would be of interest in its own right , but we would empirically set @xmath142 and do not pursue the issue further .",
    "in this section , we demonstrate how @xmath0 can be useful in real - world data analysis tasks . to monitor learning progress , we estimate the data pseudo - likelihood @xmath143 . for simplicity ,",
    "we treat @xmath4 as if it is not in @xmath40 and replace @xmath144 by @xmath40 .",
    "this enables us to use the same predictive methods in section  [ sub : prediction ] .",
    "see fig .",
    "[ fig : vector - versus - matrix](a ) for an example of the learning curves . to sample from the truncated gaussian",
    ", we employ methods described in @xcite , which is more efficient than standard rejection sampling techniques . mapping parameters @xmath145",
    "are initialised randomly , bias paramters are from zeros , and thresholds @xmath146 are spaced evenly at the begining .       on 2d using t - sne @xcite , where @xmath147 is a shorthand for @xmath148 .",
    "best viewed in colours .",
    "[ fig : visualisation - of - world - opinions].,scaledwidth=60.0%,scaledwidth=45.0% ]    in this experiments we validate the capacity to discover meaningful latent profiles from people s opinions about their life and the social / political conditions in their country and around the world .",
    "we use the public world - wide survey by pewresearch centre in 2008 which interviewed @xmath149 people from @xmath150 countries . after re - processing , we keep @xmath151 ordinal responses per respondent .",
    "example questions are : `` ( q1 ) [ .. ] how would you describe your day todayhas it been a typical day , a particularly good day , or a particularly bad day ? '' , `` ( q5 ) [ ... ] over the next 12 months do you expect the economic situation in our country to improve a lot , improve a little , remain the same , worsen a little or worsen a lot ? '' .",
    "the data is heterogeneous since question types are different ( see section  [ sub : handling - heterogeneous - data ] ) . for this",
    "we use a vector - based @xmath0 with @xmath152 hidden units . after model fitting",
    ", we obtain a posterior vector @xmath153 , which is then used as the representation of the respondent s latent profile . for visualisation , we project this vector onto the 2d plane using a locality - preserving dimensionality reduction method known as _ _ t - sne",
    "_ _ @xcite .",
    "the opinions of citizens of @xmath154 countries are depicted in fig .",
    "[ fig : visualisation - of - world - opinions ] .",
    "this clearly reveals how cultures ( e.g. , islamic and chinese ) and nations ( e.g. , the us , china , latin america ) see the world .",
    "we verify our models on three public rating datasets : movielens  containing @xmath155 million ratings by @xmath156 thousand users on nearly @xmath157 thousand movies ; dating  consisting of @xmath158 million ratings by @xmath159 thousand users on nearly @xmath160 thousand profiles ; and netflix  @xmath161 millions ratings by @xmath162 thousand users on nearly @xmath163 thousand movies .",
    "the dating ratings are on the @xmath164-point scale and the other two are on the @xmath165-star scale .",
    "we then transform the dating ratings to the @xmath165-point scale for uniformity . for each data",
    "we remove those users with less than @xmath166 ratings , @xmath165 of which are used for tuning and stopping criterion , @xmath164 for testing and the rest for training . for movielens and netflix , we ensure that rating timestamps are ordered from training , to validation to testing . for the dating dataset ,",
    "the selection is at random .",
    "for comparison , we implement state - of - the - art methods in the field , including : matrix factorisation ( mf ) with gaussian assumption @xcite , mf with cumulative ordinal assumption @xcite ( without item - item neighbourhood ) , and rbm with multinomial assumption @xcite.for prediction in the crbm , we employ the variational method ( section  [ eq : prediction ] ) .",
    "the training and testing protocols are the same for all methods : training stops where there is no improvement on the likelihood of the validation data .",
    "two popular performance metrics are reported on the test data : _ _ the _ root - mean square error _ ( rmse ) , the _",
    "mean absolute error _ ( mae ) .",
    "prediction for ordinal mf and rbms is a numerical mean in the case of rmse : @xmath167 , and an map estimation in the case of mae : @xmath168 .",
    "[ fig : vector - versus - matrix](a ) depicts the learning curve of the vector - based and matrix - based @xmath0s , and fig .",
    "[ fig : vector - versus - matrix](b ) shows their predictive performance on test datasets .",
    "clearly , the effect of matrix treatment is significant .",
    "tables  [ tab : movielens-1m],[tab : dating-16m],[tab : netflix-100 m ] report the performances of all methods on the three datasets .",
    "the ( matrix ) @xmath0 are often comparable with the best rivals on the rmse scores and are competitive against all others on the mae .    [ cols=\"^,^,^ \" , ]",
    "this work partly belongs to the thread of research that extends rbms for a variety of data types , including _ categories _ @xcite _ _ , counts _ _",
    "@xcite , _ bounded _",
    "variables @xcite and a mixture of these types @xcite .",
    "gaussian rbms have been only used for continuous variables @xcite  thus our use for ordinal variables is novel .",
    "there has also been recent work extending gaussian rbms to better model highly correlated input variables @xcite . for ordinal data , to the best of our knowledge , the first rbm - based work is @xcite , which also contains a treatment of matrix - wise data__. _",
    "_ however , their work indeed models multinomial data with knowledge of orders rather than modelling the ordinal nature directly .",
    "the result is that it is over - parameterised but less efficient and does not offer any underlying generative mechanism for ordinal data .",
    "ordinal data has been well investigated in statistical sciences , especially quantitative social studies , often under the name of _ ordinal regression _ , which refers to single ordinal output given a set of input covariates .",
    "the most popular method is by @xcite which examines the level - wise cumulative distributions .",
    "another well - known treatment is the sequential approach , also known as continuation ratio @xcite , in which the ordinal generation process is considered stepwise , starting from the lowest level until the best level is chosen . for reviews of recent development , we refer to @xcite . in machine learning , this has attracted a moderate attention in the past decade @xcite , adding machine learning flavours ( e.g. , large - margins ) to existing statistical methods .",
    "multivariate ordinal variables have also been studied for several decades @xcite .",
    "the most common theme is the assumption of the latent multivariate normal distribution that generates the ordinal observations , often referred to as _ multivariate probit _",
    "models @xcite .",
    "the main problem with this setting is that it is only feasible for problems with small dimensions .",
    "our treatment using rbms offer a solution for large - scale settings by transferring the low - order interactions among the gaussian variables onto higher - order interactions through the hidden binary layer .",
    "not only this offers much faster inference , it also enables automatic discovery of latent aspects in the data .    for matrix data ,",
    "the most well - known method is perhaps matrix factorisation @xcite .",
    "however , this method assumes that the data is normally distributed , which does not meet the ordinal characteristics well .",
    "recent research has attempted to address this issue @xcite . in particular , @xcite adapt cumulative models of @xcite , and @xcite tailors the sequential models of @xcite for task .",
    "we have presented @xmath0 , a novel probabilistic model to handle vector - variate and matrix - variate ordinal data .",
    "the model is based on gaussian restricted boltzmann machines and we present the model architecture , learning and inference procedures . we show that the model is useful in profiling opinions of people across cultures and nations .",
    "the model is also competitive against state - of - art methods in collaborative filtering using large - scale public datasets .",
    "thus our work enriches the rbms , and extends their use on multivariate ordinal data in diverse applications .",
    "bo  chen benjamin  marlin , kevin  swersky and nando de  freitas . .",
    "in _ proceedings of the 13rd international conference on artificial intelligence and statistics _ , chia laguna resort , sardinia , italy , may 2010 .",
    "r.  salakhutdinov , a.  mnih , and g.  hinton .",
    "restricted boltzmann machines for collaborative filtering . in _ proceedings of the 24th international conference on machine learning ( icml ) _ , pages 791798 , 2007 ."
  ],
  "abstract_text": [
    "<S> ordinal data is omnipresent in almost all multiuser - generated feedback - questionnaires , preferences etc . </S>",
    "<S> this paper investigates modelling of ordinal data with gaussian restricted boltzmann machines ( rbms ) . </S>",
    "<S> in particular , we present the model architecture , learning and inference procedures for both vector - variate and matrix - variate ordinal data . we show that our model is able to capture latent opinion profile of citizens around the world , and is competitive against state - of - art collaborative filtering techniques on large - scale public datasets . </S>",
    "<S> the model thus has the potential to extend application of rbms to diverse domains such as recommendation systems , product reviews and expert assessments .    </S>",
    "<S> v  </S>"
  ]
}