{
  "article_text": [
    "kernel methods have proven to be a highly successful technique for solving many problems in machine learning , ranging from classification and regression to sequence annotation and feature extraction @xcite . at their heart",
    "lies the idea that inner products in high - dimensional feature spaces can be computed in implicit form via kernel function @xmath5 : @xmath6 here @xmath7 is a feature map transporting elements of the observation space @xmath8 into a possibly infinite - dimensional feature space @xmath9 .",
    "this idea was first used by @xcite to show nonlinear separation .",
    "there exists a rich body of literature on reproducing kernel hilbert spaces ( rkhs ) @xcite and one may show that estimators using norms in feature space as penalty are equivalent to estimators using smoothness in an rkhs @xcite .",
    "furthermore , one may provide a bayesian interpretation via gaussian processes .",
    "see e.g. @xcite for details .    more concretely , to evaluate the decision function @xmath10 on an example @xmath11 , one typically employs the kernel trick as follows @xmath12 this has been viewed as a strength of kernel methods , especially in the days that datasets consisted of ten thousands of examples .",
    "this is because the representer theorem of @xcite states that such a function expansion in terms of finitely many coefficients must exist under fairly benign conditions even whenever the space is infinite dimensional .",
    "hence we can effectively perform optimization in infinite dimensional spaces .",
    "this trick that was also exploited by @xcite for evaluating pca . frequently the coefficient space is referred to as _",
    "dual space_. this arises from the fact that the coefficients are obtained by solving a dual optimization problem .",
    "unfortunately , on large amounts of data , this expansion becomes a significant liability for computational efficiency .",
    "for instance , @xcite show that the number of nonzero @xmath13 ( i.e. , @xmath14 , also known as the number of `` support vectors '' ) in many estimation problems can grow linearly in the size of the training set . as a consequence ,",
    "as the dataset grows , the expense of evaluating @xmath15 also grows .",
    "this property makes kernel methods expensive in many large scale problems : there the sample size @xmath16 may well exceed billions of instances .",
    "the large scale solvers of @xcite and @xcite work in primal space to sidestep these problems , albeit at the cost of limiting themselves to linear kernels , a significantly less powerful function class .",
    "numerous methods have been proposed to mitigate this issue . to compare computational cost of these methods",
    "we make the following assumptions :    * we have @xmath16 observations and access to an @xmath17 with @xmath18 algorithm for solving the optimization problem at hand . in other words ,",
    "the algorithm is linear or worse .",
    "this is a reasonable assumption  almost all data analysis algorithm need to inspect the data at least once to draw inference .",
    "* data has @xmath3 dimensions . for simplicity",
    "we assume that it is dense with density rate @xmath19 , i.e.  on average @xmath20 coordinates are nonzero . *",
    "the number of nontrivial basis functions is @xmath21 .",
    "this is well motivated by @xcite and it also follows from the fact that e.g.  in regularized risk minimization the subgradient of the loss function determines the value of the associated dual variable . * we denote the number of ( nonlinear ) basis functions by @xmath2 .",
    "[ [ reduced - set - expansions ] ] reduced set expansions + + + + + + + + + + + + + + + + + + + + + +    @xcite focused on compressing function expansions after the problem was solved by means of reduced - set expansions .",
    "that is , one first solves the full optimization problem at @xmath22 cost and subsequently one minimizes the discrepancy between the full expansion and an expansion on a subset of basis functions .",
    "the exponent of @xmath23 arises from the fact that we need to compute @xmath24 kernels @xmath17 times .",
    "evaluation of the reduced function set costs at least @xmath25 operations per instance and @xmath25 storage , since each kernel function @xmath26 requires storage of @xmath27 .",
    "[ [ low - rank - expansions ] ] low rank expansions + + + + + + + + + + + + + + + + + + +    subsequent work by @xcite and @xcite aimed to reduce memory footprint and complexity by finding subspaces to expand functions .",
    "the key difference is that these algorithms reduce the function space _ before _ seeing labels .",
    "while this is suboptimal , experimental evidence shows that for well designed kernels the basis functions extracted in this fashion are essentially as good as reduced set expansions .",
    "this is to be expected .",
    "after all , the kernel encodes our prior belief in which function space is most likely to capture the relevant dependencies between covariates and labels .",
    "these projection - based algorithms generate an @xmath2-dimensional subspace :    * compute the kernel matrix @xmath28 on an @xmath2-dimensional subspace at @xmath29 cost . *",
    "the matrix @xmath28 is inverted at @xmath30 cost .",
    "* for all observations one computes an explicit feature map by projecting data in rkhs onto the set of @xmath2 basis vectors via @xmath31}$ ] .",
    "that is , training proceeds at @xmath32 cost . * prediction costs @xmath25 computation and @xmath25 memory , as in reduced set methods , albeit with a different set of basis functions .",
    "note that these methods temporarily require @xmath33 storage during training , since we need to be able to multiply with the inverse covariance matrix efficiently .",
    "this allows for solutions to problems where @xmath16 is in the order of millions and @xmath2 is in the order of thousands : for @xmath34 we need approximately @xmath35 gb of memory to store and invert the covariance matrix .",
    "preprocessing can be parallelized efficiently .",
    "obtaining a minimal set of observations to project on is even more difficult and only the recent work of @xcite provides usable performance guarantees for it .",
    "[ [ multipole - methods ] ] multipole methods + + + + + + + + + + + + + + + + +    fast multipole expansions @xcite offer one avenue for efficient function expansions whenever the dimensionality of the underlying space is relatively modest .",
    "however , for high dimensions they become computationally intractable in terms of space partitioning , due to the curse of dimensionality .",
    "moreover , they are typically tuned for localized basis functions , specifically the gaussian rbf kernel .",
    "[ [ random - subset - kernels ] ] random subset kernels + + + + + + + + + + + + + + + + + + + + +    a promising alternative to _ approximating an existing kernel function _ is to design new ones that are immediately compatible with scalable data analysis .",
    "a recent instance of such work is the algorithm of @xcite who map observations @xmath11 into set membership indicators @xmath36 , where @xmath37 denotes the random partitioning chosen at iterate @xmath37 and @xmath38 indicates the particular set .",
    "while the paper suggests that the algorithm is scalable to large amounts of data , it suffers from essentially the same problem as other feature generation methods insofar as it needs to evaluate set membership for each of the partitions for all data , hence we have an @xmath39 computational cost for @xmath2 partitions into @xmath5 sets on @xmath16 observations .",
    "even this estimate is slightly optimistic since we assume that computing the partitions is independent of the dimensionality of the data . in summary , while the function class is potentially promising , its computational cost considerably exceeds that of the other algorithms discussed below , hence we do not investigate it further .    [",
    "[ random - kitchen - sinks ] ] random kitchen sinks + + + + + + + + + + + + + + + + + + + +    a promising alternative was proposed by @xcite under the moniker of _ random kitchen sinks_. in contrast to previous work the authors attempt to obtain an _ explicit _ function space expansion directly .",
    "this works for translation invariant kernel functions by performing the following operations :    * generate a ( gaussian ) random matrix @xmath40 of size @xmath41 . * for each observation @xmath11 compute @xmath42 and apply a nonlinearity @xmath43 to each coordinate separately , i.e.  @xmath44_i)$ ] .",
    "the approach requires @xmath45 storage both at training and test time .",
    "training costs @xmath46 operations and prediction on a new observation costs @xmath25 .",
    "this is potentially much cheaper than reduced set kernel expansions .",
    "the experiments in @xcite showed that performance was very competitive with conventional rbf kernel approaches while providing dramatically simplified code .",
    "note that explicit spectral finite - rank expansions offer potentially much faster rates of convergence , since the spectrum decays as fast as the eigenvalues of the associated regularization operator @xcite .",
    "nonetheless random kitchen sinks are a very attractive alternative due to their simple construction and the flexility in synthesizing kernels with predefined smoothness properties .    [ [ fastfood ] ] fastfood + + + + + + + +    our approach hews closely to random kitchen sinks .",
    "however , it succeeds at overcoming their key obstacle  the need to _ store _ and to _ multiply _ by a random matrix .",
    "this way , fastfood , accelerates random kitchen sinks from @xmath4 to @xmath47 time while only requiring @xmath1 rather than @xmath4 storage .",
    "the speedup is most significant for large input dimensions , a common case in many large - scale applications .",
    "for instance , a tiny 32x32x3 image in the cifar-10  @xcite already has 3072 dimensions , and non - linear function classes have shown to work well for mnist  @xcite and cifar-10 .",
    "our approach relies on the fact that hadamard matrices , when combined with gaussian scaling matrices , behave very much like gaussian random matrices .",
    "that means these two matrices can be used in place of gaussian matrices in random kitchen sinks and thereby speeding up the computation for a large range of kernel functions .",
    "the computational gain is achieved because unlike gaussian random matrices , hadamard matrices admit fft - like multiplication and require no storage .",
    "we prove that the fastfood approximation is unbiased , has low variance , and concentrates almost at the same rate as random kitchen sinks .",
    "moreover , we extend the range of applications from radial basis functions @xmath48 to any kernel that can be written as dot product @xmath49 .",
    "extensive experiments with a wide range of datasets show that fastfood achieves similar accuracy to full kernel expansions and random kitchen sinks while being 100x faster with 1000x less memory .",
    "these improvements , especially in terms of memory usage , make it possible to use kernel methods even for embedded applications .",
    "our experiments also demonstrate that fastfood , thanks to its speedup in training , achieves state - of - the - art accuracy on the cifar-10 dataset  @xcite among permutation - invariant methods .",
    "table  [ tab : compcost ] summarizes the computational cost of the above algorithms .",
    ".computational cost for reduced rank expansions .",
    "efficient algorithms achieve @xmath50 and typical sparsity coefficients are @xmath51 . [ cols=\"<,<,<,<,<\",options=\"header \" , ]     in figure  [ fig : expansion ] , we show regression performance as a function of the number of basis functions @xmath2 on the cpu dataset .",
    "as is evident , it is necessary to have a large @xmath2 in order to learn highly nonlinear functions .",
    "interestingly , although the fourier features do not seem to approximate the gaussian rbf kernel , they perform well compared to other variants and improve as @xmath2 increases .",
    "this suggests that learning the kernel by direct spectral adjustment might be a useful application of our proposed method .          in the previous experiments ,",
    "we observe that fastfood is on par with exact kernel computation , the nystrom method , and random kitchen sinks .",
    "the key point , however , is to establish whether the algorithm offers computational savings .    for this purpose",
    "we compare random kitchen sinks using eigen and our method using spiral .",
    "both are highly optimized numerical linear algebra libraries in c++ .",
    "we are interested in the time it takes to go from raw features of a vector with dimension @xmath3 to the label prediction of that vector . on a small problem with @xmath52 and @xmath53",
    ", performing prediction with random kitchen sinks takes 0.07 seconds .",
    "our method is around 24x faster , taking only 0.003 seconds to compute the label for one input vector .",
    "the speed gain is even more significant for larger problems , as is evident in table  [ tab : improvements ] .",
    "this confirms experimentally the @xmath47 vs.  @xmath54 runtime and the @xmath1 vs.@xmath54 storage of fastfood relative to random kitchen sinks . in other words ,",
    "the computational savings are substantial for large input dimensionality @xmath3 .      to understand the importance of nonlinear feature expansions for a practical application , we benchmarked fastfood , random kitchen sinks on the cifar-10 dataset  @xcite which has 50,000 training images and 10,000 test images .",
    "each image has 32x32 pixels and 3 channels ( @xmath55 ) . in our experiments ,",
    "linear svms achieve 42.3% accuracy on the test set .",
    "non - linear expansions improve the classification accuracy significantly .",
    "in particular , fastfood fft ( `` fourier features '' ) achieve 63.1% while fastfood ( `` hadamard features '' ) and random kitchen sinks achieve 62.4% with an expansion of @xmath56 .",
    "these are also best known classification accuracies using permutation - invariant representations on this dataset . in terms of speed ,",
    "random kitchen sinks is 5x slower ( in total training time ) and 20x slower ( in predicting a label given an image ) compared to both fastfood and and fastfood fft .",
    "this demonstrates that non - linear expansions are needed even when the raw data is high - dimensional , and that fastfood is more practical for such problems .    in particular , in many cases ,",
    "linear function classes are used because they provide fast training time , and especially test time , but not because they offer better accuracy .",
    "the results on cifar-10 demonstrate that fastfood can overcome this obstacle .",
    "we demonstrated that it is possible to compute @xmath2 nonlinear basis functions in @xmath47 time , a significant speedup over the best competitive algorithms .",
    "this means that kernel methods become more practical for problems that have large datasets and/or require real - time prediction .",
    "in fact , fastfood can be used to run on cellphones because not only it is fast , but it also requires only a small amount of storage .    note that our analysis is not limited to translation invariant kernels but it also includes inner product formulations .",
    "this means that for most practical kernels our tools offer an easy means of making kernel methods scalable beyond simple subspace decomposition strategies .",
    "extending our work to other symmetry groups is subject to future research . also note that fast multiplications with near - gaussian matrices are a key building block of many randomized algorithms .",
    "it remains to be seen whether one could use the proposed methods as a substitute and reap significant computational savings .",
    "i.  bogaert , b.  michiels , and j.  fostier . computation of legendre polynomials and gauss  legendre nodes and weights for parallel computing .",
    "_ siam journal on scientific computing _ , 340 ( 3):0 c83c101 , 2012 .",
    "b.  boser , i.  guyon , and v.  vapnik .",
    "a training algorithm for optimal margin classifiers . in d.",
    "haussler , editor , _ proc .  annual conf .",
    "computational learning theory _",
    ", pages 144152 , pittsburgh , pa , july 1992 .",
    "acm press .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein . distributed optimization and statistical learning via the alternating direction method of multipliers . _",
    "foundations and trends in machine learning _ , 30 ( 1):0 1123 , 2010 .",
    "a.  das and d.  kempe .",
    "submodular meets spectral : greedy algorithms for subset selection , sparse approximation and dictionary selection . in l.",
    "getoor and t.  scheffer , editors , _ proceedings of the 28th international conference on machine learning , icml _ , pages 10571064 .",
    "omnipress , 2011 .",
    "a.  dasgupta , r.  kumar , and t.  sarls . fast locality - sensitive hashing . in _ proceedings of the 17th acm",
    "sigkdd international conference on knowledge discovery and data mining _ , pages 10731081 .",
    "acm , 2011 .",
    "f.  girosi and g.  anzellotti .",
    "rates of convergence for radial basis functions and neural networks . in r.",
    "j. mammone , editor , _ artificial neural networks for speech and vision _ , pages 97113 , london , 1993 .",
    "chapman and hall .",
    "s.  matsushima , s.v.n .",
    "vishwanathan , and a.j .",
    "linear support vector machines via dual cached loops . in q.",
    "yang , d.  agarwal , and j.  pei , editors , _ the 18th acm sigkdd international conference on knowledge discovery and data mining , kdd _ , pages 177185 .",
    "acm , 2012 .",
    "url http://dl.acm.org/citation.cfm?id=2339530 .",
    "a.  rahimi and b.  recht .",
    "random features for large - scale kernel machines . in j.c .",
    "platt , d.  koller , y.  singer , and s.  roweis , editors , _ advances in neural information processing systems 20_. mit press , cambridge , ma , 2008 .",
    "n.  ratliff , j.  bagnell , and m.  zinkevich .",
    "( online ) subgradient methods for structured prediction . in _ eleventh international conference on artificial intelligence and statistics ( aistats ) _ , march 2007 .",
    "a.  j. smola and b.  schlkopf . sparse greedy matrix approximation for machine learning . in",
    "_ proceedings of the international conference on machine learning _ , pages 911918 , san francisco , 2000 .",
    "morgan kaufmann publishers .",
    "a.  j. smola , b.  schlkopf , and k .-",
    "general cost functions for support vector regression . in t.  downs , m.  frean , and m.  gallagher , editors , _ proc .  of the ninth australian conf .  on neural networks _ , pages 7983 , brisbane , australia , 1998",
    "university of queensland .",
    "a.  j. smola , z.  l. vri , and r.  c. williamson .",
    "regularization with dot - product kernels . in t.",
    "k. leen , t.  g. dietterich , and v.  tresp , editors , _ advances in neural information processing systems 13 _ , pages 308314 . mit press , 2001 .",
    "b.  taskar , c.  guestrin , and d.  koller .",
    "max - margin markov networks . in s.  thrun , l.  saul , and b.  schlkopf , editors , _ advances in neural information processing systems 16 _ , pages 2532 , cambridge , ma , 2004 .",
    "mit press .          v.  vapnik , s.  golowich , and a.  j. smola .",
    "support vector method for function approximation , regression estimation , and signal processing . in m.  c. mozer , m.  i. jordan , and t.  petsche , editors , _ advances in neural information processing systems 9 _ , pages 281287 , cambridge , ma , 1997 . mit press .      c.  k.  i. williams .",
    "prediction with gaussian processes : from linear regression to linear prediction and beyond . in m.",
    "i. jordan , editor , _ learning and inference in graphical models _ , pages 599621 .",
    "kluwer academic , 1998 .",
    "christoper k.  i. williams and matthias seeger .",
    "using the nystrom method to speed up kernel machines . in t.",
    "k. leen , t.  g. dietterich , and v.  tresp , editors , _ advances in neural information processing systems 13 _ , pages 682688 , cambridge , ma , 2001 . mit press .",
    "r.  c. williamson , a.  j. smola , and b.  schlkopf .",
    "generalization bounds for regularization networks and support vector machines via entropy numbers of compact operators .",
    "_ ieee trans .",
    "inform . theory _",
    ", 470 ( 6):0 25162532 , 2001 ."
  ],
  "abstract_text": [
    "<S> despite their successes , what makes kernel methods difficult to use in many large scale problems is the fact that storing and computing the decision function is typically expensive , especially at prediction time . in this paper </S>",
    "<S> , we overcome this difficulty by proposing fastfood , an approximation that accelerates such computation significantly . </S>",
    "<S> key to fastfood is the observation that hadamard matrices , when combined with diagonal gaussian matrices , exhibit properties similar to dense gaussian random matrices . </S>",
    "<S> yet unlike the latter , hadamard and diagonal matrices are inexpensive to multiply and store . </S>",
    "<S> these two matrices can be used in lieu of gaussian matrices in random kitchen sinks proposed by @xcite and thereby speeding up the computation for a large range of kernel functions . </S>",
    "<S> specifically , fastfood requires @xmath0 time and @xmath1 storage to compute @xmath2 non - linear basis functions in @xmath3 dimensions , a significant improvement from @xmath4 computation and storage , without sacrificing accuracy .    </S>",
    "<S> our method applies to any translation invariant and any dot - product kernel , such as the popular rbf kernels and polynomial kernels . </S>",
    "<S> we prove that the approximation is unbiased and has low variance . </S>",
    "<S> experiments show that we achieve similar accuracy to full kernel expansions and random kitchen sinks while being 100x faster and using 1000x less memory . </S>",
    "<S> these improvements , especially in terms of memory usage , make kernel methods more practical for applications that have large training sets and/or require real - time prediction . </S>"
  ]
}