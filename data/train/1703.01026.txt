{
  "article_text": [
    "when using traditional reinforcement learning ( rl ) algorithms ( such as @xmath4-learning or sarsa ) to evaluate policies in environments with large state or action spaces , it is common to introduce some form of architecture with which to approximate the value function ( vf ) , for example a parametrised set of functions .",
    "this approximation architecture allows algorithms to deal with problems which would otherwise be computationally intractable .",
    "one issue when introducing vf approximation , however , is that the accuracy of the algorithm s vf estimate is highly dependent upon the exact form of the architecture chosen .",
    "accordingly , a number of authors have explored the possibility of allowing the approximation architecture to be _ learned _ by the agent , rather than pre - set manually by the designer ( see @xcite for an overview ) .",
    "it is common to assume that the approximation architecture being adapted is linear ( so that the value function is represented as a weighted sum of basis functions ) in which case such methods are known as _ basis function adaptation_. when designing a method to adapt approximation architectures , we assume that we have an underlying rl algorithm which , for a given linear architecture , will generate a vf estimate .",
    "if we assume that our basis function adaptation occurs on - line , then the rl algorithm will be constantly updating its vf estimate as the basis functions are updated ( the latter typically on a slower time - scale ) .    a simple and",
    "perhaps , as yet , under - explored method of basis function adaptation involves using an estimate of the frequency with which an agent has visited certain states to determine which states to more accurately represent .",
    "such methods are _ unsupervised _ in the sense that no direct reference to the reward or to any estimate of the value function is made .",
    "the concept of using visit frequencies in an unsupervised manner is not completely new @xcite@xcite however it remains relatively unexplored when compared to methods based on direct estimation of vf error @xcite@xcite@xcite@xcite .",
    "however it is possible that such methods can offer some unique advantages . in particular : ( i ) estimates of visit frequencies are very cheap to calculate , ( ii ) accurate estimates of visit frequencies can be generated with a relatively small number of samples , and , perhaps most importantly , ( iii ) in many cases visit frequencies contain a lot of the most important information regarding where accuracy is required in the vf estimate .",
    "our aim here is to further explore and quantify , where possible , these advantages . in the next section",
    "we outline the details of an algorithm ( pasa , short for `` probabilistic adaptive state aggregation '' ) which performs unsupervised basis function adaptation based on state aggregation .",
    "this algorithm will form the basis upon which we develop theoretical results in section [ theoretic ] .",
    "the agent interacts with an environment over a sequence of iterations @xmath5 . for each @xmath6 it will be in a particular state @xmath7 ( @xmath8 ) and",
    "will take a particular action @xmath9 ( @xmath10 ) according to a policy @xmath11 ( we denote as @xmath12 the probability the agent takes action @xmath9 in state @xmath7 ) .",
    "transition and reward functions are denoted as @xmath13 and @xmath14 respectively ( and are unknown , however we assume we are given a prior distribution for both )",
    ". hence we use @xmath15 to denote the probability the agent will transition to state @xmath16 given it takes action @xmath9 in state @xmath7 .",
    "we assume the reward function ( which maps each state - action pair to a real number ) is bounded : @xmath17 for all @xmath18 .",
    "we are considering the problem of policy evaluation , so we will always assume that the agent s policy @xmath11 is fixed .",
    "a _ state aggregation _ approximation architecture we will define as a mapping @xmath19 from each state @xmath7 to a _ cell _ @xmath20 ( @xmath21 ) , where typically @xmath22 @xcite . given a state aggregation approximation architecture ,",
    "an rl algorithm will maintain an estimate @xmath23 of the true value function @xmath24 @xcite , where @xmath25 specifies a weight associated with each cell - action pair .",
    "provided for any particular mapping @xmath19 the value @xmath25 converges , then each mapping @xmath19 has a corresponding vf estimate for the policy @xmath11 .",
    "many different methods can be used to score a vf estimate ( i.e. measure its accuracy ) .",
    "a common score used is the squared _ bellman error _",
    "@xcite for each state - action , weighted by the probability of visiting each state .",
    "this is called the _",
    "mean squared error _ ( mse ) .",
    "when the true vf @xmath24 is unknown we can use @xmath26 , the _ bellman operator _",
    ", to obtain an approximation of the mse .",
    "this approximation we denote as @xmath27 .",
    "hence : @xmath28    this is where @xmath29 is a discount factor and where @xmath30 is a vector of the probability of each state given the stationary distribution associated with @xmath11 ( given some fixed policy @xmath11 , the transition matrix , obtained from @xmath11 and @xmath13 , has a corresponding stationary distribution ) .",
    "we assume that our task in developing an adaptive architecture is to design an algorithm which will adapt @xmath19 so that @xmath27 is minimised .",
    "the pasa algorithm , which we now outline , adapts the mapping @xmath19 .",
    "pasa will store a vector @xmath31 of integers of dimension @xmath32 , where @xmath33 .",
    "suppose we start with a partition of the state space into @xmath34 cells , indexed from @xmath35 to @xmath34 , each of which is approximately the same size .",
    "using @xmath31 we can now define a new partition by splitting ( as evenly as possible ) the @xmath36th cell in this partition .",
    "we leave one half of the @xmath36th cell with the index @xmath36 and give the other half the index @xmath37 ( all other indices stay the same ) . taking this new partition ( consisting of @xmath37 cells )",
    "we can create a further partition by splitting the @xmath38th cell .",
    "continuing in this fashion we will end up with a partition containing @xmath1 cells ( which gives us the mapping @xmath19 ) .",
    "we need some additional mechanisms to allow us to update @xmath31 .",
    "denote as @xmath39 the set of states in the @xmath40th cell of the @xmath41th partition ( so @xmath42 and @xmath43 ) .",
    "the algorithm will store a vector @xmath44 of real values of dimension @xmath1 .",
    "this will record the approximate frequency with which certain cells have been visited by the agent .",
    "we define a new vector @xmath45 of dimension @xmath1 : @xmath46 where @xmath47 is the indicator function for a logical statement ( such that @xmath48 if @xmath49 is true ) .",
    "the resulting mapping from each state to a vector @xmath45 we denote as @xmath50 .",
    "we then update @xmath44 in each iteration as follows ( i.e. using a simple stochastic approximation algorithm ) : @xmath51    this is where @xmath52 $ ] is a constant step size parameter . to update @xmath31 , at certain intervals @xmath53",
    "the pasa algorithm performs a sequence of @xmath32 operations . a temporary copy of @xmath44",
    "is made , which we call @xmath54 .",
    "we also store an @xmath1 dimensional boolean vector @xmath55 and set each entry to zero at the start of the sequence ( this keeps track of whether a particular cell has only one state , as we do nt want singleton cells to be split ) . at each stage @xmath56 of the sequence we update @xmath31 , @xmath54 and @xmath55 , in order , as follows ( for @xmath31 ,",
    "if multiple indices satisfy the @xmath57 function , we take the lowest index ) : @xmath58 where @xmath59 is a constant designed to ensure that a ( typically small ) threshold must be exceeded before @xmath31 is adjusted . the idea behind each step in the sequence is that the non - singleton cell @xmath60 with the highest value @xmath61 ( an estimate of visit frequency which is recalculated at each step ) will be split .",
    "details of these steps , as well as the overall pasa process , are outlined in algorithm [ pasa ] .",
    "note that the algorithm calls a procedure to cells .",
    "this procedure simply updates @xmath50 and @xmath55 given the latest value of @xmath31 .",
    "it also calls a procedure , which converts the mapping @xmath50 to a mapping @xmath19 .",
    "2    @xmath62 @xmath63 @xmath64 @xmath65 @xmath66 @xmath67 @xmath68 @xmath69 @xmath70 @xmath71      pasa requires only a modest increase in computational resources compared to fixed state aggregation . in relation to time complexity , @xmath44 can be updated in parallel with the rl algorithm s update of @xmath25 ( and the update of @xmath44 would not be expected to have any greater time complexity than the update to @xmath25 if using a standard rl algorithm such as sarsa ) , whilst @xmath31 can be updated at large intervals @xmath72 ( and this update can also be run in parallel ) . applying the mapping @xmath19 to a state",
    "has a very low order of time complexity  @xmath73 for an rl algorithm using pasa compared to @xmath74 for @xmath1 equally sized cells .",
    "hence , pasa involves no material increase in time complexity .",
    "pasa does involve additional space complexity with respect to storing the vector @xmath44 : we must store @xmath1 real values .",
    "if we also store @xmath19 and @xmath50 ( as well as @xmath54 temporarily ) the overall space complexity becomes @xmath75 .",
    "the rl component has space complexity @xmath76 ( reflecting the @xmath77 cell - action pairs ) , so that the introduction of pasa as a pre - processing algorithm will not impact the overall space complexity at all if @xmath78 .",
    "( note also that the space complexity of pasa is independent of @xmath49 . ) regarding sampling efficiency , since methods based on explicitly estimating the bellman error ( or mse ) require a vf estimate ( generated by the rl algorithm ) , as well as information about reward for all actions in the action space , we can expect pasa ( and other unsupervised methods ) to require comparatively less sampling to generate the estimates it requires to update the approximation architecture .",
    "we can reassure ourselves ( somewhat informally ) that pasa will converge ( for fixed @xmath11 ) in the the following sense .",
    "we can set @xmath79 small enough so that the sum of the elements @xmath80 for @xmath81 remains within some interval of size @xmath82 over some arbitrarily large number of iterations with arbitrarily high probability ( after allowing a sufficient number of iterations ) for all possible @xmath31 .",
    "then @xmath36 will eventually remain the same with arbitrarily high probability ( i.e. is `` fixed '' ) .",
    "suppose @xmath83 remains fixed for @xmath84 .",
    "then , since each element @xmath85 ( for @xmath86 ) of @xmath54 calculated at the @xmath87th step of the sequence described in algorithm [ pasa ] will remain within an interval of size @xmath82 with arbitrarily high probability , the value @xmath88 will also remain fixed .",
    "hence , by induction , there exists @xmath79 such that @xmath31 will eventually remain fixed .",
    "we now set out our main result .",
    "the key idea is that , in many important circumstances , which are reflective of real world problems , when following a fixed policy ( even when this is generated randomly ) an agent will have a tendency to spend nearly all of its time in only a small subset of the state space .",
    "we can use this property to our advantage .",
    "it means that by focussing on this small area ( which is what pasa does ) we can eliminate most of the terms which significantly contribute to @xmath27 .",
    "the trick will be to quantify this tendency .",
    "we must make the following assumptions : ( 1 ) @xmath13 is `` close to '' deterministic ( i.e. @xmath13 can expressed by a deterministic transition function @xmath89 , which at each @xmath6 is applied with probability @xmath90 , and an arbitrary transition function @xmath91 which is applied with probability @xmath92 , where @xmath92 is small ; what constitutes `` small '' will be made clearer below ) , ( 2 ) @xmath13 has a uniform _ prior _ distribution , in the sense that , according to our prior distribution for @xmath13 , for each @xmath93 , @xmath94 is independently distributed and @xmath95 for all @xmath96 $ ] , @xmath16 and @xmath97 , and ( 3 ) @xmath11 is also `` close to '' deterministic ( i.e. the probability of _ not _ taking the most probable action is no greater than @xmath92 for each state ) .",
    "we can make the following observation .",
    "if @xmath11 and @xmath13 are deterministic , and we pick a starting state @xmath98 , then the agent will create a path through the state space and will eventually revisit a previously visited state , and will then enter a cycle .",
    "call the set of states in this cycle @xmath99 and denote as @xmath100 the number of states in the cycle .",
    "if we now place the agent in a state @xmath101 ( arbitrarily chosen ) it will either create a new cycle or it will terminate on the path or cycle created from @xmath98 . call @xmath102 the states in the second cycle ( and @xmath103 the number of states in the cycle , noting that @xmath104 is possible ) .",
    "if we continue in this manner we will have @xmath0 sets @xmath105 .",
    "call @xmath106 the union of these sets and denote as @xmath107 the number of states in @xmath106 .",
    "we denote as @xmath108 the event that the @xmath40th path created in such a manner terminates on itself , and note that , if this does not occur , then @xmath109 .    if ( 2 ) holds then @xmath110 and @xmath111 . ) .",
    "the expectation is over the prior distribution for @xmath13 . for a description of the problem and a formal proof see , for example , page 114 of flajolet and sedgewick @xcite .",
    "the variance can be derived from first principles using similar techniques to those used for the mean in the birthday problem . ] supposing that @xmath11 and @xmath13 are no longer deterministic then , if ( 1 ) and ( 3 ) hold , we can set @xmath92 sufficiently low so that the agent will spend an arbitrarily large proportion of its time in @xmath106 .",
    "if ( 2 ) holds we also have the following :    [ genmoments ] @xmath112 and @xmath113 .",
    "we will have : @xmath114    and for the variance : @xmath115 where we have used the fact that the covariance term must be negative for any pair of lengths @xmath116 and @xmath117 , since if @xmath116 is greater than its mean the expected length of @xmath117 must decrease , and vice versa .",
    "[ error ] for all @xmath118 and @xmath119 , there is sufficiently large @xmath0 and sufficiently small @xmath92 such that pasa in conjunction with a suitable rl algorithm will  provided @xmath120 for some @xmath121  generate , with probability no less than @xmath122 , a vf estimate with @xmath123 .    using chebyshev s inequality , and lemma [ genmoments ]",
    ", we can choose @xmath0 sufficiently high so that @xmath124 with probability no greater than @xmath125 .",
    "since @xmath14 is bounded and @xmath126 then for any @xmath119 , @xmath19 and @xmath0 we can also choose @xmath92 so that @xmath27 summed only over states not in @xmath106 is no greater than @xmath127 . we choose @xmath92 so that this is satisfied , but also so that @xmath128 for all elements of @xmath129 . will be bounded from below for all @xmath130 .",
    "this can be verified by more closely examining the geometric distributions which govern the `` jumping '' between distinct cycles . ]",
    "now provided that @xmath131 then each state in @xmath106 will eventually be in its own cell .",
    "the rl algorithm will have no error for each such state so therefore @xmath27 will be no greater than @xmath127 .",
    "the bound on @xmath1 provided represents a significant reduction in complexity when @xmath0 starts to take on a size comparable to many real world problems ( and could make the difference between a problem being tractable and intractable ) .",
    "it also seems likely that the bound on @xmath1 in theorem [ error ] can be improved upon , as the one provided is not necessarily as tight as possible .",
    "conditions ( 1 ) and ( 3 ) are commonly encountered in practice , in particular ( 3 ) which can be taken to reflect a `` greedy '' policy .",
    "condition ( 2 ) can be interpreted as the transition function being `` completely unknown '' ( it seems possible that similar results may hold under other , more general , assumptions regarding the prior distribution ) .",
    "note finally that the result can be extended to exact mse if mse is redefined so that it is also weighted by @xmath11 .",
    "the key message from our discussion is that there are commonly encountered circumstances where unsupervised methods can be very effective in creating an approximation architecture .",
    "however , given their simplicity , they can at the same time avoid the cost ( both in terms of computational complexity , and sampling required ) associated with more complex adaptation methods . in the setting of policy _ improvement _ these advantages have the potential to be particularly important , especially when dealing with large state spaces .",
    "some initial experimentation suggests that the pasa algorithm can have a significant impact on rl algorithm performance in both policy evaluation and policy improvement settings .",
    "the nature of the vf estimate generated by pasa and its associated rl algorithm is that the vf will be well estimated for states which are visited frequently under the existing policy .",
    "this does come at a cost , however , as estimates of the value of deviating from the current policy will be made less accurate .",
    "thus , even though @xmath27 or mse may be low , it does not immediately follow that an algorithm can use this to optimise its policy via standard policy iteration ( since the consequences of deviating from the current policy are less clearly represented ) .",
    "ultimately , however , the theoretical implications of the improved vf estimate in the context of policy iteration are complex , and would need to be the subject of further research ."
  ],
  "abstract_text": [
    "<S> when using reinforcement learning ( rl ) algorithms to evaluate a policy it is common , given a large state space , to introduce some form of approximation architecture for the value function ( vf ) . </S>",
    "<S> the exact form of this architecture can have a significant effect on the accuracy of the vf estimate , however , and determining a suitable approximation architecture can often be a highly complex task . consequently there is a large amount of interest in the potential for allowing rl algorithms to adaptively generate approximation architectures .    </S>",
    "<S> we investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail . </S>",
    "<S> this method is `` unsupervised '' in the sense that it makes no direct reference to reward or the vf estimate . </S>",
    "<S> we introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on - line .    </S>",
    "<S> a common method of scoring a vf estimate is to weight the squared bellman error of each state - action by the probability of that state - action occurring . adopting this scoring method , and assuming @xmath0 states , we demonstrate theoretically that </S>",
    "<S>  provided ( 1 ) the number of cells @xmath1 in the state aggregation architecture is of order @xmath2 or greater , ( 2 ) the policy and transition function are close to deterministic , and ( 3 ) the prior for the transition function is uniformly distributed  our algorithm , used in conjunction with a suitable rl algorithm , can guarantee a score which is arbitrarily close to zero as @xmath0 becomes large . </S>",
    "<S> it is able to do this despite having only @xmath3 space complexity and negligible time complexity . </S>",
    "<S> the results take advantage of certain properties of the stationary distributions of markov chains . </S>"
  ]
}