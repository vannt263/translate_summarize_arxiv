{
  "article_text": [
    "factorization of statistical language models is the task that we resolve the most discriminative model into factored models and determine a new model by combining them so as to provide better estimate to the most discriminative model event .",
    "for instance , a new model for trigram can be obtained by combining the factored models : a unigram model , a bigram model and a trigram model ; a model for pp - attachment @xcite can be obtained by considering both more discriminative models like @xmath0 and less discriminative ones like @xmath1 ; a lexicalized parsing model can be approximated by combining a lexical dependency model and a syntactic structure model @xcite .",
    "the former two examples are usually called backing - off .",
    "therefore , factorization of language models should answer two questions : how to factorize , and how to combine .",
    "most of previous works on language modeling @xcite @xcite focus on sequential model event ( such as n - gram ) , and thus need not to answer the first question because the sequential model event like n - gram gives a natural factorization order : an n - gram has exactly one type of ( n-1)-gram to backoff .",
    "however , for nonsequential model event , we need to specify them both .    in this paper",
    ", we formulated a framework for language model factorization .",
    "we adopt a backing - off lattice to reflect parallel factorization and to define the paths along which a model is resolved into factored models ; we use a mixture model to combine parallel paths in the lattice ; and generalize katz s backing - off method to integrate all the mixture models got by traversing the entire lattice .    based on this framework , we formulate two types of model factorizations that are used in natural language modeling .    the remainder of this paper is organized as follows , we first introduce the backing - off lattice , then explain the mixture model , next formulate the backing - off formula , next describe two types of model factorizations , and finally draw the conclusions .",
    "the backing - off lattice specifies the ways how an event can be `` factorized '' into sub - events .",
    "each lattice node represents a _ set _ of factored events .",
    "each lattice edge connects a parent node to a child node , and represents a factorization manner that factorizes an event in the parent event set into a set of factorized events represented by its child node .",
    "different lattice nodes may have common child .    in most of previous works ,",
    "the backing - off lattice is only a list , in which no node has more than one edges ( backing - off paths ) .",
    "our backing - off lattice is , however , an directed acyclic graph ( dag ) , which means a model event represented by a lattice node might have several factorization manners .",
    "figure  [ backoff - lattice ] shows a backing - off lattice that illustrates how to factorize a dependency event in a bilexical context - free grammar @xcite .",
    "each lattice node is denoted by a solid oval and represents a set of events , each of which is represented by a dotted oval ( if there is only one element in the set , we omit the dotted oval ) .",
    "each lattice edge represents a factorization manner that resolves an event in the parent node ( e.g. , the left dotted oval in node ( 3 ) ) into a set of factored events ( e.g. , the set of events in node ( 4 ) ) .",
    "the backing - off lattice should be tailored in accordance with the requirement of the task that it is applied to .",
    "if it is used for smoothing purpose , we may want to use each slice of the lattice to represent model events with the same specificity , which is less than its previous slice . to combine different resources",
    ", we may want to factorize a complex model whose statistics are unavailable to factored models whose statistics are available .",
    "through the backing - off lattice , a model is factorized recursively into sub - models .",
    "each node can be applied with more than one factorization manners .",
    "we therefore are concerned with the problem of how to approximate the model represented by a lattice node through the models represented by all its children nodes .",
    "for instance , how to approximate the distribution of events in node 1 of figure  [ backoff - lattice ] with factored models represented by nodes 2 and 3 .",
    "we use a mixture model to interpolates all the factored models .",
    "we formulate the mixture model in the following .",
    "let @xmath2 denote a backed - off event ( e.g. , the dependency event in the lattice root in figure [ backoff - lattice ] ) . to get its factored events , we introduce a series of _ factorization function_s : @xmath3 , each of which corresponds to a lattice edge from @xmath2 , and factorize @xmath2 into a set of sub model events @xmath4 , that is , @xmath5    where , @xmath6 is the @xmath7th sub - event among the set of events obtained by factorizing @xmath2 using factorization function @xmath8",
    ".    we can view @xmath8 as a hidden random variable , corresponding to different a factorization manner .",
    "it has a prior distribution : @xmath9 , specifying the confidence of selecting the @xmath10th path to backoff .",
    "we can get , @xmath11 the distribution of @xmath2 can be derived in the following way , @xmath12 from formula  [ 2 ] and [ mixture - formula ] , we get @xmath13    formula  [ mixture - of - factored - model ] shows that the probabilistic model governing event @xmath2 is approximated by a mixture of its factored models @xmath14 using normalized coefficients ( @xmath9 ) .",
    "each coefficient reflects the confidence of selecting a factorization manner .",
    "the sum of @xmath8 should be equal to 1 .",
    "their values can be handcrafted just for simplicity or trained from held - out data using em algorithm @xcite or other numerical methods such as powell s method @xcite .",
    "if we assume that the factored events @xmath15 in the value set of a factorization function are independent of each other , we arrive @xmath16    formula  [ independent ] shows that if we assume the events in the value set of each projection function are independent of each other , the probability of the value set given factorization function is equal to the multiplication of the probability of each event in the set .",
    "we can derive the mixture model for conditional distributions similarly .",
    "let us give an example to illustrates the above idea . in the backing - off lattice shown in figure  [ backoff - lattice ] , for the event in node 1 to be factored into event sets in node 2 and 3 ,",
    "respectively , we need to introduce the following factorization functions ,    1 .",
    "@xmath17= _ factorize @xmath2 into a lexical dependency and a syntactic dependency .",
    "@xmath18= _ factorize @xmath2 into sub - events , each of which describes the dependency between ( parent or dependent ) lexical head and ( dependent or parent ) nonterminal label .",
    "_    these functions will project the event into @xmath19 where each of the two sets contains two factored dependency events .    then based on formula",
    "[ mixture - of - factored - model ] we get @xmath20\\rightarrow nn[june ] ) \\hspace{2 cm }   \\nonumber \\\\",
    "= \\pr(\\phi_{0})\\pr(in \\rightarrow june , in \\rightarrow nn )    \\nonumber\\\\ + \\pr(\\phi_{1})\\pr(in \\rightarrow nn , in \\rightarrow june ) \\end{aligned}\\ ] ]    assume that factored events in the same set are independent of each other , from formula  [ independent ] , we can get @xmath20\\rightarrow nn[june ] ) \\hspace{2.5 cm }   \\nonumber \\\\",
    "= \\pr(\\phi_{0})\\pr(in \\rightarrow june)\\pr(in \\rightarrow nn ) \\nonumber\\\\ + \\pr(\\phi_{1})\\pr(in \\rightarrow nn)\\pr(in \\rightarrow june ) \\end{aligned}\\ ] ]    now , distribution of @xmath21 \\rightarrow nn[june])$ ] is approximated by the mixture of the factored models @xmath22 , @xmath23 , @xmath24 and @xmath25 that are obtained based on a pre - defined backing - off lattice .",
    "we have presented a mixture model to approximate a more discriminative model with less discriminative factored models based on a backing - off lattice and a set of factorization functions .",
    "a mixture model , however , only combines the factored models obtained by factorizing one lattice node and if we traverse the backing - off lattice , we will get a series of mixtures .",
    "we therefore generalize katz s backing - off method @xcite to organize these mixtures by firing correspondent mixture model when backing - off takes place .    @xmath26    where , @xmath27 represents the conditional version of the mixture model in formula  [ mixture - of - factored - model ] .",
    "@xmath28 is a frequency threshold for discounting .",
    "@xmath29 and @xmath30 refer to , in general , two events that adjacently co - occur in a corpus .",
    "the basic idea of this backing - off method is the same as that of katz s .",
    "that is , the backing - off formula has a recursive format . at each step of the recursion",
    ", there are three branches associated with their firing conditions .",
    "if the frequency of the current model event is large enough ( such as greater than @xmath28 , katz used the value of 5 for @xmath28 ) , the maximum - likelihood estimator ( mle ) is used .",
    "if the occurrence frequency is within the range of @xmath31 $ ] , the mle probabilities are discounted in some manner so that some probability mass is reserved for those unseen events . if the model event never occurs in the training data , we use the estimates from the factored model events .",
    "the difference therefore lies in the combination of estimates of factored events . in traditional backing - off methods",
    ", there is only one backing - off path to go when the backing - off condition is satisfied .",
    "for example , an n - gram only has exactly one ( n-1)-gram to be backed - off . however , in our case , we have more than one backing - off paths to go through .",
    "none is a branch of another .",
    "then the mixture model obtained in the previous section is embedded here .",
    "@xmath32 are for normalization and can be computed according to @xcite .",
    "@xmath33 is also for normalization .",
    "it is computed from the amount of reserved probability mass for unseen events .",
    "@xmath33 is a function of @xmath30 because @xmath30 is the given event of a conditional probability , and each conditional probability should satisfy the normalization requirement .",
    "it is computed similarly to that in katz original paper :    @xmath34",
    "now that we have presented a framework that allows a model event to be factorized along more than paths and combines different paths in a backing - off formula , we now formulate two types of model factorization that are used in natural language modeling .",
    "we first introduce some notations .    let a matrix @xmath35 of random variables represent a linguistic object that simultaneously expresses two types of information in its row and column directions .",
    "for example , matrix , @xmath36\\end{aligned}\\ ] ]    denotes two lexical dependencies , each of which is the translation the other .",
    "it expresses the dependency relationship information in the row direction , and translation relationship information in the column direction .",
    "let @xmath37 , and @xmath38 and @xmath38 have identical distributions for simplicity of explanation .",
    "they need nt have to be identical in general . and matrix @xmath39 need not to have only row and column , but might be like @xmath40 . ]",
    "@xmath41 , we want to determine @xmath42 using its factored models .",
    "we can either factorize both @xmath43 and @xmath38 synchronously , or factorize only the conditioning event @xmath38 , which results in two types of factorizations for different tasks : _ synchronous factorization _ , and _",
    "asynchronous factorization_.      in synchronous factorization , both @xmath43 and @xmath38 are factorized in the same manner according to some correspondence assumption , and the factored models determines the marginal information of the entire model .",
    "based on formula  [ mixture - formula ] ( the mixture model ) , and the assumption that @xmath43 and @xmath38 are in sync with with each other on row ( or column ) , we formulate the synchronous factorization as follows :    @xmath44    where ,    * @xmath45 projects @xmath43 and @xmath38 on row , respectively , and therefore results in row vectors : 1 .   @xmath46= @xmath47 + the set of row vectors of @xmath43 2 .",
    "@xmath48= @xmath49 + the set of row vectors of @xmath38 * @xmath50 projects @xmath43 and @xmath38 on column , respectively , and therefore results in column vectors : 1 .",
    "@xmath51= @xmath52 + the set of column vectors of @xmath43 2 .",
    "@xmath53= @xmath54 + the set of column vectors of @xmath38    factored models @xmath55 and",
    "@xmath56 will be further factorized by other factorization functions , and the results of these factorization functions constitute the backing - off lattice .",
    "all these factored models are then combined by the backing - off formula .",
    "let us give an example .",
    "let @xmath57\\end{aligned}\\ ] ] and let @xmath58\\end{aligned}\\ ] ]    under factorization manner @xmath45 , and from formula  [ synchronous - factorization ] , [ depend ] and [ parent ] we get @xmath59 &            \\left[\\begin{array}{c}flights \\\\",
    "vuelos\\end{array}\\right ] \\\\",
    "\\end{array}\\right ) \\nonumber \\\\ & = & \\pr(\\phi_{row } ) \\pr\\left (                   \\mathcal{s}_{row}^{\\mathcal{a}}|\\mathcal{s}_{row}^{\\mathcal{b } }                       \\right ) \\end{aligned}\\ ] ] where @xmath60,\\left[sin\\right ]                                    \\right\\ } \\\\",
    "\\mathcal{s}_{row}^{\\mathcal{b } } & = & \\phi_{row}(\\mathcal{b } )                                   = \\left\\ {                      \\left[flights\\right],\\left[vuelos\\right ]                                    \\right\\}\\end{aligned}\\ ] ] and @xmath61 ( normalized ) .",
    "if we assume that an element in @xmath46 is only dependent on the correspondent element in @xmath48 , we get ,    @xmath62    formula  [ 17 ] and [ 20 ] indicate that , by synchronous factorization , a bilingual lexical dependency model like that in @xcite is approximated by two factored lexical dependency models , each of which corresponds to one language .",
    "the factorization of a bilexical context - free grammar into a lexical dependency model and a syntactic structure model in @xcite is actually synchronous factorization .",
    "synchronous factorization is usually used for information combination in the cases that we only have the statistics of those factored models and want to use them to approximate a more complex model ; or that we want to simplify a complex model into factored models to gain efficiency ( e.g. ,   * ? ? ?",
    "another type of factorization that is frequently used in statistical language modeling is asynchronous factorization , where only the conditioning event of the conditional probability @xmath63 is recursively factorized while keeping the conditioned event fixed .",
    "the following formula describes the idea .    @xmath64    where @xmath8",
    "= `` drop the @xmath10th element in matrix @xmath38 '' , and @xmath65 is the number of the elements in matrix @xmath38 .",
    "if we further factorize model @xmath66 , matrix @xmath38 will be a partial matrix that contains a part of elements of the original matrix .",
    "formula  [ asynchronous ] indicates that the original model is recursively factorized into sub - models , and each factorization recursion step has @xmath65 factorization manners , each of which only drops one element from @xmath38 .    in the following , we give an example to illustrate the idea of asynchronous factorization . in pp - attachment ,",
    "suppose we want to factorize model @xmath67 , which determines the probability of the attachment of preposition phrase `` from research '' to the noun `` revenue '' instead of to the verb `` is '' .",
    "based on formula  [ asynchronous ] , we can get ,    @xmath68    where @xmath69 refers to the factorization function that drops @xmath70 from the conditioning event of the left hand side model . and the factored models on the right hand side can be further factorized by continuing to traversing the backing - off lattice .",
    "in contrast to that synchronous factorization is used for information combination , asynchronous factorization is usually used for smoothing purpose .    in practice",
    ", there might be a compromise between the above two , where we factorize both the conditioned and conditioning events , but not in a synchronous manner . and",
    "matrices @xmath43 and @xmath38 need not to have the same number of rows and columns .",
    "@xcite puts forward a method , which actually is asynchronous factorization , for backing - off of models of prepositional phrase attachment , providing a way to mixing the frequencies of different backing off choices at certain recursion step by dividing the sum of frequencies of all the more discriminative model events by those of less discriminative ones if the sum of the frequencies of the less discriminative ones are greater than zero , otherwise , backing - off continues on .",
    "one characteristic of this method is that if one of the less discriminative model events has non - zero frequency , the backing - off terminates , no matter whether other events in the same backing - off level are zero or not , whereas the mixture model we introduced to combine parallel backing - off paths is able to to make those zero count event further backoff so that they still can contribute to the final result .",
    "and we think this is necessary when we use the backing - off framework for information combination .",
    "it turned out that @xcite were independently working on some similar ideas .",
    "they introduced factored models and also generalized the backing - off framework to handle parallel backing - off paths .",
    "the differences between their work and ours are ( 1 ) their factored models can actually be categorized into the asynchronous factorization type , where only the conditioning matrix ( the feature vector in their paper ) is factorized ; ( 2 ) we also formulated the synchronous factorization type , where both the conditioned and conditioning matrices are factorized synchronously . and we showed that this is useful for combining different information sources ; ( 3 ) we use a mixture model to combine parallel paths while they selected the the path with the maximum value . we think that combining the contribution of each backing - off path is useful when we want to combine different information resources ; ( 3 ) in our framework , the result of each factorization function ( backing - off path ) is a set of events ( see formula  [ project - function ] ) , not merely one event . and",
    "this is usefull when we do the kind of factorization like @xcite using formula  [ independent ] .",
    "we have presented a framework for language model factorization .",
    "we adopt a backing - off lattice to reflect parallel factorizations and to define the paths along which a model is resolved into factored models , we use a mixture model to combine parallel paths in the lattice , and generalize katz s backing - off method to integrate all the mixture models got by traversing the entire lattice ."
  ],
  "abstract_text": [
    "<S> factorization of statistical language models is the task that we resolve the most discriminative model into factored models and determine a new model by combining them so as to provide better estimate . </S>",
    "<S> most of previous works mainly focus on factorizing models of _ sequential _ events , each of which allows only one factorization manner . </S>",
    "<S> to enable _ parallel _ factorization , which allows a model event to be resolved in more than one ways at the same time , we propose a general framework , where we adopt a backing - off lattice to reflect parallel factorizations and to define the paths along which a model is resolved into factored models , we use a mixture model to combine parallel paths in the lattice , and generalize katz s backing - off method to integrate all the mixture models got by traversing the entire lattice . based on this framework </S>",
    "<S> , we formulate two types of model factorizations that are used in natural language modeling . </S>"
  ]
}