{
  "article_text": [
    "with advancements in data collection and video recording methods , high - volume datasets of animal groups , such as fish schools @xcite , bird flocks @xcite , and insect and bacterial swarms @xcite , are now ubiquitous . however , analyzing these datasets is still a nontrivial task , even when individual trajectories of all members are available . a desirable step that may ease the experimenter s task of locating events of interest is to identify coarse observables @xcite and behavioral measures @xcite as the group navigates through space . in this context ,",
    "nonlinear dimensionality reduction ( ndr ) offers a large set of tools to infer properties of such complex multi - agent dynamical systems .",
    "traditional dimensionality reduction ( dr ) methods based on linear techniques , such as principal components analysis ( pca ) , have been shown to possess limited accuracy when input data is nonlinear and complex @xcite .",
    "dr entails finding the axes of maximum variability @xcite or retaining the distances between points @xcite .",
    "multi dimensional scaling ( mds ) with euclidean metric is another dr method which attains low - dimensional representation by retaining the pairwise distance of points in low dimensional representations @xcite .",
    "however , euclidean distance calculates the shortest distance between two points on a manifold instead of the genuine manifold distance , which may lead to difficulty of inferring low - dimensional embeddings .",
    "the isometric mapping algorithm ( isomap ) resolves the problem associated with mds by preserving the pairwise geodesic distance between points @xcite ; it has recently been used to analyze group properties in collective behavior , such as the level of coordination and fragmentation @xcite . within isomap , however , short - circuiting @xcite created by faulty connections in the neighborhood graph , manifold non - convexity @xcite and holes in the data @xcite can degrade the faithfulness of the reconstructed embedding manifold .",
    "diffusion maps @xcite have also been shown to successfully identify coarse observables in collective phenomena @xcite that would otherwise require hit - and - trial guesswork @xcite . beyond isomap and diffusion maps , the potential of other ndr methods to study collective behavior is largely untested .",
    "for example , kernel pca ( kpca ) requires the computation of the eigenvectors of the kernel matrix instead of the eigenvectors of the covariance matrix of the data @xcite but this is computationally expensive @xcite .",
    "local linear embedding ( lle ) embeds high - dimensional data through global minimization of local linear reconstruction errors @xcite .",
    "hessian lle ( hlle ) minimizes the curviness of the higher dimensional manifold by assuming that the low - dimensional embedding is locally isometric @xcite .",
    "laplacian eigenmaps ( le ) perform a weighted minimization ( instead of global minimization as in lle ) of the distance between each point and its given nearest neighbors to embed high dimensional data @xcite .",
    "iterative ndr approaches have also been recently developed in order to bypass spectral decomposition which is common in most of ndr methods @xcite .",
    "curvilinear component analysis ( cca ) employs a self - organized neural network to perform two tasks , namely , vector quantization of submanifolds in the input space and nonlinear projection of quantized vectors onto a low dimensional space @xcite .",
    "this method minimizes the distance between the input and output spaces .",
    "manifold sculpting ( ms ) transforms data by balancing two opposing heuristics : first , scaling information out of unwanted dimensions , and second , preserving local structure in the data .",
    "the ms method , which is robust to sampling issues , iteratively reduces the dimensionality by using a cost function that simulates the relationship among points in a local neighborhoods@xcite .",
    "the local spline embedding ( lse ) is another ndr method which embeds the data points using splines that map each local coordinate into a global coordinate of the underlying manifold by minimizing the reconstruction error of the objective function @xcite .",
    "this method reduces the dimensionality by solving an eigenvalue problem while the local geometry is exploited by the tangential projection of data .",
    "lse assumes that the data is not only unaffected by noise or outliers , but also , sampled from a smooth manifold which ensures the existence of a smooth low dimensional embedding .    due to the global perspective of all these methods ,",
    "none of them provide sufficient control over the mapping from the original high - dimensional dataset to the low - dimensional representation , limiting the analysis in the embedding space . in other words ,",
    "the low - dimensional coordinates are not immediately perceived as useful , whereby one must correlate the axes of the embedding manifold with selected functions of known observables to deduce their physical meaning @xcite . in this context , a desirable feature of dr that we emphasize here is the regularity in the spatial structure and range of points on the embedding space , despite the presence of noise .    with regard to datasets of collective behavior ,",
    "nonlinear methods have limited use for detailed analysis at the level of the embedding space .",
    "this is primarily because the majority of these methods collapse the data onto a lower dimensional space , whose coordinates are not guaranteed to be linear functions of known system variables @xcite . in an idealized simulation of predator induced mobbing @xcite , a form of collective behavior where a group of animals crowd around a moving predator , two degrees of freedom are obvious , namely , the translation of the group and the rotation about the predator ( center of the translating circle ) .",
    "this two - dimensional manifold is not immediately perceived by isomap , even for the idealized scenario presented in figure [ fig : predator_mobbing_isomap ] , where a group of twenty individuals rotate about a predator moving at a constant speed about a line bisecting the first quadrant . specifically , the algorithm is unable to locate a distinct elbow in the residual variance vs. dimensionality curve , notwithstanding substantial tweaking of the parameters .",
    "thus , the inferred dimensionality is always 1 ( fig .",
    "[ fig : predator_mobbing_isomap]b ) . for a two - dimensional embedding ( fig .",
    "[ fig : predator_mobbing_isomap]c ) , visual comparison of the relative scale of the axes indicates that the horizontal axis represents a greater translation than the vertical axis .",
    "it is likely that the horizontal axis captures the motion of the group along the translating circle .",
    "the vertical axis could instead be associated with ( i ) motion about the center of the circle , or ( ii ) noise , which is independent and identically distributed at each time step .",
    "the ambiguity in determining the meaning of such direction indicates a drawback of isomap in provide meaningful interpretations of the low - dimensional coordinates .",
    "angle ) , axes @xmath0 , @xmath1 generally represent coordinates for the @xmath2-th agent .",
    "( b ) scaled residual variance of candidate low - dimensional embeddings produced by isomap using different nearest neighbor values @xmath3 ( green - circle , brown - square , and black - triangle ) , and ( c ) two - dimensional representation of the data for five nearest neighbors ( black - triangle ) .",
    "green and blue crosses mark the start and end points of the trajectory . ]",
    "an alternative approach to dr , one that does not require heavy matrix computations or orthogonalization , involves working directly on raw data in the high - dimensional space @xcite .",
    "we propose here a method for dr that relies on geodesic rather than euclidean distance and emphasizes manifold regularity .",
    "our approach is based on a spline representation of the data that allows us to control the expected manifold regularity .",
    "typically , this entails conditioning the data so that the lower dimensions are revealed .",
    "for example , in @xcite , raw data is successively clustered through a series of lumping and splitting operations until a faithful classification of points is obtained . in @xcite ,",
    "one - dimensional parameterized curves are used to summarize the high - dimensional data by using a property called self - consistency , in which points on the final curve coincide with the average of raw data projected on itself . in a similar vein , we construct a pm of the high - dimensional data using cubic smoothing splines . summarizing the data using splines to construct a pm of the data has two advantages : ( i ) it respects the data regularity by enabling access to every point of the dataset , and ( ii ) it gives direct control over the amount of noise rejection in extracting the true embedding through the smoothing parameter .",
    "we illustrate the algorithm using the standard swiss roll dataset , typically used in ndr methods .",
    "then we validate the method on three different datasets , including an instance of collective behavior similar to that in figure [ fig : predator_mobbing_isomap ] .",
    "this paper is organized as follows .",
    "section [ sec : algorithm ] describes the three main steps of the algorithm using the swiss roll dataset as an illustrative example . in section [ sec : examples ] , we validate and compare the algorithm on a paraboloid , a swiss roll with high additive noise , and a simulation of collective animal motion .",
    "section [ sec : performance ] analyzes the performance of the algorithm by comparing topologies in the original and embedding space as a function of the smoothing parameter , noise intensity , and data density .",
    "we conclude in section [ sec : conclusion ] with a discussion of the algorithm performance and ongoing work .",
    "individual steps of the algorithm are detailed in [ sec : algorithms ] .",
    "computational complexity is discussed in [ sec : complexity ] .",
    "dimensionality reduction is defined as a transformation of high - dimensional data into a meaningful representation of reduced dimensionality @xcite . in this context , a @xmath4-dimensional input dataset is embedded onto an @xmath5-dimensional space such that @xmath6 .",
    "the mapping from point @xmath7 in the embedding space to the corresponding point @xmath8 in the original dataset is @xmath9    differently from other approaches , the proposed dr algorithm is based on a pm of the raw dataset , obtained by constructing a series of cubic smoothing splines on slices of data that are partitioned using locally orthogonal hyperplanes .",
    "the resulting pm summarizes the data in the form of a two - dimensional embedding , that is , @xmath10 .",
    "the pm finding algorithm proceeds in three steps : clustering , smoothing , and embedding .",
    "the clustering step partitions the data using reference points into non - overlapping segments called slices .",
    "this is followed by locating the longest geodesic within each slice .",
    "a cubic smoothing spline is then fitted to the longest geodesic .",
    "a second set of slices , and corresponding splines , are created in a similar way resulting in a two - dimensional pm surface .",
    "the pm is constructed in the form of intersection points between pairs of lines , one from each reference point .",
    "any point in the input space is projected on the pm in terms of the intersection points and their respective distances along the cubic splines .",
    "table [ tab : list_of_symbols ] lists the notation used in this paper .",
    ".nomenclature [ cols=\"^,<\",options=\"header \" , ]     [ tab : comparison ]              the three - dimensional paraboloid ( fig .",
    "[ fig : paraboloid_embedding]a ) is generated in the form of 2000 points using @xmath11 where @xmath12 is a point in the three - dimensional space with @xmath13 $ ] , and @xmath14 is a noise variable sampled from a uniform distribution ranging between -0.05 and 0.05 . since the data is generated along the @xmath15 axes , the two reference points are selected along the same just outside the range for each value .",
    "we run the clustering algorithm with the value of @xmath16 for each reference point to generate corresponding sets of clusters with algorithm [ alg : cluster ] .",
    "next , we run the smoothing algorithm with a smoothing parameter @xmath17 to produce a set of representative splines for the data ( fig . [",
    "fig : paraboloid_embedding]b ) . finally , we embed the manifold by first finding the intersection points and , then , computing the distance of each point from the axis splines ( fig . [",
    "fig : paraboloid_embedding]c ) .",
    "the two - dimensional embedding manifold generated using our algorithm preserves the topology of the data as shown in figure [ fig : paraboloid_embedding]c .",
    "furthermore , the axes in the embedding manifold retain the scale of the original data in terms of the distance between individual points . for a comparison ,",
    "we run other ndr methods on this data set with parameters specified in table [ tab : comparison ] ( fig.[fig : paraboloid_comparison ] ) .",
    "results show that except isomap and the proposed method , none of the ndr methods are able to preserve the two - dimensional square embedding after dimensionality reduction . between isomap and the pm approach ,",
    "the pm approach retains the scale of the manifold as well .      in a second example",
    ", we run our algorithm on a 2500 point three - dimensional noisy swiss roll dataset with high additive noise ( fig .",
    "[ fig : noised_swiss_roll_embedding]a ) .",
    "the dataset is generated using @xmath18 where @xmath19 , \\ \\phi \\in [ -2,2]$ ] , and @xmath20 is a noise variable sampled from a uniform distribution ranging between -0.4 and 0.4 .",
    "two reference points are chosen along two major principal component directions @xmath21 and @xmath22 at a distance of @xmath23 and @xmath24 units away from the data centroid @xmath25 .",
    "we run the clustering algorithm with the value of @xmath26 for each reference point , however , due to folds and therefore gaps in the data , the clusters along @xmath22 are further split into 42 subclusters .",
    "the value of smoothing parameter @xmath27 .",
    "figure [ fig : noised_swiss_roll_embedding]b and [ fig : noised_swiss_roll_embedding]c show the smoothing splines in a grid pattern that are used to embed the manifold , and the resulting embedding . for a comparison ,",
    "we run other ndr methods on this data set with parameters specified in table [ tab : comparison ] ( fig .",
    "[ fig : noised_swiss_roll_comparison ] ) .",
    "comparison between existing ndr methods show that while only isomap and kpca are able to flatten the swiss roll into two dimensions , isomap preserves the general rectangular shape of the flattened swiss roll .",
    "a majority of ndr methods suppress one component out of the two principal components in the raw data revealing an embedding that resembles a one - dimesional curve .",
    "in contrast , the pm approach is able to embed the swiss roll into a rectangle while preserving the scale .",
    "we note that by changing the value of the smoothing parameter we control for the level of noise - rejection in the original data , a feature that is not available in the existing dr algorithms @xcite .",
    "although the splines form illegal connections , our algorithm is still able to preserve the two - dimensional embedding ; this is due to the inherent embedding step which is able to overcome such shortcuts while computing the low - dimensional coordinates .",
    "furthermore , we note that our method is able to better exclude noisy data from the embedding automatically in the form of outliers ; neighborhood based algorithms would be mislead to attempt to include these as valid points @xcite .",
    "for example , the two - dimensional embedding obtained using isomap shows an unforeseen bend demonstrating that , despite fewer outliers , the general topological structure is compromised more heavily .",
    "this global anomaly is a direct consequence of the isomap s attempt to include noisy points in the embedding ; points that are otherwise ignored by the principal manifold .                  in a third example of low - dimensional embedding",
    ", we generate a dataset representing a form of collective behavior .",
    "we simulate point - mass particles revolving on a translating circle to represent a form of behavior called predator induced mobbing @xcite . in prey animals , predator mobbing , or crowding around a predator ,",
    "is often ascribed to the purpose of highlighting the presence of a predator @xcite . in our idealized model",
    ", we assume that @xmath28 agents ( @xmath29 ) are revolving on the circumference of a half circle whose center marks the predator and translating on a two - dimensional plane which is orthogonal to the axis of the revolution . to generate a two - dimensional trajectory with @xmath30 revolutions around a translating center",
    "we represent the two - dimensional position of an agent @xmath2 at @xmath3 , @xmath31=\\begin{bmatrix}y_{2i-1}[k ] , & y_{2i}[k]\\end{bmatrix}^{\\mathrm{t}}$ ] , @xmath32 \\in \\mathbb{r}^2 $ ] @xmath33=s_i[k]\\begin{bmatrix}\\cos(2\\pi \\rho k / n_k",
    "+ \\pi i / n ) \\\\                      \\sin(2\\pi \\rho",
    "k / n_k + \\pi i / n )                      \\end{bmatrix } + k\\bs{\\mathrm{v}}_p[k]+ \\bs{\\epsilon}[k ] ,   \\label{eqn : predator_mobbing}\\ ] ] where @xmath34=[1 , 1]^{\\mathrm{t}}/80 $ ] is the velocity of the predator @xmath35 = 3 $ ] is the speed of the agent @xmath2 , @xmath36 is the number of total time - steps , and @xmath37 \\sim\\mathbb{n}(\\boldsymbol{0},\\boldsymbol{1}/100)$ ] is the gaussian noise variable with mean @xmath38 and standard deviation @xmath39 .",
    "the first term of eqn .",
    "( [ eqn : predator_mobbing ] ) assigns the relative positions of @xmath28 agents onto equally spaced points on the circumference of a half circle of units @xmath35 $ ] , and the second term gives the velocity of the virtual center ( predator ) of the translating circle .",
    "the agent - dependent phase @xmath40 creates spatial separation of individual members around the virtual center .",
    "figure [ fig : predator_mobbing_isomap]a shows the resulting trajectories for 2000 time - steps with @xmath41 in a @xmath4-dimensional space ( note that @xmath42 in this case , where @xmath28 is the number of agents ) .",
    "the interaction between the agents is captured in the form of noise @xmath43 .",
    "a high value means that the agents interact less with each other .",
    "to cluster the dataset , we use @xmath44 .",
    "the value of the smoothing parameter @xmath17 .",
    "the embedding manifold is shown in fig.[fig : predator_mobbing_algorithm2]b with the start and end points of the trajectory .",
    "for comparison , we also run the isomap algorithm on this dataset with a neighborhood parameter value set to 10 . to ensure that the embedding is consistent and robust to our choice of neighborhood parameter ,",
    "we run the algorithm with neighborhood parameters 5 and 15 and verify that the output is similar .",
    "figure [ fig : predator_mobbing_algorithm2]c shows the resulting two - dimensional embedding coordinates each as a function of time .    unlike the paraboloid and the noisy swiss roll , the predator mobbing dataset represents a dynamical system with a characteristic temporal evolution . therefore , we use cross - correlation between the embedding of each method and a reference ground truth to compare the performance of our algorithm to isomap ( fig . [",
    "fig : predator_mobbing_isomap ] ) . for ground truth",
    ", we transform the original data by a clockwise rotation of @xmath45 such that @xmath46=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\ 1 & 1\\end{bmatrix}\\bs{r}_i[k]\\ ] ] and use the position of the first agent @xmath47 = \\begin{bmatrix}\\tilde{r}_{1,1}[k ] , & \\tilde{r}_{1,2}[k]\\end{bmatrix}^{\\mathrm{t}}$ ] ( this value serves as a descriptive global observable of the predator mobbing agents separated by a constant phase , than , for example , the group centroid , where the revolving motion is suppressed due to the instantaneous two - dimensional arrangement ) .",
    "we then cross - correlate each component of the two - dimensional embedding signal @xmath48 with the corresponding component of the ground - truth and add the two values .",
    "we compute correlations of the embedded data of our algorithm and isomap with the ground truth along the oscillating directions as @xmath49 and @xmath50 .",
    "thus , the correlation of our algorithm with the ground truth is fifty times higher than the correlation of isomap with the ground truth .",
    "correlation @xmath51-values under our method and isomap with the ground truth are @xmath52 and @xmath53 respectively , showing that under 95% of confidence interval , the values from our method are related to the ones available from the ground truth .",
    "figure [ fig : predator_mobbing_isomap ] visually demonstrates the same where @xmath54 from the principal surface follows the same trend as @xmath55 .",
    "in addition , the range of values of the embedding coordinates is similar to the original values .",
    "to analyze the performance of the algorithm , we use a distance - preserving metric between the original and the embedding data in terms of adjacency distance of graph connectivity . for both the original and the embedding data , we first search @xmath3-nearest neighbors through the algorithm given in @xcite and then produce two individual weighted graphs based on points connectivity in the data sets .",
    "a graph constructed through nearest neighbor search is a simple graph that does not contain self - loops or multiple edges .",
    "we compute the adjacency distance matrix @xmath56 for the original data as @xmath57 and the adjacency matrix @xmath58 for the embedding data as @xmath59 here , @xmath60 and @xmath61 are the @xmath62th entries of the adjacency matrices @xmath56 and @xmath58 , and @xmath63 is the euclidean distance between nodes @xmath2 and @xmath64 . for @xmath65 points ,",
    "this metric is computed as the normalized number of pairwise errors between entries of the adjacency distance matrices as @xmath66 equation ( [ eqn : pairwise_error ] ) is a modification of the structure preserving metric in @xcite where the connectivity is replaced by the distance and the denominator @xmath67 with the number of edges @xmath68 .",
    "we study the dependence of @xmath69 on the smoothing parameter , the noise in the dataset , and the data density .",
    "the primary input to the algorithm described in this paper is the smoothing parameter that controls the degree of fitness and noise - rejection by the cubic smoothing splines .",
    "figure [ fig : analysis_delta_p ] shows the dependence of the distance preservation error @xmath70 in term of adjacency distance on the smoothing parameter @xmath51 . using a noise - free swiss roll dataset comprising 3000 points in three dimensions , we vary the smoothing parameter between 0 and 1 with an increment of 0.01 while keeping the location of reference points and cluster width consistent .",
    "the error dependence shows a linear decay with @xmath71-value - value ( coefficient of determination ) ranges between 0 and 1 , and measures the goodness of the fit so that 1 is the best while 0 is the worst . for a set of points",
    "@xmath72 associated with fitted values @xmath73 , coefficient of determination or @xmath71-value is defined as , @xmath74 where @xmath75 . ] of 0.9728 and becomes nearly constant at a @xmath76 , beyond which the change in error for an increase in the value of @xmath51 is negligible",
    ". a higher value of @xmath51 improves the data - fit but a low value improves smoothness . since the graph connectivity is dependent on the relative point configuration , it is expected that @xmath70 will rise with increasing smoothness . on the other hand",
    ", the error dependence shows that a given degree of smoothing can be attained beyond the value of @xmath76 without an accompanied increase error .",
    "thus , as a design parameter , the value @xmath17 may be used as a starting point for all datasets to investigate the embedding manifold .     with respect to @xmath51 and corresponding linear fit for a noise free swiss - roll .",
    "@xmath70 is computed using eq .",
    "( [ eqn : pairwise_error ] ) on raw and embedded data . ]      to analyze the change in pairwise error with increase in noise , we create multiple same - sized swiss - roll datasets generated using ( [ eqn : noisy_sr ] ) with noise value @xmath77 ranging between 0 and 1.035 with an increment of 0.015 .",
    "the number of points in the dataset are 3000 and the value of smoothing parameter @xmath17 .",
    "figure [ fig : analysis_delta_noise ] shows the variation of @xmath70 with respect to @xmath77 .",
    "the error @xmath70 increases quadratically with @xmath71-value 0.9983 when noise increases .",
    "this relation shows that , the error of the embedding is affected quadratically by the intense of the associated noise in the data .     with respect to noise ( @xmath77 ) and corresponding quadratic fit @xmath70",
    "is computed over adjacency matrices for the raw data and embedded data by eq .",
    "( [ eqn : pairwise_error ] ) . ]      to analyze the dependence of @xmath70 on data density we sub - sample the swiss roll dataset such that the number of points are systematically decreased .",
    "the amount of noise is set at @xmath78 and a sequence of data sets are generated with number of points between 500 and 3500 with increment of 40 .",
    "the value of the smoothing parameter @xmath51 is fixed at 0.9 .",
    "we see that the pairwise error ( @xmath70 ) decreases exponentially with @xmath71-value @xmath79 from an initial high value ( fig .",
    "[ fig : analysis_delta_sparsity ] ) .     with respect to number of points ( @xmath65 ) and corresponding exponential fit with @xmath17 and @xmath78 .",
    "@xmath70 is computed over adjacency matrices for the raw data and embedded data by eq .",
    "( [ eqn : pairwise_error ] ) . ]",
    "in this paper , we introduce a pm finding dimensionality reduction algorithm that works directly on raw data by approximating the data in terms of cubic smoothing splines .",
    "we construct the splines on two sets of non - overlapping slices of the raw data created using parallel hyperplanes that are known distance apart .",
    "the resulting pm is a grid - like structure that is embedded on a two - dimensional manifold .",
    "the smoothness of the splines can be controlled via a smoothing parameter that selectively weighs fitting error versus the amount of noise rejection .",
    "the pm is represented by the intersection points between all pairs of splines , while the embedding coordinates are represented in the form of distance along these splines .",
    "the spline representation improves regularity of an otherwise noisy dataset .",
    "we demonstrate the algorithm on three separate examples including a paraboloid , a noisy swiss roll , and a simulation of collective behavior . upon embedding these datasets on two - dimensional manifolds , we find that in the case of the paraboloid , the algorithm is successful in preserving the topology and the range of the data . in the case of the noisy swiss roll , we find that the algorithm is able to reject high noise , thereby preserving the regularity in the original dataset .",
    "unlike isomap , our algorithm for finding pm is able to maintain a general structure . in other words , the algorithm fails gracefully giving the user enough indication of the trend of increasing error differently than isomap , which fails abruptly and dramatically @xcite .",
    "this property has a distinct advantage over other methods that are more sensitive to input parameters and therefore make it difficult to identify when the algorithm is embedding correctly . in contrast , the proposed algorithm on the swiss - roll dataset shows that in its current form , is inefficient to embed the dataset with holes faithfully onto a two - dimensional space , since it is highly focused on revealing a smooth underlying manifold .",
    "we used a simulation of twenty particles moving in a translating circle as a representation of a two - dimensional data embedded in a forty - dimensional state . in collective behavior , this form of motion approximates predator mobbing .",
    "cross - correlation between the embedded signals and the ground - truth available in the form of a transformed signal shows that our algorithm is able to replicate the time - history on a low - dimensional space than for example isomap .",
    "furthermore , we see that the range of axes in the embedding manifold available from our algorithm is closer to that in the original data .",
    "the analysis of the distance preserving ability of our algorithm with respect to the smoothing parameter reveals that an increase in the smoothing parameter reduces the error in the structure .",
    "this is expected because a high value of @xmath51 implies that the cubic smoothing spline weights data fit more than smoothing .",
    "however , we also note that the amount of error saturates near @xmath76 , showing that values close to but not equal to one may also be used to represent the data .",
    "we observe a similar trend with respect to the amount of noise introduced in a swiss - roll dataset , where we find that the amount of error increases quadratically with an increase in noise .",
    "finally , we find that the algorithm is able to work on sparse datasets up to a representation with five hundred points .",
    "operating and representing raw data for dimensionality reduction provides an opportunity for extracting true embeddings that preserve the structure as well as regularity of the dataset . by demanding a two - dimensional embedding",
    "we also provide a useful visualization tool to analyze high - dimensional datasets .",
    "finally , we show that this approach is amenable to high - dimensional dynamical systems such as those available in dataset of collective behavior .",
    "kelum gajamannage and erik m. bollt have been supported by the national science foundation under grant no .",
    "cmmi- 1129859 .",
    "sachit butail and maurizio porfiri have been supported by the national science foundation under grants nos .",
    "cmmi- 1129820 .",
    "here , we state the algorithms of dimensionality reduction by principal manifold by three components as , clustering , smoothing , and embedding .    compute the mean @xmath80 of the input data @xmath81 . perform principal component analysis ( pca ) @xcite on the input data matrix @xmath82 to obtain two largest principal components @xmath83 and @xmath84 , where @xmath85 is the @xmath4-dimensional coefficients and @xmath86 is the eigenvalue of the @xmath2-th pc for @xmath87 in order to assure that two reference points are in the directions of two pcs from the mean , compute the first reference point @xmath88 , and the second @xmath89 .",
    "segment the line joining the reference points @xmath90 and the point @xmath91 into @xmath92 parts with equal width using the ratio formula given in equation ( [ eqn : line_partition ] ) to obtain a set of points @xmath93 @xcite , where @xmath94 and @xmath95 for @xmath96 , choose data between hyperplanes , which are made through points @xmath97 and @xmath98 normal to the line joining @xmath99 and @xmath100 , into the cluster @xmath101 by satisfying the inequalities in ( [ eqn : enq_clustering ] ) .",
    "compute the neighborhood graph using range - search with the distance set as the cluster width @xmath102 .",
    "compute the longest geodesic @xmath103 using dijkstra s algorithm @xcite .",
    "@xmath103 is a set of points , where each point is in @xmath104 . for points in @xmath103 ,",
    "use ( [ eqncss ] ) and the value of the smoothing parameter @xmath51 to produce a smoothing spline @xmath105 representation for data in the cluster @xmath101 .    approximate the minimum distance between the two splines after discretizing each spline .",
    "choose the midpoint between the two closest points of the splines as the intersection point @xmath106 .",
    "pick a random intersection point as the origin @xmath107 , and smoothing splines corresponding to the origin as axis splines .",
    "find the closest intersection point @xmath108 for @xmath109 in terms of euclidean distance by equation ( [ eqn : closest_intersection ] ) , and the tangents to the splines at this point are called the local spline directions @xmath110 .",
    "compute the distances @xmath111^{\\mathrm{t}}$ ] from the manifold origin @xmath112 to @xmath108 along axis splines by equation ( [ eqn : axis_distance ] ) .",
    "project the vector @xmath113 onto the local coordinate system created using the spline directions as equation ( [ eqn : local_distance ] ) at the intersection point @xmath108 and find the projections @xmath114^{\\mathrm{t}}$ ] .",
    "the final embedding coordinates are given as @xmath115 , @xmath116 .",
    "the three steps of the algorithm , namely , clustering , smoothing , and embedding have different computational complexities . in particular , for @xmath65 points with dimensionality @xmath4 , the clustering algorithm has a complexity @xmath117 which is dominated by the pca @xcite step .    the computational complexity of the smoothing algorithm is dominated by the dijkstra s",
    "algorithm @xcite for calculating the longest geodesics . here",
    ", we first assume that each cluster with respect to the first reference point contains an equal number of points .",
    "thus , partitioning a data set of @xmath65 point into @xmath118 clusters yields @xmath119 points in a cluster .",
    "the complexity of generating the longest geodesic in each such cluster is @xmath120 @xcite , which sums - up @xmath118 times and implies the total complexity @xmath121 of making all geodesics with respect to the first reference point .",
    "similarly , for the second reference point , the same procedure is followed for all @xmath122 clusters , each containing @xmath123 points , to obtain a complexity of @xmath124 .",
    "altogether , geodesics from both reference points contributes a computational complexity of @xmath125 for the smoothing algorithm .    in the embedding algorithm ,",
    "discretizing splines and approximating the minimum distance are dominant in the total cost of that algorithm . here",
    ", we first calculate the mean length of splines with respect to the first reference point , denoted as @xmath126 , and the one for the second reference point , denoted as @xmath127 .",
    "then , the computational cost associated with approximating intersection of these two splines are computed . for a pair of @xmath4-dimensional splines , computation of the euclidean distance between any two points , one from each spline , has complexity of @xmath128 from the operations of subtraction , squaring , and summation along all @xmath4 dimensions .",
    "since each spline @xmath129 is discretized at a mesh size @xmath130 , while @xmath131 has @xmath132 points on it , @xmath133 has @xmath134 points .",
    "the computational cost of approximating closest distance between any two points one from each spline @xmath129 is @xmath135 . here , for simplicity",
    ", we assume that the length of each spline with respect to the first and second reference points have same lengths as @xmath131 and @xmath136 consecutively .",
    "thus , the total complexity of the embedding algorithm , which has @xmath137 and @xmath138 number of splines with respect to the first and second reference points , is @xmath139 clustering , embedding algorithms and steps @xmath140 in the embedding algorithm are performed only once for a given data set , thus after they are completed , @xmath65 new points are embedded with @xmath141 .",
    "m.  ballerini , n.  cabibbo , r.  candelier , a.  cavagna , e.  cisbani , i.  giardina , a.  orlandi , g.  parisi , a.  procaccini , m.  viale , empirical investigation of starling flocks : a benchmark study in collective animal behaviour , animal behaviour 76  ( 1 ) ( 2008 ) 201215 .",
    "a.  kolpas , j.  moehlis , i.  g. kevrekidis , coarse - grained analysis of stochasticity - induced switching between collective motion states . ,",
    "proceedings of the national academy of sciences of the united states of america 104  ( 14 ) ( 2007 ) 59315935 .",
    "t.  a. frewen , i.  d. couzin , a.  kolpas , j.  moehlis , r.  coifman , i.  g. kevrekidis , coarse collective dynamics of animal groups , in : coping with complexity : model reduction and data analysis , 2011 , pp .",
    "299309 .",
    "t.  a. frewen , i.  d. couzin , a.  kolpas , j.  moehlis , r.  coifman , i.  g. kevrekidis , coarse collective dynamics of animal groups , in : coping with complexity : model reduction and data analysis , springer , 2011 , pp ."
  ],
  "abstract_text": [
    "<S> while the existence of low - dimensional embedding manifolds has been shown in patterns of collective motion , the current battery of nonlinear dimensionality reduction methods are not amenable to the analysis of such manifolds . </S>",
    "<S> this is mainly due to the necessary spectral decomposition step , which limits control over the mapping from the original high - dimensional space to the embedding space . here , we propose an alternative approach that demands a two - dimensional embedding which topologically summarizes the high - dimensional data . in this sense , </S>",
    "<S> our approach is closely related to the construction of one - dimensional principal curves that minimize orthogonal error to data points subject to smoothness constraints . </S>",
    "<S> specifically , we construct a two - dimensional principal manifold directly in the high - dimensional space using cubic smoothing splines , and define the embedding coordinates in terms of geodesic distances . </S>",
    "<S> thus , the mapping from the high - dimensional data to the manifold is defined in terms of local coordinates . through representative examples , </S>",
    "<S> we show that compared to existing nonlinear dimensionality reduction methods , the principal manifold retains the original structure even in noisy and sparse datasets . </S>",
    "<S> the principal manifold finding algorithm is applied to configurations obtained from a dynamical system of multiple agents simulating a complex maneuver called predator mobbing , and the resulting two - dimensional embedding is compared with that of a well - established nonlinear dimensionality reduction method .    </S>",
    "<S> dimensionality reduction , algorithm , collective behavior , dynamical systems </S>"
  ]
}