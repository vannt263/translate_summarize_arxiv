{
  "article_text": [
    "low - rank matrix reconstruction ( lrmr ) from under - sampled measurements has attracted considerable interest in recent times @xcite .",
    "an example of lrmr is matrix completion @xcite .",
    "lrmr can be viewed as a further generalization of the thriving topic called compressed sensing @xcite .",
    "typically reconstruction algorithms are of two main types , convex and greedy methods .",
    "convex methods try to minimize the nuclear norm of the underlying matrix @xcite . under some conditions ,",
    "the convex relaxation algorithms are optimal . on the other hand greedy search techniques also",
    "have been developed based on heuristics , providing reasonable practical performance .    in estimation theory ,",
    "the bayesian framework has a strong role .",
    "while attempts have been made to develop non - bayesian tools for lrmr  such as convex and iterative greedy techniques  not much effort has been made to develop a fully bayesian framework .",
    "a recent attempt in this direction is @xcite . in @xcite an indirect approach",
    "is based on reparameterization of the original low - rank matrix by two component matrix factorization and then inducing low - rank structures on the two matrices through standard variational bayesian tools .",
    "hence , the approach of @xcite does not use a low - rank prior for the matrix itself .",
    "the method of @xcite provides good performance when the matrix is nearly square , but suffers in performance for skewed dimensional matrices .    in this paper",
    "we attempt to use a low - rank prior directly for the matrix itself .",
    "the low - rank prior is constituted via the property that the number of relevant singular vectors ( singular vectors spanning the range space of the matrix ) are limited for low - rank matrices .",
    "we describe the singular vectors by introducing precisions ( inverse variances ) for the left and right singular vectors .",
    "since our method naturally generalizes the relevance vector machine ( rvm ) for sparse vectors @xcite to low - rank matrices we refer to it as the relevance singular vector machine ( rsvm ) .",
    "the development of the rsvm comes with a non - trivial prior formulation followed by the map rule for parameter estimation . through several experiments we compare the efficiency of the rsvm to the variational bayesian based technique of @xcite and the standard nuclear norm minimization technique .",
    "in our model a low rank matrix @xmath0 is observed through noisy linear measurements @xmath1 where @xmath2 and @xmath3 .",
    "the rank of the matrix is low , @xmath4 , and unknown .",
    "the noise is zero - mean white gaussian , @xmath5 = \\sigma_n^2 \\mathbf{i}_m$ ] , with unknown variance @xmath6 . in the matrix completion scenario @xcite , the linear operator @xmath7 chooses a set of elements from @xmath8 .",
    "the goal is to reconstruct the matrix @xmath8 from the measurements @xmath9 .",
    "the problem of reconstructing @xmath8 from has been studied using several approaches @xcite .",
    "one convex approach is the relaxed nuclear norm heuristic @xcite , which sets@xmath10 where @xmath11 is a regularization parameter and @xmath12 is the nuclear norm @xcite .",
    "the estimate depends on the parameter @xmath13 , which needs to be set in advance .",
    "greedy methods for have also been developed @xcite . however , most greedy methods require either the rank or noise variance to be known a priori .",
    "lrmr can be viewed as a further generalization of compressed sensing . in bayesian",
    "compressed sensing ( bcs ) @xcite , the relevance vector machine ( rvm ) @xcite is used to estimate a sparse vector from an underdetermined set of measurements .",
    "the rvm uses hyperpriors ( or hierarchical priors ) to promote sparsity .",
    "that is , the distributions of the hyperpriors promote sparsity in the estimate .",
    "the rvm does not require the sparsity or noise variance to be known a priori .",
    "one analogue of bcs for matrix completion and robust pca was developed by babacan et .",
    "al . @xcite . in @xcite ,",
    "the matrix @xmath8 was factorized as @xmath14 where @xmath15 , @xmath16 and @xmath17 is a user defined parameter .",
    "the authors used the variational bayesian framework to iteratively estimate the column vectors of @xmath18 and @xmath19 . in this model , low - rank",
    "is promoted by reparameterization the problem and using sparsity inducing priors , rather than using low - rank inducing priors for the matrix itself .",
    "to generalize bcs to low - rank matrix reconstruction , we construct matrix precisions that induce low - rank . the precisions give information about the low - rank structure of the matrix , i.e. , the left and right singular vectors of the matrix .",
    "to introduce precisions that give information about the singular vectors , we make the ansatz @xmath20 where @xmath21 and @xmath22 are left and right precision matrices that are positive definite .",
    "we use two separate precisions since the left and right singular vectors of a matrix are not equal in general .",
    "the ansatz is equivalent to setting @xmath23 to make the precisions promote low - rank , we use wishart distributions @xcite as priors for the precisions , i.e. @xmath24 where @xmath25 are symmetric and positive definite , @xmath26 , @xmath27 and @xmath28 denotes the determinant .",
    "the wishart distributions reduce to gamma distributions if we restrict the precision matrices to be diagonal .",
    "since the wishart distribution can be seen as a generalization of the gamma - distribution , this choice of priors naturally generalizes the standard rvm . by marginalizing over @xmath29",
    "we get that @xmath30 the map estimate of @xmath8 and the precisions can thus be interpreted as a weighted log - det heuristic .",
    "we model the noise as @xmath31 i.e. , the noise is modeled as white gaussian with a gamma distributed precision @xcite .",
    "we refer to the rvm with the priors , and as the relevance singular vector machine ( rsvm ) .",
    "the rsvm iteratively updates the estimate @xmath32 and the precisions . for fixed precisions ,",
    "the map estimate of @xmath8 becomes @xmath33    the precisions are updated by maximizing the marginal distribution @xmath34 .",
    "this gives us the update equations @xmath35 where @xmath36 , @xmath37 and @xmath38 and @xmath39 are matrices with components @xmath40_{kl } = tr(\\boldsymbol{\\sigma } ( \\boldsymbol{\\alpha}_r \\otimes \\mathbf{e}_{kl}^{(l ) } ) ) , \\,\\ , [ \\boldsymbol{\\sigma}_l]_{kl } = tr(\\boldsymbol{\\sigma } ( \\mathbf{e}_{kl}^{(r ) } \\otimes \\boldsymbol{\\alpha}_l ) ) & [ \\boldsymbol{\\sigma}_r]_{kl } = tr(\\boldsymbol{\\sigma } ( \\boldsymbol{\\alpha}_r \\otimes \\mathbf{e}_{kl}^{(l)}))\\\\ & [ \\boldsymbol{\\sigma}_l]_{kl } = tr(\\boldsymbol{\\sigma } ( \\mathbf{e}_{kl}^{(r ) } \\otimes \\boldsymbol{\\alpha}_l))\\end{aligned}\\ ] ] where @xmath41 ( @xmath42 ) is a matrix with @xmath43 in position @xmath44 and zeros otherwise .",
    "we note that the map estimate resembles the iterative reweighted least squares approach @xcite , but that the weights are chosen using the bayesian framework .",
    "one problem with using two precisions is that they can become unbalanced , i.e. one precision can become large and the other small . to balance the precisions , we rescale them as @xmath45 in each iteration , where @xmath46    the balancing of the precisions removes the dependence on @xmath47 and @xmath48 . to make the prior non - informative we take the limits @xmath49 in computations , however ,",
    "the parameters are given small but non - zero values to avoid numerical instabilities .",
    "when @xmath50 is symmetric , the left and right singular vectors are equal ( up to sign changes ) . the left and right precisions can therefore be chosen to be equal .",
    "the prior of @xmath8 then becomes @xmath51 where @xmath52 has the prior distribution @xmath53 and @xmath54 is positive definite . let @xmath55 be the kronecker product decomposition of @xmath56 where @xmath57 .",
    "the update equation for the precision then becomes @xmath58 where @xmath59",
    "we note that the computational complexity of the rsvm is dominated by the computation of @xmath32 and @xmath60 which requires the inversion of a @xmath61 matrix .",
    "the computational complexity is thus @xmath62 .",
    "using the woodbury matrix identity @xcite to rewrite the inverse reduces the complexity to @xmath63 , which can still be large . here",
    "we further reduce the complexity by using the variational bayesian framework .",
    "let @xmath64 \\times [ q]$ ] be a set of indices with corresponding parameters @xmath65 we want to estimate and let @xmath66 be the complement of @xmath67 .",
    "we decompose the problem as @xmath68 the variational bayesian framework gives us that @xmath69 \\\\ & = \\text{constant } - \\frac{\\beta}{2 } \\left|\\left| \\mathbf{y } - \\mathbf{a}_{\\omega^c } ( e[\\mathbf{x}_{\\omega^c } ] ) - \\mathbf{a}_\\omega ( \\mathbf{x}_\\omega ) \\right|\\right|_2 ^ 2 \\\\ & - \\frac{1}{2 } \\mathbf{x}_\\omega^\\top \\boldsymbol{\\alpha}_{\\omega,\\omega } \\mathbf{x}_\\omega - \\mathbf{x}_\\omega^\\top \\boldsymbol{\\alpha}_{\\omega,\\omega^c } e[\\mathbf{x}_{\\omega^c}]\\end{aligned}\\ ] ] where the constant contains terms which do not depend on @xmath65 . by assuming that @xmath70 is a random variable with mean @xmath71 and that it is independent from @xmath65 , we find that the map estimate of @xmath65 becomes @xmath72 we see that the variational bayesian approach becomes the block descent method , i.e. the method of minimizing the objective ( the negative log likelihood ) over different blocks of variables iteratively .",
    "since the objective is convex , the iterative method converges to the minimum of the objective ( the full map estimate ) @xcite .    to accelerate the computation of @xmath73 and @xmath74 , we make the approximation that the parameter estimates are unbiased and uncorrelated , i.e. @xmath75 = \\left\\ { \\begin{array}{ll } \\mathbf{0 } & \\text { if } i \\neq j\\\\ \\boldsymbol{\\sigma}_{\\omega_i } & \\text { if } i = j \\end{array } \\right . , \\end{aligned}\\ ] ] for different blocks @xmath76 and @xmath77 such that @xmath78 .",
    "this reduces the complexity of calculating @xmath79 from @xmath62 to @xmath80 .",
    "since the matrix @xmath60 becomes sparse , the computation of @xmath73 and @xmath74 can be performed efficiently .",
    "the precisions are then updated using , and .",
    "if we split @xmath81 into @xmath82 blocks of size @xmath83 and the estimator is iterated over the blocks @xmath84 times , the number of floating point operations in each iteration reduce from @xmath62 to @xmath85 however , the reduction in computational complexity comes at the cost of possible loss of accuracy .",
    "to numerically compare the performance of the different algorithms we generated low - rank matrices @xmath86 where @xmath87 , @xmath88 have @xmath89 components ( so @xmath90 with probability @xmath43 ) .",
    "we measure the performance of the algorithms in terms of the normalized mean square error @xmath91}{e[||\\mathbf{x}||_f^2]},\\end{aligned}\\ ] ] which was evaluated through numerical simulations .",
    "the signal - to - noise ratio ( snr ) @xmath92}{e[||\\mathbf{n}||_2 ^ 2 } = \\left\\ { \\begin{array}{lr } r/ \\sigma_n^2 & \\text{(completion)}\\\\   pqr/ m \\sigma_n^2 & \\text{(reconstruction ) } \\end{array } \\right.\\end{aligned}\\ ] ] was set to 20 db . for each scenario we generated @xmath93 low - rank matrices and @xmath93 measurements of every matrix for each parameter value .",
    "we set the precisions of the rsvm to identity matrices in the first iteration . for the variational bayes procedure ( vb )",
    "we set @xmath94 and for the accelerated rsvm , we partitioned the column vectors of @xmath8 into @xmath95 blocks .",
    "for the nuclear norm heuristic we assumed the noise variance @xmath6 to be known and used @xmath96 as proposed in @xcite , where @xmath97 is the number of measurements .",
    "we used the cvx toolbox @xcite for the nuclear norm minimization .      to test the algorithms for the matrix completion problem @xcite ,",
    "@xmath97 components was chosen uniformly at random from a matrix and the matrix was reconstructed using rsvm , the accelerated rsvm , the variational bayes procedure ( vb ) @xcite and the nuclear norm . in the experiments we set @xmath98 , @xmath99 , @xmath100 and varied the number of measurements @xmath97.we found that rsvm had @xmath101 db lower nmse than vb and @xmath102 db lower nmse than the nuclear norm for @xmath103 ( see figure [ completion_rho ] ) .",
    "we found that the performance of the variational bayesian technique in @xcite was dependent on ratio of the matrix dimensions . to compare how the performance of the algorithms depend on the ratio we set @xmath104 , @xmath105 , @xmath106 and varied @xmath107 .",
    "we found that vb works better than the other methods when @xmath108 , but its performance degrades when @xmath109 ( see figure [ completion_q ] ) .",
    "the performance of the nuclear norm , rsvm and the accelerated rsvm is less sensitive to the ratio of the matrix dimensions .      in matrix reconstruction ,",
    "the sensing matrix @xmath7 in takes linear combinations of the elements in @xmath8 . to examine the performance of the algorithms for this scenario we generated @xmath3 by drawing its elements from a @xmath89 distribution and normalizing the column vectors to unit length . in the simulation",
    "we choose @xmath110 and @xmath111 . to estimate @xmath8 we used rsvm , the accelerated rsvm and the nuclear norm .",
    "we found that rsvm had @xmath112 db lower nmse than the nuclear norm for @xmath113 while the accelerated rsvm had higher nmse than the nuclear norm for @xmath114 and smaller nmse than the nuclear norm for @xmath115 ( see figure [ rec_rho ] ) .    in a second experiment",
    "we set @xmath110 , @xmath106 and varied the rank of @xmath8 .",
    "we found that the rsvm and the accelerated rsvm had @xmath116 db lower nmse than the nuclear norm when @xmath117 ( see figure [ rec_rank ] ) .",
    "in the gamut of bayesian sparse kernel machines , we show that it is possible to use priors on singular vectors to promote low rank .",
    "the priors were used to construct a relevance vector machine for low - rank matrix reconstruction .",
    "we call the algorithm the relevance singular vector machine ( rsvm ) .",
    "we show that iterative algorithms can be designed to achieve the maximum - a - priori ( map ) solution .",
    "even computational complexity can be further reduced by appropriate approximations leading to accelerated algorithms .    through simulations",
    ", it was shown that the rsvm outperformed the variational bayesian method for matrix completion when the matrix is skewed .",
    "the rsvm also outperformed the relaxed nuclear norm heuristic in the matrix reconstruction scenario .",
    "k.  mohan and m.  fazel , `` iterative reweighted least squares for matrix rank minimization , '' in _ annual allerton conference on communication , control , and computing _",
    "( allerton ) , 48th , sept 2010 , pp .",
    "653 - 661 ."
  ],
  "abstract_text": [
    "<S> in this paper we develop a new bayesian inference method for low rank matrix reconstruction . </S>",
    "<S> we call the new method the relevance singular vector machine ( rsvm ) where appropriate priors are defined on the singular vectors of the underlying matrix to promote low rank . to accelerate computations , a numerically efficient approximation is developed . </S>",
    "<S> the proposed algorithms are applied to matrix completion and matrix reconstruction problems and their performance is studied numerically .    </S>",
    "<S> low rank matrix reconstruction , sparse bayesian learning , relevance vector machine . </S>"
  ]
}