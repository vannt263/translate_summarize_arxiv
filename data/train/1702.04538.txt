{
  "article_text": [
    "several learning problems in modern cyber - physical network systems involve a large number of very - high - dimensional input data .",
    "the related research areas go under the names of _ big - data analytics _ or _ big - data classification_. from an optimization point of view , the problems arising in this area involve a large number of constraints and/or local cost functions typically distributed among computing nodes communicating asynchronously and unreliably .",
    "an additional challenge arising in big - data classification problems is that not only the number of constraints and local cost functions is large , but also the dimension of the decision variable is big and may depend on the number of nodes in the network .",
    "we organize the literature in two parts .",
    "first , we point out some recent works focusing the attention on _ big - data optimization _ problems ,",
    "i.e. , problems in which all the data of the optimization problem are big and can not be handled using standard approaches from sequential or even parallel optimization .",
    "the survey paper @xcite reviews recent advances in convex optimization algorithms for big - data , which aim to reduce the computational , storage , and communications bottlenecks .",
    "the role of parallel and distributed computation frameworks is highlighted . in @xcite big - data , possibly non - convex ,",
    "optimization problems are approached by means of a decomposition framework based on successive approximations of the cost function . in @xcite dictionary",
    "learning tasks motivate the development of non - convex and non - smooth optimization algorithms in a big - data context .",
    "the paper develops an online learning framework by jointly leveraging the stochastic approximation paradigm with first - order acceleration schemes .",
    "second , we review distributed optimization algorithms applied to learning problems and highlight their limitations when dealing with big - data problems .",
    "an early reference on peer - to - peer training of support vector machines is @xcite .",
    "a distributed training mechanism is proposed in which multiple servers compute the optimal solution by exchanging support vectors over a fixed directed graph .",
    "the work is a first successful attempt to solve svm problems over networks .",
    "however , the local memory and computation at each node does not scale with the problem and data sizes and the graph is time - invariant . in @xcite",
    "a distributed alternating direction method of multipliers ( admm ) is proposed to solve a linear svm training problem , while in @xcite the same problem is solved by means of a random projected gradient algorithm . both the algorithms are proven to solve the centralized problem ( i.e. , all the nodes reach a consensus on the global solution ) , but again show some limitations : the graph topology must be ( fixed , @xcite , and ) undirected , and the algorithms do not scale with the dimension of the training vector space . in @xcite a survey on admm algorithms applied to statistical learning problems is given .",
    "in @xcite the problem of exchanging only those measurements that are most informative in a network svm problem is investigated . for separable problems",
    "an algorithm is provided to determine if an element in the training set can become a support vector .",
    "the distributed optimization algorithm proposed in @xcite solves part of these problems : local memory is scalable and communication can be directed and asynchronous .",
    "however , the dimension of the training vectors is still an issue .",
    "the core - set idea used in this paper was introduced in @xcite as a building block for clustering , and refined in @xcite . in @xcite",
    "the approach was shown to be relevant for several learning problems and the algorithm re - stated for such scenarios .",
    "a multi - processor implementation of the core - set approach was proposed in @xcite .",
    "however , differently from our approach , that algorithm : ( i ) is not completely distributed since it involves a coordinator , and ( ii ) does not compute a global core - set , but a larger set approximating it .",
    "the main contribution of this paper is twofold .",
    "first , we identify a distributed big - data optimization framework appearing in modern classification problems arising in cyber - physical network systems . in this framework",
    "the problem is characterized by a large number of input data distributed among computing processors .",
    "the key challenge is that the dimension of each input vector is very - high , so that standard local updates in distributed optimization can not be used . for this big - data scenario",
    ", we identify a class of quadratic programs that model several interesting classification problems as , e.g. , training of support vector machines .",
    "second , for this class of big - data quadratic optimization problems , we propose a distributed algorithm that solves the problem up to an arbitrary @xmath0 tolerance and scales both with the number and the dimension of the input vectors . the algorithm is based on the notion of core - set used in geometric optimization to approximate the value function of a given set of points with a smaller subset of points . from an optimization point of view , a subset of active constraints is identified , whose number depends only on the tolerance @xmath0 .",
    "the resulting approximate solution is such that an @xmath0-relaxation of the constraints guarantees no constraint violation .",
    "the paper is organized as follows . in section",
    "[ sec : distrib_optim_framework ] we introduce the distributed optimization problem addressed in the paper and describe the network model .",
    "section  [ sec : distributed_svm ] motivates the problem set - up by showing a class of learning problems that can be cast in this set - up . in section",
    "[ sec : core - set consensus ] the core - set consensus algorithm is introduced and analyzed .",
    "finally , in section  [ sec : simulations ] a numerical example is given to show the algorithm correctness .",
    "in this section we introduce the problem set - up considered in the paper .",
    "we recall that we will deal with optimization problems in which both the number of constraints and decision variables are `` big '' .",
    "we consider a set of processors @xmath1 , each equipped with communication and computation capabilities .",
    "each processor @xmath2 has knowledge of a vector @xmath3 and needs to cooperatively solve the quadratic program @xmath4 the above quadratic program is known in geometric optimization as _ minimum enclosing ball _",
    "problem , since it computes the center of the ball with minimum radius enclosing the set of points @xmath5 .    by applying standard duality arguments",
    ", it can be shown that solving is equivalent to solving its dual @xmath6 with @xmath7 , @xmath8 .",
    "the problem can be written in a more compact form as @xmath9 where @xmath10 \\in{{\\mathbb{r}}}^{d\\times n}$ ] , @xmath11 is the vector with elements @xmath12 , @xmath8 , @xmath13^t \\in{{\\mathbb{r}}}^n$ ] and @xmath14 is meant component - wise .",
    "we will show in the next sections that this class of quadratic programs arises in many important big - data classification problems .",
    "each node has computation capabilities meaning that it can run a routine to solve a local optimization problem .",
    "since the dimension @xmath15 can be big , the distributed optimization algorithm to solve problem needs to be designed so that the local routine at each node scales `` nicely '' with @xmath15 .    the communication among the processors is modeled by a time - varying ,",
    "directed graph ( digraph ) @xmath16 , where @xmath17 represents a slotted universal time , the node set @xmath18 is the set of processor identifiers , and the edge set @xmath19 characterizes the communication among the processors .",
    "specifically , at time @xmath20 there is an edge from node @xmath2 to node @xmath21 if and only if processor @xmath2 transmits information to processor @xmath21 at time @xmath20 .",
    "the time - varying set of outgoing ( incoming ) _ neighbors _ of node @xmath2 at time @xmath20 , i.e. , the set of nodes to ( from ) which there are edges from ( to ) @xmath2 at time @xmath20 , is denoted by @xmath22 ( @xmath23 ) .",
    "a static digraph is said to be _ strongly connected _ if for every pair of nodes @xmath24 there exists a path of directed edges that goes from @xmath2 to @xmath21 . for the time - varying communication graph we rely on the concept of a jointly strongly connected graph .",
    "[ ass : periodicconnectivity ] for every time instant @xmath25 , the union digraph @xmath26 is strongly connected .",
    "it is worth noting that joint strong connectivity of the directed communication graph is a fairly weak assumption ( it just requires persistent spreading of information ) for solving a distributed optimization problem , and naturally embeds an asynchronous scenario .",
    "we want to stress once more that in our paper all the nodes are peers , i.e. , they run the same local instance of the distributed algorithm , and no node can take any special role .",
    "consistently , we allow nodes to be asynchronous , i.e. , nodes can perform the same computation at different speed , and communication can be unreliable and happen without a common clock ( the time @xmath20 is a universal time that does not need to be known by the nodes ) .",
    "in this section we present a distributed set up for some fundamental classification problems and show , following @xcite , how they can be cast into the distributed quadratic programming framework introduced in the previous section .",
    "we consider classification problems to be solved in a distributed way by a network of processors following the model in section  [ sec : distrib_optim_framework ] .",
    "each node in the network is assigned a subset of input vectors and the goal for the processors is to cooperatively agree on the optimal classifier without the help of any central coordinator .",
    "informally , the svm training problem can be summarized as follows .",
    "given a set of positively and negatively labeled points in a @xmath27-dimensional space , find a hyperplane separating `` positive '' and `` negative '' points with the maximal separation from all the data points .",
    "the labeled points are commonly called _",
    "examples _ or _ training vectors_.    linear separability of the training vectors is usually a strong assumption . in many important concrete scenarios",
    "the training data can not be separated by simply using a linear function ( a hyperplane ) . to handle the nonlinear separability ,",
    "nonlinear kernel functions are used to map the training samples into a feature space in which the resulting features can be linearly separated .",
    "that is , given a set of points @xmath28 in the _ input space _",
    "they are mapped into a _ feature space _ through a function @xmath29 .",
    "the key aspect in svm is that @xmath30 does not need to be known , but all the computations can be done through a so called _ kernel function _",
    "@xmath31 satisfying @xmath32 .",
    "it is worth noting that the dimension of the feature space can be much higher than the one of the input space , even infinite ( e.g. , gaussian kernels ) .",
    "following @xcite and @xcite we will adopt the following common assumption in svm . for any @xmath33 in the input space @xmath34 with @xmath35 independent of @xmath2 .",
    "this condition is satisfied by the most common kernel functions used in svm as , e.g. , the isotropic kernel ( e.g. , gaussian kernel ) , the dot - product kernel with normalized inputs or any normalized kernel .",
    "for fixed @xmath36 , let @xmath37 , @xmath8 , be a set of @xmath38 feature - points with associated label @xmath39 .",
    "the training vectors are said to be linearly separable if there exist @xmath40 and @xmath41 such that @xmath42 for all @xmath8 . the _ hard - margin svm training _",
    "problem consists of finding the optimal hyperplane @xmath43 , @xmath44 , ( @xmath45 is a vector orthogonal to the hyperplane and @xmath46 is a bias ) that linearly separates the training vectors with maximal margin , that is , such that the distance @xmath47 is maximized . combining the above equations it follows easily that @xmath48 .",
    "thus the svm training problem may be written as a quadratic program @xmath49    in most concrete applications the training data can not be separated without outliers ( or training errors ) . a convex program that approximates",
    "the above problem was introduced in @xcite .",
    "the idea is to introduce positive slack variables in order to relax the constraints and add an additional penalty in the cost function to weight them .",
    "the resulting classification problems are known as _ soft marging problems _ and the solution is called _ soft margin hyperplane_.    next , we will concentrate on a widely used soft - margin problem , the _",
    "@xmath50-norm problem _ , which adopts a quadratic penalty function .",
    "following @xcite , we will show that its dual version is a quadratic program with the structure of .",
    "the @xmath50-norm optimization problem turns out to be @xmath51 solving problem is equivalent to solving the dual problem @xmath52 where @xmath53 if @xmath54 and @xmath55 otherwise .",
    "the vector @xmath45 defining the optimal hyperplane can be written as linear combination of training vectors , @xmath56 , where @xmath57 and @xmath58 only for vectors satisfying @xmath59 .",
    "these vectors are called _ support vectors_. support vectors are basically active constraints of the quadratic program .",
    "now , we can notice that defining @xmath60 , it holds @xmath61 so that the constant term @xmath62 , can be added to the cost function .",
    "thus , posing @xmath63 with @xmath64 the @xmath2th canonical vector ( e.g. , @xmath65^t$ ] ) , problem can be equivalently rewritten as @xmath66 which has exactly the same structure as problem  .",
    "it is worth noting that even if the dimension @xmath15 of the training samples in the feature space ( @xmath67 ) is small compared to the number of samples @xmath68 ( so that in problem the dimension of the decision variable is much smaller than the number of constraints ) , in the `` augmented '' soft - margin problem we have @xmath69 .",
    "thus , in the primal problem the dimension of the decision variable is of the same order as the number of constraints .",
    "next , we recall from @xcite that also some unsupervised soft margin classification problems can be cast into the same problem set - up of the paper .    first , from @xcite and",
    "references therein , it can be shown that problem is equivalent to the hard - margin support vector data description ( svdd ) problem .",
    "indeed , given a kernel function @xmath31 and feature map @xmath30 , the hard - margin svdd primal problem is @xmath70 in other words , problem is simply problem in the feature space .",
    "another unsupervised learning problem that can be cast into the problem set - up is the so called _ one - class l2 svm _ , @xcite .",
    "given a set of unlabeled input vectors the goal is to separate outliers from normal data . from an optimization point of view , the problem can be written as problem , but with @xmath71 and @xmath72 for all @xmath8 .",
    "thus , using the same arguments as in the previous subsection , the problem can be rewritten in the form .    to conclude this motivating section",
    ", we recall from @xcite that algorithms solving problem are important building blocks for clustering problems .",
    "in this section we introduce the core - set consensus algorithm to solve problem   ( or equivalently its dual ) in a distributed way .",
    "we start by introducing the notion of core - set borrowed from geometric optimization and a routine from @xcite that is proven to compute an efficient core - set for , which in geometric optimization is known as _ minimum enclosing ball problem_.      in the following we will a little abuse notation by denoting with @xmath73 both the @xmath74 matrix and the set of @xmath75 vectors ( or points ) of dimension @xmath15 .",
    "let @xmath76 be a matrix of `` points '' @xmath3 , @xmath8 , ( i.e. , a matrix in which each column represents a vector in @xmath77 ) with @xmath78 and @xmath79 respectively the center and radius of the minimum enclosing ball containing the points of @xmath80 .",
    "we say that @xmath81 is an _ @xmath0-core - set _ for the minimum enclosing ball ( meb ) problem , , if all the points of @xmath80 are at distance at most @xmath82 from the center @xmath83 of the minimum enclosing ball containing @xmath84 .",
    "note that @xmath85 , with @xmath86 being the optimal value of .",
    "next , we introduce the algorithm in @xcite that is proven to compute a core - set of dimension @xmath87 for the minimum enclosing ball problem .    given a set of points @xmath88 , the algorithm can be initialized by choosing any subset @xmath89 of @xmath87 points .",
    "then the algorithm evolves as follows :    * select a point @xmath90 of @xmath88 farthest from the center of the minimum enclosing ball of @xmath84 ; * let @xmath91 ; * remove a point @xmath92 so that the minimum enclosing ball of the set @xmath93 is the one with largest radius ; * if the new radius is equal to the radius of minimum enclosing ball of @xmath84 , then return @xmath84 .",
    "otherwise set @xmath94 and repeat the procedure .",
    "more formally , the routing is described in the following table .",
    "as before , we let @xmath95 and @xmath96 be respectively the center and radius of the minimum enclosing ball containing all the points in a set of points @xmath88 .",
    "* function * @xmath97",
    "@xmath98 @xmath91 @xmath99 * if * @xmath100 @xmath94 and go to step 1 * else * * return * @xmath84 * end if *    it is worth pointing out once more that if @xmath101 , with @xmath80 the one in , then the @xmath102 algorithm finds an @xmath0-core - set for ( or equivalently ) .    the @xmath102 algorithm will be the local routine implemented in the distributed optimization algorithm we propose in this paper . that is , each node will use the algorithm to solve a ( smaller ) local version of the main problem .",
    "next we provide a lemma that states the results in @xcite by formally itemizing the properties of the algorithm that we will need in our distributed optimization algorithm .",
    "[ lem : badoiu ] let @xmath103 be any point set in @xmath77",
    ". then    1 .",
    "@xmath88 has an @xmath0-core - set of size at most @xmath87 ; 2 .",
    "the @xmath102 algorithm computes an @xmath0-core - set for @xmath88 in a finite number of iterations ; 3 .   for any @xmath104",
    "the radius of @xmath105 is larger than or equal to the radius of @xmath84 .    statements ( i ) and ( ii ) are proven in ( * ? ? ?",
    "* theorem 3.5 ) , while ( iii ) follows immediately by step  4 of the algorithm .",
    "let @xmath80 be the matrix characterizing problem or consistently the set of vectors @xmath3 , @xmath8 . as stated in section  [ sec : distrib_optim_framework ] , each node is assigned one input vector .",
    "this assumption is just for clarity of presentation and can be easily removed .",
    "in fact , the algorithm can be run even if each node is assigned more than one vector .",
    "for this reason we denote @xmath106 the set of initial vectors , so that under the above assumption we have @xmath107 .",
    "an informal description of the _ core - set consensus _ distributed algorithm is the following .",
    "each node stores a candidate core - set @xmath108 , i.e. , a set of @xmath87 vectors that represent node-@xmath2 s current estimate for the core - set of @xmath80 . at each communication",
    "round each node receives the candidate core sets ( sets of @xmath87 vectors ) from its in - neighbors and initializes its local routine to the core - set with highest value .",
    "let @xmath109 be the set of vectors from all neighboring core - sets plus the initial vectors assigned to node @xmath2 .",
    "the local routine at each node finds a core set ( of @xmath87 vectors ) of @xmath110 , and updates the candidate core set with the returned value .",
    "a pseudo - code of the algorithm is given in the following table .",
    "we assume each node can run two routines , namely @xmath111 and @xmath112 returning respectively the core - set of a given set of vectors @xmath113 ( the routine is initialized with @xmath114 ) and the value of a given core set , i.e. , the optimal value of problem with matrix @xmath115 ( squared radius of the minimum enclosing ball ) .",
    "@xmath116 @xmath107 , @xmath117 @xmath108 @xmath118 * return * @xmath108    the algorithm works also if a larger set of vectors is assigned to each node .",
    "only the initialization needs to be changed . if a node is assigned more than @xmath87 vectors , it will initialize @xmath108 with a random set of @xmath87 vectors .",
    "it is worth noting that the nodes need to know the common tolerance @xmath0 ( and thus the core - set dimension @xmath87 ) to run the algorithm .    to analyze the algorithm",
    ", we associate a universal , discrete time @xmath119 to each step of the distributed algorithm evolution , i.e. , to each computation and communication round . this time @xmath20 is the one defining the time varying nature of the communication graph and , thus , the same used in assumption  [ ass : periodicconnectivity ] .",
    "we are now ready to analyze the convergence properties of the algorithm .",
    "[ ass : non_degenerate ] given @xmath80 in , for any @xmath120 with @xmath121 , then @xmath122 .",
    "the above assumption can be removed by using a total ordering for the choice of @xmath123 in algorithm  [ alg : core - set_consensus ] .",
    "for example , if two candidate sets @xmath124 and @xmath125 have @xmath126 , then one of the two could be uniquely chosen by using a lexicographic ordering on the vectors .",
    "consider a network of processors with set of identifiers @xmath127 and communication graph @xmath16 , @xmath25 , satisfying assumption  [ ass : periodicconnectivity ] .",
    "suppose problem   satisfies assumption  [ ass : non_degenerate ] and has a minimum value @xmath128 .",
    "then the core - set consensus algorithm ( algorithm  [ alg : core - set_consensus ] ) computes an @xmath0-core - set for problem   in a finite - number of communication rounds .",
    "that is , there exists @xmath129 such that    1 .",
    "@xmath130 for all @xmath8 and for all @xmath131 ; 2 .",
    "@xmath132 for all @xmath8 .",
    "we prove the statement in three steps .",
    "first , we prove that each core set converges in a finite - number of communication rounds to a stationary set of vectors .",
    "second , we prove that ( due to assumption  [ ass : non_degenerate ] ) all the stationary core sets are equal .",
    "third and finally , we prove that the common steady - state set is a core set for problem  .    to prove the first part ,",
    "notice that by the choice of @xmath123 in algorithm  [ alg : core - set_consensus ] and by lemma  [ lem : badoiu ] , @xmath133 is a monotone nondecreasing function for all @xmath8 along the algorithm evolution .",
    "thus , due to the finite possible values that @xmath108 can assume ( it is a set of @xmath87 vectors out of @xmath68 vectors ) , @xmath133 converges to a stationary value in finite time .    to prove the second part , suppose that at some time @xmath129 all the @xmath133s have converged to a stationary value and that there exist at least two nodes @xmath2 and @xmath21 such that @xmath134 . without loss of generality , from assumption  [ ass : periodicconnectivity ] , we can choose the two nodes so that @xmath135 , i.e. , @xmath24 is an edge in @xmath136 for some time instant @xmath137 .",
    "but from algorithm  [ alg : core - set_consensus ] at time @xmath138 node @xmath21 would choose @xmath108 to initialize its @xmath102 routine , thus leading to a contradiction . from assumption  [ ass : non_degenerate ]",
    ", it follows @xmath139 for all @xmath8 .",
    "finally , we just need to prove that @xmath140 is a core - set for @xmath80 .",
    "but from the properties of the @xmath102 algorithm , for each node @xmath8 , @xmath139 is a core - set for the a set of points including @xmath141 , so that @xmath140 is a core set for @xmath142 $ ] , thus concluding the proof .",
    "a core - set @xmath84 is a set of `` active constraints '' in problem   with a cost @xmath143 ( i.e. , @xmath144 ) .",
    "clearly , some of the constraints will be violated for this value of @xmath145 , but no one will be violated for @xmath146 , with @xmath86 being the optimal value of @xmath145 .",
    "an equivalent characterization for the core - set is that no constraint is violated if @xmath147 is relaxed to @xmath148 .",
    "this test is easier to run , since it does not involve the computation of the optimal value and will be used in the simulations .",
    "in this section we provide a numerical example showing the effectiveness of the proposed strategy .",
    "we consider a network with @xmath149 nodes communicating according to a directed , time - varying graph obtained by extracting at each time - instant an erds - rnyi graph with parameter @xmath150 .",
    "we choose a small value , so that at a given instant the graph is disconnected with high probability , but the graph turns out to be jointly connected .",
    "we solve a quadratic program , , with @xmath151 and choose a tolerance @xmath152 so that the number of vectors in the core - set is @xmath153 .    in figure",
    "[ fig : rr_transient ] and figure  [ fig : cc_transient ] the evolution of the squared - radius and center - norm of the core - sets at each node are depicted . as expected from the theoretical analysis ,",
    "the convergence of the radius to the consensus value is monotone non - decreasing .    , @xmath8 .",
    "]    , @xmath8 . ]",
    "in this paper we have proposed a distributed algorithm to solve a special class of quadratic programs that models several classification problems .",
    "the proposed algorithm handles problems in which not only the number of input data is large , but furthermore their dimension is big .",
    "the resulting learning area is known as _ big - data classification_. we have proposed a distributed optimization algorithm that computes an approximate solution of the global problem .",
    "specifically , for any chosen tolerance @xmath0 , each local node needs to store only @xmath87 active constraints , which represent a solution for the global quadratic program up to a relative tolerance @xmath0 .",
    "future research developments include the extension of the algorithmic idea , based on core - sets , to other big - data optimization problems .",
    "v.  cevher , s.  becker , and m.  schmidt , `` convex optimization for big data : scalable , randomized , and parallel algorithms for big data analytics , '' _ ieee signal processing magazine _ , vol .",
    "31 , no .  5 , pp . 3243 , 2014 .",
    "k.  slavakis and g.  b. giannakis , `` online dictionary learning from big data using accelerated stochastic approximation algorithms , '' in _ 2014 ieee international conference on acoustics , speech and signal processing ( icassp ) _ , 2014 , pp .",
    "y.  lu , v.  roychowdhury , and l.  vandenberghe , `` distributed parallel support vector machines in strongly connected networks , '' _ ieee transactions on neural networks _ , vol .",
    "19 , no .  7 , pp .",
    "11671178 , 2008 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ ,",
    "vol .  3 , no .  1 , pp . 1122 , 2011 .",
    "d.  varagnolo , s.  del  favero , f.  dinuzzo , l.  schenato , and g.  pillonetto , `` finding potential support vectors in separable classification problems , '' _ ieee transactions on neural networks and learning systems _ , vol .  24 , no .  11 , pp . 17991813 , 2013 .",
    "g.  notarstefano and f.  bullo , `` distributed abstract optimization via constraints consensus : theory and applications , '' _ ieee transactions on automatic control _ ,",
    "56 , no .",
    "10 , pp . 22472261 , october 2011 .",
    "s.  lodi , r.  nanculef , and c.  sartori , `` single - pass distributed learning of multi - class svms using core - sets , '' in _ proceedings of the 2010 siam international conference on data mining _ , 2010 , pp .",
    "257268 .",
    "s.  s. keerthi , s.  k. shevade , c.  bhattacharyya , and k.  r. murthy , `` a fast iterative nearest point algorithm for support vector machine classifier design , '' _ ieee transactions on neural networks _ ,",
    "11 , no .  1 ,",
    "124136 , 2000 ."
  ],
  "abstract_text": [
    "<S> a new challenge for learning algorithms in cyber - physical network systems is the distributed solution of big - data classification problems , i.e. , problems in which both the number of training samples and their dimension is high . motivated by several problem set - ups in machine learning , </S>",
    "<S> in this paper we consider a special class of quadratic optimization problems involving a `` large '' number of input data , whose dimension is `` big '' . to solve these quadratic optimization problems over peer - to - peer networks , </S>",
    "<S> we propose an asynchronous , distributed algorithm that scales with both the number and the dimension of the input data ( training samples in the classification problem ) . </S>",
    "<S> the proposed distributed optimization algorithm relies on the notion of `` core - set '' which is used in geometric optimization to approximate the value function associated to a given set of points with a smaller subset of points . by computing local core - sets on a smaller version of the global problem and exchanging them with neighbors , </S>",
    "<S> the nodes reach consensus on a set of active constraints representing an approximate solution for the global quadratic program .    distributed optimization , big - data optimization , support vector machine ( svm ) , machine learning , core set , asynchronous networks . </S>"
  ]
}