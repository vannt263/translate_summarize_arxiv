{
  "article_text": [
    "peter hall wrote many influential works on high - dimensional data , though notably he largely eschewed the notions of sparsity and penalised likelihood that have become so popular in recent years .",
    "nevertheless , he was interested in variable selection , and wrote several papers that involved ranking variables in some way . perhaps his most well - known papers in this area , though , concern geometrical representations of high - dimensional data .",
    "@xcite was one of the pioneering works in the early days of high - dimensional data analysis that tried to understand the properties of low - dimensional projections of a high - dimensional isotropic random vector @xmath0 in @xmath1 .",
    "as motivation , let @xmath2 have @xmath3 and suppose that @xmath4 this condition says that the regression function of @xmath5 on @xmath6 is linear . then , using the isotropy of @xmath0 , @xmath7 moreover , @xmath8 and we conclude that @xmath9 , or equivalently , @xmath10 the left - hand side of   is always non - negative , so can be used as a measure of the extent to which the condition   holds .",
    "remarkably , under very mild conditions on the distribution of @xmath0 , @xcite proved that if @xmath11 is drawn from the uniform distribution on the unit euclidean sphere in @xmath1 , then @xmath12 as @xmath13 .",
    "this is equivalent to the statement @xmath14 as @xmath13 .",
    "see also @xcite , who showed that under mild conditions , most low - dimensional projections of high - dimensional data are nearly normal . of course , when @xmath0 has a spherically symmetric distribution ,   holds for every @xmath2 with @xmath3 .",
    "but the result of this paper shows that even without spherical symmetry , there is a good chance ( in the sense of random draws of @xmath11 as described above ) that   holds , at least approximately , when @xmath15 is large . an important statistical consequence of this is that even if the relationship between a response @xmath16 and a high - dimensional predictor is non - linear , say @xmath17 for some unknown link function @xmath18 and error @xmath19 , standard linear regression procedures can often be expected to yield an approximately correct estimate of @xmath11 up to a constant of proportionality .",
    "the generalisation of this result that replaces @xmath6 with @xmath20 , where @xmath21 is a random @xmath22 matrix with orthonormal columns , also plays an important role in justifying the use of sliced inverse regression for dimension reduction @xcite .",
    "another seminal paper that articulated many of the key geometrical properties of high - dimensional data is @xcite .",
    "this paper begins with the simple , yet remarkable , observation that if @xmath23 , then @xmath24 as @xmath13 .",
    "thus , data drawn from this distribution tend to lie near the boundary of a large ball .",
    "similarly , the pairwise distances between points are almost a deterministic distance apart , and the observations tend to be almost orthogonal .",
    "in fact , the authors go on to explain that , under much weaker assumptions than gaussianity , the data lie approximately on the vertices of a regular simplex , and that the stochasticity in the data essentially appears as a random rotation of this simplex . as well as clarifying the relationship between support vector machines ( e.g. * ? ? ? * ) and distance weighted discrimination classifiers @xcite in high dimensions , the paper forced researchers to rewire their intuition about high - dimensional data , and precipitated a flood of subsequent papers on high - dimensional asymptotics .",
    "the last 15 years or so have seen variable selection emerge as one of the most prominently - studied topics in statistics .",
    "although peter s instinct was to think nonparametrically , he realised that he could contribute to a prominent line of research in the variable selection literature , namely marginal screening ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , via the deep understanding he developed for rankings .",
    "@xcite defined variable rankings through their generalised correlation with a response , while @xcite studied variable transformations prior to ranking based on correlation as a method for dealing with heavy - tailed data . for classification ,",
    "@xcite proposed a cross - validation based criterion for assessing variable importance , while in the unsupervised setting , @xcite suggested ranking the importance of variables for clustering based on nonparametric tests of modality .",
    "these works above were underpinned by peter s realisation that he could explain how perhaps his favourite tool of all , namely the bootstrap , could be used to quantify the authority of a ranking @xcite .",
    "in fact , there are some subtle issues here , particularly surrounding the issue of ties .",
    "peter developed an ingenious method for proving that even though the standard @xmath25-out - of-@xmath25 bootstrap does not handle this issue well , the @xmath26-out - of-@xmath25 bootstrap overcomes it in an elegant way .",
    "i believe that peter may have become interested in classification problems in the early 2000s at least partly through ideas of bootstrap aggregating , or bagging @xcite .",
    "indeed , in @xcite , a preprint of which was already available in early 2000 , peter had attempted to understand the effect of bagging in @xmath27-estimation problems .",
    "this is a typical example of peter s extraordinary ability to explain empirically observed effects through asymptotic expansions .",
    "one of the other interesting contributions of this work is that subsampling ( i.e.  sampling without replacement ) half of the observations closely mimics ordinary @xmath25-out - of-@xmath25 bootstrap sampling , a very useful fact that has been observed and exploited in several other contexts , including stability selection for choosing variables in high - dimensional inference @xcite and stochastic search methods for semiparametric regression @xcite .",
    "classification problems are ideally suited to bagging , because the discrete nature of the response variable means that small changes to the training data can often yield different outputs from a classifier ; in the terminology of @xcite , many classifiers are ` unstable ' .",
    "suppose we are given training data @xmath28 , where each @xmath29 is a covariate taking values in a general normed space @xmath30 , and @xmath31 is a response taking values in @xmath32 .",
    "assume further that we have access to a classifier @xmath33 constructed from the training data , so that @xmath34 is assigned to class @xmath35 . to form the bagged version @xmath36 of the classifier , we draw @xmath37 bootstrap resamples @xmath38 from @xmath39 , and set @xmath40 peter got me interested in bagging nearest neighbour classifiers .",
    "ironically , the nearest neighbour classifier had been described by breiman as stable , since the nearest neighbour appears in more than half  in fact , around @xmath41  of the bootstrap resamples ; thus the bagged nearest neighbour classifier is typically identical to the unbagged version . in @xcite",
    ", however , we studied the effect of drawing resamples ( either with or without replacement ) of smaller size @xmath26 . naturally , this reduces the probability of including the nearest neighbour in the resample , and the bagged classifier is now well approximated by a weighted nearest neighbour classifier with geometrically decaying weights ; see also @xcite .",
    "in order for bagging to yield any asymptotic improvement over the basic nearest neighbour classifier , we require @xmath42 ( when sampling without replacement ) and @xmath43 ( when sampling with replacement ) ; in order to converge to the theoretically - optimal bayes classifier , we require @xmath44 but @xmath45 .    once classification problems had piqued his interest , peter set about trying to answer some of the key questions on rates of convergence and tuning parameter selection that would naturally have occurred to him given his earlier work on nonparametric inference .",
    "@xcite studied the performance of classifiers constructed from kernel density estimates of the class conditional distributions on @xmath46 . a particularly curious discovery he made",
    "there is that even in the simplest case where @xmath47 and where the class conditional densities @xmath48 and @xmath18 cross only at the single point @xmath49 , the rate of convergence and order of the asymptotically optimal bandwidth depends on the sign of @xmath50 . in @xcite , we considered similar problems in the context of @xmath51-nearest neighbour classification , obtaining an asymptotic expansion for the regret ( i.e.  the difference between the risk of the @xmath51-nearest neighbour classifier and that of the bayes classifier ) which implied that the usual nonparametric error rate of order @xmath52 was attainable with @xmath51 chosen to be of order @xmath53 .",
    "the form of the expansion made me realise that the limiting ratio of the regrets of the bagged nearest neighbour classifier and the @xmath51-nearest neighbour classifier ( with both the resample size @xmath26 and the number of neighbours @xmath51 chosen optimally ) depended only on @xmath54 , and not on the underlying distributions . to my great surprise , this limiting ratio was greater than 1 when @xmath47 , equal to 1 when @xmath55 and less than 1 for @xmath56 ( though approaching 1 for large @xmath54 ) .",
    "it took me some years to explain this phenomenon in terms of the optimal weighting scheme @xcite .    in more recent years",
    ", peter turned his attention to a wealth of other important , though perhaps less well studied , issues in classification .",
    "some of these were motivated by what he saw as drawbacks of existing classifiers .",
    "for instance , in @xcite , he developed classifiers based on componentwise medians , to alleviate the difficulties of both computing and interpreting multivariate medians ; such methods can be highly effective for high - dimensional data that may have heavy tails . in @xcite , he studied robust versions of nearest neighbour classifiers for high - dimensional data that try to perform an initial variable selection step to reduce variability .",
    "@xcite presented simple scale adjustments to make distance - based classifiers ( primarily designed to detect location differences ) less sensitive to scale variation between populations ; see also @xcite .",
    "@xcite and @xcite concerned settings where one might want to incorporate the prior probabilities into a classifier , and where these prior probabilities may be significantly different from @xmath57 , respectively .",
    "finally , @xcite discovered the phenomenon that estimating the risk of a classifier , and estimating the tuning parameters to minimise that risk , are two rather different problems , requiring the use of different methodologies .",
    "i first met peter as a phd student when he visited cambridge in 2002 .",
    "i spent an hour or so discussing a problem i was working on that involved using ideas of james  stein estimation to find small confidence sets for the location parameter of a spherically symmetric distribution @xcite .",
    "i was blown away at the speed with which he was able to understand where my difficulties lay , and make helpful suggestions . shortly afterwards",
    ", he invited me to spend six weeks at the australian national university in canberra in july ",
    "august 2003 .",
    "i arrived utterly exhausted after nearly 24 hours in the air , but peter was full of energy when he kindly picked me up from the bus station .",
    "almost the first thing he said to me was : ` i ve got a problem i thought we could think about ... ' , and he proceeded to take out a pen and pad of paper ; one could nt help but be drawn along by his enthusiasm for research .    everything with peter happened at breakneck speed , whether it was dashing around the supermarket , a driving tour through the rural australian capital territory or , of course , writing papers .",
    "many of his collaborators will have experienced discussing a problem with peter one evening and returning to the office the following morning to find that he had typed up a draft manuscript that would form the basis of a joint paper .",
    "his prose was always elegant , and he had a wonderful ability to see his way through technical asymptotic arguments , aided by almost physicist - like intuition for what ought to be true .",
    "one of my favourite peter stories , which i initially heard second - hand but which he later confirmed was true , concerned a time when he d been asked to teach an elementary statistics course to students with really very little quantitative background . realising that he d lost some of the students along the way , and in order not to ruin their grades , peter had a cunning idea and spent the last class before the final going through the problems that he d set on the exam . to his horror",
    ", however , the students still flunked the exam .",
    "when peter bumped into one of the students and asked in bemusement ` what happened ?",
    "i went through the questions in the last class ' , the student replied ` yes , but you did them in a different order ' !",
    "peter had seemingly boundless energy and capacity to work , but he was also a very gentle individual in many ways .",
    "he was extraordinarily generous to others , and particularly junior researchers for whom he did so much .",
    "he was a remarkable person and i miss him very deeply .",
    "99 biau , g. and devroye , l. ( 2010 ) on the layered nearest neighbour estimate , the bagged nearest neighbour estimate and the random forest method in regression and classification _ j. mult",
    "_ , * 101 * , 24992518 .",
    "dmbgen , l. , samworth , r. j. and schuhmacher , d. ( 2013 ) stochastic search for semiparametric linear regression models . in _ from probability to statistics and back : high - dimensional models and processes  a festschrift in honor of jon a. wellner_. eds m. banerjee , f. bunea , j. huang , v. koltchinskii , m. h. maathuis , pp ."
  ],
  "abstract_text": [
    "<S> in this article , i summarise peter hall s contributions to high - dimensional data , including their geometric representations and variable selection methods based on ranking . </S>",
    "<S> i also discuss his work on classification problems , concluding with some personal reflections on my own interactions with him . </S>"
  ]
}