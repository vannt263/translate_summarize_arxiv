{
  "article_text": [
    "in our previous work @xcite , we developed a reduced order modeling ( rom ) technique that could be applied to problems with a stochastic component . in that work ,",
    "the application was to radiation transport problems .",
    "the approach to generate the model was to first obtain a set of low - noise training data corresponding to different physical scenarios . in the radiation transport problem , this was done by performing high fidelity and computationally intensive monte - carlo simulations to generate radiation energy spectra at different locations relative to the radiation source for different radiation source mixtures .",
    "principal component analysis ( pca ) @xcite ( or equivalently the proper orthogonal decomposition ( pod @xcite ) was then applied to generate a set of basis functions that could compactly represent all of the training data .",
    "this basis was then used to make accurate estimates of the radiation energy spectrum given noisy data from some new scenario .",
    "depending on the application , the noisy data , which we call the `` trial '' data , could come from experimental measurement or a low fidelity stochastic simulation .",
    "the rom estimates were obtained using an @xmath0 projection of this trial data onto the space spanned by the basis functions . in the radiation transport problem , we found that the energy spectra associated with a new mixture of materials could be calculated with four orders of magnitude reduction in the computational cost required relative to that of a training data simulation .",
    "although @xcite did define the basic procedure for generating a rom of a stochastic problem , there were several issues that were not addressed .",
    "the formulation in @xcite did not properly account for noise in the generation of the basis functions , had no mechanism for selecting the number of basis functions in the rom , and used @xmath0 projection of the trial data without mathematical justification . in this work ,",
    "a probabilistic approach is used to remedy these deficiencies . to generate the basis ,",
    "the stochastic formulation of pca , known as probabilistic principal component analysis ( ppca ) @xcite , is used .",
    "ppca identifies the noise in the training data , which can not be estimated with conventional pca .",
    "we have also improved ppca to relax the standard assumption that the latent variables have unit variance .",
    "the new formulation provides more physical insight into the eigenvalues of the ppca and their relation to the variance of the latent variables .",
    "this information was necessary for the new projection procedure that was developed .",
    "the probabilistic formulation also provides a method for selecting the number of basis functions to include in the rom .",
    "this can be done by combining ppca with bayesian model selection @xcite criterion . here",
    "the bayesian information criteria ( bic ) @xcite is used to identify the optimal number of basis functions to include in the rom , and it is demonstrated that this approach reliably chooses the number of basis functions that can be identified given that the training data itself includes noise .",
    "lastly , a new approach for projecting the trial data is derived .",
    "this approach uses prior information obtained from the ppca of the training data to improve the projection of the trial data . in our previous work ,",
    "@xmath0 projection was used , which basically corresponds to a projection with no prior knowledge of the projection coefficients . in the following",
    ", it is demonstrated that using the training data to define a prior for the projection coefficients leads to significantly improved results when the trial data has more noise than the training data , which is typically the case .",
    "the paper is organized as follows .",
    "first the framework for the reduced order model generation and the modified ppca approach is derived .",
    "this is then followed by a discussion of the model selection approach ( bic ) and the derivation of the method for projecting the trial data . to demonstrate the benefits of this new probabilistic formulation ,",
    "the rom procedure is applied to a simple stochastic model problem , and predictions from the rom are compared to our previous approach and to noise - free data , which was known for the model problem .",
    "the rom is formulated assuming the data is created by a process of the following form @xmath1 where @xmath2 is a single data realization for a particular physical scenario which could be obtained either from a numerical simulation or an experiment .",
    "the dimension of @xmath2 is @xmath3 , which depends on the physical problem being studied .",
    "for example , in the radiation transport problem , @xmath3 was the number of energy bins used to describe the radiation energy spectrum . the functions @xmath4 $ ] are basis functions that are scenario independent and the @xmath5 s are latent variables that vary depending on the physical scenario .",
    "the @xmath5 s are assumed to be random variables .",
    "this implies that there is a probability associated with the occurrence of each physical scenario .",
    "the mean of the latent variables is assumed to be zero such that @xmath6 is the mean of the data over all scenarios .",
    "@xmath6 is thus a scenario independent constant vector .",
    "@xmath7 is a random variable that represents the noise in the process .",
    "this noise could represent noise in the experimental measurements or in the stochastic numerical simulation approach used to generate the data .",
    "the noise is assumed to be generated by a zero - mean gaussian process with covariance @xmath8 where @xmath9 is the identity matrix of dimension @xmath3 .",
    "although the data is assumed to be generated by a process of the form given by , none of the parameters of the model ( @xmath10 ) are known .",
    "the first step of the reduced order modeling process is to generate a set of `` training data '' that can be used to estimate these parameters .",
    "the training data is a set @xmath11 for @xmath12 of realizations of the process .",
    "the scenarios associated with these realizations are chosen randomly according to the probability density function predicting the occurrence of any given scenario .",
    "the generation of the data can be through either numerical simulation or experiment and both are assumed to also include random noise .      to estimate @xmath13 and @xmath6 given @xmath14 , ppca is used .",
    "our formulation of ppca is similar to that of @xcite .",
    "the main difference is that in @xcite it is assumed that the latent variables , @xmath15 , are uncorrelated and follow a gaussian distribution with unit covariance . in the following",
    ", it is also assumed that the latent variables are uncorrelated and follow a gaussian distribution , but the covariance is not a - priori assumed to be 1 .",
    "instead the variances , @xmath16 , are estimated as part of the derivation .",
    "the derivation is similar to the original derivation provided in @xcite so in the following , a condensed derivation is given that highlights the main differences .",
    "bayes formula to estimate the unknowns , @xmath13 , @xmath17 , @xmath18 , and @xmath6 in the model is @xmath19 where @xmath20 is the posterior distribution , @xmath21 is the prior distribution and @xmath22 is the likelihood distribution .",
    "the ppca uses a maximum likelihood estimator ( mle ) @xcite to find the unknown parameters assuming no prior knowledge about their values i.e. the prior is assumed to be uniform .",
    "the mle is thus obtained by maximizing the log - likelihood function , @xmath23 .    in order to obtain the likelihood function",
    ", the probability distribution of an individual realization , @xmath2 , conditioned on @xmath24 , first needs to be identified .",
    "this distribution , @xmath25 , is called the predictive distribution and can be obtained using the following relations @xmath26 where the integral is a multidimensional integral over all components of the vector @xmath15 . using the assumption that the noise in is gaussian with zero mean , the probability distribution of @xmath2 conditioned on the latent variable , @xmath15 , and the parameters @xmath27 , @xmath6 , and @xmath13 is given by @xmath28 where the notation @xmath29 indicates a gaussian distribution with mean @xmath30 and co - variance matrix @xmath31 .    as mentioned above , in this analysis",
    "the only difference is that the latent variables are assumed to be uncorrelated and follow a zero - mean gaussian distribution with a covariance of @xmath18 instead of 1 , thus @xmath32 where @xmath33 is an @xmath34 diagonal matrix with the values @xmath35 on the diagonal .",
    "based on this assumption , one can show that the predictive distribution is gaussian of the form @xmath36 , which is similar to @xcite except for the introduction of the diagonal matrix @xmath33 .",
    "the likelihood distribution , @xmath37 for the set of training data , @xmath38 , for @xmath12 is the product of the individual predictive distributions .",
    "the log likelihood can be shown to be @xmath39 where @xmath40 denotes the determinant of a matrix .",
    "as the prior is uniform , the most probable values of the posterior distribution can be determined by maximizing @xmath41 with respect to the unknown parameters , @xmath13 , @xmath6 , @xmath17 . in all the following",
    ", the subscript @xmath42 indicates these most probable values .",
    "the maximization process gives the following estimate for the mean @xmath6 , @xmath43 the estimate of @xmath27 is @xmath44 where the @xmath45 s are the eigenvalues of the data covariance matrix , + @xmath46 .",
    "the maximum likelihood estimate for the @xmath47 can be interpreted as the average magnitude of the eigenvalues of dimension greater than @xmath48 .",
    "these eigenvalues can only be caused by noise , as there are only @xmath48 latent variables .",
    "minimizing with respect to @xmath13 , the following equation can be obtained .",
    "@xmath49 where @xmath50 is a @xmath51 matrix whose columns are given by a complete subset of ( orthonormal ) eigenvectors of the data covariance matrix @xmath52 , @xmath53 is the @xmath34 diagonal matrix consisting of the first @xmath48 largest eigenvalues of @xmath52 , and @xmath54 is an arbitrary @xmath34 orthonormal matrix .",
    "this equation is almost the same as in @xcite , except for the @xmath55 term . in @xcite ,",
    "@xmath54 was chosen to be the identity matrix , which then determined @xmath13 .",
    "the disadvantage of this choice is that the column vectors of @xmath13 then each have a magnitude determined by the diagonal matrix @xmath56 .",
    "this scaling of the basis function is necessary to ensure that the latent variables all have a variance of unity . in the new formulation ,",
    "we can satisfy by choosing @xmath57 and the estimate for @xmath55 as @xmath58 this allows us to have unit basis functions , and also correctly identifies the variance of the latent variables ( as we confirm in the example problem below ) . manipulating ,",
    "the relation can put in the following form @xmath59 that shows that the eigenvalues of the covariance matrix @xmath52 are the variance of latent variables summed with the variance of the measurement error .",
    "the first @xmath48 eigenvalues consists of both variances , however the eigenvalues greater than @xmath48 are strictly due to random measurement error .      in the previous section ,",
    "the model parameters were estimated assuming that the dimension of the rom , @xmath48 , was a known , fixed number . in our previous work , no systematic method for choosing @xmath48 was provided .",
    "ppca together with bayesian model selection criteria can be used to predict the number of basis functions required for the rom .",
    "there are many different bayesian model selection criteria @xcite . here the bayesian information criterion ( bic ) @xcite is used .",
    "bic chooses the value of @xmath48 that minimizes the following function @xmath60 where @xmath61 is the maximum value of the likelihood distribution and the term in the outer parenthesis in is the number of estimatable parameters in the model , both of which depend on @xmath48 .",
    "the number of estimatable parameters arise from , @xmath62 , @xmath63 and @xmath64 .",
    "there are @xmath65 parameters in @xmath62 . here",
    "the @xmath66 comes from the fact that the first basis vector is required to be normalized to have magnitude 1 so when @xmath67 one can only choose @xmath66 independent variables . because of the requirement of orthogonality the number of free parameters in choosing a basis vector decreases by 1 for each additional basis vector .",
    "this results in the number of free parameters in @xmath13 being @xmath68 .",
    "the number of parameters in @xmath63 and @xmath64 are @xmath3 and 1 , respectively giving the total shown in parentheses above .",
    "@xmath61 is obtained by inserting the maximum likelihood estimates of the parameters , , , and ) in . following the simplification techniques in @xcite but with our maximum likelihood results ,",
    "this becomes    @xmath69    to find the most probable value for @xmath48 , @xmath70 is calculated as a function of @xmath48 , @xmath71 $ ] , and the value of @xmath48 that minimizes the function is chosen .",
    "the above sections determined the model parameters , @xmath13 , @xmath6 and @xmath48 . in this section , a latent variable vector @xmath15",
    "is estimated given a `` trial '' data vector @xmath2 that is obtained from a new scenario drawn from the distribution of scenario probabilities .",
    "@xmath2 also includes noise , @xmath72 , which is drawn from a zero - mean gaussian distribution @xmath73 where the magnitude of this noise , @xmath74 , is assumed to be different ( typically larger ) than that of the training data . in our previous work @xmath0 projection of the trial data",
    "was used to estimate the latent variables and no estimate was given for @xmath74 .",
    "here the latent variables are estimated using bayesian parameter estimation with a gaussian prior .",
    "the estimate of @xmath75 obtained from the training data is used as the covariance of the prior on the latent variables .    assuming the model given by holds for the trial data as well , the probability distribution of @xmath2 is , @xmath76 as assumed before , the probability of @xmath15 for a given scenario is gaussian with mean zero and covariance @xmath33 @xmath77 to estimate the probability distribution of @xmath15 and @xmath78 conditioned on the observed data , @xmath2 , and parameters @xmath6 , @xmath33 , and @xmath13 , bayes theorem is used . applying bayes theorem assuming that @xmath33 and @xmath78 are independent we have , @xmath79 assuming a uniform prior distribution for @xmath80 , the log posterior",
    "can then be obtained from and as @xmath81 setting the derivatives of @xmath82 with respect to @xmath15 and @xmath78 to zero to find the maximum gives @xmath83 and @xmath84 equation @xmath85 and @xmath86 are a system of non - linear equations with unknowns @xmath87 and @xmath88 .",
    "the pair of equations can be solved using a fixed point iteration where it is first assumed that @xmath89 is zero , then is used to calculate @xmath90 .",
    "equation , which can be interpreted as a calculation of the noise in the data assuming the true data is given by @xmath91 , can then be used to calculate @xmath89 .",
    "this new value of @xmath89 is then used in @xmath85 and the process is repeated until @xmath85 and @xmath86 are satisfied to a specified tolerance .",
    "if @xmath88 is small relative to the values of @xmath18 , then the matrix in parentheses in is essentially the identity matrix and the @xmath0 projection result , @xmath92 , is recovered .",
    "this is the approach that was used in our previous work and is also the result that would be obtained assuming a uniform prior on @xmath15 instead of a gaussian prior .",
    "however when the noise in the data is large relative to the variance of a latent variable , i.e. @xmath93 is large , the gaussian - prior projection reduces the magnitude of the @xmath0 projection value of @xmath94 to account for the fact that the noise in the data is causing an estimation for @xmath94 that is larger than the expected variation of that latent variable . in the model problem , we show that this significantly improves the accuracy of the projection for these conditions .",
    "this section illustrates the above discussed rom techniques for a model problem where the data is generated using a model of the form given by i.e. the data is generated as a linear combination of a finite number of basis functions and latent variables with a mean vector and added random noise .",
    "as all of the parameters of the data generation are known , the rom process can be validated by comparing the estimated parameters to those used to generate the data .",
    "the @xmath48 basis functions used to generate the data are discrete sine waves given by @xmath95,\\ ] ] where @xmath96 is a vector of @xmath97 uniformly spaced points from the domain @xmath98 $ ] including endpoints . in the above ,",
    "the norm @xmath99 is the euclidean vector norm such that @xmath100 . with this normalization ,",
    "the peak value of the basis functions is @xmath101 .",
    "the latent variables , @xmath102 , were sampled from gaussian distributions with variances of @xmath103 .",
    "the values of @xmath104 were given by @xmath105.\\ ] ] unless stated otherwise , @xmath106 basis functions and latent variables were used to create the data .",
    "the mean , @xmath6 , was a vector of ones .",
    "the noise vector , @xmath7 was also sampled from a gaussian distribution , either @xmath107 for the training data or @xmath108 for the trial data .",
    "the data generation process was repeated @xmath109 times to generate the training data set @xmath14 . to investigate the effect of noise in the training data ,",
    "two training data sets were studied , one with @xmath110 and the other with @xmath111 .",
    "[ data_0_1 ] shows a typical realization of a data vector from the two training data sets . from fig .",
    "[ data_0_1](a ) it can be seen that the case of @xmath110 the noise in the data is significant with a magnitude on the order @xmath112 . similarly in fig .",
    "[ data_0_1](b ) , the noise magnitude is @xmath113 .",
    "( a ) = 1/10 ( b ) @xmath17 = 1/400.,title=\"fig:\",width=207 ] ( b ) = 1/10 ( b ) @xmath17 = 1/400.,title=\"fig:\",width=207 ]    averaging over all of the data vectors of the data set @xmath14 determines the most probable mean vector @xmath63 .",
    "this is shown in fig .",
    "[ mean ] for both data sets .",
    "the fluctuations in the mean should scale as @xmath114 which is equal to 0.0088 and 0.0061 for @xmath115 and @xmath116 respectively .",
    "this is in good agreement with what is observed in fig .",
    "[ mean ] . the dominant source of error in determining",
    "the mean is not the noise in the data , but rather determining the average outcome of the scenarios .",
    "for this reason , the fluctuations in both estimated means are of similar magnitude .     and @xmath117 training data.,width=302 ]      according to in section [ mppca ] ,",
    "the estimate for the most probable basis functions are the dominant subset of @xmath48 eigenvectors of the data covariance matrix , @xmath118 .",
    "[ eig ] shows the eigenvalue spectrum for the two sets of training data with @xmath115 and @xmath116 .",
    "based on , the eigenvalues are expected to decay like @xmath119 because of the @xmath35 term and then plateau at a value of @xmath120 .",
    "the predicted eigenvalues based on are also shown on the plot .",
    "the curves agree well indicating that the new formulation of ppca accurately predicts the dependence of the eigenvalues on both @xmath35 and @xmath121 .     and @xmath117 training data .",
    "predicted spectra based on are also shown.,width=302 ]    fig .",
    "[ basisfun1 ] shows the first , third , fifth , and seventh basis functions for the @xmath122 and @xmath117 training data .",
    "the ppca is able to extract basis functions that are less affected by noise than the actual data ( compare the magnitude of the noise in fig .",
    "[ basisfun1]a for @xmath122 with the noise in fig .",
    "[ data_0_1]a ) . even for the 7th mode in the case of @xmath115 shown in fig .",
    "[ basisfun1]d , which had @xmath123 being much smaller than @xmath120 , the ppca is able to roughly obtain the correct functional form .",
    "this is primarily because of the large number of training data vectors used ( @xmath109 ) , which allows the ppca to detect the form of the basis in spite of the numerical noise .",
    "this indicates that one can obtain accurate basis functions for the rom by either reducing the noise in the data or by increasing the number of data vectors in the training data .",
    "= 1/10 and @xmath17 = 1/400.,title=\"fig:\",width=207](a )   = 1/10 and @xmath17 = 1/400.,title=\"fig:\",width=207](b )   = 1/10 and @xmath17 = 1/400.,title=\"fig:\",width=207](c )   = 1/10 and @xmath17 = 1/400.,title=\"fig:\",width=207](d )      as described in section [ dimension ] , the bayesian information criteria ( bic ) given in is used determine the dimensionality @xmath48 .",
    "[ bic_fig ] shows @xmath70 as a function of @xmath48 for the two training data sets .",
    "the minimum of @xmath70 occurs at @xmath124 and @xmath125 for the @xmath122 and @xmath117 training data respectively . examining fig .",
    "[ eig ] , it seems that bic chooses @xmath48 at the point of the change in decay rate of the eigenvalues . for @xmath122 ,",
    "this occurs at @xmath124 even though only the first three latent variables have @xmath126 . for @xmath117 ,",
    "this occurs at @xmath125 , even though only the first eight latent variables have @xmath127 .",
    "( a ) for the @xmath110 training data ( minimum is at 5 basis functions ) ( b ) for the @xmath111 training data ( minimum is at 10 basis functions).,title=\"fig:\",width=207](a )   ( a ) for the @xmath110 training data ( minimum is at 5 basis functions ) ( b ) for the @xmath111 training data ( minimum is at 10 basis functions).,title=\"fig:\",width=207](b )    once the number of latent variables is estimated , can be used to estimate @xmath27 and to estimate @xmath18 .",
    "table  [ var_less ] summarizes the estimated and true values of @xmath128 and @xmath18 for the two sets of training data .",
    "there are only five values of @xmath18 estimated for the training data with @xmath115 because bic estimated that @xmath129 for this data .",
    "the first thing to observe from the data is that the estimates of @xmath18 are accurate ; the percentage errors are less than 5% in all cases and in most cases the error is less than 2% .",
    "this is also true of the estimates of @xmath120 ; the errors are less than 6% for both training data sets .",
    "this confirms that the new formulation of ppca correctly estimates @xmath18 .",
    "another observation from the estimated values of @xmath18 is that the errors do not vary significantly between the @xmath27 = 1/400 and 1/10 data sets .",
    "this implies that for this particular data set , the main source of error is not the noise in the measurements , but rather the number of data vectors used to create the data set ( @xmath130 ) .",
    "both data sets used @xmath131 so the accuracy of the estimates of @xmath18 are similar .",
    ".estimated and true values of @xmath18 and @xmath120 for the two training data sets .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     to verify that the above identified trends are not particular to the trial data vector examined , for 10000 trial data realizations an error was computed by comparing the true signal to the projections . for the above described cases , the @xmath0 norm of the error , @xmath132 is shown in fig .",
    "[ training_error ] for each realization .",
    "the horizontal red and blue curves in the figure are the mean value of the error over the 10000 realizations for the gaussian - prior projection and the @xmath0 projection respectively .",
    "for all cases , the gaussian - prior projection on average has less error than the @xmath0 projection .",
    "consistent with the differences seen in fig .",
    "[ training_reconstruction]b and c , this difference is most significant for the case with @xmath133 using the roms with a larger numbers of basis functions . for the case of @xmath117 ,",
    "the average error for the gaussian - prior projection was 0.74 and the average error for @xmath0 projection was 1.38 . for @xmath134 , the errors were 0.74 and 2.84 .",
    "note that the scale on fig .",
    "[ training_error]c is larger because the @xmath0 error was large for this case . comparing the mean gaussian - prior projection error between plots",
    "a , b , and c shows that the gaussian - prior projection errors are relatively insensitive to the number of basis functions in the model .",
    "figure  [ training_error]d shows that as @xmath135 decreases the mean error of both approaches decreases .",
    "norm of the error of gaussian - prior projections and @xmath0 projections for 10000 trial realizations : ( a ) @xmath133 , @xmath122 , ( b ) @xmath133 , @xmath117 , ( c ) @xmath133 , @xmath134 , ( d ) @xmath136 , @xmath117 . the red line represents the average error for gaussian - prior projection and the blue line represents the average error for @xmath137 projection.,title=\"fig:\",width=207](a )   norm of the error of gaussian - prior projections and @xmath0 projections for 10000 trial realizations : ( a ) @xmath133 , @xmath122 , ( b ) @xmath133 , @xmath117 , ( c ) @xmath133 , @xmath134 , ( d ) @xmath136 , @xmath117 .",
    "the red line represents the average error for gaussian - prior projection and the blue line represents the average error for @xmath137 projection.,title=\"fig:\",width=207](b ) +   norm of the error of gaussian - prior projections and @xmath0 projections for 10000 trial realizations : ( a ) @xmath133 , @xmath122 , ( b ) @xmath133 , @xmath117 , ( c ) @xmath133 , @xmath134 , ( d ) @xmath136 , @xmath117 .",
    "the red line represents the average error for gaussian - prior projection and the blue line represents the average error for @xmath137 projection.,title=\"fig:\",width=207](c )   norm of the error of gaussian - prior projections and @xmath0 projections for 10000 trial realizations : ( a ) @xmath133 , @xmath122 , ( b ) @xmath133 , @xmath117 , ( c ) @xmath133 , @xmath134 , ( d ) @xmath136 , @xmath117 .",
    "the red line represents the average error for gaussian - prior projection and the blue line represents the average error for @xmath137 projection.,title=\"fig:\",width=207](d )",
    "in this work , statistical approaches were used for each step of the rom process .",
    "basis functions were generated using the probabilistic formulation of the principal component analysis ( ppca ) .",
    "the conventional ppca was improved so that the derived basis functions are orthonormal and the variance of the latent variables is estimated rather than assumed to be one .",
    "these improvements allowed a more intuitive interpretation of the eigenvalues of the ppca and their relation to the noise in the system and the variance of the latent variables .",
    "bayesian model selection was used to select the number of basis functions for the rom .",
    "the bayesian information criteria ( bic ) was shown to give reasonable predictions of the number of detectable latent variables in the model , @xmath48 .",
    "training data with lower noise allowed a larger number of latent variables to be identified , and bic correspondingly gave a larger estimate for @xmath48 .    using the information obtained from the improved ppca to create a gaussian prior for the latent variables , bayesian parameter estimation was used to estimate the latent variables associated with a data vector from a new scenario with an unknown amount of noise .",
    "equations were derived that simultaneously predict the most - likely latent variable vector and the amount of noise in the new scenario .",
    "the model problem showed that the true ( noise - free ) signal could be accurately reproduced from noisy data using this approach .",
    "this approach also outperforms a standard @xmath0 projection to determine the latent variables especially when the noise in the data vector is large and there are many basis functions in the model .",
    "the above formulation provides a comprehensive probabilistic approach for performing reduced order modeling of stochastic systems .",
    "these reduced order models can then be used to provide rapid , accurate predictions for stochastic problems where repeated analyses of similar scenarios must be performed .",
    "this manuscript was authored in part by national security technologies , llc , under contract de - ac52 - 06na25946 with the u.s .",
    "department of energy and supported by the site - directed research and development program .",
    "the united states government retains and the publisher , by accepting the article for publication , acknowledges that the united states government retains a nonexclusive , paid - up , irrevocable , worldwide license to publish or reproduce the published form of this manuscript , or allow others to do so , for united states government purposes . the u.s .",
    "department of energy will provide public access to these results of federally sponsored research in accordance with the doe public access plan ( http://energy.gov/downloads/doe-public-access-plan ) .",
    "doe / nv/25946 - -3089 .",
    "i.  udagedara , b.  t. helenbrook , a.  luttman , s.  e. mitchell , reduced order modeling for accelerated monte carlo simulations in radiation transport , applied mathematics and computation 267 ( 2015 ) 237251 .",
    "t.  toni , d.  welch , n.  strelkowa , a.  ipsen , m.  p. stumpf , approximate bayesian computation scheme for parameter inference and model selection in dynamical systems , journal of the royal society interface 6  ( 31 ) ( 2009 ) 187202 .",
    "d.  posada , t.  r. buckley , model selection and model averaging in phylogenetics : advantages of akaike information criterion and bayesian approaches over likelihood ratio tests , systematic biology 53  ( 5 ) ( 2004 ) 793808 .",
    "r.  a. fisher , on the mathematical foundations of theoretical statistics , philosophical transactions of the royal society of london .",
    "series a , containing papers of a mathematical or physical character 222 ( 1922 ) 309368 .",
    "h.  d.  g. acquah , comparison of akaike information criterion ( aic ) and bayesian information criterion ( bic ) in selection of an asymmetric price relationship , journal of development and agricultural economics 2  ( 1 ) ( 2010 ) 001006 ."
  ],
  "abstract_text": [
    "<S> in our previous work , a reduced order model ( rom ) for a stochastic system was made , where noisy data was projected onto principal component analysis ( pca)-derived basis vectors to obtain an accurate reconstruction of the noise - free data . that work used techniques designed for deterministic data ; </S>",
    "<S> pca was used for the basis function generation and @xmath0 projection was used to create the reconstructions . in this work , </S>",
    "<S> probabilistic approaches are used . </S>",
    "<S> the probabilistic pca ( ppca ) is used to generate the basis , which then allows the noise in the training data to be estimated . </S>",
    "<S> ppca has also been improved so that the derived basis vectors are orthonormal and the variance of the basis expansion coefficients over the training data set can be estimated . </S>",
    "<S> the standard approach assumes a unit variance for these coefficients . </S>",
    "<S> based on the results of the ppca , model selection criteria are applied to automatically choose the dimension of the rom . in our previous work , </S>",
    "<S> a heuristic approach was used to pick the dimension . </S>",
    "<S> lastly , a new statistical approach is used for the projection step where the variance information obtained from the improved ppca is used as a prior to improve the projection . </S>",
    "<S> this gives improved accuracy over @xmath0 projection when the projected data is noisy . in addition , the noise statistics for the projected data are not assumed to be the same as that of the training data , but are estimated in the projection process . </S>",
    "<S> the entire approach gives a fully stochastic method for computing a rom from noisy training data , determining ideal model selection , and projecting noisy test data , thus enabling accurate predictions of noise - free data from data that is dominated by noise . </S>"
  ]
}