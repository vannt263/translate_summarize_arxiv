{
  "article_text": [
    "as pointed out in various papers see , e.g. , @xcite stream mining poses unique challenges to machine learning : examples must be efficiently processed one at a time as they arrive from the stream , and an up - to - date predictive model must be available at all times .",
    "incremental learning systems are well suited to address these requirements : the key difference between a traditional ( batch ) learning system and an incremental one is that the latter learns by performing small adjustments to the current predictor .",
    "each adjustment uses only the information provided by the current example in the stream , allowing an efficient and timely update of the predictive model .",
    "this is unlike batch learning , where training typically involves a costly global optimization process involving multiple passes over the data .",
    "another important feature of stream mining is that the true structure of the problem is progressively revealed as more data are observed . in this context ,",
    "nonparametric learning methods , such as decision trees or nearest neighbour ( nn ) , are especially effective , as a nonparametric algorithm is not committed to any specific family of decision surfaces . for this reason , incremental algorithms for decision trees  @xcite and nearest neighbour  @xcite are extremely popular in stream mining applications .    since in nonparametric methods the model size keeps growing to fit the stream with increasing accuracy , we seek a method able to improve predictions while growing the model as slowly as possible .",
    "however , as the model size can not grow unbounded , we also introduce a variant of our approach that prevents the model size from going beyond a given limit . in the presence of concept drift  @xcite ,",
    "bounding the model size may actually improve the overall predictive accuracy , provided the data point supporting the model are selected in the right way .    a further issue in stream mining concerns the way prediction methods",
    "are evaluated see , e.g. , @xcite for a discussion . in this paper",
    ", we advocate the use of the online error ( also called sequential risk , prequential risk , or prequential error  @xcite ) .",
    "this quantity measures the average of the errors made by the sequence of incrementally learned models , where one first tests the current model on the next example in the stream and then uses the same example to update the model .",
    "the sequential risk is therefore measured on each individual stream and does not specifically require stochastic assumptions on the way the stream is generated .    in this paper , we propose a novel incremental and nonparametric approach for the classification of data streams .",
    "we present four different instances of our approach ( called base , base - adj , auto , and auto - adj ) characterized by an increasing degree of adaptivity to the data .",
    "in particular , auto - adj is fully parameterless , a feature especially important in streaming settings where tuning is a hard task .",
    "even though our algorithms are instance - based like nearest neighbour , the learned models are significantly smaller than those produced by competing baselines and more accurate when the online performance is measured against the model size .",
    "finally , our methods ( except base ) are natively multiclass and can dynamically accommodate new classes as they appear in the stream .    in a nutshell , our algorithms work by incrementally covering the input space with balls of possibly different radii .",
    "each new example that falls outside of the current cover becomes the center of a new ball .",
    "examples are classified according to nn over the ball centers , where each ball predicts according to the majority of the labels of previous examples that fell in that ball .",
    "the set of balls is organized in a tree structure  @xcite , so that predictions can be computed in time logarithmic in the number of balls . in order to increase the ability of the model to fit new data ,",
    "the radii of the balls shrink , thus making room for new balls .",
    "the shrinking of the radius may depend on time or , in the more sophisticated variants of our algorithms , on the number of classification mistakes made by each ball classifier .",
    "similarly to decision trees , where leaves are split according to their impurity , our method locally adapts the complexity of the model by allocating more balls in regions of the input space where the stream is harder to predict .",
    "a further improvement concerns the relocation of the ball centers in the input space : as our methods are completely incremental , the positioning of the balls depends on the order of the examples in the stream , which may result in a model using more balls than necessary . in order to mitigate this phenomenon , while avoiding a costly global optimization step to reposition the balls , we also consider a variant in which a k - means step is used to move the center of a ball being updated towards the median of the data points that previously fell in that ball . a further modification which we consider is aimed at keeping the model size bounded even in the presence of an arbitrarily long stream .",
    "this is achieved by introducing a randomized mechanism for discarding balls when the size bound is reached .",
    "specifically , the mechanism discards a ball with probability proportional to the mistake rate of the ball classifier . the underlying idea is to get rid of the model parts that contribute the most to the global error and",
    "may replaced by a better arrangement of balls .    in summary , we introduce a simple and flexible approach for nonparametric classification of data streams . our approach is fully modular : we predict using majority voting , but a fully trainable classifier could be used instead .",
    "the simplest version of our approach , applicable to streams with binary labels , enjoys strong theoretical guarantees : its mistake rate on any arbitrary stream converges to that of the best classification function that satisfies a certain regularity condition .",
    "the more complex versions of our approach learn multiclass classifiers without knowning the number of distinct labels in advance .",
    "we empirically show that our methods are excellent at trading - off classification accuracy with model size .",
    "our most sophisticated method is fully parameterless .",
    "finally , we show that a simple modification of our approach allows to keep the model size bounded , outperforming other methods measured at the same value of model size .",
    "the rest of the paper is organized as follows .",
    "section  [ sc : related ] discusses related work . in section  [ sc :",
    "setting ] , we define the problem setting . in section  [ sc : abc ] , we present our nonparametric classification approach . in section  [ sc : ci ] , we discuss the theoretical properties of our approach and derive a formal performance guarantee for the simplest algorithm .",
    "we then introduce three more sophisticated versions that are empirically more effective . in section  [ sc : exp ] , we test the behaviour of our algorithms against state - of - the - art baselines . in section  [ sec : fix_bud ] , we introduce a simple modification of our approach to keep the model size bounded . finally , section  [ s : concl ] concludes the paper .",
    "within the vast area of stream mining  @xcite , we focus our analysis of related work on the subarea that is most relevant to this study : nonparametric methods for stream classification .",
    "the most important approaches in this domain are :    * incremental decision and rule tree * learning systems , such as very fast decision tree ( vfdt )  @xcite and decision rules ( rules )  @xcite which use an incremental version of the split function computation see also  @xcite .    * incremental variants of nn * , such as condensed nearest neighbour ( cnn )  @xcite that stores only the misclassified instances , lazy - tree ( l - tree )  @xcite condensing historical stream records into compact exemplars , and iblstreams  @xcite , an instance - based learning algorithms removing outliers or examples that have become redundant .    * incremental kernel - based * algorithms , such as the kernel perceptron  @xcite with gaussian kernels .",
    "note that our methods do not belong to any of the above three families : they do not perform a recursive partition of the feature space as decision trees , they do not allocate ( or remove ) instances based on the heuristics used by iblstreams , and they do not use kernels .    as we explain next , our most basic algorithm is a variant for classification tasks of the algorithm proposed in  @xcite for nonparametric regression in a streaming setting .",
    "a similar algorithm was previously proposed in  @xcite and analyzed without resorting to stochastic assumptions on the stream generation .",
    "a preliminary instance of our approach , without any theoretical analysis , was developed in  @xcite for an action recognition application in video feeds .",
    "our analysis applies to streams of data points belonging to an arbitrary metric space and depends on the _ metric dimension _ of data points in the stream .",
    "this notion of dimension extends to general metric spaces the traditional notions of dimension ( e.g. , euclidean dimension and manifold dimension ) @xcite .",
    "the metric dimension of a subset @xmath0 of a metric space @xmath1 is @xmath2 if there exists a constant @xmath3 such that , for all @xmath4 , @xmath0 has an @xmath5-cover of size at most @xmath6 ( an @xmath5-cover is a set of balls of radius @xmath5 whose union contains @xmath0 ) . in practice",
    ", the metric dimension of the stream may be much smaller than the dimension of the ambient space @xmath7 .",
    "this is especially relevant in case of nonparametric algorithms , which typically have a bad dependence on the dimensionality of the data .",
    "note that our algorithms do not require knowledge of @xmath2 : the metric dimension of the stream is automatically estimated from the data .",
    "the learner receives a sequence @xmath8 of examples , where each data point @xmath9 is annotated with a label @xmath10 from a set @xmath11 of possible class labels , which may change over time .",
    "the learner s task is to predict each label @xmath10 minimizing the overall number of prediction mistakes over the data stream .",
    "we derive theoretical performance guarantees for base , the simplest algorithm in our family ( algorithm  [ alg : base ] ) , without making stochastic assumptions on the way the examples in the stream are generated .",
    "note that this is a very strong type of guarantee :",
    "our results hold on _ any _ individual stream of annotated data points .",
    "the adaptive ball covering at the roots of our method was previously used in a theoretical work  @xcite . here , we distillate the main ideas behind that approach in a generic algorithmic approach ( the template algorithm  [ alg : main ] ) called abacoc ( adaptive ball cover for classification ) .",
    "we then present our methods as specific instances of this generic template .",
    "metric @xmath12 initialize set of ball centers @xmath13 ` initprocedure ( ) ` get input example @xmath14 set @xmath15 @xmath16 add new class on the fly let @xmath17 be the ball in @xmath18 closest to @xmath19 ` ouputprediction`@xmath20 @xmath21=`updateballinformation`@xmath22 @xmath21=`addnewball`@xmath23 `",
    "updateepsilon`(@xmath21 )      our first instance of abacoc is base ( algorithm  [ alg : base ] ) , a randomized variant for binary classification of the itbr ( incremental tree - based regressor ) algorithm proposed in  @xcite .",
    "base shrinks the radius ( line 28 ) of the balls depending on ( 1 ) an estimate of the metric dimension of the stream and ( 2 ) the number of data points so far observed from the stream .",
    "this implies that the radii of all the balls shrink at the same rate . in the prediction phase ,",
    "the ball nearest to the input example is considered and a randomized binary prediction is made based on the class distribution estimate locally computed in the ball .",
    "laplace estimators ( line 5 ) and randomized predictions ( lines 68 ) are new features of base that were missing in itbr .",
    "we now analyze the performance of base using the notion of _ regret _  @xcite .",
    "the regret of a randomized algorithm is defined as the difference between the expected number of classification mistakes made by the algorithm over the stream and the expected number of mistakes made by the best element in a fixed class of randomized classifiers .",
    "a randomized binary classifier is a mapping @xmath24 $ ] , where @xmath25 is the probability of predicting label @xmath26 .",
    "we consider the class @xmath27 of @xmath28-lipschitz predictors @xmath24 $ ] w.r.t .",
    "the metric @xmath12 of the space .",
    "namely , @xmath29 hence , a predictor is lipschitz if , when we perturb the data point @xmath30 , the prediction changes by an amount linear in the perturbation size .",
    "lipschitz functions are a standard reference in the analysis of nonparametric algorithms .",
    "the regret of base generating randomized predictions @xmath31 is defined by ( see also  @xcite ) @xmath32 for the base algorithm we can prove the following regret bound against _ any _ lipschitz randomized classifier , without _ any _ assumption on the way the stream is generated .",
    "moreover , similarly to itbr , the regret upper bound depends on the unknown metric dimension @xmath2 of the space , automatically estimated by the algorithm .",
    "the proof is in the next section  [ app : proof ] .",
    "note that the algorithm does not know @xmath28 , hence the regret bound above holds for all values of @xmath28 simultaneously .",
    "this theorem tells us that base is not an heuristic , but rather a principled approach with a specific performance guarantee .",
    "the performance guarantee implies that , on any stream , the expected mistake rate of base converges to that of the best @xmath28-lipschitz randomized classifier at rate of order @xmath33 .",
    "next , we generalize the base algorithm to multiclass classification , and make some modifications aimed at improving its empirical performance .",
    "@xmath34 ( space diameter ) @xmath13 , @xmath35 , @xmath36 , and @xmath37 @xmath38 set @xmath39 set @xmath40 predict @xmath41 with probability @xmath42 and @xmath43 otherwise .",
    "@xmath44 @xmath45 @xmath46 @xmath47 @xmath48 @xmath36 @xmath49 @xmath50 @xmath51 @xmath52 @xmath16 radius dependent on current time step @xmath53",
    "we use the following well - known fact : if @xmath54 for predicting @xmath55 using a randomized label @xmath56 , then @xmath57 .",
    "even if our algorithm is different from itbr , we can still use the following lemma from itbr analysis  @xcite . in the following ,",
    "we say that a phase ends each time condition in line 15 of base is verified and use @xmath58 to denote the time steps included in phase @xmath59 .",
    "finally , @xmath60 denotes the maximum number of balls used in phase @xmath59 .",
    "[ lem : di ] suppose base is run with parameter @xmath61 .",
    "the following invariants hold throughout the procedure for all phases @xmath62 :    * @xmath63 . * for any @xmath64 we have @xmath65 .",
    "define @xmath66 . unlike the analysis in @xcite , here we can not use a bias - variance decomposition .",
    "so , the key in the proof is to decompose the regret in two terms with behaviour similar to the bias and variance terms in the stochastic setting .",
    "[ lemma : one_epoch ] let @xmath2 be the metric dimension of the set @xmath0 of data points in the stream .",
    "assume that @xmath61 .",
    "then , in any phase @xmath59 and for any @xmath67 we have that @xmath68    we use the notation @xmath69 to say that @xmath19 is assigned to a ball with center @xmath70",
    ". we also denote by @xmath71 the number of points assigned to a ball of center @xmath70 .",
    "define @xmath72 } \\sum_{t \\,:\\ , { \\boldsymbol{x}}_t \\rightarrow { \\boldsymbol{x}}_s } \\ell_t(p).\\ ] ] for each @xmath70 in @xmath60 , we proceed by upper bounding the error as a sum of two components @xmath73 using the definition of @xmath74 and the lipschitz property of @xmath75 , we have @xmath76 the prediction strategy in each ball is equivalent to the approach followed in @xcite ( see also exercise 8.8 in @xcite ) . the only important thing to note",
    "is that the first prediction of the algorithm in a ball is made using the probability of the closest ball , even if it is further than @xmath77 , instead of at random as in the original strategy in @xcite .",
    "it is easy to see that this adds an additional @xmath78 to the regret stated in @xcite .",
    "so we have @xmath79 hence overall we have @xmath80 summing over all the @xmath81 , we have @xmath82 to bound @xmath83 we use lemma  [ lem : di ] , while to bound the last term , we have @xmath84 where @xmath85 .",
    "overall we have @xmath86    we finish with the proof of theorem  [ theo : regret ] .",
    "let @xmath87 denote the number of phases up to time @xmath88 .",
    "let @xmath89 .",
    "we use lemma  [ lemma : one_epoch ] in each phase and sum over the phases , to have @xmath90 where in the second inequality we use jensen s inequality , and in the second to last inequality the first statement of lemma  [ lem : di ] .",
    "@xmath34 ( space diameter ) @xmath91 @xmath92 predict @xmath93 @xmath16 update ball centre on correct prediction @xmath94 @xmath95 updates label counts @xmath96 in the ball @xmath97 using @xmath10    a natural way of generalizing the base algorithm to the multiclass case is by estimating the class probabilities in each ball .",
    "note that this approach is naturally incremental w.r.t .  the number of classes : new bins for counting are created on the fly as data points of new classes arrive .    recall that the base algorithm greedly covers the input space .",
    "in particular , balls are always centered on input points . however , constraining the centers on data points is an intuitively sub - optimal strategy : it might be possible to cover the same region with a smaller number of balls if we could freely move their centers . as a full optimization of the position of the centers",
    "is not realistic in a streaming scenario , we introduce the base - adj variant which makes a partial optimization by using a step of the k - means algorithm  @xcite . more precisely , base - adj ( algorithm  [ alg : base - adj ] , only the main changes w.r.t .",
    "base are shown ) moves the center of each ball towards the average of the correct classified data points falling into it . in this way",
    ", the center of the ball tends to move towards the centroid of a cluster of points of a certain class .",
    "we expect this variant to generate less balls and also to have a better empirical performance .",
    "we drop from base - adj the laplace correction of class estimates and the randomization in the computation of the predicted label .",
    "although these ingredients were used in the theoretical analysis , we noticed that they do not significantly affect the empirical results .",
    "hence , base - adj always predicts the class with the largest class probability estimate ( majority voting on the collected labels ) within the ball closest to the current data point .",
    "@xmath98 @xmath16 wait until at least two different labels fed @xmath99 and initialize label counts @xmath100 initialize label counts * continue * @xmath91 @xmath92 predict @xmath93 @xmath16 shrink radius on errors set @xmath101 @xmath16 update ball centre if correct prediction @xmath102 @xmath103 updates label counts @xmath96 in the ball @xmath97 using @xmath10 @xmath49 , @xmath104 @xmath105 @xmath106 initialize label counts @xmath96 in the ball @xmath107 using @xmath10 @xmath16 radius dependent on mistakes @xmath108    one of the biggest issues with base ( and itbr ) is the use of a common radius for all the balls . in fact , in line 28 of algorithm  [ alg : base ] we have that the radii @xmath109 shrink uniformly with time @xmath110 at rate @xmath111 , where @xmath112 is the estimated metric dimension .",
    "however , we would like the algorithm to use smaller balls in regions of the input space where labels are more irregularly distributed and bigger balls in easy regions , where labels tend to be the same .    in order to overcome this issue , in this section",
    "we introduce two other instances of abacoc : auto and auto - adj . in these variants",
    "we let the radius of each ball shrink at a rate depending on the number of mistakes made by each local ball classifier , lines 20 and 36 in algorithm  [ alg : auto ] .",
    "moreover , in order to get rid of the parameter @xmath34 used to estimate the metric dimension , we initialize the radius of each ball to the distance to its closest ball , line 29 in algorithm  [ alg : auto ] . in other words , everytime a new ball is added its radius is set equal to the distance to the nearest already - existing ball .    auto - adj differs from auto because it implements the same strategy , introduced in base - adj , for updating the position of the centers . note that this strategy , coupled with the shrinkage depending on the number of mistakes , makes",
    "a ball stationary once it is covering a region of the space that contains data points always annotated with the same label .    using balls of different radii",
    "makes it impossible to work with the automatic estimate of the metric dimension used in base , base - adj and itbr .",
    "for this reason , we further simplify the algorithms by resorting to a fixed estimate @xmath98 of the intrinsic dimension @xmath2 as an input parameter .",
    "in this section , we describe baselines and datasets used in the experiments and report on the obtained results .",
    "we conducted an extensive evaluation on standard machine learning datasets for the streaming setting .",
    "generally , in real applications for high - speed data streams , when the system can not afford to revise the current model after each observation of a data point , stream sub - sampling is used to keep the model size and the prediction efficiency under control . in order to emphasize the distinctive features of our approaches ( i.e. , good trade - off between accuracy and model size ) , we tested the online ( prequential ) performance using sub - sampling see algorithm  5 . in this",
    "setting , the algorithms have access to each true class label only with a certain probability . by varying this probability",
    ", we can explore different model sizes for each baseline algorithm and compare the resulting performances .",
    "note also that , while in this work we only consider random sub - sampling , different and more active sampling schedules could be also envisioned .",
    "we considered eleven popular datasets for stream mining listed in table  [ tab : data ] .",
    ".datasets used for benchmarking .",
    "[ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     as we can observe from these tables , auto - adj fix generally outperforms the other methods at the same model sizes .",
    "this is very evident on the datasets with drift , such as electricity , and when the budget limit is very small ( @xmath113 of the total stream length ) .",
    "along the same lines of figure  [ fig : budget_summary ] , we show in figure  [ fig : budget_summaryfix ] the overall performance of the compared methods using all the budget / rate values @xmath114 .",
    "auto - adj fix clearly outperforms all the other methods .",
    "this is not surprising , as auto - adj fix has a better way of choosing the data points supporting the model as opposed to the random selection imposed on the other methods .",
    "we presented an intuitive and easy to implement approach for nonparametric classification of data streams .",
    "our more sophisticated algorithms feature the most appealing traits in stream mining applications : nonparametric classification , incremental learning , dynamic addition of new classes , small model size , fast prediction at testing time ( logarithmic in the model size ) , essentially no parameters to tune .",
    "we empirically showed the effectiveness of our approach in different scenarios and against several standard baselines .",
    "in addition , we proved strong theoretical guarantees on the online performance of the most basic version of our approach .",
    "further research will focus on finding a confidence measure for the prediction scores , which could be used in a semi - supervised framework ( e.g. , active learning ) .",
    "another interesting line of research is concerned with finding a more sophisticated and theoretically justified strategy to keep the model size bounded .",
    "a further , very challenging research line is in the direction of taming the curse of dimensionality problem that affects all nonparametric approaches .",
    "for instance , we plan on investigating notions of local dimensions that allow to perform dimensionality reduction locally and incrementally .",
    "r.  de  rosa , n.  cesa - bianchi , i.  gori , and f.  cuzzolin .",
    "online action recognition via nonparametric incremental learning . in _ proceedings of the 25th british machine vision conference ( bmvc 2014 ) _ , 2014 .",
    "r.  de  rosa , f.  orabona , and n.  cesa - bianchi .",
    "the abacoc algorithm : a novel approach for nonparametric classification of data streams . in _ data mining ( icdm ) , 2015 ieee international conference on_. ieee , 2015 .",
    "p.  duda , m.  jaworski , l.  pietruczuk , and l.  rutkowski .",
    "a novel application of hoeffding s inequality to decision trees construction for data streams . in _ neural networks ( ijcnn ) , 2014 international joint conference on_. ieee , 2014 .",
    "g.  hulten , l.  spencer , and p.  domingos . mining time - changing data streams . in _ proceedings of the seventh acm",
    "sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 97106 .",
    "acm , 2001 .",
    "r.  krauthgamer and j.  r. lee .",
    "navigating nets : simple algorithms for proximity search . in",
    "_ proceedings of the fifteenth annual acm - siam symposium on discrete algorithms _ , soda 04 , pages 798807 , philadelphia , pa , usa , 2004 .",
    "society for industrial and applied mathematics ."
  ],
  "abstract_text": [
    "<S> stream mining poses unique challenges to machine learning : predictive models are required to be scalable , incrementally trainable , must remain bounded in size ( even when the data stream is arbitrarily long ) , and be nonparametric in order to achieve high accuracy even in complex and dynamic environments . </S>",
    "<S> moreover , the learning system must be parameterless traditional tuning methods are problematic in streaming settings and avoid requiring prior knowledge of the number of distinct class labels occurring in the stream . in this paper , we introduce a new algorithmic approach for nonparametric learning in data streams . </S>",
    "<S> our approach addresses all above mentioned challenges by learning a model that covers the input space using simple local classifiers . </S>",
    "<S> the distribution of these classifiers dynamically adapts to the local ( unknown ) complexity of the classification problem , thus achieving a good balance between model complexity and predictive accuracy . </S>",
    "<S> we design four variants of our approach of increasing adaptivity . by means of an extensive empirical evaluation against standard nonparametric baselines , </S>",
    "<S> we show state - of - the - art results in terms of accuracy versus model size . for the variant that imposes a strict bound on the model size , </S>",
    "<S> we show better performance against all other methods measured at the same model size value . </S>",
    "<S> our empirical analysis is complemented by a theoretical performance guarantee which does not rely on any stochastic assumption on the source generating the stream . </S>"
  ]
}