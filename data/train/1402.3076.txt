{
  "article_text": [
    "reaction networks are frequently encountered in several scientific disciplines , such as , chemistry @xcite , systems biology @xcite , epidemiology @xcite , ecology @xcite and pharmacology @xcite .",
    "it is well - known that when the population sizes of the reacting species are small , then a deterministic formulation of the reaction dynamics is inadequate in understanding many important properties of the system ( see @xcite ) .",
    "hence stochastic models are necessary and the most prominent class of such models is where the reaction dynamics is described by a continuous time markov chain . our paper is concerned with sensitivity analysis of these markovian models of reaction networks .    generally , reaction networks involve many kinetic parameters that influence their dynamics . with sensitivity analysis one can measure the receptiveness of an outcome with respect to small changes in parameter values .",
    "these sensitivity values give insights about network design and its robustness properties @xcite .",
    "they also help in identifying the important reactions for a given output , estimating the model parameters and in fine - tuning the system s behaviour @xcite .",
    "the existing literature contains many methods for sensitivity analysis of stochastic reaction networks , but all these methods have certain drawbacks .",
    "they either introduce a _ bias _ in the sensitivity estimate @xcite or the associated estimator can have large variances @xcite or the method becomes impractical for large networks @xcite . due to these reasons , the search for better methods for sensitivity analysis of stochastic reaction networks is still an active research problem .",
    "we now formally describe a stochastic reaction network consisting of @xmath0 species @xmath1 . under the well - stirred assumption @xcite ,",
    "the state of the system at any time is given by a vector in @xmath2 , where @xmath3 is the set of non - negative integers .",
    "when the state is @xmath4 then the population size corresponding to species @xmath5 is @xmath6 .",
    "the state evolves as the species interact through @xmath7 reaction channels . if the state is @xmath8 , then the @xmath9-th reaction fires at rate @xmath10 and it displaces the state by the _ stoichiometric vector _ @xmath11 . here",
    "@xmath12 is called the _ propensity _",
    "function for the @xmath9-th reaction .",
    "we can represent the reaction dynamics by a continuous time markov chain and the distribution of this process evolves according to the chemical master equation @xcite which is quite difficult to solve except in some restrictive cases .",
    "fortunately , the sample paths of this process can be easily simulated using monte carlo procedures such as gilespie s _ stochastic simulation algorithm _ ( ssa ) and its variants @xcite .",
    "suppose that the propensity functions of the reaction network depend on a scalar parameter @xmath13 .",
    "hence when the state is @xmath8 , the @xmath9-th reaction fires at rate @xmath14 .",
    "let @xmath15 denote the markov process representing the reaction dynamics .",
    "for a function @xmath16 and an observation time @xmath17 , our output of interest is @xmath18 .",
    "we would like to determine how much the expected value of this output changes with infinitesimal changes in the parameter @xmath13 .",
    "in other words , our aim is to compute @xmath19 since the mapping @xmath20 is generally unknown , it is difficult to evaluate @xmath21 directly .",
    "therefore we need to estimate this quantity using simulations of the process @xmath22 . for this purpose , many methods use a finite - difference scheme such as @xmath23 for a small @xmath24 .",
    "these methods reduce the variance of the associated estimator by coupling the processes @xmath22 and @xmath25 in an intelligent way .",
    "three such couplings are : common reaction numbers ( crn ) ( see @xcite ) , common reaction paths ( crp ) ( see @xcite ) and coupled finite - differences ( cfd ) ( see @xcite ) .",
    "the two best performing finite - difference schemes are crp and cfd and we shall discuss them in greater detail in section [ subsec : fdschemes ] .",
    "it is immediate that a finite - difference scheme produces a _ biased _ estimate of the true sensitivity value @xmath26 .",
    "this problem of bias is compounded by the fact that in most cases , the size and even the sign of the bias is unknown , thereby casting a doubt over the estimated sensitivity values .",
    "the magnitude of the bias must be proportional to @xmath24 , but it can still be significant for a small @xmath24 ( see section [ sec : ex ] ) .",
    "one can reduce the bias by decreasing @xmath24 , but as @xmath24 gets smaller , the variance of the finite - difference estimator gets larger , making it necessary to generate an extremely large number of samples to obtain a statistically accurate estimate . in many cases , for small values of @xmath24 ,",
    "the computational cost of generating a large sample is so high that finite - difference schemes become inefficient in comparison to the unbiased methods .",
    "unfortunately there is no way to select an optimum value of @xmath24 , which is small enough to ensure that the bias is small and simultaneously large enough to ensure that the estimator variance is low .",
    "this is the main source of difficulty in using finite - difference schemes and we shall revisit this point in section [ sec : ex ] .    to obtain highly accurate estimates of @xmath27 we need unbiased methods for sensitivity analysis",
    "the first such method was given by plyasunov and arkin @xcite and it relies on the girsanov measure transformation .",
    "hence we refer to it as the _ girsanov method _ in this paper . in this method the sensitivity value @xmath27 is expressed as @xmath28 where @xmath29 is a random variable whose realizations can be easily obtained by simulating paths of the process @xmath22 in the time interval @xmath30 $ ] .",
    "the girsanov method is easy to implement , and its unbiasedness guarantees convergence to the right value @xmath26 as the sample size tends to infinity .",
    "however in many situations , the estimator associated with this method has a very high variance , making it extremely inefficient ( see the examples in @xcite ) .",
    "one such situation that commonly arises is when the sensitive parameter @xmath13 is a reaction rate constant with a small size ( see @xcite ) . to remedy this problem of high estimator variances",
    ", we proposed another unbiased scheme in @xcite , which is based on a different sensitivity formula of the form . in this formula , the expression for the random variable @xmath31 is derived using the random time - change representation of kurtz ( see chapter 7 in @xcite ) .",
    "unfortunately this expression is such that realizations of the random variable @xmath31 can not be easily computed from the paths of the process @xmath32 as the expression involves several expectations of functions of the underlying markov process at various times and various initial states .",
    "if all these expectations are estimated _ serially _ using independent paths then the problem becomes computationally intractable .",
    "hence we devised a scheme in @xcite , called the _",
    "auxiliary path algorithm_(apa ) , to estimate all these expectations in _ parallel _ using a fixed number of _ auxiliary _ paths .",
    "the implementation of apa is quite difficult and the memory requirements are very high because one needs to store the paths and dynamically maintain a large table to estimate the relevant quantities .",
    "these reasons make apa impractical for large networks and also for large observation times @xmath33 .",
    "however , in spite of these difficulties , we showed in @xcite that apa can be far more efficient than the girsanov method , in examples where sensitivity is computed with respect to a small reaction rate constant .",
    "the above discussion suggests that all the existing methods for sensitivity analysis , both biased and unbiased , have certain drawbacks .",
    "motivated by this issue , we develop a new method for sensitivity estimation in this paper which is unbiased , easy to implement , has low memory requirements and is more versatile than the existing unbiased schemes .",
    "we use the main result in @xcite , to construct another random variable @xmath34 such that holds .",
    "we then provide a simple scheme , called the _ poisson path algorithm_(ppa ) , to obtain realizations of @xmath34 and this gives us a method to obtain unbiased estimates of parameter sensitivities for stochastic reaction networks .",
    "similar to apa , ppa also requires estimation of certain quantities using auxiliary paths , but the number of these quantities can be controlled to be so low , that they can be estimated serially using independent paths .",
    "consequently ppa does not require any storage of paths or the dynamic maintenance of a large table as in apa .",
    "hence the memory requirements are low , the implementation is easier , and ppa works well for large networks and for large observation times @xmath33 . in section [ sec : ex ] we consider many examples to compare the performance of ppa with the girsanov method , cfd and crp .",
    "we find that ppa is usually far more efficient than the girsanov method . perhaps surprisingly",
    ", in many situations ppa can also outperform the finite - difference schemes ( crp and crn ) if we impose the restriction that the bias is small .",
    "this makes ppa an attractive method for sensitivity analysis because one can efficiently obtain sensitivity estimates and not have to worry about the ( unknown ) bias caused by finite - difference approximations .",
    "this paper is organized as follows . in section [ sec : pre ] we provide the relevant mathematical background and also discuss the existing schemes for sensitivity analysis in greater detail .",
    "we also present our main result which shows that for a suitably defined random variable @xmath34 , relation is satisfied . in section [ sec : ppa ]",
    "we describe ppa which is a simple method to obtain realizations of @xmath34 . in section [ sec : ex ] we consider many examples to illustrate the efficiency of ppa and compare its performance with other methods . finally in section [ sec : conc ] we conclude and discuss future research directions .",
    "recall the description of the reaction network from the previous section and suppose that the propensity functions depend on a real - valued parameter @xmath13 .",
    "we can model the reaction dynamics by a continuous time markov process whose generator is given by @xmath35 where @xmath36 is any bounded function and @xmath37 . under some mild conditions on the propensity functions",
    "( see condition 2.1 in @xcite ) , a markov process @xmath15 with generator @xmath38 and any initial state @xmath39 , exists uniquely .",
    "the random time - change representation of kurtz ( see chapter 7 in @xcite ) shows that this process can be expressed as @xmath40 where @xmath41 is a family of independent unit rate poisson processes .",
    "in this paper we are interested in computing the sensitivity value @xmath42 defined by .",
    "assume that we can express the sensitivity value as for some random variable @xmath31 .",
    "if we are able to generate @xmath43 independent samples @xmath44 from the distribution of @xmath45 , then @xmath42 can be estimated as @xmath46 which is a random variable with mean @xmath47 and variance @xmath48 given by @xmath49 due to the central limit theorem , for large values of @xmath43 , the distribution of @xmath50 is approximately normal with mean @xmath47 and variance @xmath48 .",
    "hence for any interval @xmath51 \\subset \\r$ ] the probability @xmath52)$ ] can be approximated as @xmath53 \\right ) \\approx \\phi\\left ( \\frac{b - \\mu_n } { \\sigma_n } \\right ) -\\phi\\left ( \\frac{a - \\mu_n } { \\sigma_n } \\right),\\end{aligned}\\ ] ] where @xmath54 is the cumulative distribution function for the standard normal distribution and @xmath55 is the standard deviation .",
    "generally @xmath47 and @xmath55 are unknown but they can be estimated from the sample as @xmath56    note that the mean @xmath47 is just the _ one - point estimate _ for the sensitivity value while the standard deviation @xmath55 measures the statistical spread around @xmath47 .",
    "the standard deviation @xmath55 can also be seen as the estimation error . from it",
    "is immediate that @xmath55 is directly proportional to @xmath57 but inversely proportional to @xmath43 .",
    "hence if @xmath57 is high then a larger number of samples is needed to achieve a certain statistical accuracy .    in finite - difference schemes , instead of @xmath42 one",
    "estimates a finite - difference @xmath58 of the form which can be written as @xmath59 for @xmath60 here the markov processes @xmath22 and @xmath25 are defined on the same probability space and they have generators @xmath61 and @xmath62 respectively . in this case",
    ", the estimator mean @xmath47 and variance @xmath48 are given by with @xmath63 replaced by @xmath64 .",
    "the main idea behind the various finite - difference schemes is that by coupling the processes @xmath22 and @xmath25 , one can increase the _ covariance _ between @xmath65 and @xmath66 , thereby reducing the variance of @xmath67 and making the estimation procedure more efficient . as mentioned in the introduction ,",
    "three such couplings suggested in the existing literature are : common reaction numbers ( crn ) ( see @xcite ) , common reaction paths ( crp ) ( see @xcite ) and coupled finite - differences ( cfd ) ( see @xcite ) . among these",
    "we only consider the two best performing schemes , crp and cfd , in this paper .    in crp",
    "the processes @xmath22 and @xmath25 are coupled by their random time - change representations according to @xmath68 where @xmath41 is a family of independent unit rate poisson processes .",
    "note that these poisson processes are same for both processes @xmath22 and @xmath25 , indicating that their reaction paths are the same .    in cfd the processes @xmath22 and @xmath25 are coupled by their random time - change representations according to @xmath69   ds\\right ) \\zeta_k \\\\ \\",
    "\\textnormal { and } \\ x_{\\theta + h } ( t ) & = x_0 + \\sum_{k = 1}^k y_k \\left ( \\int_{0}^t \\lambda_k ( x_\\theta(s ) , \\theta ) \\wedge \\lambda_k ( x_{\\theta + h}(s ) , \\theta + h )     ds\\right ) \\zeta_k \\\\ & + \\sum_{k = 1}^k y^{(2)}_k \\left (   \\int_{0}^t \\left [ \\lambda_k ( x_{\\theta+h}(s ) , \\theta+h ) - \\lambda_k ( x_\\theta(s ) , \\theta ) \\wedge \\lambda_k ( x_{\\theta + h}(s ) , \\theta + h )   \\right ]   ds\\right ) \\zeta_k , \\end{aligned}\\ ] ] where @xmath70 and @xmath71 is again a family of independent unit rate poisson processes . under this coupling ,",
    "the processes @xmath22 and @xmath25 have the same state until the first time the counting process corresponding to @xmath72 or @xmath73 fires for some @xmath9 .",
    "the probability that this _ separation _ time will come before @xmath33 is proportional to @xmath24 , which is usually a small number .",
    "hence for the majority of simulation runs , the processes @xmath22 and @xmath25 are together for the whole time interval @xmath30 $ ] , suggesting that they are _ strongly coupled_.    note that both crp and crn are estimating the same quantity @xmath58 and hence they both suffer from the same bias @xmath74 .",
    "the only difference between these two methods is in the coupling of the processes @xmath22 and @xmath25 , which alters the variance of @xmath64 . for a given example , the method with a lower variance will be more efficient as it will require lesser number of samples @xmath75 to achieve the desired statistical accuracy .    for any finite - difference scheme , one can show that the bias @xmath74 is proportional to @xmath24 while @xmath76 is proportional to @xmath77 .",
    "therefore by making @xmath24 smaller we may reduce the bias but we will have to pay a computational cost by generating a large number of samples for the required estimation . as mentioned in the introduction , this trade - off between bias and computational efficiency is the major drawback of finite - difference schemes as one generally does not know the _",
    "_ value of @xmath24 for which @xmath78 is _ close enough _ to @xmath79 , while at the same time the variance of @xmath76 is _ not too large_.      unbiased schemes are desirable for sensitivity estimation because one does not have to worry about the bias and the accuracy of the estimate can be improved by simply increasing the number of samples .",
    "the girsanov method is the first such unbiased scheme ( see @xcite and @xcite ) .",
    "recall the random time - change representation of the process @xmath80 . in the girsanov method",
    ", we express @xmath42 as where @xmath81 and @xmath82 is the counting process given by @xmath83 in , @xmath84 if and only if the @xmath9-th reaction fires at time @xmath85 . for any other @xmath85 , @xmath86 .",
    "hence if the @xmath9-th reaction fires at times @xmath87 in the time interval @xmath30 $ ] , then we can write @xmath88    the girsanov method is simple to implement because realizations of the random variable @xmath31 can be easily generated by simulating the paths of the process @xmath22 until time @xmath33 .",
    "however , as mentioned before , the variance of @xmath31 can be very high ( see @xcite ) , which is a serious problem as it implies that a large number of samples are needed to produce statistically accurate estimates . since simulations of the process @xmath22 can be quite cumbersome , generating a large sample",
    "can take an enormous amount of time .",
    "now consider the common situation where we have _ mass - action kinetics _ and @xmath13 is the rate constant of reaction @xmath89 . in this case , @xmath90 has the form @xmath91 and for every @xmath92 , @xmath93 does not depend on @xmath13 .",
    "therefore @xmath94 and hence the formula for @xmath95 simplifies to @xmath96 where @xmath97 is the number of times reaction @xmath89 fires in the time interval @xmath30 $ ] .",
    "this formula clearly shows that the girsanov method can not be used to estimate the sensitivity value at @xmath98 even though @xmath27 is well - defined .",
    "this is a big disadvantage since the sensitivity value at @xmath98 is useful for understanding network design as it informs us whether the presence or absence of reaction @xmath89 influences the output or not .",
    "unfortunately the problem with the girsanov method is not just limited to @xmath99 . even for @xmath13 close to @xmath100 , the variance of @xmath63 can be very high , rendering this method practically infeasible ( see @xcite ) .",
    "this is again a serious drawback as reaction rate constants with small values are frequently encountered in systems biology and other areas .    these issues with the girsanov method",
    "severely restrict its use and also highlight the need for new unbiased schemes for sensitivity analysis . in our recent paper @xcite , we provide a new unbiased scheme based on another sensitivity formula of the form . to motivate this formula",
    "we first discuss the problem of computing parameter sensitivity in the deterministic setting .",
    "consider the deterministic model corresponding to the parameter - dependent reaction network described in the introduction .",
    "if @xmath101 denotes the vector of species _ concentrations _ at time @xmath85 , then the process @xmath102 is the solution of the following system of ordinary differential equations @xmath103 with initial condition @xmath104 . here",
    "@xmath105 is the parameter dependent reaction flux ( see @xcite ) of the @xmath9-th reaction .",
    "we assume that each @xmath106 is a differentiable function of its arguments .",
    "pick an observation time @xmath107 and a differentiable output function @xmath108 , and suppose we are interested in computing the sensitivity value @xmath109 using we can write @xmath110 where @xmath111 denotes the gradient of the map @xmath112 and @xmath113 denotes the standard inner product on @xmath114 . differentiating the last equation with respect to @xmath13 gives us @xmath115 where @xmath116 .",
    "note that the values of @xmath117 can be easily computed by solving the ordinary differential equation obtained by differentiating with respect to @xmath13 .",
    "hence in the deterministic setting , computation of the parameter sensitivity is relatively straightforward .",
    "observe that relation helps us in viewing parameter sensitivity as the sum of two parts .",
    "the first part measures the contribution due to the sensitivity of the reaction fluxes ( @xmath106 ) while the second part measures the contribution due to the sensitivity of the states @xmath118 for @xmath119 .",
    "the main result in @xcite provides such a decomposition for parameter sensitivity in the stochastic setting .",
    "however , unlike the deterministic scenario , measuring the second contribution is computationally very challenging . in order to present this result",
    ", we need to define certain quantities .",
    "let @xmath15 be a markov process with generator @xmath38 .",
    "for any @xmath16 , @xmath119 and @xmath120 define @xmath121 and for any @xmath122 let @xmath123 define @xmath124 and let @xmath125 denote the successive jump times for convenience ] of the process @xmath22 .",
    "theorem 2.3 in @xcite shows that @xmath27 satisfies with @xmath126 where @xmath127    this result was proved by coupling the processes @xmath22 and @xmath25 as in cfd ( see section [ subsec : fdschemes ] ) and computing the limit of the finite - difference @xmath58 ( see ) as @xmath128 .",
    "similar to the deterministic case , this result shows that parameter sensitivity @xmath58 can be seen as the sum of two parts : the first part measures the contribution due to the sensitivity of the propensity functions ( @xmath12 ) while the second part measures the contribution due to the sensitivity of the states @xmath129 of the markov process at the jump times @xmath130 .",
    "computing the latter contribution is difficult because it involves the function @xmath131 which generally does not have an explicit formula . to overcome this problem , one needs to estimate all the quantities of the form @xmath132 that appear in .",
    "however the number of these quantities is proportional to the number of jumps of the process before time @xmath33 , which can be quite high even for small networks .",
    "if we estimate each of these quantities _ serially _ , as and when they appear , using a collection of independently generated paths of the process @xmath22 , then the problem becomes computationally intractable for most examples of interest .",
    "in @xcite we devised a scheme , called the _",
    "auxiliary path algorithm_(apa ) , to estimate all these quantities in _ parallel _ by generating a fixed number of _ auxiliary _ paths in addition to the main path .",
    "apa stores information about all the required quantities in a big _",
    "hashtable _ and tracks the states visited by the auxiliary paths to estimate those quantities . due to",
    "all the necessary book - keeping , apa is hard to implement and it also has high memory requirements .",
    "in fact the space and time complexity of apa scales linearly with the number of jumps in the time interval @xmath30 $ ] and this makes apa practically unusable for large networks or for large values of observation time @xmath33 .",
    "moreover apa only works well if the stochastic dynamics visits the same states again and again , which may not be generally true .",
    "the above discussion suggests that if we can modify the expression for @xmath31 in such a way that only a small fraction of unknown quantities ( of the form @xmath132 ) require estimation , then it can lead to an efficient method for sensitivity estimation .",
    "we exploit this idea in this paper . by adding _ extra randomness _ to the random variable @xmath31",
    ", we construct another random variable @xmath34 , which has the same expectation as @xmath31 @xmath133 even though its distribution may be different .",
    "we then show that realizations of the random variable @xmath34 can be easily obtained through a simple procedure .",
    "this gives us a new method for unbiased parameter sensitivity estimation for stochastic reaction networks .",
    "we now describe the construction of the random variable @xmath134 introduced in the previous section .",
    "let @xmath135 be the markov process with generator @xmath61 and initial state @xmath39 .",
    "let @xmath136 denote the successive jump times of this process .",
    "the total number of jumps until time @xmath33 is given by the random variable @xmath137 note that for any @xmath107 we have @xmath138 because @xmath139 .",
    "if the markov process @xmath22 reaches a state @xmath8 for which @xmath140 , then @xmath8 is an _ absorbing _ state and the process stays in @xmath8 forever . from , it is immediate that for any non - negative integer @xmath141 , @xmath142 can not be an absorbing state .",
    "let @xmath143 indicate if the final state @xmath144 is absorbing @xmath145 for each @xmath146 let @xmath147 be an independent exponentially distributed random variable with rate @xmath148 and define @xmath149    for each @xmath146 and @xmath122 let @xmath150 be given by @xmath151 fix a normalizing constant @xmath152 .",
    "the choice of @xmath153 and its role will be explained later in the section .",
    "if @xmath154 and @xmath155 , then let @xmath156 be an independent @xmath3-valued random variable whose distribution is poisson with parameter @xmath157 here the denominator @xmath158 is non - zero because for @xmath159 the state @xmath129 can not be absorbing . on the event",
    "@xmath160 we define another random variable as @xmath161 where @xmath162 and @xmath163 are two processes which are coupled by the following random time - change representations : @xmath164 where @xmath165 is an independent family of unit rate poisson processes .",
    "this coupling is similar to the coupling used by cfd ( see section [ subsec : fdschemes ] ) .",
    "note that @xmath162 and @xmath163 are markov processes with generator @xmath38 and initial states @xmath166 and @xmath129 respectively .",
    "therefore @xmath167 where @xmath168 is defined by . in other words , given @xmath169 and @xmath170 , the mean of the random variable @xmath171 is just @xmath172 . the above coupling between @xmath162 and @xmath163 makes them strongly correlated , thereby lowering the variance of the difference @xmath173 .",
    "this strong correlation is evident from the fact that if @xmath174 for some @xmath175 then @xmath176 for all @xmath177 . finally ,",
    "if the last state @xmath144 is absorbing ( that is , @xmath178 ) and @xmath179 is non - zero then we define another random variable @xmath180 as @xmath181 where @xmath182 is an independent markov process with generator @xmath38 and initial state @xmath183 .",
    "note that @xmath184    we are now ready to provide an expression for @xmath34 .",
    "let @xmath185 and define @xmath186 \\\\ & + \\alpha \\sum_{k=1}^k   \\left [    \\frac { \\partial \\lambda_k ( x_\\theta (   \\sigma_\\eta   ) , \\theta ) } { \\partial \\theta    } \\left ( \\hat{i } _ { k \\eta }    - ( t - \\sigma_\\eta ) f ( x_\\theta (   \\sigma_\\eta   ) ) \\right)\\right ]   .",
    "\\notag\\end{aligned}\\ ] ] by a simple conditioning argument we prove in section [ asec : proof ] of _ supplementary materials _ that relation holds .",
    "we mentioned before that it is difficult to obtain realizations of @xmath31 because one needs to estimate a quantity like @xmath187 at each jump time @xmath188 , that requires simulation of new paths of the process @xmath22 which is computationally expensive .",
    "similarly for @xmath34 we need to compute @xmath171 at each @xmath188 which also requires simulation of new paths of the process @xmath22 .",
    "however the main difference is that to compute @xmath34 , @xmath171 is only needed if the poisson random variable @xmath189 is strictly positive . if we can effectively control the number of positive @xmath189-s then we can efficiently generate realizations of @xmath34 .",
    "we later explain how this control can be achieved using the positive parameter @xmath153 introduced in the definition of the random variable @xmath189 ( see ) .",
    "the construction of @xmath34 outlined above , also provides a recipe for obtaining realizations of this random variable and hence gives us a method for estimating the parameter sensitivity @xmath27 .",
    "we call this method , the _ poisson path algorithm _ ( ppa ) because at each jump time @xmath130 , the crucial decision of whether new paths of the process @xmath22 are needed ( for @xmath171 ) is based on the value of a poisson random variable @xmath189 .",
    "we describe ppa in greater detail in the next section .",
    "we now discuss how the positive parameter @xmath153 can be selected .",
    "let @xmath190 denote the total number of positive @xmath189-s that appear in .",
    "this is the number of @xmath171-s that are required to obtain a realization of @xmath34 .",
    "it is immediate that @xmath190 is bounded above by @xmath191 , which is a poisson random variable with parameter @xmath192 , where @xmath193 by picking a small @xmath152 we can ensure that @xmath194 is small , which would also guarantee that @xmath195 and @xmath190 are small .",
    "specifically we choose a small positive integer @xmath196 ( like @xmath197 , for instance ) and set @xmath198 where @xmath199 is estimated using @xmath200 simulations of the process @xmath22 in the time interval @xmath30 $ ] .",
    "the choice of @xmath200 is not critical and typically a small value ( like @xmath201 , for example ) is sufficient to provide a decent estimate of @xmath199 .",
    "the role of parameter @xmath196 is also not very important in determining the efficiency of ppa .",
    "if @xmath196 increases then @xmath190 increases as well , and the computational cost of generating each realization of @xmath34 becomes higher .",
    "however as @xmath190 increases the variance of @xmath34 decreases and hence fewer realizations of @xmath34 are required to achieve the desired statistical accuracy .",
    "these two effects usually offset each other and the overall efficiency of ppa remains relatively unaffected .",
    "note that ppa provides unbiased estimates for the sensitivity values , regardless of the choice of @xmath200 and @xmath196 .",
    "we now describe ppa which is essentially a method for obtaining realizations of the random variable @xmath202 ( defined by ) for some observation time @xmath33 and some output function @xmath203 .",
    "since @xmath204 , we can estimate @xmath42 by generating @xmath43 realizations @xmath205 of the random variable @xmath34 and then computing their empirical mean .",
    "let @xmath39 denote the initial state of the process @xmath22 and assume that the function _ rand ( ) _ returns independent samples from the uniform distribution on @xmath206 $ ] .",
    "we simulate the paths of our markov process using gillespie s ssa @xcite .",
    "when the state of the process is @xmath8 , the next time increment ( @xmath207 ) and reaction index ( @xmath9 ) is given by the function @xmath208 ( see algorithm [ ssa ] in section [ app : methods ] of _ supplementary materials _ ) .",
    "before we can generate @xmath202 we need to fix a normalization parameter @xmath153 according to . for this",
    "we estimate @xmath199 using @xmath200 simulations of the markov process ( see algorithm [ estimatenormalization ] in section [ app : methods ] of _ supplementary materials _ ) .",
    "once @xmath153 is chosen , a single realization of the random variable @xmath34 ( given by ) can be computed using @xmath209 ( algorithm [ gensensvalue ] ) .",
    "this method simulates the process @xmath32 according to ssa and at each state @xmath8 and jump time @xmath85 , the following happens :    * if @xmath8 is an absorbing state ( @xmath210 ) then @xmath85 is the last jump time before @xmath33 ( @xmath211 ) . for each @xmath122",
    "such that @xmath212 , the quantity @xmath180 ( see ) is evaluated using @xmath213 ( see algorithm [ evaluateintegralindependent ] in section [ app : methods ] of _ supplementary materials _ ) and then used to update the sample value according to . *",
    "if @xmath8 is _ not _ an absorbing state , then @xmath214 for some jump time @xmath130 with @xmath215 .",
    "the exponential random variable @xmath216 ( where @xmath217 in ) is generated and for each @xmath122 such that @xmath212 , the poisson random variable @xmath218 ( where @xmath219 in ) is also generated . if @xmath220 and @xmath221 then the quantity @xmath171 ( see ) is evaluated using @xmath222 ( see algorithm [ gensensvalue ] in section [ app : methods ] of _ supplementary materials _ ) and then used to update the sample value according to . to generate a poisson random variable with parameter @xmath223 we use the function @xmath224 ( see algorithm [ genpoissrv ] in section [ app : methods ] of _ supplementary materials _ ) .",
    "set @xmath225 , @xmath226 and @xmath227 calculate @xmath228 ssa@xmath229 update @xmath230 set @xmath231 set @xmath232 and @xmath233 update @xmath234 set @xmath235 update @xmath236 update @xmath237 update @xmath238 update @xmath239 and @xmath240 @xmath241",
    "in this section , we present many examples to compare the performance of ppa with the other methods for sensitivity estimation . among these other methods , we consider the girsanov method which is unbiased , and the two best - performing finite - difference schemes , cfd and crp , which are of course biased .",
    "we can compare the performance of different methods by comparing the time they need to produce a _ statistically accurate estimate _ of the true sensitivity value @xmath42 given by .",
    "our first task is to find a way to judge if an estimate of @xmath27 is statistically accurate .",
    "suppose that a method estimates parameter sensitivity using @xmath43 samples whose mean is @xmath47 and standard deviation is @xmath55 ( see section [ sec : pre ] ) .",
    "assume that the true value of @xmath42 is @xmath242 .",
    "let @xmath243 and @xmath244 , where @xmath245 denotes the absolute value function .",
    "then @xmath51 $ ] is the @xmath246 interval around the true value @xmath242 . under the central limit approximation ,",
    "the estimator @xmath50 can be seen as a normally distributed random variable with mean @xmath47 and variance @xmath48 . hence using",
    "we can calculate the probability @xmath247 ) \\ ] ] that the estimator lies within the @xmath246 interval around @xmath242 .",
    "henceforth we refer to @xmath248 as the _ confidence level _ for the estimator .",
    "observe that the statistical accuracy of an estimate can be judged from the value of @xmath248 : the higher the value of @xmath248 , the more accurate is the estimate . by calculating the probability @xmath248 using the @xmath246 interval around @xmath242 ,",
    "we ensure that the statistical precision is high or low depending on whether the sensitivity value is small or large .",
    "we mentioned in section [ sec : pre ] that the standard deviation @xmath55 is inversely proportional to the number of samples @xmath43 , which shows that @xmath249 as @xmath250 . for an unbiased scheme ( girsanov and ppa ) ,",
    "@xmath251 as @xmath250 , due to the law of large numbers .",
    "therefore from it follows that for an unbiased scheme , the confidence level @xmath248 can be made as close to @xmath252 as desired by picking a large sample size @xmath43 .",
    "however the same is not true for finite - difference schemes such as cfd and crp . in such schemes @xmath253 as @xmath250 , where @xmath58 given by is generally different from the true value @xmath254 because of the bias",
    ". if the bias is large , then @xmath58 does not lie inside the @xmath246 interval @xmath51 $ ] around @xmath242 , and in this case the confidence level @xmath248 is close to @xmath100 for large values of @xmath43 .",
    "therefore if high confidence levels are desired with finite - difference schemes , then a small @xmath24 must be chosen to ensure that the bias @xmath255 is low .",
    "however the variance @xmath48 of the associated estimator scales like @xmath77 ( see section [ subsec : fdschemes ] ) , implying that if @xmath24 is _ too small _ then an extremely large sample size is required to make @xmath55 sufficiently small in order to achieve high confidence levels . generating a large sample may impose a heavy computational burden as simulations of the underlying markov process can be quite cumbersome .    in this paper , we compare the efficiency of different methods by measuring the cpu time that is needed to produce a sample with least possible size , such that the corresponding estimate has a certain threshold confidence level .",
    "this threshold confidence level is _ close _ to @xmath252 ( either @xmath256 or @xmath257 ) and hence the produced estimate is statistically quite accurate .",
    "the discussion in the preceding paragraph suggests that for finite - difference schemes , there is a trade - off between the statistical accuracy and the computational burden . this trade - off",
    "is illustrated in figure [ fig : tradeoff ] which depicts @xmath258 cases that correspond to @xmath258 values of @xmath24 arranged in decreasing order .",
    "hence @xmath24 in case 1 is the largest while the @xmath24 in case 4 is the smallest .",
    "we describe these cases below .    *",
    "since @xmath24 is large , the computational burden is low but due to the large bias , the estimator distribution ( normal curve in figure [ fig : tradeoff ] ) puts very little mass on the @xmath246 interval around the true sensitivity value .",
    "therefore the confidence level @xmath248 is low and the estimate is unacceptable . * in comparison to case 1 , @xmath24 is smaller , the computational burden is higher and the bias is lower .",
    "however the estimator distribution still does not put enough mass on the @xmath246 interval and so the value of @xmath248 is not high enough for the estimate to be acceptable . * in comparison to case 2 , @xmath24 is smaller and so the computational burden is higher , but fortunately the bias is small enough to ensure that the estimator distribution puts nearly all its mass on the @xmath246 interval .",
    "therefore the confidence level @xmath248 is close to @xmath252 and the estimate is acceptable . * here @xmath24 is smallest and so the computational burden is the highest among all the cases .",
    "however in comparison to case 3 , the additional computational burden in unnecessary because there is only a slight improvement in the bias and the confidence level .      in order to use a finite - difference scheme",
    "efficiently one needs to use the largest value of @xmath24 for which an estimate with a high confidence level can be produced ( case 3 in figure [ fig : tradeoff ] ) .",
    "however , in general , it is impossible to determine this value of @xmath24 .",
    "a blind choice of @xmath24 is likely to correspond to situations @xmath252 or @xmath258 . to make matters worse ,",
    "since the correct sensitivity value @xmath27 is usually unknown one can not measure the statistical accuracy of a finite - difference scheme , without re - estimating @xmath27 using an unbiased method .",
    "most practitioners who use finite - difference schemes , assume that the statistical accuracy is high if @xmath24 is small .",
    "however as our examples illustrate , an estimate can be inaccurate even for a @xmath24 that appears small .",
    "therefore if high accuracy is needed for an application , then an unbiased method should be preferred over finite - difference schemes .",
    "since it is difficult to determine the _ optimum _ value of @xmath24 , we adopt the following strategy to gauge the efficiency of a finite - difference scheme .",
    "we start with @xmath260 and check if the threshold confidence level @xmath248 can be achieved for some sample size @xmath43 .",
    "if it does , then we stop here , otherwise we decrease @xmath24 by a factor of @xmath197 ( to @xmath261 ) and again perform the check . continuing this way",
    ", we eventually arrive at a value of @xmath24 in the sequence @xmath262 , for which the threshold confidence level @xmath248 is achievable for a large enough sample size . for performance comparison ,",
    "we _ only _ use the last cpu time corresponding to the value of @xmath24 for which the check was successful . note that we do not account for the time wasted in all the unsuccessful attempts .",
    "therefore our strategy for performance comparison is more lenient towards the finite - difference schemes and more conservative towards the unbiased schemes .",
    "this leniency is intentionally adopted to compensate for the arbitrariness in choosing the  test \" values of @xmath24 .    to compute the confidence level @xmath248 we need to know the exact sensitivity value @xmath21 . in some of our examples ,",
    "@xmath42 can be analytically evaluated and hence the computation of @xmath263 is quite straightforward .",
    "in other examples , where @xmath42 can not be explicitly computed , we use ppa to produce an estimate with a sample whose size is large enough to have very low sample variance .",
    "the rationale behind this is that since ppa is unbiased , if the sample variance is low then the estimate would be close to the true sensitivity value @xmath27 .",
    "we now start discussing the examples . in all the examples ,",
    "the propensity functions @xmath12 are in the form of mass - action kinetics unless stated otherwise .",
    "recall that our method ppa depends on two parameters @xmath200 and @xmath196 whose choice is not very important ( see section [ sec : constrcstheta ] ) . in this paper",
    "we always use ppa with @xmath264 and @xmath265 . in all the examples we provide a bar chart with the cpu time required by each method to produce an estimate with the desired confidence level @xmath266 or @xmath267 .",
    "this facilitates an easy performance comparison between various methods .",
    "the exact values of the cpu times , sample size @xmath43 , estimator mean @xmath47 , estimator standard derivation @xmath55 and @xmath24 ( for finite - difference schemes ) are provided in section [ sec : data ] of _ supplementary materials_.      our first example is a simple birth - death model in which a single species @xmath268 is created and destroyed according to the following two reactions : @xmath269 let @xmath270 and assume that the sensitive parameter is @xmath271 . let @xmath272 be the markov process representing the reaction dynamics . hence the population of @xmath268 at time @xmath85 is given by @xmath273 .",
    "assume that @xmath274 .",
    "for @xmath275 we wish to estimate @xmath276 for @xmath277 and @xmath201 .",
    "since the propensity functions of this network are affine , we can compute @xmath27 exactly ( see section [ sec : data ] of _ supplementary materials _ ) .",
    "these exact values help in computing the confidence level of an estimate .",
    "we estimate the sensitivity values with all the methods with two threshold confidence levels ( @xmath266 and @xmath267 ) , and plot the corresponding cpu times in figure [ fig : bd ] . for finite - difference schemes ( cfd and crp ) , the value of @xmath24 for which the desired confidence level was achieved is also shown .      from figure",
    "[ fig : bd ] , it is immediate that for this example ppa generally performs better than the other methods . to measure this gain in performance",
    "more quantitatively , we compute the _ average speed - up factor _ of ppa with respect to another method . for each threshold confidence level ,",
    "this factor is calculated by simply taking the ratio of the aggregate cpu times required by a certain method and ppa , to perform all the sensitivity estimations . in this example",
    ", this aggregate involves the cpu times for @xmath277 and @xmath278 .",
    "these average speed - up factors are presented in table [ tb : speedup ] .",
    "note that in this example , ppa is significantly faster than the finite - difference schemes ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating parameter sensitivity for markovian models of reaction networks . sensitivity values measure the responsiveness of an output with respect to the model parameters . </S>",
    "<S> they help in analyzing the network , understanding its robustness properties and identifying the important reactions for a specific output . </S>",
    "<S> sensitivity values are commonly estimated using methods that perform finite - difference computations along with monte carlo simulations of the reaction dynamics . </S>",
    "<S> these methods are computationally efficient and easy to implement , but they produce a _ biased _ estimate which can be unreliable for certain applications . moreover the size of the bias is generally unknown and hence the accuracy of these methods can not be easily determined . </S>",
    "<S> there also exist unbiased schemes for sensitivity estimation but these schemes can be computationally infeasible , even for very simple networks . </S>",
    "<S> our goal in this paper is to present a new method for sensitivity estimation , which combines the computational efficiency of finite - difference methods with the accuracy of unbiased schemes . </S>",
    "<S> our method is easy to implement and it relies on an exact representation of parameter sensitivity that we recently proved in an earlier paper . through examples we demonstrate that the proposed method can outperform the existing methods , both biased and unbiased , in many situations .    </S>",
    "<S> [ section ] [ theorem]lemma [ theorem]condition [ theorem]proposition [ theorem]remark [ theorem]definition [ theorem]hypothesis [ theorem]corollary [ theorem]example [ theorem]description [ theorem]assumption        </S>",
    "<S> |    keywords : parameter sensitivity ; reaction networks ; markov process ; finite - difference schemes ; random time - change representation ; common reaction path ( crp ) ; coupled finite - difference ( cfd ) ; girsanov method . + mathematical subject classification ( 2010 ) : 60j22 ; 60j27 ; 60h35 ; 65c05 . </S>"
  ]
}