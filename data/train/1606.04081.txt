{
  "article_text": [
    "exploring a document in order to find specific content can be time consuming  @xcite . for example , skimming a video to determine where a particular topic is addressed is cumbersome since the video might have a considerable length .",
    "if only the raw video is provided , the only option is to randomly skim it , which is inconvenient and might lead to important parts being skipped .",
    "this problem is further aggravated when multiple documents from different sources and different media need to be considered .",
    "identifying cross - document relationships addresses this problem .",
    "the relationships allow a more efficient browsing of the documents since as soon as we are able to find relevant information on one document , pointers to related content in other documents can be provided . in this context",
    ", we focus on equivalence relationships between documents .",
    "the nature of the relationships is defined from a user perspective rather than a strict semantic equivalence .",
    "this means that when users find some specific information in a document they might request `` more like this '' , which does not mean that semantically equivalent content needs to be provided .",
    "an example of an application scenario for this situation is when students browse different learning materials looking for a specific part of a lecture they did not understand . since different students might learn more efficiently with different learning materials , it is useful to have quick access to equivalent topic segments .",
    "the access to information in different documents can be done efficiently if a document s content structure can be perceived  @xcite . in the literature",
    "we find distinct ways of highlighting the structure of documents based on establishing relationships between them .",
    "the main difference in the approaches is the level of granularity of the relationships .",
    "a schematic representation of the types of relationships found in the literature is in figure  [ fig : relstax ] , where we depict the different constituents of documents : words , sentences , topic segments , and the document itself .",
    "depending on the constituents , different relationships can be established , namely : , , , , and relationships .",
    "the level of granularity of the relationships determines the types of support that are provided .",
    "for instance , when a word in a document is linked to another document ( ) , the user expects to find information that can help him better understand the concepts regarding that word , whereas when two documents are linked ( ) , users expect that these share a common general topic .",
    "this is helpful when in - depth knowledge about a topic is necessary .",
    "despite the usefulness of these relationships , they can not help when users want to access information regarding specific topics discussed in the documents . to deal with this ,",
    "cross - document relationships need to be established at the topic segment level  ( relationships ) .",
    "other relationships will require users to manually skim the documents . by using relationships it is possible to have immediate access to content on the same topic in different documents .",
    "current research work has given proof that structuring documents based on relationships brings both browsing efficiency and understanding of the topics in the documents  @xcite .",
    "therefore , we believe that similar advantages should also be brought to document topic segments . to the best of our knowledge , the problem of automatically finding cross - document relationships in topic segments is novel , as previous research work as focused on other levels of granularity .    to motivate the use of relationships for browsing documents , figure  [ fig : cd - rels ] provides an example that compares them to relationships .",
    "the example is in an educational domain on trees , a subject taught in computer science curricula .",
    "the example corresponds to manual transcripts from video lectures . in figure",
    "[ fig : d2w ] , the target word is `` bst '' ( binary search tree ) . from this example it is possible to understand why relationships are not helpful when specific information in a document needs to be found . in @xmath0",
    "there are multiple occurrences of the word `` bst '' , indicating that it is used in different topics of the lecture .",
    "the problem is that from the word we can only reach a document , which gives no indication of the context where it was used . even if the word is highlighted , such as in the example ,",
    "we still would not know which of the occurrences is the appropriate one .",
    "this contrasts with the relationship in figure  [ fig : ts2ts ] , where it is possible to easily find in both documents where the topic of binary search trees is covered .",
    "since the relationship is established at the topic segment level on both ends , users can immediately analyze the corresponding text , since it is topically self - contained .",
    "+    the research area in which this work is framed is graph - community detection  @xcite .",
    "we propose the use of these methods to explore a co - occurrence graph structure , which models how words relate in different documents .",
    "the goal is to use the discovered word communities to perform cross - document relationship identification .",
    "we hypothesize that approaches based on text similarity , such as clustering , can not cope with this task , since a substantial part of the vocabulary is shared in related documents . in this context ,",
    "one of the challenges consists of studying ways to use the discovered words communities to identify cross - document topic segment equivalence relationships .",
    "our main contributions are :    * create an annotated corpus for studying topic segment relationships . *",
    "formalize a graph - community detection technique to automatically discover topic segment relationships . *",
    "evaluate the task of topic segment relationship identification with techniques from different research areas .",
    "different approaches to document relationship identification can be found in the literature . as previously mentioned , what characterizes them is the level of granularity at the which the relationships are established ( words , sentences , topic segments , or documents ) .",
    "based on these levels of granularity , this section summarizes the research done in this area .",
    "the _ wikify ! _ system  @xcite identifies relationships between keywords in a document and wikipedia articles ( relationships ) .",
    "the difficulty in this task is the ambiguity of the extracted keyword .",
    "for example , if the keyword `` bar '' is extracted from a document about music , then it is necessary to relate it to an article with the same context . to address this problem",
    "a classification approach was used .",
    "the identification of relationships has been performed in different scenarios .",
    "for example , the work described in  @xcite finds relationships with the goal of structuring lectures from into a graph .",
    "the motivation is that the way of searching in platforms is to enter a query and examine the retrieved list of courses .",
    "the problem is that a single course might not have all the information . to address this issue , prerequisite and",
    "equivalence relationships are established between lectures .",
    "another example where relationships are established is described in  @xcite . in this work the problem of structuring a set of related news articles is addressed . the goal is to highlight how different documents contribute to the main overall topic .",
    "the structuring of the documents is based on the metro map metaphor .",
    "a metro map of documents consists of a set of lines , which can intersect each other .",
    "each individual line follows a coherent narrative thread , covering a specific aspect of the main topic being analyzed . in turn , each metro line is composed of a set of metro stops .",
    "these metro stops correspond to clusters of words .",
    "associated to each metro stop are the corresponding documents that originated them .",
    "the line intersections of their lines show that different aspects of a topic have some sort of relation .",
    "the generation of metro maps is based on the bigclam graph - community detection algorithm  @xcite , which analyses a word co - occurrence graph in order to identify word communities .",
    "the use of relationships is studied in @xcite .",
    "this work uses a linguistic taxonomy for describing the different types of sentence relationships .",
    "the taxonomy is defined within the context of the  @xcite , whose goal is to understand how different parts of different documents related to one another . the describes a total of 24 domain - independent semantic relations , such as `` elaboration '' , `` contradiction '' , or `` equivalence '' .",
    "this contrasts with previous work where either no specific relationship was attributed or a more addhoc approach was used .",
    "the approach to automatically identify relationships used a classification approach .    in what respects relationships ,",
    "the work described in @xcite uses them to link sections of a textbook and video lectures . to discover the relationships ,",
    "the first step consists of constructing a set of queries based on keywords extracted from the textbook .",
    "the queries are then submitted to a video search engine .",
    "the task of identifying relationships is viewed as an optimization problem where the minimal set of textbook sections to cover a video needs to be found .",
    "the task in the mediaeval campaign  @xcite is the most closely related to ours , since it targets relationships .",
    "the task is defined as receiving an audio segment of interest and returning an ordered list of jump - in points for regions similar to it . by doing so ,",
    "whenever a relevant piece of information is found , other related information can also be accessed efficiently . for this purpose ,",
    "a corpus of dialogs among university students was created .",
    "some examples of the topics discussed in the dialogs are : classes , new technologies , or movies .",
    "an example of a work that addresses this task is described in  @xcite .",
    "this study assesses the influence of passage retrieval when approaching the task from an perspective .",
    "this is done by creating supervised segmentation models .",
    "the main difference between our work and the task is that of social speech context .",
    "dialogs in this context do not follow an overarching topic .",
    "this means that they contain a variety of topics which are not related to each other . in this work",
    "we propose the study of topic segment relationships given a set of related documents .",
    "therefore , we expect the topic segments to be much more similar , due to the interweaving of their related topics .",
    "in this section , we describe our graph - community detection - based approach to the relationship identification task . given that we want to group similar document topic segments",
    ", we can formalize this problem in a clustering setting :    input : : :    a set of items @xmath1 .",
    "each item corresponds to    the textual content of a topic segment .",
    "output : : :    a mapping from each item to a particular cluster    @xmath2 .    to perform the clustering task",
    "we propose the use of graph - community detection techniques . the idea is to analyze a weighted co - occurrence graph representation of document segments and find word communities that are representative of the topics in the documents .",
    "this is similar to the metro maps approach in @xcite .",
    "we hypothesize that co - occurrence graphs can better model how words relate to one another in different documents .",
    "this contrasts with cluster approaches where features relate to individual words and the presence or absence of the features is based on whether the corresponding word occurred in the document or not .",
    "the graph - community detection problem can be formalized as follows :    input : : :    a weighted co - occurrence graph @xmath3 , where    @xmath4 is the set of nodes and @xmath5 the set of    edges .",
    "@xmath4 corresponds to the set of words from a given    set @xmath6 of document segments .",
    "an edge    @xmath7 exists if words @xmath8 and    @xmath9 occur in some segment @xmath10 .",
    "output : : :    a mapping from each word @xmath11 to a particular    community @xmath12 .    appropriately setting the weights @xmath13 of the edge is a topic of research in this work .",
    "depending on how these weights are set , different word communities can be obtained .",
    "therefore , it is necessary to develop an appropriate weighting scheme to the cross - document topic relationship identification task . in a related problem described in @xcite , a weighting scheme that counts the number of co - occurrences between words",
    "is used to discover a metro map structure of news articles .",
    "we hypothesize that having all word co - occurrence contribute in the same way to a weight score might not work in the document segment relationship task .",
    "therefore , we designed the following _ tf - idf_-based weighting schemes :    * * count * : the number of times the words co - occurred in different segments ( equivalent to @xcite ) . * * best _ tf - idf _ * : the sum of the highest _ tf - idf _ values of the words . * * count + best _ tf - idf _ * : the sum of the previous weights . * * count + avg _ tf - idf _ * : the sum of the _ count _ weight and the sum of the average _ tf - idf _ values of the words .    after obtaining the word - communities , it is necessary to perform a mapping between them and the topics segments .",
    "this task needs to be performed since the previous step only provides groups of words .",
    "therefore , it is necessary to determine how to group the topic segments .",
    "doing this using word communities is , to the best of our knowledge , novel .",
    "it should be noted that although the work in @xcite used graph - community detection , the problem being solved is different .",
    "the relationships between the documents were given _ a priori _ by grouping documents if they occurred in the same time span .",
    "the graph - community detection task was then carried out in those groups of documents to determine the metro stops .",
    "our work is different since we can not rely on this criterion to discover the document relationships . in this work",
    "we propose the use of a function to assign scores between a document segment and word communities .",
    "segments are assigned to the highest scoring community .",
    "if two document segments are assigned to the same community they are considered as equivalent . the mathematical formula that expresses this idea",
    "is defined as follows : @xmath14 where @xmath15 is the set of communities discovered in @xmath16 , and @xmath17 is the set of words in a segment . it should be noted that different formulations of the @xmath18 function can be designed . in the experiments to identify similar document segments we considered the following scoring functions : @xmath19    the first two functions count the common words between the segment and the community . the score is then normalized either by the total number of words in @xmath20 or @xmath17 , respectively .",
    "the previous functions treat all words in the same way .",
    "therefore , we also defined a function that makes words contribute according to their relevance , @xmath21 .",
    "the difference is that common words have a score corresponding to their normalized _ tf - idf _ value .      in this section ,",
    "a description of the evaluation procedures for the cross - document relationship identification task is provided . for the evaluation a comparison between techniques from different research areas",
    "was carried out .",
    "the following research areas were surveyed : clustering , topic models , and graph - community detection .",
    "the evaluation was carried out in two different domains : a corpus with trees learning materials , and the task  @xcite .      since the cross - document relationship identification task is viewed as a clustering problem , standard evaluation metrics of this area were used .",
    "one of these metrics is the adjusted rand index ( ari )  @xcite , which computes the similarity between clusterings by analyzing all pairwise combinations of data points in a predicted clustering and a ground truth clustering .",
    "another standard cluster quality measure is the @xmath22 score . in clustering precision and recall",
    "are computed over pairs of items .",
    "for example , a true positive means that both the hypothesized clustering and the true clustering assigned some pair of items to the same cluster .    the last evaluation metric we consider is @xmath23 , which measures the percentage of items that are assigned to their correct clusters .",
    "this requires a mapping between the hypothesized clusters and the true clusters .",
    "this mapping is found using the kuhn - munkres algorithm  @xcite .",
    "the evaluation of the cross - document relationship identification task presents the problem of not having available datasets with topic segment annotations .",
    "the only exception is the corpus  @xcite .",
    "the problem with this corpus is that it contains social spontaneous speech .",
    "therefore , there is not an overall topic , since the dialogs have unrelated topics .",
    "another characteristic of the corpus is its user - centered approach to the annotation .",
    "this means that the use case for the annotation is a scenario where users are browsing a set of related documents and when relevant information is found they request `` more like this '' .",
    "this means that users needs have priority over annotating semantic equivalence relationships in a strict way . despite not being ideal ,",
    "we still consider it in our evaluation .",
    "the dataset contains a total of 25 dialogs , with a total duration of approximately 4 hours , 26 different topics , and a total of 185 document segments .    in order to use the corpus it was first necessary to align the transcripts with the segment annotations",
    "a perfect alignment could not be obtained because the time stamps in the annotations and in the transcripts did not have an exact match .",
    "given these circumstances , we opted to consider as part of a segment any portion of the transcripts that overlapped with the segment annotations . in the evaluation",
    "only the manual transcripts were used , since the ones provided by an automatic speech recognizer had a high error rate .",
    "another problem with this corpus is that some of the topics allow a broad scope of segments to be grouped .",
    "for instance , the label `` computer science topics '' contains segments discussing classes students are taking , assignments , career paths , _",
    "etc_. given this situation , we opted to choose a subset of the corpus containing more coherent topics . in this context , we used 8 topics with a total of 106 segments .",
    "a description of this subset is provided in table  [ tab : corpus_ssss_ri ] .",
    "c1.7c1.5c1.5c1.3 * topic * & * segments * & * # words * & * # vocab * + * courses * & 37 & 889 & 341 + * math * & 7 & 209 & 110 + * internships * & 6 & 69 & 46 + * research * & 8 & 656 & 266 + * family * & 7 & 208 & 130 + * games * & 12 & 1182 & 458 + * movies * & 17 & 410 & 192 + * tv shows * & 12 & 646 & 270 +    facing the problem of not having available corpora for the evaluation , we annotated a corpus of tree - related learning materials .",
    "the learning materials come from different media , namely : slides , video lectures , and wikipedia articles .",
    "a summary of the characteristics of this corpus is given in table  [ tab : corpus ] .",
    "all of the documents refer to the topic of trees except for one wikipedia article which specifically addresses tree rotations ( an essential operation in trees ) .",
    "the corpus was segmented into topically cohesive segments by the first author , who has taught avl trees in an algorithms and data structures course . for the segmentation of video lectures , manual audio transcripts",
    "were available .",
    "the main guideline for segment annotation was to identify places where a topic shift occurred such that disregarding this topic would make it harder to follow the higher - level structure of the document , as suggested in  @xcite .",
    "this emphasizes that documents should be segmented individually , instead of , for instance , trying to fit parts of documents in a predefined list of subtopics . in total 86 segmentation boundaries",
    "were annotated , from a corpus containing 3181 sentences .",
    "r1.5c.7c1.3c1.9c1.2 & * docs * & * segs ( @xmath24 ) * & * words ( @xmath24 ) * & * # vocab * + * slides * & 5 & 7 @xmath25 1.2 & 1402 @xmath25 683.5 & 776 + * video * & 3 & 11 @xmath25 5.6 & 6396 @xmath25 307.2 & 1265 + * wikipedia * & 2 & 7 @xmath25 1.4 & 1195 @xmath25 221.3 & 536 +    the segmented corpus was then annotated with equivalence relationships . this annotation was done in the same spirit as the corpus , using a user - centered approach .",
    "the difficulty in this task is in the presence of multiple topics in a single segment .",
    "the annotation process consisted of going through each document individually . in the case",
    "of the first document , the segments were tagged with the topic they discussed . on subsequent documents ,",
    "it was assessed if the segments should be assigned with an existed tag or if a new tag should be created . after the annotation process",
    ", a total of 15 different relationships were found in the set of 86 segments .",
    "the experiments only used the topics that appeared in the majority of the documents ( 49 segments ) .",
    "a description of this subset of segments is provided in table  [ tab : corpus_ri ] .",
    "c2.6c1.5c1.3c1.3 * topic * & * segments * & * # words * & * # vocab * + * bst * & 7 & 1822 & 284 + * tree height * & 5 & 1800 & 338 + * tree rotation * & 13 & 3762 & 538 + * tree balance * & 13 & 3670 & 483 + * avl rebalance * & 11 & 8142 & 700 +      in this section , we describe the algorithms tested in the cross - document relationship identification task .",
    "since we frame the task in a clustering setting , we survey algorithms designed specifically for this task , namely :    * k - means  @xcite : * given a @xmath26 value that specifies the number of clusters to be obtained , the process is based on minimizing a criterion function measures the distance of data points and cluster centroids .    *",
    "agglomerative clustering  @xcite : * a hierarchical clustering that uses a bottom - up approach , considering in the beginning all points as individual clusters .",
    "the procedure consists of a series of iterations , and , in each of them , two clusters are merged .",
    "therefore , in each step it is necessary to decide which clusters to merge , which is done by using a similarity measure and criterion function .",
    "* dbscan  @xcite : * based on 2 values , a radius ( @xmath27 ) and minimum number of neighbors ( @xmath28 ) .",
    "when a point in a contains at least @xmath28 it is defined as a _ core _ point .",
    "when a core point is found , a cluster is formed with all its neighbors .",
    "then , an expansion process takes place by checking if the neighbors are also core points . in the positive case ,",
    "these are added to the cluster .",
    "* mean shift  @xcite : * the points are regarded as a sample of a distribution . by applying a kernel to each point a probability surface",
    "is generated .",
    "depending on the kernel bandwidth , the resulting probability surface will vary .",
    "this surface is characterized by its peaks , which corresponds to clusters .",
    "the clustering procedure is based on iteratively shifting each point uphill , until the nearest peak is reached .",
    "* spectral clustering  @xcite : * uses an adjacency matrix of a similarity graph between the points to cluster . then , the first @xmath26 eigenvectors of the laplacian matrix are calculated .",
    "the final step consists of using the k - means algorithm to obtain a clustering from the eigenvectors .",
    "* clustering  @xcite : * based on the matrix factorization technique that factorizes a non - negative matrix @xmath29 into two other non - negative matrices . the goal of the factorization is to obtain @xmath30 .",
    "the factorization is found by using an optimization approach .",
    "the non - negative property of makes the reduced space easy to interpret .",
    "each element @xmath31 indicates the degree of association of point @xmath32 with cluster @xmath26 .",
    "therefore , it suffices to take the highest value of @xmath33 to find the corresponding cluster .    in section  [ sec : rel_id ] we proposed an approach to document relationship identification based on word communities .",
    "this can also be viewed as a topic modeling task  @xcite , since its output are groups of words that relate to a topic .",
    "therefore , we experiment with the use of topic models . in topic models documents",
    "are assumed to be observed from a generative process that contains hidden variables , which represent the topic structure of the document .",
    "each topic is a distribution over words and documents are mixtures of those topics .",
    "finally , words are viewed as draws from one of the topics . using the previous concept ,",
    "a probabilistic graphical model estimates the distribution of the hidden variables .",
    "taking into account that we propose the use of the graph - community detection approach , we survey different techniques that can perform this task , namely :    * label propagation ( lp ) *  @xcite : the algorithm starts with each node assigned to a different community .",
    "the nodes are then visited and assigned with the label which is more frequent among its neighbors .",
    "the algorithm terminates when there are no changes in the labels .    * *  @xcite : based on the _ modularity _ criterion .",
    "high modularity means that dense connections are observed in nodes from the same community and sparse connections are observed between nodes from different communities .",
    "the algorithm evaluates for each node the modularity gain from removing the node from its community and placing it in its neighbors .",
    "if the modularity increases , the node is reassigned .",
    "the process is repeated until no changes in the graph are observed .",
    "* louvain *  @xcite : similar to the previous algorithm , with the difference that in each iteration nodes in the same community are merged into a single node and the corresponding edge weights added .",
    "* walktraps *  @xcite : the algorithm is based on",
    "_ random walks_. a random walk means that we start at a node , we pick a neighbor at random and move to it , then repeat the procedure . by repeating this procedure",
    "it is possible to compute statistics about the visited nodes .",
    "the statistics are summarized in a transition matrix , which expresses the probability of going from one node to another through a random walk of length @xmath34 . using the distance metric , the problem of graph - community detection",
    "is then approached as a clustering task .    * leading eigenvector *  @xcite : the algorithm starts by having all nodes belonging to the same community . in each iteration ,",
    "the graph is split into two communities in a way that a significant increase in modularity is obtained .",
    "the split is determined by evaluating the leading eigenvector of the modularity matrix .    * bigclam *  @xcite : based on the optimization of a likelihood community membership .      in this section ,",
    "we report the document relationship identification experiments done with the corpus .",
    "each of the clustering algorithms described in section  [ sec : ri_algs ] has a particular set of parameters that can be tested . in this study",
    "we set those parameters in the following way :    * k - means : the number @xmath26 of clusters was set to 5 , the number of subtopics in the corpus .",
    "* agglomerative clustering : the following similarity metrics where assessed : cosine , euclidean , and gaussian .",
    "for the gaussian metric , variances ( @xmath35 ) within the range from 1 to 500 ( step size 1 ) were tested .",
    "different merge functions were assessed , namely : ward , complete , and average . *",
    "dbscan : all combinations of @xmath27 , within the range of 0.1 and 0.9 ( step size 0.1 ) , and @xmath28 , within the range of 1 and 14 ( step size 1 ) , were tested .",
    "all the previously mentioned similarity metrics were tested .",
    "* mean shift : the kernel was used and tested with bandwidth values from 1 to 1000 ( step size 1 ) .",
    "* spectral clustering : the number of desired clusters was set to 5 .",
    "the same similarity metrics referred previously were used to test different similarity graphs .",
    "* clustering : the number of desired clusters was set to 5 .    the implementation used for all clustering algorithms , except , was the one provided by  @xcite .",
    "the -based clustering implementation was the one in  @xcite .    using the previous experimental setup ,",
    "the best results obtained are described in table  [ tab : clus_bl ] .",
    "results show that most of the techniques obtained low scores in all evaluation metrics .",
    "the only exceptions were the agglomerative and spectral clustering .",
    "it should be noted that not all evaluation metrics agree as to the best technique , since spectral clustering had higher scores in @xmath36 and @xmath22 , whereas agglomerative clustering had the best @xmath37 score . also , there is not a strict correlation between the evaluation metrics .",
    "for example , mean shift obtained the lowest @xmath36 , but would be considered as one of the best performing techniques in @xmath22 . in this context , we set as a baseline the spectral clustering technique , since it was the best in two of evaluation metrics and still performed well on the third one .",
    "cc1.2c1.2c0.9 * clustering algorithm * & @xmath38 & @xmath39 & @xmath40 + k - means & 0.011 & 0.34 & 0.31 + agglomerative & 0.11 & 0.32 & * 0.52 * + dbscan & 0.03 & 0.26 & 0.27 + mean shift & 0.009 & 0.35 & 0.29 + spectral & * 0.15 * & * 0.36 * & 0.41 + & 0.046 & 0.22 & 0.45 +    in order to understand the space where the clustering task is performed , figure  [ fig : hms ] depicts the heat maps of the segment similarity matrix using the assessed similarity metrics . in the heat maps ,",
    "darker tones indicate more similar segments . in all cases",
    ", one can observe that it is impossible to distinguish the true clusters .",
    "this means that no contrast can be found between segments that should belong to the same cluster and the ones that should not .",
    "the cosine similarity case ( figure  [ fig : hm_cos ] ) is the best example of this situation , since the map prominently has light colored cells .",
    "this means that all segments were considered not similar .",
    "the euclidean and gaussian cases ( figure  [ fig : hm_eu ] and [ fig : hm_gaus ] ) present a better contrast . despite this , it is still far from obvious what the clusters should be . in practice ,",
    "most of the clustering techniques had a tendency to concentrate the majority of the segments in a single cluster .",
    "this constitutes a limitation that the similarity space imposes over the clustering techniques .",
    "this means that if the similarity space does not properly reflect how the segments should be grouped , then there is not much chance for the clustering techniques to perform well .",
    "this is a particularity of our task , since the segments are related to a common overarching topic and come from different media , which causes them to be wrongly perceived by a similarity metric .",
    "the experiments in the context of topic modeling were made using software made available by @xcite .",
    "all the proposed scoring functions ( equations  [ eq : sf1 ] , [ eq : sf2 ] , and [ eq : sf3 ] ) were tested .",
    "it was also necessary to choose the top-@xmath41 most probable words from each topic word distribution .",
    "for this purpose a range between the top-@xmath42 and top-@xmath43 words was assessed .",
    "the probabilistic nature of topic modeling makes it non - deterministic .",
    "this means that different runs will yield different topic distributions .",
    "therefore , each of the topic models were run 10 times and the corresponding results averaged .",
    "the number of topics was set to 5 , since it is the number of topics in the corpus .",
    "the results obtained were close between the tested scoring functions .",
    "nevertheless , using @xmath44 with the top-@xmath45 words from each topic distribution obtains the best results : @xmath46 , @xmath47 , @xmath48 , for @xmath49 .",
    "therefore , this parameterization is our topic modeling baseline in the corpus .",
    "the experiments carried out in the corpus were performed by trying the graph - community detection techniques , described in section  [ sec : ri_algs ] , using all the proposed weighting schemes and scoring functions . it should be noted that some of the techniques are not sensitive to edge weight .",
    "the implementation of the techniques we use was the one provided by  @xcite .",
    "another aspect to consider is that there are words in the documents that should not be taken into account , since they are either too common or too rare to be representative of a subtopic in a document .",
    "@xcite addressed this problem using a _ tf - idf _ to filter the vocabulary . in this experiment",
    "we also adopted this approach by setting for each segment a cutoff at the top-@xmath50 words with the highest _ tf - idf _ score .",
    "a summary of the results is provided in table  [ tab : gcd_bl ] .",
    "it is possible to observe that the walktraps algorithm performed better in all evaluation metrics .",
    "c1.5c1.4c1.5c0.6c0.6c0.6 * algorithm * & * weighting * & * scoring function * & @xmath38 & @xmath39 & @xmath40 + lp & - & @xmath44 & 0.0 & 0.35 & 0.27 + & - & @xmath51 & 0.05 & 0.31 & 0.42 + louvain & best _ tf - idf _ &",
    "@xmath44 & 0.12 & 0.31 & 0.42 + walktraps &    .best results obtained with graph - community detection algorithms .",
    "in bold are the best results obtained . [ cols=\"^ \" , ]     & @xmath52 & 0.15 & 0.30 & 0.40 + bigclam & - & @xmath52 & 0.05 & 0.09 & 0.19 +    given the previous results , we performed the experiments varying the top-@xmath41 words with the louvain technique and the count weighting scheme .",
    "the results obtained are in figure  [ fig : louvain_ssss_topn ] . as with other similar experiments , oscillations in the results",
    "were observed .",
    "the best results were obtained with the top-@xmath53 words with 0.26 , 0.40 , and 0.48 scores in @xmath36 , @xmath22 , and @xmath37 , respectively .",
    "words in the louvain algorithm . ]",
    "after having carried out all experiments with the corpus , this section provides a discussion of the obtained results . in table",
    "[ tab : res_all_4s ] , a summary of all results is provided .",
    "contrary to the corpus , results were close between two techniques : spectral clustering and louvain .",
    "the former obtained a better overall performance , since it had the highest scores in two of the metrics .",
    "cc1.5c1c1 & @xmath38 & @xmath39 & @xmath40 + spectral clustering & * 0.27 * & * 0.41 * & 0.47 + topic models & 0.17 & 0.30 & 0.46 + louvain & 0.26 & 0.40 & * 0.48 * +    in order to have a visual perception of the types of errors the surveyed techniques are making , figure  [ fig : ssss_clust ] shows the corresponding obtained clusterings . from the figure it is possible to conclude that spectral clustering ( figure  [ fig : ssss_spectral ] ) is the most accurate one",
    ". one of the problems is again the merging of topics .",
    "spectral clustering merged the topic of `` courses '' with `` math '' ( red cluster ) .",
    "this makes sense , since math is a type of course and , thus , the same type of discussions might arise when talking about this topic .",
    "the general problem is that the remaining clusters are scattered across all topics .",
    "as for as the remaining clusters , they scattered the topics much more .",
    "for example , the blue cluster seems to group segments from the `` movies '' topic , but also includes segments from all other topics .",
    "this kind of issue generally occurs in the clusterings obtained with all other techniques .",
    "+    the advantage of the louvain clusterings ( figure  [ fig : ssss_louvain ] over the topic models one ( figure  [ fig : ssss_lda ] ) is that the `` games '' topic was better captured .",
    "the problem is that these approaches discover a number of topics higher than the actual value .",
    "this reflects how different the corpus is from the corpus .",
    "the corpus contains more heterogeneous segments ( even when considering a common topic ) , whereas the segments have an overarching topic .",
    "in this paper we proposed a methodology for discovering cross - document topic segment relationships . given that this task is done in a collection of related documents , we expected some document segments to be similar and not equivalent , since they have a common overarching topic , but also dissimilar and equivalent ( they can come from different media ) .",
    "this dichotomy in syntactic similarity motived us to develop a graph - community detection approach instead of a direct clustering approach .",
    "this allows the exploration of a word co - occurrence graph that better models interactions between words in different documents .",
    "the discovery of topic segment relationships is not a straightforward process as the output of a graph - community detection task is clusters of words .",
    "therefore , we developed an approach that assigns topic segments to word communities based on a scoring function .",
    "topic segments are considered as equivalent if they were assigned to the same word community .",
    "an evaluation of the proposed approach was made using subsets of a corpus with learning materials in the tree domain using the corpus .",
    "the results were not unanimous . in the corpus a walktraps graph - community approach performed better , corroborating our hypothesis that clustering approaches are not suitable for this task . in the corpus ,",
    "the spectral clustering performed better , indicating that segments that more loosely relate with one another at the subtopic level and that do not have a common overarching topic should be modeled differently .",
    "it should be noted though that the louvain technique obtained close results .",
    "therefore , we do not discard the possibility that a graph - community detection approach can not perform well in this domain . given these results ,",
    "a conclusion to draw is that the way documents relate to one another ( if they have an overarching topic or not ) plays an important role in how their relationships should be discovered .",
    "regarding future work , one of the main questions that the results leave open is whether the _ tf - idf _ is a good measure of importance of the words .",
    "the argument is that if it was indeed , then straighter correlation in the number of words used and the evaluation metrics should be observed , rather than the constant oscillations .",
    "therefore , we will consider the investigation of other criteria to perform the initial word filtering .",
    "another possibility that can lead to the improvement of the results is the use of edge pruning .",
    "co - occurrence graphs are characterized by high indegree nodes .",
    "therefore , if we prune the edges with less weight , it is possible that more representative word communities are found .",
    "the way edges are weighted is another research direction to follow . in this work",
    "we weigh all the edges in the graph in the same way .",
    "this does not take into account indicators such as the relative position of the words .",
    "this can be relevant since topic segments can have a considerable length and , thus , it might be important to consider closer words as having a stronger relation with each other .",
    "finally , we also want to improve the scoring function in order to take into account possible patterns in topic sequences .",
    "this means we expect some topics to appear frequently in documents in the same order .",
    "this is especially relevant in documents that correspond to learning materials , since many concepts have both a logical and hierarchical structure .",
    "d.  k. sil , s.  h. sengamedu , and c.  bhattacharyya , `` supervised matching of comments with news article segments , '' in _ proc . of the intern .",
    "conf . on information and knowledge management _ , pp .  21252128 , 2011 .",
    "j.  allan , j.  carbonell , g.  doddington , j.  yamron , y.  yang , j.  a. umass . topic detection and tracking pilot study final report . in _ proc . of the darpa broadcast news transcription and understanding workshop _ ,",
    "pages 194218 , 1998 .",
    "s.  shen , h.  lee , s.  li , v.  zue , and l.  lee . structuring lectures in massive open online courses ( moocs ) for efficient learning by linking similar sections and predicting prerequisites . in _ proc . of the intern .",
    "speech communication association _ ,",
    "pages 13631367 , 2015 .",
    "n.  x. vinh , j.  epps , and j.  bailey .",
    "information theoretic measures for clusterings comparison : is a correction for chance necessary ? in _ proc . of the intern .",
    "conf . on machine learning _ ,",
    "pages 10731080 , 2009 .",
    "j.  sander , m.  ester , h .-",
    "kriegel , and x.  xu . density - based clustering in spatial databases : the algorithm dbscan and its applications .",
    "_ data mining and knowledge discovery _ , 20 ( 2):0 169194 , jun ."
  ],
  "abstract_text": [
    "<S> in this paper we propose a graph - community detection approach to identify cross - document relationships at the topic segment level . </S>",
    "<S> given a set of related documents , we automatically find these relationships by clustering segments with similar content ( topics ) . in this context , we study how different weighting mechanisms influence the discovery of word communities that relate to the different topics found in the documents . finally , we test different mapping functions to assign topic segments to word communities , determining which topic segments are considered equivalent .    by performing this task </S>",
    "<S> it is possible to enable efficient multi - document browsing , since when a user finds relevant content in one document we can provide access to similar topics in other documents . </S>",
    "<S> we deploy our approach in two different scenarios . </S>",
    "<S> one is an educational scenario where equivalence relationships between learning materials need to be found . </S>",
    "<S> the other consists of a series of dialogs in a social context where students discuss commonplace topics . </S>",
    "<S> results show that our proposed approach better discovered equivalence relationships in learning material documents and obtained close results in the social speech domain , where the best performing approach was a clustering technique .    </S>",
    "<S> < ccs2012 > < concept > < concept_id>10002951.10003227.10003351.10003444</concept_id > < concept_desc > information systems  clustering</concept_desc > < concept_significance>500</concept_significance > < </S>",
    "<S> /concept > < concept > </S>",
    "<S> < concept_id>10002951.10003317.10003318</concept_id > < concept_desc > information systems  document representation</concept_desc > < concept_significance>500</concept_significance > < /concept > < </S>",
    "<S> concept > < concept_id>10002951.10003227.10003251.10003256</concept_id > < concept_desc > information systems  multimedia content creation</concept_desc > < concept_significance>300</concept_significance > < /concept > </S>",
    "<S> < /ccs2012 > </S>"
  ]
}