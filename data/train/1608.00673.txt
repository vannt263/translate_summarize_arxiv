{
  "article_text": [
    "consider the problem of maximizing a submodular function @xmath0 over a set of elements , subject to some given constraints .",
    "this has been a very useful abstraction for many problems , both theoretical ( e.g. , the classical @xmath7-coverage problem  @xcite ) , or practical ( e.g. , the influence maximization problem  @xcite , or many problems in machine learning  @xcite ) .",
    "we now know how to perform constrained submodular maximization , both when the function is monotone  @xcite , and also when the function may be non - monotone but non - negative  @xcite . in this paper",
    "we investigate how well we can solve this problem if the instance is not deterministically known up - front , but there is uncertainty in the input .",
    "consider the following setting .",
    "we have a submodular function over a ground set of elements @xmath8 .",
    "but the elements are not all active , and we can get value only for active elements .",
    "the bad news is that _ a priori _ we do nt know the elements status  whether it is active or not .",
    "the good news is that each element @xmath1 is active independently with some known probability @xmath2 .",
    "we find out an element @xmath1 s status only by _ probing _ it .",
    "once we know its status , we can use this information to decide which other elements to probe next , and in what order .",
    "( i.e. , be _",
    "adaptive_. ) we have some constraints on which subsets we are allowed to probe .",
    "eventually , we stop with some probed set @xmath3 and a known subset @xmath4 of the active elements in @xmath3 . at that time we can pick any @xmath5 and get value @xmath6",
    ".. ] what is a good strategy to probe elements to maximize expected value ?",
    "since this sounds quite abstract , here is an example . in the setting of influence",
    "maximization , the ground set is a set of email addresses ( or facebook accounts ) , and for a set @xmath3 of email addresses @xmath9 is the fraction of the network that can be influenced by seeding the set @xmath3 .",
    "but not all email addresses are still active . for each email address @xmath1 , we know the probability @xmath2 that it is active .",
    "( based , e.g. , on when the last time we know it was used , or some other machine learning technique . ) now due to time constraints , or our anti - spam policies , or the fact that we are risk - averse and do not want to make introductory offers to too many people  we can only probe some @xmath10 of these addresses , and make offers to the active ones in these @xmath10 , to maximize our expected influence . observe that it makes sense to be adaptive ",
    "if ` t.theorist@cs.cmu.edu ` happens to be active we may not want to probe ` t.theorist@cmu.edu ` , since we may believe they are the same person .",
    "a different example is robot path - planning .",
    "we have a robot that can travel at most distance @xmath11 each day and is trying the maximize value by picking items .",
    "the elements are locations , and an element is active if the location has an item to be picked up .",
    "( location @xmath1 has an item with probability @xmath2 independent of all others . )",
    "having probed @xmath3 , if @xmath12 is the subset of active locations , the value @xmath6 is some submodular function of the set of elements  e.g . ,",
    "the number of distinct items .",
    "there are other examples : e.g. , bayesian mechanism design problems ( see  @xcite for details ) and stochastic set cover problems that arise in database applications  @xcite .",
    "the question that is of primary interest to us is the following : _ even though our model allows for adaptive queries , what is the benefit of this adaptivity ?",
    "_ note that there is price to adaptivity : the optimal adaptive strategy may be an exponentially - large decision tree that is difficult to store , and also may be computationally intractable to find .",
    "moreover , in some cases the adaptive strategy would require us to be sequential ( probe one email address , then probe the next , and so on ) , whereas a non - adaptive strategy may be just a set of @xmath10 addresses that we can probe in parallel .",
    "so we want to bound the _ adaptivity gap _ : the ratio between the expected value of the best adaptive strategy and that of the best non - adaptive strategy .",
    "secondly , if this adaptivity gap is small , we would like to find the best non - adaptive strategy efficiently ( in polynomial time ) .",
    "this would give us our ideal result : a non - adaptive strategy that is within a small factor of the best adaptive strategy .",
    "the goal of this work is to get such results for as broad a class of functions , and as broad a class of probing constraints as possible .",
    "recall that we were not allowed to probe all the elements , but only those which satisfied some problem - specific constraints ( e.g. , probe at most @xmath10 email addresses , or probe a set of locations that can be reached using a path of length at most @xmath11 . )      in this paper , we allow very general probing constraints : the sequence of elements we probe must satisfy a given _ prefix - closed constraint_e.g .",
    ", these may be given by a matroid , or an orienteering constraint , or deadline , or precedence constraint , or an arbitrary downward - closed constraint ",
    "if we can probe some sequence of elements we can probe any prefix of it .",
    "we can not hope to get small adaptivity gaps for arbitrary functions ( see  [ sec : adapgapbad ] for a monotone @xmath13 function where the gap is exponential in @xmath14 even for cardinality constraint ) , and hence we have to look at interesting sub - classes of functions .",
    "[ [ submodular - functions . ] ] submodular functions .",
    "+ + + + + + + + + + + + + + + + + + + + +    our first set of results are for the case where the function @xmath0 is a non - negative submodular function .",
    "the first result is for monotone functions .",
    "[ thm : monotsubmod ] for any monotone non - negative submodular function @xmath0 and any prefix - closed probing constraints , the stochastic probing problem has adaptivity gap at most @xmath15 .    the previous results in this vein either severely restricted the function type ( e.g. , we knew a logarithmic gap for matroid rank functions  @xcite ) or the probing constraints ( e.g. , asadpour et al .  @xcite give a gap of @xmath16 for matroid probing constraints ) .",
    "we discuss these and other prior works in ",
    "[ sec : related ] .",
    "there is a lower bound of @xmath16 on the adaptivity gap for monotone submodular functions with prefix - closed probing constraints ( in fact for the rank function of a partition matroid , with the constraint being a simple cardinality constraint ) .",
    "it remains an interesting open problem to close this gap .",
    "we then turn to non - monotone submodular functions , and again give a constant adaptivity gap .",
    "while the constant can be improved slightly , we have not tried to optimize it , focusing instead on clarity of exposition .",
    "[ thm : nmonotsubmod ] for any non - negative submodular function @xmath0 and any prefix - closed probing constraints , the stochastic probing problem has adaptivity gap at most @xmath17 .",
    "both theorems  [ thm : monotsubmod ] and  [ thm : nmonotsubmod ] just consider the adaptivity gap .",
    "what about the computational question of finding the best non - adaptive strategy ?",
    "this is where the complexity of the prefix - closed constraints come in .",
    "the problem of finding the best non - adaptive strategy with respect to some prefix - closed probing constraint can be reduced to the problem of maximizing a submodular function with respect to the same constraints .    [",
    "[ xos - functions . ] ] xos functions .",
    "+ + + + + + + + + + + + + +    we next consider more general classes of functions .",
    "we conjecture that the adaptivity gap for all subadditive functions is poly - logarithmic in the size of the ground set . since we know that any subadditive function can be approximated to within a logarithmic factor by an xos ( a.k.a .",
    "_ max - of - sums _ , or _ fractionally subadditive _ ) function  @xcite , and every xos function is subadditive , it suffices to focus on xos functions . as a step towards our conjecture ,",
    "we show a nearly - tight logarithmic adaptivity gap for monotone xos functions of small `` _ _ width _ _ '' , which we explain below .",
    "a monotone xos function @xmath18 } \\to { { \\ensuremath{\\mathbb{r}}}}_{\\geq 0}$ ] is one that can be written as the maximum of linear functions : i.e. , there are vectors @xmath19 for some @xmath20 such that @xmath21 we define the _ width _ of ( the representation of ) an xos function as @xmath22 , the number of linear functions in this representation .",
    "e.g. , a width-@xmath23 xos function is just a linear function .",
    "in general , even representing submodular functions in this xos form requires an exponential width  @xcite .    for any monotone xos function @xmath0 of width @xmath22 , and any prefix - closed probing constraints",
    ", the adaptivity gap is @xmath24 .",
    "moreover , there are instances with @xmath25 where the adaptivity gap is @xmath26 .    in this case",
    ", we can also reduce the computation problem to _ linear _ maximization over the constraints .",
    "suppose we are given a width-@xmath22 monotone xos function @xmath0 explicitly in the max - of - sums representation , and an oracle to maximize positive linear functions over some prefix - closed constraint .",
    "then there exists an algorithm that runs in time @xmath27 and outputs a non - adaptive strategy that has expected value at least an @xmath28-fraction of the optimal adaptive strategy .      before talking about our techniques , a word about previous approaches to bounding the adaptivity gap .",
    "several works , starting with the work of dean et al .",
    "@xcite have used geometric `` relaxations '' ( e.g. , a linear program for linear functions  @xcite , or the multilinear extension for submodular settings",
    "@xcite ) to get an estimate of the value achieved by the optimal adaptive strategy . then one tries to find a non - adaptive strategy whose expected value is not much less than this relaxation .",
    "this is particularly successful when the probing constraints are amenable to being captured by linear programs ",
    ", matroid or knapsack constraints . dealing with general constraints ( which include orienteering constraints , where no good linear relaxations are known )",
    "means we can not use this approach .",
    "the other approach is to argue about the optimal decision - tree directly .",
    "an induction on the tree was used , e.g. , by chen et al .",
    "@xcite and adamczyk  @xcite to study stochastic matchings .",
    "a different approach is to use concentration bounds like freedman s inequality to show that for most paths down the tree , the function on the path behaves like the path - mean  this was useful in @xcite , and also in our previous work on adaptivity gaps of matroid rank functions  @xcite .",
    "however , this approach seems best suited to linear functions , and loses logarithmic factors due to the need for union bounds .",
    "given that we prove a general result for any submodular function , how do we show a good non - adaptive strategy ? our approach is to take a random path down the tree ( the randomness coming from the element activation probabilities ) and to show the expected value of this path , when viewed as a non - adaptive strategy , to be good . to prove this , surprisingly , we use induction .",
    "it is surprising because the natural induction down the tree does not seem to work .",
    "so we perform a non - standard inductive argument , where we consider the all-`no`path ( which we call the _ stem _ ) , show that a non - adaptive strategy would get value comparable to the decision tree on the stem , and then induct on the subtrees hanging off this stem .",
    "the proof for monotone functions , though basic , is subtle  requiring us to change representations and view things `` right '' .",
    "this appears in  [ sec : monotone ] .    for non - monotone submodular functions",
    ", the matter is complicated by the fact that we can not pick elements in an `` online '' fashion when going down the tree  greedy - like strategies are bad for non - monotone functions .",
    "hence we pick elements only with some probability , and show this gives us a near - optimal solution .",
    "the argument is complicated by the fact that having picked some elements @xmath29 , the marginal - value function @xmath30 may no longer be non - negative .",
    "finally , for monotone xos functions of small width , we use the approach based on freedman s concentration inequality to show that a simple algorithm that either picks the set optimizing one of the linear functions , or a single element , is within an @xmath24 factor of the optimum .",
    "we then give a lower bound example showing an ( almost-)logarithmic factor is necessary , at least for @xmath31 .",
    "the adaptivity gap of stochastic packing problems has seen much interest : e.g. , for knapsack  @xcite , packing integer programs  @xcite , budgeted multi - armed bandits  @xcite and orienteering  @xcite .",
    "all except the orienteering results rely on having relaxations that capture the constraints of the problem via linear constraints .    for stochastic monotone submodular functions where the probing constraints are given by matroids , asadpour et al .",
    "@xcite bounded the adaptivity gap by @xmath16 ; hellerstein et al .",
    "@xcite bound it by @xmath32 , where @xmath33 is the smallest probability of some set being materialized .",
    "( see also  @xcite . )",
    "the work of chen et al .",
    "@xcite ( see also  @xcite ) sought to maximize the size of a matching subject to @xmath34-matching constraints ; this was motivated by applications to online dating and kidney exchange .",
    "more generally , see , e.g.  @xcite , for pointers to other work on kidney exchange problems .",
    "the work of  @xcite abstracted out the general problem of maximizing a function ( in their case , the rank function of the intersection of matroids or knapsacks ) subject to probing constraints ( again , intersection of matroids and knapsacks ) .",
    "this was improved and generalized by adamczyk , et al .",
    "@xcite to submodular objectives .",
    "all these results use lp relaxations , or non - linear geometric relaxations for the submodular settings .    the previous work of the authors  @xcite gave results for the case where @xmath0 was the rank function of matroids ( or their intersections ) . that work bounded the adaptivity gap by logarithmic factors , and gave better results for special cases like uniform and partition matroids .",
    "this work both improves the quantitative bounds ( down to small constants ) , generalizes it to all submodular functions with the hope of getting to all subadditive functions , and arguably also makes the proof simpler .",
    "we denote the ground set by @xmath29 , with @xmath35 . each element @xmath36 has an associated probability @xmath2 . given a subset @xmath37 and vector @xmath38 ,",
    "let @xmath39 denote the distribution over subsets of @xmath3 obtained by picking each element @xmath40 independently with probability @xmath2 .",
    "( specifying a single number @xmath41 $ ] in @xmath42 indicates each element is chosen with probability @xmath43 . )",
    "a function @xmath44 is    * _ monotone _ if @xmath45 for all @xmath46 . *",
    "_ linear _ if there exist @xmath47 for each @xmath48 such that @xmath49 . * _ submodular _ if @xmath50 for all @xmath51 .",
    "we will normally assume that @xmath0 is non - negative and @xmath52 .",
    "* _ subadditive _ if @xmath53 . a non - negative submodular function is clearly subadditive . *",
    "_ fractionally subadditive _ ( or _ xos _ ) if @xmath54 for all @xmath55 and @xmath56 . for a discussion . ] + an alternate characterization : a function is xos if there exist linear functions @xmath57 such that @xmath58 .",
    "the _ width _ of an xos function is the smallest number @xmath22 such that @xmath0 can be written as the maximum over @xmath22 linear functions .",
    "all objective functions @xmath0 that we deal with are non - negative with @xmath59 .    given any function @xmath44 ,",
    "define @xmath60 to be the maximum value subset contained within @xmath3 .",
    "the function @xmath0 is monotone if and only if @xmath61 .",
    "in general , @xmath62 may be difficult to compute given access to @xmath0 . however , feige et al .",
    "@xcite show that for submodular functions @xmath63 \\leq { f^{\\max}}(s ) .",
    "\\label{eq : fmv}\\end{gathered}\\ ] ]    also , for a subset @xmath3 , define the `` contracted '' function @xmath64 . note that if @xmath0 is non - monotone , then @xmath65 may be negative - valued even if @xmath0 is not .",
    "[ [ adaptive - strategies ] ] adaptive strategies + + + + + + + + + + + + + + + + + + +    an adaptive strategy tree @xmath66 is a a binary rooted tree where every internal node @xmath67 represents some element @xmath36 ( denoted by @xmath68 ) , and has two outgoing arcs  the ` yes`arc indicating the node to go to if the element @xmath69 is active ( which happens with probability @xmath2 ) when probed , and the ` no`arc indicating the node to go to if @xmath1 is not active ( which happens with the remaining probability @xmath70 ) .",
    "no element can be represented by two different nodes on any root - leaf path .",
    "moreover , any root - leaf path in @xmath12 should be feasible according to the constraints .",
    "hence , each leaf @xmath71 in the tree @xmath66 is associated with the root - path @xmath72 : the elements probed on this path are denoted by @xmath73 .",
    "let @xmath74 denote the active elements on this path @xmath72i.e .",
    ", the elements represented by the nodes on @xmath72 for which we took the ` yes`arc .",
    "the tree @xmath66 naturally gives us a probability distribution @xmath75 over its leaves : start at the root , at each node @xmath67 , follow the ` yes`branch with probability @xmath76 and the `` no '' branch otherwise , to end at a leaf .",
    "given a submodular function @xmath0 and a tree @xmath66 , the associated adaptive strategy is to probe elements until we reach a leaf @xmath71 , and then to pick the max - value subset of the active elements on this path @xmath72 .",
    "let @xmath77 denote the expected value obtained this way ; it can be written compactly as @xmath78 .",
    "\\label{eq : adap}\\end{gathered}\\ ] ]    [ defn : stem ] for any adaptive strategy tree @xmath66 the _ stem _ represents the all-`no`path in @xmath66 starting at the root , i.e. , when all the probed elements turn out inactive .",
    "[ defn : depth ] the _ deepness _ of a strategy tree @xmath66 is the maximum number of _ active _ nodes that sees along a root - leaf path of @xmath66 .",
    "note that this notion of deepnessis not the same as that of depth used for trees : it measures the number of ` yes`-arcs on the path from the root to the leaf , rather than just the number of arcs seen on the path .",
    "this definition is inspired by the induction we will do in the submodular sections .",
    "we can also define the _ natural non - adaptive _ algorithm given the tree @xmath66 : just pick a leaf @xmath79 from the distribution given by @xmath66 , probe all elements on that path , and choose the max - value subset of the active elements .",
    "we denote the expected value by @xmath80 : @xmath81 ] .",
    "\\label{eq : alg}\\end{gathered}\\ ] ]",
    "we now prove theorem  [ thm : monotsubmod ] , and bound the adaptivity gap for _ monotone _ submodular functions @xmath0 over any prefix - closed set of constraints .",
    "the idea is a natural one in retrospect : we take an adaptive tree @xmath66 , and show that the natural non - adaptive strategy ( given by choosing a random root - leaf path down the tree , and probing the elements on that path ) is within a factor of @xmath15 of the adaptive tree .",
    "the proof is non - trivial , though .",
    "one strategy is to induct on the two children of the root ( which , say , probes element @xmath1 ) , but note that the adaptive and non - adaptive algorithms recurse having seen different sets of active elements . as active when it takes the ` yes`branch ( with probability @xmath2 ) , and nothing as active when taking the ` no`branch .",
    "non - adaptive recurses on the ` yes`branch with the same probability @xmath2 because it picks a random path down the tree  but it then also probes @xmath1 .",
    "so when it takes the ` yes`branch , it has either seen @xmath1 as active ( with probability @xmath2 ) or not ( with probability @xmath82 ) .",
    "hence the set of active elements on both sides are quite different .",
    "] this forced previous results to proceed along different lines , using massive union bounds over the paths in the decision tree , and hence losing logarithmic factors .",
    "they were also restricted to matroid rank functions , instead of all submodular functions .",
    "a crucial insight in our proof is to focus on the _ stem _ of the tree ( the all-`no`path off the root , see definition  [ defn : stem ] ) , and induct on the subtrees hanging off this stem .",
    "again we have issues of adaptive and non - adaptive recursing with different active elements , but we control this by giving the adaptive strategy some elements for free , and contracting some elements in the non - adaptive strategy without collecting value for them .",
    "the proof for non - monotone functions in  [ sec : non - mono ] will be even more tricky , and will build on ideas from this monotone case .",
    "formally , the main technical result is the following :    [ thm : monadapvsalg ] for any adaptive strategy tree @xmath66 , and any monotone non - negative submodular function @xmath83 with @xmath52 , @xmath84    theorem  [ thm : monotsubmod ] follows by the observation that each root - leaf path in @xmath66 satisfies the prefix - closed constraints , which gives us a feasible non - adaptive strategy .",
    "some comments on the proof : because the function @xmath0 is monotone , @xmath61 . plugging this into  ( [ eq : adap ] ) and  ( [ eq : alg ] ) , we want to show that @xmath85 ] ~~\\geq~~ { \\frac}13\\ ,    \\e_{\\ell \\gets \\pi_{\\ensuremath{\\mathcal{t}}\\xspace } } [ f(a_\\ell ) ] .",
    "\\label{eq : simpler - mono}\\end{gathered}\\ ] ] since both expressions take expectations over the random path , the proof proceeds by induction on the deepnessof the tree .",
    "( recall the definition of deepnessin definition  [ defn : depth ] . )",
    "we argue that for the stem starting at the root , gets a value close to in expectation ( lemma  [ lemma : stemmass ] ) .",
    "however , to induct on the subtree that the algorithms leave the stem on , the problem is that the two algorithms may have picked up different active elements on the stem , and hence the `` contracted '' functions may look very different . the idea now is to give the elements picked by for `` free '' and disallow ( just for the analysis ) to pick elements picked by after exiting the stem .",
    "now both the algorithms work after contracting the same set of elements in @xmath0 , and we are able to proceed with the induction .",
    "we prove by induction on the deepnessof the adaptive strategy tree @xmath66 . for the base case of deepness  @xmath86",
    ", @xmath66 contains exactly one node , and there are no internal nodes representing element .",
    "hence both @xmath87 and @xmath88 get zero value , so the theorem is vacuously true .    to prove the induction step , recall that the stem is the path in @xmath66 obtained by starting at the root node and following the ` no ` arcs until we reach a leaf .",
    "( see figure  [ fig : stem ] . )",
    "let @xmath89 denote the nodes along the stem of @xmath66 with @xmath90 being the root and @xmath91 being a leaf ; let @xmath92 .",
    "for @xmath93 , let @xmath94 denote the subtree hanging off the ` yes ` arc leaving @xmath95 .",
    "the probability that a path following the probability distribution @xmath75 enters @xmath94 is @xmath96 , where @xmath97 denotes the probability that the @xmath98 element is active .",
    "= [ circle , draw , fill = black!30 , inner sep=0pt , minimum width=2pt ]    = [ draw opacity=0.7,line width=1.4 cm ] = [ circle , draw , fill = black!15 , inner sep=0pt , minimum width=12pt ]    = [ rectangle , draw , fill = black!75,inner sep=3pt , inner ysep=3pt , minimum width=4pt ] = [ graphnode , fill = black!0 ] = [ graphnode , fill = black!20 ] = [ graphnode , fill = black!100 ] = [ ultra thick ]    in 0,1,2,3 ( -,- )  ( --1,--1 ) ; ( -,- )  ( -+1,--1 ) ;    ( 0,0 ) to [ out=200,in=90 ] ( -1,-1 ) ; ( -1,-1 ) to [ out=290,in=30 ] ( -2,-2 ) ; ( -2,-2 ) to [ out=200,in=90 ] ( -3,-3 ) ; ( -3,-3 ) to [ out=240,in=90 ] ( -2,-4 ) ;    in 0,1,2,3 at ( -,- ) [ graphnode ] ;    in 1,2,3,4 ( 2-,- ) ",
    "( 2 - -0.5 , --1 ) ",
    "( 2-+0.5 , --1) ( 2-,- ) ; at ( 2-,--1)[label = above:@xmath99 ;    at ( 2.5,-1.5)[label = above:`yes ` ] ; at ( -3,-1.5)[label = above:`no ` ] ;    let @xmath100 be the first @xmath101 elements probed on the stem , and @xmath102 be a random subset of @xmath103 that contains each element @xmath1 of @xmath103 independently w.p .  @xmath2 .",
    "we can now rewrite and in a form more convenient for induction . here",
    "we recall the definition of a marginal with respect to subset @xmath104 : @xmath105 .",
    "note that the leaf @xmath91 has no associated element ; to avoid special cases we define a dummy element @xmath106 with @xmath107 and @xmath108 .",
    "[ claim : montboundalgs ] let @xmath109 be the r.v . denoting the index of the node",
    "at which a random walk according to @xmath75 leaves the stem .",
    "( if @xmath110 then the walk does not leave the stem , and @xmath111 is a deepness - zero tree . )",
    "then , @xmath112 \\label{eq : monsub1 } \\\\",
    "& \\leq   \\e_{i , r \\sim s_i(\\p ) } { \\big } [ f(e_i ) + f(r ) +          \\adap({\\ensuremath{\\mathcal{t}}\\xspace}_i , f_{r \\cup e_i } ) { \\big } ] \\label{eq : monsub1a}\\\\        \\alg_h({\\ensuremath{\\mathcal{t}}\\xspace } ) & = \\e_{i , r \\sim s_i(\\p)}{\\big } [ f(r ) + \\alg({\\ensuremath{\\mathcal{t}}\\xspace}_i , f_{r}){\\big } ]        \\label{eq : monsub2}\\\\        & \\geq \\e_{i , r \\sim s_i(\\p)}{\\big } [ f(r ) + \\alg({\\ensuremath{\\mathcal{t}}\\xspace}_i , f _ { r \\cup            e_i}){\\big } ] \\label{eq : monsub2a }      \\end{aligned}\\ ] ]    equation  ( [ eq : monsub1 ] ) follows from the definition of ; follows from the monotonicity of @xmath0 .",
    "( we are giving the adaptive strategy elements in @xmath113 `` for free '' . ) equation  ( [ eq : monsub2 ] ) follows from the definition of , and ( [ eq : monsub2a ] ) uses the consequence of submodularity that marginals can only decrease for larger sets .",
    "observe the expressions in  ( [ eq : monsub1a ] ) and  ( [ eq : monsub2a ] ) are ideally suited to induction .",
    "indeed , since the function @xmath114 also satisfies the assumptions of theorem  [ thm : monadapvsalg ] , and the height of @xmath94 is smaller than that of @xmath66 , we use induction hypothesis on @xmath94 with monotone non - negative submodular function @xmath115 to get @xmath116 \\geq     { \\frac}13 \\ ,     \\e_{i , r \\sim s_i(\\p)}{\\big } [ \\adap({\\ensuremath{\\mathcal{t}}\\xspace}_i , f _ { r \\cup e_i}){\\big } ] .",
    "\\end{gathered}\\ ] ] finally , we use the following lemma  [ lemma : stemmass ] to show that @xmath117 \\geq     { \\frac}13 \\ ,     \\e_{i , r \\sim s_i(\\p)}{\\big } [ f(r ) + f(e_i ) { \\big } ] .    \\end{gathered}\\ ] ] substituting these two into  ( [ eq : monsub1a ] ) and  ( [ eq : monsub2a ] ) finishes the induction step .    [ lemma : stemmass ] let @xmath109 be the r.v .  denoting the index of the node",
    "at which a random walk according to @xmath75 leaves the stem .",
    "( if @xmath110 then the walk does not leave the stem . ) then , @xmath117 \\geq     { \\frac}12 \\ ,     \\e_{i}{\\big } [ f(e_i ) { \\big } ] .    \\end{gathered}\\ ] ]    for brevity , we use @xmath118 $ ] as shorthand for @xmath119 $ ] in the rest of the proof .",
    "we prove this lemma by showing that @xmath120 \\geq     \\e_{i , r}{\\big } [ \\max_{e_j \\in r } f(e_j )   { \\big } ] \\geq     { \\frac}12 \\ ,     \\e_{i}{\\big } [ f(e_i ) { \\big } ] .",
    "\\end{gathered}\\ ] ] the first inequality uses monotonicity .",
    "the rest of the proof shows the latter inequality .",
    "for any real @xmath121 , let @xmath122 denote the indices of the elements @xmath123 on the stem with @xmath124 , and let @xmath125 denote the indices of stem elements not in @xmath122 .",
    "then , @xmath126 & = \\int_0^{\\infty } \\pr_i[f(e_i ) \\geq x ] \\ ,      dx = \\int_0^{\\infty } \\pr_i [ i \\in w_x ] \\ , dx =   \\int_0^{\\infty }      \\sum_{i \\in w_x } { \\big } ( p_i \\ ,   \\prod_{j < i }   q_j { \\big } ) \\ , dx ,      \\label{eq : monotadap }    \\end{aligned}\\ ] ] where the last equality uses that the probability of exiting stem at @xmath101 is @xmath127 .    on the other hand , we have @xmath128   =   \\int_0^{\\infty } \\pr_{i , r}[\\max_{e_j        \\in r } f(e_j ) \\geq x ] \\ , dx    = \\int_0^{\\infty } \\pr_{i , r } [ r\\cap w_x\\ne \\emptyset ] \\ , dx   \\notag \\\\         & = \\int_0^{\\infty }   \\sum_{k \\in w_x }      \\pr_{i , r}[e_k\\in",
    "r \\mbox { and $ e_{j } \\not\\in r$ for all $ j < k$ with $ j\\in w_x$ } ] \\ , dx   \\notag \\\\      & = \\int_0^{\\infty }   \\sum_{k \\in w_x } \\pr_i [ i \\geq k ] \\cdot \\pr[e_k \\text { active } ]   \\cdot      \\pr_i[\\text { $ e_j$ inactive for all $ j < k$ with $ j\\in w_x$ } ]      \\,dx \\label{eq : nonadp - stem1}\\\\ & = \\int_0^{\\infty }   \\sum_{k \\in w_x }   { \\big } ( \\prod_{j < k }   q_j   { \\big } ) \\cdot p_k \\cdot { \\big } ( \\prod_{j < k \\ , \\&\\ ,   j \\in w_x } q_j { \\big } ) \\ , dx \\,=\\ , \\int_0^{\\infty }   \\sum_{k \\in w_x } { \\big } ( \\prod_{j < k \\ , \\&\\ , j \\in w_x } q_j^2 { \\big } ) \\cdot { \\big } (        \\prod_{j < k \\ , \\&\\ , j \\not\\in w_x } q_j { \\big } ) \\cdot p_k \\ , dx .",
    "\\label{eq : nonadp - stem2}\\end{aligned}\\ ] ] recall that @xmath129 .",
    "above is because , for @xmath130 to be the first element in @xmath131 ( i ) the index @xmath109 must go past @xmath7 , ( ii ) @xmath130 must be active , and ( iii ) all elements before @xmath7 on the stem with indices in @xmath122 must be inactive ( which are all independent events ) .",
    "equation is by definition of these probabilities .",
    "renaming @xmath7 to @xmath101 , @xmath132 =   \\int_0^{\\infty }   \\sum_{i \\in w_x } { \\big } ( p_i { \\big}(\\prod_{j < i \\ , \\&\\ ,   j\\in w_x } q_j^2 { \\big } ) { \\big } ( \\prod_{j < i \\ , \\&\\",
    ",   j\\not\\in w_x } q_j { \\big } ) { \\big } ) \\ , dx.\\end{aligned}\\ ] ]    to complete the proof , we compare equations  ( [ eq : monotadap ] ) and  ( [ eq : monotalg ] ) and want to show that for every @xmath133 , @xmath134 while the expressions look complicated , things simplify considerably when we condition on the outcomes of elements outside @xmath122 .",
    "indeed , observe that the lhs of  ( [ eq : finalcomp ] ) equals @xmath135,\\\\ \\intertext{where $ \\textbf{1}_{q_j}$ is an independent indicator r.v.\\ taking    value $ 1 $ w.p.\\ $ q_j$ , and we take the expectation over coin tosses    for elements of stem outside $ w_x$. similarly , the rhs    of~(\\ref{eq : finalcomp } ) is } { \\frac}12\\ , \\sum_{i\\in w_x } { \\big } ( p_i \\prod_{j",
    "< i } q_j { \\big } ) & =    \\e_{\\overline{w}_x } { \\big } [ { \\frac}12\\ , \\sum_{i \\in w_x } { \\big } ( p_i        { \\big}(\\prod_{j < i \\ , \\&\\ , j\\in w_x } q_j { \\big } ) { \\big } ( \\prod_{j < i            \\ , \\&\\ , j\\not\\in w_x } \\textbf{1}_{q_j } { \\big } ) { \\big } )    { \\big}].\\end{aligned}\\ ] ] hence , after we condition on the elements outside @xmath122 , the remaining expressions can be related using the following claim .",
    "claimstemineq [ claim : cuteineq ] for any ordered set @xmath136 of probabilities @xmath137 , let @xmath138 denote @xmath139 for @xmath140 $ ] .",
    "then , @xmath141    @xmath142    where we have repeatedly used @xmath143 for all @xmath144 .",
    "the equalities marked @xmath145 move between two ways of expressing the probability of at least one `` heads '' when the tails probability is @xmath146 and @xmath138 respectively .",
    "applying the claim to the elements in @xmath122 , in order of their distance from the root , completes the proof .",
    "our analysis can not be substantially improved , since claim  [ claim : cuteineq ] is tight .",
    "consider the setting with @xmath147 being infinite for now , and @xmath148 for all @xmath101 .",
    "then the lhs of claim  [ claim : cuteineq ] is @xmath149 , whereas the sum on the right is @xmath23 .",
    "making @xmath147 finite but large compared to @xmath150 would give similar results .",
    "however , there is still hope that a smaller adaptivity gap can be proved using other techniques .",
    "the best lower bound on adaptivity gaps for monotone submodular functions we currently know is @xmath16 .",
    "the function is the rank function of a partition matroid , where the universe has @xmath7 parts ( each with @xmath151 elements ) for a total of @xmath152 elements .",
    "each element has @xmath153 .",
    "the probing constraint is a cardinality constraint that at most @xmath151 elements can be probed . in this case",
    "the optimal adaptive strategy can get @xmath154 value , whereas any non - adaptive strategy will arbitrarily close to @xmath155 in expectation .",
    "( see , e.g. ,  ( * ? ? ?",
    "* section  3.1 ) . )      a non - adaptive policy is given by a fixed sequence @xmath156 of elements to probe ( such that @xmath157 satisfies the given prefix - closed probing constraint . if @xmath136 is the set of active elements , then the value we get is @xmath158 = \\e_a[f(a \\cap \\{e_1 , \\ldots , e_k\\})]$ ] , the inequality holding for monotone functions .",
    "if we define @xmath159 $ ] , @xmath160 is also a monotone submodular function . hence finding good non - adaptive policies for @xmath0 is just optimizing the monotone submodular function @xmath160 over the allowed sequences .",
    "e.g. , for the probing constraint being a matroid constraint , we can get a @xmath16-approximation  @xcite ; for it being an orienteering constraint we can get an @xmath161-approximation in quasi - polynomial time  @xcite .    for non - monotone functions ( discussed in the next section )",
    ", we can approximate the @xmath162 function by @xmath163 $ ] , and losing a factor of @xmath164 , reduce finding good non - adaptive strategies to ( non - monotone ) submodular optimization over the probing constraints .",
    "we now prove theorem  [ thm : nmonotsubmod ] . the proof for the monotone case used monotonicity in several places , but perhaps the most important place was to claim that going down the tree , both and could add all active elements to the set .",
    "this `` online '' feature seemed crucial to the proof .",
    "in contrast , when the adaptive strategy reaches a leaf in the non - monotone setting , it chooses the best subset within the active elements ; a similar choice is done by the non - adaptive algorithm .",
    "this is why we have @xmath165 in  ( [ eq : adap ] ) versus @xmath166 in  ( [ eq : simpler - mono ] ) .",
    "fortunately , feige el al .",
    "@xcite show that for non - negative non - monotone submodular functions , the simple strategy of picking every active element independently w.p .",
    "half gives us a near - optimal possible subset .",
    "losing a factor of four , this result allows us to analyze the performance relative to an adaptive _ online _",
    "algorithm @xmath167 which selects ( with probability @xmath168 ) each probed element that happens to be active .",
    "the rest of the proof is similar ( at a high level ) to the monotone case : to relate @xmath167 and we bound them using comparable terms ( @xmath169 and @xmath170 in definition  [ defn : proxy ] ) and apply induction .",
    "altogether we will obtain : @xmath171    in the inductive proof , we will work with `` contracted '' submodular functions @xmath160 obtained from @xmath0 , which may take negative values but have @xmath172 . in order to deal with such issues , the induction here is more complex than in the monotone case .",
    "we first define the surrogates @xmath169 and @xmath170 for and recursively as follows .",
    "[ defn : proxy ] for any strategy tree @xmath66 and submodular function @xmath160 with @xmath172 , let    @xmath109 be the node at which a random walk according to @xmath75 exits the stem .",
    "@xmath129 where @xmath173 denotes the elements on the stem until node @xmath109 .",
    "@xmath174 w.p .",
    "@xmath175 and @xmath176 w.p .",
    "@xmath175 .",
    "then we define : @xmath177 \\quad \\mbox{and }",
    "\\quad { \\overline{\\alg}}({\\ensuremath{\\mathcal{t}}\\xspace},g ) : =      \\e_{i , j}\\left [ g(j ) + { \\overline{\\alg}}({\\ensuremath{\\mathcal{t}}\\xspace}_i , g_{i\\cup j})\\right ] .    \\end{gathered}\\ ] ]    above we account for the non - monotonicity of the function , via this process of random sampling used in the definition of @xmath169 and @xmath170 .",
    "one problem with following the proof from  [ sec : monotone ] is that when we induct on the `` contracted '' function @xmath65 for some set @xmath3 , this function may not be non - negative any more .",
    "instead , our proof considers the entire path down the tree and argues about it at one shot ; to make the analysis easier we imagine that the non - adaptive algorithm picks at most one item from the stem , i.e. , the one with the highest marginal value .",
    "lemmanonmonolemma [ lem : big - lemma ] for any strategy tree @xmath66 , the following hold :    1 .   for any non - negative submodular function @xmath0 , @xmath178 .",
    "2 .   for any submodular function @xmath160 , @xmath179 .    we make use of the following property of submodular functions .",
    "[ lem : bfns ] for any non - negative submodular function @xmath180 ( possibly with @xmath181 ) let @xmath182 be a random subset that contains each element of @xmath136 with probability _ at most _ @xmath183 ( and not necessarily independently ) .",
    "then , @xmath184 \\geq ( 1-p )    \\cdot f(\\emptyset)$ ]",
    ".    we condition on a random leaf @xmath71 drawn according to @xmath185 .",
    "let @xmath186 denote the sequence of nodes that correspond to active elements on the path @xmath72 , i.e. , @xmath187 is the point where @xmath72 exits the stem of @xmath188 , @xmath189 is the point where @xmath72 exits the stem of @xmath190 etc .",
    "then , the adaptive online value is exactly @xmath191 . for any @xmath192 let @xmath193 $ ] denote the elements on path @xmath72 between @xmath194 and @xmath195 .",
    "also let @xmath113 denote the random subset where each element @xmath1 on path @xmath72 is chosen independently w.p .",
    "@xmath2 .    for @xmath192 ,",
    "define @xmath196 as follows : @xmath197 , f_{l_{k-1 } } ( e ) > 0 \\ } \\mbox { w.p . }",
    "\\frac{1}{2 } \\quad      \\mbox{and } \\quad j_k={\\bot}\\mbox { w.p . }",
    "\\frac{1}{2 } ,    \\end{gathered}\\ ] ] where @xmath198 . in words , the sets @xmath199 contain the exit points from the stems , and for each stem also the element with maximum marginal value ( if any ) with probability half .",
    "for ( i ) , by definition  [ defn : proxy ] , the value of @xmath200 conditioned on path @xmath72 and elements @xmath201 is @xmath202 the inequality follows from the following two cases :    * if @xmath203 , then by submodularity of @xmath204 , @xmath205 * if @xmath206 , then by choice of @xmath196 we have @xmath207 and @xmath208    using   and taking expectation over the @xmath209s , @xmath200 conditioned on path @xmath72 is at least @xmath210 \\ge      \\frac12\\cdot f(\\{i_1,\\ldots",
    "i_d\\ } ) .",
    "\\end{gathered}\\ ] ] above we used lemma  [ lem : bfns ] on the non - negative submodular function @xmath211 , using the fact that the set @xmath212 contains each element with probability at most half .",
    "finally , deconditioning over @xmath71 ( i.e. , over @xmath213 ) proves part  ( i ) .    for part",
    "( ii ) , by definition  [ defn : proxy ] , the value of @xmath214 conditioned on path @xmath72 and elements @xmath201 is @xmath215 where the inequality is by submodularity of @xmath160 . since chooses the maximum value subset in @xmath113 and @xmath216 , taking expectations over @xmath71 and @xmath113 , we prove part  ( ii ) .    [",
    "lem : algbar - adapbar ] for any strategy tree @xmath66 and submodular function @xmath160 with @xmath172 , @xmath217 .",
    "we proceed by induction .",
    "recall the notation in definition  [ defn : proxy ] .",
    "for each node @xmath101 on the stem of @xmath188 define @xmath218 .",
    "note that @xmath219 by choice of @xmath209 : if @xmath220 we have @xmath221 and if @xmath176 , @xmath222 .",
    "we will show that @xmath223 \\le 4\\cdot \\e_{i , j}[a_j].\\ ] ] then the definition of @xmath224 and @xmath214 , and induction on @xmath225 and @xmath226 , would prove the lemma .",
    "let @xmath227 be the r.v .  denoting the maximum weight active ( i.e. , in @xmath113 ) element on the stem .",
    "then , by definition of @xmath209 , we have @xmath228 = \\frac12 \\e_{i , k }    [ a_{k}]$ ] .",
    "finally we can use lemma  [ lemma : stemmass ] from section  [ sec : monotone ] to obtain @xmath229\\ge \\frac12    \\e_{i , j}[a_i]$ ] , which proves  .",
    "in this section we study adaptivity gaps for monotone non - negative xos functions .",
    "to recall , a function is monotone xos if there exist linear functions @xmath230 such that @xmath231 . to simplify notation we use @xmath232 for any @xmath101 and subset @xmath233 .",
    "the width of an xos function is the smallest number @xmath22 such that @xmath0 can be written as the maximum over @xmath22 linear functions .",
    "let @xmath234 denote the optimal adaptive strategy .",
    "by monotonicity @xmath61 and gives @xmath235.\\ ] ]    the following is our main result in this section .",
    "[ thm : xos ] the stochastic probing problem for monotone xos functions of width @xmath22 has adaptivity gap @xmath24 for any prefix - closed constraints .",
    "moreover , there are instances with @xmath236 and adaptivity gap @xmath237 .    in  [ sec : nonadapalgo ]",
    ", we also present an efficient non - adaptive algorithm for xos functions of width @xmath22 that makes @xmath238 calls to the following linear oracle .",
    "given a prefix - closed constraint family @xmath239 and linear function @xmath240 , oracle @xmath241 returns a set @xmath242 that maximizes @xmath243 .",
    "we first state a useful property that is used critically later .",
    "[ defn : subtree ] for any node @xmath244 in the optimal adaptive strategy tree @xmath234 , if we consider the subtree @xmath245 rooted at @xmath244 then the expected value of @xmath245 is at most that of @xmath234 : @xmath246    this is because otherwise a better strategy would be to go directly to @xmath244 ( probing all the element along the way , so that we satisfy the prefix - closed constraint , but ignore these elements ) , and then to run strategy @xmath245 .",
    "[ [ proof - idea . ] ] proof idea .",
    "+ + + + + + + + + + +    the proof consists of three steps . in the first step we argue that one can assume that every coefficient in every linear function @xmath247 is smaller than @xmath248 ( else there is a simple non - adaptive strategy that is comparable to the adaptive value obtained from a single active item ) .",
    "the second step shows that by losing a constant factor , one can truncate the tree @xmath234 to obtain tree @xmath66 , where the instantiated value at each leaf is at most @xmath249 .",
    "the combined benefit of these steps is to ensure that root - leaf paths have neither high variance nor too large a value . in the third step ,",
    "we use freedman s concentration inequality ( which requires the above properties of @xmath66 ) to argue that for any linear function @xmath247 , the instantiated value on a random root - leaf path is close to its mean with high probability . taking union bound over the @xmath22 linear functions",
    ", we can then show that ( again with high probability ) , no linear function has an instantiation much more than its mean .",
    "hence , for a random root - leaf path , @xmath88 gets value ( the maximum instantiation over linear functions ) that is not much more than the corresponding mean , which is a lower bound on the non - adaptive value .    below we use @xmath250 to denote the optimal adaptive value .",
    "[ [ small - and - large - elements . ] ] small and large elements .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    define @xmath251 .",
    "an element @xmath252 is called _ large _ if @xmath253 ; it is called _ small _ otherwise .",
    "let @xmath199 be the set of large elements , and let @xmath254 ( resp .",
    ", @xmath255 ) denote the value obtained by tree @xmath234 from large ( resp . , small ) elements . by subadditivity , we have @xmath256 .",
    "lemma  [ lem : adaplargecoeff ] shows that that when @xmath257 , a simple non - adaptive strategy proves that the adaptivity gap is @xmath258",
    ". then lemma  [ lem : adapsmallcoeff ] shows that when @xmath259 , the adaptivity gap is @xmath260 .",
    "choosing between the two by flipping an unbiased coin gives a non - adaptive strategy that proves the adaptivity gap is @xmath261 .",
    "this would prove the first part of theorem  [ thm : xos ] .",
    "[ lem : adaplargecoeff ] assuming that @xmath257 , there is a non - adaptive solution of value @xmath262 .",
    "moreover , there is a solution @xmath3 satisfying the probing constraint with @xmath263 .",
    "we restrict the optimal tree @xmath234 to the large elements .",
    "so each node in @xmath234 either contains a large element , or corresponds to making a random choice ( and adds no value ) .",
    "the expected value of this restricted tree is @xmath254 .",
    "we now truncate @xmath234 to obtain tree @xmath264 as follows .",
    "consider the first active node @xmath244 on any root - leaf path , remove the subtree below the ` yes`(active ) arc from @xmath244 , and assign exactly a value of @xmath265 to this instantiation .",
    "the subtree property ( assumption  [ defn : subtree ] ) implies that the expected value in this subtree below @xmath244 is at most @xmath266 . on the other hand , just before the truncation at @xmath244 , the adaptive strategy gains value of @xmath267 since it observed an active large element at node @xmath244 . by taking expectations",
    ", we obtain that the value of @xmath264 is at least @xmath268 .",
    "note that @xmath264 is a simpler adaptive strategy .",
    "in fact @xmath264 is a feasible solution to the stochastic probing instance with the probing constraint @xmath269 and a different objective @xmath270 which is the rank function of the uniform - matroid of rank  1 ( scaled by @xmath265 ) over all large elements .",
    "as any matroid rank function is a monotone submodular function , theorem  [ thm : monotsubmod ] implies that there is a non - adaptive strategy which probes a feasible sequence of elements @xmath271 , having value @xmath272 \\geq { \\frac}13    \\cdot \\adap(\\overline{{\\ensuremath{\\mathcal{t}}\\xspace}}^ * , g ) \\geq { \\frac}13 \\cdot    \\frac{{\\ensuremath{\\mathsf{opt}}\\xspace}_l}{1+\\lambda}$ ] . note that for any subset @xmath273 of large elements @xmath274 ; the first inequality is by monotonicity of @xmath0 and the second is by definition of large elements .",
    "so we have : @xmath275 \\ge \\e_{r\\sim s(\\p ) } [ g(r ) ] = \\omega(1/\\log    { \\ensuremath{w}\\xspace})\\cdot { \\ensuremath{\\mathsf{opt}}\\xspace}_l.\\ ] ] it follows that @xmath3 is the claimed non - adaptive solution for the original instance with objective @xmath0 .",
    "we now show the second part of the lemma using the above solution @xmath3 .",
    "note that @xmath276 = h\\cdot \\e_{r\\sim s(\\p ) } [ \\mathbf{1}(r\\cap l\\ne \\emptyset ) ] \\le h\\cdot \\min\\left\\{\\sum_{e\\in s\\cap l } p_e,\\,1 \\right\\},\\ ] ] as desired .",
    "in the rest of this section we prove the following , which implies an @xmath24 adaptivity gap .",
    "[ lem : adapsmallcoeff ] assuming that @xmath277 , there is a non - adaptive solution of value @xmath278 .",
    "we start with the restriction of the optimal tree @xmath234 to the small elements ; recall that @xmath255 is the expected value of this restricted tree .",
    "the next step is to truncate tree @xmath234 to yet another tree @xmath188with further useful properties . for any root - leaf path in @xmath234 drop the subtree below the first node @xmath244 ( including @xmath244 ) where @xmath279",
    "; here @xmath280 denotes the set of active elements on the path from the root to @xmath244 . the subtree property ( assumption  [ defn : subtree ] )",
    "implies that the expected value in the subtree below @xmath244 is at most @xmath266 . on the other hand , before the truncation at @xmath244 , the adaptive value obtained is more that @xmath281 .",
    "hence , the expected value of @xmath234 obtained at or above the truncated nodes is at least @xmath282 .",
    "finally , since all elements are small and thus the expected value from any truncated node itself is at most @xmath283 , the tree @xmath66 has at least @xmath284 value .",
    "this implies the next claim :    [ clm : xos - trunc ] tree @xmath188has expected value at least @xmath285 and @xmath286 .",
    "next , we want to claim that each linear function behaves like its expectation ( with high probability ) on a random path down the tree . for any @xmath287 $ ] and root - leaf path @xmath72 in @xmath66 , define @xmath288 = \\sum_{v \\in      p_\\ell } \\left ( p_{{\\ensuremath { { \\sf elt}}}(v)}\\cdot { \\ensuremath{\\mathbf{a}}\\xspace}_i({\\ensuremath { { \\sf elt}}}(v ) ) \\right).\\ ] ]    [ clm : concxos ] for any @xmath289 $ ] , @xmath290 \\leq \\frac{1}{{\\ensuremath{w}\\xspace}^2}.    \\end{gathered}\\ ] ]    our main tool in this proof is the following concentration inequality for martingales .",
    "[ thm : freedman ] consider a real - valued martingale sequence @xmath291 such that @xmath292 , and @xmath293 = 0 $ ] for all @xmath294 .",
    "assume that the sequence is uniformly bounded , i.e. , @xmath295 almost surely for all @xmath294 .",
    "now define the predictable quadratic variation process of the martingale to be @xmath296 $ ] for all @xmath297 . then for all @xmath298 and @xmath299 , and any stopping time @xmath33 we have @xmath300 \\quad\\leq \\quad 2      \\exp\\left(-\\frac{\\ell^2/2}{\\sigma^2 + m\\ell/3 } \\right).\\ ] ]    consider a random root - leaf path @xmath301 in @xmath66 , and let @xmath302 .",
    "now define a sequence of random variables @xmath303 where @xmath304 let @xmath305 be a filter denoting the sequence of variables before @xmath306 .",
    "observe that @xmath307 = 0 $ ] , which implies @xmath308 forms a martingale .",
    "clearly @xmath309 .",
    "now , @xmath310 & \\leq      \\sum_{j=0}^{t } \\left ( p_{e_t}(1-p_{e_t } ) + ( 1-p_{e_t})p_{e_t}\\right )      \\cdot { \\ensuremath{\\mathbf{a}}\\xspace}_i(e_t ) \\\\ & \\leq \\frac12 \\sum_{j=0}^{t } { \\ensuremath{\\mathbf{a}}\\xspace}_i(e_t ) \\le      \\frac12 \\cdot \\max_\\ell \\max_{i=1}^{{\\ensuremath{w}\\xspace } } { \\ensuremath{\\mathbf{a}}\\xspace}_i(p_\\ell ) \\leq { { \\ensuremath{\\mathsf{opt}}\\xspace } } ,    \\end{aligned}\\ ] ] where the last inequality is by claim  [ clm : xos - trunc ] .",
    "we use @xmath311 and the above equation to bound the variance , @xmath312 \\leq    h\\cdot \\sum_{j=0}^{t } \\e \\left [ |x_{j}| \\ , \\mid \\ , \\calh_j \\right ]    \\leq \\frac { { \\ensuremath{\\mathsf{opt}}\\xspace}^2}{\\lambda}.\\ ] ] applying theorem  [ thm : freedman ] , we get @xmath313 & = \\pr [ |{\\ensuremath{\\mathbf{a}}\\xspace}_i(a_\\ell ) -      \\mu_i(p_\\ell)| >   0.1 ~{\\ensuremath{\\mathsf{opt}}\\xspace}]\\\\      & \\leq 2 \\exp\\left(-\\frac{(0.1 ~{\\ensuremath{\\mathsf{opt}}\\xspace})^2/2 } { { \\ensuremath{\\mathsf{opt}}\\xspace}^2/ ( \\lambda ) \\,+\\ ,          ( { \\ensuremath{\\mathsf{opt}}\\xspace}/ ( \\lambda ) ) \\cdot ( 0.1 ~{\\ensuremath{\\mathsf{opt}}\\xspace})/3 } \\right ) \\\\      & \\leq \\frac{1}{{{\\ensuremath{w}\\xspace}}^2}.    \\end{aligned}\\ ] ] this completes the proof of claim  [ clm : concxos ] .",
    "now we can finish the proof of lemma  [ lem : adapsmallcoeff ] .",
    "we label every leaf @xmath71 in @xmath66 according to the linear function @xmath247 that achieves the value @xmath166 , breaking ties arbitrarily .",
    "i.e. , for leaf @xmath71 we define @xmath314 also define @xmath315 for @xmath101 as above .",
    "using claim  [ clm : concxos ] and taking a union bound over all @xmath316 $ ] , @xmath317 ~~\\le~~ \\frac1{{\\ensuremath{w}\\xspace}}.\\ ] ] consider the natural non - adaptive solution which selects @xmath318 and probes all elements in @xmath72 .",
    "this has expected value at least : @xmath319    ~~\\stackrel{\\small ( \\ref{eq : xos - inst - mean})}{\\ge}~~ \\e_{\\ell\\gets      \\pi_{\\ensuremath{\\mathcal{t}}\\xspace } } \\left[c^{max}_\\ell ( a_\\ell)\\right ] - 0.1 ~{\\ensuremath{\\mathsf{opt}}\\xspace}-    { \\frac}{1}{{\\ensuremath{w}\\xspace}}(2~{\\ensuremath{\\mathsf{opt}}\\xspace } ) ~~\\stackrel{\\small      ( \\text{claim~\\ref{clm : xos - trunc}})}{\\ge}~~ ( 0.15-{\\frac}{2}{{\\ensuremath{w}\\xspace}})\\cdot { \\ensuremath{\\mathsf{opt}}\\xspace}.\\end{gathered}\\ ] ] if @xmath320 then we obtain the desired non - adaptive strategy .",
    "the remaining case of @xmath321 is trivial : the adaptivity gap is  @xmath23 for a single linear function , and taking the best non - adaptive solution among the @xmath22 possibilities has value at least @xmath322 .",
    "this completes the proof of lemma  [ lem : adapsmallcoeff ] .",
    "let us record an observation that will be useful for the non - adaptive algorithm .",
    "[ rem : smallcoeff ] observe that the above proof shows that when @xmath277 , there exists a path @xmath323 in @xmath234 ( i.e. @xmath323 satisfies the probing constraints ) and a linear function @xmath324 with mean value @xmath325 = \\omega({\\ensuremath{\\mathsf{opt}}\\xspace})$ ] .",
    "consider any instance of the stochastic probing problem with a width-@xmath326 monotone xos objective and prefix - closed constraint @xmath269 .",
    "our non - adaptive algorithm is the following ( here @xmath327 is as in  [ subsec : xos - ub ] ) .",
    "[ h ! ]    * define * @xmath328 } { \\ensuremath{\\mathbf{a}}\\xspace}_i(e ) \\}$ ] * define * @xmath329 with @xmath330 if @xmath331 } \\{{\\ensuremath{\\mathbf{a}}\\xspace}_i(e)\\ } \\geq \\frac{2^j m}{\\lambda}$ ] and @xmath332 otherwise .",
    "@xmath333 and @xmath334 . *",
    "define * @xmath335 with @xmath336 @xmath337 and @xmath338 .",
    "set @xmath339 that maximizes @xmath340 .",
    "[ [ case - i - ensuremathmathsfoptxspace_l - geq - ensuremathmathsfoptxspace2 .",
    "] ] case i : @xmath341 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    lemma  [ lem : adaplargecoeff ] shows that in this case it suffices to consider only the set of large elements and to maximize the probability of selecting a single large element .",
    "while we do not know @xmath266 , and the large elements are defined in terms of @xmath266 , we do know @xmath342 } { \\ensuremath{\\mathbf{a}}\\xspace}_i(e ) \\ } \\leq { \\ensuremath{\\mathsf{opt}}\\xspace}\\leq n\\cdot m$ ] . in the above algorithm , consider the value of @xmath343 when @xmath344 is between @xmath265 and @xmath345 .",
    "let @xmath199 denote the set of large elements ; note that these correspond to the elements with positive @xmath346 values . by the second part of lemma  [ lem : adaplargecoeff ] , the solution @xmath347 returned by the oracle",
    "will satisfy @xmath348 . now interpreting this solution @xmath349 as a non - adaptive solution , we get an expected value at least : @xmath350 \\quad = \\quad h\\cdot \\left(1 - \\pi_{e\\in t_j } ( 1-\\mathbf{b}_j(e))\\right ) \\quad \\ge \\quad h\\cdot \\left(1 - e^{-\\mathbf{b}_j(t_j)}\\right ) \\\\ & \\ge ( 1 - 1/e)h\\cdot \\min\\{\\mathbf{b}_j(t_j),1 \\ } \\quad = \\quad ( 1 - 1/e ) \\cdot v(t_j ) \\quad \\ge",
    "\\quad \\frac{{\\ensuremath{\\mathsf{opt}}\\xspace}}{o(\\log w)}\\end{aligned}\\ ] ]    [ [ case - ii - ensuremathmathsfoptxspace_s - geq - ensuremathmathsfoptxspace2 . ] ] case ii : @xmath351 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this case remark  [ rem : smallcoeff ] following the proof of lemma  [ lem : adapsmallcoeff ] shows that there exists a solution @xmath323 satisfying the probing constraints @xmath269 and a linear function @xmath324 with mean value @xmath352 = \\omega({\\ensuremath{\\mathsf{opt}}\\xspace})$ ] . since the above algorithm calls @xmath353 for each @xmath354 $ ] and chooses the best one , it will return a set with value @xmath355 .",
    "consider a @xmath7-ary tree of depth @xmath7 , whose edges are the ground set . each edge",
    "/ element has probability @xmath356 . here",
    ", imagine @xmath357 , so that the total number of edges is @xmath358 .",
    "for each of the @xmath359 leaves @xmath360 , consider the path @xmath361 from the root to that leaf .",
    "the xos function is @xmath362 .",
    "note that the width @xmath25 in this case .",
    "suppose the probing constraint is the following prefix - closed constraint : there exists a root - leaf path @xmath361 such that all probed edges have at least one endpoint on this path .",
    "this implies that we can probe at most @xmath151 edges .",
    "* for an adaptive strategy , probe the @xmath7 edges incident to the root .",
    "if any one of these happens to be active , start probing the @xmath7 edges at the next level below that edge .",
    "( if none were active , start probing the edges below the left - most child , say . )",
    "each level will have at least one active edge with probability @xmath363 , so we will get an expected value of @xmath364 .",
    "* now consider any non - adaptive strategy : it is specified by the path @xmath361 whose vertices hit every edge that is probed .",
    "there are @xmath151 such edges , we can probe all of them .",
    "but the xos function can get at most @xmath23 from an edge not on @xmath361 , and it will get at most @xmath365 in expectation from the edges on @xmath361 .    this shows a gap of @xmath366 for xos functions with a prefix - closed ( in fact subset - closed ) probing constraint .",
    "we can show a near - logarithmic lower bound for xos functions even for the most simple cardinality constraints .",
    "the setup is the same as above , just the constraint is that a subset of at most @xmath151 edges can be probed .    *",
    "the adaptive strategy remains the same , with expected value @xmath364 .",
    "* we claim that any non - adaptive strategy gets expected value @xmath367 .",
    "such a non - adaptive strategy can fix any set @xmath3 of @xmath151 edges to probe . for each of these edges , choose an arbitrary root - leaf path passing through it , let @xmath12 be the edges lying in these @xmath151 many root - leaf paths of length @xmath7 .",
    "so @xmath368 .",
    "let us even allow the strategy to probe all the edges in @xmath12clearly this is an upper bound on the non - adaptive value .",
    "+ the main claim is that the expected value to be maximized when @xmath12 consists of @xmath151 many disjoint paths .",
    "( the @xmath7-ary tree does not have these many disjoint paths , but this is just a thought - experiment . ) the claim follows from an inductive application of the following simple fact .",
    "+ given independent non negative random variables @xmath369 , where @xmath370 and @xmath29 have the same distribution , the following holds : @xmath371 \\leq \\e_{x , x',y , z } [ \\max\\{x +      y , x ' + z\\}].\\ ] ] + follows from the fact that @xmath372 .",
    "+ finally , for any path with @xmath7 edges , we expect to get value @xmath23 in expectation .",
    "the probability that any one path gives value @xmath373 is @xmath374 , for suitable constant @xmath375 .",
    "so a union bound implies that the maximum value over @xmath151 path is at most @xmath373 with probability @xmath376 .",
    "finally , the xos function can take on value at most @xmath7 , so the expected value is at most @xmath377 .",
    "this shows an adaptivity gap of @xmath378 even for cardinality constraints .",
    "in this paper we saw that submodular functions , both monotone and non - monotone , have a constant adaptivity gap , with respect to all prefix - closed probing constraints .",
    "moreover , for monotone xos functions of width @xmath326 , the adaptivity gap is @xmath261 , and there are nearly - matching lower bounds for all @xmath236 .",
    "the most obvious open question is whether for _ all _ xos functions , the adaptivity gap is @xmath379 for some constant @xmath380 .",
    "this would immediately imply an analogous result for all _ subadditive _ functions as well .",
    "( in  [ sec : non - monotone - xos ] we show that it suffices to bound the adaptivity gap for _ monotone _ xos and subadditive functions . )",
    "other questions include : can we get better bounds for special submodular functions of interest ? e.g. , for matroid rank functions , can we improve the bound of @xmath15 from theorem  [ thm : monotsubmod ] .",
    "we can improve the constants of @xmath17 for the non - monotone case with more complicated analyses , but getting ( near)-tight results will require not losing the factor of @xmath164 from  ( [ eq : fmv ] ) , and may require a new insight . or can we do better for special prefix - closed constraints .",
    "our emphasis was to give the most general result we could , but it should be possible to do quantitatively better for special cases of interest .",
    "our definition of fractionally subadditive / xos differs from the usual one , since it allows the function to be non - monotone .",
    "to show the difference , here is the usual definition :    * a function @xmath0 is _ monotone fractionally subadditive _ if @xmath381 for all @xmath382 with @xmath55 .",
    "note the subtle difference : we now place a constraint when the set @xmath12 is fractionally covered by the sets @xmath103 .",
    "note that such a function is always monotone : for @xmath383 we have @xmath384 and hence @xmath385 .",
    "+ similarly , we can define a function @xmath0 to be _ monotone xos _",
    "( a.k.a .",
    "max - of - sums ) if there exist linear functions @xmath386 such that @xmath387 . the difference is that we only allow non - negative coefficients in the linear functions .",
    "the equivalence of these definitions is shown in  @xcite .",
    "it is also known that the class of monotone xos functions lies between monotone submodular and monotone subadditive functions .",
    "these same proofs , with minor alterations , show that xos functions are the same as fractionally subadditive functions ( according to the definitions in  [ sec : prelims ] ) , and lie between general submodular and general subadditive functions .",
    "finally , if @xmath0 satisfies the xos definition in  [ sec : prelims ] , and @xmath0 is monotone , it also satisfies the definition above .",
    "indeed , by duplicating sets and dropping some elements , we can take the sets @xmath388 and values @xmath55 satisfying @xmath389 , and get sets @xmath390 satisfying @xmath391 . by the general xos definition ,",
    "we get @xmath392 , which by monotonicity is at most @xmath393 .",
    "hence , the definitions in  [ sec : prelims ] and above are consistent .",
    "it suffices to prove the adaptivity gap conjecture for monotone xos or subadditive functions , since for any xos function @xmath0 , the function @xmath62 is also xos ( shown below ) .",
    "note that @xmath62 is clearly monotone .",
    "so we can just deal with the monotone xos / subadditive function @xmath62 .",
    "( we note that such a property is not true for submodular functions , i.e. @xmath0 being submodular does not imply that @xmath62 is . )",
    "consider any ( possibly non - monotone ) xos function @xmath0 .",
    "we will show that @xmath62 is fractionally subadditive , i.e. , for any @xmath394 , @xmath395 and @xmath396 with @xmath397 , @xmath398 .",
    "consider any @xmath399 as above .",
    "let @xmath400 be the set achieving the maximum in @xmath401 , i.e. @xmath402 .",
    "now consider the linear combination of the sets @xmath403 with multipliers @xmath404 .",
    "we have @xmath405 .",
    "so , by the fractionally subadditive property of @xmath0 , @xmath406 the last inequality above is by definition of @xmath62 as @xmath407 .",
    "thus we have @xmath398 as desired .",
    "consider a monotone function @xmath0 on @xmath408 types of items , with @xmath7 items of each type ( total @xmath409 items ) . on any set @xmath3 of items ,",
    "function @xmath0 takes value @xmath23 if @xmath3 contains at least one item of every type , and takes value @xmath86 otherwise .",
    "suppose each item is active independently w.p .",
    "@xmath410 and the constraint allows us to probe at most @xmath411 items .",
    "the optimal non - adaptive strategy here is to probe @xmath164 items of each type .",
    "this strategy has an expected value of @xmath412 . on the other hand , consider an adaptive strategy that arbitrarily orders the types and probes items of a type until it sees an active copy , and then moves to the next type .",
    "since in expectation this strategy only probes @xmath413 items of a type before moving to the next , with constant probability it will see an active copy of every type within the @xmath411 probes .",
    "hence , the adaptivity gap for this example is @xmath414 ."
  ],
  "abstract_text": [
    "<S> suppose we are given a submodular function @xmath0 over a set of elements , and we want to maximize its value subject to certain constraints . </S>",
    "<S> good approximation algorithms are known for such problems under both monotone and non - monotone submodular functions . </S>",
    "<S> we consider these problems in a stochastic setting , where elements are not all active and we can only get value from active elements . </S>",
    "<S> each element @xmath1 is active independently with some known probability @xmath2 , but we do nt know the element s status _ a priori_. we find it out only when we _ probe _ the element @xmath1probing reveals whether it s active or not , whereafter we can use this information to decide which other elements to probe . eventually , if we have a probed set @xmath3 and a subset @xmath4 of active elements in @xmath3 , we can pick any @xmath5 and get value @xmath6 . </S>",
    "<S> moreover , the sequence of elements we probe must satisfy a given _ prefix - closed constraint_e.g . </S>",
    "<S> , these may be given by a matroid , or an orienteering constraint , or deadline , or precedence constraint , or an arbitrary downward - closed constraint  </S>",
    "<S> if we can probe some sequence of elements we can probe any prefix of it . </S>",
    "<S> what is a good strategy to probe elements to maximize the expected value ?    in this paper </S>",
    "<S> we study the gap between adaptive and non - adaptive strategies for @xmath0 being a submodular or a fractionally subadditive ( xos ) function . </S>",
    "<S> if this gap is small , we can focus on finding good non - adaptive strategies instead , which are easier to find as well as to represent . </S>",
    "<S> we show that the adaptivity gap is a constant for monotone and non - monotone submodular functions , and logarithmic for xos functions of small _ width_. these bounds are nearly tight . </S>",
    "<S> our techniques show new ways of arguing about the optimal adaptive decision tree for stochastic problems . </S>"
  ]
}