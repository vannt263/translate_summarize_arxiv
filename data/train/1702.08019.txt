{
  "article_text": [
    "high - dimension , low - sample - size ( hdlss ) data situations occur in many areas of modern science such as genetic microarrays , medical imaging , text recognition , finance , chemometrics , and so on .",
    "suppose we have independent and @xmath2-variate two populations , @xmath3 , having an unknown mean vector @xmath4 and unknown covariance matrix @xmath5 .",
    "we assume that @xmath6 as @xmath7 for @xmath8 . here , for a function , @xmath9 , ",
    "@xmath10 as @xmath7 \" implies @xmath11 and @xmath12 .",
    "let @xmath13 , where @xmath14 denotes the euclidean norm .",
    "we assume that @xmath15 .",
    "we have independent and identically distributed ( i.i.d . )",
    "observations , @xmath16 , from each @xmath17 .",
    "we assume @xmath18 .",
    "let @xmath19 be an observation vector of an individual belonging to one of the two populations .",
    "let @xmath20 .    in the hdlss context , @xcite , @xcite and @xcite considered distance weighted classifiers .",
    "@xcite , @xcite and @xcite considered distance - based classifiers . in particular , @xcite gave the misclassification rate adjusted classifier for multiclass , high - dimensional data in which misclassification rates are no more than specified thresholds .",
    "on the other hand , @xcite considered geometric classifiers based on a geometric representation of hdlss data .",
    "@xcite considered a classifier based on the maximal data piling direction .",
    "@xcite considered quadratic classifiers in general and discussed asymptotic properties and optimality of the classifies under high - dimension , non - sparse settings . in particular",
    ", @xcite showed that the misclassification rates tend to @xmath21 as @xmath2 increases , i.e. , @xmath22 under the non - sparsity such as @xmath23 as @xmath7 , where @xmath24 denotes the error rate of misclassifying an individual from @xmath17 into the other class .",
    "we call ( [ 1.1 ] )  the consistency property \" .",
    "we note that a linear classifier can give such a preferable performance under the non - sparsity .",
    "also , such non - sparse situations often appear in real high - dimensional data .",
    "see @xcite for the details .",
    "hence , in this paper , we focus on linear classifiers .    in the field of machine learning , there are many studies about the classification in the context of supervised learning . a typical method is the support vector machine ( svm ) .",
    "the svm has versatility and effectiveness both for low - dimensional and high - dimensional data .",
    "see @xcite , @xcite , @xcite , @xcite and @xcite for the details . even though the svm is quite popular , its asymptotic properties seem to have not been studied sufficiently . in this paper",
    ", we investigate asymptotic properties of the svm for hdlss data .",
    "now , let us use the following toy examples to see the performance of the hard - margin linear svm given by ( [ 2.4 ] ) .",
    "we set @xmath25 and @xmath26 .",
    "independent pseudo random observations were generated from @xmath27 , @xmath8 .",
    "we set @xmath28 and @xmath29 , so that @xmath30 .",
    "we considered three cases :    \\(a ) @xmath31 and @xmath32 ; + ( b ) @xmath33 and @xmath32 ; and + ( c ) @xmath31 , @xmath34 and @xmath35 ,    where @xmath36 denotes the @xmath2-dimensional identity matrix .",
    "note that @xmath37 for ( a ) to ( c ) .",
    "then , from theorem 1 in @xcite , the classifier should hold ( [ 1.1 ] ) for ( a ) to ( c ) .",
    "we repeated 2000 times to confirm if the classifier does ( or does not ) classify @xmath38 correctly and defined @xmath39 accordingly for each @xmath40 .",
    "we calculated the error rates , @xmath41 , @xmath8 . also , we calculated the average error rate , @xmath42 . in figure [ f1 ]",
    ", we plotted @xmath43 , @xmath44 and @xmath45 for ( a ) to ( c ) .",
    "we observe that the svm gives a good performance as @xmath2 increases for ( a ) .",
    "contrary to expectations , it leads undesirable performances both for ( b ) and ( c ) .",
    "the error rates becomes small as @xmath2 increases , however , @xmath43 and @xmath44 are quite unbalanced .",
    "we discuss some theoretical reasons in section 2.2 .    ) in hdlss settings .",
    "the left panel displays @xmath43 , the right panel displays @xmath44 and the top panel displays @xmath45 . ]    in this paper , we investigate the svm in the hdlss context . in section 2 , we show that the svm holds ( [ 1.1 ] ) under certain severe conditions .",
    "we show that the svm is very biased in hdlss settings and its performance is affected by the bias directly . in order to overcome such difficulties ,",
    "we propose a bias - corrected svm ( bc - svm ) in section 3 .",
    "we show that the bc - svm improves the svm even when @xmath46s or @xmath47s are unbalanced as in ( b ) or ( c ) in figure 1 . in section 4",
    ", we check the performance of the bc - svm by numerical simulations and use the bc - svm in actual data analyses . in section 5 ,",
    "we discuss multiclass svms in hdlss settings .",
    "in this section , we give asymptotic properties of the svm in hdlss settings . since hdlss data are linearly separable by a hyperplane , we consider the hard - margin linear svm .",
    "we consider the following linear classifier : @xmath48 where @xmath49 is a weight vector and @xmath50 is an intercept term .",
    "let us write that @xmath51 .",
    "let @xmath52 for @xmath53 and @xmath54 for @xmath55 .",
    "the hard - margin svm is defined by maximizing the smallest distance of all observations to the separating hyperplane .",
    "the optimization problem of the svm can be written as follows : @xmath56 a lagrangian formulation is given by @xmath57 where @xmath58 and @xmath59s are lagrange multipliers . by differentiating the lagrangian formulation with respect to @xmath49 and @xmath50 , we obtain the following conditions : @xmath60 after substituting them into @xmath61 , we obtain the dual form : @xmath62 the optimization problem can be transformed into the following : @xmath63 subject to @xmath64 let us write that @xmath65 there exist some @xmath66s satisfying that @xmath67 ( i.e. , @xmath68 ) .",
    "such @xmath66s are called the support vector .",
    "let @xmath69 and @xmath70 , where @xmath71 denotes the number of elements in a set @xmath72 . the intercept term is given by @xmath73 then , the linear classifier in ( [ 2.1 ] ) is defined by @xmath74 finally , in the svm , one classifies @xmath19 into @xmath75 if @xmath76 and into @xmath77 otherwise .",
    "see @xcite for the details .",
    "we assume the following assumptions :    ( a - i ) : :    @xmath78    as @xmath7 for @xmath8 ; ( a - ii ) : :    @xmath79    as @xmath7 for @xmath8 .",
    "note that @xmath80 when @xmath17 is gaussian , so that ( a - i ) and ( a - ii ) are equivalent when @xmath17s are gaussian .    under ( [ 2.3 ] ) , it holds that as @xmath7 @xmath81    let @xmath82 and @xmath83 . under the constraint that @xmath84 for a given positive constant @xmath85",
    ", we can claim that @xmath86 when @xmath87 and @xmath88 under ( [ 2.3 ] ) .",
    "then , from lemma 1 it holds that @xmath89 for given @xmath90 .",
    "hence , by choosing @xmath91 , we have the maximum of @xmath92 asymptotically .",
    "it holds that as @xmath7 @xmath93 furthermore , it holds that as @xmath7 @xmath94    from lemma 2 , all the data points are the support vectors under ( a - i ) and ( a - ii ) in the hdlss context .",
    "@xcite called this phenomenon the  data piling \" .",
    "see sections 1 and 2 in @xcite for the details .",
    "let @xmath95 . from lemma 2",
    ", it holds that as @xmath7 @xmath96 when @xmath97 , @xmath8 . hence ,  @xmath98 \" is the bias term of the ( normalized ) svm .",
    "we consider the following assumption :    ( a - iii ) : :    @xmath99 .    under ( a - i ) to ( a - iii ) ,",
    "the svm holds ( [ 1.1 ] ) .    under ( a - i ) and ( a - ii )",
    ", the svm holds the following properties : @xmath100    for the svm , @xcite and @xcite also showed ( [ 1.1 ] ) and the results in corollary 1 under different conditions .",
    "we emphasize that ( a - i ) , ( a - ii ) and ( a - iii ) are milder than their conditions .",
    "moreover , we can evaluate the bias of the svm by using ( [ 2.7 ] ) .",
    "we expect from ( [ 2.7 ] ) that , for sufficiently large @xmath2 , @xmath101 and @xmath102 for the svm become small and @xmath101 ( or @xmath102 ) is larger than @xmath102 ( or @xmath101 ) if @xmath103 ( or @xmath104 ) .",
    "actually , in figure 1 , we observe that @xmath43 is larger than @xmath44 for ( b ) in which @xmath105 and @xmath44 is larger than @xmath43 for ( c ) in which @xmath106 . as for ( a ) in which @xmath107 , the svm gives a preferable performance .",
    "as discussed in section 2.2 , if @xmath109 , the svm gives an undesirable performance . from corollary 1 , if @xmath110 , one should not use the svm . in order to overcome such difficulties , we consider a bias correction of the svm .",
    "we estimate @xmath4 and @xmath47 by @xmath111 and @xmath112 .",
    "we estimate @xmath113 by @xmath114 .",
    "note that @xmath115 .",
    "let @xmath116 .",
    "note that @xmath117 .    under ( a - i ) and ( a - ii )",
    ", it holds that as @xmath7 @xmath118    now , we define the bias - corrected svm ( bc - svm ) by @xmath119 where @xmath120 is given by ( [ 2.4 ] ) . in the bc - svm ,",
    "one classifies @xmath19 into @xmath75 if @xmath121 and into @xmath122 otherwise .    by combining ( [ 2.7 ] ) with lemma 4 , under ( a - i ) and ( a - ii )",
    ", it holds that as @xmath7 @xmath123 when @xmath97 , @xmath8 .    under ( a - i ) and",
    "( a - ii ) , the bc - svm holds ( [ 1.1 ] ) .",
    "one should note that the bc - svm has the consistency property without ( a - iii ) . @xcite",
    "considered a different bias correction for the svm .",
    "they showed the consistency property under some stricter conditions than ( a - i ) and ( a - ii ) .",
    "@xcite considered the distance - based classifier as follows : one classifies an individual into @xmath75 if @xmath124 and into @xmath122 otherwise , where @xmath125 .",
    "then , from theorem 1 in @xcite , under ( a - ii ) , it holds that as @xmath7 @xmath126 when @xmath97 , @xmath8 .",
    "in this section , we check the performance of the bc - svm both in numerical simulations and actual data analyses .",
    "first , we checked the performance of the bc - svm by using the toy examples in figure 1 . similar",
    "to section 1 , we calculated the error rates , @xmath43 , @xmath44 and @xmath45 , by 2000 replications and plotted the results in figure 2 .",
    "we laid @xmath43 , @xmath44 and @xmath45 for the svm by borrowing from figure 1 . as expected theoretically , we observe that the bc - svm gives preferable performances even for ( b ) and ( c ) in which @xmath109 .    , the middle panels display @xmath44 and the right panels display @xmath45 .",
    "the corresponding error rates by the svm are denoted by the dashed lines .",
    ", title=\"fig : \" ] + ( a ) @xmath31 and @xmath32 ( i.e. , @xmath127 ) + , the middle panels display @xmath44 and the right panels display @xmath45 .",
    "the corresponding error rates by the svm are denoted by the dashed lines .",
    ", title=\"fig : \" ] + ( b ) @xmath33 and @xmath32 ( i.e. , @xmath105 ) + , the middle panels display @xmath44 and the right panels display @xmath45 .",
    "the corresponding error rates by the svm are denoted by the dashed lines .",
    ", title=\"fig : \" ] + ( c ) @xmath31 , @xmath34 and @xmath35 ( i.e. , @xmath106 )    next , we compared the performance of the bc - svm with the svm in complex settings .",
    "we set @xmath28 , @xmath128 and @xmath129 , where @xmath130.\\ ] ] note that @xmath131 .",
    "we generated @xmath132 , @xmath133 independently either from ( i ) @xmath134 , or ( ii ) a @xmath2-variate @xmath135-distribution , @xmath136 , with mean zero , covariance matrix @xmath47 and degrees of freedom 10 .",
    "note that ( a - i ) holds under ( a - ii ) for ( i ) .",
    "let @xmath137 , where @xmath138 denotes the smallest integer @xmath139 .",
    "similar to section 1 , we calculated the error rates , @xmath43 , @xmath44 and @xmath45 , by 2000 replications and plotted the results in figure 3 .    , the middle panels display @xmath44 and the right panels display @xmath45 .",
    ", title=\"fig : \" ] + ( d ) @xmath140 @xmath141 , @xmath142 and @xmath143 , for ( i ) @xmath144 + , the middle panels display @xmath44 and the right panels display @xmath45 . , title=\"fig : \" ] + ( e ) @xmath140 @xmath141 , @xmath145 and @xmath146 , for ( ii ) @xmath147 + , the middle panels display @xmath44 and the right panels display @xmath45 . , title=\"fig : \" ] +   ( f ) @xmath145 , @xmath148 and @xmath149 , for ( ii ) @xmath147   + , the middle panels display @xmath44 and the right panels display @xmath45 . , title=\"fig : \" ] +   ( g ) @xmath145 , @xmath148 and @xmath150 , for ( ii ) @xmath147    we observe that the svm gives quite bad performances for ( d ) in figure 3",
    ". the main reason must be due to the bias term in the svm .",
    "note that @xmath151 as @xmath7 for ( d ) .",
    "thus @xmath43 becomes close to @xmath152 as @xmath2 increases .",
    "see corollary 1 for the details .      first , we used colon cancer data with @xmath153 genes given by @xcite which consists of @xmath154 colon tumor ( 40 samples ) and @xmath155 normal colon ( 22 samples ) .",
    "we set @xmath156 .",
    "we randomly split the data sets from @xmath157 into training data sets of sizes @xmath158 and test data sets of sizes @xmath159 .",
    "we had the average misclassification rates as @xmath160 , @xmath161 and @xmath162 for the bc - svm , and @xmath163 , @xmath164 and @xmath165 for the svm . by using all the samples",
    ", we considered estimating @xmath166 .",
    "we set @xmath167 and @xmath168 . from section 3.1 in @xcite , an unbiased estimator of @xmath169",
    "was given by @xmath170 .",
    "we estimated @xmath166 by @xmath171 and had @xmath172 for the @xmath173 samples . in view of (",
    "[ 3.1 ] ) , we expect that the bc - svm is asymptotically equivalent to the svm in such cases .",
    "we summarized the results for various @xmath46s in table 1 .",
    ".average misclassification rates of the bc - svm and the svm , together with @xmath174 , for @xcite s colon cancer data [ cols=\"^,^,^,^,^,^,^,^ \" , ]     [ [ section ] ]    throughout , let @xmath175 and @xmath176 .    under ( a - ii ) , we have that as @xmath7 @xmath177 then , by using chebyshev s inequality , for any @xmath178 , under ( a - ii ) , we have that @xmath179 \\notag \\\\ & = o\\{{\\mbox{\\rm tr}}({\\mbox{\\boldmath $ \\sigma $ } } _ 1 ^ 2)+{\\mbox{\\boldmath $ \\mu $ } } ^t{\\mbox{\\boldmath $ \\sigma",
    "$ } } _ 1{\\mbox{\\boldmath $ \\mu $ } } \\}/\\delta^2=o(1)\\quad \\mbox{for $ 1\\le j < k\\le n_1",
    "$ } ; \\notag \\\\ & p(|({\\mbox{\\boldmath { $ x$}}}_j-{\\mbox{\\boldmath $ \\mu $ } } _ * ) ^t({\\mbox{\\boldmath { $ x$}}}_k-{\\mbox{\\boldmath $ \\mu $ } } _ * ) -\\delta/4 |\\ge \\tau \\delta ) \\notag \\\\ & = o\\{{\\mbox{\\rm tr}}({\\mbox{\\boldmath $ \\sigma $ } } _ 2 ^ 2)+{\\mbox{\\boldmath $ \\mu $ } } ^t{\\mbox{\\boldmath $ \\sigma",
    "$ } } _ 2{\\mbox{\\boldmath $ \\mu $ } } \\}/\\delta^2=o(1)\\quad   \\mbox{for $ n_1 + 1 \\le j < k\\le n$};\\quad \\mbox{and } \\notag \\\\ & p(|({\\mbox{\\boldmath { $ x$}}}_j-{\\mbox{\\boldmath $ \\mu $ } } _ * ) ^t({\\mbox{\\boldmath { $ x$}}}_k-{\\mbox{\\boldmath $ \\mu $ } } _ * ) + \\delta/4",
    "|\\ge \\tau \\delta ) \\notag \\\\ & = o\\{{\\mbox{\\rm tr}}({\\mbox{\\boldmath $ \\sigma $ } } _ 1{\\mbox{\\boldmath $ \\sigma $ } } _ 2)+{\\mbox{\\boldmath $ \\mu $ } } ^t({\\mbox{\\boldmath $ \\sigma",
    "$ } } _ 1+{\\mbox{\\boldmath $ \\sigma $ } } _ 2){\\mbox{\\boldmath $ \\mu $ } } \\}/\\delta^2=o(1)\\notag \\\\ & \\quad \\ \\mbox{for $ 1 \\le j \\le n_1 $ and $ n_1 + 1\\le k\\le n$ }   \\label{a.2}\\end{aligned}\\ ] ] from the fact that @xmath180 . from ( [ a.1 ] ) , for any @xmath178 , we have that @xmath181 under ( a - i ) and ( a - ii ) . here",
    ", subject to ( [ 2.3 ] ) , we can write for ( [ 2.2 ] ) that @xmath182 then , by noting that @xmath183 for all @xmath184 subject to ( [ 2.3 ] ) , from ( [ a.2 ] ) and ( [ a.3 ] ) , we have that @xmath185 subject to ( [ 2.3 ] ) under ( a - i ) and ( a - ii ) .",
    "it concludes the result .    by combining lemma 1 with ( [ 2.5 ] ) and",
    "( [ 2.6 ] ) , we can claim the first result .    when @xmath186 , by noting that @xmath187 , we have that @xmath188 from the first result of lemma 2 , ( [ a.2 ] ) and ( [ a.3 ] ) , we have that as @xmath7 @xmath189 under ( a - i ) and ( a - ii ) .",
    "similar to ( [ a.2 ] ) , under ( a - ii ) , we obtain that @xmath190 for @xmath53 , and @xmath191 for @xmath55 , when @xmath192 @xmath193 .",
    "then , from the first result of lemma 2 , under ( a - i ) and ( a - ii ) , it holds that @xmath194 when @xmath192 for @xmath8 . by combining ( [ a.6 ] ) with ( [ a.7 ] ) and ( [ a.8 ] )",
    ", we can conclude the second result .    by using ( [ 2.7 ] ) , the results are obtained straightforwardly .",
    "we have that @xmath195 note that @xmath196=o(\\delta^2)$ ] as @xmath7 under ( a - i ) for all @xmath197 .",
    "also , note that @xmath198={\\mbox{\\boldmath $ \\mu $ } } ^t{\\mbox{\\boldmath $ \\sigma $ } } _ i{\\mbox{\\boldmath $ \\mu $ } } /n_i\\le \\delta { \\mbox{\\rm tr}}({\\mbox{\\boldmath $ \\sigma $ } } _",
    "i^2)^{1/2}/ n_i = o(\\delta^2/n_i)$ ] as @xmath7 under ( a - ii ) for @xmath8 .",
    "then , from ( [ a.11 ] ) , we can claim that @xmath199 under ( a - i ) and ( a - ii ) , so that @xmath200 . on the other hand",
    ", we have that @xmath201 then , similar to @xmath202 , we can claim that @xmath203 for @xmath8 , under ( a - i ) and ( a - ii ) , so that @xmath204 .",
    "hence , by noting that @xmath205 , we can claim the result .    by using ( [ 3.2 ] ) , the result is obtained straightforwardly .    by using theorems 1 and 2 ,",
    "the results are obtained straightforwardly .",
    "research of the second author was partially supported by grant - in - aid for young scientists ( b ) , japan society for the promotion of science ( jsps ) , under contract number 26800078 .",
    "research of the third author was partially supported by grants - in - aid for scientific research ( a ) and challenging exploratory research , jsps , under contract numbers 15h01678 and 26540010 .",
    "alon , u. , barkai , n. , notterman , d.a . , gish , k. , ybarra , s. , mack , d. , levine , a.j .",
    "broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays .",
    "usa 96 , 6745 - 6750 .",
    "golub , t.r .",
    ", slonim , d.k . ,",
    "tamayo , p. , huard , c. , gaasenbeek , m. , mesirov , j.p . ,",
    "coller , h. , loh , m.l . ,",
    "downing , j.r . , caligiuri , m.a . ,",
    "bloomfield , c.d . ,",
    "lander , e.s . , 1999 .",
    "molecular classification of cancer : class discovery and class prediction by gene expression monitoring .",
    "science 286 , 531 - 537 ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider asymptotic properties of the support vector machine ( svm ) in high - dimension , low - sample - size ( hdlss ) settings . </S>",
    "<S> we show that the hard - margin linear svm holds a consistency property in which misclassification rates tend to zero as the dimension goes to infinity under certain severe conditions . </S>",
    "<S> we show that the svm is very biased in hdlss settings and its performance is affected by the bias directly . in order to overcome such difficulties , </S>",
    "<S> we propose a bias - corrected svm ( bc - svm ) . </S>",
    "<S> we show that the bc - svm gives preferable performances in hdlss settings . </S>",
    "<S> we also discuss the svms in multiclass hdlss settings . </S>",
    "<S> finally , we check the performance of the classifiers in actual data analyses .    </S>",
    "<S> distance - based classifier , hdlss , imbalanced data , large @xmath0 small @xmath1 , multiclass classification primary 62h30 , secondary 62g20 </S>"
  ]
}