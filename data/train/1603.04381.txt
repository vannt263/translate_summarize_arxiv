{
  "article_text": [
    "in many applications such as complex system design or hyperparameter calibration for learning systems , the goal is to optimize some output value of a non - explicit function with as few evaluations as possible .",
    "indeed , in such contexts , one has access to the function values only through numerical evaluations by simulation or cross - validation with significant computational cost .",
    "moreover , the operational constraints generally impose a sequential exploration of the solution space with small samples .",
    "the generic problem of sequentially optimizing the output of an unknown and potentially _ non - convex _ function is often referred to as _ global optimization _ ( @xcite ) , black - box optimization ( @xcite ) or derivative - free optimization ( @xcite ) .",
    "there are several algorithms based on various heuristics which have been introduced in order to address complicated optimization problems with limited regularity assumptions , such as genetic algorithms , model - based algorithms , branch - and - bound methods ...  see @xcite for a recent overview .",
    "this paper follows the line of the approaches recently considered in the machine learning literature ( @xcite ) .",
    "these approaches extend the seminal work on lipschitz optimization of @xcite and they led to significant relaxations of the conditions required for convergence , _",
    "e.g. _  only the existence of a local _ smoothness _ around the optimum is required ( @xcite ) .",
    "more precisely , in the work of @xcite and @xcite , specific conditions have been identified to derive a finite - time analysis of the algorithms .",
    "however , these guarantees do not hold when the unknown function is not assumed to be locally smooth around ( one of ) its optimum .    in the present work ,",
    "we propose to explore concepts from ranking theory based on overlaying estimated level sets ( @xcite ) in order to develop global optimization algorithms that do not rely on the smoothness of the function .",
    "the idea behind this approach is simple : even if the unknown function presents arbitrary large variations , most of the information required to identify its optimum may be contained in its induced ranking rule , _",
    "i.e. _  how the level sets of the function are included one in another . to exploit this idea ,",
    "we introduce a novel optimization scheme where the complexity of the function is characterized by the underlying pairwise ranking which it defines .",
    "our contribution is twofold : first , we introduce two novel global optimization algorithms that learn the ranking rule induced by the unknown function with a sequential scheme , and second , we provide mathematical results in terms of statistical consistency and convergence to the optimum .",
    "moreover , the algorithms proposed lead to efficient implementation and display good performance on the classical benchmarks for global optimization as shown at the end of the paper .    the rest of the paper is organized as follows . in section [ sec : setup ]",
    "we introduce the framework and give the main definitions . in section [ sec : rankopt ] we introduce and analyze the rankopt algorithm which requires a prior information on the ranking structure underlying the unknown function . in section [ sec : adarank ] , an adaptive version of the algorithm is presented .",
    "companion results which establish the equivalence between learning algorithms and optimization procedures are discussed in section [ sec : equivalence ] as they support implementation choices . finally , the adaptive version of the algorithm is compared to other global optimization algorithms in section [ sec : implementation ] .",
    "all proofs are postponed to the appendix section .",
    "* setup . *",
    "we consider the problem of sequentially maximizing an unknown real - valued function @xmath3 where @xmath1 is a compact and convex set . the objective is to identify some point @xmath4 with a minimal amount of function evaluations .",
    "the setup we consider is the following : at each iteration @xmath5 , an algorithm selects an evaluation point @xmath6 which depends on the previous evaluations @xmath7 and receives the evaluation of the unknown function @xmath8 at this point .",
    "after @xmath9 iterations , the algorithm returns the argument of the highest value observed so far : @xmath10 the analysis provided in the paper considers that the number @xmath9 of evaluation points is not fixed and it is assumed that function evaluations are noiseless .",
    "* notations . * for any @xmath11 , we define the standard @xmath12-norm as @xmath13 , we denote by @xmath14 the corresponding inner product and we denote by @xmath15 the @xmath12-ball centered in @xmath16 of radius @xmath17 . for any bounded set @xmath1 , we define its inner - radius as @xmath18 , its diameter as @xmath19 and we denote by @xmath20 its volume where @xmath21 stands for the lebesgue measure .",
    "we denote by @xmath22 the set of continuous functions defined on @xmath23 taking values in @xmath24 and we denote by @xmath25 the set of ( multivariate ) polynomial functions of degree @xmath26 defined on @xmath23 .",
    "finally , we denote by @xmath27 the uniform distribution over a bounded measurable domain @xmath28 and we denote by @xmath29 the indicator function taking values in @xmath30 .      in this section",
    ", we introduce the ranking structure as a complexity characterization for a general real - valued function to be optimized .",
    "first , we observe that real - valued functions induce an order relation over the input space @xmath23 , and the underlying ordering induces a ranking rule which records pairwise comparisons between evaluation points .",
    "( induced ranking rule ) the ranking rule @xmath31 induced by a function @xmath3 is defined by : @xmath32 for all @xmath33 .",
    "the key argument of the paper is that the optimization of any weakly regular real - valued function only depends on the nested structure of its level sets .",
    "hence there is an equivalence class of real - valued functions that share the same induced ranking rule as shown by the following proposition .",
    "( ranking rule equivalence ) [ prop : rankingequivalence ] let @xmath34 be any continuous function .",
    "then , a function @xmath35 shares the same induced ranking rule with @xmath36 , i.e. ,  @xmath37 , @xmath38 if and only if there exists a strictly increasing ( not necessary continuous ) function @xmath39 such that @xmath40 .    [ pics : sameranking ]      proposition [ prop : rankingequivalence ] states that even if the unknown function @xmath0 admits non - continuous or large variations , up to a transformation @xmath42 , there might exist a simpler function @xmath43 that shares the same induced ranking rule .",
    "figure [ pics : sameranking ] gives an example of three functions that share the same ranking while they display highly different regularity properties . as a second example",
    ", we may consider the problem of maximizing the function @xmath44 if @xmath45 and @xmath46 otherwise over @xmath47 $ ] . in this case , the function @xmath0 is not _",
    "smooth _ around its unique global maximizer @xmath48 but shares the same induced ranking rule with @xmath49 over @xmath23 .",
    "+ we can now introduce a complexity characterization of real - valued functions of a set @xmath23 through the complexity class of its induced ranking rule .",
    "we call this class a ranking structure .",
    "( continuous ranking structure and continuous ranking rules ) let @xmath0 be a real - valued function .",
    "we say that @xmath0 has a continuous ranking rule if @xmath50 where @xmath51 denotes the set of continuous ranking rules ( i.e.  ranking rules induced by continuous functions ) .    in the continuation of this definition , we further introduce two examples of more stringent ranking structures .",
    "( polynomial ranking rules ) the set of polynomial ranking rules of degree @xmath52 is defined as @xmath53    we point out that even a polynomial function of degree @xmath54 may admit a lower degree polynomial ranking rule .",
    "for example , consider the polynomial function @xmath55 .",
    "since @xmath56 where @xmath57 is a strictly increasing function , the ranking rule induced by @xmath0 is a polynomial ranking rule of degree @xmath58 .    the second class of ranking structures we introduce is a class of non - parametric ranking rules .",
    "( convex ranking rules ) the set of convex ranking rules of degree @xmath59 is defined as @xmath60    it is easy to see that the ranking rule of a function @xmath0 is a convex ranking rule of degree @xmath54 if and only all the level sets of the function @xmath0 are unions of at most @xmath54 convex sets .",
    "we now state two conditions that will be used in the theoretical analysis : the first condition is about the identifiability of the maximum of the function and the second is about the regularity of @xmath0 around its maximum .",
    "( identifiability ) [ cond : id ] the maximum of a function @xmath35 is said to be identifiable if for any @xmath61 arbitrary small , @xmath62    condition [ cond : id ] prevents the function from having a jump on its maximum .",
    "it will be useful to state asymptotic results of the type @xmath63 when @xmath64 .",
    "( regularity of the level sets ) [ cond : levelset ] a function @xmath35 has @xmath65-regular level sets for some @xmath66 , @xmath67 if :    1 .",
    "the global optimizer @xmath68 is unique 2 .   for any @xmath69 @xmath70 ,",
    "the iso - level set @xmath71 satisfies @xmath72    condition [ cond : levelset ] guarantees that the points associated with high evaluations are close to the unique optimizer with respect to the euclidean distance .",
    "however , note that for any iso - level set @xmath73 with finite distance to the optimum , the condition is satisfied with @xmath74 and @xmath75 .",
    "hence , this condition concerns the local behavior of the level sets when @xmath76 . as an example , the iso - level sets of three simple functions satisfying the condition with different values of @xmath77 are shown in figure [ pics : lvlset ] .",
    "in this section , we consider the problem of optimizing an unknown function @xmath0 given the prior knowledge that its ranking @xmath79 belongs to a given ranking structure @xmath80 .",
    "[ fig : rankopt ]    * 1 .",
    "initialization : * let @xmath81 +    evaluate @xmath82 , @xmath83 + @xmath84 , @xmath85 + * 2 .",
    "iterations : * repeat while @xmath86 : + let @xmath87 + if there exists @xmath88 such that @xmath89   + evaluate @xmath8 , @xmath90 + @xmath91 + @xmath92 + * 3",
    ". output : * return @xmath93      the input of the rankopt algorithm ( displayed in figure [ fig : rankopt ] ) are a number @xmath9 of iterations , the unknown function @xmath0 , a compact and convex set @xmath1 and a ranking structure @xmath80 . at each iteration @xmath94 , a point @xmath95",
    "is uniformly sampled over @xmath23 and the algorithm decides , whether or not , to evaluate the function at this point .",
    "the decision rule involves the active subset of @xmath96 which contains the ranking rules that are consistent with the ranking rule induced by @xmath0 over the points sampled so far .",
    "we thus set @xmath97 where @xmath98 is the empirical ranking loss : @xmath99 as a direct consequence of the definition of the active subset , if there does not exist any ranking rule @xmath88 satisfying @xmath100 , it also implies that @xmath101 which means that @xmath102 .",
    "thus , the algorithm never evaluates the function at a point that will not return certainly an evaluation at least equal to the highest evaluation @xmath103 observed so far .",
    "( approximation of the level sets ) it is noteworthy that the sampling area implicitly defined by the decision rule @xmath104 can be written in terms of unions of level sets , _",
    "@xmath105 thus , since @xmath79 might be any ranking rule in @xmath106 , the sampling area @xmath107 is also the smallest subset that contains certainly the _ true _ level set of the highest evaluation observed so far @xmath103 .",
    "( connection with active learning ) the proposed algorithm might be seen as an extension to ranking of the baseline _ active learning _",
    "algorithm introduced in @xcite and further analyzed in @xcite .",
    "however , in active learning we aim at estimating a binary classifier @xmath108 where the goal in global optimization is to estimate the winner of a tournament deriving from the ranking rule @xmath109 .",
    "( adaptation to noisy evaluations ) [ rem : noisy ] following the steps of @xcite and @xcite , the algorithm can be extended to noisy evaluations by considering a relaxed version of the active subset @xmath110 where @xmath111 comes out of some standard generalization bound on @xmath112 .",
    "we now state some convergence properties of the rankopt algorithm .",
    "the results are stated in a probabilistic framework .",
    "the source of randomness comes from the algorithm itself  which generates uniform random variables  and not from the evaluations which are assumed noiseless .",
    "the next result will be important in order to formulate the consistency property of the algorithm .",
    "[ th : fasterprs ] fix any @xmath113 , let @xmath1 be any compact and convex set , let @xmath80 be any ranking structure and let @xmath3 be any function such that @xmath114 .",
    "then , if @xmath93 denotes the random output of the rankopt@xmath115 algorithm , we have that @xmath116 , @xmath117 where @xmath118 are @xmath9 independent random variables , uniformly distributed over @xmath23 .",
    "combining the previous proposition with the identifiability condition gives the following asymptotic result .",
    "( consistency ) [ coro : consistencyrank ] consider the same notations and assumptions as in proposition [ th : fasterprs ] .",
    "then , if the function @xmath0 has an identifiable maximum ( condition [ cond : id ] ) , we have that @xmath119    we now provide our first finite - sample bound on the distance @xmath120 between the exact solution and its approximation .",
    "( upper bound ) [ coro : upperbound ] consider the same assumptions as in proposition [ th : fasterprs ] .",
    "then , if function @xmath0 has @xmath65-regular level sets ( condition [ cond : levelset ] ) , for any @xmath113 and any @xmath121 , with probability at least @xmath122 , @xmath123 where @xmath124 .",
    "more surprisingly , a lower bound can also be derived by connecting the rankopt algorithm with a theoretical algorithm which uses the prior knowledge of the level sets of the unknown function  pure adaptive search ( @xcite ) .",
    "the next proposition shows the link between the two algorithms .",
    "[ th : slowerpas ] fix any @xmath113 and let @xmath125 be a sequence of @xmath9 random variables distributed as the markov process defined by : @xmath126 where @xmath127 denotes the level set of @xmath128 .",
    "then , under the same assumptions as in proposition [ th : fasterprs ] , we have that @xmath116 , @xmath129    equipped with proposition [ th : slowerpas ] , we are ready to establish our second finite - time bound .",
    "( lower bound ) [ coro : lowerbound ] consider the same assumptions as in proposition [ th : fasterprs ] .",
    "then , if the function @xmath0 has @xmath65-regular level sets ( condition [ cond : levelset ] ) , for any @xmath121 and any @xmath113 , with probability at least @xmath122 , @xmath130 where @xmath131 .",
    "( on the performance criterion ) observe that the level set assumption  used in theorem [ coro : upperbound ] and theorem [ coro : lowerbound ]  is invariant to any strictly increasing composition @xmath42 ( i.e. ,  if @xmath0 has @xmath65-regular level sets , so has @xmath132 ) .",
    "it implies that the bounds derived on the distance @xmath133 hold independently of the smoothness of the unkown function .",
    "we now consider the problem of optimizing a function @xmath0 when no information is available on its induced ranking rule @xmath79 .      the adarankopt algorithm ( shown in figure [ fig : adarank ] ) is an extension of the rankopt algorithm which involves model selection following the principle of structural risk minimization @xcite .",
    "we consider a parameter @xmath134 and a nested sequence of ranking structures @xmath135 satisfying @xmath136    [ fig : adarank ]    let @xmath81 + evaluate @xmath82 , @xmath137 + @xmath138 , @xmath85 + repeat while @xmath86 + let @xmath139 + if @xmath140 + let @xmath141 + if @xmath142 + let @xmath143 + evaluate @xmath8 , @xmath90 + @xmath92 + @xmath144 + @xmath145 + return @xmath93    the algorithm is initialized by evaluating the function at a point @xmath146 uniformly distributed over @xmath23 and by considering the smallest ranking structure @xmath147 of the sequence . at each iteration @xmath86 , a bernoulli random variable @xmath148 of parameter @xmath149 is sampled .",
    "if @xmath140 , the algorithm _ explores _ the space by evaluating the function at a point uniformly sampled over @xmath23 .",
    "if @xmath142 , the algorithm _ exploits _ the previous evaluations by making an iteration of the rankopt algorithm with the smallest ranking structure @xmath150 of the sequence that probably contains the true ranking @xmath79 . once a new evaluation @xmath8 has been made , the index @xmath151 is updated .",
    "the parameter @xmath149 thus drives the trade - off between the exploitation phase and the exploration phase  which prevents the algorithm from getting stuck in a local maximum .",
    "we start by casting the consistency result for the algorithm .",
    "( consistency ) [ prop : cons_adarank ] fix any @xmath134 and let @xmath152 be any nested sequence of ranking structures ( [ eq : nested ] ) .",
    "then , for any function @xmath35 which has an identifiable maximum ( condition [ cond : id ] ) , if @xmath93 denotes the random output of the adarankopt @xmath153 algorithm , we have that @xmath119    informally , proposition [ prop : cons_adarank ] states that even if the algorithm is poorly tuned ( _ e.g. , _ an inappropriate sequence of ranking structures or a bad choice of @xmath149 ) , it remains consistent over the class of functions that have an identifiable maximum .",
    "+ to derive a finite - time analysis of the algorithm , we start to investigate the number of iterations required to identify a ranking structure that contains the _ true _ ranking rule .",
    "( stopping time ) let @xmath154 be the index of the smallest ranking structure that contains the true ranking rule and let @xmath155 be the sequence of random variables driving the model selection , defined in the algorithm .",
    "we define the stopping time which corresponds to the number of iterations required to identify the index @xmath156 as @xmath157    in order to bound @xmath158 , we need to control the complexity of the sequence of ranking structures .",
    "let us denote by @xmath159 the true ranking loss where @xmath160 is a couple of independent random variables uniformly distributed over @xmath23 and define the rademacher average of a ranking structure @xmath96 as @xmath161 where @xmath162 are @xmath9 independent copies of @xmath163 and @xmath164 are @xmath165 independent rademacher random variables ( _ i.e. , _  random symmetric sign variables ) , independent of @xmath166 .",
    "( stopping time upper bound ) [ prop : model ] assume that the index @xmath167 is finite , assume that @xmath168 and assume that there exists a constant @xmath169 such that @xmath170 , the rademacher complexity of @xmath171 satisfies @xmath172 .",
    "then , for any @xmath121 , with probability at least @xmath122 , @xmath173    in the situation described above , a ranking structure which contains the true ranking rule can be idenified in a finite number of iterations .",
    "it is then possible to recover an upper bound similar to the one of theorem [ coro : upperbound ]  where the structure structure is assumed to be known .",
    "( upper bound ) [ coro : upper_ada ] consider the same assumptions as in proposition [ prop : model ] and assume further that the function @xmath0 has @xmath65-regular level sets ( condition [ cond : levelset ] ) .",
    "then , if @xmath93 denotes the random output of adarankopt@xmath174 , for any @xmath121 and any @xmath175 , we have that @xmath176 with probability at least @xmath177 where @xmath178 is the same constant as in theorem [ coro : upperbound ] .",
    "( on the complexity assumption ) as pointed out in @xcite ( remark 2 therein ) , standard vc - type arguments can be used in order to bound @xmath179 .",
    "if the set of functions @xmath180 is a vc major class with finite vc dimension @xmath181 , then @xmath182 for a universal constant @xmath183 . in particular , this covers the case of polynomial ranking rules .",
    "( related work ) to the best of our knowledge , this is the first analysis of an optimization algorithm which learn the ranking rule induced by the unknown function .",
    "however , for a different approach to global optimization , we refer to the work of @xcite where the function is assumed to be locally smooth around ( one of ) its optimum . in the latter work , finite - time bounds on the difference @xmath184",
    "are obtained when the smoothness is known ( doo ) and the smoothness is unknown ( soo ) .",
    "we discuss here some technical aspects involved in the practical implementation of the adarankopt algorithm .",
    "fix any nested sequence of ranking structures @xmath135 and suppose that we have collected a sample @xmath185 of @xmath9 evaluations we address the questions of ( i ) sampling @xmath186 uniformly over the non - trivial subset @xmath187 and ( ii ) updating the index @xmath188 once @xmath189 has been evaluated .",
    "we start to show that both these steps can be done using a single generic procedure that tests , for a given @xmath190 , if there exists a ranking rule @xmath191 that perfectly ranks a sample of @xmath192 evaluations , _",
    "i.e. , _ @xmath193",
    "\\(i ) sampling @xmath163 until @xmath194 allows to sample uniformly over @xmath195 . by definition of the non - trivial subset ,",
    "we know that @xmath194 if and only if there exists a ranking rule @xmath196 such that @xmath197 or @xmath198 . rewriting the previous statement in terms of minimal error",
    "gives that @xmath194 if and only if :    * either @xmath199 where the empirical ranking loss is taken over the sample @xmath200 ( case @xmath201 ) , * or @xmath199 where @xmath202 is taken over the sample @xmath203 where @xmath183 is any strictly positive constant ( case @xmath204 ) .",
    "\\(ii ) assume now that @xmath189 has been evaluated .",
    "since @xmath205 forms a nested sequence of ranking structures , it implies that the sequence @xmath206 is stricly increasing so we have that @xmath207 where the empirical loss is taken over the sample @xmath208 .",
    "hence , @xmath188 can be updated by sequentially testing if @xmath209 for @xmath210 + as stated above , for any nested sequence of ranking structures , both the steps ( i ) and ( ii ) can be done using a generic procedure that tests if ( [ eq : test ] ) holds true .",
    "we now provide some equivalences that can be used in order to design such a procedure for the two classes of ranking structures introduced in section [ sec : setup ] . for simplicity , we will assume in the sequel that all the evaluations of the sample are distinct , _",
    "@xmath211 where @xmath212 denote the indexes of the corresponding reordering .",
    "consider the sequence of polynomial ranking rules @xmath213 and let @xmath214 be the function that maps @xmath215 into the corresponding polynomial feature space of degree @xmath54 where @xmath216 , _",
    "we provide here some equivalences that can be exploited in order to implement the algorithm with the polynomial rankings . as a starting point , we start to show that ( [ eq : test ] ) holds true if and only if the set of points @xmath218 satisfies some property .    [ prop : binary ] fix any degree @xmath219 and assume that the @xmath192 evaluations of the sample are distinct ( [ eq : sample_ranked ] ) .",
    "then , there exists a polynomial ranking rule of degree @xmath54 that perfectly ranks the sample if and only if there exists an axis @xmath220 satisfying : @xmath221    since only the _ existence _ of a separating axis is required , one can then use the following lemma , presented in the usual binary classification framework and illustrated in figure [ pics : separability ] .    ( separability ) [ lem : zero ] let @xmath222 be any set of binary classification where @xmath223 , let ch@xmath224 be the convex hull of @xmath225 and let us denote by @xmath226 the zero vector .",
    "then , there exists a separating axis @xmath227 satisfying : @xmath228 if and only if @xmath229    thus , applying lemma [ lem : zero ] on the sample @xmath230 and using the vertices representation of the associated convex hull leads us to the next equivalence .",
    "[ coro : lp ] under the same assumptions as in proposition [ prop : binary ] , there exists a polynomial ranking rule of degree @xmath54 that perfectly ranks the sample if and only if the polyhedral set defined by @xmath231 is empty where @xmath232 is the @xmath233-matrix with its @xmath234-th column equal to @xmath235 and @xmath236 stands for the inequality @xmath237 component - wise @xmath238i.e .",
    ",  @xmath239 ) .",
    "notice that , in practice , the problem of testing the emptiness of a polyhedral set admits a tractable solution , as discussed in remark [ rem : algo ] .",
    "[ pics : separability ]        consider the non - parametric sequence of convex ranking rules @xmath241 .",
    "we provide here two equivalences related to the practical implementation of the algorithm with the convex rankings .",
    "first , following the steps of @xcite allows to formulate the next equivalence .",
    "( overlaying classifiers ) [ prop : binary_cvx ] fix any degree @xmath219 , let @xmath242 and assume that all the evaluations are distinct .",
    "then , ( [ eq : test ] ) holds true if and only if there a exists a sequence of classifiers @xmath243 of the form @xmath244 satisfying :    1 .",
    "@xmath245 , @xmath246 , 2 .",
    "@xmath247 .",
    "the problem of overlaying classifiers admits a tractable solution when @xmath242 . in the specific case where @xmath248 and @xmath249 , testing the existence of nested convex classifiers is equivalent to testing the emptiness of a cascade of polyhedral sets , as shown in the next proposition .",
    "[ prop : lp_cvx ] fix any @xmath250 , let @xmath249 and assume that all the evaluations of the sample are distinct",
    ". then , ( [ eq : test ] ) holds true if and only if for each @xmath251 the polyhedral set defined by : @xmath252 is empty where @xmath253 is the @xmath254-matrix where its @xmath234-th column is equal to @xmath255 .",
    "( algorithmic aspects ) [ rem : algo ] testing the emptiness of a polyhedral set can be seen as the problem of finding a feasible point of a linear program . in particular",
    ", this problem can be solved using the simplex algorithm .",
    "for further details , we refer to chapter 11.4 in @xcite where practical examples as well as algorithmic solutions are discussed .",
    "the purpose of this section is to show that the main algorithm of the paper is empirically competitive with existing state - of - the - art global optimization algorithms .",
    "to do so , we compare the _ general _ performances of the adarankopt algotihm with four different algorithms , developed from various approaches of global optimization :    * bayesopt ( @xcite ) is a bayesian optimization algorithm .",
    "it uses a distribution over functions to build a surrogate model of the unkown function .",
    "the parameters controlling the distribution are estimated during the optimization process . *",
    "cma - es ( @xcite ) is an evolutionary algorithm . at each iteration ,",
    "the new evaluation points are sampled according to a multivariate normal distribution .",
    "the covariance matrix of the distribution is estimated over the previous evaluations . *",
    "crs ( @xcite ) is a controlled random search with local mutations .",
    "it starts with a random population and randomly evolves these points by an heuristic rule .",
    "the algorithm was taken from the nlopt library ( @xcite ) . *",
    "direct ( @xcite ) is a lipschitz optimization algorithm where the lipschitz constant is unknown .",
    "it uses a deterministic splitting technique of the search space .",
    "the algorithm was also taken from the nlopt library .    for a fair comparison ,",
    "the tuning parameters were all set to default and the adarankopt algorithm was used with the polynomial ranking rules and a parameter @xmath149 fixed to @xmath256 . +    [ fig : comparison ]      in the first series of experiments ( displayed in figure [ fig : comparison ] ) , four standard optimization problems were considered and the the horizon @xmath9 was set to @xmath258 evaluations . for each problem ,",
    "the task consists in maximizing :    * the mccormick function @xmath259 over the domain @xmath260 \\times [ -3 , 4]$ ] which presents two local maxima . * a perturbed version of the styblinski - tang function @xmath261 which has four local maxima over @xmath262 ^ 2 $ ] . * the levi n.13 function @xmath263 over the domain @xmath264 ^ 2 $ ] .",
    "the levi function has a strong global structure but presents about 100 local maxima . * the himmelblau function @xmath265 over @xmath266 ^ 2 $ ] .",
    "this function has four global maxima .",
    "[ fig : comparison2 ]      in the second series of experiments ( displayed in figure [ fig : comparison2 ] ) , the horizon @xmath9 was set to @xmath267 evaluations and the algorithms were compared on four specific problems .",
    "the objectives consist in maximizing :    * the one - dimensional function @xmath268 over @xmath269 $ ] where @xmath270 .",
    "this function has 17 local maxima and presents a large discontinuity around its unique optimum @xmath271 . for this problem ,",
    "the convex rankings were temporally used . * the hlder table function @xmath272 over the domain @xmath273 ^ 2 $ ] .",
    "this function has a very weak global structure and presents about 36 local maxima . *",
    "the griewank function @xmath274 over the domain @xmath275 ^ 4 $ ] .",
    "the griewank function has many widespread and regulary distributed local minima . * the function @xmath276 over @xmath277^{10}$ ] .",
    "a special feature of this function is that it is maximized over the whole hyperplane @xmath278 .    in most cases ,",
    "we remark that the adarankopt converges fast and avoids falling in local maxima , see for instance * ( a ) * , * ( b ) * , * ( d ) * and * ( e)*. moreover , experiment * ( h ) * also suggests that the algorithm is robust against the dimensionality of the input space when the ranking structure of the unkown function is regular enough .",
    "these performances can be explained by the ability of the algorithm to learn the ranking structure of the unkown function , which allows to adapt to a wide variety of functions .",
    "however , experiments * ( c ) * and * ( g ) * highlight some limits of the algorithm . indeed , when the unkown function presents a strong global structure but has a large amount of local mimima , the algorithm succefully manage to learn the _ global _ ranking structure but fails at locally optimizing the function .",
    "an approach to solve this problem would be to extend the algorithm to noisy evaluations , as stated in remark [ rem : noisy ] .",
    "we have provided a global optimization strategy based on a sequential estimation of the ranking of the unknown function .",
    "we introduced two algorithms : rankopt which requires a prior knowledge of the ranking rule of the unknown function and its adaptive version adarankopt which performs model selection .",
    "a theoretical analysis is provided and the adaptive algorithm is shown to be empirically competitive with the state - of - the - art methods on synthetic optimization problems .",
    "* proof of proposition [ prop : rankingequivalence ] * @xmath279 assume that there exists a strictly increasing function @xmath39 such that @xmath40 and let @xmath280 be the standard sign function defined by @xmath281 . since @xmath42 is a strictly increasing function , @xmath282 , @xmath283     + @xmath284 assume that @xmath285 , @xmath38 . if @xmath282 , @xmath286 , then @xmath287 and @xmath288 are constant over @xmath23 and we have that @xmath289 where @xmath290 is a strictly increasing function .",
    "assume that @xmath0 is not constant over @xmath23 and introduce the function @xmath291 $ ] defined by : @xmath292 we start to show that there exists a strictly increasing function @xmath39 such that @xmath293 . to properly define the function @xmath42 ,",
    "we show by contradiction that @xmath294 the function @xmath0 is constant over the iso - level set @xmath295 . fix any @xmath296 , pick any @xmath297 and assume , without loss of generality , that @xmath298 .",
    "the equality of the rankings implies that ( i ) @xmath299 , @xmath300 and ( ii ) @xmath301 . putting ( i ) and ( ii ) altogether and using the continuity of @xmath36 leads to the contradiction @xmath302 then , denoting by @xmath303 the unique value of the function @xmath0 over @xmath304",
    ", we can now introduce the function @xmath305 defined by @xmath306 since @xmath307 , @xmath308 , the continuity of @xmath36 implies that @xmath309 , @xmath310 .",
    "therefore , @xmath293 where @xmath39 is any strictly increasing extension of the function @xmath311 over @xmath24 . reproducing the same steps",
    ", one can show that there exists a strictly increasing function @xmath312 such that @xmath313 .",
    "hence @xmath314 where @xmath315 is a strictly increasing function .",
    "@xmath316        ( adarankopt process ) [ prop : adarankopt ]",
    "let @xmath1 be any compact and convex set , let @xmath522 be any sequence of nested ranking structures , fix any @xmath134 and let @xmath35 be any function such that @xmath50 .",
    "then , the adarankopt@xmath317 @xmath0 , @xmath23 , @xmath149 , @xmath523 algorithm evaluates the function on a sequence of random variables @xmath524 defined by : @xmath525 where @xmath526 , @xmath148 is a bernoulli random variable of parameter @xmath149 independent of @xmath527 and @xmath323 and @xmath528 are as defined as in the algorithm .",
    "[ prop : rankconcentration ] ( from @xcite ) let @xmath524 be a sequence of @xmath9 independent copies of @xmath163 , let @xmath96 be any ranking structure , let @xmath35 be any real - valued function and let @xmath529 be the empricial ranking loss taken over @xmath530 . then , denoting by @xmath531 the rademacher average of @xmath96 and by @xmath532 the true ranking loss defined in section [ sec : adarank ] , for any @xmath121 , with probability at least @xmath122 , @xmath533        * proof of proposition [ prop : cons_adarank ] * fix any @xmath534 and let @xmath535 be the corresponding level set .",
    "we show by induction that @xmath170 , @xmath536 since @xmath81 and @xmath537 , we have that @xmath538 and the result holds when @xmath325 .",
    "assume that the statement holds for a given @xmath113 and let @xmath442 be the sequence of @xmath192 random variables defined in proposition [ prop : adarankopt ] .",
    "conditioning on @xmath539 gives that @xmath540 .",
    "\\end{aligned}\\ ] ] since @xmath541 ( see proposition[prop : adarankopt ] ) , we have that @xmath542 therefore , ( [ eq : consada ] ) is proved by plugging the induction assumption into the previous equation .",
    "we finally get that @xmath543 using the fact that @xmath134 and that @xmath544 ( condition [ cond : id ] ) .",
    "@xmath316     + * proof of proposition [ prop : model ] * fix any @xmath121 , let @xmath545 where @xmath546 is the integer part of the upper bound and let @xmath547 be the sequence of random variables defined in proposition [ prop : adarankopt ] . then , denoting by @xmath548 the empirical ranking loss taken over the first @xmath549 samples @xmath550 and since @xmath135 forms a nested sequence , we have that @xmath551 .",
    "hence @xmath552 we start to lower bound the empirical risk by only keeping the first @xmath553 ( i.i.d . )",
    "explorative samples : @xmath554 where @xmath555 .",
    "conditioning on @xmath556 , we know that @xmath557 is a sequence of @xmath556 independent random variables , uniformly distributed over @xmath23 ( see proposition[prop : adarankopt ] ) .",
    "therefore , on the event @xmath558 , the right hand term of inequality ( [ eq : model1 ] ) has the same distribution as @xmath559 where @xmath560 is a sequence of @xmath553 independent copies of @xmath163 , also independent of @xmath561 .",
    "hence @xmath562 where the empirical ranking loss is taken over @xmath560 .",
    "for any @xmath563 we have that @xmath564 and so , by proposition [ prop : concentration ] , we have with probability at least @xmath565 that @xmath566 hence , noticing that the right hand term of the previous inequality is stricly positive , due to the definition of @xmath553 , gives that @xmath567 .",
    "finally , by hoeffding s inequality we have that @xmath568 and we deduce that @xmath569 . @xmath316     + * proof of theorem [ coro : upper_ada ] * fix any @xmath121 , let @xmath570 be the integer part of the upper bound of proposition [ prop : model ] ( with probability @xmath571 ) and let @xmath572 be the upper bound of the theorem [ coro : upperbound ] ( with probability @xmath571 ) .",
    "now , fix any @xmath573 and let @xmath574 be the sequence of random variables defined in proposition [ prop : adarankopt ] .",
    "then , denoting @xmath575 , applying the bayes rules gives that @xmath576 due to the definition of @xmath577 , we have that @xmath578 by proposition [ prop : model ] .",
    "moreover , on the event @xmath579 , the true ranking structure @xmath580 is identified for any @xmath581 .",
    "therefore , the distance @xmath582 can be bounded using the last @xmath583 samples with a similar technique as the one used in the proof of theorem [ coro : upperbound ] ( where the ranking structure is assumed to be known ) and we get that @xmath584 . finally , noticing that @xmath585 ends the proof .",
    "@xmath316                pick any @xmath593 . by definition of the @xmath590-ball",
    ", there exists @xmath594 such that @xmath595 and @xmath596 ( resp .",
    "@xmath597 where @xmath598 and @xmath599 ) .",
    "the convexity of @xmath23 implies that @xmath600 , @xmath601 hence @xmath602 and we deduce that @xmath603 is a convex set .    [ lem : equi ] let @xmath80 be any continuous ranking structure and let @xmath604 be any sample satisfying @xmath605 .",
    "then , if @xmath202 denotes the empirical ranking loss taken over the sample , we have the following equivalence : @xmath606      @xmath612",
    "let @xmath613 be any ranking rule satisfying @xmath614 . since @xmath615 is a continuous ranking , there exists a function @xmath34 such that @xmath616 , @xmath37 .",
    "pick any @xmath617 and assume , without loss of generality , that @xmath618 . using the function @xmath36 , we have that @xmath619 hence , @xmath620 , @xmath621 and so @xmath622 .",
    "* proof of proposition [ prop : binary ] * @xmath623 assume that there exists @xmath624 satisfying @xmath622 . since @xmath397 is a polynomial ranking , there exists a polynomial function @xmath625 where @xmath626 such that @xmath285 , @xmath627 . since @xmath622 , applying lemma [ lem : equi ] gives that @xmath628 , @xmath629 hence , there exists @xmath630 s.t .",
    "@xmath631 , @xmath632 .",
    "+ @xmath633 assume that there exists @xmath220 s.t .",
    "@xmath634 , @xmath635 .",
    "pick any constant @xmath636 , define the polynomial function @xmath637 and let @xmath638 be its induced polynomial ranking rule . then , @xmath635 , we have that @xmath639 and applying lemma [ lem : equi ] gives that @xmath640 . @xmath316",
    "+ * proof of lemma [ lem : zero ] * note that @xmath641 .",
    "therefore , one can assume without loss of generality that @xmath642 , @xmath643 by replacing @xmath644 by @xmath645 .",
    "+ @xmath623 assume that there exists @xmath227 such that @xmath635 , @xmath646 and assume by contradiction that @xmath647 . since @xmath647 , we know by proposition [ def : cvx ] that there exists @xmath648 such that @xmath649 , @xmath650 and @xmath651 , @xmath652 and it gives the contradiction @xmath653    @xmath654 assume that @xmath655",
    ". since @xmath9 and @xmath656 are finite , @xmath587 is a closed , compact and convex set and @xmath657 exists and the condition @xmath658 implies that @xmath659 . now , let @xmath660 be the ( unique ) point of the convex hull satisfying @xmath661 .",
    "we show by contradiction that @xmath662 , @xmath663 .",
    "assume that there exists @xmath664 s.t .",
    "the convexity of the convex hull implies that the whole line @xmath666 also belongs to the convex hull .",
    "however , since @xmath667 and @xmath668 , the line @xmath669 is not tangent to the ball @xmath670 and intersects it .",
    "hence , there exists @xmath671 s.t .",
    "since @xmath673 belongs to the convex hull , it leads us to the contradiction @xmath674 we deduce that @xmath662 , @xmath675 .",
    "finally , since @xmath676 , there exists @xmath677 such that @xmath678 , @xmath679 . @xmath316",
    "+ * proof of corollary [ coro : lp ] * combining proposition [ prop : binary ] and lemma [ lem : zero ] gives the next equivalences : @xmath680 by proposition [ def : cvx ] , we know that @xmath681 if and only if there exists @xmath682 such that @xmath683 , @xmath650 and @xmath684 , @xmath685 .",
    "putting those constraints into matricial form gives the result .",
    "@xmath316     + * proof of proposition [ prop : binary_cvx ] * @xmath623 assume that there exists @xmath686 such that @xmath687 and let @xmath688 be the sequence of classifiers defined by @xmath689 . by lemma [ lem : equi ] , we know that @xmath635 , @xmath690 and so @xmath691 . on the other hand , by definition of the convex rankings of degree @xmath54 , we know that all the classifiers are of the form @xmath692 .",
    "+ @xmath654assume that there exists a sequence of classifiers @xmath693 satisfying : ( i ) @xmath694 , ( ii ) @xmath695 and ( iii ) @xmath696 , @xmath697 .",
    "define the non - continuous function @xmath698 and observe that @xmath699 , @xmath700 and so @xmath687 .",
    "now , let @xmath701 be an approximation of the function @xmath702 where @xmath703\\\\      1 - \\frac{x - u}{\\epsilon } & \\ \\ \\",
    "\\text{if}\\ \\",
    "\\ x \\in ] u , u + \\epsilon ] \\\\      0 & \\ \\ \\",
    "\\text{otherwise}.      \\end{cases}\\ ] ] first , by continuity of @xmath704 we know that @xmath705 is continuous .",
    "second , note that @xmath706 and @xmath707 , we have that @xmath708 and so @xmath709 . finally , using the same decomposition , it is easy to see that @xmath710 , the level sets of @xmath705 are unions of at most @xmath54 segements ( convex sets ) . hence , there exists @xmath711 satisfying @xmath622 .",
    "@xmath316        now , fix any @xmath719 and observe that the set @xmath720 is a convex set by convexity of the ranking rule @xmath397 .",
    "however , since @xmath721 is the smallest convex set that contains @xmath722 ( see proposition [ def : cvx ] ) , we necessarily have that @xmath723 .",
    "therefore , putting the previous statements altogether gives that @xmath724 , @xmath725 by proposition [ def : cvx ] , it implies that @xmath724 , there does not exist any @xmath726 such that @xmath727 , @xmath728 and @xmath684 .",
    "putting those constraints into matricial form gives the result .",
    "+ @xmath654 assume that all the polyhedrons are empty .",
    "reproducing the same ( inverse ) steps as in the first equivalence gives that @xmath729 , @xmath730 and so @xmath731 now , define the non - continuous function @xmath732 , observe that @xmath733 and let @xmath734 be an approximation of the function @xmath0 , where @xmath735 , @xmath736 if @xmath737 and @xmath738 otherwise .",
    "first , note that for any convex set @xmath23 , the function @xmath739 is continuous and so is the function @xmath705 .",
    "second , note that @xmath740 and @xmath741 , we have that @xmath708 and so @xmath742 . finally , since the @xmath590-ball @xmath603 of any convex set @xmath23 is also a convex set ( see lemma [ lem : epsiball ] ) , @xmath307 and @xmath743 , the level set @xmath744 is a convex set .",
    "hence , there exists @xmath745 satisfying @xmath746 . @xmath316",
    "( rankopt process ) [ prop : chain ] let @xmath1 be any compact and convex set , let @xmath80 be any ranking structure and let @xmath3 be any function such that @xmath114 . then , the rankopt@xmath317 @xmath318 @xmath319 @xmath320 algorithm evaluates the function @xmath0 on a random sequence @xmath162 defined by : @xmath321 where @xmath322 denotes the sampling area and @xmath323 and @xmath106 are defined as in the algorithm .",
    "the result is proved by induction . since @xmath324 ,",
    "the result trivially holds when @xmath325 .",
    "assume that the satement holds for a given @xmath113 .",
    "by definition of the algorithm and using the induction assumption , we know that the rankopt@xmath326 algorithm evaluates the function on a sequence of random variables @xmath327 where @xmath328 rankopt@xmath329 , @xmath330 and @xmath331 is a sequence of independant random variables uniformly distributed over @xmath23 and independant of @xmath162 . therefore , for any borelian @xmath332 , @xmath333 and the result is proved by induction .",
    "noticing that @xmath107 is a subset of @xmath23 gives the first inclusion . to state the second inclusion ,",
    "pick any @xmath337 and observe that @xmath338 . since @xmath79 always perfectly ranks the sample ( _ i.e. , _",
    "@xmath339 ) , there exists @xmath340 such that @xmath341 and we deduce that @xmath342 .    [ def : pas ] ( pure adaptive search process , from @xcite ) .",
    "let @xmath1 be any compact and convex set and let @xmath35 be any function such that @xmath50 .",
    "we say that the sequence @xmath343 is distributed as a pure adaptive search process pas@xmath344 if it has the same distribution as the markov process defined by : @xmath345 where @xmath346 is the level set of the highest value observed so far .",
    "[ lem : fasteruniform ] let @xmath347 be a sequence of @xmath9 random variables distributed as a pas@xmath344 process .",
    "then , for any @xmath348 $ ] , we have that @xmath349 where @xmath350 are @xmath9 independent copies of @xmath351)$ ] and @xmath352 denotes the sampling area of the pas@xmath344 process at time @xmath9 .",
    "note that if @xmath353 the result trivially holds for any @xmath354 and any @xmath113 .",
    "therefore , we assume without loss of generality that @xmath355 and we set some additional notations : @xmath356 $ ] define @xmath357 and let @xmath358 be the corresponding level set . observing that @xmath356 $ ] , @xmath359 , we are ready to prove the lemma by induction .",
    "+ fix any @xmath348 $ ] and let @xmath360)$ ] be a random variable independent of @xmath361 .",
    "since @xmath362 and @xmath363 , we have that @xmath364 and the result holds when @xmath325 .",
    "assume that the statement holds for a given @xmath113 , fix any @xmath348 $ ] and let @xmath365 be a sequence of @xmath192 random variables distributed as a pas@xmath326 process . since @xmath366 ( definition [ def : pas ] ) , conditioning on @xmath367 gives that @xmath368.\\ ] ] since the level sets form a nested sequence , we have the following equivalences : ( i ) @xmath369 and ( ii ) @xmath370 .",
    "hence @xmath371 and so @xmath372.\\ ] ] now , let @xmath373)$ ] be a random variable independent of @xmath374 . since @xmath375 is uniformly distributed over @xmath376 $ ] and independent of @xmath377 , we have that @xmath378 therefore , @xmath379",
    "= \\p \\left (   u_{n+1 } \\cdot \\frac{\\mu(\\x^{\\star}_n)}{\\mu(\\x ) }       \\leq \\frac{\\mu(\\x_{y_u})}{\\mu(\\x ) }    \\right).\\ ] ] finally , using the fact that @xmath359 and plugging the induction assumption into the previous equation gives ( by independence ) that : @xmath380 where @xmath381 are @xmath192 independent copies of @xmath351)$ ] .      taking the logarithm on both sides",
    "gives that @xmath385 @xmath386 @xmath387 . since @xmath388)$ ] , we have that @xmath389 and so @xmath390 gamma@xmath391 .",
    "the result follows from the application of concentration results of sub - gamma random variables ( see chapter 2.4 in @xcite ) .",
    "[ lem : inclusion ] let @xmath1 be any compact and convex set and let @xmath392 be any function that has ( @xmath393)-regular level sets ( condition [ cond : levelset ] ) .",
    "then , for any @xmath394 where @xmath271 denotes the unique optimizer of the function @xmath0 over @xmath23 , we have that @xmath395 where @xmath396 denotes the intersection of @xmath23 with the @xmath12-sphere centered in @xmath271 of radius @xmath397 .    to prove the second inclusion , we first show that @xmath398 $ ] , there exists @xmath399 which satisfies @xmath400 . fix any @xmath401 $ ] ,",
    "pick any @xmath402 and introduce the function @xmath403 \\rightarrow \\r$ ] defined by @xmath404 which returns the value of @xmath0 over the segment @xmath405 $ ] . note that the function is continuous and well - defined due to the convexity of @xmath23 and the continuity of @xmath0 .",
    "since @xmath406 , @xmath407 and @xmath408 $ ] , applying the intermediate value theorem gives us that there exists @xmath409 $ ] such that @xmath410 .",
    "hence , there exists @xmath411 such that @xmath412 .",
    "assume now that there exists @xmath413 such that @xmath414 .",
    "as a direct consequence , it implies that @xmath415 .",
    "however , we also have that @xmath416 and so putting altogether the previous inequalities with the level set assumption leads us to the next contradiction : @xmath417 the contradiction holds for any @xmath418 $ ] .",
    "we deduce that @xmath419 which proves the second inclusion .",
    "+ we use a similar technique to prove the first inclusion .",
    "assume that there exists @xmath420 s.t .",
    "@xmath421 . by introducing the function @xmath422",
    ", one can show that there exists @xmath423 s.t .",
    "hence , @xmath425 . on the other hand ,",
    "it leads to a similar contradiction and we deduce that @xmath427 .",
    "introduce the similarity transformation @xmath431 defined by : @xmath432 and let @xmath433 be the image of @xmath23 by the similarity transformation . by definition",
    "we have that @xmath434 .",
    "hence , the convexity of @xmath23 implies that @xmath435 and we have that @xmath436 .",
    "now , since @xmath437 is a similarity transformation ( and conserves the ratios of the volumes before / after transformation ) we have that @xmath438 noticing that @xmath439 , @xmath440 where @xmath441 stands for the standard gamma function gives the result .",
    "* proof of proposition [ th : fasterprs ] * the statement is proved by induction . since @xmath81 ,",
    "the result trivially holds when @xmath325 .",
    "assume that the statement holds for a given @xmath113 and let @xmath442 be a sequence of @xmath192 random variables distributed as a rankopt@xmath443 process . since the result trivially holds @xmath444 , fix any @xmath445 $ ] and let @xmath446 be the corresponding level set .",
    "applying proposition [ prop : chain ] gives that @xmath447    we start to bound the second term . since @xmath448 ( see proposition [ prop : chain ] ) ,",
    "conditioning on @xmath449 gives that @xmath450 .",
    "\\end{aligned}\\ ] ] lemma [ lem : rep ] gives the following inclusion of events : @xmath451 .",
    "hence @xmath452 plugging ( [ eq : pop12 ] ) into ( [ eq : prop11 ] ) and noticing that @xmath453 leads to the next inequality : @xmath454    now , let @xmath455 be a sequence of @xmath192 independent copies of @xmath163 . reproducing the same steps as previously and using the fact that @xmath456 gives that @xmath457 finally , plugging the induction assumption into ( [ eq : prop13 ] ) and comparing the result with ( [ eq : prop14 ] ) yields to the desired result . @xmath316     + * proof of corollary [ coro : consistencyrank ] * pick any @xmath458 and let @xmath459 be the corresponding level set .",
    "proposition [ th : fasterprs ] gives that @xmath170 , @xmath460 where @xmath461 are @xmath9 independent copies of @xmath462 .",
    "therefore , we have that @xmath463 since @xmath464 by condition [ cond : id ] , @xmath465 @xmath316      * proof of theorem [ coro : upperbound ] * since @xmath466 is a continuous ranking , there exists @xmath34 such that @xmath37 , @xmath467 ( see proposition [ prop : rankingequivalence ] ) .",
    "since all the arguments only use function comparisons , one can assume without loss generality that @xmath468 .",
    "we also set some additional notations : let @xmath469 be the upper bound of the theorem , let @xmath470 and let @xmath471 @xmath472 @xmath473 @xmath474 @xmath475 . since the result trivially holds when @xmath469 @xmath237 @xmath476 , assume that @xmath477  which also implies by the level set assumption that @xmath478 . using the second inclusion of lemma [ lem : inclusion ] and proposition [ th : fasterprs ]",
    "gives that @xmath479 where @xmath480 are @xmath9 independent copies of @xmath163 and @xmath481 is defined in lemma [ lem : inclusion ] . since the random variables @xmath482 are independent and uniformly distributed over @xmath23 , applying the first inclusion of lemma [ lem : inclusion ] gives that @xmath483 lemma [ lem : zab ] gives that @xmath484 .",
    "therefore @xmath485 finally , using the fact that @xmath486 , @xmath487 yields to the desired result .",
    "@xmath316     + * proof of proposition [ th : slowerpas ] * the statement is proved by induction . since @xmath146 and @xmath488 are both uniformly distributed over @xmath23 , the result holds when @xmath325 . assume that the statement holds for a given @xmath113 and let @xmath442 be a sequence of @xmath192 random variables distributed as the rankopt@xmath489 process . since the result trivially holds @xmath490 , fix any @xmath491 $ ] and let @xmath492 be the corresponding level set .",
    "then , denoting by @xmath195 the sampling area of @xmath186 and by @xmath493 the level set of the highest value observed so far , reproducing the same steps as in the proof of proposition [ th : fasterprs ] gives that @xmath494 .    \\end{aligned}\\ ] ] lemma [ lem : rep ] gives the following inclusion @xmath495 .",
    "hence @xmath496 .",
    "\\end{aligned}\\ ] ] for any random variable @xmath497 $ ] , we have that @xmath498 and so @xmath499 now , let @xmath500 be a sequence of @xmath192 random variables distributed as a pas@xmath501 process . reproducing the same steps as previously gives that @xmath502      = \\frac{\\mu(\\x_y ) } { \\mu(\\x ) } +       \\int_{\\frac{\\mu(\\x_y ) } { \\mu(\\x ) } } ^1      \\p\\left ( \\mu(\\x_{f(x^{\\star}_{n } ) } ) <   \\frac{\\mu(\\x_y)}{t }   \\right)~dt.\\ ] ] finally , by the induction assumption @xmath503 , we have that @xmath504 where @xmath505 and the proof is complete by plugging ( [ eq : slowerpas3 ] ) into ( [ eq : slowerpas1 ] ) and by comparing the result with ( [ eq : slowerpas2 ] ) .",
    "@xmath316     + * proof of theorem [ coro : lowerbound ] * as mentioned in the proof of theorem [ coro : upperbound ] , since @xmath466 , there exists @xmath506 such that @xmath507 and one can assume without loss of generality that @xmath468 .",
    "we also set some additional notations : let @xmath508 be the lower bound of the theorem , let @xmath509 and let @xmath510 . using the first inclusion of lemma [ lem : inclusion ] and proposition [ th : slowerpas ]",
    "gives that @xmath511 where @xmath343 is a sequence of @xmath9 random variables distributed as the pas@xmath329 process . applying the second inclusion of lemma [ lem : inclusion ] and denoting by @xmath512",
    "the level set of the highest value observed so far gives that @xmath513 since @xmath514 , there exists @xmath428 such that @xmath515 which implies that @xmath516",
    ". therefore @xmath517 by definition of @xmath518 , we have that @xmath519 and so , applying lemma [ lem : fasteruniform ] gives that @xmath520 where @xmath350 are @xmath9 independent copies of @xmath351)$ ] .",
    "we finally get that @xmath521 by lemma [ prop : concentration ] ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider the problem of maximizing an _ unknown _ function @xmath0 over a compact and convex set @xmath1 using as few observations @xmath2 as possible . </S>",
    "<S> we observe that the optimization of the function @xmath0 essentially relies on learning the induced bipartite ranking rule of @xmath0 . </S>",
    "<S> based on this idea , we relate global optimization to bipartite ranking which allows to address problems with high dimensional input space , as well as cases of functions with weak regularity properties . </S>",
    "<S> the paper introduces novel meta - algorithms for global optimization which rely on the choice of any bipartite ranking method . </S>",
    "<S> theoretical properties are provided as well as convergence guarantees and equivalences between various optimization methods are obtained as a by - product . </S>",
    "<S> eventually , numerical evidence is given to show that the main algorithm of the paper which adapts empirically to the underlying ranking structure essentially outperforms existing state - of - the - art global optimization algorithms in typical benchmarks .    </S>",
    "<S> global optimization , ranking , statistical analysis , convergence rate bounds </S>"
  ]
}