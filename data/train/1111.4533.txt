{
  "article_text": [
    "there is a continuous and rapid global growth in data storage needs .",
    "archival and backup storage form a specific niche of importance to both businesses and individuals .",
    "a recent market analysis from idc stated that the global revenue of the data archival business is expected to reach $ 6.5 billion in 2015 .",
    "the necessity to cost - effectively scale - up data backup systems to meet this ever growing storage demand poses a challenge to storage systems designers .",
    "when a large volume of data is involved , deploying a networked distributed storage system becomes essential , since a single storage node can not scale .",
    "furthermore , distribution provides opportunities for fault tolerance and parallelized i / o . examples of such distributed storage systems are readily found in datacenter environments , including distributed file systems such as gfs  @xcite or hdfs  @xcite , distributed key - value stores like dynamo  @xcite or cassandra  @xcite , for storing huge volume of scientific or multimedia content @xcite as well as in ad - hoc end user resource based peer - to - peer ( p2p ) settings such as oceanstore  @xcite and friend - to - friend ( f2f ) storage systems  @xcite , a special kind of peer - to - peer systems often considered particularly suitable for personal data backup .",
    "an important design aspect in distributed storage systems is redundancy management .",
    "data replication provides a simple way to achieve high fault - tolerance , while erasure codes such as reed - solomon codes  @xcite are more sophisticated alternatives , capable of significantly reducing the data storage footprint for different levels of fault - tolerance  @xcite .",
    "various trade - offs in adopting erasure codes in storage systems , such as storage overhead & fault - tolerance , access frequency & decoding overheads , but also due to the need of replenishment of lost redundancy , repair bandwidth & repair time after failures , have been studied in the literature , revealing in particular that erasure codes are particularly suited for backup and archival storage , where data access is infrequent , and hence the effects of decoding are marginal .",
    "consequently , many modern data center oriented storage and file systems such as microsoft s azure @xcite , hdfs @xcite and the next generation of google s file system ( colossus ) have incorporated erasure codes to enhance the systems storage efficiency . however , a relatively unexplored aspect of the usage of erasure codes in storage systems both in data center as well as peer - to - peer environments is that of the time required for inserting the data ( along with the necessary redundancy for fault - tolerance and durability ) .    when using replication , a source node aiming to store new data can upload one replica of this data to the first storage node , which can concurrently forward the same data to a second storage node , and so on . by applying such a pipelining mechanism ,",
    "the load for redundancy insertion can be shared , and the source node does not need to upload any redundant information itself .",
    "replication thus naturally supports `` in - network '' generation of redundancy , that is , generation of new redundancy within the network , through data exchange among storage nodes , which in turn leads to fast insertion of data .",
    "in contrast , in erasure encoded systems , the source node is the only one responsible for computing and uploading all the encoded redundant data to the corresponding storage nodes .",
    "this has traditionally been the case , because typically the coding process is centralized in nature , and hence all encoded data fragments are generated at a single node , which then also bears the load of inserting the encoded fragments at other storage nodes .",
    "the amount of data the source node uploads is then considerably larger , including the data object and its corresponding redundancy , resulting in lower data insertion throughput .",
    "insertion throughput may further be exacerbated when the source node and the set of storage nodes have additional ( mismatched ) temporal constraints on resources availability , in which case in - network redundancy generation can provide partial mitigation .",
    "we elaborate the effect of temporal resource ( un)availability issues with two distinct example scenarios , which we also use later in our experiments to determine how ( much ) in - network redundancy generation may improve the data insertion throughput :    a.   in datacenters , storage nodes might be used for computation processes which require efficient access to local disks .",
    "since backup processes consume large amounts of local disk i / o , system administrators might want to avoid backup transfers while nodes are executing i / o intensive tasks  e.g. , mapreduce tasks .",
    "b.   in peer - to - peer settings , users exchange some of their spare disk resources in order to realize a collaborative data backup service .",
    "such resource sharing may furthermore be driven by other constraints such as trust or friendship .",
    "however , desirable users may be online at different times of the day , complicating the data insertion process .    in both cases",
    "the insertion of new redundancy by the source node is restricted to the periods when the availability windows of the source overlap that of the storage nodes .    unlike replication , where in - network redundancy generation is achieved trivially ,",
    "traditional erasure codes are not easily amenable .",
    "our solution is based on new classes of erasure codes designed for better repair efficiency , obtained through local repairability , that is the property of being able to repair a failure by contacting only a small number of live nodes ( e.g. @xcite ) . in particular , to provide a concrete instance that will be used later for experimentation , we will focus on a novel family of erasure codes called self - repairing codes ( src )  @xcite , whose salient property is that the encoded data stored at each node can be easily regenerated by using information from typically only two live storage nodes .",
    "as we will show , local repairability is the key property to achieve in - network redundancy generation .",
    "however , src have strict constraints on how storage nodes can combine their data to generate content for other nodes , which , along with the temporal availability constraints of nodes , complicate the design of efficient in - network redundancy generation .",
    "in this paper we provide an analytical framework to define valid transfer schedules for the in - network redundancy generation using src .",
    "we then identify the main requirements that valid scheduling algorithms must satisfy , and show that determining optimal schedules is computationally intractable .",
    "accordingly , we explore several heuristic algorithm implementations which aim at maximizing the utilization of the spare resources of the storage nodes , thus improving the backup throughput .",
    "we determine the efficacy of our approach in different environments by using google data center availability and workload traces , and availability traces from real friend - to - friend ( f2f )  @xcite and peer - to - peer ( p2p ) applications  @xcite .",
    "our results show that for data center traces the algorithms proposed for the in - network redundancy generation can increase the throughput of the storage process up to 90% as compared to classical naive storage approaches , while only requiring 50% more of the network resources , but using it only when it is otherwise unused . for",
    "p2p / f2f traces the throughout can increase up to 60% while requiring 38% additional ( but again , spare ) network resources .",
    "the main contributions of this paper can be summarized as follows :    a.   we introduce the concept of _ in - network redundancy generation _ for reducing data insertion latency in erasure code based storage systems , and demonstrate its feasibility using one specific instance of locally repairable code , namely self - repairing codes .",
    "b.   we define an _ analytical framework _ to explore valid data transfer schedules where the in - network redundancy generation process maximizes the use of the available network resources .",
    "c.   we show that besides requiring a node availability prediction , determining the optimal data insertion schedule is computationally intractable .",
    "d.   we propose a set of _ heuristics _ for efficient in - network redundancy generation .",
    "e.   we determine the efficacy of in - network redundancy generation in diverse distributed storage environments using real workload and availability trace driven simulations .",
    "the rest of the paper is organized as follows . in section  [",
    "s : background ] we provide the background on erasure codes and self - repairing codes , a particular instance of locally repairable codes . in section  [ s : redgen ]",
    "we define our in - network redundancy generation process and in section  [ s : scheduling ] we show how to schedule the redundancy generation to speedup the insertion of new data .",
    "due to the complexity of determining optimal schedules , we propose in section  [ s : algorithm ] several heuristic scheduling algorithms , which are evaluated in section  [ s : eval ] using real availability traces . finally in section  [ s : conclusions ] we state our conclusions and further research directions .",
    "the p2p research community has long studied the applicability of erasure codes in low availability environments with limited storage capacity  @xcite .",
    "the growing interest in applying erasure codes in data centers is more recent , and aims at reducing the storage costs  @xcite .",
    "it further triggered a line of research around repairability of storage systems , i.e. , how lost redundancy can be replenished , which includes the application of network coding @xcite to carry out the repair process in a decentralized manner , designing novel codes with inherent local repairability @xcite , as well as engineering solutions such as applying multiple levels of encoding on the same @xcite or even across different data objects @xcite .    while fault - tolerance , storage overhead , repairability , but also i / o and bandwidth are well recognized critical bottlenecks for the storage of huge amounts of data , existing literature does not explore yet how data insertion can be optimized in the context of erasure codes based storage .",
    "this work leverages on one of these recent approaches on distributed storage systems repairability , namely , novel codes with local repairability , in order to improve the very process of data insertion .    in the following ,",
    "we provide some background on erasure codes as classically used for distributed storage , as well as on one specific instance of locally repairable codes , self - repairing codes ( src ) , which we use in the rest of the paper to demonstrate the feasibility and quantify the benefits of in - network redundancy generation in erasure code based distributed storage systems .",
    "a classical @xmath0 erasure code allows to redundantly encode an object of size @xmath1 into @xmath2 redundant fragments of size @xmath3 , each to be stored in a different storage node . the data storage overhead ( or redundancy factor )",
    "is then given by @xmath4 , and the stored object can be reconstructed by downloading an amount of data equal to @xmath1 , from @xmath5 or more different nodes out of @xmath2 .",
    "one of the main drawbacks of using classical erasure codes for storage is that redundant fragments can only be generated by applying coding operations on the original data .",
    "the generation of new redundancy is then restricted to nodes that possess the original object ( or a copy ) , namely : the source node , storage nodes that previously reconstructed the original object , or possibly were storing a copy ( as is the case in a hybrid model where a full copy of the object is kept , together with encoded fragments ) . when the original raw object is not available , repairing a single node failure consequently entails downloading an amount of information equivalent to the size of the original object , causing a significant communication overhead .    in order to mitigate this communication overhead ,",
    "a new family of erasure codes called regenerating codes ( see e.g. @xcite and reference therein ) was recently designed by adopting ideas from network coding  @xcite , a popular mechanism deployed to improve the throughput utilization of a given network topology .",
    "the main advantage of regenerating codes is that new redundant fragments can be generated by downloading an amount of data @xmath6 from @xmath7 other redundant fragments , where @xmath8 , and @xmath9 . unlike in classical erasure codes",
    ", regenerating codes can thus repair missing fragments by downloading only an amount of data equal to @xmath10 , where usually @xmath11 .",
    "however , the maximum communication savings occur for large values of @xmath7 , in which cases however , the chance to find @xmath7 available nodes might be very low , limiting the practicality of such codes .      self - repairing codes ( src )  @xcite are a new family of erasure codes designed to minimize the maintenance overhead by reducing the number of nodes @xmath7 required to be contacted to recreate lost fragments . a specific family of src , named homomorphic self - repairing codes ( hsrc ) , has the property that two encoded fragments can be xored for such a regeneration , i.e. @xmath12 , as long as not more than half of the nodes have failed .",
    "this locally repairable property makes hsrc suitable for the in - network redundancy generation since partial redundant data stored in two different nodes can be used to generate data for a third node , without requiring the intervention of the source node .",
    "however , as we will show below , the pairs of nodes used for that purpose can not be arbitrary chosen .",
    "let us recall briefly the construction of hsrc .",
    "we denote finite fields by @xmath13 .",
    "the cardinality of @xmath13 is given by its index , that is , @xmath14 is the binary field with two elements ( the two bits 0 and 1 ) , and @xmath15 is the finite field with @xmath16 elements . if @xmath17 , for some positive integer @xmath18 , we can fix a @xmath19 of @xmath15 and represent an element @xmath20 using an @xmath18-dimensional vector @xmath21 where @xmath22 , @xmath23 .",
    "let @xmath24 be the object to be stored over a set of @xmath2 nodes , which is represented as a data vector of size @xmath25 bits , @xmath26 , i.e. : @xmath27 given these @xmath5 original elements , the @xmath2 redundant fragments are obtained by evaluating the polynomial    @xmath28 \\label{e : poly}\\ ] ]    in @xmath2 non - zero values @xmath29 of @xmath30 , yielding the redundant vector @xmath31 of size @xmath32 bits , i.e. : @xmath33 in particular we need the code parameters @xmath0 to satisfy @xmath34",
    "the main important property of hsrc is its homomorphic property . from  @xcite",
    "we have that :    let @xmath35 and let @xmath36 be the polynomial defined in ( [ e : poly ] ) , then @xmath37 .",
    "[ l : homo ]    this implies that we can generate a redundant element @xmath38 from @xmath39 and @xmath40 if and only if @xmath41 .",
    "this homomorphic property is one way of obtaining local repair by contacting only @xmath12 nodes .",
    "[ ex : k3n7_1 ] consider a @xmath42 hsrc and an object @xmath43 of size @xmath44 bits , where @xmath45 , @xmath46 .",
    "we write @xmath47 , @xmath48 , @xmath49 , from which we compute @xmath50 .",
    "we evaluate @xmath36 in @xmath51 values of @xmath52 , represented in vector form as @xmath53 , @xmath54 , @xmath55 , @xmath56 , @xmath57 , @xmath58 , @xmath59 , yielding : @xmath60 with corresponding redundant vector @xmath61    we can check that @xmath62 which illustrates the local repairability of the code . note that we have not used the vector @xmath63 here , which would have resulted in a longer code @xmath64 .",
    "we now discuss how hsrc operate in two different scenarios : ( i ) when the source introduces data in the system , and ( ii ) during the in - network redundancy generation .",
    "the homomorphic property described in lemma  [ l : homo ] has been introduced to repair node failures , though it can similarly serve to generate redundancy from the source .",
    "recall from lemma  [ l : homo ] that @xmath65 , where both @xmath66 can be seen as @xmath18-dimensional binary vectors , by fixing a @xmath67-basis of @xmath68 .",
    "let us denote this basis by @xmath69 .",
    "thus @xmath70 can be written as @xmath71 , @xmath72 , and by virtue of the homomorphic property , we get that @xmath73 this means that the source only needs to compute @xmath74 for a given basis @xmath69 , after which all the other encoded fragments are obtained by xoring pairs of elements in @xmath75 .",
    "thus , when using an @xmath76 hsrc , the source computes @xmath5 ( @xmath26 ) encoded fragments @xmath77 , where @xmath78 are linearly independent , for example , @xmath79 , and then performs the corresponding xoring .",
    "the source then injects the @xmath2 encoded fragments in the network .",
    "[ ex : basis ] in example [ ex : k3n7_1 ] , we have @xmath80 , and a natural @xmath67-basis for @xmath52 is @xmath81 , @xmath82 , @xmath83 , @xmath84 .",
    "the source can generate redundancy by first computing @xmath85 , @xmath86 , @xmath87 , then @xmath88 , @xmath89 , @xmath90 and @xmath91 .",
    "the @xmath51 encoded fragments are then ready to be sent over the network .",
    "further notice that the set @xmath92 can be seen as a basis for the set of redundant fragments , since they are linearly independent , and can be combined to generate every redundant fragment .",
    "let us now consider the case where the source might not inject the whole set of @xmath2 encoded fragments , but only a subset @xmath93 of the encoded fragments .",
    "we use the triplet notation @xmath94 to represent the possibility to generate the element @xmath95 by xoring @xmath96 and @xmath97 , @xmath98 .",
    "note that due to the commutativity property of the additive operator , triplets @xmath94 and @xmath99 can be indistinguishably used to denote the same redundancy generation process .",
    "we denote by @xmath100 the set with all the feasible repair triplets from a set of @xmath2 redundant elements .",
    "finally , let us define the following two sets :    let @xmath101 be the set of all the possible @xmath94 triplets where fragment @xmath96 is used to generate some other fragment : @xmath102 [ d : outset ]    let @xmath103 be the set of all the possible @xmath94 triplets that can be used to create @xmath95 : @xmath104 [ d : inset ]    finally , given a number of redundant elements @xmath105 , for any positive integer @xmath106 , we have from  @xcite that : @xmath107 , we have that @xmath108      previously we detailed how to encode a data vector @xmath24 of size @xmath25 bits into a redundant vector @xmath31 of size @xmath32 bits .",
    "we showed that hsrc allow this encoding by using data from the source node as well as by using data from other storage nodes . in this subsection",
    "we describe one method to practically implement hsrc to encode larger data objects of size @xmath1 , where @xmath109 .    the first step to encode an object of size",
    "@xmath1 is to split it into @xmath110 vectors is multiple of @xmath25 , otherwise the object can be encoded by parts and/or zero - padded to meet this requirement . ] of size @xmath25 bits .",
    "let us represent the object to be encoded as @xmath111 .",
    "after the splitting process , @xmath112 , where @xmath113 , @xmath114 , @xmath115 .",
    "each of these vectors @xmath116 is individually encoded using the polynomial  ( [ e : poly ] ) to obtain an encoded vector @xmath117 , with @xmath118 , @xmath119 .",
    "finally , the vector @xmath120 with the @xmath2 fragments to be stored in the system is obtained by concatenating individual elements of @xmath121 so that @xmath122 , namely @xmath123 contains those coefficients @xmath124 of @xmath125 that have @xmath126 as second index .",
    "[ x : hsrc ] consider the @xmath42 hsrc and the object @xmath127 , where @xmath128 .",
    "we split the object into @xmath129 vectors @xmath130 , where @xmath131 , @xmath132 and @xmath133 . after encoding each of the individual vectors we obtain the set of redundant vectors @xmath134 ( @xmath116 is encoded to obtain @xmath135 ) , where @xmath136 .",
    "finally , we can obtain @xmath137 , and similarly for all the fragments .",
    "note that this encoding technique allows stream encoding .",
    "as soon as the source node receives the first @xmath25 bits to store , it can generate the vector @xmath138 , encode it to @xmath139 , and distribute @xmath140 to the @xmath2 storage nodes .",
    "similarly , when a storage node receives @xmath141 it can forward it to other nodes for in - network redundancy generation , for instance , when the source does not have adequate bandwidth to upload all the @xmath2 redundant fragments .    to implement computationally efficient codes one can set @xmath142 , or @xmath143 , for",
    "which addition can simply be done by xoring system words , and for which efficient arithmetic libraries are available  @xcite .    in the rest of this paper",
    "we will assume that hsrc are implemented using the method described here .",
    "we will use the term * _ redundant fragment _ * to refer to each of the redundant elements @xmath144 , i.e. , each node stores one redundant fragment . and",
    "similarly we will use the term * _ redundant chunk _ * to refer to each of the sub - elements @xmath145 stored in each node @xmath126 , i.e. , each node @xmath126 can store up to @xmath146 redundant chunks .",
    "in - network redundancy generation has the potential to speedup the insertion of new data in distributed storage systems .",
    "however , the magnitude of actual benefit depends on two factors : ( i ) the availability pattern of the source and storage nodes , which determines the achievable throughput , and ( ii ) the specific schedule of data transfer among nodes subject to the constraints of resource availability , which determines the actual achieved throughput for data backup . in this section",
    ", we explore the scheduling problem , demonstrating that finding an optimal schedule is computationally very expensive even with a few simplifying assumptions , and accordingly motivate some heuristics instead .",
    "let @xmath147 be a source node aiming to store a new data object to @xmath2 different storage nodes , and let @xmath126 , @xmath148 , represent each of these @xmath2 storage nodes .",
    "we model our system using discrete time steps of duration @xmath149 , where at each time step nodes can be available or unavailable to send / receive redundant data .",
    "the binary variable @xmath150 denotes this availability for each node @xmath126 for the corresponding time step @xmath151 . using this binary variable",
    "we can define the maximum amount of data that node @xmath126 can upload during time step @xmath151 _ upload capacity _ ) by @xmath152 where @xmath153 is the upload bandwidth of node @xmath126 during time step @xmath151 .",
    "similarly , the amount of data each node can download during time step  @xmath151 ( _ download capacity _ ) is given by @xmath154 where @xmath155 represents the download bandwidth of node @xmath126 during time step @xmath151 .",
    "then , we define the _ in - network redundancy generation network _ as a weighted temporal directed graph @xmath156 , @xmath157 , with the set of nodes @xmath158 , and the set of edges @xmath159 . the amount of data that nodes might send among themselves is a mapping @xmath160 , denoted by @xmath161 , @xmath162 , @xmath163 .",
    "hsrc characteristics constrain the mapping @xmath164 since nodes can only send or receive data trough valid redundancy creation triplets : @xmath165 furthermore , we assume ( for algorithmic simplicity ) that nodes send data through each of the redundancy generation triplets symmetrically : @xmath166 for ease of notation we will refer to the data sent through each of the redundancy generation triplets simply by @xmath167 .    similarly , because of the upload / download bandwidth constraints , the mapping @xmath164 must also satisfy the following constraints :    * the amount of data the source uploads is constrained by its upload capacity : @xmath168 * the amount of data storage nodes upload is also constrained by their upload capacity : @xmath169 * the amount of data storage nodes download is restricted by their download capacity : @xmath170    a _ bandwidth - valid in - network redundancy generation scheduling _ is any mapping @xmath164 on @xmath171 that satisfies the constraints defined in equations  ( [ e : c : triplets ] ) , ( [ e : c : symmetry ] ) , ( [ e : c : uploadsrc ] ) , ( [ e : c : uploadbw ] ) and ( [ e : c : downloadbw ] ) .",
    "let @xmath172 be the amount of data that node @xmath126 had received at the end of time step @xmath151 . for sufficiently large enough files and small values of @xmath18 ( e.g. @xmath142 ) , we can assume without loss of generality that @xmath173 corresponds to the index of the last _ redundant chunk _ received by node @xmath126 .",
    "let @xmath174 denote the size of the largest possible file that a schedule @xmath164 can store in @xmath175 time steps , which represents a data insertion throughput of @xmath176 . then , by definition of erasure codes , to consider that a file of size @xmath174 has been successfully stored after @xmath175 time steps , each node must receive an amount of data equal to @xmath177 .",
    "using this fact we can define @xmath174 as : @xmath178    for a given network @xmath171 and a duration @xmath175 , an in - network redundancy generation scheduling @xmath164 is then _ optimal _ if it maximizes @xmath174 .",
    "note that to maximize @xmath174 an optimal schedule will tend to even out the amount of data @xmath179 sent to each node , obtaining a maximum insertion throughput and a minimum consumption of network resources when @xmath180 . we can measure the overall network traffic required by any schedule @xmath164 , after @xmath175 time steps , namely @xmath181 , by : @xmath182 accordingly , we define an in - network redundancy generation schedule @xmath164 to be an _ optimal minimum - traffic _ schedule if besides maximizing @xmath174 , it also minimizes @xmath181 .    from ( [ e : innettraf ] ) we also want to note that in the total traffic required by the schedule @xmath164 , @xmath181 , the amount of redundant data each node @xmath126 receives comes from two distinct components ( the two summands within parentheses ) : @xmath183 and @xmath167 , where @xmath184 .",
    "the first component represents the redundant data inserted by the source while the second component represents the redundant data created through the in - network redundancy generation process . since in the last case",
    "there are two nodes ( i.e. , @xmath185 and @xmath186 ) uploading data to node @xmath126 the value has two be multiplied by two , leading to the following remark :    the new redundant data created through the in - network redundancy generation process requires twice the traffic required by the source redundancy generation .",
    "[ r : sourcetraffic ]    however , although the use of the in - network redundancy generation increases the required network traffic , it has the potential to increase the opportunities of redundancy generation , and hence , increase the data insertion throughput . in section  [ s :",
    "eval ] we will show that the relative increase of insertion throughput surpass the relative increase of network traffic , demonstrating the practicality of the in - network redundancy generation process and its scalability .          in this section we elaborate that while being a bandwidth - valid schedule is a necessary condition , it is not a sufficient condition for the schedule to be actually valid .",
    "for that , we will use example  [ x : hsrc ] , an in - network redundancy generation network using hsrc with parameters @xmath42 , where the redundant fragments @xmath187 have to be stored in nodes @xmath188 respectively .",
    "recall also that each redundant fragment @xmath189 is composed of 3 redundant chunks , hence @xmath190 .",
    "for ease of notation we will assume that each redundancy generation triplet @xmath191 , @xmath192 , satisfies the property @xmath193 where @xmath194 denotes the _ bitwise xor operation_. based on example  [ x : hsrc ] , we consider three different scheduling policies , all depicted in figure  [ f : onions ] .",
    "we assume that due to the limited upload capacity of the source node it can only upload three redundant fragments simultaneously .    in the first scenario , at time @xmath195 the source node sends to nodes 1 , 2 and 3 their first redundant chunk ; at time time",
    "@xmath196 it does the same for nodes 5 , 6 and 7 .",
    "note that if at time step @xmath196 the mapping @xmath164 tries to make use of the in - network redundancy generation triplets @xmath197 , @xmath198 and @xmath199 ; nodes 5 , 6 and 7 end up receiving the same redundant fragment twice . in this case",
    "the in - network redundancy traffic does not contribute in speeding up the backup process and only consumes communication resources . although avoiding this problem is implicit in the definition of a _ minimum - traffic _ scheduling , it needs to be explicitly considered during the scheduling .",
    "consider a second scheduling policy trying to solve the previous problem by sending to nodes 5 , 6 and 7 the second chunk instead of the first .",
    "it allows these nodes to receive two different fragments by time @xmath196 .",
    "however , it appears a circular dependency problem with triplets @xmath197 , @xmath198 and @xmath199 . to show this dependency ,",
    "imagine that we want to generate fragment @xmath200 using non - source data .",
    "note that @xmath200 requires of @xmath201 , and @xmath201 requires of @xmath202 , which at the same time requires of the fragment we aim to generate , @xmath200 .",
    "although it is a _ bandwidth - valid _ schedule , the circular dependency problem makes it an unfeasible schedule .",
    "finally , in the third case we see how the circular dependency problems can be avoided if the source sends uncorrelated fragments at each time step .",
    "it is easy to see from this example that a valid schedule needs to be not only bandwidth - valid , but also ensure that : ( i ) nodes do not receive duplicated data , and ( ii ) circular triplet dependencies are prevented .      we show that finding an optimal schedule satisfying all the previous requirements is computationally very expensive , even under further simplifying assumptions :    the amount of data that the source node @xmath147 sends during each time step @xmath151 to any storage node @xmath126 , @xmath183 , is a constant value and is not part of the optimization problem . [ a : simpleprob1 ]    storage nodes can only receive redundant chunks sequentially .",
    "it means that node @xmath126 will never receive chunk @xmath203 before previously receiving chunk @xmath204 .",
    "[ a : simpleprob2 ]    it is easy to see that the simplified problem subject to these two assumptions corresponds to a specific instance of the generic case described above .",
    "the interesting property about this simplified version of the problem is that we can reduce the decision of choosing the optimal schedule @xmath164 to an algorithm `` sortedvector '' which sorts @xmath100 , as it is shown in algorithm  [ a : algo ] .",
    "it is also easy to see , how due to the iterative use of redundancy generation triplets , algorithm  [ a : algo ] avoids both the `` duplicate data '' and the `` circular dependencies '' problems .",
    "however , since @xmath205 , it means that there are @xmath206 possible ways of sorting @xmath100 , and thus , @xmath207 different scheduling possibilities .",
    "thus , a brute force algorithm to determine the best schedule would have a @xmath208 cost .",
    "@xmath209 @xmath210 @xmath211 @xmath212 $ ] / * @xmath191 * / availdatabybw @xmath213 min(@xmath214 ) availdatabyindex @xmath213 min(@xmath215 availdatabyindex @xmath213 max(availdatabyindex ,  0 ) availdata = min(availdatabybw ,  availdatabyindex ) @xmath216 availdata @xmath217 availdata    .",
    "we assume in this example a hypothetical system where @xmath218 . ]",
    "if we focus on a single time step @xmath151 , then the scheduling problem can be restated as how to choose the best permutation of @xmath100 .",
    "we can represent this decision problem using a permutation tree as is depicted in figure  [ f : permtree ] .",
    "the weight of the edges in this permutation tree correspond to the negative amount that choosing each edge contributes to @xmath219 .",
    "choosing the best scheduling algorithm tis the same than finding the shortest path between vertices @xmath220 and @xmath7 in the permutation tree .",
    "the bellman - ford algorithm can find the shortest past with cost @xmath221 where @xmath222 and @xmath223 respectively represent the number of edges and vertices in the permutation tree .",
    "however , in our permutation tree the number of edges and vertices are both @xmath206 , which makes finding the optimal schedule for even the simplified problem computationally exorbitantly expensive , even for small number of nodes @xmath2 .",
    "hence , we consider the general problem described in  [ s : optimal ] to be also intractable .",
    "in this section we investigate several heuristics for scheduling the in - network redundancy generation .",
    "we split the scheduling problem into two parts , following the strategy presented in algorithm  [ a : algo ] .",
    "the heuristics do not require assumption  [ a : simpleprob1 ] , thus allowing the source node to send different amounts of data to each storage node .",
    "we however still rely on assumption  [ a : simpleprob2 ] , which allows us to model the decision problem with a sorting algorithm , as previously outlined in algorithm  [ a : algo ] .",
    "thus , the overall scheduling problem is decomposed into the following two decisions : ( i ) how does the source node schedule its uploads ? ( ii )",
    "how are redundancy generation triplets sorted ?      recall that generating redundancy directly from the source node involves less bandwidth than doing it with in - network techniques ( remark  [ r : sourcetraffic ] ) .",
    "thus , a good source traffic scheduling should aim at maximizing the source s upload capacity utilization .",
    "furthermore , the schedule must also try to ensure that the source injected data can be further used for the in - network redundancy generation .    given a @xmath76 hsrc , where @xmath224 , any subset of @xmath5 linearly independent encoded fragments",
    "forms a basis , denoted by @xmath225 ( see example [ ex : basis ] for an illustration ) .",
    "let @xmath226 be the set of all the possible bases @xmath225 .",
    "since each storage node stores one redundant fragment , we use @xmath227 to represent all the basis of @xmath226 whose corresponding storage nodes are available at a time step @xmath151 ( and likewise , refer to each combination of such nodes as an _ available basis _ ) : @xmath228    from the set of available basis , @xmath227 , the source node selects one basis @xmath225 and uploads some data to each node @xmath229 .",
    "the amount of data the source uploads to each node @xmath229 is set to guarantee that at the end of time step @xmath151 , all these nodes have received the same amount of data , @xmath230 . from equations  ( [ e :",
    "throughput ] ) and  ( [ e : innettraf ] ) we know that evening out the data all nodes receives allows to minimize network traffic and maximize insertion throughput .",
    "to even out the amount of data each node in basis receives and maximizing the utilization of the upload capacity of the source , the source needs to send to each node @xmath229 an amount of data equal to @xmath231    besides determining how to distribute the upload capacity of the source between nodes of a basis @xmath225 , the source node also needs to select the basis @xmath225 from all the available ones @xmath227 ( if more than one basis is available ) .",
    "we consider the following heuristic policies for the source node to select a specific basis @xmath225 :    * * random : * @xmath225 is randomly selected from @xmath232 . repeating this procedure for several time steps is expected to ensure that all nodes receive approximately the same amount of data from the source . *",
    "* minimum data : * the source selects the basis @xmath225 that on an average has received less redundant data .",
    "it means that @xmath225 is the basis that minimizes @xmath233 .",
    "this policy tries to homogenize the amount of data all nodes receive . *",
    "* maximum data : * the source selects the basis @xmath225 that on an average has received more redundant data .",
    "it means that @xmath225 is the basis that maximizes @xmath233 .",
    "this policy tries to have a basis of nodes with enough data to allow the in - network redundancy generation for the entire data object even when the source may not be available . * * no basis : * the source does not considers any basis and instead uploads data to all the online nodes .",
    "the upload bandwidth of the source is also distributed to guarantee that , after time step @xmath151 , all online nodes have received the same amount of data .      at each time",
    "step @xmath151 , once the source allocates its upload capacity to nodes from a specific available basis @xmath225 , the remaining upload / download capacity of the available nodes is used for in - network redundancy generation . for that purpose , the list of _ available triplets _ , @xmath234 ,",
    "is determined as follows : @xmath235 then , the set of available triplets @xmath234 is sorted , and the available upload / download capacity of storage nodes allocated according to this priority ( i.e. , the first available triplets have more preference ) .",
    "we consider the following sorting heuristics :    * * random : * repair triplets are randomly sorted .",
    "this policy tries to uniformly distribute the utilization of network resources to maximize the amount of in - network generated data . *",
    "* minimum data : * the list of available triplets are sorted in ascending order according to the amount of data @xmath236 the destination is the destination of a triplet @xmath237 , @xmath191 . ]",
    "node @xmath5 has received .",
    "this policy tries to prioritize the redundancy generation in those nodes that have received less redundant data . *",
    "* maximum data : * similarly to the _ minimum data _ policy , however , triplets are sorted in descending order .",
    "this policy tries to maximize the amount of data some specific subset of nodes receive , to allow them to sustain the redundancy generation process even when the source is not available . * * maximum flow : * the triplets are sorted in descending order according to the amount of redundant data these nodes can help generate .",
    "note that the amount of data a triplet @xmath237 can generate at each time step @xmath151 , where @xmath191 , is given by : @xmath238 this policy tries to maximize the amount of new redundancy generated per time step .",
    ".different policy combinations . [ cols=\"^,^,^\",options=\"header \" , ]     he have proposed four different policies for the source traffic scheduling problem and four policies for the triplets sorting problem . however , after an extensive experimental evaluation of all polices we will only report for each case the two best policies ( in terms of achieved throughput ) . at the source ,",
    "the _ random _ and _ minimum data _ policies consistently outperform the others , and at the storage nodes , the _ maximum flow _ and _ minimum data _ sorting policies for the triplets likewise outperform the others .",
    "we will refer to each of the combinations as denoted in table  [ t : policies ] .",
    "first , the interpretation of the good performance of the random policy in the source node is that the use of random bases favors the diversity of information among the nodes , which in turn enables more redundancy generation triplets .",
    "second , it is interesting to note that the _ minimum data _ policy obtains good storage throughput in both cases , which leads us to infer that _ in general , prioritizing redundancy generation in those nodes that have received less data _ is a good strategy to maximize the throughput of the backup process .",
    "we considered a @xmath42-hsrc , which is a code that can achieve a static data resiliency similar to a 3-way replication , but requiring only a redundancy factor of @xmath239.@xcite using this erasure code we simulated various backup processes with different node ( un)availability patterns for a fixed number of time steps @xmath175 .",
    "in all the simulated cases we consider three different metrics :    a.   the maximum amount of data that can be stored in @xmath175 time steps , @xmath240 . b.   the amount of data the source node uploads per unit of useful data backed up , @xmath241 c.   the total traffic generated per unit of useful data stored , @xmath242 .",
    "we evaluate the three metrics for a system using an in - network redundancy generation algorithm and we compare our results with a system using the naive erasure coding backup process , where the source uploads all the data directly to each storage node .",
    "our results depict the savings and gains , in percentage , of using an in - network redundancy algorithm with respect to the naive approach .    regarding the ( un)availability patterns of nodes and their bandwidth constraints we consider two different distributed storage cases :    a.   a p2p - like environment where we assume , to simplify simulations , that nodes have an upload bandwidth uniformly distributed between 20kbps and 200kbps , and an asymmetric download bandwidth equal to four times their upload bandwidth . nodes in this category follow two different availability traces from real decentralized application : ( i ) traces from users of an instant messaging ( i m ) service  @xcite and traces from p2p nodes in the amule kad dht overlay  @xcite . in both cases",
    "we filter the nodes that on average stay online more than 4 , 6 and 12 daily hours , obtaining different mean availability scenarios .",
    "b.   real availability traces collected from a google datacenter .",
    "the traces contain the normalized i / o load of more than 12,000 servers monitored for a period of one month .",
    "we consider that a server is available to upload / download data when its i / o load is under the @xmath243-percentile load .",
    "we consider three different percentiles , @xmath244 , giving us three different node availability constraints .    finally the time step duration is set to @xmath245hour and we obtain the results by averaging the results of 500 backup processes of @xmath246 time steps each ( 5 days ) .    before discussing the results",
    ", we will like to note that the experiments make a few simplifying assumptions .",
    "furthermore , real deployments have somewhat different workload characteristics than what have been considered above .",
    "hence , the quantitative results we report are only indicative ( and many more settings could possibly be experimented ) , and instead the specific choices help us showcase the potential benefits of our approach in only a qualitative manner .      [ [ storage - throughput . ] ] storage throughput .",
    "+ + + + + + + + + + + + + + + + + + +    in figure [ f : stored ] we show the increment of the data insertion throughput achieved by the in - network redundancy generation process .",
    "we can see how the gain is higher when nodes are more available for redundancy generation .",
    "this fact is a consequence of the constraint in eq .",
    "( [ e : c : symmetry ] ) requiring redundancy generation triplets to be symmetric , which requires the three involved nodes in each triplet to be available simultaneously .",
    "the higher the online availability , the higher the chances to find online three nodes from a triplet .",
    "further , we observe that the _ rndflw _ policy achieves significantly better results in comparison to other policies ; the second best policy is _",
    "minflw_. it is easy then to see that the _ maximum flow",
    "_ heuristic plays an important role on the overall redundancy generation throughput , which tries to maximize the use of those nodes that can potentially generate more redundancy .",
    "additionally , a _",
    "random _ source selection policy provides more benefits than the _ minimum data _ policy .",
    "[ [ network - traffic . ] ] network traffic .",
    "+ + + + + + + + + + + + + + + +    in figure  [ f : traffic ] we show the increment on the required network traffic of the in - network redundancy generation strategy as compared to the traditional redundancy generation .",
    "as noted previously ( in remark  [ r : sourcetraffic ] ) , the total traffic required for in - network redundancy generation can be up to twice the needed by the traditional process ( i.e. 100% traffic increment ) .",
    "however , since the in - network redundancy generation can not always take place due to the node availability constraints , the traffic increment is always below 100% .",
    "as it is expected then , the traffic increment is minimized when nodes are less available , in which case the source has to generate and introduce larger amounts of redundancy ( i.e. , less reduction in the data uploaded by source , as shown in figure  [ f : source ] ) .",
    "it is also important to note that the increase in traffic is approximately the same or even less than the increase in storage throughput even for low availability scenarios .",
    "thus the in - network redundancy generation scales well by achieving a better utilization of the available network resources than the classical storage process .",
    "[ [ data - uploaded - by - the - source . ] ] data uploaded by the source .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    in figure  [ f : source ] we show the reduction of data uploaded by the source . in the traditional approach , the source needs to upload @xmath239 times the size of the actual data to be stored ; @xmath247 of this data is redundant , however the in - network redundancy generation process allows to reduce the amount of data uploaded by the source . in this figure",
    "we can see how in the best case ( _ rndflw _ policy ) our approach reduces the source s load by 40% ( out of a possible 57% ) , yielding 40 - 60% increase in storage throughput",
    ".    finally , we want to note that the in - network redundancy performance requires finding three available nodes simultaneously , which becomes difficult on environments with fewer backup opportunities . to solve this problem , we would need to look at more sophisticated in - network redundancy generation strategies not subjected to the symmetric constraint ( defined in eq .",
    "( [ e : c : symmetry ] ) ) , so that nodes can forward and store partially - generated data .",
    "however , the scheduling problem will be much more complicated , and is beyond the reach of this first work .",
    "furthermore , in real traces , nodes will have correlation ( e.g. , based on batch jobs ) , which are missing in the synthetic traces , and such correlations can be leveraged in practice . exploring both",
    "these aspects will be part of our future work .",
    "in this work we propose and explore how storage nodes can collaborate among themselves to generate erasure encoded redundancy by leveraging novel erasure codes local - repairability property . doing so not only reduces a source node s load to insert erasure encoded data , but also significantly improves the overall throughput of the data insertion process .",
    "we demonstrate the idea using self - repairing codes .",
    "we show that determining an optimal schedule among nodes to carry out in - network redundancy generation subject to resource constraints of the system ( nodes and network ) is computationally prohibitive even under simplifying assumptions .",
    "however , experiments supported by real availability traces from a google data center , and p2p / f2f applications show that some heuristics we propose yield significant gain in storage throughput under these diverse settings , proving the practicality of not only the idea in general , but also that of the specific proposed heuristics .",
    "calder , b. , wang , j. , ogus , a. , nilakantan , n. , skjolsvold , a. , mckelvie , s. , xu , y. , srivastav , s. , wu , j. , simitci , h. , haridas , j. , uddaraju , c. , khatri , h. , edwards , a. , bedekar , v. , mainali , s. , abbasi , r. , agarwal , a. , haq , m.f.u . ,",
    "haq , m.i.u .",
    ", bhardwaj , d. , dayanand , s. , adusumilli , a. , mcnett , m. , sankaran , s. , manivannan , k. , and rigas , l. `` windows azure storage : a highly available cloud storage service with strong consistency . '' in _ acm symposium on operating systems principles ( sosp)_. 2011 .",
    "ford , d. , labelle , f. , popovici , f.i .",
    ", stokely , m. , truong , v.a . , barroso , l. , grimes , c. , and quinlan , s. `` availability in globally distributed storage systems . '' in _ usenix conference on operating systems design and implementation ( osdi)_. 2010 .",
    "hastorun , d. , jampani , m. , kakulapati , g. , pilchin , a. , sivasubramanian , s. , vosshall , p. , and vogels , w. `` dynamo : amazon s highly available key - value store . '' in _",
    "symposium on operating systems principles ( sosp)_. 2007 .",
    "kubiatowicz , j. , bindel , d. , chen , y. , czerwinski , s. , eaton , p. , geels , d. , gummadi , r. , rhea , s. , weatherspoon , h. , weimer , w. , wells , c. , and zhao , b. `` oceanstore : an architecture for global - scale persistent storage . '' in _ intl .",
    "conference on architectural support for programming languages and operating systems ( asplos)_. 2000 .",
    "liu , s. , schulze , j.p . ,",
    "herr , l. , weekley , j.d .",
    ", zhu , b. , van osdol , n. , plepys , d. , and wan , m. `` cinegrid exchange : a workflow - based peta - scale distributed storage platform on a high - speed network . ''",
    "_ future generation comp .",
    "_ , 27(7):966976 , 2011 ."
  ],
  "abstract_text": [
    "<S> erasure coding is a storage - efficient alternative to replication for achieving reliable data backup in distributed storage systems . during the storage process , traditional erasure codes require a unique source node to create and upload all the redundant data to the different storage nodes . </S>",
    "<S> however , such a source node may have limited communication and computation capabilities , which constrain the storage process throughput </S>",
    "<S> . moreover , the source node and the different storage nodes might not be able to send and receive data simultaneously  </S>",
    "<S> e.g. , nodes might be busy in a datacenter setting , or simply be offline in a peer - to - peer setting  which can further threaten the efficacy of the overall storage process . </S>",
    "<S> in this paper we propose an `` in - network '' redundancy generation process which distributes the data insertion load among the source and storage nodes by allowing the storage nodes to generate new redundant data by exchanging partial information among themselves , improving the throughput of the storage process . </S>",
    "<S> the process is carried out asynchronously , utilizing spare bandwidth and computing resources from the storage nodes . </S>",
    "<S> the proposed approach leverages on the local repairability property of newly proposed erasure codes tailor made for the needs of distributed storage systems . </S>",
    "<S> we analytically show that the performance of this technique relies on an efficient usage of the spare node resources , and we derive a set of scheduling algorithms to maximize the same . </S>",
    "<S> we experimentally show , using availability traces from real peer - to - peer applications as well as google data center availability and workload traces , that our algorithms can , depending on the environment characteristics , increase the throughput of the storage process significantly ( up to 90% in data centers , and 60% in peer - to - peer settings ) with respect to the classical naive data insertion approach .    </S>",
    "<S> * keywords : distributed storage systems , data insertion , locally repairable codes , self - repairing codes * </S>"
  ]
}