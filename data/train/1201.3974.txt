{
  "article_text": [
    "to the computational physicist interested in one - dimensional quantum lattice models , the density matrix renormalization group ( dmrg ) @xcite is a dream come true .",
    "it provides an essentially unbiased , extremely accurate variational approach to ground state properties of a large class of local hamiltonians in one dimensional lattices .",
    "dmrg operates by approximating the ground state of the system with a matrix product state ( mps ) @xcite , which is a simple tensor network with tensors connected according to a one - dimensional array . in recent years",
    ", the success and broad applicability of dmrg has been understood to follow from ( i ) the existence of a characteristic , universal pattern of entanglement common to most ground states in one spatial dimension ; and ( ii ) the ability of the mps to reproduce this universal pattern of entanglement , thanks to having its tensors connected into a one - dimensional geometry .",
    "the above insight has since then guided the development of new tensor network approaches that aim to repeat , in other geometries or physical regimes of interest , the unprecedented success of dmrg @xcite in one dimension .",
    "the recipe is quite simple : first , identify a pattern of entanglement common to a large class of ground states ; then , connect tensors so that they can reproduce this pattern , and use the resulting tensor network as a variational ansatz . in this way",
    "the multi - scale , layered pattern of entanglement observed in ground states near a continuous quantum phase transition motivated the proposal of the multi - scale entanglement renormalization ansatz ( mera ) @xcite to address quantum critical phenomena .",
    "similarly , the characteristic spatial pattern of entanglement in the ground states in two and higher dimensions motivated higher - dimensional generalizations of both the mps ( known as projected entangled - pair states , peps@xcite ) and the mera @xcite .",
    "the cost of simulating a lattice of @xmath0 sites with any of the above tensor networks is roughly proportional to @xmath0 , which underlies the efficiency of the approaches@xcite .",
    "importantly , however , this cost also grows as @xmath1 , that is as a power @xmath2 of the dimension @xmath3 of the indices connecting the tensors into a network . on the one hand , this bond dimension @xmath3 determines the size of the tensors and therefore the number of variational parameters contained in the tensor network ansatz . on the other , @xmath3 is also a measure of how much entanglement the tensor network can carry .",
    "it then follows that the cost of simulations increases with the amount of entanglement in the ground state of the system .",
    "entanglement is indeed the key factor limiting the range of applicability of tensor network approaches .",
    "more specifically , for an mps , a small power @xmath2 , namely @xmath4 , implies that very large values of @xmath3 ( of up to a few thousands ) can be considered even with a high - end desktop computer .",
    "correspondingly , dmrg can address one - dimensional systems with robustly entangled ground states .",
    "in contrast , the cost of two dimensional simulations with peps and mera scales with a much larger power @xmath2 of @xmath3 , e.g. @xmath5 in ref . and",
    "@xmath6 in ref . , and this considerably reduces the affordable values of @xmath3 .",
    "in other words , peps and mera calculations have so far been restricted to systems with relatively small amounts of ground state entanglement .",
    "a major present challenge for these approaches is to obtain more efficient tensor contraction schemes that could lower their cost .",
    "a possible route to reducing the scaling of computational cost with @xmath3 in tensor network algorithms is by using monte carlo sampling techniques , as proposed in refs . .",
    "as reviewed in the next section , the cost of manipulating the tensor network ( for a single sample ) is reduced to @xmath7 , where @xmath8 is significantly smaller than @xmath2 ( typically of the order of @xmath9 ) . the proposals in refs .",
    "are best suited for tensor networks , such as mps and peps , where the coefficients in the tensors are unconstrained . however , in the mera , as well as in other unitary tensor networks such as unitary versions of mps ( umps ) and of tree tensor network@xcite ( uttn ) , tensors are subject to unitary constraints .    the purpose of this paper is to address the use of monte carlo sampling in the context of unitary tensor networks , including umps , uttn and mera .",
    "[ notice that this excludes tensor networks such as a periodic mps or peps , which can not be generically re - expressed as a unitary tensor network ] .",
    "an important difference with respect to refs .",
    "is that in a unitary tensor network , sampling is performed on an effective lattice corresponding to the past causal cone of the local operator whose expectation value is being computed .",
    "this means that sampling typically occurs over some reduced number of sites ( less than the system size @xmath0 ) .",
    "a second difference is that in unitary tensor networks there is no need to use a markov chain monte carlo scheme .",
    "indeed , our main result is the proposal and benchmark of perfect sampling schemes for unitary tensor networks , by means of which one can obtain completely uncorrelated samples directly according to the correct probability .",
    "therefore , one can sample without incurring additional computational costs due to equilibration and autocorrelations times .",
    "this is particularly of interest near a quantum phase transition , where equilibration and autocorrelation times diverge with system size @xmath0 .",
    "we consider both complete ( perfect ) sampling and incomplete ( perfect ) sampling schemes . in the former ,",
    "the indices for all sites of the effective lattice are sampled . in the latter , only the indices of a subset of sites",
    "is sampled , while the indices of the rest of sites are contracted exactly , with an insignificant or minor increase of computational cost as far as the scaling @xmath10 is concerned .",
    "importantly , the statistical variance ( due to sampling ) of an expectation value obtained with incomplete sampling can decrease dramatically with a proper chose of sampling basis , as illustrated in fig .",
    "[ fig : partialmps3 ] with a drop of @xmath11 in error .",
    "the paper is organized in sections as follows .",
    "first , in section [ sec : background ] we briefly review the use of monte carlo sampling techniques to evaluate the expectation value of local operators in context of tensor networks , and introduce the notions of complete and incomplete sampling . then in section [ sec : unitary ] we explain how the proposals of refs . can be adapted to the case of a unitary tensor network by sampling within the past causal cone of the local operator . in section [ sec : perfect ] we propose a complete perfect sampling scheme for unitary tensor networks .",
    "its performance is demonstrated for a umps with the quantum ising chain at criticality . in section [",
    "sec : partial ] we then present an incomplete perfect sampling scheme .",
    "we discuss computational costs in section [ sec : costs ] .",
    "the conclusions in section [ sec : conclusion ] and an appendix analyzing the variance in different schemes close the paper .",
    "we emphasize that this paper is only concerned with the evaluation of local expectation values from a unitary tensor network .",
    "that is , here we assume that the unitary tensor network has already been optimized and focus on how to extract information from it .",
    "the optimization of unitary tensor networks using variational monte carlo is discussed in ref . .",
    "let us start by introducing our notation and by reviewing some basic concepts .",
    "let @xmath12 be a lattice made of @xmath0 sites , with vector space @xmath13 , where @xmath14 is the @xmath15-dimensional vector space of one site .",
    "let @xmath16 denote the wave - function encoded in the tensor network and let @xmath17 be a local operator on @xmath18 .",
    "an important task in tensor network algorithms is to compute the expectation value @xmath19 , which can be expressed as @xmath20 where @xmath21 denotes a product state of the @xmath0 sites of the lattice , with @xmath22 labelling the elements of an orthonormal basis @xmath23 on site @xmath24 , @xmath25 . here , @xmath26 is the set of all @xmath27 possible _ configurations _ @xmath28 of the system . the expectation value of eq .",
    "( [ eq : expect ] ) can be obtained exactly by contracting the corresponding tensor network . however",
    ", a large computational cost motivates the search for an alternative approach based on sampling .    , with a sum over ( or exact contraction of ) indices @xmath29 ( exact contraction ) . contracting this tensor network",
    "has a cost that scales as @xmath1 with the bond index @xmath3 , for some power @xmath2 .",
    "( b ) tensor networks corresponding to @xmath30 for a given configuration @xmath31 , corresponding to a single sample .",
    "the cost of contracting these two networks scales as @xmath10 with the bond index @xmath3 , where power @xmath8 is smaller than power @xmath2 .",
    "( c ) tensor network corresponding to @xmath32 for a given incomplete configuration @xmath33 ( these three indices are being sampled ) , where in addition there is a sum over ( or exact contraction of ) indices @xmath34 and @xmath35 .",
    "the cost of contracting this tensor network scales as @xmath36 , with @xmath37 somewhere between @xmath8 and @xmath2 .",
    "[ fig : sampling],width=321 ]    in preparation for an approximate evaluation of the expectation value @xmath38 , let us first introduce the probability @xmath39 of projecting state @xmath40 into the product state @xmath41 , and the estimator @xmath42 , and rewrite eq .",
    "( [ eq : expect ] ) as @xmath43 this expression emphasizes that @xmath19 can be regarded as a probabilistic average of estimator @xmath44 according to the probabilities @xmath45 , where @xmath46 , @xmath47 .",
    "let us replace the sum over the set @xmath26 of all @xmath48 configurations @xmath31 with a sum over some subset @xmath49 containing @xmath50 configurations @xmath31 , where @xmath51 , that is @xmath52 where @xmath53 is a normalization factor .",
    "( [ eq : sampling ] ) states that an approximate evaluation of @xmath38 is obtained by considering a probabilistic sum over @xmath54 configurations @xmath31 .",
    "if the @xmath54 configurations in @xmath55 have been randomly chosen from @xmath26 according to the probability @xmath45 , then importance sampling allow us to replace the previous expression with @xmath56    equation  ( [ eq : importance ] ) estimates @xmath38 by means of @xmath54 independent samples of a random variable @xmath57 . by construction , the mean @xmath58 of this random variable , @xmath59",
    "is given by the expectation value @xmath38 of operator @xmath17 , see eq .",
    "( [ eq : average ] ) .",
    "notice that , in addition , its variance @xmath60 , defined by @xmath61 also equals the variance @xmath62 of operator @xmath17 , @xmath63 that is @xmath64 , see appendix .",
    "it follows that the error @xmath65 in the approximation of eq .",
    "( [ eq : importance ] ) , as measured by the standard deviation @xmath66 of @xmath54 independent samples , scales with @xmath54 as @xmath67    let us analyze in which sense the above monte carlo sampling strategy could be of interest .",
    "the cost ( i.e. computational time ) of an exact contraction , eq .",
    "( [ eq : expect ] ) , scales as @xmath1 with the bond dimension @xmath3 .",
    "on the other hand , notice that for each specific configuration @xmath31 , the contribution @xmath68 to @xmath38 consists of two tensor networks , namely one for @xmath69 and another for @xmath70 , whose contraction can be accomplished with a cost @xmath10 , for some @xmath71 , see fig .",
    "[ fig : sampling ] .",
    "[ this is also the cost of computing @xmath45 and @xmath44 in eq .",
    "( [ eq : average ] ) ] .",
    "if the number of samples required to obtain an acceptably small error @xmath72 is @xmath73 , the use of sampling incurs a computational cost of @xmath74 instead of @xmath1 .",
    "we conclude that if @xmath75 , then ( for large @xmath3 ) the sampling strategy will have a lower computational cost than the exact contraction .",
    "more generally , one can consider a hybrid strategy which combines exact contraction and sampling .",
    "this is accomplished by sampling over only a subset of the @xmath0 indices corresponding to the @xmath0 sites of lattice @xmath12 , while performing an exact contraction on the remaining sites .",
    "for instance , fig .",
    "[ fig : sampling](c ) considers a lattice @xmath12 made of @xmath76 sites where the first three sites are being sampled , with configuration @xmath77 , whereas the remaining three of sites are being addressed with an exact contraction .",
    "if we denote by @xmath78 a configuration of the @xmath79 indices to be sampled ( @xmath80 ) , then eq . [ eq : expect ] is replaced with @xmath81 we can again rewrite eq .",
    "( [ eq : expect_dia ] ) as a probabilistic sum of an estimator @xmath82 according to probabilities @xmath83 , @xmath84 similarly , we could generalize eqs .",
    "[ eq : sampling]-[eq : importance ] and apply importance sampling .",
    "we note that in this case the variance @xmath85 , defined by @xmath86 might be smaller than the variance @xmath62 of operator @xmath17 ( eq . [ eq : varianceahat ] ) , since a single incomplete sample @xmath87 corresponds to many complete samples @xmath31 .",
    "[ for instance , in the example of fig .",
    "[ fig : sampling](c ) , the incomplete sample @xmath88 corresponds to all complete samples @xmath89 that coincide with @xmath87 in the first three sites . ] in other words , the statistical error might be reduced .",
    "this should not come as a surprise .",
    "after all , in the extreme case where no sampling at all is performed ( @xmath90 ) but all indices are exactly contracted , there is no statistical error left .      in refs .",
    "the random configurations @xmath31 were generated by means of a markov chain process based on local updates . given a stored configuration @xmath31 ,",
    "let us denote @xmath91 a configuration obtained from @xmath31 by replacing in site @xmath24 the value @xmath92 with @xmath93 .",
    "then , visiting the sites sequentially , @xmath94 , in what is known as a _ sweep _ , a change on site @xmath24 is introduced according to the metropolis probability @xmath95.\\ ] ] in this way , after one sweep a new configuration @xmath96 is obtained from @xmath31 , and by iteration a sequence of configurations @xmath97 is produced . however , these configurations will in general",
    "be correlated .",
    "the number @xmath98 of sweeps required between configurations @xmath31 and @xmath99 in order for them to be essentially independent to be independent is known as the autocorrelation time .",
    "sweeping @xmath98 times between samples is necessary in order for the error @xmath72 to scale as in eq .",
    "( [ eq : error ] ) , since that expression for the error assumed the samples to be independent .",
    "( if only a single sweep mediates the samples , the statistical error in eq .",
    "( [ eq : error ] ) increases by a factor which scales as @xmath100 due to autocorrelations ) .",
    "in addition , the first sample @xmath31 will be obtained after applying @xmath101 sweeps to some random initial configuration .",
    "the equilibration time @xmath101 is necessary in order to guarantee that the first sample is picked - up according to the correct probability distribution .",
    "the autocorrelation time @xmath98 and the equilibration time @xmath101 are known to diverge with systems size @xmath0 for critical systems .",
    "large equilibration and autocorrelation times , e.g. near or at a critical point , increase the cost of simulations .",
    "this increase can be prevented if somehow independent configurations @xmath31 can be directly generated according to probabilities @xmath45 . in section [ sec",
    ": perfect ] we show how this is possible for a specific class of tensor networks , namely unitary tensor networks , which are introduced next .",
    "of lattice @xmath12 .",
    "notice the ( fictitious ) time direction , which provides each tensor with a sense of which indices are incoming and which are outgoing .",
    "( b ) the past causal cone @xmath102 of a local operator @xmath17 acting on a single site of @xmath12 ( denoted by a discontinuous circle ) defines an effective lattice @xmath103 , which is found in state @xmath104 .",
    "notice that the effective lattice @xmath103 is made of two types of sites , namely sites already present in the original lattice @xmath12 and one site not present in @xmath12 , with @xmath15-dimensional and @xmath3-dimensional vector spaces , respectively .",
    "( c ) tensor networks representing @xmath19 and @xmath105 .",
    "the inset shows unitarity reductions [ eq .  ( [ eq : unitary ] ) ] used to transform @xmath19 into @xmath105 .",
    "[ fig : causalmps],width=321 ]     of lattice @xmath12 .",
    "( b ) effective lattice @xmath103 .",
    "( c ) tensor networks for @xmath106 and @xmath107 .",
    "the inset shows a reduction due to the unitary constrain of tensors in the uttn .",
    "[ fig : causalttn],width=321 ]    let us specialize to the particular case of unitary tensor networks , namely tensor networks that are based on a unitary quantum circuit .",
    "examples include the mera and unitary versions of mps ( with open boundary conditions ) and ttn , which we will refer as umps and uttn @xcite .",
    "unitary tensor networks are special in that each tensor @xmath108 is constrained to be unitary / isometric . figs .",
    "[ fig : causalmps ] and [ fig : causalttn ] exemplify the discussion for umps and uttn respectively . specifically , we first note that in one such tensor network there is a well - defined direction of time throughout , see e.g. figs .",
    "[ fig : causalmps](a ) and [ fig : causalttn](a ) .",
    "each index of a tensor @xmath108 is either an incoming index ( if time flows towards the tensor ) or an outgoing index ( if time flows away from the tensor ) .",
    "the constraint on @xmath108 can be expressed in the following way .",
    "let us group all incoming indices of @xmath108 into a composite incoming index @xmath109 and all outgoing indices of @xmath108 into a composite outgoing index @xmath110 , so that tensor @xmath108 becomes a matrix @xmath111 .",
    "then the unitary / isometric constraint on @xmath108 reads @xmath112 a direct implication of this property is that the tensor network corresponding to the expectation value @xmath19 can be replaced with a simplified tensor network where the pairs of tensors @xmath113 outside the so - called past causal cone @xmath102 of @xmath17 have been removed , see figs .",
    "[ fig : causalmps](c ) and [ fig : causalttn](c ) .",
    "this new tensor network can be interpreted to represent the expectation value @xmath105 of the local operator @xmath17 on a state @xmath114 of an effective lattice @xmath103 defined by the causal cone @xmath102 of the operator @xmath17 , see figs .",
    "[ fig : causalmps](b ) and [ fig : causalttn](b ) , where by construction @xmath115 .",
    "the effective lattice @xmath103 is made of @xmath116 sites that can be of two types : those already contained in the original lattice @xmath12 , which are described by a @xmath15-dimensional vector space , and those which did not belong to @xmath12 , which are described by a @xmath3-dimensional vector space .",
    "we use @xmath117 to denote a configuration of the effective lattice @xmath103 , and @xmath118 the corresponding product vector , where for some sites @xmath119 and for some others",
    ". we denote @xmath121 the set of all configurations @xmath122 .    .",
    "in ( a ) , the original state @xmath40 was represented with an umps , see fig . [",
    "fig : causalmps ] . in ( b ) , the original state @xmath40 was represented with an uttn , see fig .",
    "[ fig : causalttn ] .",
    "however , in both cases the state @xmath104 is represented by an umps that runs through the causal cone .",
    "[ fig : samplempsandttn],width=321 ]    the exact contraction of the tensor network corresponding to @xmath105 , may still be very expensive and again we might be interested in exploring the use of sampling to lower the computational cost . for that purpose ,",
    "we repeat the discussion in section [ sec : background ] .",
    "first we write the expectation value @xmath38 as @xmath123 see fig .",
    "[ fig : samplempsandttn ] for umps and uttn",
    ". then we rewrite eq .",
    "( [ eq : expect2 ] ) in terms of the estimator @xmath124 and probabilities @xmath125 , @xmath126 we can again limit the sum over configurations @xmath122 to a subset @xmath127 containing just @xmath54 configurations which , when chosen from @xmath121 randomly according to the probabilities @xmath128 , results in @xmath129 the error in the approximation scales with @xmath54 as in eq .",
    "( [ eq : error ] ) .",
    ", @xmath130 , @xmath131 , @xmath132 , and so on , see eqs .",
    "( [ eq : p1][eq : p123n ] ) .",
    "importantly , all these tensor networks can be contracted with a cost that scales as @xmath133 with the bond dimension @xmath3 , and are therefore computational less expensive than an exact contraction , which has cost @xmath134 .",
    "[ fig : perfectmps],width=321 ]    in this section we describe how to randomly draw configurations @xmath122 according to probability @xmath128 in a unitary tensor network .",
    "we refer to this scheme as perfect sampling because , in contrast with markov chain monte carlo , the present scheme produces perfectly uncorrelated samples .",
    "we will also refer to this scheme as complete perfect sampling , to distinguish it from the incomplete perfect sampling scheme discussed in the next section , where sampling is performed only on a subset of sites .",
    "recall that as a quantum circuit , the tensor network is equipped with a notion of ( fictitious ) time . from now on we assume that the labeling of the sites in the effective lattice @xmath135 has been chosen so as to progress forward with respect to this notion of time . thus , site @xmath136 corresponds to the earliest time , site @xmath137 corresponds to a later time , and so on , until site @xmath116 corresponds to the latest time ( when two sites correspond to the same time , e.g. sites @xmath138 and @xmath139 in fig .",
    "[ fig : samplempsandttn ]  ( a ) , we order them arbitrarily ) .    our _ perfect sampling _ algorithm consists of sequentially computing a series of conditional single - site density matrices @xmath140 and conditional single - site probabilities @xmath141 .",
    "first we compute the reduced density matrix @xmath142 for site @xmath136 exactly , i.e. without sampling , @xmath143 from which we can compute the probabilities @xmath144 we can then randomly choose a value for @xmath145 according to probability @xmath130 , and compute ( exactly ) the conditional reduced density matrix @xmath146 for site @xmath137 , which is obtained from the state @xmath147 of sites @xmath137 to @xmath116 , @xmath148 again , we can use the reduced density matrix to compute the conditional probabilities @xmath149 and we can therefore randomly select a value of @xmath150 according to probabilities @xmath132 . let us notice at this point that so far we have randomly chosen values for @xmath145 and @xmath150 according to the probability @xmath151 we can now iterate the above process , that is , compute the conditional density matrix @xmath152 and the conditional probabilities @xmath153 and so on for the rest of sites in the effective lattice @xmath103 . in this way , and since @xmath154 we end up indeed randomly choosing a configuration @xmath155 with probability given precisely by @xmath156 .",
    "[ fig : perfectmps ] illustrates the sequence of computations in the case of a one - site operator @xmath17 specifically for a umps , assuming as in figs .",
    "[ fig : causalmps ] and [ fig : samplempsandttn](a ) that the operator @xmath17 is supported on the fourth site of the original chain .",
    "this algorithm is similar to one used for thermal state sampling with mps  @xcite described in ref .  .",
    "analogous computations for a uttn are very similar , since the causal cone of a single - site operator @xmath17 is described also by a umps , see fig .",
    "[ fig : causalttn](b ) . for the case of a mera ,",
    "more details on the implementation of eqs .",
    "( [ eq : p1][eq : p123n ] ) can be found in ref . .",
    "a key point is that , for unitary tensor networks such as umps , uttn , and mera , the computational cost of generating the above sequence of density matrices and probabilities often does not exceed ( to leading order in @xmath3 and effective size @xmath116 ) the cost of a single sweep in markov chain monte carlo @xcite .     basis .",
    "comparison between configurations obtained using ( a ) the presented perfect sampling scheme and ( b ) a markov chain scheme ( single sweep ) on 50 sites .",
    "blue sites represent spin up and yellow for spin down .",
    "the correlations between configurations obtained using a markov chain scheme are evidenced by the appearance of domains of well defined color that extend vertically . in ( c )",
    "we have calculated the expected statistical error on the estimate of @xmath157 for the perfect sampling ( blue line ) and markov chain sampling ( blue dots ) .",
    "while with perfect sampling the error decreases with the usual @xmath158 factor , correlations between subsequent samples increase the error on the estimate in the markov chain scheme .",
    "in ( d ) we plot the same for @xmath159 by projecting all the spins into the @xmath160 basis . in this case",
    "the markov scheme used utilizes a 2-site update so as to be compatible with the wave - function symmetry  @xcite . in ( e )",
    "we present the correlations on the centre site ( in the @xmath161 basis ) after @xmath162 markov chain sweeps using @xmath163 samples for 50 sites ( blue dots ) and 250 sites ( black crosses ) . in the perfect sampling scheme ( blue line )",
    ", there are no correlations between configurations . in ( f )",
    "we plot the estimated autocorrelation time for different system sizes .",
    "[ fig : comparison],title=\"fig:\",width=151 ]   basis .",
    "comparison between configurations obtained using ( a ) the presented perfect sampling scheme and ( b ) a markov chain scheme ( single sweep ) on 50 sites .",
    "blue sites represent spin up and yellow for spin down .",
    "the correlations between configurations obtained using a markov chain scheme are evidenced by the appearance of domains of well defined color that extend vertically . in ( c )",
    "we have calculated the expected statistical error on the estimate of @xmath157 for the perfect sampling ( blue line ) and markov chain sampling ( blue dots ) .",
    "while with perfect sampling the error decreases with the usual @xmath158 factor , correlations between subsequent samples increase the error on the estimate in the markov chain scheme .",
    "in ( d ) we plot the same for @xmath159 by projecting all the spins into the @xmath160 basis . in this case",
    "the markov scheme used utilizes a 2-site update so as to be compatible with the wave - function symmetry  @xcite . in ( e )",
    "we present the correlations on the centre site ( in the @xmath161 basis ) after @xmath162 markov chain sweeps using @xmath163 samples for 50 sites ( blue dots ) and 250 sites ( black crosses ) . in the perfect sampling scheme ( blue line ) , there are no correlations between configurations . in ( f )",
    "we plot the estimated autocorrelation time for different system sizes .",
    "[ fig : comparison],title=\"fig:\",width=151 ]   basis .",
    "comparison between configurations obtained using ( a ) the presented perfect sampling scheme and ( b ) a markov chain scheme ( single sweep ) on 50 sites .",
    "blue sites represent spin up and yellow for spin down .",
    "the correlations between configurations obtained using a markov chain scheme are evidenced by the appearance of domains of well defined color that extend vertically . in ( c )",
    "we have calculated the expected statistical error on the estimate of @xmath157 for the perfect sampling ( blue line ) and markov chain sampling ( blue dots ) .",
    "while with perfect sampling the error decreases with the usual @xmath158 factor , correlations between subsequent samples increase the error on the estimate in the markov chain scheme .",
    "in ( d ) we plot the same for @xmath159 by projecting all the spins into the @xmath160 basis . in this case",
    "the markov scheme used utilizes a 2-site update so as to be compatible with the wave - function symmetry  @xcite . in ( e )",
    "we present the correlations on the centre site ( in the @xmath161 basis ) after @xmath162 markov chain sweeps using @xmath163 samples for 50 sites ( blue dots ) and 250 sites ( black crosses ) . in the perfect sampling scheme ( blue line ) , there are no correlations between configurations . in ( f )",
    "we plot the estimated autocorrelation time for different system sizes .",
    "[ fig : comparison],title=\"fig:\",width=151 ]   basis .",
    "comparison between configurations obtained using ( a ) the presented perfect sampling scheme and ( b ) a markov chain scheme ( single sweep ) on 50 sites .",
    "blue sites represent spin up and yellow for spin down .",
    "the correlations between configurations obtained using a markov chain scheme are evidenced by the appearance of domains of well defined color that extend vertically . in ( c )",
    "we have calculated the expected statistical error on the estimate of @xmath157 for the perfect sampling ( blue line ) and markov chain sampling ( blue dots ) .",
    "while with perfect sampling the error decreases with the usual @xmath158 factor , correlations between subsequent samples increase the error on the estimate in the markov chain scheme .",
    "in ( d ) we plot the same for @xmath159 by projecting all the spins into the @xmath160 basis . in this case",
    "the markov scheme used utilizes a 2-site update so as to be compatible with the wave - function symmetry  @xcite . in ( e )",
    "we present the correlations on the centre site ( in the @xmath161 basis ) after @xmath162 markov chain sweeps using @xmath163 samples for 50 sites ( blue dots ) and 250 sites ( black crosses ) . in the perfect sampling scheme ( blue line )",
    ", there are no correlations between configurations . in",
    "( f ) we plot the estimated autocorrelation time for different system sizes .",
    "[ fig : comparison],title=\"fig:\",width=151 ]   basis .",
    "comparison between configurations obtained using ( a ) the presented perfect sampling scheme and ( b ) a markov chain scheme ( single sweep ) on 50 sites .",
    "blue sites represent spin up and yellow for spin down .",
    "the correlations between configurations obtained using a markov chain scheme are evidenced by the appearance of domains of well defined color that extend vertically . in ( c )",
    "we have calculated the expected statistical error on the estimate of @xmath157 for the perfect sampling ( blue line ) and markov chain sampling ( blue dots ) .",
    "while with perfect sampling the error decreases with the usual @xmath158 factor , correlations between subsequent samples increase the error on the estimate in the markov chain scheme .",
    "in ( d ) we plot the same for @xmath159 by projecting all the spins into the @xmath160 basis . in this case",
    "the markov scheme used utilizes a 2-site update so as to be compatible with the wave - function symmetry  @xcite . in ( e )",
    "we present the correlations on the centre site ( in the @xmath161 basis ) after @xmath162 markov chain sweeps using @xmath163 samples for 50 sites ( blue dots ) and 250 sites ( black crosses ) . in the perfect sampling scheme ( blue line ) , there are no correlations between configurations . in ( f )",
    "we plot the estimated autocorrelation time for different system sizes .",
    "[ fig : comparison],title=\"fig:\",width=151 ]   basis .",
    "comparison between configurations obtained using ( a ) the presented perfect sampling scheme and ( b ) a markov chain scheme ( single sweep ) on 50 sites .",
    "blue sites represent spin up and yellow for spin down .",
    "the correlations between configurations obtained using a markov chain scheme are evidenced by the appearance of domains of well defined color that extend vertically . in ( c ) we have calculated the expected statistical error on the estimate of @xmath157 for the perfect sampling ( blue line ) and markov chain sampling ( blue dots ) .",
    "while with perfect sampling the error decreases with the usual @xmath158 factor , correlations between subsequent samples increase the error on the estimate in the markov chain scheme .",
    "in ( d ) we plot the same for @xmath159 by projecting all the spins into the @xmath160 basis . in this case",
    "the markov scheme used utilizes a 2-site update so as to be compatible with the wave - function symmetry  @xcite . in ( e )",
    "we present the correlations on the centre site ( in the @xmath161 basis ) after @xmath162 markov chain sweeps using @xmath163 samples for 50 sites ( blue dots ) and 250 sites ( black crosses ) . in the perfect sampling scheme ( blue line )",
    ", there are no correlations between configurations . in ( f )",
    "we plot the estimated autocorrelation time for different system sizes .",
    "[ fig : comparison],title=\"fig:\",width=151 ]      to illustrate the performance of the perfect sampling scheme and compare it to markov chain monte carlo , we have considered a duly optimized umps for the ground state @xmath40 of the quantum ising model with critical transverse magnetic field , @xmath164 on an open chain of @xmath0 spins.@xcite the two sampling schemes are then used in order to compute the expectation value of local operators .",
    "[ fig : comparison](a ) and ( b ) show a history of 150 configurations of a chain of @xmath165 spins obtained with perfect sampling and markov chain monte carlo , respectively .",
    "the existence of correlations in the second case is manifest .",
    "[ fig : comparison](c ) and ( d ) show the error in the expectation value @xmath166 and @xmath166 for the local operators @xmath167 and @xmath168 on site @xmath169 , as a function of the number of samples @xmath54 . in both cases , the effect of autocorrelations in markov chain monte carlo results in an error larger than the error obtained with perfect sampling , which is given by eq .",
    "( [ eq : error ] ) .",
    "the ratio between statistical errors , as given in terms of the autocorrelation time @xmath98 by @xmath170 , is seen to depend on the choice of local operator  this autocorrelation time is larger for @xmath166 than for @xmath171 .    finally , fig .",
    "[ fig : comparison ] ( e ) and ( f ) explore the autocorrelation time @xmath98 for @xmath172 as a function of the size @xmath0 of the spin chain .",
    "in particular , fig . [",
    "fig : comparison ] ( f ) reveals that @xmath98 grows linearly in @xmath0 .",
    "this means @xcite that in order to achieve a fixed accuracy in @xmath173 , the number of samples @xmath54 with markov chain monte carlo has to grow linearly in @xmath0 , whereas a constant number of samples is enough with perfect sampling .",
    "it is important to stress , however , that the markov chain monte carlo update scheme discussed here , based on single spin updates , is used as a reference only  more sophisticated markov chain monte carlo schemes , based e.g. on global spin updates , could lead to smaller autocorrelation times .",
    "so far we have considered perfect sampling over the whole causal cone , that is , over the indices associated to all the sites of the effective lattice @xmath103 .",
    "however , it is also possible to use an incomplte perfect sampling scheme , which combines perfect sampling over most of the sites of @xmath103 and an exact contraction over a small set of sites , without altering the scaling @xmath10 of the cost of a single sample . because we are sampling over fewer indices",
    ", we can expect a decrease in the statistical error with little change in the cost . in some cases",
    "the reduction in statistical uncertainty can be dramatic .     for a umps , to be compared with fig .",
    "[ fig : samplempsandttn](a ) .",
    "notice that sampling does not affect two of the indices , over which an exact contraction is still performed .",
    "[ fig : partialmps1],width=226 ]    , @xmath130 , @xmath131 , @xmath132 , @xmath174 and @xmath175 necessary in order to generate a configuration @xmath176 with probability @xmath177 .",
    "notice that the cost still scales as @xmath133 , as in the complete ( perfect ) sampling scheme .",
    "[ fig : partialmps2],width=264 ]      the incomplete perfect sampling scheme is illustrated in fig .",
    "[ fig : partialmps1 ] for a umps .",
    "the first step is to rewrite the expectation value @xmath178 as @xmath179 where @xmath180 is the set of incomplete configurations @xmath181 , where @xmath79 is the number of sites over which sampling takes place , with @xmath182 . for the case of the umps illustrated in fig .",
    "[ fig : partialmps1 ] , one can perform an exact contraction on two sites of @xmath103 , namely the site on which the local operator @xmath17 is supported and the effective , @xmath3-dimensional site corresponding to the bond index of the umps .",
    "notice that now the term @xmath183 does not factorize into two terms , since @xmath184 and @xmath185 are no longer complex numbers but @xmath186-dimensional vectors .",
    "we can still rewrite eq .",
    "( [ eq : expect3 ] ) as a probabilitistic sum of an estimator @xmath187 according to probabilities @xmath188 , @xmath189 limiting the sum over configurations @xmath190 to a subset @xmath191 containing just @xmath54 configurations , and use ( perfect ) importance sampling to obtain the estimate @xmath192 an important difference between the incomplete perfect sampling scheme and the comnplete perfect sampling scheme of eqs .",
    "( [ eq : expect2][eq : importance2 ] ) is that the estimator @xmath193 , whose mean is @xmath194 as indicated in eq .",
    "( [ eq : average3 ] ) , has a variance @xmath195 , @xmath196 that is no longer necessarily equal to the variance @xmath197 of eq .",
    "( [ eq : varianceahat ] ) , but is instead upper bounded by it , @xmath198 , see the appendix .",
    "in other words , the error @xmath199 in the approximation of eq .",
    "( [ eq : importance3 ] ) , given by @xmath200 can be smaller than the error @xmath72 of a complete sampling scheme .",
    "we have implemented the incomplete perefect sampling scheme in conjunction with the complete perfect sampling scheme described in section [ sec : perfect ] .",
    "we notice , however , that incomplete sampling can also be incorporated into markov chain monte carlo .    as in section",
    "[ sec : perfect ] , we proceed by constructing a sequence of conditional single - site reduced density matrices @xmath140 and conditional probabilities @xmath201 .",
    "however , in this occasion the sequence concludes at site @xmath79 , after which we can already evaluate the estimator @xmath202 .",
    "this is illustrated for the case of a umps in fig .",
    "[ fig : partialmps2 ] , which is to be compared with fig .",
    "[ fig : perfectmps ] .      as in section",
    "[ sec : perfect ] , we use sampling to compute the expectation value of local observables from a umps with @xmath203 that has been previously optimized to approximate the ground state of the quantum ising chain at criticality , eq .",
    "( [ eq : ising ] ) .",
    "the exact structure that we sample can bee seen in fig . [ fig : partialmps1 ] .",
    "figure  [ fig : partialmps3 ] shows the sampling error , as a function of the number of samples @xmath54 , in the computation of @xmath204 and @xmath205 in a chain of @xmath165 spins .",
    "the error is seen to depend on two factors . on the one hand , it depends on which operator ( @xmath172 or @xmath206 ) is being measured , as it did in section [ sec : perfect ] .",
    "in addition , now it also drastically depends on which product basis @xmath207 is used . in particular",
    ", we see that a very substantial reduction of sampling error , of seven orders of magnitude , is obtained by measuring on the @xmath160 basis while computing @xmath204 .",
    "it should be noted that the two - site markov chain update scheme used for the @xmath160-basis calculations,@xcite although appears competitive , is more computationally demanding than the perfect sampling scheme and runs approximately 23 times slower .    .",
    "with perfect sampling , errors in the incomplete perfect sampling scheme are upper - bounded by the errors in a complete sampling scheme , as proven in the appendix .",
    "interestingly , for estimates of @xmath208 the incomplete perfect sampling scheme obtains an error @xmath11 times smaller by measuring in the @xmath160 basis on sites @xmath209 .",
    "( b ) sampling errors in the computation of @xmath205 .",
    "again , the errors with incomplete perfect sampling are smaller than those with complete perfect sampling , and depend on the choice of product basis . [",
    "fig : partialmps3],title=\"fig:\",width=283 ] . with perfect sampling ,",
    "errors in the incomplete perfect sampling scheme are upper - bounded by the errors in a complete sampling scheme , as proven in the appendix .",
    "interestingly , for estimates of @xmath208 the incomplete perfect sampling scheme obtains an error @xmath11 times smaller by measuring in the @xmath160 basis on sites @xmath209 .",
    "( b ) sampling errors in the computation of @xmath205 .",
    "again , the errors with incomplete perfect sampling are smaller than those with complete perfect sampling , and depend on the choice of product basis . [",
    "fig : partialmps3],title=\"fig:\",width=283 ]",
    "for completeness , we include a brief summary of the computational costs incurred in extracting , from a given unitary tensor network , the expectation value of a local operator by using ( i ) exact contraction , ( ii ) markov chain monte carlo and ( iii ) a perfect sampling scheme . for simplicity , we consider only one - site local operators .",
    "the scaling of the costs in the bond dimension @xmath3 is presented in table [ table : cost ] .",
    "we emphasize that in the sampling schemes , we only consider the cost of obtaining one sample",
    ". a fair comparison of costs with an exact contraction should also take into account the number of samples required in order to approximate the exact result with some pre - agreed accuracy .",
    ".the leading - order costs of contracting unitary tensor networks with and without sampling techniques , with the goal of estimating the expectation value of a one - site operator . for the mera we have also included the cost calculating arbitrary ( long - range ) two - point correlators.@xcite [ table : cost ] [ cols=\"^,^,^,^ \" , ]     the table shows that for both a umps and the mera , the cost of markov chain monte carlo and perfect sampling scale with the same power . instead , for the uttn , the of markov chain monte carlo is one power smaller than that of perfect sampling .",
    "[ the same would happen with umps if the local dimension of each site was also @xmath3 ] .",
    "more significant speed - ups can be seen with the mera , both for the computation of two - point correlators , and in systems in two dimensions ( not in the table ) , where sampling techniques to increase computational efficiency are required most .",
    "the authors present an in - depth analysis of perfect sampling with the mera in ref .  .",
    "a further remark is in order .",
    "the above analysis assumes that a tensor network has been provided in a unitary circuit form .",
    "in particular , the costs in table [ table : cost ] do not include operations such as converting a non - unitary version of the tensor network into its unitary form ( typically through the qr - decomposition ) .",
    "in particular , the cost of qr - decompositions required to turn an mps into a umps scales as @xmath211  that is , the same scaling as an exact contraction .",
    "what is then the practical interest in a perfect sampling scheme for a umps ?",
    "on the one hand , the umps might conceivably have been generated through some procedure ( e.g. along the lines of the algebraic bethe ansatz mps constructions described in ref . ) , with a cost @xmath133 ( notice that a umps tensor only contains @xmath133 coefficients ) . in this case",
    ", the perfect sampling scheme would allow for a very efficient , approximate evaluation of expectation values without increasing this cost . on the other hand",
    ", although we have focused our analysis on the evaluation of local expectation values , more complex tasks involving a umps , such as the computation of entanglement entropy , can exploit the perfect sampling schemes presented in this paper at a cost significantly lower than that of an exact contraction ( see e.g. ref . ) .",
    "we have explained how to perform monte carlo sampling on unitary tensor networks such as the mera , umps and uttn . in order to compute the expectation value @xmath19 of a local operator @xmath17",
    ", sampling is performed on the past causal cone @xmath102 of operator @xmath17 . in addition , by exploiting the unitary character of the tensors , it is possible to directly sample configurations @xmath122 of the causal cone according to their weight in the wave - function , resulting in uncorrelated samples and thus avoiding the equilibration and autocorrelation times of markov chain monte carlo schemes .",
    "this last property makes the perfect sampling scheme particularly interesting to study critical systems .    in principle",
    ", one can also proceed as in eqs .",
    "( [ eq : p1][eq : p123n ] ) for non - unitary tensor networks , e.g. peps , and obtain perfect sampling",
    ". however , in non - unitary tensor networks the cost of computing e.g. @xmath142 is already the same as that of computing the expectation value @xmath19 without sampling .",
    "therefore perfect sampling in non - unitary tensor networks seems to be of very limited interest .",
    "here we have only considered sampling in the context of computing expectation values .",
    "however , the same approach can also be applied in order to optimize the variational ansatz , as discussed in full detail in ref . for the mera .",
    "the authors thank glen evenbly for useful discussions .",
    "support from the australian research council ( ff0668731 , dp0878830 , dp1092513 ) , the visitor programme at perimeter institute , nserc and fqrnt is acknowledged .",
    "given a vector @xmath16 and a local operator @xmath17 , the expectation value of @xmath17 is given by @xmath212 and its variance is @xmath213      consider the complex random variable @xmath214 , where @xmath44 is the estimator @xmath215 and @xmath45 is the probability @xmath216 here @xmath217 denotes an orthonormal basis in the vector space @xmath18 .",
    "notice that @xmath218 is a resolution of the identity in @xmath18 and therefore @xmath219 .",
    "consider now a new complex random variable @xmath223 , where @xmath44 is the estimator @xmath224 and @xmath45 is the probability @xmath225 here @xmath226 denotes a complete set of projectors on the vector space @xmath18 , that is @xmath227 , and @xmath228 is a resolution of the identity in @xmath18 , so that @xmath219 .",
    "notice that if all the projectors @xmath229 have rank one , then we recover the situation analyzed in the previous subsection .",
    "notice also that this more general setting includes the case addressed in sect .",
    "[ sec : partial ] in the context of incomplete sampling .",
    "the mean @xmath58 is again given by the expectation value @xmath19 , @xmath230 however , this time the variance @xmath221 is only upper bounded by the variance @xmath197 of operator @xmath17 .",
    "this follows from , @xmath231          m. fannes , b. nachtergaele , and r. f. werner , commun .",
    "phys . * 144 * , 443 ( 1992 ) .",
    "s. ostlund and s. rommer , phys .",
    "75 * , 3537 ( 1995 ) .",
    "g. vidal , phys .",
    "lett . , * 91 * , 147902 ( 2003 ) d. perez - garcia , f. verstraete , m. m.wolf , and j. i. cirac , quant .",
    "* 7 * , 401 ( 2007 ) .",
    "g. vidal , phys .",
    ", * 99 * , 220405 ( 2007 ) ; g. vidal , phys .",
    "lett . , * 101 * , 110501 ( 2008 ) .",
    "g. vidal , in _ understanding quantum phase transitions _ , edited by l. d. carr ( taylor @xmath235 francis , boca raton , 2010 ) , arxiv:0912.1651v2 .",
    "f. verstraete , and j. i. cirac , arxiv : cond - mat/0407066v1 ( 2004 ) .",
    "g. sierra and m.a .",
    "martin - delgado , arxiv : cond - mat/9811170v3 ( 1998 ) .",
    "t. nishino and k. okunishi , j. phys .",
    "jpn . , * 67 * , 3066 , 1998 . v. murg , f. verstraete , and j. i. cirac , phys .",
    "rev . a , * 75 * , 033605 ( 2007 ) .",
    "j. jordan , r. orus , g. vidal , f. verstraete , and j. i. cirac , phys .",
    "lett . , * 101 * , 250602 ( 2008 ) .",
    "gu , m. levin , and x .-",
    "wen , phys .",
    "b , * 78 * , 205116 ( 2008 ) .",
    "h. c. jiang , z. y. weng , and t. xiang , phys .",
    ", * 101 * , 090603 ( 2008 ) .",
    "g. evenbly and g. vidal , phys .",
    "b , * 81 * , 235102 ( 2010 ) .",
    "g. evenbly and g. vidal , new j. phys .",
    ", * 12 * , 025007 ( 2010 )",
    ". l. cincio , j. dziarmaga , and m. m. rams phys .",
    "lett . , * 100 * , 240603 ( 2008 ) .",
    "g. evenbly and g. vidal , phys .",
    "lett . , * 102 * , 180406 ( 2009 ) .",
    "exploitation of space symmetries ( e.g. translation invariance and scale invariance ) can significantly reduce the computational cost of simulations from @xmath236 to @xmath237 or even to a constant ( independent of @xmath0 ) , allowing to reach the thermodynamic limit .                  from an mps ( ttn ) for a state @xmath40 , with given bond dimension @xmath3 , one can always use the gauge freedom in these tensor networks to obtain a unitary mps ( respectively ttn ) for the same state @xmath40 and with the same bond dimension @xmath3 , by writing the tensor network in its canonical form @xcite .    we have found some tensor networks for which perfect sampling costs one more power of @xmath3 than a markov chain sweep , see ref .  .",
    "further , for local operators supported on two or more sites , perfect sampling may incur a slightly larger cost than that of a single sweep of markov chain monte carlo .",
    "for example , this occurs with a ttn , where perfect sampling of a two - site operator costs @xmath238 instead of @xmath239 , but not with mps or mera .",
    "the transverse ising model contains a @xmath240 symmetry as the operator @xmath241 commutes with the hamiltonian . for a wave - function chosen from one of the @xmath242 sectors of this operator , after making a projective measurement of all the spins in the @xmath160-basis , one always gets an even ( odd ) number number of spin downs ( actually , lefts ) .",
    "the overlap after flipping a single spin of such a configuration is always zero , and thus a two - site markov chain update scheme that can preserve parity is required .",
    "our hamiltonian is actually defined in terms of bond operators , leading to effectively half the magnetic field at the ends of the chain .",
    "this change does not affect any of the critical properties of the system ."
  ],
  "abstract_text": [
    "<S> tensor network states are powerful variational anstze for many - body ground states of quantum lattice models . </S>",
    "<S> the use of monte carlo sampling techniques in tensor network approaches significantly reduces the cost of tensor contractions , potentially leading to a substantial increase in computational efficiency . </S>",
    "<S> previous proposals are based on a markov chain monte carlo scheme generated by locally updating configurations and , as such , must deal with equilibration and autocorrelation times , which result in a reduction of efficiency . here </S>",
    "<S> we propose perfect sampling schemes , with vanishing equilibration and autocorrelation times , for unitary tensor networks  </S>",
    "<S> namely tensor networks based on efficiently contractible , unitary quantum circuits , such as unitary versions of the matrix product state ( mps ) and tree tensor network ( ttn ) , and the multi - scale entanglement renormalization ansatz ( mera ) . </S>",
    "<S> configurations are directly sampled according to their probabilities in the wave - function , without resorting to a markov chain process . </S>",
    "<S> we consider both complete sampling , involving all the relevant sites of the system , as well as incomplete sampling , which only involves a subset of those sites , and which can result in a dramatic ( basis - dependent ) reduction of sampling error . </S>"
  ]
}