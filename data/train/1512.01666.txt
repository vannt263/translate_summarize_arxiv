{
  "article_text": [
    "a hidden markov model ( hmm ) @xcite consists of a hidden state sequence @xmath0 and a corresponding observation sequence @xmath1 .",
    "let there be @xmath2 hidden states . for convenience",
    ", we let the start state be @xmath3 and set @xmath4 .",
    "let @xmath5 be the transition matrix where @xmath6 , and @xmath7 be the initial state distribution where @xmath8 .",
    "a hierarchical dirichlet process hmm ( hdp - hmm ) @xcite allows to use an unbounded number of hidden states by constructing an infinite mean vector @xmath9 from a stick breaking process and drawing transition vectors @xmath10 from the shared @xmath9 .",
    "we have @xmath11 and @xmath12 , @xmath13    a hidden sequence is generated by a first order markov process , and each observation is generated conditioned on its hidden state .",
    "we have for @xmath14 , @xmath15 where @xmath16 parametrizes the observation likelihoods for @xmath17 , with @xmath18 .",
    "we assume that the observation likelihoods and their conjugate prior take exponential forms : @xmath19    the base measure @xmath20 and log normalizer @xmath21 are scalar functions ; and the parameter @xmath16 and sufficient statistics @xmath22 are vector functions .",
    "the subscripts @xmath23 and @xmath24 represent the local hidden variables and global model parameters , respectively .",
    "the dimensionality of the prior hyperparameter @xmath25 is equal to @xmath26 . for a complete bayesian treatment ,",
    "we place vague gamma priors on @xmath27 and @xmath28 , @xmath29 and @xmath30 .",
    "the graphical model is shown in figure [ hdphmm ] ( left ) .",
    "times to fit the page . ]",
    "hmms and hdp - hmms are popular probabilistic models for modelling sequential data",
    ". however , their traditional inference methods such as variational inference ( vi ) @xcite and markov chain monte carlo ( mcmc ) @xcite are not readily scalable to large datasets ( e.g. ,  one dataset in our experiment contains @xmath31 million sequences with combined length over @xmath32 million ) . in this paper",
    ", we follow the success of stochastic collapsed variational inference ( scvi ) for latent dirichlet allocation ( lda ) @xcite , and we propose a scalable scvi algorithm for hmms and hdp - hmms . our algorithm achieved better predictive performances than the stochastic variational inference ( svi ) @xcite applied to hmms @xcite and to hdp - hmms @xcite .",
    "we present our derivation in the following three steps : 1 .",
    "we marginalize out the model parameters @xmath33 ; 2 .",
    "we derive stochastic updates for each sequence ; 3 .",
    "we derive stochastic updates for the posteriors of the hdp parameters @xmath34 ( for hdp - hmms only ) . for notational simplicity",
    ", we consider a dataset of @xmath35 sequences each of length @xmath36 .",
    "that is @xmath37 and @xmath38 .",
    "similarly , we write for hidden sequences @xmath39 and @xmath40 .",
    "there is substantial empirical evidence @xcite that marginalizing the model parameters is helpful for both accurate and efficient inference .",
    "the marginal data likelihood of an hdp - hmm is : @xmath41 the gamma functions and log normalizers come from the marginalization .",
    "@xmath42 denotes the transition count from the hidden state @xmath43 to @xmath44 , @xmath45 .",
    "dot denotes the summed out column , e.g. ,  @xmath46 .",
    "@xmath47 denotes the posterior hyperparameter for the hidden state @xmath44 , @xmath48 and @xmath49 , where @xmath50 is the standard delta function .",
    "the gamma functions are a nuisance to take derivatives of ( [ collapsedhdphmm ] ) .",
    "following @xcite , we replace them by integrals of some auxiliary variables @xmath51 and @xmath52 and the joint likelihood becomes : @xmath53 where @xmath54 $ ] is beta distributed , @xmath55 is the number of tables labelled with @xmath44 in the @xmath56 chinese restaurant in a chinese restaurant franchise , and @xmath57 is unsigned stirling number of the first kind .",
    "the factor graph with the auxiliary variables is given in figure [ hdphmm ] ( right ) .",
    "we are interested in the posterior @xmath58 . as the exact computation is intractable ,",
    "we introduce a variational distribution in a tractable family , @xmath59 and we maximize the evidence lower bound ( elbo ) denoted by @xmath60 , @xmath61   - \\mathbb{e}[\\log q(\\textbf{z},\\eta,\\textbf{s},\\gamma,\\alpha,\\tilde{\\pi } ) ] \\triangleq \\mathcal{l}(q ) .",
    "\\label{elbo}\\end{aligned}\\ ] ]    by the ` direct assignment truncation ' @xcite , we set the truncation level to be @xmath2 .",
    "that is @xmath62 if for any @xmath63 and @xmath22 such that @xmath64 .      to infer @xmath65",
    ", we factorize it as a product of independent sequences , @xmath66 . combining the work of scvi for lda @xcite and cvi for hmm @xcite ,",
    "we randomly sample @xmath67 with @xmath68 $ ] , and we derive the update for @xmath69 with a zeroth order taylor approximation @xcite : @xmath70+\\mathbb{e}[c_{kk ' } ]   \\label{scvihmm1 - 1 } \\\\",
    "\\hat{\\phi}_{k',w } & \\propto h(w ) \\exp \\ { a_g ( \\lambda^\\circ_1+t(w)+\\mathbb{e}[t_{k'}(\\textbf{x},\\textbf{z } ) ] ,   \\lambda^\\circ_2 + 1+\\mathbb{e}[c_{\\cdot k ' } ] ) \\ } , \\label{scvihmm1 - 2}\\end{aligned}\\ ] ] in which @xmath71 denotes the geometric expectation , @xmath72 $ ] denotes the expected transition count from state @xmath43 to @xmath44 , and @xmath73 = \\sum_{n=1}^n \\sum_{t=1}^t q(z^n_t = k')t(x^n_t)$ ] denotes the emission statistics at the hidden state @xmath44 .",
    "the details on expectations that appear in the paper are in appendix a.    as @xmath69 is proportional to a hmm parametrized by the surrogate parameters @xmath74 and @xmath75 , we can use the forward backward algorithm @xcite . after collecting the local transition",
    "counts @xmath76 $ ] and emission statistics @xmath77 $ ] , we update the global statistics by taking a weighted average : @xmath78 & = ( 1-\\rho_n ) \\mathbb{e}[c_{kk ' } ] + \\rho_n n \\mathbb{e}[c^n_{kk ' } ] \\label{scvihmm2 - 1}\\\\ \\mathbb{e}[t_{k'}(\\textbf{x},\\textbf{z } ) ] & = ( 1-\\rho_n ) \\mathbb{e}[t_{k'}(\\textbf{x},\\textbf{z } ) ] + \\rho_n n \\mathbb{e}[t_{k'}(\\textbf{x}^n,\\textbf{z}^n ) ] , \\label{scvihmm2 - 2}\\end{aligned}\\ ] ] where @xmath79 is the step size satisfying @xmath80 and @xmath81 .    unlike cvi for hmm @xcite ,",
    "our algorithm is memory efficient , since we update @xmath69 without subtracting the local statistics , as such they do not need to be explicitly stored .      for notational clarity , we write the variational posteriors of the hdp parameters to be governed by their variational parameters .",
    "we have , @xmath82    we derive stochastic updates for the hdp posteriors .",
    "for a randomly selected sequence @xmath67 , we form an artificial dataset @xmath83 consisting @xmath35 replicates of the observed and hidden sequences @xmath84 .",
    "assuming we can compute @xmath85 $ ] and @xmath86 $ ] based on the artificial dataset , we derive the intermediate variational parameters and take a weighted average with their old estimates .",
    "hence , we have the following updates ( @xmath87 $ ] in ( [ scvihdp6 ] ) is also a function of @xmath85 $ ] ) : @xmath88 ) & \\ ! v_{k ' } & = ( 1-\\rho_n ) v_{k ' } + \\rho_n ( \\mathbb{e}[\\gamma ] + \\mathbb{e}[s^{(n)}_{\\cdot > k ' } ] )   \\label{scvihdp2 } \\\\ \\ ! \\ !",
    "a_\\alpha & = ( 1-\\rho_n)a_\\alpha + \\rho_n(a^\\circ_\\alpha + \\mathbb{e}[s^{(n)}_{\\cdot\\cdot } ] ) & \\ !",
    "b_\\alpha & = ( 1-\\rho_n)b_\\alpha + \\rho_n ( b^\\circ_\\alpha - \\textstyle \\sum_{k } \\mathbb{e}[\\log \\eta^{(n)}_k ] )   \\label{scvihdp4 } \\\\ \\ ! \\ ! a_\\gamma & = ( 1-\\rho_n)a_\\gamma + \\rho_n ( a^\\circ_\\gamma + k ) & \\ ! b_\\gamma & = ( 1-\\rho_n)b_\\gamma + \\rho_n ( b^\\circ_\\gamma -   \\textstyle \\sum_{k ' } \\mathbb{e}[\\log ( 1-\\tilde{\\pi}_{k ' } ) ] ) \\label{scvihdp6}\\end{aligned}\\ ] ] where dot denotes the summed out column , and @xmath89 denotes summing over @xmath23 for @xmath90 .",
    "the details on computing @xmath85 $ ] and @xmath86 $ ] are in appendix b. stochastic optimizations often benefit from the use of minibatches , to reduce the variance of noisy samples and the updating time of variational parameters .",
    "thus we propose to update the global statistics after a minibatch is processed , and to update the hdp posteriors after a larger batch is processed .",
    "altogether , our scvi algorithm for hdp - hmms is given in appendix c , and it applies to hmms by removing the outermost loop .     and forgetting rates @xmath91 .",
    "bottom row : comparison on nyt under various ( truncated ) numbers of hidden states @xmath2 . ]",
    "we evaluated our scvi algorithm applied to hmms ( denoted by scvi ) and applied to hdp - hmms ( denoted by scvihdp ) compared to the svi algorithm applied to hmms @xcite ( denoted by svi ) .",
    "svi applied to hdp - hmms was omitted , since we were unable to make noticeable improvement over svi using a point estimate of the top level stick @xmath9 @xcite .",
    "we used two discrete datasets , the wall street journal ( wsj ) and new york times ( nyt ) .",
    "both datasets are made of sentences . for each sentence",
    ", the underlying sequence can be understood as a markov chain of hidden part - of - speech ( pos ) tags @xcite and words are drawn conditioned on pos tags , making ( hdp)-hmms natural models .",
    "we used the predictive log likelihoods as our evaluation metrics .    for svi and scvi , we set the transition priors to @xmath92 , to encourage sparsity . for scvihdp",
    ", we set the hdp priors to be vague , @xmath93 .",
    "we set @xmath94=0.1 $ ] for the first iteration such that all the algorithms started with the same transition prior counts . finally",
    ", all the emission priors were set to @xmath92 ; all the global statistics @xmath95 $ ] and @xmath73 $ ] were initialized using exponential distributions , as suggested by @xcite .",
    "the first three rows in figure [ combined_online ] presents the predictive log likelihood results of three inferences on wsj ( @xmath96 sentences , @xmath97 for training and @xmath98 for testing ) .",
    "we fixed the number of hidden states ( or truncation level ) @xmath99 and varied the minibatch sizes @xmath100 and forgetting rates @xmath91 , which parametrize the step sizes @xmath101 .",
    "the large batch size was set to be @xmath102 for scvihdp .",
    "we let each inference run through the dataset @xmath103 times and reported the per time step likelihoods . in all the settings , our scvi outperformed svi by large margins , extending the success of scvi for lda @xcite to time series data .",
    "further , our collapsed hdp inference helped scvihdp to surpass our scvi by noticeable margins .",
    "the forth row in figure [ combined_online ] presents the predictive log likelihood results of three inferences on nyt ( @xmath31 million sentences , @xmath104 for training and @xmath105 for testing ) using the complementary settings to wsj .",
    "we fixed @xmath106 and @xmath107 and varied @xmath2 .",
    "we ran all the algorithms ( implemented in cython ) for @xmath108 hours and reported the likelihood results versus wall - clock time .",
    "we see that given the same time , our scvi converged much better than svi .",
    "our scvihdp overlapped with our scvi towards the end , but it was always better prior to that , making better use of its time .",
    "in this paper , we have presented a general stochastic collapsed variational inference algorithm that is scalable to very large time series datasets , memory efficient and significantly more accurate than the existing svi algorithm .",
    "our algorithm is also the first truly variational algorithm for hdp - hmms , avoiding point estimates , and it comes with performance gains . for future work , we aim to derive the true nature gradients of the elbo to prove and further speed up the convergence of our algorithm @xcite , although we never saw a nonconverging case in our experiments .",
    "in this section , we present the standard ( geometric ) expectations that appeared in the main paper",
    ".    if @xmath109 is beta distributed , @xmath110 , ( e.g. ,  @xmath111 in the main paper ) , we have , @xmath112 = \\psi(u)-\\psi(u+v ) & \\mathbb{e}[\\log ( 1-x ) ] = \\psi(v)-\\psi(u+v ) & \\end{aligned}\\ ] ] if @xmath109 is gamma distributed , @xmath113 ( e.g. ,  @xmath27 in the main paper ) , we have , @xmath114 = a / b & \\mathbb{g}[x ] = e^{\\psi(a)}/b & \\end{aligned}\\ ] ] if @xmath109 and @xmath115 are independent , ( e.g. ,  @xmath27 and @xmath116 in the main paper ) , we have @xmath117 = \\mathbb{g}[x]\\mathbb{g}[y]$ ] .",
    "in this section , we present the details on computing @xmath85 $ ] and @xmath86 $ ] . for @xmath85",
    "$ ] , we notice the inequality$ ] is the expected number of tables @xmath44 in the @xmath118 restaurant .",
    "the inequality holds by the property of crp : the expected number of tables grows not linearly but logarithmically with the number of customers . ] : @xmath85 \\neq n\\mathbb{e}[s_{kk'}^{n}]$ ] .",
    "thus we compute it as follows : @xmath119 & \\approx \\mathbb{g}[\\alpha\\pi_{k'}]q(c^{(n)}_{kk'}>0 ) ( \\psi(\\mathbb{g}[\\alpha\\pi_{k ' } ] + \\mathbb{e}_+[c^{(n)}_{kk ' } ] ) - \\psi(\\mathbb{g}[\\alpha\\pi_{k ' } ] ) ) \\label{s_1 } \\\\",
    "q(c^{(n)}_{kk'}>0 ) & = 1-q(c^{(n)}_{kk'}=0 ) = 1- \\exp \\",
    "{ n \\log q(c^n_{kk'}=0 ) \\ } \\label{s_2 } \\\\",
    "q(c^n_{kk'}=0 ) & \\approx \\exp \\{\\textstyle \\sum_t \\log ( 1 - q((z^n_{t-1},z^n_t ) = ( k , k ' ) ) ) \\ } \\label{s_3 } \\\\ \\mathbb{e}_+[c^{(n)}_{kk ' } ] & \\approx n\\mathbb{e}[c^{n}_{kk'}]/q(c^{(n)}_{kk'}>0 ) \\label{s_4}\\end{aligned}\\ ] ]",
    "the approximation in ( [ s_1 ] ) comes from the technique proposed by teh et al .",
    "detailed in @xcite . in ( [ s_2 ] ) , @xmath120 denotes the probability of at least one transition from state @xmath43 to state @xmath44 ; and the second equality comes from the fact that @xmath121 is repeated @xmath35 times under exactly the same distribution . in ( [ s_3 ] ) and ( [ s_4 ] ) , we propose a fast approximate method .",
    "we partition a hidden sequence @xmath121 into a set of overlapping but independent clusters @xmath122 .",
    "allowing to overlap is sufficient to preserve all the pairwise transition information , while making the independence assumption permits the above linear computations as in @xcite .",
    "the same strategy applies to computing @xmath86 $ ] .",
    "@xmath123 & \\approx q(c^{(n)}_{k\\cdot}>0 ) ( \\psi(\\mathbb{e}[\\alpha ] ) - \\psi(\\mathbb{e}[\\alpha ] + \\mathbb{e}_+[c^{(n)}_{k\\cdot } ] ) ) \\\\",
    "q(c^{(n)}_{k\\cdot}>0 ) & = 1-q(c^{(n)}_{k\\cdot}=0 ) = 1- \\exp \\",
    "{ n \\log q(c^n_{k\\cdot}=0 ) \\ } \\label{eta_2 } \\\\",
    "q(c^n_{k\\cdot}=0 ) & \\approx \\exp \\{\\textstyle \\sum_t \\log ( 1 - q((z^n_{t-1 } ) = ( k ) ) ) \\ } \\label{eta_3 } \\\\ \\mathbb{e}_+[c^{(n)}_{k\\cdot } ] & \\approx n\\mathbb{e}[c^{n}_{k\\cdot}]/q(c^{(n)}_{k\\cdot}>0 ) \\label{eta_4}\\end{aligned}\\ ] ]",
    "in this section , we present our scvi algorithm for hdp - hmms .    randomly initialize @xmath95 $ ] and @xmath73 $ ] update @xmath69 by eqs .",
    "( [ scvihmm1 - 1],[scvihmm1 - 2 ] ) update @xmath95,\\mathbb{e}[t_{k'}(\\textbf{x},\\textbf{z})]$ ] by eqs .",
    "( [ scvihmm2 - 1],[scvihmm2 - 2 ] ) update @xmath124 by eqs .",
    "( [ scvihdp2],[scvihdp4],[scvihdp6 ] )"
  ],
  "abstract_text": [
    "<S> stochastic variational inference for collapsed models has recently been successfully applied to large scale topic modelling . in this paper </S>",
    "<S> , we propose a stochastic collapsed variational inference algorithm in the sequential data setting . </S>",
    "<S> our algorithm is applicable to both finite hidden markov models and hierarchical dirichlet process hidden markov models , and to any datasets generated by emission distributions in the exponential family . </S>",
    "<S> our experiment results on two discrete datasets show that our inference is both more efficient and more accurate than its uncollapsed version , stochastic variational inference . </S>"
  ]
}