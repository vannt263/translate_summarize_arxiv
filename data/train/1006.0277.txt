{
  "article_text": [
    "we consider recovering a vector @xmath0 in @xmath1 from corrupted measurements @xmath2 , where @xmath3(@xmath4 ) is the coding matrix and @xmath5 is an arbitrary and unknown vector of errors .",
    "obviously , if the fraction of the corrupted entries is too large , there is no hope of recovering @xmath0 from @xmath16 .",
    "however , if the fraction of corrupted measurements is small enough , one can actually recover @xmath0 from @xmath2 .",
    "as the sparsity of @xmath5 is represented by the @xmath17 norm , @xmath18 , one natural way is to find a vector @xmath8 such that the number of terms where @xmath19 and @xmath20 differ is minimized .",
    "mathematically , we solve the following @xmath17-minimization problem : @xmath21 however , ( [ eqn : l0 ] ) is combinatorial and computationally intractable , and one commonly used approach is to solve a closely related @xmath13-minimization problem :    @xmath22    where @xmath23 .",
    "( [ eqn : l1 ] ) can be recast as a linear program , thus can be solved efficiently .",
    "conditions under which ( [ eqn : l1 ] ) can successfully recover @xmath0 have been extensively studied in the literature of compressed sensing ( @xcite ) .",
    "for example , @xcite gives a sufficient condition known as the restricted isometry property ( rip ) .",
    "recently , there has been great research interest in recovering @xmath0 by @xmath6-minimization for @xmath24 ( @xcite ) as follows , @xmath25 recall that @xmath26 for @xmath27 .",
    "we say @xmath0 can be recovered by @xmath6-minimization if and only if it is the unique solution to ( [ eqn : lp ] ) .",
    "then the question is what is the relationship between the sparsity of the error vector and the successful recovery with @xmath6-minimization ? ( [ eqn : lp ] ) is non - convex , and thus it is generally hard to compute the global minimum .",
    "however , @xcite shows numerically that we can recover @xmath0 by finding a local minimum of ( [ eqn : lp ] ) , and @xmath6-minimization outperforms @xmath13-minimization in terms of the sparsity restriction for @xmath5 .",
    "@xcite extends rip to @xmath6-minimization and analyzes the ability of @xmath6-minimization to recover signals from noisy measurements .",
    "@xcite also provides a condition for the success recovery via @xmath6-minimization , which can be generalized to @xmath28 case .",
    "both conditions are sufficient but not necessary , and thus are too restrictive in general .",
    "let @xmath29 be an arbitrary and unknown vector of errors on support @xmath30 .",
    "we say @xmath5 is @xmath31-sparse if @xmath32 for some @xmath33 where @xmath34 is the cardinality of set @xmath35 .",
    "our main contribution is a sharp threshold @xmath36 for all @xmath37 such that for @xmath38 , if @xmath39 for some constant @xmath40 and the entries of @xmath41 are i.i.d .",
    "gaussian , then @xmath6-minimization can recover @xmath0 with overwhelming probability .",
    "we provide two thresholds : one ( @xmath42 ) is for the case when @xmath5 is an arbitrary unknown vector , and the other ( @xmath43 ) assumes that @xmath5 has fixed support and fixed signs . in the latter case ,",
    "the condition of successful recovery with @xmath13-minimization from any possible error vector is the same , while the condition of successful recovery with @xmath6-minimization ( @xmath24 ) from different error vectors differs . using worst - case performance as criterion",
    ", we prove that though @xmath6 outperforms @xmath13 in the former case , it is not comparable to @xmath13 in the latter case .",
    "both bounds @xmath42 and @xmath43 are tight in the sense that once the fraction of errors exceeds @xmath42 ( or @xmath44 ) , @xmath6-minimization can be made to fail with overwhelming probability .",
    "our technique stems from @xcite , which only focuses on @xmath13-minimization and the case that @xmath5 is arbitrary .",
    "in this section , we shall give a function @xmath36 such that for a given @xmath10 , for any @xmath45 , when the entries of @xmath41 are i.i.d .",
    "gaussian , the @xmath6-minimization can recover @xmath0 with overwhelming probability as long as the error @xmath5 is @xmath31-sparse .",
    "the following theorem gives an equivalent condition for the success of @xmath6 minimization (  @xcite ) .",
    "[ thm : slp ] @xmath0 is the unique solution to @xmath6 minimization problem @xmath46 for every @xmath0 and for every @xmath47-sparse @xmath5 if and only if @xmath48 for every @xmath49 , and every support @xmath35 with @xmath50 .",
    "one important property is that if the condition ( [ eqn : slp ] ) is satisfied for some @xmath7 , then it is also satisfied for all @xmath51 ( @xcite ) .",
    "now we define the threshold of successful recovery @xmath42 as a function of @xmath10 .",
    "let @xmath52 , @xmath53, ... ,@xmath54 be i.i.d @xmath55 random variables and let @xmath56 , @xmath57, ... ,@xmath58 be the sorted ordering ( in non - increasing order ) of @xmath59 , @xmath60, ...",
    ",@xmath61 for some @xmath62 $ ] . for a @xmath63 , define @xmath64 as @xmath65 . let @xmath66 denote",
    "@xmath67 $ ] , the expected value of @xmath68 .",
    "then there exists a constant @xmath36 such that @xmath69}{s}=\\frac{1}{2}$ ] .",
    "let @xmath70 and let @xmath71 .",
    "let @xmath72 denote the p.d.f . of @xmath73 and @xmath74 be its c.d.f .",
    "define @xmath75 .",
    "@xmath76 is continuous and decreasing in @xmath77 $ ] , and @xmath78=\\frac{s}{m}$ ] , @xmath79",
    ". then there exists @xmath80 such that @xmath81 , we claim that @xmath82 has the desired property .",
    ". then @xmath84=mg(z^*)$ ] .",
    "since @xmath85 $ ] is bounded by @xmath86 , and @xmath87 , thus @xmath88}{s}=\\frac{1}{2}.$ ]    [ prop : rho ] the function @xmath36 is strictly decreasing in @xmath10 on @xmath89 $ ] .    from the definition of @xmath80 and @xmath36 , we have @xmath90 and @xmath91 where @xmath92 and @xmath93 are the p.d.f . and",
    "c.d.f . of @xmath94 ,",
    "@xmath95 .    from the implicit function theorem , @xmath96    from the chain rule ,",
    "we know @xmath97 , thus @xmath98    note the numerator of ( [ eqn : drhodp ] ) is less than 0 from ( [ eqn : zp ] ) , thus @xmath99 .",
    "we plot @xmath42 against @xmath10 numerically in fig .",
    "[ fig : rho ] .",
    "@xmath36 goes to @xmath100 as @xmath10 tends to zero .",
    "note that @xmath101 , which coincides with the result in @xcite .     of successful recovery with @xmath6-minimization",
    "]    now we proceed to prove that @xmath42 is the threshold of successful recovery with @xmath6 minimization for @xmath10 in @xmath89 $ ] .",
    "first we state the concentration property of @xmath64 in the following lemma .",
    "[ lemma : srho ] for any @xmath62 $ ] , let @xmath52, ...",
    ",@xmath54 , @xmath56, ... ,@xmath58 , @xmath64 and @xmath66 be as above .",
    "for any @xmath102 and any @xmath103 , there exists a constant @xmath104 such that when @xmath105 is large enough , with probability at least @xmath106 , @xmath107| \\leq \\delta s$ ] .",
    "let @xmath108^t$ ] . if two vectors @xmath109 and @xmath110 only differ in co - ordinate @xmath111 , then for any @xmath10 , @xmath112 since @xmath113 for all @xmath114 $ ] , latexmath:[\\[\\label{eqn : srho }     from the isoperimetric inequality for the gaussian measure ( @xcite ) , for any set @xmath41 with measure at least a half , the set @xmath116 has measure at least @xmath117 , where @xmath118 .",
    "let @xmath119 be the median value of @xmath120 .",
    "define set @xmath121 , then @xmath122\\geq 1-e^{-t^2/2}.\\ ] ] we claim that @xmath123 implies that @xmath124 . if @xmath125 , then @xmath126 , thus the claim holds as @xmath127 is non - negative .",
    "if @xmath128 , then there exists @xmath129 such that @xmath130 . let @xmath131 for all @xmath111 and let @xmath132 . from hlder s inequality @xmath133    from ( [ eqn : srho ] ) and ( [ eqn : holder ] ) , @xmath134 . since @xmath135 and @xmath129 , then @xmath136 .",
    "thus @xmath137 , which verifies our claim",
    ". then @xmath138\\geq pr[d(x , a ) \\leq t]\\geq 1-e^{-t^2/2}.\\ ] ] similarly , @xmath139\\geq 1-e^{-t^2/2}.\\ ] ] combining ( [ eqn : srholb ] ) and ( [ eqn : srhoub ] ) , @xmath140\\leq 2e^{-t^2/2}.\\ ] ]    the difference of @xmath141 $ ] and @xmath142 can be bounded as follows , @xmath143 dy\\\\ & \\leq & \\int_0^\\infty 2e^{-\\frac{1}{2}y^{\\frac{2}{p}}m^{(1-\\frac{2}{p } ) } } dy\\\\ & = & m^{(1-\\frac{p}{2})}\\int_0^\\infty 2e^{-\\frac{1}{2}s^{\\frac{2}{p}}}ds\\\\ \\ ] ]    note that @xmath144 is a finite constant for all @xmath62 $ ] .",
    "as @xmath27 and @xmath145 $ ] , thus for any @xmath146 , @xmath147 when @xmath105 is large enough .",
    "let @xmath148)^{\\frac{1}{p}}\\sqrt{m}$ ] , from ( [ eqn : srhomrho ] ) with probability at least ( @xmath149)^{\\frac{2}{p}}m}$ ] ) , @xmath150 .",
    "thus @xmath107| \\leq    @xmath106 for some constant @xmath151 .",
    "[ cor : srho ] for any @xmath152 , there exists a @xmath103 and a constant @xmath153 such that when @xmath105 is large enough , with probability @xmath154 , @xmath155 .    when @xmath152 , @xmath156&=&e[s_{\\rho^*}]-\\sum \\limits_{i=\\lceil",
    "\\rho m\\rceil+1}^{\\lceil \\rho^*m \\rceil } e[|x_i|^p]\\\\ & \\leq & e[s_{\\rho^*}]-(\\lceil \\rho^*m \\rceil-\\lceil \\rho m\\rceil ) e[|x_i|^p]\\end{aligned}\\ ] ] then @xmath157/s \\leq \\frac{1}{2}-2\\delta$ ] for a suitable @xmath158 as @xmath159 $ ]",
    ". the result follows by combining the above with lemma [ lemma : srho ] .",
    "[ cor : s1 ] for any @xmath160 , there exists a constant @xmath161 such that when @xmath105 is large enough , with probability @xmath162 , it holds that @xmath163 .",
    "the above two corollaries indicate that with overwhelming probability the sum of the largest @xmath164 terms of @xmath165 s is less than half of the total sum @xmath68 if @xmath166 .",
    "the following lemma extends the result to every vector @xmath167 where matrix @xmath3 has i.i.d .",
    "gaussian entries and @xmath168 is any vector in @xmath1 .",
    "[ lemma : slp ] for any @xmath7 , given any @xmath169 , there exist constants @xmath170 , @xmath171 , @xmath146 such that when @xmath172 and @xmath173 is large enough , with probability @xmath174 , an @xmath175 matrix @xmath41 with i.i.d .",
    "@xmath55 entries has the following property : for every @xmath49 and every subset @xmath176 with @xmath32 , @xmath177 .    for any given @xmath178",
    ", there exists a @xmath179-net @xmath180 of cardinality less than @xmath181(@xcite ) .",
    "a @xmath179-net @xmath180 is a set of points such that @xmath182 for all @xmath183 in @xmath180 and for any @xmath168 with @xmath184 , there exists some @xmath183 such that @xmath185 .",
    "since @xmath41 has i.i.d @xmath55 entries , then @xmath186 has @xmath105 i.i.d .",
    "@xmath55 entries . applying a union bound to corollary [ cor : srho ] and [ cor : s1 ] , we know that for some @xmath103 and for every @xmath187 , with probability @xmath188 for some @xmath189 , we have @xmath190 and @xmath191 hold for a vector @xmath183 in @xmath180 . taking @xmath192 for large enough @xmath170 , from union",
    "bound we get that ( [ prop : srho ] ) and ( [ prop : s1 ] ) hold for all the points in @xmath180 at the same time with probability at least @xmath174 for some @xmath193 .    for any @xmath168 such that @xmath184 , there exists @xmath194 in @xmath180 such that @xmath195 .",
    "let @xmath196 denote @xmath197 , then @xmath198 for some @xmath199 in @xmath180 .",
    "repeating this process , we have @xmath200 where @xmath201 , @xmath202 and @xmath203 .    thus for any @xmath49",
    ", we have @xmath204 . for any index set @xmath35 with @xmath32 ,    @xmath205    @xmath206",
    "thus @xmath207 .",
    "for a given @xmath158 , we can pick @xmath208 and @xmath209 small enough such that @xmath210 .",
    "we can now establish one main result regarding the threshold of successful recovery with @xmath6-minimization .    for any @xmath7 , given any @xmath169 , there exist constants @xmath170 , @xmath211 such that when @xmath172 and @xmath173 is large enough , with probability @xmath174 , an @xmath212 matrix @xmath41 with i.i.d .",
    "@xmath55 entries has the following property : for every @xmath213 and every error @xmath5 with its support @xmath35 satisfying @xmath32 , @xmath0 is the unique solution to the @xmath6-minimization problem ( [ eqn : lp ] ) .",
    "lemma [ lemma : slp ] indicates that @xmath214 for every non - zero @xmath168 , then from theorem [ thm : slp ] , @xmath0 is the unique solution to the @xmath6-minimization problem ( [ eqn : lp ] ) .",
    "we remark here that @xmath42 is a sharp bound for successful recovery . for any @xmath215 , from lemma [ lemma : srho ] , with overwhelming probability the sum of the largest @xmath216 terms of @xmath217 s is more than the half of the total sum @xmath68 , then theorem [ thm : slp ] indicates that the @xmath6-recovery fails in this case .",
    "in fact , for any vector @xmath218 , let @xmath219 , and let @xmath35 be the support of the largest @xmath220 terms of @xmath217 s .",
    "if the error vector @xmath5 agrees with @xmath217 on the support @xmath35 and is zero elsewhere , then with large probability @xmath221 is no greater than that of @xmath222 , which implies that @xmath6-minimization can not correctly return @xmath0 .",
    "proposition [ prop : rho ] thus implies that the threshold strictly decreases as @xmath10 increases .",
    "the performance of @xmath223-minimization is better than @xmath224-minimization for @xmath225 in the sense that the sparsity requirement for the arbitrary error vector is less strict for smaller @xmath10 .",
    "in section [ sec : sbd ] , for some @xmath102 , we call @xmath6-minimization successful if and only if it can recover @xmath0 from any error @xmath5 whose support size is at most @xmath47 . here",
    "we only require @xmath6-minimization to recover @xmath0 from errors with fixed but unknown support and signs .",
    "we will provide a sharp threshold @xmath43 of the proportion of errors below which @xmath6-minimization is successful .",
    "once the support and the signs of an error vector is fixed , the condition of successful recovery with @xmath13-minimization from any such error vector is the same , however , the condition of successful recovery with @xmath6-minimization from different error vectors differs even the support and the signs of the error is fixed . here",
    "we consider the worst case scenario in the sense that the recovery with @xmath6-minimization is defined to be `` successful '' if @xmath0 can be recovered from any such error @xmath5 .",
    "we characterize this case in theorem [ thm : wb ] .",
    "note that if there is further constraint on @xmath5 , then the condition of successful recovery with @xmath6-minimization may be different from the one stated in theorem [ thm : wb ] .",
    "[ thm : wb ] given any @xmath226 , for every @xmath213 and every error @xmath5 with fixed support @xmath35 and fixed sign for each entry @xmath227 , if @xmath0 is always the unique solution to @xmath6-minimization problem ( [ eqn : lp ] ) , then @xmath228 .",
    "let @xmath231 for every @xmath111 in @xmath232 , let @xmath233 for every @xmath111 in @xmath234 .",
    "for every @xmath111 in @xmath235 , let @xmath236 satisfy @xmath237 . as @xmath226 , we can pick @xmath236 ( @xmath238 ) with @xmath239 large enough such that @xmath240 thus @xmath241 , @xmath0 is not a solution to ( [ eqn : lp ] ) , which is a contradiction .",
    "the first inequality holds as for each @xmath111 in @xmath245 , @xmath246 has the same sign as that of @xmath236 if not zero ; and for @xmath226 , @xmath247 holds .",
    "the second inequality comes from the assumption that @xmath248>@xmath249 for all @xmath242 .",
    "[ lemma : rhow ] let @xmath52 , @xmath53, ... ,@xmath54 be i.i.d .",
    "@xmath55 random variables and @xmath35 be a set of indices with size @xmath250 for some @xmath102 .",
    "let @xmath29 be any vector on support @xmath35 with fixed signs for each entry .",
    "if @xmath251 , for every @xmath252 , when @xmath105 is large enough , with probability @xmath253 for some constant @xmath254 , the following two properties hold :        define a random variable @xmath258 for each @xmath111 in @xmath35 that is equal to 1 if @xmath259 and equal to 0 otherwise . then @xmath260 .",
    "@xmath261=\\frac{1}{2}\\mu$ ] for every @xmath111 in @xmath35 as @xmath262 . from chernoff bound , for any @xmath160 , there exist @xmath263 and @xmath264 such that            lemma [ lemma : rhow ] implies that @xmath272 . applying the similar net argument in section [ sec : sbd ]",
    ", we can extend the result to every vector @xmath167 where matrix @xmath3 has i.i.d .",
    "gaussian entries and @xmath168 is any vector in @xmath1",
    ". then we can establish the main result regarding the threshold of successful recovery with @xmath6-minimization from errors with fixed support and signs .    for any @xmath226 , given any @xmath273",
    ", there exist constants @xmath274 , @xmath275 such that when @xmath276 and @xmath173 is large enough , with probability @xmath277 , an @xmath212 matrix @xmath41 with i.i.d .",
    "@xmath55 entries has the following property : for every @xmath213 and every error @xmath5 with fixed support @xmath35 satisfying @xmath32 and fixed signs on @xmath35 , @xmath0 is the unique solution to the @xmath6-minimization problem ( [ eqn : lp ] ) .",
    "we remark here that @xmath44 is a sharp bound for successful recovery in this setup . for any @xmath291 , from lemma [ lemma : rhow ] , with overwhelming probability that @xmath292 , then theorem [ thm : wb ] indicates that the @xmath6-recovery fails for some error vector @xmath5 in this case .",
    "surprisingly , the successful recovery threshold @xmath42 when fixing the support and the signs of an error vector is @xmath11 for all @xmath10 in @xmath12 and is strictly less than the threshold for @xmath293 , which is 1 ( @xcite ) .",
    "thus in this case , @xmath13-minimization has better recovery performance than that of @xmath6-minimization ( @xmath24 ) in terms of the sparsity requirement for the error vector .",
    "the result seems counterintuitive , however , it largely depends on the definition of successful recovery in terms of worse case performance .",
    "the condition of successful recovery via @xmath13-minimization from any error vector on the fixed support with fixed signs is the same , while the condition of @xmath6-minimization from different error vectors differs .",
    "s.  foucart and m .- j .",
    "lai , `` sparsest solutions of underdetermined linear systems via @xmath296-minimization for @xmath297 , '' _ applied and computational harmonic analysis _ , vol .",
    "26 , no .  3 , pp . 395  407 , 2009 ."
  ],
  "abstract_text": [
    "<S> an unknown vector @xmath0 in @xmath1 can be recovered from corrupted measurements @xmath2 where @xmath3(@xmath4 ) is the coding matrix if the unknown error vector @xmath5 is sparse . </S>",
    "<S> we investigate the relationship of the fraction of errors and the recovering ability of @xmath6-minimization ( @xmath7 ) which returns a vector @xmath8 minimizing the `` @xmath6-norm '' of @xmath9 . </S>",
    "<S> we give sharp thresholds of the fraction of errors that determine the successful recovery of @xmath0 . </S>",
    "<S> if @xmath5 is an arbitrary unknown vector , the threshold strictly decreases from 0.5 to 0.239 as @xmath10 increases from 0 to 1 . </S>",
    "<S> if @xmath5 has fixed support and fixed signs on the support , the threshold is @xmath11 for all @xmath10 in @xmath12 , while the threshold is 1 for @xmath13-minimization .    </S>",
    "<S> # 1@xmath14**#1**@xmath15 </S>"
  ]
}