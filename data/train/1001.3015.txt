{
  "article_text": [
    "a considerable amount of research effort has been devoted to deterministic receding horizon control , see , for example , @xcite and references therein .",
    "currently we have readily available conclusive proofs of successive feasibility and stability of receding horizon control laws in the noise - free deterministic setting .",
    "these techniques can be readily extended to the robust case , i.e. , whenever there is an additive noise of bounded nature entering the system .",
    "the counterpart for stochastic systems subject to process noise and imperfect state measurements and bounded control inputs , however , is still missing .",
    "the principal obstacle is posed by the fact that it may not be possible to determine an a priori bound on the support of the noise , e.g. , whenever the noise is additive and gaussian .",
    "this extra ingredient complicates both the stability and feasibility proofs manifold : the noise eventually drives the state outside any bounded set no matter how large the latter is taken to be , and employing any standard linear state feedback means that any hard bounds on the control inputs will eventually be violated .    in this article",
    "we propose a solution to the general receding horizon control problem for linear systems with noisy process dynamics , imperfect state information , and bounded control inputs .",
    "both the process and measurement noise sequences are assumed to enter the system in an additive fashion , and we require that the designed control policies satisfy hard bounds .",
    "periodically at times @xmath0 , where @xmath1 is the control horizon , a certain finite - horizon optimal control problem is solved for a prediction horizon @xmath2 .",
    "the underlying cost is the standard expectation of the sum of cost - per - state functions that are quadratic in the state and control inputs @xcite .",
    "we also impose some variance - like bounds on the predicted future states and inputs  this is one possible way to impose soft state - constraints that are in spirit similar to integrated chance - constraints , e.g. , in @xcite .",
    "there are several key challenges that are inherent to our setup .",
    "first , since state - information is incomplete and corrupt , the need for a filter to estimate the state from the corrupt state measurements naturally presents itself .",
    "second , it is clear that in the presence of unbounded ( e.g. , gaussian ) noise , it is not in general possible to ensure any bound on the control values with linear state feedback alone ( assuming complete state information is available)the additive nature of the noise ensures that the states exit from any fixed bounded set at some time almost surely .",
    "we see at once that nonlinear feedback is essential , and this issue is further complicated by the fact that only incomplete and imperfect state - information is available .",
    "third , it is unclear whether applicatino of the control policies stabilizes the system in any reasonable sense .    in the backdrop of these challenges ,",
    "let us mention the main contributions of this article .",
    "we    1 .",
    "provide a _ tractable _",
    "method for designing , in a receding horizon fashion , bounded causal nonlinear feedback policies , and 2 .",
    "guarantee that applying the designed control policies in a receding horizon fashion renders the state of the closed - loop system _ mean - square bounded for any initial state statistics_.    we elaborate on the two points above :    1 .",
    "_ tractability : _ given a subclass of causal policies , we demonstrate that the underlying optimal control problem translates to a convex optimization problem to be solved every @xmath1 time steps , that this optimization problem is feasible for any statistics of the initial state , and equivalent to a quadratic program .",
    "the subclass of polices we employ is comprised of open - loop terms and nonlinear feedback from the innovations . under the assumption that the process and measurement noise is gaussian , ( even though the bounded inputs requirement makes the problem inherently nonlinear and it may appear that standard kalman filtering may not apply , ) it turns out that kalman filtering techniques can indeed be utilized .",
    "we provide a low - complexity algorithm ( essentially similar to standard kalman filtering ) for updating this conditional density , and report tractable solutions for the _ off - line _ computation of the time - dependent optimization parameters",
    "stability : _ it is well known that for noise - free discrete - time linear systems it is not possible to globally asymptotically stabilize the system , if the usual matrix @xmath3 has unstable eigenvalues , see , for example , @xcite and references therein . moreover , in the presence of stochastic process noise the hope for achieving asymptotic stability is obviously not realistic .",
    "therefore , we naturally relax the notion of stability into mean - square boundedness of the state and impose the extra conditions that the system matrix @xmath3 is lyapunov ( or neutrally ) stable and that the pair @xmath4 is stabilizable . under such standard assumptions ,",
    "we show that it is possible to augment the optimization problem with an extra _ stability constraint _ , and , consequently , that the successive application of our resulting policies renders the state of the overall system ( plant and estimator ) mean - square bounded .",
    "the research on stochastic receding horizon control is broadly subdivided into two parallel lines : the first treats multiplicative noise that enters the state equations , and the second caters to additive noise .",
    "the case of multiplicative noise has been treated in some detail in @xcite , where the authors consider also soft constraints on the state and control input . however , in this article we exclusively focus on the case of additive noise .",
    "the approach proposed in this article stems from and generalizes the idea of affine parametrization of control policies for finite - horizon linear quadratic problems proposed in @xcite , utilized within the robust mpc framework in @xcite for full state feedback , and in @xcite for output feedback with gaussian state and measurement noise inputs .",
    "more recently , this affine approximation was utilized in @xcite for both the robust deterministic and the stochastic setups in the absence of control bounds , and optimality of the affine policies in the scalar deterministic case was reported in @xcite . in @xcite",
    "the authors reformulate the stochastic programming problem as a deterministic one with bounded noise and solve a robust optimization problem over a finite horizon , followed by estimating the performance when the noise can take unbounded values , i.e. , when the noise is unbounded , but takes high values with low probability .",
    "similar approach was utilized in @xcite as well .",
    "there are also other approaches , e.g. , those employing randomized algorithms as in @xcite .",
    "results on obtaining lower bounds on the value functions of the stochastic optimization problem have been recently reported in @xcite .",
    "the remainder of this article is organized as follows . in section [ sec : problem ] we formulate the receding horizon control problem with all the underlying assumptions , the construction of the estimator , and the main optimization problems to be solved . in section [ sec : mainresult ] we provide the main results pertaining to tractability of the optimization problems and mean - square boundedness of the closed - loop system .",
    "we comment on the obtained results and provide some extensions in section [ sec : discussion ] .",
    "we provide all the needed preliminary results , derivations , and proofs in section [ sec : proofs ] , and some numerical examples in section [ sec : examples ] .",
    "finally , we conclude in section [ sec : conclusions ] .",
    "let @xmath5 be a general probability space , we denote the conditional expectation given the sub-@xmath6 algebra @xmath7 of @xmath8 as @xmath9 $ ] .",
    "for any random vector @xmath10 we let @xmath11 $ ] .",
    "hereafter we let @xmath12 and @xmath13 be the sets of positive and nonnegative integers , respectively .",
    "we let @xmath14 denote the trace of a square matrix , @xmath15 denote the standard @xmath16 norm , and simply @xmath17 denote the @xmath18-norm . in a euclidean space we denote by @xmath19 the closed euclidean ball or radius @xmath20 centered at the origin . for any two matrices @xmath3 and @xmath21 of compatible dimensions , we denote by @xmath22 the @xmath23-th step reachability matrix @xmath24}$ ] . for any matrix @xmath25 ,",
    "we let @xmath26 and @xmath27 be its minimal and maximal singular values , respectively .",
    "we let @xmath28 denote the sub - matrix obtained by selecting the rows @xmath29 through @xmath30 of @xmath25 , and let @xmath31 denote its @xmath32-th row .",
    "we are given a certain plant with discrete - time dynamics , process noise @xmath33 , and imperfect state measurements @xmath34 tainted through noise @xmath35 ( figure [ fig : main ] ) .",
    "the objective is to design a receding horizon controller which renders the overall closed - loop system mean - square bounded .",
    "we discuss the structure and the main assumptions on the dynamics of the system , the performance index to be minimized , and the construction of the optimal mean - square estimator @xmath36 in subsection [ sec : dynamics ] .",
    "then , we formalize the optimization problem to be solved in a receding horizon fashion ( to generate the input policies @xmath37 ) in subsection [ sec : optimization ] .",
    "consider the following affine discrete - time stochastic dynamical model :    [ eq : system ] @xmath38    where @xmath39 , @xmath40 is the state , @xmath41 is the control input , @xmath42 is the output , @xmath43 is a random process noise , @xmath44 is a random measurement noise , and @xmath45 , @xmath46 , and @xmath47 are known matrices .",
    "we posit the following main assumption :    [ ass : dynamics ]    1 .",
    "the system matrices in satisfy the following:[ass : dynamics1 ] 1 .",
    "the pair @xmath48 is stabilizable .",
    "[ ass : dynamics1a ] 2 .",
    "@xmath45 is discrete - time lyapunov stable ( * ? ? ?",
    "* chapter 12 ) , i.e. , the eigenvalues @xmath49 lie in the closed unit disc , and those eigenvalues @xmath50 with @xmath51 have equal algebraic and geometric multiplicities .",
    "[ ass : dynamics1c ] 2 .   the initial condition and the process and measurement noise vectors",
    "are normally distributed , i.e. , [ ass : distributions ] @xmath52 , @xmath53 , and @xmath54 .",
    "moreover , @xmath55 , @xmath56 and @xmath57 are mutually independent .",
    "the control inputs take values in the control constraint set:[ass : dynamics6 ] @xmath58 i.e. , @xmath59 for each @xmath60 .    without any loss of generality",
    ", we also assume that @xmath45 is in real jordan canonical form .",
    "indeed , given a linear system described by system matrices @xmath61 , there exists a coordinate transformation in the state - space that brings the pair @xmath61 to the pair @xmath48 , where @xmath45 is in real jordan form @xcite . in particular , choosing a suitable ordering of the jordan blocks , we can ensure that the pair @xmath62 has the form @xmath63 , where @xmath64 is schur stable , and @xmath65 has its eigenvalues on the unit circle . by hypothesis [ ass : dynamics1]-[ass : dynamics1c ] of assumption [ ass : dynamics ] , @xmath66 is therefore block - diagonal with elements on the diagonal being either @xmath67 or @xmath68 rotation matrices . as a consequence , @xmath66 is orthogonal .",
    "moreover , since @xmath69 is stabilizable , the pair @xmath70 must be reachable in a number of steps @xmath71 that depends on the dimension of @xmath66 and the structure of @xmath70 , i.e. , @xmath72 . summing up",
    ", we can start by considering that the state equation has the form @xmath73 where @xmath74 is schur stable , @xmath66 is orthogonal , and there exists a nonnegative integer @xmath75 such that the subsystem @xmath70 is reachable in @xmath76 steps .",
    "this integer @xmath76 is fixed throughout the rest of the article .",
    "fix a prediction horizon @xmath77 , and let us consider , at any time @xmath78 , the cost @xmath79 defined by : @xmath80,\\label{eq : totalcost}\\end{aligned}\\ ] ] where @xmath81 is the set of outputs up to time @xmath78 , ( or more precisely the @xmath6-algebra generated by the set of outputs , ) and @xmath82 , @xmath83 , and @xmath84 are some given symmetric matrices of appropriate dimension , @xmath85 . at each time",
    "instant @xmath78 , we are interested in minimizing over the class of causal output feedback strategies @xmath86 defined as : @xmath87}\\!\\ ! = \\!\\!\\left [ \\begin{array}{l } g_t({\\mathcal y}_t ) \\\\ g_{t+1}({\\mathcal y}_{t+1 } ) \\\\ \\vdots \\\\ g_{t+n-1}({\\mathcal y}_{t+n-1})\\end{array}\\right],\\ ] ] for some measurable functions @xmath88 @xmath89 which satisfy the hard constraint on the inputs .",
    "the cost @xmath90 in is a conditional expectation given the observations up through time @xmath78 , and as such requires the conditional density @xmath91 of the state given the previous and current measurements .",
    "our choice of causal control policies in   motivates us to rewrite the cost @xmath90 in   as : @xmath92\\bigg ] .",
    "\\label{e : conditionalversionofcost}\\end{aligned}\\ ] ]    the propagation of @xmath91 in time is not a trivial task in general . in what follows",
    "we shall report an iterative method for the computation of @xmath91 , whenever the control input is a measurable deterministic feedback from the past and present outputs .",
    "for @xmath93 , define @xmath94 $ ] and @xmath95 $ ] .",
    "[ ass : noisematrices ] we require that @xmath96 and @xmath97 .    the following result is a slight extension of the usual kalman filter for which a proof can be found in appendix a. an alternative proof can also be found in @xcite .",
    "[ prop:1 ] under assumption [ ass : dynamics]-[ass : distributions ] and assumption [ ass : noisematrices ] , @xmath91 and @xmath98 are the probability densities of gaussian distributions @xmath99 and @xmath100 , respectively , with @xmath101 and @xmath102 . for @xmath103",
    "their conditional means and covariances can be computed iteratively as follows : @xmath104 where @xmath105    in particular , the matrix @xmath106 together with @xmath107 characterize the conditional density @xmath108 , which is needed in the computation of the cost .",
    "proposition [ prop:1 ] states that the conditional mean and covariances of @xmath109 can be propagated by an iterative algorithm which resembles the kalman filter .",
    "since the system is generally nonlinear due to the function @xmath110 being nonlinear , we can not assume that the probability distributions in the problem are gaussian ( in fact , the a priori distributions of @xmath109 and of @xmath111 are not ) and the proof can not be developed in the standard linear framework of the kalman filter .",
    "hereafter , we shall denote for notational convenience by @xmath112 the estimate @xmath113 , and let @xmath114},\\ ] ] which corresponds to the jordan decomposition in .",
    "having discussed the iterative update of the laws of @xmath115 given the measurements @xmath111 in section [ sec : dynamics ] , we return to our optimization problem in .",
    "we can rewrite our optimization problem as : @xmath116 where the collection of functions @xmath117 were defined following .",
    "the explicit solution via dynamic programming to problem over the class of causal feedback policies @xmath86 , is difficult if not impossible to obtain in general @xcite .",
    "one way to circumvent this difficulty is to restrict our search for feedback policies to a subclass of @xmath86 .",
    "this will result in a suboptimal solution to our problem , but may yield a tractable optimization problem .",
    "this is the track we pursue next .    given a control horizon @xmath1 and a prediction horizon @xmath118 , we would like to periodically solve problem at times @xmath0 over the following class of affine - like control policies @xmath119 where @xmath120 , and for any vector @xmath121 , @xmath122 @xmath123{^\\mathsf{t}}$ ] , where @xmath124 is the @xmath125-th element of the vector @xmath126 and @xmath127 is any function with @xmath128 .",
    "the feedback gains @xmath129 and the affine terms @xmath130 are the decision variables .",
    "the value of @xmath131 in depends on the values of the measured outputs from the beginning of the prediction horizon at time @xmath78 up to time @xmath132 only , which requires a _ finite amount of memory_. note that we have chosen to saturate the measurements we obtain from the vectors @xmath133 before inserting them into the control policy .",
    "this way we do not assume that either the process noise or the measurement noise distributions are defined over a compact domain , which is a crucial assumption in robust mpc approaches @xcite . moreover , the choice of element - wise saturation functions @xmath134 is left open . as such",
    ", we can accommodate standard saturation , piecewise linear , and sigmoidal functions , to name a few .",
    "finally , we collect all the ingredients of the stochastic receding horizon control problem in algorithm [ algo : general ] below .",
    "density @xmath135 set @xmath136 , @xmath137 , and @xmath138    measure @xmath139    update @xmath140 and @xmath141 using - solve the optimization problem with the control policies in to obtain @xmath142 apply @xmath143 calculate @xmath144 and @xmath145 using -    set @xmath146      in order to state the main results in section [ sec : mainresult ] , it is convenient to utilize a more compact notation that describes the evolution of all signals over the entire prediction horizon @xmath2 .",
    "the evolution of the system dynamics - over a single prediction horizon @xmath2 , starting at @xmath78 , can be described in a _ lifted _ form as follows : @xmath147 where @xmath148}$ ] , @xmath149}$ ] , @xmath150}$ ] , @xmath151}$ ] , @xmath152}$ ] , @xmath153}$ ] , @xmath154 } , { \\mathcal d}={\\left[\\begin{matrix }   \\begin{smallmatrix } { 0 } &   \\cdots & \\cdots & { 0}\\\\ { i } & \\ddots & & \\vdots \\\\ { a } & { i}&\\ddots & \\vdots \\\\ \\vdots & & \\ddots & { 0}\\\\ { a}^{n-1 } & \\cdots & { a } & { i}\\end{smallmatrix } \\end{matrix}\\right]},\\ ] ] and @xmath155 .",
    "let @xmath156 be the usual kalman gain and define @xmath157 , and @xmath158 .",
    "then , we can write the estimation error vector as @xmath159 where @xmath160 , @xmath161 }      \\end{tiny}$ ] , @xmath162 }      \\end{tiny } , \\\\      { \\mathcal f}^{{v}}_{t } \\!&=\\!\\ !",
    "\\begin{tiny }      { \\left[\\begin{matrix } \\begin{array}{l|l|l|l|l }      { 0 } & { 0 } & & { 0 } & { 0}\\\\      { 0 } & { k}_{t } &   & { 0}&{0}\\\\      { 0 } & \\phi_{t+1}{k}_{t } & & { 0}&{0}\\\\      \\vdots & \\vdots & \\!\\!\\cdots\\!\\!&\\vdots&\\vdots   \\\\      { 0}&\\phi_{t+n-2}\\cdots\\phi_{t+1}{k}_{t } & & { k}_{t+n-2 } & { 0}\\\\      { 0 } & \\phi_{t+n-1}\\cdots\\phi_{t+1}{k}_{t } & & \\phi_{t+n-1}{k}_{t+n-2 } & { k}_{t+n-1 }     \\end{array } \\end{matrix}\\right ] }      \\end{tiny}\\!\\!.\\end{aligned}\\ ] ] finally , the innovations sequence can be written as @xmath163 where @xmath164 .",
    "consequently , _ the innovations sequence over the prediction horizon is independent of the inputs vector @xmath165_.    the cost function at time @xmath78 can be written as @xmath166 , \\ ] ] where @xmath167 and @xmath168 .",
    "also , the control policy at time @xmath78 is given by @xmath169 where @xmath170}\\ ! , \\varphi({y}_t\\!-\\!\\hat { y}_t)\\!{\\coloneqq}\\!\\!{\\left[\\begin{matrix } \\varphi_0({y}_t\\!-\\!\\hat{y}_t)\\\\ \\vdots \\\\ \\varphi_{n-1}({y}_{t+n-1}\\!-\\!\\hat{y}_{t+n-1 } ) \\end{matrix}\\right]}\\!,\\end{aligned}\\ ] ] and @xmath171 has the following lower block diagonal structure @xmath172}.\\ ] ] since the innovation vector @xmath173 in is not a function of @xmath174 and @xmath171 , the control inputs @xmath165 in remain affine in the decision variables .",
    "this fact is important to show convexity of the optimization problem , as will be seen in the next section .",
    "finally , the constraint can be rewritten as : @xmath175",
    "the optimization problem we have stated so far , even if it is successively feasible , does not in general guarantee stability of the resulting receding horizon controller in algorithm [ algo : general ] . unlike deterministic stability arguments utilized in mpc ( see , for example , @xcite ) , we _ can not _ assume the existence of a final region that is control positively invariant and which is crucial to establish stability .",
    "this is simply due to the fact that the process noise sequence is not assumed to have a compact domain of support .",
    "however , guided by our earlier results in @xcite , we shall enforce an extra _ stability constraint _ which , if feasible , can render the state of the closed - loop system mean - square bounded .    for @xmath60 , the state estimate at time @xmath176 given the state estimate at time @xmath78 , the control inputs from time @xmath78 through @xmath177 , and the corresponding process and measurement noise sequences can be written as @xmath178 } + \\xi_t,\\ ] ] where we define @xmath179}{\\left[\\begin{matrix } { e}_t\\\\ \\vdots \\\\ { e}_{t+\\kappa-1 } \\end{matrix}\\right]}\\\\ & \\quad + { \\left[\\begin{matrix } { a}^{\\kappa-1}{k}_{t}{c},{a}^{\\kappa-2}{k}_{t+1}{c } , \\cdots , { k}_{t+\\kappa-1}{c}\\end{matrix}\\right]}{\\left[\\begin{matrix } { w}_t\\\\ \\vdots \\\\ { w}_{t+\\kappa-1 } \\end{matrix}\\right]}\\\\ & \\quad     + { \\left[\\begin{matrix } { a}^{\\kappa-1}{k}_{t},{a}^{\\kappa-2}{k}_{t+1 } , \\cdots , { k}_{t+\\kappa-1 } \\end{matrix}\\right]}{\\left[\\begin{matrix } { v}_{t+1}\\\\ \\vdots \\\\ { v}_{t+\\kappa } \\end{matrix}\\right]}. \\end{aligned}\\ ] ]    [ sch : thehorror ] there exists an integer @xmath180 and a positive constant @xmath181 such that @xmath182{\\leqslant}{\\zeta}\\qquad \\text{for all } t{\\geqslant}t.\\ ] ]    a proof of scholium [ sch : thehorror ] appears in section [ sec : proofs ] . using the constant @xmath183 , we now require the following `` drift condition '' to be satisfied : for any given @xmath184 and for every @xmath0 , there exists a @xmath185 , such that the following condition is satisfied @xmath186}}\\right\\rvert}{\\leqslant}\\bigl\\|{{{\\hat{x}}}^{(2)}}_{t}\\bigr\\|-({\\zeta}+ \\tfrac{{\\varepsilon}}{2})\\;\\;\\text{whenever } \\bigl\\|{{{\\hat{x}}}^{(2)}}_{t}\\bigr\\|{\\geqslant}{\\zeta}+{\\varepsilon}.      \\end{aligned}\\ ] ] note that @xmath187 } = ( { \\boldsymbol{\\eta}}_t)_{1:\\kappa m } + ( { \\boldsymbol{\\theta}}_t)_{1:\\kappa m}\\varphi({y}- \\hat{y})$ ] .",
    "( for notational convenience , we have retained @xmath188 with the knowledge that the matrix @xmath189 causally selects the outputs as they become available , see . )",
    "the constraint pertains only to the second subsystem of the estimator , as the first subsystem ( @xmath190 ) is schur stable ( see and ) .",
    "we augment problem with the stability constraint to obtain @xmath191    [ ass : stability1 ] in addition to assumption [ ass : dynamics ] and assumption [ ass : noisematrices ] , we stipulate that :    1 .   the control authority @xmath192 , where @xmath193 .",
    "2 .   the control horizon @xmath194 , where @xmath76 is the reachability index .",
    "@xmath195 is stabilizable , and @xmath196 is observable.[ass : stability1:systemandnoise ]    [ thm : main ] consider the system - , and suppose that assumption [ ass : stability1 ] holds . then :    1 .   for every time @xmath197 ,",
    "the optimization problem is convex , feasible , and can be approximated to the following quadratic optimization problem:[maintheorem : optproblem ] @xmath198 where @xmath199 , @xmath200 , &   { \\lambda^{\\varphi x}}_t & = \\ee_{{\\mathcal y}_t}[\\varphi({y}-\\hat{y}){x}_t{^\\mathsf{t}}],\\\\                  { \\lambda^{w\\varphi}}_t & = \\ee_{{\\mathcal y}_t}[{w}\\varphi({y}-\\hat{y}){^\\mathsf{t } } ] , &   { \\lambda^{\\varphi\\varphi}}_t & = \\ee_{{\\mathcal y}_t}[\\varphi({y}-\\hat{y})\\varphi({y}-\\hat{y}){^\\mathsf{t } } ] .    \\\\",
    "\\end{aligned}\\ ] ] 2 .   for every initial state and noise statistics @xmath201 , successive application of the control laws given by the optimization problem in [ maintheorem : optproblem ] for @xmath1 steps",
    "renders the closed - loop system mean - square bounded in the following sense : there exists a constant @xmath202 such that[maintheorem : msbdd ] @xmath203{\\leqslant}\\gamma.\\ ] ]    in practice , it may be also of interest to impose further some soft constraints both on the state and the input vectors . for example",
    ", one may be interested in imposing quadratic or linear constraints on the state , both of which can be captured in the following @xmath204{\\leqslant}\\alpha_t,\\ ] ] where @xmath205 and @xmath206 .",
    "moreover , expected energy expenditure constraints can be posed as follows @xmath207{\\leqslant}\\beta_t,\\ ] ] where @xmath208 and @xmath209 . in the absence of hard input constraints ,",
    "such expectation - type constraints are commonly used in the stochastic mpc @xcite and in stochastic optimization in the form of integrated chance constraints  @xcite .",
    "this is partly because it is not possible , without posing further restrictions on the boundedness of the process noise @xmath210 , to ensure that hard constraints on the state are satisfied .",
    "for example , in the standard lqg setting nontrivial hard constraints on the system state would generally be violated with nonzero probability . moreover ,",
    "in contrast to chance constraints where a bound is imposed on the probability of constraint violation , expectation - type constraints tend to give rise to convex optimization problems under weak assumptions  @xcite",
    ".    we can also augment problem with the constraints and to obtain @xmath211 notice that the constraints and are not necessarily feasible at time @xmath78 for any choice of parameters @xmath212 and @xmath213 .",
    "as such , problem may become infeasible over time if we simply apply algorithm [ algo : general ] .",
    "we therefore replace the optimization step 7 in algorithm [ algo : general ] with the following subroutine for some given @xmath214 and @xmath215 that make the constraints feasible , precision number @xmath216 and maximal number of iterations @xmath217 :    ' '' ''    ' '' ''    * subroutine 7 *    ' '' ''    1 .",
    "set @xmath218 , @xmath219 , @xmath220 , and @xmath221 2 .",
    "solve the optimization problem using @xmath222 and @xmath223 to obtain the sequence @xmath224 3 .",
    "set @xmath225 4 .",
    "* repeat * 5 .",
    "@xmath226 set @xmath227 and @xmath228 6 .",
    "@xmath226 solve solve the optimization problem using the new @xmath212 and @xmath213 to obtain + @xmath229 the sequence @xmath230 7 .",
    "@xmath226 * if * step 5f is feasible * then * 8 .",
    "@xmath231 set @xmath232 and @xmath233 9 .",
    "@xmath231 set @xmath234 10 .",
    "@xmath226 * else * 11 .",
    "@xmath231 set @xmath235 and @xmath236 12 . @xmath226 * end if * 13 .",
    "@xmath226 set @xmath237 14 .",
    "* until * ( @xmath238 and @xmath239 ) or @xmath240    ' '' ''    it may be argued that replacing step 7 in algorithm [ algo : general ] with subroutine 7 above increases the computational burden ; however , the parameter @xmath241 guarantees that this iterative process is halted after some prespecified number of steps in case the required precision @xmath242 is not reached in the meantime . in some instances , we may be given some fixed parameters @xmath243 and @xmath244 with which the constraints and have to be satisfied , respectively .",
    "this requirement can be easily incorporated into subroutine 7 by setting @xmath245 and @xmath246 in step 7a .",
    "[ ass : stability2 ] at each time step @xmath0 , the constants @xmath247 and @xmath215 in subroutine 7 are chosen as @xmath248+{\\mathcal d}{^\\mathsf{t}}\\mathcal s{\\mathcal d}{\\sigma_{{w}}}\\right ) }              + 3nm\\sigma_{\\max}({\\mathcal b}{^\\mathsf{t}}\\mathcal s{\\mathcal b})u_{\\max}^2 + \\mathcal l{^\\mathsf{t}}{\\mathcal a}\\hat{x}_t+{\\left\\lvert{\\mathcal l{^\\mathsf{t}}{\\mathcal b}}\\right\\rvert}_1u_{\\max},\\\\              \\beta^*_{t } & { \\coloneqq}nm\\sigma_{\\max}(\\tilde{\\mathcal s})u_{\\max}^2 .          \\end{aligned}\\ ] ]    [ cor : main ] consider the system - , and suppose that assumption [ ass : stability1 ] , and [ ass : stability2 ] hold .",
    "then :    1 .   for every time",
    "@xmath0 the optimization problem solved in subroutine 7 is convex , feasible , and equivalent to the following quadratically constrained quadratic optimization problem:[maincorollary : optproblem ] @xmath249\\right)}+{\\mathsf{tr}\\!\\left({\\mathcal d}{^\\mathsf{t}}\\mathcal s{\\mathcal d}{\\sigma_{w}}\\right)}{\\nonumber}\\\\          & \\quad + \\mathcal l{^\\mathsf{t}}{\\mathcal a}\\hat { x}_t { \\leqslant}\\alpha_t,\\label{eq : constraint-2}\\\\          & { \\boldsymbol{\\eta}}_t { ^\\mathsf{t}}\\tilde{\\mathcal s}{\\boldsymbol{\\eta}}_t + 2{\\boldsymbol{\\eta}}_t { ^\\mathsf{t}}\\tilde{\\mathcal s}{\\boldsymbol{\\theta}}_t { \\lambda^{\\varphi}}_t+{\\mathsf{tr}\\!\\left({\\boldsymbol{\\theta}}_t { ^\\mathsf{t}}\\tilde{\\mathcal s}{\\boldsymbol{\\theta}}_t { \\lambda^{\\varphi\\varphi}}_t\\right ) } { \\leqslant}\\beta_t .",
    "\\label{eq : constraint-3}\\end{aligned}\\ ] ] 2 .   for every initial state and noise statistics @xmath201 , successive application of the control laws given by the optimization problem in [ maincorollary : optproblem ] for @xmath1 steps renders",
    "the closed - loop system mean - square bounded in the following sense : there exists a constant @xmath202 such that @xmath250{\\leqslant}\\gamma.\\ ] ]",
    "the optimization problem being solved in theorem [ thm : main ] is a quadratic program ( qp ) , whereas the optimization problem being solved in corollary is a quadratically constrained quadratic program ( qcqp ) in the optimization parameters @xmath251 . as such ,",
    "both can be easily solved via software packages such as ` cvx `  @xcite or ` yalmip ` @xcite .",
    "it is not difficult to see that constraints on the variation of the inputs of the form @xmath252 where @xmath253}\\end{tiny}$ ] , can be incorporated into the optimization problems and .",
    "moreover , we can easily solve the problem using quadratic policies of the form @xmath254 instead of , where @xmath255 } } , \\",
    "\\tilde\\varphi(z){\\coloneqq}{\\tiny { \\left[\\begin{matrix } \\tilde\\varphi_0({y}_t-\\hat{y}_t){^\\mathsf{t}}\\tilde\\varphi_0({y}_t-\\hat{y}_t)\\\\ \\vdots \\\\ \\tilde\\varphi_{n-1}({y}_{t+n-1}-\\hat{y}_{t+n-1}){^\\mathsf{t}}\\tilde\\varphi_{n-1}({y}_{t+n-1}-\\hat{y}_{t+n-1 } ) \\end{matrix}\\right]}},\\ ] ] and @xmath256 .",
    "the underlying optimization problems and with the policy are still convex and both theorem and corollary still apply with minor changes .      at any time @xmath0 ,",
    "our ability to solve the optimization problems and in theorem [ thm : main ] and corollary [ cor : main ] , respectively , hinges upon our ability to compute the following matrices @xmath258 , & { \\lambda^{w\\varphi}}_t & = \\ee_{{\\mathcal y}_t}[{w}\\varphi({y}_t-\\hat{y}_t){^\\mathsf{t}}],\\\\ { \\lambda^{\\varphi}}_t & = \\ee_{{\\mathcal y}_t}[\\varphi({y}_t-\\hat{y}_t ) ] , & { \\lambda^{\\varphi\\varphi}}_t & = \\ee_{{\\mathcal y}_t}[\\varphi({y}_t-\\hat{y}_t)\\varphi({y}_t-\\hat{y}_t){^\\mathsf{t}}],\\\\ { \\lambda^{\\varphi x}}_t & = { \\lambda^{\\varphi e}}_t + { \\lambda^{\\varphi}}_t\\hat{x}_{t}{^\\mathsf{t}}.   & & \\end{aligned}\\ ] ] recall that @xmath173 is the innovations sequence that was given in , and @xmath112 is the optimal mean - square estimate of @xmath115 given the history @xmath111 .",
    "the matrices may be computed by numerical integration with respect to the independent gaussian measures of @xmath259 , of @xmath260 @xmath261 , and of @xmath109 given @xmath262 . due to the large dimensionality of the integration space",
    ", this approach may be impractical for online computations .",
    "one alternative approach relies on the observation that @xmath263 , and @xmath264 depend on @xmath115 via the difference @xmath265 . since @xmath265 is conditionally",
    "zero - mean given @xmath262 , we can write the dependency of   on the time - varying statistics of @xmath109 given @xmath262 as follows : @xmath266 in principle one may compute _ off - line _ and store the matrices @xmath267 , and @xmath268 , which depend on the covariance matrices @xmath106 , and just update online the value of @xmath269 as the estimate @xmath112 becomes available .",
    "however , this poses serious requirements in terms of memory .",
    "a more appealing alternative is to exploit the convergence properties of @xmath106 .",
    "the following result can be inferred , for instance , from ( * ? ? ?",
    "* theorem 5.1 ) .",
    "[ prop : are ] under assumptions [ ass : noisematrices ] and [ ass : stability1]-[ass : stability1:systemandnoise ]    * the ( discrete - time ) algebraic riccati equation in @xmath270 @xmath271{a}{^\\mathsf{t}}+{\\sigma_{w}}\\ ] ] has a unique solution @xmath272 , and * the sequence @xmath273 defined by   and   converges to @xmath274 as @xmath78 tends to @xmath275 , for any initial condition @xmath276 .",
    "the assumption that @xmath277 can be relaxed to @xmath278 at the price of some additional technicality ( more on this can be found in @xcite ) . as a consequence of this result , under detectability and stabilizability assumptions",
    ", @xmath106 converges to @xmath279 which is the asymptotic error covariance matrix of the estimator @xmath280 .",
    "thus , neglecting the initial transient , it makes sense to just compute off - line and store the matrices @xmath281 , and @xmath282 , and just update the matrix @xmath283 for new values of the estimate @xmath112 .",
    "the proofs of the main results are presented as follows : we begin by showing the result in scholium [ sch : thehorror ] .",
    "then , we state a fundamental result pertaining to mean - square boundedness for general random sequences in proposition [ p : pr99 ] , which is utilized to show the mean - square boundedness conclusions of theorem [ thm : main ] and corollary [ cor : main ] . we proceed to show the first assertion [ maintheorem : optproblem ] of theorem [ thm : main ] in lemma [ l : maintheorem : opt ] .",
    "the proof of the second assertion [ maintheorem : msbdd ] of theorem [ thm : main ] starts by showing lemmas [ l : prcondition1 ] and [ l : prcondition2 ] to conclude mean - square boundedness of the orthogonal subsystem @xmath284 .",
    "we conclude the proof of theorem [ thm : main ] by showing mean - square boundedness of the schur stable subsystem @xmath285 .",
    "we end this section by proving the extra conclusions of corollary [ cor : main ] , beyond those present in theorem [ thm : main ] .",
    "let us now look at the estimation equation @xmath286 in and combine it with and the output equation to obtain @xmath287 where @xmath288 is the kalman gain .",
    "our next fact pertains to the boundedness of the error covariance matrices @xmath106 in .",
    "[ fact : factsonbounds ] there exists @xmath289 and @xmath290 such that @xmath291 for all @xmath292 , and @xmath293 for all @xmath292 .",
    "fact [ fact : factsonbounds ] follows , for example , immediately from lemma [ prop : pbounds ] in appendix [ appendixb ] and since by assumption @xmath294 , one possible bound on @xmath295 is given by @xmath296 @xmath297 .",
    "note that there are many alternative bounds in the literature , see , for example , @xcite .",
    "now , using the bounds in fact [ fact : factsonbounds ] , we can proceed to prove scholium [ sch : thehorror ] .",
    "recall the expression of @xmath298 in and define the following quantities : @xmath299 then , @xmath300 can be rewritten as @xmath301 but for @xmath302 , , @xmath303 denotes the smallest integer that upper - bounds @xmath10 .",
    "] we have that @xmath304 using fact [ fact : factsonbounds ] we know that @xmath305 .",
    "thus , it suffices to take @xmath306 in order to upper - bound the expectation of @xmath307 in after time @xmath308 .",
    "the following result pertains to mean - square boundedness of a random sequence @xmath309 :    [ p : pr99 ] let @xmath309 be a sequence of nonnegative random variables on some probability space @xmath310 , and let @xmath311 be any filtration to which @xmath309 is adapted .",
    "suppose that there exist constants @xmath312 , and @xmath313 , such that @xmath314 and for all @xmath60 : @xmath315 { \\leqslant}-b\\quad \\text{on the event } \\{\\xi_t > j\\},\\quad\\text{and}\\label{e : flcond}\\\\              \\ee\\bigl[{\\left\\lvert{\\xi_{t+1 } - \\xi_t}\\right\\rvert}^4\\big|\\xi_0,\\ldots , \\xi_t\\bigr ] { \\leqslant}m.\\label{e : pcond }          \\end{gathered}\\ ] ] then there exists a constant @xmath316 such that @xmath317 { \\leqslant}\\gamma}$ ] .",
    "[ l : maintheorem : opt ] consider the system - , and suppose that assumption [ ass : stability1 ] holds .",
    "then the first assertion [ maintheorem : optproblem ] of theorem [ thm : main ] holds .",
    "it is clear that @xmath318 is convex quadratic in @xmath319 and @xmath165 , and both @xmath319 and @xmath165 are affine functions of the design parameters @xmath320 for every realization of the noise @xmath56 . since taking expectation of a convex function",
    "retains convexity @xcite , we conclude that @xmath321 $ ] is convex in @xmath320 .",
    "also , note that the constraint sets described by and are convex in @xmath320 .",
    "this settles the claim concerning convexity of the optimization program in theorem [ thm : main]-[maintheorem : optproblem ]",
    ".    concerning the objective function , we have that @xmath322\\\\              & \\ ; = \\ee_{{\\mathcal y}_t}\\bigl[\\bigl({\\mathcal a}{x}_t + { \\mathcal b}{u}_t + { \\mathcal d}{w}_t\\bigr){^\\mathsf{t}}{\\mathcal q}\\bigl({\\mathcal a}{x}_t + { \\mathcal b}{u}_t + { \\mathcal d}{w}_t\\bigr ) + { u}_t{^\\mathsf{t}}{\\mathcal r}{u}_t\\bigr]\\\\              & \\ ; = \\ee_{{\\mathcal y}_t}\\bigl[\\bigl\\|{\\mathcal a}{x}_t + { \\mathcal b}\\bigl({\\boldsymbol{\\eta}}_t + { \\boldsymbol{\\theta}}_t\\varphi({y}_t - \\hat{y}_t)\\bigr ) + { \\mathcal d}{w}_t\\bigr\\|_{{\\mathcal q}}^2 + \\bigl\\|{\\boldsymbol{\\eta}}_t + { \\boldsymbol{\\theta}}_t\\varphi({y}_t - \\hat{y}_t)\\bigr\\|_{{\\mathcal r}}^2\\bigr]\\\\              & \\ ; = \\ee_{{\\mathcal y}_t}\\bigl[{x}_t{^\\mathsf{t}}{\\mathcal a}{^\\mathsf{t}}{\\mathcal q}{\\mathcal a}{x}_t + 2{x}_t{^\\mathsf{t}}{\\mathcal a}{^\\mathsf{t}}{\\mathcal q}{\\mathcal b}{\\boldsymbol{\\eta}}_t + 2 { x}_t{^\\mathsf{t}}{\\mathcal a}{^\\mathsf{t}}{\\mathcal q}{\\mathcal b}{\\boldsymbol{\\theta}}_t\\varphi({y}_t - \\hat{y}_t ) + 2 { x}_t{^\\mathsf{t}}{\\mathcal a}{^\\mathsf{t}}{\\mathcal q}{\\mathcal d}{w}_t\\\\              & \\qquad + { \\boldsymbol{\\eta}}_t{^\\mathsf{t}}({\\mathcal b}{^\\mathsf{t}}{\\mathcal q}{\\mathcal b}+ { \\mathcal r}){\\boldsymbol{\\eta}}_t + 2{\\boldsymbol{\\eta}}_t{^\\mathsf{t}}({\\mathcal b}{^\\mathsf{t}}{\\mathcal q}{\\mathcal b}+ { \\mathcal r}){\\boldsymbol{\\theta}}_t\\varphi({y}_t - \\hat{y}_t ) + 2{\\boldsymbol{\\eta}}_t{^\\mathsf{t}}{\\mathcal b}{^\\mathsf{t}}{\\mathcal q}{\\mathcal d}{w}_t\\\\              & \\qquad + \\varphi({y}_t - \\hat{y}_t){^\\mathsf{t}}{\\boldsymbol{\\theta}}_t{^\\mathsf{t}}({\\mathcal b}{^\\mathsf{t}}{\\mathcal q}{\\mathcal b}+ { \\mathcal r}){\\boldsymbol{\\theta}}_t\\varphi({y}_t - \\hat{y}_t ) + 2\\varphi({y}_t - \\hat{y}_t){^\\mathsf{t}}{\\boldsymbol{\\theta}}_t{^\\mathsf{t}}{\\mathcal b}{^\\mathsf{t}}{\\mathcal q}{\\mathcal d}{w}_t\\\\              & \\qquad + { w}_t{^\\mathsf{t}}{\\mathcal d}{^\\mathsf{t}}{\\mathcal q}{\\mathcal d}{w}_t\\bigr ]          \\end{aligned}\\ ] ] since @xmath323 = 0 $ ] and in view of the definitions of the various matrices in we have @xmath324 this tallies the objective function in with the objective @xmath90 in .    concerning the constraints , we have shown in @xcite that combining the constraint @xmath325 and the class of policies is equivalent to the constraints @xmath326 for all @xmath327 , which accounts for the constraint . substituting into the stability constraint ,",
    "we obtain @xmath328 accordingly , if condition constraint is satisfied , then the stability constraint is satisfied as well .",
    "it remains to show that the constraints are simultaneously feasible .",
    "inspired by the work in @xcite , we consider the candidate controller @xmath329}{\\coloneqq}-{\\mathfrak{r}}_\\kappa({a}_2,{b}_2)^\\dagger \\operatorname{sat}_r({a}_2^\\kappa\\hat{x}_t^{(2)}),\\ ] ] i.e. , with @xmath330 , where @xmath331 .",
    "first , we have that @xmath332 and the constraint is satisfied . concerning constraint , we have that @xmath333 where the first equality follows from the orthogonality of @xmath334 ( see @xcite ) , and the constraint is also satisfied .",
    "the optimization program subject to - is therefore a quadratic program that is equivalent to .",
    "[ l : prcondition1 ] consider the system - , and suppose that assumption [ ass : stability1 ] holds . then there exist constants @xmath335 such that the notation @xmath336 stands for the smallest integer greater or equal to @xmath126 . ]",
    "@xmath337 { \\leqslant}-b \\quad\\text{on the set } \\bigl\\{\\bigl\\|{{{\\hat{x}}}^{(2)}}_{\\kappa t}\\bigr\\| > j\\bigr\\}\\qquad\\text{for all } t{\\geqslant}{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}.\\ ] ]    let @xmath338 be a projection operator that picks the last @xmath339 components of any vector in @xmath340 , and consider the subsampled process @xmath341 given by @xmath342 by utilizing the triangle inequality we get @xmath343\\\\              & \\ ; { \\leqslant}\\ee_{{\\mathcal y}_{\\kappa t}}\\bigl[\\bigl\\|{a}_2^\\kappa { { { \\hat{x}}}^{(2)}}_{\\kappa t } + { \\mathfrak{r}}_\\kappa({a}_2 , { b}_2){u}_{\\kappa t:\\kappa(t+1)-1}\\bigr\\| -   { \\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa t}}\\right\\rvert}\\bigr ] + \\ee_{{\\mathcal y}_{\\kappa t}}\\bigl[{\\left\\lvert{\\pi^{(2)}\\bigl(\\xi_{\\kappa t}\\bigr)}\\right\\rvert}\\bigr ] .          \\end{aligned}\\ ] ] we know from scholium [ sch : thehorror ] that there exists a uniform ( with respect to time @xmath78 ) upper bound @xmath183 for the last term on the right - hand side of the preceding inequality for @xmath344 .",
    "we rewrite the inequality as @xmath345 & { \\leqslant}\\ee_{{\\mathcal y}_{\\kappa t}}\\bigl[{\\left\\lvert{{a}_2^\\kappa { { { \\hat{x}}}^{(2)}}_{\\kappa t } + { \\mathfrak{r}}_\\kappa({a}_2 , { b}_2){u}_{\\kappa t:\\kappa(t+1)-1}}\\right\\rvert } - { \\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa t}}\\right\\rvert}\\bigr ] + { \\zeta}\\\\   & { \\leqslant}- \\frac{{\\varepsilon}}{2},\\qquad \\text{whenever } \\bigl\\|{{{\\hat{x}}}^{(2)}}_{t}\\bigr\\|{\\geqslant}{\\zeta}+{\\varepsilon } ,          \\end{aligned}\\ ] ] where the last inequality follows from .",
    "setting @xmath346 and @xmath347 completes the proof .",
    "[ l : prcondition2 ] consider the system - , and suppose that assumption [ ass : stability1 ] holds . then there exists a constant @xmath348 such that @xmath349 { \\leqslant}m\\qquad \\text{for all } t{\\geqslant}{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}.\\ ] ]    fix @xmath302 , and consider again the subsampled process in .",
    "we have , by orthogonality of @xmath334 , that @xmath350 and , by the triangle inequality , that @xmath351{\\nonumber}\\\\              & = \\ee\\bigl[{\\left\\lvert{{\\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa(t+1)}}\\right\\rvert } - { \\left\\lvert{{a}_2^\\kappa { { { \\hat{x}}}^{(2)}}_{\\kappa t}}\\right\\rvert}}\\right\\rvert}^4\\,\\big|\\,{\\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa i}}\\right\\rvert},\\;i = { { { \\left\\lceil{{{t}'}/\\kappa}\\right\\rceil } } } , \\ldots , t\\bigr]{\\nonumber}\\\\              & { \\leqslant}\\ee\\bigl[{\\left\\lvert{{a}_2^\\kappa { { { \\hat{x}}}^{(2)}}_{\\kappa t } + { \\mathfrak{r}}_\\kappa({a}_2 , { b}_2){u}_{\\kappa t:\\kappa(t+1)-1 } + \\xi_{\\kappa t } - { a}_2^\\kappa { { { \\hat{x}}}^{(2)}}_{\\kappa t}}\\right\\rvert}^4\\,\\big|\\,{\\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa i}}\\right\\rvert},\\;i = { { { \\left\\lceil{{{t}'}/\\kappa}\\right\\rceil } } } , \\ldots , t\\bigr]{\\nonumber}\\\\              & = \\ee\\bigl[\\bigl({\\left\\lvert{{\\mathfrak{r}}_\\kappa({a}_2 , { b}_2){u}_{\\kappa t:\\kappa(t+1)-1}}\\right\\rvert } + { \\left\\lvert{\\xi_{\\kappa t}}\\right\\rvert}\\bigr)^4\\,\\big|\\,{\\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa i}}\\right\\rvert},\\;i = { { { \\left\\lceil{{{t}'}/\\kappa}\\right\\rceil } } } , \\ldots , t\\bigr].\\label{e : fourthmomentbnd }          \\end{aligned}\\ ] ] recall that for any two positive numbers @xmath352 and @xmath353 , @xmath354 . using this latter fact , we arrive at the following upper bound @xmath351{\\nonumber}\\\\              & { \\leqslant}8 \\ee\\bigl[{\\left\\lvert{{\\mathfrak{r}}_\\kappa({a}_2 , { b}_2){u}_{\\kappa t:\\kappa(t+1)-1}}\\right\\rvert}^4+{\\left\\lvert{\\xi_{\\kappa t}}\\right\\rvert}^4\\,\\big|\\,{\\left\\lvert{{{{\\hat{x}}}^{(2)}}_{\\kappa i}}\\right\\rvert},\\;i = { { { \\left\\lceil{{{t}'}/\\kappa}\\right\\rceil } } } , \\ldots , t\\bigr ] .          \\end{aligned}\\ ] ] by design @xmath355 and @xmath300 is gaussian and independent of @xmath356 and has its fourth moment bounded .",
    "therefore , we can easily infer that there exists an @xmath357 such that @xmath358 { \\leqslant}m\\ ] ] for all @xmath302 .",
    "we are finally ready to prove theorem [ thm : main ] .",
    "claim [ maintheorem : optproblem ] of theorem [ thm : main ] was proved in lemma [ l : maintheorem : opt ] .",
    "it remains to show claim [ maintheorem : msbdd ] .",
    "we start by asserting the following inequality @xmath359 { \\leqslant}2 \\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\bigl[{\\left\\lvert{{x}_t - \\hat{x}_{t}}\\right\\rvert}^2\\bigr ] + 2\\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\bigl[{\\left\\lvert{\\hat{x}_{t}}\\right\\rvert}^2\\bigr].\\ ] ] we know from fact [ fact : factsonbounds ] that @xmath360 { \\leqslant}{\\rho}$ ] for all @xmath361 . as such ,",
    "if we are able to show that the state of the estimator is mean - square bounded , we can immediately infer a mean - square bound on the state of the plant .",
    "we first start by splitting the squared norm of the estimator state as @xmath362 , where @xmath190 and @xmath363 are states corresponding to the schur and orthogonal parts of the system , respectively .",
    "it then follows that @xmath364 = \\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\bigl[\\bigl\\|\\hat{x}_t^{(1)}\\bigr\\|^2\\bigr ] + \\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\bigl [ \\bigl\\|\\hat{x}_t^{(2)}\\bigr\\|^2\\bigr],\\qquad t\\in\\nz.\\ ] ] letting @xmath365 for @xmath60 , we see from lemma [ l : prcondition1 ] and lemma [ l : prcondition2 ] the conditions and of proposition [ p : pr99 ] are verified for the sequence @xmath366 .",
    "thus , by proposition [ p : pr99 ] , there exists a constant @xmath367 such that @xmath368 { \\leqslant}\\gamma^{(2)}\\qquad \\text{for all } t{\\geqslant}{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}.\\ ] ] hence , the orthogonal subsystem is mean - square bounded .    since the matrix @xmath369 is schur stable ,",
    "we know ( * ? ? ?",
    "* proposition 11.10.5 ) that there exists a positive definite matrix @xmath370 that satisfies @xmath371 .",
    "it easily follows that there exists @xmath3720 , 1[$ ] such that @xmath373 ; in fact , @xmath374 can be chosen from the interval @xmath3750 , \\min\\bigl\\{1 , \\lambda_{\\min}\\bigl({i}\\bigr)/\\lambda_{\\max}\\bigl({m}\\bigr)\\bigr\\}\\bigr[$ ] .",
    "therefore , we see that for any @xmath292 , @xmath376 - { \\left\\lvert{{{{\\hat{x}}}^{(1)}}_{t}}\\right\\rvert}^2_{{m } } { \\leqslant}- \\rho{\\left\\lvert{{{{\\hat{x}}}^{(1)}}_{t}}\\right\\rvert}^2_{{m } } + 2\\ee_{{\\mathcal y}_t}\\bigl[({{{\\hat{x}}}^{(1)}}_t){^\\mathsf{t}}{a}_1{^\\mathsf{t}}{m}{b}_1{{u}^{(1)}}_t\\bigr ] + \\ee_{{\\mathcal y}_t}\\bigl[\\pi^{(1)}(\\xi_t)\\bigr ] ,          \\end{aligned}\\ ] ] where @xmath377 is the projection onto the first @xmath378 coordinates and @xmath379 was defined in .",
    "young s inequality shows that @xmath380 { \\leqslant}\\epsilon\\ee_{{\\mathcal y}_t}\\bigl[\\bigl\\|{a}_1{{{\\hat{x}}}^{(1)}}_t\\bigr\\|_{{m}}^2\\bigr ] + \\frac{1}{\\epsilon}\\ee_{{\\mathcal y}_t}\\bigl[\\bigl\\|{b}_1{{u}^{(1)}}_t\\bigr\\|_{{m}}^2\\bigr]$ ] for @xmath184 .",
    "choosing @xmath381 and utilizing fact [ fact : factsonbounds ] and the upper bound on the inputs @xmath382 , it follows that for all @xmath292 , @xmath383 - ( 1-\\tfrac{\\rho}{2}){\\left\\lvert{{{{\\hat{x}}}^{(1)}}_{t}}\\right\\rvert}^2_{{m } } { \\leqslant}\\frac{2(1-\\rho)}{3\\rho}\\lambda_{\\max}\\bigl({m}\\bigr){\\left\\lvert{{b}_1}\\right\\rvert}^2 u_{\\max}^2 n_1+{\\zeta}=:c .",
    "\\end{aligned}\\ ] ] therefore , for @xmath292 , we have that @xmath384 & = \\ee_{{\\mathcal y}_{\\kappa{{t}'}}}\\left[\\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}+1}}\\left[{\\left\\lvert{{{{\\hat{x}}}^{(1)}}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}+2}}\\right\\rvert}^2_{{m}}\\right]\\right ]",
    "\\\\              & { \\leqslant}\\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\left[\\ee_{{\\mathcal y}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}+1}}\\left[(1-\\tfrac{\\rho}{2}){\\left\\lvert{{{{\\hat{x}}}^{(1)}}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}+1}}\\right\\rvert}^2_{{m } } + c\\right]\\right]\\\\              & { \\leqslant}(1-\\tfrac{\\rho}{2})^2 { \\left\\lvert{{{{\\hat{x}}}^{(1)}}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\right\\rvert}^2_{{m } } + \\bigl(1 + ( 1-\\tfrac{\\rho}{2})\\bigr)c .",
    "\\end{aligned}\\ ] ] iterating the last inequality , it follows that @xmath385 { \\leqslant}(1-\\tfrac{\\rho}{2})^t { \\left\\lvert{{{{\\hat{x}}}^{(1)}}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\right\\rvert}^2_{{m } } + \\sum_{i=0}^{t-1}(1-\\tfrac{\\rho}{2})^ic,\\qquad t{\\geqslant}{{t}'},\\ ] ] or @xmath386   { \\leqslant}{\\left\\lvert{{{{\\hat{x}}}^{(1)}}_{\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}}}\\right\\rvert}^2_{{m } } +   ( 1-\\tfrac{\\rho}{2})^{-1}c , \\qquad t{\\geqslant}{{t}'}.          \\end{aligned}\\ ] ] we can conclude that there exists @xmath387 such that @xmath388 { \\leqslant}\\gamma^{(1)}\\qquad \\text{for all } t{\\geqslant}\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}.\\ ] ] we can therefore conclude that @xmath389 { \\leqslant}\\gamma^{(1)}+\\gamma^{(2)}\\qquad \\text{for all } t{\\geqslant}\\kappa{{{\\left\\lceil{{{t}'}/\\kappa}\\right\\rceil}}}.\\ ] ] since the sequence @xmath390 in is generated through the addition of independent mean - square bounded random variables and a bounded control input , and since @xmath391 , it follows that there exists @xmath392 such that @xmath393 { \\leqslant}\\gamma \\qquad\\text{for all } t\\in\\nz,\\ ] ] establishing the second claim [ maintheorem : msbdd ] of theorem [ thm : main ] .",
    "the proof of corollary [ cor : main ] follows exactly the same reasoning as in the proof of theorem [ thm : main ] , up to the constraints in and .",
    "also , rewriting the constraints and as and , respectively , can be done similarly to the way we rewrote the cost in theorem .",
    "it remains to show the upper bounds @xmath394 and @xmath395 in assumption .",
    "the constraint can be upper - bounded as follows : @xmath396\\\\      & \\quad = \\ee_{{\\mathcal y}_t}\\left[{\\left\\lvert{{\\mathcal a}{x}_t+{\\mathcal b}{u}_t+{\\mathcal d}{w}_t}\\right\\rvert}_{\\mathcal s}^2 + \\mathcal l{^\\mathsf{t}}({\\mathcal a}{x}_t+{\\mathcal b}{u}_t+{\\mathcal d}{w}_t)\\right]\\\\      & \\quad { \\leqslant}3\\ee_{{\\mathcal y}_t}\\left[{\\left\\lvert{{\\mathcal a}{x}_t}\\right\\rvert}^2_{\\mathcal s}+{\\left\\lvert{{\\mathcal b}{u}_t}\\right\\rvert}_{\\mathcal s}^2+{\\left\\lvert{{\\mathcal d}{w}_t}\\right\\rvert}_{\\mathcal s}^2\\right]+\\mathcal l{^\\mathsf{t}}{\\mathcal a}\\hat{x}_t+|\\mathcal l{^\\mathsf{t}}{\\mathcal b}{u}_t|\\\\      & \\quad { \\leqslant}3{\\mathsf{tr}\\!\\left({\\mathcal a}{^\\mathsf{t}}\\mathcal s{\\mathcal a}\\ee_{{\\mathcal y}_t}\\left[{x}_t{x}_t{^\\mathsf{t}}\\right]+{\\mathcal d}{^\\mathsf{t}}\\mathcal s{\\mathcal d}{\\sigma_{{w}}}\\right ) } + \\mathcal l{^\\mathsf{t}}{\\mathcal a}\\hat{x}_t\\\\      & \\quad\\quad 3nm\\sigma_{\\max}({\\mathcal b}{^\\mathsf{t}}\\mathcal s{\\mathcal b})u_{\\max}^2 + + { \\left\\lvert{\\mathcal l{^\\mathsf{t}}{\\mathcal b}}\\right\\rvert}_1u_{\\max } ,     \\end{aligned}\\ ] ] where the first inequality follows from the fact that @xmath397 , for any @xmath398 , and the noise being zero - mean , the second inequality follows from applying norm bounds between the 2 and @xmath275-norms and hlder s inequality . as for the constraint , it can be upper - bounded as follows : @xmath399{\\leqslant}\\sigma_{\\max}(s){\\left\\lvert{{u}_t}\\right\\rvert}^2{\\leqslant}nm\\sigma_{\\max}(s){\\left\\lvert{{u}_t}\\right\\rvert}_\\infty^2{\\leqslant}nm\\sigma_{\\max}(s)u_{\\max}^2.\\end{aligned}\\ ] ] this completes the proof .",
    "consider the system - with the following matrices : @xmath400 } , { b}= { \\left[\\begin{matrix } 1\\\\ 0 \\\\ 1 \\end{matrix}\\right ] } , \\text { and } c = i.\\ ] ] the simulation data was chosen to be @xmath401 , @xmath402 , @xmath403 , @xmath404 , @xmath405 , @xmath406 , @xmath407 , and @xmath408 the usual piecewise linear saturation function with @xmath409 . for this example",
    "the theoretical bound on the input is @xmath410 for a choice of @xmath411 .",
    "we simulated the system for the discrete - time interval @xmath412 $ ] using algorithm [ algo : general ] , without the constraints and .",
    "the coding was done using matlab and the optimization problem was solved using ` yalmip ` @xcite and ` sdpt3 ` @xcite .",
    "the computation of the matrices @xmath413 , @xmath414 , @xmath415 , and @xmath416 was done off - line using the steady state error covariance matrix @xmath417 , as discussed in the previous section , via classical monte carlo integration ( * ? ? ?",
    "* section 3.2 ) using @xmath418 samples .",
    "the norms of the state trajectories for 200 different sample paths of the process and measurement noises are plotted in figure [ fig : states ] starting from @xmath419}^t$ ] .",
    "the average state norm as well as the standard deviation of the state norm are depicted in figure [ fig : statistics ] , where it is clear that the proposed controller renders the system mean - square bounded .",
    "the average total cost normalized by time for this simulation is plotted in figure [ fig : cost ] .",
    "+     +     +",
    "we presented a method for stochastic receding horizon control of discrete - time linear systems with process and measurement noise and bounded input policies .",
    "we showed that the optimization problem solved periodically is successively feasible and convex .",
    "moreover , we illustrated how a certain stability condition can be utilized to show that the application of the receding horizon controller renders the state of the system mean - square bounded .",
    "we discussed how certain matrices in the cost function can be computed off - line and provided an example that illustrates our approach .",
    "44 natexlab#1#1url # 1`#1`urlprefix    agarwal , m. , cinquemani , e. , chatterjee , d. , lygeros , j. , 2009 .",
    "on convexity of stochastic optimization problems with constraints . in : european control conference .",
    ". 28272832 .",
    "anderson , b. , moore , j. , 1979 .",
    "optimal filtering .",
    "prentice - hall .",
    "batina , i. , 2004 . model predictive control for stochastic systems by randomized algorithms .",
    "thesis , technische universiteit eindhoven .",
    "bemporad , a. , morari , m. , 1999 . robust model predictive control : a survey .",
    "robustness in identification and control 245 , 207226 .",
    "ben - tal , a. , boyd , s. , nemirovski , a. , 2006 .",
    "extending scope of robust optimization : comprehensive robust counterparts of uncertain problems .",
    "journal of mathematical programming 107 , 6389 .",
    "ben - tal , a. , goryashko , a. , guslitzer , e. , nemirovski , a. , 2004 .",
    "adjustable robust solutions of uncertain linear programs . mathematical programming 99  ( 2 ) , 351376 .",
    "bernstein , d.  s. , 2009 .",
    "matrix mathematics , 2nd edition .",
    "princeton university press .",
    "bertsekas , d.  p. , 2000 .",
    "dynamic programming and optimal control , 2nd edition .",
    "vol .  1 .",
    "athena scientific .",
    "bertsekas , d.  p. , 2005 .",
    "dynamic programming and suboptimal control : a survey from adp to mpc .",
    "european journal of control 11  ( 4 - 5 ) .",
    "bertsekas , d.  p. , 2007 .",
    "dynamic programming and optimal control , 3rd edition .",
    "2 . athena scientific .",
    "bertsimas , d. , brown , d.  b. , 2007 . constrained stochastic lqc : a tractable approach .",
    "ieee transactions on automatic control 52  ( 10 ) , 18261841 .",
    "bertsimas , d. , iancu , d.  a. , parrilo , p.  a. , 2009 .",
    "optimality of affine policies in multi - stage robust optimizationpreprint available at : http://arxiv.org/abs/0904.3986 .",
    "blackmore , l. , williams , b.  c. , july 2007 . optimal , robust predictive control of nonlinear systems under probabilistic uncertainty using particles . in : proceedings of the american control conference .",
    "1759  1761 .",
    "boyd , s. , vandenberghe , l. , 2004 .",
    "convex optimization .",
    "cambridge university press , cambridge , sixth printing with corrections , 2008 .",
    "cannon , m. , kouvaritakis , b. , wu , x. , july 2009 .",
    "probabilistic constrained mpc for systems with multiplicative and additive stochastic uncertainty .",
    "ieee transactions on automatic control 54  ( 7 ) , 16261632 .",
    "chatterjee , d. , hokayem , p. , lygeros , j. , 2009 .",
    "stochastic receding horizon control with bounded control inputs : a vector space approach .",
    "ieee transactions on automatic controlunder review .",
    "http://arxiv.org/abs/0903.5444 .",
    "ferrante , a. , picci , g. , pinzoni , s. , 2002 .",
    "silverman algorithm and the structure of discrete - time stochastic systems .",
    "linear algebra and its applications 351/352 , 219242 .",
    "goulart , p.  j. , kerrigan , e.  c. , maciejowski , j.  m. , 2006 .",
    "optimization over state feedback policies for robust control with constraints .",
    "automatica 42  ( 4 ) , 523533 .",
    "grant , m. , boyd , s. , december 2000 .",
    "cvx : matlab software for disciplined convex programming ( web page and software ) .",
    "http://stanford.edu/~boyd/cvx .",
    "hokayem , p. , chatterjee , d. , lygeros , j. , 2009",
    ". on stochastic model predictive control with bounded control inputs . in : proceedings of the combined 48th ieee conference on decision & control and 28th chinese control conference .",
    "63596364 , available at http://arxiv.org/abs/0902.3944 .    horn , r.  a. , johnson , c.  r. , 1990 .",
    "matrix analysis .",
    "cambridge university press , cambridge .",
    "jazwinski , a.  h. , 1970 . stochastic processes and filtering theory . academic press .",
    "kamen , e.  w. , su , j.  k. , 1999 .",
    "introduction to optimal estimation .",
    "springer , london , uk .",
    ", w.  k. , 1983 . on integrated chance constraints . in : stochastic programming ( gargnano ) .",
    "76 of lecture notes in control and inform .",
    "springer , berlin , pp .",
    "194209 .    , w.  k. , van  der vlerk , m.  h. , 2006 .",
    "integrated chance constraints : reduced forms and an algorithm .",
    "computational management science 3  ( 4 ) , 245269 .    kumar , p.  r. , varaiya , p. , 1986 .",
    "stochastic systems : estimation , identification , and adaptive control .",
    "prentice hall .",
    "lazar , m. , heemels , w. p. m.  h. , bemporad , a. , weiland , s. , 2007 .",
    "discrete - time non - smooth nonlinear mpc : stability and robustness . in : lecture notes in control and information sciences .",
    "springer - verlag , pp .",
    "93103 .",
    "lfberg , j. , 2003 .",
    "minimax approaches to robust model predictive control .",
    "thesis , linkpings universitet .",
    "lfberg , j. , 2004 .",
    "yalmip : a toolbox for modeling and optimization in matlab . in : proceedings of the cacsd conference .",
    "taipei , taiwan .",
    "maciejowski , j.  m. , 2001 .",
    "predictive control with constraints .",
    "prentice hall .",
    "maciejowski , m. , lecchini , a. , lygeros , j. , 2005 .",
    "nmpc for complex stochastic systems using markov chain monte carlo .",
    "358/2007 of lecture notes in control and information sciences .",
    "springer , stuttgart , germany , pp .",
    "269281 .",
    "mayne , d.  q. , rawlings , j.  b. , rao , c.  v. , scokaert , p. o.  m. , jun 2000 . constrained model predictive control : stability and optimality .",
    "automatica 36  ( 6 ) , 789814 .",
    "oldewurtel , f. , jones , c. , morari , m. , dec 2008 . a tractable approximation of chance constrained stochastic mpc based on affine disturbance feedback . in : conference on decision and control , cdc .",
    "cancun , mexico .",
    "pemantle , r. , rosenthal , j.  s. , 1999 .",
    "moment conditions for a sequence with negative drift to be uniformly bounded in @xmath420 .",
    "stochastic processes and their applications 82  ( 1 ) , 143155 .",
    "primbs , j. , 2007 . a soft constraint approach to stochastic receding horizon control . in : proceedings of the 46th ieee conference on decision and control .",
    "4797  4802 .",
    "primbs , j.  a. , sung , c.  h. , feb .",
    "stochastic receding horizon control of constrained linear systems with state and control multiplicative noise .",
    "ieee transactions on automatic control 54  ( 2 ) , 221230 .",
    "qin , s.  j. , badgwell , t. , jul .",
    "2003 . a survey of industrial model predictive control technology .",
    "control engineering practice 11  ( 7 ) , 733764 .",
    "ramponi , f. , chatterjee , d. , milias - argeitis , a. , hokayem , p. , lygeros , j. , 2009 . attaining mean square boundedness of a marginally stable noisy linear system with a bounded control input .",
    "http://arxiv.org/abs/0907.1436 , submitted to ieee transactionson automatic control , revised jan 2010 .",
    "robert , c.  p. , casella , g. , 2004 .",
    "monte carlo statistical methods , 2nd edition . springer .",
    "skaf , j. , boyd , s. , 2009 .",
    "design of affine controllers via convex optimization .",
    "http://www.stanford.edu/~boyd/papers/affine_contr.html .",
    "toh , k. , todd , m. , tatuncu , r. , 1999 .",
    "sdpt3 a matlab software package for semidefinite programming .",
    "optimization methods and software  ( 11 ) , 545581 , http://www.math.nus.edu.sg/~mattohkc/sdpt3.html .",
    "van hessem , d.  h. , bosgra , o.  h. , 2003 .",
    "a full solution to the constrained stochastic closed - loop mpc problem via state and innovations feedback and its receding horizon implementation . in : proceedings of the 42nd ieee conference on decision and control .",
    "vol .  1 .",
    "pp . 929934 .",
    "wang , y. , boyd , s. , 2009 .",
    "peformance bounds for linear stochastic control . systems and",
    "control letters 58  ( 3 ) , 178182 .",
    "yang , y.  d. , sontag , e.  d. , sussmann , h.  j. , 1997 .",
    "global stabilization of linear discrete - time systems with bounded feedback .",
    "systems and control letters 30  ( 5 ) , 273281 .",
    "for @xmath136 , @xmath421 , with @xmath422 , by assumption . assume now that , for a given @xmath423 , @xmath424 is normal with mean @xmath425 and covariance matrix @xmath426 . by assumption [ ass : dynamics]-[ass : distributions ] and dynamics in , @xmath427 is also normal with mean @xmath428 and covariance matrix @xmath429 .",
    "hence , applying bayes rule we may write @xmath430 where we have @xmath431 by the chapman - kolmogorov equation .",
    "it follows that @xmath432\\biggr),\\end{aligned}\\ ] ] where the proportionality constants do not depend on @xmath115 .",
    "let us now focus on the term within square brackets and write @xmath433 , @xmath434 , @xmath435 and @xmath436 in place of @xmath115 , @xmath34 , @xmath437 and @xmath438 for shortness .",
    "expanding the products and collecting the linear and quadratic terms in @xmath433 one gets @xmath439 since @xmath440 and @xmath97 by assumption , it follows that @xmath441 .",
    "if we let @xmath442 , the expression above can be rewritten as @xmath443 completing the square , the latter expression becomes @xmath444{^\\mathsf{t}}p _ * [ p_*^{-1 } { x}-({c}{^\\mathsf{t}}{\\sigma_{{v}}}^{-1}{y}+p^{-1}\\hat{{x } } ) ] + c,\\end{aligned}\\ ] ] where @xmath445 depends on @xmath435 and @xmath434 but not on @xmath433 .",
    "factoring @xmath446 out of the two terms @xmath447 and simplifying yields @xmath448{^\\mathsf{t}}p_*^{-1 } [ { x}-p_*({c}{^\\mathsf{t}}{\\sigma_{{v}}}^{-1}{y}+p^{-1}\\hat{{x } } ) ] + c.\\end{aligned}\\ ] ] hence , @xmath449 with @xmath450 , where the missing normalization constant does not depend on @xmath109 .",
    "hence , @xmath108 is normal with mean @xmath451 and covariance matrix @xmath452 . using the matrix inversion lemma , @xmath453 , which is  . to obtain  ,",
    "replace @xmath454 by @xmath455 in the expression of @xmath456 to get @xmath457 \\\\ & = p_*{c}{^\\mathsf{t}}{\\sigma_{v}}^{-1}(y-{c}\\hat{x } ) + p _ * ( { c}{^\\mathsf{t}}{\\sigma_{v}}^{-1}{c}+ p^{-1})\\hat{x } \\\\ & = p_*{c}{^\\mathsf{t}}{\\sigma_{v}}^{-1}(y-{c}\\hat{x } ) + \\hat{x}.\\end{aligned}\\ ] ] finally , using the identity @xmath458 ( where the inverse exists because @xmath459 and @xmath277 ) and the definition of @xmath460 , @xmath461 which leads to  .    to conclude the proof ,",
    "we need to compute the density @xmath462 and prove  . using the chapman - kolmogorov equation , we can compute the density as follows @xmath463 however , the explicit computation of the integral in is a very tedious exercise .",
    "we shall instead rely on characteristic functions .",
    "recall that the characteristic function of an @xmath464-dimensional gaussian random vector @xmath465 with mean @xmath466 and covariance @xmath467 is given by @xmath468=\\exp\\left({i}z{^\\mathsf{t}}\\mu-\\frac{1}{2}z{^\\mathsf{t}}\\sigma z\\right)$ ] , where @xmath469 and @xmath470 .",
    "the characteristic function of @xmath471 given @xmath111 is then @xmath472 \\\\ & = \\ee\\left[\\ee\\big[\\exp\\big({i}z{^\\mathsf{t}}({a}{x}_t+{b}{u}_t+{w}_t ) \\big)\\,\\big|\\,{x}_t,{\\mathcal y}_t\\big]\\,\\big|\\,{\\mathcal y}_t\\right ] \\\\ & = \\ee\\left[\\exp\\big({i}z{^\\mathsf{t}}{a}{x}_t+ { i}z{^\\mathsf{t}}{b}g_t({\\mathcal y}_t)\\big)\\right.\\times \\left.\\ee\\big[\\exp({i}z{^\\mathsf{t}}{w}_t ) \\big|{x}_t,{\\mathcal y}_t\\big]\\big|{\\mathcal y}_t\\right],\\end{aligned}\\ ] ] where we have used system dynamics in and the general definition of the feedback policies . since @xmath33 is independent of @xmath115 and @xmath111 and it is gaussian with mean @xmath473 and covariance @xmath474 , @xmath475=\\exp\\left(-\\frac{1}{2}z{^\\mathsf{t}}{\\sigma_{{w}}}z\\right)$ ] .",
    "therefore the last expression in the chain becomes @xmath476\\exp\\left({i}z{^\\mathsf{t}}{b}g_t({\\mathcal y}_t)\\right)\\exp\\left(-\\frac{1}{2}z{^\\mathsf{t}}{\\sigma_{{w}}}z\\right).\\ ] ] it was proved above that , conditionally on @xmath111 , @xmath115 is gaussian with mean @xmath477 and covariance matrix @xmath478 .",
    "hence @xmath479 & = \\ee\\left[\\exp\\big({i}({a}{^\\mathsf{t}}z){^\\mathsf{t}}{x}_t\\big)\\big|{\\mathcal y}_t\\right]\\\\      & = \\exp\\left({i}({a}{^\\mathsf{t}}z){^\\mathsf{t}}{{\\hat{x}}}_{t|t}-\\frac{1}{2}({a}{^\\mathsf{t}}z){^\\mathsf{t}}{p}_{t|t}({a}{^\\mathsf{t}}z)\\right),\\end{aligned}\\ ] ] and consequently @xmath480 which is the characteristic function of a gaussian random vector with mean @xmath481 and covariance matrix @xmath482 .",
    "[ prop : pbounds ] consider the system - , and suppose that @xmath483 be stabilizable and @xmath196 be observable .",
    "in addition , assume that @xmath484 .",
    "then there exist constants @xmath485 and an integer @xmath180 large enough such that @xmath486 = { \\mathsf{tr}\\!\\left(p_{t|t}\\right ) } { \\leqslant}\\rho \\qquad \\text{for all } t{\\geqslant}t.\\ ] ]    first , observe that @xmath487              \\bigl[\\sigma_{{w}}^{1/2 }     \\ ;   { a}\\sigma_{{w}}^{1/2 }    \\ ;   \\cdots\\ ;     { a}^{\\kappa_1 - 1 } \\sigma_{{w}}^{1/2}\\bigr]{^\\mathsf{t}},\\ ] ] and since @xmath488 is controllable by assumption [ ass : stability1]-[ass : stability1:systemandnoise ] , we see that there exists @xmath489 such that for all @xmath490 the rank of @xmath491 = n$ ] ; indeed , @xmath492 is the reachability index of @xmath488 .",
    "thus , @xmath493 is positive definite , and therefore , there exists some @xmath494 such that @xmath495 second , observe that denotes the standard kronecker product . ]",
    "@xmath496 since @xmath196 is observable by assumption , there exists @xmath497 such that the rank of the matrix @xmath498{^\\mathsf{t}}$ ] is @xmath464 .",
    "the matrix @xmath499 is clearly positive definite by assumption [ ass : stability1][ass : stability1:systemandnoise ] , and therefore , we see that there exists @xmath500 such that      third , the conditions of lemma 7.1 in @xcite are satisfied , as and hold with @xmath502 and @xmath503 , and the bound @xmath504 for some @xmath505 is established for all @xmath506 .",
    "the assertion now follows immediately from : @xmath507 = { \\mathsf{tr}\\!\\left(p_{t|t}\\right)}{\\leqslant}n\\lambda_{\\max}(p_{t|t}){\\leqslant}n\\rho ' { \\eqqcolon}\\rho.\\qedhere\\ ] ]"
  ],
  "abstract_text": [
    "<S> we provide a solution to the problem of receding horizon control for stochastic discrete - time systems with bounded control inputs and imperfect state measurements . for a suitable choice of control policies , we show that the finite - horizon optimization problem to be solved on - line is convex and successively feasible . due to the inherent nonlinearity of the feedback loop , a slight extension of the kalman filter </S>",
    "<S> is exploited to estimate the state optimally in mean - square sense . </S>",
    "<S> we show that the receding horizon implementation of the resulting control policies renders the state of the overall system mean - square bounded under mild assumptions . </S>",
    "<S> finally , we discuss how some of the quantities required by the finite - horizon optimization problem can be computed off - line , reducing the on - line computation , and present some numerical examples . </S>"
  ]
}