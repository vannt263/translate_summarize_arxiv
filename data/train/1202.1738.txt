{
  "article_text": [
    "the primary aim of this article is to provide an objective comparison of two methods for bayesian inference in the spatial log - gaussian cox process : a relatively slow but asymptotically exact method , markov chain monte carlo ( mcmc ) ; and a faster but approximate method , the integrated nested laplace approximation ( inla ) .",
    "the secondary aim is to provide a tutorial in some of the technical aspects involved with computation and inference for this class of models .",
    "the log - gaussian cox process is but one of a number of possible model classes that we could have used as the basis for a comparative evaluation of mcmc and inla methods .",
    "our specific motivation for focusing on this model is its use in spatial epidemiology , and specifically in health surveillance applications , where interest is in the predictive probability that the relative risk of disease at a certain spatial location exceeds a threshold set by public health experts . for an example in the spatio - temporal setting see @xcite .",
    "there , the data consisted of locations of incident cases each day , i.e.  a spatio - temporal point process , and the cox process was used to represent spatio - temporal variation in risk as a product of deterministic and stochastic terms representing , respectively , known risk - factors and unexplained variation , prediction of which was the primary goal .",
    "a high predictive probability that risk in a particular area exceeds the pre - declared threshold may activate a costly public health intervention , hence it is important that such predictive probability statements are as accurate as possible .",
    "for this reason we will compare mcmc and inla using a metric that directly measures their ability to make accurate predictive probability statements .",
    "mcmc methods have made an enormous impact on statistical practice by making bayesian inference tractable for complex statistical models , including models whose specification includes a latent gaussian process .",
    "however , they can be computationally burdensome and , more importantly , their inferential validity rests on the convergence of a markov chain to its equilibrium distribution , which can be difficult to verify empirically .",
    "inla @xcite is a recently developed competitor to mcmc methods . by using a combination of analytical approximation and numerical integration rather than monte carlo simulation",
    ", inla circumvents the convergence issues that arise with mcmc methods , and typically leads to quicker computation .",
    "however , the price paid is that the analytic approximations potentially introduce errors in the calculation of posterior probabilities .",
    "the goal of the simulation study in this paper is to assess the trade - off between faster computation and errors of approximation .",
    "our focus is on predicting ( functions of ) a latent spatially continuous gaussian process @xmath0 , which we approximate by a gaussian field , @xmath1 , on a finely spaced , regular square grid of points on the plane .",
    "this is in contrast to the methods discussed in @xcite , in which a representation is constructed on a triangulation of a set of irregularly spaced points .",
    "@xcite assume that @xmath0 has matrn second order structure , that is for @xmath2 , @xmath3 where @xmath4 and @xmath5 is a modified bessel function of the second kind .",
    "the major advantage of their approach is its low computational cost : for any choice of the admissible parameters of the covariance function , they are able to compute the precision matrix of the gmrf approximation in @xmath6 time , where @xmath7 is the number of triangulation points .",
    "we prefer to retain greater flexibility in choosing an appropriate model for the covariance , allowing data and scientific knowledge to inform this choice . for this reason",
    ", we focus instead on the method described in chapter 5 of @xcite , which allows effectively any covariance model to be fitted to the data .",
    "in section [ sect : gmrfapprox ] , we discuss the approximation of a spatially continuous gaussian process by a gaussian markov random field , and give the results of a simulation study detailing the effectiveness of this procedure . in section [",
    "sect : lgcpmodel ] , we describe the spatial log - gaussian cox process",
    ". sections [ sect : mcmc ] and [ sect : inlaapprox ] give details of the mcmc and inla methods , respectively . section [ sect : simulationresults ] summarises the findings .",
    "section [ sect : discussion ] is a concluding discussion . throughout the article",
    ", we use @xmath8 to denote a generic probability density function .",
    "all of the methods discussed in the article are implemented in the r package ` lgcp ` ; see @xcite .",
    "in this section , we consider how to approximate a spatially continuous gaussian process by a gmrf and , via simulation , how good such an approximation is .",
    "note that a discussion of this topic is given in chapter 5 of @xcite . to begin ,",
    "we introduce some more general concepts .      a spatially continuous gaussian process , @xmath9 , is a real - valued continuous gaussian process on the plane , @xmath10 .",
    "this means that @xmath9 is a continuous function from @xmath11 with the property that for any finite collection of locations , @xmath12 , the joint distribution of the random variables representing the value of the process at each of the locations , @xmath13 , is multivariate gaussian .",
    "@xmath14 is called _ strictly stationary _ if @xmath15 for some @xmath16 and any spatial location @xmath17 and _ strictly second - order stationary and isotropic _ , if the covariance between @xmath18 and @xmath19 only depends on the euclidean distance between @xmath17 and @xmath20 , denoted by @xmath21 @xcite .",
    "the covariance between @xmath2 will be assumed to have the form , @xmath22 where @xmath23 is a standard isotropic correlation function : for example a matrn function .",
    "the parameter @xmath24 dictates the point - wise variability of the field , whilst the scale parameter @xmath25 governs the rate of decay of the correlation in space . in what follows",
    ", an italic roman , @xmath1 , will be used to denote the values of @xmath14 at a finite set of locations in space .",
    "we say that @xmath1 _ represents _ the process @xmath14 .",
    "a gaussian markov random field is a collection of random variables , @xmath26 , that have a multivariate gaussian distribution , @xmath27 , where for any @xmath28 , @xmath29 = [ \\tilde y_j|\\text{the neighbours of $ \\tilde y_j$}],\\ ] ] where @xmath30 denotes @xmath31 and @xmath32 $ ] means ` the distribution of ' .",
    "we use @xmath1 @xmath33 to distinguish between respectively the gaussian field and the gaussian markov random field representation of a process @xmath14 at the same finite set of locations .",
    "the _ neighbours _ of @xmath28 , @xmath34 , are usually a much smaller subset of @xmath33 ; all other elements of @xmath33 are conditionally independent of @xmath35 , given @xmath34 .",
    "the pattern of conditional independence is evident in the precision matrix , @xmath36 : theorem 2.2 in @xcite states that , @xmath37 in the case that the neighbourhoods of each element are very small subsets of @xmath33 , the matrix @xmath36 is sparse .",
    "this allows otherwise computationally prohibitive operations , such as matrix inversion , to be implemented with fast algorithms .    to simplify matters , consider a square observation window , @xmath38 , on which a spatially continuous gaussian process is represented by a finite collection of random variables @xmath39 spaced on a regular square grid , @xmath40 , where the @xmath41 are the centroids of grid cells , that cover @xmath38 . for computational reasons to be explained , we assume that @xmath42 for some positive integer @xmath43 . in order to obtain a representation of the _ stationary _ second order structure of the process , it is necessary to extend this grid , typically to a grid of size @xmath44 , which is wrapped on a torus .",
    "this action gives rise to the notion of a toroidal distance metric , by which is meant the minimum distance between two points , travelling either directly ( e.g.  if the points were very close together on the torus ) or around the minor and/or major radii",
    ". a precise definition is given in @xcite .",
    "let @xmath45 be random variables at grid locations @xmath46 on the extended space .",
    "note that in cases where the spatial decay parameter , @xmath25 , is quite quite large compared with the size of @xmath38 , then in order to obtain a valid covariance structure , the grid may have to be extended further , e.g.  onto a @xmath47 toroidal grid , see for example @xcite .",
    "the covariance matrix , @xmath48 , of the discrete field @xmath49 on the extended grid is typically massive , dense and with a dense inverse , @xmath50 . as an example , for a @xmath51 grid in the extended space , the covariance matrix has dimension @xmath52 : the storage and manipulation of such matrices under ordinary circumstances is not computationally feasible on a desktop pc .",
    "however , in the extended space , @xmath48 is block - circulant ( see below ) and symmetric positive definite ( spd ) with a block circulant spd inverse , @xmath50 .",
    "the symmetry induced in the covariance matrix by extending the grid and wrapping it on a torus means that each entry of @xmath48 is one of exactly @xmath53 elements , instead of a possible @xmath54 different elements .",
    "furthermore , matrix computation in the extended space is greatly aided by using the discrete fourier transform ( dft ) , which is why the focus of this article is on grids of dimension @xmath55 .",
    "in fact , it is possible to drop this assumption , at the price of reduced speed in the @xmath56 .",
    "algorithms are available to construct optimised computational plans for implementing the @xmath56 on other grid sizes , for example the fftw library @xcite and an r wrapper library @xcite . in what follows ,",
    "@xmath56 and @xmath57 will denote , respectively , the discrete fourier and discrete inverse - fourier transforms ; as an abuse of notation , the same abbreviations will be used for the 1- and 2-dimensional transforms , with the choice being context dependent @xcite .    a full discussion of why and how the dft is used in matrix computations on block circulant matrices is given in chapter 2 of @xcite , but for completeness a very brief summary follows .",
    "an @xmath58 matrix @xmath59 is said to be _ circulant _ if it has the following structure : @xmath60,\\ ] ] where @xmath61 belong to a field , for example the real numbers .",
    "the ordered set of elements , @xmath62 , is called the base of @xmath59 .",
    "a real @xmath63 block circulant matrix @xmath64 is one with the following structure : @xmath65,\\ ] ] where each @xmath66 is a circulant matrix with base @xmath67 .",
    "the matrix @xmath68\\ ] ] is known as the base matrix of @xmath64 .",
    "the eigenvectors of a circulant matrix @xmath59 ( as defined above ) can be written as @xmath69 the complete set of eigenvectors are stored in columns of the ( unitary ) discrete fourier transform matrix , @xmath70,\\ ] ] where @xmath71 .",
    "now , let @xmath72 be a diagonal matrix with the eigenvalues @xmath73 on the leading diagonal .",
    "by expanding the matrix product analytically it is straightforward to verify that the matrix @xmath59 has spectral decomposition , @xmath74 , where the superscript @xmath75 denotes the conjugate transpose .",
    "the most useful aspect of the matrix @xmath76 is that matrix - vector products @xmath77 and @xmath78 are available directly as the @xmath56 and @xmath57 , respectively , of the vector @xmath79 .",
    "the vector @xmath72 is available as @xmath80 and matrix square roots are computed from expressions such as @xmath81 .",
    "these results massively simplify computation with ordinary circulant matrices ; furthermore , the theory extends to block circulant matrices , where the 2-dimensional @xmath56 and @xmath57 are used .    in",
    "what is to follow the ` full covariance matrix ' will mean the covariance matrix of @xmath49 , i.e.  @xmath48 .",
    "for the application in mind , namely the spatial log - gaussian cox process , it has been argued that markov chain monte carlo using the full covariance matrix is inefficient @xcite .",
    "the suggested alternative is to use the integrated nested laplace approximation to perform approximate bayesian inference with a sparse gmrf approximation to the full covariance matrix so as to reduce computational cost .",
    "we now discuss the construction of such a gmrf approximation .    for the full covariance matrix defined by an arbitrary choice of correlation function ,",
    "the dependence structure is ` dense ' : there is no sparse conditional independence structure  the precision matrix @xmath50 is a dense matrix .",
    "a gmrf approximation to @xmath48 is constructed by parametrising the precision matrix , @xmath82 , and choosing @xmath83 to be such that @xmath84 is ` as close as possible ' to @xmath48 .     illustrating the choice of 1- and 2-neighbourhoods , respectively the left and right hand plots .",
    "the black square is the grid cell of interest and the grey cells are those specified to be the neighbours in the chosen level of dependence.,scaledwidth=60.0%,scaledwidth=30.0% ]    the parametrisation considered here is similar to that presented in chapter 5 of @xcite , in which the neighbourhood of a grid cell consists of all the cells in the box surrounding the cell of interest , up to a certain distance away .",
    "figure [ fig : neigh ] illustrates what will be referred to in this article as 1- and 2-neighbourhoods : the box obtained by specifying respectively dependence on all cells in the box a distance of up to 1 and up to 2 cells both vertically and horizontally around the cell of interest . for the 1-neighbourhoods , @xmath85 has 3 elements , corresponding to the dependences between cells a distance 0 apart , between directly adjacent cells and between diagonally adjacent cells . in a similar way , for the 2-neighbourhood dependence structure , @xmath85 has six elements .",
    "let @xmath86 be the base matrix of @xmath48 and let @xmath87 be the base of the inverse of @xmath88 , with base matrix @xmath89 .",
    "note that , @xmath87 can be computed from @xmath89 using the 2-dimensional discrete fourier transform , @xmath90\\}}{\\mathrm{idft}}({\\mathrm{dft}}(\\tilde\\psi(\\theta))\\varowedge(-1)),\\ ] ] where @xmath91 denotes the raising of each element of matrix @xmath92 to the power @xmath93 and @xmath94 is the vector obtained by stacking the columns of @xmath92 on top of each other .",
    "the optimal @xmath85 is found as @xmath95 where , @xmath96 where a subscript @xmath97 denotes the @xmath98-element of the matrix , @xmath99 here , @xmath100 is the distance from cell @xmath98 to the reference origin @xmath101 and @xmath102 is a constant , set equal to 1 in the experiments below , see @xcite for a justification of this choice .",
    "differentiating @xmath103 with respect to the @xmath104th component of the parameter , @xmath105 , gives @xmath106 which can also be computed using the dft , since @xmath107\\}}{\\mathrm{idft}}\\left\\{-[{\\mathrm{dft}}(\\tilde\\psi(\\theta))\\varowedge(-2)]\\odot{\\mathrm{dft}}\\left[\\frac{\\partial } { \\partial \\theta_k}\\tilde\\psi(\\theta)\\right]\\right\\}.\\ ] ] where @xmath108 denotes element - wise multiplication , and the matrix @xmath109 is a matrix of 1 s where @xmath105 appears and 0 s otherwise .    with the above ingredients , standard software such as the ` optim ` function in r can be used to compute optimal parameters . in particular",
    ", the availability of the gradient function enables the user to take advantage of gradient - based optimisation methods such as the ` bfgs ` method implemented in ` optim ` . a sensible starting point for the optimiser",
    "is given by the base matrix of the diagonal precision matrix , @xmath110 .",
    "the reader who is daunted by the prospect of implementing the above functions should note that this has already been done in the ` lgcp ` r package @xcite .",
    "we performed a simulation study to investigate the ability of the algorithm detailed above to approximate a spatially continuous gaussian process , and hence choose an appropriate neighbourhood size for use later in the article ( see section [ sect : simulationresults ] ) .",
    "given a set of parameters of the latent field , an observation window and a grid size , it is possible to compute the base matrix of @xmath48 . with the same inputs and an optimisation step , it is also possible to compute the base matrix of the sparse representation , @xmath111 .",
    "a measure of the performance of the approximation is given by the mean square error in simulating gaussian random variables .",
    "given a vector of standard gaussian variates , @xmath112 , a realisation of a gaussian field with mean @xmath113 and the correct second order properties is given by @xmath114 , whilst an approximation of the field is @xmath115 . to compare how well the field has been approximated , an appropriate measure is the integrated mean square error .",
    "we estimate this from a repeated sequence of @xmath7 independent realisations of @xmath112 as @xmath116 where @xmath117 is the value of the @xmath118 cell of @xmath119 for the @xmath120th realisation of @xmath112 and @xmath121 is the number of grid cells in each direction ( here , the grid is assumed to be square ) .",
    "figure [ fig : approxfieldsim ] is a visualisation of a true field and two possible sparse approximations using 1- and 2-neighbourhoods .     simulated gaussian fields .",
    "middle plot : the true field with full covariance structure ; left - hand plot : approximate field with a 1-neighbourhood ; right - hand plot : approximate field with a 2-neighbourhood .",
    "the simulation took place on a @xmath122 grid , the optimisation took respectively 64 and 355 seconds to compute the parameters of the 1- and 2- neighbourhoods . the result from the 2-neighbourhood is virtually indistinguishable to the eye from the true field , whereas the 1-neighbourhood has a grainy appearance .",
    "the respective mse s were 0.38 and 0.007.,title=\"fig:\",scaledwidth=30.0%,scaledwidth=30.0% ]   simulated gaussian fields .",
    "middle plot : the true field with full covariance structure ; left - hand plot : approximate field with a 1-neighbourhood ; right - hand plot : approximate field with a 2-neighbourhood .",
    "the simulation took place on a @xmath122 grid , the optimisation took respectively 64 and 355 seconds to compute the parameters of the 1- and 2- neighbourhoods . the result from the 2-neighbourhood is virtually indistinguishable to the eye from the true field , whereas the 1-neighbourhood has a grainy appearance .",
    "the respective mse s were 0.38 and 0.007.,title=\"fig:\",scaledwidth=30.0%,scaledwidth=30.0% ]   simulated gaussian fields .",
    "middle plot : the true field with full covariance structure ; left - hand plot : approximate field with a 1-neighbourhood ; right - hand plot : approximate field with a 2-neighbourhood .",
    "the simulation took place on a @xmath122 grid , the optimisation took respectively 64 and 355 seconds to compute the parameters of the 1- and 2- neighbourhoods .",
    "the result from the 2-neighbourhood is virtually indistinguishable to the eye from the true field , whereas the 1-neighbourhood has a grainy appearance .",
    "the respective mse s were 0.38 and 0.007.,title=\"fig:\",scaledwidth=30.0%,scaledwidth=30.0% ]    for the simulation study , the value of @xmath24 was fixed at 1 and @xmath25 allowed to vary between 0.025 and 0.2 with the unit square as the observation window .",
    "the parameter @xmath24 was fixed because the computed mean square errors would simply scale linearly with @xmath123 .",
    "the range of values of @xmath25 was chosen to include a selection of scenarios that might be encountered in practice ; the important factor is the size of @xmath25 with respect to the observation window and the grid .",
    "when @xmath25 is small compared to the size of the grid , the cells become approximately independent . it should be harder to obtain a good approximation to the latent field for larger values of @xmath25 , as in this case spatial dependence can be significant for cells a moderate distance apart on the grid . to reduce the possibility of results depending on an artefact of the choice of grid size , two different resolutions were used : @xmath51 and @xmath124 in the extended space . for the comparison of @xmath125 and @xmath126 ,",
    "the parameter @xmath113 was set to zero .",
    ".[tab : gmrfapproxtest ] table illustrating the ability of a gmrf to approximate a gf on a unit square observation window . `",
    "grid ' is the size of the dft grid used , giving respective output grid sizes of @xmath127 and @xmath51 ; ` mse 13 ' denotes the mean square error for respective neighbourhood sizes 13 , with computation time in seconds in parenthesis ; and ` bias 13 ' is the mean bias .",
    "optimisation was performed using the ` bfgs ' method in r s optim command .",
    "using the nelder - mead simplex method , the two exceptional values , marked @xmath128 and @xmath129 had improved mse s of 0.284 and 0.378 . on these occasions , it appeared that the bfgs optimiser had converged to a sub - optimal set of parameters .",
    "[ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : estprobabilitiesgmrf]mean square error in estimating probabilities , mse@xmath130 , using each of the three inla approximations , relative to mse@xmath130 for the mala algorithm .",
    "a @xmath128 in the first column indicates the scenarios where mala outperformed inla2 .",
    "the left table are the results for fixed spatial @xmath131 and the right table gives the values for fixed spatial @xmath132 .",
    "results from the second simulation study are shown in table [ tab : estprobabilitiesgmrf ] and figure [ fig : inlavsmalaqtgmrf ] .",
    "there are two main conclusions to be drawn from these results .",
    "firstly , in figure [ fig : inlavsmalaqtgmrf ] , the ` s ' shape is no longer apparent for the inla methods , however in the plot for inla2 , there is a noticeable upward bias in the centre of the plot . also from the plots ,",
    "it is clear that mala did not perform well on two occasions , and inla on one .",
    "the main second conclusion from this simulation study is despite the fact that inla now shows less bias , mala nevertheless still outperforms both inla2 and inla3 .",
    "the median relative increase in mse comparing mala to inla2 was 3 over all scenarios and 1.1 comparing mala to inla3 . as was the case for scenarios 118 , mala performs better for fixed spatial @xmath132 , with a median increase of 1.37 for inla3 whereas for @xmath131 , inla3 performed better at a median increase of 0.87 .     illustrating the estimation of quantiles @xmath133 by those inferred by each algorithm @xmath134 $ ] in scenarios @xmath135@xmath136 .",
    "mala in left plot , gain2 in the middle plot and gain3 in the right hand plot.,title=\"fig:\",scaledwidth=30.0%,scaledwidth=30.0% ]   illustrating the estimation of quantiles @xmath133 by those inferred by each algorithm @xmath134 $ ] in scenarios @xmath135@xmath136 .",
    "mala in left plot , gain2 in the middle plot and gain3 in the right hand plot.,title=\"fig:\",scaledwidth=30.0%,scaledwidth=30.0% ]   illustrating the estimation of quantiles @xmath133 by those inferred by each algorithm @xmath134 $ ] in scenarios @xmath135@xmath136 .",
    "mala in left plot , gain2 in the middle plot and gain3 in the right hand plot.,title=\"fig:\",scaledwidth=30.0%,scaledwidth=30.0% ]",
    "in this article , we have provided a tour of the mathematical and statistical techniques behind spatial prediction for log - gaussian cox processes .",
    "we have independently evaluated a previously published method for approximating spatially continuous gaussian processes with gmrf and conducted a critical comparison of two methodologies for predictive bayesian inference in this class of models .",
    "a suite of functions ( as well as wrapper functions for the approximate bayesian predictive inference for inla ) have been made freely available in the ` lgcp ` r package @xcite .",
    "our restriction in this paper to spatial , rather than spatio - temporal prediction is primarily ease of exposition .",
    "however , and unsurprisingly , the spatio - temporal concept is computationally more demanding . in pilot runs ,",
    "even inla was found to be quite slow for spatio - temporal prediction on a regular grid , due to the hugely increased dimensionality of the problem and a corresponding increase in the complexity of dependence patterns in the precision matrix ( see section [ sect : gmrfapprox ] ) .",
    "furthermore , we have restricted our choice of mcmc methods to the metropolis adjusted langevin algorithm ( mala ) rather than investigating more sophisticated sampling techniques such as riemann manifold langevin or hamiltonian monte carlo @xcite .",
    "our reasons for this choice are ease of implementation , stability and the fact that mala has been well studied in the literature , with various theoretical results available concerning practical implementation .",
    "the authors are aware that the methods of @xcite have better theoretical mixing properties .",
    "we have demonstrated that mcmc can yield more accurate estimates of predictive probabilities compared with inla - based methods for this class of models , depending on the chosen settings .",
    "furthermore the predictive probabilities from mcmc can be obtained comparatively quickly , and show less bias compared with those from inla .",
    "the inferential technique of producing a gmrf approximation to a gaussian field and then performing inference via the integrated nested laplace approximation should therefore be regarded with appropriate caution .",
    "this article also opens up the question of the utility of gradient - based mcmc methods for inference in log - gaussian cox process models assuming a latent gaussian markov random field ; the default method of inference for these models would appear to be a blocked gibbs sampling strategy eg .",
    "@xcite .",
    "we have not addressed the full capabilities of both software implementations : ` lgcp ` and ` inla ` . in particular , `",
    "inla ` provides access to inference for a wide class of latent gaussian models , whilst ` lgcp ` is restricted to spatial and spatio - temporal log - gaussian cox processes .",
    "furthermore , the ` inla ` package also provides a framework for the estimation of hyperparameters , which ` lgcp ` does not ; this is known to be a very challenging sampling task for mcmc .",
    "taylor , b. , t.  davies , b.  rowlingson , and p.  diggle ( 2011 ) .  an r package for inference with spatiotemporal log - gaussian cox processes .",
    "submitted , available from urlhttp://www.arxiv.org / pdf/1110.6054 ."
  ],
  "abstract_text": [
    "<S> we investigate two options for performing bayesian inference on spatial log - gaussian cox processes assuming a spatially continuous latent field : markov chain monte carlo ( mcmc ) and the integrated nested laplace approximation ( inla ) . </S>",
    "<S> we first describe the device of approximating a spatially continuous gaussian field by a gaussian markov random field on a discrete lattice , and present a simulation study showing that , with careful choice of parameter values , small neighbourhood sizes can give excellent approximations . </S>",
    "<S> we then introduce the spatial log - gaussian cox process and describe mcmc and inla methods for spatial prediction within this model class . </S>",
    "<S> we report the results of a simulation study in which we compare mala and the technique of approximating the continuous latent field by a discrete one , followed by approximate bayesian inference via inla over a selection of 18 simulated scenarios . </S>",
    "<S> the results question the notion that the latter technique is both significantly faster and more robust than mcmc in this setting ; 100,000 iterations of the mala algorithm running in 20 minutes on a desktop pc delivered greater predictive accuracy than the default ` inla ` strategy , which ran in 4 minutes and gave comparative performance to the full laplace approximation which ran in 39 minutes . </S>"
  ]
}