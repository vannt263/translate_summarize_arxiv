{
  "article_text": [
    "main problem of the mathematical statistics and simulation is connected with insufficiency of primary statistical data . in this situation",
    ", the bootstrap method can be used successfully ( efron , tibshirani 1993 , davison , hinkley 1997 ) . if a dependence between characteristics of interest and input data is very composite and is described by numerical algorithm then usually it applies a simulation . by this the probabilistic distributions of input data are not estimated because the given primary data has small size and such estimation gives a bias and big variance .",
    "the bootstrap method supposes that random variables are not generated by a random number generator during simulation in accordance with the estimated distributions but ones are extracted from given primary data at random .",
    "various problems of this approach were considered in previous papers ( andronov et al . 1995 , 1998 ) .",
    "we will consider the known function @xmath0 of m independent continuos random variables @xmath1 it is assumed that distributions of random variables @xmath2 are unknown , but the sample population @xmath3 is available for each @xmath4 . here @xmath5 is the size of the sample @xmath6 .",
    "the problem consists in estimation of the mathematical expectation @xmath7    the bootstrap method use supposes an organization of some realizations of the values @xmath8 . in each realization",
    "the values of arguments are extracted randomly from the corresponding sample populations @xmath9 .",
    "let @xmath10 be a number of elements which were extracted from the population @xmath6 in the @xmath11-th realization .",
    "we denote @xmath12 and name it the l - th subsample .",
    "the estimator @xmath13 of the mathematical expectation @xmath14 is equal to an average value for all r realizations : @xmath15    our main aim is to calculate and to minimize the variance of this estimator .",
    "the variance will depend upon two factors : 1 ) a calculation method of the function @xmath0 ; 2 ) a formation mode of subsamples @xmath16 .",
    "the next two sections will be dedicated to these questions . in section 4",
    "we will show how to decrease variance @xmath17 using the dynamic programming method .",
    "we suppose that the function @xmath0 is calculated by a calculation tree .",
    "a root of this tree corresponds to the computed function @xmath18 .",
    "it is the vertex number k. the vertices numbers @xmath19 correspond to the input variables @xmath20 .",
    "the rest vertices are intermediate ones .",
    "they correspond to intermediate functions @xmath21 ( see fig.1 ) .",
    "only one arc @xmath22 comes out from each vertex @xmath23 , @xmath24 .",
    "it corresponds to a value of the function @xmath25 .",
    "we suppose that @xmath26 .",
    "we denote @xmath27 as a set of vertices from which arcs come into the vertex @xmath23 , and @xmath28 as a corresponding set of variables ( arcs ) : @xmath29 .",
    "it is clear that @xmath30 for @xmath31 ; @xmath32 , @xmath33 .",
    "we suppose that a numbering of the vertices is correct : if @xmath34 than @xmath35 .",
    "now , function value can be calculated by the sweep method . at the beginning , we extract separate elements @xmath36 from populations @xmath9 , then calculate the function values @xmath37 , @xmath38 successively . after r such runs the estimator @xmath13",
    "is computed according to the formula ( [ thetaest ] ) .",
    "an analysis of this method was developed in the previous papers of authors .",
    "the hierarchical bootstrap method is based on the wave algorithm . here",
    "all values of the function @xmath25 for each vertex @xmath39 should be calculated all at once .",
    "they form the set @xmath40 , where @xmath41 is number of realizations ( sample size ) .",
    "getting one realization @xmath42 consists of choosing value from each corresponding population @xmath43 , and calculation of @xmath25 value . by this",
    "we suppose that a random sample with replacement is used when each element from @xmath6 is choosen independendly with the probability @xmath44 .",
    "further on , this procedure is repeated for next vertex .",
    "finally we get @xmath45 as values of the population @xmath46 .",
    "their average gives the estimator @xmath13 by analogy with formula ( [ thetaest ] ) .",
    "the aim of this section is to show how to calculate variance @xmath17 of the estimator ( [ thetaest ] ) .",
    "it is easy to see that in the case of hierarchical bootstrap the variance @xmath17 is function of sample sizes @xmath47 .    in the previous papers of authors",
    "the variance @xmath17 was calculated using the @xmath48-pairs notion ( andronov et .",
    "al , 1996 , 1998 ) .",
    "then , it was considered as continuos function of variables @xmath49 , and reduced gradient method was used .",
    "but now we need other approach for the calculation of @xmath17 .",
    "we use taylor decomposition of function @xmath50 in respect to mean @xmath51 : @xmath52    where = @xmath53 is the matrix of second derivatives of the function @xmath50 , + @xmath54 is euclidean norm of vector @xmath55 .",
    "it gives the following decomposition : @xmath56    if @xmath57 is a random vector with mutual independent components @xmath2 , @xmath58 , @xmath59 , then @xmath60    @xmath61    now we suppose that @xmath62 and @xmath63 are some values from sample population @xmath64 .",
    "let @xmath65 denote the covariance of two elements @xmath66 and @xmath67 with different numbers @xmath11 and @xmath68 : @xmath69    let @xmath62 and @xmath63 correspond to the elements @xmath66 and @xmath67 accordingly . because we extract @xmath62 and @xmath70 from @xmath6 at random and with replacement ,",
    "then the event @xmath71 occurs with the probability @xmath44",
    ". then @xmath72 therefore @xmath73    if the values @xmath74 and @xmath75 correspond to subfunction @xmath76 and the sample population @xmath77 , then formulas ( [ expr2 ] ) , ( [ expr31 ] ) and ( [ expr32 ] ) give @xmath78    now we can get from ( [ expr6 ] ) @xmath79 where the variance @xmath80 can be determined from ( [ expr7 ] ) by @xmath81 : @xmath82    finally we have @xmath83+\\ldots , \\label{expr10}\\ ] ] or @xmath84 @xmath85 .",
    "\\label{expr11}\\ ] ]    by this we suppose that variances @xmath86 of input random variables @xmath87 are known .",
    "for example , it is possible to use estimators of these variances , calculated on given sample populations @xmath88 .",
    "if the vertex @xmath89 belongs to the initial level of the calculation tree ( it means that @xmath90 ) then @xmath91 , @xmath92 is known value , @xmath93 .",
    "therefore @xmath94    another covariances and variances are calculated recurrently in accordance with formulas ( [ expr7]),([expr9 ] ) , ( [ expr11 ] ) , ( [ expr12 ] ) from vertices with less numbers to vertices with great numbers . finally we get the variance @xmath17 of interest as the variance for root of the calculation tree : @xmath95 where @xmath96 .",
    "as it was just mentioned , we will consider the variance @xmath17 as a function of sample sizes @xmath49 and denote it @xmath97 .",
    "our aim is to minimize this function in respect to variables @xmath49 by linear restriction , or , by other words , to solve the following optimization problem : @xmath98 by restriction @xmath99 where @xmath100 , @xmath101 and @xmath102 are integer non - negative numbers .",
    "now we intent to apply the dynamic programming method ( minox 1989 ) .",
    "let us solve the optimization problem ( [ probl ] ) , ( [ restric ] ) . our function of interest @xmath50 is calculated and simulated recurrently , using the calculation tree ( see section 2 ) . in accordance to the dynamic programming technique",
    ", we have `` forward '' and `` backward '' procedure .    during `` backward '' procedure , we calculate recurrently so - called bellman function @xmath103 , @xmath24 , @xmath104 , @xmath105 .",
    "let us consider the subfunction @xmath106 , that corresponds to the vertex @xmath23 .",
    "this subfunction directly depends on variables @xmath107 , @xmath108 , which correspond to incoming arcs for the vertex @xmath23 .",
    "additionally @xmath25 depends on variables @xmath109 and @xmath110 from which there exists path from leaves to the vertex @xmath23 of our calculation tree .",
    "let @xmath111 denote corresponding set of variables @xmath109 and @xmath110 .",
    "now we need to denote some auxiliary functions .",
    "let us introduce the following notation @xmath112    then we are able to write in accordance with ( [ expr10 ] ) : @xmath113    now we have from ( [ dyn1 ] ) , ( [ expr9 ] ) and ( [ expr10 ] ) @xmath114\\right\\}=\\ ] ] @xmath115,\\ ] ] so @xmath116    note that it follows from ( [ expr8 ] ) and ( [ dyn1 ] ) that our variance of interest ( [ expr13 ] ) is @xmath117    values @xmath118 depend on the sample sizes @xmath5 for all @xmath119 .",
    "we will mark this fact as @xmath120 .",
    "now we are able to introduce above mentioned bellman functions : @xmath121 where minimization is realized with respect to non - negative integer variables @xmath5 that are satisfied the linear restriction @xmath122    it is clear that optimal value of variance @xmath123 for the problem ( [ probl ] ) , ( [ restric ] ) is equal to @xmath124 .",
    "bellman functions @xmath103 are calculated recurrently from @xmath125 to @xmath126 for @xmath105 and @xmath104 .",
    "basic functional equation of dynamic programming has the following form : @xmath127 where minimaiztion is realized with respect to non - negative integer variables @xmath41 and @xmath128 that satisfy the linear restriction @xmath129    the initial values of @xmath130 are determined with the tree leaves by formulas ( [ expr12 ] ) , ( [ dyn1 ] ) and ( [ dyn5 ] ) : @xmath131}\\sigma_v^2= \\label{dyn9}\\ ] ] @xmath132}\\right),\\qquad v=1,2,\\ldots , m,\\ ] ] where @xmath133 $ ] - integer part of number @xmath134 ,    thus the `` backward '' procedure is a recurrent calculation of bellman functions @xmath103 for @xmath24 , @xmath105 , @xmath104 by using formulas ( [ dyn9 ] ) , ( [ dyn7 ] ) .",
    "finally we get the minimal variance @xmath135    to calculate the optimal sample sizes @xmath136 we should apply `` forward '' procedure of dynamic programming technique . at first , we find @xmath137 and @xmath138 by solving the equation @xmath139 where minimization is realized by condition @xmath140    let @xmath141 , @xmath142",
    ". then we recurrently determine by analogy the rest @xmath143 and @xmath144 for @xmath145 : @xmath146 by condition @xmath147    moreover we put @xmath148    finally the optimal sizes @xmath149 for @xmath90 are determined by the following way : @xmath150 .",
    "\\label{dyn16}\\ ] ]",
    "andronov , a. , merkuryev ,  yu .  ( 1996 ) optimization of statistical sizes in simulation . in : _ proceedings of the 2-nd st .",
    "petersburg workshop on simulation _ , st .",
    "petersburg state university , st .",
    "petersburg , 220 - 225 .",
    "andronov ,  a. , merkuryev ,  yu .  ( 1998 ) controlled bootstrap method and its application in simulation of hierarchical structures . in : _ proceedings of the 3-d st .",
    "petersburg workshop on simulation _ , st .",
    "petersburg state university , st .",
    "petersburg , 271 - 277 ."
  ],
  "abstract_text": [
    "<S> the bootstrap method application in simulation supposes that value of random variables are not generated during the simulation process but extracted from available sample populations . in the case of hierarchical bootstrap the function of interest is calculated recurrently using the calculation tree . in the present paper </S>",
    "<S> we consider the optimization of sample sizes in each vertex of the calculation tree . </S>",
    "<S> the dynamic programming method is used for this aim . </S>",
    "<S> proposed method allows to decrease a variance of system characteristic estimators . </S>",
    "<S> + bootstrap method , simulation , hierarchical calculations , variance reduction </S>"
  ]
}