{
  "article_text": [
    "singular value decomposition has proved to be useful in a wide range of applications , where a linear relation is a suitable model for a big number of variables .",
    "its main strength is in its ability to abstract most of the meaningfull relation in a much smaller subspace  @xcite,@xcite,@xcite,@xcite .    even though the calculations are very simple in essence , the method is at its best when dealing with big dimension matrices of data , and the computational resources to perform the calculations are often insufficient .    in this document",
    "i propose an algorithm that allows to deal with the matrix by pieces , so it does not need to define big matrices or operate with them , but only smaller blocks .",
    "the usual algorithm to perform the decomposition is made in two steps : first , a transformation is found that takes the matrix to bidiagonal form , and then , the bidiagonal matrix is decomposed with a different procedure .",
    "the first step is carried by means as a certain class of symmetrical orthogonal ( or unitary ) matrices called householder transformations  @xcite .",
    "a householder transformation is defined by a unitary vector this way : @xmath1 where @xmath2 is an unitary vector of dimension ( number of components ) n , and @xmath3 is the identity with n rows and columns .",
    "it is easy to see that the matrix that corresponds to this transformation is symmetric , which means that it is not changed by transposition ( changing rows by columns , and vice versa ) . that impplies that it is its own inverse , e.i . , that its square is the identity matrix .",
    "a householder operation can be found that , when multiplied by the left , turns the all but one of the entries of the first column of a matrix into zero , but preserving the sum of the squares of the entries of that column .",
    "@xmath4    the unitary vector that defines the householder transformation can be computed as having one part proportional to the part that is to turn into zero .",
    "being an unitary vector , the proportionality factor can be best represented by some unknown factor @xmath5 divided by the norm of that part of the vector , the square root of the sum of the squares . in our example : @xmath6    imposing the condition that it makes the required entries of the vector 0 , the unknown factor turns out to be : @xmath7    we can express any matrix in block form , separating the first row and column , in the same way as the householder matrix .",
    "this latter has a very simple form :    @xmath8    where @xmath9 is the first diagonal entry of the matrix , and the remaining of the first row and first column are expressed by a norm and an unit vector , @xmath10 and @xmath11 .",
    "n would be the norm of the first column , that is @xmath12    multiplying the matrix @xmath13 by the householder matrix by left , we get the following result : @xmath14    a householder transformation applied on the right , can have the same effect of annihilating all but one of the entries of the first row .",
    "but , if we try to get a diagonal block matrix using two consecutive householder transformation , one on the left and one on the right , the second is going to turn the zero entries produced by the first into other number , thus failing to produce a block diagonal matrix .",
    "we can , however , leave one nonzero element in the first column , and two nonzero elements in the first row , using on the left a householder transformation that mixes the contents of the rows only from the second element on :        next we can focus on the right - bottom block of the matrix where the zero elements have not yet been produced .",
    "the right and left householder transformation for that block can be computed , so it will be left with only two nonzero elements in the first row and column .",
    "then , the same is done for the remaining block , and so on",
    ".        the number of steps necessary to bring the matrix to a bidiagonal form has been @xmath15 , where @xmath16 is the lower dimension of the matrix .",
    "it is a very fast step .      to take the bidiagonal matrix to a diagonal form",
    "is not so easy , and can not be done in a fixed number of steps , but has to be an iterative process .",
    "the most efficient method for this is the * qr factorisation *  @xcite .",
    "starting from our upper bidiagonal matrix , the steps to be performed are :    1 .",
    "construct the orthogonal transformation to bring the matrix to _ lower _ triangular form , with householder transformations like the ones we used in the last part , but only applied by right .",
    "+    2 .   apply the transformation by left as well .",
    "the result will be a matrix that is not either upper or lower triangular , but has the values more concentrated on the diagonal .",
    "+    3 .   repeat the procedure .",
    "+     all the steps are done with orthogonal transformation , and that ensures that the singular values are preserved . in each step , besides , the square of every diagonal element are increased with the squares of the other elements in the row . that , together with the preservation of singular values , ensures convergence .",
    "the last procedure yields the two square unitary matrices , and the singular values diagonal must be nonsquare .",
    "that means that a lot of memory is needed for the unitaries , wich are huge matrices with double precision .",
    "a lighter alternative is to compute the _",
    "decomposition  @xcite , wich give us just slices of unitary matrices .",
    "the left one has as much rows as our matrix , but only as much columns as the rank ( number of nonzero singular values ) of the matrix .",
    "and the right one has as much columns as our matrix , but only as much rows as the rank of the matrix .",
    "@xmath17        to compute the economy decomposition , a lesser computational effort is needed .",
    "suppose that we have in our matrix more rows than columns , or that we took it to that form by transposing it .",
    "then , the symmetric matrix @xmath18 will be smaller in dimensions than a. we can use one of the usual algorithms to diagonalise it , and get @xmath19 it is very easy to compute the inverse of square root of this matrix , all that is necessary is to take inverses of square roots of the diagonal elements of d. then , we can express our matrix like this : @xmath20 it can be easily shown that @xmath21 is a slice of a unitary matrix : @xmath22 if the square matrix @xmath18 hapens to be singular , then @xmath23 is also a slice of a square unitary , and @xmath24 is smaller , but allways invertible .",
    "latent semantic analysis , and other techniques , are based on the fact that some big matrices can be accurately represented only by the bigger terms of their spectral decomposition , that is , only the bigger singular values . the usual convention to represent",
    "the diagonal matrix of singular values is in an ordered form , from the biggest , in the first element , to the smallest .",
    "the matrices are usually rather sparse , and with only some column and row swaps , we can take the bigger elements up and to the left , so the matrix is going to be closer to the desired form .",
    "the singular value problem , as the eigenvalue problem , can be seen as a maximisation of a certain value .",
    "the solution of the following problem gives the left and right eigenvectors corresponding to the highest singular value : @xmath25 all along this work , the maximisation of the values in the upper left corner of the matrix is going to be used to arrive near to the diagonal form .",
    "the first thing that can be done , is just arranging the rows and columns to take the higher values to the upper left corner of the matrix .",
    "there is a well known result , that tells us that the frobenius norm of a matrix  @xcite , that is , the trace of its square @xmath26 , is the sum of the squares of the singular values , and is as well the sum of the squares of all the elements of the matrix .",
    "@xmath27 if we consider the other square matrix @xmath28 the result is the same , because in that case we just swap indexes @xmath29 and @xmath30 . with the svd decomposition of the matrix",
    ", we only need to remember that a unitary matrix does not affect the trace : @xmath31 where @xmath32 is a square @xmath33 diagonal matrix with the singular values .",
    "a vector can be computed with the norms of each row , and the cartesian norm of this vector will be the frobenius norm of the matrix .",
    "the same can be done with the columns .",
    "this two vectors can be used to sort the rows and columns of the matrix to get the higher values in the upper left corner .",
    "afther that sorting , a definition of the blocks can be done , with some criterion based on those row and column norms .",
    "the blocks can be defined , for example , as to put a certain percentage of the whole frobenius norm in the first column - block , and a certain percentage on the first row - block .    if only the higher singular values are needed , it is not necessary to decompose the whole matrix , but instead two steps can be taken :    1 .",
    "separate the subspace of the highest singular values from that of the lower singular values 2 .",
    "decompose only the block corresponding to the highest singular values",
    "in the proposed algorithm , the matrix is prepared so as to have more rows than columns ( transposed if necessary ) and the columns are cut in such a way that a fraction of the total square frobenius norm is enclosed in the first column block .",
    "the rows are separated in such a way to have square upper left block .",
    "then , each block can be considered separately , and that can require considerably less computer resources .",
    "the first thing that can be done , is to generalise the concept of a householder transformations to any partition of the rows and columns of the matrix in four blocks .",
    "the general form of such a transformation is the following : @xmath34 where @xmath35 is a slice of a unitary matrix with @xmath16 rows and @xmath36 columns , and @xmath23 is a slice of unitary matrix with @xmath37 rows and @xmath36 columns .",
    "this transformation would be a @xmath38 unitary matrix , to be multiplied by left to a matrix with @xmath39 rows divided in two blocks with @xmath16 and @xmath37 rows , or by right to a matrix with @xmath39 columns split in blocks with @xmath16 and @xmath37 columns . @xmath40 and @xmath41 are diagonal matrices of rank @xmath36 with the property @xmath42    this matrix y symmetric and it is its own inverse , two of the properties of a householder transformation .",
    "a householder transformation shifts the sign of only one vector , but this transformation can be shown to change the sign of any vector that lies within a subspace . this subspace is defined by a set of mutually orthogonal vectors , which can be arranged in a column or row block .",
    "the matrix can be written also like this :    @xmath43    a transformation like this can be used , for example , to annihilate a block of a matrix , just as in the usual svd method . here",
    "are the steps to annihilate a nondiagonal block by multiplication by left :    1 .",
    "the two relevant blocks , the ones that are going to be transformed to annihilate one of them , are decomposed .",
    "full svd is not necessary , a simple decomposition unitary - symmetric will do .",
    "@xmath44 2 .",
    "the unitary factors of the two blocks are taken as the @xmath35 and @xmath23 matrices of the householder matrix 3 .",
    "if we take the first column of a 2x2 blocks matrix as the relevant blocks , the action of the householder matrix will give in the nondiagonal block : @xmath45 for this block to be zero , the parameters of the matrix must be : @xmath46    there is another way of doing it as well , wich will probably take more time , but is based on a well known technique : the * gsvd : generalised single value decomposition*. this is the simultaneous decomposition of two matrices : @xmath47 where , in the economy representation , @xmath35 and @xmath23 are slices of unitary matrices with the same dimensions than those of the other method .",
    "x is a square matrix .",
    "the matrices @xmath48 and @xmath49 are square and diagonal , and fulfil @xmath50 .",
    "they can be used indeed as @xmath40 and @xmath41 respectively ( @xmath51 ) in the householder matrix .    with this kind of householder transformations",
    ", we can perform a complete ( not economy ) blockwise decomposition of a matrix , iterating the annihilation of the two nondiagonal blocks as shown in the figure :",
    "to be able to perform the decomposition directly in the economy representation , a version of the * eigenvalue * ( spectral ) decomposition is needed .",
    "the formula shown for annihilating blocks only works multiplied by one side , but it does not work to annihilate nondiagonal blocks acting on both sides , as an equivalence transformation .    for a 2x2 number symmetric matrix , the problem of diagonalising it amounts to finding a certain number @xmath52 that fullfills : @xmath53 the condition is better derived from the null elements of the matrix , and is : @xmath54 defining @xmath55 the condition becomes very simple , because @xmath56 @xmath57 the solution is easily found to be : @xmath58 on the other hand , if the entries of the matrix are suitably sized blocks , the condition is a lot more complicated . we can represent the unitary matrix as being constructed with blocks @xmath52 , @xmath59 and @xmath60 having the form shown above for householder matrices . @xmath61",
    "the noncommutativity of the matrices does not allow for an easy solution as that with numbers .",
    "furthermore , to solve the condition for the nondiagonal elements should not be possible , except in the 2x2 or 2x3 blocks case , because that could be translated to solve analytically a general equation of order higher than five . that , according to abel s theorem , is not possible .",
    "but there is something we can do , and it is working with traces .",
    "we can either maximise the trace of the first diagonal block , or minimise the trace of the square of the nondiagonal block .    on the other hand , using slices of the unitary matrices , and their complement ( the slice that is lacking for the total unitary ) we can build an unitary matrix @xmath48 that allow us to isolate just a subspace to work on it , thus reducing substantially the dimensionality of the problem . @xmath62 note that if block @xmath63 is full rank , then @xmath64 would be square and there would not be a @xmath65 .    applying this unitary transformation to the matrix",
    "we get : @xmath66 the trace of the first diagonal block can be recovered from this matrix as the sum of the first and third diagonal blocks .",
    "if @xmath64 and @xmath67 are chosen as the unitaries that take the nondiagonal block @xmath63 to diagonal form @xmath68 , things are very simplified in the above expression @xmath69 @xmath70 now , a transformation should be chosen that maximises the trace of the two first blocks .",
    "this is accomplished by a transformation that diagonalises the reduced matrix @xmath71 .",
    "the computation of such transformation does not represent a big computational cost , because of the relatively small size of this matrix .",
    "@xmath72 where @xmath73 and @xmath74 .    the final form of the matrix will be : @xmath75 where : @xmath76 and @xmath77 the trace of the two first blocks will now be bigger , because the highest eigenvalues of the reduced matrix are concentrated in @xmath78 .",
    "this procedure can be iterated , and each time the trace of the first blocks will be bigger .",
    "then , an algorithm can is proposed for block - svd of a big matrix :    1 .",
    "make sure that there are more ( or the same ) rows as columns , or transpose otherwise .",
    "this first steps are performed with the sparse csv triplet representation .",
    "2 .   compute the rows and columns euclidean norm 3 .",
    "order rows and columns in descending norm order 4 .",
    "choose a cutting point for the rows and columns .",
    "this can be made in several ways .",
    "the one tried here is taking the point where at least 2/3 of the frobenius norm is in the first column block , and cut the row blocks as to yield a square first diagonal block .",
    "create the blocks the appropriate size 6 .",
    "create the householder unitary matrix that annihilates block 12 7 .   from that starting point ,",
    "iterate the maximisation of the trace of block 11 of the square matrix @xmath18 until a certain tolerance 8 .",
    "perform svd of the first block 9 .",
    "this gives an approximation of the eigenvalue decomposition of @xmath18 .",
    "multiplying the initial matrix ( by blocks ) times the inverse of the square root ( also by blocks ) the two - block relevant vertical slice of the economy unitary left ( @xmath35 in @xmath80matrix are obtained .",
    "the relevant slice of the other unitary is the first vertical 2-block slice of the unitary obtained by iteration ( @xmath23 )",
    "a matrix of the occurrences of 3204 words in 17780 documents was used .",
    "the procedure of cutting the blocks gave blocks with the following characteristics :    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     the criterion used for convergence was that the ratio @xmath81 became 1/10000 of its initial value ( about 1.16 ) .",
    "it can be seen that the trace of the first block is allways increased , as expected , but the sum of eigenvalues of the nondiagonal block oscillates after some iterations .",
    "the availability of memory for matlab 6.5 does not allow to perform the complete svd decomposition of the matrix , but it is possible to compute the singular values .",
    "the singular values contained in the first @xmath82 block then account for 75% of the square block of the matrix .",
    "the first 215 obtained singular values had differences under 1e-10 with those calculated by the usual algorithm , except for the lowest four .",
    "this work was sponsored by the european comission under the contract fp6 - 027026 k - space and foundation for the future of colombia colfuturo .",
    "i would also like to aknowledge the valuable guidance of professor c. j. van rijsbergen and useful advise from mark girolami in the developement of this work .",
    "1 e.  anderson , bai , c.  bischof , j.  demmel , j.  dongarra , j.  du  croz , a.  greenbaum , s.  ammarling , a.  mckenney , and d.  sorensen . , chapter singular value decomposition , pages 1923 .",
    "society for industrial mathematics , 1999 .",
    "david gleich and leonid zhukov .",
    "svd based term suggestion and ranking system . in _",
    "icdm 04 : proceedings of the fourth ieee international conference on data mining ( icdm04 ) _ , pages 391394 , washington , dc , usa , 2004 .",
    "ieee computer society .",
    "p.  wiemer - hastings , k.  wiemer - hastings , and a.  graesser .",
    "how latent is latent semantic analysis ? in _ proceedings of the 16th international joint congress on artificial intelligence _ , pages 932937 , san francisco , 1999 .",
    "morgan kaufmann ."
  ],
  "abstract_text": [
    "<S> two methods to decompose block matrices analogous to singular matrix decomposition are proposed , one yielding the so called economy decomposition , and other yielding the full decomposition . </S>",
    "<S> this method is devised to avoid handling matrices bigger than the biggest blocks , so it is particularly appropriate when a limitation on the size of matrices exists . </S>",
    "<S> the method is tested on a document - term matrix ( 17780@xmath03204 ) divided in 4 blocks , the upper - left corner being 215@xmath0215 . </S>"
  ]
}