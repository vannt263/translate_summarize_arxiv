{
  "article_text": [
    "since its induction @xcite , distance correlation has had many applications in , e.g. , life science @xcite and variable selection @xcite , and has been analyzed @xcite , extended @xcite in various aspects . if distance correlation were implemented straightforwardly from its definition , its computational complexity can be as high as a constant times @xmath0 for a sample size @xmath2 .",
    "this fact has been cited for numerous times in the literature as a disadvantage of adopting the distance correlation . in this paper , we demonstrate that an o(@xmath3 ) algorithm for a version of the distance correlation exits .",
    "the main idea behind the proposed algorithm is to use an idea rooted in the the avl tree structure @xcite .",
    "the same idea has been utilized to develop fast algorithm for computing the kendall s @xmath4 rank correlation coefficient @xcite @xcite .",
    "we extend it to make it suitable for our purpose .",
    "the derivation of the fast algorithm also involves significant reformulation from the original version of the distance correlation .",
    "details are presented in this paper .    in simulations ,",
    "not only we demonstrate the effectiveness of the fast algorithm , but also we testify that the advantage of using distance correlation ( in comparison with other existing methods ) become more evident when the sample sizes increase .",
    "these experiments become feasible due to the availability of the proposed fast algorithm . in one experiment ( see details in section [ sec : screening ] ) , we increased the sample size by @xmath5 fold from a previously published simulation study .",
    "the rest of this paper is organized as follows .",
    "section [ sec : reviewdistcorr ] reviews the distance covariance / correlation and its relevant properties . in section [ sec : unbiased ] , we consider a reformulation of the distance covariance , such that the new estimator is both unbiased and a u - statistic . in section [ sec : algorithm ] , an algorithm with the average complexity of @xmath6 was presented .",
    "extensive simulations are presented in section [ sec : simulations ] to demonstrate the additional capability we obtained due to the proposed fast algorithm .",
    "finally , some concluding remarks are made in section [ sec : conclude ] .",
    "detailed description of the algorithm is relegated to the appendix , along with most of the technical proofs .",
    "distance covariance and distance correlation was introduced in 2005 by one of the co - authors of this paper , g. j. szkely , in several lectures to address the deficiency of pearson s correlation , namely that the pearson s correlation can be zero for dependent variables . in the following , we start with a definition of the distance covariance .",
    "the population distance covariance between random vectors @xmath7 and @xmath8 with finite first moments is the nonnegative number @xmath9 defined by a weighted @xmath10 norm measuring the distance between the joint characteristic function ( c.f . )",
    "@xmath11 of @xmath7 and @xmath8 , and the product @xmath12 of the marginal c.f.s of @xmath7 and @xmath8 .",
    "if @xmath7 and @xmath8 take values in @xmath13 and @xmath14 , respectively , @xmath15 is @xmath16 where @xmath17 .",
    "the integral exists provided that @xmath7 and @xmath8 have finite first moments .",
    "this immediately shows that distance covariance is zero if and only if the underlying random variables are independent .",
    "the beauty of kernel @xmath17 is that the corresponding sample statistic has the following surprisingly simple form .",
    "denote the pairwise distances of the @xmath7 observations by @xmath18 and the pairwise distances of the @xmath8 observations by @xmath19 for @xmath20 and denote the corresponding double centered distance matrices by @xmath21 , and @xmath22 where @xmath23    @xmath24    it is clear that the row sums and column sums of these double centered matrices are 0 .",
    "the squared sample distance covariance is the following simple formula @xmath25    the corresponding squared sample variance is @xmath26 and we can define the sample distance correlation as the standardized sample covariance :    @xmath27    for more details see and a discussion paper @xcite .",
    "it is clear that @xmath28 is rigid motion invariant and scale invariant . for recent applications of distance correlation ,",
    "see e.g. , * li2012 and .",
    "the population version of distance covariance and distance correlation can be defined without characteristic functions , see .",
    "this definition is as follows .",
    "let @xmath29 and @xmath30 be random variables with finite expectations .",
    "the random distance functions are @xmath31 and @xmath32 . here",
    "the primed random variable @xmath33 denotes an independent and identically distributed ( i.i.d . )",
    "copy of the variable @xmath7 , and similarly @xmath34 are i.i.d .",
    "introduce the real - valued function @xmath35 = e|x - x| = \\int |x - x'| df_x(x'),\\ ] ] where @xmath36 is the cumulative distribution function ( cdf ) of @xmath7 , and @xmath37 which is a real - valued random variable .",
    "for simplicity we write @xmath38 and @xmath39 .",
    "next we define the counterpart of centered distance matrices .",
    "the centered distance function is @xmath40.\\ ] ] for random variables we have @xmath41,\\end{aligned}\\ ] ] where @xmath42 = e[m(x , f_x ) ] = \\int \\int |x - x'| \\",
    ", df_x(x ' ) \\ , df_x(x).\\ ] ]    similarly define the centered distance function @xmath43 and the random variable @xmath44 . now for @xmath45 i.i.d . , and @xmath34 i.i.d . ,",
    "such that @xmath7 and @xmath8 have finite expectations , the population distance covariance @xmath46 is defined by @xmath47.\\ ] ] we have that @xmath48 is always nonnegative , and equates zero if and only if @xmath7 and @xmath8 are independent .",
    "it is clear by inspection that without further efforts the implementation of the sample distance covariance and the corresponding sample distance correlation requires o(@xmath0 ) steps . in this paper",
    "we show that for real - valued random variables @xmath7 and @xmath8 , we do not need more than o(@xmath1 ) steps .",
    "in this section , a reformulation is given in section [ sec : reformulation ] .",
    "we then show in section [ sec : u - stat ] that the newly formed unbiased estimator is a u - statistic .",
    "we will work with the unbiased version of the squared sample distance covariance , which is published in .",
    "the definition is as follows .",
    "let @xmath49 be a symmetric , real valued @xmath50 matrix with zero diagonal , @xmath51 .",
    "define the @xmath52-centered matrix @xmath53 as follows : the @xmath54-th entry of @xmath53 is @xmath55    here the ",
    "@xmath52-centered \" is so named because as shown below , the corresponding inner product ( which will be specified in ) defines an unbiased estimator of the squared distance covariance .",
    "let @xmath56 denote a sample of observations from the joint distribution @xmath57 of random vectors @xmath7 and @xmath8 .",
    "let @xmath49 be the euclidean distance matrix of the sample @xmath58 from the distribution of @xmath7 , and @xmath59 be the euclidean distance matrix of the sample @xmath60 from the distribution of @xmath8 .",
    "then if @xmath61 , for @xmath62 , the following @xmath63 is an unbiased estimator of squared population distance covariance @xmath15 .",
    "the proof of the above proposition is in the appendix of .",
    "let @xmath64 denote the inner product defined in .",
    "the following notations will be used . define the column and row sums as follows : @xmath65",
    "we will need the following lemma .",
    "[ lem : omega1 ] if @xmath64 is the inner product defined in then we have @xmath66    for the proof see the appendix .",
    "formula will be used to prove that ( i ) the estimator in is a u - statistic and thus we can apply the relevant limit theorems to study its asymptotic behavior ; ( ii ) the estimator in can be computed in @xmath67 steps .",
    "suppose @xmath68 is a sample . for positive integer @xmath69 ,",
    "let @xmath70 denote all the distinct @xmath69-subsets of @xmath71 .",
    "for a set @xmath72 , we define notation @xmath73 .",
    "let @xmath74 be a symmetric real - valued or complex - valued kernel function of @xmath69 variables . for each @xmath75 ,",
    "the associated u - statistic of order @xmath69 , @xmath76 , is equal to the average over ordered samples of size @xmath69 of the sample values @xmath77 . in other words ,",
    "@xmath78    for u - statistics , we can verify the following lemma .",
    "[ lem : check01 ] for @xmath79 , we denote @xmath80 where @xmath81 is defined in after removing the element @xmath82 .",
    "then we must have @xmath83    in , each term @xmath77 is counted @xmath84 times on both sides .",
    "hence the equality holds .",
    "in fact , using arithmetic deduction , one can prove that the converse of the above is also true .",
    "in other words , the _ jackknife invariance _ is a necessary and sufficient condition for being u - statistics . for a very similar ( equivalent ) approach",
    "[ lem : check02 ] if there exists a positive integer @xmath85 , such that for any @xmath86 , function @xmath87 satisfies and , then there must be a kernel function @xmath88 of order @xmath69 , such that @xmath87 can be written in a form as in ; i.e. , @xmath87 is a u - statistic .",
    "a proof of the above can be found in the appendix .",
    "the two lemmas above show that the recursive relation is a necessary and sufficient condition for a u - statistic . for later use ,",
    "we explicitly restate the result below .",
    "[ th : check01 ] let @xmath89 be a statistic of a sample @xmath90 .",
    "let @xmath91 , @xmath92 , be a statistic of a reduced sample @xmath93 ; i.e. , @xmath91 is the statistic after removing the observation @xmath82 .",
    "the necessary and sufficient condition for @xmath89 to be a u - statistic of order @xmath69 is @xmath94 holds for all @xmath95 .",
    "the above can be extended to a two - sample problem , in which a sample is @xmath96 for @xmath97 . by replacing @xmath82 with @xmath98 ,",
    "all the previous arguments still hold .    .",
    "combine lemma [ lem : check01 ] and lemma [ lem : check02 ] , and simplify , we have .",
    "let @xmath64 denote the inner product that is defined in .",
    "note @xmath64 is based on the entire sample ( i.e. , @xmath99 . for @xmath100 ,",
    "let @xmath101 denote the corresponding statistic after knocking out pair @xmath98 from the entire sample .",
    "the following lemma establish counterpart for @xmath102 , where @xmath103 .",
    "[ lem : omega2 ] for @xmath104 , let @xmath105 , @xmath106 , @xmath107 , and @xmath108 denote the corresponding sums after entry @xmath109 is removed from the sample . if @xmath102 is the inner product that is defined in after knocking off the @xmath110-th entry @xmath109 , we have @xmath111    we will not provide the proof for lemma [ lem : omega2 ] , because it will be identical with the proof of lemma [ lem : omega1 ] .    [",
    "th : u - stat ] estimator @xmath64the inner product that is defined in is a u - statistic .",
    "the kernel function of the corresponding u - statistic is the inner product that was defined in with @xmath112 .",
    "see a proof in the appendix .",
    "now we know that is a u - statistic and it is easy to see that is in fact a u - statistic with a degenerate kernel under the null hypothesis of independence of @xmath7 and @xmath8 , thus we can see from corollary 4.4.2 of that if the second moments of @xmath7 and @xmath8 are finite then under the null hypothesis , the limit distribution of @xmath113 has the form @xmath114 , where @xmath115 , and @xmath116 are i.i.d .",
    "standard normal random variables . under the alternative hypothesis",
    "we have that @xmath117 , thus we can easily construct a consistent test of independence . for a technically much more difficult approach ,",
    "see where a similar result was derived for a related v - statistic using deep results on complex - valued gaussian processes .",
    "we now argue that when @xmath7 and @xmath8 are univariate , there is an o@xmath118 algorithm to implement .",
    "we start with several intermediate results , which are presented as lemmas below .",
    "[ lem : aidot ] denote @xmath119 for @xmath100 , we also denote @xmath120 we have @xmath121    a proof is relegated to the appendix .    due to symmetry , the following is the counterpart fact for @xmath8 .",
    "we state it without a proof .",
    "[ lem : bidot ] denote @xmath122 for @xmath100 , we denote @xmath123 we have @xmath124    using formulas and , the following two equations can be easily established .",
    "we state them without a proof .",
    "[ cor : abdots ] we have @xmath125 and @xmath126    the following lemma will be used .",
    "[ lem : aijbij ] we define a sign function , for @xmath127 , @xmath128 for any sequence @xmath129 , for @xmath100 , we define @xmath130 the following is true : @xmath131.\\ ] ]    we have @xmath132.\\end{aligned}\\ ] ] per the definition of @xmath133 , one can verify that the above equates to .",
    "[ lem : gamma ] for any sequence @xmath129 , there is an o@xmath134 algorithm to compute for all @xmath135 ( @xmath136 ) , where @xmath137 .",
    "again , we relegate the proof to the appendix .",
    "the main idea of the proposed algorithm is a modification as well as an extension of the idea that was used in and , which developed a fast algorithm for computing the kendall s @xmath4 rank correlation coefficient .",
    "the principle of the avl tree structure @xcite was adopted .",
    "despite they are in a similar spirit , the algorithmic details are different .",
    "we now present the main result in the following theorem .",
    "[ th : fastalgo ] the unbiased estimator of the squared population distance covariance ( that was defined in ) can be computed by an o(@xmath1 ) algorithm .    in lemma",
    "[ lem : omega1 ] , the unbiased statistic has been rewritten as in . for the first term on the right hand side of , per lemmas [ lem : aijbij ] and",
    "[ lem : gamma ] , there is an o(@xmath1 ) algorithm to compute it .    for the second term on the right hand side of , note that quantities @xmath138 , and @xmath139 that were defined in lemmas [ lem : aidot ] and [ lem : bidot ] , respectively , are partial sums , which can be computed for all @xmath140 s with o(@xmath1 ) algorithms .",
    "the @xmath141 factor is inserted , because one may need to sort @xmath82 s or @xmath142 s in order to compute for @xmath138 , and @xmath139 .",
    "then by and , all @xmath143 and @xmath144 can be computed at order o(@xmath1 ) .",
    "consequently , the second term on the right hand side of can be computed by using an o(@xmath1 ) algorithm .    for the third term on the right hand side of , using and in corollary [ cor : abdots ] , we can easily see that it can be computed via an o(@xmath1 ) algorithm .",
    "from all the above , the theorem is established .",
    "for readers convenience , we present a detailed algorithm description in appendix , where algorithm [ alg : partialsum2d ] realizes the idea that is described in the proof of lemma [ lem : gamma ] ; algorithm [ alg : sub01 ] is a subroutine that will be called in algorithm [ alg : partialsum2d ] ; and the algorithm [ alg : fadcor ] is the algorithm that can compute for the distance covariance at o(@xmath1 ) .",
    "in section [ sec : implement ] , we describe a matlab and c based implementation of the newly proposed fast algorithm .",
    "this fast algorithm enables us to run some simulations with sample sizes that were impossible to experiment with before its appearance .",
    "we report some numerical experiments in section [ sec : effectiveness ] .",
    "distance correlation has been found helpful in feature screening . in section [ sec : screening ] , we redo experiments on this regard , increasing the sample size from @xmath145 to @xmath146 . it is observed that the advantage of using the distance correlation is more evident when the sample size becomes larger .",
    "the fast algorithm was implemented in matlab , with a key step ( of dyadic updating ) being implemented in c. it was then compared against the direct ( i.e. , slow ) implementation .",
    "table [ tab : fadcor01 ] presents the average running time for the two different implementations in matlab with @xmath147 replications at each sample size .",
    "the sample size goes from @xmath148 ( @xmath149 ) to @xmath150 ( @xmath151 ) . in all these cases , the two methods ended with identical solutions ; this validates our fast algorithm .",
    "note a comparison in matlab is not desirable for our fast algorithm .",
    "the direct method calls some matlab functions , which achieve the speed of a low - level language implementation , while the implementation of the fast method is not . in theory",
    ", the fast algorithm will compare more favorably if both methods are implemented in a low - level language , such as in c or c++ .",
    ".running times ( in seconds ) for the direct method and the fast method for computing the distance correlations .",
    "the values in the parentheses are sample standard errors . at each sample size ,",
    "@xmath147 repetitions were run.[tab : fadcor01 ] [ cols=\">,>,>\",options=\"header \" , ]     the fast method allows us to study how the sample distance correlation converge to the population counterpart as a function of the sample size .",
    "[ fig : converge01 ] shows the convergence of the sample distance correlation and pearson s correlation .",
    "it is worth noting that in cases ( 3)-(8 ) , the pearson s correlation quickly converges to zero , while the sample distance correlation clearly stays away from zero .",
    "this experiments shows that a previous observation in fig .",
    "[ fig : showcases02 ] should occur with large probability .     covering interval of both sample pearson s correlation ( solid line , with low and upper sample quartiles marked by ` @xmath152 ' ) and sample distance correlation ( dotted lines , with both quartiles marked by ` @xmath153 ' ) .",
    "the horizontal axis equates the @xmath154sample size@xmath155 .",
    "the vertical axis corresponds to the values of correlations . in cases ( 3 ) through ( 8) , the two correlations clearly converge to different constants , when the pearson s correlation always seems to converge to zero . ]      in * li2012 , distance correlation has been proposed to facilitate feature screening in ultrahigh - dimensional data analysis .",
    "the proposed sure independence screening procedure based on the distance correlation ( dc - sis ) has been proven to be effective in their simulation study . due to the use of the direct method , they restricted their sample size to @xmath145 .",
    "we redo the simulations as in * li2012 , however increases the sample size to @xmath146 , i.e. , @xmath5 times of the originally attempted .",
    "it is observed that the use of distance correlation becomes more advantageous when the sample size increases",
    ".    the screening algorithm , which was initially advocated by , works as follows . for each covariate @xmath156 , a ` marginal utility ' function was computed .",
    "such a marginal utility function can be the pearson s correlation , the distance correlation that was discussed in this paper , or other dependence measure such as the one in * jasa2011sir that was also used in the simulation studies of * li2012 .",
    "the ` screening ' is based on the magnitude of the values of these marginal utility function . sometimes , forward ,",
    "backward , or a hybrid stepwise approach is proposed . in this paper , we refrain from further discussion in this potential research direction .",
    "our simulation setup follows the one in .",
    "note that an alternative approach named _ sure independent ranking and screening _ ( sirs ) @xcite was compared against . for a sample , @xmath157 , of two random variable @xmath7 and @xmath8 , the sirs dependence measure ( i.e. , the marginal utility function )",
    "is defined as @xmath158 ^ 2,\\ ] ] where @xmath159 is an indicator function . the formulation in the above definition seemingly hint an o@xmath160 algorithm .",
    "the following theorem will show that it can be computed via an o@xmath134 algorithm .",
    "the proof and the algorithmic details are relegated to the appendix .",
    "[ th : sirs ] for a sample , @xmath157 , of a bivariate random vector @xmath161 , the sirs measure @xcite in can be computed via an algorithm whose average complexity is o@xmath134 .    for completeness , we state our simulation setup below .",
    "we generate @xmath162 from normal distribution with zero mean and covariance matrix @xmath163 , and the error term @xmath164 from the standard normal distribution @xmath165 .",
    "two covariance matrices are considered to assess the performance of the dc - sis and to compare with existing methods : ( 1 ) @xmath166 and ( 2 ) @xmath167 .",
    "note that a covariance matrix with entries @xmath168 enjoys a known cholesky decomposition : @xmath169 , where @xmath170 if @xmath171 , and @xmath172 , @xmath173 , for @xmath174 and @xmath175 , @xmath176 . in our simulations , we take advantage of this known decomposition .",
    "the dimension @xmath177 varies from @xmath178 to @xmath179 .",
    "each experiment was repeated @xmath180 times , and the performance is evaluated through the following three criteria :    1 .",
    "@xmath181 : the minimum model size to include all active predictors .",
    "we report the @xmath182 , and @xmath183 quantiles of @xmath181 out of @xmath180 replications .",
    "@xmath184 : the proportion that an individual active predictor is selected for a given model size @xmath185 in the @xmath180 replications .",
    "@xmath186 : the proportion that all active predictors are selected for a given model size @xmath185 in the @xmath180 replications .",
    "the @xmath181 is used to measure the model complexity of the resulting model of an underlying screening procedure .",
    "the closer to the minimum model size the @xmath181 is , the better the screening procedure is .",
    "the sure screening property ensures that @xmath184 and @xmath186 are both close to one when the estimated model size @xmath185 is sufficiently large .",
    "different from , the @xmath185 is chosen to be @xmath187 $ ] , @xmath188 , and @xmath189 throughout our simulations to empirically examine the effect of the cutoff , where @xmath190 $ ] denotes the integer part of @xmath191 .",
    "an innovative stopping rule is introduced in for dc - sis .",
    "we did not implement it here , because the new stopping rule requires a multivariate version of the distance correlation , which is not covered by this paper .",
    "the example is designed to compare the finite sample performance of the dc - sis with the sis @xcite and the sirs @xcite . in this example , we generate the response from the following four models :    * @xmath192 , * @xmath193 , * @xmath194 , * @xmath195 ,    where @xmath196 is an indicator function .",
    "the regression functions @xmath197 in models ( 1.a)-(1.d ) are all nonlinear in @xmath198 .",
    "in addition , models ( 1.b ) and ( 1.c ) contain an interaction term @xmath199 , and",
    "model ( 1.d ) is heteroscedastic . following fan and lv ( 2008 ) ,",
    "we choose @xmath200 for @xmath201 , and @xmath202 , where @xmath203 , @xmath204bernoulli@xmath205 and @xmath206 .",
    "we set @xmath207 in this example to be consistent with the experiments in : challenging the feature screening procedures under consideration . for each independence screening procedure",
    ", we compute the associated marginal utility between each predictor @xmath208 and the response @xmath8 .",
    "that is , we regard @xmath209 as the predictor vector in this example .",
    "tables [ tab : ex1a ] , [ tab : ex1b ] , and [ tab : ex1b-2 ] present the simulation results for @xmath210 , and @xmath186 .",
    "the performances of the dc - sis , sis , and sirs are quite similar in model ( 1.a ) , indicating that the sis has a robust performance if the working linear model does not deviate far from the underlying true model .",
    "the dc - sis outperforms the sis and the sirs significantly in models ( 1.b)-(1.d ) .",
    "both the sis and the sirs have little chance to identify the important predictors @xmath211 and @xmath212 in models ( 1.b ) and ( 1.c ) , and @xmath213 in model ( 1.d ) .    comparing tab.s [ tab : ex1a ] and [ tab : ex1b ] with the counterparts in",
    ", one can clearly see that the advantage of using the distance correlation becomes more evident , observing smaller sample quantiles of @xmath181 for dc - sis , and larger coverage probabilities in @xmath184 and @xmath186 .",
    "distance correlation has been found useful in many applications @xcite .",
    "a direct implementation of the distance correlation led to an @xmath214 algorithm with sample size @xmath2 . we propose a fast algorithm .",
    "its computational complexity is @xmath6 on average .",
    "armed with this fast algorithm , we carry out some numerical experiments with sample sizes that have not been attempted before .",
    "we found that in many cases , the advantage of adopting the distance correlation becomes even more evident .",
    "the proposed fast algorithm certainly makes the distance correlation more applicable in situations where statistical dependence needs to be evaluated .",
    "algorithm [ alg : fadcor ] is the algorithm that can compute for the distance covariance at o(@xmath1 ) .",
    "algorithm [ alg : partialsum2d ] realizes the idea that is described in the proof of lemma [ lem : gamma ] .",
    "algorithm [ alg : sub01 ] is a subroutine that will be called in algorithm [ alg : partialsum2d ] .",
    "[ alg : fadcor ]    algorithm : fast computing for distance covariance ( fadcor )    * inputs : * observations @xmath90 , and @xmath215 . +",
    "* outputs : * the distance covariance that was defined in .    1 .   sort @xmath90 , and @xmath215 .",
    "let @xmath216 and @xmath217 denote the order indices ; i.e. , if for @xmath218 , @xmath219 , then @xmath82 is the @xmath110th smallest observations among @xmath90 . + similarly if for @xmath218 , @xmath220 , then @xmath142 is the @xmath110th smallest observations among @xmath215 .",
    "2 .   let @xmath221 , and @xmath222 denote the order statistics .",
    "+ denote the partial sums : @xmath223 they can be computed using the following recursive relation : @xmath224 , @xmath225 3 .",
    "compute @xmath226 , @xmath227 , @xmath228 , and @xmath139 that are defined in lemma [ lem : aidot ] and [ lem : bidot ] , using the following formula : for @xmath229 , we have @xmath230 4 .",
    "compute @xmath231 and @xmath232 per their definitions in lemma [ lem : aidot ] and [ lem : bidot ] .",
    "5 .   using and ,",
    "compute @xmath233 .",
    "[ alg:1.part2 ] 6 .   using and ,",
    "compute @xmath234 and @xmath235 .",
    "[ alg:1.part3 ] 7 .",
    "_ partialsum2d _ to compute for @xmath236 , @xmath237 , @xmath238 , and @xmath239 . 8 .   using to compute @xmath240 .",
    "[ alg:1.part1 ] 9 .",
    "finally , apply the results of steps [ alg:1.part2 ] .",
    ", [ alg:1.part3 ] . , and [ alg:1.part1 ] .",
    "[ alg : partialsum2d ]    algorithm : fast algorithm for a @xmath241-d partial sum sequence ( partialsum2d )    * inputs : * observations @xmath90 , @xmath215 , and @xmath242 . + * outputs : * quantity @xmath243 that is defined in lemma [ lem : aijbij ] .    1 .",
    "compute for the order statistics @xmath244 for @xmath90 . then rearrange triplets @xmath245 s such that we have @xmath246 .",
    "each triplet @xmath245 ( @xmath100 ) stay unchanged .",
    "2 .   let @xmath247 denote the order statistics for @xmath215 , and assume that @xmath248 , are the order indices ; i.e. , if @xmath220 , then @xmath142 is the @xmath110-th smallest among @xmath215 . without loss of generality",
    ", we may assume that @xmath249 .",
    "evidently aforementioned function @xmath250 is invertible .",
    "let @xmath251 denote its inverse .",
    "define the partial sum sequence : for @xmath100 , @xmath252 the following recursive relation enables an o(@xmath2 ) algorithm to compute for all @xmath253 s , @xmath254 4 .   for @xmath100 ,",
    "define @xmath255 again the above partial sums can be computed in o(@xmath2 ) steps .",
    "compute @xmath256 6 .   call",
    "subroutine _ dyadupdate _ to compute for @xmath257 for all @xmath258",
    "by , we have that @xmath259    [ alg : sub01 ]    subroutine : a dyadic updating scheme ( dyadupdate )    * inputs : * sequence @xmath215 and @xmath242 , where @xmath215 is a permutation of @xmath260 .",
    "+ * outputs : * quantities @xmath261 , @xmath92",
    ".    1 .   recall that we have assumed @xmath262 .",
    "if @xmath2 is not dyadic , we simply choose the smallest @xmath263 such that @xmath264 . recall that for @xmath265 , @xmath266 , we define a close interval @xmath267.\\ ] ] 2 .",
    "assign @xmath268 , and @xmath269 .",
    "3 .   for @xmath270 , we do the following . 1 .",
    "fall all @xmath271 s , such that @xmath272 .",
    "then for these @xmath271 s , do update @xmath273 2 .",
    "find nonnegative integers @xmath274 such that @xmath275 let @xmath276 .",
    "for @xmath277 , compute @xmath278 3 .",
    "one can verify the following equalities : @xmath280 the following will be used in our simplification too .",
    "we have @xmath281 similarly , we have @xmath282    in the following , we simplify the statistic in .",
    "we have @xmath283.\\end{aligned}\\ ] ] furthermore , we have @xmath284 \\\\ & & + \\frac{1}{n(n-2)^2 ( n-3 ) } \\sum_{i \\neq j } ( a_{i\\cdot } + a_{\\cdot j})(b_{i\\cdot } + b_{\\cdot j})\\\\ & & -\\frac{a_{\\cdot \\cdot } b_{\\cdot \\cdot}}{(n-1)(n-2)^2(n-3 ) } \\\\ & \\stackrel{\\mbox{\\eqref{eq : check04},\\eqref{eq : check06}}}{= } & \\frac{1}{n(n-3 ) } \\sum_{i \\neq j } a_{ij}b_{ij } -\\frac{4}{n(n-2)(n-3 ) } \\sum_{i=1}^n a_{i\\cdot}b_{i\\cdot } \\\\ & & + \\frac{1}{n(n-2)^2 ( n-3 ) } \\sum_{i \\neq j } ( a_{i\\cdot } + a_{\\cdot j})(b_{i\\cdot } + b_{\\cdot j } )   -\\frac{a_{\\cdot \\cdot } b_{\\cdot \\cdot}}{(n-1)(n-2)^2(n-3)}.\\end{aligned}\\ ] ] now bringing in and , we have @xmath285 \\\\ & = & \\frac{1}{n(n-3 ) } \\sum_{i",
    "\\neq j } a_{ij}b_{ij } -\\frac{2}{n(n-2)(n-3 ) } \\sum_{i=1}^n a_{i\\cdot}b_{i\\cdot } + \\frac{a_{\\cdot \\cdot } b_{\\cdot \\cdot}}{n(n-1)(n-2)(n-3)},\\end{aligned}\\ ] ] which is .",
    "we use arithmetic induction .",
    "suppose @xmath286 , becomes @xmath287 by defining @xmath288 , we can verify that @xmath88 is a kernel function with @xmath69 variables .",
    "consequently , @xmath289 can be written as .",
    "now suppose for any @xmath290 , @xmath87 has the form as in , with the function @xmath88 that was defined above .",
    "applying with @xmath291 , we can show that @xmath292 still has the form as in , with the same function @xmath88 that was defined above .",
    "we omit further details .",
    "it is evident to verify that the followings are true : for @xmath293 , @xmath294 we then have @xmath295 for the right hand side of , we have the following : @xmath296}{(n-1)(n-3)(n-4 ) } \\\\ & & + \\frac{(n-4)a_{\\cdot \\cdot } b_{\\cdot \\cdot } + 4\\sum^n_{k=1 } a_{k\\cdot } b_{k \\cdot}}{(n-1)(n-2)(n-3)(n-4 ) } \\\\ & = & \\frac { \\sum_{i \\neq j } a_{ij}b_{ij}}{n-3 } - \\frac{2}{(n-2)(n-3 ) } \\sum^n_{i=1 } a_{i\\cdot } b_{i \\cdot } + \\frac{a_{\\cdot \\cdot } b_{\\cdot \\cdot}}{(n-1)(n-2)(n-3)}.\\end{aligned}\\ ] ] compare with , we can verify that the above equates to @xmath297 , which ( per theorem [ th : check01 ] ) indicates that @xmath64 is a u - statistic .",
    "the kernel function of the corresponding u - statistic is the inner product that was defined in with @xmath112 .      without loss of generality ( wlog ) , we assume that @xmath302 .",
    "we have @xmath303 note that we can verify the following equations : @xmath304 where @xmath305 .",
    "we can rewrite @xmath135 as follows : @xmath306 we will argue that the three summations on the right hand side can be implemented by o@xmath134 algorithms . first , term @xmath307 is a formula for partial sums .",
    "it is known that an o(@xmath2 ) algorithm exists , by utilizing the relation : @xmath308 second , after sorting @xmath309 s at an increasing order , sums @xmath310 is transferred into a partial sums sequence .",
    "hence it can be implemented via an o(@xmath2 ) algorithm .",
    "if quicksort @xcite ( * ? ? ?",
    "* section 5.2.2 : sorting by exchanging ( pages 113 - 122 ) ) is adopted , the sorting of @xmath309 s can be done via an o(@xmath1 ) algorithm .",
    "we will argue that sums @xmath311 can be computed in an o(@xmath1 ) algorithm .",
    "wlog , we assume that @xmath312 , is a permutation of the set @xmath313 .",
    "wlog , we assume that @xmath2 is dyadic ; i.e. , @xmath262 , where @xmath314 or @xmath263 is a nonnegative integer . for @xmath265 , @xmath266",
    ", we define an close interval @xmath267.\\ ] ] we then define the following function @xmath315 where @xmath229 , @xmath265 , and @xmath266 .",
    "we argue that computing the values of @xmath316 for all @xmath317 , can be done in o(@xmath1 ) .",
    "first of all , it is evident that for all @xmath318 , we have @xmath319 suppose for all @xmath320 , @xmath321 s have been computed for all @xmath322 and @xmath110 . for each @xmath323 , there is only one @xmath324 , such that @xmath325 $ ] . by the definition of @xmath326",
    ", we have @xmath327 the above dynamic programming style updating scheme needs to be run for @xmath2 times ( i.e. , for all @xmath328 ) , however each stage requires no more than @xmath329 updates .",
    "overall , the computing for all @xmath316 takes no more than o(@xmath1 ) .    for a fixed @xmath140 , @xmath100 ,",
    "we now consider how to compute for @xmath330 .",
    "if @xmath331 , obviously we have @xmath332 . for @xmath333 , there must be a unique sequence of positive integers @xmath334 , such that @xmath335 since @xmath336 , we must have @xmath337 .",
    "we then define @xmath338 as follows @xmath339 one can then verify the following : for @xmath340 , @xmath341 since @xmath337 , the above takes no more than o(@xmath141 ) numerical operations .",
    "consequently , computing @xmath330 for all @xmath258 , can be done in o(@xmath1 ) .",
    "( we realized that the above approach utilized the avl tree structure @xcite . ) from all the above , we established the result .",
    "we have the following sequence of equations : @xmath342 ^ 2 & = & \\sum_{j=1}^n \\left[\\sum_{i=1}^n x_i",
    "\\mathbf{1}(y_i < y_j ) \\right ] \\cdot   \\left[\\sum_{k=1}^n x_k \\mathbf{1}(y_k < y_j ) \\right ] \\\\ & = & \\sum_{j=1}^n \\sum_{i=1}^n \\sum_{k=1}^n x_i \\cdot x_k \\cdot \\mathbf{1}(y_i < y_j \\mbox { and } y_k < y_j)\\\\ & = & \\sum_{i=1}^n x_i \\left [ \\sum_{k : y_i \\le y_k } x_k \\sum_{j=1}^n \\mathbf{1}(y_k",
    "< y_j )   + \\sum_{k : y_i > y_k } x_k \\sum_{j=1}^n \\mathbf{1}(y_i < y_j ) \\right ] .\\end{aligned}\\ ] ] the last expression implies the following steps to compute for sirs@xmath161 .      since @xmath348 s , @xmath349 s , and @xmath350 s are partial sums , it is easy to verify that each of the above steps can be done within o@xmath134 operations on average , hence the entire algorithm takes o@xmath134 operations on average .",
    "adelson - velskii , g. , and  landis , e.  m. 1962 , `` an algorithm for the organization of information , '' _ proceedings of the ussr academy of sciences _ , 146 ,  263266 .",
    "( russian ) english translation by myron j. ricci in soviet math .",
    "doklady , 3:1259 - 1263 , 1962 .",
    "kong , j. , klein , b. e.  k. , klein , r. , and  wahba , g. 2012 , `` using distance correlation and ss - anova to acess associations of familial relationnships , lifestyle factors , diseases , and mortality , '' _ proceedings of the national academy of sceineces _ , 109(50 ) ,  2035220357 .",
    "koroljuk , v.  s. , and  borovskich , y.  v. 1994 , _ theory of u - statistics _ , vol .",
    "273 of _ mathematics and its applications _ , dordrecht : kluwer academic publishers group .",
    "translated by p. v. malyshev , d.v .",
    "malyshev from the 1989 russian original edition ."
  ],
  "abstract_text": [
    "<S> distance covariance and distance correlation have been widely adopted in measuring dependence of a pair of random variables or random vectors . </S>",
    "<S> if the computation of distance covariance and distance correlation is implemented directly accordingly to its definition then its computational complexity is o(@xmath0 ) which is a disadvantage compared to other faster methods . in this paper </S>",
    "<S> we show that the computation of distance covariance and distance correlation of real valued random variables can be implemented by an o(@xmath1 ) algorithm and this is comparable to other computationally efficient algorithms . </S>",
    "<S> the new formula we derive for an unbiased estimator for squared distance covariance turns out to be a u - statistic . </S>",
    "<S> this fact implies some nice asymptotic properties that were derived before via more complex methods . </S>",
    "<S> we apply the fast computing algorithm to some synthetic data . </S>",
    "<S> our work will make distance correlation applicable to a much wider class of applications .    </S>",
    "<S> * author s footnote : *    dr . </S>",
    "<S> xiaoming huo is a professor in the school of industrial and systems engineering at the georgia institute of technology . </S>",
    "<S> he has been serving as a rotator at the national science foundation since august 2013 . </S>",
    "<S> mailing address : 765 ferst dr , atlanta , ga 30332 ( email : xiaoming@isye.gatech.edu ) .    </S>",
    "<S> dr . </S>",
    "<S> gbor j. szkely is a program officer at the national science foundation . </S>",
    "<S> mailing address : 4201 wilson blvd , arlington , va 22203 ( email : gszekely@nsf.gov ) .    </S>",
    "<S> keywords : distance correlation , fast algorithm , statistical dependence </S>"
  ]
}