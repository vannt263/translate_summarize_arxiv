{
  "article_text": [
    "a lack of consistent definition of chiral fermions on the lattice has hampered definitive and convincing investigations of chiral aspects of quantum chromodynamics ( qcd ) until now .",
    "thus important physics issues , such as the spontaneous breaking of the chiral symmetry at low temperatures and its restoration at finite temperature , have remained hostages to technical questions such as the fine - tuning of the bare quark mass ( wilson fermions ) or the precise number of massless flavours ( staggered fermions ) .",
    "recent developments in defining exact chiral symmetry on the lattice have therefore created exciting prospects of studying an enormous amount of physics in a cleaner manner from first principles . however , the corresponding dirac operators are much more complicated . without good control of the algorithms needed to deal with them ,",
    "one is unlikely to derive the full benefit of their better chiral properties .",
    "our goal in this paper is to evaluate the efficiency of the most widely used , or most promising , algorithms . by efficiency we mean both the speed of the algorithm , which is measured by the computer time required to achieve a certain accuracy in the solution , and the adaptability , which is measured by how the speed scales as the problem becomes harder .",
    "this study is made for various values of the required accuracy along with the corresponding analysis on the accuracy obtained for the expected properties of the resulting dirac operator such as the ginsparg - wilson relation , the central circle relation , @xmath0 hermiticity or normality .",
    "in particular , we have observed that these properties can be satisfied accurately only if the sign computation of the wilson dirac operator has high enough precision .",
    "one version of chiral fermions on the lattice is the overlap formalism .",
    "the overlap dirac operator ( @xmath1 ) is defined @xcite in terms of the wilson - dirac operator ( @xmath2 ) by the relation d = 1 + d_w ( d_w^d_w)^-1/2 .",
    "[ overlap]in this paper we shall use the shorthand notation m = d_w^d_w  .",
    "[ defm]the wilson - dirac operator @xmath2 ( for lattice spacing @xmath3 ) is given by d_w = _ + m  ,   [ wilsond]where @xmath4 and @xmath5 are ( gauge covariant ) forward and backward difference operators respectively .",
    "it has been shown that as long as the mass @xmath6 is in the range @xmath7 , the above overlap dirac operator is well - defined , and corresponds to a single massless fermion .",
    "furthermore , it satisfies the ginsparg - wilson relation @xcite _ 5 d + d_5 = d _ 5 d , [ gwrel]which leads to a good definition of chirality on the lattice and has been shown to correspond to an exact chiral symmetry on the lattice .",
    "the overlap dirac operator , @xmath1 , enjoys many nice properties in addition to the ginsparg - wilson relation in eq .",
    "( [ gwrel ] ) .",
    "in particular , it satisfies @xmath0-hermiticity d^= _ 5d_5 .",
    "[ g5her]together with the ginsparg - wilson relation , this implies that @xmath1 is normal , _",
    "i.e. _ , [ d , d^]=0 .",
    "[ normal]normality clearly means that @xmath1 and @xmath8 have the same eigenvectors .",
    "( [ gwrel],[g5her ] ) also imply d+d^= d^d , [ circle]and hence the eigenvalues of @xmath1 and @xmath8 lie on the unit circle centered at unity on the real line , implying that @xmath9 is unitary .",
    "we define measures of numerical errors on each of these quantities , and relations between them in section [ sc.errors ] .",
    "all computations of hadronic correlators involve the determination of the fermion propagator @xmath10 , and need a nested series of two matrix iterations for their evaluation , since each step in the numerical inversion of @xmath1 involves the evaluation of @xmath11 .",
    "this squaring of effort makes a study of qcd with overlap quarks very expensive .",
    "this problem defines for us the properties that an efficient algorithm to deal with @xmath11 must have .",
    "first , it should achieve a given desired accuracy as quickly as possible .",
    "the need for accuracy is clear : the accuracy to which the ginsparg - wilson relation in eq .",
    "( [ gwrel ] ) is satisfied depends on the accuracy achieved in the computation of @xmath11 . the second , and equally important , requirement is that the method should adapt itself easily to matrices with widely different condition numbers = , [ cond]where @xmath12 and @xmath13 are , respectively , the minimum and the maximum eigenvalues of @xmath14 .",
    "adaptability is needed because in qcd applications the eigenvalue spectrum of @xmath14 can fluctuate over many orders of magnitude from one configuration to the next .",
    "since the condition number on a configuration is _ a priori _ unknown , a method with low adaptability will end up either being inefficient or inaccurate or even both .",
    "algorithms , which are adaptable in principle , may require tuning of parameters by hand , or they may incorporate a procedure for self - tuning . clearly , self - tuning algorithms are the ones that can best deal with fluctuating condition numbers in any real situation .    in this paper",
    "we examine the speed and adaptability of several different algorithms to compute the inverse square root , @xmath11 , of hermitean matrices ( in our applications the eigenvalues of @xmath14 are non - negative ) acting on a vector . several algorithms for this",
    "have been proposed in the literature .",
    "we do not consider the first algorithm to be proposed , since this requires a matrix inversion to be performed at each step of an iteration to determine @xmath11 @xcite .",
    "later algorithms are more efficient .",
    "these fall into two classes expansions of @xmath15 in appropriate classes of functions ( rational functions @xcite or orthogonal polynomials @xcite ) , and iterative methods @xcite .",
    "we have analyzed four derived algorithms , namely the optimized rational approximation ( ora ) @xcite , the zolotarev approximation ( za , which is also a rational expansion ) @xcite , the chebychev approximation ( ca , a polynomial expansion ) @xcite and the conjugate gradient approximation ( cga , an iterative method ) @xcite .",
    "we find that an expansion in chebychev polynomials is the fastest when @xmath16 is moderately large , but it suffers from various instabilities including a lack of adaptability .",
    "rational expansions cure many of the instabilities of polynomial expansions ; indeed the za is the fastest and is adaptable but not self - tuning .",
    "an iterative method is the only fully self - tuned algorithm , and it turns out to be reasonable also from the point of view of speed .",
    "we make two different estimates of the cost of each algorithm .",
    "the complexity , @xmath17 , counts the number of arithmetic operations required to achieve the solution to the problem and is a measure of speed independent of the specific machine on which the algorithm is implemented .",
    "the spatial complexity , @xmath18 , is the memory requirement for the problem . while timing runs on particular machines on chosen test configurations are instructive , the scaling of speed for each algorithm with physical and algorithmic parameters is provided by our estimates of @xmath17 .",
    "our estimate of the adaptability , @xmath19 , of each algorithm is the following .",
    "if the scalar algorithm for @xmath15 is tuned to have maximum relative error @xmath20 in the range @xmath21 $ ] , then we find the smallest range @xmath22 $ ] where the error is at most @xmath23 .",
    "note that the second interval can not be smaller than the first . in terms of these quantities",
    "we define = -1 .",
    "[ adapt]the least adaptable algorithms have small values of @xmath19 ( @xmath24 ) .",
    "@xmath19 is a measure of the relative accuracy achieved in a fixed cpu time for the same algorithm running on two different configurations with condition numbers @xmath25 and @xmath26 . in conjunction with estimates of @xmath17",
    ", it also contains information about the scaling of cpu time required to achieve the same accuracy on the two configurations .",
    "our numerical tests were performed with three typical @xmath27 gauge configurations on a @xmath28 lattice at @xmath29 ( _ i.e. _ , @xmath30 ) . the configuration a had eigenvalues of @xmath14 in the range @xmath31 $ ] so that @xmath32 .",
    "the configuration b had eigenvalues in the range @xmath33 $ ] , giving @xmath34 .",
    "finally , configuration c had eigenvalues in the range @xmath35 $ ] and hence @xmath36 .",
    "configuration a is one of the easiest configuration we found in our simulations , and there were several configurations with @xmath16 of order @xmath37@xmath38 in the data set we worked with in @xcite . if there is a single scale in the level spacing of the eigenvalues of @xmath14 , then we expect @xmath39 on our test configurations .",
    "we conclude that a is indeed a little easier than the generic configuration and b and c are successively harder .",
    "the cpu times we quote in our tables are obtained on a , which is a vector computer .",
    "our computations ran on this machine at a speed of around 4.1 gigaflops .    in sections",
    "[ sc.ca][sc.cga ] we describe and analyze the four algorithms and also present data on precision and time measurements on these three test configurations . in this work",
    "we have not investigated the performance of the algorithms with deflation ( explicit subtraction ) of some eigenvectors . however",
    ", we do remark on the precision required for deflation and the propagation of errors due to such a subtraction .",
    "section [ sc.comp ] contains a comparison of the numerical results and our conclusions .",
    "in general , every numerical method to compute @xmath11 constructs an operator @xmath40 which applied to a vector @xmath41 gives the vector    x = l [ ] l [ ] = m^-1/2+e [ ] , [ approx]where @xmath42 is the error in the approximation , @xmath40 , to the matrix @xmath11 . typically , these operators @xmath40 and @xmath42 are not matrices because the algorithms can introduce a dependence on @xmath41 which is not linear . the error @xmath43 $ ] on the computation of @xmath11 leads to the violation of the properties in eqs .",
    "( [ gwrel]-[circle ] ) . in our numerical tests",
    "we have investigated five measures of the accuracy of the algorithms through norms of the following operators    = ml^2 - 1 , & = d_5+_5d - d_5d , & = dd^-d^d , + = d^-_5 d_5 , & = d+d^-d^d , & [ analyse]where @xmath44 and @xmath45 .",
    "each of these operators is zero when @xmath43 $ ] = 0 . with gaussian random vectors",
    "@xmath41 , we have measured the deviations from this exact value through _",
    "i = |z_i|/|| _i = ^z_i/||^2 .",
    "[ errors]here , and later , we use the notation @xmath46 for any complex vector @xmath47 .",
    "note that @xmath48 are real and non - negative whereas @xmath49 are complex in general .",
    "the polynomial approximation consists of writing l = _",
    "i=1^b_i m^i , [ iter]where @xmath50 are constants .",
    "it is clear that both @xmath40 and @xmath42 are matrices in this case and = 0 .",
    "[ lmcom]since in numerical implementations of @xmath2 , @xmath0-hermiticity is accurate to machine precision , _",
    "i.e. _ , @xmath51 , one has the following relation : _ 5d_wm^n_5=m^nd_w^. [ polher]its direct consequence is = ld_w^- _ 5d_wl_5 = 0 .",
    "[ gherm ]    using eqs.([lmcom]-[polher ] ) , one can easily obtain the following relations between the various @xmath52 s : & = & - , + & = & _ 5 , + & = & -_5 _ 5 .",
    "[ zrelations]as a consequence , || & = & || , + & = & = , + & = & = 0 .",
    "[ rel1 ]    expanding @xmath53 in powers of @xmath42 and retaining only the leading order terms , we find that @xmath54 and @xmath55 . defining the averages of @xmath48 and @xmath49 over an ensemble of @xmath41 as @xmath56 and @xmath57 , one obtains , = = = 4d()^2 ( ) , + = -=2d ( ) ( ) , + = 2d ( ) ( ) , [ ehfsq3]where @xmath58 is the density of eigenvalues of @xmath14 , @xmath59 is the error in the approximation and @xmath60 , the difference between the spectral densities ( of @xmath14 ) in the chiral positive and negative sectors .",
    "note that @xmath61 is the relative error in the determination of the inverse square root , and all the integrals depend only on this relative error . since there is no reason for @xmath62 to vanish ,",
    "it is clear that the error in the expansion must remain under control even as @xmath63 .",
    "clearly , this is impossible to arrange in polynomial expansions for @xmath64 .",
    "however , a finite sample of gauge configurations does not need full control over @xmath65 , but only for @xmath66 , where @xmath67 is the smallest eigenvalue encountered in the sample . to achieve this while optimizing cpu costs on configurations where all the eigenvalues are much larger requires the algorithm to be adaptive .",
    "it was assumed above that no deflation has been performed , or that deflation has been performed with no arithmetic errors .",
    "we comment on the effects of deflation in section [ sc.comp ] .      in case of a rational function approximation to @xmath11 ,",
    "one writes the operator @xmath40 as l=_i=1^    @xmath42 here depends on the order @xmath68 and the accuracy of the inversion of @xmath69 .",
    "if the inversion can be achieved with infinite precision , then @xmath40 is a matrix again which commutes with @xmath14 and the analysis of the previous subsection applies in full . if , on the other hand , the error due to the inversion dominates , then for many algorithms , such as the conjugate gradient , @xmath40 depends explicitly on the vector @xmath41 in a complicated way and it is not a matrix",
    "one has to compute the different errors explicitly and study their behavior as in iterative methods .",
    "thus the behavior of errors from rational approximation case interpolates between that of the polynomial approximation and an iterative method according to the relation between the order and the precision of the inversion .",
    "the first use of the polynomial approximation utilized legendre polynomials @xcite .",
    "later the same group proposed a more robust version using the chebychev approximation ( ca ) @xcite . as is well known , when expanding any function , @xmath70 in a fixed range @xmath71 , to a given order @xmath68 through orthogonal polynomials , the use of chebychev polynomials minimizes the maximum error on the function to be approximated .",
    ".runs with the ca adjusted to the interval @xmath31 $ ] for varying @xmath68 on the configuration a. the last column gives the cpu seconds used on a . [ cols=\">,^,^,^,^\",options=\"header \" , ]     the first adaptive method used to compute @xmath11 was based on lanczs algorithm @xcite . in the original suggestion , the number of lanczs steps to be taken in order to reach a given precision was investigated in terms of the variation of the eigenvalues of @xmath14 with the number of lanczs steps . a stopping criterion _  la _",
    "conjugate gradient was proposed but its relation to the precision was not direct .",
    "a related adaptive method based on the conjugate gradient algorithm was used in @xcite . here",
    "the stopping criterion is put on the residual vector in the inversion of @xmath14 .",
    "this enables a direct control over the precision .",
    "the cga starts with an iteration which is almost the same as the usual cg algorithm for the inversion of @xmath14    1 .",
    "start from @xmath72 , @xmath73 and @xmath74 , 2 .",
    "iterate as in regular cg , @xmath75 , @xmath76 , @xmath77 , and @xmath78 .",
    "3 .   stop when @xmath79 .",
    "note that the the only difference from the usual cg is that the vector which is @xmath80 does not need to be obtained during the iteration .    in the orthonormal basis of @xmath81 ,",
    "the matrix @xmath14 is the composition of the matrix @xmath82 whose @xmath83-th column is @xmath84 , and a symmetric tridiagonal matrix , @xmath85 , m = q^tq , t_ii=1_i+ , t_i , i+1=- , [ diag]where @xmath86 and @xmath87 are defined in the iteration above .",
    "then compute the eigenvalues and eigenvectors of this truncated tridiagonal matrix t , t = uu^[trid]where @xmath88 is the diagonal matrix of the eigenvalues and @xmath89 the matrix of the eigenvectors in the basis @xmath82 .",
    "the cga solution is l [ ] = q^t u ^-1/2u^q /|| .",
    "[ solve]the adaptability of the algorithm arises from the fact that we retain only the vectors @xmath84 which contribute significantly to the inverse of @xmath14 and we stop the iterations for @xmath90 when @xmath91 .",
    "the contribution to @xmath11 of the smallest eigenvalue @xmath92 of @xmath14 will be only @xmath93 .",
    "since the stopping criterion @xmath79 is meant to compute @xmath94 it is more stringent than required .",
    "one can be more generous for @xmath11 , and use instead the stopping criterion |r_i+1|</ [ stop]where @xmath95 is an upper bound of @xmath12 . fortunately a reasonable estimate can be obtained at each iteration @xmath83 without large overheads .",
    "for any tridiagonal matrix @xmath85 of order @xmath68 , the number of eigenvalues greater than a fixed number @xmath96 is the number of positive values of @xmath97 , where this set of numbers is defined by @xmath98 and d^(j)=t_jj-_0^(i)-(t_j-1,j)^2/d^(j-1 ) , [ next]for @xmath99 @xcite .",
    "an upper bound for @xmath95 can always be fixed by searching for a number for which at least one of @xmath97 is non - positive .",
    "this can be done by bisection , starting from the initial estimate at the first step , @xmath100 . while this procedure increases the complexity by order @xmath101 , the new stopping criterion in eq .",
    "( [ stop ] ) has two advantages over the usual cg stopping criterion first , @xmath102 is reduced and , second , the method becomes better adaptable since the observed @xmath103 for a given @xmath104 becomes independent of @xmath105 .",
    "practically , to do the computation without storing the orthonormal basis @xmath82 , one makes @xmath102 iterations to get the truncated matrix @xmath85 , computes the matrix @xmath89 and the diagonal @xmath88 using standard methods @xcite , and then repeats the @xmath102 iterations to compute the solution @xmath106 $ ] .",
    "the most stringent restriction on the algorithm seems to be that one can not use any pre - conditioning and must always start the iterations from @xmath107 .",
    "this algorithm has only one parameter , @xmath104 .",
    "the algorithm automatically adjusts the number of iterations to achieve the specified precision irrespective of the condition number .",
    "thus , no configuration dependent tuning of algorithmic parameters is necessary when employing the cga for qcd applications .",
    "the complexity of the cga is _",
    "cga 2wv+^2 2wv(1)[compcga]where @xmath108 is a number independent of @xmath109 .",
    "the @xmath110 term comes from the handling of the tridiagonal matrix , and can be neglected since @xmath111 .",
    "the space complexity is the same as that of a standard cg _",
    "cga 8n_c(n_c+3)v .",
    "[ spaccga]since the method is adaptive , no deflation is necessary .",
    "however , deflation reduces the condition number of the matrix , and hence could improve the complexity by reducing @xmath102 .",
    "nevertheless , for reasons that we have already discussed in connection with ca and ora , deflation is unlikely to improve the performance at fixed physics when taking the limit of large @xmath109 .",
    "the results of our numerical tests for this algorithm are collected in tables [ tb.cga1][tb.cga3 ] .",
    "note that for all three test configurations there is a threshold in @xmath104 above which @xmath112 and below which @xmath113 is roughly constant .",
    "the threshold value of @xmath104 is somewhat larger than @xmath12 for the configuration .",
    "similar thresholds are also seen for the ora and za .",
    "this behaviour possibly reflects the existence of a large unconverged subspace in the cg iterations .",
    "with @xmath16 and @xmath104 . these are results of computations with configurations a ( dashed lines ) , b ( dotted lines ) and c ( full lines ) . ]        in figure [ fg.comp ] we have collected different measures of performance of the four algorithms we investigated in this paper , namely , the optimized rational approximation ( ora ) @xcite , the zolotarev approximation ( za , which is also a rational expansion ) @xcite , the chebychev approximation ( ca , a polynomial expansion ) @xcite and the conjugate gradient approximation ( cga , an iterative method ) @xcite .",
    "further details can be found in tables [ tb.cheby1][tb.cga3 ] .",
    "it is clear that for modest values of the condition number of @xmath14 , @xmath114 , the ca is the preferred algorithm .",
    "this is clear from the figure , as well as our results for the algorithmic complexities in eqs .",
    "( [ compca ] ) , ( [ compora ] ) , ( [ compza ] ) and ( [ compcga ] )",
    ". however , with increasing condition numbers the performance of ca rapidly degrades .",
    "this is visible in the figure as well as in our analysis of the adaptability in eq .",
    "( [ adaptca ] ) .",
    "we have argued earlier that these drawbacks of the ca are generic to all polynomial expansions .",
    "the ora , in its present form with fixed @xmath68 , also suffers from a lack of adaptability . in principle , this can be alleviated if the order of the approximation can be chosen adaptively .",
    "we have implemented the za , which is another rational approximation , for several different choices of order . as can be seen from tables [ tb.zolo1]- [ tb.zolo3 ] , and from figure [ fg.comp ] ,",
    "this improves the performance tremendously . for comparable cpu times , corresponding to low order za , the performance is at least one order better than that of ora on all configurations .",
    "the key to improving the performance of rational approximations is the automatic variation of the order @xmath68 with the condition numbers . in our tests",
    "we have simulated adaptability by working with several different orders and retained the one corresponding only to a small fraction of the inversion error .",
    "the so - tuned order is only slightly higher than that obtained in an automatic procedure defined at the end of section [ sc.za ] .",
    "the cga depends on only one parameter @xmath104 . for a given value of @xmath104 , the corresponding errors @xmath103 and @xmath115 are almost independent of the condition number of the matrix , thanks to the relaxed stopping criterion .",
    "the price for such a good adaptability is a computing time which is 50% higher than za for a given accuracy ( 70% excess if @xmath116 is small , 20% for large @xmath116 ) .",
    "the price , however , ensures that for all the configurations one guarantees the same order of accuracy from a given value of @xmath104 and with a predicted value of @xmath115 .",
    "the variation in the number of conjugate gradient iterations , @xmath102 , as the stopping criterion , @xmath104 , is changed for the three configurations is shown in figure [ fg.complex ] .",
    "the data for the ora are not shown in the figure because they are very similar to those of the za .",
    "note that the curves for the cga lie below that for the za ( despite the shift in za as compared to cga ) , which is the influence of the relaxed stopping criterion discussed in section [ sc.cga ] .",
    "@xcite has devised a similar modification for za which can reduce @xmath102 in that case .",
    "note that our above results for za did not use any such modifications ; using it will further enhance the performance of za reported above .",
    "we have noted in section [ sc.errors ] that the relations ( [ rel1 ] ) between the errors are valid for those approximations to @xmath11 which commute with @xmath14 . in particular",
    ", we noted that for the iterative algorithms these relations become valid , provided that @xmath104 is sufficiently small . in figure [ fg.nonlin ]",
    "we demonstrate this for @xmath117 , which is expected to be zero when @xmath104 is small enough . for the cga and za",
    "( data for the ora are not shown because they almost coincide with that for za ) , @xmath117 decreases with @xmath104 .",
    "the slopes in this plot correspond to linear decrease when @xmath104 is sufficiently small .",
    "clear non - linearities are present for larger @xmath104 when the condition number is large .",
    "we believe that these non - linearities are due to large non - converged subspaces , implying a need for high accuracy .",
    "for fixed order algorithms the adaptability , @xmath19 , quantifies the configuration dependence of speed .",
    "the numerical study can be used more directly to illustrate the adaptability by studying the slowdown in going from configuration a to b ( , from @xmath32 to @xmath118 ) or from a to c ( @xmath16 changes from @xmath119 to @xmath120 ) . as shown in figure [ fg.adapt ] ,",
    "both za and cga are adaptable algorithms over a wide range of @xmath103 .",
    "since za is faster , as seen in figure [ fg.comp ] , it is thus the method of choice .",
    "note , however , that cga is very comparable to it , and may be preferred for its self - tuning ability .",
    "we emphasise that a fair test of relative performance of algorithms is to work without deflation .",
    "first , deflation improves the performance of each of the algorithms we have investigated .",
    "details are given in the sections on each algorithm .",
    "nevertheless the algorithms based on rational approximation seems to be less sensitive to deflation than other ones because of the positive shifts introduced in the matrix .",
    "second , since the computation of the eigensystem of @xmath14 , necessary to deflation , is done at finite accuracy , it introduces extra errors .",
    "if the error in the computation of the eigenvalue @xmath121 is @xmath122 , then the contribution to @xmath103 is @xmath123 .",
    "thus , if we want to achieve a given @xmath103 , then we must keep @xmath124 .",
    "when the condition number @xmath16 increases , this criterion becomes impossible to satisfy , leading to catastrophic loss of accuracy .",
    "we feel it worth pointing out that deflation is only one of many possible methods to decrease the effective condition number of the problem .",
    "other preconditioning methods have not been seriously explored for overlap fermions .",
    "the cost of accurate numerical methods seems to suggest that numerically stable preconditioning methods will pay a big dividend in this problem .",
    "99 h.  neuberger , _ phys .",
    "_ , b 417 ( 1998 ) 141 [ hep - lat/9707022 ] .",
    "p.  h.  ginsparg and k.  g.  wilson , _ phys .",
    "_ , d 25 ( 1982 ) 2649",
    ". t.  w.  chiu , _ phys .",
    "_ , d 58 ( 1998 ) 074511 [ hep - lat/9804016 ] .",
    "h.  neuberger , _ phys .",
    "_ , 81 ( 1998 ) 4060 [ hep - lat/9806025 ] .",
    "p.  hernndez , k.  jansen , m.  lscher , _ nucl .",
    "_ , b 552 ( 1999 ) 363 [ hep - lat/9808010 ] .",
    "a.  borii , _ j.  comput",
    "_ , 162 ( 2000 ) 123 [ hep - lat/9910045 ] .",
    "r.  g.  edwards , u.  heller , r.  narayanan , _ nucl .  phys .",
    "_ , b 540 ( 1999 ) 457 [ hep - lat/9905028 ]",
    ". j.  van den eshof _",
    "_ , _ computer phys .",
    "comm . _ 146 ( 2002 ) 203 [ hep - lat/0202025 ] .",
    "w .  chiu and t .- h .",
    "hsieh , hep - lat/0204009 ; s.  j.  dong _ et al .",
    "_ , hep - lat/0304005 .",
    "p.  hernndez , k.  jansen , l.  lellouch , _ phys .",
    "_ , b 469 ( 1999 ) 198 [ hep - lat/9907022 ] .",
    "r.  v.  gavai , s.  gupta , r.  lacaze , _ phys .",
    "_ , d 65 ( 2002 ) 094504 [ hep - lat/0107022 ]",
    ". for a general analysis of error propagation in gram - schmidt orthogonalization , see g.  h.  golub and c.  f.  van loan , _ matrix computations _ , john hopkins university press , 1996 .",
    "r.  g.  edwards , u.  heller , r.  narayanan , _ phys . rev .",
    "_ , d 61 ( 2000 ) 074504 [ hep - lat/9910041 ]",
    ". s.  j.  dong _ et al .",
    "_ , _ phys .",
    "_ , 85 ( 2000 ) 5051 [ hep - lat/0006004 ] . t. degrand , _ phys .  rev . _ ,",
    "d 63 ( 2001 ) 034503 [ hep - lat/0007046 ] .",
    "t.  w.  chiu _ et al .",
    "_ , hep - lat/0206007 .",
    "see , section 11.4 in p.  lascaux and r.  thodor , _ analyse numrique matricielle applique  lart de lingnieur ,  ( 2 ) _ , masson , paris , 1994 ; see also section 8.5.2 in @xcite .",
    "see , for example , the section on matrix square roots in r.  a.  horn and c.  r.  johnson , _ topics in matrix analysis _",
    ", cambridge university press , 1991 ."
  ],
  "abstract_text": [
    "<S> we compare the efficiency of four different algorithms to compute the overlap dirac operator , both for the speed , _ </S>",
    "<S> i.e. _ , time required to reach a desired numerical accuracy , and for the adaptability , _ i.e. _ , the scaling of speed with the condition number of the ( square of the ) wilson dirac operator . </S>",
    "<S> although orthogonal polynomial expansions give good speeds at moderate condition number , they are highly non - adaptable . </S>",
    "<S> one of the rational function expansions , the zolotarev approximation , is the fastest and is adaptable . </S>",
    "<S> the conjugate gradient approximation is adaptable , self - tuning , and nearly as fast as the za . </S>"
  ]
}