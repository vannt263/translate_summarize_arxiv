{
  "article_text": [
    "in the field of science and engineering , a black - box is considered as a device ( figure [ bb ] ) , system or object which can only be observed in terms of inputs and outputs . in other words , someone working with black - box is not aware of the internal process of the system ; but the output result can only be observed for any given input . in the field of optimization ,",
    "black - box optimization is one of the challenging problems .",
    "suppose a black - box function needs to be minimized .",
    "note that , a black - box function might have multiple minimums . due to unknown form of the explicit function",
    ", derivatives can not be evaluated analytically at any point . which makes this problem harder than the non - convex minimization problem where unlike black - box functions derivatives can be evaluated analytically .",
    "+        in the field of convex optimization , ` gradient descent ( gd ) ' method ( @xcite ) , ` trust region reflective ( trf ) ' algorithm ( @xcite ) , ` interior - point ( ip ) ' algorithm ( @xcite ) and ` sequential quadratic programming ( sqp ) ' algorithm ( @xcite ) are most common and widely used nowadays .",
    "most of these algorithms ( e.g. , gd , trf , sqp ) use derivatives to find the direction of the movement while minimizing any objective function .",
    "another problem with these convex algorithms is they look for local solution and hence they are more likely to get struck at any local minimum in case the objective function has multiple minimums . hereby due to requirement of analytical derivative ( for most of them ) and the tendency to stop iterations after reaching any local solution , convex optimization algorithms are not suitable for minimizing black - box functions",
    ". for low dimensional non - convex optimization problems , the strategy of using convex optimization techniques with multiple starting points might be affordable , but with increasing dimension of the parameter space , this strategy proves to be computationally very expensive since with increasing dimension , the requirement of the number of starting points increases exponentially .",
    "+ in many types of existing optimization algorithms , the main motivation is to minimize the number of function evaluations required to find a reasonable local minimum ( e.g. , @xcite ) which might not be desirable if finding the global minimum is our main objective . in the last century , many non - convex global optimization strategies were proposed among which ` genetic algorithm ( ga ) ' ( see @xcite ) and ` simulated annealing ( sa ) ' ( see @xcite ) remained quite popular and are being widely used .",
    "but as mentioned in @xcite , one of the problem of ga is it does not scale well with complexity because in higher dimensional optimization problems there is often an exponential increase in search space size . besides , one major problem with these two afore - mentioned methods is they might be much expensive in case we use these methods for optimizing simple convex functions without the knowledge of it s convexity ( has been shown in the simulation study ) . among other methods ,",
    "one of the most commonly used black - box optimization technique ` particle swarm optimization ( pso ) ' was first proposed in @xcite .",
    "a few modification of this method can be found in @xcite .",
    "+ to solve black - box functions , various derivative free and coordinate search based techniques have evolved . in 1952",
    ", @xcite proposed a very simple but effective coordinate search algorithm for minimizing unconstrained black - box functions .",
    "while minimizing a function , the main principle was to set a step - size and then move each coordinate by that step size in positive and negative direction one at a time , thus evaluating the objective function value at @xmath1 points in the neighborhood .",
    "out of these @xmath2 points ( including the starting point of the iteration ) , the point with minimum function value is considered as the updated solution .",
    "if the solution stops improving , the same steps are repeated by decreasing the step - size to the half of its previous value .",
    "thus it allows a finer search . generalized pattern search ( gps )",
    "algorithm ( @xcite ) can be noted as more general version of the afore - mentioned algorithm .",
    "later , a few other coordinate search based and derivative free methods have been proposed in @xcite .",
    "some other derivative - free optimization methods have been proposed in @xcite .",
    "+ in this paper a derivative - free pattern search based method is proposed for minimizing a black - box function on a hyper - rectangular domain .",
    "similar to the fermi s principle ( @xcite , figure [ fermi ] ) and the principle of gps ( @xcite ) , in the proposed algorithm in each iteration , the value of the objective function is evaluated at @xmath1 neighboring points which are obtained by making @xmath1 ( where @xmath0 is the dimension of the parameter space ) coordinate - wise movements with step - sizes .",
    "but unlike the fermi s principle and gps , in the proposed method inside an iteration the coordinate - wise movement step - sizes are modified for each coordinates and directions and hereby their values might not be the same during an iteration . in the proposed method , during an iteration , the coordinate - wise step - size is changed only if the corresponding jump yields a point outside the domain .",
    "thus before evaluating the function value at some point , it is verified that the point is within the domain of search . again , unlike fermi s method and gps , another strategy of the proposed algorithm is to restart the search procedure starting from the obtained local solution with large step sizes again .",
    "the algorithm terminates when two consecutive restarts yield the same solution .",
    "this principle of restart helps to jump out of the local solution .",
    "one of the biggest advantage of the proposed algorithm is that the objective functions can be evaluated parallely in @xmath1 directions since once the step - size for the iteration is fixed , the jumps and the functional value evaluation steps in @xmath1 possible directions are independent of each other .",
    "another great benefit of this technique is that in this way the requirement of parallel computing increases in the order of the number of parameters which is very convenient for gpu computing .",
    "the proposed algorithm is termed as ` recursive modified pattern search ( rmps ) ' .",
    "movements starting from any initial point inside an iteration with fixed step - size @xmath3 while optimizing any @xmath0-dimensional objective function.,scaledwidth=80.0% ]",
    "suppose we have a objective function @xmath4 where @xmath5 is the parameter of dimension @xmath0 .",
    "our objective is to @xmath6 where @xmath7 , @xmath8 $ ] are closed and bounded intervals on @xmath9 for @xmath10 . now consider the bijection @xmath11^n\\end{aligned}\\ ] ] where @xmath12^n$ ] is such that @xmath13 .",
    "so , without loss of generality , we can assume the domain of @xmath14 to be @xmath15^n$ ] .",
    "+ the proposed algorithm consists of several _",
    "runs_. each _ run _ is an iterative procedure and a _ run _ stops based on some convergence criteria ( see below ) . at the end of each _",
    "run _ a solution is returned .",
    "after the first _ run _ , following _ runs _ will start from the solution obtained by the _",
    "just before it .",
    "for example , @xmath16 _ run _ will start from the solution obtained by @xmath17 _",
    "run_. so the user should set the starting point for the first _ run _ ( see figure [ flowchart ] ) only .",
    "each run tries to improve the solution in a ` greedy ' manner by making coordinate - wise jumps through a sequence of decreasing step sizes gradually decreasing to zero , around the solution obtained by the previous _ run _ ( see below for details ) .",
    "thus , with each _ run _ , the solution either gets improved or remains unchanged .",
    "if two consecutive _ runs _ give the same solution up to some fixed decimal place @xmath18 ( e.g. , if accuracy up to 3 decimal place of the solution point is desired , user should set @xmath19 ) set by the user , the algorithm stops returning the current value of the solution ( which is the final solution ) . + in the proposed algorithm , each _ run _ is similar except the values of the tuning parameters which can be reset after each _",
    "run_. inside a _ run _ , we have three tuning parameters which are initial _ global step size _",
    "@xmath20 , _ step decay rate _ @xmath21 ( it is either equal to @xmath22 or @xmath23 , see below for details ) , _ step size threshold _",
    "@xmath24 respectively . for the first _ run _ , we set @xmath25 and for following _ runs _ , we set @xmath26 .",
    "other tuning parameters are kept same of all the _",
    "runs_. in every iteration we have a parameter called _ global step size _ ( denoted by @xmath27 for @xmath28-th iteration ) and @xmath1 local parameters called _ local step sizes _ ( denoted by @xmath29 and @xmath30 ) . in a _ run _ , we start the first iteration setting the _ global step size _ @xmath31 . within every iteration ,",
    "the value of the _ global step size _ is kept unchanged throughout all the operations .",
    "but at the end of the iteration , based on a convergence criteria ( see step ( 6 ) of stage 1 ) either it is kept same or decreased by a factor of @xmath21 . the next iteration is started with that new value of _ global step size_. so the value of @xmath32 ( i.e. , the _ global step size _ at @xmath33-th iteration ) can be either @xmath27 or @xmath34 . at the beginning of any iteration ,",
    "the _ local step size _ @xmath29 and @xmath30 are set to be equal to the _ global step size _ of the corresponding iteration .",
    "for example , at the beginning of @xmath28th iteration , we set , @xmath35 for @xmath10 . the _ local step sizes _ which generate points outside the domain are updated so that all the new @xmath1 points obtained by moving with the _ local step sizes _ @xmath29 and @xmath30 are within the domain .",
    "assume the current value of @xmath14 at the @xmath28-th iteration is @xmath36 .",
    "if moving @xmath37 by @xmath38 ( @xmath39 at the beginning ) in the positive direction generates a point outside the domain ( i.e. , @xmath40 ) , then @xmath38 is updated to the value @xmath41 where @xmath42 is the smallest possible integer such that @xmath43 . similarly",
    "if moving @xmath37 by @xmath44 ( @xmath45 at the beginning ) in the negative direction generates a point outside the domain ( i.e. , @xmath46 ) , then @xmath44 is updated to the value @xmath41 where @xmath42 is the smallest possible integer such that @xmath47 .",
    "it should be noted that while choosing @xmath42 for any given coordinate and given direction , it is made sure that the updated _ local step size _ is greater than _ step size threshold _ @xmath24 . in case such",
    "a @xmath42 is not feasible ( for example , if @xmath48 , then no such @xmath42 exists such that @xmath43 and @xmath49 ) , that particular coordinate is not updated in the corresponding direction .",
    "so in short , in an iteration , there will be always only one _ global step size _ and @xmath1 _ local step sizes _ which are initialized within the iteration being equal to _ global step size _ and at the end of the iteration , each of them end up being less than or equal to the _ global step size _ of that iteration .",
    "again it should be noted that in a _ run _ , the _ global step size _ might decrease or remain same after each iteration .",
    "on the other hand , the _ local step sizes _ have memory - less properties since their values do not depend of their old values in the previous iteration .",
    "_ ends when _ global step size _ becomes smaller than @xmath24 .",
    "+      @xmath50 _ step decay rate _ ( @xmath21 ) : : :    @xmath21 determines the rate of change of _ global step size _    at the end of each iteration . .",
    "so it is understandable that the value    of @xmath21 must be greater than 1 .",
    "taking smaller values of    @xmath21 will make the decay of step sizes slower , which    would allow finer search within the domain at the cost of more    computation time .",
    "once we get a solution from first _ run _ setting    @xmath25 , to incorporate finer search , we again start    following _ runs _ with smaller decay rate @xmath23 .",
    "it is    noted that a reasonable range for @xmath22 and    @xmath23 are @xmath51 $ ] and    @xmath52 $ ] respectively . but",
    "based on simulation    experiments , it is noted that @xmath53    yields satisfactory performance for a wide range of benchmark    functions of lower , moderate and higher dimensions .",
    "+ @xmath50 _ step size threshold _ ( @xmath24 ) : : :    @xmath24 controls the precision of the solution .",
    "this is the    minimum possible value that the _ global step size _ and the _ local step    sizes _ can take . once the _ global step size _ goes below    @xmath24 , the _ run _ stops . setting the value of    @xmath24 to be smaller results in better precision in the    cost of higher computation time . the default value of _ step size    threshold _",
    "@xmath24 is taken to be @xmath54 . in    case more precision",
    "is required , or if there is knowledge of    possibility of multiple local minimas within a very small    neighborhood , @xmath24 can be taken to be smaller .",
    "+ @xmath50 @xmath55 : : :    @xmath55 denotes the maximum number of    iterations allowed inside a _ run_. its default values is set to be    @xmath56 .",
    "+ @xmath50 @xmath57 : : :    @xmath57 denotes the maximum number of _ runs _",
    "allowed in the algorithm .",
    "its default values is set to be    @xmath58 .",
    "+ @xmath50 @xmath59 : : :    @xmath59 is another precision parameter which    determines the minimum amount of movement after an iteration in the    solution which is required to keep the value of _ global step size _    unchanged .",
    "in other words , if the sum of squares of differences of    solutions obtained in two consecutive iterations is less than    _ tol_fun _ , the improvement is not considered to be significant and the    _ global step size _ is decreased for a finer search .",
    "its default value    has been taken to be @xmath60 .",
    "it    should be noted that taking smaller value of this parameter would    yield finer local solution in the cost of more computation time .",
    "+ @xmath50 @xmath18 : : :    the second _ run _ onwards , whenever a _ run _ ends , it is checked whether    the solution returned by the current _ run _ is the same or different    with the solution returned by the previous _",
    "run_. however , to check    whether they are exactly equal , they need to be matched up to several    decimal places depending on the type of storage variable and the type    of software used .",
    "thus it might result into a lot of extra _ runs _ just    to improve the solution at distant decimal places which might be    unnecessary . hereby , the value of _",
    "round_factor _ should be fixed by    the user and if the solution returned by two consecutive _ runs _",
    "match    up to this many decimal place , the final solution is returned .",
    "its    default value is taken to be @xmath61 .",
    "based on simulation    studies , it is recommended to choose    @xmath62 . choosing    larger value of @xmath18 results in finer    solution up to larger number of decimal places in the cost of higher    number of function evaluations .",
    "+      as mentioned in section [ sec_algo ] , the algorithm consists of a series of _ runs _ and each _ run _ is similar except the values of the tuning parameters .",
    "below the algorithm has been described dividing into two stages . in stage 1 ,",
    "the internal mechanism of a single _ run _ has been described . in stage 2 , it is decided whether to exit the algorithm or to perform the next _ run _ based of the value of current solution . before going through stage 1 for the very first time , we set @xmath63 and initial guess of the solution @xmath64 .",
    "+ stage 1 :    1 .",
    "set @xmath65 .",
    "set @xmath66 go to step ( 2 ) .",
    "2 .   if @xmath67 , set @xmath68 , go to step ( 8) .",
    "else , set @xmath69 and @xmath70 for all @xmath71 .",
    "set @xmath72 and go to step ( 3 ) .",
    "if @xmath73 , set @xmath72 and go to step ( 4 ) . else evaluate vector @xmath74 such that @xmath75 if @xmath76 , go to step ( 3d ) . if @xmath77 and @xmath78 , go to step ( 3a ) .",
    "else go to step ( 3b ) .",
    "set @xmath79 where @xmath80 + 1 $ ] and go to step ( 3c)(here @xmath81 $ ] denotes the greatest smaller integer function , e.g. , @xmath82=1 $ ] ) .",
    "2 .   set @xmath83 , @xmath84 and go to step ( 3 ) .",
    "3 .   set @xmath85 and go to step ( 3d ) .",
    "4 .   evaluate @xmath86 .",
    "set @xmath84 and go to step ( 3 ) .",
    "if @xmath73 , set @xmath72 and go to step ( 5 ) . else evaluate vector @xmath87 such that @xmath88 if @xmath89 , go to step ( 4d ) . if @xmath90 and @xmath91 , go to step ( 4a ) .",
    "else go to step ( 4b ) .",
    "set @xmath92 where @xmath93 + 1 $ ] and go to step ( 4c ) .",
    "2 .   set @xmath94 , @xmath84 and go to step ( 4 ) .",
    "3 .   set @xmath95 and go to step ( 4d ) .",
    "4 .   evaluate @xmath96 .",
    "set @xmath84 and go to step ( 4 ) .",
    "set @xmath97 and @xmath98 . if @xmath99 , go to step ( 5a ) .",
    "else , set @xmath100 and @xmath101 , set @xmath102 .",
    "go to step ( 6 ) .",
    "1 .   if @xmath103 , set @xmath104 , else ( if @xmath105 ) , set @xmath106 . set @xmath102 .",
    "go to step ( 6 ) .",
    "if @xmath107 , set @xmath108 . go to step ( 7 ) .",
    "else , set @xmath109 .",
    "go to step ( 2 ) .",
    "7 .   if @xmath110 ( convergence criteria 1 ) , set @xmath111 .",
    "go to step ( 8) . 8 .",
    "stop execution .",
    "set @xmath112 .",
    "set @xmath113 .",
    "go to stage 2 .",
    "stage 2 :    1 .",
    "if @xmath114 or @xmath115 , stop , return @xmath116 as final solution and exit . else go to step ( 2 ) 2 .",
    "set @xmath117 keeping other tuning parameters ( @xmath24 and @xmath20 ) intact .",
    "repeat algorithm described in stage 1 setting @xmath118 . if else repeat step ( 1 ) .",
    ".,scaledwidth=80.0% ]      setting _ step size threshold _ to a sufficiently small value , it can be shown that if the function is differentiable and convex then the the value of the objective function at the solution is the global minimum ( see appendix : a ) .",
    "however for non - convex functions , there is no way to ensure whether that is a global minimum or not . to increase the likelihood of reaching the global minimum , the stage 1 of the algorithm is repeated starting from the solution obtained from the last _ run _ until the solution returned by two consecutive _ runs _ are same . in the first _ run _ , we set @xmath119 and for the following runs , we set @xmath117 . after first _",
    "run _ , setting smaller value of @xmath21 results in slower decay of step size in the iterations of the following _",
    "runs_. thus , we look for better solution in the domain changing each co - ordinate one at a time for a sequence of finite step sizes slowly decaying to zero .",
    "+ note that the jump - start strategy at the end of each run is performed to ensure that the solution does not get struck in some local solution . but in case it is known that the objective function is convex , clearly , there is no need to jump - start since each _ run _ is designed in such a way that it returns a local solution ( see appendix : a ) which is the global solution in case of convex objective function .",
    "so for minimizing a convex function , only one _ run _ is sufficient . thus using the prior knowledge of convexity , computation time",
    "can be reduced while minimizing a convex function ( see section [ sec_convex ] ) .",
    "+      it should be noted that although this idea of coordinate - wise movement with a given step size in possible @xmath1 directions is similar to that proposed in @xcite and ` generalized pattern search ( gps ) ' ( @xcite ) , there are several novel strategies and modifications in the proposed algorithm which makes it quite different .",
    "firstly , the restart strategy with smaller step - size decay rate is something which is possibly proposed for the first time in the context of pattern search to the best of our knowledge .",
    "specially , using finer search induced by the smaller step - size decay rate from the second _ run _ and onwards has been noted to work very well for various ranges of benchmark functions in simulation study .",
    "this strategy makes the proposed algorithm quite different form all existing direct search ( ds ) @xcite and pattern search ( ps)(@xcite,@xcite ) algorithms .",
    "secondly , unlike algorithm 1 of @xcite , instead of unconstrained minimization , the proposed algorithm minimizes the black - box function on a hyper - rectangle . in @xcite and @xcite ,",
    "the coordinate - wise jump sizes were kept equal inside an iteration while in the proposed algorithm , the domain of each coordinate being bounded , in every iteration , local - step sizes are modified separately for each coordinates in each direction as required . in gps , each coordinate - wise jump step - sizes are evaluated using ` exploratory moves algorithm ' ( see @xcite ) while in the proposed algorithm it s straightforward and does not use ` exploratory moves algorithm ' . while optimizing a function on a hyper - rectangle , since the domain is transformed into unit hyper - cube , the global step - size is kept same for each coordinate .",
    "so while determining the step - sizes of coordinate - wise movements , the proposed algorithm uses different strategy than the ` exploratory moves algorithm ' . in gps , the step size ( which plays the same role as the _ global step size _ in the proposed algorithm ) is decreased if the there is no improvement in the objective function . while in the proposed algorithm , the value of _ global step size _ is reduced only if there is no ` significant ' change in the solution between two consecutive iterations which is determined by the tuning parameter @xmath120 .",
    "introduction of the tuning parameter @xmath120 plays an important role .",
    "there might be a scenario when moving with a given step size is improving very slowly .",
    "since improvements are there ( even if relatively very small ) , the iterations will be still going on with same step size in gps . while in that scenario , the proposed algorithm would reduce the _ global step size _ earlier instead of wasting function evaluation costs for very small improvements . and also that amount of ` significant ' improvement , which is controlled by @xmath120 , should be defined by the user in the proposed algorithm .",
    "+ at the beginning of each _ run _ , the strategy of making jumps within the unit cube domain with varying global step - sizes is the most unique feature of the proposed algorithm and to the best of our knowledge , have never been proposed before .",
    "most of the global optimization methods ( e.g. , ga , pso ) can be thought as a combination of movement around the sample space and local minimization around the potential solutions found . in the proposed algorithm , at the beginning of each _",
    "run _ , in search of a better solution starting from a local minimum , the sample space is searched in such a way that at each iteration , the number of new points checked is @xmath1 which is of order @xmath0 . also it can be easily verified from the algorithm described in section [ sec_algo ] that the number of operations performed in each iteration is also of order @xmath0 . once the algorithm gets stuck at some local solution ( as it happens at the end of each _ run _ ) , to avoid higher order or exponential rated search for better solution , the above - mentioned step - size decaying strategy has been considered since using this technique , the sample space can be traversed making operations of order @xmath0 only at each iteration .",
    "thus it makes it very convenient for high - dimensional optimization . starting from several initial points , using this strategy results in very high proportions of returned solutions near the true solution even in 1000 - 5000 dimensional challenging benchmark problems unlike existing and well recognized global optimization methods like ga , sa . +",
    "in this section , we compare the performance of the proposed algorithm with well known ` simulated annealing ( sa ) ' ( @xcite ) , and ` genetic algorithm ( ga ) ' ( @xcite ) .",
    "both of these algorithms are available in matlab r2014a ( the mathworks ) via the optimization toolbox functions _ simulannealbnd _",
    "( for sa ) , and _ ga _ ( for ga ) respectively . in our comparative study , we set the maximum number of allowed iterations and evaluations of objective function to be infinity for _ simulannealbnd _ function . in case of _",
    "ga _ , we use the default values .",
    "our proposed algorithm recursive modified pattern search ( rmps ) is implemented in matlab 2014a and the values of the tuning parameters have been taken to be default ( as mentioned in section [ sec_algo ] ) .",
    "we consider 45 benchmark functions and each test function is minimized starting from 10 randomly generated points ( under 10 consecutive random number generating seeds in matlab ) within the considered domain of solution space using rmps , ga and sa . the domains of the search regions can be found in table [ domains ] ( in appendix : b ) .",
    "all simulation studies in this paper have been performed in a machine with 64-bit windows 8.1 , intel i7 3.60ghz processors and 32 gb ram . in table",
    "[ comparison_table_1 ] , the obtained minimum values in each occasions have been noted for all considered algorithms with average computation time . it is noted that rmps and ga perform more or less better than sa . it should be also noted that rmps yields reasonable solution in lesser time than ga and sa in most of the cases . using rmps up to 9 and 15 folds improvement in computation times are obtained over ga ans sa respectively .",
    "although ga yielded better solutions in some of the cases at the cost of more computation time , it should be noted that in the case of rmps , taking the value of _ step size threshold _ ( @xmath24 ) smaller than its default value ( i.e. , @xmath54 ) , we can obtain finer solution with more computation time .",
    "as mentioned in section [ restart ] , to minimize any convex function , in general , it is sufficient to perform only one _ run_. therefore the prior knowledge of convexity can be used to save computational time .",
    "along with that , it is noted that using @xmath121 makes it even faster ( since the function is convex , in general , steep decrease in _ global step size _ should not affect the result ) . in table",
    "[ table_convex ] , a comparison study of performances of rmps , and changed rmps with the prior knowledge of convexity ( rmps(c ) ) , ga and sa has been provided for minimizing sphere and sum squares function for various dimensions starting from 10 randomly generated starting points ( under 10 consecutive random number generating seeds in matlab ) in each cases . unlike rmps , in rmps(c ) we perform only 1 _ run _ and take @xmath121 .",
    "values of all other tuning parameters in rmps(c ) are kept same as that of rmps .",
    "it is noted that in each cases , rmps(c ) performs faster than rmps .",
    "also in terms of computation times , using rmps(c ) we get up to 40 folds improvement with respect to ga and up to 92 folds improvement with respect to sa .",
    "to compare the performance of rmps with that of ga and sa , 100 and 1000 dimensional ackley s function , griewank function , rastrigin function , schwefel function , sphere function and the sum of square function have been considered .",
    "the domains on which these functions are minimized are @xmath122^d$ ] , @xmath123^d$ ] , @xmath124^d$ ] , @xmath125^d$ ] , @xmath124^d$ ] and @xmath124^d$ ] respectively .",
    "schwefel function attains the global minimum value 0 at @xmath126 @xmath127 within the corresponding domain while all other functions attain the global minimum value 0 at the origin .",
    "we use _ ga _ and _ simulannealbnd _ of matlab r2014a ( the mathworks ) for ga and sa with tuning parameter values as described in section [ sec_app ] . as mentioned in the earlier section , rmps has been implemented in matlab and default parameter values ( as mentioned in section [ sec_algo ] ) has been used .",
    "while solving a particular function on a given domain , 10 distinct randomly generated ( under 10 consecutive random number generating seeds in matlab ) initial points has been considered and that same set of 10 starting points are used for ga , sa and rmps for a fair comparison . for ga and sa ,",
    "the smallest of the 10 obtained objective values have been noted along with average computation times ( table [ table_high_dim ] ) . for rmps , both the maximum and the minimum of the objective values and the average computation times have been noted down for each cases . in table [ table_high_dim ] , it is noted that rmps generally outperforms ga and sa . in all the cases , the maximum value of the obtained solutions by rmps is better than the minimum values obtained by ga and sa .",
    "a significant improvement for using rmps over ga and sa is visible especially in the case of solving the 1000 dimensional problems .",
    "using rmps we get up to 32 folds improvement in computation over ga ( in case of 100-dimensional sum squares function ) and 368 folds improvement in computation over sa ( in case of 1000-dimensional sum squares function ) . in figures [ 1000_ackley ] , [ 1000_griewank ] , [ 1000_rastrigin ] , [ 1000_schwefel ] , [ 1000_sphere ] , [ 1000_sumsquare ] , a comparative study of rmps , ga and",
    "sa has been made based on the improvement of the value of the 1000 dimensional objective functions in first 30 minutes .",
    "for all the functions except sum squares function , the objective values have been plotted in absolute scale ( figures [ 1000_ackley ] , [ 1000_griewank ] , [ 1000_rastrigin ] , [ 1000_schwefel ] , [ 1000_sphere ] ) , while for sum squares function ( figure [ 1000_sumsquare ] ) , the objective values have been plotted in natural @xmath128 scale .",
    "+    [ table_high_dim ]    ) ]    ) ]    ) ]    ) ]    ) ]    ) ]    [ plots ]    to compare the performances of rmps for the case where the solution is at a boundary point , we repeat the above - mentioned simulation study with changed domains . in this case , the domains of ackley s function , griewank function , rastrigin function , schwefel function , sphere function and the sum of square function are taken to be @xmath129^d$ ] , @xmath130^d$ ] , @xmath131^d$ ] , @xmath132^d$ ] , @xmath131^d$ ] and @xmath131^d$ ] respectively .",
    "note that , in each cases , the true solution is a boundary point . in table",
    "[ table_high_dim_boundary ] , it is noted that in this case also rmps generally outperforms ga ans sa . using rmps we get up to 40 folds improvement in computation over ga ( in case of 100-dimensional sum squares function ) and 77 folds improvement in computation over sa ( in case of 1000-dimensional sum squares function ) .",
    "+    [ table_high_dim_boundary ]    in table [ table_5000 ] , we note down the maximum and minimum values of the obtained solutions after minimizing the 5000 dimensional objective functions with rmps starting from 3 distinct randomly generated ( under 3 consecutive random number generating seeds in matlab ) initial points .",
    "the average computation time in each case has been also noted down in table [ table_5000 ] . due to requirement of excessive amount of time to solve these problems using ga and sa , we could not evaluate their performances in this case .",
    "the problem of recovering an unknown matrix from only a given fraction of its entries is known as matrix completion problem . @xcite",
    "first proposed a method to recover a matrix from a few given entries solving a convex optimization problem . later to solve this problem , @xcite minimized nuclear norm of the matrix subject to the constraint that the given entries of the matrix should be the same .",
    "in other words , suppose we have a matrix @xmath133 with some missing values .",
    "in that case , as mentioned in @xcite , the complete matrix @xmath134 can be obtained by solving the following problem , @xmath135 where @xmath136 denotes the nuclear norm , @xmath137 being the @xmath138-th singular value of matrix @xmath139 .",
    "this problem can be solved using convex optimization technique . on a closer look ,",
    "it is noticeable that minimizing nuclear norm in this fashion in similar to the lasso ( @xcite ) penalty term .",
    "@xcite proposed smoothly clipped absolute deviation ( scad ) penalty which was shown to have more desirable properties compared to lasso for solving shrinkage based variable selection problems .",
    "but unlike lasso , scad penalty is not a convex minimization ( or concave maximization ) problem . in this section",
    ", the matrix completion problem has been solved using the scad penalty with rmps .",
    "the matrix completion problem using scad penalty can be re - formulated as @xmath140 where @xmath141 are singular values and @xmath142 is the scad penalty function dependent of tuning parameters @xmath143 and @xmath144 ( see @xcite ) . + we consider a picture ( figure [ fig : given_pic ] ) with @xmath145 pixels where approximately half ( 1877 to be precise ) of its pixels are missing .",
    "the problem given by equation can been seen as a black - box function of dimension of 1877 ( i.e. , the number of missing pixels ) .",
    "it is also known that the numerical value of grey level of each pixel must be between 0 and 255 .",
    "this problem is solved using rmps method .",
    "we fit the model for 30 values of @xmath143 which are @xmath146 and we obtain the best visual output for @xmath147 ( figure [ fig : scad_final_point_900 ] ) .",
    "+            [ figfig6a ]    unlike the functions considered in the previous simulations studies , it should be noted that the evaluation of scad penalty based on the singular values of the matrix is more computationally intensive .",
    "therefore , the buffer time required of initialization and result collection part ( see section [ sec_discussion ] ) while using parallel threading is comparatively less in this case compared to the time required to perform the operations at each iteration .",
    "thus , in this case using parallel computing is beneficial .",
    "it should be noted that this is a 1877 dimensional problem and therefore up to 3754 parallel threads can be used while solving it using rmps algorithm .",
    "we use 4 parallel threads to derive the complete image given in figure [ fig : scad_final_point_900 ] . for comparison of computation time required by single threading and parallel threading with 4 threads , the required computation times for first 50 , 100 and 200 iterations have been provided for either cases in table [ parallel ] .",
    "we note more than 3 folds improvement in time for using parallel threading ( with 4 threads ) instead of single threading .",
    "@cccc@ iterations &    .computation times ( in seconds ) required for first 50 , 100 and 200 iterations of rmps using single thread and 4 parallel threads . [ cols=\"^ \" , ]      + 50 & 1325.93 & 391.38 & 3.39 + 100 & 3189.97 & 881.75 & 3.62 + 200 & 7622.24 & 2162.56 & 3.52 +",
    "as mentioned in section [ sec_algo ] , the proposed algorithm is parallelizable and up to @xmath148 parallel threads can be used while solving a @xmath149-dimensional black - box problem .",
    "however in the simulation study part , the time required for optimizing each function has been noted down without using parallel computing . in matlab , in case ` parfor ` loop is used instead of ` for ` loop to perform parallel computing , depending on the operations performed within the loops , a scenario might arise where ` for ` loop works faster than ` parfor ` loop . because at the beginning of the ` parfor ` loop , an amount of time is spent in shipping the parallelizable works to different workers and at the end again , some time is spent in collecting the results .",
    "but this additional time is not spent in case of using ` for ` loop .",
    "so , in case the amount of tasks or operations performed in each loop is not much computationally intensive compared to the amount of buffer time required by ` parfor ` loop , it may actually spend more time than that required by ` for ` loop . in case of our test functions",
    ", it is observed that ` parfor ` loop takes more time than ` for ` loop which is why all the final computation times have been noted down using ` for ` loop only . + while using parallel computing , the time required for allocation and collection of works sent to different workers varies for different softwares .",
    "so , in case some software takes comparatively less time for these operations , parallel computing might work faster than single thread computation even for the considered benchmark functions in this paper . in case the black - box function is really complicated ( e.g. , it s evaluation involves linear search , differentiation or integration , matrix inversion , products etc ) parallelization should work much faster than single thread computing .",
    "thus benefits of parallelizable algorithm would be more visible and much useful . in section [ sec_scad ] , it has been shown that the parallel threading can improve the computation time by while solving complex black - box functions using rmps .",
    "this paper presents an efficient derivative - free algorithm for solving black - box function on a hyper - rectangle . unlike ga , sa and most of the other meta - heuristic black - box optimization techniques",
    ", rmps is a deterministic algorithm and therefore while minimizing any objetive function , it returns the same solution in any software under any random number generating seed if the same staring point is considered . for several benchmark functions , it is noted that rmps generally outperforms ga and sa in terms of accuracy of the solution or the computation time or simultaneously in both perspectives . in section [ sec_convex ] , it is shown that the prior knowledge of convexity of the objective function can be exploited and in that case solution can be found in less computation time . in section [ high_dim ] , it is noted that using rmps we get up to 40 folds improvement over ga and up to 368 fold improvement over sa in terms of computation time .",
    "boundary solution cases are also considered for moderate and high - dimensional benchmark functions and in that case also rmps generally outperforms ga and sa . +",
    "as seen in the simulation studies of sections [ sec_convex ] and [ high_dim ] , the worst solution obtained using rmps , after starting point 10 randomly generated starting points , is quite accurate in all the cases and better than the best solution obtained by ga and sa for those cases .",
    "therefore , it is noted that rmps is less dependent on the starting point and it returns almost equally good solution starting from any initial guess . it is more prominent from the simulation results for 5000-dimensional cases given in table [ table_5000 ] .",
    "+ another important feature of rmps is that the number of required function evaluations in each iteration for this proposed algorithm increases only in the order of the dimension of the black - box function .",
    "@xmath148 parallel threads can be used while solving a @xmath149-dimensional problem . in section [ sec_scad ] ,",
    "rmps is used to solve the matrix completion problem using scad penalty over the sum of singular values .",
    "the benefits of parallel implementation of this algorithm is also noted .",
    "it is well known that it is not possible for any algorithm to reach to a global minimum of any black - box function every time .",
    "but under restrictive conditions like convexity , it is desirable for any optimization algorithm to return the global minimum . in this section",
    "we show that under some regularity conditions , rmps algorithm reaches a global minimum of the objective function . consider the following theorem    [ theorem ] suppose @xmath150^n \\mapsto \\mathbb{r}$ ] is a differentiable convex function .",
    "consider a point @xmath151^n$ ] .",
    "consider a sequence @xmath152 for @xmath153 and @xmath154 .",
    "define @xmath155 and @xmath156 for @xmath10 . if for all @xmath157 , @xmath158 and @xmath159 for all @xmath160 , the global minimum of @xmath42 occurs at @xmath161 .",
    "+    take an open neighborhood @xmath162^n$ ] w.r.t .",
    "@xmath163-norm containing @xmath161 at the center .",
    "so , there exists @xmath164 such that @xmath165 where @xmath166 for @xmath167 . for some @xmath168 ,",
    "define @xmath169 such that @xmath170 . since @xmath42 is convex on @xmath171",
    ", it can be easily shown that @xmath172 is also convex on @xmath173 .",
    "we claim that @xmath174 for all @xmath175 .",
    "+ suppose there exist a point @xmath176 such that @xmath177 .",
    "take @xmath178 .",
    "clearly @xmath179 .",
    "without loss of generality , assume @xmath180 .",
    "hence @xmath181 .",
    "since @xmath182 is a strictly decreasing sequence going to @xmath183 , there exists a @xmath184 such that for all @xmath185 , @xmath186 .",
    "now we have @xmath187 .",
    "now there exists a @xmath188 such that @xmath189 . from convexity of @xmath172 , we have @xmath190    . ]    . ]",
    "but , we know @xmath191 .",
    "hence it is a contradiction .",
    "+ since partial derivatives of @xmath42 exist at @xmath192 , @xmath172 is differentiable at @xmath193 .",
    "since @xmath194 is a local minima of @xmath172 in @xmath173 , we have @xmath195 .",
    "so we have @xmath196 . by similar argument",
    "it can be shown that @xmath197 for all @xmath198 .",
    "since @xmath42 is convex and @xmath199 , @xmath161 is a local minima in @xmath171 .",
    "now , since @xmath162^n$ ] , @xmath161 is also a local minima of @xmath200^n$ ] . but",
    "@xmath42 is convex on @xmath200^n$ ] .",
    "since any local minimum of a convex function is necessarily global minimum , the global minimum of @xmath42 occurs at @xmath161 .",
    "suppose the objective function is convex and all the partial derivatives exist at the obtained solution @xmath201^n$ ] which is an interior point .",
    "the proposed algorithm terminates when two consecutive _ runs _ yield the same solution .",
    "it implies in the last _ run _ , the objective function values at all the sites obtained by making jumps of sizes @xmath202 ( until @xmath182 gets smaller than _ step size threshold _ ) around @xmath161 , i.e. @xmath203 and @xmath204 , are greater than or equal to @xmath205 for @xmath167 .",
    "so , taking _ step size threshold _ sufficiently small , our algorithm will reach the global minimum under the assumed regularity conditions of the objective function . from theorem [ theorem ]",
    "it is concluded that if the objective function is convex and differentiable then taking _ step size threshold _ sufficiently small yields the global minimum .",
    "it is to be noted that if the function is convex and differentiable and it takes minimum value at some interior point , evaluation of only the first _ run _ is sufficient to obtain the solution .",
    "i would really like to thank dr .",
    "rudrodip majumdar for helping me correcting the earlier versions of this draft .",
    "i appreciate the help of my colleagues debraj das and suman chakraborty who helped me correcting the theoretical properties in the earlier versions .",
    "i am really glad to have dr .",
    "hua zhou s insight and suggestions regarding the application part of this paper .",
    "i would like to acknowledge my statistics ph.d .",
    "advisor dr .",
    "subhashis ghoshal for suggesting me possible challenging optimization problems which made me think of the proposed algorithm .",
    "d. w. marquardt , an algorithm for least - squares estimation of nonlinear parameters , journal of the society for industrial and applied mathematics , vol .",
    "11 , 431441 ( 1963 ) r. h. byrd , m. e. hribar , j. nocedal , an interior point algorithm for large scale nonlinear programming , siam journal on optimization , vol .",
    "9(4 ) , 877900 ( 1999 ) + r. h. byrd , j. c. gilbert , j. nocedal , a trust region method based on interior point techniques for nonlinear programming , mathematical pro gramming , vol .",
    "89(1 ) , 149185 ( 2000 ) + t. f. coleman , y. li , an interior , trust region approach for nonlinear minimization subject to bounds , siam journal on optimization , vol . 6 , 418445 ( 1996 ) + f. a. potra , s. j. wright , interior - point methods , journal of computational and applied mathematics , vol . 4 , 281302 ( 2000 ) + n. karmakar , new polynomial - time algorithm for linear programming , combinator1ca , vol . 4 , 373395 ( 1984 ) + s. boyd , l. vandenberghe , convex optimization , cambridge university press , cambridge , 2006 + m. h. wright , the interior - point revolution in optimization : history , recent developments , and lasting consequences , bulletin of american mathematical society , vol . 42 , 3956 ( 2005 )",
    "+ j. nocedal , s. j. wright , numerical optimization , 2nd edition , operations research series , springer , 2006 + p. t. boggs , j. w. tolle , sequential quadratic programming , acta numerica , 152 , ( 1996 ) + j. goodner , g. a. tsianos , y. li , g. loeb , biosearch : a physiologically plausible learning model for the sensorimotor system , proceedings of the society for neuroscience annual meeting , 275.22/ll11 ( 2012 ) + a. s. fraser , simulation of genetic systems by automatic digital computers i. introduction , australian journal of biological sciences , vol .",
    "10 , 484491 ( 1957 ) + a. d. bethke , genetic algorithms as function optimizers ( 1980 ) + d. e. goldberg , genetic algorithms in search , optimization , and machine learning , operations research series , addison - wesley publishing company , ( 1989 ) + s. kirkpatrick , c. d. g. jr , m. p. vecchi , optimization by simulated annealing , australian journal of biological sciences , vol .",
    "220(4598 ) , 671680 ( 1983 ) + v. granville , m. krivanek , j. p. rasson , simulated annealing : a proof of convergence , ieee transactions on pattern analysis and machine intelligence , vol . 16 , 652656 ( 1994 ) + l. geris , computational modeling in tissue engineering , springer , 2012 + j. kennedy , r. eberhart , particle swarm optimization , proceedings of ieee international conference on neural networks , vol . 4 , 19421948 ( 1995 ) + a.i.f .",
    "vaz , l.n .",
    "vicente , a particle swarm pattern search method for bound constrained global optimization , journal of global optimization , vol .",
    "39(2 ) , 197219 ( 2007 ) y. shi , r. c. eberhart , a modified particle swarm optimizer , proceedings of ieee international conference on evolutionary computation , vol .",
    ", 6973 ( 1998 ) + h. zeineldin , e. el - saadany , m. salama , optimal coordination of overcurrent relays using a modified particle swarm optimization , electr .",
    "power syst .",
    "76 ( 11 ) , 988995 ( 2006 ) + m.m .",
    "mansour , s. mekhamer , n. s. el - kharbawe , a modified particle swarm optimizer for the coordination of directional overcurrent relays , ieee trans .",
    "power deliv .",
    "22(3 ) , 14001410 ( 2007 ) + e. fermi , n. metropolis , numerical solution of a minimum problem .",
    "los alamos unclassified report la1492 , los alamos national laboratory , los alamos , usa ( 1952 ) + v. torczon , on the convergence of pattern search algorithms , siam journal on optimization , vol .",
    "7(1 ) , 125 ( 1997 ) + c. audet , j. e. dennis jr , mesh adaptive direct search algorithms for constrained optimization , siam journal on optimization , vol .",
    "17(1 ) , 188217 ( 2006 ) + a. l. custodio , j . f. a. madeira , glods : global and local optimization using direct search , journal of global optimization , vol .",
    "62(1 ) , 128 ( 2015 ) + j. m. martnez , f. n. c. sobral , constrained derivative - free optimization on thin domains , journal of global optimization , vol .",
    "56(3 ) , 12171232 ( 2003 ) + t. g. kolda , r. m. lewis , v. torczon , optimization by direct search : new perspectives on some classical and modern methods , siam review , vol .",
    "45(3 ) , 385482 ( 2003 ) + r. m. lewis , v. torczon , pattern search algorithms for bound constrained minimization , siam journal on optimization , vol .",
    "9(4 ) , 10821099 ( 1999 ) + c. audet , a survey on direct search methods for blackbox optimization and their applications , mathematics without boundaries : surveys in interdisciplinary research , chapter 2 , 31-56 ( 2014 ) + a.r .",
    "conn , k. scheinberg , l.n .",
    "vicente , introduction to derivative - free optimization .",
    "mos - siam series on optimization , siam ( 2009 ) + d.r jones , m. schonlau , w.j .",
    "welch , efficient global optimization of expensive black box functions , journal of global optimization , 13(4 ) , 455492 ( 1998 ) + s. le digabel , algorithm 909 : nomad : nonlinear optimization with the mads algorithm , acm transactions on mathematical software , vol .",
    "37(4)(44 ) , 115 ( 2011 ) + e. martelli , e. amaldi , pgs - com : a hybrid method for constrained non - smooth black - box optimization problems : brief review , novel algorithm and comparative evaluation , computers and chemical engineering , vol .",
    "63 , 108139 ( 2014 ) + c. audet , v. bechard , s. le digabel , nonsmooth optimization through mesh adaptive direct search and variable neighborhood search , journal of global optimization , vol .",
    "41(2 ) , 299318 ( 2008 ) . + c. audet , j.e .",
    "dennis jr . , s. le digabel , parallel space decomposition of the mesh adaptive direct search algorithm .",
    "siam journal on optimization , vol .",
    "19(3 ) , 11501170 ( 2008 ) + e. j. candes , b. recht , exact matrix completion via convex optimization , foundations of computational mathematics , vol . 9 ,",
    "717772 ( 2009 ) + e. j. candes , t. tao , the power of convex relaxation : near - optimal matrix completion , ieee transactions on information theory , vol .",
    "56(5 ) , 20532080 ( 2010 ) + r. tibshirani , regression shrinkage and selection via the lasso , journal of the royal statistical society . series b ( methodological ) , vol .",
    "58(1 ) , 267288 ( 1996 ) + j. fan , r. li , variable selection via nonconcave penalized likelihood and its oracle properties , journal of the american statistical association , vol .",
    "96(456 ) , 13481360 ( 2001 )"
  ],
  "abstract_text": [
    "<S> in this paper , a pattern search based optimization technique is developed to optimize any black - box function on a hyper - rectangle . </S>",
    "<S> this algorithm consists of a series of ` runs ' and inside each ` run ' iterations are performed until a convergence criteria is satisfied following the principle which is similar to that of generalized pattern search . during an iteration , jumps are made along the co - ordinates of the parameter one at a time with varying step - sizes within the restricted parameter space to search for the best direction to move . while solving a problem on @xmath0-dimensional hyper - rectangle , inside each iteration the objective function </S>",
    "<S> is evaluated at @xmath1 independent directions . </S>",
    "<S> hereby parallel computing can be easily incorporated using up to @xmath1 ( i.e. , in the order of @xmath0 ) threads which is very convenient for gpu computing . unlike other existing black - box optimization techniques ( e.g. , genetic algorithm ( ga ) , simulated annealing ( sa ) ) , the prior knowledge of convexity about the objective function can be exploited and in that case it can be solved in lesser time . </S>",
    "<S> the comparative study of the performances of the proposed algorithm , ga and sa have been provided for several low - dimensional , a few moderate and high - dimensional benchmark functions with corresponding computation times . </S>",
    "<S> simulation study has also been performed for moderate and high - dimensional cases where the solution is a boundary point in the domain . </S>",
    "<S> this black - box optimization technique has been used to solve matrix completion problem with non - convex regularization incorporating parallel computing . </S>"
  ]
}