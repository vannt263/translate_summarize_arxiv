{
  "article_text": [
    "since the first large scale simulations of self gravitating systems the direct @xmath0-body method has gained a solid footing in the research community . at the moment @xmath0-body techniques are used in astronomical studies of planetary systems , debris discs , stellar clusters , galaxies all the way to simulations of the entire universe @xcite . outside astronomy the main areas of research which utilise the same techniques are molecular dynamics , elementary particle scattering simulations , plate tectonics , traffic simulations and chemical reaction network studies . in the latter non - astronomical applications ,",
    "the main force evaluating routine is not as severe as in the gravitational @xmath0-body simulations , but the backbone simulation environments are not very different .    the main difficulty in simulating self gravitating systems is the lack of antigravity , which results in the requirement of global communication ; each object feels the gravitational attraction of any other object .    the first astronomical simulation of a self gravitating @xmath0-body system was carried out by @xcite with the use of 37 light bulbs and photoelectric cells to evaluate the forces on the individual objects .",
    "holmberg spent weeks in order to perform this quite moderate 37-particle simulation . over the last 60 or so years many different techniques",
    "have been introduced to speed up the kernel calculation .",
    "today , such a calculation requires about 50000 integration steps for one dynamical time unit . at a speed of @xmath6gflop /",
    "s the calculation would be performed in a few seconds .",
    "the gravitational @xmath0-body problem has made enormous advances in the last decade due to algorithmic design .",
    "the introduction of digital computers in the arena @xcite led to a relatively quick evaluation of mutual particle forces .",
    "advanced integration techniques , introduced to turn the particle forces in a predicted space - time trajectory , opened the way to predictable theoretical results .",
    "one of the major developments in the speed - up and improved accuracy of the direct @xmath0-body problem was the introduction of the block - time step algorithm @xcite .    in the late 1980s it became quite clear that the advances of modern computer technology via moore s law @xcite was insufficient to simulate large star clusters by the new decade @xcite .",
    "this realization brought forward the initiatives employed around the development of special hardware for evaluating the forces between the particles @xcite , and of the efficient use of assembler code on general purpose hardware @xcite .",
    "one method to improve performance is by parallelising force evaluation eq.[eq : force ] for use on a beowulf or cluster computer ( with or without dedicated hardware)@xcite , a large parallel supercomputer @xcite or for grid operations @xcite .",
    "in particular for distributed hardware it is crucial to implement an algorithm that limits communication as much as possible , otherwise the bottleneck simply shifts from the force evaluation to interprocessor communication .",
    "a breakthrough in direct - summation @xmath0-body simulations came in the late 1990s with the development of the grape series of special - purpose computers @xcite , which achieve spectacular speedups by implementing the entire force calculation in hardware and placing many force pipelines on a single chip .",
    "the latest special purpose computer for gravitational @xmath0-body simulations , grape-6 , performs at a peak speed of about 64tflop / s @xcite .    in our standard setup , one grape-6af processor board",
    "is attached to a host workstation , in much the same way that a floating - point or graphics accelerator card is used .",
    "we use a smaller version : the grape-6af which has four chips connected to a personal workstation via the pci bus delivering a theoretical peak performance of @xmath7 gflop / s for systems of up to 128k particles at a cost of @xmath8k @xcite .",
    "advancement of particle positions [ @xmath9 is carried out on the host computer , while interparticle forces [ @xmath10 are computed on the grape .",
    "the latest developments in this endeavour is the design and construction of the grape - dr , the special purpose computer which will break the pflop / s barrier by the summer of 2008 @xcite .",
    "one of the main arguments to develop such a high powered and relatively diverse computer is to perform simulations of entire galaxies @xcite .",
    "the main disadvantages of these special purpose computers , however , are the relatively short mean time between failure , the limited availability , the limited applicability , the limited on - board memory to store particles , the simple fact that they are basically build by a single research team led by prof . j. makino and the lack of competing architectures .    the gaming industry , though not deliberately supportive of scientific research , has been developing high power parallel vector processors for performing specific rendering applications , which are in particular suitable for boosting the frame - rate of games . over the last 7 years graphics processing units ( gpus )",
    "have evolved from fixed function hardware for the support of primitive graphical operations to programmable processors that outperform conventional cpus , in particular for vectorizable parallel operations .",
    "regretfully , the precision of these processors is still 32-bit ieee which is below the average general purpose processor , but for many applications it turns out that the higher ( double ) precision is not crucial or can be emulated at some cost .",
    "it is because of these developments , that more and more people use the gpu for wider purposes than just for graphics @xcite .",
    "this type of programming is also called general purpose computing on graphics processing units ( gpgpu ) . earlier attempts to use a gpu for gravitational n - body simulations",
    "were carried out using approximate force evaluation methods with shared time steps @xcite , but provide little improvement in performance . a 25-fold speed increase compared to an intel pentium",
    "iv processor was reported by @xcite , but details of their implementation of the force evaluation algorithm are yet unclear .",
    "recently , @xcite proposed the chamomile scheme for running @xmath0-body simulations with a shared time - step algorithm on gpus .",
    "though , their method , using the cuda programming environment , outperforms our implementations , the shared time step renders their code unpractical for simulating dense star clusters .    using gpus as a general purpose vector processor works as follows .",
    "colours in computer graphics are represented by one or more numbers .",
    "the luminance can be represented by just a single number , whereas a coloured pixel may contain separate values indicating the amount of red , green and blue .",
    "a fifth value alpha may be included to indicate the amount of transparency . using this information ,",
    "a pixel can be drawn .",
    "for general purpose computing , the colour information of a pixel is used to represent attributes of the computation .",
    "there are many pixels in a frame , and ideally , these should be updated all at the same time and at a rate exceeding the response time of the human eye .",
    "this requires fast computations for updating the pixels when , for example , a camera moves or a new object comes into view .",
    "such operations usually have an impact on many or even all pixels and therefore fast computations are required .",
    "as the majority of pixels do not require information from other pixels , processing can be done efficiently in parallel .",
    "all information required to build a pixel should go through a series of similar operations , a technique which is better known as single instruction , multiple data ( simd ) .",
    "there are many different kinds of operations this information needs to go through .",
    "the stream programming model has been designed to make the information go through these operations efficiently , while exposing as much parallelism as possible .",
    "the stream programming model views all informations as `` streams '' of ordered data of the same data type .",
    "the streams pass through `` kernels '' that operate on the streams and produce one or more streams as output .",
    "in this paper we report on our endeavour to convert a high precision production quality @xmath0-body code to operate with graphics processing units . in  [ sect : nbody ] we explain the adopted @xmath0-body integration algorithm , in  [ sect : cg ] we address the programming environment we used to program the gpu . in the sections ",
    "[ sect : results ] and  [ sect : performance ] we present the results on two gpus and compare them with grape-6af and we discuss a model to explain the gpu s performance . in ",
    "[ sect : discussion ] we summarise our findings , and in the appendix we present a snippet of the source code in cg .",
    "the gravitational evolution of a system consisting of @xmath0 stars with masses @xmath11 and at position @xmath12 is computed by the direct summation of the newtonian force between each of the @xmath0 stars .",
    "the force @xmath13 acting on particle @xmath14 is then obtained by summation of all other @xmath15 particles @xmath16 here @xmath17 is the newton constant . for further readability",
    "we omit the particle index @xmath14 and present vectors in boldface .",
    "a cluster consisting of @xmath0 stars evolves dynamically due to the mutual gravity of the individual stars . for an accurate force calculation on each star a total of @xmath18 partial forces",
    "have to be computed .",
    "this @xmath19 operation is the bottleneck for the gravitational @xmath0-body problem .",
    "the gpu scheme described in this paper is implemented in the @xmath0-body integrator .",
    "here particle motion is calculated using a fourth - order , individual - time step `` hermite '' predictor - corrector scheme ( makino and aarseth 1992 ) .",
    "this scheme works as follows . during a time step the positions ( @xmath20 ) and velocities ( @xmath21 )",
    "are first predicted to fourth order using the acceleration ( @xmath22 ) and the `` jerk '' ( @xmath23 , the time derivative of the acceleration ) which are known from the previous step .    at the start of each simulation",
    "an initial time step is calculated @xmath24 here we introduce @xmath25 as an accuracy control parameter ( @xmath26 for most of our simulations , see also eq.[eq : timestep ] ) .",
    "the predicted position ( @xmath27 ) and velocity ( @xmath28 ) are calculated for all particles @xmath29    the acceleration ( @xmath30 ) and jerk ( @xmath31 ) are then recalculated at the predicted time from @xmath32 and @xmath33 using direct summation . finally , a correction is based on the estimated higher - order derivatives : @xmath34 here @xmath35 . which then leads to the new position and velocity at time @xmath36 .",
    "@xmath37    the new timestep is calculated using a new predicted second derivative of the acceleration @xmath38 for each particle @xmath14 individually with @xcite @xmath39 here we use for accuracy parameter @xmath26 .",
    "a single integration step in the integrator proceeds as follows :    * determine which stars are to be updated .",
    "each star has an individual time ( @xmath40 ) associated with it at which it was last advanced , and an individual time step ( @xmath41 ) .",
    "the list of stars to be integrated consists of those with the smallest @xmath42 .",
    "time steps are constrained to be powers of 2 , allowing `` blocks '' of many stars to be advanced simultaneously @xcite .",
    "* before the step is taken , check for system reinitialization , diagnostic output , termination of the run , storing data . *",
    "perform low - order prediction of all particles to the new time @xmath42 .",
    "this operation may be performed on the gpu or grape , whatever is available .",
    "* recompute the acceleration and jerk on all stars in the current block ( using the gpu or grape , if available ) , and correct their positions and velocities to fourth - order .",
    "note that this scheme is rather simple as it does not include treatment for close encounters , binaries or higher order ( hierarchical or democratic ) stable multiple systems .    .",
    "detailed information on the hardware used in our experiments .",
    "the first column gives the parameter followed by the four different hardware setups ( grape-6af , geforce 8800gtx , quadro fx1400 and information about the host computer .",
    "the information for the grape is taken from @xcite , the gpu information is from http://www.nvidia.com .",
    "the hardware details are the number of processor pipelines ( ) , the processor s clock frequency ( @xmath43 ) , the memory bandwidth for communication between host and attached processor ( @xmath44 ) , the amount of memory ( in number of particles , one particle requires 84bytes , here we adopt @xmath45 ) . for measured hardware parameters ,",
    "see tab.[tab : hardware ] .",
    "note that the measured internal communication speed between the device memory and the gpu for the 8800gtx and the fx1400 are 86.4 gbyte / s and 19.2 gbyte / s , respectively .",
    "[ tab : gpu ] [ cols=\"<,>,>,>,>,>\",options=\"header \" , ]     in fig.[fig : speedup ] we present the speed - up for the various hardware configurations , compared to running on the host workstation .",
    "here it is quite clear that for low @xmath0 the gpus do not give a appreciable speedup , but for a large number of particles , the geforce 8800gtx gives a speedup of at least an order of magnitude , but not as much as the grape .",
    "the latter , however , will not be able to perform simulations of more than 128k particles .",
    "we have successfully implemented the direct gravitational force evaluation calculation using cg on two graphics cards , the nvidia quadro fx1400 and the nvidia geforce 8800gtx , and compared their performance with the host workstation and the grape-6af special purpose computer .    for @xmath46 particles the workstation outperforms the gpus .",
    "this is mainly due to additional overhead introduced by the communication to the gpu and memory allocation on the gpu . for a larger number of particles the more modern gpu ( 8800gtx ) outperforms the workstation by up to about a factor of 50 ( for 9 million particles ) .",
    "such a large number of particles can not be simulated on the grape-6af , due to memory limitations . for up to 256k , the maximum number of particles that can be stored on the grape , the 8800gtx is slower than the grape by a factor of a few .",
    "still , at this particle number the gpu is faster than the workstation by an order of magnitude .    for the adopted accuracy @xmath26 ,",
    "the average mean error in the energy measured over 0.25n - body time unit is @xmath47 for the 8800gtx and @xmath48 for the fx1400 ( averaged over the simulations for @xmath49 to @xmath50k ) , whereas for the grape we measured @xmath51 , which is comparable to the mean error on the host .",
    "for the adopted accuracy , both the host and grape produce an energy error which is about an order of magnitude smaller than that of the gpus . for smaller values of @xmath25",
    "the energy errors in the grape continue to decrease whereas for the gpu this is not the case , as we have reached the precision of the hardware . for many applications",
    "an energy error of @xmath52 may be satisfactory .    in the release notes of cuda version 0.8",
    ", nvidia announced that gpus supporting 64-bit double precision floating point arithmetic in hardware will become available in late 2007 . in the meantime",
    ", we could improve the accuracy of the gpu by sorting the forces on size before adding them , summing the smallest forces first .",
    "the grape-6 is much more efficient in using its clock cycles , allowing effectively one operation per clock cycle , whereas the nvidia architecture requires more cycles .",
    "this turns out to be an important reason why the 8800gtx is slower than the grape-6 .",
    "the main advantage of the gpu over that of the dedicated grape hardware , is the much larger memory , the wider applicability and the much lower cost of the former . the large memory on the gpu allows simulations of up to about 9 million particles , though one has to wait for about two years for one dynamical time scale .",
    "in theory the 8800gtx should be able to outperform the grape-6af , but due to relatively inefficient memory access and additional overhead cost , which is not present in the grape hardware , many clock cycles seem to get lost . with a more efficient use of the hardware the gpu could ,",
    "in principle , improve performance by about two orders of magnitude . for the next generation of gpus",
    "we hope that this efficiency bottleneck will be lifted . in that case",
    ", the gpu would outperform grape by almost an order of magnitude .",
    "note , however , that the grape-6 is based on 5 year old technology , and the next generation grape is likely to outperform modern gpus by a sizable margin .    these current bottlenecks in the gpu",
    "may be reduced using the compute unified device architecture ( cuda ) programming environment , which is supposed to provide an improved environment for general purpose programming on the gpu . in fig.[fig : future ] we present the possible future performance assuming that the additional communication overhead on the gpu is lifted , the clock cycles are used more efficiently without any assumptions of improved hardware speed . in the first step",
    "we simple reduce communication to blocks rather than having to transport all particles each block time step ( solid curve ) .",
    "this relatively simple improvement has recently been carried out using cuda @xcite .",
    "the second optimalization ( dashed curve in fig.[fig : future ] ) is achieved when , in addition to reducing the communication we also carry the predictor and corrector steps to the gpu .",
    "this improvement , however , may be associated with a quite severe accuracy penalty . for both improvements we used the performance data for the current design 8800gtx .",
    "further improvement can be achieved when , in addition to more efficient communication and force computing pipeline in further optimized .",
    "the result of this hypothetical case would improve performance by more than a factor 100 compared to the workstation over the entire range of @xmath0 .",
    "we are grateful to mark harris and david luebke of nvidia for supplying us with the two nvidia geforce 8800gtx graphics cards on which part of the simulations were performed .",
    "we also like to thank jeroen bdorf , derek groen , alessia gualandris and jun makino for numerous discussions , and the referee piet hut for pointing us to the importance of discussing the accuracy of the gpu .",
    "this work was supported by nwo ( via grant # 635.000.303 and # 643.200.503 ) and the netherlands advanced school for astrophysics ( nova ) .",
    "the calculations for this work were done on the hewlett - packard xw8200 workstation cluster and the modesta computer in amsterdam , both are hosted by sara computing and networking services , amsterdam .",
    ", s.  j. 1999 , , 111 , 1333    , s.  j. , henon , m. , wielen , r. 1974 , , 37 , 183    , s.  j. , hoyle , f. 1964 , astrophysica norvegica , 9 , 313    , s.  j. 1985 , direct methods for n - body simulations , in brackbill , j.c . , cohen , b.i . , _ multiple time scales _ , p.377    , s.  j. , lecar , m. 1975 , , 13 , 1    , j.  h. , douglas , m.  r. , grsel , y. , hunter , p. , seitz , c.  l. , sussman , g.  j. 1986 , in p. hut , s.  l.  w. mcmillan ( eds . ) , lnp vol . 267 : the use of supercomputers in stellar dynamics , p.  86",
    "buck , i. , foley , t. , horn , d. , sugerman , j. , mike , k. , pat , h. 2004 , brook for gpus : stream computing on graphics hardware    , e.  n. , hemsendorf , m. , merritt , d. 2003 , journal of computational physics , 185 , 484    elsen , e. , houston , m. , vishal , v. , darve , e. , hanrahan , p. , pand , v. , 2006 , to appear in `` sc 06 : proceedings of the 2006 acm / ieee conference on supercomputing '' , acm press , new york .    , 2005 ,",
    "gpgpu  basic math tutorial , _ ergebnisberichte des instituts fr angewandte mathematik , nummer 300 _    fernando , r. 2004 , gpu gems ( programming techniques , tips , and tricks for real - time graphics ) , addison wesley , 0 - 321 - 22832 - 4    fernando , r. , kilgard , m.  j. 2003 , the cg tutorial ( the definitive guide to programmable real - time graphics ) , addison wesley , 0 - 321 - 19496 - 9    , t. , makino , j. , kawai , a. 2005 , , 57 , 1009    bdorf , p. , belleman , r. , portegies zwart , s , submitted to _ the international conference for high performance computing , networking , storage , and analysis_.    , a. , portegies zwart , s. , tirado - ramos , a. 2007 , parco in press , arxiv astrophysics e - prints ( astro - ph/0608125 )    hamada , t. , iitaka , t.  2007 , submitted to new astronomy , arxiv astrophysics e - prints ( astro - ph/0703100 )    , s. , gualandris , a. , merritt , d. , spurzem , r. , portegies zwart , s. , berczik , p. 2007",
    ", new astronomy in press , arxiv astrophysics e - prints ( astro - ph/0608125 )    , d.  c. , mathieu , r.  d. 1986 , in p. hut , s.  l.  w. mcmillan ( eds . ) , lnp vol .",
    "267 : the use of supercomputers in stellar dynamics , p.  233    , a. , portegies zwart , s. , bubak , m. , sloot , p. , 2007",
    ", submitted to crc press llc , arxiv astrophysics e - prints ( astro - ph/0703485 )    , e. 1941 , , 94 , 385    , p. 2007 , presented at a life with stars ( conference in honor of ed van den heuvel ) , amsterdam , august , 2007 , arxiv astrophysics e - prints ( astro - ph/0601232 )    , j. 1991 , , 369 , 200    , j. 2001 , in s. deiters , b. fuchs , a. just , r. spurzem , r. wielen ( eds . ) , asp conf . ser . 228 : dynamics of star clusters and the milky way , p.  87    , j. 2002 , new astronomy , 7 , 373    , j. 2005a , journal of korean astronomical society , 38 , 165    , j. 2007 , arxiv astrophysics e - prints ( astro - ph/0509278 )    , j. , aarseth , s.  j. 1992 , , 44 , 141    , j. , fukushige , t. , koga , m. , namura , k. 2003 , , 55 , 1163    , j. , hut , p. 1988",
    ", , 68 , 833    , j. , hut , p. 1990",
    ", , 365 , 208    , j. , taiji , m. 1998 , , scientific simulations with special - purpose computers : the grape systems /by junichiro makino & makoto taiji .",
    "chichester ; toronto : john wiley & sons , c1998 .    , s.  l.  w. , aarseth , s.  j. 1993 , , 414 , 200    moore , g.  e. 1965 , electronics , 38(8 )    nitadori k. , makino j. , hut p. , 2006 ,",
    "newa , 12 , 169    nitadori k. , makino j. , abe g. , 2007 , to appear in the conference proceedings of computational science 2006 , arxiv astrophysics e - prints ( astro - ph/0606105 )    nyland , l. , harris , m. , prins , j. , 2004 , poster presented at _ the acm workshop on general purpose computing on graphics hardware _ , aug .",
    "7 - 8 , los angeles , ca .",
    "pharr , m. , fernando , r. 2005 , gpu gems 2 ( programming techniques for high - performance graphics and general - purpose computation ) , addison wesley , 0 - 321 - 33559 - 7    , h.  c. 1911 , , 71 , 460    , m. , makino , j. , fukushige , t. , ebisuzaki , t. , sugimoto , d. 1996 , in p. hut , j. makino ( eds . ) , iau symp . 174 : dynamical evolution of star clusters : confrontation of theory and observations , p.  141    , t.  s. 1968 , , 19 , 479    , s. 1963 , zeitschrift fur astrophysik , 57 , 47",
    "the n - body code presented in this paper consists of a part implemented in c ( running on a cpu ) and a part implemented in cg ( running on the gpu ) . in this appendix",
    "we show the routine that evaluates the acceleration , jerk and potential in cg ( which was based on a tutorial available from @xcite ) .",
    "the c code which handles communication between cpu and gpu and supporting data structures is not presented here .",
    "a copy of the entire working version of the code is available via http://modesta.science.uva.nl .",
    ".... void compute_acc_jerk_and_pot (    in   float2 coords      : texcoord0 ,       //",
    "2d texture coordinate of this particle    out float3 acc         : color0 ,          //",
    "output texture with acceleration     out float4 jerkandpot : color1 ,          // output texture with jerk and potential    uniform samplerrect acctexture ,          //",
    "input texture with all particles ' acceleration    uniform samplerrect jerkandpottexture ,   //   , ,      , ,      , ,    , ,     , ,       jerk and potential    uniform samplerrect masstexture ,         //   , ,      , ,      , ,    , ,     , ,       mass    uniform samplerrect postexture ,          //   , ,      , ,      , ,    , ,     , ,       position    uniform samplerrect veltexture ,          //   , ,      , ,      , ,    , ,     , ,       velocity    uniform float eps2 ,                      // softening parameter    uniform float otherparticle ,             // index to other particle    uniform float texsizex ,                  //",
    "width of all textures    uniform float texsizey ,                  //",
    "height of all textures    uniform float offset )                    // number of unused texture elements {    float   coords1d , newcoords1d , othermass ,           r2 , xdotv , r2inv , rinv , r3inv , r5inv , xdotvr5inv ;    float2 newcoords ;    float3 pos , otherpos , vel , othervel , dx , dv , thisacc , thisjerkandpot ;       //",
    "get data from the textures    acc         = texrect(acctexture , coords).rgb ;    jerkandpot = texrect(jerkandpottexture , coords).rgba ;    pos         = texrect(postexture , coords).rgb ;    vel         = texrect(veltexture , coords).rgb ;      // convert the 2d texture coordinate to 1d and increase with otherparticle    // to obtain the coordinate of this iteration 's other particle .",
    "because our    // textures are defined as samplerrect , texture elements must be addressed     // as ( x+0.5,y+0.5 ) . when converting to 1d",
    ", we must compensate for this offset ) .",
    "coords1d = round(coords.y-0.5)*texsizex + round(coords.x-0.5 ) ;    newcoords1d = coords1d + otherparticle ;      //",
    "skip over unused texture elements    if ( newcoords1d + offset > texsizex*texsizey - 1 )      newcoords1d = newcoords1d - ( texsizex*texsizey - offset ) ;       // convert the other particle 's 1d coordinate to 2d . as above , we must add    //",
    "0.5 to obtain correct texture element coordinates .",
    "newcoords = 0.5 + float2 ( frac(newcoords1d / texsizex)*texsizex ,                              floor(newcoords1d / texsizex ) ) ;      // get the position , velocity and mass of this iteration 's other particle    otherpos   = texrect(postexture , newcoords).rgb ;    othervel   = texrect(veltexture , newcoords).rgb ;    othermass = texrect(masstexture , newcoords).r ;      // compute acceleration , jerk and potential    dx = otherpos - pos ;    dv = othervel - vel ;    r2 = eps2 + dot(dx , dx ) ;    xdotv = dot(dx , dv ) ;    r2inv = 1.0/r2 ;    rinv = sqrt(r2inv ) ;    r3inv = r2inv*rinv ;    r5inv = r2inv*r3inv ;    xdotvr5inv = 3.0*xdotv*r5inv ;    thisacc = othermass*r3inv*dx ;    thisjerkandpot = othermass*(r3inv*dv - xdotvr5inv * dx ) ;    acc = acc + thisacc ;    jerkandpot.rgb = jerkandpot.rgb + thisjerkandpot ;    jerkandpot.a = jerkandpot.a - othermass*rinv ; } ...."
  ],
  "abstract_text": [
    "<S> we present the results of gravitational direct @xmath0-body simulations using the commercial graphics processing units ( gpu ) nvidia quadro fx1400 and geforce 8800gtx , and compare the results with grape-6af special purpose hardware . </S>",
    "<S> the force evaluation of the @xmath0-body problem was implemented in cg using the gpu directly to speed - up the calculations . </S>",
    "<S> the integration of the equations of motions were , running on the host computer , implemented in c using the 4th order predictor - corrector hermite integrator with block time steps . </S>",
    "<S> we find that for a large number of particles ( @xmath1 ) modern graphics processing units offer an attractive low cost alternative to grape special purpose hardware . </S>",
    "<S> a modern gpu continues to give a relatively flat scaling with the number of particles , comparable to that of the grape . </S>",
    "<S> the grape is designed to reach double precision , whereas the gpu is intrinsically single - precision . </S>",
    "<S> for relatively large time steps , the total energy of the n - body system was conserved better than to one in @xmath2 on the gpu , which is impressive given the single - precision nature of the gpu . </S>",
    "<S> for the same time steps , the grape gave somewhat more accurate results , by about an order of magnitude . </S>",
    "<S> however , smaller time steps allowed more energy accuracy on the grape , around @xmath3 , whereas for the gpu machine precision saturates around @xmath4 for @xmath5 the geforce 8800gtx was about 20 times faster than the host computer . though still about a factor of a few slower than grape , modern gpus outperform grape in their low cost , long mean time between failure and the much larger onboard memory ; the grape-6af holds at most 256k particles whereas the geforce 8800gtx can hold 9 million particles in memory .    </S>",
    "<S>   a&a     gravitation  stellar dynamics  methods : n - body simulation  methods : numerical  </S>"
  ]
}