{
  "article_text": [
    "subset selection problems lie at the heart of many applications where a small subset of items must be selected to represent a larger population .",
    "typically , the selected subsets are expected to fulfill various criteria such as sparsity , grouping , or diversity .",
    "our focus is on _ diversity _ , a criterion that plays a key role in a variety of applications , such as gene network subsampling  @xcite , document summarization  @xcite , video summarization  @xcite , content driven search  @xcite , recommender systems  @xcite , sensor placement  @xcite , among many others  @xcite .",
    "diverse subset selection amounts to sampling from the set of all subsets of a ground set according to a measure that places more mass on subsets with qualitatively different items .",
    "an elegant realization of this idea is given by determinantal point processes ( dpps ) , which are probabilistic models that capture diversity by assigning subset probabilities proportional to ( sub)determinants of a kernel matrix .",
    "dpps enjoy rising interest in machine learning  @xcite ; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions , sampling , and extracting marginals  @xcite . but despite being polynomial - time , these tasks remain infeasible for large data sets .",
    "dppsampling , for example , relies on an eigendecomposition of the dppkernel , whose cubic complexity is a huge impediment .",
    "cubic preprocessing costs also impede wider use of the cardinality constrained variant @xmath0-dpp  @xcite .",
    "these drawbacks have triggered work on approximate sampling methods .",
    "much work has been devoted to approximately sample from a dppby first approximating its kernel via algorithms such as the nystrm method  @xcite , random kitchen sinks  @xcite , or matrix ridge approximations  @xcite , and then sampling based on this approximation .",
    "however , these methods are somewhat inappropriate for sampling because they aim to project the dppkernel onto a lower dimensional space while minimizing a matrix norm , rather than minimizing an error measure sensitive to determinants .",
    "alternative methods use a dual formulation @xcite , which however presupposes a decomposition @xmath1 of the dpp kernel , which may be unavailable and inefficient to compute in practice .",
    "finally , mcmc @xcite offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique .",
    "we pursue a yet different approach . while being similar to matrix approximation methods in exploiting redundancy in the data , in sharp contrast to methods that minimize matrix norms , we focus on minimizing the total variation distance between the original dppand our approximation . as a result ,",
    "our approximation models the true dppprobability distribution more faithfully , while permitting faster sampling .",
    "we make the following key contributions :    =1.5em    an algorithm that constructs coresets for approximating a @xmath0-dppby exploiting latent structure in the data .",
    "the construction , aimed at minimizing the total variation distance , takes @xmath2 time ; linear in the number @xmath3 of data points .",
    "the construction works as the overhead of sampling algorithm and is much faster than standard cubic - time overhead that exploits eigendecomposition of kernel matrices .",
    "we also investigate conditions under which such an approximation is good .    a sampling procedure that yields approximate @xmath0-dppsubsets using the constructed coresets . while most other sampling methods sample diverse subsets in @xmath4 time , the sampling time for our coreset - based algorithm is @xmath5 , where @xmath6 is a user - specified parameter _ independent of @xmath3_.    our experiments indicate that our construction works well for a wide range of datasets , delivers more accurate approximations than the state - of - the - art , and is more efficient , especially when multiple samples are required .",
    "[ [ overview - of - our - approach . ] ] overview of our approach .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    our sampling procedure runs in two stages .",
    "its first stage constructs an approximate probability distribution close in total variation distance to the true @xmath0-dpp .",
    "the next stage efficiently samples from this approximate distribution .",
    "our approximation is motivated by the diversity sampling nature of dpps : in a dppmost of the probability mass will be assigned to diverse subsets .",
    "this leaves room for exploiting redundancy .",
    "in particular , if the data possesses latent grouping structure , certain subsets will be much more likely to be sampled than others .",
    "for instance , if the data are tightly clustered , then any sample that draws two points from the same cluster will be very unlikely .",
    "the key idea is to reduce the effective size of the ground set .",
    "we do this via the idea of coresets  @xcite , small subsets of the data that capture function values of interest almost as well as the full dataset . here , the function of interest is a @xmath0-dppdistribution .",
    "once a coreset is constructed , we can sample a subset of core points , and then , based on this subset , sample a subset of the ground set . for a coreset of size @xmath7 ,",
    "our sampling time is @xmath5 , which is _ independent of @xmath3 _ since we are using @xmath0-dpps  @xcite .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    dpps have been studied in statistical physics and probability  @xcite ; they have witnessed rising interest in machine learning @xcite .",
    "cardinality - conditioned dppsampling is also referred to as `` volume sampling '' , which has been used for matrix approximations @xcite .",
    "several works address faster dppsampling via matrix approximations  @xcite or mcmc  @xcite . except for mcmc ,",
    "even if we exclude preprocessing , known sampling methods still require @xmath4 time for a single sample ; we reduce this to @xmath5 . finally , different lines of work address learning dpps  @xcite and map estimation @xcite .",
    "coresets have been applied to large - scale clustering @xcite , pca and cca  @xcite , and segmentation of streaming data  @xcite .",
    "a determinantal point process @xmath8 is a distribution over all subsets of a ground set @xmath9 of cardinality @xmath3 .",
    "it is determined by a positive semidefinite kernel @xmath10 .",
    "let @xmath11 be the submatrix of @xmath12 consisting of the entries @xmath13 with @xmath14 .",
    "then , the probability @xmath15 of observing @xmath16 is proportional to @xmath17 ; consequently , @xmath18 .",
    "conditioning on sampling sets of fixed cardinality @xmath0 , one obtains a @xmath0-dpp  @xcite : @xmath19 where @xmath20 is the @xmath0-th coefficient of the characteristic polynomial @xmath21 .",
    "we assume that @xmath22 for all subsets @xmath16 of cardinality @xmath0 . to simplify notation",
    ", we also write @xmath23 .",
    "our goal is to construct an approximation @xmath24 to @xmath25 that is close in _ total variation distance _ @xmath26 and permits faster sampling than @xmath25 .",
    "broadly , we proceed as follows .",
    "first , we define a partition @xmath27 of @xmath9 and extract a subset @xmath28 of @xmath7 core points , containing one point from each part . then , for the set @xmath29 we construct a special kernel @xmath30 ( as described in section  [ sec : sampling ] ) . when sampling , we first sample a set @xmath31 and then , for each @xmath32 we uniformly sample one of its assigned points @xmath33 .",
    "these second - stage points @xmath34 form our final sample .",
    "we denote the resulting distribution by @xmath35 .",
    "algorithm  [ smplalgo ] formalizes the sampling procedure , which , after one eigendecomposition of the small matrix @xmath30 .",
    "we begin by analyzing the effect of the partition on the approximation error , and then devise an algorithm to approximately minimize the error .",
    "we empirically evaluate our approach in section  [ sec : exp ] .",
    "let @xmath36 be a _ partition _ of @xmath9 , i.e. , @xmath37 and @xmath38 for @xmath39 .",
    "we call @xmath40 a _ coreset _ with respect to a partition @xmath41 if @xmath42 for @xmath43 $ ] . with a slight abuse of notation ,",
    "we index each part @xmath44 by its core @xmath45 . based on the partition @xmath41",
    ", we call a set @xmath16 _ _ singular _ _ is an independent set in the partition matroid defined by @xmath41 . ] with respect to @xmath46 , if for @xmath47 we have @xmath48 and for @xmath49 we have @xmath50 .",
    "we say @xmath51 is @xmath0-singular if @xmath51 is singular and @xmath52 .",
    "[ sec : sampling ] given a partition @xmath41 and core @xmath29 , we construct a rescaled core kernel @xmath53 with entries @xmath54 .",
    "we then use this smaller matrix @xmath30 and its eigendecomposition as an input to our two - stage sampling procedure in algorithm  [ smplalgo ] , which we refer to as coredpp .",
    "the two stages are : ( i ) sample a @xmath0-subset from @xmath29 according to @xmath55 ; and ( ii ) for each @xmath56 , pick an element @xmath33 uniformly at random .",
    "this algorithm uses only the much smaller matrix @xmath30 and samples a subset from @xmath9 in @xmath5 time .",
    "when @xmath6 and we want many samples , it translates into a notable improvement over the @xmath4 time of sampling directly from @xmath57 .",
    "the following lemma shows that coredppis equivalent to sampling from a @xmath0-dppwhere we replace each point in @xmath9 by its corresponding core point , and sample with the resulting induced kernel @xmath58 .",
    "[ lem : lemma_smpl.main ] coredppis equivalent to sampling from @xmath59 , where in @xmath58 we replace each element in @xmath60 by @xmath56 , for all @xmath61 .",
    "we denote the distribution induced by  algo .",
    "[ smplalgo ] by @xmath62 and that induced by @xmath59 by @xmath63 .",
    "first we claim that both sampling algorithms can only sample @xmath0-singular subsets . by construction",
    ", @xmath62 picks one or zero elements from each @xmath64 . for @xmath63 , if @xmath51 is @xmath0-nonsingular , then there would be identical rows in @xmath65 , resulting in @xmath66 .",
    "hence both @xmath62 and @xmath63 only assign nonzero probability to @xmath0-singular sets @xmath51 . as a result",
    ", we have @xmath67 for any @xmath68 that is @xmath0-singular , we have @xmath69 which shows that these two distributions are identical , i.e. , sampling from @xmath55 followed by uniform sampling is equivalent to directly sampling from @xmath59 .",
    "* input : * core kernel @xmath53 and its eigendecomposition ; partition @xmath41 ; size @xmath0 sample @xmath70 sample @xmath71 for @xmath72 @xmath73",
    "let us provide some insight on quantities that affect the distance @xmath74 when sampling with  algo .",
    "[ smplalgo ] . in a nutshell , this distance depends on three key quantities ( defined below ) : the probability of nonsingularity @xmath75 , the distortion factor @xmath76 , and the normalization factor .    for a partition @xmath41",
    "we define the _ nonsingularity probability _ @xmath75 as the probability that a draw @xmath77 is not singular with respect to  any @xmath46 .    given a coreset @xmath29",
    ", we define the _ distortion factor _ @xmath76 ( for @xmath78 ) as a partition - dependent quantity , so that for any @xmath61 , for all @xmath79 , and for any @xmath80-singular set @xmath81 with respect to @xmath82 the following bound holds : @xmath83 if @xmath84 is the feature map corresponding to the kernel @xmath12 , then geometrically , the numerator of   is the length of the projection of @xmath85 onto the orthogonal complement of @xmath86 .",
    "the _ normalization factor _ for a @xmath0-dpp(@xmath12 ) is simply @xmath20 .",
    "given @xmath41 , @xmath29 and the corresponding nonsingularity probability and distortion factors , we have the following bound :    [ lem : epsk.main ] let @xmath87 and @xmath88 be the set where we replace each @xmath89 by its core @xmath90 , i.e. , @xmath91 . with probability @xmath92 , it holds that @xmath93    let @xmath61 and consider any @xmath80-singular set @xmath81 with respect to @xmath82 . then , for any @xmath94 , using schur complements and by the definition of @xmath95 we see that @xmath96 here , @xmath97 is the projection onto the orthogonal complement of @xmath86 , and @xmath84 the feature map corresponding to the kernel @xmath12 .    with a minor abuse of notation ,",
    "we denote by @xmath98 the core point corresponding to @xmath34 , i.e. , @xmath91 . for any @xmath99",
    ", we then define the sets @xmath100 , where we gradually replace each point by its core point , with @xmath101 .",
    "if @xmath51 is @xmath0-singular , then @xmath102 whenever @xmath103 , and , for any @xmath104 , it holds that @xmath105 hence we have @xmath106 this bound holds when @xmath51 is @xmath0-singular , and , by definition of @xmath75 , this happens with probability @xmath92 .",
    "assuming @xmath95 is small , lemma  [ lem : epsk.main ] states that if replacing a single element in a given subset with another one in the same part does not cause much distortion , then replacing all elements in the subset with their corresponding cores will cause little distortion .",
    "this observation is key to our approximation : if we can construct such a partition and coreset , we can safely replace all elements with core points and then approximately sample with little distortion .",
    "more precisely , we then obtain the following result that bounds the variational error .",
    "our subsequent construction aims to minimize this bound .",
    "[ thm : varbound.main ] let @xmath107 and let @xmath62 be the distribution induced by  algo .  [ smplalgo ] . with the normalization factors",
    "@xmath108 and @xmath109 , the total variation distance between @xmath25 and @xmath62 is bounded by @xmath110    from the definition of @xmath111 and @xmath112 we know that @xmath113 and @xmath114 the last equality follows since , as argued above , @xmath66 for nonsingular @xmath51 .",
    "it follows that @xmath115 for the first term , we have @xmath116 where the first inequality uses the triangle inequality and the second inequality relies on lemma  [ lem : epsk.main ] . for the second term in",
    ", we use that , by definition of @xmath75 , @xmath117 thus the total variation difference is bounded as @xmath118    in essence , if the probability of nonsingularity and the distortion factor are low , then it is possible to obtain a good coreset approximation .",
    "this holds , for example , if the data has intrinsic ( grouping ) structure . in the next subsection we provide further intuition on when we can achieve low error .",
    "theorem  [ thm : varbound.main ] depends on the data and the partition @xmath41 . here",
    ", we aim to obtain some further intuition on the properties of @xmath41 that govern the bound . at the same time , these properties suggest sufficient conditions for a `` good '' coreset @xmath29 . for each @xmath60",
    ", we define the diameter @xmath119 next , define the minimum distance of any point @xmath120 to the subspace spanned by the feature vectors of points in a `` complementary '' set @xmath81 that is singular with respect to  @xmath121 : @xmath122 lemma  [ lem : boundeps.main ] connects these quantities with @xmath95 ; it essentially poses a separability condition on @xmath41 ( i.e. , @xmath41 needs to be `` aligned '' with the data ) so that the bound on @xmath95 holds .",
    "[ lem : boundeps.main ] if @xmath123 for all @xmath61 , then @xmath124    for any @xmath90 and any @xmath125 and @xmath81 @xmath80-singular with respect to @xmath126 , we have @xmath127 without loss of generality , we assume @xmath128 . by definition of @xmath129 we know that @xmath130 since @xmath131 by assumption , we have @xmath132 then , by definition of @xmath95 , we have @xmath133 from which it follows that @xmath134",
    "[ thm : varbound.main ] states an upper bound on the error induced by coredppand relates the total variation distance to @xmath41 and @xmath29 .",
    "next , we explore how to efficiently construct @xmath41 and @xmath29 that approximately minimize the upper bound .",
    "any set @xmath51 sampled via coredppis , by construction , singular with respect to @xmath41 . in other words , coredppassigns zero mass to any nonsingular set .",
    "hence , we wish to construct a partition @xmath41 such that its nonsingular sets have low probability under @xmath57 .",
    "the optimal such partition minimizes the probability @xmath75 of nonsingularity .",
    "a small @xmath75 value also means that the parts of @xmath41 are dense and compact , i.e. , the diameter @xmath129 in equation   is small .",
    "finding such a partition optimally is hard , so we resort to local search . starting with a current partition @xmath41",
    ", we re - assign each @xmath34 to a part @xmath60 to minimize @xmath75 . if we assign @xmath34 to @xmath60 , then the probability of sampling a set @xmath51 that is singular with respect to the new partition @xmath41 is @xmath135\\ , = \\ , {",
    "1\\over z}\\sum_{y \\text{$k$-singular}}\\det(l_y)\\\\    & = { 1\\over z } \\big(\\sum_{y \\text{$k$-sing . } , y\\notin y } \\det(l_y ) + \\sum_{y\\text{$k$-sing . } , y\\in y } \\det(l_y)\\big)\\\\    & = { 1\\over z}\\big(\\mathrm{const } + \\sum_{y'\\,(k-1)\\text{-sing .",
    "w.r.t $ \\pi\\setminus{\\mathcal{y}}_c$ } } \\det(l_{y'\\cup\\{y\\}})\\big)\\\\    & = { 1\\over z}\\big(\\mathrm{const } + l_{yy } s_{k-1}^\\pi(l_{{\\backprime } c}^{y})\\big),\\end{aligned}\\ ] ] where @xmath136 .",
    "the matrix @xmath137 denotes @xmath12 with rows @xmath60 and columns @xmath60 deleted , and @xmath138 . for local search",
    ", we would hence compute @xmath139 for each point @xmath34 and core @xmath56 , assign @xmath34 to the highest - scoring @xmath56 . since this testing is still expensive , we introduce further speedups in section  [ sec : approx ] .      when constructing @xmath29 , we aim to minimize the upper bound on the total variation distance between @xmath25 and @xmath62 stated in theorem  [ thm : varbound.main ] .",
    "since @xmath75 and @xmath95 only depend on @xmath41 and not on @xmath29 , we here focus on minimizing @xmath140 , i.e. , bringing @xmath141 as close to @xmath111 as possible .",
    "to do so , we again employ local search and subsequently swap each @xmath61 with its best replacement @xmath94 .",
    "let @xmath142 be @xmath29 with @xmath56 replaced by @xmath143 .",
    "we aim to find the best swap @xmath144 computing @xmath111 requires computing the coefficients @xmath20 , which takes a total of @xmath145 time time  @xcite , but the eigendecompositions and dynamic programming used in practice typically take cubic time . ] . in the next section ,",
    "we therefore consider a fast approximation .",
    "local search procedures for optimizing @xmath41 and @xmath29 can be further accelerated by a sequence of relaxations that we found to work well in practice ( see section  [ sec : exp ] ) .",
    "we begin with the quantity @xmath146 that involves summing over sub - determinants of the large matrix @xmath12 . assuming the initialization is not too bad",
    ", we can use the current @xmath29 to approximate @xmath9 .",
    "in particular , when re - assigning @xmath34 , we substitute all other elements with their corresponding cores , resulting in the kernel @xmath147 .",
    "this changes our objective to finding the @xmath61 that maximizes @xmath148 .",
    "key to a fast approximation is now lemma  [ lem : approx.main ] , which follows from lemma  [ lem : lemma_smpl.main ] .",
    "[ lem : approx.main ] for all @xmath149 , it holds that @xmath150    @xmath151    the last equality was shown in the proof of  thm .",
    "[ thm : varbound.main ] .",
    "computing the normalizer",
    "@xmath152 only needs @xmath153 time . we refer to this acceleration as coredpp - z .",
    "second , when constructing @xmath29 , we observed that @xmath141 is commonly much smaller than @xmath111 .",
    "hence , a fast approximation merely greedily increases @xmath141 without computing @xmath111 .",
    "third , we can be lazy in a number of updates : for example , we only consider changing cores for the part that changes .",
    "when a part @xmath60 receives a new member , we check whether to switch the current core @xmath56 to the new member .",
    "this reduction keeps the core adjustment at time @xmath153 .",
    "moreover , when re - assigning an element @xmath34 to a different part @xmath60 , it is usually sufficient to only check a few , say , @xmath154 parts with cores closest to @xmath34 , and not all parts .",
    "the resulting time complexity for each element is @xmath153 .",
    "@xmath155 group in which @xmath34 lies currently : @xmath33 continue @xmath156groups of @xmath154 cores nearest to @xmath157 @xmath158 @xmath159 @xmath160 @xmath161 @xmath162 @xmath163    with this collection of speedups , the approximate construction of @xmath41 and @xmath29 takes @xmath164 for each iteration , which is linear in @xmath3 , and hence a huge speedup over direct methods that require @xmath145 preprocessing .",
    "the iterative algorithm is shown in algorithm [ dcsalgo ] .",
    "the initialization also affects the algorithm performance , and in practice we find that kmeans++ as an initialization works well .",
    "thus we use coredppto refer to the algorithm that is initialized with kmeans++ and uses all the above accelerations . in practice",
    ", the algorithm converges very quickly , and most of the progress occurs in the first pass through the data . hence ,",
    "if desired , one can even use early stopping .",
    "we next evaluate coredpp , and compare its efficiency and effectiveness against three competing approaches :    -=1.2em    partitioning using @xmath0-means ( with kmeans++ initialization  @xcite ) , with @xmath29 chosen as the centers of the clusters ; referred to as _",
    "k++ _ in the results .    the adaptive , stochastic nystrm sampler of  @xcite ( _ nysstoch _ ) .",
    "we used @xmath7 dimensions for nysstoch , to use the same dimensionality as coredpp .",
    "the metropolis - hastings dpp sampler _ mcdpp _  @xcite .",
    "we use the well - known gelman and rubin multiple sequence diagnostic  @xcite to empirically judge mixing .",
    "in addition , we show results using different variants of coredpp : coredpp - zdescribed in  sec .  [ sec : approx ] and variants that are initialized either randomly  ( coredpp - r ) or via kmeans++  ( coredpp ) .",
    "we first explore the effect of our fast approximate sampling on controllable synthetic data .",
    "the experiments here compare the accuracy of the faster coredppfrom section  [ sec : approx ] to coredpp - z , coredpp - rand _",
    "we generate an equal number of samples from each of _ nclust _ 30-dimensional gaussians with means of varying length ( @xmath165-norm ) and unit variance , and then rescale the samples to have the same length .",
    "as the length of the samples increases , @xmath95 and @xmath75 shrink . finally , @xmath12 is a linear kernel . throughout this experiment we set @xmath166 and @xmath167 to be able to exactly compute @xmath168 .",
    "we extract @xmath169 core points and use @xmath170 neighboring cores .",
    "recall from  sec .",
    "[ sec : approx ] that when considering the parts that one element should be assigned to , it is usually sufficient to only check @xmath154 parts with cores closest to @xmath34 .",
    "thus , @xmath170 means we only consider re - assigning each element to its three closest parts .    [",
    "[ results . ] ] results . + + + + + + + +    -norm . ]    fig .",
    "[ fig : syn ] shows the total variation distance @xmath168 defined in equation   for the partition and cores generated by _ k++ _ , coredpp , coredpp - rand coredpp - zas _ nclust _ and the length vary .",
    "we see that in general , most approximations improve as @xmath95 and @xmath75 shrink .",
    "remarkably , the coredppvariants achieve much lower error than _",
    "k++_. moreover , the results suggest that the relaxations from section  [ sec : approx ] do not noticeably increase the error in practice .",
    "also , coredpp - rperforms comparable with coredpp , indicating that our algorithm is robust against initialization . since , in addition , the coredppconstruction makes most progress in the first pass through the data , and the kmeans++ initialization yields the best performance , we use only one pass of coredppinitialized with kmeans++ in the subsequent experiments .",
    "we apply coredppto two larger real data sets :    1 .",
    "mnist  @xcite .",
    "mnist consists of images of hand - written digits , each of dimension @xmath171 .",
    "2 .   genes  @xcite .",
    "this dataset consists of different genes .",
    "each sample in genes corresponds to a gene , and the features are shortest path distances to 330 different hubs in the biogrid gene interaction network .    for our first set of experiments on both datasets , we use a subset of 2000 data points and an rbf kernel to construct @xmath12 . to evaluate the effect of model parameters on performance , we vary @xmath7 from 20 to 100 and @xmath0 from 2 to 8 and fix @xmath172 ( see section  [ sec : synth ] for an explanation of the parameters ) .",
    "larger - scale experiments on these datasets are reported in section  [ sec : ls ] .    [ [ performance - measure - and - results . ] ] performance measure and results .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +     varying from 20 to 100 and fixed @xmath173.,scaledwidth=80.0% ]     varying from 8 to 2 and fixed @xmath174.,scaledwidth=80.0% ]    on these larger data sets , it becomes impossible to compute the total variation distance exactly . we therefore approximate it by uniform sampling and computing an empirical estimate",
    ".    the results in figure  [ real_perform ] and figure  [ real_perform_k ] indicate that the approximations improve as the number of parts @xmath7 increases and @xmath0 decreases .",
    "this is because increasing @xmath7 increases the models approximation power , and decreasing @xmath0 leads to a simpler target probability distribution to approximate . in general , coredppalways achieves lower error than _",
    "k++ _ , and _ nysstoch _ performs poorly in terms of total variation distance to the original distribution .",
    "this phenomenon is perhaps not so surprising when recalling that the nystrm approximation minimizes a different type of error , a distance between the kernel _",
    "matrices_. these observations suggest to be careful when using matrix approximations to approximate @xmath12",
    ".    for an intuitive illustration , figure  [ mnist_showcase ] shows a core @xmath29 constructed by coredpp , and the elements of one part @xmath175 .     of size 40 , each figure is a core in the coreset constructed by our algorithm ; ( b , c ) two different parts corresponding to the first and second core . ]",
    "lastly , we address running times for coredpp , _ nysstoch _ and the markov chain @xmath0-dpp  ( _ mcdpp _  @xcite ) . for the latter ,",
    "we evaluate convergence via the gelman and rubin multiple sequence diagnostic  @xcite ; we run 10 chains simultaneously and use the _ coda _  @xcite package to calculate the potential scale reduction factor  ( psrf ) , and set the number of iterations to the point when psrf drops below 1.1 .",
    "finally we run _ mcdpp _",
    "again for this specific number of iterations .    for overhead time ,",
    "i.e. , time to set up the sampler that is spent once in the beginning , we compare against _ nysstoch _ : coredppconstructs the partition and @xmath30 , while _ nysstoch _ selects landmarks and constructs an approximation to the data . for sampling time , we compare against both _",
    "nysstoch _ and _ mcdpp _ : coredppuses algo .",
    "[ smplalgo ] , and _ nysstoch _ uses the dual form of @xmath0-dppsampling  @xcite .",
    "we did not include the time for convergence diagnostics into the running time of _ mcdpp _ , giving it an advantage in terms of running time .    ) on mnist  ( left ) and genes  ( right).,title=\"fig:\",scaledwidth=37.0% ] ) on mnist  ( left ) and genes  ( right).,title=\"fig:\",scaledwidth=37.0% ]    [ [ overhead . ] ] overhead .",
    "+ + + + + + + + +    fig .",
    "[ time_init ] shows the overhead times as a function of @xmath3 . for mnist",
    "we vary @xmath3 from 6,000 to 20,000 and for genes we vary @xmath3 from 6,000 to 10,000 .",
    "these values of @xmath3 are already quite large , given that the dppkernel is a dense rbf kernel matrix ; this leads to increased running time for all compared methods .",
    "the construction time for _ nysstoch _ and coredppis comparable for small - sized data , but _ nysstoch _ quickly becomes less competitive as the data gets larger .",
    "the construction time for coredppis linear in @xmath3 , with a mild slope . if multiple samples are sought",
    ", this construction can be performed offline as preprocessing as it is needed only once .",
    "[ [ sampling . ] ] sampling .",
    "+ + + + + + + + +    fig .",
    "[ time_smpl ] shows the time to draw one sample as a function of @xmath3 , comparing coredppagainst _",
    "nysstoch _ and _ mcdpp_. coredppyields samples in time independent of @xmath3 and is extremely efficient  it is orders of magnitude faster than _ nysstoch _ and _",
    "mcdpp_.    ) varies on mnist  ( left ) and genes  ( right ) .",
    "note that the time axis is shown in log scale.,title=\"fig:\",scaledwidth=38.0% ] ) varies on mnist  ( left ) and genes  ( right ) .",
    "note that the time axis is shown in log scale.,title=\"fig:\",scaledwidth=38.0% ]    we also consider the time taken to sample a large number of subsets , and compare against both _",
    "nysstoch _ and _ mcdpp_the sampling times for drawing approximately independent samples with _ mcdpp _ add up .",
    "[ time_tot ] shows the results . as more samples",
    "are required , coredppbecomes increasingly efficient relative to the other methods .    , @xmath176 and @xmath177 on mnist  ( left ) and genes  ( right ) .",
    ", title=\"fig:\",scaledwidth=38.0% ] , @xmath176 and @xmath177 on mnist  ( left ) and genes  ( right ) .",
    ", title=\"fig:\",scaledwidth=38.0% ]",
    "in this paper , we proposed a fast , two - stage sampling method for sampling diverse subsets with @xmath0-dpps . as opposed to other approaches , our algorithm directly aims at minimizing the total variation distance between the approximate and original probability distributions .",
    "our experiments demonstrate the effectiveness and efficiency of our approach : not only does our construction have lower error in total variation distance compared with other methods , it also produces these more accurate samples efficiently , at comparable or faster speed than other methods .",
    "d.  feldman , m.  schmidt , and c.  sohler .",
    "turning big data into tiny data : constant - size coresets for k - means , pca and projective clustering . in _",
    "siam - acm symposium on discrete algorithms ( soda ) _ , pages 14341453 , 2013 .",
    "a.  krause , a.  singh , and c.  guestrin .",
    "near - optimal sensor placements in gaussian processes : theory , efficient algorithms and empirical studies .",
    "_ journal of machine learning research _ , 9:0 235284 , 2008 .",
    "t.  zhou , z.  kuscsik , j .-",
    "liu , m.  medo , j.  r. wakeling , and y .- c .",
    "zhang . solving the apparent diversity - accuracy dilemma of recommender systems .",
    "_ proceedings of the national academy of sciences _ , 1070 ( 10):0 45114515 , 2010 ."
  ],
  "abstract_text": [
    "<S> determinantal point processes ( dpps ) are elegant probabilistic models of repulsion and diversity over discrete sets of items . but their applicability to large sets is hindered by expensive cubic - complexity matrix operations for basic tasks such as sampling . in light of this , we propose a new method for approximate sampling from discrete @xmath0-dpps . </S>",
    "<S> our method takes advantage of the diversity property of subsets sampled from a dpp , and proceeds in two stages : first it constructs coresets for the ground set of items ; thereafter , it efficiently samples subsets based on the constructed coresets . as opposed to previous approaches , our algorithm aims to minimize the total variation distance to the original distribution . </S>",
    "<S> experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets , and yields more accurate samples than previous approaches . </S>"
  ]
}