{
  "article_text": [
    "in breast examinations , such as mammography , detected actionable tumors are further examined through invasive histology .",
    "objective interpretation of these modalities is fraught with high inter - observer variability and limited reproducibility  @xcite . in this context , a reference based assessment , such as presenting prior cases with similar disease manifestations ( termed content based image retrieval ( cbir ) ) could be used to circumvent discrepancies in cancer grading . with growing sizes of clinical databases , such a cbir system ought to be both scalable and accurate . towards this",
    ", hashing approaches for cbir are being actively investigated for representing images as compact binary codes that can be used for fast and accurate retrieval  @xcite .",
    "malignant carcinomas are often co - located with benign looking manifestations and suspect normal tissues . in such cases , describing the whole image with a single label is often inadequate for objective machine learning and alternatively requires expert annotations delineating the exact location of the region of interest .",
    "this argument extends to screening modalities like mammograms , where multiple anatomical views are acquired . in such scenarios ,",
    "the status of the tumor is best represented to a cbir system by constituting a bag of all associated images , thus veritably becoming multiple instance ( mi ) in nature .",
    "this is illustrated in fig .",
    "[ fig : examplebag ] . with this as our premise we present , for the first time , a novel deep learning based mi hashing method , termed as robust multiple instance hashing ( rmih ) .",
    "6k @xmath0 4k ) labeled as _ malignant _ is shown wherein a few patches overlapping with the actual tumor represent the underlying malignant concept while proximal patches are potentially benign or less discriminative connective / lipidic tissues .",
    "it must be noted that these are not individually identified and only a bag - level weak annotation is available for learning . ]",
    "seminal works on shallow learning - based hashing include iterative quantization ( itq )  @xcite , kernel sensitive hashing ( ksh )  @xcite _ etc . _",
    "that propose a two - stage framework involving extraction of hand - crafted features followed by binarization .",
    "_ extend these methods to mi learning scenarios with two variants : instance level mi hashing ( imih ) and bag level mi hashing ( bmih )  @xcite . however , these approaches are not end - to - end and are susceptible to semantic gap between features and associated concepts .",
    "alternatively , deep hashing methods such as simultaneous feature learning and hashing ( sflh )  @xcite , deep hashing networks ( dhn )  @xcite and deep residual hashing ( drh )  @xcite to name a few , propose the learning of representations and hash codes in an end - to - end fashion , in effect bridging this semantic gap .",
    "it must be noted that all the above deep hashing works targeted single instance ( si ) hashing scenarios and an extension to mi hashing was not investigated .",
    "earlier works on mi deep learning in computer vision include work by wu _",
    "et al . _",
    "@xcite , where the concept of an mi pooling ( mipool ) layer is introduced to aggregate representations for multi - label classification .",
    "leveraged mi deep learning for efficient body part recognition  @xcite . unlike mi classification",
    "that potentially substitutes the decision of the clinician , retrieval aims at presenting them with richer contextual information similar to the case at hand to facilitate decision - making .",
    "rmih effectively bridges the two concepts for cbir systems by combining the representation learning strength of deep mi learning with the potential for scalability arising from hashing . within cbir for breast cancer ,",
    "notable prior art includes work on mammogram image retrieval by jiang _",
    "_  @xcite and large - scale histology retrieval by zhang _ et al . _",
    "both these works pose cbir as an si retrieval problem .",
    "contrasting with  @xcite and  @xcite , within rmih we create a bag of images to represent a particular pathological case and generate a bag - level hash code , as shown in fig .",
    "[ fig : overview ] .",
    "our contributions in this paper include : * 1 ) * introduction of a robust supervised retrieval loss for learning in presence of weak labels and potential outliers ; * 2 ) * propose training with an auxiliary si arm with gradual loss trade - off for improved trainability ; and * 3 ) * incorporation of the mipool layer to aggregate representations across variable number of instances within a bag , generating bag - level discriminative hash codes .",
    "r0.39     lets consider database @xmath1 with @xmath2 bags . each bag , @xmath3 , with varying number ( @xmath4 ) of instances ( @xmath5 ) is denoted as @xmath6 .",
    "we aim at learning @xmath7 that maps each bag to a @xmath8-d hamming space @xmath9 , such that _ bags with similar instances and labels are mapped to similar codes_. for supervised learning of @xmath7 , we define a bag - level pairwise similarity matrix @xmath10 , such that @xmath11 if the bags are similar and zero otherwise . in applications , such as this one , where retrieval ground truth is unavailable we can use classification labels as a surrogate for generating @xmath12 .    * architecture * : as shown in fig .",
    "[ fig : architecture ] , the proposed rmih framework consists of a deep cnn terminating in a fully connected layer ( fcl ) .",
    "its outputs @xmath13 are fed into the mipool layer to generate the aggregated representation @xmath14 that is pooled ( @xmath15 , mean@xmath16 , _ etc .",
    "_ ) across instances within the bag .",
    "@xmath14 is an embedding in the space of the bags and is the input of a fully connected mi hashing layer .",
    "the output of this layer is squashed to @xmath17 $ ] by passing it through a tanh@xmath18 function to generate @xmath19 , which is quantized to produce bag - level hash codes as @xmath20 .",
    "the deep cnn mentioned earlier could be a pretrained network , such as vggf @xcite , googlenet  @xcite , resnet50 ( r50 )  @xcite or an application specific network .    during training of rmih",
    ", we introduce an auxiliary si hashing ( aux - si ) arm , as shown in fig .",
    "[ fig : architecture ] .",
    "it taps off at the fcl layer and feeds directly into a fully connected si hashing layer with tanh@xmath18 activation to generate instance level non - quantized hash codes , denoted as @xmath21 . while training rmih using backpropagation , the mipool layer significantly sparsifies the gradients ( analogous to using very high dropout while training cnns ) , thus limiting the trainability of the preceding layers .",
    "the si hashing arm helps to potentially mitigate this by producing auxiliary instance level gradients . *",
    "model learning and robust optimization * : to learn similarity preserving hash codes , we propose a robust version of supervised retrieval loss based on neighborhood component analysis employed by  @xcite .",
    "the motivation to introduce robustness within the loss function is two - fold : ( 1 ) robustness induces immunity to potentially noisy labels due to high inter - observer variability and limited reproducibility for the applications at hand  @xcite ; ( 2 ) it can effectively counter ambiguous label assignment while training with the aux - si hashing arm .    given @xmath12 ,",
    "the robust supervised retrieval loss @xmath22 is defined as :    @xmath23    where @xmath24 is the probability that two bags ( indexed as @xmath25 and @xmath26 ) are neighbors .",
    "given hash codes @xmath27 and @xmath28 , we define a bit - wise residual operation @xmath29 as @xmath30 .",
    "we estimate @xmath24 as : @xmath31 the huber norm s robustness operation @xmath32 is defined as : @xmath33 in eq .",
    ", the tuning factor @xmath34 is estimated inherently from the data and is set to @xmath35 .",
    "the factor of @xmath36 is chosen to provide approximately 95% asymptotic efficiency and @xmath37 is a robust measure of bit - wise variance of @xmath38 .",
    "specifically , @xmath39 is estimated as @xmath40 times the median absolute deviation of @xmath38 as empirically suggested in  @xcite .",
    "this robust formulation provides immunity to outliers during training by clipping their gradients . for training with the aux - si hashing arm",
    ", we employ a similar robust retrieval loss @xmath41 defined over single instances with bag - labels assigned to member instances .",
    "to minimize loss of retrieval quality due to quantization , we use a differentiable quantization loss @xmath42 proposed in  @xcite .",
    "this loss also counters the effect of using continuous relaxation in definition of @xmath24 over using hamming distance . as a standard practice in deep learning",
    ", we also add an additional weight decay regularization term @xmath43 , which is the frobenius norm of the weights and biases , to regularize the cost function and avoid over - fitting .",
    "r[0pt]0pt    the following composite loss is used to train rmih : @xmath44 where @xmath45 , @xmath46 , @xmath47 and @xmath48 are hyper - parameters that control the contribution of each of the loss terms .",
    "specifically , @xmath45 and @xmath46 control the trade - off between the mi and si hashing losses .",
    "the si arm plays a significant role only in the early stages of training and can be traded off eventually to avoid sub - optimal mi hashing . for this",
    "we introduce a weight trade - off formulation that gradually down - regulates @xmath46 , while simultaneously up - regulating @xmath45 . here , we use @xmath49 and @xmath50 , where @xmath51 is the current epoch and @xmath52 is the maximum number of epochs ( see fig .",
    "[ wrap - weights ] ) .",
    "we train rmih with mini - batch stochastic gradient descent ( sgd ) with momentum .",
    "due to potential outliers that can occur at the beginning of training , we scale @xmath53 up by a factor of 7 for @xmath54 to allow a stable state to be reached . specifically , the gradient of @xmath55 w.r.t . to @xmath56",
    "is derived as : @xmath57 where @xmath58 .",
    "the derivative of the huber term @xmath59 can be computed as : @xmath60    regarding the quantization loss function , the derivative can be computed by @xmath61 .",
    "having computed these gradients , we use backpropagation to compute the derivatives of the preceding layers .",
    "[ fig : randomimages ]",
    "* databases : * clinical applicability of rmih has been validated on two large scale datasets , namely , digital database for screening mammography ( ddsm )  @xcite and a retrospectively acquired histology dataset from the indiana university health pathology lab ( _ iuphl _ )  @xcite .",
    "the _ ddsm _",
    "dataset comprises of 11,617 expert selected regions of interest ( roi ) curated from 1861 patients .",
    "multiple rois associated with a single breast from anatomical views constitute a bag ( size : 1 - 12 ; median : 2 ) , which has been annotated as normal , benign or malignant by expert radiologists .",
    "a bag labeled _ malignant _ could potentially contain multiple suspect normal and benign masses , which have not been individually identified .",
    "the _ iuphl _",
    "dataset is a collection of 653 rois from histology slides from 40 patients ( 20 with precancerous ductal hyperplasia ( udh ) and rest with ductal carcinoma _ in situ _ ( dcis ) ) with roi level annotations done by expert histopathologists . due to high variability in sizes of these rois ( upto 9k @xmath0 8k pixels )",
    ", we extract multiple patches ( of size @xmath62 ) and populate a roi - level bag ( size : 1 - 15 ; median : 8) . as cellular and nuclei level characteristics are important to distinguishing dcis from udh , it is not recommended to rescale these images to standard input sizes used by cnns ( typically , 244 @xmath0 224 in  @xcite ) .",
    "[ fig : randomimages ] illustrates select images from the two datasets to showcase anatomical variability within and across the constituent classes . from both the datasets",
    ", we use patient - level non - overlapping splits to constitute the training ( 80% ) and testing ( 20% ) sets .    * model settings and validations * : to validate proposed contributions , namely robustness within nca loss and trade - off from the aux - si arm , we perform ablative testing with combinations of their baseline variants by fine - tuning multiple network architectures . additionally , we compare rmih against four state - of - the art methods : itq  @xcite , ksh  @xcite , sflh  @xcite and dhn  @xcite . for a fair comparison , we use r50 for both sflh and dhn , since as discussed later it performs the best .",
    "since sflh and dhn were originally proposed for si hashing , we introduce additional mi variants by hashing through the mipool layer . for itq and ksh ,",
    "we further create two comparative settings : * 1 ) * using imih  @xcite that learns instance - level hash codes followed by bag - level distance computation and * 2 ) * utilizing bmih  @xcite using bag - level kernalized representations followed by binarization . for imih and si variants of sflh , dhn and",
    "rmih , given two bags @xmath63 and @xmath64 with si hash codes , say @xmath65 and @xmath66 , the bag - level distance is computed as : @xmath67    all images were resized to @xmath68 and training data were augmented to create equally balanced classes . @xmath45 and",
    "@xmath46 were set assuming @xmath52 as 150 epoch ; @xmath47 and @xmath48 were set at 0.05 and 0.001 respectively .",
    "the momentum term within sgd was set to 0.9 and batch size to 128 for _ ddsm _ and 32 for _",
    "iuphl_. for efficient learning , we use an exponentially decaying learning rate initialized at 0.01 .",
    "the rmih framework was implemented in matconvnet  @xcite .",
    "we use standard retrieval quality metrics : nearest neighbor classification accuracy ( nnca ) and precision - recall ( pr ) curves to perform the aforementioned comparisons .",
    "the results ( nnca ) from ablative testing and comparative methods are tabulated in table  [ wrap - tab : ablative ] and table  [ table : cm ] respectively . within table  [ table : cm ] , methods were evaluated at two different code sizes ( 16 bits and 32 bits ) .",
    "we also present the pr curves of select bag - level methods ( 32 bits ) in fig .",
    "[ fig : prcurves ] .",
    "* effect of aux - si loss * : to justify using the aux - si loss , we introduce a variant of rmih without it ( e in table  [ wrap - tab : ablative ] ) , which leads to a significant decline of 3% to 14% in contrast to rmih .",
    "this could be potentially attributed to the prevention of the gradient sparsification caused by the mipool layer . from table  [ wrap - tab :",
    "ablative ] , we observe a 3%-10% increase in performance , comparing cases with gradual decaying trade - off ( b ) against baseline setting ( @xmath69 , a , c ) .    *",
    "effect of robustness * : for robust - nca , we compared against the original nca formulation proposed in  @xcite ( a , b , d in table  [ wrap - tab : ablative ] ) .",
    "robustness helps handle potentially noisy mi labels , inconsistencies within a bag ( like non - informative patches ) and the ambiguity in assigning si labels .",
    "comparing the effect of robustness for baselines sans the si hashing arm ( d _ vs. _ e ) we observe marginally positive improvement across the architectures and datasets , with a substantial 7% in resnet50 for _ ddsm_. robustness contributes more with the addition of the aux - si hash arm ( proposed _ vs. _ e ) with improved performance in the range of 4%-5% across all settings .",
    "this observation further validates our prior argument .",
    "* effect of quantization * : to assess the effect of quantization , we define two baselines : ( 1 ) setting @xmath70 and ( 2 ) using non - quantized hash codes for retrieval ( rmih - nb ) .",
    "the latter potentially acts as an upper bound for performance evaluation . from table  [ wrap - tab :",
    "ablative ] , we observe a consistent increase in performance by margins of 3%-5% if rmih is learnt with an explicit quantization loss to limit the associated error .",
    "it must also be noted that comparing with rmih - nb , there is only a marginal fall in performance ( 2%-4% ) , which is desired .    comparing max _ vs. _",
    "mean mi pool variants , we observe that max achieves marginally better performance , since it is more selective than mean , which is particularly important in cases of detecting malignancy .",
    "[ fig : prcurves ]    as a whole , the two - pronged proposed approach , including robustness and trade - off , along with quantization loss delivers the highest performance , proving that rmih is able to learn effectively , despite the ambiguity induced by the si hashing arm .",
    "[ fig : retrievedimages ] demonstrates the retrieval performance of rmih on the target databases . for _ iuphl _ , the retrieved images are semantically similar to the query as consistent anatomical signatures are evident in the retrieved neighbors . for _ ddsm _ , in the cancer and normal cases the retrieved neighbors are consistent , however it is hard to distinguish between benign and malignant .",
    "the retrieval time for a single query for rmih was observed at 31.62 ms ( for _ iuphl _ ) and 17.48 ms ( for _ ddsm _ ) showing potential for fast and scalable search .",
    "l6.0 cm    * comparative methods * in the contrastive experiments against itq and ksh , hand - crafted gist @xcite features underperformed significantly , while the improvement with the r50 features ranged from 5%-30% .",
    "however , rmih still performed 10%-25% better , proving that even if deep learnt features severely boost the performance , the shallow methods can not fully breach the gap to the deep ones . comparing the si with the mi variations of dhn , sflh and rmih , it is observed that the performance improved in the range of 3%-11% , suggesting that end - to - end learning of mi hash codes is preferred over two - stage hashing _",
    "i.e. _ hashing at si level and comparing at bag level with eq .  .",
    "however , rmih fares comparably better than both the si and mi versions of sflh and dhn , owing to the robustness of the proposed retrieval loss function to potentially noisy labels and inconsistent instances within bags ( also evident from pr curves in fig . [ fig : prcurves ] ) . in all fairness ,",
    "the concepts of training with aux - si hashing arm with gradual trade - off and robustness could be potentially adapted to sflh and dhn to improve their mi hashing performance . as also seen from the associated pr curves in fig .",
    "[ fig : prcurves ] , the performance gap between shallow and deep hashing methods remains significant despite using r50 features .",
    "comparative results strongly support our premise that end - to - end learning of mi hash codes is preferred over conventional two - stage approaches .",
    "in this paper , for the first time , we proposed an end - to - end deep robust hashing framework , termed rmih , for retrieval under a multiple instance setting .",
    "we incorporate the mipool layer to aggregate representations across instances to generate a bag - level discriminative hash code .",
    "we introduce the notion of robustness into our supervised retrieval loss and improve the trainability of rmih by utilizing an aux - si hashing arm regulated by a trade - off .",
    "extensive validations and ablative testing on two public breast cancer datasets demonstrate the superiority of rmih and its potential for future extension to other mi applications .",
    "duijm lem , louwman mwj , groenewoud jh , van de poll - franse lv , fracheboud j , coebergh jw .",
    "inter - observer variability in mammography screening and effect of type and number of readers on screening outcome . in bjc 2009 ,",
    "pp . 901907 .",
    "zhennan y , yiqiang z , zhigang p , shu l , shinagawa y , shaoting z , metaxas dn , xiang sz .",
    "multi - instance deep learning : discover discriminative local anatomies for bodypart recognition . in trans",
    "med imaging 2016 , pp .",
    "13321343 , ieee .",
    "heath m , bowyer k , kopans d , kegelmeyer jr wp , moore r , chang k , munishkumaran s. current status of the digital database for screening mammography . in digital mammography 1998 , pp .",
    "457460 , springer netherlands .",
    "badve s , bilgin g , dundar m , grcan mn , jain rk , raykar vc , sertel o. computerized classification of intraductal breast lesions using histopathological images . in biomed .",
    "engineering 2011 , vol .",
    "58 , pp . 19771984 , ieee ."
  ],
  "abstract_text": [
    "<S> in this paper , for the first time , we introduce a _ multiple instance _ ( mi ) deep hashing technique for learning discriminative hash codes with weak bag - level supervision suited for large - scale retrieval . </S>",
    "<S> we learn such hash codes by aggregating deeply learnt hierarchical representations across bag members through a dedicated mi pool layer . for better trainability and retrieval quality </S>",
    "<S> , we propose a two - pronged approach that includes robust optimization and training with an auxiliary single instance hashing arm which is down - regulated gradually . </S>",
    "<S> we pose retrieval for tumor assessment as an mi problem because tumors often coexist with benign masses and could exhibit complementary signatures when scanned from different anatomical views . </S>",
    "<S> experimental validations on benchmark mammography and histology datasets demonstrate improved retrieval performance over the state - of - the - art methods .    </S>",
    "<S> [ 1]>m#1 </S>"
  ]
}