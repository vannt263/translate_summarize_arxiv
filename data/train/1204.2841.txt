{
  "article_text": [
    "the basic task of regression analysis  the estimation of values of an unknown , sufficiently smooth function @xmath0 of one or several real arguments , given a finite amount of possibly noisy data",
    " occurs pervasively in many kinds of quantitative scientific research .",
    "most existing approaches to this task can roughly be classified into two groups : _ parametric _ or model - based approaches such as polynomial regression , spline smoothing @xcite , or kriging @xcite , and _ nonparametric _ approaches such as local regression , nearest or natural neighbour estimation @xcite , inverse distance weighting @xcite , or kernel smoothing .",
    "although parametric methods are usually based on more rigorous reasoning such as maximum likelihood estimation , bayesian updating , approximation theory , or some other kind of optimization , their results are only guaranteed to be reliable if the sought function is assumed to belong to some particular model or function class with a small number of parameters , e.g. , polynomials or splines of a fixed degree and fixed set or number of break points . if , as is often the case in empirical research , this strong assumption can not be justified , nonparametric methods are available which , however , are usually based on various forms of more heuristic reasoning , frequently involve the choice of several control parameters such as the number of neighbours or the choice of a weight or kernel function , and sometimes do not provide an easily interpreted assessment of reliability such as a standard error or confidence interval .",
    "also , many regression methods are not or only restrictedly applicable if one or several of the following conditions apply : ( i ) there might be measurement errors not only in the function values ( the `` dependent '' variable ) but also in the function arguments ( the `` independent '' variables ) , ( ii ) measurement errors might not be independent and identically distributed , ( iii ) the function is of more than one real argument , ( iv ) the measured sample is irregularly distributed in the function s argument space , and/or ( v ) also several derivatives of the function shall be estimated .    in order to illustrate how plausible estimates can depend on the assumed amount and correlation of measurement errors ,",
    "consider the minimal data @xmath1 , @xmath2 , and @xmath3 and assume that we want to estimate @xmath0 and @xmath4 on the interval @xmath5 $ ] on the basis of this data . in the left diagram in fig.[fig : hat3 ] , four different such estimates of @xmath0 are shown , which were produced with the method we will describe in this article , using different assumptions on the measurement error contained in the data . without measurement errors , i.e. , if both @xmath6 and @xmath7 data are precise , it is plausible to estimate @xmath8 and @xmath9 , as on the blue dotted line . without errors in @xmath6 but with large independent errors in @xmath7 ,",
    "the apparent slope becomes quite uncertain , and one would rather estimate @xmath10 by the sample mean , @xmath11 , as on the yellow line .",
    "however , if the errors in @xmath7 are highly correlated , e.g. , because of a systematic but unknown bias in the measurement equipment , then one knows that the data is basically only shifted in the @xmath7 direction , which has no influence on slopes , so one would probably expect @xmath12 again , as on the green dashed line .",
    "the smaller the errors , the more the slope should resemble @xmath13 , as on the cyan dash - dotted line .",
    "similarly , if the @xmath7 data is precise but the @xmath6 data is highly uncertain , one would use the sample estimate if errors are uncorrelated , as on the yellow line in the right diagram in fig.[fig : hat3 ] .",
    "if errors in @xmath6 are correlated , one would retain some slope information which suggests that the values to the left and right of the three data points are rather below than above three .",
    "hence the estimate would be lower than with uncorrelated errors , as on the green dashed line .",
    "the occurrence of this lowering effect also shows that the effect of errors in the @xmath6 dimension is usually not the same as the effect of some `` equivalent '' amount of errors in the @xmath7 dimension .",
    "( colour online ) illustration of the effect of error correlation on some motabar  estimates . ,",
    "title=\"fig:\",scaledwidth=49.0% ] ( colour online ) illustration of the effect of error correlation on some motabar  estimates .",
    ", title=\"fig:\",scaledwidth=49.0% ]      in this article , we develop a regression method which is nonparametric in the sense that it can estimate any sufficiently smooth function , but is still based on rigorous statistical reasoning , can deal with any of the above situations ( i)(v ) , and contains several existing methods as special or limiting cases .",
    "the method , _ moving taylor bayesian regression ( motabar ) _ , is based on the following ideas :    1 .",
    "approximate @xmath0 locally by a taylor polynomial at a moving position of interest @xmath14 .",
    "2 .   treat the unknown taylor coefficients as parameters in a statistical model .",
    "3 .   use the measured data to update prior beliefs about these parameters using bayesian updating .",
    "4 .   use the posterior mean ( or mode ) and variance of the parameters as estimates of the function s value and derivatives and the corresponding estimation uncertainty .    in principle",
    ", these steps can be performed for any form of error and prior distribution , but the method becomes particularly transparent if the value measurement error is assumed to be multivariate gaussian and the involved prior distributions are also multivariate gaussian . in that case , the resulting posterior distributions can be written as mixtures of gaussians whose mean and variance can be derived analytically , and the resulting motabar  estimator @xmath15 of @xmath16 turns out to be a mixture of smooth rational functions of @xmath14 that can be computed using simple linear algebra .",
    "the mixing is related to the distribution of argument measurement errors . without argument measurement errors ,",
    "@xmath15 is a smooth rational function of @xmath14 whose algebraic form can be seen as a generalization of ordinary least squares regression estimators . for these reasons , we will focus on the case of gaussian priors and value errors in this article .    the input data needed to apply motabar  are then",
    "* the measured data , * variances and possibly covariances of both argument and value measurement errors , * a prior covariance matrix for the value of @xmath0 and its derivatives of order @xmath17 for some @xmath18 , * prior variances for @xmath0 s derivatives of order @xmath19 , * and , as the only free control parameter , an integer @xmath19 larger than the order of any derivative of @xmath0 one wants to estimate .    for particular cases of error and prior distributions , it will turn out that the motabar  estimate equal or approximate the results of some well - known other methods , including ordinary polynomial regression , inverse distance weighting , and linear interpolation on regular grids .",
    "this behaviour of our estimator is similar to that of a related approach by @xcite in which @xmath0 is also approximated by a taylor polynomial at a moving position of interest @xmath14 , but in which the taylor coefficients are then estimated not by bayesian updating but by minimizing a heuristic loss function motivated by approximation theory ( see sec.[sec : wang ] ) .",
    "other cases of error and prior distributions result in plausible generalizations or variants of well - known methods , e.g. , a new form of local polynomial interpolation and a bayesian variant of inverse distance weighted smoothing .    in practise ,",
    "the needed prior and error variances and covariances might themselves be estimated from other or even the same data , a question we do however not address in detail in this article .",
    "[ [ comparison - with - other - methods . ] ] comparison with other methods .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    motabar  can be interpreted as a kind of _ local _ regression since although it takes into account also data points far away from @xmath14 , it gives them much less influence on the estimate than those close to @xmath14 .",
    "this fact is reflected in the occurrence of a _ weight matrix _ @xmath20 in the estimator equation . but unlike other local or piece - wise methods such as nearest or natural neighbour regression , locally weighted scatterplot smoothing ( loess ) @xcite , or splines , the motabar  estimate @xmath15 is infinitely smooth ( i.e. , infinitely often differentiable ) , at least as long as the value measurement errors have nonzero variance , and there are either no argument measurement errors or their probability density is infinitely smooth ( e.g. , when argument measurement errors are gaussian as well ) .",
    "this is because in motabar the weight of each individual data point in the estimate depends smoothly on its distance from @xmath14 , whereas in other methods the weights can switch from zero to nonzero in a nonsmooth way as @xmath14 moves .",
    "the nonsmooth change in weights that other methods involve is also counter - intuitive when arguments can only be measured with some error .",
    "e.g. , suppose that measurements resulted in the argument - value pairs @xmath21 , @xmath22 , @xmath23 , and @xmath24 , where the argument measurements involved an error of magnitude @xmath25 , so that the last measurement might actually reflect @xmath26 instead of @xmath27 .",
    "a linear interpolant of the four measurements would have a slope of @xmath28 , but when the @xmath24 measurement is moved towards @xmath24 , the slope would discontinuously switch to @xmath29 .",
    "a similar effect occurs with splines . in other words ,",
    "when the ranking of the argument measurements with respect to their distance from @xmath14 is uncertain due to measurement errors , it seems inappropriate to use a method that strongly relies on the correctness of this ranking , e.g. , by using only the @xmath30 nearest neighbours of @xmath14 ; although disregarding faraway measurements or extreme observations completely instead of just downweighting them might still be advisable to increase the robustness of the method if outliers might exist , e.g. , due to fat - tailed error distributions .",
    "one existing class of methods , inverse distance weighting ( idw ) @xcite , also uses smoothly decaying weights that only depend on the distance from @xmath14 . in a commonly used variant of idw ,",
    "the weights are inversely proportional to the square or a larger power of the distance , and we will show that this method can be derived as a special case of motabar   in which a noninformative prior distribution for @xmath31 is used .",
    "a common feature of many interpolation and smoothing methods , including idw and other weightings- or kernel - based methods , linear interpolation , and nearest or natural neighbour interpolation , is that the estimate can not exceed the largest measured values , even if the data strongly suggest a nonzero slope at the largest data point .",
    "such methods will therefore always underestimate the maxima of @xmath0 .",
    "other methods , like polynomial regression , can in some situations ` overshoot ' and result in estimates that lie far outside the measured range .",
    "spline methods are often considered a good compromise between these two behaviours regarding maxima .",
    "depending on the choice of @xmath19 , motabar  will behave quite similar to spline methods in this respect . as a consequence",
    ", the weights of individual data values in the estimate might be @xmath32 or @xmath33 .",
    "similar to other nonparametric methods or methods with many parameters , the motabar  estimate might fit the data too narrowly and show too much fluctuations due to this ` overfitting ' when @xmath19 and the prior variances are badly chosen .",
    "if the prior variances for higher derivatives grow too fast , the estimate is allowed to vary on smaller scales than the sampling density can resolve reliably in view of the assumed error distributions .",
    "the amount of overfitting can thus be controlled by varying @xmath19 and the priors .",
    "assume that we are interested in the value of a certain function @xmath34 and all its derivatives of order @xmath17 at a certain _ position of interest _",
    "@xmath35 , where @xmath18 and @xmath0 is assumed to be @xmath19 times continuously differentiable .",
    "we take the convention to write elements of @xmath36 as column vectors and to enumerate their @xmath37 components with a subscript index , hence @xmath38 where the @xmath39 symbol denotes transposition .",
    "assume that the only information we have about @xmath0 is ( i ) @xmath40 _ measured data points _",
    "@xmath41 with @xmath42 , ( ii ) some estimates of the magnitude of _ measurement errors _ , and ( iii ) some beliefs about the variability of @xmath0 and some of its derivatives .",
    "these assumptions will be made more precise later .",
    "we enumerate measurements with superscript indices in guillemots , hence @xmath43 .",
    "assume the @xmath44-th data point @xmath41 is the result of trying to measure @xmath0 at the argument @xmath45 , but the measurement might involve an error in both the argument and the value , so that the actual result @xmath46 is the value of @xmath0 at a slighly different argument @xmath47 ( where @xmath48 is the _ argument error _ ) plus some _ value error _ @xmath49 . in other words , @xmath50    [ [ multi - index - notation . ] ] multi - index notation .",
    "+ + + + + + + + + + + + + + + + + + + + +    for dealing with higher - order derivatives and multidimensional taylor polynomials of @xmath0 , it is convenient to use a _ multi - index _ @xmath51 consisting of nonnegative integers @xmath52 for @xmath53 .",
    "we then use the denotations @xmath54 @xmath55 where @xmath56 and @xmath57 .",
    "note that the order of differentiation in @xmath58 is unimportant for @xmath57 .",
    "our quantities of interest are then the derivatives @xmath59",
    "for a given position of interest @xmath35 , a relationship between our quantities of interest @xmath60 and the data @xmath61 is given by taylor s theorem , which leads to @xmath62 where @xmath63\\quad ( |\\alpha|=p).\\end{aligned}\\ ] ] note that there are @xmath64 many choices for @xmath65 with @xmath66 and @xmath67 many choices for @xmath65 with @xmath68 .",
    "it will be convenient to stack all relevant quantities into the column vectors @xmath69 @xmath70 , @xmath71 , @xmath72 , @xmath73 , @xmath74 and the matrix @xmath75 note that the column vector @xmath76 has a row for each combination of @xmath65 and @xmath44 , and we write this row index as @xmath77 .",
    "this must not be confused with the row - and - column index pair @xmath78 of the matrix @xmath79 . in other words",
    ", @xmath80 is an @xmath81 vector , @xmath76 is @xmath82 , and @xmath79 is an @xmath83 matrix .",
    "eq.[eqn : taylor ] is now summarised in matrix notation as @xmath84 although this is formally a linear regression model , we are not interested in estimating its coefficient matrix @xmath79 ( which we know already up to some measurement errors ) , but in estimating the regressor @xmath80 , and this we need to do for each position of interest @xmath14 separately .",
    "this is the reason why we next apply bayes theorem to eq.[eqn : taylorshort ] .",
    "let us model our information about the quantities of interest @xmath85 and about the other unknown terms in eq.[eqn : taylorshort ] as bayesian beliefs , i.e. , in the form of ( subjective ) probability distributions , by treating @xmath80 and @xmath76 as random variables with lebesgue - integrable densities @xmath86 .",
    "because in general , each @xmath87 corresponds to an argument @xmath88 that is different from @xmath14 , we assume @xmath80 and @xmath76 are independent . also , we assume that the function - related quantities @xmath80 are independent from the measurement - related quantities @xmath89 and @xmath90 .",
    "the measured data @xmath91 then allow us to update our prior beliefs @xmath92 about @xmath80 by applying bayes theorem to eq.[eqn : taylorshort ] , which leads to posterior beliefs @xmath93 note that we follow the general convention to use the same symbol , here @xmath94 , to refer to all occurring probability densities since it will always be clear from the context which variable s density is meant . also , we suppress the domain of integration in the following , and we are not interested in multiplicative constants that do not depend on @xmath80 , and use the symbol @xmath95 to denote equality up to such a constant .",
    "to utilise eq.[eqn : bayes ] , we need to specify    * a distribution @xmath96 for the argument errors that might depend on the arguments , * a distribution @xmath97 of the value errors that might depend on the arguments and argument errors , and * prior distributions @xmath92 and @xmath98 expressing our initial information about the values and derivatives of @xmath0 at @xmath14 before the measurements .",
    "the term @xmath99 can then be determined from @xmath98 using eq.[eqn : r ] , while the term @xmath100 can be determined from @xmath97 using eq.[eqn : taylorshort ] . as an estimate of @xmath80 one",
    "can then use any measure of central tendency of the posterior distribution as given by eq.[eqn : bayes ] , e.g. , the posterior mean , median , or mode , while an estimate of the estimation error would be given by a suitable measure of dispersion , e.g. , the posterior variance or the median distance from the median .",
    "although this strategy can in principle be applied to error and prior distributions of any form , our approach becomes especially simple in the gaussian case .      for the rest of this article",
    ", we will assume that value errors and priors are ( multivariate ) gaussian , @xmath101 where we use _ precision matrices _ @xmath102 and omit to denote the dependency on @xmath103 to simplify the following equations .",
    "note that @xmath104 is a @xmath105 square matrix whose rows and columns we address using indices of the form @xmath106 with @xmath107 and @xmath108 .",
    "the case in which it is known that there are no value errors requires some special treatment since then @xmath109 so that @xmath104 does not exist . in that case",
    ", @xmath97 behaves like a dirac delta function , giving @xmath110 for all integrable functions @xmath111 of @xmath112 .",
    "the taylor remainders @xmath113 given @xmath89 and @xmath90 are then also gaussian with mean @xmath114 and covariance matrix @xmath115 given by @xmath116 note that for almost all choices of @xmath90 and @xmath117 , the matrix @xmath115 is nonsingular .",
    "we will therefore assume that @xmath90 has a continuous distribution and @xmath117 is nonsingular , so that almost surely @xmath115 is nonsingular .",
    "if it is known that @xmath118 , we assume instead that @xmath119 are in general position , in which case @xmath115 is also nonsingular .    to get an understanding of the relative sizes of the entries in @xmath115 , consider the special case in which @xmath117 is block - diagonal with identical @xmath120 blocks @xmath121 , so that @xmath122",
    ". then @xmath123 , showing how the covariance of the remainders @xmath124 and @xmath125 grows with the @xmath19-th power of the scalar product of @xmath126 and @xmath127 . if @xmath121 is diagonal , also @xmath115 is diagonal and the variance of the remainder @xmath124 grows with the @xmath128-th power of the distance between the actual argument of measurement @xmath126 and the position of interest @xmath14 .",
    "now the main step to solving eq.[eqn : bayes ] is to see that the posterior of @xmath80 given @xmath89 , @xmath129 , and @xmath90 is still gaussian , @xmath130 with precision matrix @xmath131 and mean @xmath132 given by @xmath133 where we call @xmath134 the _ squared weight matrix_. note that the basic behaviour of @xmath135 is to decrease with a growing distance of @xmath126 and @xmath127 from @xmath14 , similar to the weights used in idw .",
    "[ [ singularities . ] ] singularities .",
    "+ + + + + + + + + + + + + +    although in pathological cases , some of the involved matrices can be singular , they are nonsingular in general .",
    "more precisely , assume that @xmath90 has a continuous distribution and @xmath117 is nonsingular , so that @xmath115 is nonsingular , too ( see eq.[eqn : postr ] ) .",
    "then @xmath20 exist and is nonsingular for almost all choices of @xmath136 , including the case @xmath109 . in that case , and for generic @xmath137 and @xmath14 , @xmath79 has full rank @xmath138 ( like a vandermonde matrix ) , so that also @xmath139 is nonsingular when @xmath140 .",
    "finally , under these assumptions @xmath141 is nonsingular for almost all choices of @xmath142 , including the case @xmath143 .",
    "integrating over all possible values of the argument errors @xmath90 finally shows that the posterior of @xmath80 given @xmath89 and @xmath129 is a mixture of gaussians whose mean and covariance matrix are @xmath144 this posterior mean , which we call the _ motabar  estimator _ , can now be used as a natural estimate of @xmath80 given @xmath89 , @xmath129 , and one can see that this estimate is    * an affinely linear function of the measured values @xmath46 , with weights that decrease with growing distance between @xmath14 and @xmath45 , and * a mixture of values @xmath145 each of which is an infinitely smooth rational function of the position of interest @xmath14 that has no poles .",
    "because the entries of the inverse of an @xmath146 square matrix @xmath147 are rational functions of the entries of @xmath147 of degree at most @xmath148 , the degree of @xmath145 is at most @xmath149 . in practice , one can determine @xmath145 and @xmath150 like this , avoiding the large @xmath120 matrix inversions and solving instead two systems of linear equations :    1 .",
    "given @xmath14 , @xmath89 , and @xmath90 , compute @xmath79 , @xmath114 , and @xmath115 from eqns.[eqn : x ] and [ eqn : r ] .",
    "2 .   determine @xmath151 and @xmath152 by solving @xmath153 .",
    "3 .   compute @xmath154 .",
    "4 .   determine @xmath155 by solving @xmath156 .",
    "note that in pathological cases , either of the two systems might not be solvable uniquely , which possibility we do not discuss here .",
    "although the usually smaller @xmath146 matrix inversions needed to determine @xmath157 from in eq.[eqn : sigmapost ] are not as easily avoided , one can at least determine the posterior variance of @xmath158 by solving @xmath159 for @xmath160 and using @xmath161 in the integral in eq.[eqn : sigmapost ] to determine @xmath162 .",
    "if the argument errors @xmath90 are not known to be zero , the integrals over @xmath90 will usually have to be evaluated numerically even when @xmath90 is gaussian as well , since the integrand is a nonlinear function of @xmath90 .",
    "if @xmath90 has small variance , it can however be feasible to approximate the integrals by using a quadratic or 4th - order approximation of @xmath145 , which will be explored in a separate article .",
    "in addition to the obvious translation invariance in all dimensions , the motabar  estimator is equivariant under various linear scaling transformations : ( i ) when @xmath136 , @xmath163 , and @xmath117 are all multiplied with the same constant @xmath164 , then @xmath165 remains unchanged and @xmath157 is multiplied by @xmath166 as well .",
    "( ii ) when for all @xmath44 , all @xmath65 , and some @xmath30 , @xmath167 , @xmath168 , and @xmath169 are all multiplied with the same constant @xmath164 , the distribution of @xmath90 is stretched by a factor of @xmath166 along the @xmath30-th axis , and when @xmath170 , @xmath171 , the @xmath65-th row and column of @xmath163 , and the @xmath77-th row and column of @xmath117 are all divided by @xmath172 , then also @xmath173 and the @xmath65-th row and column of @xmath157 are divided by @xmath172 for all @xmath65 .",
    "to better understand the effects of the various inputs to motabar , let us consider a number of special and limit cases , some of which turn out to be equivalent to well - known existing methods , whereas others are presented to show limitations of our approach .",
    "fig.[fig : interpol12 ] illustrates the diversity of results one can get from the same data .",
    "( colour online ) illustration of special and limit cases of motabar  interpolation for an equidistant sample ( black dots ) from runge s function @xmath174 with one missing measurement at 0.2 . left : spline interpolation",
    "compared to motabar  cases [ sec : nnbs ] ( piecewise polynomial based on four nearest neighbours ) and [ sec : lagrange ] ( lagrange polynomial ) .",
    "right : motabar  cases [ sec : localpoly ] ( local cubic polynomial regression interpolation ) , [ sec : p2uncorr ] ( @xmath175 with uncorrelated errors and priors ) , [ sec : idw4 ] ( 4-th order idw ) , and a smoothing case ( @xmath176 , small error , penalised intermediate derivatives ) , all resulting in nonpolynomial rational functions . ,",
    "title=\"fig:\",scaledwidth=49.0% ] ( colour online ) illustration of special and limit cases of motabar  interpolation for an equidistant sample ( black dots ) from runge s function @xmath174 with one missing measurement at 0.2 . left : spline interpolation",
    "compared to motabar  cases [ sec : nnbs ] ( piecewise polynomial based on four nearest neighbours ) and [ sec : lagrange ] ( lagrange polynomial ) .",
    "right : motabar  cases [ sec : localpoly ] ( local cubic polynomial regression interpolation ) , [ sec : p2uncorr ] ( @xmath175 with uncorrelated errors and priors ) , [ sec : idw4 ] ( 4-th order idw ) , and a smoothing case ( @xmath176 , small error , penalised intermediate derivatives ) , all resulting in nonpolynomial rational functions . ,",
    "title=\"fig:\",scaledwidth=49.0% ]      for vanishing argument errors , @xmath96 becomes a dirac delta function , so that @xmath177 hence the estimate is a pole - free rational function of @xmath14 of degree at most @xmath149 . for most of the remainder of this section",
    ", we only derive @xmath178 and @xmath179 which can then be plugged into eqs.[eqn : estimator ] and [ eqn : sigmapost ] if argument errors exist .",
    "if there is no prior information about @xmath0 , an improper @xmath31-prior with @xmath180 and a centralised @xmath181-prior with @xmath182 can be used , in which case @xmath183 in this case , @xmath184 is that value of @xmath80 which minimises the quadratic function @xmath185 which provides an alternative way of numerical computation .",
    "although this is formally a weighted least - squares estimator , it does not estimate a global set of parameters @xmath186 as in a global linear regression model @xmath187 , but is a local model in which @xmath79 and @xmath20 depend on @xmath14 , so that the terms in the estimator have to be computed for each @xmath14 separately .",
    "[ [ consistent - estimation - of - polynomial - components . ] ] consistent estimation of polynomial components .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if @xmath188 and @xmath189 , we have @xmath190 .",
    "hence if @xmath191 and @xmath129 is a monomial with @xmath192 for some @xmath65 with @xmath66 , then @xmath129 equals the @xmath65-th column of @xmath79 and hence @xmath193 equals the @xmath65-th column of @xmath194 .",
    "this means that the estimate @xmath195 is zero for @xmath196 and one for @xmath197 , which are the correct derivatives of the given monomial at @xmath191 .",
    "consequently , motabar  with improper priors is also equivariant under the addition of polynomials , e.g. , a quadratic trend in a time series , a property shared with numerical differentiation via discrete legendre polynomials .",
    "however , while the latter has @xmath198 with an orthogonal coefficient matrix @xmath199 , the motabar  coefficient matrix @xmath200 is not orthogonal in general .    for proper priors with @xmath201 and @xmath202 ,",
    "the absolute value of the @xmath65-th derivative is underestimated since the prior drags the estimate towards its mean at zero , and the absolute value of the other derivatives is slightly overestimated ( nonzero ) .",
    "this effect becomes smaller with increasing @xmath19 .      for vanishing value errors",
    ", we have @xmath203 , hence @xmath204 and @xmath205 in the limit case of @xmath109 , this can also be derived directly from eq.[eqn : bayes ] by substituting @xmath206 . in that case , however , @xmath115 can become singular when @xmath207 for some @xmath44 , which has to be taken care of in the numerical solution , e.g. , by assuming very small but nonzero @xmath136 , or by using the following exact solution for the case @xmath208 : in that case , @xmath209 and , using the notation @xmath210 for a matrix @xmath211 without its @xmath212th row and @xmath30th column and a vector @xmath213 without its @xmath212th element , @xmath214 where @xmath215 are the prior precision and mean of @xmath216 conditional on @xmath209 , and @xmath217 is a vector of ones .",
    "if in addition to @xmath218 we have @xmath188 and @xmath189 , we get @xmath219 for general @xmath14 , while for @xmath220 , we get @xmath209 , @xmath221 , and @xmath222 with @xmath223 .",
    "this is interpolation based on local polynomial regression in which the weights @xmath224 decrease as a power of a quadratic form of @xmath225 and @xmath226 ( e.g. , the dash - dotted red line in fig.[fig : interpol12 ] right ) .",
    "hence , in contrast to other local polynomial methods such as loess , far away observations get a positive though small weight .      at the other end of the error scale , for @xmath227 , we get @xmath228 and thus @xmath229 and @xmath230 .",
    "so if the @xmath31-prior is proper , the posterior approximates it , otherwise it diverges .",
    "the same behaviour obtains for a decreasingly informative @xmath181-prior , i.e. , for @xmath231 .",
    "this shows that while a noninformative @xmath31-prior might be chosen , the @xmath181-prior must be proper since it regulates the overall variability of the estimate .",
    "if the value error variance is finite ( @xmath232 ) , one might , on the other hand , assume that the derivatives of @xmath0 of degree @xmath233 are negligible compared to the scale of @xmath112 , e.g. , because @xmath0 is believed to be approximately a polynomial of degree at most @xmath234 .",
    "then , taking the limit @xmath182 and @xmath235 , we get @xmath236 which is a form of bayesian polynomial regression . although the squared weight matrix @xmath237 no longer depends on @xmath14 here , this is still not a global regression method since the @xmath31-prior might dependent on @xmath14 , e.g. , because one assumes some linear or nonlinear trend or some change in variability with @xmath14 .",
    "a truly global regression method can be obtained by using the noninformative @xmath31-prior with @xmath180 in addition to @xmath182 and @xmath235 .",
    "if @xmath238 , then @xmath239 in the limit , this is just ordinary global least - squares polynomial regression with polynomials of order @xmath234 and possibly correlated value errors of differing magnitude .      for @xmath240 , @xmath175 , @xmath241 , and nonvanishing argument measurement errors",
    ", our model becomes the `` errors in the variables '' linear regression model @xmath242 since this is linear in @xmath243 , the integral over @xmath90 in eq.[eqn : bayes ] can be solved analytically . for @xmath191 , @xmath143 , @xmath244 , @xmath245 , @xmath246 , and @xmath247 , this results in the non - gaussian posterior @xmath248",
    "the posterior mode has @xmath249 and @xmath250 where @xmath251 and @xmath252 , @xmath253 , @xmath254 , and @xmath255 .",
    "if @xmath256 , @xmath257 is the largest solution of the above equation , otherwise the smallest , and it has the same sign as @xmath258 .    when the same model is estimated with the _ total least - squares _ method ( aka _ deming regression _ ) studied already by @xcite , the equation is @xmath259 instead , which is symmetric under an exchange of `` dependent '' and `` independent '' variables and leads to a larger absolute value of @xmath257 . for @xmath260 ,",
    "this difference vanishes , and for @xmath261 , both solutions converge to the ordinary least - squares linear regression line with @xmath262 . for @xmath263 ,",
    "total least squares gives @xmath264 independently of @xmath265 , while the cubic equation gives @xmath266 which still depends on @xmath265 and has @xmath267 for @xmath268 .",
    "this comparison shows that unlike in total least squares , it is essential in motabar  whether we consider @xmath7 a function of @xmath6 or vice versa .    to understand why the effects of value and argument errors are different in the linear model and why the cubic equation is not symmetric in @xmath269 , consider the simple case where the measurements are @xmath270 and only the middle one , @xmath271 , has errors in both dimensions which are independent with @xmath272 with equal probability",
    "then the real data @xmath273 are either @xmath274 or @xmath275 , both giving slope @xmath276 in linear regression , or @xmath277 or @xmath278 , both giving slope @xmath279 , so that the motabar  posterior mean is @xmath280 , whereas total least squares results in @xmath276 because of the obvious symmetry .      for @xmath240 , @xmath281 , and @xmath143 ,",
    "we get lagrange interpolation with @xmath282 ( e.g. , the solid green line in fig.[fig : interpol12 ] left ) . for @xmath283 and @xmath284 , it depends on the placement of the @xmath126 whether the resulting polynomial is an interpolant or not ( see @xcite for an overview ) .",
    "a particularly simple case occurs for the minimal choice of @xmath19 , @xmath285 , and when both value errors and priors are uncorrelated , so that @xmath136 and @xmath117 are diagonal , and @xmath286 .",
    "then we have @xmath287 , @xmath288 , and @xmath289 , where @xmath290 the latter is the squared distance between @xmath126 and @xmath14 as measured by the quadratic norm that is weighted with the prior derivative variances @xmath291 .",
    "the posterior distribution of @xmath16 given @xmath90 is then gaussian with mean and variance given by @xmath292 this is a generalization of inverse distance weighting that takes into account value error via the occurrence of @xmath293 in eq.[eqn : bidww ] and prior information via the occurrence of @xmath294 in eq.[eqn : bidw ] .",
    "we propose to call this method _ bayesian idw smoothing .",
    "_ note that , as expected , @xmath295 is a rational function of @xmath14 of degree at most @xmath296 .",
    "setting @xmath109 , @xmath143 , and @xmath297 in the preceding , we get ordinary idw with squared distances , @xmath298 where @xmath295 is now a rational function of @xmath14 of degree exactly @xmath299 . note",
    "that while @xmath295 is independent from @xmath300 , the posterior variance is proportional to @xmath300 .",
    "in other words , our bayesian approach shows that the uncertainty of the idw estimate of order two is directly proportional to the scale of the derivatives of @xmath0 .",
    "if @xmath175 , @xmath240 , @xmath202 , @xmath244 , @xmath136 and @xmath117 are diagonal , and @xmath301 , then @xmath302 note that @xmath303 does not only depend on the distances of the @xmath126 from @xmath14 as encoded in @xmath304 , but also on the relative position of @xmath126 and @xmath127 , via the mixture terms in @xmath305 .",
    "see the dashed blue line in fig.[fig : interpol12 ] right for an example .",
    "this dependency vanishes if we let @xmath306 and @xmath307 , where we get an instance of the following case :      assume that @xmath202 , @xmath244 , @xmath136 is diagonal , @xmath308 , and @xmath309 . if we let the prior for @xmath310 become noninformative by letting @xmath306 , but let the prior for @xmath60 with @xmath311 become sharp at @xmath312 by letting @xmath307 , then @xmath313 in other words , the motabar  estimate converges to the ordinary idw smoothing ( or interpolation , if @xmath109 ) solution with exponent @xmath128 .",
    "e.g. , @xmath175 and @xmath109 gives 4-th idw interpolation ( solid green line in fig.[fig : interpol12 ] right ) .",
    "note that this , however , also shows that using idw with higher powers than two corresponds to implicitly assuming that some derivatives vanish which the result shows not to vanish after all .",
    "hence the only plausible form of idw is the one with squared distances .",
    "[ [ inconsistent - derivatives . ] ] inconsistent derivatives .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    this example also illustrates an unintuitive feature of motabar : the estimates derivative @xmath314 need not coincide with the derivative of the estimated value @xmath315 with respect to @xmath14 . in idw smoothing , the estimated slope is zero as enforced by the sharp prior , although the estimated value is not constant .      on the upper end of the order spectrum , for certain choices of priors , one can choose very large orders @xmath19 without running into numerical infeasibilities .",
    "a special limit case obtains with similar priors as above : assume @xmath109 , a noninformative prior for the function value @xmath158 ( so that @xmath316 ) , an uncorrelated and centralised @xmath181-prior with @xmath244 and @xmath309 , and that all derivatives of @xmath0 of order @xmath317 vanish at @xmath14 ( so that @xmath318 and @xmath319 for @xmath311 ) . in the limit for @xmath320 ,",
    "then only the nearest neighbour @xmath126 of @xmath14 is used in the estimation .",
    "this is because then @xmath321 is diagonal and @xmath322 for @xmath323 , so that @xmath324 . in other words ,",
    "motabar  then estimates @xmath16 by @xmath325 , as in simple _",
    "nearest neighbour estimation_. that is , for large @xmath19 and these priors , motabar  estimate approximates a step function that is constant on the voronoi cells ( aka thiessen polyhedrons ) of the sample . as the 4-th order idw example in fig.[fig : interpol12 ] ( right ) shows , the step - like shape can already be seen for small values of @xmath19 .",
    "a step - like function with a similar smoothening at the edges also obtains for @xmath320 when argument errors @xmath90 are not vanishing , since then @xmath326 . on the other hand ,",
    "nonvanishing value errors , even if very small , can lead to additional levels at the means of more than one nearest neighbour , showing that this limit behaviour is quite unstable ( see , e.g. , the thin yellow line in fig.[fig : interpol12 ] right ) .",
    "one can also get higher - order piecewise polynomial estimates , by assuming that only the derivatives of @xmath0 of order @xmath327 vanish at @xmath14 for some @xmath328 , and using an improper prior with @xmath329 for @xmath330 .",
    "then the motabar  estimate approximates a piecewise polynomial interpolant of degree @xmath331 that uses the @xmath332 observations nearest to @xmath14 ( assuming those are in general position ) . for @xmath333 ,",
    "this leads to a piecewise linear estimate that need not , however , coincide with ordinary linear interpolation since , e.g. , the two nearest neighbours of @xmath334 might both lie left of @xmath14 , leading to a discontinuity in the estimate to the right of @xmath14 .",
    "only when the positions @xmath126 build a regular polyhedral grid , the estimate approximates ordinary linear interpolation . for @xmath240 ,",
    "this requires equidistant measurement positions , and for @xmath335 a regular triangular grid is required , whereas on a square grid , one gets a discontinuous linear interpolation with four square interpolation cells per grid cell , each corresponding to a different choice of three of the four corners of the grid cell .",
    "analogous results hold for higher - dimensional rectangular grids . for @xmath336 and @xmath240 ,",
    "the limit result is a piecewise cubic polynomial which is , however , not a cubic spline since although its 2nd derivative is continuous , its value and slope are not ( e.g. , the dashed blue line in fig.[fig : interpol12 ] left ) .      if the distance between @xmath14 and the data positions @xmath126 diverges , i.e. , if @xmath337 , we have @xmath338 .",
    "if the @xmath31-prior is informative ( @xmath201 ) , we then have @xmath339 that is , the data are too far away to drag the posterior significantly away from the prior . with a noninformative @xmath31-prior ,",
    "on the other hand , the estimate for @xmath337 will either approximate the sample mean ( if @xmath285 ) or will diverge to @xmath340 ( almost surely for @xmath341 ) .",
    "if the @xmath126 are either regularly distributed or drawn from a sufficiently smooth distribution , the distance between @xmath14 and the closest @xmath126 will decay at a rate @xmath342 for @xmath343 .",
    "we conjecture that then the posterior precision grows at a rate @xmath344 for nonpathological error distributions and priors .",
    "the rationale for this conjecture is this : assume that @xmath109 , @xmath345 , and the @xmath40 arguments @xmath126 are sufficiently uniformly distributed over a @xmath37-dimensional ball of unit radius about @xmath14 , and let @xmath346 .",
    "then @xmath347 and this sum is asymptotically proportional to the integral @xmath348 } d^dx\\,(x-\\xi)^{\\alpha+\\beta}||x-\\xi||_2^{-2p}\\\\      & \\sim n\\int_{z = n^{-1/d}}^1 dz\\,z^{|\\alpha|+|\\beta|}z^{-2p }      \\sim n^{1+(2p-|\\alpha|-|\\beta|-1)/d}.\\end{aligned}\\ ] ] the conjecture implies @xmath349 , in particular @xmath350 . in particular , for @xmath240 , the conjectured rate of convergence of @xmath351 to @xmath16",
    "is @xmath352 , the same as for spline interpolation .",
    "@xcite introduced a different approximation scheme that is also based on a local taylor expansion . adapting their terminology and notation to ours",
    ", that scheme can be described as defining an estimator @xmath353 for @xmath16 , where the weights @xmath354 are chosen to minimise the squared norm @xmath355 subject to a number of constraints @xmath356 for all @xmath65 with @xmath357 for some @xmath328 , where @xmath358 and @xmath359 are diagonal matrices whose entries are certain parameters @xmath360 and error variances , @xmath361 although the parameters @xmath362 correspond to the prior variances in our approach , wang et al.s rationale is not bayesian .",
    "their motivation is rather that @xmath363 is an estimator of the squared approximation error @xmath364 , and the constraints make sure that the estimator is correct if the true @xmath0 is a polynomial of degree @xmath365 .",
    "as we see from the usage of @xmath358 and @xmath359 instead of @xmath163 and @xmath366 , their scheme implicitly assumes uncorrelated `` priors '' and errors , but one can easily adapt it to deal with correlations by using @xmath367 instead of @xmath363 .",
    "also , they do not consider the case of argument errors , and for these it does not seem obvious how to deal with them consistently in their framework .    to compare their scheme to ours in the case of no argument errors ,",
    "note that the motabar  estimate @xmath184 is a linear function of @xmath129 only when the prior for @xmath80 is improper and the prior for @xmath76 is centralised , while otherwise the estimate is only an _ affine _ function of @xmath129 .",
    "hence let us assume @xmath143 and @xmath244 in this section . in that case",
    ", the motabar  estimator @xmath184 itself is that @xmath80 which minimises the quadratic function @xmath368 however , while in wang et al.s approach a constrained problem obtains , the above is an unconstrained problem .",
    "taking the corresponding limit of @xmath180 in wang et al.s scheme , their target function becomes @xmath369 so that their coefficients depend on the relative variances ( and correlation structure ) of @xmath80 but no longer on @xmath370 , while ours depends on the latter but not on the former .    despite these differences , both approaches seem to have the same rate of convergence for @xmath343 and reduce to ordinary polynomial regression or idw for certain choices of control parameters .",
    "prior knowledge about @xmath80 , such as constraints of the form @xmath371 $ ] for some @xmath65 and @xmath372 , can easily be incorporated into motabar  estimation by choosing a suitable prior distribution for @xmath80 , such as a uniform distribution of @xmath60 on @xmath373 $ ] or a gaussian restricted to this interval .",
    "if one then rewrites the chosen prior in the form @xmath374 which can be seen as a generalization of eq.[eqn : phiprior ] , it is easy to see that the resulting posterior simply involves the same factor @xmath375 , leading to a posterior distribution of @xmath376 with the same @xmath155 and @xmath141 as before .",
    "however , since the posterior mean is then no longer equal to @xmath155 , one either needs to integrate eq.[eqn : posteriorgeneralized ] explicitly or use the posterior mode instead .    in some cases , the posterior mean can be determined analytically .",
    "e.g. , two - sided constraints of the form @xmath371 $ ] with finite bounds @xmath377 can most easily be modelled by putting @xmath329 and @xmath378 , where @xmath379 is the heaviside step function with @xmath380 for @xmath381 and @xmath382 otherwise . for one - sided constraints of the form @xmath383",
    ", one can use @xmath384 .      using the prior factor @xmath385 ,",
    "the marginal posterior density of @xmath310 is proportional to @xmath386 with @xmath387 and @xmath388 . for a density of this form",
    ", the mean is given by @xmath389 for @xmath390 outside @xmath373 $ ] , this asymptotically equals @xmath391 with @xmath392 for @xmath393 ( good approximation for @xmath394 ) , or @xmath395 with @xmath396 for @xmath397 ( good approximation for @xmath398 ) .",
    "with this constraint , the motabar  estimator becomes @xmath399 where @xmath400 depends on @xmath90 via @xmath390 and @xmath401 .",
    "letting @xmath307 in the preceding , we get @xmath402      to see that our methodology can be fruitful also in the case of non - gaussian @xmath181-priors and value errors , consider the case of @xmath240 , vanishing argument errors , an improper @xmath31-prior , independent laplace @xmath181-priors with @xmath403 for some @xmath404 , and independent value errors with asymmetric laplace distributions @xmath405 , where @xmath406 , @xmath407 , and @xmath408 $ ] . then @xmath409 where @xmath410 and the posterior mode of @xmath80 given @xmath89 , @xmath129 is the one that maximises @xmath411 . for @xmath412 , @xmath413 , and sharp @xmath181-priors with @xmath414 ,",
    "this reduces to minimizing the quantile regression loss function , @xmath415 , so the above can be considered a novel form of local polynomial quantile regression .",
    "similar analytical forms of @xmath111 obtain also for other forms of @xmath181-priors , e.g. , gaussian or uniform ones .",
    "with a noninformative @xmath31-prior , the inconsistency between @xmath314 and the @xmath65-th derivative of @xmath416 with respect to @xmath14 will decrease with @xmath19 and vanish for @xmath320 . therefore ,",
    "if one is interested in all derivatives of order @xmath365 , we suggest to use a noninformative prior for all @xmath60 with @xmath330 , and use a @xmath19 somewhat larger than @xmath331 , using either informative or noninformative priors for the derivatives of order @xmath417 .",
    "( colour online ) typical effect of priors for data points placed uniformly at random ( black dots ) from @xmath418 ( thin dashed black line ) , with @xmath419 and @xmath176 .",
    "left : @xmath420 , no value error .",
    "right : @xmath421 , medium - sized iid value error with @xmath422 .",
    "in 100 such samples , the median @xmath423 distance between true and estimated function in the left[right ] case was 0.068[0.24 ] for proper correlated @xmath31-priors growing as @xmath424 with the correct frequency @xmath425 ( thick solid cyan line ) , 0.15[0.29 ] for proper uncorrelated priors of the same size ( thick dash - dotted cyan line ) , 0.22[0.48 ] for proper correlated priors with @xmath426 ( thin solid blue line ) , 0.25[1.7 ] for improper priors ( thin dash - dotted red line ) , and 0.56[0.55 ] for proper correlated priors with @xmath427 ( thin dotted green line ) .",
    ", title=\"fig:\",scaledwidth=49.0% ] ( colour online ) typical effect of priors for data points placed uniformly at random ( black dots ) from @xmath418 ( thin dashed black line ) , with @xmath419 and @xmath176 . left : @xmath420 , no value error .",
    "right : @xmath421 , medium - sized iid value error with @xmath422 . in 100 such samples ,",
    "the median @xmath423 distance between true and estimated function in the left[right ] case was 0.068[0.24 ] for proper correlated @xmath31-priors growing as @xmath424 with the correct frequency @xmath425 ( thick solid cyan line ) , 0.15[0.29 ] for proper uncorrelated priors of the same size ( thick dash - dotted cyan line ) , 0.22[0.48 ] for proper correlated priors with @xmath426 ( thin solid blue line ) , 0.25[1.7 ] for improper priors ( thin dash - dotted red line ) , and 0.56[0.55 ] for proper correlated priors with @xmath427 ( thin dotted green line ) .",
    ", title=\"fig:\",scaledwidth=49.0% ]      [ [ correlation - of - derivatives . ] ] correlation of derivatives .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    assume that @xmath240 and @xmath428 is of the form @xmath429 with a spectrum @xmath430 and phases @xmath431 chosen uniformly at random .",
    "then the covariance of @xmath432 and @xmath433 is @xmath434 in other words , the @xmath30-th and @xmath435-th derivatives are fully correlated , and the @xmath30-th and @xmath436-th derivatives are fully anti - correlated ( see also @xcite ) .",
    "if the spectrum of @xmath0 is known approximately , eq.[eqn : corr ] the above is therefore a natural choice for the @xmath31-prior covariance structure . for unknown spectrum , however , using a prior with strong correlation between derivatives can result in worse results than using an uncorrelated or improper prior , as can be seen in the example in fig.[fig : sin_noerr_proper_improper ] .",
    "[ [ growth - of - derivative - variance . ] ] growth of derivative variance .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if @xmath430 is concentrated at a single frequency @xmath437 , i.e. , @xmath438 , the variance of @xmath432 is growing exponentially : @xmath439 . for",
    "a broader but bounded spectrum , it is growing sub - exponentially .",
    "e.g. , a uniform spectrum of the form @xmath440 for @xmath441 and @xmath442 for @xmath443 gives @xmath444 .",
    "for typical unbounded spectra , in contrast , the variance growth is super - exponential .",
    "e.g. , for an exponentially decaying spectrum with @xmath445 for some @xmath446 , as is often assumed of chaotic dynamical systems , we get @xmath447 .",
    "similarly , for a gaussian decaying spectrum with @xmath448 for some @xmath449 , we get @xmath450 . in case of a power - law decaying spectrum ,",
    "the variance even diverges for large @xmath30 .",
    "e.g. , for @xmath451 , we get @xmath452 for @xmath453 and @xmath454 for @xmath455 , justifying the use of an improper prior .      without argument error ,",
    "the _ posterior predictive distribution _ of @xmath46 from the taylor model about @xmath456 has @xmath457 if the model is correct , one therefore expects that approximately @xmath458 likewise , for any fixed @xmath14 , and if @xmath143 and @xmath459 , the posterior predictive distribution of @xmath460 from the taylor model about @xmath14 has @xmath461 if the model is correct , one therefore expects that approximately @xmath462 hence , assuming @xmath463 with a known matrix @xmath464 and unknown @xmath449 , one could either use the same @xmath465 for all @xmath14 and choose it so that the `` global '' eq.[eqn : postpredglobal ] is fulfilled , which is similar to the suggested parameter choice in @xcite , or use a different @xmath465 for each @xmath14 and choose it so that the `` local '' eq.[eqn : postpredlocal ] is fulfilled .      if @xmath109 and @xmath466 , @xmath184 does not depend on @xmath465 , and eq.[eqn : postpredlocal ] becomes @xmath467 ( see eq.[eqn : postr ] ) .",
    "this value is also identical to the posterior mode of @xmath465 when @xmath465 is treated as a scale hyperparameter with a noninformative jeffreys prior of @xmath468 .",
    "hence the motabar  interpolant with noninformative priors is @xmath469",
    "to demonstrate the performance of motabar  for complex data , we simulated a trajectory @xmath470 of the standard chaotic lorenz system given by the odes @xmath471 , @xmath472 , @xmath473 , where @xmath79 basically oscillates between @xmath474 with a frequency of @xmath475 ( see fig.[fig : lorenz],a ) .",
    "we then generated a sample of @xmath476 noisy observations @xmath477 , with iid errors @xmath478 ( which corresponds to @xmath479 of noise ) , for time - points @xmath480 that were regularly spaced at a distance @xmath481 ( which corresponds to @xmath482 data points per oscillation , see the black dots in fig.[fig : lorenz],f ) . from the sample",
    ", we reconstructed the original trajectory ( fig.[fig : lorenz],a ) up to a diffeomorphism , following the approach of @xcite by estimating the time evolution of the derivatives @xmath483 ( fig.[fig : lorenz],b ) , using different estimation methods .",
    "for motabar  estimation , we used @xmath484 , an improper @xmath31-prior , and a @xmath181-prior whose variance @xmath485 was chosen to fit the approximate range and frequency of the oscillation .",
    "fig.[fig : lorenz](e ) shows that the motabar  approach reproduces the shape of the trajectory much better than either spline smoothing ( c ) or numerical differentiation ( d ) , the latter being basically equivalent to an alternative phase space reconstruction method , the often used `` method of delays '' .",
    "note that there are other , more sophisticated state space reconstruction methods based on pca or discrete legendre polynomials , which deal better with noise @xcite and which will be compare to motabar  in a separate paper .",
    "( a ) ( colour online ) reconstruction of the lorenz attractor from noisy measurements of its @xmath79 component .",
    "( a ) simulated true phase space trajectory .",
    "( b ) derivatives of @xmath79 from numerical differentiation from simulated true @xmath79 data .",
    "( c ) derivatives from natural cubic smoothing spline for noisy measurements of @xmath79 .",
    "( d ) numerical differentiation from noisy measurements .",
    "( e ) derivatives from motabar  with @xmath484 and an improper @xmath31-prior .",
    "( f ) detail of motabar  estimated @xmath79 ( thick blue line ) compared to simulated true @xmath79 ( thin gray line ) , and noisy measurements ( black dots ) .",
    ", title=\"fig:\",scaledwidth=45.0%](b ) ( colour online ) reconstruction of the lorenz attractor from noisy measurements of its @xmath79 component .",
    "( a ) simulated true phase space trajectory .",
    "( b ) derivatives of @xmath79 from numerical differentiation from simulated true @xmath79 data .",
    "( c ) derivatives from natural cubic smoothing spline for noisy measurements of @xmath79 .",
    "( d ) numerical differentiation from noisy measurements .",
    "( e ) derivatives from motabar  with @xmath484 and an improper @xmath31-prior .",
    "( f ) detail of motabar  estimated @xmath79 ( thick blue line ) compared to simulated true @xmath79 ( thin gray line ) , and noisy measurements ( black dots ) . ,",
    "title=\"fig:\",scaledwidth=45.0% ] + ( c ) ( colour online ) reconstruction of the lorenz attractor from noisy measurements of its @xmath79 component .",
    "( a ) simulated true phase space trajectory .",
    "( b ) derivatives of @xmath79 from numerical differentiation from simulated true @xmath79 data .",
    "( c ) derivatives from natural cubic smoothing spline for noisy measurements of @xmath79 .",
    "( d ) numerical differentiation from noisy measurements .",
    "( e ) derivatives from motabar  with @xmath484 and an improper @xmath31-prior .",
    "( f ) detail of motabar  estimated @xmath79 ( thick blue line ) compared to simulated true @xmath79 ( thin gray line ) , and noisy measurements ( black dots ) . ,",
    "title=\"fig:\",scaledwidth=45.0%](d ) ( colour online ) reconstruction of the lorenz attractor from noisy measurements of its @xmath79 component .",
    "( a ) simulated true phase space trajectory .",
    "( b ) derivatives of @xmath79 from numerical differentiation from simulated true @xmath79 data .",
    "( c ) derivatives from natural cubic smoothing spline for noisy measurements of @xmath79 .",
    "( d ) numerical differentiation from noisy measurements .",
    "( e ) derivatives from motabar  with @xmath484 and an improper @xmath31-prior .",
    "( f ) detail of motabar  estimated @xmath79 ( thick blue line ) compared to simulated true @xmath79 ( thin gray line ) , and noisy measurements ( black dots ) .",
    ", title=\"fig:\",scaledwidth=45.0% ] + ( e ) ( colour online ) reconstruction of the lorenz attractor from noisy measurements of its @xmath79 component .",
    "( a ) simulated true phase space trajectory .",
    "( b ) derivatives of @xmath79 from numerical differentiation from simulated true @xmath79 data .",
    "( c ) derivatives from natural cubic smoothing spline for noisy measurements of @xmath79 .",
    "( d ) numerical differentiation from noisy measurements .",
    "( e ) derivatives from motabar  with @xmath484 and an improper @xmath31-prior .",
    "( f ) detail of motabar  estimated @xmath79 ( thick blue line ) compared to simulated true @xmath79 ( thin gray line ) , and noisy measurements ( black dots ) . ,",
    "title=\"fig:\",scaledwidth=45.0%](f ) ( colour online ) reconstruction of the lorenz attractor from noisy measurements of its @xmath79 component .",
    "( a ) simulated true phase space trajectory .",
    "( b ) derivatives of @xmath79 from numerical differentiation from simulated true @xmath79 data .",
    "( c ) derivatives from natural cubic smoothing spline for noisy measurements of @xmath79 .",
    "( d ) numerical differentiation from noisy measurements .",
    "( e ) derivatives from motabar  with @xmath484 and an improper @xmath31-prior .",
    "( f ) detail of motabar  estimated @xmath79 ( thick blue line ) compared to simulated true @xmath79 ( thin gray line ) , and noisy measurements ( black dots ) .",
    ", title=\"fig:\",scaledwidth=45.0% ]",
    "[ [ outlook - alternative - local - approximations . ] ] outlook : alternative local approximations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the simple form of the motabar  estimator is due to our assumption of gaussian value measurement errors and the fact that taylor polynomials are linear in their coefficients .",
    "alternatively , one might approximate @xmath0 by other functions that can be parameterised by the low - order derivatives of @xmath0 at @xmath14 . for @xmath240 and if it is known that @xmath0 displays oscillatory behaviour , one such approximation could be @xmath486 with a remainder function @xmath487 with @xmath488 . because then @xmath489 , @xmath490 , @xmath491 , and @xmath492 , the four derivatives can be estimated from the model if a plausible prior for @xmath493 is used , although the estimate will not be a linear function in @xmath129 since the above approximation is not linear in the coefficients . if @xmath0 is known to be periodic with frequency @xmath425 , it might seem that one could also use partial sums of the corresponding fourier series instead , which are linear in their coefficients , but they are not parameterizable by a finite number of derivatives of @xmath0 at @xmath14 .    [ [ software . ] ] software .",
    "+ + + + + + + + +    an open - source software package implementing motabar  for use with the python programming language is under development and will be made available at http://www.pik-potsdam.de/members/heitzig/motabar .",
    "this work was supported by the german federal ministry for education and research ( bmbf ) via the potsdam research cluster for georisk analysis , environmental change and sustainability ( progress ) .",
    "the author thanks forest w.  simmons , kira rehfeld , norbert marwan , bedartha goswami , and jrgen kurths for fruitful discussions ."
  ],
  "abstract_text": [
    "<S> we present a nonparametric method for estimating the value and several derivatives of an unknown , sufficiently smooth real - valued function of real - valued arguments from a finite sample of points , where both the function arguments and the corresponding values are known only up to measurement errors having some assumed distribution and correlation structure . </S>",
    "<S> the method , _ moving taylor bayesian regression _ ( motabar ) , uses bayesian updating to find the posterior mean of the coefficients of a taylor polynomial of the function at a moving position of interest . when measurement errors are neglected , motabar  becomes a multivariate interpolation method . </S>",
    "<S> it contains several well - known regression and interpolation methods as special or limit cases . </S>",
    "<S> we demonstrate the performance of motabar   using the reconstruction of the lorenz attractor from noisy observations as an example . </S>"
  ]
}