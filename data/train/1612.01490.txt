{
  "article_text": [
    "an artificial neural network ( nn ) is a simplified computational model of how the neurons in our brains operate to solve certain kinds of problems @xcite .",
    "a nn comprises of an input layer , an output layer , and one or more hidden layers in between with nonlinear processing units for feature extraction and transformation .",
    "when there are more than one hidden layer in a nn , the nn is often referred to as a deep nn .",
    "deep learning is often regarded as a re - branding of nns with a class of machine learning algorithms @xcite . in the deep learning of a nn ,",
    "each successive layer uses the output from the previous layer as input , representing a higher level set features derived from the lower - level features in the previous layer , and thus formulating a hierarchical structure of feature extraction .",
    "the work by @xcite marked a reactivation in the research and applications of deep learning of nn , which has seen many successful applications in pattern recognition and classification such as image an speech recognition , natural language processing , drug discovery and toxicology , among others .",
    "deep nns are prone to overfitting given the high - dimensionality of the parameters involved in the multiplicity of layers and the often large number of nodes .",
    "regularization methods such as early stopping , max - norm , ivakhnenko s unit pruning @xcite , @xmath1-regularization ( weight decay ) and @xmath0-regularization ( sparsity ) can be used to impose smoothness constraints on the learned nn model to help combat overfitting .",
    "a recent regularization method is dropout @xcite , where some number of units are randomly dropped from the hidden layers ( with recommended proportion 0.5 ) and the input layer ( with recommended proportion 0.2 ) during the training period of a deep nn .",
    "the dropout regularization can be viewed a stochastic version of model averaging and prevents the nodes from co - adapting too much . as such , dropout can significantly reduce the generalization errors in a trained deep nn . from an implementation perspective",
    ", dropout is noise injection ( ni ) , by adding bernoulli noise to hidden and inputs units in a nn @xcite . in the simple case of linear regression models @xmath2 , dropping a predictor @xmath3 from the model dropped out with a probability @xmath4 is equivalent , to the following tikhonov regularization in expectation : minimization of @xmath5 , where @xmath6 . in general , ni helps to improve the generalization ability of a trained nn , especially in fully connected nn layers @xcite , , and has been shown to be related to kernel smoothing @xcite and heat equation @xcite .    due to the optimistic performance of dropout , various algorithmic and computational extensions and improvements have been proposed .",
    "the maxout technique is designed to facilitate optimization by dropout and improve the accuracy of dropout with a new activation function @xcite .",
    "fast dropout speeds up the dropout algorithm by sampling from or integrating a gaussian approximation instead of randomly dropping nodes @xcite .",
    "dropconnect sets a random subset of weights in a nn to zero @xcite .",
    "standout or adaptive dropout selectively ( rather than randomly ) sets nodes to zero and chooses to enhance or inverse the contributions of a node to the next layer @xcite . in all these regularization techniques ( dropout , dropconnect , standout , shakeout ) , noises being injected to a deep nn are bernoulli noises .",
    "shakeout also leads to a sparse model and has better performance when the training data set is not large in size given that the bernoulli noises in shakeout are designed to allow both @xmath0 and @xmath1 regularization ( elastic - net type of regularization ) on the weights of a nn .",
    "on the other hand , there is still room to improve on the current available set of regularization techniques through bernoulli ni .",
    "first , the final nn trained with bernoulli ni is an average of a finite number of sub - models that may not exhibit concentration patterns in sub - models in large @xmath7-small @xmath8 settings .",
    "therefore , the `` average '' final trained nn can be biased .",
    "second , in shakeout , updates of weights through the backpropagation ( bp ) algorithm includes derivative of the sign function which can only be approximated at best .",
    "third , due to the restriction of bernoulli s noise ( two possible noise values ) , dropout corresponds to the @xmath1 regularization and the elastic - net - type regularization is the only form of extension from the @xmath1 regularization in shakeout .",
    "@xcite hinted that gaussian noise works just as well as , or perhaps better than , bernoulli noise .    in this paper",
    ", we propose _ whiteout _ , a _ gaussian adaptive regularization ni _ technique , to regularize learning of deep nns .",
    "whiteout is named so because it is a ni technique as dropout , and the injected gaussian noise is a `` white '' noise .",
    "whiteout perturbs the input and hidden nodes in a deep nn with gaussian noise , the variance of which adapts to the weights during the iterations of a deep learning algorithm .",
    "whiteout has a corresponding deterministic optimization objective function with a closed - from penalty term that includes lasso , ridge regression , elastic net , adaptive lasso , and adaptive elastic net as special cases .",
    "the involvement of the @xmath0 penalty enable whiteout to train a nn model in data sets of relatively small sizes .",
    "whiteout offers 3 tuning parameters , and is more flexible in terms of training than the above mentioned bernoullis ni injection .",
    "in addition to being reviewed as regularization technique to combat overfitting , whiteout can also be regarded as a technique to improve robustness of the learned model to small and insignificant perturbations in data .",
    "we also show that the empirical loss function with whiteout is consistent for the ideal loss function and has a tighter bound , and empirically has a higher convergence rate than those of bernoulli noise based methods .",
    "computationally , whiteout can be easily incorporated in the back - propagation procedure and is computationally efficient .",
    "the rest of the paper is organized as follows .",
    "the concept of whiteout is introduced in section [ sec : whiteout ] .",
    "the connection between whiteout and regularization is established in section [ sec : regularization ] .",
    "whiteout as a way to improve the robustness of a trained nn is presented in section [ sec : sensitivity ] .",
    "we define multiple types of loss function in nns in section [ sec : consistency ] , and prove that the noise - perturbed empirical loss function with whiteout converges almost surely to the ideal loss function .",
    "section [ sec : bp ] presents the step - by - step whiteout - augmented bp algorithm and proposes a weights updating algorithm without having to approximate the derivative of the sign function ( an improvement over the shakeout algorithm ) .",
    "the whiteout technique is applied to the classification problem in the mnist data , and compared to the regular bp algorithm , dropout and shakeout in section [ sec : experiment ] .",
    "concluding remarks are presented in section [ sec : discussion ] .",
    "most of the technical proofs are provided in the appendix .",
    "whiteout adds independent noises drawn from the gaussian distribution in eqn ( [ eqn : whiteout ] ) to each node @xmath9 in layer @xmath10 of a nn in each case of the training data in an epoch cycle of the deep learning algorithm , @xmath11 where @xmath12 is a node in layer @xmath13 , @xmath14 is the weight between nodes @xmath15 and @xmath12 , @xmath16 , @xmath17 , and @xmath18 $ ] are tuning parameters . in the practical implementation of whiteout",
    ", all three tuning parameters can be user - specified or chosen by cross validation ( cv ) .",
    "intuitively , @xmath19 should be inversely proportional to @xmath8 ; that is , given the same nn model , overfitting is less a serious problem with a large @xmath8 thus less regularization is required .",
    "whiteout can be regarded as a process of generating noisy versions of the training data set , and the auxiliary noise is then averaged out in the final trained nn @xcite .",
    "some convenient choices and special cases of whiteout noise are gaussian lasso noise ( gala ) , gaussian adaptive lasso noise ( gaala ) , gaussian ridge noise ( gar ) , gaussian elastic - net noise ( gen ) , and gaussian group noise ( gag ) presented in definition [ def : noise ] ( for notation simplicity , @xmath20 is used in place of @xmath14 ) .",
    "[ def : noise ] in whiteout ,    * gala draws gaussian noise from @xmath21 * gaala draws noises from @xmath22 * gar draws gaussian noise from @xmath23 . * gala , gaala and gar noises are collectively referred to as _ gallr _ noise , covering the scenarios when @xmath24 $ ] and @xmath25 .",
    "* gen draws noises from @xmath26 * gag penalizes a group of input variables @xmath27 ( e.g.,indicator variables generated from the same categorical attribute ) rather than singly , by drawing noises from @xmath28 , where @xmath29 is the scale constant concerning the group structure .",
    "noise @xmath30 can be injected to the nodes in both the input and the hidden layers in gallr and gen , whereas gag makes the most sense in perturbing the nodes in the input layer , since grouping of hidden nodes , which are abstract extracted features that do not necessarily have any physical meanings , are hard to justify .",
    "[ rem1 ] whiteout with gen noise is equivalent to adding the sum of gala noise and the regular gaussian noise with a constant variance @xmath31 .",
    "therefore , all theoretical properties established in the framework gallr noise ( sections [ sec : regularization ] , [ sec : sensitivity ] , [ sec : consistency ] ) also hold in case of gen noise .    injecting additive gaussian noise of mean",
    "= 0 as given in eqn ( [ eqn : whiteout ] ) is equivalent to injecting multiplicative gaussian noise to node @xmath32 as in @xmath33 , where @xmath34 .",
    "[ rem2 ] the minor difference between additive and multiplicative gaussian ni does not affect the theoretical properties concerning whiteout ( sections [ sec : regularization ] , [ sec : sensitivity ] , [ sec : consistency ] ) given the inherent properties of activation functions and the structures of deep nns .",
    "without loss of generality , we will establish the theoretical properties of whiteout in the framework of additive gallr noises , which will be extended to other noises types and to the multiplicative noise case , given the statements in remarks [ rem1 ] and [ rem2 ] .      a common framework where ni as a regularization technique is established is the the exponential family , where generalized linear models ( glms ) are based on @xcite . in a glm",
    ", the conditional distribution of output @xmath35 given input @xmath36 and parameters @xmath37 is modelled with the exponential family distribution with parameter @xmath38 that is often expressed as @xmath39 where @xmath40 is the natural parameter and @xmath41 is the log - partition function .",
    "the corresponding negative log - likelihood function is @xmath42 .",
    "given training data @xmath43 for @xmath44 , where @xmath8 is the sample size of the training data , the maximum likelihood estimator ( mle ) @xmath45 is @xmath46 whiteout substitutes the observed @xmath47 with its noisy version @xmath48 as defined in eqn ( [ eqn : whiteout ] ) in @xmath49 .",
    "that is , the noise perturbed version of the negative log - likelihood function is @xmath50 .",
    "minimization of the expectation @xmath50 over the distribution of noise @xmath51 leads to @xmath52    [ lem : rw ] the expected negative log - likelihood function over the distribution of noise @xmath53 @xmath54 can be expressed as @xmath55 @xmath56    the proof of eqns ( [ eqn : regloss ] ) and ( [ eqn : rw ] ) in lemma [ lem : rw ] is given in appendix [ app : rw ] .",
    "eqn ( [ eqn : regloss ] ) suggests that @xmath57 is a regularization term that penalizes the minimization of negative log - likelihood function . in glms",
    ", @xmath58 is a convex and smooth function of @xmath37 @xcite , and @xmath57 is always positive by jensen s inequality @xcite .",
    "the actual analytical form of @xmath57 can be readily derived .",
    "for example , in linear regression , @xmath59 and @xmath60 , thus @xmath61 in logistic regression , @xmath62 and @xmath63 , where @xmath64 , thus @xmath65    [ thm : regularization ] in the framework of glms , in expectation ,    * the whiteout procedure with additive gallr noise in definition [ def : noise ] is approximately equivalent to the adaptive lasso penalization @xcite , with the regularization term @xmath66 where @xmath67 , @xmath68 is a column vector of dimension @xmath8 . *",
    "the whiteout procedure with gen noise in definition [ def : noise ] is approximately equivalent to the elastic net penalization @xcite , with the regularization term @xmath69 * the whiteout procedure with gag noise in definition [ def : noise ] is approximately equivalent to the grouped lasso penalization @xcite , with the regularization term @xmath70 where @xmath71 is an indicator function .",
    "the results presented in theorem [ thm : regularization ] with additive noises can be easily extended to the case of multiplicative noises .",
    "[ cor : multi.regularization ] in the framework of glms , in expectation ,    * the whiteout procedure with multiplicative gallr noise leads to penalty term @xmath72 * the whiteout procedure with multiplicative gen noises leads to penalty term @xmath73 * the whiteout procedure with multiplicative gag noises leads to penalty term @xmath74 where @xmath75 is the subset of @xmath76 corresponding to @xmath77 , @xmath78 is the subset of @xmath76 corresponding to @xmath79 .",
    "we have shown in section [ sec : regularization ] that the whiteout procedure can be regarded as a regularization approach in the maximization of the likelihood function in glms . in this section",
    ", we will examine the whiteout procedure from the perspective of stabilizing a learned deep nn ( robustness to noisy small perturbation , and thus generalization of the learned nn ) .",
    "the mathematical framework , which is established below , can be used to justify nns of multiple hidden layers .",
    "it is motivated by the work in @xcite who examined the stability of a shallow nn with noise injection to the input layer only .",
    "we explore the more general case where ni can occur in the input and the hidden layers in deep nns . for the purposes of presentation clarity , we demonstrate the framework in nns with a single hidden layer and a single binary output node .",
    "the proof can be easily extended to a deep nn with multiple hidden layers and output with multiple outcomes ( remark [ rem3 ] ) .",
    "denote the training data by @xmath80 where @xmath81 and @xmath82 for @xmath83 , and the hidden nodes by @xmath84 for @xmath85 . inputs @xmath47 and hidden node @xmath84 are connected through the activation function @xmath86 with weights @xmath87 and bias @xmath88 .",
    "denote the injected noises via whiteout to @xmath47 during the training of @xmath89 by @xmath90 .",
    "similarly , the hidden and output layers are connected through the activation function @xmath91 with bias @xmath92 and weights @xmath93 , where @xmath94 .",
    "denote the noises injected to hidden node @xmath95 during the training of @xmath96 by @xmath97 .",
    "given an input vector @xmath47 , the predicted output from the nn is @xmath98 where @xmath99 .",
    "@xmath37 and @xmath100 are estimated by minimizing the empirical loss that measures the distance between the observed and predicted outcomes @xmath101 ni in the input and hidden layers will lead to a change in the predicted output , which is @xmath102 that is approximated , through the first - order taylor expansion at @xmath47 , by @xmath103 where @xmath104 and @xmath105 .",
    "[ def : s ] sensitivity of a nn model @xmath106 is defined as the ratio between the variance of @xmath107 and the variance of the total noise injected per case during the training of the nn , @xmath108 where @xmath109 , @xmath110 is given in eqn ( [ eqn : deltay2 ] ) and @xmath111 is a column vector with @xmath112 1 s .    [ rem3 ] in a deep nn that contains multiple hidden layers and more than one output nodes , the only modification to eqns ( [ eqn : deltay1 ] ) and ( [ eqn : deltay2 ] ) and definition [ def : s ] is the inclusion of more partial derivatives terms of those layers activation functions and expressing @xmath107 as a vector .",
    "as such , theorem [ thm : sensitivity ] holds in the general framework of deep nns .",
    "[ thm : sensitivity ] minimizing the loss function in eqn ( [ eqn : loss ] ) in a deep nn coupled with the whiteout procedure is first - order equivalent to minimizing simultaneously the loss function with and the sensitivity of the nn model .",
    "@xmath113    the proof of theorem [ thm : sensitivity ] is given in appendix [ app : sensitivity ] .",
    "as demonstrated in sections [ sec : regularization ] and [ sec : sensitivity ] , whiteout can be regarded as a technique to mitigate the over - fitting problem and to improve the generalization of a trained nn model with an additional penalization term . in this section ,",
    "we establish theoretically that the noise - perturbed empirical loss function with whiteout converges almost surely to the ideal loss function , and the estimates of nn parameters obtained from minimizing the former loss function are consistent with those obtained from minimizing the ideal loss function .",
    "we also investigate the boundary properties of the empirical loss functions with a finite @xmath8 , which is important from a practical implementation perspective when whiteout is incorporated in the bp algorithm .",
    "denote the true and unknown underlying distribution of @xmath114 by @xmath115 from which training data @xmath116 are sampled .",
    "denote the assumed relation between input @xmath117 and output @xmath118 by @xmath119 . in the framework of nns",
    ", @xmath120 is a composition of activation functions connecting the nodes between layers , with parameters @xmath37 and @xmath121 . per the universal approximation theorem @xcite",
    ", a feed - forward nn is an universal approximator for any function under some mild assumptions .",
    "we start with defining several types of loss functions , and assume the nn model @xmath119 is the same across the loss functions .",
    "* the _ ideal loss function ( ilf ) _ is @xmath122 .",
    "@xmath123 is not computable with unknown @xmath115 .",
    "* the _ empirical loss function ( elf ) _ is the realized version of ilf in data @xmath124 : + @xmath125 . *",
    "the _ perturbed empirical loss function ( pelf ) _ in data @xmath124 is + @xmath126 , where @xmath127 is the number of epochs in a deep learning algorithm ( e.g. , bp ) and @xmath128 represents the collective noises added to case @xmath15 ( in both input and hidden layers ) in the @xmath129 epoch during training . *",
    "the _ noise - marginalized empirical loss function ( nm - pelf ) _ is the expectation of pelf over the distribution of noise : @xmath130 .",
    "nm - pelf can be interpreted as training a nn model by minimizing the perturbed empirical loss function with a finite @xmath8 but with infinite number of epochs ( @xmath131 ) . *",
    "the _ marginalized perturb empirical loss function ( m - pelf ) _ is the expectation of nm - pelf over the distribution of data : @xmath132 .",
    "since @xmath133 is often unknown , either assumed parameter distribution , or nonparametric density estimation can be used to approximate @xmath133 .",
    "minimizes nm - pelf @xmath134 is a regularized minimization of elf with an added penalty term to mitigate overfitting ( sections [ sec : regularization ] and [ sec : sensitivity ] ) . in the practical implementation of the deep learning algorithm ,",
    "since the number of epochs @xmath127 is finite , the whiteout procedure , eventually leads to minimization of pelf @xmath135 rather than nm - pelf @xmath134 ( requires @xmath131 ) . in what follows , we establish the convergence of @xmath135 to @xmath134 ( lemmas [ lem : pointwise1 ] and [ lem : as1 ] ) , the convergence of @xmath134 to @xmath136 ( lemma lemma [ lem : as2 ] ) , and eventually @xmath135 to @xmath136 ( [ lem : as3 ] ) .",
    "the proof of the lemmas are provided in the appendix [ app : pointwise1 ] to appendix [ app : as2 ] .",
    "[ lem : pointwise1 ] * pointwise convergence of pelf to nm - pelf : * there exists an upper bound dimension - free lipschtz constant @xmath137 ( @xmath138 ) on @xmath139 , that is independent of nn model parameters @xmath140 , such that @xmath141    since @xmath137 is independent of the values and dimension of @xmath140 , the upper bound given in lemma [ lem : pointwise1 ] is uniform with regard to @xmath140 , and thus guarantees that as @xmath131 , the difference between pelf @xmath135 and nm - pelf @xmath134 approaches @xmath142 with probability @xmath143 .",
    "note that a dimension - free lipschtz constant @xmath137 only exists for strictly log - concave distributions such as gaussian distribution , hence the tail bound of in the pelf is much tighter than that of shakeout where bernoulli noise in injected , given that the variance of the noise terms between the two are comparable .    [ lem : as1 ] * almost sure convergence of pelf to nm - pelf : * @xmath135 converges to @xmath144 almost surely as @xmath145 .",
    "@xmath146    [ lem : as2 ] * almost sure convergence of nm - pelf to m - pelf : * @xmath134 converges to @xmath136 uniformly as @xmath147 .",
    "@xmath148 where @xmath149 is the dimension of the nn model ( dimension of @xmath150 ) .    by the the triangle inequality , @xmath151 . with lemma [ lem : as1 ] and lemma [ lem : as2 ]",
    "established , we can easily obtain the almost sure convergence of @xmath152 to @xmath153 .",
    "[ lem : as3 ] * almost sure convergence of pelf to m - pelf : * @xmath135 converges to @xmath136 uniformly as @xmath131 and @xmath154 @xmath155    estimates of @xmath150 and thus predicted @xmath35 obtained from minimizing m - pelf @xmath156 will always be different from those obtained by minimizing elf @xmath157 regardless of @xmath8 and @xmath127 , due to the regularization effect . given this ,",
    "the variance @xmath31 of the gaussian distribution in whiteout should kept small so that the estimates of @xmath150 wo nt deviate much from an ideal local optimum .",
    "we present below a sufficient but not necessary condition in lemma [ lem : bound ] , a bound on @xmath31 , to achieve consistency between pelf @xmath158 and ilf @xmath159 as @xmath160 and @xmath131 . the proof is provided in appendix [ app : bound ] .",
    "[ lem : bound ] suppose that @xmath149 is a class-@xmath161 kernel ( @xmath162 ) .",
    "let @xmath163 denote be the true density of random variable @xmath117 ( of dimension @xmath164 ) whose weak partial derivatives @xmath165 are integrable ( @xmath166 ) , and @xmath167 is the kernel smoothed density estimate of @xmath117 .",
    "assume that for some @xmath168 and @xmath169 .",
    "when @xmath161 is even and @xmath170 where @xmath171 as @xmath160 .",
    "if @xmath31 of the whiteout gaussian noise satisfies eqn ( [ eqn : sigma ] ) , and lemma[lem : as3 ] holds , then @xmath150 estimated by minimizing @xmath172 ( pelf ) are consistent with the estimates from minimizing @xmath123 ( ilf ) as @xmath160 and @xmath131 .",
    "we present in this section the algorithmic realization of whiteout in the context of the bp procedure in deep nns .",
    "let @xmath173 and @xmath174 denote the number nodes in layer @xmath10 and layer @xmath13 , respectively , where @xmath175 and @xmath176 is the total number of fully - connected layers .",
    "weight @xmath177 connects the @xmath178 node in layer @xmath10 and the @xmath129 node in layer @xmath13 . the _ training loss _ is @xmath179 , where @xmath180 is the predicted label via the learned nn and @xmath181 is the observed label in case @xmath127 ; and the activation function between two layers is denoted by @xmath119 .",
    "the detailed steps in the bp algorithm with whiteout are illustrated with additive gen noise @xmath182 in table [ tab : bpsteps ] .",
    "the steps are similar with multiplicative noise , except for the calculations of @xmath183 and @xmath184 , which are provided in the table footnote .",
    "p0.95 * input * : learning step : @xmath185 ; tuning parameters in whiteout gaussian noise : @xmath186 + 1 . *",
    "feed forward ( ff ) * : +  sample @xmath187 from @xmath188 : +  calculate @xmath189 + 2 . *",
    "back propagation * : +  @xmath190 , where @xmath191 for + @xmath192 , and @xmath193 for @xmath194 +  weight update : @xmath195 , where @xmath196 + @xmath197 +  bias update : @xmath198 , where @xmath199 + * output * : final estimates of @xmath150 after @xmath127 epochs of ff / bp in all @xmath127 cases +   +    as can be seen in table [ tab : bpsteps ] , the bp algorithm with whiteout involves calculation of many derivatives during training , which can be time consuming and prone to large rounding errors .",
    "a computationally more efficient way , accompanied with improved accuracy , is to calculate the regular bp derivatives and the noise related partial derivatives separately , suggested by proposition [ pro ] ( the proof is provided in appendix [ app : pro ] ) . in other words , we can think of whiteout ni as superimposing a `` noise nn '' onto the `` data nn '' . during the bp step in an epoch iteration , the two superimposed",
    "nns receive the same values passed down from the higher layers , and calculate the partial derivatives in their respective cases , which are then combined only to lead to the same derivatives as those listed in table [ tab : bpsteps ] .",
    "[ pro ] let the left superscript below denote the two `` superimposed '' nns : 1 is the the ",
    "data nn and 2 refers to the `` noise '' nn , then the derivatives involved in the computation of @xmath200 and updating of weights and bias in whiteout - augmented bp algorithm can be achieved by computation by part in the two `` superimposed '' nns . @xmath201",
    "in this section , we compare whiteout with the regular bp , dropout and shakeout using the hand written digits data mnist . we employed the same nn structure in @xcite ( table [ tab : exp1nn ] ) to facilitate the comparisons across the different algorithms .",
    "layer 1 and layer 2 were convolutionart layers followed by relu nonlinear activation and max - pooling ; layer 3 and layer 4 were fully - connected layers .",
    "dropout , shakeout , and whiteout were applied to hidden nodes in the fully connected layers ( layers 3 and 4 ) .",
    ".adopted nn model in the mnist data ( reproduced from @xcite ) [ cols=\"<,^,^,^,^\",options=\"header \" , ]     the evolvement of the classification accuracy in the training data set during the iterations of the regular bp , dropout , shakeout , and whiteout",
    "algorithms are depicted in figure [ fig : result ] .",
    "the error stabilized in all algorithm after a certain number of epochs , but to different error rate level .",
    "whiteout yielded the smallest error rate among all algorithms .",
    "the training loss was bounded in all algorithms , serving as empirical evidence on the feasibility of minimizing the pelf @xmath156 . since the magnitude of noise injected in shakeout and whiteout depended on the weights being updated in each iteration ( adaptive ) , there was more fluctuation around error rates and training loss compared to the regular bp ( no ni ) and dropout ( dropping nodes at a constant rate )",
    "; whiteout nevertheless fluctuated much less than shakeout because of the tighter bound ( lemma [ lem : pointwise1 ] ) .",
    "whiteout is a flexible and efficient approach to regulate overfitting and improve generalization and prediction in deep nns .",
    "whiteout is associated with an optimization objective function in the context of glms with a closed - form penalty term that includes lasso , ridge regression , adaptive lasso , and elastic net as special cases , and can also incorporate group structures among the features .",
    "whiteout can also be viewed as robust learning of nn models in the presence of small and insignificant perturbations in the input and hidden nodes .",
    "computationally , whiteout can be incorporated in the popular iterative bp algorithm in deep learning , where whiteout noises are sampled from gaussian distributions with variance terms adaptive to the weights trained up to the latest epoch .",
    "contrast to shakeout where the derivatives of sign functions are approximated , our proposed algorithm with whiteout does not involve such as approximation . in terms of performance , whiteout has better prediction performance than dropout , when training data are relatively small in size ; compared to shakeout , the penalized objective function in whiteout is more stable and has better convergence behaviors during training .",
    "we established the almost sure convergence of noise - perturbed empirical loss function to the ideal loss function as the number of epochs and the size of the training data approach infinity , and the consistency of estimated parameters in a trained nn under mild assumptions .",
    "future work include examination of convergence rates of the noise - perturbed empirical loss function to the ideal loss function , and applications of whiteout in more real - life data sets .",
    "@xmath202    a second -order taylor expansion of @xmath58 around @xmath203 and taking expectation on the approximation with regardless to the distribution of noise leads to @xmath204      proof of lemma [ lem : pointwise1 ] employs concentration inequality of independent variables @xcite .",
    "let @xmath206 be a vector of @xmath8 independent standard gaussian variables , and @xmath207 be @xmath176-lipschitz continuous with respect to the euclidean norm . then @xmath208 is sub - gaussian with parameter at most @xmath176 , and for any @xmath209 @xmath210 the gaussian noise injected through whiteout per case is @xmath211 of dimension @xmath212 ( @xmath7 is the number of nodes in layer @xmath10 and @xmath213 is the number of nodes in layer @xmath13 ) with mean @xmath214 and covariance @xmath215 . to utilize concentration inequality , we re - write @xmath216 , where @xmath217 is a standard gaussian vector containing @xmath218 independent elements . in other words",
    ", @xmath53 is linear function of @xmath219 ( since @xmath53 appears inside @xmath119 in the inner product @xmath220 , working with @xmath219 wo nt change lipschitz continuity of the nn model @xmath119 ) .",
    "@xmath119 considered in the context of whiteout is pelf @xmath152 , a composition of lipschitz continuous activation functions , with the output function uniformly bounded . to determine the lipschitz constant @xmath176",
    ", it suffices to bound weights @xmath37 such as gradient @xmath139 is bounded . in the case of gallr noise , weights are naturally bounded due to the constraints imposed by the @xmath0 and @xmath0 regularization .",
    "we denote upper bound of @xmath139 as @xmath137 , which is independent the dimension and values of @xmath140 , and thus @xmath221 per concentration inequality .      since @xmath222 and @xmath223 is a concave function of the empirical measure defined by @xmath53 ,",
    "hence it is a backward super - martingale , and thus converges almost surely toward a random variable as @xmath131 . by the law of large numbers , @xmath224 .",
    "@xmath225    by the law of large numbers , @xmath226 almost surely . taken together",
    ", @xmath227      expectation of the loss function with regard to random variable @xmath228 can be seen as averaging over infinitely many samples of @xmath229 where @xmath230 .",
    "the average becomes smoothed bootstrap with each noise injected as a kernel . in the feed forward process between layers",
    "@xmath10 and @xmath13 , each iteration is thus a smoothed bootstrap sampling from layer @xmath10 .",
    "@xcite showed that in order for the difference in the predicted outcomes from minimizing two loss functions asymptotically approaching @xmath142 , we only need the difference between the true density @xmath163 of @xmath228 and its kernel smoothed density @xmath167 asymptotically approaching @xmath142 . under the mild assumption that the @xmath231 belongs to sobolev space , the weak partial derivatives @xmath232",
    "of which are integrable .",
    "@xmath233 where @xmath7 is the dimension of @xmath7 . by the basic properties of gaussian kernels ( @xmath234 ) ,",
    "it is trivial to show that all integrations in eqn ( [ eqn : app ] ) exist and with an upper bound that is a function of @xmath7 .",
    "as mentioned in section [ sec : bp ] , we can view whiteout ni as a `` noise '' nn superimposed over the `` data '' nn . let the left superscript below denote the two nns ( 1 is the the data nn and 2 refers to the noise nn ) , then @xmath235 the feed forward process calculates @xmath236 and @xmath237 @xmath238 after the calculation of @xmath236 and @xmath237 , the bp process updates weights as in @xmath239 bias as in @xmath240 and continues to the lower layer as in @xmath241"
  ],
  "abstract_text": [
    "<S> noise injection is an off - the - shelf method to mitigate over - fitting in neural networks ( nns ) . </S>",
    "<S> the recent developments in bernoulli noise injection as implemented in the dropout and shakeout procedures demonstrates the efficiency and feasibility of noise injection in regularizing deep nns . </S>",
    "<S> we propose whiteout , a new regularization technique via injection of adaptive gaussian noises into a deep nn . </S>",
    "<S> whiteout offers three tuning parameters , offering flexibility during training of nns . </S>",
    "<S> we show that whiteout is associated with a deterministic optimization objective function in the context of generalized linear models with a closed - form penalty term and includes lasso , ridge regression , adaptive lasso , and elastic net as special cases . </S>",
    "<S> we also demonstrate that whiteout can be viewed as robust learning of nn model in the presence of small and insignificant perturbations in input and hidden nodes . </S>",
    "<S> compared to dropout , whiteout has better performance when training data of relatively small sizes with the sparsity introduced through the @xmath0 regularization . </S>",
    "<S> compared to shakeout , the penalized objective function in whiteout has better convergence behaviors and has a tighter bound for tail probabilities . </S>",
    "<S> we establish theoretically that the noise - perturbed empirical loss function with whiteout converges almost surely to the ideal loss function , and the estimates of nn parameters obtained from minimizing the former loss function are consistent with those obtained from minimizing the ideal loss function . </S>",
    "<S> computationally , whiteout can be incorporated in the back - propagation algorithm and is computationally efficient . </S>",
    "<S> the superiority of whiteout over dropout and shakeout in training nns in classification is demonstrated using the mnist data .    </S>",
    "<S> * keywords * : ( adaptive ) lasso ; elastic net ; regularization ; robustness ; consistency ; backpropagation </S>"
  ]
}