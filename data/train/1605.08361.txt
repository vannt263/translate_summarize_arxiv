{
  "article_text": [
    "multilayer neural networks ( mnns ) have achieved state - of - the - art performances in many areas of machine learning @xcite .",
    "this success is typically achieved by training complicated models , using a simple stochastic gradient descent ( sgd ) method , or one of its variants .",
    "however , sgd is only guaranteed to converge to critical points in which the gradient of the expected loss is zero @xcite , and , specifically , to stable local minima @xcite ( this is true also for regular gradient descent @xcite ) .",
    "since loss functions parametrized by mnn weights are non - convex , it has long been a mystery why does sgd work well , rather than converging to `` bad '' local minima , where the training error is high ( and thus also the test error is high ) .",
    "previous results ( section [ sec : related - work ] ) suggest that the training error at all local minima should be low , if the mnns have extremely wide layers .",
    "however , such wide mnns would also have an extremely large number of parameters , and serious overfitting issues .",
    "moreover , current state of the art results are typically achieved by deep mnns @xcite , rather then wide .",
    "therefore , we are interested to provide training error guarantees at a more practical number of parameters .    as a common rule - of - the - thumb ,",
    "a multilayer neural network should have at least as many parameters as training samples , and use regularization , such as dropout @xcite to reduce overfitting .",
    "for example , alexnet @xcite had 60 million parameters and was trained using 1.2 million examples .",
    "such over - parametrization regime continues in more recent works , which achieve state - of - the - art performance with very deep networks @xcite .",
    "these networks are typically under - fitting @xcite , which suggests that the training error is the main bottleneck in further improving performance .    in this work we focus on mnns with a single output and leaky rectified linear units .",
    "we provide a guarantee that the training error is zero in every differentiable local minimum ( dlm ) , under mild over - parametrization , and essentially for every data set . with one hidden layer ( theorem [ thm : committee machine ] )",
    "we show that the training error is zero in all dlms , whenever the number of weights in the first layer is larger then the number of samples @xmath0 , _",
    "i.e. _ , when @xmath1 , where @xmath2 is the width of the activation @xmath3-th layer .",
    "for mnns with @xmath4 layers we show that , if @xmath5 , then convergence to potentially bad dlms ( in which the training error is not zero ) can be averted by using a small perturbation to the mnn s weights and then fixing all the weights except the last two weight layers ( corollary [ cor : mnn instability ] ) .",
    "a key aspect of our approach is the presence of a multiplicative dropout - like noise term in our mnns model .",
    "we formalize the notion of validity for essentially every dataset by showing that our results hold almost everywhere with respect to the lebesgue measure over the data and this noise term .",
    "this approach is commonly used in smoothed analysis of algorithms , and often affords great improvements over worst - case guarantees ( _ e.g. _ , @xcite ) .",
    "intuitively , there may be some rare cases where our results do not hold , but almost any infinitesimal perturbation of the input and activation functions will fix this .",
    "thus , our results assume essentially no structure on the input data , and are unique in that sense .",
    "at first , it may seem hopeless to find any training error guarantee for mnns .",
    "since the loss of mnns is highly non - convex , with multiple local minima @xcite , it seems reasonable that optimization with sgd would get stuck at some bad local minimum .",
    "moreover , many theoretical hardness results ( reviewed in @xcite ) have been proven for mnns with one hidden layer .    despite these results",
    ", one can easily achieve zero training error @xcite , if the mnn s last hidden layer has more units than training samples ( @xmath6 ) .",
    "this case is not very useful , since it results in a huge number of weights ( larger than @xmath7 ) , leading to strong over - fitting .",
    "however , such wide networks are easy to optimize , since by training the last layer we get to a global minimum ( zero training error ) from almost every random initialization @xcite .",
    "qualitatively similar training dynamics are observed also in more standard ( narrower ) mnns .",
    "specifically , the training error usually descends on a single smooth slope path with no `` barriers''@xcite , and the training error at local minima seems to be similar to the error at the global minimum @xcite .",
    "the latter was explained in @xcite by an analogy with high - dimensional random gaussian functions , in which any critical point high above the global minimum has a low probability to be a local minimum .",
    "a different explanation to the same phenomenon was suggested by @xcite .",
    "there , a mnn was mapped to a spin - glass ising model , in which all local minima are limited to a finite band above the global minimum .    however , it is not yet clear how relevant these statistical mechanics results are for actual mnns and realistic datasets .",
    "first , the analogy in @xcite is qualitative , and the mapping in @xcite requires several implausible assumptions ( _ e.g. _ , independence of inputs and targets ) .",
    "second , such statistical mechanics results become exact in the limit of infinite parameters , so for a finite number of layers , each layer should be infinitely wide .",
    "however , extremely wide networks may have serious over - fitting issues , as we explained before .",
    "previous works have shown that , given several limiting assumptions on the dataset , it is possible to get a low training error on a mnn with one hidden layer : @xcite proved convergences for linearly separable datasets ; @xcite either required that @xmath8 , or clustering of the classes . going beyond training error , @xcite showed that mnns with one hidden layer can learn low order polynomials , under a product of gaussians distributional assumption on the input .",
    "also , @xcite devised a tensor method , instead of the standard sgd method , for which mnns with one hidden layer are guaranteed to approximate arbitrary functions .",
    "note , however , the last two works require a rather large @xmath0 to get good guarantees .",
    "[ [ model . ] ] model .",
    "+ + + + + +    we examine a multilayer neural network ( mnn ) optimized on a finite training set @xmath9 , where @xmath10\\in\\mathbb{r}^{d_{0}\\times n}$ ] are the input patterns , @xmath11\\in\\mathbb{r}^{1\\times n}$ ] are the target outputs ( for simplicity we assume a scalar output ) , and @xmath0 is the number of samples .",
    "the mnn has @xmath12 layers , in which the layer inputs @xmath13 and outputs @xmath14 ( a component of @xmath15 is denoted @xmath16 ) are given by @xmath17 where * @xmath18 * is the input of the network , @xmath19 are the weight matrices ( a component of @xmath20 is denoted @xmath21 , bias terms are ignored for simplicity ) , and @xmath22 are piecewise constant activation slopes defined below .",
    "we set @xmath23 $ ] .",
    "[ [ activations . ] ] activations .",
    "+ + + + + + + + + + + +    many commonly used piecewise - linear activation functions ( _ e.g. _ , _ _ rectified linear unit , maxout , max - pooling ) can be written in the matrix product form in eq . .",
    "we consider the following relationship : @xmath24 when @xmath25=\\mathbf{1}$ ] we recover the common leaky rectified linear unit ( leaky relu ) nonlinearity , with some fixed slope @xmath26 .",
    "the matrix @xmath27 can be viewed as a realization of dropout noise  in most implementations @xmath28 is distributed on a discrete set ( _ e.g. _ , @xmath29 ) , but competitive performance is obtained with continuous distributions ( _ e.g. _ gaussian ) @xcite .",
    "our results apply directly to the latter case .",
    "the inclusion of @xmath27 is the innovative part of our model  by performing smoothed analysis jointly on @xmath30 and @xmath31 we are able to derive strong training error guarantees .",
    "however , our use of dropout is purely a proof strategy ; we never expect dropout to reduce the training error in realistic datasets .",
    "this is further discussed in sections [ sec : numerical - experiments ] and [ sec : discussion ] .",
    "[ [ measure - theoretic - terminology ] ] measure - theoretic terminology + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    throughout the paper , we make extensive use of the term @xmath32-almost everywhere , or a.e . for short .",
    "this is taken to mean , almost everywhere with respect of the lebesgue measure on all of the entries of @xmath33 .",
    "a property hold a.e . with respect to some measure , if the set of objects for which it does nt hold has measure 0 .",
    "in particular , our results hold with probability 1 whenever @xmath31 is taken to have i.i.d .",
    "gaussian entries , and arbitrarily small gaussian i.i.d .",
    "noise is used to smooth the input @xmath34 .",
    "[ [ loss - function . ] ] loss function .",
    "+ + + + + + + + + + + + + + +    we denote @xmath35 as the output error , where @xmath36 is output of the neural network with @xmath37 , * @xmath38 $ ] * , and @xmath39 as the empirical expectation over the training samples .",
    "we use the mean square error , which can be written as one of the following forms @xmath40 the loss function depends on @xmath41 , @xmath31 , and on the entire weight vector @xmath42^{\\top}\\in\\mathbb{r}^{\\omega}$ ] , where @xmath43 is the flattened weight matrix of layer @xmath3 , and @xmath44 is total number of weights .",
    "mnns are typically trained by minimizing the loss over the training set , using stochastic gradient descent ( sgd ) , or one of its variants ( _ e.g. _ , adam @xcite ) . in this section and",
    "the next , we guarantee zero training loss in the common case of an over - parametrized mnn .",
    "we do this by analyzing the properties of differentiable local minima ( dlms ) of the mse ( eq . ) .",
    "we focus on dlms , since under rather mild conditions @xcite , sgd asymptotically converges to dlms of the loss ( for finite @xmath0 , a point can be non - differentiable only if @xmath45 ) .",
    "we first consider a mnn with one hidden layer @xmath46 .",
    "we start by examining the mse at a dlm @xmath47 to simplify notation , we absorb the redundant parameterization of the weights of the second layer into the first @xmath48 , obtaining @xmath49    note this is only a simplified notation  we do not actually change the weights of the mnn , so in both equations the activation slopes remain the same , _",
    "i.e. _ , @xmath50 . if there exists an infinitesimal perturbation which reduces the mse in eq . , then there exists a corresponding infinitesimal perturbation which reduces the mse in eq . . therefore , if @xmath51 is a dlm of the mse in eq .",
    ", then @xmath52 must also be a dlm of the mse in eq . .",
    "clearly , both dlms have the same mse value .",
    "therefore , we will proceed by assuming that @xmath52 is a dlm of eq .",
    ", and any constraint we will derive for the mse in eq . will automatically apply to any dlm of the mse in eq . .",
    "if we are at a dlm of eq .",
    ", then its derivative is equal to zero . to calculate this derivative we rely on two facts .",
    "first , we can always switch the order of differentiation and expectation , since we average over a finite training set .",
    "second , at any a differentiable point ( and in particular , a dlm ) , the derivative of @xmath53 with respect to the weights is zero .",
    "thus , we find that , at any dlm , @xmath54=0\\,.\\label{eq : linear regression}\\ ] ] to reshape this gradient equation to a more convenient form , we denote kronecker s product by @xmath55 , and define the `` gradient matrix '' ( without the error @xmath56 ) @xmath57\\in\\mathbb{r}^{d_{0}d_{1}\\times n}\\,,\\label{eq : g}\\ ] ] where @xmath58 denotes the khatari - rao product ( _ cf .",
    "_ @xcite , @xcite ) . using this notation , and recalling that * @xmath59,$ ] * eq . becomes @xmath60 therefore , @xmath61 lies in the right nullspace of @xmath62 , which has dimension @xmath63 .",
    "specifically , if @xmath64 , the only solution is @xmath65 .",
    "this immediately implies the following lemma .",
    "[ lem : full rank ( g)]suppose we are at some dlm of of eq . .",
    "if @xmath64 , then @xmath66 .    to show that @xmath62 has , generically , full column rank , we state the following important result , which a special case of ( * ? ? ? * lemma 13 ) ,    [ fact : khatri - rao multiplies rank]for @xmath67 and @xmath68 with @xmath69 , we have , @xmath70 almost everywhere , @xmath71",
    "however , since @xmath72 depends on @xmath41 , we can not apply eq . directly to @xmath73 .",
    "instead , we apply eq . for all",
    "( finitely many ) possible values of @xmath74 ( appendix [ subsec : single - hidden - layer proof ] ) , and obtain    [ lem : when g full rank?]for @xmath75 , if @xmath76 , then simultaneously for every @xmath77 , @xmath78 , @xmath79 almost everywhere .",
    "combining lemma [ lem : full rank ( g ) ] with lemma [ lem : when g full rank ? ] , we immediately have    [ thm : committee machine]if @xmath76 , * * then all differentiable local minima of eq .",
    "are global minima with @xmath66 , @xmath79 almost everywhere .",
    "note that this result is tight , in the sense that the minimal hidden layer width @xmath80 , is exactly the same minimal width which ensures a mnn can implement any dichotomy @xcite for inputs in general position .",
    "we examine the implications of our approach for mnns with more than one hidden layer . to find the dlms of a general mnn , we again need to differentiate the mse and equate it to zero . as in section",
    "[ sec : single hidden layer ] , we exchange the order of expectation and differentiation , and use the fact that @xmath81 are piecewise constant . differentiating near a dlm with respect to @xmath82 , the vectorized version of * @xmath20 * , we obtain @xmath83=0\\label{eq : grad mse}\\ ] ] to calculate @xmath84 for to the @xmath3-th weight layer , we write . ]",
    "its input @xmath15 and its back - propagated `` delta '' signal ( without the error @xmath56 ) @xmath85 where we keep in mind that @xmath86 are generally functions of the inputs and the weights . using this notation",
    "we find @xmath87 thus , defining @xmath88;\\,\\mathbf{v}_{l}=\\left[\\mathbf{v}_{l}^{\\left(1\\right)},\\dots,\\mathbf{v}_{l}^{\\left(n\\right)}\\right]\\ ] ] we can re - formulate eq . as @xmath89\\in\\mathbb{r}^{d_{l-1}d_{l}\\times",
    "n}\\label{eq : gme=00003d0}\\ ] ] similarly to eq . the previous section .",
    "therefore , each weight layer provides as many linear constraints ( rows ) as the number of its parameters .",
    "we can also combine all the constraints and get @xmath90^{\\top}\\in\\mathbb{r}^{\\omega\\times n},\\label{eq : ge=00003d0}\\ ] ] in which we have @xmath91 constraints ( rows ) corresponding to all the parameters in the mnn .",
    "as in the previous section , if @xmath92 and @xmath93 we must have @xmath65 . however , it is generally difficult to find the rank of @xmath94 , since we need to find whether different @xmath95 have linearly dependent rows .",
    "therefore , we will focus on the last hidden layer and on the condition @xmath96 , which ensures @xmath65 , from eq .",
    ". however , since @xmath97 depends on the weights , we can not use our results from the previous section , and it is possible that @xmath98 .",
    "for example , when @xmath99 and @xmath4 , we get @xmath100 so we are at a differentiable critical point ( note it @xmath94 is well defined , even though @xmath101 ) , which is generally not a global minimum . intuitively , such cases seem fragile , since if we give @xmath77 any random perturbation , one would expect that `` typically '' we would have @xmath96 .",
    "we establish this idea by first proving the following stronger result ( appendix [ subsec : general - case- proof ] ) ,    [ thm : mnn instability]for @xmath5 and fixed values of @xmath102 , any differentiable local minimum of the mse ( eq . [ eq : mse ] ) as a function of @xmath103 and @xmath104 , is also a global minimum , with @xmath66 , @xmath105 almost everywhere .",
    "theorem [ thm : mnn instability ] means that for any ( lebesgue measurable ) random set of weights of the first @xmath106 layers , every dlm with respect to the weights of the last two layers is also a global minimum with loss 0 .",
    "note that the condition @xmath5 implies that @xmath103 has more weights then @xmath0 ( a plausible scenario , _",
    "e.g. _ , @xcite ) .",
    "in contrast , if , instead we were only allowed to adjust the last layer of a random mnn , then low training error can only be ensured with extremely wide layers ( @xmath107 , as discussed in section [ sec : related - work ] ) , which require much more parameters ( @xmath108 ) .",
    "theorem [ thm : mnn instability ] can be easily extended to other types of neural networks , beyond of the basic formalism introduced in section [ sec : preliminaries ] .",
    "for example , we can replace the layers below @xmath106 with convolutional layers , or other types of architectures .",
    "additionally , the proof of theorem [ thm : mnn instability ] holds ( with a trivial adjustment ) when @xmath109 are fixed to have identical nonzero entries  that is , with dropout turned off except in the last two hidden layers .",
    "the result continues to hold even when @xmath110 is fixed as well , but then the condition @xmath5 has to be weakened to @xmath111 .",
    "next , we formalize our intuition above that dlms of deep mnns must have zero loss or be fragile , in the sense of the following immediate corollary of theorem [ thm : mnn instability ] ,    [ cor : mnn instability]for @xmath5 , let @xmath77 be a differentiable local minimum of the mse ( eq . [ eq : mse ] ) . consider a new weight vector @xmath112 , where @xmath113 has i.i.d .",
    "gaussian ( or uniform ) entries with arbitrarily small variance",
    ". then , @xmath114 almost everywhere and with probability 1 w.r.t .",
    "@xmath115 , if @xmath116 are held fixed , all differentiable local minima of the mse as a function of @xmath103 and @xmath104 are also global minima , with @xmath66 .",
    "note that this result is different from the classical notion of linear stability at differentiable critical points , which is based on the analysis of the eigenvalues of the hessian @xmath117 of the mse .",
    "the hessian can be written as a symmetric block matrix , where each of its blocks @xmath118 corresponds to layers @xmath119 and @xmath3 . specifically , using eq .",
    ", each block can be written as a sum of two components @xmath120+\\e\\left[\\nabla_{\\mathbf{w}_{l}}e\\nabla_{\\mathbf{w}_{m}^{\\top}}e\\right]\\triangleq\\e\\left[e\\boldsymbol{\\lambda}_{ml}\\right]+\\frac{1}{n}\\mathbf{g}_{m}\\mathbf{g}_{l}^{\\top}\\,,\\label{eq : hessian-1}\\ ] ] where , for @xmath121 @xmath122 while @xmath123 , and @xmath124 for @xmath125 . combining all the blocks , we get @xmath126+\\frac{1}{n}\\mathbf{g}\\mathbf{g}^{\\top}\\in\\mathbb{r}^{\\omega\\times\\omega}\\,.\\ ] ]",
    "if we are at a dlm , then @xmath117 is positive semi - definite .",
    "if we examine again the differentiable critical point @xmath99 and @xmath4 , we see that @xmath127 , so it is not a strict saddle . however , this point is fragile in the sense of corollary [ cor : mnn instability ] .",
    "interestingly , the positive semi - definite nature of the hessian at dlms imposes additional constraints on the error .",
    "note that the matrix @xmath128 is symmetric positive semi - definite of relatively small rank @xmath129 .",
    "however , @xmath130 $ ] can potentially be of high rank , and thus may have many negative eigenvalues ( the trace of @xmath130 $ ] is zero , so the sum of all its eigenvalues is also zero ) .",
    "therefore , intuitively , we expect that for @xmath117 to be positive semi - definite , @xmath61 has to become small , generically ( _ i.e. _ , except at some pathological points such as * @xmath99 * ) .",
    "this is indeed observed empirically ( * ? ? ?",
    "* fig 1 ) .",
    "in this section we examine numerically our main results in this paper , theorems [ thm : committee machine ] and [ thm : mnn instability ] , which hold almost everywhere with respect to the lebesgue measure over the data and dropout realization .",
    "however , without dropout , this analysis is not guaranteed to hold .",
    "for example , our results do not hold in mnns where all the weights are negative , so @xmath131 has constant entries and therefore @xmath132 can not have full rank .    nonetheless , if the activations are sufficiently `` variable '' ( formally , @xmath132 has full rank ) , then we expect our results to hold even without dropout noise and with the leaky relu s replaced with basic relu s ( @xmath133 ) .",
    "we tested this numerically and present the result in figure [ fig : training error ] .",
    "we performed a binary classification task on a synthetic random dataset and subsets of the mnist dataset , and show the mean classification error ( mce , which is the fraction of samples incorrectly classified ) , commonly used at these tasks .",
    "note that the mnist dataset , which contains some redundant information between training samples , is much easier ( a lower error ) than the completely random synthetic data .",
    "thus the performance on the random data is more representative of the `` typical worst case '' , _ _",
    "( _ i.e. _ , hard yet non - pathological input ) , which our smoothed analysis approach is aimed to uncover .    for one hidden layer",
    ", the error goes to zero when the number of non - redundant parameters is greater than the number of samples ( @xmath134 ) , as predicted by theorem [ thm : committee machine ] .",
    "theorem [ thm : mnn instability ] predicts a similar behavior when @xmath134 for a mnn with two hidden layers ( note we trained all the layers of the mnn ) .",
    "this prediction also seems to hold , but less tightly .",
    "this is reasonable , as our analysis in section [ sec : general - case ] suggests that typically the error would be zero if the total number of parameters is larger the number of training samples ( @xmath135 ) , though this was not proven .",
    "we note that in all the repetitions in figure [ fig : training error ] , for @xmath136 , the matrix @xmath132 always had full rank .",
    "however , for smaller mnns than shown in figure [ fig : training error ] ( about @xmath137 ) , sometimes @xmath132 did not have full rank .",
    "recall that theorems [ thm : committee machine ] and [ thm : mnn instability ] both give guarantees only on the training error at a dlm .",
    "however , for finite @xmath0 , since the loss is non - differentiable at some points , it is not clear that such dlms actually exist , or that we can converge to them . to check if this is indeed the case , we performed the following experiment .",
    "we trained the mnn for many epochs , using batch gradient steps .",
    "then , we started to gradually decrease the learning rate .",
    "if the we are at dlm , then all the activation inputs @xmath138 should converge to a distinctly non - zero value , as demonstrated in figure [ fig : the - existence - of - dlms ] . in this figure , we tested a small mnn on synthetic data , and all the neural inputs seem to remain constant on a non - zero value , while the mse keeps decreasing .",
    "this was the typical case in our experiments .",
    "however , in some instances , we would see some @xmath138 converge to a very low value ( @xmath139 ) .",
    "this may indicate that convergence to non - differentiable points is possible as well .",
    "[ [ implementation - details ] ] implementation details + + + + + + + + + + + + + + + + + + + + + +    weights were initialized to be uniform with mean zero and variance @xmath140 , as suggested in @xcite . in each epoch",
    "we randomly permuted the dataset and used the adam @xcite optimization method ( a variant of sgd ) with @xmath141 . in figure",
    "[ fig : training error ] the training was done for no more than @xmath142 epochs ( we stopped if @xmath143 was reached ) .",
    "different learning rates and mini - batch sizes were selected for each dataset and architecture .",
    "in this work we provided training error guarantees for mildly over - parameterized mnns at all differentiable local minima ( dlm ) . for a single hidden layer ( section [ sec : single hidden layer ] )",
    ", the proof is surprisingly simple .",
    "we show that the mse near each dlm is locally similar to that of linear regression ( _ i.e. _ , a single linear neuron ) .",
    "this allows us to prove ( theorem [ thm : committee machine ] ) that , almost everywhere , if the number of non - redundant parameters @xmath144 is larger then the number of samples @xmath0 , then all dlms are a global minima with @xmath66 , as in linear regression . with more then one hidden layers , theorem [ thm : mnn instability ]",
    "states that if @xmath5 ( _ i.e. _ , so @xmath103 has more weights than @xmath0 ) then we can always perturb and fix some weights in the mnn so that all the dlms would again be global minima with @xmath66 .    note that in a realistic setting , zero training error should not necessarily be the intended objective of training , since it may encourage overfitting .",
    "our main goal here was to show that that essentially all dlms provide good training error ( which is not trivial in a non - convex model ) .",
    "however , one can decrease the size of the model or artificially increase the number of samples ( _ e.g. _ , using data augmentation , or re - sampling the dropout noise ) to be in a mildly under - parameterized regime , and have relatively small error , as seen in figure [ fig : training error ] .",
    "for example , in alexnet @xcite @xmath103 has @xmath145 weights , which is larger than @xmath146 , as required by theorem [ thm : mnn instability ] .",
    "however , without data augmentation or dropout , alexnet did exhibit severe overfitting .",
    "our analysis is non - asymptotic , relying on the fact that , near differentiable points , mnns with piecewise linear activation functions can be differentiated similarly to linear mnns @xcite .",
    "we use a smoothed analysis approach , in which we examine the error of the mnn under slight random perturbations of worst - case input and dropout . our experiments ( figure [ fig : training error ] ) suggest that our results describe the typical performance of mnns , even without dropout .",
    "note we do not claim that dropout has any merit in reducing the training loss in real datasets  as used in practice , dropout typically trades off the training performance in favor of improved generalization .",
    "thus , the role of dropout in our results is purely theoretical .",
    "in particular , dropout ensures that the gradient matrix @xmath132 ( eq . ) has full column rank .",
    "it would be an interesting direction for future work to find other sufficient conditions for @xmath132 to have full column rank .",
    "many other directions remain for future work .",
    "for example , we believe it should be possible to extend this work to multi - output mnns and/or other convex loss functions besides the quadratic loss .",
    "our results might also be extended for stable non - differentiable critical points ( which may exist , see section [ sec : numerical - experiments ] ) using the necessary condition that the sub - gradient set contains zero in any critical point @xcite .",
    "another important direction is improving the results of theorem [ thm : mnn instability ] , so it would make efficient use of the all the parameters of the mnns , and not just the last two weight layers .",
    "such results might be used as a guideline for architecture design , when training error is a major bottleneck @xcite .",
    "last , but not least , in this work we focused on the empirical risk ( training error ) at dlms .",
    "such guarantees might be combined with generalization guarantees ( _ e.g. _ , @xcite ) , to obtain novel excess risk bounds that go beyond uniform convergence analysis .",
    "the authors are grateful to o. barak , d. carmon , y. han . , y. harel , r. meir , e. meirom , l. paninski , r. rubin , m. stern , u. smbl and a. wolf for helpful discussions .",
    "the research was partially supported by the gruss lipper charitable foundation , and by the intelligence advanced research projects activity ( iarpa ) via department of interior/ interior business center ( doi / ibc ) contract number d16pc00003 .",
    "government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon .",
    "disclaimer : the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , of iarpa , doi / ibc , or the u.s .",
    "government .    9    elizabeth  s. allman , catherine matias , and john  a. rhodes . . , 37(6 a):30993132 , 2009 .    a  andoni , r  panigrahy , g  valiant , and l  zhang .",
    ". in _ icml _ , 2014 .",
    "eric  b. baum . .",
    ", 4(3):193215 , 1988 .",
    "aditya bhaskara , moses charikar , ankur moitra , and aravindan vijayaraghavan . .",
    ", page  32 , 2013 .",
    "l  bottou . . ,",
    "pages 134 , 1998 .",
    "anna choromanska , mikael henaff , michael mathieu , grard  ben arous , and y  lecun . . ,",
    "38 , 2015 .",
    "yn  dauphin , razvan pascanu , and caglar gulcehre . .",
    ", pages 19 , 2014 .",
    "k.  fukumizu and s.  amari . .",
    ", 13:317327 , 2000 .",
    "ian  j. goodfellow , oriol vinyals , and andrew  m. saxe . . , 2015 .",
    "marco gori and alberto tesi . ,",
    "benjamin  d haeffele and ren vidal . .",
    ", ( 1):7 , 2015 .",
    "moritz hardt , benjamin recht , and y  singer . .",
    ", pages 124 , 2015 .",
    "k  he , x  zhang , s  ren , and j.  sun . .",
    ", 2015 .    kaiming he , xiangyu zhang , shaoqing ren , and jian sun . .",
    "in _ in proceedings of the ieee international conference on computer vision _ , pages 10261034 , 2015 .",
    "geoffrey  e hinton , nitish srivastava , alex krizhevsky , ilya sutskever , and ruslan  r. salakhutdinov . . ,",
    "pages 118 , 2012 .",
    "guang - bin huang , qin - yu zhu , and chee - kheong siew . .",
    ", 70(1 - 3):489501 , 2006 .",
    "m  janzamin , h  sedghi , and a  anandkumar . .",
    ", pages 125 , 2015 .",
    "diederik  p kingma and jimmy  lei ba . . ,",
    "pages 113 , 2015 .",
    "a  krizhevsky , i  sutskever , and g  e hinton . .",
    "in _ nips _ , 2012 .",
    "y  lecun , yoshua bengio , and geoffrey hinton . .",
    ", 521(7553):436444 , 2015 .",
    "y  lecun , l  bottou , y  bengio , and p  haffner . .",
    ", 86(11):22782323 , 1998 .",
    "jason  d. lee , max simchowitz , michael  i. jordan , and benjamin recht . . , 2016",
    "roi livni , s  shalev - shwartz , and ohad shamir . . , 2014",
    "nils  j. nilsson . .",
    "mcgraw - hill new york , 1965 .",
    "r  pemantle . .",
    ", 18(2):698712 , 1990 .",
    "r  t rockafellarf . .",
    ", 39(77):331355 , 1979 .",
    "itay safran and ohad shamir . . , 2015",
    "a  m saxe , j  l. mcclelland , and s  ganguli . , 2014 .",
    "jir sma . , 14(11):270928 , 2002 .",
    "daniel  a spielman and shang - hua teng . . ,",
    "52(10):7684 , 2009 .",
    "nitish srivastava , geoffrey  e. hinton , alex krizhevsky , ilya sutskever , and ruslan salakhutdinov . .",
    ", 15:19291958 , 2014 .",
    "bing xu , naiyan wang , tianqi chen , and mu  li . .",
    ", 2015 .    in this appendix",
    "we give the proofs for our main results in the paper . but first , we define some additional notation . recall that for every layer @xmath3 , data instance @xmath147 and index @xmath148 , the activation slope @xmath149 takes one of two values : @xmath28 or @xmath150(with @xmath26 ) .",
    "hence , for a single realization of @xmath151 $ ] , the matrix @xmath152\\in\\mathbb{r}^{d_{l}\\times n}$ ] can have up to @xmath153 distinct values , and the tuple @xmath154 can have at most @xmath155 distinct values .",
    "we will find it useful to enumerate these possibilities by an index @xmath156 which will be called the activation pattern .",
    "we will similarly denote @xmath157 to be the value of @xmath158 , given under activation pattern @xmath159 .",
    "lastly , we will make use of the following fact :    [ fact : intersect - a.e.]if properties @xmath160 hold almost everywhere , then @xmath161 also holds almost everywhere .",
    "we prove the following lemma [ lem : when g full rank ? ] , using the previous notation and results from section .    for @xmath75 ,",
    "if @xmath76 , then simultaneously for every @xmath77 , @xmath78 , @xmath79 almost everywhere .",
    "we fix an activation pattern @xmath159 and set @xmath162 .",
    "we apply eq . to conclude that @xmath163 , @xmath164-a.e . and",
    "hence also @xmath79-a.e",
    ".. we repeat the argument for all @xmath165 values of @xmath159 , and use fact [ fact : intersect - a.e . ] .",
    "we conclude that @xmath166 for all @xmath159 simultaneously , @xmath79-a.e .. since for every set of weights we have @xmath167 for some @xmath159 , we have @xmath64 , @xmath79-a.e .",
    "first we prove the following helpful lemma , using a technique similar to that of @xcite .",
    "let @xmath168 be a matrix with @xmath169 , with entries that are all polynomial functions of some vector @xmath170 .",
    "also , we assume that for some value @xmath171 , we have @xmath172 .",
    "then , for almost every @xmath170 , we have @xmath173 .",
    "[ lem : polynomial proof ]    there exists a polynomial mapping @xmath174 such that @xmath175 does not have full column rank if and only if @xmath176 .",
    "since @xmath177 we can construct @xmath178 explicitly as the sum of the squares of the determinants of all possible different subsets of @xmath179 rows from @xmath175 . since @xmath180 , we find that @xmath181 is not identically equal to zero . therefore , the zeros of such a ( `` proper '' ) polynomial , in which @xmath176 , are a set of measure zero .      for @xmath5 and fixed values of @xmath102 , any differentiable local minimum of the mse ( eq . [ eq : mse ] ) as",
    "a function of @xmath103 and @xmath104 , is also a global minimum , with @xmath66 , @xmath182 almost everywhere .    without loss of generality ,",
    "assume @xmath183 , since we can absorb the weights of the last layer into the @xmath184 weight layer , as we did in single hidden layer case ( eq . ) .",
    "fix an activation pattern @xmath156 as defined in the beginning of this appendix . set @xmath185\\in\\mathbb{r}^{d_{l-2}\\times n}\\,\\ ] ] and",
    "@xmath186 note that , since the activation pattern is fixed , the entries of @xmath187 are polynomials in the entries of @xmath182 , and we may therefore apply lemma [ lem : polynomial proof ] to @xmath187 .",
    "thus , to establish @xmath188 @xmath182-a.e . , we only need to exhibit a single @xmath189 for which @xmath190 .",
    "we note that for a fixed activation pattern , we can obtain any value of @xmath191 with some choice of @xmath192 , so we will specify @xmath193 directly .",
    "we make the following choices : @xmath194\\ \\forall l\\leq l-2\\\\ { \\bf a}_{l-2}^{\\prime p}=\\left[{\\bf 1}_{1\\times d_{l-1}}\\otimes\\mathbf{i}_{d_{l-2}}\\right]_{1, ... ,n}\\:,\\ { \\bf a}_{l-1}^{\\prime p}=\\left[\\mathbf{i}_{d_{l-1}}\\otimes{\\bf 1}_{1\\times d_{l-2}}\\right]_{1, ... ,n}\\end{gathered}\\ ] ] where @xmath195 ( respectively , @xmath196 ) denotes an all ones ( zeros ) matrix of dimensions @xmath197 , @xmath198 denotes the @xmath199 identity matrix , and @xmath200_{1, ... ,n}$ ] denotes a matrix composed of the first @xmath0 columns of @xmath201 .",
    "it is easy to verify that with this choice , we have @xmath202 for any @xmath147 , and so @xmath203 and @xmath204_{1, ... ,n}\\ ] ] which obviously satisfies @xmath190 .",
    "we conclude that @xmath188 , @xmath182-a.e . , and",
    "remark this argument proves fact [ fact : khatri - rao multiplies rank ] , if we specialize to @xmath75 .    as we did in the proof of lemma [",
    "lem : when g full rank ? ] , we apply the above argument for all values of @xmath159 , and conclude via fact [ fact : intersect - a.e . ]",
    "that @xmath188 for every @xmath159 , @xmath182-a.e .. since for every @xmath205 , @xmath206 for some @xmath159 which depends on @xmath205 , this implies that , @xmath182-a.e .",
    ", @xmath207 simultaneously for all values of @xmath208 .",
    "thus in any dlm of the mse , with all weights except @xmath103 fixed , we can use eq .",
    "[ eq : gme=00003d0 ] ( @xmath209 ) , and get @xmath65 ."
  ],
  "abstract_text": [
    "<S> we use smoothed analysis techniques to provide guarantees on the training loss of multilayer neural networks ( mnns ) at differentiable local minima . </S>",
    "<S> specifically , we examine mnns with piecewise linear activation functions , quadratic loss and a single output , under mild over - parametrization . </S>",
    "<S> we prove that for a mnn with one hidden layer , the training error is zero at every differentiable local minimum , for almost every dataset and dropout - like noise realization . </S>",
    "<S> we then extend these results to the case of more than one hidden layer . </S>",
    "<S> our theoretical guarantees assume essentially nothing on the training data , and are verified numerically . </S>",
    "<S> these results suggest why the highly non - convex loss of such mnns can be easily optimized using local updates ( e.g. , stochastic gradient descent ) , as observed empirically . </S>"
  ]
}