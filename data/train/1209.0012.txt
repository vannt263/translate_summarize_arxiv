{
  "article_text": [
    "consider the linear model @xmath4 where @xmath5 and @xmath6 are observed outcomes and @xmath1-dimensional predictors , respectively , @xmath7 are unobserved iid errors with @xmath8 and @xmath9 , and @xmath10 is an unknown @xmath1-dimensional parameter . to simplify notation ,",
    "let @xmath11 denote the @xmath2-dimensional vector of outcomes and @xmath12 denote the @xmath13 matrix of predictors .",
    "also let @xmath14 .",
    "then ( [ lm ] ) may be re - expressed as @xmath15 in this paper , we focus on the case where the predictors @xmath16 are random . more specifically , we assume that @xmath17 are iid random vectors with mean 0 and @xmath18 positive definite covariance matrix @xmath19 ( many of the results in this paper are applicable if @xmath20 upon centering the data ; however , this is not pursued further here ) .",
    "let @xmath21 , where @xmath22 denotes the @xmath0-norm .",
    "then @xmath23 is a measure of the overall ( @xmath0- ) signal strength .",
    "the residual variance @xmath24 and the signal strength @xmath23 are important quantities in many problems in statistics . for example , in estimation and prediction problems , @xmath25 typically determines the scale of an estimator s risk under quadratic loss .",
    "more broadly , @xmath25 , @xmath23 , and associated quantities , such as the signal - to - noise ratio @xmath26 , all play a key role in regression diagnostics .",
    "thus , reliable estimators of @xmath25 and @xmath23 are desirable .    for invertible @xmath27 ,",
    "let @xmath28 be the ordinary least squares estimator for @xmath29 .",
    "if @xmath30 , then @xmath31 is a consistent estimator for @xmath25 and , under fairly mild additional conditions , is asymptotically normal .",
    "consistent estimators for @xmath23 can also be constructed .",
    "for instance , if @xmath32 , it is easily seen that @xmath33 is a consistent estimator for @xmath23 under mild conditions .",
    "it is more challenging to construct reliable estimators for @xmath25 and @xmath23 in high - dimensional linear models , where @xmath34 .",
    "indeed , if @xmath34 , then the estimator @xmath35 breaks down ; however , estimating @xmath25 and @xmath23 remains important . in high - dimensional linear models with @xmath34 , @xmath25 plays an important role in selecting effective shrinkage parameters for many popular regularized regression methods @xcite .",
    "the signal - to - noise ratio @xmath26 is also important for shrinkage parameter selection , and it determines performance limits in certain high - dimensional regression problems @xcite .    in this paper , we propose new estimators for @xmath25 and @xmath23 that are consistent and asymptotically normal , with rate @xmath36 , in an asymptotic regime where @xmath37 ( whenever we write @xmath38 , it is implicit that @xmath39 as well ) . we also show that these estimators may be used to derive consistent and asymptotically normal estimators for function sof @xmath25 and @xmath23 , like the signal - to - noise ratio . previous work on estimating @xmath25 in high - dimensional linear models where @xmath34 has been conducted by @xcite and @xcite .",
    "these authors assume that @xmath29 is sparse ( e.g. the @xmath40-norm or @xmath41-norm of @xmath29 is small ) and their results for estimating @xmath25 are related to the fact that @xmath29 itself is estimable under the specified sparsity assumptions .",
    "though sun and zhang s ( 2011 ) and fan et al.s ( 2012 ) results even apply in settings where @xmath42 , their sparsity assumptions may be untenable in certain instances and this can dramatically affect the performance of their estimators . in this paper , we make no sparsity assumptions ( however , @xmath25 and @xmath23 are required to be bounded ) and we show that the proposed estimators for @xmath25 and @xmath23 perform well in situations where @xmath34 and @xmath29 is provably non - estimable .",
    "this is one of the main messages of the paper : though some type of sparsity is required to consistently estimate @xmath29 in high - dimensional linear models , sparsity in @xmath29 is _ not _ required to estimate @xmath25 and @xmath23 .",
    "though sparsity is not required in this paper , we do make strong distributional assumptions about the data .",
    "in particular , we henceforth assume that @xmath43 while normality is used heavily throughout our analysis , we expect that key aspects of many of the results in this paper remain valid under weaker distributional assumptions .",
    "this is explored via simulation in section 4 .",
    "not surprisingly , the analysis in this paper is simplified by the normality assumption ( [ normality ] ) . to explain the relevance of ( [ normality ] ) in more detail , we first point out that our primary consistency results for the proposed estimators of @xmath25 and @xmath23 ( theorem 1 below ) follow from exact calculations of the estimators mean and variance .",
    "if the normality assumption ( [ normality ] ) is violated , then these calculations are generally invalid ; similar techniques may be applicable , if other conditions hold , but exact finite sample calculations are not likely to be possible and any corresponding approximation may be more involved .",
    "the normality assumption ( [ normality ] ) also facilitates the use of a collection of `` soft - tools '' for random matrices developed by @xcite to prove that the estimators proposed in this paper are asymptotically normal .",
    "these tools are related to second order poincar inequalities and stein s method @xcite .",
    "asymptotic normality for the proposed estimators follows by bounding the total variation distance to a normal random variable .",
    "these bounds contain information about how the variability of the proposed estimators may depend @xmath1 , @xmath2 , @xmath44 , @xmath25 , and @xmath23 .",
    "this is easily leveraged to obtain consistent and asymptotically normal estimators for functions of @xmath25 and @xmath23 ( such as the signal - to - noise ratio , @xmath26 ; see corollary 2 below ) , which is an important practical objective .",
    "thus , one of the appealing aspects of the `` soft tools '' used in this paper is their flexibility . on the other hand ,",
    "paraphrasing @xcite , other existing methods for asymptotic analysis in random matrix theory rely heavily on the exact calculation of limits @xcite ; we suggest that this may be a more delicate endeavor in some instances .",
    "if the normality assumption ( [ normality ] ) does not hold , then it is unclear if the soft tools used in this paper are still applicable and , consequently , other techniques may be required .",
    "existing work in random matrix theory suggests that this may be possible ( see , for example , @xcite ) ; however , the computations are likely more involved and the breadth of applicability of alternative techniques seems unclear .      another challenging issue for estimating @xmath25 and @xmath23 when @xmath45 involves the covariance matrix @xmath46 .",
    "our initial estimators for @xmath25 and @xmath23 are devised under the assumption that @xmath44 is known ( equivalently , @xmath47 ; see section 2 ) .",
    "these estimators are unbiased , consistent , and asymptotically normal .",
    "we subsequently propose modified estimators for @xmath25 and @xmath23 in cases where @xmath44 is unknown , but ( i ) a norm - consistent estimator for @xmath44 is available , or ( ii ) @xmath44 and @xmath29 satisfy certain conditions described in section 3.2 .",
    "if a norm - consistent estimator for @xmath44 is available , then the proposed estimators for @xmath25 and @xmath23 are consistent ; if , furthermore , @xmath44 is estimated at rate @xmath48 , then the estimators are asymptotically normal . on the other hand , if @xmath49 , then norm - consistent estimators for @xmath44 are not generally available ( though there are important examples where norm - consistent estimators for @xmath44 can be found  this is discussed in more detail in section 3.1 ) .",
    "thus , it is important to construct estimators for @xmath25 and @xmath23 that perform reliably when @xmath44 is completely unknown . while it remains an open problem to find estimators for @xmath25 and @xmath23 that are consistent for completely general @xmath44 , in section 3.2 we propose estimators that are consistent and asymptotically normal , provided @xmath44 and @xmath29 satisfy conditions that are closely related to other conditions that have appeared in the random matrix theory literature @xcite .",
    "these conditions basically require that @xmath29 and @xmath44 are _ asymptotically free _ in the sense of free probability ( see , for example , @xcite for a brief overview of free probability and random matrix theory ) .",
    "the problems considered in this paper have at least a passing resemblance to the neyman - scott problem @xcite . in a simplified version of this problem , observations @xmath50 , @xmath51 , @xmath52 are available , and the goal is to estimate @xmath25 .",
    "the means @xmath53 are nuisance parameters and , without additional specification , none of the @xmath53 are estimable , as @xmath39 .",
    "furthermore , the profile maximum likelihood estimator for @xmath54 , which is given by @xmath55 is inconsistent ; indeed , @xmath56 . on the other hand ,",
    "the simple method of moments estimator @xmath57 is consistent for @xmath54 and asymptotically normal .    in linear models ( [ lm ] ) with @xmath34 , which are the main focus of this paper ,",
    "the parameter @xmath29 is typically non - estimable .",
    "however , we show below that @xmath25 may still be consistently estimated in a variety of circumstances . moreover , as in the neyman - scott problem , it is unclear how to proceed with likelihood inference . indeed ,",
    "the mle @xmath58 is degenerate when @xmath34 and it can even be troublesome when @xmath59 : if @xmath60 , then @xmath61 . furthermore , similar to the neyman - scott problem described in the previous paragraph , the basic estimator for @xmath25 derived in section 2.1 is a method of moments estimators .    in our view ,",
    "the major implication of the preceding discussion is that the ambiguities of likelihood inference which arise in this problem contribute to difficulties in devising a systematic approach to estimation and efficiency when studying @xmath25 , @xmath23 , and related quantities in high - dimensional linear models .",
    "while the estimators proposed in this paper are shown to have reasonable properties , further research into these broader issues may be warranted .",
    "section 2 is primarily devoted to the case where @xmath62 .",
    "a motivating discussion and the definition of the basic estimators for @xmath25 and @xmath23 may be found in section 2.1 .",
    "section 2.2 and section 2.3 address consistency and asymptotic normality for the basic estimators , respectively .",
    "the case where @xmath63 is unknown is addressed in section 3 .",
    "section 3.1 is concerned with the case where a norm - consistent estimator for @xmath44 is available ; section 3.2 covers the case where no such estimator may be found , but @xmath29 and @xmath44 satisfy certain additional conditions .",
    "the results of three simulation studies are reported in section 4 .",
    "two of these studies illustrate basic properties of the estimators proposed in this paper . in the third study",
    ", we compare the performance of our estimators for @xmath25 to the performance of estimators for @xmath25 proposed by @xcite .",
    "section 5 contains a concluding discussion , where we briefly mention some potential alternatives to the estimators proposed in this paper and issues related to efficiency .",
    "proofs may be found in the appendix ; some of the more extended calculations required for these proofs are contained in the supplemental text ( which may be found after the bibliography below ) .",
    "throughout the discussion in this section , we assume that @xmath47 .",
    "all of the calculations in section 2.1 - 2.2 require @xmath47 .",
    "however , the main result of section 2.3 ( theorem 3 , on asymptotic normality ) holds for arbitrary positive definite @xmath44 .",
    "notice that if @xmath64 , but @xmath44 is known , then one easily reduces to the case where @xmath47 be replacing @xmath65 with @xmath66 .",
    "for illustrative purposes , suppose for the moment that @xmath59 .",
    "the estimator @xmath35 , defined in ( [ s0 ] ) , may be interpreted as the projection of @xmath67 onto @xmath68 , the orthogonal complement of the column space of @xmath65 .",
    "this well - known interpretation highlights one of the obstacles to estimating @xmath25 in linear models with more predictors than observations : if @xmath69 , then @xmath70 ; thus , @xmath71 and any projection onto @xmath72 is trivial .",
    "an alternative interpretation of @xmath35 suggests methods for estimating @xmath25 and @xmath23 in high - dimensional linear models .",
    "consider the linear combination of @xmath73 and @xmath74 , @xmath75 for @xmath76 and observe that @xmath77 are non - redundant linear combinations of @xmath25 and @xmath23 .",
    "since @xmath78 it follows that there exist @xmath79 such that @xmath80 is an unbiased estimator of @xmath25 , i.e. @xmath81 .",
    "in particular , we have @xmath82 and , moreover , @xmath83 .",
    "thus , for @xmath84 , @xmath85 may be viewed as the unique linear combination of @xmath73 and @xmath74 that yields an unbiased estimator of @xmath25 .",
    "the identities ( [ lc01])-([lc02 ] ) also imply that there exist @xmath86 such that @xmath87 is an unbiased estimator for @xmath23 . indeed , @xmath88 and @xmath89 is the estimator defined initially in ( [ tau0 ] ) .",
    "the ideas above are easily adapted to a more general setting that is useful for problems where @xmath34 .",
    "broadly , we seek statistics @xmath90 and @xmath91 such that @xmath92 in other words , the expected value of the statistics @xmath93 , @xmath94 should form a pair of non - degenerate linear combinations of @xmath25 and @xmath23 . if such @xmath93 and @xmath94 can be found , then unbiased estimators for @xmath25 , @xmath23 may be formed by taking linear combinations of @xmath93 and @xmath94 .",
    "moreover , asymptotic properties of these estimators are determined by the asymptotic properties of @xmath93 , @xmath94 .    in the example discussed above , where @xmath59 , @xmath95 and @xmath96 .",
    "if @xmath34 , then alternatives to @xmath96 must be sought ; in this paper , we focus on @xmath97 ( remarks on other potential alternatives may be found in section 5 ) . using basic facts about the wishart distribution ( see supplemental text for formulas involving various moments of the wishart distribution , which are obtained using techniques from @xcite and are used throughout the paper ) , we have @xmath98 since @xmath99",
    ", it follows that @xmath100 and @xmath97 satisfy ( [ et ] ) .",
    "moreover , @xmath97 is defined and ( [ et2 ] ) is valid even when @xmath34 .",
    "now let @xmath101 and define @xmath102 making use of ( [ lc01 ] ) and ( [ et2 ] ) , a basic calculation implies that @xmath103 and @xmath104 are unbiased estimators for @xmath25 and @xmath23 .",
    "thus , we have the following theorem .    _ [ unbiasedness ] _ suppose that @xmath47",
    ". then @xmath105 and @xmath106 .",
    "let @xmath107 and let @xmath108 .",
    "the covariance matrix of @xmath109 is important for understanding the asymptotic properties of @xmath103 and @xmath104 . since @xmath110 , where @xmath111 it follows that @xmath112 .",
    "the covariance matrices for @xmath109 and @xmath113 are both computed explicitly in the appendix .",
    "asymptotic approximations for the entries of @xmath114 that are valid as @xmath37 are given below : @xmath115 the following theorem contains a slightly more detailed version of these approximations , and gives an explicit consistency result for @xmath103 , @xmath104 . the theorem is proved in the appendix .",
    "_ [ consistency ] _ suppose that @xmath47 .",
    "then @xmath116 in particular , @xmath117    if @xmath37 , then the asymptotic approximations ( [ v1])-([v3 ] ) follow immediately from theorem 2 .    it is instructive to compare the asymptotic variance and covariance of @xmath103 , @xmath104 to that of the estimators @xmath35 , @xmath118 , defined in ( [ s0])-([tau0 ] ) . if @xmath39 and @xmath119 , then @xmath120 notice that in ( [ v1 ] ) , @xmath121 increases with the signal strength @xmath23 , while @xmath122 does not depend on @xmath23 . on the other hand , @xmath123 when @xmath23 is small or @xmath124",
    "is close to 1 .",
    "suppose that @xmath125 are fixed .",
    "theorem 2 implies that if @xmath126 , then @xmath103 , @xmath104 are consistent in the sense that @xmath127 on the other hand , @xcite proved that if @xmath128 , then it is impossible to estimate @xmath29 in this setting . in particular , if @xmath129 , then @xmath130 where the infimum is over all measurable estimators for @xmath29 .",
    "thus , theorem 2 describes methods for consistently estimating @xmath25 and @xmath23 in high - dimensional linear models , where it is impossible to estimate @xmath29 . if @xmath131 , then ( [ rka ] ) holds with @xmath35 , @xmath118 in place of @xmath103 , @xmath104",
    ". however , theorem 2 also applies to settings where @xmath3 ( i.e. @xmath132 ) and the estimators @xmath35 , @xmath118 are undefined .",
    "@xmath133      define the total variation distance between random variables @xmath134 and @xmath135 to be @xmath136 where @xmath137 denotes the collection of borel sets in @xmath138 .",
    "the next theorem is this paper s main result on asymptotic normality .",
    "it is a direct application of results in @xcite .",
    "theorem 3 is proved in the appendix and it is valid for arbitrary positive definite covariance matrices @xmath44 .",
    "_ [ asymptotic normality ] _ let @xmath139 be the operator norm of @xmath140 ( i.e. @xmath141 is the largest eigenvalue of @xmath140 ) .",
    "let @xmath142 be a function with continuous second order partial derivatives , let @xmath143 denote the gradient of @xmath144 , and let @xmath145 denote the hessian of @xmath144 .",
    "suppose that @xmath146 and let @xmath147 be a normal random variable with the same mean and variance as @xmath148 .",
    "then @xmath149 where @xmath150 and @xmath151 are defined as follows : @xmath152 and , for non - negative integers @xmath153 , @xmath154    if @xmath155 is bounded , then the asymptotic behavior of the upper bound ( [ thm3bd ] ) is determined by that of @xmath150 , @xmath156 , and @xmath157 , which , in turn , is determined by the function @xmath144 . for the functions @xmath144 considered in this paper , if @xmath158 , then @xmath150 , @xmath156 , and @xmath159 are bounded by rational functions in @xmath25 and @xmath23 .",
    "thus , if @xmath155 is bounded , @xmath37 , and @xmath160 lie in some compact set , then we typically have @xmath161 in other words , @xmath148 converges to a normal random variable at rate @xmath36 . under these conditions , if @xmath162 is known or estimable ( as it is for the @xmath144 studied here ) , then asymptotically valid confidence intervals for @xmath163 may be constructed using theorem 3 .",
    "@xmath133    now let @xmath164 be the matrix ( [ mata ] ) and let @xmath165 , @xmath166 denote the first and second rows of @xmath164 , respectively .",
    "applying theorem 3 with @xmath47 and @xmath167 , @xmath168 , and @xmath169 gives bounds on the total variation distance between @xmath103 , @xmath104 , and @xmath170 and corresponding normal random variables .",
    "these examples are pursued in more detail below .",
    "let @xmath167 in theorem 3 and suppose that @xmath47",
    ". then @xmath171 , because @xmath172 . to bound @xmath173 , we have @xmath174 thus , @xmath175 by theorem 2 , @xmath176 now let @xmath177 and let @xmath178 . then theorem 3 implies @xmath179.\\ ] ] similar calculations imply that @xmath180,\\ ] ] where @xmath181 thus , we have the following corollary to theorem 3 .",
    "suppose that @xmath47 and @xmath182 is compact .",
    "let @xmath183 . if @xmath37 , then @xmath184 where @xmath185 are defined in ( [ psi1])-([psi2 ] ) .",
    "suppose that @xmath47 .",
    "define the function @xmath186 by @xmath187 and let @xmath188 be defined by @xmath189 , where @xmath164 is the @xmath190 matrix given in ( [ mata ] ) .",
    "then @xmath191 is an estimate of the signal - to - noise ratio .",
    "however , theorem 3 can not be applied directly because @xmath192 is not defined on all of @xmath193 ( if @xmath194 , then @xmath195 is undefined ) . to remedy this",
    ", we assume that @xmath196 , where @xmath197 is compact and , moreover , that @xmath198 .",
    "now let @xmath199 be a function with continuous second order partial derivatives such that @xmath200 and @xmath201 on @xmath202 , where @xmath203 is a compact set containing @xmath204 in its interior .    to show that the estimated signal - to - noise ratio is asymptotically normal , we apply theorem 3 with @xmath205 . working under the assumption that @xmath206 and @xmath37",
    ", it is straightforward to check that @xmath207 , for @xmath208 ; thus , @xmath209 . to approximate the variance of @xmath148 ,",
    "let @xmath210 and @xmath211 . a second order taylor expansion yields @xmath212 where @xmath213 .",
    "theorem 2 and a straightforward calculation imply that @xmath214 since @xmath215 and @xmath216 , ( [ ex2a ] ) implies @xmath217 thus , theorem 3 implies that @xmath218 = o(n^{-1/2}),\\ ] ] where @xmath178 and @xmath219 finally , in order to relate ( [ ex2b ] ) directly to @xmath220 and the signal - to - noise ratio @xmath26 , notice that theorem 2 implies @xmath221 and equation ( [ ex2a ] ) implies @xmath222 combining these facts with ( [ ex2b ] ) , we obtain the following result .",
    "suppose that @xmath47 and @xmath182 is compact .",
    "let @xmath183 . if @xmath37 , then @xmath223 where @xmath224 is defined in ( [ psi0 ] ) .",
    "in this section , we propose estimators for @xmath25 , @xmath23 for use when @xmath44 is an unknown @xmath18 positive definite matrix . in section 3.1 , we consider the case where a norm - consistent estimator for @xmath44 is available . in this",
    "setting , consistent ( and , under certain conditions , asymptotically normal ) estimators for @xmath25 , @xmath23 are obtained by essentially transforming the problem to the @xmath47 case . in section 3.2 , we consider the case where a norm - consistent estimator for @xmath44 is not available . here",
    "we derive alternative estimators for @xmath25 , @xmath23 and these estimator are shown to be consistent and asymptotically normal under additional conditions on @xmath44 and @xmath225 .",
    "an estimator @xmath226 for @xmath44 is norm consistent if @xmath227 , where @xmath228 is the operator norm of @xmath229 and the convergence holds in some appropriate sense ( e.g. convergence in probability or squared - mean ) . in high - dimensional data analysis",
    "where @xmath230 , the sample covariance matrix @xmath140 is not a norm - consistent estimator for @xmath44 ; furthermore , in the absence of additional information about @xmath44 , it is generally not possible to find a norm - consistent estimator for @xmath44 .",
    "however , @xcite , @xcite , @xcite , and others have shown that for wide classes of matrices @xmath44 , norm - consistent estimators are available when @xmath231 .",
    "moreover , one can reasonably envision situations in practice where pertinent prior information about the population predictor covariance matrix @xmath44 is available ( so that a reliable estimator of @xmath44 may be found ) , but there is little prior information about @xmath232 ( so that @xmath232 is not estimable and estimates of @xmath25 , @xmath23 based on residual sums of squares @xmath233 are suspect ) . @xcite",
    "discuss relevant examples from genomics and fmri with highly structured high - dimensional predictors , though they focus on variable selection problems .",
    "suppose that @xmath226 is a positive definite estimator for @xmath44 and define the estimators @xmath234 notice that @xmath235 and @xmath236 .",
    "now let @xmath237 .",
    "then @xmath238 and all of the results from section 2 apply to the estimators @xmath239 , @xmath240 , with @xmath241 , @xmath242 in place of @xmath65 , @xmath29 , respectively . since @xmath243 and @xmath244 we conclude that if @xmath245 is small , then asymptotic properties of @xmath246 and @xmath247 are determined by those of @xmath239 and @xmath240 .",
    "this is illustrated in the following proposition , which is a direct consequence of ( [ est1])-([est2 ] ) and the results of section 2 .",
    "let @xmath226 be a positive definite estimator for @xmath44 .",
    "suppose further that @xmath155 , @xmath248 , @xmath249 , @xmath250 .    * _ [ consistency ] _",
    "* _ [ asymptotic normality ] _ let @xmath252 , @xmath253 , and @xmath254 be as defined in ( [ psi1 ] ) , ( [ psi2 ] ) , and ( [ psi0 ] ) .",
    "suppose that @xmath37 and that @xmath196 for some compact set @xmath255 . if @xmath256 , then @xmath257 where @xmath258 indicates convergence in distribution .",
    "part ( i ) of proposition 1 implies if @xmath160 are bounded , @xmath259 , and @xmath260 , then @xmath246 and @xmath247 are weakly consistent for @xmath25 and @xmath23 , respectively .",
    "if @xmath261 and the other conditions of proposition 1 are met , then @xmath246 , @xmath247 , and @xmath262 are asymptotically normal with the same asymptotic variance as @xmath239 , @xmath240 , and @xmath263 , respectively .",
    "the condition @xmath261 is quite strong .",
    "however , @xcite and @xcite describe broad classes of covariance matrices @xmath44 that can be estimated at this rate . for concreteness , we note that if the entries of @xmath16 follow one of many common time series models ( e.g. @xmath264 for fixed @xmath153 ) , then there exist estimators @xmath226 such that @xmath265 when @xmath49 .",
    "@xmath133      define @xmath266 and @xmath267 , @xmath268",
    ". then @xmath269 . for general positive definite matrices",
    "@xmath44 , one easily checks that @xmath270 and @xmath271 thus , if @xmath272 , then @xmath103 , @xmath104 are typically _ not _ unbiased estimators for @xmath25 , @xmath23 , respectively .",
    "more generally , it follows that if @xmath272 , then the expected value of the linear combination @xmath273 typically depends on @xmath25 , @xmath274 , @xmath275 , and @xmath276 .",
    "by contrast , as seen in section 2 , if @xmath47 , then @xmath277 and @xmath278 is determined by @xmath25 and @xmath23 ( in addition to @xmath279 , @xmath280 , @xmath1 , @xmath2 ) ; indeed , in the @xmath47 case , this fact is precisely what is leveraged to obtain unbiased estimators for @xmath25 , @xmath23 .",
    "this suggests that an alternative method for estimating @xmath25 , @xmath23 may be necessary when @xmath44 is unknown and non - estimable .    in this section , we do not completely abandon our strategy of estimating @xmath25 , @xmath23 by using linear combinations of @xmath73 and @xmath281",
    "rather , we propose modified versions of @xmath103 and @xmath104 that are consistent and asymptotically normal , provided @xmath29 and @xmath44 satisfy certain conditions that have appeared previously in the random matrix theory literature .",
    "these conditions are stated below .    * as @xmath282 ,",
    "the empirical distribution of the eigenvalues of @xmath44 converges weakly to a probability distribution with support contained in a compact subset of @xmath283 and cumulative distribution function @xmath284 .",
    "* let @xmath285 where the distribution @xmath284 is given in condition ( a ) . then , as @xmath282 , @xmath286    condition ( a ) is fairly standard and is frequently assumed to hold in asymptotic analyses in random matrix theory @xcite .",
    "the compact support requirement in condition ( a ) can likely be relaxed ; however , this is not pursued further here .",
    "condition ( b ) is more specialized and requires that the parameter @xmath29 interacts with @xmath44 as determined by ( [ b ] ) .",
    "in fact , while condition ( b ) is sufficient for our consistency results in this section , we require a stronger version of condition ( b ) ( stated precisely in proposition 2 ( ii ) ) to obtain asymptotic normality . @xcite and @xcite have proposed conditions that are closely related to ( b ) and the strengthened version of ( b ) appearing in proposition 2 ( ii ) ( in fact , their conditions are stronger , if @xmath284 has finite moments ) . @xcite",
    "have noted that under condition ( a ) , if @xmath44 is an independent , orthogonally invariant random matrix ( e.g. if @xmath44 is a wishart matrix and @xmath287 , for some constant @xmath288 ) , then condition ( b ) holds for any @xmath29 .",
    "furthermore , @xcite point out that for any @xmath44 there must exist some @xmath29 such that condition ( b ) holds ; for instance , take @xmath289 , where @xmath290 and @xmath291 are orthonormal eigenvectors of @xmath44 .",
    "more broadly , ( b ) may be interpreted as requiring that @xmath29 and @xmath44 are asymptotically free .",
    "presently , we provide a heuristic to motivate estimators for @xmath25 and @xmath23 under conditions ( a ) and ( b ) . following the method of moments ,",
    "the identities @xmath292 suggest that @xmath293 are reasonable estimators for @xmath294 and @xmath295 , respectively .",
    "now assume that @xmath296 are large and @xmath297 . then , for @xmath298 , condition ( a ) implies that @xmath299 and ( b ) implies @xmath300 . combining these approximations with equations ( [ ge1])-([ge2 ] ) yields @xmath301 observe that the right - hand side of ( [ approx1])-([approx2 ] ) consists of linear combinations of @xmath25 and @xmath23 , with coefficients determined by the known quantities @xmath1 , @xmath2 , @xmath302 , and @xmath303 .",
    "thus , we are able to obtain _ nearly _ unbiased estimators of @xmath25 and @xmath23 by taking linear combinations of @xmath73 and @xmath281 , with coefficients determined by @xmath1 , @xmath2 , @xmath302 , and @xmath303 . in particular , define the estimators @xmath304 a basic calculation using ( [ approx1])-([approx2 ] ) suggests that @xmath305 and @xmath306 .",
    "proposition 2 summarizes some asymptotic properties of @xmath307 and @xmath308 .",
    "an outline of the proof , which is fairly straightforward , may be found in the appendix .",
    "suppose that condition ( a ) holds , that @xmath182 is a compact set , and that @xmath206 .",
    "suppose further that there exist constants @xmath309 in @xmath138 such that either @xmath310 or @xmath311 , and suppose that @xmath312 .",
    "define @xmath313 , where @xmath314 and @xmath315 are defined in condition ( b ) , and @xmath316 .    * _ [ consistency ] _",
    "@xmath317 thus , if condition ( b ) holds , then @xmath318 _ suppose that condition ( b ) holds , with the additional requirement that @xmath319 , and let @xmath320 then @xmath321    the conditions in proposition 2 that require @xmath322 and @xmath323 to be bounded away from 1 are related to the fact that @xmath324 appears in both @xmath307 and @xmath308 . in particular",
    ", the mean - squared error of @xmath307 and @xmath308 may be infinite if @xmath325 is not large enough .",
    "the condition @xmath326 in part ( ii ) of proposition 2 is quite strong .",
    "for instance , if @xmath44 is a sample covariance matrix formed from iid @xmath327 data with a constant aspect ratio , then condition ( b ) is satisfied , but @xmath328 . on the other hand ,",
    "if @xmath44 is a constant multiple of the identity matrix , then @xmath326 .",
    "we emphasize that only conditions ( a ) and ( b ) are required for @xmath307 and @xmath308 to be consistent ; @xmath326 is required for asymptotic normality .    if @xmath47 , then @xmath329 and @xmath330 , @xmath331 , where @xmath332 are given in ( [ psi1])-([psi2 ] ) and ( [ psi0 ] ) . in other words , if @xmath47 , then the asymptotic variance of @xmath307 , @xmath308 , and @xmath333 is the same as that of @xmath103 , @xmath104 , and @xmath170 , respectively .",
    "this is driven by the fact that if @xmath49 , then @xmath334 converges at rate @xmath335 .",
    "in this section , we study the performance of the proposed estimators for @xmath25 , @xmath23 , and the signal - to - noise ratio @xmath26 via simulation .",
    "we consider three examples . in the first example",
    ", we report the results of a simulation study that illustrates the performance of the estimators from section 2 ( for @xmath47 ) and section 3.2 ( unknown , non - estimable @xmath44 ) ; the predictors @xmath16 are generated from various distributions ( including non - normal distributions ) that are described below . in the second example",
    ", we compare the performance of @xmath235 to that of @xmath336 in settings where @xmath59 . in the final example",
    ", we compare the performance of estimators proposed in this paper to that of the scaled lasso and mc+ estimators for @xmath25 .",
    "these estimators for @xmath25 were proposed by @xcite for settings where @xmath29 is sparse ; in our simulation study , we consider cases where @xmath29 is sparse and non - sparse .      in this example ,",
    "@xmath337 and the predictors @xmath338 were generated according to one of three distributions . in the first",
    "setting , @xmath339 . in the second",
    "setting , we generated a @xmath340 random matrix @xmath241 with iid @xmath341 entries and took @xmath342 ; the iid predictors @xmath16 were then generated according to a @xmath343 distribution ( the same matrix @xmath44 was used for all datasets generated under this setting ) . in the third setting , the individual predictors @xmath344 , @xmath345 , @xmath346 , were iid random variables taking values in @xmath347 with @xmath348 .",
    "to generate the parameter @xmath349 , we created a 1000-dimensional vector with the first @xmath350 coordinates iid @xmath351 and the remaining @xmath350 coordinates iid @xmath341 ; @xmath29 was obtained by standardizing this vector so that @xmath352 ( the same @xmath29 was used for all simulated datasets in this example ) .",
    "the residual variance was fixed at @xmath353 and we considered datasets with @xmath354 and @xmath355 observations .    for each setting in this example , we generated 500 independent datasets and computed the estimators @xmath235 , @xmath356 , @xmath357 and @xmath307 , @xmath308 , @xmath333 ( the estimators proposed in section 2 and section 3.2 , respectively ) for each dataset .",
    "recall that the estimators from section 2 were derived under the assumption that @xmath358 and the estimators from section 3.2 were derived under the assumption that @xmath359 , where @xmath44 satisfies conditions ( a)-(b ) .",
    "summary statistics for the various estimators are reported in table 1 .",
    ".summary statistics for example 1 ( @xmath360 ) . means and standard errors of various estimators , computed over 500 independent datasets for each configuration . in each setting , @xmath361 ; thus , unbiased estimators should have mean close to 1 . in the standard error column corresponding to @xmath362 , numbers in parentheses",
    "are theoretically predicted standard errors ( denoted @xmath252 , @xmath253 , and @xmath254 in the text ; see corollaries 1 - 2 and proposition 2 ) . theoretically predicted standard errors for @xmath363 and @xmath364 binary are not known ; more details may be found in the discussion in section 4.1 .",
    "[ cols= \" < , < , > , > , > , > , > , > \" , ]     table 2 indicates that in each setting , the estimators are nearly unbiased : the means of the estimators are close to 1 .",
    "the empirical standard errors of @xmath365 and @xmath85 both increase with @xmath1 ; however , the standard errors increase more rapidly for @xmath85 . at @xmath366 ,",
    "the empirical standard error of @xmath85 is smaller than that of @xmath365 ; at @xmath367 , the trend reverses and the empirical standard error of @xmath365 is smaller than that of @xmath85 .",
    "as @xmath1 becomes closer to @xmath354 , the empirical standard error of @xmath365 should remain bounded , while that of @xmath85 should diverge to @xmath368 .",
    "the results reported in this example suggest that even when @xmath59 , there may be settings where the estimators proposed in this paper may be preferred to over other commonly used estimators for @xmath25 ; for instance , when @xmath59 , but @xmath1 is very close to @xmath2 .",
    "@xcite proposed methods for estimating @xmath25 in high - dimensional linear models that are very effective when @xmath29 is sparse .",
    "these methods use modified versions of lasso @xcite and mc+ @xcite , ( referred to as `` scaled lasso '' and `` scaled mc+ , '' respectively ) to simultaneously estimate @xmath25 and @xmath29 .",
    "let @xmath369 and @xmath370 denote the scaled lasso and scaled mc+ estimators for @xmath25 .",
    "in this example , we compared the performance of @xmath369 and @xmath370 with some of the estimators for @xmath25 proposed in this paper , in settings where @xmath29 was both sparse and non - sparse .    with @xmath371 , the predictors in this example were generated according to @xmath359 , where @xmath372 and @xmath373 .",
    "we fixed @xmath353 .",
    "sparse and non - sparse ( dense ) parameters @xmath374 were generated as follows .",
    "first , to generate the sparse @xmath29 , five random multiples of 25 between @xmath375 and @xmath376 were selected .",
    "that is , we selected @xmath377 from @xmath378 independently and uniformly at random .",
    "next , we took @xmath379 to be the vector with the 7-dimensional sub - vector @xmath380 centered at the coordinates corresponding to @xmath377 ( so that the @xmath381-th entry of @xmath382 was 4 , the @xmath383-th was 3 , etc . ) ; the remaining entries in @xmath382 were set equal to 0 .",
    "we then set @xmath384 , so that @xmath385 . note that this sparse @xmath29 was generated only once ; in other words , the same sparse @xmath29 was use throughout the simulations in this example . to generate the dense @xmath29 used in this example",
    ", we followed the same procedure as for the sparse @xmath29 , except that in @xmath382 , the 7-dimensional subvector @xmath380 was centered at coordinates corresponding to _ each _ multiple of 25 between 25 and @xmath386 .",
    "notice that for the sparse @xmath29 , we had @xmath387 , where @xmath388 denotes the number of non - zero coordinates in @xmath29 , and for the dense @xmath29 we had @xmath389 ; however , @xmath390 was the same for both the sparse and dense @xmath29 . in this simulation study , we considered datasets with @xmath391 and @xmath392 observations . with sparse @xmath29 and @xmath393 ,",
    "the simulation settings in this example are very similar to those in example 1 from section 4.1 of @xcite .    under each of the settings described above",
    ", we generated @xmath394 independent datasets and , for each simulated dataset , we computed @xmath395 , @xmath396 , @xmath246 , @xmath239 , and @xmath307 . for the scaled lasso and mc+ estimators",
    ", we used the shrinkage parameter @xmath397 ( this value of @xmath398 yielded the best performance in the numerical examples in @xcite ) .",
    "the scaled mc+ estimator requires specification of an additional parameter @xmath399 ; following @xcite , we took @xmath400 $ ] , where @xmath401 denotes the @xmath402-th column of @xmath65 .",
    "the estimator @xmath246 was introduced in section 3.1 of this paper . here",
    "we take advantage of the ar(1 ) structure of @xmath44 and set @xmath403 , where @xmath404 and @xmath405 we view the estimator @xmath239 as an `` oracle estimator , '' which utilizes full knowledge of actual covariance matrix @xmath44 ; this estimator should perform similarly to the estimator @xmath365 in settings where @xmath62 and @xmath406 ( see the discussion in section 3.1 ) . finally , the estimator @xmath307 is the `` unknown covariance '' estimator from section 3.2 .",
    "recall that our theoretical performance guarantees for @xmath307 ( proposition 2 ) require that latexmath:[$\\left|\\bb^t\\s^k\\bb -    this example , for the sparse @xmath29 we had @xmath408 ( the corresponding quantities are essentially the same for the dense @xmath29 ) .",
    "summary statistics for the various estimators computed in this numerical study are reported in table 3 .",
    "ll|r|r + & & mean & std . err .",
    "+ @xmath391 & @xmath395 & 1.1117 & 0.0651 + & @xmath396 & 1.0477 & 0.0633 + & @xmath246 & 0.9704 & 0.5049 + & @xmath239 & 0.9693 & 0.5021 + & @xmath307 & -0.6023 & 0.5182 + @xmath392 & @xmath395 & 1.0310 & 0.0295 + & @xmath396 & 1.0060 & 0.0293 + & @xmath246 & 0.9808 & 0.1633 + & @xmath239 & 0.9809 & 0.1631 + & @xmath307 & - 0.5827 & 0.2084    ll|r|r + & & mean & std . err . + @xmath391 & @xmath395 & 3.2600 & 0.2070 + & @xmath396 & 3.1005 & 0.2107 + & @xmath246 & 0.9820 & 0.5641 + & @xmath239 & 0.9835 & 0.5596 + & @xmath307 & -0.5747 & 0.5876 + @xmath392 & @xmath395 & 2.3232 & 0.0706 + & @xmath396 & 1.9997 & 0.0778 + & @xmath246 & 1.0095 & 0.1538 + & @xmath239 & 1.0095 & 0.1537 + & @xmath307 & -0.5702 & 0.2228    for sparse @xmath29 , the results in table 3 indicate that @xmath395 , @xmath396 , @xmath246 , and @xmath239 are all nearly unbiased ( recall that @xmath409 in this example ) .",
    "however , the empirical standard errors for the scaled lasso and mc+ estimators are considerably smaller than the standard errors for @xmath246 and @xmath239 . note that in this example , the performance of @xmath246 is very similar to that of the oracle estimator @xmath239 .",
    "the estimator @xmath307 is significantly biased in this example . indeed , the mean value of @xmath307 is negative , while @xmath410 . the poor performance of @xmath307 in this example is not completely unexpected , given that @xmath411 ( see ( [ ex3a ] ) ) .",
    "in fact , more can be said .",
    "using the approximation @xmath412 , @xmath298 , one can check that @xmath413 thus , the bias of @xmath307 is approximately @xmath414 . in this example ,",
    "@xmath415 and @xmath416 ( this calculation is for the sparse @xmath29 ; the result is almost exactly the same for the dense @xmath29 ) .",
    "note the similarity between this approximation and the empirical means of @xmath307 in table 3 .    for dense @xmath29 ,",
    "the performance of @xmath369 and @xmath370 breaks down , while the performance of @xmath246 , @xmath239 , and @xmath307 remains virtually unchanged , as compared to the sparse @xmath29 case .",
    "when @xmath417 , the empirical means of @xmath369 and @xmath370 are both greater than 3 ; when @xmath392 , the empirical means of @xmath369 and @xmath370 are both nearly greater than 2 . both @xmath395 and @xmath396",
    "depend on associated lasso and @xmath418 estimators for @xmath29 .",
    "the performance break - down of @xmath395 and @xmath396 when @xmath29 is dense is likely related to the fact that the corresponding estimators for @xmath29 perform poorly when @xmath29 is dense and @xmath323 is large . in table 4 , we report the empirical mean squared error for the lasso and @xmath418 estimators for @xmath29 that are associated with @xmath395 and @xmath396 ; note that mean squared error is substantially higher for estimating dense @xmath29 .",
    "l|r|r + @xmath2 & lasso & mc+ + 600 & 0.1888 & 0.3696 + 2400 & 0.0514 & 0.0894    l|r|r + @xmath2 & lasso & mc+ + 600 & 1.2176 & 1.2457 + 2400 & 0.8961 & 0.9337    overall , the results of this simulation study suggest that estimators proposed in this paper may be useful for estimating @xmath25 in settings where @xmath323 is large and little is know about sparsity in @xmath29 .",
    "however , we emphasize two important points : ( i ) additional information about the covariance matrix @xmath44 may be required to obtain consistent estimators for @xmath25 ( e.g. that @xmath44 has ar(1 ) structure ) and ( ii ) the estimators for @xmath25 proposed in this paper may have larger standard error than estimators derived from a reliable estimate of @xmath29 .",
    "in this paper , we proposed new estimators for @xmath25 , @xmath23 , and the signal - to - noise ratio @xmath26 in high - dimensional linear models .",
    "these estimators are based on linear combinations of @xmath95 and @xmath97 .",
    "working under the assumption that @xmath62 , the key observation in deriving these estimators was that @xmath419 , @xmath420 form a pair of non - degenerate linear combinations involving @xmath25 and @xmath23 .",
    "in fact , as described in section 2.1 , unbiased estimators for @xmath25 and @xmath23 may be derived from any pair of statistics @xmath421 satisfying this property . with @xmath422 fixed , we presently discuss two alternatives for @xmath94 , which may yield other estimators for @xmath25 , @xmath23 in this manner .",
    "these examples are not meant to be exhaustive ; rather , they are illustrative of this technique s flexibility and raise some broader questions about estimating @xmath25 and @xmath23 in high - dimensional linear models .",
    "first , let @xmath423 be a @xmath18 haar - distributed orthogonal matrix independent of @xmath424 and let @xmath425 denote the first @xmath153 columns of @xmath426 , where @xmath427 .",
    "then one may take @xmath428 , where @xmath429 and @xmath430 , so that @xmath431 is a random rank-@xmath153 projection . as a second alternative to @xmath97",
    ", one could take @xmath432 , where @xmath433 is some ridge regression estimator for @xmath29 @xcite .",
    "one aspect of these alternatives potential appeal is that they might yield consistent estimators for @xmath25 and @xmath23 with smaller variance than the estimators studied in this paper .",
    "however , a theoretical analysis of these estimators properties may be somewhat involved . indeed , for @xmath428 , it is easy to calculate @xmath420 and find the corresponding unbiased estimators for @xmath25 and @xmath23 using symmetry arguments ( provided @xmath62 ) , but computing the variance of these estimators appears to be fairly challenging . if @xmath434 , then closed - form expressions for @xmath420 and , consequently , for the associated unbiased estimators of @xmath25 , @xmath23 are generally not available ; however , results from random matrix theory suggest that simplified asymptotic analyses may be possible .",
    "note that in order to implement either of these alternatives to @xmath435 , specification of an additional tuning parameter is required : for @xmath436 , the rank parameter @xmath153 must be specified ; for @xmath434 , the ridge shrinkage parameter ( typically , a nonnegative constant denoted by @xmath437 ) must be specified .",
    "a number of questions are raised by the examples discussed in the previous paragraph .",
    "for instance , it is clear that estimators for @xmath25 , @xmath23 derived using different statistics @xmath93 , @xmath94 may ( or may not ! ) be more efficient than the estimators @xmath103 , @xmath104 studied here ; however , an exhaustive study of all pairs @xmath421 aimed at identifying the optimal estimators for @xmath25 , @xmath23 is likely impossible .",
    "this suggests the need for a more unified approach to studying efficiency and optimality for estimating @xmath25 and @xmath23 in high - dimensional linear models , which , given the ambiguity of likelihood - based approaches noted in section 1.3 , may be challenging . additionally , while we have shown that the proposed approach to estimating @xmath25 and @xmath23 based on linear combinations of statistics @xmath93 , @xmath94 is effective when @xmath438 , and that this approach may be successfully modified when @xmath44 satisfies additional conditions , it is unclear whether a similar approach may be applied effectively when @xmath44 is unknown and arbitrary . studying different statistics @xmath93 , @xmath94 may provide additional insight into this problem , but other methodologies may be required to handle more general @xmath44 .",
    "theorem 2 is an immediate consequence of the following lemma and its corollary .",
    "suppose that @xmath47",
    ". then @xmath439 \\\\ \\label{lemma1c } \\cov\\left(\\frac{1}{n}||\\y||^2,\\frac{1}{n^2}||x^t\\y||^2\\right ) \\!\\!\\ ! & = & \\!\\!\\ !",
    "\\frac{2}{n}\\left\\{\\frac{d}{n}\\s^4 + \\left(\\frac{2d}{n } + 2 +      \\frac{3}{n}\\right)\\s^2\\tau^2 + \\left(\\frac{d}{n } + 2 +      \\frac{3}{n}\\right)\\tau^4\\right\\}.\\end{aligned}\\ ] ]    equation ( [ lemma1a ] ) is obvious because @xmath440 . to prove ( [ lemma1b ] ) , we condition on @xmath65 and use properties of expectations involving quadratic forms and normal random vectors to obtain @xmath441 ^ 2.\\end{aligned}\\ ] ] given this expression for @xmath442 , ( [ lemma1b ] ) follows from proposition s1 in the supplemental text .",
    "equation ( [ lemma1c ] ) is proved similarly : we have @xmath443 and ( [ lemma1c ] ) follows from proposition s1 in the supplemental text .    under the conditions of lemma 1 , @xmath444    corollary 1 follows from lemma 1 and the fact that @xmath445      theorem 3 is a direct application of theorem 2.2 from @xcite , which is stated here for ease of reference .    _",
    "[ theorem 2.2 , @xcite ] _ let @xmath446 .",
    "suppose that @xmath447 and let @xmath448 and @xmath449 denote the gradient and the hessian of @xmath450 , respectively .",
    "let @xmath451 where @xmath452 is the operator norm of @xmath453 .",
    "suppose that @xmath454 and let @xmath455 .",
    "let @xmath147 be a normal random variable having the same mean and variance as @xmath456 .",
    "then @xmath457    chatterjee s theorem 2.2 does not actually require gaussian @xmath458 .",
    "however , for non - gaussian @xmath458 , an additional term appears in the bound ( [ thm3a ] ) , which is not sufficiently small for our purposes .",
    "furthermore , the class of distributions covered by the full version of chatterjee s theorem 2.2 is not all - encompassing : @xmath459 must be a @xmath460-function of a normal random variable .    to prove theorem 3 , we apply theorem a1 with @xmath461 .",
    "let @xmath462 and let @xmath463 where @xmath464 .",
    "first , we bound the quantities @xmath465 , @xmath466 in theorem a1 . in order to bound @xmath465 , we compute the gradient of @xmath450 .",
    "let @xmath467 , @xmath468 denote the partial derivatives of @xmath144 with respect to the first and second variables , respectively .",
    "then @xmath469 for @xmath345 , @xmath346 .",
    "let @xmath470 denote the @xmath471 matrix with @xmath472-entry @xmath473 ( @xmath474 if @xmath475 and @xmath476 otherwise ) .",
    "since @xmath477 and @xmath478 it follows that @xmath479 for @xmath480 , the partial derivative of @xmath450 with respect to @xmath481 is given by @xmath482 where @xmath483 is the @xmath153-th standard basis vector in @xmath484 ( i.e. the @xmath485-th entry of @xmath486 is @xmath487 ) and we have used the facts @xmath488 now recall that @xmath489 .",
    "equations ( [ thm1a])-([thm1b ] ) and the elementary inequality @xmath490 imply that @xmath491 let @xmath139 be the largest eigenvalue of @xmath140 . applying the triangle inequality and ( [ thm1c ] ) yields @xmath492 thus , @xmath493\\right)^{1/4 } \\\\",
    "\\label{thm1d } & = & o\\left[\\frac{1}{\\sqrt{n}}\\left\\{\\g_4^{1/4 } + \\g_2^{1/4 } + \\g_0^{1/4}\\tau(\\tau      + 1)\\right\\}\\right],\\end{aligned}\\ ] ] where @xmath494.\\ ] ]    to bound @xmath495 , we bound the operator norm of the hessian @xmath496 .",
    "let @xmath497 be the collection of partitioned @xmath498 matrices with frobenius norm equal to one . for @xmath499",
    ", define the differential operator @xmath500 then @xmath501 from our previous calculations , @xmath502 to compute @xmath503 , we need the second order partial derivatives of @xmath504 and @xmath505 ; these are given below : @xmath506 and @xmath507 for @xmath508 and @xmath509 .",
    "it follows that the entries of @xmath510 are @xmath511 and @xmath512 we conclude that @xmath513\\end{aligned}\\ ] ] and @xmath514 combining ( [ thm1e])-([thm1 g ] ) , we obtain @xmath515,\\ ] ] where @xmath516.\\ ] ] appealing to theorem a1 , the bounds ( [ thm1d ] ) and ( [ thm1h ] ) imply @xmath517 where @xmath518 and @xmath519 this completes the proof of theorem 3 .",
    "let @xmath520 where @xmath521 . with @xmath522",
    ", consider the estimators @xmath523 and @xmath524 . under the conditions of proposition 2 ,",
    "proposition s1 from the supplemental text implies that @xmath525 , @xmath298 ; furthermore , existing results on the eigenvalues of wishart matrices imply that @xmath526 for @xmath527 sufficiently small ( see , for example , the appendix of @xcite ; this is where the conditions that @xmath528 and @xmath323 is bounded away from 1 are required ) .",
    "these facts can be combined to obtain @xmath529 additionally , it can be shown that @xmath530 and @xmath531 where proposition s1 in the supplemental text and the variance / covariance decompositions in the proof of lemma a1 are useful for proving ( [ prop2c ] ) .",
    "part ( i ) of proposition 2 ( consistency ) follows from ( [ prop2a])-([prop2c ] ) .",
    "part ( ii ) of proposition 2 ( asymptotic normality ) also follows from ( [ prop2a])-([prop2c ] ) , upon noticing that theorem 3 may be applied to @xmath523 and @xmath524 , as in corollary 1 .",
    "asymptotic normality for @xmath333 follows from the delta method .    27 natexlab#1#1url # 1`#1`urlprefix    bai , z. , miao , b. and pan , g. ( 2007 ) . on asymptotics of eigenvectors of large sample covariance matrix .",
    "_ the annals of probability _ * 35 * 15321572 .",
    "bai , z. and silverstein , j. ( 2004 ) . for linear spectral statistics of large - dimensional sample covariance matrices . _",
    "the annals of probability _ * 32 * 553605 .",
    "bickel , p. and levina , e. ( 2008 ) .",
    "regularized estimation of large covariance matrices .",
    "_ the annals of statistics _ * 36 * 199227 .",
    "bickel , p. , ritov , y. and tsybakov , a. ( 2009 ) . .",
    "_ the annals of statistics _ * 37 * 17051732 .",
    "cai , t. , zhang , c. and zhou , h. ( 2010 ) .",
    "optimal rates of convergence for covariance matrix estimation . _",
    "the annals of statistics _ * 38 * 21182144 .",
    "cands , e. and tao , t. ( 2007 ) . .",
    "_ the annals of statistics _ * 35 * 23132351 .",
    "chatterjee , s. ( 2009 ) . .",
    "_ probability theory and related fields _ * 143 * 140 .    dicker , l. ( 2012 ) . .",
    "preprint .",
    "dicker , l. ( 2012 ) .",
    "optimal estimation and prediction for dense signals in high - dimensional linear models .",
    "preprint .    el  karoui , n. ( 2008 ) . .",
    "_ the annals of statistics _ * 36 * 27172756 .",
    "el  karoui , n. ( 2008 ) . .",
    "_ the annals of statistics _ * 36 * 27572790 .",
    "el  karoui , n. and koesters , h. ( 2011 )",
    ". geometric sensitivity of random matrix results : consequences for shrinkage estimators of covariance and related statistical methods . _ arxiv preprint arxiv:1105.1404 _",
    ".    fan , j. , guo , s. and hao , n. ( 2012 ) .",
    "variance estimation using refitted cross - validation in ultrahigh dimensional regression . _ journal of the royal statistical society : series b ( statistical methodology ) _ * 74 * 3765 .",
    "graczyk , p. , letac , g. and massam , h. ( 2005 ) . . _ journal of theoretical probability _ * 18 * 142 .",
    "hoerl , a. and kennard , r. ( 1970 ) . .",
    "_ technometrics _ * 12 * 5567 .",
    "jonsson , d. ( 1982 ) .",
    "some limit theorems for the eigenvalues of a sample covariance matrix .",
    "_ journal of multivariate analysis _ * 12 * 138 .",
    "lancaster , t. ( 2000 ) .",
    "the incidental parameter problem since 1948 .",
    "_ journal of econometrics _ * 95 * 391413 .",
    "letac , g. and massam , h. ( 2004 ) . .",
    "_ scandinavian journal of statistics _ * 31 * 295318 .    li , f. and zhang , n. ( 2010 ) .",
    "bayesian variable selection in structured high - dimensional covariate spaces with applications in genomics .",
    "_ journal of the american statistical association _ * 105 * 12021214 .",
    "marenko , v. and pastur , l. ( 1967 ) . .",
    "_ mathematics of the ussr",
    " sbornik _ * 1 * 457483 .",
    "neyman , j. and scott , e. ( 1948 ) .",
    "consistent estimates based on partially consistent observations .",
    "_ econometrica : journal of the econometric society _ 132 .",
    "pan , g. and zhou , w. ( 2008 ) .",
    "central limit theorem for signal - to - interference ratio of reduced rank linear receiver .",
    "_ the annals of applied probability _ * 18 * 12321270 .",
    "speicher , r. ( 2003 ) .",
    "free probability theory and random matrices . in _",
    "asymptotic combinatorics with applications to mathematical physics , lecture notes in mathematics , vol .",
    "1815_. springer , 5373 .",
    "stein , c. ( 1986 ) .",
    "_ approximate computation of expectations _ , vol .  7 of _ ims lecture notes  monograph series_. institute of mathematical statistics .",
    "sun , t. and zhang , c. ( 2011 ) . scaled sparse linear regression .",
    "arxiv preprint arxiv:1104.4595 .",
    "tibshirani , r. ( 1996 ) . . _ journal of the royal statistical society : series b ( methodological ) _ * 58 * 267288",
    ".    zhang , c. ( 2010 ) . .",
    "_ the annals of statistics _ * 38 * 894942 .",
    "suppose that @xmath12 is an @xmath13 matrix with iid rows @xmath532 and that @xmath44 is a @xmath18 positive definite matrix .",
    "then @xmath533 is a @xmath534 random matrix .",
    "let @xmath374 . in this supplemental text",
    "we provide formulas for various moments involving @xmath535 that are used in the paper . @xcite and @xcite",
    "provide techniques for computing all such moments .",
    "these techniques are utilized here .",
    "[ [ the - symmetric - group - and - a - formula - for - a - class - of - moments - involving - w ] ] the symmetric group and a formula for a class of moments involving @xmath535 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    let @xmath536 denote the symmetric group on @xmath153 elements . then each permutation @xmath537 can be uniquely as a product of disjoint cycles @xmath538 , where @xmath539 , @xmath540 , and all of the @xmath541 are distinct .",
    "let @xmath542 be @xmath18 symmetric matrices and define the polynomial @xmath543 theorem 1 in @xcite and proposition 1 in @xcite give the following formula : @xmath544 this is our main tool for deriving the explicit formulas in the next section .",
    "now let @xmath548 be an orthonormal basis of @xmath549 , with @xmath550 . define the @xmath551 symmetric matrices @xmath552 and @xmath553 , @xmath554 .",
    "then @xmath555 since @xmath556 , the formula ( [ letac ] ) and lemma 1 below imply @xmath557 to prove ( [ prop1h ] ) , observe that @xmath558 for ( [ prop1i ] ) , equation ( [ letac ] ) implies @xmath559 to prove ( [ prop1j ] ) , first notice that @xmath560 and that ( [ letac ] ) implies @xmath561 it is clear that @xmath562 thus , by lemma 1 , @xmath563 combining this with ( [ prop1pfb ] ) yields @xmath564 the proof of ( [ prop1k ] ) is similar to the proof of ( [ prop1j ] ) . by",
    "( [ letac ] ) and lemma 1 , @xmath565 it follows that @xmath566 to prove ( [ prop1l ] ) , consider the decomposition @xmath567 equation([letac ] ) implies that @xmath568 since @xmath569 it follows that @xmath570 by lemma 1 , @xmath571 using these results with ( [ prop1pfc ] ) we obtain @xmath572 finally , we prove ( [ prop1 m ] ) .",
    "similar to the proof of ( [ prop1k])-([prop1l ] ) , we have the decomposition @xmath573 by ( [ letac ] ) , @xmath574 it follows that @xmath575 where @xmath576 one can easily see that @xmath577 thus , @xmath578 it only remains to evaluate the @xmath579 .",
    "it follows from lemma 1 that @xmath580 combining this with ( [ prop1pfd ] ) , we conclude that @xmath581      the identity ( [ lemma1a ] ) is trivial . to prove ( [ lemma1b ] ) , we have @xmath585 equation ( [ lemma1c ] ) follows from @xmath586 for ( [ lemma1d ] ) , we have @xmath587 to prove ( [ lemma1e])-([lemma1f ] ) , observe that @xmath588 and @xmath589 finally , to prove ( [ lemma1g])-([lemma1h ] ) , we have @xmath590 and @xmath591"
  ],
  "abstract_text": [
    "<S> residual variance and the signal - to - noise ratio are important quantities in many statistical models and model fitting procedures . </S>",
    "<S> they play an important role in regression diagnostics , in determining the performance limits in estimation and prediction problems , and in shrinkage parameter selection in many popular regularized regression methods for high - dimensional data analysis . </S>",
    "<S> we propose new estimators for the residual variance , the @xmath0-signal strength , and the signal - to - noise ratio that are consistent and asymptotically normal in high - dimensional linear models with gaussian predictors and errors , where the number of predictors @xmath1 is proportional to the number of observations @xmath2 . </S>",
    "<S> existing results on residual variance estimation in high - dimensional linear models depend on sparsity in the underlying signal . </S>",
    "<S> our results require no sparsity assumptions and imply that the residual variance may be consistently estimated even when @xmath3 and the underlying signal itself is non - estimable . </S>",
    "<S> basic numerical work suggests that some of the distributional assumptions made for our theoretical results may be relaxed . </S>"
  ]
}