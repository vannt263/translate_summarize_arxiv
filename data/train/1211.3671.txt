{
  "article_text": [
    "a crucial step in understanding how a complex network operates is inferring its connectivity from observables in a systematic and controlled way .",
    "this learning of the connections from data is an inverse problem .",
    "recently , with the ongoing growth of available data , especially in biological systems , such inverse problems have attracted a lot of attention in statistical physics community .",
    "examples of applications include the reconstruction of a gene regulation network from gene expression levels @xcite and identification of the protein - protein interactions from the correlations between amino acids @xcite .",
    "one proxy for such a problem is the inverse ising model , where the parameters of the model ( fields and interactions ) are inferred from observed spin history .",
    "there has been a long history of inferring gibbs equilibrium models , such as the equilibrium ising model , where the fields and couplings are inferred from the measured means and correlations @xcite .",
    "the methods developed for learning the connections in these models as originally formulated , do not assume any prior belief about the network architecture and they only use the data to decide about that .",
    "recently , though it has been shown that the connections can be inferred much more efficiently when a sparse prior , specifically the l@xmath0 regularizer , is taken into account @xcite . however , our theoretical understanding of how l@xmath0 regularization works is limited .    in many practical applications , equilibrium models would be of limited use . for instance",
    ", most biological systems operate in out - of - equilibrium regimes .",
    "consequently , equilibrium models are not usually good candidates to infer the interactions and may fall short even as generative models for describing the statistics of the data @xcite .",
    "several recent studies have thus moved to kinetic models , prescribing exact and approximate learnings for inferring the connections in non - equilibrium models @xcite . however",
    "this body of work has not yet exploited the potential power of l@xmath0 regularization in inferring the connections .    in this paper , focusing on the asynchronously updated ising model , we will describe how l@xmath0 regularization helps in inferring the connections in a non - equilibrium model .",
    "we try to shed light on the mechanics by which the regularization works through developing approximate ways of performing l@xmath0 .",
    "we study how the regularization shrink the connections gradually with the increase of regularization parameter .",
    "the paper is organized as follows : the dynamics and the underlying network are described in section [ sec2 ] , an l@xmath0-regularized learning rule for an asynchronously updated kinetic ising model is described in section [ sec3 ] , approximate learning algorithms , based on an expansion of the cost function of section [ sec3 ] , are derived in section [ sec4 ] , and the performance of the learning rules is studied in section [ sec5 ] .",
    "the effects of different coupling strengths are explored in section [ sec6 ] .",
    "a discussion is given in section [ sec7 ] .",
    "we consider a kinetic ising model endowed with glauber dynamics @xcite .",
    "glauber dynamics describes the evolution of the joint probability of the spin states @xmath1 in time @xmath2 , following the master equation    @xmath3    where , @xmath4}=\\frac{\\gamma_0}{2}\\left[1-s_i(t)\\tanh h_i(t)\\right],\\ ] ] is the probability for spin @xmath5 to change its state from @xmath6 to @xmath7 during the time interval @xmath8 . here , we choose time units so that @xmath9 .",
    "the quantity @xmath10 is the instantaneous field acting on spin @xmath5 .",
    "the external field @xmath11 can be dependent on time , but for the sake of simplicity we focus on the stationary case , i.e. , time - independent @xmath11 , here .",
    "one way to implement the glauber dynamics is as follows : make a discretization of the evolution process with very small time steps @xmath12 . at each step , every spin is selected for updating with probability @xmath13 . for @xmath14 , almost certainly only one spin at a time will be updated .",
    "the next value of the spin selected for updating is chosen according to @xmath15}{2\\cosh h_i(t ) } = { \\mbox{$\\frac{1}{2}$ } } [ 1 + s_i(t+\\delta t ) \\tanh h_i(t)].\\ ] ] note that the updated spin might not change its value ; an update is not necessarily a flip . in this paper",
    ", we will take the dynamics to be defined in this doubly stochastic way and assume that the data accessible to us include both the times at which every spin is selected for updating ( determined by an independent poisson processes for each spin ) and the result of those updates ( whose outcomes are given by ( [ eq : update ] ) , i.e. , the spin history ) .",
    "the problem may also be treated by other algorithms that only assume knowledge of the spin history ( not of all the update times ) ; these are discussed in other work @xcite , but we do not consider them here . in our computations , in order not to waste lots of time not updating any spins , we have , at each time step , chosen exactly one spin at random for updating . for finite @xmath16",
    "this is not exactly the dynamics described above , but we do not see any difference when we compare the results of our computations with those done following the correct dynamics exactly .    we study a diluted binary asymmetric sherrington - kirkpatrick ( sk ) model with these dynamics . for the original sk model , the pairwise interactions @xmath17 between spins",
    "@xmath5 and @xmath18 were i.i.d .",
    "gaussian variables ( except @xmath19 ) with variance @xmath20 and mean 0 .",
    "in the model we study here , the network is diluted , @xmath17 is independent of @xmath21 , and the interactions vary only in sign , not in magnitude : each coupling has the distribution @xmath22 where @xmath23 is the average in - degree ( and out - degree ) .",
    "we are interested in sparse networks , i.e. , @xmath24 . in our computations",
    ", we use @xmath25 and @xmath26 . furthermore ,",
    "as mentioned above , we model asymmetrically coupled spins , taking each @xmath17 independent of @xmath21 .",
    "this model can have a stationary distribution ( and does for the parameters we use here ) , but it is not of gibbs - boltzmann form , and no simple expression for it is known .",
    "as described above , we suppose we know the full history of the system  both the @xmath27 , with @xmath28 and @xmath29 , where @xmath30 is the data length , and the update times @xmath31",
    ". we can reconstruct the couplings @xmath17 and external fields @xmath11 by performing the gradient descent on the negative log - likelihood of this history , which is given by @xmath32 .",
    "\\label{eq : l0}\\ ] ] we can minimize the log - likelihood by simple gradient descent with a learning rate @xmath33 : @xmath34s_j(\\tau_i ) .",
    "\\label{eq : learning_nol1}\\ ] ] this equation includes the learning rule for the external field @xmath11 under the convention @xmath35 , @xmath36 .",
    "it has the same form as that for a synchronous model , except that changes for spin @xmath5 are made only at times @xmath37 .    for finite @xmath30",
    ", this procedure will in general produce a densely - connected network . to sparsify it , we add a simple regularization term that penalizes dense connectivity in a controllable fashion .",
    "we then minimize a cost function @xmath38 where the first term is the negative log - likelihood and the second term is the @xmath39 norm .",
    "there are several efficient methods have been used to minimize the cost function ( [ eq : cost ] ) , e.g. , the interior - point method @xcite . however , in order to see how l@xmath0 regularization works in detail , we study a simple gradient descent algorithm here .",
    "gradient descent on this cost function leads to an additional term in the learning rule for couplings : @xmath40s_j(\\tau_i ) -\\lambda { \\rm sgn}(j_{ij})\\right\\}.\\ ] ]    the log - likelihood function @xmath41 is smooth and convex as a function of the @xmath17 and @xmath11 , so the cost function is concave except on the hyperplanes where any @xmath42 .",
    "this leads to complications in the minimization whenever a minimum of @xmath43 is at @xmath42 : we deal with this problem by setting @xmath42 whenever the change ( [ eq : learning_rule ] ) would cause @xmath17 to change sign .",
    "then , if the minimum of @xmath43 truly lies at this @xmath42 , the estimated @xmath17 will oscillate between zero and a small nonzero value ( using @xmath44 ) .",
    "however , the size of these oscillations is proportional to the learning rate @xmath33 , so a sufficiently small @xmath33 ensures that these couplings can be pruned by a simple rounding procedure , with negligible chance of removing coupling that are not truly zero at the minimum . in the case",
    "that @xmath17 is not zero at the minimum , its estimated value will continue to change and it will move toward its optimal value after the step where it was set to zero .",
    "another way to deal with the non - differentiability of the cost function ( [ eq : cost ] ) is to use @xmath45 as the penalty term and take the limit @xmath46 .",
    "this term leads to the replacement of the @xmath47 by @xmath48 . for any non - zero @xmath49 , this modified cost function is totally convex .",
    "we checked some of our computations by doing the regularization this way .",
    "no difference between these results and those done as described above was found .",
    "we can get some insight into the dynamics of the learning with regularization by expanding the cost function ( [ eq : cost ] ) to second order around its minimum @xmath50 when @xmath51 . up to a constant ,",
    "we have @xmath52 where @xmath53 , @xmath54 is the number of updates per spin , @xmath55 , and @xmath56 since the quantities in the sum in ( [ eq : fisher1 ] ) are insensitive to whether spin @xmath5 is updated , the average over updates may safely be replace by an average over all times , @xmath57 the fisher information matrix for spin @xmath5 , which is a more robust quantity .    minimizing ( [ eq : cost1 ] ) , we get , to first order in @xmath58 , @xmath59 solving this equation for @xmath60,we obtain : @xmath61^{-1}_{jk}{\\rm sgn}(j_{ik}^0).\\ ] ] this equation shows how the regularization term shrinks the magnitudes of the couplings .    in the weak coupling limit ( small @xmath62 or , equivalently , high temperature ) , @xmath63^{-1}_{jk } = \\delta_{jk}$ ] , so the @xmath17 are just shrunk in magnitude proportional to @xmath58 until they reach zero and are pruned .",
    "this is a trivial kind of regularization : we know that the couplings that survive the pruning procedure the longest are simply the ones with the biggest initial absolute values . in this case , there is no need to go through the elaborate learning - with - regularization procedure of ( [ eq : learning_rule ] ) .",
    "however , at larger coupling this is not the case .",
    "some @xmath17 will be shrunk more rapidly than others , depending on the size and signs of the terms in the sum in ( [ fisher ] ) .",
    "based on the quadratic expansion ( [ eq : cost1 ] ) , we can carry out the pruning in an approximate alternative fashion , as follows : starting from @xmath64 and a small value of @xmath58 , we calculate the shifts @xmath60 by ( [ fisher ] ) and remove any @xmath17 that would go though zero . starting from the resulting new @xmath17s ( some of them now equal to zero ) , increase @xmath58 , recalculate the fisher information matrix and calculate new shifts in the parameter values . again",
    "remove any couplings that change sign , and continue until the desired degree of pruning has been achieved .",
    "this amounts to numerical integration of the differential equation , describing a kind of dynamics of regularization under increasing @xmath58 .",
    "@xmath65^{-1}_{jk}{\\rm sgn}(j_{ik}(\\lambda ) ) .",
    "\\label{eq : djdlambda}\\ ] ] note that this procedure requires only equal - time average quantities , unlike the full computation following the learning rule ( [ eq : learning_rule ] ) .",
    "if one knows _ a priori _ what value of @xmath58 to use , it is probably not an advantage to use this algorithm .",
    "one can simply do the full computation once , at that value , while with this approximate algorithm we have to simulating the model to estimate the fisher matrices at all the intermediate @xmath58s in integrating ( [ eq : djdlambda ] ) . on the other hand",
    ", one may not know the optimal @xmath58 .",
    "it then becomes necessary to explore the regularized model over some wide range of @xmath58 . in this case",
    ", the approximate algorithm will have a speed advantage , because it only requires a learning loop at the initial @xmath58 ( zero , in the case described here ) .",
    "we consider the problem of identifying the positive and negative couplings in the network , i.e. , correctly classifying every potential bond as @xmath66 , @xmath67 or 0 .",
    "consider first the couplings @xmath17s found with no regularization , i.e. , @xmath68 .",
    "for given @xmath62 , @xmath23 and @xmath16 , for very large @xmath69 the inferred @xmath17 will be very close to the true ones .",
    "a histogram of their values will have three narrow peaks around @xmath70 and @xmath71 , and it will be trivial to identify the true nonzero couplings and their signs ( figure  1a , b ) . in the opposite limit ( small @xmath69 ) , the data are not sufficient to estimate the couplings well",
    ". the histogram will be unimodal , and it will be more or less hopeless to solve the problem , even with the help of l@xmath0 regularization ( figure  1c , d ) .",
    "the interesting case is that of intermediate data length , for which the partial histograms from the zero and nonzero - j classes overlap , but the separations between their means are not much smaller than their widths ( figure  1e , f ) .",
    "we would also like to avoid the trivial weak - coupling case mentioned above , so in the following results we report here we take @xmath72 .",
    "for this case , a @xmath69 of 200 realizes the interesting intermediate - data - length case .",
    "regularization , @xmath72 for various data lengths .",
    "top : @xmath73 updates / spin .",
    "middle : @xmath74 .",
    "bottom : @xmath75 . in each row",
    ", the left panel shows a histogram of the @xmath17 obtained , and the right panel shows these sorted according to whether the bond was present ( green ) or absent ( red ) in the network that generated the data.,scaledwidth=80.0% ]    based on @xmath76s inferred with @xmath77 as shown in figure [ fig1]e and f , four pruning methods were employed .",
    "figure [ fig2 ] shows how the @xmath76s inferred by each method vary as the regularization coefficient @xmath58 is increased . here",
    ", we only show positive @xmath78s ; graphs of the negative ones would look like the ones shown , reflected through the horizontal axis .",
    "bonds actually present in the model ( a realization of ( [ eq : structure ] ) ) are plotted in black and bonds which are absent in red .",
    "figure  [ fig2]a shows the @xmath76s inferred using exact learning with @xmath39 regularization ( [ eq : learning_rule ] ) .",
    "it is apparent that the pruning process for the case shown here is not trivial in the way it would be in the weak - coupling limit : some true ( black ) bonds , for which rather small values were inferred at @xmath68 because of insufficient data , are  rescued \" ( they fall off more slowly with @xmath58 than red ones with nearly the same initial inferred @xmath76s ) , and some spurious ( red ) bonds with high inferred values at @xmath68 are driven to zero faster than black ones with the same initial inferred @xmath76s . thus , the red and black lines tend to be separated , and one can do the pruning almost correctly just by turning @xmath58 up until the desired number of bonds have been removed .    figure  [ fig2]b shows the inferred @xmath76s using the quadratic expansion ( [ eq : cost1 ] ) in the fashion described at the end of sec .  [ sec4 ] .",
    "we call this  approximation 1 \" . the qualitative features of figure  [ fig2]a are apparently reproduced in this approximation .    figure  [ fig2]c shows the result when off - diagonal elements of @xmath79^{-1}_{jk}$ ] are ignored in ( [ eq : djdlambda ] ) .",
    "we refer to this procedure as  approximation 2 \" .",
    "the separation of red and black curves is not as good in this case .",
    "we also tried making a diagonal approximation of the fisher matrix itself , rather than its inverse : @xmath80 by @xmath81 .",
    "however , this gave much worse results ( not shown ) than making the diagonal approximation on the inverse fisher matrix .",
    "in figure  [ fig2]c , it is evident that the slopes of the @xmath82 curves vary rather slowly with @xmath58 .",
    "therefore , we also tried a linear extrapolation based on the slopes of the curves in figure  [ fig2]c at @xmath83 . we denote this method as  approximation 3 \" . to the extent that this simple procedure works",
    ", one can identify the nonzero bonds with very little computation : one needs only to do the learning at @xmath83 ( to get the @xmath82 ) and calculate the fisher matrices ( to get the @xmath84 ) .",
    "figure  [ fig2]d shows the result of this minimal algorithm .    for approximation 3 , the inferred @xmath76s that have been shrunk to zeros",
    "have no chance to be rescued again .",
    "but for the other three approaches , the inferred @xmath76s for the positive ones ( as shown in black lines ) have that chance to be back again with increasing of @xmath58 .",
    "however , in the results presented in figure [ fig2 ] , we havent observe such phenomena .",
    "one could also try similar linear extrapolation based on the initial slopes of the upper panels of figure  [ fig2 ] .",
    "however , these curves show significant curvature for @xmath85 or so , so the initial slopes are not good guides to the ultimate fate of the bonds at large @xmath58 , and we do not present any results for these methods .     for four methods : ( a ) full @xmath39 regularization using ( [ eq : learning_rule ] ) , ( b ) integration of ( [ eq : djdlambda ] ) , ( c ) integration of ( [ eq : djdlambda ] ) with diagonal approximation of the inverse fisher matrix , ( d ) linear extrapolation in @xmath58 of the curves in ( c ) .",
    "black lines represent bonds actually presents , while red lines represent ones equal to zero in the network used to generate the data .",
    "we show equal number of red and black ones.,scaledwidth=90.0% ]    in what follows , we quantify the performances of these four pruning algorithms .",
    "for the three classes of bonds in the actual network , @xmath67 , @xmath66 and 0 , we can compute the empirical classification errors .",
    "these errors can be either false positives ( fp ) ( identifying a bond which is really absent as present ) , or false negatives ( fn ) ( identifying a bond which is actually present as absent ) .",
    "in addition , a @xmath66 bond could be misclassified as @xmath67 or vice versa , but this does not happen for the data length we are studying here .    at @xmath68 , where in general all bonds",
    "will be estimated to have nonzero values , there will be no fns and @xmath86 fps . in the other limit @xmath87 ,",
    "all bonds will be removed , so there will be @xmath88 fns and no fps .",
    "the empirical numbers of fps and fns versus @xmath58 are plotted in the left panels of figure [ fig3 ] .",
    "the total misclassification error , i.e. , the sum of the fps and fns ( shown in the right panel of figure [ fig3 ] ) has a minimum at @xmath89 for full l@xmath0 regularization . for approximation 1",
    "we find a minimum at @xmath90 , while for approximation 2 the minimum is at @xmath91 , and for approximation 3 it is at @xmath92 .    .",
    "left column : number of misclassified @xmath67 , @xmath66 , and @xmath70 ( absent ) bonds .",
    "numbers of false negatives for @xmath67s are shown in green , for @xmath66s in red , and false positives for zero - bonds in blue .",
    "right column : the sum of false negatives and false positives versus @xmath58 . because the js are symmetrically distributed",
    ", red and green curves almost coincide , with mostly only the green ones visible here . from top to bottom : full l@xmath0 regularization and approximations 1 , 2 , 3 , respectively.,scaledwidth=60.0% ]    in applications , fns and fps may not have the same cost associated with them : it may be appropriate to weight the blue and green curves in the left - hand panels of figure  [ fig3 ] differently . to compare algorithms in a more general way that is not specific to a particular relative weighting ,",
    "we calculate receiver operating characteristic ( roc ) curves for them . for a given @xmath58 , the false positive rate ( fpr )",
    "is defined as the number of fps divided by the actual number of zero bonds , and the false negative rate ( fnr ) is defined as the number of fns divided by the number of actual number of non - zero bonds . a true positive ( tp )",
    "is the identification of a bond which is actually present as present , and the true positive rate ( tpr ) is the number of tps normalized by the actual number of bonds present .",
    "it is equal to @xmath93 .",
    "the roc curve is a plot of tpr versus fpr .",
    "each value of @xmath58 gives one point on the curve . in figure",
    "[ fig4 ] , we plot the roc curves for all of our methods .",
    "we also measure the performance of the different methods quantitatively by defining an error measure , @xmath94 : @xmath95 the values of @xmath94s for full l@xmath0 and approximations 1 , 2 and 3 are 0.03 , 0.06 , 0.08 , 0.09 respectively .",
    "thus , full l@xmath0 algorithm performs best , followed by approximation 1 .",
    "approximation 2 works worse than them and it is only little better than approximation 3 for most values of @xmath58 , as can be seen in figure  [ fig4 ] .    to establish a baseline for the goodness of our methods",
    ", we also performed a simple pruning procedure that does not require any l@xmath0 regularization calculation . for a given cut value @xmath96",
    ", we identify the bonds whose @xmath76s lie in the range @xmath97 $ ] as absent and those outside that interval as present .",
    "the green @xmath76s in figure  [ fig1]f which lie within the interval are fns and the red ones outside the interval are fps .",
    "varying @xmath96 , we obtain an roc curve .",
    "we refer to this procedure as  j0-cut \" .",
    "the curve with light blue squares in figure  [ fig4 ] is for it .",
    "the curve nearly coincides with that for approximation 3 .",
    "its @xmath94 is 0.09 , the same as that of approximation 3 .",
    "thus , this trivial method works as well as approximation 3 .",
    "regularization , approximations 1 , 2 , 3 , and the j0-cut method are shown in red , green , blue , pink and light blue , respectively.,scaledwidth=60.0% ]",
    "the above results were all obtained for @xmath72 .",
    "we are also interested in how the different regularization methods behave for other @xmath62 values . as we mentioned in section [ sec4 ] that in the weak couplings limit @xmath98 , the inverse of the fisher information matrices for different @xmath5s are the same , equal to identity , thus the regularizations by approximation methods will be equal to that of the trivial j0-cut .",
    "we repeat the calculation roc curve for two other @xmath62s : 1 and 1/2 .",
    "to get problem of the same level of difficulty , we first calculate the roc curve by j0-cut method and make sure that the area under the curves are the same for each @xmath62 .",
    "a set of data lengths @xmath99 , for which the same areas under the curves can be obtained are found to be 11608 , 8862 and 6730 for @xmath100 , @xmath101 and 1 respectively .",
    "no bigger @xmath62 values are tested because they need shorter data length to get the same area , however , short data length increases the difficulties of the learning rule .",
    "the roc curves for all three @xmath62s are shown by the dashed lines in both figure [ fig5](a ) and ( b ) .",
    "the @xmath94 value are all around 0.94 for them .    with this stating point , we calculate the roc curves for full l@xmath0 regularization and approximation 1 for all three @xmath62s .",
    "as noted in figure [ fig4 ] , the regularization by full l@xmath0 and approximation 1 have obviously better performances compared with that of the trivial j0-cut method , thus we next focus on this two methods to test whether regularization helps more at larger @xmath62 . in figure [ fig5](a ) ,",
    "the solid lines represent the roc curves by full l@xmath0 regularization .",
    "the @xmath94 are 0.033 for @xmath100 , 0.023 for @xmath72 and 0.012 for @xmath102 .",
    "similarly , in figure [ fig5](b ) , the solid lines are for roc curves by approximation 1 , with area 0.047 , 0.039 and 0.035 for @xmath100 , @xmath101 and 1 respectively .",
    "as shown by the solid lines in both figure [ fig5](a ) and ( b ) , we can see that with increasing of @xmath62 , the regularization methods work better . both of them perform better than the trivial method , which is accordant with the results shown in figure [ fig4 ] .",
    "regularization ( left , solid lines ) and approximations 1 ( right , solid lines ) with @xmath103 , @xmath104 , @xmath105 respectively .",
    "the green lines for @xmath103 , red for @xmath106 and black for @xmath102 .",
    "corresponding dashed lines are for j0-cut method of these @xmath62s.,scaledwidth=99.0% ]",
    "we have studied the reconstruction of sparse asynchronously updated kinetic ising networks . with finite data length , simple maximization of the log likelihood of the system history",
    "will infer nonzero values to many bonds that are actually not present . for large data length , this is generally not a problem , since the inferred bond distribution will consist of well - separated peaks . the ones with the smallest absolute values",
    "can then safely be identified as spurious and removed `` by hand '' . however , for smaller data lengths , these peaks can overlap strongly , and nontrivial methods are required to make an optimal pruning of the inferred coupling set .",
    "here we used l@xmath0 regularization to do this , minimizing a cost function that includes the l@xmath0-norm of the parameter vector as a penalty term .",
    "we performed this minimization in four ways , one exact and the other three involving various degrees of approximation .",
    "calculations on a model network at intermediate coupling strength revealed that the exact l@xmath0 regularization classified the bonds significantly better than a naive method based on retaining the strongest bonds .",
    "our approximation 1 was somewhat worse than the exact algorithm , but still significantly better than the naive method .",
    "our other two approximations , obtained by successive simplifications of approximation 1 , however , did not perform measurably better than the naive way , as measured by the areas under their roc curves .",
    "these conclusions are general to various coupling strength we used .",
    "the regularizations helps more with stronger coupling strengths .",
    "this work is the first that we know of that takes a detailed look at how l@xmath0regularization works in the non - equilibrium model , by studying how bonds are removed successively as the regularization parameter @xmath107 is increased .",
    "some insight into how this happens was made possible by studying the quadratic expansion of the cost function about its minimum , which also led to our relatively successful approximation 1 .",
    "the process would have been more transparent if we could have made further simplifying approximations , as we did for approximation 2 , where we neglected off - diagonal elements of the inverse fisher matrices .",
    "the fact that this approximation performed rather poorly ( while approximation 1 did quite well ) indicates that the off - diagonal terms in ( [ eq : djdlambda ] ) are necessary , and we lack generic insight about them .",
    "we performed our analysis here on a rather simple model network .",
    "however , we expect that our methods will be useful in analyzing date from a wide variety of biological , financial , and other complex systems with sparse structure .",
    "we are grateful to e. aurell and m. alava for useful discussions about the work and nordita and niels bohr institute for hospitality . the work of h .-",
    "z. was supported by the academy of finland as part of its finland distinguished professor program , project 129024/aurell ."
  ],
  "abstract_text": [
    "<S> the couplings in a sparse asymmetric , asynchronous ising network are reconstructed using an exact learning algorithm . </S>",
    "<S> l@xmath0 regularization is used to remove the spurious weak connections that would otherwise be found by simply minimizing the minus likelihood of a finite data set . in order to see how l@xmath0 regularization works in detail , we perform the calculation in several ways including ( 1 ) by iterative minimization of a cost function equal to minus the log likelihood of the data plus an l@xmath0 penalty term , and ( 2 ) an approximate scheme based on a quadratic expansion of the cost function around its minimum . in these schemes , </S>",
    "<S> we track how connections are pruned as the strength of the l@xmath0 penalty is increased from zero to large values . </S>",
    "<S> the performance of the methods for various coupling strengths is quantified using roc curves .    _ * keywords * _ : l@xmath0 regularization , non - equilibrium ising model , asynchronous update , asymmetric , sparse , sherrington - kirkpatrick ( sk ) model </S>"
  ]
}