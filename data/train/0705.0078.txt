{
  "article_text": [
    "dynamical systems are often classified with respect to their long - time behaviors , which might be , e.g. , chaotic or regular @xcite .",
    "of special interest are attractors , cycles and limiting cycles , as they determine the fate of all orbits starting within their respective basins of attraction .",
    "attractor states play a central role in the theory of recurrent neural networks , serving the role of memories with the capability to generalize and to reconstruct a complete memory from partial initial information @xcite .",
    "attractor states in recurrent neural networks face however a fundamental functional dichotomy , whenever the network is considered as a functional subunit of an encompassing autonomous information processing system , _ viz _ an autonomous cognitive system @xcite .",
    "the information processing comes essentially to a standstill once the trajectory closes in at one of the attractors .",
    "restarting the system ` by hand ' is a viable option for technical applications of neural networks , but not within the context of autonomously operating cognitive systems .",
    "one obvious way out of this dilemma would be to consider only dynamical systems without attractor states , i.e.with a kind of continously ongoing ` fluctuating dynamics ' , as illustrated in fig .",
    "[ fig_transstates ] , which might possibly be chaotic in the strict sense of dynamical system theory .",
    "the problem is then , however , the decision - making process . without",
    "well defined states , which last for certain minimal periods , the system has no definite information - carrying states onto which it could base the generation of its output signals .",
    "it is interesting to note in this context , that indications for quasi - stationary patterns in cortical neural activity have been observed @xcite .",
    "these quasi - stationary states can be analyzed using multivariate time - series analysis , indicating self - organized patterns of brain activity @xcite .",
    "interestingly , studies of eeg recordings have been interpreted in terms of brain states showing aperiodic evolution states going through sequences of attractors that on access support the experience of remembering @xcite .",
    "these findings suggest that ` transient state dynamics ' , as illustrated in fig .",
    "[ fig_transstates ] , might be of importance for cortical firing patterns .",
    "it is possible , from the viewpoint of dynamical system theory , to consider transient states as well defined periods when the orbit approaches an attractor ruin . with a transient attractor , or attractor ruin ,",
    "we denote here a point in phase space which could be turned continously into a stable attractor when tuning certain of the parameters entering the evolution equations of the dynamical system .",
    "the dynamics slows down close to the attractor ruin and well defined transient states emerge within the ensemble of dynamical variables .",
    "the notion of transient state dynamics is related conceptually to chaotic itinerancy @xcite , a term used to characterize dynamical systems for which chaotic high - dimensional orbits stay intermittently close to low - dimensional attractor ruins for certain periods .",
    "instability due to dynamic interactions or noise is necessary for the appearance of chaotic itinerancy .",
    "having argued that transient - state dynamics might be of importance for a wide range of real - world dynamical systems , the question is then of how to generate such kind of dynamical behavior in a controllable fashion and in a manner applicable to a variety of starting systems .",
    "viz _ we are interested in neural networks which generate transient states dynamics in terms of a meaningful time series of states approaching arbitrarily close predefined attractor ruins .",
    "the approach we will follow here is to start with an original attractor neural network and to transform then the set of stable attractors into transient attractors by coupling to auxiliary local variables , which we denote ` reservoirs ' , governed by long time scales .",
    "we note , that related issues have been investigated in the context of discrete - time , phase coupled oscillators @xcite , for networks aimed at language processing in terms of ` latching transitions ' @xcite , and in the context of ` winnerless competitions ' @xcite .",
    "further examples of neural networks capable of generating a time - series of subsequent states are neural networks with time - dependent asymmetric synaptic strengths @xcite or dynamical thresholds @xcite .",
    "we also note that the occurrence of spontaneous fluctuating dynamics has been studied @xcite , especially in relation to the underlying network geometry @xcite .",
    "an intrinsic task of neural networks is to learn and to adapt to incoming stimuli .",
    "this implies , for adaptive neural networks , a continuous modification of their dynamical properties .",
    "the learning process could consequently take the network , if no precautions are taken , out of its intended working regime , the regime of transient state dynamics . here",
    "we will show that it is possible to formulate local learning rules which keep the system in its proper dynamical state by optimizing continously its own working point . to be concrete ,",
    "let us denote with @xmath0 the average duration of quasi - stable transient states and with @xmath1 the typical time needed for the transition from one quasi - stationary state to the next .",
    "the dynamical working point can then be defined as the ratio @xmath2 .",
    "these time scales , @xmath0 and @xmath1 , result , for the network of cortical neurons , from the properties of the individual neurons , which are essentially time - independent , and from the synaptic strengths , which are slow dynamical variables subject to hebbian - type learning @xcite .",
    "it then follows , that the modifications of the inter - neural synaptic strengths have a dual functionality : on one side they are involved in memory storage tasks @xcite , and on the other side they need to retain the working point in the optimal regime . here",
    "we show that this dual functionality can be achieved within a generalized neural network model .",
    "we show that working - point optimization is obtained when the hebbian learning - rule is reformulated as an optimization procedure , resulting in a competition among the set of synapses leading to an individual neuron .",
    "the resulting learning - rule turns out to be closely related to rules found to optimize the memory - storage capacity @xcite .",
    "neural networks with sparse coding , _ viz _ with low mean firing rates , have very large memory storage capacities @xcite .",
    "sparse coding results , in extremis , in a ` one - winner - take - all ' configuration , for which a single unit encodes exactly one memory . in this limit",
    "the storage capacity is , however , reduced again and linearly proportional to the network size , as in the original hopfield model @xcite . here",
    "we opt for the intermediate case of ` clique encoding ' .",
    "a clique is , in terms of graph theory , a fully interconnected subgraph , as illustrated in fig .",
    "[ fig_7sites ] for a 7-site network .",
    "clique encoding corresponds to a ` several - winners - take - all ' setup .",
    "all members of the winning clique mutually excite each other while suppressing the activities of all out - of - clique neurons to zero .",
    "we note , that the number of cliques can be very large . for illustration",
    "let us consider a random erds ",
    "rnyi graph with @xmath3 vertices and linking probability @xmath4 .",
    "the overall number of cliques containing @xmath5 vertices is then statistically given by @xmath6 where @xmath7 is the probability of having @xmath5 sites of the graph fully interconnected by @xmath8 edges and where the last term is the probability that every single of the @xmath9 out - of - cliques vertices is not simultaneously connected to all @xmath5 sites of the clique .",
    "networks with clique encoding are especially well suited for transient state dynamics , as we will discuss further below , and are biologically plausible .",
    "extensive sensory preprocessing is known to occur in the respective cortical areas of the brain @xcite , leading to representations of features and objects by individual neurons or small cell assemblies . in this framework",
    "a site , _ viz _ a neural center , of the effective neural network considered here corresponds to such a small cell assembly and a clique to a stable representation of a memory , by binding together a finite set of features extracted by the preprocessing algorithms from the sensory input stream .",
    ", containing six cliques , ( 0,1 ) , ( 0,6 ) , ( 3,6 ) , ( 1,2,3 ) ( which is highlighted ) , ( 4,5,6 ) and ( 1,2,4,5 ) .",
    "right : as a function of time , the activities @xmath10 ( solid lines ) and the respective reservoirs @xmath11 ( dashed lines ) for the transient state dynamics @xmath12 . for the parameters values",
    "see sect .",
    "[ sec_strict ] .",
    "[ fig_7sites ] , title=\"fig:\",scaledwidth=40.0% ] , containing six cliques , ( 0,1 ) , ( 0,6 ) , ( 3,6 ) , ( 1,2,3 ) ( which is highlighted ) , ( 4,5,6 ) and ( 1,2,4,5 ) .",
    "right : as a function of time , the activities @xmath10 ( solid lines ) and the respective reservoirs @xmath11 ( dashed lines ) for the transient state dynamics @xmath12 . for the parameters values",
    "see sect .",
    "[ sec_strict ] .",
    "[ fig_7sites ] , title=\"fig:\",scaledwidth=45.0% ]      for our study of possible mechanisms of transient state dynamics in the context of neural networks we consider @xmath13 artificial neurons with rate encoding @xmath10 and continuous time @xmath14 $ ] .",
    "let us comment shortly on the last point .",
    "the majority of research in the field of artificial neural networks deals with the case of discrete time @xmath15 @xcite .",
    "we are however interested , as discussed in the introduction , in networks exhibiting autonomously generated dynamical behaviors , as they typically occur in the context of complete autonomous cognitive systems .",
    "we are therefore interested in networks having update rules being compatible with the interaction with other components of a cognitive system .",
    "discrete time updating is not suitable in this context , since the resulting dynamical characteristics ( i ) depend on the choice of synchronous vs.  asynchronous updating and ( ii ) are strongly influenced when effective recurrent loops arise due to the coupling to other components of the autonomous cognitive system .",
    "we therefore consider and study here a model with continuous time .",
    ", see eq .",
    "[ cogsys_ri ] , of sigmoidal form @xcite with respective turning points @xmath16 , a width @xmath17 and a minimal value @xmath18 .",
    "right : distribution of the synaptic strength for the inhibitory links @xmath19 and the active excitatory links @xmath20 leading to clique encoding .",
    "note , that @xmath21 is not a strict upper bound , due to the optimization procedure ( [ cogsys_w_l_dot_opt ] ) .",
    "the shaded area just below zero is related to the inactive @xmath22 , see eqs .",
    "( [ cogsys_z_t ] ) and ( [ cogsys_w_l_dot_opt ] ) .",
    ", title=\"fig:\",scaledwidth=30.0% ] , see eq .",
    "[ cogsys_ri ] , of sigmoidal form @xcite with respective turning points @xmath16 , a width @xmath17 and a minimal value @xmath18 .",
    "right : distribution of the synaptic strength for the inhibitory links @xmath19 and the active excitatory links @xmath20 leading to clique encoding .",
    "note , that @xmath21 is not a strict upper bound , due to the optimization procedure ( [ cogsys_w_l_dot_opt ] ) .",
    "the shaded area just below zero is related to the inactive @xmath22 , see eqs .",
    "( [ cogsys_z_t ] ) and ( [ cogsys_w_l_dot_opt ] ) .",
    ", title=\"fig:\",scaledwidth=30.0% ]      we denote the state variables encoding the activity level by @xmath10 and assume them to be continuous variables , @xmath23 $ ] .",
    "additionally , we introduce for every site a variable @xmath24 $ ] , termed ` reservoir ' , which serves as a fatigue memory facilitating the self generated time series of transient states .",
    "we consider the following set of differential equations : @xmath25 x_j\\ \\ \\ \\\\ \\label{cogsys_phidot } \\dot\\varphi_i & = &    \\gamma_\\varphi^+\\ , ( 1-\\,\\varphi_i)(1-x_i / x_c ) \\theta(x_c - x_i ) \\,-\\ , \\gamma_\\varphi^-\\,\\varphi_i\\,\\theta(x_i - x_c ) \\\\",
    "\\label{cogsys_z_t } z_{ij } & = & -|z|\\,\\theta(-w_{ij})\\end{aligned}\\ ] ] we now discuss some properties of ( [ cogsys_xdot]-[cogsys_z_t ] ) , which are suitably modified lotka - volterra equations :    *   + eqs .",
    "( [ cogsys_xdot]-[cogsys_phidot ] ) respect the normalization @xmath26 $ ] , due to the prefactors @xmath27,@xmath28 , @xmath29 and @xmath30 in eqs .",
    "( [ cogsys_xdot ] ) and ( [ cogsys_phidot ] ) , for the respective growth and depletion processes .",
    "@xmath31 is the heaviside - step function : @xmath32 and @xmath33 . *",
    "+ the synaptic strength is split into an excitatory contribution @xmath34 and an inhibitory contribution @xmath35 , with @xmath36 being the primary variable : the inhibition @xmath37 is present only when the link is not excitatory ( [ cogsys_z_t ] ) .",
    "we have used @xmath38 , _ viz _ @xmath39 throughout the manuscript , which then defines the inverse reference unit for the time development . *",
    "( [ cogsys_xdot ] ) and ( [ cogsys_ri ] ) describe , in the absence of a coupling to the reservoir via @xmath40 , a competitive winners - take - all neural network with clique encoding .",
    "the system relaxes towards the next attractor made up of a clique of @xmath5 sites @xmath41 connected via excitatory links @xmath42 ( @xmath43 ) . *   + the reservoir functions @xmath44 $ ] govern the interaction in between the activity levels @xmath27 and the reservoir levels @xmath29 .",
    "they may be chosen as washed out step functions of sigmoidal form @xcite , with a suitable width @xmath45 and inflection points @xmath46 , see fig .",
    "[ cogsys_fig_gaps ] . *",
    "+ the reservoir levels of the winning clique depletes slowly , see eq .",
    "( [ cogsys_phidot ] ) and fig .",
    "[ fig_7sites ] , and recovers only once the activity level @xmath27 of a given site has dropped below @xmath47 , which defines a site to be active when @xmath48 .",
    "the factor @xmath49 occurring in the reservoir growth process , see the r.h.s .  of ( [ cogsys_phidot ] ) , serves for a stabilization of the transition between two subsequent memory states .",
    "when the activity level @xmath27 of a given center @xmath50 drops below @xmath47 , it can not be reactivated immediately ; the reservoir can not fill up again for @xmath51 , due to the @xmath49 in ( [ cogsys_phidot ] ) .",
    "*   + a separation of time scales is obtained when the @xmath52 are much smaller than the typical strength of an active excitatory link , i.e.  of a typical @xmath53 , leading to transient state dynamics .",
    "once the reservoir of a winning clique is depleted , it looses , via @xmath54 , its ability to suppress other sites and the mutual intra - clique excitation is suppressed via @xmath55 . *   + there are no stationary solutions with @xmath56 ( @xmath57 ) for eqs .",
    "( [ cogsys_xdot ] ) and ( [ cogsys_phidot ] ) , whenever @xmath58 do not vanish and for any non - trivial coupling functions @xmath59 $ ] .",
    "+ when decoupling the activities and the reservoir by setting @xmath60 one obtains stable attractors with @xmath61 and @xmath62 for sites belonging / not - belonging to the winning clique , compare fig .",
    "[ fig_escapemanifold ] .    in fig .",
    "[ fig_7sites ] the transient state dynamics resulting from eqs .",
    "( [ cogsys_xdot]-[cogsys_z_t ] ) is illustrated .",
    "presented in fig .",
    "[ fig_7sites ] are data for the autonomous dynamics in the absence of external sensory signals , we will discuss the effect of external stimuli further below .",
    "we present in fig .",
    "[ fig_7sites ] only data for a very small network , containing seven sites , which can be easily represented graphically .",
    "we have also performed extensive simulations for very large networks , containing several thousands of sites , and found stable transient state dynamics .",
    "the dynamical system discussed here represents in first place a top - down approach to cognitive systems and a one - to - one correspondence with cortical structures is not intended .",
    "the setput is however inspired by biological analogies and we may identify the sites @xmath50 of the artificial neural network described by eq .",
    "( [ cogsys_xdot ] ) not with single neurons , but with neural assemblies or neural centers .",
    "the reservoir variables @xmath11 could therefore be interpreted as effective fatigue processes taking place in continuously active neural assemblies , the winning coalisions .",
    "it has been been proposed @xcite , that the neural coding used for the binding of heterogeneous sensory information in terms of distinct and recognizable objects might be temporal in nature . within this temporal coding hypothesis , which has been investigated experimentally @xcite , neural assemblies fire in phase , _ viz _ synchronous , when defining the same object and asynchronous when encoding different objects .",
    "there is a close relation between objects and memories in general .",
    "an intriguing possibility is therefore to identify the memories of the transient - state network investigated in the present approach with the synchronous firing neurons of the temporal coding theory .",
    "the winning coalition is characterized by high reservoir levels which would then correspond to the degree of synchronization within the temporal encoding paradigm and the reservoir depletion time @xmath63 would correspond to the decoherence time of the object binding neurons .",
    "we note , that this analogy can however not be carried to far , since synchronization is at its basis a cooperative effect , the reservoir levels describing on the other side single - unit properties . in terms of a popular physics phrase",
    "one might speak of a ` poor man s ' approach to synchronization , via coupling to a fatigue variable .",
    "the reason for the observed numerical and dynamical robustness can be traced back to its relaxational nature . for short time",
    "scales we can consider the reservoir variables @xmath64 to be approximatively constant and the system relaxes into the next clique attractor ruin . once close to a transient attractor ,",
    "the @xmath65 are essentially constant , _ viz _",
    "close to one / zero and the reservoir slowly depletes . the dynamics is robust against noise , as fluctuations affect only details of both relaxational processes , but not their overall behavior .    to be precise we note , that the phase space contracts with respect to the reservoir variables , namely @xmath66 \\",
    "\\le\\ 0 , \\quad\\qquad \\forall x_i\\in[0,1]~,\\ ] ] where we have used ( [ cogsys_phidot ] ) .",
    "we note that the diagonal contributions to the link matrices vanish , @xmath67 , and therefore @xmath68 .",
    "the phase space contracts consequently also with respect to the activities , @xmath69\\ , r_i    \\ \\le\\ 0~,\\ ] ] where we have used ( [ cogsys_xdot ] ) .",
    "the system is therefore strictly dissipative , leading to the observed numerically robust behavior .",
    "the self - generated transient state dynamics shown in fig .",
    "[ fig_7sites ] exhibits well characterized plateaus in the @xmath10 , since small values have been used for the depletion and the growth rate of the reservoir , @xmath70 and @xmath71 .",
    "the simulations presented in fig .",
    "[ fig_7sites ] were performed using @xmath72 for all non - zero excitatory interconnections .",
    "we define a dynamical system to have ` strict transient state dynamics ' if there exists a set of control parameters allowing to turn the transient states adiabatically into stable attractors .",
    "( [ cogsys_xdot]-[cogsys_z_t ] ) fulfill this requirements , for @xmath73 the average duration @xmath0 of the steady - stated plateaus observed in fig .",
    "[ fig_7sites ] diverges .    alternatively , by selecting appropriate values for @xmath74 and @xmath75 , it is possible to regulate the ` speed ' of the transient state dynamics , an important consideration for applications . for a working cognitive system , such as the brain ,",
    "it is enough that the transient states are stable just for a certain minimal period needed to identify the state and to act upon it .",
    "anything longer would just be a ` waste of time ' .",
    ", correspond to @xmath61 and @xmath62 ( @xmath76 ) for member / non - members of the winning clique .",
    "a finite coupling to the local reservoirs @xmath29 leads to orbit @xmath77 which are attracted by the attractor ruins for short time scales and repelled for long time scales .",
    "this is due to a separation of time scales , as the time evolution of the reservoirs @xmath11 occurs on time scales substantially slower than that of the primary dynamical variables @xmath10 .",
    ", scaledwidth=70.0% ]      we note that the mechanism for the generation of stable transient state dynamics proposed here is universal in the sense that it can be applied to a wide range of dynamical systems in a frozen state , i.e.  which are determined by attractors and cycles .",
    "physically , the mechanism we propose here is to embed the phase space @xmath78 of an attractor network into a larger space , @xmath79 , by coupling to additional local slow variables @xmath29 .",
    "stable attractors are transformed into attractor ruins since the new variables allow the system to escape the basin of the original attractor @xmath80 ( for in - clique / out - of - clique sites ) via local escape processes which deplete the respective reservoir levels @xmath11 .",
    "note , that the embedding is carried out via the reservoir functions @xmath40 in eq .",
    "( [ cogsys_ri ] ) and that the reservoir variables keep a slaved dynamics ( [ cogsys_phidot ] ) even when the coupling is turned off by setting @xmath81 in eq .",
    "( [ cogsys_ri ] ) .",
    "this mechanism is illustrated in fig .",
    "[ fig_escapemanifold ] .",
    "locality is an important ingredient for this mechanism to work .",
    "the trajectories would otherwise not come close to any of the attractor ruins again , _",
    "viz _ to the original attractors , being repelled by all of them with similar strengths and fluctuating dynamics of the kind illustrated in fig .",
    "[ fig_transstates ] would result .",
    "the systems illustrated in figs .",
    "[ fig_7sites ] and [ fig_9sites ] are very small and the transient state dynamics soon settles into a cycle of attractor ruins , since there are no incoming sensory signals considered in the respective simulations . for networks containing a larger number of sites , the number of attractors can be however very large and such the resulting cycle length .",
    "we performed simulations for a 100-site network , containing 713 clique - encoded memories .",
    "we found no cyclic behavior even for sequences of transient states containing up to 4400 transient states .",
    "we note , that the system does not necessarily retrace its own trajectory once a given clique is stabilized for a second time , an event which needs to occur in any finite system . the reason being , that the distribution of reservoir levels is in general different when a given clique is revisited for a second time .",
    "we note , that time reversal symmetry is ` spontaneously ' broken in the sense that repetitive transient state dynamics of type    ( clique a )  @xmath82  ( clique b )  @xmath82  ( clique a )  @xmath82  ( clique b )  @xmath82      does generally not arise .",
    "the reason is simple .",
    "once the first clique is deactivated its respective reservoir levels need a certain time to fill up again , compare fig .",
    "[ fig_7sites ] .",
    "time reversal symmetry would be recovered however in the limit @xmath83 , i.e.  when the reservoirs would be refilled much faster than depleted .",
    ".left : the links with @xmath84 , containing six cliques , ( 0,1 ) , ( 1,2,3 ) ( which is highlighted ) , ( 3,4 ) , ( 4,5,6 ) , ( 6,7 ) and ( 7,8,0 ) .",
    "right : as a function of time , the activities @xmath10 for the cyclic transient state dynamics @xmath85 , for the parameters values see sect .",
    "[ sec_strict ] .",
    "both directions ( clockwise / anticlockwise ) of ` rotation ' are dynamically possible and stable , the actual direction being determined by the dynamical initial conditions . [ fig_9sites ] , title=\"fig:\",scaledwidth=30.0% ] 3ex      animals need to generate sequences of neural activities for a wide range of purposes , e.g.  for movements or for periodic internal muscle contractions , the heart - beat being a prime case .",
    "these sequences need to be successions of well defined firing patterns , usable to control actuators , _ viz _ the muscles .",
    "the question then arrises under which condition a dynamical system generates reproducible sequences of well defined activity patterns , i.e.controlled time series of transient states @xcite .",
    "there are two points worth noting in this context .",
    "i. : :    the dynamics described by eqs .",
    "( [ cogsys_xdot]-[cogsys_phidot ] ) works    fine for randomly selected link matrices @xmath22 which    may , or may not change with time passing .",
    "in particular one can select    the cliques specifically in order to induce the generation of a    specific succession of transient states , an example is presented in    fig .",
    "[ fig_9sites ] .",
    "the network is capable , as a matter of principle to    generate robustly large numbers of different sequences of transient    states . for geometric arrangements of the networks sites , and of the    links @xmath22 ,",
    "one finds waves of transient states    sweeping through the system .",
    "ii . : :    in sect .  [ sec_autonomous_online_learning ] we will discuss how    appropriate @xmath22 can be learned from training patterns    presented to the network by an external teacher .",
    "we will concentrate    in sect .  [ sec_autonomous_online_learning ] on the training and learning    of individual memories , _ viz _ of cliques , but suitable sequences of    training patters could be used also for learning temporal sequences of    memories .",
    "an external stimulus , @xmath86 , influences the activities @xmath10 of the respective neural centers .",
    "this corresponds to a change of the respective growth rates @xmath87 , @xmath88 compare eq .",
    "( [ cogsys_ri ] ) , where @xmath89 is an appropriate coupling function , depending on the local reservoir level @xmath29 . when the effect of the external stimulus is strong , namely when @xmath90 is strong , it will in general lead to an activation @xmath91 of the respective neural center @xmath50 .",
    "a continously active stimulus does not convey new information and should , on the other hand , lead to habituation , having a reduced influence on the system .",
    "a strong , continously present stimulus leads to a prolonged high activity level @xmath91 of the involved neural centers , leading via ( [ cogsys_phidot ] ) to a depletion of the respective reservoir levels , on a time scale given by the inverse reservoir depletion rate , @xmath92 .",
    "habituation is then mediated by the coupling function @xmath89 in ( [ cogsys_stim ] ) , since @xmath89 becomes very small for @xmath93 , compare fig .",
    "[ cogsys_fig_gaps ] .",
    "the effect of habituation incorporated in ( [ cogsys_stim ] ) therefore allows the system to turn its ` attention ' to other competing stimuli , with novel stimuli having a higher chance to affect the ongoing transient state dynamics .",
    "we now provide a set of learning rules allowing the system to acquire new patterns on the fly , _ viz _ during its normal phase of dynamical activity .",
    "the alternative , modeling networks having distinct periods of learning and of performance , is of widespread use for technical applications of neural networks , but is not of interest in our context of continuously active cognitive systems .",
    "there are two fundamental considerations for the choice of synaptic plasticities adequate for neural networks with transient state dynamics .",
    "* learning is a very slow process without a short term memory .",
    "training patterns need to be presented to the network over and over again until substantial synaptic changes are induced @xcite .",
    "a short term memory can speed - up the learning process substantially , as it stabilizes external patterns and hence gives the system time to consolidate long term synaptic plasticity .",
    "* systems using sparse coding are based on a strong inhibitory background , the average inhibitory link - strength @xmath94 is substantially larger than the average excitatory link strength @xmath95 , @xmath96 it is then clear that gradual learning is effective only when it affects dominantly the excitatory links : small changes of large parameters do not lead to new transient attractors , nor do they influence the cognitive dynamics substantially .",
    "it then follows , that it is convenient to split the synaptic plasticities into two parts , @xmath97 where the @xmath98 correspond to the short term and to the long term synaptic plasticities respectively .",
    "( [ cogsys_z_t ] ) , @xmath99 , implies that the inhibitory link strength is either zero or @xmath100 , but is not changed directly during learning , in accordance to above discussion .",
    "we may therefore consider two kinds of ` excitatory links strengths ' :    * : an active @xmath84 is positive and enforces @xmath101 for the same link , via eq .",
    "( [ cogsys_z_t ] ) . * : an inactive @xmath102 is slightly negative , we use @xmath103 as a default .",
    "it enforces @xmath104 for the same link , via eq .",
    "( [ cogsys_z_t ] ) and does not contribute to the dynamics , since the excitatory links enter as @xmath105 in ( [ cogsys_ri ] ) .",
    "when @xmath36 acquires , during learning , a positive value , the corresponding inhibitory link @xmath106 is turned off via eq .",
    "( [ cogsys_z_t ] ) and the excitatory link @xmath36 determines the value of the respective term , @xmath107 , in eq .",
    "( [ cogsys_ri ] ) .",
    "we have used a small negative baseline of @xmath108 throughout the simulations .     and the network illustrated in fig .",
    "[ fig_7sites ] , without the link ( 3,6 ) .",
    "the transient states are @xmath109 .",
    "an external stimulus at sites ( 3 ) and ( 6 ) acts for @xmath110 $ ] with strength @xmath111 .",
    "[ cogsys_fig_7_ai_learn_s ] , scaledwidth=70.0% ]      it is reasonable to have a maximal possible value @xmath112 for the short term synaptic plasticities .",
    "we consider therefore the following hebbian - type learning rule : @xmath113 @xmath114 increases rapidly , with rate @xmath115 , when both the pre- and the post - synaptic neural centers are active , _ viz _ when their respective activities are above @xmath47 .",
    "otherwise it decays to zero , with a rate @xmath116 .",
    "the coupling functions @xmath54 preempt prolonged self - activation of the short term memory . when the pre- and the post - synaptic centers are active long enough to deplete their respective reservoir levels , the short term memory is shut - off via the @xmath54 .",
    "we have used @xmath117 , @xmath118 and @xmath119 and @xmath120 throughout the simulations .    in fig .",
    "[ cogsys_fig_7_ai_learn_s ] we present the time evolution of some selected @xmath114 , for a simulation using the network illustrated in fig .   [ fig_7sites ] .",
    "the short term memory is activated in three cases :    * when an existing clique , _ viz _ a clique encoded in the long term memory @xmath121 , is activated , as it is the case of ( 0,1 ) for the data presented in fig .",
    "[ cogsys_fig_7_ai_learn_s ] , the respective intra - clique @xmath122 are also activated .",
    "this behavior is a side effect since , for the parameter values chosen here , the magnitude of the short term link strengths is substantially smaller than those of the long term link strengths . * during the transient state dynamics there is a certain overlapping of a currently active clique with the subsequent active clique . for this short time",
    "span the short term plasticities @xmath122 for synapses linking these two cliques get activated .",
    "an example is the link ( 2,4 ) for the simulation presented in fig .",
    "[ cogsys_fig_7_ai_learn_s ] .",
    "* when external stimuli act on two sites not connected by an excitatory long term memory link @xmath121 , the short term plasticity @xmath122 makes a qualitative difference .",
    "it transiently stabilizes the corresponding link and the respective link becomes a new clique @xmath123 either by itself , or as part of an enlarged and already existing clique .",
    "an example is the link ( 3,6 ) for the simulation presented in fig .",
    "[ cogsys_fig_7_ai_learn_s ] .",
    "note however that , without subsequent transferal into the long term memory , these new states would disappear with a rate @xmath116 once the causing external stimulus is gone .",
    "the last point is the one of central importance , as it allows for the temporal stabilization of new patterns present in the sensory input stream .     and",
    "the network illustrated in fig .",
    "[ fig_7sites ] , without the link ( 3,6 ) .",
    "the transient states are @xmath109 . an external stimulus at sites ( 3 ) and ( 6 ) acts for @xmath110 $ ] with strength @xmath111 .",
    "the stimulus pattern ( 3,6 ) has been learned by the system , as the @xmath124 and @xmath125 turned positive during the learning - interval @xmath126 $ ] .",
    "the learning interval is substantially longer than the bare stimulus length due to the activation of the short term memory .",
    "the decay of certain @xmath121 in the absence of an external stimulus is due to forgetting ( [ cogsys_w_l_dot_opt ] ) , which should normally be a very weak effect , but which has been choose here to be a sizeable @xmath127 , for illustrational purposes , scaledwidth=70.0% ]      information processing dynamical systems retain their functionalities only when they keep their dynamical properties within certain regimes , they need to regulate their own working point . for the type of systems discussed here , exhibiting transient - state dynamics , the working point is , as discussed in the introduction , defined as the time @xmath1 the system needs for a transition from one quasi - stationary state to the subsequent , relative to the length @xmath0 of the individual quasi - stationary states , which is given by @xmath92 .",
    "the cognitive information processing within neural networks occurs on short to intermediate time scales . for these processes to work well the mean overall synaptic plasticities , _ viz _ the average strength of the long term memory links @xmath128 , needs to be regulated homeostatically .",
    "the average magnitude of the growth rates @xmath87 , see eq .",
    "( [ cogsys_ri ] ) , determines the time @xmath1 needed to complete a transition from one winning clique to the next transient state .",
    "it therefore constitutes a central quantity regulating the working point of the system , since @xmath129 is fixed , the reservoir depletion rate @xmath74 is not affected by learning processes which affect exclusively the inter - neural synaptic strengths .",
    "the bare growth rates @xmath130 are quite strongly time dependent , due to the time - dependence of the postsynaptic reservoirs entering the reservoir function @xmath131 , see eq .",
    "( [ cogsys_ri ] ) . the effective incoming synaptic signal strength @xmath132x_j~ ,",
    "\\label{cogsys_tilde_r}\\ ] ] which is independent of the post - synaptic reservoir @xmath29 , is a more convenient local control parameter .",
    "the working point of the cognitive system is optimal when the effective incoming signal is , on the average , of comparable magnitude @xmath133 for all sites , @xmath134 the long term memory has two tasks : to extract and encode patterns present in the external stimulus , eq .",
    "( [ cogsys_stim ] ) , via unsupervised learning and to keep the working point of the dynamical system in its desired range .",
    "both tasks can be achieved by a single local learning rule , @xmath135 \\\\ & & \\cdot \\,\\theta(x_i - x_c)\\,\\theta(x_j - x_c ) , \\nonumber \\\\ & - & \\gamma_l^- \\ ,",
    "d(w_{ij}^l)\\ , \\theta(x_i - x_c)\\,\\theta(x_c - x_j)~ , \\label{cogsys_w_l_dot_decay}\\end{aligned}\\ ] ] where @xmath136 .",
    "for the numerical simulations we used @xmath137 , @xmath108 and @xmath138 .",
    "we now comment on some properties of these evolution equations for @xmath139 :    * the learning rule ( [ cogsys_w_l_dot_opt ] ) is local and of hebbian type .",
    "learning occurs only when the pre- and the post - synaptic neuron are active , _ viz _ when their respective activity levels are above the threshold @xmath47 .",
    "weak forgetting , i.e.  the decay of seldom used links is governed by ( [ cogsys_w_l_dot_decay ] ) .",
    "the function @xmath140 determines the functional dependence of forgetting on the actual synaptic strength , we have used @xmath141 for simplicity .",
    "* when the effective incoming signal @xmath142 is weak / strong , relative to the optimal value @xmath133 , the active links are reinforced / weakened , with @xmath143 being the minimal value for the @xmath22 .",
    "the baseline @xmath143 is slightly negative , compare figs .",
    "[ cogsys_fig_gaps ] and [ cogsys_fig_7_ai_learn_l ] .",
    "+ the hebbian - type learning then takes place in the form of a temporal competition among incoming synapses - frequently active incoming links will gain strength , on the average , on the expense of rarely used links .",
    "* in fig .",
    "[ cogsys_fig_7_ai_learn_l ] the time evolution of some selected @xmath121 is presented .",
    "a simple input pattern is learned by the network . in this simulation",
    "the learning parameter @xmath144 has been set to a quite large value such that the learning occurs in one step ( fast learning ) .",
    "* when a neural network is exposed repeatedly to the same , or to similar external stimuli , unsupervised learning generally then leads to uncontrolled growth of the involved synaptic strengths .",
    "this phenomena , termed ` runaway synaptic growth ' can also occur in networks with continuous self - generated activities , when similar activity patterns are auto - generated over and over again .",
    "both kinds of synaptic runaway - growth is suppressed by the proposed link - dynamics ( [ cogsys_w_l_dot_opt ] ) . *",
    "note that @xmath145 enters the evolution equation ( [ cogsys_ri ] ) as @xmath146 .",
    "we can therefore distinguish between active ( @xmath53 ) and inactive ( @xmath147 ) configuration , compare fig .",
    "[ cogsys_fig_gaps ] .",
    "the negative baseline @xmath148 entering ( [ cogsys_w_l_dot_opt ] ) then allows for the removal of positive links and provides a barrier against small random fluctuations , compare sect .",
    "[ sec_neg_baseline ] .    during a transient state",
    "we have @xmath91 for all vertices belonging to the winning coalition and @xmath149 for all out - of - clique sites , leading to @xmath150",
    "compare eq .",
    "( [ cogsys_tilde_r ] ) . the working - point optimization rule ( [ cogsys_r_to_r_opt ] ) ,",
    "@xmath151 is therefore equivalent to a local normalization condition enforcing the sum of active incoming link - strengths to be constant , i.e.  site independent .",
    "this rule is closely related to a mechanism of self regulation of the average firing rate of cortical neurons proposed by bienenstock , cooper and munro @xcite .",
    "the neural network we consider here is continously active , independently of whether there is sensory input via eq .",
    "( [ cogsys_stim ] ) or not .",
    "the reason being , that the evolution equations ( [ cogsys_xdot ] ) and ( [ cogsys_phidot ] ) generate a never ending time series of transient states . it is a central assumption of the present study , that continuous and self - generated neural activity is a condition sine qua no for modeling overall brain activity or for developing autonomous cognitive systems @xcite .    the evolution equations for the synaptic plasticities , namely ( [ cogsys_w_s_dot ] ) for the short - term memory and ( [ cogsys_w_l_dot_opt ] ) for the long - term memory are part of the dynamical system , _ viz _ they determine the time evolution of @xmath114 and of @xmath139 at all times , irrespectively of whether external stimuli are presented to the network via ( [ cogsys_stim ] ) or not .",
    "the evolution equations for the synaptic plasticities need therefore to fulfill , quite in general for a continously active neural network , two conditions :    ( a ) : :    under training conditions , namely when input patterns are presented to    the system via ( [ cogsys_stim ] ) , the system should be able to modify    the synaptic link strength accordingly , such that the training    patterns are stored in the form of new memories , _ viz _ cliques    representing attractor ruins and leading to quasistationary states .",
    "( b ) : :    in the absence of input the ongoing transient - state dynamics will lead    constantly to synaptic modifications , via ( [ cogsys_w_s_dot ] ) and    ( [ cogsys_w_l_dot_opt ] ) .",
    "theses modification may not induce qualitative    changes , such as the the autonomous destruction of existing memories    or the spontaneous generation of spurious new memories .",
    "new memories    should be acquired exclusively via training by external stimuli .    in the following section we will present simulations in order to investigate these points .",
    "we find that the evolution equations formulated in this study conform with both conditions ( a ) and ( b ) above , due to the optimization principle for the long - term synaptic plasticities in eq .",
    "( [ cogsys_w_l_dot_opt ] ) .    .learning results for systems with @xmath3 sites and @xmath152 excitatory links and @xmath153 cliques containing @xmath154 sites .",
    "@xmath155 is the total number memories to be learned . @xmath156 and @xmath157 denote the number of memories learned completely / partially .",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab_results_learning ]",
    "we have performed extensive simulations of the dynamics of the network with ongoing learning , for systems with up to several thousands of sites .",
    "we found that the dynamics remains long - term stable even in the presence of continuous online learning governed by eqs .",
    "( [ cogsys_w_s_dot ] ) and ( [ cogsys_w_l_dot_opt ] ) , exhibiting semi - regular sequences of winning coalition , as shown in figs .",
    "[ fig_7sites ] .",
    "the working point is regulated adaptively and no prolonged periods of stasis or trapped states were observed in the simulations , neither did periods of rapid or uncontrolled oscillations occur .",
    "any system with a finite number of sites @xmath3 and a finite number of cliques settles in the end , in the absence of external signals , into a cyclic series of transient states .",
    "preliminary investigations of systems with @xmath158 resulted in cycles spanning on the average a finite fraction of the set of all cliques encoded by the network .",
    "this is a notable result , since the overall number of cliques stored in the network can easily be orders of magnitudes larger than the number of sites @xmath3 itself , compare eq .",
    "( [ cogsys_n_z ] ) .",
    "detailed studies of the cyclic behavior for autonomous networks will be presented elsewhere .",
    "training patterns @xmath159 presented to the system externally via @xmath160 , for @xmath161 , are learned by the network via the activation of the short term memory for the corresponding intra - pattern links . in figs .",
    "[ cogsys_fig_7_ai_learn_s ] and [ cogsys_fig_7_ai_learn_l ] we present a case study .",
    "the ongoing internal transient state dynamics is interrupted at time @xmath162 by an external signal which activates the short term memory , see fig .",
    "[ cogsys_fig_7_ai_learn_s ] .",
    "note that the short term memory is activated both by external stimuli and internally whenever a given link becomes active , i.e.  when both pre- and post - synaptic sites are active coinstantaneous .",
    "the internal activation does however not lead to the internal generation of spurious memories , since internally activated links belong anyhow to one or more already existing cliques .    in fig .",
    "[ cogsys_fig_7_ai_learn_l ] we present the time development of the respective long term synaptic modifications , @xmath139 .",
    "the parameters for learning chosen here allow for fast learning , the pattern corresponding to the external signal , retained temporarily in the short term memory , is memorized in one step , _ viz _ the corresponding @xmath163 becomes positive before the transition to the next clique takes place . for practical applications smaller learning rates might be more suitable , as they allow to avoid learning of spurious signals generated by environmental noise .    in table",
    "[ tab_results_learning ] we present the results for the learning of two networks with @xmath164 and @xmath165 from scratch .",
    "the initial networks contained only two connected cliques , in order to allow for a non - trivial initial transient state dynamics , all other links where inhibitory .",
    "learning by training and storage of the externally presented patterns , using the same parameters as for figs .",
    "[ cogsys_fig_7_ai_learn_s ] and [ cogsys_fig_7_ai_learn_l ] , is nearly perfect .",
    "the learning rate can be chosen over a very wide range , as we tested . here",
    "the training phase was completed , for the 100-site network , by @xmath166 .",
    "coming back to the discussion in section [ subsec_online_learning ] , we then conclude that the network fulfills the there formulated condition ( a ) , being able to store efficiently training patterns as attractor ruins in the form of cliques .     for the positive @xmath121 for a 100-site network with 713 cliques at time @xmath167 , corresponding to circa 4500 transient states .",
    "left : after learning from scratch .",
    "training was finished at @xmath168 .",
    "right : starting with @xmath169 for all links belonging to one or more cliques .",
    "[ cogsys_fig_100_links ] , title=\"fig:\",scaledwidth=45.0% ]    for the positive @xmath121 for a 100-site network with 713 cliques at time @xmath167 , corresponding to circa 4500 transient states . left : after learning from scratch .",
    "training was finished at @xmath168 .",
    "right : starting with @xmath169 for all links belonging to one or more cliques .",
    "[ cogsys_fig_100_links ] , title=\"fig:\",scaledwidth=45.0% ]      we note that the hebbian learning via the working - point optimization , eq .",
    "( [ cogsys_w_l_dot_opt ] ) , leads to the spontaneous generation of asymmetries in the link matrices , _ viz _ to @xmath170 , since the synaptic plasticity depends on the postsynaptic growth rates .    in fig .",
    "[ cogsys_fig_100_links ] we present , for two simulations , the distribution of the link - asymmetry @xmath171 for all positive @xmath121 , for the 100-site network of table [ tab_results_learning ] , at time @xmath172 .",
    "the distributions shown in fig .",
    "[ cogsys_fig_100_links ] are particular realizations of steady - state distributions , _ viz _ they did not change appreciably for wide ranges of total simulation times .",
    "( i ) : :    in the first simulation the network had been learned from scratch .",
    "the    set of 713 training patterns were presented to the network for    @xmath173 $ ] .",
    "after that , for    @xmath174 $ ] the system evolved freely .",
    "a    total of 3958 transient states had been generated at time    @xmath172 , but the system had nevertheless not yet    settled into a cycle of transient states , due to the ongoing synaptic    optimization , eqs .",
    "( [ cogsys_w_s_dot ] ) and ( [ cogsys_w_l_dot_opt ] ) .",
    "661    cliques remained at @xmath172 , as the link    competition had led to the suppression of some seldom used links .",
    "( ii ) : :    in the second simulation , uniform and symmetric starting excitatory    links @xmath175 had been set by hand at    @xmath176 , for all intra - clique links . the same    @xmath165 network as in ( i ) was used and the simulation ran    in the absence of external stimuli .",
    "all 713 cliques were still present    at @xmath172 , despite the substantial reorganization    of the link - strength distribution , from the initial uniform to the    stationary distribution shown in fig .",
    "[ cogsys_fig_100_links ] . a total    of",
    "4123 transient states had been generated in the course of the    simulation , without the system entering into a cycle .    for both simulations all evolution equations ,",
    "namely ( [ cogsys_xdot ] ) and ( [ cogsys_phidot ] ) for the activities and reservoir levels , as well as ( [ cogsys_w_s_dot ] ) for the short - term memory and ( [ cogsys_w_l_dot_opt ] ) and ( [ cogsys_w_l_dot_decay ] ) for the long term memory determined the dynamics for all times @xmath177 $ ] .",
    "the difference between ( i ) and ( ii ) being the way the memories are determined , via training by external stimuli , eq .",
    "( [ cogsys_stim ] ) , as in ( i ) or by hand as in ( ii ) .    comparing the two link distributions shown in fig .",
    "[ cogsys_fig_100_links ] , we note the overall similarity , a consequence of the continuously acting working - point optimization .",
    "the main differences turn - up for small link strengths , since these two simulations started from opposite extremes ( vanishing / strong initial excitatory links ) .",
    "the details of the link distribution shown in fig .",
    "[ cogsys_fig_100_links ] depend sensitively on the parameters . for the results",
    "show in fig .",
    "[ cogsys_fig_100_links ] we used for illustrational purposes @xmath127 , which is a very big value for a parameter regulating weak forgetting .",
    "we also performed simulations with @xmath178 , the other extreme , and found that the link - asymmetry distribution was somewhat more scattered .",
    "coming back to the discussion in section [ subsec_online_learning ] , we then conclude that the network fulfills the there formulated condition ( b ) , since essentially no memories acquired during the training state were destroyed , or spurious new memories spontaneously creasted , during the subsequent free evolution .",
    "we have investigated a series of issues regarding neural networks with autonomously generated transient state dynamics . we have presented a general method allowing to transform an initial attractor network into a network capable of generating an infinite time series of transient states .",
    "the resulting dynamical system has strictly contracting phase space , with a one - to - one adiabatic correspondence between the transient states and the attractors of the original network .",
    "we then have discussed the problem of homeostasis , namely the need for the system to regulate its own working point adaptively .",
    "we formulated a simple learning rule for unsupervised local hebbian - type learning , which solves the homeostasis problem .",
    "we note here , that this rule , eq .",
    "( [ cogsys_w_l_dot_opt ] ) is similar to learning rules shown to optimize the overall storage capacity for discrete - time neural networks @xcite .",
    "we have studied a continuous time neural network model using clique encoding and showed that this model is very suitable for studying transient state dynamics in conjunction with ongoing learning - on - the - fly for a wide range of learning conditions .",
    "both fast and slow online learning of new memories is compatible with the transient state dynamics self - generated by the network .",
    "finally we turn to the interpretation of the transient state dynamics .",
    "examination of a typical time series of subsequently activated cliques , as the one shown in fig .",
    "[ fig_7sites ] , reveals that the sequence of cliques is not random .",
    "every single clique is connected to its predecessor via excitatory links , they are said to be ` associatively ' connected @xcite .",
    "the sequence of subsequently active cliques can therefore be viewed , cum grano salis , as an ` associative thought process ' @xcite . the possible use of such processes for cognitive information processing needs , however , yet to be investigated .",
    "10        c. gros , _ `` autonomous dynamics in neural networks : the dhan concept and associative thought processes '' _ , cooperative behaviour in neural systems ( ninth granada lectures ) , p.l .",
    "garrido , j.  marro , j.j .",
    "torres ( eds . ) , aip conference proceedings * 887 * , 129 - 138 ; also available as q-bio.nc/0703002 .",
    "m. rabinovich , a. volkovskii , p. lecanda , r. huerta , h d.i .",
    "abarbanel , g. laurent , _ `` dynamical encoding by networks of competing neuron groups : winnerless competition '' _ , phys .",
    "lett . * 87 * , 068102 ( 2001 ) .",
    "the reservoir functions have the form of generalize fermi - functions . a possible mathematical implementation for @xmath179 , with @xmath180 , which we used is @xmath181 - { \\rm atan}[(0-\\varphi_c^{(\\alpha)})/\\gamma_\\varphi ] \\over   { \\rm atan}[(1-\\varphi_c^{(\\alpha)})/\\gamma_\\varphi ] - { \\rm atan}[(0-\\varphi_c^{(\\alpha)})/\\gamma_\\varphi ] } $ ] with @xmath182 , @xmath183 @xmath184 , @xmath185 and @xmath18",
    ".        d.j .",
    "amit , h. gutfreund , h. sompolinsky , _ `` storing infinite numbers of patterns in a spin - glass model of neural networks '' _ , phys .",
    "* 55 * , 1530 ( 1985 ) . c. von der malsburg and w. schneider , _ `` a neural cocktail - party processor '' _ , biological cybernetics , * 54 * , 29 ( 1886 ) .",
    "gray , p. knig , a.k .",
    "engel and w. singer , _",
    "`` oscillatory responses in cat visual cortex exhibit incolumnar synchronization which reflects global stimulus properties '' _ , nature * 338 * , 334 ( 1989 ) .",
    "r. huerta , m. rabinovich , _ `` reproducible sequence generation in random neural ensembles '' _ , phys .",
    "lett . * 93 * , 238104 ( 2004 ) .",
    "rabinovich , r. huerta , p. varona and v.s .",
    "afraimovich , _",
    "`` generation and reshaping of sequences in neural systems '' _ biol cybern . * 95 * , 519 ( 2006 ) .",
    "c. gros , _",
    "`` self - sustained thought processes in a dense associative network '' _ , springer lecture notes in artificial intelligence ( ki2005 ) * 3698 * , 375 ( 2005 ) ; also available as q-bio.nc/0508032 ."
  ],
  "abstract_text": [
    "<S> we investigate dynamical systems characterized by a time series of distinct semi - stable activity patterns , as they are observed in cortical neural activity patterns . we propose and discuss a general mechanism allowing for an adiabatic continuation between attractor networks and a specific adjoined transient - state network , which is strictly dissipative . </S>",
    "<S> dynamical systems with transient states retain functionality when their working point is autoregulated - avoiding prolonged periods of stasis or drifting into a regime of rapid fluctuations . </S>",
    "<S> we show , within a continuous - time neural network model , that a single local updating rule for online learning allows simultaneously ( a ) for information storage via unsupervised hebbian - type learning ( b ) for adaptive regulation of the working point and ( c ) for the suppression of runaway synaptic growth . </S>",
    "<S> simulation results are presented , the spontaneous breaking of time - reversal symmetry and link symmetry are discussed . </S>"
  ]
}