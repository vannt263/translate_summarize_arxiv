{
  "article_text": [
    "search time & accuracy +   brute force & none & none & v.bad & exact +   cover tree & bad(3 ) & good(2 ) & good(=2 ) & exact(1 )   +   lsh & good(2 ) & bad(3 ) & v.good(1 ) & high(=2 )   +   * angle tree * & * v.good(1 ) * @xmath0 & * v.good(1 ) * @xmath0 & * good(=2 ) * & * high(=2 ) *   +     +    the nearest neighbor search ( nns ) problem is of fundamental importance with wide applicability in search , pattern recognition and data mining .",
    "the problem is simply defined as : + _ given a data set @xmath1 of size @xmath2 and dimension @xmath3 , efficiently preprocess @xmath1 so that given a query point @xmath4 , we can quickly find the nearest points to @xmath4 ( nearest neighbors ) in @xmath1 . _",
    "+ without any preprocessing of @xmath1 the brute force time complexity of nns is @xmath5 .",
    "this is impractical for very large databases .    in low dimensional space , data structures like",
    "kd - trees @xcite are very efficient resulting in expected time complexity of @xmath6 .",
    "a long standing open problem is to design data structures which can scale up to high dimensions because experience shows that in high dimensions , space partitioning data structures become inefficient .",
    "for example , kd - trees require that @xmath7 or else search performance degenerates to brute force @xcite .",
    "this has been the status quo for some time .",
    "tenenbaum et al .",
    "@xcite introduced a novel approach to analysing low dimensional manifolds embedded in high dimensions .",
    "this approach is widely applicable as many real world high - dimensional data sets only have a small number of non - linear degrees of freedom .",
    "for example , consider a collection of images of human faces .",
    "the variance of this data set can be described by considering the size of nose , distance between eyes , and a small number of other such degrees of freedom .",
    "each image can be described by these degrees of freedom much more succinctly than by considering the thousands of pixels that make up the image .",
    "any data set containing images ( or other kinds of sampling of the real world , which is usually fairly ordered ) can not be completely `` random '' .",
    "an example of a near random data set would be images of a television snow screen .",
    "many real - world data sets are likely to be reducible to a much lower dimension , where each axis represents a non - linear degree of freedom",
    ". we do not propose to find these intrinsic degrees of freedom .",
    "we only assume their existence in the data set in question .",
    "several definitions of intrinsic dimensionality have been proposed , such as small covering numbers , assouad dimension , low dimensional manifolds , and local covariance dimension ( lcd ) . in this paper",
    "we mostly consider lcd .",
    "random projection trees ( rp - trees ) were introduced by @xcite .",
    "this data structure differs from the kd - tree only in the nature of the splitter used . while the kd - tree always splits parallel to a single axis , the rp - tree splits in essentially a random direction .",
    "@xcite show that the rp - tree adapts to the intrinsic dimensionality of data ( in terms of grouping nearby points in the same cells ) much better than the kd - tree .",
    "however , nns ( using rp - trees ) with the standard kd - tree pruning method still degenerates to brute force search in high dimensions .",
    "the cover tree@xcite is a new non - space partitioning nns data structure that exploits the intrinsic dimensionality of data .",
    "it achieves very good search time and space usage , but suffers from slow preprocessing @xcite .",
    "the preprocessing requirements are also very sensitive to noise@xcite . for this reason ,",
    "it is not practical for use on large , high - dimensional and noisy data , such as image databases .",
    "the most popular current high - dimensional nns technique is locality - sensitive hashing .",
    "it is almost practical for the real world , except for its large space usage@xcite .",
    "the index it builds is often many times larger than the original data , again making it unsuitable for very large databases .",
    "the approximate nns technique described in @xcite proposes two solutions .",
    "one is hierarchical k - means trees , which is an example of a geometric nearest - neighbor access tree ( gnat ) @xcite .",
    "these trees can suffer from slow preprocessing .",
    "the other solution is multiple randomized kd - trees . since searching through kd - trees in high dimensions degenerates to brute force , @xcite",
    "obtain approximate seach by simply searching through a fixed number of leaf nodes for each tree .",
    "this is similar in spirit to multi - probe lsh , and as we discuss in section [ sec : disc ] , this approach is very likely to be improved by combining it with our method .",
    "the process described in @xcite is also not practical for large , high - dimensional data sets as it requires an eigendcomposition of the gram matrix - an @xmath8 operation .",
    "however , to perform efficient nns , we will show that the expensive process of manifold learning can be avoided .",
    "we propose a data structure ( angle tree ) that for the first time gives us the ability to index large and high - dimensional data sets in practice .",
    "its preprocessing and space requirements are not much greater than a regular kd - tree , while providing fast and highly - accurate nns capability .",
    "it also maintains the simplicity of the original kd - tree .",
    "as we will show , in our implementation using a space partitioning data structure , all we need to extract from the data are the angles that the data region in question makes with the splitting hyperplane ( splitter ) .",
    "this angle has meaning even in high - dimensional space by the definition of the dot product .",
    "the angle tree is described in algorithms 1 , 2 and 3 , and is included in the appendix .",
    "it is conceptually a small modification to any current tree - based indexing technique , such as a kd or rp - tree@xcite , that adds the power to exploit the intrinsic structure of data during nns , while introducing a very small probability of error ( not finding the true nearest neighbors ) . while these tree structures can already perform near - neighbor search ( ie .",
    "only search one cell of the tree ) , we introduce a new method of efficiently deciding which other cells , if any , also need to be searched .",
    "this method is a generalization of the classic kd - tree nns algorithm for low intrinsic dimensionality data embedded in very high - dimensional space .",
    "if there is no intrinsic structure to exploit in a particular data set , then the angle tree will behave like a regular kd or rp - tree , with minimal additional overhead .",
    "the remainder of the paper is organized as follows . in section [ sec : def ] we list all definitions used in the paper . in section [ sec : keyidea ] we present the key idea of this paper and prove its correctness . in section [ sec : highdimbehav ] we discuss the behaviour of the dihedral angle ( between the splitter and the low dimensional manifold region ) in high dimensions . in section [ sec : estimdihed ] we introduce our method for finding the dihedral angle . in section [ sec : errorreg ]",
    "we analyze the accuracy of nns using angle trees . in section",
    "[ sec : exp ] we report the results of our experiments . in section [ sec : disc ]",
    "we provide some further comments and suggest directions for future work .",
    "in this section we collect all the definitions used in the paper for the convenience of the reader .    *",
    "* ambient dimension ( @xmath3 ) : * is the dimension of the raw data .",
    "for example , if data is presented as an @xmath9 matrix , then the ambient dimension is @xmath3 . * * intrinsic dimension ( @xmath10 ) : * is the number of degrees of freedom in the data .",
    "again , if data is presented as an @xmath9 matrix then the intrinsic dimensionality ( @xmath10 ) is typically much smaller than @xmath3 .",
    "the intrinsic dimension is usually not known but can be estimated @xcite . *",
    "* local covariance dimension ( lcd ) : * a set s has local covariance dimension @xmath11 ) if it has @xmath12 fraction of its variance concentrated in a @xmath10-dimensional subspace .",
    "more precisely , let @xmath13 denote the eigenvalues of the covariance matrix ; these are the variances in each of the eigenvector directions .",
    "`` set @xmath14 has local covariance dimension @xmath11 ) if its restriction to any ball of radius @xmath15 has covariance matrix whose largest @xmath10 eigenvalues satisfy @xmath16 '' @xcite * * intrinsic plane ( ip ) : * is the @xmath10-dimensional affine subspace associated with the lcd defined above .",
    "* * splitter : * the splitting hyperplane of dimension @xmath17 to @xmath18 .",
    "it splits the data region in question into two not necessarily equal parts . * * dihedral angle ( @xmath19 ) : * the angle between the splitter and the ip .",
    "if the ip has codimension 1 , then this angle is simply the angle between their normal vectors ( the splitter always has codimension of 1 ) . otherwise , it is @xmath20 minus the angle between the normal vector to the splitter and its projection onto the ip @xcite . *",
    "* error angle ( @xmath21 ) : * this angle reflects the accuracy of the dihedral angle returned by the getangle ( ) function ( see section [ sec : estimdihed ] ) . * * ambient distance ( @xmath22 ) : * is the distance in the ambient space between points or hyperplanes @xmath4 and @xmath23 . * * manifold distance ( @xmath24 ) : * is the distance between points or hyperplanes @xmath4 and @xmath23 when the path is restricted to the data manifold . *",
    "* intrinsic plane distance ( @xmath25 ) : * is the distance between points or hyperplanes @xmath4 and @xmath23 when the path is restricted to the ip .",
    "if @xmath4 or @xmath23 does not lie on the ip , then we project it onto the ip before calculating the distance .",
    "note that @xmath26 if @xmath23 and @xmath4 are both points . * * locality - sensitive hashing ( lsh ) : * a popular current nns solution proposed by @xcite . in this paper",
    "we refer to the random projection implementation of lsh .",
    "its main problem is space usage @xcite . *",
    "* cover tree : * a relatively new nns tree structure that is designed to exploit @xmath10 .",
    "it does not partition the space , and in fact only requires a metric that satisfies the triangle inequality .",
    "its main problem is preprocessing time@xcite and sensitivity to noise @xcite . *",
    "* random projection tree ( rp - tree ) : * data structure introduced by @xcite .",
    "similar to kd - tree , but splits in a random direction .",
    "has the property that every @xmath27 levels , the diameter ( distance between two furthest points ) of each cell is halved @xcite .",
    "in order to appreciate the key insight of the proposed approach we first explain how a query point @xmath4 uses the classical kd - tree to carry out the nns .    a query point @xmath4 will navigate down a branch of the kd - tree ( each pair of branches being split by a splitter ) until it reaches the leaf cell in which it is contained . the nearest neighbor in that cell will be identified ( @xmath28 in figure [ fig : search - a ] ) .",
    "a search sphere of radius @xmath29 will be constructed and any neighboring cell which intersects the search sphere will be explored for points which are potentially closer to @xmath4 than @xmath28 .",
    "effectively this implies that the perpendicular distance between @xmath4 and the splitter(s ) , which form the walls of the cell , serve as a lower bound for pruning .",
    "it is well known that in high - dimensional space , the kd - tree nns degenerates to little better than brute force .",
    "we will provide an alternate explanation in section [ sec : highdimbehav ] in terms of the distance from @xmath4 to the walls of its cell .",
    "now if the data lives in a low dimensional manifold , for example , on the dotted line in figure [ fig : mainidea ] then a better lower bound is the manifold distance @xmath30 .",
    "however , generally the manifold is not known , so we instead approximate @xmath30 with the ip distance @xmath31 .    in order to calculate the @xmath31 we will use the trigonometric relation @xmath32    the following is the crucial observation which underpins the whole approach .    * in low dimensional space , @xmath33 . thus @xmath34 and thus the angle tree and kd - tree will behave in a similar fashion . * in high dimensional space ( with low intrinsic dimensionality ) , @xmath35 , and",
    "thus @xmath36 will be potentially a much tighter lower bound than @xmath37 * in practice , real data sets rarely follow the equations of a smooth manifold and there will be data points which lie outside the manifold . also , even if the data forms a smooth manifold , this manifold can be highly curved making it difficult to approximate its local regions with an affine hyperplane .",
    "these two factors can cause data points to lie in the _ error region _ shown in figure [ fig : mainidea ] .",
    "then the true nearest neighbors could be accidentally pruned .",
    "we provide a rigorous justification for the behavior of the dihedral angle ( @xmath19 ) in high dimensional space as noted in section [ sec : keyidea ] .",
    "theorem 1 below is based on figure [ fig : mainidea ] but generalizes to higher dimensions .    given a random @xmath10-dimensional hyperplane ( @xmath38 ) in @xmath18 and a random affine hyperplane of dimension d-1 ( splitter ) defined by its normal vector @xmath39 .",
    "let @xmath19 be the dihedral angle .",
    "if @xmath3 is large , @xmath40 converges to @xmath41 , where @xmath42 is the chi - distribution with @xmath10 degrees of freedom @xcite .",
    "if @xmath10 is also large , @xmath43 [ thm : alphadistribution ]    as @xmath44 is a d - dimensional random vector , we can express it as @xmath45 and fix the d - dimensional ip as the hyperplane spanned by the first d axes of @xmath18 . here",
    "each @xmath46 . based on figure [ fig : mainidea ] , note @xmath47 where @xmath48 is the projection of @xmath44 onto @xmath38 . thus @xmath49 .",
    "@xmath50    the numerator and denominator are both chi - distributions with @xmath10 and @xmath3 degrees of freedom respectively . for large @xmath51 @xcite .",
    "since we assume @xmath3 to be very large , the denominator s variance becomes insignificant relative to its mean , and so we can replace it by its mean . @xmath52 for the case when @xmath10 is also large , we apply @xcite to the numerator as well , yielding @xmath43",
    "indicated by dashed line .",
    "dots represent the data .",
    "@xmath53 is a representative point of the data region .",
    "@xmath23 an arbitrary point on the opposite side of the splitter ( red hyperplane ) to @xmath53 .",
    "we let this region have a lcd with @xmath54 for clarity . ]    [ flo:3dmainidea ]    the dihedral angle @xmath19 between two hyperplanes @xmath55 and @xmath56 of codimension 1 is related through their unit normals @xmath57 and @xmath58 by the equation @xcite @xmath59 thus if we know the two hyperplanes and their unique normals , calculating the dihedral angle is straightforward .",
    "the challenge in our case is that we do not know the equation of the intrinsic plane ( ip ) and furthermore we assume that the ip will change from region to region .",
    "in fact , by definition , manifolds are locally like hyperplanes .",
    "we could use a pca - like technique but that would be computationally prohibitive as the complexity of pca is @xmath60 where @xmath3 is the ambient dimension .",
    "we will propose a much simpler and more efficient method which is also mathematically rigorous .",
    "we refer the reader to figure [ flo:3dmainidea ] for the discussion in this section .    in figure [ flo:3dmainidea ]",
    ", @xmath53 is the representative point and @xmath61 is the normal from @xmath53 to the splitter .",
    "@xmath62 is the dihedral angle which can be obtained by projecting @xmath61 onto the ip .",
    "let @xmath1 be an arbitary point on the @xmath38 ( here it is shown to be on the intersection of the two planes but it does not have to be ) .",
    "an important observation is captured in the following theorem .",
    "[ thm : cosses ] @xmath63 where @xmath19 is the dihedral angle and @xmath64 is the angle between @xmath65 and @xmath66 ( see figure [ flo:3dmainidea ] ) .    since @xmath19 is a dihedral angle , @xmath67 .",
    "also @xmath68 substituting @xmath69 and @xmath70 in the above equation we get + @xmath71 since @xmath72 , @xmath73 where @xmath74 is the projection of @xmath61 onto the @xmath38",
    ". clearly also @xmath75 , since that is a vector normal to the @xmath38 .",
    "then @xmath76 is normal to the plane spanned by @xmath77 and @xmath78 .",
    "therefore @xmath79 and @xmath80 + then equation [ dihedbehav ] becomes @xmath81    we should note that the proof of this theorem does not make use of @xmath1 lying on the intersection between the @xmath38 and the splitter , and so this angular relation is true for any @xmath82 on the @xmath38 .",
    "@xmath83    when @xmath84 , @xmath85 .",
    "as @xmath86 , @xmath87 and hence @xmath88 from below .    the dihedral angle @xmath89    since @xmath90 , @xmath91    theorem [ thm : cosses ] and the corollaries suggest the following algorithm for estimating alpha :    we take the center of the data region and sample a constant number ( @xmath92 of random data points within the region",
    "we then subtract the center from all of these points to obtain @xmath93 random vectors that will represent the region ( there are many other methods for obtaining these @xmath93 random vectors ) .",
    "we calculate the angle that all of these vectors make with the normal to the splitter @xmath61 ( each region has one splitter ) .",
    "the smallest of these @xmath93 angles will be within a small range of @xmath94 ( @xmath95 in figure [ flo:3dmainidea ] ) , the true angle between @xmath61 and the @xmath38 ( see the appendix for pseudocode of this procedure ) . in order for this to work ,",
    "at least one of the random vectors must be within a small angle of @xmath96 ( as close to parallel as possible ) .",
    "the probability of this occurring depends on the dimensionality of @xmath38 , as we will show .      in order to evaluate this strategy ,",
    "we form the following problem .",
    "we assume the data is evenly distributed within a d - ball of unit radius , centered at the origin , and we have some fixed vector @xmath97 within this space .",
    "we then generate @xmath93 random vectors within the ball , and calculate the probability that not one of these is within some small angle @xmath21 ( error angle ) of @xmath97 ( please refer to figure [ probmisscone ] for illustration ) .",
    "we denote the volume of the d - ball segment spanning the vectors that are close to @xmath97 by @xmath98 , and the volume of the entire d - ball by @xmath1 .",
    "since we assumed the data to be evenly distributed , every random vector has a constant probability @xmath99 of landing within the aforementioned segment .",
    "@xmath98 has volume given by the formula for the d - dimensional cone @xmath100 @xcite plus the volume of the hypersphere cap given by the formula described in @xcite . combining these two formulas ,",
    "the ratio @xmath99 has the value given by : @xmath101}{\\sqrt{\\pi}\\gamma[\\frac{d+1}{2}]}{}_{2}f_{1}\\left(\\tfrac{1}{2},\\tfrac{1-d}{2};\\tfrac{3}{2};\\cos^{2}\\theta\\right)\\right)+\\frac{2\\cos\\theta\\sin^{d-1}\\theta\\cdot\\gamma[1+\\frac{d}{2}]}{d{\\sqrt{\\pi}\\cdot\\gamma[1+\\frac{d-1}{2}]}}\\ ] ] where @xmath102 is the gamma function and @xmath103 is gauss hypergeometric function @xcite .",
    "note that this ratio depends only on @xmath21 and the intrinsic dimension @xmath10 .",
    "monte carlo experiments confirm the values given by this formula",
    ".     determines the width of the target region ( shaded ) .",
    "if a random vector makes an angle close to @xmath104 to @xmath97 , we simply multiply it by -1 in order to obtain a vector close to @xmath97 .",
    "hence our target region s volume is doubled .",
    "the dashed line divides the target region into two parts ; a hypercone and a hypersphere cap . ]",
    "the probability that all @xmath93 vectors miss the segment is then @xmath105 .",
    "we show that this probability can be made very small for @xmath106 , @xmath107 while @xmath108 beyond that dimension , to maintain a small @xmath21 , @xmath93 would have to grow exponentially .",
    "so in section [ sec : estimdihed ] , as long as @xmath10 is not too large , we can get a vector @xmath82 that is within @xmath21 of @xmath61 .     within a d - dimensional ball",
    "is sensitive to both @xmath21 and @xmath10 .",
    "the formula used to build this graph is @xmath109 where s / s is given by equation [ eq : sons ] . ]    one obvious problem with this approach is that if the data is noisy , then the vectors we generate do not lie exactly on the @xmath38 .",
    "this causes some inaccuracy in determining @xmath19 .",
    "our approach for dealing with this involves simply ignoring a constant portion of the most extreme angles , attributing them to noise .",
    "this is described further in section [ sec : exp ] .",
    "another problem occurs when the region in question is large , where the manifold curves sharply with respect to the splitter . in this case",
    "there is no low dimensional @xmath38 that approximates the data well .",
    "the case in figure [ flo : bigregion ] is not a pathological one . despite the data having intrinsic dimension of 1 ,",
    "the region in question is too large to be approximated by a 1-dimensional @xmath38 .",
    "for this reason , we consider this data region to be 2-dimensional for our purposes .    in order for this technique to not prune any regions incorrectly",
    ", we must find that @xmath110 here - we use the regular kd - tree search lower bound , since @xmath111 in this case . in order for this to happen",
    ", at least one of the randomly generated vectors must make an angle of @xmath20 ( or close to it ) with the splitter .",
    "then , we do not gain or lose anything compared to the standard kd - tree pruning method , in this case . as we continue to split the data ,",
    "the regions will become small enough that they can be approximated by a low dimensional @xmath38 .",
    "another possibility : if the data lies on a d - dimensional manifold , but we consider a region where the manifold curves sharply , but only in one other dimension . in this case , the region can still be approximated by a d+1 dimensional @xmath38 .",
    "for example , the data in figure [ flo : bigregion ] could have a very large ambient dimension , but this region can still be approximated by a 2-dimensional @xmath38 .",
    "our method will be robust to this possibility , since from theorem [ thm : alphadistribution ] , the angle between the d+1 dimensional @xmath38 and the d-1 dimensional splitter is still very likely to be much smaller than @xmath20 , giving the angle tree a large performance boost over the standard kd - tree pruning .",
    "we refer again to figure [ flo:3dmainidea ] .",
    "let @xmath82 be an extension of the randomly generated vector making the smallest angle with @xmath61 .",
    "then if we have some estimation of @xmath10 , and by referring to figure [ flo : probmis ] , we can say that with high probability @xmath82 is within @xmath21 ( of some appropriate size ) of @xmath61  @xmath112 . then making use of theorem [ thm : cosses ] again , @xmath113 , and from this we can derive @xmath114 where @xmath115 is",
    "our slightly erroneous estimate for @xmath19 and @xmath116 is compensation for this error .",
    "putting the query point @xmath4 in the place of @xmath53 , we now have a safe lower bound on @xmath36 , and it tightens as @xmath21 is made smaller ( by generating more random vectors , for instance ) .",
    "this bound should still be much tighter than @xmath117 .",
    "is small , then the distance of data point @xmath23 from @xmath38 is a relatively small component ( on average over all @xmath23 ) of the distance of @xmath23 from the mean of the data region . ]",
    "( a d - ball with radius @xmath118 - represented by the horizontal dashed line ) and an infinite number of ( d - d)-balls centered on each point in the @xmath38 ( not data point ) and perpendicular to the @xmath38 . in the case",
    "when @xmath119 and @xmath120 , the hypercylinder is a rectangle as shown .",
    "data is evenly distributed within the hypercylinder . ]    in figure [ fig : mainidea ] we noted an error region , where any point @xmath23 falling into it has the property @xmath121 and hence breaks one of our assumptions .",
    "the cell containing this point might be pruned using our algorithm , even though it contains a neighbor that is nearer to @xmath4 than the best - so - far found neighbor . in order to analyze this possibility we need a model of the region in question .",
    "for this , we make use of the property of lcd as described in @xcite ; if the data has covariance dimension @xmath122 for some @xmath123 , then any data region that fits entirely inside a ball of radius @xmath123 will have @xmath124 where the @xmath125 and @xmath126 ( geometric mean on all the dimensions ) are taken across all the data points in the region ( see figure [ fig : cov - dim ] ) .",
    "we assume that @xmath127 is small ( @xmath128 ) .",
    "if we let the noise be uniform , we have the following model : points are uniformly distributed within the hypercylinder shown in figure [ flo : errorregion ] . the @xmath38 itself is a @xmath10-ball with radius @xmath118 .",
    "centered on each point on the @xmath38 we have a @xmath129-ball normal to @xmath38 with radius @xmath130 . @xmath118 and @xmath130 are chosen so that the data being distributed uniformly within the hypercylinder , has @xmath131 within @xmath38 equal to 1 , and the @xmath131 from subspace equal to @xmath127 .",
    "the @xmath131 from mean is then equal to @xmath132 , so the model satisfies the above constraint ( since @xmath133 ) while being close to the maximal noise case ( since @xmath134 is insignificant for small @xmath127 ) .",
    "then the ratio of the volume of the error region @xmath44 to the volume of the entire region @xmath135 described by the model corresponds to the proportion of points that fall within the error region .",
    "we obtain it by integrating the error region volume along @xmath136 ( any axis within @xmath38 ) from @xmath137 til @xmath138 and multiplying by 2 .",
    "we denote the volume of a @xmath93-ball with radius @xmath139 by @xmath140 and the volume of a hypersphere cap of height @xmath53 and dimension @xmath93 by @xmath141 . by considering the intersection of the @xmath38 ( which we assumed to be a d - ball ) and a 2-dimensional plane through @xmath142 , and noting that this intersection must be a circle , we see that the radius of the ( d-1)-ball in the @xmath38 subtended by the integration slice @xmath136 units away from @xmath142 will have radius @xmath143 .",
    "the volume subtended from the ( d - d)-ball in the noisy directions will equal @xmath144 .",
    "hence the total volume of the error region is given by :    @xmath145    where @xmath3 is the ambient dimension , @xmath10 is the intrinsic dimension of @xmath38 , @xmath146 , @xmath147 , and @xmath127 is the lcd coefficient .",
    "we often find that this ratio is very small when @xmath148 and @xmath149 +    when @xmath146 and @xmath147 , then the data distributed uniformly within the hypercylinder , has @xmath131 within @xmath38 equal to 1 , and the @xmath131 from subspace equal to @xmath127 .",
    "we assumed that the first d axes are : @xmath150 $ ] . then @xmath151    because of the linearity of expectation , and all the @xmath152 s are equal . + @xmath153    giving @xmath146 + by a similar process we get @xmath154 +    it can be seen that the error region is smaller if @xmath19 is made larger ( our splitter is closer to the ideal splitter ) . in section",
    "[ sec : disc ] we give one possible way to achieve this .",
    "in this section the angle tree is experimentally compared against the cover tree and locality sensitive hashing ( lsh ) .",
    "they are the two most popular and most recent developments in high - dimensional nns .",
    "the angle tree is also compared against partial brute force ( pbf ) .",
    "if for an experiment the angle tree achieved an average accuracy of 90% or 0.9 when searching for k - nearest neighbors ( meaning that all k neighbors returned were the true nearest neighbors 90% of the time , as as verified by a full brute force search ) , then pbf , searching randomly through unindexed data , must search through @xmath155 to achieve the same average accuracy .    the comparison will be based on preprocessing time , space complexity , query time and accuracy .",
    "the comparison vis - a - vis kd - trees and rp - trees is not reported as the nns query using the latter two data structures degenerates to a brute force search in high dimensions if the standard pruning bound is used .",
    "it it worth recalling that the original rp - tree ( as proposed by the authors ) can only be used to efficiently answer _ near neighbor search _ _ _ ( where we only search through a single cell and terminate - see section [ sec : keyidea ] for details of why this often misses the true nearest neighbors ) .",
    "_ _ if the kd - tree search strategy is used with the rp - tree , then the search degenerates to brute force .",
    "in fact , the principal aim of angle trees is to extend rp - trees ( and kd - trees ) to make nns in high dimensions efficient and accurate .",
    "the angle tree was implemented on top of the rp - tree code base available from the authors website @xcite .",
    "we did not use the error angle adjustment in our implementation , since speed seemed to be more of a bottleneck than accuracy .",
    "all experiments were performed on an amd dual core 1 ghz processor with 4 gb of ram . in our experiments",
    "we will refer to the angle tree as angle rp - tree to emphasize its connection with random projections and rp - trees .",
    "cover tree code was obtained from the authors website @xcite .",
    "this code was used to attempt to preprocess all of the data sets used .",
    "the cover tree performance data is taken from @xcite .",
    "lsh ( where the hash functions are a series of random projections ) performance was inferred from the performance of _ near neighbor search _ _ _ in a single rp - tree . _ _ if @xmath156 independent rp - trees are built , then each one has a similar probability @xmath23 of having both the query point @xmath4 and its nearest neighbor hashed to the same cell .",
    "then the probability of not finding the nearest neighbor in any of the @xmath156 trees is @xmath157 .",
    "the average search time is then @xmath158 , where @xmath159 is the average search time for a single rp - tree near neighbor search .",
    "the probability @xmath23 obviously depends on the size of the cells - or the number of lsh hash buckets .",
    "this data is presented in table [ flo : lshresults ] .",
    "several well known real data sets were used for comparison .",
    "we also generated two synthetic data sets on high - dimensional spheres .",
    "the details are listed in table [ tab : data ] in the appendix .",
    "it is worth emphasizing that we used extremely high - dimensional , real world data sets ( e.g. , reuters bag of words , mnist and yale face image database ) which are extremely noisy and hard to index . for overview of all the data sets used ,",
    "see appendix .",
    "the fundamental metric used to compare the data structures is _ number of distance calculations ( ndc ) _ - which is defined as the number of times euclidean distance between two points is calculated .",
    "this metric is hardware independent .",
    "ndc is used as a proxy to measure the running time of the algorithm , being by far the most computationally intensive part of the algorithm .",
    "ndc correlates very strongly with the actual running time of the algorithm , and was used in @xcite to evaluate the cover tree .",
    "there is one important parameter in angle rp - trees whose effect needs to be measured , namely , the _ ignore outlier ( iout ) _ parameter .",
    "iout controls the effect of noise in the data during the estimation of the dihedral angle .",
    "iout is the proportion of the angles between the random vector and the normal to the splitter ( @xmath61 in figure [ flo:3dmainidea ] ) that will be ignored .",
    "the assumption is that very small angles are caused due to the presence of outliers , since outliers do not lie on the @xmath38 and do not conform to theorem [ thm : cosses ] .",
    "for example , suppose one thousand random data vectors are generated and for each such vector , the angle with the normal to the splitter is calculated .",
    "these angles are then sorted in an increasing order and if the iout value is 0.1 , then the @xmath160 angle is the estimated value of the dihedral angle .",
    "iout controls the trade - off between accuracy and search time .",
    "high values of iout will inflate the estimation of @xmath161 .",
    "this will result in more aggressive pruning but could result in some nearest neighbors being missed .",
    "another parameter that we vary is the number of levels in the tree .",
    "this is significant for the lsh analysis as it determines the number of hash buckets .",
    "more and smaller buckets usually translates to lower accuracy but faster search time .",
    "the other lsh parameter is @xmath156 , the number of trees ( hash functions ) over which we infer the performance of lsh",
    ".      the following three experiments were conducted . in e1 ,",
    "we measure preprocessing efficiency , and in e2 and e3 we measure search efficiency .    00.00.0000    for each data set , the angle rp - tree was constructed and the ndc was recorded .",
    "we note that ndc has the same complexity as angle computations and projection onto splitter ( o(d ) , since they all involve a dot product ) and so for the angle rp - tree we include them in the ndc value .",
    "we also used the cover tree code to see whether preprocessing the various data sets would cause a crash .",
    "we used the standard rp - tree ( with various number of levels ) as an implementation for lsh .    for each data set ,",
    "one thousand random data points were chosen and used to simulate queries . in this way",
    "the query points were guaranteed to also come from the data manifold .",
    "the angle rp - tree was then used to search for 1-nn with various iout parameter values for each query point .",
    "the ndc was recorded during the search process and then averaged over the one thousand queries .    for the mnist and kdd data , a search for 2-nn",
    "was also carried out as comparative data was available from @xcite .",
    "we now report on the results of the three different experiments under varying conditions .",
    "[ [ e1 ] ] e1 ^^    [ [ cover - tree ] ] cover tree + + + + + + + + + +    see figure [ fig : preprocess ] .",
    "theoretically the preprocessing time of angle rp - trees is nearly of the order of a standard kd - tree or rp - tree , which is @xmath162 .",
    "the only additional overhead is the calculation and storage of the dihedral angle for each data region . since the number of data regions ( nodes ) is @xmath163 , and we calculate a constant number of angles ( @xmath93 ) in each data region , the complexity of angle rp - tree is @xmath164 which is @xmath162 .    for cover trees ,",
    "the theoretical preprocessing time is : + @xmath165 where @xmath166 ( the kr - dimension ) @xmath167 and potentially much larger for noisy data .",
    "a single noisy point can make @xmath166 grow arbitrarily @xcite .",
    "the preprocessing in our experiments for the cover tree and angle rp - tree are shown in figure [ fig : preprocess ] .",
    "it is clear that the preprocessing time of the angle rp - tree is a small fraction ( less than 1% ) of the cover tree .",
    "another factor that is not reflected in the results is that even though the cover tree space requirements are o(n ) just like the kd - tree , the data points appear multiple times in the structure . thus cover tree s space complexity has a higher constant .",
    "its memory usage is often three to five times greater than the size of the data set @xcite .",
    "we have only shown results on three data sets for which the comparison data was available .",
    "the cover tree crashed during construction for the reuters bag of words and the synthetic sphere databases .",
    "it crashed likely due to overflow of the recursion stack during construction @xcite due to those data sets having an intrinsic dimension that is too large .",
    "[ [ lsh ] ] lsh + + +    see table [ flo : lshresults ] .",
    "the preprocessing time of lsh is similar to rp - trees except that there are @xmath156 trees ( hash functions ) and so roughly @xmath156 times as much preprocessing ( the constant number of angle calculations that are not required in lsh become relatively trivial anyway , when the data set is large ) .",
    "however , lsh preprocessing time has no dependence on @xmath10 and is unaffected by noise , and so is fairly efficient .",
    "due to there being @xmath156 trees and each data point is _ hashed _ into each tree , lsh space complexity is significantly greater than the angle rp - tree .",
    "it is difficult to say how much more exactly since only the hash values of the data are stored , but @xcite reports it to be significant - several times the size of the original data . as we will see later ,",
    "when @xmath156 is reduced , then accuracy and/or search speed suffers . for larger data sets than the ones tested here , @xcite note that lsh often requires @xmath156 to be of the order of 100 to obtain acceptable search speed and accuracy .",
    "[ [ e2 ] ] e2 ^^    [ [ pbf ] ] pbf + + +    see figure [ flo : angletreevspbf ] . here",
    "we see the speedup of angle rp - tree over pbf . even in 30,000 + dimensional data like the images database",
    ", we find the true nearest neighbors 95%+ of the time while searching < 20% of the data .",
    "this indicates very strongly that the curse of dimensionality can be managed when data has a low intrinsic dimensionality , without any dependence on the ambient dimension .",
    "we believe that if the database had more items ( yale face has < 2500 items ) the angle rp - tree indexing would give an even greater speedup , similar to the kdd data sets .    the noisiest ( or least structured ) and hence most difficult data set was the reuters bag of words data set . for this data set we had to search through 42% of the data in order to achieve 95% accuracy for 1-nns .",
    "we could also search through 24% of data to achieve 85% accuracy .",
    "this second result seems to be more practical to us , though the user would have to accept a 15% error rate .",
    "additionally , in the 15% of cases where an incorrect neighbor was returned , it was usually a small error .",
    "as far as we know , no one else had indexed this kind of data set with any significant success .",
    "the sin3d , as well as the 15d and 20d sphere synthetic data sets are included to indicate what sort of complexity of structure the real world data sets must have , if they have similar performance to the synthetic data sets whose structure is known . the 15d sphere data set gave much better results , and was close in performance to the kdd bio data sets , whereas the 20d sphere data set was closer to the reuters bag of words data set .",
    "this is consistent with our analysis in section [ sec : estimdihed ] , which suggested that the intrinsic dimensionality must be not much greater than ten in order for the dihedral angle to be accurately estimated .",
    "results for the sin3d data set were 100% accurate , with most of the angles being over @xmath168 .",
    "the angle rp - tree reduced to the kd - tree for this low - dimensional data set .",
    "[ [ cover - tree-1 ] ] cover tree + + + + + + + + + +    see figure [ flo : cover1nns ] . the angle rp - tree achieves accuracy well over 95% with search speed similar to or faster than the cover tree , although the cover tree achieves 100% accuracy .",
    "the angle rp - tree seems to slow down significantly as we try to approach 100% accuracy with noisy data .",
    "we search through twice as much data for the mnist data set in order to increase accuracy from 98.3% to 99.8% .",
    "this is likely because the @xmath38 starts to include more and more rare , noisy directions as we reduce the iout parameter value .",
    "as the @xmath38 grows in dimension , the dihedral angle grows quite rapidly causing the pruning multiplier @xmath169 to shrink .",
    "this causes the algorithm to prune a lot less cells for very little gain in accuracy ( when the noisy points turn out to be the true nearest neighbors ) . due to this",
    "we can only really achieve 100% accuracy as well as fast running time when the data is not noisy , and hence can be well estimated by a low dimensional @xmath38 . for noisy real world data , we must at this stage settle for 95+% accuracy .",
    "[ [ lsh-1 ] ] lsh + + +    see table [ flo : lshresults ] . in this table @xmath156 and @xmath23 are chosen so that the accuracy corresponds to one of the results in figure [ flo : angletreevspbf ] .",
    "we can see that for the reuters bag of words and mnist data sets , lsh with large @xmath156 parameter is often several times faster on average than the angle rp- tree , while having a similar accuracy .",
    "however , when we reduce @xmath156 ( in order to alleviate the space usage ) but wish to maintain accuracy , a tree with less levels must be built .",
    "then , since the leaf cells will contain more data , the probability that the leaf cell into which the query point is hashed will contain its true nearest neighbor , is made higher .",
    "this in turn makes lsh slower .",
    "in fact , it can be seen that when @xmath156 is made too small , lsh becomes significantly slower _ and _ less accurate than the angle rp - tree .",
    "it is also worth noting that in table [ flo : lshresults ] , when considering the yale face image database , lsh is significantly slower and less accurate ( as well as more space consuming ) than the angle rp - tree .",
    "we believe this is because the yale face image database is less noisy than mnist and reuters bow , and can be better approximated by a low dimensional @xmath38 .      [ [ pbf-1 ] ] pbf + + +    see figure [ flo : angletreevspbf ] - those columns labeled 2nn .",
    "the angle rp - tree searches through approximately twice as much data on average for 2-nns than 1-nns .",
    "this is logical since the second nearest neighbor is further away from the query point , making the search sphere ( see section [ sec : keyidea ] ) larger , causing the algorithm to prune less subtrees . in this way",
    "the angle rp - tree behaves as it should .",
    "the kd - tree operating in the intrinsic space of the data would likely perform in a similar way .",
    "[ [ cover - tree-2 ] ] cover tree + + + + + + + + + + +    see figure [ flo : cover2nn ] .",
    "these results are roughly the same as in e2 .",
    "the angle rp - tree introduces a small probability of error while achieving comparable search speed to the cover tree .",
    "[ [ multi - probe - locality - sensitive - hashing ] ] multi - probe locality sensitive hashing + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this is a variation of lsh , where instead of only checking one bucket , multiple buckets are probed . in some implementations ,",
    "the buckets are sorted by probability that they contain the true nearest neighbor , and only the top few are checked . the purpose of this is to reduce the space requirement of lsh by reducing the number of hash functions that are needed to maintain high accuracy .",
    "the problem with this approach is that a constant number of buckets will be checked with each query , whereas with the angle rp - tree it is often the case that the entire tree may be pruned after checking one or two cells .",
    "there is no set number of buckets to check .",
    "additionally , current implementations of multi - probe lsh are based on calculating the distance of the query point to the splitter @xcite . however , as we saw in theorem [ thm : alphadistribution ] , since random splitters make significantly varying angles with the data , this distance alone is not the best way of ranking the cells .",
    "the angle rp - ree seems to be a promising way to improve current multi - probe lsh implementations .",
    "we have shown that even at this early stage , our algorithm can compete with the state of the art algorithms in terms of running time and accuracy , while being significantly superior in terms of space and preprocessing requirements .",
    "additionally , it is likely to be much simpler to use and implement , since it is not a major modification to the well known kd - tree .",
    "we have also provided some new insight on the nature of the curse of dimensionality in the context of nns .",
    "[ [ future - work ] ] future work + + + + + + + + + + +    a simple improvement to the random splitter ( as used in the rp - tree ) would be a splitter whose normal vector lies on or very close to the @xmath38 , but is still random within this restricted space .",
    "this can be achieved by generating some constant number of random vectors in the data as before , and then averaging them , finding their median , or combining them in some arbitrary way .",
    "a splitter generated thus would make a much larger angle with @xmath38 than a random splitter , and make the error region discussed earlier much smaller .",
    "it seems very likely to us that there is a superior and more robust method for estimating the dihedral angle with noisy data , such as a kalman filter type of process .",
    "the way we deal with noise in the data , and the effect it has on the dihedral angle estimation , is quite simplistic .",
    "it is almost certain that there are more robust ways for doing this .",
    "the exact relationship between the error angle compensation ( multiplying by @xmath116 ) and the search speed hasnt been analyzed yet .",
    "in fact , no guarantees on search speed , even in the average case , have been established .",
    "sin3d & 10,000 & 3 & 3 & rp - tree example data +   mnist @xcite & 60,000 & 784 & 5 - 10 & handwriting +   bio_train @xcite & 145,751 & 75 & 6 & kdd +   bio_test @xcite & 139,658 & 74 & 6 & kdd +   extended yale face b cropped @xcite & 2432 & 32,256 & 8 & human faces .",
    "various people .",
    "varying lighting +   reuters bag of words ( bow ) @xcite & 11,887 & 6100 & 10 - 40 & news articles word counts +   15d sphere & 100,000 & 15 & 14 & synthetic +   20d sphere & 100,000 & 20 & 19 & synthetic +   sift@xcite & 1,000,000 & 128 & 15 - 30 & image descriptors +     +    > p0.5in|>p0.5in|r|>p0.5in|>p0.9in|>p0.9in||>p0.5in|r & avg . per search & accuracy ( % ) & number of trees &",
    "projected accuracy ( % ) & avg . per search over all hashes & avg . per search &   +   &   +    +   & 205.8 & 21 & 13 & 95.4 & 2675.4 & 10272.0 &   +   & 269.1 & 21.2 & 13 & 95.5 & 3498.3 &  &   +   & 613.8 & 27.4 & 10 & 96 & 6138 &  &   +   & 5855.3 & 51.7 & 4 & 94.6 & 23,421.2 &  &   +   & 9144.8 & 61.1 & 3 & 94.1 & 27,434.4 &  &   +    +   +   & 195.9 & 28.3 & 9 & 95 & 1763.1 & 3561.8 &   +    +   +   & 183 & 29.6 & 10 & 97.1 & 1830 & 3635.8 &   +    +   +   & 169.2 & 57.4 & 5 & 98.6 & 846 & 620.5 &   +    +   +   & 217.9 & 33.3 & 5 & 86.8 & 1089.5 & 2917.8 &   +    +   +   & 192.3 & 13.2 & 19 & 93.2 & 3653.7 & 11,507 &   +    +   +   & 204.3 & 10.6 & 26 & 94.6 & 5311.8 & 20,757 &   +     +    * procedure * @xmath170    tree_node node @xmath171 @xmath172 @xmath173 @xmath174 @xmath175",
    "* procedure * @xmath176    @xmath177 $ ] @xmath178median ( or mean ) of pointlist for each axis @xmath179    @xmath180 random point from pointlist @xmath181 @xmath182 angles.sort ( ) @xmath183 $ ]    * pruning procedure during nns *    @xmath184 @xmath185 we now modify our pruning criterion during nns from : prune node s other child ( where @xmath4 did not come from ) to : prune node s other child ( where @xmath4 did not come from ) .",
    "40 a global geometric framework for nonlinear dimensionality reduction",
    ". j. b. tenenbaum , v. de silva and j. c. langford ( 2000 ) .",
    "science 290 ( 5500 ) , 2319 - 2323 .",
    "freund , yoav ; dasgupta , sanjoy random projection trees and low dimensional manifolds lecture , university of california , san diego , http://www.stanford.edu/group/mmds/slides2008/dasgupta.pdf accessed on 15/10/2009 , 2008      jose a. costa and alfred o. hero iii manifold learning using euclidean k - nearest neighbor graphs acoustics , speech , and signal processing",
    ". proceedings .",
    "( icassp 04 ) .",
    "ieee international conference , may 2004      alexandr andoni ; piotr indyk near - optimal hashing algorithms for approximate nearest neighbor in high dimensions communications of the acm , special issue : breakthrough research : a preview of things to come , 2008      alina beygelzimer ; sham kakade ; john langford cover trees for nearest neighbor acm international conference proceeding series ; vol .",
    "148 , proceedings of the 23rd international conference on machine learning , 2006      piotr indyk near - optimal hashing algorithms for approximate near(est ) neighbor problem lecture , massachusetts institute of technology , http://www.mit.edu/~andoni/papers/csquared.pdf accessed on 12/10/2009      peter j. verveer , robert p.w .",
    "duin an evaluation of intrinsic dimensionality estimators ieee transactions on pattern analysis and machine intelligence , vol .",
    "81 - 86 , jan .",
    "1995 , doi:10.1109/34.368147    d. t. lee1 and c. k. wong ( 1977 ) worst - case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees acta informatica , volume 9 , pages 23 - 29 , 1977        peter n. yianilos locally lifting the curse of dimensionality for nearest neighbor search symposium on discrete algorithms , proceedings of the eleventh annual acm - siam symposium on discrete algorithms , 2000      donghui yan ; ling huang ; michael i. jordan fast approximate spectral clustering international conference on knowledge discovery and data mining , proceedings of the 15th acm sigkdd international conference on knowledge discovery and data mining , 2009    roger weber ; hans - jorg schek ; stephen blott a quantitative analysis and performance study for similarity - search methods in high - dimensional spaces very large data bases , proceedings of the 24rd international conference on very large data bases , 1998                    david r. karger , matthias ruhl finding nearest neighbors in growth - restricted metrics annual acm symposium on theory of computing archive proceedings of the thiry - fourth acm symposium on theory of computing , 2002            qin lv , william josephson , zhe wang , moses charikar , kai li multi - probe lsh : efficient indexing for high - dimensional similarity search very large data bases ; proceedings of the 33rd international conference on very large data bases , 2007    jerome h. friedman , jon louis bentley , raphael ari finkel an algorithm for finding best matches in logarithmic expected time acm transactions on mathematical software ( toms ) volume 3 , issue 3 , september 1977            brin s. near neighbor search in large metric spaces .",
    "proceedings of the international conference on very large data bases .",
    "institute of electrical & electronics engineers ( ieee ) ; 1995:574 - 584 .",
    "available at : http://scholar.google.com/scholar?hl=en&btng=search&q=intitle:near+neighbor+search+in+large+metric+spaces#0 ."
  ],
  "abstract_text": [
    "<S> we propose an extension of tree - based space - partitioning indexing structures for data with low intrinsic dimensionality embedded in a high dimensional space . </S>",
    "<S> we call this extension an angle tree . </S>",
    "<S> our extension can be applied to both classical kd - trees as well as the more recent rp - trees .    </S>",
    "<S> the key idea of our approach is to store the angle ( the `` dihedral angle '' ) between the data region ( which is a low dimensional manifold ) and the random hyperplane that splits the region ( the `` splitter '' ) .    </S>",
    "<S> we show that the dihedral angle can be used to obtain a tight lower bound on the distance between the query point and any point on the opposite side of the splitter . </S>",
    "<S> this in turn can be used to efficiently prune the search space . </S>",
    "<S> we introduce a novel randomized strategy to efficiently calculate the dihedral angle with a high degree of accuracy .    </S>",
    "<S> experiments and analysis on real and synthetic data sets shows that the angle tree is the most efficient known indexing structure for nearest neighbor queries in terms of preprocessing and space usage while achieving high accuracy and fast search time .    </S>",
    "<S> high - dimensional indexing , image indexing , very large databases , approximate search . </S>"
  ]
}