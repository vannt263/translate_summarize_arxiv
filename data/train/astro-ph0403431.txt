{
  "article_text": [
    "today , there are many impressive archives painstakingly constructed from observations associated with an instrument",
    ". the hubble space telescope ( hst ) , the chandra x - ray observatory , the sloan digital sky survey ( sdss ) , the two micron all sky survey ( 2mass ) , and the digitized palomar observatory sky survey ( dposs ) are examples of this .",
    "furthermore yearly advances in electronics bring new instruments , doubling the amount of data we collect each year . for example ,",
    "approximately a gigapixels is deployed on all telescopes today , and new gigapixel instruments are under construction .",
    "this trend is bound to continue . just like what szalay says , the astronomy is facing `` data avalanche '' ( see e.g. , szalay & gray 2001 ) . how to organize , use , and make sense of the enormous amounts of data generated by today s instruments and experiments ?",
    "it is very time consuming and demands high quality human resources .",
    "therefore , better features and better classifiers are required . in addition",
    ", expert systems are also useful to get quantitative information .",
    "it is possible to solve the above questions with neural networks ( nns ) , because they permit application of expert knowledge and experience through network training .",
    "furthermore , astronomical object classification based on neural networks requires no priori assumptions or knowledge of the data to be classified as some conventional methods need .",
    "neural networks , over the years , have proven to be a powerful tool capable to extract reliable information and patterns from large amounts of data even in the absence of models describing the data ( cf .",
    "bishop 1995 ) and are finding a wide range of applications also in the astronomical community : catalogue extraction ( andreon et al .",
    "2000 ) , star / galaxy classification ( odewahn et al . 1992 ; naim et al .",
    "1995 ; miller & coe 1996 ; m@xmath0h@xmath1nen & hakala 1995 ; bertin & arnout 1996 ; bazell & peng 1998 ) , galaxy morphology ( storrie - lombardi et al . 1992 ; lahav et al .",
    "1996 ) , classification of stellar spectra ( bailer - jones et al . 1998",
    "; allende prieto et al .",
    "2000 ; weaver 2000 ) .",
    "just to name a few , the rising importance of artificial neural networks is confirmed in this kind of task .",
    "there is also a very important and promising recent contribution by andreon et al .",
    "( 2000 ) covering a large number of neural algorithms .    in this work , a class of supervised neural networks called learning vector quantization ( lvq ) was proposed .",
    "lvq shares the same network architecture as the kohonen self - organizing map ( som ) , although it uses a supervised learning algorithm .",
    "bazell & peng ( 1998 ) pioneered the use of it in astronomical applications .",
    "another class of supervised neural networks named multi - layer perceptrons ( mlp ) was presented .",
    "goderya & mcguire ( 2000 ) summarized progress made in the development of automated galaxy classifiers using neural networks including mlp .",
    "qu et al . ( 2003 ) experimented and compared multi - layer perceptrons ( mlp ) , radial basis function ( rbf ) , and support vector machines ( svm ) classifiers for solar - flare detection .",
    "meanwhile , an automated algorithm called support vector machines ( svm ) for classification was introduced .",
    "the approach was originally developed by vapnik ( 1995 ) .",
    "wozniak et al . ( 2001 ) and humphreys et al . (",
    "2001 ) have pioneered the use of svm in astronomy .",
    "wozniak et al . ( 2001 ) evaluated svm , k - means and autoclass for automated classification of variable stars and compared their effectiveness .",
    "their results suggested a very high efficiency of svm in isolating a few best defined classes against the rest of the sample , and good accuracy for all classes considered simultaneously .",
    "humphreys et al .",
    "( 2001 ) used different classification algorithms including decision trees , k - nearest neighbor and support vector machines for classifying the morphological type of the galaxy .",
    "furthermore , they got the very promising results of their first experiments with different algorithms .",
    "celestial objects radiate energy over an extremely wide range of wavelengths from radio waves to infrared , optical to ultraviolet , x - ray and even gamma rays .",
    "each of these observations carries important information about the nature of objects .",
    "different physical processes show different properties in different bands . based on these",
    ", we apply learning vector quantization ( lvq ) , single - layer perceptron ( slp ) and support vector machines ( svm ) to classify agns , stars and normal galaxies with data from optical , x - ray , infrared bands . in this paper",
    "we present the principles of lvq , slp and svm in section 2 . in section 3",
    ", we discuss the sample selection and analysis the distribution of parameters . in section 4",
    "the computed results and discussion are given .",
    "finally , in section 5 we conclude this paper with a discussion of general technique and its applicability .",
    "here the adopted learning vector quantization ( lvq ) algorithm is based upon the lvq_pak routines developed at the laboratory of computer and information sciences , helsinki university of technology , finland .",
    "their software can be obtained via the www from www.cis.hut.fi / research / lvq_pak/. if interested in the application of lvq in astronomy , we can refer to the papers of bazell & peng ( 1998 ) and cortiglioni et al .",
    "( 2001 ) .",
    "the lvq method was developed by kohonen ( 1989 ) who also developed the popular unsupervised classification technique known as the self - organizing map or topological map neural networks ( kohonen 1989 , 1990 ) .",
    "som performs a mapping from an @xmath2-dimensional input vector onto two - dimensional array of nodes that is usually displayed in a rectangular or hexagonal lattice .",
    "the mapping is performed in such a way as to preserve the topology of the input data .",
    "this means that input vectors that are similar to each other in some sense , are mapped to neighboring regions of the two - dimensional output lattice .",
    "each node in the output lattice has an @xmath2-dimensional reference vector of weights associated with it , one weight for each element of the input vector .",
    "the som functions compare the distance , in some suitable form , between each input vector and each reference vector in an iterative manner . with each iteration ,",
    "the reference vectors are moved around in the output space until their positions converge to a stable state . when the reference vector that is closest to a given input vector is found ( the winning reference vector ) , that reference vector",
    "is updated to more closely match the input vector .",
    "this is the learning step .",
    "lvq uses that same internal architecture as som : a set of @xmath2-dimensional input vectors are mapped onto a two - dimensional lattice , and each node on the lattice has an @xmath2-dimensional reference vector associated with it . the learning algorithm for lvq ,",
    "i.e. , the method of updating the reference vectors , is different from that of som . because lvq is a supervised method , during the learning phase",
    "the input data are tagged with their correct class .",
    "we define the input vector @xmath3 as : @xmath4 reference vector for @xmath5th output neuron @xmath6 as : @xmath7 define euclidean distance between the input vector and the reference vector of the neuron @xmath5 as : @xmath8 when @xmath9 is a minimum , the input vectors are compared to the reference vectors and the closest match is found using the formula @xmath10 where @xmath3 is an input vector , @xmath6 are the reference vectors , and @xmath11 is the winning reference vector .",
    "the reference vectors are then updated using the following rules : + if @xmath3 is in the same class as @xmath11 , @xmath12 if @xmath3 is in a different class from @xmath11 , @xmath13 if @xmath5 is not the index of the winning reference vector , @xmath14    the learning rate @xmath15 should generally be made to decrease monotonically with time , yielding larger changes for early iterations and more fine tuning as convergence is approached .",
    "the time @xmath16 is taken as positive integers . here",
    "we adopt the optimized - leaning - rate @xmath17 ( see kohonen et al .",
    "1995 ) @xmath18 where @xmath19 if the classification is correct and @xmath20 if the classification is wrong . in this work ,",
    "the initial value of @xmath17 is selected , 0.3 , whereby learning is significantly speeded up , especially in the beginning , and the @xmath11 quickly find their approximate asymptotic values .",
    "two hundred codebook vectors in the codebook is adopted , meanwhile , 7 neighbors is used in knn - classification .",
    "the network is trained for 5000 epochs .",
    "there are several versions of the lvq algorithm for which the learning rules differ in some details .",
    "see kohonen ( 1995 ) for an explanation of the differences between these algorithms .",
    "when the learning phase is over , the reference vectors can be frozen , and any further inputs to the system will be placed into one of the existing classes , but the classes will not change .",
    "support vector machines ( svm ) are learning machines that can perform binary classification ( pattern recognition ) and real valued function approximation ( regression estimation ) tasks .",
    "svm creates functions from a set of labeled training data and operate by finding a hypersurface in the space of possible inputs .",
    "this hypersurface will attempt to split the positive examples from the negative examples .",
    "the split will be chosen to have the largest distance from the hypersurface to the nearest of the positive and negative examples .",
    "intuitively , this makes the classification correct for testing data that is near , but not identical to the training data . in detail , during the training phase svm takes a data matrix as input , and labels each sample as either belonging to a given class ( positive ) or not ( negative ) .",
    "svm treats each sample in the matrix as a point in a high - dimensional feature space , where the number of attributes determines the dimensionality of the space .",
    "svm learning algorithm then identifies a hyperplane in this space that best separates the positive and negative training samples .",
    "the trained svm can then be used to make predictions about a test sample s membership in the class . in brief , svm non - linearly maps their n - dimensional input space into a high dimensional feature space . in this high dimesional feature space",
    "a linear classifier is constructed .",
    "more information can be found in burges tutorial ( 1998 ) or in vapnik s book ( 1995 ) .",
    "given some training data + @xmath21    if the data is linearly separable , one can separate it by an infinite number of linear hyperplanes .",
    "we can write these hyperplanes as @xmath22    among these hyperplanes , the one with the maximum margin is called by the optimal separating hyperplane .",
    "this hyperplane is uniquely determined by the support vectors on the margin .",
    "it satisfies the conditions @xmath23\\ge 1,\\qquad i=1,\\ldots , l.\\ ] ]    besides satisfying the above conditions , the optimal hyperplane has the minimal norm @xmath24    the optimal hyperplane can be found by finding the saddle point of the lagrange functional : @xmath25y_i-1)\\ ] ] where @xmath26 are lagrange multipliers .",
    "the lagrangian has to be minimized with respect to @xmath27,@xmath28 and maximized with respect to @xmath29 .",
    "the saddle point is defined as follows : @xmath30 where @xmath31 is the maximum point of @xmath32 subject to constraints @xmath33    therefore the optimal separating hyperplane has the form @xmath34    this solution only holds for linearly separable data , but has to be slightly modified for linearly non - separable data , the @xmath35 has to be bounded : @xmath36 where c is a constant chosen a priori .    to generalize to non - linear classification",
    ", we replace the dot product with a kernel [ @xmath37 . for binary classification , stitson et al.(1996 ) and",
    "gunn ( 1998 ) stated it in detail .",
    "as for the multi - class classification can refer to weston and watkins ( 1998 ) .",
    "multi - layer perceptrons ( mlp ) are feedforward neural networks trained with the standard backpropagation algorithm . if no hidden layer , mlp are also called single - layer perceptron .",
    "they are supervised networks so they require a desired response to be trained .",
    "they learn how to transform input data into a desired response , so they are widely used for pattern classification . with one or two hidden layers , they can approximate virtually any input - output map .",
    "they have been shown to approximate the performance of optimal statistical classifiers in difficult problems .",
    "most neural network applications involve mlp .",
    "the basic mlp building unit is a model of artificial neuron .",
    "this unit computes the weighted sum of the inputs plus the threshold weight and passes this sum through the activation function ( usually sigmoid ) ( 18 ) , ( 19 ) : @xmath38 @xmath39 where @xmath40 is the linear combination of inputs @xmath41 of neuron @xmath42 , @xmath43 is the threshold weight connected to a special input @xmath44 , @xmath45 is the output of neuron @xmath42 , and @xmath46 is its activation function .",
    "herein we use a special form of sigmoidal ( non - constant , bounded , and monotone - increasing ) activation function - logistic function @xmath47 in a multilayer perceptron , the outputs of the units in one layer form the inputs to the next layer .",
    "the weights of the network are usually computed by training the network using the back - propagation ( bp ) algorithm .",
    "a multilayer perceptron represents a nested sigmoidal scheme ( 18 ) , its form for a single output neuron is @xmath48 where @xmath46 is the sigmoidal activation function , @xmath49 is the synaptic weight from neuron @xmath42 in the last hidden layer to the single output neuron @xmath50 , and so on for the other synaptic weights , @xmath51 is the @xmath5-th element of the input vector @xmath3 .",
    "the weight vector @xmath52 denotes the entire set of synaptic weights ordered by layer , then the neurons in a layer , and then their number in a neuron .",
    "usually , astronomical object classification is based on the properties of spectra , photometry , multiwavelength and so on . in order to check the effectiveness and the efficiency of our provided methods , we classified objects with data from x - ray ( rosat ) , optical ( usno - a2.0 ) and infared ( 2mass ) bands . by positional cross - correlation of rosat , usno - a2.0 and 2mass released databases",
    ", we obtain the multi - wavelength data .",
    "the three catalogs are described in detail as follows :    the rosat all - sky ( rass ) using an imaging x - ray telescope ( tr@xmath53mper 1983 ) , are well suited for investigating the x - ray properties of astronomical objects .",
    "the rass bright source catalogue ( rbsc ) includes 18,811 sources , with a limiting rosat pspc countrate of 0.05 counts s@xmath54 in the 0.1 - 2.4 kev energy band .",
    "the typical positional accuracy is 30  .",
    "similarly , the rass faint source catalogue ( rfsc ) contains 105,924 sources and represents the faint extension to rbsc .",
    "the rbsc and rfsc catalogues contain the rosat name , positions in equatorial coordinates , the positional error , the source countrate ( @xmath55 ) and error , the background countrate , exposure time , hardness - ratios @xmath56 and @xmath57 and errors , extent ( @xmath58 ) and likelihood of extent ( @xmath59 ) , and likelihood of detection . the two hardness ratios @xmath56 and @xmath57 represent x - ray colors . from the count rate @xmath60 in the 0.1 - 0.4 kev energy band and the count rate @xmath61 in the 0.5 - 2.0 kev energy band",
    ", @xmath56 is given by : @xmath62 .",
    "@xmath57 is determined from the count rate @xmath63 in the 0.5 - 0.9 kev energy band and the count rate @xmath64 in the 0.9 - 2.0 kev energy band by : @xmath65 .",
    "@xmath55 is rosat total count rate in counts s@xmath54 .",
    "the parameters of @xmath58 and @xmath59 are source extent in arcsecond and likelihood of source extent in arcsecond , respectively .",
    "the amount of @xmath58 is specified , by which the source image exceeds the point spread function .",
    "the parameters of @xmath58 and @xmath59 reflect that sources are point sources or extent sources .",
    "for example , stars or quasars are point sources ; galaxies or galaxy clusters are extent sources",
    ". therefore @xmath58 and @xmath59 are useful for classification of objects .",
    "the usno - a2.0 ( monet et al . 1998 ) is a catalog of 526,280,881 stars over the full sky , compiled in the u.s .",
    "naval observatory , which contains stars down to about 20 mag over the whole sky .",
    "its astrometric precision is non - uniform , depending on position on schmidt plates , typically better than 1  .",
    "usno - a2.0 presents right ascension and declination ( j2000 , epoch of the mean of the blue and red plate ) and the blue and red magnitude for each star .",
    "the infrared data is the first large incremental data release from the two micron all sky survey ( 2mass ) .",
    "this release covers 2,483 square degree of northern sky observed from the 2mass facility at mt .",
    "hopkins , az . the catalogue contains 20.2 million point and 73,980 extended sources , and includes three bands j ( 1.25 @xmath66 m ) , h ( 1.65 @xmath66 m ) , and k@xmath67 ( 2.17 @xmath66 m ) magnitudes .    for supervised methods , the input sample must be tagged with known classes .",
    "so the catalogues of known classes of astronomical objects need to be adopted .",
    "we choose known agns from the catalog of agn ( vron - cetty & vron , 2000 ) , which contains 13214 quasars , 462 bl lac objects and 4428 active galaxies ( of which 1711 are seyfert 1 ) .",
    "stars include all spectral classes of stars , dwarfs and variable stars , which are adopted from simbad database .",
    "normal galaxies are from third reference catalogue of bright galaxies ( rc3 ; de vaucouleurs et al . 1991 ) .    studying the clustering properties of astronomical objects in a multidimensional parameter space",
    "needs catalogue cross - correlation to get multi - wavelength parameters available for all sources .",
    "firstly , within a search radius of 3 times the rbsc and rfsc positional error , we positionally cross - identified the catalogue of usno - a2.0 with the rbsc and rfsc x - ray sources , and then cross - matched the data from x - ray and optical bands with infared sources in 2mass first released database within 10 arcsec radius . secondly , we similarly cross - identified the data from three bands with the catalogues of agns , stars and normal galaxies within 5 arcsec radius . only considering the unique entries",
    ", the total sample contains 1656 ( 29.9% ) agns , 3718 ( 67.0% ) stars , 173 ( 3.1% ) normal galaxies .    in the whole process ,",
    "the obtained data of agns , stars and galaxies with catalogue counterparts are divided into four subclasses , ( i ) unique entries , ( ii ) multiple entries , ( iii ) the same entries , ( iv ) no entries . in detail , unique entries refer to the objects which have only one catalogue entry in the various catalogues , or which have a unique identification in private catalogues .",
    "multiple entries refer to the objects that have more than one catalogue entries in various catalogues .",
    "the same entries point to the two or three kinds of objects which have the same catalogue counterparts .",
    "no entries show that the objects may not be matched from one or more catalogues , by the reason of the incompleteness of catalogues .",
    "in addition , we point out the sample here is obtained by multi - wavelength cross - identification .",
    "for positional error , some sources unavoidably match the unrelated or fake sources . in order to keep sources as true as possibly",
    ", we only consider the unique entries , cross out the multiple entries , the same entries and no entries . certainly , knowing which are true sources , we need to compute the probability to assess the validity of identifications of the counterparts from three bands , just like what mattox et al .",
    "1997 , rutledge et al .",
    "2000 do with cross - association . owing to the restrictive aim of this work",
    ", we do nt investigate this respect in detail .    in the paper ,",
    "the plausibility is based on the optical classification , x - ray characteristics like hardness ratios and extent parameter , and the infrared classification ( stocke et al .",
    "1991 ; motch et al .",
    "1998 ; pietsch et al . 1998 ; he et al .",
    "according to the results of the @xmath68 medium sensitivity survey ( emss ; stocke et al .",
    "1991 ) , x - ray - to - optical flux ratio , @xmath69 , was found to be very different for different classes of x - ray emitters .",
    "motch et al .",
    "( 1998 ) stated that , for source classification , the most interesting parameters are flux ratios in various energy bands , including the conventional x - ray hardness ratios , @xmath69 ratios as well as optical colors .",
    "they also presented that , although stars and agns have similar x - ray colors , their mean x - ray to optical ratios are obviously quite different and they are well separated in the @xmath70 vs. @xmath69 diagram .",
    "cataclysmic variables exhibit a large range of x - ray colors and @xmath69 ratios and can be somewhat confused with both agns and the most active part of the stellar population .",
    "however , the addition of a @xmath71 or @xmath72 optical index would allow to further distinguish between these overlapping population .",
    "( 2001 ) stated that galactic stars usually have bright optical magnitudes and weak x - ray emission , galaxies with fainter optical magnitudes and median x - ray emission , and agns with the faintest magnitudes and strongest x - ray emission . in their figure 1 .",
    "of @xmath69 vs. @xmath73 , agns and non - agns occupy different zones .",
    "pietsch et al . ( 1998 ) also used a conservative extent criterion ( @xmath74 and @xmath75 ) as an indicator that the x - ray emission does not originate from a nuclear source .",
    "since the corresponding parameter spaces overlap significantly for different classes of objects , an unambiguous identification based on one band data alone is not possible . in order to classify sources",
    ", we consider the data from optical , x - ray and infrared bands .",
    "the chosen parameters from different bands are @xmath76 ( optical index ) , @xmath77 ( optical - x - ray index ) , @xmath55 , @xmath56 ( x - ray index ) , @xmath57 ( x - ray index ) , @xmath58 , @xmath59 , @xmath78 ( infrared index ) , @xmath79 ( infrared index ) , @xmath80 ( infrared - x - ray index ) .",
    "motch et al .",
    "( 1998 ) showed that the x - ray to optical flux ratio can be approximate to @xmath81 , assuming an average energy conversion factor of 1 pspc cts s@xmath54 for a 10@xmath82 erg @xmath83 s@xmath54 flux in the range of 0.1 to 2.4 kev .",
    "so @xmath77 can be viewed as an x - ray - to - optical flux ratio , similarly , @xmath80 is an x - ray - to - infrared flux ratio .",
    "the mean values of parameters for the sample are given in table 1 .",
    "table 1 indicates that some mean values of parameters have rather big scatter .",
    "the @xmath76 value of normal galaxies is obviously larger than those of agns and stars ; the @xmath55 value of agns is higher than those of stars and normal galaxies . for the mean values of @xmath57 , which subdivides the hard range ,",
    "there are only marginal differences between the individual classes of objects .",
    "this applies to the total sample .",
    "there is a trend that galaxies seem to have somewhat higher @xmath84hr2@xmath85 values than agns and stars .",
    "agns and stars have on the average the lower @xmath56 , i.e. , they have the softer spectral energy distribution ( sed ) . a significantly harder sed is found for normal galaxies with @xmath84hr1@xmath86 .",
    "this is indeed what is expected for this class of objects which exhibit a rather hard intrinsic spectrum caused by thermal bremsstrahlung from a hot ( @xmath87k ) plasma(cf .",
    "e.g. b@xmath1hringer 1996 ) .",
    "the mean values of @xmath58 and @xmath59 of normal galaxies is apparently larger than agns and stars .",
    "furthermore , those of agns are larger than stars .",
    "as table 1 shows , galaxies are not only 0.76 mag in @xmath78 , but they also have @xmath88 values , 0.37 mag , redder than stars .",
    "likewise , agns are redder than stars , too .",
    "we also find that the mean @xmath84b+2.5log(cr)@xmath85 and @xmath84j+2.5log(cr)@xmath85 values of agns are much higher than those of stars and galaxies .",
    "this can be explained by the fact that agns are strong x - ray emitters .    [",
    "cols=\">,<,<,<,<,<,<\",options=\"header \" , ]      table 2 shows that the efficiency of classification is rather high , more than 90% when only considering the important features .",
    "apparently , it is simple and applicable to choose a few good features for classification . but compared to the results by the automated algorithms , such a method is a little inefficient .",
    "after all , the method is limited by itself for it ca nt avoid losing information only with a few features .",
    "what s more , sometimes it is very difficult to find such good features . only depending on other tools , such as principal component analysis ( folkes et al .",
    "1996 , zhang et al .",
    "2003 ) , we can find the principal features . if the number of principal components is more than 3 , it is not appliable to use simple cutoff for the difficulty of visualization . as a result , it is better to apply automatic approaches under such situations .    for lvq and slp ,",
    "as shown by tables 3 and 5 , the results are rarely affected by the number of space dimension when the space owns the important features .",
    "but for svm , in contrast , the result of table 4 is closely connected with the number of space dimension even including the important features .",
    "moreover , the more parameters considered , the higher the accuracy is . for low dimensional spaces , lvq and slp are better . while for high dimensional spaces , svm shows its superiority . moreover",
    ", the statistics listed in tables 3 - 5 give a view of how well the algorithms did in classifying agn and non - agn objects .",
    "these statistics tell us how effective a given method is at correctly identifying a true agn as an agn or a true non - agn as a non - agn .",
    "in other words ,",
    "how often does the method misidentify objects ? if the number of agn objects identified as non - agns were zero , the classified accuracy of agns is 100% .",
    "conversely , if the number of non - agns identified as agns were zero , the classified accuracy of stars and normal galaxies is 100% .",
    "the generally lower values of the classified accuracy of agns compared to those of stars and normal galaxies may be a result of the smaller sample size for agns ( 1656 vs. 3891 ) .",
    "this suggests that it would be useful to run these tests again with a larger sample base for the methods examined here .",
    "given our results for the methods presented here , we are encouraged that distinguishing between a number different types of objects should be possible .",
    "for such a project , a larger number of samples of each type of object would be necessary to have an adequate ability to distinguish between the classes .",
    "comparing the computed results , we conclude that lvq , svm and slp are effective methods to classify sources with multi - wavelength data . with the data from three bands",
    ", we can classify agns from stars and normal galaxies effectively by lvq , slp or svm .",
    "this also indicates that the chosen parameters are such good feature vectors to separate agns from stars and normal galaxies .",
    "we believe the performance will increase if the data are complete or the quality and quantity of data improves .",
    "moreover , these methods can be used to preselect agns from large numbers of sources in large surveys avoiding wasting time and energy , when studying agns or cosmology .",
    "the three supervised learning methods we investigated here gave comparable results in a number of situations .",
    "generally , the more features considered , the better results svm gave ; however , the results of lvq and slp were considerable with different number of attributes . also ,",
    "the different methods , while giving different quality results in a number of cases , were comparable for most of the samples we examined .",
    "however , our results suggest that the parameters we choose did not adequately pick out characteristics of the objects in all cases .",
    "other parameters added from more bands that effectively summarize the features of sources , such as from radio band , appear to do better ( krautter et al .",
    "thus we can improve the classified accuracy of agns or stars and normal galaxies , even classify different types of agns .",
    "moreover , these methods can be used for other types of data , such as spectral data and photometric data .",
    "we believe that it would be beneficial to have more extensive comparisons between different methods . only then can we take some of the magic out of determining what parameters to choose and know which method to use better in different cases .",
    "the performances of lvq and slp are different from that of svm , which arises from different methods based on different theories .",
    "svm embodies the structural risk minimization ( srm ) principle , which is superior to empirical risk minimization ( erm ) principle that conventional neural networks employ .",
    "most neural networks including lvq and slp are designed to find a separating hyperplane .",
    "this is not necessarily optimal .",
    "in fact many neural networks start with a random line and move it , until all training points are on the right side of the line .",
    "this inevitably leaves training points very close to the line in a non - optimal way .",
    "however , in svm , a large margin classifier , i.e. a line approaching the optimal is sought . as a result",
    ", svm shows better performance than lvq and slp in the high dimensional space .",
    "sources classification depends on the quality and amount of real - time data and on the algorithm used to extract generalized mappings .",
    "availability of the high - resolution multi - wavelength data constantly increases .",
    "the best possible use of this observational information requires efficient processing and generalization of high - dimensional input data",
    ". moreover , good feature selection techniques , as well as good data mining methods , are in great demand .",
    "a very promising algorithm that combines the power of the best nonlinear techniques and tolerance to very high - dimensional data is support vector machines ( svm ) . in this work we have used histogram as the feature selection technique and applied lvq , slp and svm to multi - wavelength astronomy to classify agns from stars and normal galaxies .",
    "we conclude that the features selected by histogram are applicable and the performance of svm models can be comparable to or be superior to that of the nn - based models in the high dimensional space .",
    "the advantages of the svm - based techniques are expected to be much more pronounced in future large multi - wavelength survey , which will incorporate many types of high - dimensional , multi - wavelength input data once real - time availability of this information becomes technologically feasible .",
    "all these methods can be used for astronomical object classification , data mining and preselecting agn candidates for large survey , such as the large sky area multi - object fiber spectroscopic telescope ( lamost ) .",
    "various data , incuding morphology , photometry , spectral data and so on , can be applied to train the methods and obtain classifiers to classify astronomical objects or preselect intresting objects . when lacking training sets , we may explore some unsupervised methods or outlier finding algorithms to find unusual , rare , or even new types of objects and phenomena .",
    "in addition , with the development of the virtual observatory , these methods will be part of the toolkits of the international virtual observatory .",
    "we are very grateful to anonymous referee for his important comments and suggestions .",
    "we would like to thank lamost staff for sincere help .",
    "this research has made use of the simbad database , operated at cds , strasbourg , france .",
    "simultaneously , this paper has also made use of data products from the two micron all sky survey , which is a joint project of the university of massachusetts and the infrared processing and analysis center / california institute of technology , funded by the national aeronautics and space administration and the national science foundation .",
    "this research is supported by national natural science foundation of china under grant no.10273011 .",
    "stitson , m.o . ,",
    "weston , j.a.e . ,",
    "gammerman , a. , et al .",
    "1996 , theory of support vector machines , technical report csd - tr-96 - 17 , department of computer science , royal holloway college , university of london"
  ],
  "abstract_text": [
    "<S> data mining is an important and challenging problem for the efficient analysis of large astronomical databases and will become even more important with the development of the global virtual observatory . in this study , learning vector quantization ( lvq ) , single - layer perceptron ( slp ) and support vector machines ( svm ) were put forward for multi - wavelength data classification . </S>",
    "<S> a feature selection technique was used to evaluate the significance of the considered features to the results of classification . from the results , </S>",
    "<S> we conclude that in the situation of less features , lvq and slp show better performance . </S>",
    "<S> in contrast , svm shows better performance when considering more features . </S>",
    "<S> the focus of the automatic classification is on the development of efficient feature - based classifier . </S>",
    "<S> the classifiers trained by these methods can be used for preselecting agn candidates . </S>"
  ]
}