{
  "article_text": [
    "it is more than sixty years since claude shannon proposed his formulation of _ information theory _ @xcite . during intervening years",
    ", the information theory has found relevance in disciplines as diverse as communication theory , theory of computation , physics , neural information processing systems , statistical inference and learning , and control theory , to name a few .",
    "although the origins of shannon information lie in the search for solutions to specific compression and communication problems , shannon s information measure found itself being used in attempts to solve almost all compression and communication problems",
    ". applications of the information theory in complex communication scenarios in diverse disciplines lead to only a few instances of successful applications @xcite .    for the information theory to fulfill its promise as a systems notion",
    ", it must be able to produce successful results in diverse communication systems composed of heterogeneous communicating agents . taking a cue from shannon s solution to his original problem to understand conditions under which error free transmission of messages",
    "can take place between an information source and sink , we attempt to pose problems in communication scenarios as attempts to reach a set of design objectives the system must satisfy . in doing so , we come across useful recurring quantities that look like information measures , as in shannon s source and channel coding theorems .    posing the problem of information transfer in a communication system as a question of whether a set of design specifications can be met also allows us to characterize arbitrarily complex communication scenarios in terms of very general optimization problems that include various system - level constraints , such as energy , computation and communication resources and delay - tolerance ; and system characteristics , such as interaction among constituent agents and lack of global - knowledge of agents .",
    "_ real _ systems often operate under such constraints and possess such characteristics .",
    "however , current information - theoretic approaches and methods often ignore such system constraints and characteristics while attempting to address more realistic models of real - world systems .",
    "this disregard for such system - level details while still attempting to use the classical ( shannon ) information - theoretic results in various communication scenarios is , according to us , the primary reason for the apparent failure of the information theory in making meaningful contributions to various disciplines .",
    "we attempt to find systematic and principled generalizations of the information theory which take into account the various resource constraints and general characteristics mentioned above .",
    "we argue that such generalizations are essential to analyze realistic models of real world compression and communication systems of diverse kinds and may result in new definitions of information , novel measures to quantify information transfer , and new variants of classical information - theoretic problems to model wider class of real systems .",
    "recently , the need for such generalizations has been realized , resulting in some new approaches in this direction , @xcite .",
    "we concern ourselves with one such generalization of the classical information - theoretic problem of _ distributed source coding ( dsc)_. we first propose a new canonical scheme to classify numerous variants of the classical dsc problem",
    ". then we consider one such variant and introduce a new information measure to aid in the analysis of this variant .",
    "shannon s source coding theorem states that if an information source observes a random variable @xmath8 , then it requires to send at least @xmath9 bits , on average , so that the information sink can losslessly recover @xmath10 , @xcite .",
    "generalizing it for a pair random variables @xmath11 , it states that if an information source observes a pair of random variables @xmath12 and jointly encodes those , then @xmath13 bits from the information source , on average , are sufficient for the information sink to losslessly recover @xmath11 , as in figure  [ fig : intro_dsc ] .",
    "however , the authors in @xcite proved a surprising result that states if two correlated random variables @xmath12 are observed by two non - cooperating information sources , and @xmath10 and @xmath14 are independently encoded , then as long as the sources send a total of @xmath13 bits , on average , it is still sufficient for the sink to losslessly recover @xmath11 , with individual information rates for @xmath10 and @xmath14 being at least @xmath15 and @xmath16 , respectively , as in figure  [ fig : intro_dsc ] .    fundamentally , the information transfer in _ any _ communication system where a sink node is interested in collecting information from a set of correlated information sources , which do not communicate among themselves , can be modeled as distributed source coding problem .",
    "communication system in this context can be a model of communication in neural information processing system , learning and estimation system , or wireless communication system .",
    "however , the set of constraints and characteristics of the particular communication system and objectives of communication in the system often determine the corresponding variant of distributed source coding problem that is most appropriate to model the system in question .",
    "the resultant variants differ from each other not only in terms of the problem definition , but also in terms of the computation and communication complexities of their optimal solutions .    since the publication of the seminal paper of slepian and wolf , various attempts have been made towards solving distributed source coding problem , such as @xcite and the references therein . however , there is no single definition of dsc problem . in the absence of any unified framework to systematically generate and address the different variants of distributed source coding problem , it is often difficult to compare and reconcile the approaches and solutions of different variants .",
    "we propose a canonical framework to construct and address such different variants .",
    "the proposed framework classifies each dsc problem variant according to subset of assumptions and objectives used to define the problem variant .",
    "the sets of particular assumptions and objectives we consider are as follows .",
    "* assumptions : *    * symmetric or asymmetric communication corresponding to presence or absence of global knowledge at the information sources , respectively . * interactive ( with limited or unlimited number of messages ) or non - interactive communication between the sink and the sources .",
    "* serial or parallel communication from the sources to the sink . * block - encoding of data - samples at the sources .",
    "* objectives : *    * lossy or lossless data - gathering at the sink . *",
    "worst - case or average - case performance analysis .",
    "* function of number of bits communicated by a subset of communicating agents ( the sink and/or the sources ) that is to be minimized .",
    "examples of such functions frequently used in defining dsc problem variants are _ sum _ and _ max_. * the subset of communicating agents over which such minimizations is carried out .",
    "this is explained pictorially in figure  [ fig : dsc_variants ] .",
    "for example , the variant of dsc problem considered by slepian and wolf in @xcite is defined by the assumptions of symmetric communication and block - coding with asymptotically large block - lengths and has the objective of lossless data - gathering at sink , average - case performance analysis , minimizing the sum of source bits .",
    "the variant of dsc problem considered in this paper is constructed by selecting the dashed boxes in each block .",
    "it should be noted that we _ do not _ suggest that ours is the only way to classify the variants of distributed source coding problem , as one can come - up with alternative classification schemes with fewer or more parameters , such as the canonical classification scheme proposed in @xcite that is contained in our scheme .",
    "however , we _ do _ suggest that after almost forty years since seminal slepian - wolf paper , it is the time when we should systematically address dsc problem by following a unified scheme to classify its variants and construct practical solutions achieving optimal performance . to the best of our knowledge ,",
    "ours is the first such scheme .",
    "our motivation to consider distributed source coding problem , in particular a variant of it , comes from its strong connection to the problem of maximizing the worst - case operational lifetime of data - gathering wireless sensor networks @xcite , where the base - station collects correlated sensor data .",
    "sensor nodes constituting such networks are assumed to have limited battery resources that can not be replenished either due to sensor nodes being deployed in inaccessible locations or due to high cost of retrieving sensor nodes and changing their batteries .",
    "this makes it impossible to replace the dead nodes .",
    "however such networks , once deployed , are expected to have large operational lifetimes . in such a scenario ,",
    "a key challenge is to develop system - level strategies to efficiently utilize finite energy resources to prolong network lifetime .",
    "sensor nodes expend energy in sensing / actuating , computation , and communication .",
    "however , the harsh nature of wireless links determines the energy cost of communication that has the potential to be a major bottleneck .",
    "one of the significant factors that determines the communication energy expenditure at nodes is the number of bits exchanged in the network for successful data - gathering .",
    "therefore any scheme that reduces the number of such bits can make significant contribution to enhance the network lifetime .    in a typical data - gathering sensor network , there are two fundamental asymmetries : _ resource asymmetry _ and _ information asymmetry_. in such networks , it is reasonable to assume that the base - station has large energy , computation , and communication resources , whereas sensor nodes are resource limited ( _ resource asymmetry _ ) .",
    "further , in sensor networks , the sensor nodes hold the actual sampled information and the base - station may only know the general characteristics such as joint probability distribution of sensor data ( _ information asymmetry _ ) .",
    "we propose that the resource asymmetry in wireless sensor networks should be exploited to reduce the information asymmetry in such networks .",
    "therefore we argue that more resourceful and knowledgeable base - station that wants to gather sensor - data , should bear most of the burden of computation and communication in the network .",
    "allowing interactive communication between the base - station and sensor nodes enables us to do this : base - station forms and communicates _ efficient _ queries to the sensor nodes , which they respond to with short and easily computable messages .",
    "given the correlated nature of sensor data , the data - gathering problem in wireless sensor networks can be modeled in terms of well - known _ distributed source coding ( dsc ) _",
    "problem @xcite or its variants .",
    "in the recent past , there have been several such attempts , such as @xcite and various other schemes surveyed in @xcite .",
    "however , most of these schemes are not _ really _ effective in the data - gathering wireless sensor network due to various reasons .",
    "first , in some scenarios we are interested in minimizing the worst - case number of sensor bits ( or equivalently in maximizing the worst - case network lifetime ) .",
    "however , the schemes based on average - case information - theoretic analysis can not be used in such scenarios .",
    "second , such schemes can not address various operational constraints and requirements of sensor network , such as non - availability of global knowledge of the system , such as joint distribution of sensor data , at the sensor nodes and low - latency operation .",
    "third , these schemes do not attempt to exploit various opportunities , such as resource asymmetry , in such networks to reduce the computation and communication burden at nodes . therefore to address various aforementioned shortcomings of existing distributed source coding schemes in the context of data - gathering wireless sensor networks , we introduce a new variant of the classical distributed source coding problem as follows . the proposed variant attempts to optimally utilize the resource asymmetry in a data - gathering wireless sensor network to minimize the information asymmetry in the network .",
    "consider a distributed information - gathering scenario , where a sink collects the information from @xmath0 correlated information sources .",
    "the correlation in the sources data is modeled by joint distribution @xmath6 , which is known only to the sink .",
    "the sink and sources can interactively communicate with each other with communication proceeding in rounds .",
    "we are primarily concerned with minimizing the number of bits that the sources send , in the worst - case , for successful data - gathering at the sink , but we are also interested in minimizing both the number of communication rounds and the number of sink bits .",
    "our work mainly differs from the previous work on distributed source coding and its applications to sensor networks as follows .",
    "firstly , we assume asymmetric communication where only the sink knows the correlation structure of sources data .",
    "this is in contrast to existing dsc schemes that assume that all nodes know the correlation structure .",
    "secondly , unlike existing dsc schemes that perform average - case information - theoretic analysis , we are concerned with the worst - case performance analysis of distributed source coding . as the average - case information measure of entropy or its variants can not be used for the worst - case information - theoretic analysis , we introduce _ information ambiguity _ - a new information measure for worst - case information - theoretic analysis .",
    "thirdly , we are interested in distributed compression when only a single instance of data is available at every information source ( _ oneshot compression _ ) unlike majority of current dsc schemes that derive their results in the regime of infinite block - lengths .",
    "finally , we consider a more powerful model of communication where the sink and sources interactively communicate with each other .",
    "_ note on the terminology : _ we consider communication system consisting of communicating agents of two types : information sources and information sink .",
    "we address information sources also as informants and source nodes .",
    "similarly , we also address the information sink interchangeably as the receiver and the recipient .",
    "the paper is organized as follows . in section  [ sec : relatedwork ] , we survey the related work . section  [ sec : iambiguity ] introduces the notion of _ information ambiguity _ for the worst - case information - theoretic analyses , discusses some of its properties , and proves that it is a valid information measure . in section  [ sec : probsetting ] , we provide precise description of the communication model we assume and formally introduce the distributed data - gathering problem we address in this paper .",
    "then , section  [ sec : bcompressibility4cmprsn ] provides the solutions of this problem under two different communication scenarios .",
    "we first present an interactive communication protocol to optimally minimize the number of informant bits required in the worst - case to solve the problem .",
    "later , we provide an optimal interactive parallel communication protocol that efficiently trades - off the number of informant bits to reduce the number of communication rounds and the number of sink bits .",
    "section  [ sec : blkcoding4cmprsn ] investigates the role of block - coding in the worst - case analysis of distributed source coding problem and proves that unlike the average - case performance , worst - case performance of dsc problems derive _ almost _ no advantage from the block - coding compared to _",
    "oneshot compression_. finally , we conclude and discuss some future work in section  [ sec : conclusions ] .",
    "the slepian - wolf solution @xcite of distributed source coding problem , though fundamental , is essentially existential and non - constructive , like many other results of the classical information theory .",
    "though it establishes the lower bounds on the information rates , it does not provide us the optimal source codes or any computationally efficient method of constructing those .",
    "therefore , in the recent past , numerous attempts have been made to provide practical solutions for it . in @xcite ,",
    "the authors came up with the discus framework to give practical , though not necessarily optimal , method to construct source codes . though the result for the duality between slepian - wolf encoding and multiple - access channel was already well - known @xcite , the connection that this piece of work made between distributed source coding and channel coding , motivated the researchers to use various channel codes , such as turbo codes @xcite , ldpc codes @xcite , and convolution codes @xcite , to solve the distributed source coding problem . in @xcite and related papers , zhao and effros",
    "have addressed the lossless and near - lossless source code design and construction problem . also , given the asymmetry in the available energy and computational resources between the base - station and the sensor nodes , @xcite argues to use such asymmetric channel codes to reduce the energy consumption at the sensor nodes . a survey in @xcite and the references therein provide more details about some of these research efforts",
    ".    these developments , though pragmatic and constructive , are not very practical in the context of sensor networks , particularly due to their assumption of symmetric communication scenarios , where all nodes in the network are assumed to know the joint probability distribution of sensor - data , and requirement of large coding dimensions , where a large number of independent and identically distributed ( i.i.d ) samples are drawn and each informant encodes the sequence of these samples as a single codeword to achieve the optimal performance . given the limited communication and computation capabilities of the sensor nodes , it is neither reasonable to assume that the sensor nodes know the joint distribution of all sensors data , nor to assume that sensor nodes can carry out high - complexity encoding .",
    "also , the block - encoding with very large block - lengths ( typically , @xmath17 data samples ) required by these schemes may incur large data - gathering delays , rendering these solutions inefficient , given the time - criticality of sensor - data .",
    "the notion of interactive communication in addressing distributed source coding problem was introduced in @xcite . later in @xcite",
    ", authors attempted to deploy sink based feedback to construct practical schemes to address data - gathering problem in the data - gathering wireless sensor networks .",
    "however , with just a single feedback message , this work could not make use of the full potential of interactive communication in realizing optimum dsc performance in the sensor networks .",
    "adler in @xcite is concerned with analysing the performance of distributed source coding problem in a scenario where there is only a single instance of informant - data and the sink and informants communicate interactively , however like all previous work on distributed source coding , this work also performs only average - case analysis .",
    "the notion of information ambiguity that we propose as the worst - case equivalent of the notion of information entropy , was introduced by orlitsky in @xcite , but in a different context than ours .",
    "also , the researchers in the field of `` possibility theory '' have endeavored to define some information measures , which are closely related to the notion of information ambiguity .",
    "however , it is beyond the scope of this paper to discuss those efforts and an interested reader can find the broad survey of such work in @xcite .",
    "the original shannon s theorems and all subsequent theorems in information theory are all asymptotic results based on the large deviations theory .",
    "it implies the need to have very large set of data samples and leads to what we call average - case results .",
    "worst - case analysis deals with sparse data gathering situations and is the sole focus of this work . leaving the precise implementation motivations for its definition for the later sections , here we define a new information measure which we call _ information ambiguity _ , show that it is a valid information measure , and characterize some of its properties useful for our later results .",
    "we begin by introducing the notion of information ambiguity for two random variables and then provide its exposition for arbitrary number of variables .",
    "note that throughout the paper all the logarithms are to base two .",
    "consider a pair of random variables @xmath18 and @xmath19 , where @xmath20 is discrete and finite alphabet of size and @xmath21 , where @xmath22 and @xmath22 are discrete alphabet sets , with possibly different cardinalities .",
    "however , to keep the discussion simple , we assume henceforth that all the random variables take the values from the same discrete alphabet @xmath20 . ]",
    "@xmath23 and @xmath6 is the joint probability distribution of @xmath24 .",
    "the _ support set _ of @xmath24 is defined as : @xmath25    we also call @xmath26 as the _ ambiguity set _ of @xmath24 .",
    "the cardinality of @xmath26 is called _ joint ambiguity _ or simply _ ambiguity _ of @xmath24 and denoted as @xmath27 .",
    "the minimum number of bits required to describe all elements in @xmath26 is @xmath28 .",
    "the _ support set _ of @xmath29 , is set @xmath30 of all possible @xmath29 values .",
    "we also call @xmath31 _ ambiguity set _ of @xmath29 .",
    "the _ ambiguity _ of @xmath29 is defined as @xmath32 .",
    "the ambiguity set and the corresponding ambiguity of random variable @xmath33 is similarly defined .",
    "the _ conditional ambiguity set _ of @xmath29 , when random variable @xmath33 takes the value @xmath34 is @xmath35 the set of possible @xmath29 values when @xmath36 .",
    "the _ conditional ambiguity _ in that case is @xmath37 the number of possible @xmath29 values when @xmath38 .",
    "the _ maximum conditional ambiguity _ of @xmath29 is @xmath39 the maximum number of @xmath29 values possible with any value that @xmath33 can take .",
    "we denote the corresponding _ maximum conditional ambiguity set _ as @xmath40 .",
    "the quantities @xmath41 , and @xmath42 are similarly defined by exchanging the roles of @xmath29 and @xmath33 in the preceding discussion .",
    "define a functional called _ information ambiguity _ as @xmath43 for a set of two random variables @xmath29 and @xmath33 .",
    "next , we prove certain properties of functional @xmath44 .",
    "[ lemma : conditionalambiguity ] @xmath45 for all @xmath34 , that is , conditioning reduces information ambiguity .    from the definitions of @xmath31 and @xmath46 , it is obvious that @xmath47",
    "this implies that @xmath48 .",
    "also , it follows from lemma  [ lemma : conditionalambiguity ] and that @xmath49 .",
    "[ lemma:2subadditivity ] * ( subadditivity ) * if @xmath29 and @xmath33 are `` interacting '' , that is @xmath50 , then @xmath51 .    we know",
    "that @xmath50 .",
    "so , @xmath52 thus , proving the lemma .",
    "[ lemma:2additivity ] * ( additivity ) * if @xmath29 and @xmath33 are `` non - interacting '' , that is @xmath53 then @xmath54 , where @xmath55 denotes equality within one bit per random variable .    we know",
    "that @xmath53 .",
    "so , @xmath56 thus proving the lemma .",
    "[ lemma:2chainrule ] @xmath57    we know that @xmath58 .",
    "so , @xmath59 this proves the lemma .",
    "[ corrl:2chainrule ] @xmath60    reversing the roles of @xmath29 and @xmath33 in the proof of lemma  [ lemma:2chainrule ] , completes the proof .",
    "[ lemma:2ambiguitybound ] let @xmath61 denote the set of two possible permutations of @xmath62 , then @xmath63    the proof follows from combining lemma  [ lemma:2chainrule ] and corollary  [ corrl:2chainrule ] .",
    "we illustrate some of the definitions and properties of the notion of information ambiguity we have discussed in this section , using the probability distribution @xmath6 for two random variables @xmath64 , given in figure  [ fig : supportset ] .     for @xmath24.,width=336 ]    using , support set @xmath26 is : @xmath65 with corresponding ambiguity @xmath66 . therefore , the number of bits required to describe the elements of @xmath26 are @xmath67 bits .",
    "further , using the support sets of @xmath29 and @xmath33 are , respectively : @xmath68 the minimum number of bits required to describe the elements of @xmath31 and @xmath69 are @xmath70 bits and @xmath71 bits , respectively .",
    "computing the conditional ambiguity sets and corresponding conditional ambiguities for @xmath29 , we have using : @xmath72 therefore , the maximum conditional ambiguity in @xmath29 given @xmath33 is @xmath73 .",
    "similarly , for @xmath33 , we have : @xmath74 hence , the maximum conditional ambiguity in @xmath33 given @xmath29 is @xmath75 .",
    "above , we computed @xmath76 bits .",
    "now , let us compute @xmath77 this illustrates lemma  [ lemma:2chainrule ] , corollary  [ corrl:2chainrule ] , and lemma  [ lemma:2ambiguitybound ] .      the definitions and results of subsection  [ subsec:2ambiguity ] for the information ambiguity of two random variables are easily extended to their multiple random variable counterparts for the information ambiguity of a set of @xmath0 random variables @xmath78 , @xmath79 .",
    "consider a discrete and finite probability distribution @xmath6 for @xmath0 random variables @xmath80 , where @xmath81 and @xmath82 is the discrete and finite alphabet of size @xmath83 . consider a @xmath0-tuple of random variables",
    "the _ support set _ of @xmath85 is defined as : @xmath86 we also call @xmath87 the _ ambiguity set _ of @xmath85 .",
    "the cardinality of @xmath87 is called _ ambiguity _ of @xmath85 and denoted as @xmath88 .",
    "therefore the minimum number of bits required to describe an element in @xmath87 is @xmath89 .",
    "consider sets of random variables @xmath90 and @xmath91 such that @xmath92 .",
    "denote instances of @xmath90 and @xmath91 as @xmath93 and @xmath94 , respectively .",
    "the support - set of @xmath90 is defined as : @xmath95 we also call @xmath96 as the _ ambiguity set _ of @xmath90 , with corresponding _ ambiguity _ denoted as @xmath97 and defined as @xmath98 .",
    "so , the minimum number of bits required to describe any value of @xmath90 is @xmath99 .",
    "consider random variable @xmath100 and the set of random variables @xmath101 .",
    "denote an instance of @xmath102 as @xmath103 .",
    "the _ conditional ambiguity set _ of @xmath104 , when set @xmath102 takes value @xmath105 is @xmath106 the set of possible @xmath104 values when @xmath107 .",
    "the _ conditional ambiguity _ in that case is @xmath108 the number of possible @xmath104 values with @xmath107 . the _ maximum conditional ambiguity _ of @xmath104 is @xmath109 the maximum number of @xmath104 values possible over any @xmath103 .",
    "in fact , for any two subsets @xmath90 and @xmath102 of @xmath110 , such that @xmath111 and @xmath112 , we can define for example , _ ambiguity set _",
    "@xmath96 of @xmath90 , _ conditional ambiguity set _",
    "@xmath113 of @xmath90 given the set @xmath103 of values that @xmath102 can take , and _ maximum conditional ambiguity set _",
    "@xmath114 of @xmath90 for any set of values that @xmath102 can take , with corresponding _ ambiguity _ , _ conditional ambiguity _ , and _ maximum conditional ambiguity _ denoted by @xmath97 , @xmath115 , and @xmath116 , respectively .",
    "however , for the sake of brevity , we do not introduce these definitions here as those can be easily developed along the lines of the definitions in - .",
    "further , let us represent each of @xmath117 values that random variable @xmath104 can assume in @xmath118 bits as @xmath119 .",
    "let @xmath120 represent the value of the @xmath121 bit - location in the bit - representation of @xmath3 , @xmath122 . then , knowing that the value of @xmath121 bit - location is @xmath123 , we can define the set of possible values that @xmath104 can take as @xmath124 with corresponding cardinality denoted as @xmath125 .",
    "we can similarly define @xmath126 with @xmath100 as @xmath127 with corresponding cardinality denoted as @xmath128 .",
    "the definitions in and can be easily extended further to the situations where the values of one or more bit - locations in one or more random variable s bit - representation are known .",
    "define a functional called _ information ambiguity _ as @xmath129 for a set of @xmath0 random variables @xmath130 .",
    "next , we prove certain properties of functional @xmath131 .",
    "[ lemma : nexpansibility ] * ( expansibility ) * if a component @xmath132 with @xmath133 is added to joint distribution @xmath6 , then information ambiguity @xmath134 does not change .",
    "the proof follows from the definition of @xmath135 .",
    "[ lemma : nmonotonicity ] * ( monotonicity ) * if @xmath136 and @xmath137 are two discrete and finite sets with @xmath138 , then @xmath139 .    if @xmath138 , then with @xmath140 and @xmath141 @xmath142 thus proving the lemma .    [ lemma : nsymmetry ] * ( symmetry ) * @xmath143 for permutations @xmath144 .",
    "the proof follows from the observation that any rearrangement of the elements of universal set @xmath145 does not change the cardinality of support - set @xmath87 .",
    "[ lemma : nsubadditivity ] * ( subadditivity ) * if @xmath146 , are `` interacting '' , that is @xmath147 , then @xmath148 .",
    "the proof follows from the straightforward extension of the proof of lemma  [ lemma:2subadditivity ] for multiple random variables .",
    "[ lemma : nadditivity ] * ( additivity ) * if @xmath146 , are `` non - interacting '' , that is @xmath149 , then @xmath150 .",
    "follows from extending the proof of lemma  [ lemma:2additivity ] to multiple random variables .",
    "the above lemmas establish that functional @xmath129 is a valid information measure as it satisfies various axioms of expansibility , monotonicity , symmetry , subadditivity , and additivity of valid information measures @xcite .",
    "_ remark : _ an astute reader may note that in spite of the apparent similarities between the information measures _ information ambiguity _ proposed above and well - known hartley measure , these two measures are fundamentally different . for a set of @xmath0 random variables",
    "these two information measures define their _ unconditional _ versions identically in terms of functional @xmath151 .",
    "however , these two measures differ in their definitions of the corresponding _ conditional _ versions .",
    "while conditional hartley measure characterizes average nonspecificity , conditional information ambiguity characterizes maximum nonspecificity .",
    "for example , for a set of two random variables @xmath10 and @xmath14 , in conditional hartley measure @xmath152 , ratio @xmath153 represents the average number of elements of @xmath154 possible under the condition that an element from @xmath155 has been chosen ( * ? ? ? * chapter 2 ) , while maximum conditional ambiguity @xmath156 in conditional information ambiguity @xmath157 represents the maximum number of possible elements of @xmath154 under the condition that an element from @xmath155 has been chosen .",
    "[ lemma : nambiguitybound ] let @xmath61 denote the set of all possible permutations of @xmath158 and @xmath159 , then @xmath160    combining the proofs of lemma  [ lemma:2chainrule ] and corollary  [ corrl:2chainrule ] generalized for @xmath0 random variables , proves the lemma .",
    "these generalizations themselves are easily obtainable from their two variables counterparts , as in the proof of lemma  [ lemma : nsubadditivity ] .",
    "[ lemma : ambiguityset ] @xmath161    we prove the lemma by individually proving both directions of inclusion .",
    "* @xmath162 : consider @xmath163 .",
    "we need to prove that @xmath164 . by definition , @xmath165 is one of the values that the random variable @xmath104 can take when @xmath166 .",
    "this implies that @xmath164 . *",
    "@xmath167 : consider @xmath168 .",
    "this implies that @xmath169 .",
    "now , let us suppose that @xmath170 .",
    "however , this leads to a contradiction as @xmath171 is defined to be the set of all those values that @xmath104 can take , when @xmath166 .    combining these two proofs",
    "proves the lemma .",
    "[ lemma : ambiguity ] @xmath172    first consider the intersection of finite number of finite sets @xmath173 , where @xmath174 is some index set .",
    "@xmath175 where ( a ) follow from the definition @xmath176 , ( b ) follows from the lemma  [ lemma : ambiguityset ] , and ( c ) follows from @xmath177 .",
    "this proves the lemma .",
    "[ lemma : maxambiguity ] @xmath178    from the definition of @xmath179 , let @xmath180 be an instance of @xmath90 that maximizes @xmath176 . similarly , by the definition of @xmath181 , let @xmath182 be an instance of @xmath183 that maximizes @xmath184 . therefore , @xmath185",
    "where ( a ) follows from lemma  [ lemma : ambiguity ] , thus completing the proof .",
    "this section provides the notation used frequently in rest of the paper .    1 .",
    "the set of @xmath0 informants .",
    "2 .   finite , discrete alphabet set of size @xmath83 .",
    "3 .   @xmath0-dimensional discrete probability distribution , @xmath186 @xmath187 @xmath188 .",
    "4 .   random variable observed by the informant @xmath189 . @xmath190 .",
    "5 .   the _ ambiguity set _ of the @xmath191 informant s data , with corresponding _ ambiguity _ @xmath192 .",
    "the _ conditional ambiguity set _ of the sink in the @xmath4 informant s data when the sink has information @xmath174 , which can be the set of values of one or more bit - locations in the representation of one or more informants data .",
    "however , the exact nature of @xmath174 will be obvious from the context . 7 .   the _ conditional ambiguity _ , @xmath193",
    "the _ maximum conditional ambiguity _",
    ", computed over all instances of @xmath174 .",
    "9 .   the _ ambiguity set _ at the sink of all informants data , with @xmath88 as the corresponding _",
    "ambiguity_. 10 .",
    "the _ conditional ambiguity set _ at the sink of all informants data , with @xmath194 as the corresponding _",
    "conditional ambiguity_. 11 .",
    "the @xmath195-extension of ambiguity set @xmath196 , with corresponding _ ambiguity _ @xmath197 . 12 .",
    "the _ conditional _",
    "@xmath198-extension of ambiguity set @xmath199 when the sink has information @xmath174 , with corresponding _ conditional ambiguity _ @xmath200 . 13 .",
    "the @xmath195-extension of ambiguity set @xmath87 , with @xmath201 as the corresponding _",
    "ambiguity_. 14 .",
    "the _ conditional _",
    "@xmath198-extension of ambiguity set @xmath87 when the sink has information @xmath174 , with corresponding _ conditional ambiguity _",
    "the worst - case _ bit - compressibility _ of distributed compression problem with single instance of source data - vector . 16 .",
    "the worst - case _ bit - compressibility _ of distributed compression problem with @xmath203 , instances of source data - vectors .",
    "consider a distributed information - gathering scenario , where a sink collects the data from @xmath0 informants sampling correlated data .",
    "divide the sequence of events in this data - gathering problem in terms of _ data - generation epoch _ and _ data - gathering epoch_. in the data - generation epoch , a sample @xmath204 , is drawn from the discrete and finite support - set @xmath87 over @xmath0 binary strings , as in @xcite .",
    "the strings of @xmath7 are revealed to the informants , with the string @xmath3 being given to the @xmath191 informant , @xmath205 .",
    "then in the data - gathering epoch , the sink wants to _ losslessly _ ( error probability @xmath206 ) learn @xmath7 revealed to the informants . each data - generation epoch",
    "is followed by a data - gathering epoch and vice - versa .",
    "* problem statement : * a sample @xmath207 is drawn _ i.i.d . _ from the distribution @xmath6 over @xmath0 binary strings . the strings of @xmath7 are revealed to the informants , with the string @xmath3 being given to the @xmath191 informant .",
    "the sink wants to learn each informant s string _ losslessly _",
    "( @xmath206 ) .",
    "an informant may not learn about other informants or the sink s data .",
    "our primary objective is to minimize the total number of informant bits required , in the worst - case , to accomplish this , but we are also concerned with minimizing both , the number of rounds and the number of sink bits .",
    "this is illustrated in figure  [ fig : twoinformants ] for the scenarios with two informants and one sink .",
    "* the problem setting : * we consider an _ asymmetric communication _ scenario , but can estimate it as it collects the data from the informants , drawn from @xmath6 .",
    "for example , in @xcite , a linear predictive model is used to estimate the correlation structure .",
    "it should be noted that we assume nothing about this distribution , except that it is a discrete distribution with finite alphabet . ]",
    "communication takes place over @xmath0 binary , error - free channels , where each channel connects an informant with the sink . an informant and the sink",
    "can interactively communicate over the channel between them by exchanging messages ( finite sequences of bits determined by an agreed upon , deterministic protocol . )",
    "the informants can not communicate directly with each other .",
    "we assume that in the data - gathering epoch , communication between the sink and the informants proceeds in rounds , as in @xcite . in each round , depending on the information held by the communicators , one or other communicator may send the first message .",
    "however , as argued in @xcite , if we allow the empty messages and eliminate the last message if it is sent by the sink , then any sequence of messages can be converted into another sequence where the same communicator transmits the first message , with no increase in the worst - case communication complexity .",
    "therefore , we assume that in each communication round , first the sink communicates to the informants and then , the informants respond with their messages .",
    "each bit communicated over any channel is counted as either a sink bit or an informant bit .",
    "we assume the informants to be memoryless in the sense that they do not remember their messages sent in the previous rounds . however , we assume that the @xmath191 informant knows its support - set @xmath199 , so that it represents the binary string @xmath3 given to it in @xmath208 bits as @xmath209 .",
    "the sink knows distribution @xmath6 and the corresponding support set @xmath87 .",
    "so , every @xmath210 , can be uniquely described at the sink using @xmath129 bits .",
    "this implies that , in the worst - case , @xmath131 informant bits are _ necessary _ for the sink to learn @xmath7 unambiguously .",
    "however , these many informant bits may not be achievable , in general , for any communication protocol as the sink needs to query the informants based on some function of independent encoding of their data - strings that the informants can construct rather than some arbitrary encoding of @xmath7 that the sink can construct .",
    "however , as long as the sink can be assumed to know joint distribution @xmath6 , there is at least one coding scheme that both , the sources and sink can construct without any explicit communication between them and still achieve optimal distributed compression performance .",
    "next , we propose one such encoding scheme that the sink can construct to query the informants and informants can use to respond to the sink s queries .",
    "this scheme allows us to not only compute minimum achievable number of informant bits required for data - gathering at the sink but also provides an efficient way to achieve those .",
    "* new problem encoding scheme : * as each informant @xmath211 , knows its support - set @xmath199 , it can describe each @xmath212 , as set @xmath213 of @xmath214 bits . therefore , every @xmath7 can also be uniquely described at the sink as set @xmath215 of @xmath216 bits , constructed by concatenating @xmath214 bits long representation of each @xmath217 .",
    "this implies that @xmath216 informant bits are always _ sufficient _ for the sink to learn @xmath7 unambiguously .",
    "the following example illustrates this encoding scheme .",
    "* example 1 : * consider an example support - set shown in figure  [ fig : probexa ]",
    ". let informants 1 and 2 observe the random variables @xmath29 and @xmath33 , respectively .",
    "for the given support - set , at least @xmath218 bits are required to describe any element of @xmath26 and it requires no less than @xmath219 bits to independently describe a value that @xmath29 or @xmath33 take .        for a given support - set ,",
    "the sink can construct a figure similar to figure  [ fig : probexa ] .",
    "one of the strings from the fourth column is drawn , with first @xmath220 bits given to informant 1 , next @xmath221 bits given to informant 2 , and so on .",
    "then the data - gathering problem is that the sink wants to learn of this string , whose different parts are held by different informants , with the informants sending minimum total number of bits to the sink .    given the above encoding scheme , the worst - case _ bit - compressibility _ @xmath222 of distributed compression problem is defined as : @xmath223 where @xmath224 denotes the conditional ambiguity - set of @xmath85 when the bits corresponding to subset @xmath225 , are known at the sink can be written as concatenation of @xmath226 . ] .",
    "in other words , there is at least one @xmath227 such that no fewer than @xmath222 informant bits are sufficient to describe it at the sink unambiguously .",
    "note that @xmath228 .",
    "* definition ( worst - case bit - compressibility ) : * the distributed compression problem is called worst - case bit - compressible if @xmath229 , otherwise bit - incompressible .    _ note on the terminology : _ we call a bit _ undefined _ if the sink does not know its value , otherwise it is called _ defined_. for example , until the sink learns of actual @xmath7 revealed to the informants , one or more bits in the @xmath230 bits long representation of @xmath7 , remain _ undefined_. similarly , informant @xmath211 , is called _ undefined _ if sink does not know corresponding @xmath231 exactly , otherwise the informant is called _",
    "in this section , we address the problem of worst - case distributed compression in two different communication scenarios . in the first scenario that we call bit serial communication scenario , in each communication round in a data - gathering epoch , only one informant can send one bit of information to the sink .",
    "this allows us to compute the minimum number of informant bits ( total and individual ) required to enable the sink to learn the particular @xmath210 , revealed to the informants in the data - generation epoch , when any number of rounds and sink bits can be used . in other words , this communication scenario allows us to compute the largest worst - case achievable rate - region for this problem , as we show later . in the second scenario that we call round parallel communication scenario , one or more informants can send one or more bits in parallel to the sink .",
    "this as we argue and show later , allows us to exploit various trade - offs among the number of informant bits , the number of sink bits , and the number of rounds .",
    "* definition ( achievable rate - region ) : * the achievable rate - region @xmath232 for the worst - case distributed source coding problem with @xmath0 informants is defined as the set of all @xmath0-tuples @xmath233 of informant rates ( in bits ) such that when @xmath234 , informant sends the subset @xmath235 , of bits in the particular rate - tuple , then the sink is able to retrieve @xmath7 unambiguously , that is : @xmath236      we discuss the optimal solution of the distributed compression problem introduced in the previous section .",
    "we first provide an interactive communication protocol , called `` * * bit - serial * * '' protocol , and then prove that it optimally solves the problem .",
    "further , we show that `` * * bit - serial * * '' protocol also allows us to compute the maximum achievable rate - region of the distributed compression problem we are concerned with .",
    "next , we describe `` * * bit - serial * * '' protocol in detail .    _ * bit - serial * protocol : _ consider an interactive communication protocol where in each round only one bit is sent by the informant chosen to communicate with the sink .",
    "the chosen bit has the property that it divides the current conditional ambiguity set at the sink maximally close to half among all candidate bits .",
    "this offers the opportunity to optimally minimize the number of informant bits , as it maximally conditions the ambiguity sets of the informants at the sink .",
    "consider the @xmath237 communication round . at the beginning of the @xmath237 round ,",
    "let @xmath145 and @xmath238 denote , respectively , the sets of _ undefined _ and _ defined _ bit - locations among @xmath239 bits long representation of @xmath7 at the sink , @xmath240 .",
    "the ambiguity at the sink in all informants data is @xmath241 .",
    "let @xmath242 respectively denote the number of @xmath243 and @xmath244 at the bit location @xmath245 , over all @xmath246 strings .",
    "then the chosen bit is the one that solves @xmath247 .",
    "the sink , after receiving the value of the chosen bit , recomputes the set of undefined bits @xmath145 .",
    "this is carried out iteratively till all bits in @xmath230 bits long representation of @xmath248 are not _",
    "defined_. this is formally summarized in `` * * bit - serial * * '' protocol given below .",
    "@xmath249 let @xmath250 let @xmath251 : index set of all bit - locations in @xmath215 let @xmath145 be the index set of undefined bits in @xmath252 , @xmath253 ( @xmath254 ) @xmath255 if @xmath256 , then choose uniformly at random the bit - location @xmath257 the sink asks the informant corresponding to bit - location @xmath258 to send bit - value @xmath259 [ bsercom : bitvalue ] set @xmath260 compute @xmath261 , the set of undefined bits @xmath262    the sink can perform the worst - case performance analysis of * bit - serial * protocol by selecting on the line  [ bsercom : bitvalue ] , @xmath263 that solves : @xmath264    the binary representations of elements of @xmath87 in terms of @xmath215 , as in figure  [ fig : probexa](d ) , can be arranged as the leaves of a binary tree . for each of @xmath239 bit - locations in the @xmath215 representation of @xmath7 ,",
    "there is a binary tree rooted at that location with all other locations forming the internal nodes of the tree . at any node in the tree",
    ", the bit - value ` @xmath265 ' leads to the left subtree and ` @xmath266 ' leads to the right subtree .",
    "such a binary tree with @xmath267 leaves will have a minimum - height of @xmath131 , implying that at least @xmath131 bits are required to describe any leaf , in the worst - case .",
    "figure  [ fig : exa1 ] provides the canonical representation of one of the possible binary trees for the distributed compression problem in figure  [ fig : probexa ] .",
    "we show that the problem of minimizing the total number of bits @xmath222 that the informants must send to the sink to help it learn any @xmath227 is equivalent to the problem of constructing minimum - height binary tree for concatenated bit - representations of the elements of @xmath87 .",
    "we prove that * bit - serial * protocol constructs such trees for a given support set and so optimally solves the worst - case asymmetric distributed compression problem .",
    "[ lemma : bsercomalltrees ] * bit - serial * protocol computes all minimum - height binary trees corresponding to the given support - set .    in the canonical representation , as in figure  [ fig : exa1 ] , of a minimum - height binary tree corresponding to the given support - set , every node corresponds to the bit - location that divides the resultant conditional ambiguity set as close to half as possible . however , * bit - serial * protocol precisely chooses the same bit - location in the round corresponding to the level of node concerned , thus proving the lemma .",
    "denote the set of all minimum - height binary trees as @xmath268 .",
    "let @xmath269 denote the number of bits that the @xmath4 informant , @xmath205 , sends in the worst - case under the @xmath121 minimum - height binary tree , @xmath270 .",
    "then , we have the following lemma .",
    "[ lemma : bsercomminbits ] * bit - serial * protocol computes @xmath271 , the minimum number of bits that the @xmath4 informant needs to send to the sink to get defined .",
    "* bit - serial * protocol exploits the bit serial communication scenario where the informant chosen to communicate in a round can send only one bit of information to maximally condition the resultant ambiguity set at the sink .",
    "also , to reduce the number of bits that an informant sends , * bit - serial * protocol can postpone retrieving the bits from the informant concerned until it can be postponed no more , thus maximally conditioning the ambiguity set at the sink of the informant concerned .",
    "these two arguments together prove the lemma .",
    "combining previous two lemmas , allows us to define @xmath271 as : @xmath272    [ lemma : rateregion ] for a given support - set , each corner point of the worst - case achievable rate - region corresponds to at least one minimum - height binary tree , with height @xmath222 .",
    "for the sake of contradiction , assume that there is a corner point of the worst - case achievable rate - region to which no minimum - height binary tree corresponds to .",
    "this means that this corner point is outside the worst - case rate - region defined by the set of all the corner points visited by the set of minimum - height binary trees , @xmath273 .",
    "this further implies that at this corner point at least one informant , say the @xmath4 , sends fewer bits than @xmath271 with @xmath271 as defined above .",
    "however , this contradicts the definition of @xmath271 that it is the minimum number of bits an informant needs to send to the sink before it is defined .",
    "thus , there can not be any corner point outside the rate - region defined by the set of corner points defined by the minimum - height binary trees in @xmath273 , hence proving the lemma .",
    "[ lemma : bsercomoptimal ] protocol * bit - serial * is worst - case optimal .    combining the statements of lemmas  [ lemma : bsercomalltrees ] and [ lemma : rateregion ] , we can state that * bit - serial * protocol computes at least one minimum - height binary tree corresponding to each corner point of the worst - case achievable rate - region . therefore ,",
    "* bit - serial * protocol computes each corner point of the achievable rate - region .",
    "thus , * bit - serial * protocol computes the worst - case achievable rate - region , hence it is worst - case optimal .    for two informants , the worst - case achievable rate - region in asymmetric distributed compression problem",
    "is given by the following corollary to lemma  [ lemma : bsercomoptimal ] .",
    "[ cor:2rateregion ] for @xmath274 , if @xmath271 denotes the minimum number of bits that an informant @xmath275 , sends over all solutions of * bit - serial * protocol and @xmath222 denotes the minimum total number of bits sent by all informants , then the achievable rate region is given by : @xmath276    follows from the worst - case optimality of * bit - serial * protocol proven above .    for @xmath0 informants ,",
    "the worst - case achievable rate - region in asymmetric distributed compression problem is given by the following corollary to lemma  [ lemma : bsercomoptimal ] .",
    "[ cor : nrateregion ] the set of achievable rate - vectors for the worst - case dsc problem for the oneshot compression is given by : @xmath277 for @xmath278 , where @xmath279 is the minimum number of bits that the subset of informants @xmath280 send over all possible solutions of and @xmath281 .",
    "the proof follows from establishing the worst - case optimality of * bit - serial * protocol in computing @xmath279 .",
    "this can be achieved by the straightforward generalization of the argument above to prove the optimality of * bit - serial * protocol to arbitrary subsets of @xmath278 .    in figures  [ fig : exa1]-[fig : exa3 ] , for three different support sets , we give one of the many possible corresponding minimum - height trees computed by * bit - serial * protocol and the corresponding worst - case achievable rate regions .",
    "_ upper bound on @xmath222 _ : in * bit - serial * protocol , as only one information bit is sent per communication round , the total number of rounds required is equal to @xmath222 .",
    "assume that in the @xmath282 , communication round , the size of the ambiguity set is reduced by @xmath283 .",
    "so , after @xmath222 rounds , we have @xmath284 .",
    "define @xmath285 .",
    "assume that the size of the ambiguity set in every round is reduced by @xmath286 .",
    "assume that the data - gathering finishes now in @xmath287 rounds .",
    "it is obvious that @xmath288 .",
    "now , the size of the ambiguity set after @xmath287 rounds satisfies @xmath289 .",
    "this implies that @xmath290 .",
    "_ upper bound on number of sink bits _ : as there are @xmath0 informants , under * bit - serial * protocol , in the @xmath191 communication round , the sink addresses the chosen informant in @xmath291 bits and then in @xmath292 bits addresses the chosen bit corresponding to this informant .",
    "so , in the @xmath191 communication round , the sink sends a total of @xmath293 bits , implying that to gather @xmath137 information bits the sink sends a total of @xmath294 bits .      in this subsection",
    ", we investigate the worst - case performance of the interactive distributed compression problem in an asymmetric communication scenarios where in each communication round one or more informants may send more than one bit to the sink _ in parallel_. more precisely , we use parallel communication to mean * round parallel communication * that we define as a communication scenario where in each communication round , two or more bits can be sent by one or more informants to the sink . therefore , in the round parallel communication two or more informants may or may not communicate in parallel in the classical sense , that is , their communications may or may not overlap in time .",
    "the round parallel communication allows us to exploit the trade - off between the number of rounds and the number of informant bits .",
    "therefore , on one extreme is * bit - serial * protocol with minimum number of informant bits and unconstrained number of rounds and on other extreme is a scheme where as many as @xmath295 informant bits are sent ( as each informant @xmath189 encodes its data - value in @xmath214 bits ) in a single round .",
    "we provide a round parallel communication protocol that , as we prove , among all round parallel communication protocols minimizes the total number of informant bits in the worst - case as well as the number of sink bits and the number of rounds .",
    "_ * round - parallel * protocol : _ consider the set of concatenated bit - strings of length @xmath230 , where each bit - string corresponds to an element of @xmath296 , as in figure  [ fig : probexa](d ) .",
    "consider the @xmath237 communication round . at the beginning of the @xmath237 round ,",
    "let @xmath145 and @xmath238 denote , respectively , the sets of _ undefined _ and _ defined _ bit - locations among @xmath239 bits , @xmath240 .",
    "the ambiguity at the sink in all informants data is @xmath241 .",
    "let @xmath242 respectively denote the number of @xmath243 and @xmath244 at the bit location @xmath245 , over all @xmath246 strings .",
    "the sink computes @xmath297 , the set of indices of those @xmath298 bit locations , which divide the successive conditional ambiguity sets as close to half as possible .",
    "set @xmath297 is defined as : @xmath299 where @xmath300 is defined as : @xmath301 and @xmath302 , is defined as follows : @xmath303 we summarize this formally as `` * * round - parallel * * '' protocol , given below .",
    "let @xmath250 let @xmath304 let @xmath145 be the set of undefined bits in @xmath252 , @xmath253 , over all @xmath305 ( @xmath254 ) @xmath306 ( @xmath307 ) @xmath308 , where @xmath309 , @xmath310 is defined as in compute @xmath261 , the set of undefined bits [ parcom : undefbits ] the sink asks the informants corresponding to the bit - locations in @xmath311 [ parcom : bitvalue ] to send the bit - values @xmath312 , @xmath307 compute @xmath313 compute @xmath261 , the set of undefined bits @xmath262    the worst - case behavior of * round - parallel * protocol can be analyzed by assuming the set of informant bits in line  [ parcom : bitvalue ] is same as the set of their worst - case values , that is , @xmath314 .    _",
    "upper bound on number of rounds _",
    ": suppose that the data - gathering finishes in @xmath287 communication rounds and in every round , the informants send @xmath134 bits .",
    "assume that in @xmath315 , communication round , the size of the ambiguity set is reduced by @xmath316 .",
    "so , after @xmath287 rounds , we have @xmath317 .",
    "define @xmath285 .",
    "assume that the size of the ambiguity set in every round is reduced by @xmath318 .",
    "assume that the data - gathering finishes now in @xmath319 rounds .",
    "it is obvious that @xmath320 .",
    "now , the size of the ambiguity set after @xmath319 rounds satisfies @xmath321 .",
    "so , @xmath322 this implies that @xmath323 ; @xmath324 ; and @xmath325 .    _ upper bound on number of informant bits _ :",
    "the total number of informant bits sent in a round is upper - bounded by @xmath134 , so the total number of informant bits sent over all rounds @xmath326 is upper - bounded by @xmath327 .    _",
    "upper bound on number of sink bits _ : the sink can address each of @xmath0 informants in @xmath291 bits .",
    "so , it addresses all informants in @xmath328 bits . in @xmath0",
    "more bits , it informs all informants whether those have to transmit anything in the current communication round or not .",
    "the sink asks the informant @xmath189 in @xmath292 bits , to send the bit - value corresponding to the bit - index @xmath292 .",
    "so , the total number of bits that the sink sends over all rounds , is upper - bounded by @xmath329 . in the case , when all the informants encode their information in @xmath330 bits each , then the total number of sink bits is upper bounded by @xmath331 .",
    "[ lemma : parcomboundsbsercom ] the total number of informant bits under * round - parallel * protocol upper - bound the total number of informant bits under * bit - serial * protocol .    in * bit - serial * protocol , the optimal bit - location ( in the sense of dividing the resultant ambiguity set as close to half as possible ) to be polled in a round is determined by actual values of the previously polled optimal bit - locations . however , in * round - parallel * protocol , in the @xmath237 round , @xmath332 , out of @xmath298 bit - locations to be polled , all except the first bit - location to be polled are selected by assuming that the previously chosen bit - location assume their worst - case bit - values , as in .",
    "this implies that * round - parallel * protocol , while provisioning for the worst - case , over - estimates the total number of informant bits , compared to * bit - serial * protocol . therefore the number of informant bits under * round - parallel * protocol upper - bound the total number of informant bits under * bit - serial * protocol , thus proving the lemma .",
    "[ cor : parcomboundsbsercom ] the performance of * round - parallel * is same as that of * bit - serial * on those elements of the support - set on which latter achieves its worst - case performance , in terms of total number of informant bits .    for those members of the support - set",
    "on which * bit - serial * protocol performs the worst , * round - parallel * protocol while provisioning for the worst - case , precisely chooses the values of same bit - locations to be communicated as * bit - serial * protocol , thus achieving identical performance .",
    "all parallel protocols require more total number of informant bits , in the worst - case , compared to * bit - serial*. however , among all such round parallel protocols , * round - parallel * provides the best worst - case performance , as next lemma states .",
    "[ lemma : parcomoptimal ] the * round - parallel * protocol is optimal round parallel communication protocol .",
    "we prove the theorem by considering its following implication .",
    "no round parallel protocol can do better than * round - parallel * protocol in the following sense : the total number of informant bits and the number of rounds it requires for a given support - set are no less than as required by * round - parallel * protocol for all elements of the given support - set .",
    "assume for the sake of contradiction that there is a round parallel communication protocol , let us call it * protocol x * , that is better than * round - parallel * protocol .",
    "this implies that * protocol x * achieves at least one of the following :    1 .   under * protocol",
    "x * fewer informant bits are sent in fewer communication rounds compared to * round - parallel * protocol .",
    "2 .   under * protocol x",
    "* fewer informant bits are sent in same number of communication rounds compared to * round - parallel * protocol .",
    "3 .   under * protocol x * same number of informant bits",
    "are sent in fewer communication rounds compared to * round - parallel * protocol .    now , we prove that each of these three cases leads to a contradiction .",
    "_ case 1 : _ if * protocol x * sends fewer informant bits than * round - parallel * protocol for all elements of the support - set , then this implies that even for the elements on which * bit - serial * protocol or * round - parallel * protocol ( from corollary  [ cor : parcomboundsbsercom ] ) achieves its worst - case performance , in terms of total number of informant bits , * protocol x * can achieve better performance .",
    "however , this contradicts the worst - case optimality of * bit - serial * protocol ( from lemma  [ lemma : bsercomoptimal ] ) .",
    "_ case 2 : _ applying same reasoning as in case 1 to this case leads to a similar contradiction .    _ case 3 : _ for a given support - set if * round - parallel * protocol finishes the data - gathering in @xmath333 , rounds in the worst - case , then we can always construct a round parallel communication protocol * protocol x * that finishes the data - gathering in @xmath334 , rounds with same number of informant bits as * round - parallel * protocol in the worst - case .",
    "however , any such protocol while provisioning for the worst - case , ends - up sending more informant bits and requires more rounds than * round - parallel * protocol for those elements of the support - set on which * round - parallel * protocol does not achieve worst - case optimal performance .",
    "each of these cases shows that there are always some elements of the support - set on which * protocol x * performs worse than * round - parallel * protocol .",
    "therefore , we prove that no round parallel communication protocol can do better than * round - parallel * protocol for all elements of the given support - set .",
    "we have , thus far , discussed the notion of worst - case compressibility in distributed source coding scenario when only a single instance of data - vector is available at the informants ( _ oneshot compression _ ) .",
    "however , the majority of results in classical information theory are derived in the limit of asymptotic block - lengths , though recently the role of non - asymptotic block - lengths has been investigated @xcite .",
    "these results firmly establish the effectiveness of block - coding in achieving the optimal average - case performance of various information - theoretic problems . in this section ,",
    "we attempt to investigate the effectiveness of block - coding in realizing the optimal worst - case performance of asymmetric distributed source coding problem .",
    "formally , we are concerned with addressing the question whether solving the worst - case bit - compressibility problem over block - length @xmath333 , results in fewer informant bits and larger achievable rate - region than solving this problem @xmath287 times over single instance of data as in . to aid in our subsequent analysis",
    ", we introduce some definitions .",
    "define @xmath335 .",
    "then , for @xmath336 , the @xmath195-extension of support - set @xmath87 is : @xmath337 the @xmath195-extension of data - vector @xmath210 , is : @xmath338 then , the @xmath195-extension of support - set @xmath196 , is : @xmath339 note that @xmath340 and @xmath341 .",
    "* problem statement : * consider a distributed information - gathering scenario where a sink collects the data from @xmath0 correlated informants .",
    "as in section  [ sec : probsetting ] , divide the sequence of events in this distributed data - compression problem in terms of _ data - generation epoch _ and _ data - gathering epoch_. however , in this case , the data - generation epoch is repeated @xmath287 times , @xmath336 , before each data - gathering epoch where the sink learns each of @xmath287 strings of each informant .",
    "rest of the details of the problem setting are same as in section  [ sec : probsetting ] and we do not repeat those here .    a sample @xmath207 is drawn from the discrete and finite support - set @xmath87 over @xmath0 binary strings .",
    "the strings of @xmath7 are revealed to the informants , with string @xmath3 being given to the @xmath342 informant .",
    "this process is repeated @xmath287 times , @xmath343 , resulting in each informant accumulating @xmath287 instances of its data . at the end of this data - generation epoch",
    ", the sink begins data - gathering to learn each of @xmath287 strings of each informant .",
    "we are interested in minimizing the total number of informant bits required , in the worst - case , to enable the sink to _ losslessly _ ( @xmath206 ) learn each of the @xmath287 instances of @xmath7 revealed to the informants in the previous @xmath287 data - generation epochs .",
    "consider an alternative problem formulation in terms of a new problem - encoding scheme that also facilitates the design and analysis of optimal solutions in our setting .",
    "* alternate problem statement : * assume that @xmath287-extended data - vector @xmath344 , is drawn from discrete and finite @xmath287-extended support - set @xmath345 over @xmath0 binary - strings .",
    "the strings of @xmath346 are revealed , unbeknownst to the sink , to the informants with the string @xmath347 given to the @xmath348 , informant .",
    "the sinks knows that one of the strings from @xmath345 is drawn and its different components are given to different informants .",
    "the objective of data - gathering is to enable the sink to learn the identity of this string by communicating with different informants .    in our asymmetric communication scenario ,",
    "the sink knows support - set @xmath345 .",
    "we assume that each informant @xmath349 , knows its @xmath287-extended support - set @xmath350 .",
    "the sink can uniquely describe every @xmath351 , in @xmath352 bits .",
    "however , for the same reasons as in section  [ sec : probsetting ] to efficiently query the informants for the purpose of data - gathering , the sink can also uniquely encode every @xmath346 in terms of set @xmath353 of @xmath354 bits by concatenating @xmath355 bits long representation of each @xmath356 .    [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ table : kblkprobencoding ]    * example 2 : * we illustrate the proposed problem - encoding scheme for alternative problem statement above in table  [ table : kblkprobencoding ] .",
    "consider the support - set @xmath26 of two jointly distributed random variables @xmath29 and @xmath33 defined over alphabet @xmath357 , with @xmath358 , as given in the first column .",
    "the elements of @xmath359-extension of this support - set @xmath360 , are listed in the second column , however for the sake of brevity , we list only 10 of these elements . recall that each element of @xmath361 is the concatenation of two samples of @xmath362 . in the third column , we list the corresponding @xmath359-extended data - block at each informant . the sink and each informant can agree _ a priori _ on some deterministic binary - encoding of @xmath359-extension of informant s data . in the fourth column",
    ", we give one such encoding and the fifth column lists the concatenation of binary - encoding of data - blocks at each informant .    with this encoding scheme ,",
    "the worst - case bit - compressibility problem with block - length @xmath287 is to identify the smallest subset of bit - locations of size @xmath363 in the concatenated bit - representation of @xmath346 , whose values the sink must know to unambiguously learn @xmath346 revealed to the informants .",
    "that is , @xmath364 where @xmath365 is the @xmath287-extended conditional ambiguity set when the subset @xmath366 , is known at the sink . in the next subsection",
    ", we discuss the solution of .      from the previous discussion , it is easy to observe that @xmath363 satisfies : @xmath367 .",
    "therefore , for asymptotic block - lengths , @xmath368 , @xmath287-block bit - compressibility per block @xmath369 satisfies : @xmath370 where ( a ) follows from @xmath371 and ( b ) follows from @xmath372 as @xmath373 for any finite and fixed @xmath0 .    recall from section  [ sec : probsetting ] that the worst - case bit - compressibility for oneshot compression satisfies : @xmath374 .",
    "this implies that compared to oneshot computation , the block - coding improves the lower - bound on bit - compressibility by no more than one bit and the corresponding upper - bound is reduced by at most one bit per informant .",
    "this leads us to conclude that for the worst - case distributed compression problem in our setting , the block - coding offers _",
    "almost _ no gain with respect to oneshot compression .",
    "therefore , the oneshot compression is _ almost _ optimal with respect to solving the worst - case dsc problem in our setting .    to formally prove that @xmath375 for all @xmath336 , a communication protocol to optimally solve the problem in",
    "can be designed as a @xmath333 , block generalization of * bit - serial * protocol introduced in section  [ sec : bcompressibility4cmprsn ] for solving the worst - case dsc problem for @xmath376 ( _ oneshot compression _ ) . replacing the various support - sets and ambiguities in * bit - serial * protocol by their @xmath287-extended equivalents and using the problem - encoding scheme proposed above in this section , results in the desired protocol , called `` * * @xmath287-extended bit - serial * * '' protocol , given below .",
    "@xmath249 let @xmath377 let @xmath378 : index set of all bit - locations in @xmath353 let @xmath145 be the index set of undefined bits in @xmath252 , @xmath253 ( @xmath379 ) @xmath255 if @xmath256 , then choose uniformly at random the bit - location @xmath257 the sink asks the informant corresponding to bit - location @xmath258 to send bit - value @xmath259 set @xmath380 compute @xmath261 , the set of undefined bits @xmath262    the proof of optimality of * @xmath287-extended bit - serial * protocol for the worst - case @xmath287-block distributed compression is obtained by following the same reasoning that was used to prove the optimality of * bit - serial * protocol in section  [ sec : bcompressibility4cmprsn ] .",
    "therefore , we omit it here .    using * @xmath287-extended bit - serial * protocol , in figures  [ fig : dist6blkcoding]-[fig : dist3blkcoding ] ,",
    "we plot the worst - case achievable rate - regions for two different support - sets of two correlated informants with block - length @xmath381 , and @xmath368 .",
    "these figures demonstrate the limiting behaviour of sum and individual information rates as the function of block - length for two particular distributions and establish the _ almost _ optimality of solving the worst - case distributed compression problem with only a single instance of informant data - vector .     with @xmath382 ( b ) corresponding worst - case achievable rate - regions for data - block length @xmath381 and @xmath368.,width=384 ]     with @xmath383 ( b ) corresponding worst - case achievable rate - regions for data - block length @xmath381 and @xmath368.,width=384 ]",
    "next , we compute the achievable rate - region for the worst - case dsc problem with block - coding in terms of the achievable rate - region for the worst - case distributed compression problem with single instance of informant data - vector .",
    "the following lemma states our result .",
    "[ lemma : kblk - rateregion ] given the set of achievable rate - vectors for the worst - case distributed compression problem for the oneshot compression , as in corollary  [ cor : nrateregion ] , the set of achievable rate - vectors for the worst - case distributed compression problem with @xmath333 , block - coding is defined by @xmath384 for all @xmath385 , where @xmath386    consider constraint @xmath387 .",
    "the size of the set that can be described in @xmath388 bits from informant 1 lies between @xmath389 and @xmath390 .",
    "therefore , the size of the @xmath287-extension of this set lies between @xmath391 and @xmath392 .",
    "this implies that the minimum number of bits per block required from informant 1 to describe the @xmath287-extended set is at least @xmath393 .",
    "identical argument holds for proving other constraints in the statement of the lemma for all other subsets of @xmath394 . combining all the proofs together , proves the lemma .",
    "we consider classical problem of distributed source coding in information theory .",
    "we propose a new canonical scheme to construct and address different variants of this problem .",
    "the classical distributed source coding ( dsc ) problem in information theory finds a natural application in addressing the data - gathering problem in wireless sensor networks , where sensor data is often assumed to be correlated .",
    "however , existing approaches to address distributed source coding problem can not be employed directly to solve the data - gathering problem in wireless sensor networks . in this paper , therefore , we propose a variant of distributed source coding problem that works with single instance of sensor data - vector to reduce the latency of data - gathering , employs interactive communication to reduce expenditure of communication and computation resources at the nodes , and does not require sensor nodes to have the complete knowledge of the entire network .",
    "further , to perform the worst - case information - theoretic analysis of certain problems in wireless sensor networks , we propose the notion of _ information ambiguity _",
    ", prove that it is a valid information measure , and derive its various properties .",
    "we provide optimum and constructive solutions of the proposed variant of the distributed source coding problem in two communication scenarios in terms of two respective protocols and prove that unlike the average - case performance of distributed source coding problems , the worst - case performance of such problems is not enhanced by employing block - coding and the optimal worst - case performance can be achieved just with a single instance of source data - vector .",
    "we have also proposed a system - architecture to implement our work in actual data - gathering wireless sensor networks to enhance their lifetime . however , the details of such extensions of our work are beyond the scope of this paper and are discussed in one of our forthcoming submissions .",
    "_ future work : _ we are currently working towards generalizing classical information theory in some newer directions to carry out its more meaningful applications to various other problems that can not be addressed in the existing framework . in particular , we are working towards extending the results in the current paper to distributed compression problems where the informants do not communicate directly with the sink but do so via some intermediate nodes .",
    "the operations that the intermediate nodes can perform , depending on their computational capabilities , on informants data determine the maximum compression that can be achieved .",
    "we are also attempting to generalize the notion of information ambiguity to discrete and infinite , and continuous support - sets as such generalizations have interesting implications in some problems in distributed inference and learning .",
    "we plan to continue to work on such problems .",
    "we have striven for a systems - level understanding of the problem of communication under very general conditions and a systems - level solution to the problem .",
    "nowhere did the problem formulation nor the proposed solution rely on details and specifics of the agents that make up the system .",
    "if we change the nature of the probability distributions or the objective functions of the resulting optimization problem , the system can represent diverse models of the real - world situations .",
    "therefore , our solution techniques , approaches , and heuristics are independent of the specific application domains",
    ". our continuing goal will be to apply our framework to understand communication in complex systems like intra - cellular communication and neural information processing systems among others .",
    "we recognize that our framework is in its infancy .",
    "however our approach suggests a systematic and principled framework for generating generalized information - like measures that are fine - tuned to aid in the task of optimal design and analysis of systems with communicating agents .",
    "depending on the underlying objective functions and constraints of the optimization problem , we expect a zoo of information - like measures will arise .",
    "as readers may have noticed , the proofs and results in this work uses tools from several disciplines .",
    "as we generalize our work , we expect to find intricate and deep connections between these disciplines and hope to pursue it concurrently in the future .",
    "a. liveris , z. xiong , and c. georghiades , `` distributed compression of binary sources using conventional parallel and serial concatenated convolutional codes , '' _ proc .",
    "dcc 2003 , _ snowbird , ut , march 2003 .",
    "l. c. zhong , j. m. rabaey , and a. wolisz , `` does proper coding make single hop wireless sensor networks reality : the power consumption perspective , '' _ proc .",
    "ieee wcnc 2005 , _ new orleans , la , march 2005 ."
  ],
  "abstract_text": [
    "<S> we consider a worst - case asymmetric distributed source coding problem where an information sink communicates with @xmath0 correlated information sources to gather their data . </S>",
    "<S> a data - vector @xmath1 is derived from a discrete and finite joint probability distribution @xmath2 and component @xmath3 is revealed to the @xmath4 source , @xmath5 . </S>",
    "<S> we consider an _ asymmetric communication _ </S>",
    "<S> scenario where only the sink is assumed to know distribution @xmath6 . </S>",
    "<S> we are interested in computing the minimum number of bits that the sources must send , _ in the worst - case _ , to enable the sink to losslessly learn _ any _ @xmath7 revealed to the sources .    </S>",
    "<S> we propose a novel information measure called _ information ambiguity _ to perform the worst - case information - theoretic analysis and prove its various properties . </S>",
    "<S> then , we provide interactive communication protocols to solve the above problem in two different communication scenarios . </S>",
    "<S> we also investigate the role of block - coding in the worst - case analysis of distributed compression problem and prove that it offers _ almost _ no compression advantage compared to the scenarios where this problem is addressed , as in this paper , with only a single instance of data - vector .    distributed compression ; interactive communication ; wireless sensor networks ; generalized information theory ; information measures </S>"
  ]
}