{
  "article_text": [
    "the lessons we will draw from this work are as follows .    * very high dimensional spaces are of very simple structure . * it becomes easier to find clusters in high dimensions .",
    "* the simple high dimensional structure is hierarchical . *",
    "ease of handling high dimensional data , e.g.  reading off clusters , emulates the human perception system which similarly processes data with no evident latency .",
    "there is a burgeoning crisis in high dimensional data analysis , and many current approaches lack convincing performance guarantees . in hinneburg ,",
    "aggarwal and keim ( 2000 ) , attention is focused on `` relevant '' dimensions only , while aggarwal , hinneburg and keim ( 2001 ) ( cf .",
    "the revealing titles in the case of both of these citations ) state : `` recent research results show that in high dimensional space , the concept of proximity , distance or nearest neighbor may not even be qualitatively meaningful . '' the last - mentioned work investigates @xmath0 norms including for fractional values of @xmath1 .    in breuel ( 2007 )",
    ", the focus is the @xmath2-approximate nearest neighbor defined as follows : if the nearest neighbor point @xmath3 to some query point , @xmath4 , has distance @xmath5 , then any vector @xmath6 such that @xmath7 is an @xmath2-approximate nearest neighbor of @xmath4",
    ". then breuel ( 2007 ) points out : `` ... the relationship between approximation and cost of a solution need not be linear .",
    "for example , the cost of picking an @xmath2-approximate nearest neighbor could be proportional not to the difference of distances between the optimal answer and the approximation , but to the volume of the shell between the two , that is , as @xmath8 , where @xmath9 is the dimension of the space . ''",
    "various issues immediately ensue : ( i ) `` for large dimensions , even small values of @xmath2 include the entire database as @xmath2-approximate neighbors '' ; ( ii ) `` analyzing the worst - case asymptotic complexity of @xmath2-approximate algorithms is meaningless '' ; ( iii ) for `` large enough dimensions , a randomly chosen point becomes an @xmath2-approximate nearest neighbor with high probability '' ; ( iv ) `` the implicit assumption that a close approximation leads to only a small increase in the cost of a solution is not justifiable in the context of nearest neighbors '' ; and ( v ) an @xmath2-approximate nearest neighbor algorithm is not necessarily useless in practice but such an algorithm has `` neither useful meaning asymptotically ( as the dimension grows ) , nor does it make useful predictions about its behavior on practical problems '' .    in this article , we propose an approach which we consider appropriate for high dimensional data analysis , based on clustering and proximity searching .",
    "the morphology or inherent shape and form of an object is important . in data analysis ,",
    "the inherent form and structure of data clouds are important .",
    "so the embedding topology , with which the data clouds are studied , can be crucial .",
    "quite a few models of data form and structure are used in data analysis .",
    "one of them is a hierarchically embedded set of clusters ,  a hierarchy .",
    "it is traditional ( since at least the 1960s ) to impose such a form on data , and if useful to assess the goodness of fit . rather than fitting a hierarchical structure to data ( e.g. , rohlf and fisher , 1968 )",
    ", our recent work has taken a different orientation : we seek to find ( partial or global ) inherent hierarchical structure in data .",
    "as we will describe in this article , there are interesting findings that result from this , and some very interesting perspectives are opened up for data analysis and , potentially , perspectives also on the physics ( or causal or generative mechanisms ) underlying the data .    a formal definition of hierarchical structure is provided by ultrametric topology ( in turn , related closely to p - adic number theory ) .",
    "we will return to this in section [ sect23 ] below .",
    "first , though , we will summarize some of our findings .",
    "ultrametricity is a pervasive property of observational data .",
    "it arises as a limit case when data dimensionality or sparsity grows .",
    "more strictly such a limit case is a regular lattice structure and ultrametricity is one possible representation for it .",
    "notwithstanding alternative representations , ultrametricity offers computational efficiency ( related to tree depth / height being logarithmic in number of terminal nodes ) , linkage with dynamical or related functional properties ( phylogenetic interpretation ) , and processing tools based on well known p - adic or ultrametric theory ( examples : deriving a partition , or applying an ultrametric wavelet transform ) . in khrennikov ( 1997 ) and other works ,",
    "khrennikov has pointed to the importance of ultrametric topological analysis .",
    "local ultrametricity is also of importance .",
    "this can be used for forensic data exploration ( fingerprinting data sets ) : see murtagh ( 2005 , 2007 ) ; and to expedite search and discovery in information spaces : see chvez , navarro , baeza - yates and marroqun ( 2001 ) as discussed by us in murtagh ( 2004 , 2006 ) and murtagh , downs and contreras ( 2007 ) . in section [ sect23 ]",
    "we show how extent of ultrametricity is measured .",
    "section [ sect3 ] presents our main results on the remarkable properties of very high dimensional , or very sparse , spaces . as dimensionality or sparsity",
    "grow , so does the inherent hierarchical nature of the data in the space . in section [ appsect4 ]",
    "we then discuss application to very high frequency time series modeling .",
    "we can characterize clustering algorithms in terms of number of observables , @xmath10 , and number of attributes , @xmath9 , where by `` large '' is meant thousands upwards : ( i ) large @xmath10 , small @xmath9 ; as is fairly standard in astronomy ; ( ii ) large @xmath10 , large @xmath9 ; as is fairly typically the case in information retrieval ; and ( iii ) small @xmath10 , large @xmath9 ; as is often the case in bioinformatics , and textual forensics .",
    "it is case ( iii ) which is of most interest to us here .",
    "however our results also accommodate case ( ii ) .    in hall , marron and neeman ( 2005 ) , it was shown that `` under some conditions on underlying distributions , as the dimension tends to infinity with a fixed sample size , the @xmath10 data vectors form a regular @xmath10-simplex in @xmath11 '' ( ahn , marron , muller and chi , 2007 ) .",
    "these authors term the small @xmath10 , large @xmath9 case `` hdlss , high dimension , low sample size '' . in ahn and marron ( 2005 ) ,",
    "some other unrelated work is cited , where the ratio of @xmath12 tends to a constant . as with these authors ,",
    "our goal is to study the case of letting `` @xmath9 tend to infinity , fixing @xmath10 '' ( ahn and marron , 2005 )",
    ". hall et al .",
    "( 2005 ) discuss previous asymptotics work in the statistical literature .",
    "our focus is not on a simplex but rather on a hierarchy ( even if trivial ) in order to study implications for data analysis .    for `` the asymptotic geometric representation of hdlss data ''",
    ", it is shown in the work of ahn and collaborators that `` when @xmath13 , under a mild assumption , the pairwise distances between each pair of data points are approximately identical so that the data points form a regular @xmath10-simplex . in a binary classification",
    "setting , the training data from each class becomes two simplices ... any reasonable classification method will find the same [ discriminant result ] when @xmath9 becomes very large . ''",
    "( ahn et al . , 2007 ) .",
    "this is a very exciting for the discriminant analysis case pursued by ahn et al .",
    ", 2007 ; ahn and marron , 2005 ; hall et al . , 2005 )",
    "( naive bayes , svm or support vector machine , fisher s linear discriminant , and the simplex structure in very high dimensions leading to the `` direction of maximal data piling '' ) . in this paper , our interest is in clustering , or unsupervised classification .    the mild condition for simplex structure formation , as @xmath14 is that directionality of the gaussian cloud is `` diffuse '' , defined in terms of eigenvalues :    @xmath15    then it is shown that the covariance matrix approaches a constant times the identity matrix .    in donoho and tanner ( 2005 ) ,",
    "the gaussian case is also focused on . for a gaussian cloud , `` not only are the points on the convex hull , but all reasonable - sized subsets span faces of the convex hull '' . intuitively , if all points fly apart from one another as dimensionality grows , then ( i ) each point is a vertex of the convex hull of the cloud of points ; ( ii ) each pair of points generates an edge of the convex hull ; and ( iii ) sets of points form a regional face of the convex hull .",
    "these properties are proven by donoho and tanner ( 2005 ) who conclude : `` this is wildly different than the behavior that would be expected by traditional low - dimensional thinking . ''",
    "we may ask why we ( in this work ) lay importance on the fact that the high dimensional simplex additionally defines an ultrametric topological embedding . an ultrametric topology requires ( as will be described in sections to follow ) any triangle to be either ( i ) equilateral , or ( ii ) isosceles with small base .",
    "the equilateral case corresponds fine with the simplex structure .",
    "but it is useful to us to hang on to the isosceles with small base case , too , for inter - cluster relationships .",
    "we will look later at examples to support this viewpoint .",
    "summarizing a full description in murtagh ( 2004 ) we explored two measures quantifying how ultrametric a data set is , ",
    "lerman s and a new approach based on triangle invariance ( respectively , the second and third approaches described in this section ) .",
    "the triangular inequality holds for a metric space : @xmath16 for any triplet of points @xmath17 .",
    "in addition the properties of symmetry and positive definiteness are respected .",
    "the `` strong triangular inequality '' or ultrametric inequality is : @xmath18 for any triplet @xmath17 .",
    "an ultrametric space implies respect for a range of stringent properties .",
    "for example , the triangle formed by any triplet is necessarily isosceles , with the two large sides equal ; or is equilateral .",
    "* firstly , rammal , toulouse and virasoro ( 1986 ) used discrepancy between each pairwise distance and the corresponding subdominant ultrametric .",
    "now , the subdominant ultrametric is also known as the ultrametric distance resulting from the single linkage agglomerative hierarchical clustering method .",
    "closely related graph structures include the minimal spanning tree , and graph ( connected ) components . while the subdominant provides a good fit to the given distance ( or indeed dissimilarity ) , it suffers from the `` friends of friends '' or chaining effect . * secondly , lerman ( 1981 ) developed a measure of ultrametricity , termed h - classifiability , using ranks of all pairwise given distances ( or dissimilarities ) .",
    "the isosceles ( with small base ) or equilateral requirements of the ultrametric inequality impose constraints on the ranks . the interval between median and maximum rank of every set of triplets must be empty for ultrametricity . we have used extensively lerman s measure of degree of ultrametricity in a data set . taking ranks provides scale invariance .",
    "but the limitation of lerman s approach , we find , is that it is not reasonable to study ranks of real - valued ( values in non - negative reals ) distances defined on a large set of points . *",
    "thirdly , our own measure of extent of ultrametricity ( murtagh , 2004 ) can be described algorithmically .",
    "we examine triplets of points ( exhaustively if possible , or otherwise through sampling ) , and determine the three angles formed by the associated triangle .",
    "we select the smallest angle formed by the triplet points",
    ". then we check if the other two remaining angles are approximately equal .",
    "if they are equal then our triangle is isosceles with small base , or equilateral ( when all triangles are equal ) .",
    "the approximation to equality is given by 2 degrees ( 0.0349 radians ) .",
    "our motivation for the approximate ( `` fuzzy '' ) equality is that it makes our approach robust and independent of measurement precision .",
    "a supposition for use of our measure of ultrametricity is that we can define angles ( and hence triangle properties ) .",
    "this in turn presupposes a scalar product .",
    "thus we presuppose a complete normed vector space with a scalar product  as one example , the real part of a hilbert space  to provide our needed environment .",
    "quite a general way to embed data , to be analyzed , in a euclidean space , is to use correspondence analysis ( murtagh , 2005 ) .",
    "this explains our interest in using correspondence analysis : it provides a convenient and versatile way to take input data in many varied formats ( e.g. , ranks or scores , presence / absence , frequency of occurrence , and many other forms of data ) and map them into a euclidean , factor space .",
    "murtagh ( 2004 ) , and earlier work by rammal , angles dauriac and doucot ( 1985 ) and rammal et al .",
    "( 1986 ) , has demonstrated the pervasiveness of ultrametricity , by focusing on the fact that sparse high - dimensional data tend to be ultrametric . in such work",
    "it is shown how numbers of points in our clouds of data points are irrelevant ; but what counts is the ambient spatial dimensionality . among cases looked at are statistically uniformly ( hence `` unclustered '' , or without structure in a certain sense ) distributed points , and",
    "statistically uniformly distributed hypercube vertices ( so the latter are random 0/1 valued vectors ) . using our ultrametricity measure",
    ", there is a clear tendency to ultrametricity as the spatial dimensionality ( hence spatial sparseness ) increases .    as hall et al .",
    "( 2005 ) also show , gaussian data behave in the same way and a demonstration of this is seen in table [ tabunifgauss ] .",
    "to provide an idea of consensus of these results , the 200,000-dimensional gaussian was repeated and yielded on successive runs values of the ultrametricity measure of : 0.96 , 0.98 , 0.96 .",
    ".typical results , based on 300 sampled triangles from triplets of points . for uniform , the data are generated on [ 0 , 1 ] ; hypercube vertices are in @xmath19 , and for gaussian , the data are of mean 0 , and variance 1 .",
    "is the ambient dimensionality .",
    "isosc .  is the number of isosceles triangles with small base , as a proportion of all triangles sampled .",
    "equil .  is the number of equilateral triangles as a proportion of triangles sampled .",
    "um is the proportion of ultrametricity - respecting triangles (= 1 for all ultrametric ) . [ cols=\"<,<,<,<,<\",options=\"header \" , ]         we find clearly distinguishable peaks in figure [ fig10 ] .",
    "the lower and the higher peaks belong to the two arima components .",
    "the central peak belongs to the inter - cluster distances .",
    "we have shown that our methodology can be of use for time series segmentation and for model identifiability . given the use of a scalar product space as the essential springboard of all aspects of this work",
    ", it would appear that generalization of this work to multivariate time series analysis is straightforward .",
    "what remains important , however , is the availability of very large embedding dimensionalities , i.e.  very high frequency data streams .",
    "we use financial futures , circa march 2007 , denominated in euros from the dax exchange .",
    "our data stream is at the millisecond rate , and comprises about 382,860 records .",
    "each record includes : 5 bid and 5 asking prices , together with bid and asking sizes in all cases , and action .",
    "we extracted one symbol ( commodity ) with 95,011 single bid values , on which we now report results .",
    "see figure [ fig100 ] .",
    "embeddings were defined as follows .",
    "* windows of 100 successive values , starting at time steps : 1 , 1000 , 2000 , 3000 , 4000 , @xmath20 , 94000 .",
    "* windows of 1000 successive values , starting at time steps : 1 , 1000 , 2000 , 3000 , 4000 , @xmath20 , 94000 .",
    "* windows of 10000 successive values , starting at time steps : 1 , 1000 , 2000 , 3000 , 4000 , @xmath20 , 85000 .",
    "the histograms of distances between these windows , or embeddings , in respectively spaces of dimension 100 , 1000 and 10000 , are shown in figure [ fig110 ] .",
    "note how the 10000-length window case results in points that are strongly overlapping .",
    "in fact , we can say that 90% of the values in each window are overlapping with the next window . notwithstanding",
    "this major overlapping in regard to clusters involved in the pairwise distances , if we can still find clusters in the data then we have a very versatile way of tackling the clustering objective . because of the greater cluster concentration that we expect ( from discussion in earlier sections of this article ) from a greater embedding dimension",
    ", we use the 86 points in 10000-dimensional space , notwithstanding the fact that these points are from overlapping clusters .    we make the following supposition based on figure [ fig100 ] : the clusters will consist of successive values , and hence will be justifiably termed segments . to validate our approach we will pursue three separate attacks on the same problem of time series segmentation .",
    "firstly , from the distances histogram in figure [ fig110 ] , bottom , we will carry out gaussian mixture modeling followed by use of the bayesian information criterion ( bic , schwarz , 1978 ) as an approximate bayes factor , to determine the best number of clusters ( effectively , histogram peaks ) .",
    "secondly we will use an adjacency - respecting hierarchical clustering algorithm on the full - dimensional ( viz . , 10000 ) data .",
    "thirdly , we will use a reduced dimensionality mapping , principal coordinates analysis , using the inter - point distances .",
    "our assumptions in regard to what clusters are present in the data are minimal .",
    "furthermore our validation of segments is based on the three different ways that we have of tackling the one segmentation problem .",
    "we fit a gaussian mixture model to the data shown in the bottom histogram of figure [ fig110 ] . to derive the appropriate number of histogram peaks we fit gaussians and use the bayesian information criterion ( bic ) as an approximate bayes factor for model selection ( kass and raftery , 1995 ; murtagh and starck , 2003 ) .",
    "figure [ fig120 ] shows the succession of outcomes , and indicates as best a 5-gaussian fit . for this result",
    ", we find the means of the gaussians to be as follows : 517 , 885 , 1374 , 2273 and 3908 .",
    "the corresponding standard deviations are : 84 , 133 , 212 , 410 and 663 . the respective cardinalities of the 5 histogram peaks are : 358 , 1010 , 1026 , 911 and 350 .",
    "note that this relates so far only to the histogram of pairwise distances .",
    "we now want to determine the corresponding clusters in the input data .",
    "while we have the segmentation of the distance histogram , we need the segmentation of the original financial signal .",
    "if we had 2 clusters in the original financial signal , then we could expect up to 3 peaks in the distances histogram ( viz .",
    ", 2 intra - cluster peaks , and 1 inter - cluster peak ) .",
    "if we had 3 clusters in the original financial signal , then we could expect up to 6 peaks in the distances histogram ( viz .",
    ", 3 intra - cluster peaks , and 3 inter - cluster peaks ) .",
    "this information is consistent with asserting that the evidence from figure [ fig120 ] points to two of these histogram peaks being approximately co - located ( alternatively : the distances are approximately the same ) .",
    "we conclude that 3 clusters in the original financial signal is the most consistent number of clusters .",
    "we will now determine these .",
    "one possibility is to use principal coordinates analysis ( torgerson s , gower s metric multidimensional scaling ) of the pairwise distances .",
    "in fact , a 2-dimensional mapping furnishes a very similar pairwise distance histogram to that seen using the full , 10000 , dimensionality .",
    "the first axis in figure [ fig180 ] accounts for 88.4% of the variance , and the second for 5.8% .",
    "note therefore how the scales of the planar representation in figure [ fig180 ] point to it being very linear .",
    "benzcri ( 1979 , vol .",
    "ii , chapter 7 , section 3.1 ) discusses the guttman effect , or guttman scale , where factors that are not mutually correlated , are nonetheless functionally related .",
    "when there is a `` fundamentally unidimensional underlying phenomenon '' ( there are multiple such cases here ) factors are functions of legendre polynomials .",
    "we can view figure [ fig180 ] as consisting of multiple horseshoe shapes .",
    "a simple explanation for such shapes is in terms of the constraints imposed by lots of equal distances when the data vectors are ordered linearly : see murtagh ( 2005 , pp .",
    "46 - 47 ) .",
    "another view of how embedded ( hence clustered ) data are capable of being well mapped into a unidimensional curve is critchley and heiser ( 1988 ) .",
    "critchley and heiser show one approach to mapping an ultrametric into a linearly or totally ordered metric . we have asserted and then established how hierarchy in some form is relevant for high dimensional data spaces ; and",
    "then we find a very linear projection in figure [ fig180 ] . as a consequence",
    "we note that the critchley and heiser result is especially relevant for high dimensional data analysis .    knowing that 3 clusters in the original signal are wanted , we will use an adjacency - constrained agglomerative hierarchical clustering algorithm to find them : see figure [ fig140 ] .",
    "the contiguity - constrained complete link criterion is our only choice here if we are to be sure that no inversions can come about in the hierarchy , as explained in murtagh ( 1985 ) . as input ,",
    "we use the coordinates in figure [ fig180 ] . the 2-dimensional figure [ fig180 ] representation relates to over 94% of the variance .",
    "the most complete basis was of dimensionality 85 .",
    "we checked the results of the 85-dimensionality embedding which , as noted below , gave very similar results .",
    "reading off the 3-cluster memberships from figure [ fig140 ] gives for the signal actually used ( with a very initial segment and a very final segment deleted ) : cluster 1 corresponds to signal values 1000 to 33999 ( points 1 to 33 in figure [ fig140 ] ) ; cluster 2 corresponds to signal values 34000 to 74999 ( points 34 to 74 in figure [ fig140 ] ) ; and cluster 3 corresponds to signal values 75000 to 86999 ( points 75 to 86 in figure [ fig140 ] ) .",
    "this allows us to segment the original time series : see figure [ fig160 ] .",
    "( the clustering of the 85-dimensional embedding differs minimally .",
    "segments are : points 1 to 32 ; 33 to 73 ; and 74 to 86 . )            to summarize what has been done :    1 .",
    "the segmentation is initially guided by the peak - finding in the histogram of distances 2 .   with high dimensionality",
    "we expect simple structure in a low dimensional mapping provided by principal coordinates analysis 3 .   which we use as input to a sequence - constrained clustering method in order to determine the clusters 4 .   which can then be displayed on the original data",
    ".    in this case",
    ", the clusters are defined using a complete link criterion , implying that these three clusters are determined by minimizing their maximum internal pairwise distance .",
    "this provides a strong measure of signal volatility as an explanation for the clusters , in addition to their average value .",
    "one interesting conclusion on this work follows .",
    "traditionally , clustering algorithms have generally been considered as distance - based or model - based .",
    "the former is exemplified by agglomerative hierarchical clustering , or k - means partitioning .",
    "the latter is exemplified by gaussian mixture modeling .",
    "( one motivation for model - based clustering is the computational difficulty , in general , of taking account of all pairwise distances . )",
    "the approach described in this work is both distance - based and model - based .",
    "what we have observed in all of this work is that in the limit of high dimensionality a scalar product space becomes ultrametric .",
    "it has been our aim in this work to link observed data with an ultrametric topology for such data .",
    "the traditional approach in data analysis , of course , is to impose structure on the data .",
    "this is done , for example , by using some agglomerative hierarchical clustering algorithm .",
    "we can always do this ( modulo distance or other ties in the data ) .",
    "then we can assess the degree of fit of such a ( tree or other ) structure to our data .",
    "for our purposes , here , this is unsatisfactory .",
    "* firstly , our aim was to show that ultrametricity can be naturally present in our data , globally or locally .",
    "we did not want any `` measuring tool '' such as an agglomerative hierarchical clustering algorithm to overly influence this finding .",
    "( unfortunately rammal et al . ,",
    "1986 , suffers from precisely this unhelpful influence of the `` measuring tool '' of the subdominant ultrametric .",
    "in other respects , rammal et al . , 1986 , is a seminal paper . ) * secondly , let us assume that we did use hierarchical clustering , and then based our discussion around the goodness of fit .",
    "this again is a traditional approach used in data analysis , and in statistical data modeling .",
    "but such a discussion would have been unnecessary and futile . for , after all , if we have ultrametric properties in our data then many of the widely used hierarchical clustering algorithms will give precisely the same outcome , and furthermore the fit is by definition optimal .",
    "( our point here is that if @xmath21 for cluster @xmath22 , at all agglomerations , then single linkage and complete linkage are identical . )",
    "we have described an application of this work to very high frequency signal processing .",
    "the twin objectives are signal segmentation , and model identification .",
    "we have noted that a considerable amount of this work is model - based : we require assumptions ( on clusters , and on model(s ) ) for identifiability .",
    "motivation for this work includes the availability of very high frequency data streams in various fields ( physics , engineering , finance , meteorology , bio - engineering , and bio - medicine ) . by using a very large embedding dimensionality",
    ", we are approaching the data analysis on a very gross scale , and hence furnishing a particular type of multiresolution analysis . that this is worthwhile has been shown in our case studies .",
    "aggarwal , c.c . , hinneburg , a. and keim , d.a .",
    "( 2001 ) . `` on the surprising behavior of distance metrics in high dimensional spaces '' , _ proceedings of the 8th international conference on database theory _ , pp .  420434 , january 04 - 06 .",
    "hinneburg , a. , aggarwal , c. and keim , d. ( 2000 ) .",
    "`` what is the nearest neighbor in high dimensional spaces ? '' , _ vldb 2000 , proceedings of 26th international conference on very large data bases _ , september 10 - 14 , 2000 , cairo , egypt , morgan kaufmann , pp .",
    "506515 .",
    "murtagh , f. ( 2006 ) .",
    "`` from data to the physics using ultrametrics : new results in high dimensional data analysis '' , in a.yu .",
    "khrennikov , z. raki and i.v .",
    "volovich , eds .",
    ", _ p - adic mathematical physics _ , american institute of physics conf .",
    "826 , 151161 .",
    "murtagh , f. , downs , g. and contreras , p. ( 2008 ) .",
    "`` hierarchical clustering of massive , high dimensional data sets by exploiting ultrametric embedding '' , _ siam journal on scientific computing _",
    ", 30 , 707730 .",
    "rammal , r. , toulouse , g. and virasoro , m.a .",
    "`` ultrametricity for physicists '' , _ reviews of modern physics _ , 58 , 765788 .",
    "rohlf , f.j .",
    "and fisher , d.r .",
    "`` tests for hierarchical structure in random data sets '' , _ systematic zoology _ , 17 , 407412 ."
  ],
  "abstract_text": [
    "<S> an ultrametric topology formalizes the notion of hierarchical structure . </S>",
    "<S> an ultrametric embedding , referred to here as ultrametricity , is implied by a hierarchical embedding . </S>",
    "<S> such hierarchical structure can be global in the data set , or local . by quantifying extent or degree of ultrametricity in a data set , </S>",
    "<S> we show that ultrametricity becomes pervasive as dimensionality and/or spatial sparsity increases . </S>",
    "<S> this leads us to assert that very high dimensional data are of simple structure . </S>",
    "<S> we exemplify this finding through a range of simulated data cases . </S>",
    "<S> we discuss also application to very high frequency time series segmentation and modeling .    </S>",
    "<S> * keywords : * multivariate data analysis , cluster analysis , hierarchy , ultrametric , p - adic , dimensionality </S>"
  ]
}