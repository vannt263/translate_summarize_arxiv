{
  "article_text": [
    "fourier ptychographic microscopy ( fpm )  @xcite circumvents optical space - bandwidth ( sbp ) limitations to achieve gigapixel - scale quantitative phase images , having both wide field - of - view ( fov ) and high resolution .",
    "the method combines ideas from synthetic aperture and translational - diversity phase retrieval  @xcite , conveniently realized by replacing the light source of a microscope with an led array , then capturing multiple images under different illumination angles .",
    "when leds illuminate the sample from angles smaller than that allowed by the objective s numerical aperture ( @xmath0 ) , brightfield images result .",
    "conversely , when the illumination na is larger than the objective na , darkfield images result .",
    "although darkfield images alone do not have higher resolution than the objective allows , they do contain information about sub - diffraction - limit sized features , which occupy a shifted area of the sample s fourier space ( assuming a thin sample ) . by collecting many images that cover a wide region of fourier space and stitching them together coherently",
    ", one can achieve spatial resolution beyond the objective s diffraction limit , corresponding to the sum of illumination and objective nas ( @xmath1 ) .",
    "fpm s scan - free high sbp imaging capability has great potential for revolutionizing biomedical imaging , with applications in digital pathology  @xcite and _ in vivo _ live cell imaging  @xcite .",
    "the original fpm method only applies to 2d thin objects , however , new models and reconstruction algorithms also enable 3d reconstruction of thick samples  @xcite .",
    "multiple algorithms have been proposed for solving the nonlinear inverse fpm problem , which amounts to phase retrieval . amongst these , there are the usual trade - offs between accuracy , noise performance and computational complexity . in practice , however , we have found that a critical metric is an algorithm s performance under model mis - match  when the experimental data is imperfect ( e.g. due to misalignment ) .",
    "this is typical in computational imaging algorithms , which are often fragile and not robust enough to provide consistent high - quality results .",
    "unfortunately , model mis - match is difficult to quantify , since it is systematic , yet unpredictable . here",
    ", we aim to compare algorithms directly , in order to identify error mechanisms and determine the most accurate and robust algorithm for our experiments .",
    "[ h ]     the original fpm algorithm used a gerchberg - saxton approach  @xcite , which is a type of alternating projections  @xcite , first developed for traditional ptychography  @xcite and later for fpm  @xcite . shifted support constraints ( finite pupil size )",
    "are enforced in the fourier domain as the corresponding amplitude constraints ( measured images ) are applied in the image domain , while letting the phase evolve as each image is stepped through sequentially .",
    "the gerchberg - saxton method , which is a type of gradient descent , represents a natural way to solve phase retrieval problems by trying to directly minimize some cost function that describes the differences between actual and predicted measurements .",
    "unfortunately , these formulations are often non - convex in nature and do not come with global convergence guarantees .",
    "recently , a class of gradient descent like updates , dubbed wirtinger flows  @xcite , have been shown to have global convergence guarantees .",
    "this method has been successfully applied to fpm  @xcite , though the actual implementation deviates from theory somewhat . in the wirtinger flow framework ,",
    "the optimization procedure is similar to gradient descent , except that the step size and initial guess are carefully chosen for provable convergence .",
    "gradient descent and wirtinger flow are _ first - order _ methods , in the sense that they only use the first - order derivative of the cost function when updating the complex - field .",
    "it is also possible , and generally advantageous , to use higher - order derivatives in the updates .",
    "for example , _ second - order _ methods ( e.g. newton s method ) use both the first and second derivative of the cost function , and have been shown to provide faster convergence rates  @xcite . in our studies , we also observe improved performance when using second - order methods .",
    "for example , in the top row of fig .",
    "[ fig4 ] , the gerchberg - saxton algorithm is a first - order method , whereas the other three methods are second - order ( or approximate second - order ) methods .",
    "all results achieve a similar resolution , but the first - order ( gerchberg - saxton ) result is corrupted by low - frequency artifacts . while computing second - order derivatives increases complexity , we find that it usually reduces the number of iterations needed , enabling fast overall run times .    the final class of algorithms that have been proposed are based on convex relaxations  @xcite .",
    "this class of phase retrieval algorithms , called phaselift , re - frames the problem in higher dimensions such that it becomes convex , then aims to minimize the cost function between actual and predicted intensity via semidefinite programming .",
    "these algorithms come with the significant advantage of rigorous mathematical guarantees  @xcite and were successfully applied to fpm data  @xcite .",
    "the actual implementations of these algorithms , however , deviate from the provable case due to computational limitations .",
    "algorithms can be further classified as sequential or global , depending on whether the update is done for each image , one at a time ( sequentially ) , or all at once with the full set of images ( globally ) for each iteration .",
    "global methods are expected to perform better , at a cost of additional computational requirements . in our studies , results show little difference between the sequential and global implementation of any particular algorithm ( see fig .  [ fig4 ] ) , suggesting that sequential procedures may be sufficient , allowing reduced computational requirements .    one seemingly unimportant classification of algorithms is whether their cost function minimizes differences in _ intensity _ or _",
    "amplitude_. throughout this paper , we refer to algorithms that minimize intensity differences as _ intensity - based _ algorithms , and algorithms that minimize amplitude differences as _ amplitude - based _ algorithms . since intensity is amplitude squared , both drive the optimization in the correct direction ; hence , one might expect that the choice between the two is of little consequence .",
    "surprisingly , however , we find that the cost function is the key predictor of experimental performance for our experimental dataset . _",
    "intensity - based _ algorithms suffer from strong artifacts ( see fig .  [ fig4 ] ) , which we show to be due to noise and model mis - match errors .",
    "hence , amplitude - based algorithms perform better on imperfect data , so are more robust . our goal is to explain why this happens .",
    "we will show that in order for a phase retrieval scheme to be robust to experimental imperfections , the choice of cost function is of crucial importance .",
    "one source of error in our experimental data is measurement noise , including gaussian noise or poisson shot noise .",
    "another main source of error is model mis - match , caused by experimental imperfections such as aberrations and led misalignment .",
    "a particular problem of fpm datasets is that they contain both brightfield and darkfield images , which have drastically different intensity levels ( see fig .",
    "[ fig5 ] ) .",
    "brightfield images can have several orders of magnitude higher intensity than darkfield images ; thus , the amount of poisson noise will also be significantly higher .",
    "if this difference in the noise levels is not properly accounted for , the brightfield noise may drown out the darkfield signal .",
    "we will further show that aberrations and led mis - calibration - the two main model mis - match errors in our experiments - result also in intensity - dependent errors .",
    "thus , by carefully designing the the cost function , we can develop algorithms that are significantly more robust to _ both _ noise and model mis - match .",
    "we develop a maximum likelihood theory which provides a flexible framework for formulating the fpm optimization problem with various noise models . in particular , we will focus on gaussian and poisson noise models .",
    "we find that _ amplitude - based _ algorithms effectively use a poisson noise model , while _ intensity - based _ algorithms use a gaussian noise model . to illustrate , we simulate four fpm datasets , three of which are contaminated with measurement errors ( see fig .",
    "[ fig1 ] ) : poisson noise , aberrations , and led misalignment .",
    "we compare the performance of various algorithms on these datasets to demonstrate that the imperfections in our experimental data are more consistent with a poisson noise model .",
    "this explains our observations that _ amplitude - based _ algorithms are more experimentally robust than _ intensity - based _ algorithms .",
    ": original position , @xmath2 : perturbed position).,width=472 ]    [ fig1 ]",
    "consider a thin sample with transmission function @xmath3 , where @xmath4 represents the 2d spatial coordinates in the sample plane .",
    "assuming that the led array is sufficiently far from the sample , each led will illuminate the sample by a plane wave from a different angle , defined by @xmath5 , where @xmath6 is the spatial frequency corresponding to the @xmath7-th led , @xmath8 . after passing through the sample ,",
    "the exit wave is the product of the sample and illumination complex fields , @xmath9 .",
    "the tilted plane wave illumination means that the fourier transform of this exit wave is just a shifted version of the fourier spectrum of the object , @xmath10 , where @xmath11 and @xmath12 is the 2d fourier transform .",
    "this exit wave then passes through the objective lens , where it is low - pass filtered by the pupil function , @xmath13 , which is usually a circle with its size defined by @xmath0 . finally , with @xmath14 being the 2d inverse fourier transform",
    ", we can write the intensity at the image plane as  @xcite @xmath15      ideally , all algorithms based on the forward model above should give good reconstructions .",
    "however , noise and model mis - match errors cause deviations from our forward model .",
    "thus , a noise model that accurately describes the error will be important for noise tolerance .",
    "heuristically , we have identified three experimental non - idealities that cause error : poisson noise , aberrations and led mis - alignment .",
    "we aim to separate and analyze the artifacts caused by each through controlled simulations that incur only one type of error .",
    "the simulated data ( fig .  [ fig1 ] ) uses the same parameters as our experimental setup , where a @xmath16 green led array ( central wavelength @xmath17 nm ) is placed 77 mm above the sample .",
    "leds are nominally 4 mm apart from each other and only the central 293 leds are used , giving a maximum @xmath18 .",
    "samples are imaged with a @xmath19 objective lens having @xmath20 .    using our forward model ,",
    "we simulate four datasets :    1 .",
    "ideal data : no noise is added .",
    "the object and pupil follow exactly the fpm forward model that is assumed in the algorithm .",
    "2 .   poisson noise data : the ideal data is corrupted by poisson - distributed noise at each pixel . to emphasize the effect and to emulate experiments with lower - performance sensors ,",
    "we simulate 20@xmath21 more noise than is present in our experiments ( details in section  [ sec : model ] ) .",
    "3 .   aberrated data : simulated images are corrupted by imaging system aberrations , which are described by the aberrated complex pupil function shown in fig .  [ fig1 ] .",
    "the pupil function used in these simulations was obtained from experimental measurements .",
    "4 .   led mis - aligned data : the illumination angle of each led",
    "is perturbed slightly ( following a normal distribution with standard deviation @xmath22 ) .",
    "the black @xmath21 and blue @xmath2 in fig .  [ fig1 ]",
    "show the original and perturbed led positions , respectively .    to deal with these experimental errors , in the next section",
    "we will discuss different noise models for formulating the fpm optimization problem .",
    "most algorithms solve the fpm problem by minimizing the difference between the measured and estimated amplitude ( or intensity ) , without assuming a noise model .",
    "hence , the fpm problem can be formulated as the following optimization @xmath23 since the cost function here , @xmath24 , aims to minimize the difference between the estimated amplitude and the measured amplitude , this is the _ amplitude - based _ cost function . by optimizing this cost function ,",
    "the projection - based algorithms for fourier ptychography can be obtained  @xcite , which treat each measurement as an amplitude - based sub - optimization problem .",
    "the formulation is used in the traditional gerchberg - saxton phase retrieval approach .    if we have information about the statistics of the noise , we can use it in our optimization formulation via the maximum likelihood estimation framework  @xcite .",
    "if we assume that our measured images suffer only from white gaussian noise , then the probability of capturing the measured intensity @xmath25 at each pixel , given the estimate of @xmath26 , can be expressed as @xmath27 = \\frac{1}{\\sqrt{2\\pi\\sigma_w^2 } } \\exp \\left[-\\frac{(i_\\ell(\\mathbf{r } ) - \\hat{i}_\\ell(\\mathbf{r}))^2}{2\\sigma_w^2}\\right ] , \\label{eqn_gaussian_prob}\\ ] ] where @xmath28 and @xmath29 is the standard deviation of the gaussian noise . @xmath30 and @xmath25 denote the estimated and measured intensity , respectively .",
    "the likelihood function is the overall probability due to all the pixels in all the images and can be calculated as @xmath31 $ ] , assuming measurements from all pixels are independent . in maximum likelihood estimation ,",
    "the goal is to maximize the likelihood function .",
    "however , it is easier to solve this problem by turning the likelihood function into a negative log - likelihood function which can be minimized .",
    "the negative log - likelihood function associated with this probability distribution can be calculated as @xmath32 \\nonumber \\\\ & & = \\sum_\\ell \\sum_\\mathbf{r } \\left[\\frac{1}{2}\\log ( 2\\pi\\sigma_w^2 ) + \\frac{(i_\\ell(\\mathbf{r } ) - \\hat{i}_\\ell(\\mathbf{r}))^2}{2\\sigma_w^2 } \\right ] .",
    "\\label{eqn_gaussian_log}\\end{aligned}\\ ] ]    the next step is to minimize this negative log - likelihood function by estimating @xmath26 so that the overall probability is maximized . for white gaussian noise",
    ", it is assumed that @xmath33 are the same across all pixels for all images ( i.e. all measurements have the same amount of noise ) , though this will _ not _ be the case for fpm datasets . by making a gaussian noise assumption , the first term in ( [ eqn_gaussian_log ] ) is a constant and can be ignored .",
    "the optimization problem then reduces to @xmath34 we call this cost function , @xmath35 , the _ intensity - based _ cost function because it aims to minimize the difference between the estimated intensity and the measured intensity .",
    "it also implies that noise from each pixel is treated the same and independent of the measured intensity .",
    "it will be shown later that the previous implementations of phaselift  @xcite and wirtinger flow algorithms  @xcite for fpm aimed to optimize this _ intensity - based _ cost function .",
    "however , both can be implemented instead with a poisson likelihood cost function .",
    "if we assume instead that our measured images suffer from poisson shot noise , then the probability of the measured intensity , @xmath25 , given the estimate of @xmath26 can be expressed as @xmath27 = \\frac{[\\hat{i}_\\ell(\\mathbf{r})]^{i_\\ell(\\mathbf{r } ) } \\exp [ -\\hat{i}_\\ell(\\mathbf{r})]}{i_\\ell(\\mathbf{r } ) ! } \\approx \\frac{1}{\\sqrt{2\\pi\\sigma_{\\ell,\\mathbf{r}}^2 } } \\exp \\left[-\\frac{(i_\\ell(\\mathbf{r } ) - \\hat{i}_\\ell(\\mathbf{r}))^2}{2\\sigma_{\\ell,\\mathbf{r}}^2}\\right ] .",
    "\\label{eqn_poisson_prob}\\ ] ] note that the poisson distribution is used to describe the statistics of the incoming photons at each pixel , which is a discrete probability distribution . here",
    ", we assume that the intensity is proportional to the photon count , so we can treat the distribution of the intensity as a poisson distribution .",
    "when the expected value of the poisson distribution is large , then this poisson distribution will become more like a gaussian distribution having a standard deviation proportional to the square root of the intensity , @xmath36 , from the central limit theorem .",
    "this means that a large measured intensity at a particular pixel will imply large noise at that pixel . in the simulation",
    ", we impose poisson noise on the measured intensity by distributing each pixel value with a gaussian distribution and setting the standard deviation to @xmath37 .",
    "the negative log - likelihood of the poisson noise model can then be calculated ; the optimization problem is formed by minimizing the negative log - likelihood function with estimation of @xmath26 , @xmath38+\\hat{i}_\\ell(\\mathbf{r})+\\log[i_\\ell(\\mathbf{r } ) ! ] ) \\nonumber \\\\ & & \\approx \\min_{o(\\mathbf{u})}\\sum_\\ell \\sum_\\mathbf{r } \\frac{(i_\\ell(\\mathbf{r } ) - \\hat{i}_\\ell(\\mathbf{r}))^2}{2\\sigma_{\\ell,\\mathbf{r}}^2}. \\label{eqn_poisson_log}\\end{aligned}\\ ] ] this cost function comes from the likelihood function of the poisson distribution , so we call it the _ poisson - likelihood - based _ cost function",
    ". it implies that the pixels with larger measured intensity are weighted smaller because they suffer from more noise .",
    "since the brightfield images have more large - value pixels , they are assumed to be more noisy and thus are weighted smaller in the cost function .",
    "it is shown in the appendix that the gradient of this cost function ( [ eqn_grad_poisson ] ) is very similar to that of the amplitude - based cost function ( [ eqn_grad_fa ] ) , which suggests that the amplitude - based cost function deals well with poisson - like noise or model mis - match .      for multivariate optimization problems such as ( [ eqn_amplitude ] ) and ( [ eqn_intensity ] ) ,",
    "it is convenient to reformulate the problem using linear algebra .",
    "first , the functions need to be vectorized .",
    "each of the captured images , @xmath25 , having @xmath39 pixels , are raster - scanned into vectors , @xmath40 , with size @xmath41 . since the estimated object transmission function will have higher space - bandwidth product than the raw images",
    ", the estimated object should have @xmath42 pixels , where @xmath43 .",
    "for convenience , we actually solve for the fourier space of the object , @xmath26 , which is vectorized into a vector @xmath44 with size @xmath45 . before multiplying the pupil function",
    ", the fourier space of the object is downsampled by a @xmath46 matrix @xmath47 .",
    "the matrix @xmath47 transforms a @xmath45 vector into a @xmath41 vector by selecting values out of the original vector , so the entries of this matrix are either 1 or 0 and each row contains at most one nonzero element .",
    "the pupil function @xmath13 is vectorized into a vector @xmath48 with size @xmath41 .",
    "the 2d fourier transform and inverse transform operator are @xmath49 matrices defined as @xmath50 and @xmath51 .",
    "@xmath52 , @xmath53 , @xmath54 , and @xmath55 are element - wise operators , and the @xmath56 operator puts the entries of a vector into the diagonal of a matrix .",
    "the second step is to rewrite the optimization in vector form using the new parameters .",
    "first , the forward model ( [ eqn_forward ] ) can be vectorized as @xmath57 the _ amplitude - based _ cost function ( [ eqn_amplitude ] ) can be vectorized as @xmath58 where the hyperscript @xmath59 denotes a hermitian conjugate .",
    "likewise , the _ intensity - based _ cost function ( [ eqn_intensity ] ) can be vectorized as @xmath60    the poisson likelihood cost function is more complicated to be expressed in vector form .",
    "first , we rewrite @xmath61 as @xmath62 where @xmath63 is a @xmath64 matrix with @xmath65 row vectors , @xmath66 , @xmath67 , and @xmath68 denotes the complex conjugate of vector @xmath69 .",
    "then the likelihood function can be rewritten as @xmath70 .",
    "\\label{eqn_poisson_rew}\\ ] ]    to minimize ( [ eqn_amplitude_rew ] ) , ( [ eqn_intensity_rew ] ) or ( [ eqn_poisson_rew ] ) using an iterative optimization algorithm , the gradients ( and possibly hessians ) of the cost functions need to be calculated , both of which are shown in the appendix . since ( [ eqn_amplitude_rew ] ) ,",
    "( [ eqn_intensity_rew ] ) and ( [ eqn_poisson_rew ] ) are all real - valued functions of a complex vector @xmath44 , that means that @xmath44 and @xmath71 should be treated independently in the derivative calculation , which is based on the cr - calculus discussed in  @xcite and the similar formulation for traditional ptychography discussed in  @xcite .",
    "the basic formulation of the optimization problem in fpm has been described in the last section and the derivative calculation has been done in the appendix , so we now turn our attention to describing how each algorithm solves the optimization problem based on different cost functions .",
    "the key step will be in how each algorithm updates the estimate of the object at each iteration .",
    "we compare the existing algorithms for fpm and also implement a new second - order global newton s method under different cost functions , for comparison .",
    "the initialization for all algorithms is the same - the amplitude of the image from the on - axis led illumination .        for the implementation in  @xcite",
    ", the algorithm aims to optimize the amplitude - based cost function ( [ eqn_amplitude_rew ] ) .",
    "it is the simplest to implement and , in this case , equivalent to the gerchberg - saxton approach of simply replacing known information in real and fourier space .",
    "since the sequential strategy treats a single image as an optimization problem , the cost function for each problem is just one component of eq .   and is defined as @xmath72 where @xmath7 denotes the index of each measurement .",
    "the derivative of this cost function is thus a component of eq .   and can be expressed as @xmath73 .",
    "\\label{eqn_amplitude_seq_grad}\\end{aligned}\\ ] ]    the update equation for this sequential amplitude - based algorithm is then a gradient descent with the descent direction given by eq .   and step size",
    "@xmath74 : @xmath75 where @xmath76 indicates the iteration number , which goes to @xmath77 after running through all the measurements from @xmath78 to @xmath79 .",
    "this algorithm adopts the alternating projection phase retrieval approach .",
    "the first projection in the real domain is the amplitude replacement operation @xmath80 , and the second projection is to project the previous estimated fourier region @xmath81 onto the updated fourier region @xmath82 .",
    "it is worth noting that the algorithm in  @xcite directly replaces @xmath82 in the fourier domain at each sub - iteration .",
    "a similar algorithm in  @xcite , introduced for simultaneous aberration recovery , has the same form as eq . that implements gradient descent in the fourier domain .",
    "however , when there is no pupil estimation , then @xmath48 becomes a pure support function with one inside the support and zero outside .",
    "in this situation , these two algorithms become exactly the same , and thus we refer to both as sequential gradient descent or gerchberg - saxton algorithm .",
    "the wirtinger flow optimization framework was originally proposed to iteratively solve the coded - mask phase retrieval problem using nonlinear optimization  @xcite .",
    "it is a gradient descent method implemented with a special initialization and special step sizes . for the fpm implementation described in  @xcite ,",
    "the _ intensity - based _ cost function is used . thus , the update equation for the object transmission function @xmath44 can be expressed as @xmath83 where the step size is calculated by @xmath84 where @xmath85 is the gradient of the intensity - based cost function calculated in ( [ eqn_grad_fi ] ) , and @xmath86 and @xmath87 are user - chosen parameters to calculate the step size .    in the previously proposed fpm implementation of wirtinger flow  @xcite ,",
    "the algorithm deviates somewhat from the original theory proposed in  @xcite .",
    "first , there is an additional term in the cost function to deal with additive noise .",
    "second , the initialization used in  @xcite is not the proposed one in  @xcite , but rather a low - resolution captured image .",
    "so the algorithm in  @xcite is essentially a gradient descent method with the special step size based on the intensity - based cost function and is not guaranteed to converge to the global minimum .",
    "the wirtinger flow algorithm can be implemented with different cost functions simply by replacing the original _ intensity - based _ gradient with the other gradients derived in the appendix . for comparison",
    ", we have implemented the wirtinger flow algorithm using all three of the cost functions described here : _",
    "amplitude - based _ , _ intensity - based _ and _ poisson - likelihood - based_. the results are compared in fig .",
    "[ fig4 ] with experimental data and section  [ sec : comparison ] with simulated data .      beyond",
    "first - order , a second - order optimization method can improve the convergence speed and stability of the algorithm , especially for nonlinear and non - convex problems .",
    "second - order methods ( e.g. newton s method ) use both the first and second derivatives ( hessian ) of the cost function to create a better update at each iteration . as a result",
    ", they generally require fewer iterations and move more directly towards the solution .",
    "the difficulty of second - order implementations is in computing the hessian matrix , whose size scales quadratically with the size of the image . as a result ,",
    "approximations to the hessian are often used ( known as _ quasi - newton _ methods ) to trade performance for computational efficiency .      first , we look at a gauss - newton method based on the _ amplitude - based _ cost function , which approximates the hessian matrix as a multiplication of its jacobian matrix : @xmath88 where @xmath89 ( see appendix ) .",
    "since the inversion of this hessian matrix requires very high computational cost , we approximate the hessian by dropping all the off - diagonal terms of the hessian matrix .",
    "further , the inversion of the hessian matrix may be an ill - posed problem , so a constant regularizer is adopted . in the end , the approximated hessian inversion becomes @xmath90 where @xmath91 is a constant vector with all the entries equal to a constant regularizer @xmath92 over all pixels .    by applying newton s update , eq .",
    ", with this approximated hessian inversion , the new estimate of @xmath44 can be expressed as @xmath93 where the @xmath94 part is the step size for this descent direction .",
    "note that when @xmath48 is a constant having either 0 or 1 values , this method is reduced to the sequential gradient descent method with a tunable regularizer @xmath92 .",
    "in practice , however , we also simultaneously update @xmath48 ( see section  [ sec : pupil ] ) , so the second - order optimization procedure becomes more crucial .",
    "since we expect second - order methods to perform better than first - order , and also global methods to be more stable than sequential , we propose a new global second - order ( newton s ) method , and show the results compared against other methods . for completeness",
    ", we implement all three of amplitude , intensity , and poisson - likelihood - based cost functions , showing that the amplitude and poisson - likelihood - based cost functions indeed perform better . the difficult step in deriving a newton s method for this problem",
    "is in calculating the gradients and hessians of the cost functions directly , without approximations . in the appendix",
    ", we show our derivation , and in this section we use the results with a typical newton s update equation :    @xmath95    the inverse of the hessian matrix , @xmath96 , is solved efficiently by a conjugate gradient matrix inversion iterative solver as described in  @xcite .",
    "@xmath97 is determined by the backtracking line search algorithm at each iteration , as described in  @xcite .",
    "the exact form of the cost function and the hessian depends on the algorithm used . for _",
    "amplitude - based _",
    "newton s algorithm , @xmath98 and @xmath99 ; for _ intensity - based _",
    "newton s algorithm , @xmath100 and @xmath101 ; for _ poisson - likelihood - based _ newton s algorithm , @xmath102 and @xmath103 .",
    "the phaselift formulation for phase retrieval is conceptually quite different than the previous methods described here .",
    "the idea is to lift the non - convex problem into a higher - dimensional space in which it is convex , thereby guaranteeing convergence to the global solution . to do this ,",
    "the cost function of @xmath44 is reformulated into that of a rank-1 matrix @xmath104 and the goal is to estimate @xmath105 instead of @xmath44 .",
    "the process of reformulation can be expressed as  @xcite @xmath106 where @xmath107 is an @xmath108 operator combining the inverse fourier transform , pupil cropping , and the downsampling operation with row vectors denoted by @xmath109 .",
    "hence , the estimated intensity @xmath110 as a function of @xmath105 can be expressed @xmath111 where @xmath112 is a linear operator transforming @xmath105 into @xmath110 . in section",
    "[ sec : model ] , we discussed three different cost functions .",
    "only the intensity - based and poisson - likelihood - based cost functions are convex on the estimated intensity , @xmath30 , which is a component of @xmath113 .",
    "thus , the intensity - based and poisson - likelihood - based cost functions can be turned into a convex function on @xmath105 through this transformation . for the implementation in  @xcite , by defining @xmath114^t$ ] , the intensity - based cost function can be expressed as @xmath115    since @xmath105 is a rank-1 matrix , we then minimize the rank of @xmath105 subject to @xmath116 .",
    "however , the rank minimization problem is np - hard . therefore , a convex relaxation  @xcite is used instead to transform the problem into a trace minimization problem . under this relaxation",
    ", the optimization problem becomes @xmath117 where @xmath118 is a regularization variable that depends on the noise level .",
    "the problem with this new approach is that by increasing the dimensionality of the problem , the size of the matrix @xmath105 has become @xmath119 , which is too large to store and calculate eigenvalue decomposition on a normal computer . to avoid these computational problems , we do not directly solve ( [ eqn_phaselift_cost2 ] ) , but rather apply a factorization to @xmath120 , where @xmath121 is an @xmath122 matrix .",
    "@xmath105 is a rank-1 matrix so @xmath123 is set to be 1 ( @xmath121 becomes @xmath44 ) .",
    "this new problem is then solved effectively using the augmented lagrangian multiplier , by modifying the original cost function  @xcite @xmath124 where @xmath125 , @xmath126 vector , is the lagrangian multiplier , and @xmath127 is the augmented lagrangian multiplier .",
    "both are parameters that can be tuned to give a better reconstruction . by taking the derivative of this cost function with respect to @xmath121 and updating @xmath121 in each iteration , the optimization problem",
    "can then be solved  @xcite .",
    "unfortunately , after these modifications , the problem becomes non - convex because of the minimization with respect to @xmath121 instead of @xmath105 , and thus is no longer provable .    in order to provide a more familiar form for comparing the phaselift algorithm to the others discussed in this paper , we define @xmath128^t$ ] , where @xmath129 is @xmath41 vector , so that the minimization problem in eq .   becomes @xmath130   + \\mathbf{o}^\\dagger\\mathbf{o}. \\label{eqn_phaselift_vector}\\ ] ] now",
    ", we see that the phaselift implementation is essentially an intensity - based cost function with an additional constraint that may deal better with noise .",
    "the corresponding derivative of the cost function is calculated as in the previous section : @xmath131 when @xmath132 is large compared to the component of @xmath133 and @xmath44 , the factorized phaselift formulation with rank-1 @xmath105 is equivalent to the intensity - based optimization problem discussed in the previous section .",
    "to solve this optimization problem , a quasi - newton algorithm called l - bfgs ( limited - memory broyden - fletcher - goldfarb - shanno ) method  @xcite , which is a second - order method using an approximated hessian inversion from previous gradients , is adopted .",
    "we note that although the phaselift algorithm can also be implemented with the _ poisson - likelihood - based _ cost function , the algorithm in the rank-1 case is equivalent to our global newton s method discussed in section  [ sec : newton ] for the same reason as in the above analysis .",
    "in this section , we compare the algorithms described in section  [ sec : algos ] using experimental data , as well as simulated data that mimics the experimental errors described in section  [ subsec : datasets ] .",
    "we find that second - order optimization generally performs better than first - order , while global methods do not give significant improvement over sequential .",
    "further , we explain why the cost function is a key consideration in choosing an algorithm by explaining the cause of the high - frequency artifacts that result from _ intensity - based _ algorithms .",
    "interestingly , the two model mis - match errors ( aberrations and led mis - alignment ) behave similarly to poisson noise , in that they also give intensity - dependent errors .",
    "hence , the amplitude and poisson likelihood algorithms are more robust not only to poisson noise , but also to model mis - match errors .",
    "next , we use each of the algorithms described in section  [ sec : algos ] to reconstruct amplitude and phase from the datasets simulated in section  [ subsec : datasets ] , in order to quantify performance under various experimental error types by comparing against the ground truth input .",
    "figures  [ fig2 ] and  [ fig3 ] show the reconstructed amplitude and phase , respectively . on the top left corner of each image",
    "we give the relative error of the reconstruction , defined as @xmath134 where @xmath135 and @xmath136 are the reconstructed and true images , respectively , in vector form . in order to ensure that all algorithms converge to their stable solutions , we use 200 iterations for each algorithm , except for wirtinger flow , which requires 500 iterations .",
    "the tuning parameters for each algorithm are summarized in table  [ tab_parameter ] .",
    "we have attempted to optimize each parameter as fairly as possible ; for example , we use a large @xmath132 in the phaselift algorithm to achieve a better reconstruction .",
    "small @xmath132 trades resolution for flatter background artifacts .",
    ".tuning parameters [ cols=\"^,^,^,^,^,^ \" , ]     [ tab_speed ]    the convergence speed of each algorithm can be determined from figure  [ fig_ex ] using two metrics : _ number of iterations _ required and total _",
    "runtime_. we choose the convergence curves from the cases of ideal data and led misaligned data and compare their iteration numbers and runtimes in table  [ tab_speed ] .",
    "all the algorithms were implemented in matlab on an intel i7 2.8 ghz cpu computer with 16 g ddr3 ram under os x operating system . we define convergence as the point when the relative phase error reaches its stable point .",
    "the comparison does not consider the divergent cases .",
    "in the ideal data case , we can see that the sequential methods outperform all the other algorithms in terms of runtime .",
    "the gerchberg - saxton algorithm is the fastest in terms of both iteration number and runtime for this perfect dataset .",
    "the global newton s method using _ intensity - based _ and _ amplitude - based _ cost functions also converge very fast in terms of iteration number .",
    "the wirtinger flow algorithm takes much longer to reach convergence both in runtime and iteration number . for the case of the led misaligned data , only five algorithms converge . in terms of iteration number , the _",
    "amplitude - based _",
    "newton s method converges much faster than the other four , as expected .",
    "however , the sequential gauss - newton algorithm converges much faster in terms of the runtime .",
    "though the global newton s method is theoretically better than the others , it takes significant time to calculate the full hessian matrix .",
    "thus , the sequential gauss - newton method is our preferred algorithm in practice , because it provides excellent robustness while also enabling fast runtimes and reasonable computational complexity .",
    "the main conclusions to be drawn from this section are that the fpm optimization algorithms which are formulated from _ amplitude - based _ and _ poisson - likelihood - based _ cost functions are more tolerant to imperfect datasets with both poisson noise and physical deviations like model mis - match , which were represented by aberrations and led misalignment here . in the next section , we will explain more about the causes for this trend .",
    "the reason why _ amplitude - based _ and _ poisson - likelihood - based _ algorithms have superior tolerance to experimental errors is due to their poisson noise model .",
    "each of these algorithms makes an implicit or explicit assumption that the magnitude of the errors in the data scale with the measured intensity .",
    "this is obviously a good model for poisson noise errors , which are defined as noise which scales with intensity .",
    "it is not as obvious that the model mis - match errors ( aberrations and led misalignment ) scale with intensity as well . to demonstrate this , fig .",
    "[ fig6_2 ] shows the histogram of the difference between the deviated dataset and the ideal dataset , for the cases of both brightfield and darkfield images .",
    "the histograms show a similar trend - all of the brightfield errors are much larger than the darkfield errors , with a similar statistical variation .",
    "thus , the errors from poisson noise , aberrations _ and _ led misalignment all scale with the measured intensity . in our experimental data , there are always aberrations in the objective lens , led misalignment , and poisson shot noise . since the noise model for the _ amplitude - based _ and _ poisson - likelihood - based _ algorithms match the actual noise properties , these algorithms perform better than the _ intensity - based _ algorithms . and",
    "since the images captured by fpm have drastically different intensity values , this effect dominates the reconstruction artifacts .",
    "note that these large variations in intensity values are specific to fpm and likely do not play a major role in other phase imaging schemes ( e.g. phase from defocus or traditional ptychography ) , where images do not have such a wide range of intensity values . in our experiments , the poisson noise is fairly low ( due to use of a high - performance scmos sensor ) , but the model mismatch in the experimental data can cause effects similar to strong poisson noise .        for further understanding , we look closer at the relationship between the noise model and the cost function .",
    "our optimization algorithms are derived from three cost functions .",
    "each of the cost functions makes a noise model assumption . the _",
    "intensity - based _ cost function assumes that noise in the data follows a white gaussian noise model , which means that the standard deviation of the noise is assumed to be the same across the brightfield and darkfield images .",
    "recall that the standard deviation of a gaussian noise probability model is related to the weight in the cost function for each pixel , as shown in eq .",
    "[ eqn_gaussian_log ] .",
    "the larger the standard deviation ( amount of noise ) at any pixel in fourier space , the smaller the weighting , since noisy pixels should be trusted less . in the gaussian noise model ,",
    "the weights in the cost function for large - valued pixels and small - value pixels are the same .",
    "however , the deviation for brightfield images is much larger than that for darkfield images , as shown in fig .",
    "[ fig6_2 ] .",
    "hence , the brightfield images will contribute more to the total cost function value if the weights are all the same , due to their high intensity .",
    "the result is that the _ intensity - based _",
    "( gaussian noise model ) algorithms focus mostly on the brightfield images , which correspond to low spatial frequency information , and the darkfield images do not contribute much .",
    "the result is a failure in the high - frequency reconstruction , as we saw in figs .",
    "[ fig4 ] ,  [ fig2 ] ,  [ fig3 ] , and loss of effective resolution since the darkfield images contain all the sub - diffraction - limit information . to illustrate the dramatic difference in weights , fig .",
    "[ fig7_2 ] shows the gradient of the different cost functions .",
    "obviously , the intensity cost function gives much higher weighting to low spatial frequencies , which causes the high - frequency artifacts .",
    "since the amplitude - based cost function shares a similar gradient and hessian with the poisson likelihood function , as shown in the appendix and fig .",
    "[ fig7_2 ] , it is not surprising that they both produce a similar quality reconstruction . both of these cost functions assume the noise in the data follow a poisson distribution , with the standard deviation scaling with the measured intensity .",
    "this assumption matches the actual error better than the white gaussian assumption . the actual noise or deviations in the experiments for brightfield images have larger standard deviation , while that for darkfield images have smaller standard deviation . under the poisson noise model ,",
    "the weight in the cost function is smaller for the noisy brightfield images and larger for the darkfield images . at the end ,",
    "algorithms based on the poisson noise model put more emphasis on the darkfield images and thus get a better reconstruction compared to the _ intensity - based _ algorithms .",
    "figure  [ fig7_2 ] shows that the gradients for the _ amplitude - based _ and _ poisson - likelihood - based _ cost function are similar and are more uniform throughout the whole fourier space .",
    "there are already more sophisticated fpm extensions to correct for some model mis - match errors  @xcite , similar to the probe correction algorithms in traditional ptychography  @xcite .",
    "both of the methods previously developed for fourier ptychography are derived from the amplitude - based formulation . by taking the derivative of the cost function with respect to @xmath48 ,",
    "the decent direction to estimate the pupil function can be calculated as @xmath137 . \\label{eqn_grad_pupil}\\ ] ]    by applying the pupil estimation step after each object estimation using this gradient or approximated hessian , the sequential gradient descent  @xcite and the sequential gauss - newton method  @xcite including pupil estimation can be derived .",
    "here we only consider the amplitude - based cost function , for simplicity .",
    "we wish to investigate the improvements obtained by adding a pupil estimation step to both first and second - order optimization algorithms .",
    "figure  [ fig9 ] shows the reconstruction result from the sequential gradient descent ( first - order ) and sequential gauss - newton ( second - order ) algorithms , using the aberrated dataset from the previous simulations .",
    "the numbers at the top left corner are the relative error compared to the ground truth simulated image . as can be seen , adding the pupil estimation step gives a better complex - field reconstruction , and the second - order ( gauss - newton ) method with pupil estimation provides the best result .    surprisingly , however , the second - order reconstruction without pupil estimation is better than the first - order reconstruction with pupil estimation , for this case .",
    "this highlights the robustness to aberrations that a second - order optimization scheme enables .",
    "the second - order nature of the algorithm makes it faster in convergence , and also more stable . in terms of runtime",
    ", the pupil estimation step takes about the same time as the object reconstruction part , so the algorithm is two times slower when the pupil function step is incorporated .",
    "another possible correction scheme for model mis - match is that for led misalignment .",
    "since each led position corresponds to a certain shift of the pupil function in the fourier domain , this is similar to the shift of the probe function in traditional ptychography .",
    "there , iterative algorithms have been proposed to correct for the positioning error of the probe function  @xcite . in  @xcite ,",
    "a gradient of the cost function with respect to the shift of the probe function has been calculated and the conjugate gradient method has been applied to correct for the positioning error . in  @xcite , a simulated annealing method is adopted to estimate the shift of the probe function .",
    "the simulated annealing method is also adopted to correct for the misalignment of the spatial light modulator in a overlapped fourier coding system  @xcite , analogous to fpm . in our experiments",
    ", we observe that the simulated annealing method can locate the led positions more accurately than other methods .",
    "thus , we only compare with the simulated annealing method .",
    "simulated annealing is a method of searching unknown variables over a finite space to minimize or maximize the function of merit - the cost function in our case .",
    "instead of exhaustively testing all the possible states , simulated annealing iteratively approaches the optimal state . at the first iteration ,",
    "the algorithm randomly searches several states in the space and selects the one with the smallest cost function value .",
    "the algorithm then starts at this state for the next iteration , slowly reducing the search range in the following iterations until convergence .        in our sequential algorithm ,",
    "the whole optimization problem is divided into many sub - optimization problems for different collected images . at each sub - optimization problem",
    ", a gradient descent or gauss - newton method is applied to update that corresponding region in fourier domain . to add a led mis - alignment correction step ,",
    "the simulated annealing algorithm can be incorporated into each sub - iteration to find an optimal shift of the pupil function . in each sub - iteration",
    ", the down - sampling matrix , @xmath47 , which contains the information of the pupil shift , is tested according to the annealing process for several possible states corresponding to different shifts of the pupil . the state with the smallest cost function value",
    "is selected to update the old down - sampling matrix .",
    "then , the new down - sampling matrix is used to update the corresponding region in the fourier domain .",
    "the simulated annealing method estimates the led positions with good accuracy .",
    "figure  [ fig11 ] shows the reconstruction result from the simulated led misaligned dataset , both with and without the led correction step .",
    "the result using the led correction clearly shows better quality and smaller error , as seen in fig .",
    "[ fig11](a ) .",
    "since the led correction scheme also estimates the actual led positions , which we intentionally perturbed in order to impose a known error , we can also compare the actual and recovered led positions , shown in fig .",
    "[ fig11](b ) .    to complete the picture",
    ", we now show experimental reconstructions with and without the two correction schemes : pupil correction and led mis - alignment corrections ( see fig .",
    "[ fig12 ] ) . since we do not know ground truth for our experiments",
    ", we can only make qualitative observations .",
    "an incremental improvement is observed when adding the pupil estimation and then the led correction steps - the background variation becomes flatter .",
    "figure  [ fig12](b ) shows the corrected led positions compared to the original ones , in angular coordinates .",
    "corrected positions of leds in different regions share similar offset because the fabrication process of the led array can cause unexpected position misalignment for each led .",
    "notice that the leds at the edges ( corresponding to higher angles of illumination ) incur more variation , since these are more sensitive to calibration . also , many of the large deviations occur at the edges that are not along the horizontal and vertical axes . in these areas ,",
    "the led position recovery is poor because the object has very little information there ( the resolution test target contains only square features ) and so the data contains little information about these areas .",
    "however , any errors in led positions in this area will also not significantly affect the reconstruction if they do not contribute much energy to the object spectrum .",
    "if the goal was not to correct the image results , but rather to find the led positions accurately , then one should choose an object that contains uniformly distributed spatial frequencies ( e.g. a random diffuser or speckle field ) . although the simulated annealing further improves our reconstruction , we note that it is more than ten times slower to process the data because of the local search performed at each sub - iteration .",
    "we formulated the fourier ptychographic phase retrieval problem using maximum likelihood optimization theory . under this framework , we reviewed the existing fpm algorithms and classified them based on their cost functions : _ amplitude - based _ algorithms ( akin to a poisson noise model ) and _ intensity - based _ algorithms ( akin to a white gaussian noise model ) .",
    "we also derived a new algorithm based on the poisson likelihood function , which is better suited for dealing with measurement imperfections .",
    "we compared the tolerance of these algorithms under errors due to experimental noise and model mis - match ( aberrations and led mis - alignment ) using both simulated data and experimental data . because the noise and model mis - match error for brightfield and darkfield images depend on the measured intensity , the _ amplitude - based _ and _ poisson - likelihood - based _",
    "algorithms from the poisson noise model are more robust than the _ intensity - based _ algorithms .",
    "this can be explained by the standard deviation of the noise model determining the weight of each image in the optimization .",
    "hence , _ intensity - based _ algorithms over - weight the brightfield images , resulting in poor high - frequency reconstruction .",
    "we used existing pupil estimation algorithms and proposed a simulated - annealing - based led correction algorithm to algorithmically fix the experimental deviations .",
    "we compared the performance of the pupil estimation algorithms and found that second - order methods give the best results .",
    "we also showed the capability of the simulated annealing method to correct for misaligned leds and find their actual positions .    based on our studies",
    ", we conclude that the global newton s method gives the best reconstruction , but may have high computational cost .",
    "considering both robustness and computational efficiency , we find that sequential gauss - newton method provides the best trade - offs for large - scale applications .",
    "its experimental robustness is verified in our recent time - series _ in vitro _",
    "experiments  @xcite .",
    "our open source code for this algorithm can be downloaded at @xcite .",
    "funding was provided by the gordon and betty moore foundation s data - driven discovery initiative through grant gbmf4562 to laura waller ( uc berkeley ) .",
    "then , calculate the derivative of @xmath141 with respect to @xmath44 , and it can then be expressed as @xmath142^\\dagger = \\sum_\\ell \\left [ \\frac{\\partial(\\mathbf{f}_{a\\ell}^\\dagger \\mathbf{f}_{a\\ell})}{\\partial\\mathbf{f}_{a\\ell } } \\frac{\\partial\\mathbf{f}_{a\\ell}}{\\partial\\mathbf{o}}\\right]^\\dagger .",
    "\\label{eqn_grad}\\end{aligned}\\ ] ] using @xmath143 and @xmath144 , two chain rule parts in ( [ eqn_grad ] ) are calculated as @xmath145 if @xmath69 does not contain any zero entries for @xmath8 .        with ( [ eqn_grad_two_part_i ] ) ,",
    "it is clear to express the gradient of @xmath147 as @xmath150^\\dagger = -2 \\sum_\\ell \\mathbf{q}_\\ell^\\dagger   { \\rm diag}(\\bar{\\mathbf{p } } )   \\mathbf{f } { \\rm diag}(\\mathbf{g}_\\ell)(\\mathbf{i}_\\ell - |\\mathbf{g}_\\ell|^2 ) .",
    "\\label{eqn_grad_fi}\\ ] ]    the calculation of gradient of @xmath151 with respect to @xmath44 is different from the other two . with the expression ( [ eqn_poisson_rew ] ) , the gradient of poisson likelihood function can be calculated as @xmath152 \\right)^\\dagger \\nonumber \\\\ & & = -\\left(\\sum_\\ell \\sum_j \\left[i_{\\ell , j } - \\mathbf{a}_{\\ell , j}^\\dagger \\mathbf{o } \\right ] \\frac{1}{\\mathbf{a}_{\\ell , j}^\\dagger \\mathbf{o } } \\mathbf{a}_{\\ell , j}^\\dagger \\right)^\\dagger \\nonumber \\\\ & & = -\\left ( \\sum_\\ell ( \\mathbf{i}_\\ell - |\\mathbf{g}_\\ell|^2)^\\dagger { \\rm diag}\\left ( \\frac{1}{|\\mathbf{g}_\\ell|^2 } \\right ) { \\rm diag } ( \\bar{\\mathbf{g}}_\\ell ) \\mathbf{f}^{-1 } { \\rm diag } ( \\mathbf{p } ) \\mathbf{q}_\\ell \\right)^\\dagger \\nonumber \\\\ & & = - \\sum_\\ell \\mathbf{q}_\\ell^\\dagger { \\rm diag } ( \\bar{\\mathbf{p } } ) \\mathbf{f } { \\rm diag}\\left ( \\frac{\\mathbf{g}_\\ell}{|\\mathbf{g}_\\ell|^2 } \\right ) ( \\mathbf{i}_\\ell - |\\mathbf{g}_\\ell|^2 ) .",
    "\\label{eqn_grad_poisson}\\end{aligned}\\ ] ] this is equivalent to the gradient of the _ intensity - based _ cost function with added weight @xmath153 to the component from each image . in addition , this gradient is very similar to that from the _ amplitude - based _ cost function .",
    "since we have gradients for all cost functions , the updating equation for the gradient descent method can then be expressed as @xmath154 where @xmath76 denotes the iteration number , @xmath118 is the step size chosen by the line search algorithm , and @xmath155 can be either _ intensity - based _ or _ amplitude - based _ cost function .",
    "looking at @xmath156 , @xmath157 and @xmath158 , they all contain the term @xmath159 following by a residual term .",
    "the residual term basically finds the difference between the estimation and the measurement .",
    "this difference carries the information to update the previous estimation .",
    "since each measurement carries the information for a specific region in the fourier space , the @xmath159 term brings this updating information back to the right place corresponding to some spatial frequency . for @xmath156 ,",
    "the first term in the residual shows the replacement of the amplitude in the real domain , which is the projection from the estimation to the modulus space .",
    "thus , the gradient descent method using the _ amplitude - based _ cost function is similar to the projection - based phase retrieval solver .",
    "the second - order taylor expansion on an arbitrary real function @xmath160 with a complex vector @xmath89 at certain point @xmath161 can be written as  @xcite @xmath162 where the matrix @xmath163 is the hessian of @xmath160 .",
    "for the case of a single - value function , the second - order term in the taylor expansion denotes the curvature of the function at that expansion point .",
    "thus , this hessian matrix similarly contains the curvature information of the original multi - variate function .",
    "if the hessian is a diagonal matrix , each diagonal entry denotes the curvature in each corresponding dimension .",
    "if the hessian is not diagonal , a coordinate transformation can be found to make the hessian diagonal by using eigenvalue decomposition . for a convex problem",
    ", the hessian is positive semidefinite .",
    "the curvatures of the cost function in different dimensions are always nonnegative .",
    "a standard optimization process can lead to a global minimum .",
    "however , if the problem is non - convex , a standard optimization process will probably lead to a local minimum . calculating",
    "the hessian of a cost function is useful either to examine the optimization process or to speed up the convergence rate by using newton s method .    from  @xcite ,",
    "the definition for the hessian of a real - value function with multiple complex variables is a @xmath164 matrix and can be expressed as @xmath165 where each component @xmath119 matrices can be further calculated as @xmath166 similar to the calculation of the gradient , the components of the hessians for _ amplitude - based _ , _ intensity - based _ , and _ poisson - likelihood - based _ cost functions can be calculated by taking an additional derivative on the gradient of the cost functions .",
    "the components of the hessian for the amplitude - based cost function are @xmath167 \\mathbf{f}^{-1 } { \\rm diag}(\\mathbf{p})\\mathbf{q}_\\ell \\nonumber \\\\ & & \\mathbf{h}^a_{\\bar{\\mathbf{o}}\\mathbf{o } } = \\frac{1}{2}\\sum_\\ell \\mathbf{q}_\\ell^\\dagger { \\rm diag}(\\bar{\\mathbf{p}})\\mathbf{f } { \\rm diag}\\left(\\frac{\\sqrt{\\mathbf{i}_\\ell}\\mathbf{g}_\\ell^2}{|\\mathbf{g}_\\ell|^3}\\right ) \\bar{\\mathbf{f}}^{-1 } { \\rm diag}(\\bar{\\mathbf{p}})\\bar{\\mathbf{q}}_\\ell \\nonumber \\\\ & & \\mathbf{h}^a_{\\mathbf{o}\\bar{\\mathbf{o } } } = \\frac{1}{2}\\sum_\\ell \\mathbf{q}_\\ell^t { \\rm diag}(\\mathbf{p})\\bar{\\mathbf{f } } { \\rm diag}\\left(\\frac{\\sqrt{\\mathbf{i}_\\ell}\\bar{\\mathbf{g}}_\\ell^2}{|\\mathbf{g}_\\ell|^3}\\right ) \\mathbf{f}^{-1 } { \\rm diag}(\\mathbf{p})\\mathbf{q}_\\ell \\nonumber \\\\ & & \\mathbf{h}^a_{\\bar{\\mathbf{o}}\\bar{\\mathbf{o } } } = \\sum_\\ell \\mathbf{q}_\\ell^t { \\rm diag}(\\mathbf{p})\\bar{\\mathbf{f } } \\left[\\mathbf{1}-\\frac{1}{2}{\\rm diag}\\left(\\frac{\\sqrt{\\mathbf{i}_\\ell}}{|\\mathbf{g}_\\ell|}\\right)\\right ] \\bar{\\mathbf{f}}^{-1 } { \\rm diag}(\\bar{\\mathbf{p}})\\bar{\\mathbf{q}}_\\ell , \\label{eqn_hessian_a}\\end{aligned}\\ ] ] where @xmath168 is the @xmath49 identity matrix .        in general , newton s method , which is the second - order method using the inversion of hessian matrix , is preferred in solving nonlinear least square problems because of its fast convergence and stability compared to the first - order methods such as gradient descent . the updating equation for newton s method",
    "can be expressed as @xmath171            a.  williams , j.  chung , x.  ou , g.  zheng , s.  rawal , z.  ao , r.  datar , c.  yang , and r.  cote , `` fourier ptychographic microscopy for filtration - based circulating tumor cell enumeration and analysis , '' journal of biomedical optics * 19 * , 066007066007 ( 2014 ) .                                            e.  j. cands , t.  strohmer , and v.  voroninski , `` phaselift : exact and stable signal recovery from magnitude measurements via convex programming , '' communications on pure and applied math * 66 * , 12411274 ( 2013 ) .                  f.  zhang , i.  peterson , j.  vila - comamala , a.  diaz , f.  berenguer , r.  bean , b.  chen , a.  menzel , i.  k. robinson , and j.  m. rodenburg , `` translation position determination in ptychographic coherent diffraction imaging , '' opt .",
    "express * 21 * , 1359213606 ( 2013 ) .",
    "a.  tripathi , i.  mcnulty , and o.  g. shpyrko , `` ptychographic overlap constraint errors and the limits of their numerical recovery using conjugate gradient descent methods , '' opt .",
    "express * 22 * , 14521466 ( 2014 ) ."
  ],
  "abstract_text": [
    "<S> fourier ptychography is a new computational microscopy technique that provides gigapixel - scale intensity and phase images with both wide field - of - view and high resolution . by capturing a stack of low - resolution images under different illumination angles , </S>",
    "<S> an inverse algorithm can be used to computationally reconstruct the high - resolution complex field . here , </S>",
    "<S> we compare and classify multiple proposed inverse algorithms in terms of experimental robustness . </S>",
    "<S> we find that the main sources of error are noise , aberrations and mis - calibration ( i.e. _ model mis - match _ ) . </S>",
    "<S> using simulations and experiments , we demonstrate that the choice of cost function plays a critical role , with _ </S>",
    "<S> amplitude - based _ cost functions performing better than _ intensity - based _ ones . </S>",
    "<S> the reason for this is that fourier ptychography datasets consist of images from both brightfield and darkfield illumination , representing a large range of measured intensities . </S>",
    "<S> both noise ( e.g. poisson noise ) and model mis - match errors are shown to scale with intensity . hence , algorithms that use an appropriate cost function will be more tolerant to both noise and model mis - match . given these insights , we propose a global newton s method algorithm which is robust and accurate . finally , we discuss the impact of procedures for algorithmic correction of aberrations and mis - calibration . </S>"
  ]
}