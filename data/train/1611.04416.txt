{
  "article_text": [
    "many machine learning tasks involve minimizing an objective that has the form of a sum : @xmath0 where @xmath1 is an unknown vector of interest , and @xmath2 , for @xmath3 , are cost functions most often associated with distinct data points , or subsets of points called `` mini - batches '' .",
    "problems of type  ( [ eq : additive_objective ] ) include maximum likelihood parameter estimation under independent measurements , empirical risk minimization @xcite , or composite likelihood maximization @xcite .    while generic optimization algorithms may scale poorly with data size , it is possible to devise fast optimization procedures by taking advantage of the additive nature of  ( [ eq : additive_objective ] ) .",
    "for instance , stochastic gradient descent ( sgd ) methods @xcite have a natural implementation using a single cost gradient  @xmath4 at each iteration rather than the total gradient  @xmath5 .",
    "this makes the computational cost of an iteration proportional to the associated mini - batch size rather than to the full data size , hence providing a massive gain in efficiency over deterministic gradient methods in large - scale learning situations .",
    "sgd , however , assumes continuously differentiable cost functions .",
    "there are problems of interest where this assumption does not hold , such as support vector machine training @xcite or intensity - based image registration @xcite . in such context , a potential alternative to  sgd is the expectation - propagation ( ep ) algorithm @xcite , which is also scalable to big data while relying on larger - scale function approximations .",
    "ep is fundamentally an algorithm to approximate a factorial probability distribution , providing , in particular , an approximation to the mode .",
    "hence , ep can be used to approximately minimize an objective of the form  ( [ eq : additive_objective ] ) by converting it into a boltzmann distribution : @xmath6 for some sufficiently large constant @xmath7 . as in simulated annealing @xcite",
    ", @xmath8 could be progressively increased for exact minimization , however it will be assumed fixed in this paper .",
    "ep exploits the decomposition of  @xmath9 as a product of factors by alternatively fitting each factor  @xmath10 with an unnormalized gaussian  @xmath11 via local moment matching . to approximate factor  @xmath12 , ep  first forms the `` cavity distribution '' , @xmath13 which is the current approximation to the product of all factors except the one under consideration .",
    "the factor approximation  @xmath14 is then computed so that  @xmath15 has the same moments of order  0 , 1 and 2 as  @xmath16 .",
    "we will restrict ourselves to fully factorized gaussian approximations , in which case cross second - order moments are ignored @xcite , leading to an  ep variant referred to as fully factorial  ep ( ff - ep ) in the sequel .",
    "since the moments essentially depend on the factor shape in the neighborhood of the cavity , the successive factor approximations are performed at adaptive , non - infinitesimal scales induced by the changing cavity distributions .",
    "importantly , and similarly to  sgd , the complexity of an ep  iteration is proportional to the mini - batch size since a single factor is visited at a time .",
    "ep  variants such as averaged  ep @xcite and stochastic  ep @xcite can further save memory load by constraining all factor approximations to be identical , but rest upon the same moment matching scheme .    in comparison with  sgd , the problem of computing the gradient of  @xmath17 is replaced with that of computing the vector - valued integral of  @xmath16 , which is generally well defined but may lack a closed - form expression . unless this can be worked around by applying a functional transformation to the factor , as in the power  ep algorithm @xcite , numerical approximations are needed .",
    "both the laplace method @xcite and gaussian quadrature @xcite are common choices .",
    "i review these methods and introduce another method based on variational sampling @xcite , which combines gaussian quadrature with variational approximation , and is found empirically to yield better convergence of ff - ep with non - continuously differentiable factors .",
    "the basic ep  building block consists of fitting an unnormalized gaussian distribution to a single factor .",
    "let us assume that a given factor  @xmath10 is selected , and drop the index  @xmath18 for clarity .",
    "the associated cavity distribution  @xmath19 is computed from the other factor current approximations according to  ( [ eq : cavity ] ) , and is a product of one - dimensional gaussians by construction ; let @xmath20 and @xmath21 denote its mean and variance matrix , respectively .    in ff - ep @xcite , the @xmath22 moments of  @xmath23 up to the order  2 need to be computed , @xmath24 for the monomials of degree @xmath25 : @xmath26    the factor approximation  @xmath27 is then readily given by @xmath28 , where @xmath29 is the unique unnormalized factorial gaussian distribution with same moments as  ( [ eq : moments ] ) .",
    "the problem we address here is to approximate  @xmath27 when the moments are intractable .",
    "a strategy used in @xcite is to approximate  @xmath27 using the laplace method applied to @xmath23 , _",
    "i.e. _ , first find the value  @xmath30 that maximizes  @xmath23 , then approximate  @xmath31 by its second - order taylor expansion at  @xmath30 : @xmath32 , \\ ] ] assuming that  @xmath33 is twice continuously differentiable . for a fully factorized approximation , only the diagonal elements of the hessian of  @xmath33",
    "need to be computed , hence the complexity of  ( [ eq : simple_laplace_approx ] ) is linear in  @xmath34 .",
    "however , the maximization step may be time consuming and we shall also consider a simpler variant , hereafter referred to as `` quick laplace '' , whereby the taylor expansion is performed at the cavity center  @xmath20 rather than at the maximizer  @xmath30 of  @xmath23 . note that this makes the factor approximation independent from the cavity variance  @xmath35 .      as pointed out above",
    ", the cost derivatives may be discontinuous , or change abruptly in the cavity neighborhood , in which case they are not informative about the factor at the relevant scale of analysis .",
    "in such situations , the laplace method can lead to poor factor approximations .",
    "an alternative is to use gaussian quadrature to approximate the moments  ( [ eq : moments ] ) .",
    "for instance , @xcite use a precision-3 rule : @xmath36 which involves a weighted average of @xmath22 points : @xmath37 where @xmath38 , for @xmath39 , denote the canonical vectors of @xmath40 , and @xmath41 is a free parameter . as shown in @xcite , this quadrature rule is exact for polynomials up to degree  3 , implying that it can recover the moments up to order  2 of a constant factor ( since a factor which is both linear in  @xmath42 and positive - valued on @xmath40 has to be constant ) .",
    "the choice @xmath43 minimizes the error on the fourth - order moment of a standard gaussian  @xcite , and the central weight @xmath44 is then negative for @xmath45 .",
    "however , @xcite argue that non - negative weights should be used to guarantee non - negative second - order moments and adopt @xmath46 .",
    "this leads to @xmath47 , hence discarding the central sample point .",
    "i instead set @xmath48 to induce uniform weights and avoid zero weights .",
    "the quadrature rule  ( [ eq : quadrature_rule ] ) then becomes formally similar to a monte carlo integral estimate using random points drawn independently from the cavity distribution  @xmath19 , despite that the sampling points @xmath49 , for @xmath50 , are chosen in a deterministic manner .",
    "a computational advantage of the precision-3 quadrature rule ( [ eq : quadrature_rule ] ) is to be linear in  @xmath34 , requiring only @xmath22 evaluations of  @xmath51 , but this may come at the price of fairly inaccurate moment estimates . the variational sampling method , which i originally proposed outside the context of  ep @xcite , has the potential to improve over these estimates as it is exact for fully factorized gaussian factors , just like laplace - style approximations , yet relying on the same @xmath22 evaluations as the precision-3 rule and not on differential calculus .",
    "the key insight stems from recasting the moment - matching problem into that of minimizing the generalized kullback - leibler  ( kl ) divergence @xcite : @xmath52 d\\btheta\\ ] ] over the set of unnormalized factorized gaussian distributions , _",
    "i.e. _ , functions of the form  @xmath53 $ ] parameterized by  @xmath54 , where  @xmath55 is the vector - valued function with coordinate applications given by the monomials  ( [ eq : monomials ] ) . dropping the terms independent from  @xmath56 in  ( [ eq : kl_div ] )",
    ", we see that minimizing the kl  divergence with respect to  @xmath57 is equivalent to minimizing the function : @xmath58 d\\btheta \\nonumber\\\\   & = &   - \\balpha^\\top \\int c(\\btheta ) f(\\btheta ) \\bphi(\\btheta ) d\\btheta + \\int c(\\btheta ) e^{\\balpha^\\top \\bphi(\\btheta ) } d\\btheta \\label{eq : loc_cross_ent } .\\end{aligned}\\ ] ]    therefore , instead of attempting at a direct moment evaluation  ( [ eq : moments ] ) , we may consider minimizing an empirical approximation to  ( [ eq : loc_cross_ent ] ) based , for instance , on the precision-3 rule : @xmath59",
    "\\nonumber\\\\   & = &   - \\balpha^\\top \\sum_{j=0}^{2d }   w_j f(\\btheta_j ) \\bphi(\\btheta_j ) + \\sum_{j=0}^{2d } w_j e^{\\balpha^\\top \\bphi(\\btheta_j ) } \\label{eq : surrogate_kl } .\\end{aligned}\\ ] ]    it turns out that ( [ eq : surrogate_kl ] ) is a strictly convex function of  @xmath57 , see @xcite for a proof ( more generally , strict convexity requires that the number of sample points  @xmath60 be at least as large as the number of real - valued moments , a condition satisfied in the present case since both are  @xmath61 ) .",
    "therefore , @xmath62 has a unique minimizer  @xmath63 , defining a factorized gaussian distribution  @xmath64 which is hopefully close to the actual moment - matching distribution and kl  minimizer .    while @xmath63 lacks a closed - form expression , it can be tracked efficiently using newton s method owing to the strict convexity of  @xmath62 .",
    "the gradient and hessian expressions are then needed : @xmath65\\bphi(\\btheta_j ) ,   \\label{eq : surrogate_gradient } \\\\",
    "\\nabla\\nabla^\\top \\tilde{l}(\\balpha ) & = & \\sum_{j=0}^{2d } w_j e^{\\balpha^\\top \\bphi(\\btheta_j ) }",
    "\\bphi_j(\\btheta_j ) \\bphi_j(\\btheta_j)^\\top .",
    "\\nonumber \\end{aligned}\\ ] ]    it is important to realize that , in general , the optimal fit @xmath64 does _ not _ match the moments approximated by a direct application of gaussian quadrature  ( [ eq : quadrature_rule ] ) . in particular , if the factor  @xmath51 is a factorized gaussian , then @xmath66 and the moments are thus recovered _ exactly _ unlike using gaussian quadrature .",
    "this can be seen from the gradient formula  ( [ eq : surrogate_gradient ] ) : if there exists @xmath67 such that @xmath68 $ ] , then clearly @xmath69 , meaning that @xmath70 . in some sense ,",
    "variational quadrature bridges the gap between laplace s method and gaussian quadrature : it is error - free for factors belonging to the ep  approximating exponential family ( like laplace in the gaussian case ) , and can handle non - continuously differentiable factors provided that the integrals  ( [ eq : moments ] ) exist ( like gaussian quadrature ) .",
    "my implementation of newton s method uses the cholesky decomposition to invert hessian matrices , hence the complexity of variational quadrature is in  @xmath71 .",
    "the computational overhead with respect to gaussian quadrature therefore increases with  @xmath34 , although it is partially compensated for by the fact that variational quadrature directly outputs an approximating factor  @xmath64 , while gaussian quadrature requires dividing the moment - matching distribution by the cavity distribution , which becomes more costly as @xmath34  increases .",
    "experimental comparisons of timing are provided next .",
    "the different variants of ff - ep described in section  [ sec : approx_schemes ] were evaluated on real data from the uci  repository ( https://archive.ics.uci.edu/ml/datasets.html ) to train linear binary classifiers according to different strategies : logistic regression , hinge regression , and a non - convex approximation to the minimization of the mean classification error .      the ff - ep algorithm and the numerical approximation schemes described in section  [ sec : approx_schemes ] , respectively referred to as laplace method ( la ) , quick laplace method ( qla ) , gaussian quadrature ( gq ) and variational quadrature ( vq ) in this section , were implemented in scientific python ( www.scipy.org ) .",
    "newton s method was used for both the optimization step in  la ( see section  [ sec : laplace ] ) and for vq ( see section  [ sec : variational_quad ] ) with a tolerance of  @xmath72 on relative parameter variations .",
    "each uci  dataset was formatted as a list of examples of the form @xmath73 , for @xmath74 , where @xmath75 was a binary label and @xmath76 a feature vector . a constant value ( baseline )",
    "was appended to all feature vectors . in a pre - processing step ,",
    "all features were normalized to unit euclidean norm after subtracting the mean across examples , except for the baseline .    in all experiments ,",
    "the ff - ep algorithm incorporated a fixed gaussian factor with zero mean and scalar variance matrix @xmath77 modeling a prior distribution on the linear classification parameter vector  @xmath78 .",
    "factors were defined according to the chosen training cost ( see below ) by summing up the costs over mini - batches and exponentiating the result as in  ( [ eq : boltzmann_dist ] ) using a fixed `` inverse temperature '' parameter  @xmath79 .",
    "the corresponding factor approximations in ff - ep were initialized as identically equal to one ( _ i.e. _ , with infinite variances ) .",
    "different cost functions were considered for linear classification training :    [ [ logistic - loss . ] ] logistic loss .",
    "+ + + + + + + + + + + + + +    this choice leads to a smooth function of  @xmath42 , @xmath80    [ [ hinge - loss . ] ] hinge loss .",
    "+ + + + + + + + + + +    this is the loss classically used to train support vector machines @xcite , @xmath81    [ [ quasi-01-loss . ] ] quasi 01 loss .",
    "+ + + + + + + + + + + + + + +    in some applications , the desired training objective is to minimize the mean classification error using the 01 loss .",
    "however , because the 01 loss is discontinuous and turns out to make the proposed ff - ep variants unstable , it is approximated by a continuous non - convex function : @xmath82 for some constant  @xmath83 .",
    "while the case  @xmath84 corresponds to the hinge loss , @xmath85 provides a tighter approximation to the 01 loss for smaller  @xmath86 , see figure  [ fig : loss_funcs ] . in these experiments ,",
    "i set @xmath87 .",
    "note that the quasi 01 loss differs from the `` ramp loss '' @xcite .",
    "note that both the hinge loss and the quasi 01 loss derivatives are singular , respectively on the hyperplane @xmath88 and on the hyperplanes @xmath89 and @xmath90 .",
    "derivatives at discontinuities were conventionally defined by extending @xmath91 at @xmath92 and @xmath93 by the half sum of its left and right limits .",
    "i considered three uci  classification datasets containing less than @xmath94 examples ( haberman s survival , ionosphere , wisconsin diagnostic breast cancer ) .",
    "for the different training objectives , the question is whether the numerically approximated ff - ep algorithm converges ( knowing that convergence is not even warranted in the case of explicit updates @xcite ) and , if so , how fast and how close to the global cost minimizer .",
    "mini - batches of size  @xmath95 were used in these experiments , which enabled to loop several times over the whole set of mini - batches at reasonable computational cost .",
    "using relatively small mini - batches , ff - ep searches for a consensus between many `` weak learners '' while giving several opportunities to each learner to revise its knowledge during the process .",
    "the curves in figure  [ fig : small_datasets ] show the variations of the total cost ( computed offline ) across mini - batch updates , for the different datasets , cost functions , and ff - ep approximation schemes . for comparison ,",
    "the regression parameters  @xmath42 minimizing the respective costs were also computed offline using newton s method for the logistic loss , and powell s method ( initialized from the logistic solution ) for both the hinge and quasi  01 losses .",
    "five loops proved to be more than enough to achieve convergence in most cases using ff - ep with vq , which was clearly the most stable among the tested approximation methods in all scenarios , and converged to close - to - optimal parameters .",
    "the slight residual fluctuations observed for the ionosphere dataset could be due to the combined effect of non - convexity and a relatively small number  @xmath96 of examples compared to the number  @xmath34 of features in this dataset . both laplace - style approximations ( la and qla ) converged well for logistic and hinge regression , although the hinge loss is not continuously differentiable , but were both unstable in quasi  01 regression .",
    "gq showed oscillatory behavior or local convergence in all cases , except to some extent for logistic regression in small dimension ( haberman dataset , @xmath97 ) .    while qla performed comparably with la , it was much faster , as shown by the timings reported in table  [ tab : small_datasets ] .",
    "this is due to the function evaluations involved in the optimization step of  la , and suggests that such optimization may not be needed in  ep .",
    "vq entailed some computational overhead ( depending on the parameter dimension  @xmath34 ) compared to both qla and gq , but was also significantly faster than la .",
    ".dataset characteristics and timing on a standard single processor of different ff - ep versions for binary classification experiments .",
    "[ cols=\"^,^,^,^,^,^,^,^,^ \" , ]                  both the bank marketing and census income uci  datasets were also considered for the empirical evaluation of numerical ff - ep approximations . for such training datasets containing several thousands of examples",
    ", we can afford larger mini - batches while keeping a sufficiently large number of mini - batches to take advantage of information redundancy in the data .",
    "this is motivated by the fact that larger mini - batches are presumably stronger learners .    in both datasets",
    ", we chose mini - batches of size  @xmath98 . since there are about 100  times more examples than in the `` small datasets '' , both the size and number of mini - batches were roughly 10  times larger than in the previous classification experiments .",
    "a single loop over mini - batches was performed . in this case",
    ", memory load can be substantially reduced as there is no need to keep the approximate factors in memory ; since they are initialized as uniform distributions , the cavity distribution can be updated on - the - fly as each mini - batch is visited only once .",
    "evolution curves of the same type as for the small dataset experiments are shown in figure  [ fig : large_datasets ] . in these experiments ,",
    "the offline minimizations were not computed due to prohibitive computational cost .",
    "both laplace - style methods proved more stable and more similar to vq in quasi 01 regression , while gq had a strong tendency to underestimate factor variances and quickly get stuck on a sub - optimal solution .",
    "the better behavior of laplace - based ff - ep can be explained by the larger number of factors ( mini - batches ) in these experiments , which helped stabilizing ff - ep updates , along with the fact that factors based on larger mini - batches tend to have more regular shapes . in the cases of logistic and hinge regression , convergence using either la , qla , or vq occurred much before",
    "all mini - batches were visited . for the quasi 01 loss , qla achieved a slightly lower cost than vq , but did not converge as smoothly as  vq .",
    "table  [ tab : small_datasets ] shows that vq ran slightly faster than qla for the bank marketing data ( @xmath99 ) , and about 23  times slower for the census income data ( @xmath100 ) , consistently with theory since the complexities of vq and qla are respectively in @xmath71 and @xmath101 .",
    "la was , again , very slow without showing better performance than qla and vq .",
    "we discussed and tested several methods to approximate ep  updates when analytically intractable : laplace - style approximations , standard gaussian quadrature and deterministic variational sampling , or variational quadrature .",
    "all methods are widely applicable as they only require that the factors can be evaluated .",
    "laplace - style methods further require evaluating the first- and second - order derivatives : using analytic expressions is generally preferred , but derivatives can also be approximated using finite differences , yielding the same complexity as the precision-3 quadrature rule considered here .",
    "experimental results confirm that the laplace - based approximation works better for ep than low - order gaussian quadrature , as previously reported by others @xcite , yet it is also significantly slower .",
    "much computation time can be saved by avoiding the optimization step in the laplace method , which does not seem to hamper performance in practice . however",
    ", both laplace - based ff - ep algorithms were found to be sometimes unstable with non - smooth distributions . while the laplace method is local in essence , both gaussian quadrature and variational quadrature perform factor approximations at a coarser scale adaptively determined by the ep  algorithm via the cavity distribution .",
    "this makes them , in principle , more robust to sudden changes in factor slope or curvature , which may confuse laplace - style methods while being irrelevant to the overall function approximation problem .",
    "the precision-3 gaussian quadrature rule proved , however , too inaccurate to enable good convergence of the ff - ep algorithm in practice .",
    "while this problem could potentially be overcome using higher order quadrature rules or monte carlo methods at the price of increased computation time , the refinement provided by variational quadrature without sampling additional points led to a dramatic improvement .",
    "the intuitive reason is that variational quadrature is designed to be exact for gaussian factors , and is thus generally more accurate than gaussian quadrature , which is only exact for constant factors ( see section  [ sec : gauss_quad ] ) .",
    "one drawback of variational quadrature is to scale unfavorably with parameter dimension .",
    "using newton s method in the fitting step , the complexity is in @xmath71 , while both quick laplace and gaussian quadrature are linear in  @xmath34 .",
    "a possibility to reduce the complexity to linear would be to use an approximate minimization strategy .",
    "however , in moderate parameter dimension ( @xmath102 ) , the tested implementation proved experimentally faster than the laplace method , which seems to be the current standard for numerical  ep @xcite , and showed an order of computation time comparable with both quick laplace and gaussian quadrature , two methods chosen for computational speed .    the practical recommendation suggested by",
    "this work is to use the quick laplace method in applications where the factors vary smoothly in  @xmath42 and speed is a requirement ; otherwise , variational quadrature seems to be preferable .",
    "also note that increasing the size of mini - batches is a way to reduce the computational overhead of variational quadrature with respect to quick laplace ."
  ],
  "abstract_text": [
    "<S> several numerical approximation strategies for the expectation - propagation algorithm are studied in the context of large - scale learning : the laplace method , a faster variant of it , gaussian quadrature , and a deterministic version of variational sampling ( _ i.e. _ , combining quadrature with variational approximation ) . </S>",
    "<S> experiments in training linear binary classifiers show that the expectation - propagation algorithm converges best using variational sampling , while it also converges well using laplace - style methods with smooth factors but tends to be unstable with non - differentiable ones . </S>",
    "<S> gaussian quadrature yields unstable behavior or convergence to a sub - optimal solution in most experiments . </S>"
  ]
}