{
  "article_text": [
    "we study degrees of freedom , or the `` effective number of parameters , '' in @xmath2-penalized linear regression problems .",
    "in particular , for a response vector @xmath3 , predictor matrix @xmath4 and tuning parameter @xmath5 , we consider the lasso problem [ @xcite , @xcite ] @xmath6 the above notation emphasizes the fact that the solution @xmath7 may not be unique [ such nonuniqueness can occur if @xmath8 . throughout the paper , when a function @xmath9 may have a nonunique minimizer over its domain @xmath1",
    ", we write @xmath10 to denote the set of minimizing @xmath11 values , that is , @xmath12 .",
    "a fundamental result on the degrees of freedom of the lasso fit was shown by @xcite .",
    "the authors show that if @xmath13 follows a  normal distribution with spherical covariance , @xmath14 , and @xmath15 are considered fixed with @xmath16 , then @xmath17 where @xmath18 denotes the active set of the unique lasso solution at @xmath13 , and  @xmath19 is its cardinality .",
    "this is quite a well - known result , and is sometimes used to informally justify an application of the lasso procedure , as it says that number of parameters used by the lasso fit is simply equal to the ( average ) number of selected variables .",
    "however , we note that the assumption @xmath16 implies that @xmath20 ; in other words , the degrees of freedom result ( [ eq : lassodffull ] ) does not cover the important `` high - dimensional '' case @xmath21 . in this case",
    ", the lasso solution is not necessarily unique , which raises the questions :    * can we still express degrees of freedom in terms of the active set of a lasso solution ? *",
    "if so , which active set ( solution ) would we refer to ?    in section  [ sec",
    ": lasso ] , we provide answers to these questions , by proving a stronger result when @xmath0 is a general predictor matrix . we show that the subspace spanned by the columns of @xmath0 in @xmath22 is almost surely unique , where `` almost surely '' means for almost every @xmath3 .",
    "furthermore , the degrees of freedom of the lasso fit is simply the expected dimension of this column space .",
    "we also consider the generalized lasso problem , @xmath23 where @xmath24 is a penalty matrix , and again the notation emphasizes the fact that @xmath7 need not be unique [ when @xmath8 .",
    "this of course reduces to the usual lasso problem ( [ eq : lasso ] ) when @xmath25 , and @xcite demonstrate that the formulation ( [ eq : genlasso ] ) encapsulates several other important problems  including the fused lasso on any graph and trend filtering of any order  by varying the penalty matrix @xmath1 .",
    "the same paper shows that if @xmath13 is normally distributed as above , and @xmath26 are fixed with @xmath16 , then the generalized lasso fit has degrees of freedom @xmath27.\\ ] ] here @xmath28 denotes the boundary set of an optimal subgradient to the generalized lasso problem at @xmath13 ( equivalently , the boundary set of a dual solution at @xmath13 ) , @xmath29 denotes the matrix @xmath1 after having removed the rows that are indexed by @xmath30 , and @xmath31 , the dimension of the null space of @xmath29 .",
    "it turns out that examining ( [ eq : genlassodffull ] ) for specific choices of @xmath1 produces a number of interpretable corollaries , as discussed in @xcite .",
    "for example , this result implies that the degrees of freedom of the fused lasso fit is equal to the expected number of fused groups , and that the degrees of freedom of the trend filtering fit is equal to the expected number of knots @xmath32 @xmath33 , where @xmath34 is the order of the polynomial . the result ( [ eq : genlassodffull ] ) assumes that @xmath16 and does not cover the case @xmath21 ; in section  [ sec : genlasso ] , we derive the degrees of freedom of the generalized lasso fit for a general @xmath0 ( and still a  general @xmath1 ) . as in the lasso case",
    ", we prove that there exists a linear subspace @xmath35 that is almost surely unique , meaning that it will be the same under different boundary sets @xmath30 corresponding to different solutions of ( [ eq : genlasso ] ) .",
    "the generalized lasso degrees of freedom is then the expected dimension of this subspace .",
    "our assumptions throughout the paper are minimal . as was already mentioned , we place no assumptions whatsoever on the predictor matrix @xmath36 or on the penalty matrix @xmath24 , considering them fixed and nonrandom .",
    "we also consider @xmath37 fixed . for theorems  [ thm : lassodfequi ] ,  [ thm : lassodfact ] and  [ thm : genlassodf ]",
    "we assume that @xmath13 is normally distributed , @xmath38 for some ( unknown ) mean vector @xmath39 and marginal variance @xmath40 .",
    "this assumption is only needed in order to apply stein s formula for degrees of freedom , and none of the other lasso and generalized lasso results in the paper , namely lemmas  [ lem : lassoproj ] through  [ lem : invbound ] , make any assumption about the distribution of @xmath13 .",
    "this paper is organized as follows .",
    "the rest of the introduction contains an overview of related work , and an explanation of our notation .",
    "section  [ sec : prelim ] covers some relevant background material on degrees of freedom and convex polyhedra .",
    "though the connection may not be immediately obvious , the geometry of polyhedra plays a large role in understanding problems ( [ eq : lasso ] ) and ( [ eq : genlasso ] ) , and section  [ sec : poly ] gives a high - level view of this geometry before the technical arguments that follow in sections  [ sec : lasso ] and  [ sec : genlasso ] . in section  [ sec : lasso ] , we derive two representations for the degrees of freedom of the lasso fit , given in theorems  [ thm : lassodfequi ] and  [ thm : lassodfact ] . in section  [ sec : genlasso ] , we derive the analogous results for the generalized lasso problem , and these are given in theorem  [ thm : genlassodf ] . as the lasso problem is a special case of the generalized lasso problem ( corresponding to @xmath25 ) , theorems  [ thm : lassodfequi ] and  [ thm : lassodfact ] can actually be viewed as corollaries of theorem  [ thm : genlassodf ] .",
    "the reader may then ask : why is there a separate section dedicated to the lasso problem ?",
    "we give two reasons : first , the lasso arguments are simpler and easier to follow than their generalized lasso counterparts ; second , we cover some intermediate results for the lasso problem that are interesting in their own right and that do not carry over to the generalized lasso perspective .",
    "section  [ sec : disc ] contains some final discussion .",
    "all of the degrees of freedom results discussed here assume that the response vector has distribution @xmath14 , and that the predictor matrix @xmath0 is fixed .",
    "to the best of our knowledge , @xcite were the first to prove a result on the degrees of freedom of the lasso fit , using the lasso solution path with @xmath41 moving from @xmath42 to @xmath43 .",
    "the authors showed that when the active set reaches size @xmath34 along this path , the lasso fit has degrees of freedom exactly @xmath34 .",
    "this result assumes that @xmath0 has full column rank and further satisfies a restrictive condition called the `` positive cone condition , '' which ensures that as @xmath41 decreases , variables can only enter , and not leave , the active set .",
    "subsequent results on the lasso degrees of freedom ( including those presented in this paper ) differ from this original result in that they derive degrees of freedom for a fixed value of the tuning parameter @xmath41 , and not a fixed number of steps @xmath34 taken along the solution path .",
    "as mentioned previously , @xcite established the basic lasso degrees of freedom result ( for fixed @xmath41 ) stated in ( [ eq : lassodffull ] ) .",
    "this is analogous to the path result of @xcite ; here degrees of freedom is equal to the expected size of the active set ( rather than simply the size ) because for a fixed @xmath41 the active set is a random quantity , and can hence achieve a random size .",
    "the proof of ( [ eq : lassodffull ] ) appearing in @xcite relies heavily on properties of the lasso solution path . as also mentioned previously , @xcite derived an extension of  ( [ eq : lassodffull ] ) to the generalized lasso problem , which is stated in ( [ eq : genlassodffull ] ) for an arbitrary penalty matrix @xmath1 .",
    "their arguments are not based on properties of the solution path , but instead come from a geometric perspective much like the one developed in this paper .",
    "both of the results ( [ eq : lassodffull ] ) and ( [ eq : genlassodffull ] ) assume that @xmath16 ; the current work extends these to the case of an arbitrary matrix @xmath0 , in theorems  [ thm : lassodfequi ] ,  [ thm : lassodfact ] ( the lasso ) and  [ thm : genlassodf ] ( the generalized lasso ) . in terms of our intermediate results ,",
    "a version of lemmas  [ lem : lcequi ] ,  [ lem : lcact ] corresponding to @xmath16 appears in @xcite , and a version of lemma  [ lem : lcbound ] corresponding to @xmath16 appears in @xcite [ furthermore , @xcite only consider the boundary set representation and not the active set representation ] .",
    "lemmas  [ lem : nonexp ] , [ lem : locaff ] and the conclusions thereafter , on the degrees of freedom of the projection map onto a convex polyhedron , are essentially given in @xcite , though these authors state and prove the results in a different manner .    in preparing a draft of this manuscript ,",
    "it was brought to our attention that other authors have independently and concurrently worked to extend results ( [ eq : lassodffull ] ) and ( [ eq : genlassodffull ] ) to the general @xmath0 case .",
    "namely , @xcite prove a result on the lasso degrees of freedom , and @xcite prove a  result on the generalized lasso degrees of freedom , both for an arbitrary @xmath0 .",
    "these authors results express degrees of freedom in terms of the active sets of special ( lasso or generalized lasso ) solutions .",
    "theorems  [ thm : lassodfact ] and  [ thm : genlassodf ] express degrees of freedom in terms of the active sets of any solutions , and hence the appropriate application of these theorems provides an alternative verification of these formulas .",
    "we discuss this in detail in the form of remarks following the theorems .      in this paper ,",
    "we use @xmath44 , @xmath45 and @xmath46 to denote the column space , row space and null space of a matrix @xmath47 , respectively ; we use @xmath48 and @xmath49 to denote the dimensions of @xmath44 [ equivalently , @xmath45 ] and @xmath46 , respectively .",
    "we write @xmath51 for the the moore ",
    "penrose pseudoinverse of @xmath47 ; for a rectangular matrix @xmath47 , recall that @xmath52 .",
    "we write @xmath53 to denote the projection matrix onto a linear subspace @xmath54 , and more generally , @xmath55 to denote the projection of a  point  @xmath11 onto a closed convex set @xmath56 . for readability ,",
    "we sometimes write @xmath57 ( instead of @xmath58 ) to denote the inner product between vectors @xmath59 and @xmath60 .    for a set of indices",
    "@xmath61 satisfying @xmath62 , and a vector @xmath63 , we use @xmath64 to denote the subvector @xmath65 . we denote the complementary subvector by @xmath66 .",
    "the notation is similar for matrices . given another subset of indices @xmath67 with @xmath68 , and a matrix @xmath69 , we use @xmath70 to denote the submatrix @xmath71 \\in{\\mathbb{r}}^{k \\times\\ell}.\\ ] ] in words , rows are indexed by @xmath72 , and columns are indexed by @xmath73 . when combining this notation with the transpose operation , we assume that the indexing happens first , so that @xmath74 . as above ,",
    "negative signs are used to denote the complementary set of rows or columns ; for example , @xmath75 . to extract only rows or only columns , we abbreviate the other dimension by a dot , so that @xmath76 and @xmath77 ; to extract a single row or column , we use @xmath78 or @xmath79 . finally , and most importantly , we introduce the following shorthand notation :    * for the predictor matrix @xmath80 , we let @xmath81 . * for the penalty matrix @xmath24 , we let @xmath82 .    in other words ,",
    "the default for @xmath0 is to index its columns , and the default for @xmath1 is to index its rows .",
    "this convention greatly simplifies the notation in expressions that involve multiple instances of @xmath83 or @xmath84 ; however , its use could also cause a great deal of confusion , if not properly interpreted by the reader !",
    "the following two sections describe some background material needed to follow the results in sections  [ sec : lasso ] and  [ sec : genlasso ] .",
    "if the data vector @xmath3 is distributed according to the homoskedastic model @xmath85 , meaning that the components of @xmath13 are uncorrelated , with @xmath86 having mean @xmath87 and variance @xmath88 for @xmath89 , then the degrees of freedom of a function @xmath90 with @xmath91 , is defined as @xmath92 this definition is often attributed to @xcite or @xcite , and is interpreted as the `` effective number of parameters '' used by the fitting procedure @xmath93 .",
    "note that for the linear regression fit of @xmath3 onto a fixed and full column rank predictor matrix @xmath36 , we have @xmath94 , and @xmath95 , which is the number of fitted coefficients ( one for each predictor variable )",
    ". furthermore , we can decompose the risk of @xmath96 , denoted by @xmath97 , as @xmath98 a well - known identity that leads to the derivation of the @xmath99 statistic [ @xcite ] . for a general fitting procedure",
    "@xmath93 , the motivation for the definition ( [ eq : df ] ) comes from the analogous decomposition of the quantity @xmath100 , @xmath101 therefore a large difference between risk and expected training error implies a large degrees of freedom .",
    "why is the concept of degrees of freedom important ?",
    "one simple answer is that it provides a way to put different fitting procedures on equal footing .",
    "for example , it would not seem fair to compare a procedure that uses an effective number of parameters equal to 100 with another that uses only 10 .",
    "however , assuming that these procedures can be tuned to varying levels of adaptivity ( as is the case with the lasso and generalized lasso , where the adaptivity is controlled by @xmath41 ) , one could first tune the procedures to have the same degrees of freedom , and then compare their performances .",
    "doing this over several common values for degrees of freedom may reveal , in an informal sense , that one procedure is particularly efficient when it comes to its parameter usage versus another .    a more detailed answer to the above question",
    "is based the risk decomposition  ( [ eq : riskd ] ) .",
    "the decomposition suggests that an estimate @xmath102 of degrees of freedom can be used to form an estimate of the risk , @xmath103 furthermore , it is straightforward to check that an unbiased estimate of degrees of freedom leads to an unbiased estimate of risk ; that is , @xmath104 $ ] implies @xmath105 $ ] .",
    "hence , the risk estimate ( [ eq : riskhat ] ) can be used to choose between fitting procedures , assuming that unbiased estimates of degrees of freedom are available .",
    "[ it is worth mentioning that bootstrap or monte carlo methods can be helpful in estimating degrees of freedom  ( [ eq : df ] ) when an analytic form is difficult to obtain . ]",
    "the natural extension of this idea is to use the risk estimate ( [ eq : riskhat ] ) for tuning parameter selection . if we suppose that @xmath93 depends on a tuning parameter @xmath106 , denoted @xmath107 , then in principle one could minimize the estimated risk over @xmath41 to select an appropriate value for the tuning parameter , @xmath108 this is a computationally efficient alternative to selecting the tuning parameter by cross - validation , and it is commonly used ( along with similar methods that replace the factor of @xmath109 above with a function of @xmath110 or @xmath111 ) in penalized regression problems . even though such an estimate ( [ eq : tunsel ] ) is commonly used in the high - dimensional setting ( @xmath21 ) , its asymptotic properties are largely unknown in this case , such as risk consistency , or relatively efficiency compared to the cross - validation estimate .",
    "@xcite proposed the risk estimate ( [ eq : riskhat ] ) using a particular unbiased estimate of degrees of freedom , now commonly referred to as _ stein s unbiased risk estimate _ ( sure ) .",
    "stein s framework requires that we strengthen our distributional assumption on @xmath13 and assume normality , as stated in ( [ eq : normal ] ) .",
    "we also assume that the function @xmath93 is continuous and almost differentiable .",
    "( the precise definition of almost differentiability is not important here , but the interested reader may take it to mean that each coordinate function  @xmath112 is absolutely continuous on almost every line segment parallel to one of the coordinate axes . ) given these assumptions , stein s main result is an alternate expression for degrees of freedom , @xmath113,\\ ] ] where the function @xmath114 is called the divergence of @xmath93 .",
    "immediately following is the unbiased estimate of degrees of freedom , @xmath115 we pause for a moment to reflect on the importance of this result . from its definition ( [ eq : df ] ) , we can see that the two most obvious candidates for unbiased estimates of degrees of freedom are @xmath116 \\bigr ) y_i.\\ ] ] to use the first estimate above , we need to know @xmath117 ( remember , this is ultimately what we are trying to estimate ! ) . using the second requires knowing @xmath118 $ ] , which is equally impractical because this invariably depends on @xmath117 .",
    "on the other hand , stein s unbiased estimate ( [ eq : steindfhat ] ) does not have an explicit dependence on @xmath117 ; moreover , it can be analytically computed for many fitting procedures @xmath93 .",
    "for example , theorem  [ thm : lassodfact ] in section  [ sec : lasso ] shows that , except for @xmath13 in a set of measure zero , the divergence of the lasso fit is equal to @xmath119 with @xmath18 being the active set of a lasso solution at @xmath13 .",
    "hence , stein s formula allows for the unbiased estimation of degrees of freedom ( and subsequently , risk ) for a broad class of fitting procedures @xmath93something that may have not seemed possible when working from the definition directly .",
    "a set @xmath120 is called a _ convex polyhedron _ , or simply a _",
    "polyhedron _ , if @xmath56 is the intersection of finitely many half - spaces , @xmath121 where @xmath122 and @xmath123 .",
    "( note that we do not require boundedness here ; a bounded polyhedron is sometimes called a polytope . )",
    "see figure  [ fig : poly ] for an example . there is a rich theory on polyhedra ; the definitive reference is @xcite , and another good reference is @xcite . as this is a paper on statistics and not geometry , we do not attempt to give an extensive treatment of the properties of polyhedra .",
    "we do , however , give two properties ( in the form of two lemmas ) that are especially important with respect to our statistical problem ; our discussion will also make it clear why polyhedra are relevant in the first place .    .",
    "_ _ ]    from its definition ( [ eq : poly ] ) , it follows that a polyhedron is a closed convex set . the first property that we discuss does",
    "not actually rely on the special structure of polyhedra , but only on convexity .",
    "for any closed convex set",
    "@xmath120 and any point @xmath124 , there is a unique point @xmath125 minimizing @xmath126 . to see this , note that if @xmath127 is another minimizer , @xmath128 , then by convexity @xmath129 , and @xmath130 , a  contradiction .",
    "therefore , the projection map onto @xmath56 is indeed well defined , and we write this as @xmath131 , @xmath132 for the usual linear regression problem , where @xmath3 is regressed onto @xmath36 , the fit @xmath133 can be written in terms of the projection map onto the polyhedron @xmath134 , as in @xmath135 . furthermore , for both the lasso and generalized lasso problems , ( [ eq : lasso ] ) and ( [ eq : genlasso ] ) , it turns out that we can express the fit as the residual from projecting onto a suitable polyhedron @xmath136 , that is , @xmath137 this is proved in lemma  [ lem : lassoproj ] for the lasso and in lemma  [ lem : genlassoproj ] for the generalized lasso ( the polyhedron @xmath56 depends on @xmath15 for the lasso case , and on @xmath26 for the generalized lasso case ) .",
    "our first lemma establishes that both the projection map onto a closed convex set and the residual map are nonexpansive , hence continuous and almost differentiable everywhere .",
    "these are the conditions needed to apply stein s formula .",
    "[ lem : nonexp ] for any closed convex set @xmath120 , both the projection map @xmath138 and the residual projection map @xmath139 are nonexpansive .",
    "that is , they satisfy @xmath140 therefore , @xmath141 and @xmath142 are both continuous and almost differentiable .",
    "the proof can be found in appendix  [ app : nonexp ] .",
    "lemma  [ lem : nonexp ] will be quite useful later in the paper , as it will allow us to use stein s formula to compute the degrees of freedom of the lasso and generalized lasso fits , after showing that these fits are indeed the residuals from projecting onto closed convex sets .    the second property that we discuss uses the structure of polyhedra .",
    "unlike lemma  [ lem : nonexp ] , this property will not be used directly in the following sections of the paper ; instead , we present it here to give some intuition with respect to the degrees of freedom calculations to come .",
    "the property can be best explained by looking back at figure  [ fig : poly ] .",
    "loosely speaking , the picture suggests that we can move the point @xmath11 around a bit and it will still project to the same face of @xmath56 .",
    "another way of saying this is that there is a neighborhood of @xmath11 on which @xmath141 is simply the projection onto an affine subspace",
    ". this would not be true if @xmath11 is in some exceptional set , which is made up of rays that emanate from the corners of @xmath56 , like the two drawn in the bottom right corner of figure .",
    "however , the union of such rays has measure zero , so the map @xmath141 is locally an affine projection , almost everywhere .",
    "this idea can be stated formally as follows .",
    "[ lem : locaff ] let @xmath120 be a polyhedron . for almost every @xmath143 , there is an associated neighborhood @xmath144 of @xmath11 , linear subspace @xmath145 and point @xmath146 , such that the projection map restricted to @xmath144 , @xmath147 , is @xmath148 which is simply the projection onto the affine subspace @xmath149 .    the proof is given in appendix  [ app : locaff ] . these last two properties can be used to derive a general expression for the degrees of freedom of the fitting procedure @xmath150 , when @xmath120 is a polyhedron .",
    "[ a similar formula holds for @xmath151 .",
    "] lemma  [ lem : nonexp ] tells us that @xmath142 is continuous and almost differentiable , so we can use stein s formula ( [ eq : steindf ] ) to compute its degrees of freedom .",
    "lemma  [ lem : locaff ] tells us that for almost every @xmath152 , there is a neighborhood @xmath144 of @xmath13 , linear subspace @xmath153 , and point @xmath146 , such that @xmath154 therefore , @xmath155 and an expectation over @xmath13 gives @xmath156.\\ ] ] it should be made clear that the random quantity in the above expectation is the linear subspace @xmath157 , which depends on @xmath13 .    in a sense ,",
    "the remainder of this paper is focused on describing @xmath158the dimension of the face of @xmath56 onto which the point @xmath13 projects  in a meaningful way for the lasso and generalized lasso problems .",
    "section  [ sec : lasso ] considers the lasso problem , and we show that @xmath54 can be written in terms of the equicorrelation set of the fit at @xmath13 . we also show that @xmath54 can be described in terms of the active set of a solution at @xmath13 . in section  [ sec : genlasso ] we show the analogous results for the generalized lasso problem , namely , that @xmath54 can be written in terms of either the boundary set of an optimal subgradient at @xmath13 ( the analogy of the equicorrelation set for the lasso ) or the active set of a solution at @xmath13 .",
    "in this section we derive the degrees of freedom of the lasso fit , for a general predictor matrix @xmath0 .",
    "all of our arguments stem from the karush ",
    "tucker ( kkt ) optimality conditions , and we present these first .",
    "we note that many of the results in this section can be alternatively derived using the lasso dual problem .",
    "appendix  [ app : dual ] explains this connection more precisely .",
    "for the current work , we avoid the dual perspective simply to keep the presentation more self - contained .",
    "finally , we remind the reader that @xmath83 is used to extract columns of @xmath0 corresponding to an index set  @xmath73.=1      the kkt conditions for the lasso problem ( [ eq : lasso ] ) can be expressed as @xmath159 & \\quad if $ { { \\hat{\\beta}}}_i = 0$. } & \\ ] ] here @xmath160 is a subgradient of the function @xmath161 evaluated at @xmath162 .",
    "hence @xmath7 is a minimizer in ( [ eq : lasso ] ) if and only if @xmath7 satisfies ( [ eq : lassokkt ] ) and ( [ eq : lassosg ] ) for some  @xmath163 . directly from the kkt conditions , we can show that @xmath133 is the residual from projecting @xmath13 onto a polyhedron .",
    "[ lem : lassoproj ] for any @xmath0 and @xmath5 , the lasso fit @xmath133 can be written as @xmath164 , where @xmath165 is the polyhedron @xmath166    given a point @xmath3 , its projection @xmath167 onto a closed convex set @xmath120 can be characterized as the unique point satisfying @xmath168 hence defining @xmath169 , and @xmath56 as in the lemma , we want to show that  ( [ eq : opt ] ) holds for all @xmath125 .",
    "well , @xmath170 \\\\[-8pt ] & = & \\langle x{{\\hat{\\beta } } } , y - x{{\\hat{\\beta}}}\\rangle - \\langle x{^t}u , { { \\hat{\\beta}}}\\rangle.\\nonumber\\end{aligned}\\ ] ] consider the first term above . taking an inner product with @xmath7 on both sides of ( [ eq : lassokkt ] )",
    "gives @xmath171 .",
    "furthermore , the @xmath2 norm can be characterized in terms of its dual norm , the @xmath172 norm , as in @xmath173 therefore , continuing from ( [ eq : innerprod ] ) , we have @xmath174 which is @xmath175 for all @xmath125 , and we have hence proved that @xmath176 . to show that @xmath56 is indeed a polyhedron , note that it can be written  as @xmath177 which is a finite intersection of half - spaces",
    ".    showing that the lasso fit is the residual from projecting @xmath13 onto a polyhedron is important , because it means that @xmath178 is nonexpansive as a  function of @xmath13 , and hence continuous and almost differentiable , by lemma  [ lem : nonexp ] .",
    "this establishes the conditions that are needed to apply stein s formula for degrees of freedom .    in the next section ,",
    "we define the equicorrelation set @xmath179 , and show that the lasso fit and solutions both have an explicit form in terms of @xmath179 . following this",
    ", we derive an expression for the lasso degrees of freedom as a function of the equicorrelation set .      according to lemma  [ lem : lassoproj ] ,",
    "the lasso fit @xmath133 is always unique ( because projection onto a closed convex set is unique ) .",
    "therefore , even though the solution @xmath7 is not necessarily unique , the optimal subgradient @xmath163 is unique , because it can be written entirely in terms of @xmath133 , as shown by ( [ eq : lassokkt ] ) . we define the unique _ equicorrelation set _",
    "@xmath179 as @xmath180 an alternative definition for the equicorrelation set is @xmath181 which explains its name , as @xmath179 can be thought of as the set of variables that have equal and maximal absolute inner product ( or correlation for standardized variables ) with the residual .",
    "the set @xmath179 is a natural quantity to work with because we can express the lasso fit and the set of lasso solutions in terms of @xmath179 , by working directly from equation  ( [ eq : lassokkt ] ) .",
    "first we let @xmath182 the signs of the inner products of the equicorrelation variables with the residual .",
    "since @xmath183 by definition of the subgradient , the @xmath179 block of the kkt conditions can be rewritten as @xmath184 because @xmath185 , we can write @xmath186 , so rearranging ( [ eq : lassokkt2 ] ) we get @xmath187 therefore , the lasso fit @xmath188 is @xmath189 and any lasso solution must be of the form @xmath190 where @xmath191 . in the case",
    "that @xmath192for example , this holds if @xmath16the lasso solution is unique and is given by ( [ eq : lassosol ] ) with @xmath193 .",
    "but in general , when @xmath194 , it is important to note that not every @xmath191 necessarily leads to a lasso solution in ( [ eq : lassosol ] ) ; the vector @xmath60 must also preserve the signs of the nonzero coefficients ; that is , it must also satisfy @xmath195_i + b_i",
    "\\bigr ) = s_i\\nonumber \\\\[-8pt ] \\\\[-8pt ] \\eqntext{\\mbox{for each $ i$ such that } \\bigl[(x_{\\mathcal{e}})^+ \\bigl(y - ( x_{\\mathcal{e}}{^t})^+ \\lambda s \\bigr ) \\bigr]_i + b_i \\not= 0.}\\end{aligned}\\ ] ] otherwise , @xmath163 would not be a proper subgradient of @xmath196 .",
    "using relatively simple arguments , we can derive a result on the lasso degrees of freedom in terms of the equicorrelation set .",
    "our arguments build on the following key lemma .",
    "[ lem : lassosol0 ] for any @xmath197 and @xmath5 , a lasso solution is given by @xmath198 where @xmath179 and @xmath199 are the equicorrelation set and signs , as defined in  ( [ eq : equiset ] ) and  ( [ eq : equisigns ] ) .",
    "in other words , lemma  [ lem : lassosol0 ] says that the sign condition ( [ eq : lassosign ] ) is always satisfied by taking @xmath193 , regardless of the rank of @xmath0 .",
    "this result is inspired by the lars work of @xcite , though it is not proved in the lars paper ; see appendix  b of @xcite for a proof .    next",
    "we show that , almost everywhere in @xmath13 , the equicorrelation set and signs are locally constant functions of @xmath13 .",
    "to emphasize their functional dependence on @xmath13 , we write them as @xmath200 and @xmath201 .    [ lem : lcequi ] for almost every @xmath3 , there exists a neighborhood @xmath144 of @xmath13 such that @xmath202 and @xmath203 for all @xmath204 .",
    "define @xmath205_{(i,\\cdot ) } \\bigl(z - ( x_{\\mathcal{e}}{^t})^+ \\lambda s \\bigr ) = 0 \\bigr\\},\\ ] ] where the first union above is taken over all subsets @xmath206 and sign vectors @xmath207 , but we exclude sets @xmath179 for which a row of @xmath208 is entirely zero .",
    "the set @xmath209 is a finite union of affine subspaces of dimension @xmath210 , and therefore has measure zero .",
    "let @xmath211 , and abbreviate the equicorrelation set and signs as @xmath212 and @xmath213",
    ". we may assume no row of @xmath208 is entirely zero .",
    "( otherwise , this implies that @xmath214 has a zero column , which implies that @xmath215 , a trivial case for this lemma . ) therefore , as @xmath211 , this means that the lasso solution given in ( [ eq : lassosol0 ] ) satisfies @xmath216 for every @xmath217 .",
    "now , for a new point @xmath218 , consider defining @xmath219 we need to verify that this is indeed a solution at @xmath218 , and that the corresponding fit has equicorrelation set @xmath179 and signs @xmath199 .",
    "first notice that , after a  straightforward calculation , @xmath220 also , by the continuity of the function @xmath221 , @xmath222 there exists a neighborhood @xmath223 of @xmath13 such that @xmath224 for all @xmath225 .",
    "hence @xmath226 has equicorrelation set @xmath227 and signs .    to check that @xmath228 is a lasso solution at @xmath218",
    ", we consider the function @xmath229 , @xmath230 the continuity of @xmath93 implies that there exists a neighborhood @xmath231 of @xmath13 such that @xmath232_i \\not= 0 \\qquad\\mbox{for } i \\in{\\mathcal{e } } , \\quad\\mbox{and } \\\\ { \\operatorname{sign}}({{\\hat{\\beta}}}_{\\mathcal{e}}(y ' ) ) & = & { \\operatorname{sign}}\\bigl((x_{\\mathcal{e}})^+ \\bigl(y ' - ( x_{\\mathcal{e}}{^t})^+ \\lambda s \\bigr ) \\bigr)\\end{aligned}\\ ] ] for each @xmath233 .",
    "defining @xmath234 completes the proof .",
    "this immediately implies the following theorem .",
    "[ thm : lassodfequi ] assume that @xmath13 follows a normal distribution ( [ eq : normal ] ) . for any @xmath0 and @xmath5 , the lasso fit @xmath133 has degrees of freedom @xmath235,\\ ] ] where @xmath212 is the equicorrelation set of the lasso fit at @xmath13 .    by lemmas  [ lem : nonexp ] and  [ lem : lassoproj ] we know that @xmath178 is continuous and almost differentiable , so we can use stein s formula ( [ eq : steindf ] ) for degrees of freedom .",
    "by lemma  [ lem : lcequi ] , we know that @xmath212 and @xmath213 are locally constant for all @xmath211 . therefore , taking the divergence of the fit in ( [ eq : lassofit ] ) , we get @xmath236 taking an expectation over @xmath13 ( and recalling that @xmath209 has measure zero ) gives the result .    next",
    ", we shift our focus to a different subset of variables : the active set  @xmath22 .",
    "unlike the equicorrelation set , the active set is not unique , as it depends on a  particular choice of lasso solution .",
    "though it may seem that such nonuniqueness could present complications , it turns out that all of the active sets share a special property ; namely , the linear subspace @xmath237 is the same for any choice of active set @xmath22 , almost everywhere in @xmath13 .",
    "this invariance allows us to express the degrees of freedom of lasso fit in terms of the active set ( or , more precisely , any active set ) .",
    "given a particular solution @xmath7 , we define the _ active set _ @xmath22 as @xmath238 this is also called the support of @xmath7 and written @xmath239 . from ( [ eq : lassosol ] ) , we can see that we always have @xmath240 , and different active sets @xmath22 can be formed by choosing @xmath191 to satisfy the sign condition ( [ eq : lassosign ] ) and also @xmath241_i + b_i = 0 \\qquad\\mbox{for } i \\notin{{\\mathcal{a}}}.\\ ] ] if @xmath16 , then @xmath193 , so there is a unique active set , and furthermore @xmath242 for almost every @xmath3 ( in particular , this last statement holds for , where @xmath209 is the set of measure zero set defined in the proof of lemma  [ lem : lcequi ] ) . for the signs of the coefficients of active variables , we write @xmath243 and we note that @xmath244 . by similar arguments as those used to derive expression ( [ eq : lassofit ] ) for the fit in section  [ sec : lassoequi ] , the lasso fit can also be written as @xmath245 for the active set @xmath22 and signs @xmath246 of any lasso solution @xmath7 .",
    "if we could take the divergence of the fit in the expression above , and simply ignore the dependence of @xmath22 and @xmath246 on @xmath13 ( treat them as constants ) , then this would give @xmath247 .",
    "in the next section , we show that treating  @xmath22 and  @xmath246 as constants in ( [ eq : lassofit2 ] ) is indeed correct , for almost every @xmath13 .",
    "this property then implies that the linear subspace @xmath237 is invariant under any choice of active set @xmath22 , almost everywhere in @xmath13 ; moreover , it implies that we can write the lasso degrees of freedom in terms of any active set .",
    "we first establish a  result on the local stability of @xmath248 and @xmath249 [ written in this way to emphasize their dependence on @xmath13 , through a solution @xmath250 .",
    "[ lem : lcact ] there is a set @xmath251 , of measure zero , with the following property : for @xmath252 , and for any lasso solution @xmath253 with active set @xmath248 and signs @xmath249 , there is a neighborhood @xmath144 of @xmath13 such that every point @xmath204 yields a lasso solution @xmath228 with the same active set @xmath254 and the same active signs @xmath255 .",
    "the proof is similar to that of lemma  [ lem : lcequi ] , except that it is longer and somewhat more complicated , so it is delayed until appendix  [ app : lcact ] .",
    "combined with expression ( [ eq : lassofit2 ] ) for the lasso fit , lemma  [ lem : lcact ] now implies an invariance of the subspace spanned by the active variables .",
    "[ lem : invact ] for the same set @xmath251 as in lemma  [ lem : lcact ] , and for any @xmath252 , the linear subspace @xmath237 is invariant under all sets @xmath18 defined in terms of a lasso solution @xmath253 at @xmath13 .",
    "let @xmath252 , and let @xmath253 be a solution with active set @xmath18 and signs @xmath256 .",
    "let @xmath144 be the neighborhood of @xmath13 as constructed in the proof of lemma  [ lem : lcact ] ; on this neighborhood , solutions exist with active set @xmath22 and signs @xmath246 .",
    "hence , recalling ( [ eq : lassofit2 ] ) , we know that for every @xmath257 , @xmath258 now suppose that @xmath259 and @xmath260 are the active set and signs of another lasso solution at @xmath13",
    ". then , by the same arguments , there is a neighborhood @xmath261 of  @xmath13 such that @xmath262 for all @xmath263 . by the uniqueness of the fit",
    ", we have that for each @xmath264 , @xmath265 since @xmath266 is open , for any @xmath267 , there is an @xmath268 such that @xmath269 . plugging @xmath270 into the above equation",
    "implies that @xmath271 , so @xmath272 .",
    "a similar argument gives @xmath273 , completing the proof .",
    "again , this immediately leads to the following theorem .",
    "[ thm : lassodfact ] assume that @xmath13 follows a normal distribution ( [ eq : normal ] ) . for any @xmath0 and @xmath5 , the lasso fit @xmath133 has degrees of freedom @xmath274,\\ ] ] where @xmath18 is the active set corresponding to any lasso solution @xmath253 at  @xmath13 .",
    "note : by lemma  [ lem : invact ] , @xmath119 is an invariant quantity , not depending on the choice of active set ( coming from a lasso solution ) , for almost every @xmath13 .",
    "this makes the above result well defined .",
    "proof of theorem  [ thm : lassodfact ] we can apply stein s formula ( [ eq : steindf ] ) for degrees of freedom , because @xmath178 is continuous and almost differentiable by lemmas  [ lem : nonexp ] and  [ lem : lassoproj ] . let @xmath18 and @xmath256 be the active set and active signs of a lasso solution at @xmath252 , with @xmath275 as in lemma  [ lem : invact ] . by this same lemma , there exists a lasso solution with active set @xmath22 and signs @xmath246 at every point @xmath218 in some neighborhood @xmath144 of @xmath13 , and therefore , taking the divergence of the fit  ( [ eq : lassofit2 ] ) , we get @xmath276 taking an expectation over @xmath13 completes the proof .    the proof of lemma  [ lem : lcact ] showed that , for almost every @xmath13 , the equicorrelation set @xmath179 is actually the active set @xmath22 of the particular lasso solution defined in ( [ eq : lassosol0 ] ) .",
    "hence theorem  [ thm : lassodfequi ] can be viewed as a corollary of theorem  [ thm : lassodfact ] .    when @xmath16 , the lasso solution is unique , and there is only one active set @xmath22 . and as the columns of @xmath0 are linearly independent , we have @xmath277 , so the result of theorem  [ thm : lassodfact ] reduces to @xmath278 as shown in @xcite .",
    "an interesting result on the lasso degrees of freedom was recently and independently obtained by @xcite .",
    "their result states that , for a general @xmath0 , @xmath279 where @xmath280 is the smallest cardinality among all active sets of lasso solutions .",
    "this actually follows from theorem  [ thm : lassodfact ] , by noting that for any @xmath13 there exists a lasso solution whose active set @xmath259 corresponds to linear independent predictors @xmath281 , so @xmath282 [ e.g. , see theorem 3 in appendix  b of @xcite ] , and furthermore , for almost every @xmath13 no active set can have a cardinality smaller than @xmath280 , as this would contradict lemma  [ lem : invact ] .",
    "consider the elastic net problem [ @xcite ] , @xmath283 where we now have two tuning parameters @xmath284 .",
    "note that our notation above emphasizes the fact that there is always a unique solution to the elastic net criterion , regardless of the rank of @xmath0 .",
    "this property ( among others , such as stability and predictive ability ) is considered an advantage of the elastic net over the lasso .",
    "we can rewrite the elastic net problem ( [ eq : enet ] ) as a ( full column rank ) lasso problem , @xmath285 \\beta \\right\\|_{2}^2 + \\lambda_1 \\|\\beta\\|_{1},\\ ] ] and hence it can be shown ( although we omit the details ) that the degrees of freedom of the elastic net fit is @xmath286,\\ ] ] where @xmath18 is the active set of the elastic net solution at @xmath13 .",
    "it is often more appropriate to include an ( unpenalized ) intercept coefficient in the lasso model , yielding the problem @xmath287 where @xmath288 is the vector of all @xmath289s . defining @xmath290",
    ", we note that the fit of problem ( [ eq : lassoint ] ) can be written as @xmath291 , and that @xmath7 solves the usual lasso problem @xmath292 now it follows ( again we omit the details ) that the fit of the lasso problem with intercept ( [ eq : lassoint ] ) has degrees of freedom @xmath293,\\ ] ] where @xmath18 is the active set of a solution @xmath253 at @xmath13 ( these are the nonintercept coefficients ) .",
    "in other words , the degrees of freedom is one plus the expected dimension of the subspace spanned by the active variables , once we have centered these variables .",
    "a similar result holds for an arbitrary set of unpenalized coefficients , by replacing @xmath294 above with the projection onto the orthogonal complement of the column space of the unpenalized variables , and @xmath289 above with the dimension of the column space of the unpenalized variables .    as mentioned in the , a nice feature of the full column rank result ( [ eq : lassodffull ] )",
    "is its interpretability and its explicit nature .",
    "the general result is also explicit in the sense that an unbiased estimate of degrees of freedom can be achieved by computing the rank of a given matrix . in terms of interpretability , when @xmath16 , the degrees of freedom of the lasso fit is @xmath295this says that , on average , the lasso `` spends '' the same number of parameters as does linear regression on @xmath19 linearly independent predictor variables .",
    "fortunately , a similar interpretation is possible in the general case : we showed in theorem  [ thm : lassodfact ] that for a general predictor matrix @xmath0 , the degrees of freedom of the lasso fit is @xmath296 $ ] , the expected dimension of the linear subspace spanned by the active variables .",
    "meanwhile , for the linear regression problem @xmath297 where we consider @xmath22 fixed , the degrees of freedom of the fit is @xmath298 . in other words ,",
    "the lasso adaptively selects a subset @xmath22 of the variables to use for a linear model of @xmath13 , but on average it only `` spends '' the same number of parameters as would linear regression on the variables in @xmath22 , if @xmath22 was pre - specified .",
    "how is this possible ? broadly speaking",
    ", the answer lies in the shrinkage due to the @xmath2 penalty . although the active set is chosen adaptively , the lasso does not estimate the active coefficients as aggressively as does the corresponding linear regression problem ( [ eq : lsa ] ) ; instead , they are shrunken toward zero , and this adjusts for the adaptive selection .",
    "differing views have been presented in the literature with respect to this feature of lasso shrinkage . on the one hand , for example , @xcite point out that lasso estimates suffer from bias due to the shrinkage of large coefficients , and motivate the nonconvex _ scad _ penalty as an attempt to overcome this bias . on the other hand , for example",
    ", loubes and massart ( @xcite ) discuss the merits of such shrunken estimates in model selection criteria , such as  ( [ eq : tunsel ] ) . in the current context ,",
    "the shrinkage due to the @xmath2 penalty is helpful in that it provides control over degrees of freedom . a more precise study of this idea is the topic of future work .",
    "in this section we extend our degrees of freedom results to the generalized lasso problem , with an arbitrary predictor matrix @xmath0 and penalty matrix @xmath1 . as before ,",
    "the kkt conditions play a  central role , and we present these first . also , many results that follow have equivalent derivations from the perspective of the generalized lasso dual problem ; see appendix  [ app : dual ] .",
    "we remind the reader that @xmath84 is used to extract to extract rows of @xmath1 corresponding to an index set @xmath72 .      the kkt conditions for the generalized lasso problem ( [ eq : genlasso ] ) are @xmath299 & \\quad   if $ ( d{{\\hat{\\beta}}})_i = 0$. } & \\ ] ] now @xmath300 is a subgradient of the function @xmath301 evaluated at @xmath302 .",
    "similar to what we showed for the lasso , it follows from the kkt conditions that the generalized lasso fit is the residual from projecting @xmath13 onto a polyhedron .",
    "[ lem : genlassoproj ] for any @xmath0 and @xmath5 , the generalized lasso fit can be written as @xmath164 , where @xmath165 is the polyhedron @xmath303    the proof is quite similar to that of lemma  [ lem : lassoproj ] .",
    "as in ( [ eq : innerprod ] ) , we want to show that @xmath304 for all @xmath125 , where @xmath56 is as in the lemma . for the first term above",
    ", we can take an inner product with @xmath7 on both sides of ( [ eq : genlassokkt ] ) to get @xmath305 , and furthermore , @xmath306 therefore ( [ eq : innerprod2 ] ) holds if @xmath307 for some @xmath308 , in other words , if @xmath309 . to show that @xmath56 is a polyhedron , note that we can write it as @xmath310 where @xmath311 is taken to mean the inverse image under the linear map @xmath312 , and @xmath313 , a hypercube in @xmath314 .",
    "clearly  @xmath315 is a polyhedron , and the image or inverse image of a  polyhedron under a  linear map is still a polyhedron .    as with the lasso",
    ", this lemma implies that the generalized lasso fit @xmath178 is nonexpansive , and therefore continuous and almost differentiable as a  function of @xmath13 , by lemma  [ lem : nonexp ] .",
    "this is important because it allows us to use stein s formula when computing degrees of freedom .    in the next section",
    "we define the boundary set @xmath30 , and derive expressions for the generalized lasso fit and solutions in terms of @xmath30 .",
    "the following section defines the active set @xmath22 in the generalized lasso context , and again gives expressions for the fit and solutions in terms of @xmath22 .",
    "though neither @xmath30 nor  @xmath22 are necessarily unique for the generalized lasso problem , any choice of  @xmath30 or  @xmath22 generates a special invariant subspace ( similar to the case for the active sets in the lasso problem ) .",
    "we are subsequently able to express the degrees of freedom of the generalized lasso fit in terms of any boundary set  @xmath30 , or any active set @xmath22 .      like the lasso ,",
    "the generalized lasso fit @xmath133 is always unique ( following from lemma  [ lem : genlassoproj ] , and the fact that projection onto a  closed convex set is unique ) .",
    "however , unlike the lasso , the optimal subgradient @xmath163 in the generalized lasso problem is not necessarily unique . in particular , if @xmath316 , then the optimal subgradient @xmath163 is not uniquely determined by conditions ( [ eq : genlassokkt ] ) and ( [ eq : genlassosg ] ) . given a subgradient @xmath163 satisfying  ( [ eq : genlassokkt ] ) and ( [ eq : genlassosg ] ) for some @xmath7 , we define the _ boundary set _ @xmath30 as @xmath317 this generalizes the notion of the equicorrelation set @xmath179 in the lasso problem [ though , as just noted , the set @xmath30 is not necessarily unique unless ] .",
    "we also define @xmath318    now we focus on writing the generalized lasso fit and solutions in terms of  @xmath30 and  @xmath199 . abbreviating @xmath319 , note that we can expand @xmath320 .",
    "therefore , multiplying both sides of ( [ eq : genlassokkt ] ) by @xmath321 yields @xmath322 since @xmath323 , we can write @xmath324 . also , we have @xmath325 by definition of @xmath30 , so @xmath326 .",
    "these two facts allow us to rewrite ( [ eq : genlassokktb ] ) as @xmath327 and hence the fit @xmath328 is @xmath329 where we have un - abbreviated @xmath319 .",
    "further , any generalized lasso solution is of the form @xmath330 where @xmath331 .",
    "multiplying the above equation by @xmath29 , and recalling that @xmath325 , reveals that @xmath332 ; hence @xmath333 . in the case that @xmath334 , the generalized lasso solution is unique and is given by ( [ eq : genlassosol ] ) with @xmath193 .",
    "this occurs when @xmath16 , for example .",
    "otherwise , any @xmath335 gives a generalized lasso solution in ( [ eq : genlassosol ] ) as long as it also satisfies the sign condition @xmath336 necessary to ensure that @xmath163 is a proper subgradient of @xmath337 .",
    "we define the _ active set _ of a particular solution  @xmath7  as @xmath338 which can be alternatively expressed as @xmath339 .",
    "if @xmath7 corresponds to a subgradient with boundary set @xmath30 and signs @xmath199 , then @xmath340 ; in particular , given @xmath30 and @xmath199 , different active sets @xmath22 can be generated by taking @xmath341 such that ( [ eq : genlassosign ] ) is satisfied , and also @xmath342 if @xmath16 , then @xmath193 , and there is only one active set @xmath22 ; however , in this case , @xmath22 can still be a strict subset of @xmath30 .",
    "this is quite different from the lasso problem , wherein @xmath242 for almost every @xmath13 whenever @xmath16 .",
    "[ note that in the generalized lasso problem , @xmath16 implies that @xmath22 is unique but implies nothing about the uniqueness of @xmath30this is determined by the rank of @xmath1 .",
    "the boundary set @xmath30 is not necessarily unique if @xmath316 , and in this case we may have @xmath343 for some @xmath344 , which certainly implies that @xmath345 for any @xmath3 .",
    "hence some boundary sets may not correspond to active sets at any  @xmath13 .",
    "] we denote the signs of the active entries in @xmath346 by @xmath347 and we note that @xmath244 .",
    "following the same arguments as those leading up to the expression for the fit ( [ eq : genlassofit ] ) in section  [ sec : genlassobound ] , we can alternatively express the generalized lasso fit as @xmath348 where @xmath22 and @xmath246 are the active set and signs of any solution .",
    "computing the divergence of the fit in ( [ eq : genlassofit2 ] ) , and pretending that @xmath22 and @xmath246 are constants ( not depending on @xmath13 ) , gives @xmath349 . the same logic applied to ( [ eq : genlassofit ] ) gives @xmath350 .",
    "the next section shows that , for almost every @xmath13 , the quantities @xmath351 or @xmath352 can indeed be treated as locally constant in expressions ( [ eq : genlassofit2 ] ) or ( [ eq : genlassofit ] ) , respectively .",
    "we then prove that linear subspaces @xmath353 are invariant under all choices of boundary sets @xmath30 , respectively active sets @xmath22 , and that the two subspaces are in fact equal , for almost every @xmath13 .",
    "furthermore , we express the generalized lasso degrees of freedom in terms of any boundary set or any active set .",
    "we call @xmath354 an _ optimal pair _ provided that @xmath355 and @xmath253 jointly satisfy the kkt conditions , ( [ eq : genlassokkt ] ) and ( [ eq : genlassosg ] ) , at @xmath13 .",
    "for such a pair , we consider its boundary set @xmath356 , boundary signs @xmath201 , active set @xmath248 , active signs @xmath249 , and show that these sets and sign vectors possess a kind of local stability .",
    "[ lem : lcbound ] there exists a set @xmath357 , of measure zero , with the following property : for @xmath211 , and for any optimal pair @xmath354 with boundary set @xmath356 , boundary signs @xmath201 , active set @xmath248 , and active signs @xmath249 , there is a neighborhood @xmath144 of @xmath13 such that each point @xmath257 yields an optimal pair @xmath358 with the same boundary set @xmath359 , boundary signs @xmath360 , active set @xmath254 and active signs @xmath255 .",
    "the proof is delayed to appendix  [ app : lcbound ] , mainly because of its length .",
    "now lemma  [ lem : lcbound ] , used together with expressions ( [ eq : genlassofit ] ) and ( [ eq : genlassofit2 ] ) for the generalized lasso fit , implies an invariance in representing a ( particularly important ) linear subspace .",
    "[ lem : invbound ] for the same set @xmath357 as in lemma  [ lem : lcbound ] , and for any @xmath211 , the linear subspace @xmath361 is invariant under all boundary sets @xmath28 defined in terms of an optimal subgradient at @xmath355 at @xmath13 .",
    "the linear subspace @xmath362 is also invariant under all choices of active sets @xmath18 defined in terms of a generalized lasso solution @xmath253 at  @xmath13 .",
    "finally , the two subspaces are equal , @xmath363 .",
    "let @xmath211 , and let @xmath355 be an optimal subgradient with boundary set @xmath28 and signs @xmath213 .",
    "let @xmath144 be the neighborhood of @xmath13 over which optimal subgradients exist with boundary set @xmath30 and signs @xmath199 , as given by lemma  [ lem : lcbound ] .",
    "recalling the expression for the fit ( [ eq : genlassofit ] ) , we have that for every @xmath204 @xmath364 if @xmath253 is a solution with active set @xmath18 and signs @xmath256 , then again by lemma  [ lem : lcbound ] there is a neighborhood @xmath365 of @xmath13 such that each point @xmath366 yields a solution with active set @xmath22 and signs @xmath246 .",
    "[ note that @xmath365 and @xmath144 are not necessarily equal unless @xmath355 and @xmath253 jointly satisfy the kkt conditions at  @xmath13 . ] therefore , recalling ( [ eq : genlassofit ] ) , we have @xmath367 for each @xmath366 .",
    "the uniqueness of the generalized lasso fit now implies that @xmath368 for all @xmath369 .",
    "as @xmath370 is open , for any @xmath371 , there exists an @xmath268 such that @xmath372 . plugging @xmath270 into the equation above",
    "reveals that @xmath373 , hence @xmath374 .",
    "the reverse inclusion follows similarly , and therefore@xmath375 .",
    "finally , the same strategy can be used to show that these linear subspaces are unchanged for any choice of boundary set @xmath28 , coming from an optimal subgradient at @xmath13 and for any choice of active set @xmath18 coming from a solution at @xmath13 . noticing that @xmath376 for matrices @xmath377 gives the result as stated in the lemma .",
    "this local stability result implies the following theorem .",
    "[ thm : genlassodf ] assume that @xmath13 follows a normal distribution ( [ eq : normal ] ) . for any @xmath378 and @xmath5",
    ", the degrees of freedom of the generalized lasso fit can be expressed as @xmath379,\\ ] ] where @xmath28 is the boundary set corresponding to any optimal subgradient  @xmath355 of the generalized lasso problem at @xmath13 .",
    "we can alternatively express degrees of freedom as @xmath380,\\ ] ] with @xmath18 being the active set corresponding to any generalized lasso solution @xmath253 at @xmath13 .",
    "note : lemma  [ lem : invbound ] implies that for almost every @xmath152 , for any @xmath30 defined in terms of an optimal subgradient , and for any @xmath22 defined in terms of a  generalized lasso solution , @xmath381 .",
    "this makes the above expressions for degrees of freedom well defined .",
    "proof of theorem  [ thm : genlassodf ] first , the continuity and almost differentiability of @xmath178 follow from lemmas  [ lem : nonexp ] and  [ lem : genlassoproj ] , so we can use stein s formula ( [ eq : steindf ] ) for degrees of freedom .",
    "let @xmath211 , where @xmath209 is the set of measure zero as in lemma  [ lem : lcact ] .",
    "if @xmath28 and @xmath213 are the boundary set and signs of an optimal subgradient at @xmath13 , then by lemma  [ lem : invbound ] there is a neighborhood @xmath144 of  @xmath13 such that each point @xmath204 yields an optimal subgradient with boundary set @xmath30 and signs @xmath199 .",
    "therefore , taking the divergence of the fit in ( [ eq : genlassofit ] ) , @xmath382 and taking an expectation over @xmath13 gives the first expression in the theorem .",
    "similarly , if @xmath18 and @xmath256 are the active set and signs of a  generalized lasso solution at @xmath13 , then by lemma  [ lem : invbound ] there exists a solution with active set @xmath22 and signs @xmath246 at each point @xmath218 in some neighborhood @xmath365 of @xmath13 .",
    "the divergence of the fit in ( [ eq : genlassofit2 ] ) is hence @xmath383 and taking an expectation over @xmath13 gives the second expression .",
    "if @xmath16 , then @xmath384 for any linear subspace @xmath54 , so the results of theorem  [ thm : genlassodf ] reduce to @xmath385 = { \\mathrm{e}}[{\\operatorname{nullity}}(d_{-{{\\mathcal{a}}}})].\\ ] ] the first equality above was shown in @xcite . analyzing the null space of @xmath29 ( equivalently , @xmath386 ) for specific choices of @xmath1",
    "then gives interpretable results on the degrees of freedom of the fused lasso and trend filtering fits as mentioned in the introduction .",
    "it is important to note that , as @xmath16 , the active set @xmath22 is unique , but not necessarily equal to the boundary set @xmath30 [ since @xmath30 can be nonunique if @xmath316 ] .    if @xmath25 , then @xmath388 for any subset @xmath389 .",
    "therefore the results of theorem  [ thm : genlassodf ] become @xmath390 = { \\mathrm{e}}[{\\operatorname{rank}}(x_{{\\mathcal{a}}})],\\ ] ] which match the results of theorems  [ thm : lassodfequi ] and  [ thm : lassodfact ] ( recall that for the lasso the boundary set @xmath30 is exactly the same as equicorrelation set @xmath179 ) .",
    "recent and independent work of @xcite shows that , for arbitrary @xmath378 and for any @xmath13 , there exists a generalized lasso solution whose active set @xmath259 satisfies @xmath391 ( calling @xmath259 the `` smallest '' active set is somewhat of an abuse of terminology , but it is the smallest in terms of the above intersection . ) the authors then prove that , for any @xmath378 , the generalized lasso fit has degrees of freedom @xmath392,\\ ] ] with @xmath259 the special active set as above .",
    "this matches the active set result of theorem  [ thm : genlassodf ] applied to @xmath259 , since @xmath393 for this special active set .",
    "we conclude this section by comparing the active set result of theorem  [ thm : genlassodf ] to degrees of freedom in a particularly relevant equality constrained linear regression problem ( this comparison is similar to that made in lasso case , given at the end of section  [ sec : lasso ] ) .",
    "the result states that the generalized lasso fit has degrees of freedom @xmath394 $ ] , where @xmath18 is the active set of a generalized lasso solution at @xmath13 .",
    "in other words , the complement of @xmath22 gives the rows of @xmath1 that are orthogonal to some generalized lasso solution .",
    "now , consider the equality constrained linear regression problem @xmath395 in which the set @xmath22 is fixed .",
    "it is straightforward to verify that the fit of this problem is the projection map onto @xmath396 , and hence has degrees of freedom @xmath397 .",
    "this means that the generalized lasso fits a linear model of @xmath13 , and simultaneously makes the coefficients orthogonal to an adaptive subset @xmath22 of the rows of @xmath1 , but on average it only uses the same number of parameters as does the corresponding equality constrained linear regression problem ( [ eq : lsb ] ) , in which @xmath22 is pre - specified .",
    "this seemingly paradoxical statement can be explained by the shrinkage due to the @xmath2 penalty .",
    "even though the active set @xmath22 is chosen adaptively based on @xmath13 , the generalized lasso does not estimate the coefficients as aggressively as does the equality constrained linear regression problem ( [ eq : lsb ] ) , but rather , it shrinks them toward zero . roughly speaking",
    ", his shrinkage can be viewed as a `` deficit '' in degrees of freedom , which makes up for the `` surplus '' attributed to the adaptive selection .",
    "we study this idea more precisely in a future paper .",
    "we showed that the degrees of freedom of the lasso fit , for an arbitrary predictor matrix @xmath0 , is equal to @xmath296 $ ] . here",
    "@xmath18 is the active set of any lasso solution at @xmath13 , that is , @xmath398 .",
    "this result is well defined , since we proved that any active set @xmath22 generates the same linear subspace @xmath237 , almost everywhere in @xmath13 .",
    "in fact , we showed that for almost every @xmath13 , and for any active set @xmath22 of a solution at @xmath13 , the lasso fit can be written as @xmath399 for all @xmath218 in a neighborhood of @xmath13 , where @xmath400 is a constant ( it does not depend on @xmath218 ) .",
    "this draws an interesting connection to linear regression , as it shows that locally the lasso fit is just a translation of the linear regression fit of on  @xmath401 . the same results ( on degrees of freedom and local representations of the fit )",
    "hold when the active set @xmath22 is replaced by the equicorrelation set  @xmath179 .",
    "our results also extend to the generalized lasso problem , with an arbitrary predictor matrix @xmath0 and arbitrary penalty matrix @xmath1 .",
    "we showed that degrees of freedom of the generalized lasso fit is @xmath394 $ ] , with @xmath18 being the active set of any generalized lasso solution at @xmath13 , that is , @xmath402 .",
    "as before , this result is well defined because any choice of active set @xmath22 generates the same linear subspace @xmath403 , almost everywhere in @xmath13 .",
    "furthermore , for almost every @xmath13 , and for any active set of a solution at @xmath13 , the generalized lasso fit satisfies @xmath404 for all @xmath218 in a neighborhood of @xmath13 , where @xmath400 is a constant ( not depending on @xmath13 ) .",
    "this again reveals an interesting connection to linear regression , since it says that locally the generalized lasso fit is a translation of the linear regression fit on @xmath0 , with the coefficients @xmath405 subject to @xmath406 .",
    "the same statements hold with the active set @xmath22 replaced by the boundary set @xmath30 of an optimal subgradient .",
    "we note that our results provide practically useful estimates of degrees of freedom . for the lasso problem",
    ", we can use @xmath119 as an unbiased estimate of degrees of freedom , with @xmath22 being the active set of a lasso solution . to emphasize",
    "what has already been said , here we can actually choose any active set ( i.e. , any solution ) , because all active sets give rise to the same @xmath119 , except for @xmath13 in a set of measure zero .",
    "this is important , since different algorithms for the lasso can produce different solutions with different active sets . for the generalized lasso problem , an unbiased estimate for degrees of freedom",
    "is given by @xmath407 , where @xmath22 is the active set of a generalized lasso solution .",
    "this estimate is the same , regardless of the choice of active set ( i.e. , choice of solution ) , for almost every @xmath13 .",
    "hence any algorithm can be used to compute a solution .",
    "first , we prove the statement for the projection map . note that @xmath409 where the first inequality follows from ( [ eq : pfact ] ) , and the second is by cauchy ",
    "dividing both sides by @xmath410 gives the result .",
    "we have shown that @xmath141 and @xmath142 are lipschitz ( with constant @xmath289 ) ; they are therefore continuous , and almost differentiability follows from the standard proof of the fact that a lipschitz function is differentiable almost everywhere .",
    "we write @xmath413 to denote the set of faces of @xmath56 . to each face",
    "@xmath414 , there is an associated normal cone @xmath415 , defined as @xmath416 the normal cone of @xmath417 satisfies @xmath418 for any @xmath419 .",
    "[ we use @xmath420 to denote the relative interior of a set @xmath47 , and @xmath421 to denote its relative boundary . ]",
    "now let @xmath427 .",
    "we have @xmath428 for some @xmath424 , and by construction @xmath429 .",
    "furthermore , we claim that projecting @xmath428 onto @xmath56 is the same as projecting @xmath11 onto the affine hull of @xmath417 , that is , @xmath430 .",
    "otherwise there is some @xmath431 with @xmath432 , and as @xmath433 , this means that @xmath434 . by definition of @xmath435",
    ", there is some @xmath436 such that @xmath437 . but",
    "@xmath438 , which is a contradiction .",
    "this proves the claim , and writing @xmath439 , we have @xmath440 as desired .          in the first type of points above , vertices are excluded because @xmath448 when @xmath417 is a vertex . in the second type , @xmath56 is excluded because @xmath449 .",
    "the lattice structure of @xmath413 tells us that for any face @xmath424 , we can write @xmath450 . this , and the fact that the normal cones have the opposite partial ordering as the faces , imply that points of the first type above can be written as @xmath451 with @xmath452 and @xmath453 for some @xmath454 . note",
    "that actually we must have @xmath455 because otherwise we would have @xmath456 .",
    "therefore it suffices to consider points of the second type alone , and @xmath442 can be written as @xmath457 as @xmath56 is a polyhedron , the set @xmath413 of its faces is finite , and @xmath458 for each @xmath459 .",
    "therefore @xmath442 is a finite union of sets of dimension @xmath460 , and hence has measure zero .        now let @xmath465^\\perp } [ ( x_{\\mathcal{e}})^+]_{(-{{\\mathcal{a}}},\\cdot ) } \\bigl(z - ( x_{\\mathcal{e}}{^t})^+\\lambda s \\bigr ) = 0 \\bigr\\}.\\ ] ] the first union is taken over all possible subsets @xmath466 and all sign vectors @xmath467 ; as for the second union , we define for a fixed subset @xmath179 @xmath468^\\perp } [ ( x_{\\mathcal{e}})^+]_{(-{{\\mathcal{a}}},\\cdot ) } \\not= 0 \\bigr\\}.\\ ] ] notice that @xmath275 is a finite union of affine subspace of dimension @xmath460 , and hence has measure zero .",
    "let @xmath252 , and let @xmath253 be a lasso solution , abbreviating @xmath18 and @xmath256 for the active set and active signs . also write @xmath212 and @xmath213 for the equicorrelation set and equicorrelation signs of the fit .",
    "we know from  ( [ eq : lassosol ] ) that we can write @xmath469 where @xmath191 is such that @xmath470_{(-{{\\mathcal{a}}},\\cdot ) } \\bigl(y - ( x_{\\mathcal{e}}{^t})^+ \\lambda s \\bigr ) + b_{-{{\\mathcal{a } } } } = 0.\\ ] ] in other words , @xmath471_{(-{{\\mathcal{a}}},\\cdot ) } \\bigl(y - ( x_{\\mathcal{e}}{^t})^+ \\lambda s \\bigr ) = -b_{-{{\\mathcal{a } } } } \\in \\pi_{-{{\\mathcal{a } } } } ( { \\operatorname{null}}(x_{\\mathcal{e}})),\\ ] ] so projecting onto the orthogonal complement of the linear subspace@xmath472 gives zero , @xmath473^\\perp } [ ( x_{\\mathcal{e}})^+]_{(-{{\\mathcal{a}}},\\cdot ) } \\bigl(y - ( x_{\\mathcal{e}}{^t})^+\\lambda s \\bigr ) = 0.\\ ] ] since @xmath252 , we know that @xmath473^\\perp } [ ( x_{\\mathcal{e}})^+]_{(-{{\\mathcal{a}}},\\cdot ) } = 0,\\ ] ] and finally , this can be rewritten as @xmath474_{(-{{\\mathcal{a}}},\\cdot ) } \\bigr ) \\subseteq\\pi_{-{{\\mathcal{a}}}}({\\operatorname{null}}(x_{\\mathcal{e}})).\\ ] ]    consider defining , for a new point @xmath218 , @xmath475 where @xmath476 , and is yet to be determined .",
    "exactly as in the proof of lemma  [ lem : lcequi ] , we know that @xmath477 , and for all @xmath225 , a neighborhood of @xmath13 .",
    "now we want to choose @xmath478 so that @xmath228 has the correct active set and active signs . for simplicity of notation , first define the function @xmath479 , @xmath480 equation ( [ eq : colsp ] ) implies that there is a @xmath476 such that @xmath481 , hence @xmath482 .",
    "however , we must choose @xmath478 so that additionally @xmath483 for @xmath484 and @xmath485 .",
    "write @xmath486 by the continuity of @xmath487 , there exits a neighborhood of @xmath231 of @xmath13 such that @xmath488 for @xmath484 and @xmath489 , for all @xmath233 .",
    "therefore we only need to choose a vector @xmath476 , with @xmath481 , such that @xmath490 sufficiently small .",
    "this can be achieved by applying the bounded inverse theorem , which says that the bijective linear map @xmath491 has a bounded inverse ( when considered a function from its row space to its column space ) .",
    "therefore there exists some @xmath492 such that for any @xmath218 , there is a vector @xmath476 , @xmath481 , with @xmath493 finally , the continuity of @xmath494 implies that @xmath495 can be made sufficiently small by restricting @xmath496 , another neighborhood of @xmath13 .",
    "define the set @xmath501^\\perp}\\cdot d_{{\\mathcal{b}}\\setminus{{\\mathcal{a } } } } \\bigl(x p_{{\\operatorname{null}}(d_{-{\\mathcal{b}}})}\\bigr)^+ \\\\ & & \\hspace*{165pt}{}\\times \\bigl(z - \\bigl(p_{{\\operatorname{null}}(d_{-{\\mathcal{b } } } ) } x{^t}\\bigr)^+ d_{\\mathcal{b}}{^t}\\lambda s \\bigr ) = 0 \\bigr\\}.\\end{aligned}\\ ] ] the first union above is taken over all subsets @xmath502 and all sign vectors @xmath503 .",
    "the second union is taken over subsets @xmath504 , where @xmath505^\\perp } d_{{\\mathcal{b}}\\setminus{{\\mathcal{a } } } } \\bigl(x p_{{\\operatorname{null}}(d_{-{\\mathcal{b}}})}\\bigr)^+ \\not= 0 \\bigr\\}.\\ ] ] since @xmath209 is a finite union of affine subspaces of dimension @xmath506 , it has measure zero .",
    "now fix @xmath211 , and let @xmath354 be an optimal pair , with boundary set @xmath28 , boundary signs @xmath213 , active set @xmath18 , and active signs @xmath256 . starting from ( [ eq : genlassokktb ] ) , and plugging in for the fit in terms of @xmath352 , as in ( [ eq : genlassofit ] ) we can show that @xmath507 where @xmath508 . by ( [ eq : genlassosol ] ) , we know that @xmath509 where @xmath510 .",
    "furthermore , @xmath511 or equivalently , @xmath512 projecting onto the orthogonal complement of the linear subspace@xmath513 therefore gives zero , @xmath514^\\perp } d_{{\\mathcal{b}}\\setminus{{\\mathcal{a } } } } \\bigl(x p_{{\\operatorname{null}}(d_{-{\\mathcal{b}}})}\\bigr)^+ \\bigl(y - \\bigl(p_{{\\operatorname{null}}(d_{-{\\mathcal{b } } } ) } x{^t}\\bigr)^+ d_{\\mathcal{b}}{^t}\\lambda s \\bigr ) = 0,\\ ] ] and because @xmath211 , we know that in fact @xmath514^\\perp } d_{{\\mathcal{b}}\\setminus{{\\mathcal{a } } } } \\bigl(x p_{{\\operatorname{null}}(d_{-{\\mathcal{b}}})}\\bigr)^+ = 0.\\ ] ] this can be rewritten as @xmath515    at a new point @xmath218 , consider defining @xmath516 , @xmath517 and @xmath518 where @xmath519 is yet to be determined . by construction ,",
    "@xmath520 and @xmath228 satisfy the stationarity condition ( [ eq : genlassokkt ] ) at @xmath218 .",
    "hence it remains to show two parts : first , we must show that this pair satisfies the subgradient condition ( [ eq : genlassosg ] ) at @xmath218 ; second , we must show this pair has boundary set , boundary signs @xmath521 , active set @xmath499 and active signs @xmath522 .",
    "actually , it suffices to show the second part alone , because the first part is then implied by the fact that @xmath355 and @xmath253 satisfy the subgradient condition at @xmath13 .",
    "well , by the continuity of the function @xmath523 , @xmath524 we have @xmath525 provided that @xmath225 , a neighborhood of @xmath13 .",
    "this ensures that @xmath520 has boundary set @xmath526 and signs @xmath521 .    as for the active set and signs of @xmath228 , note first that @xmath527 , following directly from the definition .",
    "next , define the function @xmath528 , @xmath529 so @xmath530 .",
    "equation ( [ eq : colsp2 ] ) implies that there is a vector @xmath519 such that @xmath531 , which makes @xmath532 .",
    "however , we still need to choose @xmath478 such that @xmath533 for all @xmath484 and @xmath534 .",
    "to this end , write @xmath535 the continuity of @xmath536 implies that there is a neighborhood @xmath231 of @xmath13 such that @xmath537 for all @xmath484 and @xmath538 , for @xmath233 . since @xmath539 where @xmath540 is the operator norm of the @xmath541",
    ", we only need to choose @xmath519 such that @xmath531 , and such that @xmath490 is sufficiently small .",
    "this is possible by the bounded inverse theorem applied to the linear map @xmath542 : when considered a function from its row space to its column space , @xmath542 is bijective and hence has a bounded inverse .",
    "therefore there is some @xmath492 such that for any @xmath218 , there is a @xmath519 with @xmath531 and @xmath543 the continuity of @xmath544 implies that the right - hand side above can be made sufficiently small by restricting @xmath496 , a neighborhood of @xmath13 .",
    "the dual of the lasso problem ( [ eq : lasso ] ) has appeared in many papers in the literature ; as far as we can tell , it was first considered by @xcite .",
    "we start by rewriting problem ( [ eq : lasso ] )  as @xmath545 then we write the lagrangian @xmath546 and we minimize @xmath547 over @xmath548 to obtain the dual problem @xmath549 taking the gradient of @xmath547 with respect to to @xmath548 , and setting this equal to zero gives @xmath550 where @xmath160 is a subgradient of the function @xmath551 evaluated at @xmath162 . from ( [ eq : lassodual1 ] ) , we can immediately see that the dual solution @xmath552 is the projection of @xmath13 onto the polyhedron @xmath56 as in lemma  [ lem : lassoproj ] , and then ( [ eq : lassopd1 ] ) shows that @xmath553 is the residual from projecting @xmath13 onto @xmath56 .",
    "further , from  ( [ eq : lassopd2 ] ) , we can define the equicorrelation set @xmath179 as @xmath554 noting that together ( [ eq : lassopd1 ] ) , ( [ eq : lassopd2 ] ) are exactly the same as the kkt conditions  ( [ eq : lassokkt ] ) , ( [ eq : lassosg ] ) , and all of the arguments in section  [ sec : lasso ] involving the equicorrelation set @xmath179 can be translated to this dual perspective .",
    "there is a slightly different way to derive the lasso dual , resulting in a  different ( but of course , equivalent ) formulation .",
    "we first rewrite problem  ( [ eq : lasso ] ) as @xmath555 and by following similar steps to those above , we arrive at the dual problem @xmath556 each dual solution @xmath552 ( now no longer unique ) satisfies @xmath557 the dual problem ( [ eq : lassodual2 ] ) and its relationship ( [ eq : lassopd3 ] ) , ( [ eq : lassopd4 ] ) to the primal problem offer yet another viewpoint to understand some of the results in section  [ sec : lasso ] .    for the generalized lasso problem",
    ", one might imagine that there are three different dual problems , corresponding to the three different ways of introducing an auxiliary variable @xmath558 into the generalized lasso criterion : @xmath559 { { \\hat{\\beta}}},\\hat{z } & \\in&\\mathop{{\\operatorname{argmin}}}_{\\beta\\in{\\mathbb{r}}^p , z \\in{\\mathbb{r}}^p } { \\frac{1}{2}}\\|y - x\\beta\\|_{2}^2 + \\lambda\\|dz\\|_{1}\\qquad\\mbox{subject to } z=\\beta ; \\\\[2pt ] { { \\hat{\\beta}}},\\hat{z } & \\in&\\mathop{{\\operatorname{argmin}}}_{\\beta\\in{\\mathbb{r}}^p , z \\in{\\mathbb{r}}^m } { \\frac{1}{2}}\\|y - x\\beta\\|_{2}^2 + \\lambda\\|z\\|_{1}\\qquad\\mbox{subject to } z = d\\beta.\\end{aligned}\\ ] ] however , the first two approaches above lead to lagrangian functions that can not be minimized analytically over @xmath548 .",
    "only the third approach yields a dual problem in closed - form , as given by @xcite , @xmath560 \\\\[-7pt ] \\eqntext{\\mbox{subject to } \\|v\\|_{\\infty}\\leq\\lambda , d{^t}v \\in{\\operatorname{row}}(x).}\\end{aligned}\\ ] ] the relationship between primal and dual solutions is @xmath561 \\label{eq : genlassopd2 } { \\hat{v}}&= & \\lambda\\gamma,\\end{aligned}\\ ] ] where @xmath300 is a subgradient of @xmath301 evaluated at @xmath302 . directly from ( [ eq : genlassodual ] ) we can see that @xmath562 is the projection of the point @xmath563 onto the polyhedron @xmath564 by ( [ eq : genlassopd1 ] ) , the primal fit is @xmath565 , which can be rewritten as @xmath566 where @xmath56 is the polyhedron from lemma  [ lem : genlassoproj ] , and finally @xmath567 because @xmath142 is zero on @xmath568 . by ( [ eq : genlassopd2 ] ) , we can define the boundary set @xmath30 corresponding to a particular dual solution @xmath552 as @xmath569 ( this explains its name , as @xmath30 gives the coordinates of @xmath552 that are on the boundary of the box @xmath570 . ) as ( [ eq : genlassopd1 ] ) , ( [ eq : genlassopd2 ] ) are equivalent to the kkt conditions ( [ eq : genlassokkt ] ) , ( [ eq : genlassosg ] ) [ following from rewriting ( [ eq : genlassopd1 ] ) using @xmath571 , the results in section  [ sec : genlasso ] on the boundary set @xmath30 can all be derived from this dual setting ."
  ],
  "abstract_text": [
    "<S> we derive the degrees of freedom of the lasso fit , placing no assumptions on the predictor matrix @xmath0 . </S>",
    "<S> like the well - known result of zou , hastie and tibshirani [ _ ann . </S>",
    "<S> statist . _ </S>",
    "<S> * 35 * ( 2007 ) 21732192 ] , which gives the degrees of freedom of the lasso fit when @xmath0 has full column rank , we express our result in terms of the active set of a lasso solution . </S>",
    "<S> we extend this result to cover the degrees of freedom of the generalized lasso fit for an arbitrary predictor matrix @xmath0 ( and an arbitrary penalty matrix @xmath1 ) . though our focus is degrees of freedom , we establish some intermediate results on the lasso and generalized lasso that may be interesting on their own .    .    </S>"
  ]
}