{
  "article_text": [
    "random synchronous asymmetric neural networks ( rsanns ) with fixed synaptic coupling strengths and fixed neuronal thresholds have been found to have access to a very limited set of different limit - cycles ( clark , krten & rafelski , 1988 ; littlewort , clark & rafelski , 1988 ; hasan , 1989 ; rand , cohen & holmes , 1988 ) .",
    "we will show here , however , that when we add a small amount of temporal random noise by choosing the neural thresholds not to be fixed but to vary within a narrow gaussian distribution at each time step , we can cause transitions between different quasi - limit - cycle attractors . shifting the value of neuronal thresholds randomly from a gaussian distribution which varies on a time scale much slower than the fast synaptic dynamics , we control the timing of the transition between limit - cycle attractors .",
    "hence we can gain controllable and as we believe biologically motivated access to a wide variety of limit - cycles , each displaying dynamical participation by many neurons .",
    "perfect limit - cycles in which the network returns to some earlier state do not exist in real biological systems , due to many complicating factors : membrane potential noise ( little , 1974 ; clark , 1990 ; faure , 1997 ) , the complexity of biological neurons , the continuously - valued signal transmission times between neurons , and the lack of a clock to synchronously update all neurons , to mention a few .",
    "therefore the case of _ approximate _ limit - cycles is more biologically reasonable .",
    "indeed , the appearance of limit - cycle behavior in central pattern generators is evidence for such temporal behavior in biological systems ( hasan , 1989 ; marder & hooper , 1985 ) .",
    "we also believe that the biologically significant limit - cycles are those in which a great fraction of the neurons actively participate in the dynamics .",
    "if these limit - cycles represent brain activity states which we could call ` thoughts ' , and if a sequence of spontaneous or externally controlled transitions from one limit - cycle to another limit - cycle represents a ` reasoning process ' , then neural network systems which exhibit a diversity of accessible limit - cycles ( thoughts ) with capability to move rapidly between them could also exhibit a large variety of different ` trains of thought ' .",
    "a system which can access _ many _ limit - cycles should always be able to access a novel mode ; hence the system would have the potential to be a _ creative _ system .",
    "our current work thus demonstrates conditions sufficient to allow access to creative dynamical behavior .",
    "in section 2 we introduce rsanns along with the concept of threshold noise , as well as the details of an rsann s implementation . in section 3",
    ", we provide the algorithm that contrasts limit - cycles , which is of prime importance in our work . in our quantitative investigations",
    "we need to introduce with more precision concepts which intuitively are easy to grasp , but which mathematically are somewhat difficult to quantify .",
    "we define * eligibility * in section [ elig ] as an entropy - like measure of the fraction of neurons which actively participate in the dynamics of a limit - cycle . in order to quantify the rsann s accessibility to multiple limit - cycle attractors ,",
    "we define * diversity * in section 4.2 as another entropy - like measure , calculated from the probabilities that the rsann converges to each of the different limit - cycles . as a measure of the creative potential of a system ,",
    "we introduce the concept of * volatility * as the ability to switch from one particular highly - eligible cyclic mode to many other highly - eligible cyclic modes .",
    "we mathematically define volatility in section [ volatile ] to be an entropy - like measure of the number of @xmath0 limit - cycle patterns easily available to the net , weighted by the eligibility of each limit - cycle attractor .",
    "we find that in terms of these variables , as the neuronal threshold noise , @xmath1 , increases , our rsann exhibits a phase transformation at @xmath2 from a small number to a large number of different @xmath3 limit - cycle attractors ( section 4.2 ) , and another phase transformation at @xmath4 from high eligibility to low eligibility ( section 4.1 ) .",
    "our main result is that the volatility is high only in presence of threshold noise of suitable fine - tuned strength chosen between @xmath5 , so allowing access to a diversity of eligible limit - cycle attractors ( section 4.3 ) .",
    "random asymmetric neural networks ( rsanns ) ( bressloff & taylor , 1989 ; clark , rafelski & winston , 1985 ; clark , 1991 ) , with @xmath6 , differ from symmetric neural networks ( snns ) ( hopfield , 1982 ) , and offer considerably more biological realism , since real neural signals are unidirectional . due to the lack of synaptic symmetry , rsanns",
    "have a non - simple behavior with different limit - cycle attractors with possible period lengths is the time necessary for a neural signal to propagate from one neuron to the next and is assumed to be identical for all synapses ( mccullough & pitts , 1943 ) ; hereafter , we will take as the unit of time @xmath7 , so that a limit - cycle of period @xmath8 will be @xmath8 time steps long ] @xmath9 , even with extremely long periods ( greater than @xmath10 time steps ) when the neuronal thresholds have been finely tuned ( clark , krten & rafelski , 1988 ; mcguire , littlewort & rafelski , 1991 ; mcguire _ et al .",
    "_ , 1992 ) .",
    "consider a net of @xmath11 neurons with firing states which take binary values ( i.e. @xmath12 ) .",
    "each neuron is connected to m other pre - synaptic neurons by unidirectional synaptic weights .",
    "a firing pre - synaptic neuron @xmath13 will enhance the post - synaptic - potential ( psp ) of neuron @xmath14 by an amount equal to the connection strength , @xmath15 .",
    "inhibitory neurons ( chosen with probability @xmath16 ) have negative connection strengths .",
    "if the @xmath14th neuron s psp , @xmath17 , is greater than its threshold , @xmath18 , then it fires an action potential : @xmath19 .",
    "we parametrize the thresholds , @xmath18 , in terms of ` normal ' thresholds , @xmath20 , a mean bias level , @xmath21 and a multiplicative threshold - noise parameter , @xmath22 : @xmath23 if @xmath24and @xmath25 , we recover normal thresholds ( @xmath26 ) .    in the presence of even a small amplitude of neural - threshold noise",
    "effective on the same time - scale as the transmission - time of neural impulses from one neuron to the next neuron , the neural net will never stabilize into a single limit - cycle attractor .",
    "rather , if the noise amplitude is not too high , the net will continually make transitions from one almost - limit - cycle to another almost - limit - cycle .",
    "this ` non - stationary ' characteristic is common to any volatile system , but makes computer simulation and characterization difficult because one never knows _ a priori _ when the neural network will ` jump ' to a new attractor basin . in order to maintain stability and make simulation easier , but also for reasonjs of biological reality we will give the neural threshold noise a much longer time scale , @xmath27 , than the shorter signal - transmission / neural - update time scale , @xmath28 .",
    "consequently , for each discrete slow - time threshold - update step , @xmath27 , there will be several hundred to several thousand fast - time neural - update steps , @xmath28 , during which the thresholds @xmath29 are ` quenched ' ( or do not change ) and the neural states @xmath30 are allowed to change via eq .",
    "[ dynameq ] . only at the next slow - time step",
    "are the thresholds allowed to change , but then kept quenched again during the many fast - time neural - update steps , see figure [ timing ] .",
    "we see here the evolution of a threshold value and neural activityi on slow - time , @xmath27 , while in the insert the fine - structure of limit - cycle dynamics as a function of fast - time is displayed .    for a given limit - cycle search , labelled by the slow - time - scale parameter @xmath27",
    ", the neurons initially possess random firing states ; where the fraction of initially firing neurons is also chosen randomly to be between @xmath31 and @xmath32 . at a given slow - time step , @xmath33 we initialize the neuronal threshold parameters @xmath34 with zero - mean gaussian noise of width , @xmath35 , where each @xmath22 is chosen independently for all @xmath14 .",
    "this ` slow threshold noise ' makes biological sense because the concentrations of different chemicals in the brain changes on a time - scale of seconds or even minutes , thereby changing the effective thresholds of individual neurons on a time scale much longer than the update time , @xmath28 ( milliseconds ) .",
    "some biologists therefore believe that neural thresholds are ` constant ' and noiseless ( fitzhugh - nagumo model ( pei , bachmann & moss , 1995 ) ) ; others unequivocably feel that neurons live in a very noisy environment , both chemically and electrically ( little , 1974 ) ; a model which includes slow threshold noise varying randomly by @xmath36 seems consistent with both points of view  this will be the model developed here .",
    "thereafter the firing state @xmath37 of the @xmath14th neuron as a function of the rapidly - updating time parameter @xmath38 is given by the following equations : @xmath39 where @xmath40 is the step - function .",
    "the prescription given above of an rsann is implemented in a computer program on a fast workstation .",
    "we update all neuron firing states in parallel , or ` synchronously ' , as opposed to serial , or ` asynchronous ' , updating in which only one neuron or a small group of neurons is updated at a given time step .",
    "the connection strengths @xmath15 are integers chosen randomly from a uniform distribution , @xmath41 $ ] . we have performed a cross - check simulation",
    "in which the connection strengths are double - precision real numbers , and have found no difference between the integer - valued connection - strength simulations and the real - valued connection - strength simulations .",
    "the normal thresholds are double - precision numbers .",
    "we chose @xmath42 incoming connections per neuron , where @xmath43 .",
    "the fraction of inhibitory connections is randomly chosen , here with probability , @xmath44 .",
    "we have studied networks with @xmath11 neurons , where @xmath45 the network size is primarily constrained by the extremely long limit - cycles or transients which get longer for larger networks , especially when the thresholds are near-`normal ' , as given by @xmath20 in eq .",
    "[ threshnoise1 ] .",
    "we simulate slow threshold noise by reinitializing the thresholds via equation [ threshnoise1 ] , at each slow - time step @xmath27 .",
    "initially , during this slow - time step @xmath27 , the neural firing states are updated by the dynamical equations [ dynameq ] for @xmath46 fast - time steps , @xmath47 after the @xmath48 fast - time updates , an iterative computer routine carefully scans the record of the spatially - averaged firing rate , @xmath49 , for @xmath50,$ ] to check for exact periodicity : @xmath51 ,   & \\mbox{(b ) } \\end{array}\\ ] ] where the limit - cycle period is is identified to be @xmath52 the iterative routine carefully scans each candidate limit - cycle to ensure perfect periodicity of @xmath49 for at least @xmath53 periods ( typically @xmath54 or @xmath55 periods ) of the limit - cycle .",
    "if the scanning routine finds that the spatially - averaged firing rate has converged to a limit - cycle ( with @xmath56 , see figure [ finger ] ) , then we halt the fast - time updating , ` record ' the limit - cycle , increment the slow - time ( @xmath57 ) , give the thresholds new initial conditions by choosing new @xmath34 in eq .",
    "[ threshnoise1 ] and the neuronal firing states new initial conditions , and begin a new limit - cycle search . on the other hand , if the scanning routine finds that the spatially - averaged firing rate has not yet converged to a limit - cycle , we then let @xmath58 , and update the network for another @xmath59 fast - time steps . after the updating , the iterative limit - cycle scanning routine searches again for at least @xmath53 periods of exact periodicity of @xmath49 , but only within the new time - window , @xmath60 $ ] .",
    "we repeat this limit - cycle search algorithm until we find a limit - cycle , or until @xmath61 ( where @xmath62 is typically @xmath63 or @xmath64 time steps ) , whichever comes first .",
    "if the limit - cycle period is much shorter than the width of the time - window , @xmath65 we will observe many more repetitions of the limit - cycle than the requisite @xmath53 .",
    "if no limit - cycle is observed during the limit - cycle search , then the rsann either has a very long transient or a very long period limit - cycle , see clark ( 1991 ) or littlewort , clark & rafelski ( 1988 ) for a discussion of the correlation between transient - length and limit - cycle period .",
    "from the fast - time - averaged single neural firing rates and from estimates of each neuron s neural fast - time variance during a limit - cycle , we can compare two limit - cycles found at two different slow - time steps by a chi - square measure of the difference between two simulated distributions .    we will estimate the likelihood that two different limit - cycles ( labelled by the slow - time indices @xmath27 and @xmath66 ) belong to different basins of attraction , by comparing the time - averaged single - neuron firing rate spatial vectors , @xmath67 and @xmath68 , where the subscript @xmath14 is the spatial index of the neuron .",
    "often , limit - cycles with different periods @xmath69 and @xmath70 will be remarkably similar , with only an occasional slight difference between the fast - time recordings of the spatially - averaged firing rates @xmath49 and @xmath71 ( see eq .",
    "[ activity ] ) , caused by an occasional neuron misfiring :    @xmath72    conversely , limit - cycles with the same period , @xmath73 , will often have grossly different @xmath67 and @xmath68 , especially for short - period limit - cycles .",
    "we ` record ' each limit - cycle s period , @xmath69 , and its time - averaged single - neuron firing rates , @xmath67 :    @xmath74    for all neurons @xmath14 ( see figure [ finger ] ) . since @xmath75 for @xmath76 , we have @xmath77 , and we find that the variance ( of a single measurement from the mean ) , @xmath78 , will be :    @xmath79    for @xmath80 , we find that the standard deviation of a single measurement of the firing state from the mean firing rate is @xmath81 . however , for different limit - cycle attractors , the difference between firing rates of a particular neuron @xmath14 is @xmath82 .",
    "fortunately , we measure the firing state of each neuron many times ( @xmath83 ) to determine the mean firing rate , so we can also estimate the variance of the mean , @xmath84 , as :    @xmath85    for @xmath86 and @xmath87 , this gives a standard deviation of the mean of @xmath88 , which is a little smaller than the variation seen between different limit - cycle attractors ( see figure [ finger ] ) .",
    "therefore , we choose to use the variance of the mean , @xmath84 , rather than the variance of a single measurement from the mean for our limit - cycle comparison tests . if we choose a longer observation time , @xmath89 , then we can better discriminate between different limit - cycles by using @xmath90 test discussed below .",
    "since the variance @xmath84 is @xmath31 for @xmath91 or @xmath92 ( likely when the rsann has low eligibility ) , a @xmath93 comparison of @xmath67 and @xmath68 will be plagued by division - by - zero problems . in this case , a maximum likelihood comparison using poisson statistics ( smith , hersman & zondervan , 1993 ) , or maybe the kolmogorov - smirnov test would be more appropriate .",
    "however , in this work , we simply chose to ` cut - off ' the estimate ( eq .",
    "[ variance ] ) of the single - measurement variance before it reaches zero at a value of @xmath94 .",
    "since volatility ( which we define below ) requires an abundance of very _ different _ limit - cycles , we need a highly - contrasting measure of whether a given limit - cycle is different than or similar to another limit - cycle , or in essence a method of distinguishing between limit - cycles ` fingerprints ' . for each limit - cycle",
    "( labelled by the slow time @xmath27 ) , we measure each neuron s fast - time - averaged firing rate , @xmath95@xmath96 and its ( cutoff ) fast - time variance ( of the mean ) , @xmath84@xmath96 ( eq . [ variance2 ] ) , and apply the weighted chi - square ( @xmath93 ) method to decide whether two limit - cycles found at different slow - times , @xmath27 and @xmath66 , are similar or different :    @xmath97    where n is the number of neurons in the network as well as the number of degrees of freedom for the @xmath93-test .",
    "each term in the @xmath98 sum should approximate the square of a gaussian - distributed variable with unit variance .",
    "the variance of the difference of two gaussian - distributed variables is the sum of the individual variances ( not the average ) , as seen in the denominator of the @xmath93 expression .",
    "if the variance - estimate , @xmath99 , is appropriate and accurate , and if a class of similar limit - cycles have gaussian - distributed time - averaged firing rates @xmath67 , then we can estimate _ a priori _ the maximum value of @xmath100 for similar limit - cycles . for an @xmath11-neuron network ,",
    "two similar limit - cycles should have ( for @xmath101 ) a chi - squared given by : @xmath102 . accordingly , in order to ensure that most of the similar limit - cycles are determined to be similar by the chi - square test , for ` similarity ' we demand that @xmath103 . for large @xmath11 ,",
    "chi - square tables indicate that this is approximately a @xmath104 confidence level experiment .",
    "this similarity test corresponds to a tolerance of about 4 misfires per time step ( for @xmath105 ) from the @xmath93-test , each pair of limit - cycles is given a difference label @xmath106 :    @xmath107    if @xmath108 , @xmath109 , then the limit - cycle found at slow - time step , @xmath27 , is truly a _ novel _ limit - cycle , never having been observed before .",
    "if the limit - cycle is novel then it is given a ` novelty ' label of @xmath110 , otherwise @xmath111 ( by default , the first observed limit - cycle will always be novel : @xmath112 ) . simply by counting the number of slow - time steps in which a novel limit - cycle is found , we can estimate the number of different limit - cycle attractors , @xmath113 , available to the rsann :    @xmath114",
    "random asymmetric neural networks will often develop into a fixed point where the neural firing vector , @xmath115 does not change in time , or in other words , the limit - cycle has a period of one time step .",
    "for zero slow - threshold noise ( @xmath116 , if the mean threshold value is much greater(less ) than normal , then the rsann will tend to have a fixed point with very few(many ) neurons firing each time step , which is called network death(epilepsy ) .",
    "likewise , for zero noise ( @xmath116 and for normal thresholds @xmath117 , limit - cycles with very long periods are possible ( figs.[lmu],[alphamu ] ) .",
    "when the mean threshold value is normal ( @xmath118 , but the spatiotemporal threshold fluctuations from normality are large(@xmath119 ) , then there also exist _ many _ different mixed death / epilepsy fixed points in which a fraction of the neurons are firing at each time step and the remaining neurons never fire .",
    "conversely , when the mean threshold value is normal and the spatiotemporal threshold fluctuations are small ( @xmath120 ) , limit - cycles with very long periods are possible , but the number of different limit - cycle attractors is limited . for normal mean threshold values and intermediate - valued spatiotemporal threshold fluctuations ( @xmath121 many intermediate - period limit - cycles exist ( figs.[laveeps],[ndiffeps ] ) .",
    "a network is said to have a high degree of eligibility if many neurons participate in the dynamical collective activity of the network .",
    "for an attractor found at slow - time step @xmath27 , the time - averaged spatial firing vector , @xmath122@xmath123 $ ] ( eq .",
    "[ activity ] ) , will be maximally eligible if @xmath124 for all neurons @xmath125 and minimally eligible if @xmath67@xmath126 for all neurons @xmath127 the shannon information ( or entropy ) has these properties , so we will adopt the form of an entropy function as our measure of the eligibility of each limit - cycle attractor , @xmath128 :    @xmath129    where the average eligibility , @xmath130 , per limit - cycle attractor is :    @xmath131    despite its utility , we do not have a detailed dynamical motivation for using an entropy measure for our eligibility measure . in figure [ eeps ]",
    ", we find that eligibility goes through a phase transformation to zero at @xmath132 , as the non - fixed - point limit - cycles gradually become fixed points as @xmath1 increases .",
    "in other words , when the thresholds become grossly ` out - of - tune ' with the mean membrane potential , the rsann attractors become more trivial , with each neuron tending towards its own independent fixed point @xmath133 or @xmath134 .",
    "we measure the accessibility of a given attractor by estimating the probability , @xmath135 , that a given attractor ( first seen at slow - time step @xmath27 , with the novelty label @xmath136 when @xmath137 ) is found during the slow - time observation period @xmath138 $ ] : @xmath139 where @xmath140 is the total number of times the limit - cycle @xmath27 ( first observed at slow - time step @xmath27 ) is observed , and @xmath141 is defined in section [ chisquared ] .",
    "note that if a given attractor is only observed _ once _ ( at time step @xmath27 ) , then @xmath142 ; also if the same attractor is observed at each slow - time step , then @xmath143 .",
    "if each different limit - cycle attractor can be accessed by the network with a measured probability , @xmath135 , we can define the diversity , @xmath144 , as the inter - attractor occupation entropy : @xmath145 it is easily seen that a large @xmath144 corresponds to the ability to occupy many different cyclic modes with equal probability ; the diversity will reach a maximum value of @xmath146 when @xmath147 for all slow - time steps @xmath148 a small value for @xmath144 corresponds to a genuine stability of the system  very few different cyclic modes are available .",
    "we observe a phase transformation from low to high diversity by increasing the level of randomness , @xmath1 , on the threshold for each neuron past @xmath149 ( fig .",
    "[ deps ] ) .",
    "we have chosen not to explicitly divide the diversity by @xmath150 or @xmath151 , since diversity should be an extensive quantity , scaling with the observation time @xmath150 . however , a diversity - production rate for the rsann can easily be inferred , by dividing by the observation time .",
    "we have found very little dependence of the critical - point @xmath152 on the size of the network , @xmath153         volatility is defined as the ability to access a large number of highly - eligible limit - cycles , or a mixture of high eligibility and high diversity .",
    "we have defined eligibility and diversity above each in terms of entropy - type measure .",
    "therefore we shall define volatility also as an entropy - weighted entropy : @xmath154 for ( @xmath155 ( fig .",
    "[ veps ] ) , volatility is high , corresponding to two distinct transformations to volatility . at @xmath156 ,",
    "the amplitude of slow threshold noise causes a transformation to a diversity of different limit - cycles ; at @xmath157 , the slow threshold noise is so large that all limit - cycles become fixed points .",
    "we accordingly label three different regimes for the rsann with slow threshold noise :    @xmath158 stable regime : @xmath159    @xmath158 volatile regime : @xmath160    @xmath158 trivially random regime : @xmath161 .      from table",
    "[ sizetable ] , for several different network sizes ( @xmath162 ) with @xmath163 without slow threshold noise ( @xmath164 ) , we have found only a handful of different limit - cycles ( @xmath165 ) for each network , without evidence for a systematic dependence on network size .",
    "the limit - cycle periods for these networks at zero noise are dominated by one or two different periods ( typ(@xmath8 ) ) , and also have no systematic dependence on network size .",
    "we have found however that the transients ( typ(@xmath166 ) ) prior to convergence to a limit - cycle tends grow very rapidly with network size @xmath11 . when @xmath167 , which is in the volatile region , the distribution of cycle lengths is broadly distributed ( see fig .",
    "[ lhisto ] ) , and from table [ sizetable ] , the mean value @xmath168 of the cycle length grows nearly exponentially with @xmath11 ( fig .",
    "[ loglaven ] ) . since the distribution of limit - cycle periods is highly non - gaussian ( fig.[lhisto ] )",
    ", caution should be used when interpreting the properties of the _ average _ cycle - length ( as in fig .",
    "[ loglaven ] ) . potentially the maximum or median observed cycle length should be used rather than the average .",
    "also , since the cycle length distribution does _ not _ exhibit peaks at regularly spaced intervals , the possibility of an errant limit - cycle comparison algorithm is unlikely .",
    ".for different network sizes , @xmath11 , we tabulate the number , @xmath113 , of different zero - noise limit - cycle attractors , characteristic cycle length ( char(@xmath169 ) , characteristic transient ( char(@xmath170 ) , and average cycle length with non - zero noise ( @xmath171 ) .",
    "each error bar is the r.m.s .",
    "variation from the mean divided by the square root of the number of initial - conditions . at each value of @xmath35",
    ", we observe each net for @xmath172 slow - time steps , in which we require greater than @xmath173 periods ( see section 3.2 ) of limit - cycle repetition before searching for another limit - cycle.[sizetable ] [ cols= \" < , < , < , < , < \" , ]          we observe in the volatile - regime , that the number of different attractors observed , @xmath113 , is nearly equal to the observation time - period @xmath150 , when @xmath174 .",
    "conversely , in the stable - regime ( @xmath175 ) , @xmath113 is largely independent of @xmath176 these two results are complementary : the former implying a nearly inexhaustible source of different highly - eligible limit - cycle attractors , the latter implying that we can access a small group of different limit - cycle attractors with a high degree of predictability .",
    "@xmath113 is often greater than @xmath32 ( though small ) for the stable - regime , which means that the stable phase can not be used to access a particular attractor upon demand , but we can demand access to one of a small number of different attractors .",
    "one might interpret this result as showing that the rsann can think the same thought in several different ways , dependent on the initial conditions for the firing vector , @xmath177 .",
    "krten ( 1988a ) has shown the existence of a dynamical phase transition for zero - noise rsanns with @xmath178 incoming connections per neuron from a chaotic phase to a frozen phase . for @xmath179",
    ", rsanns will always remain in the frozen phase . for @xmath178 ,",
    "the chaotic - to - frozen transition can be triggered either by randomly diluting the density of connections , or by increasing an additive threshold parameter . in the chaotic phase , the rsann has sensitive dependence on initial conditions , and very long limit - cycle trajectories ( whose period scales as @xmath180 ) . in the frozen phase",
    ", the rsann has insensitivity to initial conditions and very short limit - cycle trajectories ( whose period scales as @xmath181 ) .",
    "the parameters @xmath182 and @xmath183 depend on the connectivity @xmath42 , the additive threshold parameter , and the distribution of connection strengths .",
    "therefore , at zero threshold noise ( @xmath164 ) , our results are inconclusive ; however when we choose our thresholds from a sufficently wide distribution ( @xmath167 ) , the limit - cycle period scales approximately as an exponential function of @xmath11 ( with an exponent of @xmath184 , see section [ netsize ] ) , so we might ( cautiously ) surmise that the volatile rsann is in the chaotic phase .",
    "kauffman ( 1993 , pp .",
    "191 - 235 ) and krten ( 1988b ) shows that random boolean automata with @xmath185 ( frozen phase ) typically have @xmath186 different limit - cycle attractors ( for each network realization ) , whose period also scales as @xmath187 .",
    "this small number of different attractors is qualitatively consistent with our @xmath164 results .",
    "we need to perform an ensemble average of many simulations to indeed confirm this result .",
    "we are currently unaware of any definitive prior results on the number of different attractors for networks in the chaotic phase ( except for the observed preponderance of a very few ( long ) limit - cycles , perhaps explained by ` canalization ' ( kauffman , 1993 ) .",
    "however , the arguments of derrida , gardner , and zippelius(1987 ) regarding the evolution of the overlap between a configuration and a stored configuration might apply , which would imply a strict theoretical upper limit on the number of storable / recallable limit - cycle attractors at @xmath188 however , we do not keep our thresholds fixed , so this upper limit is not applicable , as seen in our results .    for our implementation of noisy rsanns",
    ", we speculate that the number of attractors in the chaotic phase scales exponentially with @xmath11 . this speculation is based on our observation that with our volatile rsann , the number of different attractors available at @xmath167 is nearly inexhaustible since in the volatile phase we are always able to observe new limit - cycle attractors just by giving the thresholds new initial conditions from the narrow gaussian distribution .",
    "[ [ volatility - chaos - stochastic - resonance - and - qualitative - non - determinism ] ] volatility , chaos , stochastic resonance , and qualitative non - determinism ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    our original interpretation of @xmath189 as providing a slowly - varying noise on the thresholds can be reinterpreted in a slightly different perspective .",
    "the volatile rsanns are probably in the chaotic phase even when @xmath190 , but in order to observe the expected exponential dependence of the cycle - length on @xmath11 , we would need to perform an ensemble average ( or observe many different realizations of the rsann from the same class with different connection matrices ) . when @xmath189 , we are actually sampling a significant fraction of the entire ensemble .",
    "noise can give a dynamical system access to the whole ensemble of different behaviors at different times during the lifetime of the dynamical system .",
    "slowly - varying threshold noise can act as a ` scanner ' for thoughts novel or long - lost .",
    "volatility can also be considered as a form of stochastic resonance , in which an optimal noise amplitude enhances the signal - to - noise ratio of a signal filter .",
    "zero - noise or high - amplitude noise tends to reduce the information - processing capability of a stochastic resonant system .",
    "the observation of stochastic resonance without external driving force ( gang _ et al .",
    "_ , 1993 ) has interesting parallels to our observation of high volatility at intermediate noise amplitudes .    by taking advantage of the chaotic threshold parameters ( the connection strength parameters are probably also chaotic )",
    ", we can access a large number of different rsann attractors . hence with a feedback algorithm , one might be able to construct a system to control this chaos ( ott , grebogi & yorke , 1990 ) and access a given attractor upon demand . but this stability needs to be augmented by the ability to always be able to access a novel attractor .",
    "this approach to controlled creativity has been developed into the adaptive resonance formalism ( carpenter & grossberg , 1987 ) .",
    "there is a theoretical proof ( amit , 1989 , p. 66 ) that thresholds with fast - noise ( little , 1974 ) ( chosen from a zero - mean gaussian ) and sharp step - function non - linearities is equivalent to a system with zero threshold noise but with a rounded ` s'-curve non - linearity .",
    "this proof does not apply to slowly varying threshold noise . perhaps by choosing a _ discontinuous _ non - linearity , with additive noise in the argument of the non - linearity , the volatility or creativity that we observe is due to _ non_-determinism ( hbler , 1992 ) , in which prediction of final limit - cycle attractors is nearly impossible .",
    "the notion of ` qualitatively ' uncertain dynamics ( heagy , carroll & pecora , 1994 ) describes non - determinism between different attractor basins .",
    "qualitative non - determinism differs from the effective quantitative non - determinism observed in ordinary deterministic chaos , in that ordinary chaos consists of a _ single _ ` strange ' attractor , while qualitative non - determinism consists of _ multiple _ strange ( or simple ) attractors ` riddled ' with holes for transitions to other attractors .",
    "volatility is precisely the same concept as qualitative non - determinism .",
    "the main objective of this study has been to construct a volatile neural network that could exhibit a large set of easily - accessible highly - eligible limit - cycle attractors . without noise , we demonstrate that random asymmetric neural networks ( rsanns ) can exhibit only a small number of different limit - cycle attractors . with neuronal threshold noise within a rather wide range ( @xmath191 ) , we show that rsanns can access a diversity of highly - eligible limit - cycle attractors . rsanns exhibit a diversity phase transformation from a small number of distinct limit - cycle attractors to a large number at a noise amplitude of @xmath192 . likewise , rsanns exhibit a eligibility phase transformation at a threshold noise amplitude of @xmath193 .",
    "h. bohr would like to thank p. carruthers ( now deceased ) , j. rafelski , and the u. arizona department of physics for hospitality during several visits when much of this work was completed ; p. mcguire thanks the santa fe institute and a. hbler and the university of illinois center for complex systems research for hospitality and atmospheres for very fertile discussions .",
    "p. mcguire was partially supported by an nsf / u.s .",
    "dept . of education / state of arizona pre - doctoral fellowship .",
    "we all thank many individuals who have provided different perspectives to our work , including the following : g. sonnenberg , d. harley , b. skaggs , z. hasan , g. littlewort , and j. clark .",
    "clark , k.e .",
    "krten and j. rafelski .  access and stability of cyclic modes in quasirandom networks of threshold neurons obeying a determinisitic synchronous dynamics , ",
    "_ computer simulation in brain science _ , edited by r.m.j cotterill ( cambridge univ .",
    "press : cambridge,1988 ) 316 .",
    "z. hasan , ",
    "biomechanical complexity and the control of movement ,  _ lectures in the sciences of complexity _ , edited by d.l .",
    "stein ( santa fe institute studies in the sciences of complexity , addison - wesley ) ( 1989 ) 841 .                    e. marder and s.l .",
    "hooper ,  neurotransmitter modulation of the stomatogastric ganglion of decapod crustaceans ,  _ model neural networks and behavior _ , ed a.i .",
    "selverston ( plenum press : new york,1985 ) , 319 .",
    "mcguire , g.c .",
    "littlewort , c. pershing and j. rafelski ,  training random asymmetric neural networks towards chaos  a progress report , ",
    "_ proceedings of the workshop on complex dynamics in neural networks_,edited by j.g taylor , e.r .",
    "caianiello , r.m.j .",
    "cotterill and j.w .",
    "clark ( springer - verlag , london , 1992 ) pp .",
    "90 - 102 .",
    "rand , a.h .",
    "cohen and p.j .",
    "holmes ,  systems of coupled oscillators as models of central pattern generators ,  _ neural control of rhythmic movements in vertebrates _ , eds .",
    "cohen , s. rossignol , and s. grillner ( john wiley : new york , 1988 ) , 333 ."
  ],
  "abstract_text": [
    "<S> we study the diversity of complex spatio - temporal patterns of random synchronous asymmetric neural networks ( rsanns ) . </S>",
    "<S> specifically , we investigate the impact of noisy thresholds on network performance and find that there is a narrow and interesting region of noise parameters where rsanns display specific features of behavior desired for rapidly ` thinking ' systems : accessibility to a large set of distinct , complex patterns .    </S>",
    "<S> = 10000 </S>"
  ]
}