{
  "article_text": [
    "dimension reduction is ubiquitous in many areas ranging from pattern recognition , clustering , classification , to fast numerical simulation of complicated physical phenomena .",
    "the fundamental question to address is how to approximate a @xmath4-dimensional space by a @xmath5-dimensional one with @xmath6 .",
    "specifically , we are given a set of high - dimensional data @xmath7 \\in { \\mathbb r}^{m \\times n},\\ ] ] and the goal is to find its low - dimensional approximation @xmath8 \\in { \\mathbb r}^{m \\times d}\\ ] ] with reasonable accuracy .",
    "there are two types of dimension reduction methods .",
    "the first category consists of `` projective '' ones .",
    "these are the linear methods that are _ global _ in nature , and that explicitly transform the data matrix @xmath0 into a low - dimensional one by @xmath9 .",
    "the leading examples are the principal component analysis ( pca ) and its variants .",
    "the methods in the second category act locally and are inherently nonlinear . for each sample in the high - dimensional space",
    "( e.g. each column of @xmath0 ) , they directly find their low - dimensional approximations by preserving certain locality or affinity between nearby points .    in this paper , inspired by the reduced basis method ( rbm )",
    ", we propose a linear method called `` reduced basis decomposition ( rbd ) '' .",
    "it is much faster than pca / svd - based techniques .",
    "moreover , its low - dimensional vectors are equipped with error estimator indicating how close they are approximating the high - dimensional data .",
    "rbm is a relative recent approach to speed up the numerical simulation of parametric partial differential equations ( pdes ) @xcite .",
    "it utilizes an offline  online computational decomposition strategy to produce surrogate solution ( of dimension @xmath10 ) in a time that is of orders of magnitude shorter than what is needed by the underlying numerical solver of dimension @xmath11 ( called _ truth _ solver hereafter ) .",
    "the rbm relies on a projection onto a low dimensional space spanned by truth approximations at an optimally sampled set of parameter values @xcite .",
    "this low - dimensional manifold is generated by a greedy algorithm making use of a rigorous _ a posteriori _ error bounds for the field variable and associated functional outputs of interest which also guarantees the fidelity of the surrogate solution in approximating the truth approximation .",
    "the rbd method acts in a similar fashion .",
    "given the data matrix @xmath0 as in , it iteratively builds up @xmath2 whose column space approximates that of @xmath0 .",
    "it starts with a randomly selected column of @xmath0 ( or a user input if existent ) . at each step where we have @xmath12 vectors @xmath13",
    ", the next vector @xmath14 is found by scanning the columns of @xmath0 and locating the one whose error of projection into the current space @xmath15 is the largest .",
    "this process is continued until the maximum projection / compression error is small enough or until the limit on the size of the reduced space is reached .",
    "an important feature is an offline - online decomposition that allows the computation of the compression error , and thus the cost of locating @xmath14 , to be independent of ( the potentially large ) @xmath16 .",
    "this paper is organized as follows . in section",
    "[ sec : background ] , we review the background material , mainly the rbm .",
    "section [ sec : rbc ] describes the reduced basis decomposition algorithm and discuss its properties .",
    "numerical validations are presented in section [ sec : numerical ] , and finally some concluding remarks are offered in section [ sec : conclusion ] .",
    "the reduced basis method was developed for use with finite element methods to numerically solve pdes .",
    "we assume , for simplicity , that the problems ( usually parametric partial differential equations ( pde ) ) to simulate are written in the weak form : find @xmath17 in an hilbert space @xmath0 such that @xmath18 where @xmath19 is an input parameter .",
    "these simulations need to be performed for many values of @xmath19 chosen in a given parameter set @xmath20 . in this problem",
    "@xmath21 and @xmath22 are bilinear and linear forms , respectively , associated to the pde ( with @xmath23 and @xmath24 denoting their numerical counterparts ) .",
    "we assume that there is a numerical method to solve this problem and the solution @xmath25 , called the `` truth approximation '' or `` snapshot '' , is accurate enough for all @xmath26 .",
    "the fundamental observation utilized by rbm is that the parameter dependent solution @xmath27 is not simply an arbitrary member of the infinite - dimensional space associated with the pde . instead",
    ", the solution manifold @xmath28 can typically be well approximated by a low - dimensional vector space .",
    "the idea is then to propose an approximation of @xmath29 by @xmath30 where , @xmath31 are @xmath10 @xmath32 pre - computed truth approximations corresponding to the parameters @xmath33 judiciously selected according to a sampling strategy @xcite . for a given @xmath19",
    ", we now solve in @xmath34 for the reduced solution @xmath35 .",
    "the online computation is @xmath36-independent , thanks to the assumption that the ( bi)linear forms are affine @xmath37 and the fact that they can be approximated by affine ( bi)linear forms when they are nonaffine @xcite . hence , the online part is very efficient . in order to be able to `` optimally ''",
    "find the @xmath10 parameters and to assure the fidelity of the reduced basis solution @xmath35 to approximate the truth solution @xmath27 , we need an _ a posteriori _ error estimator @xmath38 which involves the residual @xmath39 and stability information of the bilinear form @xcite . with this estimator , we can describe briefly the classical * greedy algorithm * used to find the @xmath10 parameters @xmath40 and the space @xmath34 .",
    "we first randomly select one parameter value and compute the associated truth approximation .",
    "next , we scan the entire ( discrete ) parameter space and for each parameter in this space compute its rb approximation @xmath41 and the error estimator @xmath42 .",
    "the next parameter value we select , @xmath43 , is the one corresponding to the largest error estimator .",
    "we then compute the truth approximation and thus have a new basis set consisting of two elements .",
    "this process is repeated until the maximum of the error estimators is sufficiently small .",
    "the reduced basis method typically has exponential convergence with respect to the number of pre - computed solutions @xcite .",
    "this means that the number of pre - computed solutions can be small , thus the computational cost reduced significantly , for the reduced basis solution to approximate the finite element solution reasonably well .",
    "the author and his collaborators showed @xcite that it works well even for a complicated geometric electromagnetic scattering problem that efficiently reveals a very sensitive angle dependence ( the object being stealthy with a particular configuration ) .",
    "in this section , we detail our proposed methodology by stating the algorithm , studying the error evaluation , and pinpointing the computational cost .",
    "set @xmath44 , @xmath45 , and @xmath46 a random integer between @xmath47 and @xmath4 .    at the heart of the method stated in algorithm [ alg : c_greedy ] is a greedy algorithm similar to that used by rbm .",
    "it builds the reduced space dimension - by - dimension . at each step , the _ greedy _ decision for the best next dimension to pursue in the space corresponding to the data is made by examining an error indicator quantifying the discrepancy between the uncompressed data and the one compressed into the current ( reduced ) space .    in the context of the rbm",
    ", we view each column ( or row if we are compressing the row space ) of the matrix as the fine solution of certain ( virtual ) parametric pde with the ( imaginary ) parameter taking a particular value . since this solution is _ explicitly _ given already by the data , the fact that the pde and the parameter are absent does not matter .",
    "once this _ common mechanism _",
    "satisfied by each column ( or row ) is identified , the greedy algorithm still relies on an accurate and efficient estimate quantifying the error between the original data and the compressed one .",
    "this will be the topic of the next subsection .    to state the algorithm",
    ", we assume that we are given a data matrix @xmath48 , the largest dimension @xmath49 that the practitioner wants to retain , and a tolerance @xmath50 capping the discrepancy between the original and the compressed data .",
    "the output is the set of bases for the compressed data ( a low - dimensional approximation of the original data ) @xmath51 and the transformation matrix @xmath52 . here",
    ", @xmath53 is the actual dimension of the compressed data .    with this output",
    ", we can    * compress . * : :    we represent any data entry @xmath54 , the    @xmath55 column of @xmath56 ,    by the @xmath55 column of @xmath3 ,    @xmath57 , with usually    @xmath58 . * uncompress . * : :    an _ approximation _ of the data is reconstructed by    @xmath59 * evaluate the compression of out - of - sample data . * : :    given any @xmath60 that is not    equal to any column of @xmath0 , its compressed representation    in @xmath61 is    @xmath62      a critical part to facilitate the greedy algorithm and make the algorithm realistic is an efficient mechanism measuring ( or estimating ) the error @xmath63 under certain norm , @xmath64 , in step @xmath65 of the algorithm . in this work , we are using the @xmath66norm defined as follows . for a given symmetric and positive definite matrix @xmath67 ,",
    "the @xmath66norm of a vector @xmath60 is defined by @xmath68    for @xmath69 being any column of the data matrix @xmath0 and @xmath70 its low - dimensional approximation @xmath71 , it is easy to see that @xmath72    the choice of @xmath73 reflects the criteria of the data compression .",
    "typical examples are :    * * identity : * equal weights are assigned to each component of the data entry .",
    "this makes the quality of compression uniform . in this case , the evaluation of is greatly simplified and the algorithm is the fastest as shown below by the numerical results . * * general diagonal matrix : * this setting can be used if part of each data entry needs to be preserved better and other parts can afford less fidelity . * * general spd matrix : * this most general case can be helpful if the goal is to preserve data across different entries anisotropiclly .",
    "the goal is then to evaluate the error through as efficiently as possible for any given @xmath74 .",
    "this is achieved by employing an offline - online decomposition strategy where the @xmath74-independent parts are evaluated beforehand ( offline ) enabling a quick turnaround time for any given @xmath74 encountered online .",
    "the specifics are given in the next subsection .",
    "the offline - online decomposition of the computations and their complexities are as follows . here",
    ", we use @xmath75 to denote the number of nonzero entries of a sparse matrix @xmath73 .",
    "* offline * : :    the total cost is of order    @xmath76    +    * offline mgs * ; ;      every basis needs to orthogonalized against the current set of      bases .",
    "the total cost is of order @xmath77 .",
    "* offline calculation of errors * ; ;      the next basis is located by comparing each column with its      compressed version into the current space . to enable that",
    ", we      encounter the following computational cost :      +      * pre - computation * : :        of @xmath78 ( for @xmath79 in ) and        @xmath80 ( for @xmath81 in ) .",
    "the cost is of order        @xmath82 .      * expansion * : :        of @xmath83 and @xmath84 .",
    "the former takes        time of order @xmath85 , and the        latter of order @xmath86 .    * offline searching * ; ;      after these calculations , the comparison between the original and      compressed data is then only dependent on the size of      @xmath74 ( which is also the number of columns for      @xmath2 ) .",
    "the complexity is of order      @xmath87 .",
    "it will be repeated for up to      @xmath4 times in the searching process of step 2.3 of the      algorithm for each of the up to @xmath88 basis      elements .",
    "the total cost is at the level of      @xmath89 . *",
    "online * : :    given any ( possibly out - of - sample ) data    @xmath60 , its coefficients in the    compressed space is obtained by evaluating    @xmath90 .",
    "the cost is of order    @xmath91 .",
    "the decoding    ( @xmath92 ) can be done with the same cost .",
    "the online    computation has complexity of order @xmath93    we remark that , if the actual practice does not requires forming    @xmath70 ( e.g. clustering and classification etc ) and    so we only work with the coordinates @xmath74 of    @xmath69 in the compressed space , then the online cost will be    independent of @xmath16 and thus much smaller .",
    "in this section , we test the reduced basis decomposition on image compression , and data compression .",
    "lastly , we devise a simple face recognition algorithm based on rbd and test it on a database of @xmath94 images while comparing rbd with @xmath95 other face recognition algorithms",
    ". the computation is done , and thus the speedup numbers reported herein should be understood as , in matlab 2014a on a 2011 imac with a @xmath96 ghz intel core i7 processor .    [",
    "sec : numerical ]      we first test it on compressing two standard images lena and mandrill in figure [ fig : original ] .",
    "they both have an original resolution of @xmath97 .",
    "we take @xmath98 and test the algorithm . for each component of every image",
    ", we run the algorithm with @xmath99 which implies a compression ratio of @xmath100 , @xmath101 , and @xmath102 respectively .",
    "the resulting images ( formed by multiplying the corresponding @xmath2 and @xmath3 together ) are shown on the @xmath103 and @xmath104 row of figures [ fig : lenaresult ] . as a comparison ,",
    "we run svd and obtain the reconstructed matrices with the first @xmath88 singular values accordingly .",
    "the resulting images are on the second and last row .",
    "clearly , svd provides the best quality pictures among all possible algorithms ( and thus better than what rbd provides ) .",
    "however , we see that the rbd pictures are only slightly blurrier .",
    "moreover , it takes much less time .",
    "in fact , we show the comparison in time between svd and rbd in table [ tab : lenatime ] .",
    "we see that , when @xmath105 , rbd is three times faster than svd and seven times faster when @xmath106 . here",
    "the svd time is the shorter between those taken by ` vd } and { \\verb ` vds commands in matlab .     from left to right .",
    "the first and third row are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right .",
    "the first and third row are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ] +   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right .",
    "the first and third row are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ] +   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ] +   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right .",
    "the first and third row are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]   from left to right . the first and third row",
    "are from reduced basis decomposition , and the second and fourth are from singular value decomposition.,title=\"fig:\",scaledwidth=32.0% ]    .relative computational time for image compression . [ cols=\"^,^,^\",options=\"header \" , ]                 we use the umist database @xcite that is publicly available on roweis web page .",
    "table [ tab : datasetinfo ] summarizes its characteristics : it contains @xmath107 people under different poses .",
    "the number of different views per subject varies from @xmath108 to @xmath109 .",
    "we use the cropped version whose snapshot is shown in figure [ fig : umist ] .    as in @xcite , we randomly choose @xmath110 views from each class to form a training set .",
    "the rest of the samples ( @xmath111 of them ) are used as testing images .",
    "we show the average classification error rates in figure [ fig : fr_result ] left .",
    "these averages are computed over @xmath112 random formations of the training and test sets . shown in the middle",
    "are the results of six traditional dimension reduction techniques taken from @xcite .",
    "clearly , our method has similar performance as the pca method , and outperforms three of the other five methods . however , rbd is much faster than pca and other methods since they all involves solving eigenproblems @xcite .",
    "a speedup factor as a function of the number of bases is plotted in figure [ fig : fr_result ] right which demonstrates a speedup factor of larger than two for this particular test when we reach the asymptotic region ( around when the number of basis vectors is @xmath113 ) .",
    "this paper presents and tests an extremely efficient dimension reduction algorithm for data processing .",
    "it is multiple times faster than the svd / pca - based algorithms .",
    "what makes this possible is a greedy algorithm that iteratively builds up the reduced space of basis vectors .",
    "each time , the next dimension is located by exploring the errors of compression into the current space for all data entries .",
    "thanks to an offline - online decomposition mechanism , this searching is independent of the size of each entry . numerical results including one concerning a real world face recognition problem confirm these findings .",
    "m.  barrault , n.  c. nguyen , y.  maday , and a.  t. patera , _ an `` empirical interpolation '' method : application to efficient reduced - basis discretization of partial differential equations _ , c. r. acad .",
    "paris , srie i * 339 * ( 2004 ) , 667672 .",
    "a.  buffa , y.  maday , a.  t. patera , c.  prudhomme , and g.  turinici , _ a priori convergence of the greedy algorithm for the parametrized reduced basis _ , esaim - math . model .",
    "( 2011 ) , special issue in honor of david gottlieb .",
    "j.  p. fink and w.  c. rheinboldt , _ on the error behavior of the reduced basis technique for nonlinear finite element approximations _ , z. angew .",
    "* 63 * ( 1983 ) , no .  1 , 2128 .",
    "mr mr701832 ( 85e:73047 )    d.  b graham and n.  m allinson , _ characterizing virtual eigensignatures for general purpose face recognition _ , face recognition : from theory to applications ( h.  wechsler , p.  j. phillips , v.  bruce , f.  fogelman - soulie , and t.  s. huang , eds . ) , nato asi series f , computer and systems sciences , vol .",
    "163 , 1998 , pp .  446456 .",
    "m.  a. grepl , y.  maday , n.  c. nguyen , and a.  t. patera , _ efficient reduced - basis treatment of nonaffine and nonlinear partial differential equations _",
    ", mathematical modelling and numerical analysis * 41 * ( 2007 ) , no .  3 , 575605 .",
    "l.  machiels , y.  maday , i.  b. oliveira , a.  t. patera , and d.  v. rovas , _ output bounds for reduced - basis approximations of symmetric positive definite eigenvalue problems _ , c. r. acad .",
    "* 331 * ( 2000 ) , no .  2 , 153158",
    ". mr mr1781533 ( 2001d:65148 )    y.  maday , _ reduced basis method for the rapid and reliable solution of partial differential equations _ , international congress of mathematicians . vol .",
    "iii , eur .",
    "soc . , zrich , 2006 , pp .",
    "mr 2275727 ( 2007m:65099 )    y.  maday , a.  t. patera , and d.  v. rovas , _ a blackbox reduced - basis output bound method for noncoercive linear problems _ , nonlinear partial differential equations and their applications .",
    "collge de france seminar , vol .",
    "xiv ( paris , 1997/1998 ) , stud .",
    "31 , north - holland , amsterdam , 2002 , pp .",
    "mr mr1936009 ( 2003j:65120 )    y.  maday , a.  t. patera , and g.  turinici , _ a priori convergence theory for reduced - basis approximations of single - parameter elliptic partial differential equations _",
    "* 17 * ( 2002 ) , 437446 .",
    "nguyen , k.  veroy , and a.  t. patera , _ certified real - time solution of parametrized partial differential equations _",
    ", handbook of materials modeling ( sidney yip , ed . ) , springer netherlands , 2005 , pp .",
    "15291564 ( english ) .        c.  prudhomme , d.  rovas , k.  veroy , y.  maday , a.  t. patera , and g.  turinici , _ reliable real - time solution of parametrized partial differential equations : reduced - basis output bound methods _ , journal of fluids engineering * 124 * ( 2002 ) , no .  1 , 7080 .    g.  rozza , d.b.p .",
    "huynh , and a.t .",
    "patera , _ reduced basis approximation and a posteriori error estimation for affinely parametrized elliptic coercive partial differential equations : application to transport and continuum mechanics _ , arch comput methods eng * 15 * ( 2008 ) , no .  3 , 229275 .",
    "s.  sen , k.  veroy , d.b.p .",
    "huynh , s.  deparis , n.c .",
    "nguyen , and a.t .",
    "`` natural norm '' a posteriori error estimators for reduced basis approximations _ , j. comput .",
    "* 217 * ( 2006 ) , no .  1 , 37  62 ."
  ],
  "abstract_text": [
    "<S> dimension reduction is often needed in the area of data mining . </S>",
    "<S> the goal of these methods is to map the given high - dimensional data into a low - dimensional space preserving certain properties of the initial data . </S>",
    "<S> there are two kinds of techniques for this purpose . </S>",
    "<S> the first , projective methods , builds an explicit linear projection from the high - dimensional space to the low - dimensional one . on the other hand , </S>",
    "<S> the nonlinear methods utilizes nonlinear and implicit mapping between the two spaces . in both cases , </S>",
    "<S> the methods considered in literature have usually relied on computationally very intensive matrix factorizations , frequently the singular value decomposition ( svd ) . </S>",
    "<S> the computational burden of svd quickly renders these dimension reduction methods infeasible thanks to the ever - increasing sizes of the practical datasets .    in this paper , we present a new decomposition strategy , reduced basis decomposition ( rbd ) , which is inspired by the reduced basis method ( rbm ) . </S>",
    "<S> given @xmath0 the high - dimensional data , the method approximates it by @xmath1 with @xmath2 being the low - dimensional surrogate and @xmath3 the transformation matrix . </S>",
    "<S> @xmath2 is obtained through a greedy algorithm thus extremely efficient . </S>",
    "<S> in fact , it is significantly faster than svd with comparable accuracy . </S>",
    "<S> @xmath3 can be computed on the fly . moreover , unlike many compression algorithms , it easily finds the mapping for an arbitrary `` out - of - sample '' vector and it comes with an `` error indicator '' certifying the accuracy of the compression . </S>",
    "<S> numerical results are shown validating these claims </S>",
    "<S> .    data mining , lossy compression , reduced basis method , singular value decomposition , greedy algorithm </S>"
  ]
}