{
  "article_text": [
    "a new form of generalized nonextensive entropy recently proposed by us@xcite has been shown in to give small but interesting departures from the shannon case in terms of thermodynamic properties of a system in a manner similar to but also somewhat different from tsallis entropy @xcite .",
    "conceptually , the new entropy appears from a novel definition of entropy in terms of the rescaled phase cells due to correlated clusters , and in a limit similar to the tsallis case approaches shannon s classical extensive entropy .",
    "kaniadakis has also suggested @xcite the use of deformed functions leading to unusual forms of entropy , from a kinetic principle related to phase space , which gives excellent results for cosmic ray spectra . in our case",
    "the definition of entropy is particularly simple , as it can be expressed simply as the divergence of a vector representing the modified probabilities for the different possible states taking into account a rescaling due to correlations or clustering due to interactions between the microsystems .    in microscopic systems",
    "quantum entanglement of states is also a relevant issue .",
    "some authors @xcite have studied the problem of quantum entanglement of two states in the picture of tsallis type nonextensive entropy .",
    "the generalization of shannon entropy to the very similar von neumann entropy using density operators in place of probability distributions @xcite reveals common features of the stochastic and the quantum forms of uncertainties and this treatment can be extended to tsallis form too .",
    "our purpose here is to present a combined study of stochasticity and quantum entanglement , so that the former emerges from the quantum picture in a natural way , and then we intend to show that our new approach of defining entropy also allows us to obtain a measure of mutual information that involves stochasticity and entanglement together in a clear comprehensible way .",
    "the fact that our new definition of entropy , which is conceptually very simple , also gives the probability distribution function in a closed form in terms of lambert @xmath0 functions @xcite allows one to carry out many calculations with the same ease as for tsallis entropy . in this work , however , the probability distribution will not be needed for explicit use .",
    "entropy is intuitively associated with randomness , because it is a measure of the loss of information about a system , or the indeterminacy of its exact state , which in turn depends on the probability distribution for various states .",
    "a uniform probability distribution function ( pdf ) among all states indicates maximal uncertainty in state space and gives the maximal entropy , whereas dirac / kronecker delta ( continuous or discrete states ) pdf with no uncertainty has zero entropy .",
    "combinatorics gives the boltzmann form    @xmath1\\ ] ]    because in equilibrium the @xmath2 are simply @xmath3 , where @xmath4 is the total number of subsystems , and @xmath2 is the number of subsystems in the i - th state . in terms of the @xmath5 themselves",
    "one gets the shannon form given below .",
    "it is well - known that maximizing the entropy with the constraints @xmath6 and ( with @xmath7 the energy of the i - th state , and u the total energy , which is fixed ) @xmath8 gives the exponential probability distribution    @xmath9    where the lagrange multiplier constant @xmath10 can be identified as the inverse of the temperature .",
    "let us now consider shannon coding theorem @xcite : when the letters @xmath11 of the alphabet used in a code have the probabilities @xmath5 , then it can be shown fairly easily that a stream of random letters coming out of the source with the given probabilities in the long run will relate the entropy to the probability of the given sequence :    @xmath12\\ ] ]    where @xmath13 is the entropy per unit and @xmath4 is the large number of letters in the sequence .",
    "we shall now define entropy from a somewhat different viewpoint which takes into account interaction among the units , producing clusters of size @xmath14 units for the i - th state .",
    "this effective size may in general be a fraction , and if the interaction is weak , the average cluster size @xmath14 is just over unity . if we think of liquid clusters , the typical subsystem in state @xmath15 may bean assembly of @xmath16 molecules , but this may change due to environmental factors , such as ph value , to @xmath17 , so that we have a rescaling value of @xmath18 , which may be greater or less than 1 .",
    "in general we allow @xmath14 to be different for each @xmath15 .",
    "since @xmath5 is the probability of a single occurrence of the i - th state , i.e. for a cluster of size unity ( which may consist of a typical number of subunits ) , the probability for the formation of a cluster of size @xmath14 is @xmath19 . let us now consider the vector    @xmath20    this is @xmath21-dimensional , where @xmath21 is the number of single unit states available to the system components .",
    "let us now consider the  phase space \" defined by the @xmath14 co - ordinates . as we have said above ,",
    "the deviations of these parameters from unity give the effective ( which may be fractional when an average is taken ) cluster sizes in each of the states . a value smaller than unity indicates a degeneration of the micro - system to a smaller one in a hierarchical fashion , partially if it is a fraction . in other words",
    "we are considering a scenario where clusters may form superclusters or be composed of subclusters , with a corresponding change of scale in terms of the most basic unit obtainable .",
    "we have dealt elsewhere with the interesting question an oligo - parametric hierarchical structure of complex systems @xcite , but here , we restrict ourselves to cluster hierarchy changes that do no qualitatively change the description of the system .    hence , if we take the divergence of the vector @xmath22 in the @xmath14 space , it is a measure of the escape of systems from a given configuration of correlated clusterings . and , inversely , the negative of the divergence shows the net influx of systems into an infinitesimal cell with cluster sizes @xmath14 . if all the @xmath14 are unity , then we have unfragmented and also non - clustered , i.e. uncorrelated units at that hierarchy level",
    ".    we can argue first from the point of view of statistical mechanics that this negative divergence or influx of probability , may be interpreted as entropy .",
    "@xmath23    we know that the free energy is defined by    @xmath24    where @xmath25 is the free energy , @xmath26 is the internal energy , and @xmath13 is the entropy .",
    "@xmath27 is the temperature , or a measure of the average random thermal energy per unit with boltzmann constant @xmath28 chosen to be unity , and hence @xmath29 is a measure of the random influx of energy into the system due to the breaking / making of correlated clusters due to random interactions in a large system .",
    "this allows the subtracted quantity @xmath25 to be the free energy or the  useful \" energy .",
    "there are usual thermodynamic phase space factors in dealing with a macroscopic system , which we drop as common factors in what follows .    in terms of the shannon coding theorem",
    "also we arrive at the same expression .",
    "since , with @xmath14 average clustering the i - th state occurs with probability @xmath30 , a stream of units ( clustered ) emitted will correspond to the probability    @xmath31 = \\prod_i p_i^{p_i^{q_i}}\\ ] ]    which too gives us eqn .",
    "[ ent ] .    in @xcite",
    "we have developed the statistical , mechanics of this entropy in detail , by first obtaining its probability distribution function in terms of the lambert w function .",
    "however , we used , for simplicity an isotropic ( in state space ) correlation and rescaling , i.e. we had a single common @xmath32 , as in tsallis entropy . in the rest of this work we shall use the same simpler expression .",
    "when the state vector in the combined hilbert space @xmath33 of two particles ( or subsystems in @xmath34 and @xmath35 ) can not be expressed as the factorizable product of vectors in the hilbert spaces of the two subsystems , it is by definition entangled .",
    "hence entanglement is actually a property related to projection in the subspaces , and can not be expected to be measurable by properties in the bigger space alone .    given the state    @xmath36    which gives density matrix for the @xmath37 space for the trace over the @xmath38 part    @xmath39    where the @xmath40 are now the coefficient matrices . in terms of density matrices",
    "an entangled state ( for an explicit example ) of two qubits ( a  qubit \" , or quantum bit , being a quantum superposition of two possible states ) may be expressed by the reduced @xmath41 matrix from the @xmath42 basis sub - set :    @xmath43\\ ] ]    with @xmath44 for the pure quantum ( entangled ) state    @xmath45 , and @xmath46 .",
    "this entanglement occurs in the subspace of the product hilbert space involving only the two basis vectors @xmath47 and @xmath48 .",
    "other entangled combinations are equivalent to this form and may be obtained from it simply by relabeling the basis vectors , and hence we shall use this as the prototype .    for @xmath49 , we have an impure state with a classical stochastic component in the probability distribution , although we still have probability conservation as @xmath50 which remains unchanged under any unitary transformation .",
    "a possible measure of the factorizability (  purity \" @xcite ) @xmath51 of a quantum state , or its quantum non - entanglement , remains invariant under changes of @xmath52    @xmath53 \\nonumber \\\\",
    "= c^4 + s^4 .",
    "\\end{aligned}\\ ] ]    so , for the maximum entanglement @xmath54 when @xmath55 , and the minimal entanglement corresponds to @xmath56 ( pure factorizable states ) when @xmath57 .",
    "quantum impurity represented by classical stochasticity attains the maximum value when @xmath58 , and is nonexistent when @xmath44 corresponding to a pure entangled state .",
    "we note that @xmath51 does not involve the stochasticity - related parameter @xmath52 at all , but remains the quantifier of the quantum entanglement .",
    "another equivalent but conceptually possibly more interesting way of quantifying entanglement may be the parameter    @xmath59 - tr[\\rho_a]tr[\\rho_b ] ) = \\sin^2(2 \\theta)\\end{aligned}\\ ] ]    which is more symmetric in the two subspaces and is similar to a correlation function .",
    "it gives 0 for no entanglement when @xmath60 , and maximal entanglement 1 for @xmath55 , as desired .",
    "this definition of entanglement is in the spirit of mutual information , though we have not used the entropy at this stage , but only the probabilities directly .",
    "it too does not involve the stochasticity in terms of the purity parameter @xmath52 . in the relation above we have used    @xmath61\\ ] ]    and similarly for @xmath62 . in our specific case , for @xmath25 or for @xmath63 ,    @xmath64\\ ] ]    with @xmath65 = 1 $ ] ensured .",
    "it is possible to formulate the stochasticity by coupling the entangled state @xmath66 to the environment state @xmath67 quantum mechanically and then taking the trace over the environment states .",
    "@xmath68    and the trace over the environment yields    @xmath69    for the entangled mixture of @xmath47 and @xmath48 in @xmath70 and the couplings    @xmath71    with @xmath72 and @xmath73 ,    the trace over @xmath74 states gives the density matrix    @xmath75\\ ] ]    so , classical stochasticity has been introduced by taking the trace over the environment space with    @xmath76",
    "let us consider a single system @xmath25 interacting with the environment @xmath77 .",
    "the product space @xmath78 contains entanglement between the measured system and the environment , and hence the density operator for the combined system - environmental space is as given in eqn .",
    "[ rhogam ] with @xmath44 to indicate a pure entangled state , and the environment - traced density is given by eqn .",
    "[ rhoabtr ] . here",
    "@xmath79 and @xmath80 are equal .",
    "mutual information may be identified as the entanglement , and defined by    @xmath81 - ( tr_{a}[\\rho_a])^2 ) = \\sin^2(2\\theta')\\end{aligned}\\ ] ]    as before , with @xmath82 the angle of entanglement .",
    "hence , measurements on the system @xmath25 reflects the coupling of the system to the environment to the environment , and the mutual information is contained in the parameters of the system itself . in terms of von neumann entropy , which , with mixing in an orthogonal quantum basis , becomes similar to shannon entropy @xcite ,    @xmath83\\ ] ]    .",
    "we know from the araki - lieb relation@xcite    @xmath84    that , with @xmath85 in a pure quantum state , we must have @xmath86 , and hence    @xmath87\\end{aligned}\\ ] ]    which too confirms the view that the system itself contains in its parameters the mutual information in such a case , as we found above .    if we use our new form of entropy@xcite with the hypothesis that the mutual information is still given by the same form with the parameter @xmath32 not equal to @xmath88 , which is the case for shannon entropy , then we get    @xmath89 \\nonumber   \\\\   = - 2c'^{2q}\\log(c'^{2q } ) -2 s'^{2q } \\log(s'^{2q})\\end{aligned}\\ ] ]      with the 3-system entanglement shown in eqn .",
    "[ psiabe ] and the relatively simple choice of couplings in eqn .",
    "[ carr ] , we have already shown @xmath90 in eqn .",
    "[ rhoc ] .",
    "similar construction of @xmath91 , @xmath92 and @xmath93 , and defining the 3-system mutual information as    @xmath94    and with @xmath95 for any @xmath32 , for a single 3-system pure state , we may find the 3-system mutual information .",
    "tracing over the @xmath63 space gives @xmath91 , which , using as basis @xmath47 , @xmath96 , @xmath97 and @xmath48 in the @xmath98 product space , yields    @xmath99\\ ] ]    and an identical matrix for @xmath92 .",
    "finally we get using the relevant eigenvalues    @xmath100    where the eigenvalues @xmath101 and @xmath102 are for the @xmath90 matrix obtained after tracing over @xmath77-space .",
    "@xmath103)\\ ] ]    with @xmath52 given by eqn .",
    "[ gamma ] .",
    "had we started with a stochastic picture of entangled impure a - b system , with @xmath104 representing the stochasticity , as we have suggested above , then the mutual information would be    @xmath105    in fig .",
    "[ fig1 ] we first show the mutual information ( mi ) calculated according to the shannon form of the entropy , which is equivalent to our form at @xmath106 , as a function of the @xmath107 entanglement angle @xmath108 and the entanglement angle of @xmath109-system with the environment @xmath82 , which is related to the stochasticity @xmath52 as explained above .",
    "we note that mutual information is virtually independent of the angle of entanglement with the environment @xmath82 .",
    "hence , it seems that traditional entropy in this case is insensitive to details of coupling with the environment when the mutual information between two systems is measured .",
    "@xmath110 as a function of entanglement angle @xmath108 in a - b space and the entanglement angle @xmath82 with the environment , which is related to the stochasticity.,width=302 ]    in fig .",
    "[ fig2 ] and in fig .",
    "[ fig3 ] we show the deviations @xmath111 of our mi from the shannon mi , as a function @xmath32 and entanglement angles @xmath82 and @xmath108 respectively , keeping the other angle at @xmath112 in each case .",
    "there is symmetry around @xmath112 .",
    "the variation is fairly smooth for fixed @xmath82 , i.e. fixed entanglement with the environment .",
    "however , if the entanglement between @xmath25 and @xmath63 is kept fixed at near @xmath112 , then the mutual information using our form of entropy changes sharply with @xmath82 near the symmetry value @xmath113 .",
    "one can see that this comes from one of the eigenvalues of the density matrix @xmath90 approaching zero for this mixing value , and with @xmath114 , the there is either a sharp peak or dip compared to the shannon entropy case , which has fixed @xmath115 .",
    "difference of mi from our entropy with that from shannon entropy at @xmath116,width=302 ]    same as fig .",
    "[ fig2 ] with @xmath117,width=302 ]    fig .",
    "[ fig2 ] shows little variation with changing @xmath82 for almost any @xmath32 .",
    "[ fig3 ] shows pronounced changes at small q ( @xmath118)for different @xmath108 .",
    "mi difference between our entropy form and shannon for @xmath119,width=302 ]    same as fig .",
    "[ fig4 ] but for @xmath120,width=302 ]    in fig .",
    "[ fig4 ] and fig .",
    "[ fig5 ] we show the difference between our mi and the shannon mi as a function of @xmath108 and @xmath82 simultaneously , keeping @xmath32 fixed at @xmath121 and at @xmath122 .",
    "of course at @xmath106 , we get no difference , as our entropy then coincides with the shannon form . here",
    "too we notice that the mixing angle between @xmath25 and @xmath63 shows fairly smooth variation , but @xmath82 or equivalently the stochasticity , causes pronounced peak ( for @xmath123 ) , or dip ( for @xmath124 ) . here",
    "too we can conclude that our method of entropy calculation can indicate a greater role of the entanglement with the environment when this mixing is nearly equal for the @xmath107 entangled states .",
    "difference between mi from our entropy and tsallis s with @xmath55,width=302 ]    same as fig .",
    "[ fig6 ] but with @xmath117.,width=302 ]    in view of the prevalent familiarity with the tsallis form of nonextensive entropy , we have previously @xcite compared our form with results from tsallis entropy in formulating a general thermodynamics .",
    "there we showed that despite the conceptual and functional differences between tsallis entropy and our new form , the results are very similar if we take the tsallis @xmath32 to be twice as far from unity ( the shannon equivalent value ) as our value of @xmath32 . in fig .",
    "[ fig6 ] and fig .",
    "[ fig7 ] we show the difference of our mi from that derived from tsallis s entropy .",
    "we again note that among @xmath108 and @xmath82 differences are both relatively more significant for @xmath32 values different from @xmath88 , for both angles near @xmath125 , with peaks and dips similar to the comparison with the mutual information calculated with shannon entropy .",
    "it is interesting to note here that recently in a study of the entropy of a chain of spins in a magnetic field @xcite it has been found that both the usual von neumann and renyi forms of entropy yield nonzero and surprisingly simple closed expressions . though this work does not mention entanglement explicitly , the correlation functions presented here , which determine the density matrix and hence its diagonalized form needed for entropy calculation , actually are manifestations of the entanglement among the spins and between the spins and the magnetic field .",
    "the chain has been split into two parts , similar to our a and b subsystems , and the external magnetic field acts like the environment we have introduced in this work . though they carry out their extensive calculations at zero temperature , unlike our finite temperature treatment , the fact they obtain nonzero @xmath126 for the first @xmath127 spins is apparently due to the segmentation of the pure state of the fully entangled quantum system and the consideration of part @xmath25 only for the entropy calculation , which effectively is equivalent to summing the states of part @xmath63 and the entanglement with the environment , and produces entropy due to the corresponding loss of information about state of the whole system .",
    "hence , their results for this explicit model is consistent with our general result that classical stochasticity and entropy may be a reflection of segmented consideration of bigger complete systems .",
    "the values of the entropy of different types such as the canonical shannon form or generalized forms such as the renyi form , which goes to the shannon form in the usual limit , like that of the related parameter we have mentioned for tsallis entropy and for our new form of entropy in this work , reflect the extent of entanglement or interaction or , equivalently , correlation . in their work",
    "a length scale comes out of this segmentation , which appears to be similar to the angle of entanglement in our case .",
    "we do not get a phase transition as they do , because we have considered a simplified general finite system of only two or three component subsystems , not an infinite chain , and finite systems can not show any phase transitions .",
    "we have shown how a simple definition of the entropy in terms of influx of states into cells of any given cluster sizes in various states can give us a new nonextensive form of entropy with a closed form of the probability distribution function , and which coincides with the usual form for uncorrelated microsystems .",
    "we have then seen that classical stochasticity can be derived from quantum entanglement with the environment , and it influences the mutual information between quantum states .",
    "the functional form of the mi differs as per the definition of the entropy , but the numerical differences in mi resulting from various forms of entropy usually differ rather subtly according the parameters of entanglement , within the system , and with the environment , as well as the scaling type parameter @xmath32 introduced in the tsallis form and in the new form introduced by us .",
    "however , for the angle of entanglement with the environment our entropy differs from both the shannon entropy and tsallis entropy when both angles of entanglement are near the symmetry point @xmath112 .",
    "the differences between the forms become more pronounced as @xmath32 varies from unity .",
    "entanglement and mutual information are such fundamental concepts that experimental tests need to be designed to distinguish from minute quantitative differences the appropriateness of various theoretical forms of entropy .",
    "theoretical works like the study of large systems such as spin chains @xcite may also help differentiate the appropriateness of various forms of entropy such as shannon , tsallis , renyi or our suggested new form , and the quantification of mutual information . the notion of clusters changing constantly into various sizes , which is the basis of our definition of the new form of generalized entropy may be the most relevant concept for the treatment of liquids and other material , where such phenomena form an integral part of the dynamics .",
    "the author would like to thank andrew tan and ignacio sola for discussions and g. kaniadakis and j.f .",
    "collet for useful feedback .",
    "00 1999 quantum entanglement inferred by the principle of maximum tsallis entropy .",
    "a _ * 60 * , 34613466 . 2002 nonadditive entropies and quantum entanglement . _ physica a _ * 306 * , 316 1970 entropy inequalities .",
    "phys . _ * 18 * , 160170 .",
    "2001 classical and quantum complexity and non - extensive thermodynamics .",
    "_ chaos , fractals and solitons _ * 13 * , 367370 .",
    "2004 quantum spin chains , toeplitz determinants and the fisher - harwig conjecture .",
    "_ j. stat .",
    "_ * 116 * , 7995 .",
    "2001 nonlinear kinetics underlying generalized statistics . _",
    "physica a _ * 296 * , 405425 .",
    "2002 statistical mechanics in the context of special relativity .",
    "e _ * 66 * , 056125 .",
    "( john wiley , ny , 1998 ) p. 368 .",
    "( cambridge u.p . ,",
    "ny , 2000 ) 1994 the classical n - body problem within a generalized statistical mechanics._j .",
    "* 27 * , 57075757 . 2007",
    "the lambert function and a new nonextensive entropy _",
    "i m a j. appl .",
    "( in press ) 2007 oligo - parametric hierarchical structure of complex systems .",
    "_ neuroquantology journal _ * 5 * , 8599 1988 possible generalization of boltzmann - gibbs statistics . _ j. stat .",
    "phys . _ * 52 * , 479487 .",
    "2002 entanglement versus bell violations and their behaviour under local filtering operations .",
    "lett . _ * 89 * , 170401 .",
    "1999 entanglement and nonextensive statistics .",
    "lett a _ * 260 * , 335 - 339 ."
  ],
  "abstract_text": [
    "<S> we first show how a new definition of entropy , which is intuitively very simple , as a divergence in cluster - size space , leads to a generalized form that is nonextensive for correlated units , but coincides exactly with the conventional one for completely independent units . </S>",
    "<S> we comment on the relevance of such an approach for variable - size microsystems such as in a liquid . </S>",
    "<S> we then indicate how the entanglement and purity of a two - unit compound state can depend on their entanglement with the environment . </S>",
    "<S> we consider entropies of tsallis , which is used in many different real - life contexts , and also our new generalization , which takes into account correlated clustering in a more transparent way , and is just as amenable mathematically as that of tsallis , and show how both purity and entanglement can appear naturally together in a measure of mutual information in such a generalized picture of the entropy , with values differing from the shannon type of entropy . </S>",
    "<S> this opens up the possibility of using such an entropy in a quantum context for relevant systems , where interactions between microsystems makes clustering and correlations a non - ignorable characteristic . </S>"
  ]
}