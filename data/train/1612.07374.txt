{
  "article_text": [
    "_ outlier detection _ is a data analysis task that aims to find atypical behaviors , unusual outcomes , erroneous readings or annotations in data .",
    "it has been an active research topic in data mining community , and it is frequently used in various applications to identify rare and interesting data patterns , which may be associated with beneficial or malicious events , such as fraud identification @xcite , network intrusion surveillance @xcite , disease outbreak detection @xcite , patient monitoring for preventable adverse events ( pae ) @xcite , _ etc_. it is also utilized as a primary data preprocessing step that helps to remove noisy or irrelevant signals in data @xcite .    despite an extensive research",
    ", the majority of existing outlier methods are developed to detect _ unconditional _ outliers that are expressed in the joint space of all data attributes .",
    "such methods may not work well when one wants to identify _ conditional _ ( contextual ) outliers that reflect unusual responses for a given set of contextual attributes .",
    "briefly , since conditional outliers depend on the context or properties of data instances , application of unconditional outlier detection methods may lead to incorrect results .",
    "for example , assume we want to identify incorrect ( or highly unusual ) image annotations in a collection of annotated images . then by applying unconditional detection methods to the joint image - annotation space",
    "may lead to images with rare themes to be falsely identified as outliers due to the scarcity of these themes in the dataset , leading to false positives .",
    "similarly , an unusual annotation of images with frequent themes may not be judged ( scored ) as very different from images with less frequent themes leading to false negatives .",
    "this paper focuses on _ multivariate conditional outlier detection _",
    ", a special type of the conditional outlier detection problem where data consists of @xmath0-dimensional continuous input vectors ( context ) and corresponding @xmath1-dimensional binary output vectors ( responses ) .",
    "our goal is to precisely identify the instances with unusual input - output associations . following the definition of outlier given by hawkins @xcite , we give a description of multivariate conditional outlier in plain language as :    a multivariate conditional outlier is an observation , which consists of context and associated responses , whose responses are deviating so much from the others in similar contexts as to arouse suspicions that it was generated by a different response mechanism .",
    "this formulation fits well various practical outlier detection problems that require contextual understanding of data .",
    "as briefly illustrated above , for example , recent social media services allow users to tag their content ( _ e.g. _ , online documents , photos , or videos ) with keywords and thereby permit keyword - based retrieval .",
    "these user annotations sometimes include irrelevant words by mistake that could be effectively pinpointed if the conditional relations between content and tags are considered .",
    "likewise , evidence - based expert decisions ( _ e.g. _ , functional categorization of genes , medical diagnosis and treatment decisions of patients ) occasionally involve errors that could cause critical failures .",
    "such erroneous decisions would be adequately detected through contextual analysis of evidence - decision pairs .",
    "the multivariate conditional outlier detection problem is challenging because both the contextual- and inter - dependences of data instances should be taken into account when identifying outliers .",
    "we tackle these challenges by building a probabilistic model @xmath2 , where @xmath3 denotes the input variables and @xmath4 denotes the associated output variables . briefly ,",
    "the model is built ( learned ) from all available data , aiming to capture and summarize all relevant dependences among data attributes and their strength as observed in the data .",
    "conditional outliers are then identified with the help of this model .",
    "more specifically , a conditional outlier corresponds to a data instance that is assigned a low probability by the model .",
    "the exact implementation of the above approach is complicated , and multiple issues need to be resolved before it can be applied in practice .",
    "first , it is unclear how the probabilistic model @xmath2 should be represented and parameterized . to address this problem ,",
    "we resort to and adapt structured probabilistic data models of @xmath2 that provide an efficient representation of input - output relations by decomposing the model using the chain rule into a product of univariate probabilistic factors @xmath5 ; _ i.e. _ , each response @xmath6 is dependent on @xmath7 and a subset of the other responses @xmath8 .",
    "the univariate conditional models and their learning are rather common and well studied , and multiple models ( _ e.g. _ , logistic regression ) can be applied to implement them .",
    "we note the structured probabilistic data models were originally proposed and successfully applied to support structured output prediction problems @xcite . however , their application to outlier detection problems is new .",
    "the key difference is that while in prediction we seek to find outputs that maximize the probability given the inputs , in conditional outlier detection we aim to identify unusual ( or low probability ) associations in between observed inputs and outputs .",
    "the second issue is that the probabilistic model must be learned from available data which can be hard especially when the number of context and output variables is high and the sample size is small .",
    "this may lead to model inaccuracies and miscalibration of probability estimates , which in turn may effect the identification of outliers . to alleviate this problem , we formulate and present outlier scoring methods that combine the probability estimates with the help of weights reflecting their reliability in assessment of outliers .    through empirical studies ,",
    "we test our approach on datasets with multi - dimensional responses .",
    "we demonstrate that our method is able to successfully identify multivariate conditional outliers and outperforms the existing baselines .",
    "the rest of this paper is organized as follows .",
    "section [ sec : problem_def ] formally define the problem .",
    "section [ sec : related ] reviews existing research on the topic .",
    "section [ sec : approach ] describes our multivariate conditional outlier detection approach .",
    "section [ sec : experiments ] presents the experimental results and evaluations .",
    "lastly , section [ sec : concl ] summarizes the conclusions of our study .",
    "in this work , we study a special type of the conditional outlier detection problem where data consist of multi - dimensional input - output pairs ; that is , each instance in dataset @xmath9 consists of an @xmath0-dimensional continuous input vector @xmath10 and a @xmath1-dimensional binary output vector @xmath11 .",
    "our goal is to detect irregular response patterns in @xmath12 given context @xmath7 .",
    "the fundamental issues in developing a multivariate conditional outlier detection method are how to take into account the _ contextual dependences between output @xmath12 and their input @xmath7 _ , as well as the _ mutual dependences among @xmath12_. we address these issues by building a decomposable probabilistic representation for @xmath13 .",
    "note that multivariate conditional outlier detection is clearly different from unconditional outlier detection when the problems are expressed probabilistically . in conditional outlier detection , we are interested in the instances that fall into low - probability regions of the conditional joint distribution @xmath14 . on the other hand ,",
    "unconditional outlier detection approaches generally seek instances in low - probability regions of the joint distribution @xmath15 .",
    "* notation : * for notational convenience , we will omit the index superscript @xmath16 when it is not necessary .",
    "we may also abbreviate the expressions by omitting variable names ; e.g. , @xmath17 .",
    "outlier detection has been extensively studied in the data mining and statistics communities @xcite .",
    "a wide variety of approaches to tackle the detection problem for multivariate data have been proposed in the literature .",
    "accordingly , depending on the type of outliers the method aims to detects , five general categories of _ unconditional _ outlier detection approaches appear in the literature .",
    "these include density - based approaches @xcite , distance - based approaches @xcite , depth - based approaches @xcite , deviation - based approaches @xcite , and high - dimensional approaches @xcite .",
    "below we briefly summarize each of these categories .",
    "for technical details , please refer to @xcite .",
    "density - based approaches assume that the density around a normal data instance is similar to that of its neighbors @xcite .",
    "a typical representative method is local outlier factor ( lof ) @xcite , which measures a relative local density in @xmath18-nearest neighbor boundary .",
    "lof has shown good performance in many applications and is considered as an off - the - shelf outlier detection method . in section [",
    "sec : experiments ] , we use lof as the representative unconditional outlier detection method and compare the performance with our proposed approach .",
    "distance - based approaches assume that normal data instances come from dense neighborhoods , while outliers correspond to isolated points .",
    "a representative method is @xcite which gives an outlier score to each instance using a robust variant of the mahalanobis distance @xcite , measuring the distance between each instance to the main body of data distribution such that the instances located far from the center of data distribution are identified as outliers .",
    "depth - based approaches assume that outliers are at the fringe of the data regions and normal instances are close to or in the center of the region . the methods in this category assign depth @xmath18 to each instance by gradually removing data from convex hulls , and the instances with small depth are considered as outliers @xcite .",
    "a relevant method is the one - class support vector machines @xcite , which assumes all the training data belong to the `` normal '' class and finds a decision boundary defining the region of normal data , whereas instances lie across the boundary are identified as outliers .",
    "deviation - based approaches assume that outliers are the outmost data instances in the data region and can be identified by measuring the impact of each instance on the variance of the dataset .",
    "one of the well - known algorithms in this category is linear method for deviation detection ( lmdd ) @xcite . compared to the depth - based approaches",
    ", deviation - based approaches do not require complicated contour generation process .",
    "in high - dimensional spaces , the above approaches often fail because the distance metrics and density estimators become computationally intractable and analytically ineffective .",
    "moreover , due to the sparsity of data , no meaningful neighborhood can be defined .",
    "high - dimensional approaches are proposed to handle such extreme cases .",
    "typical methods in this category project the data to a lower dimensional subspace , such as grid - based subspace outlier detection @xcite . for a detailed review on related methods ,",
    "see @xcite .",
    "while the vast majority of existing work were built to solve the unconditional outlier detection problem , the approaches may not work properly when it comes to _ conditional _ outliers , since they do not take into account the conditional relations among data attributes . realizing this ,",
    "recent years have seen increased interest in the _ conditional _ outliers detection that aims to identify outliers in a set of outputs for given values of inputs .",
    "several approaches have been proposed to address the problems in this regard @xcite .",
    "however , these solutions either are limited to handle problems with a single output variable @xcite or assume a restricted relations among real - valued input and output variables through a gaussian mixture @xcite . as results",
    ", the existing methods either make an independence assumptions that is too restrictive or are unfit for modeling multi - dimensional binary output variables .",
    "in contrast to the existing methods , our proposed approach is different in that ( 1 ) it properly models multi - label binary outputs by adopting a structured probabilistic data model to represent data ; and ( 2 ) it utilizes the decomposed conditional probability estimates from individual response dimensions to identify outliers .",
    "consequently , our proposed approach drives the process of outlier detection to a more granular level of the conditional behaviors in data and ( as follows in section [ sec : experiments ] ) leads to a significant performance improvement in outlier detection .",
    "furthermore , by maintaining separate models for individual output variables , our approach provides a practical advantage that the existing multivariate outlier detection methods do not allow .",
    "that is , one can delve into a trained multivariate conditional model and investigate the quality of each univariate representation @xmath19 to decide whether the individual model could be reliably used to support outlier detection .",
    "for example , a univariate model that produces inconsistent estimates could be preemptively excluded from the outlier detection phase . since",
    "our goal is not to recover a complete data representation but to obtain a useful utility function for outlier detection , this sort of modularity allows us to utilize only the model with high confidence and , hence , to perform more robust outlier detection .",
    "this section describes our approach to identify unusual input - output pairs , which we refer to as mcode : _ multivariate conditional outlier detection_. to facilitate an effective detection method , we utilize a decomposable probabilistic data representation for @xmath2 to capture the dependence relations among inputs and outputs , and to assess outliers by seeking low - probability associations between them . accordingly , having a precise probabilistic data model and proper outlier scoring methods is of primary concern . in section [ subsec :",
    "approach_model ] , we discuss how to obtain an efficient data representation and accurate conditional probability estimates of observed input - output pairs , using the probabilistic structured data modeling approach @xcite in section [ subsec : approach_score ] , we treat the probability estimates as a proxy representation of observed instances and present two outlier scoring methods by analyzing the reliability of these estimates .",
    "our mcode approach works by analyzing data instances come in input - output pairs with a statistical model representing the conditional joint distribution @xmath2 . a direct learning of the conditional joint from data , however , is generally very expensive or even infeasible , because the number of possible output combinations grows exponentially with @xmath1 . to avoid such a high cost of learning yet",
    "achieve an accurate data representation for outlier detection , we decompose the conditional joint into a product of conditional univariate distributions using the chain rule of probability : @xmath20 where @xmath8 denotes the parents of @xmath6 ; _ i.e. _ , all the output variables preceding @xmath6 @xcite .",
    "this decomposition lets us represent @xmath2 by simply specifying each univariate conditional factor , @xmath21 . in this work",
    ", we use a logistic regression model for each of the output dimensions , because it can effectively handle high - dimensional feature space defined by a mixture of continuous and discrete variables ( _ i.e. _ , @xmath22 conditioning @xmath6 ) using regularization @xcite .    in theory",
    ", the result of the above product should be invariant regardless of the chain order ( order of @xmath6 ) . nevertheless ,",
    "in practice , different chain orders produce different conditional joint distributions as they draw in models learned from different data @xcite . for this reason ,",
    "several structure learning methods that determine the optimal set of parents have been proposed @xcite . however , these methods require at least @xmath23 of time , where @xmath24 denotes the time of learning a classifier , that would not be preferable , especially when the output dimensionality @xmath1 is high .    in mcode",
    "we address the above problem by relaxing the chain rule and by permitting circular dependences among the output variables .",
    "that is , we let @xmath8 , the parents of @xmath6 , be all the remaining output variables , and assume the true dependence relations among them could be recovered through a proper regularization of logistic regression . to summarize , our structural decomposition allows us to capture the interactions among the output variables , as well as the input - output relations , using a collection of individually trained probabilistic functions with a relaxed conditional independence assumption .",
    "we use @xmath25 to denote this structured data representation , where @xmath26 is the parameters of the probabilistic model for the @xmath27-th output dimension . assuming logistic regression , these base statistical functions are parameterized using @xmath28 as : @xmath29 this defines a pseudo - conditional joint probability of an observation pair @xmath30 as : @xmath31 where @xmath32 denotes the values of all other output variables except @xmath6 .",
    "now let us apply our data representation @xmath33 to estimate the conditional probabilities of observed outputs . for notational convenience ,",
    "we introduce an auxiliary vector @xmath34 of @xmath1 random variables , each defined in a conditional probability space @xmath35 $ ]",
    ". each element of @xmath36 is quantized by a probabilistic estimation process that is formalized as below by unleashing the product in equation ( [ eq : chain_relax ] ) : @xmath37 where @xmath38          { \\hspace*{.25em}}1 - \\widetilde{p}(y_i^{(n)}|\\mathbf{x}^{(n)},\\mathbf{y}_{-i}^{(n ) } ; \\theta_{\\mathcal{m}(i ) } ) & \\text { otherwise . }",
    "\\end{cases}\\ ] ] accordingly , space of @xmath36 is projecting a normalized confidence level ( _ i.e. _ , conditional probability estimate ) of each observation @xmath39 across individual output dimensions , using the data representation @xmath33 .",
    "figure [ fig : concept ] shows an illustrative example of this estimation where the input - output data instances ( left ) are projected to a 2-dimensional conditional probability space ( right ) .",
    "after the above probabilistic estimation process using @xmath33 , we consider the resultant conditional probabilities @xmath36 as proxies of the original instances , and further hypothesize that multivariate conditional outliers could be effectively detected in this proxy space where instances are analyzed and expressed in terms of univariate posterior probabilities .",
    "our goal is now to define an _",
    "outlier score _ that measures how unusual each input - output association is .",
    "the most straightforward approach to define an outlier score is to use the probability @xmath40 of data instances calculated by the model @xmath33 : @xmath41 please note that this assumes all probability estimates and the models generating them are of high quality .",
    "however , in practice , the models that produce the probability estimates ( _ i.e. _ , @xmath26 in equation ( [ eq : chain_relax ] ) ) may not be all equally reliable as they are trained from a finite number of samples ( this is important especially when the number of input and output variables is high , and the sample size is small ) . also , some dimensions of @xmath42 may not fit well the base statistical assumption ( which in this work is a logistic curve ) and result in miscalibrated estimations .",
    "consequently , if we treat all dimensions of @xmath36 equally and merely search for the regions with low probabilities , the resulting scores degenerate to a noisy vector , which makes the detection of true irregularities hard .    to alleviate the issues , we propose to consider the reliability of each estimate dimension in @xmath36 ( _",
    "i.e. _ , the quality of model @xmath26 ) and adjust their influence in outlier scoring by weights that reflect their reliability .",
    "we formalize our outlier score as : @xmath43 where @xmath44 denotes the reliability weight of the model built for the @xmath27-th dimension .",
    "note that , when @xmath45 for all dimensions @xmath46 , the score becomes equivalent to equation ( [ eq : oscore_prod ] ) , the negative log of the pseudo - conditional joint probability .",
    "one way to define reliability weights would be to use the brier score @xcite that measures the quality of the model in terms of model s probability outputs .",
    "the brier score is defined by averaging the squared errors of the probability estimates over all data instances : @xmath47 where @xmath48 and @xmath49 respectively denotes the predicted probability and actual outcome of the @xmath50-th instance .",
    "however , the assessment of the model quality for weighting purposes ( equation ( [ eq : oscore ] ) ) by the brier score may not be the best as the score imposes different penalties for different errors ( the mean squared error penalizes larger errors more than smaller errors ) and varies the distribution of errors @xcite . to address this",
    ", we propose our reliability weight be based on the mean estimated error , which gives the equal penalty to all errors :    without loss of generality , let @xmath51 be the estimated error of probability of an instance on dimension @xmath27 .",
    "reliability weight @xmath44 is defined by taking the inverse of the mean estimated error : @xmath52    our variant of the brier - like score estimates the quality of each estimate dimension @xmath53 without distorting the distribution of errors . by taking the inverse of the score , we can effectively assign reliability weights to the dimensions , such that more on reliable dimensions become more important and the influence of noisy ( unreliable ) dimensions for outlier scoring is reduced .",
    "notice that the above weighting scheme ( equation ( [ eq : weight ] ) ) implicitly assumes that the reliability of probability estimates ( _ i.e. _ , the quality of a model ) is invariant across all data regions .",
    "however , the assumption often does not hold because in most practical problems especially with high - dimensional data spaces , data is not uniformly distributed in its attribute space .",
    "that is , modeling and estimation of @xmath54 can not be achieved properly in sparse regions of the attribute space .",
    "we tackle such a sparsity issue by evaluating the reliability of each dimension of @xmath36 locally in the region around the instance we want to check .",
    "this localized approach can be implemented as follows : @xmath55 where @xmath56 and @xmath57 denotes @xmath18-nearest neighbors of the @xmath50-th instance in the original attribute space . in the next section ,",
    "we show the benefits of our reliability weights and outlier scores through experimental results .",
    "to validate and demonstrate the performance of our mcode approach , we conduct experiments with data obtained from various domains . through the empirical analysis in this section , we would like to verify the advantages of ( 1 ) adopting the conditional outlier detection approach , ( 2 ) considering the dependence relations among outputs , ( 3 ) weighting via reliability estimation , and ( 4 ) local reliability estimates and local outlier scores .",
    "below we describe our experimental design and present the evaluation results .      to achieve our objectives , we perform experiments with the following methods :    * _ local outlier factor _ ( lof )",
    "@xcite  lof is an unconditional method that estimates outliers using a relative local density measure in the joint space of all data attributes : @xmath58 where @xmath59 denotes the @xmath18-nearest neighborhood of instance @xmath60 and @xmath61 is the local reachability density which measures the geometric dispersion of the @xmath18-nearest neighborhood .",
    "lof effectively finds the instances fall in sparse regions of data . * _ conditional outlier detection with @xmath1 independent response models _ ( i - prod )  we apply @xcite to the multivariate conditional setting by learning @xmath1 independent conditional probability models @xmath62",
    "( @xmath6 is not dependent on other output variables ) and scoring based on the product of their estimates ( equation ( [ eq : oscore_prod ] ) ) .",
    "we refer to this method as i - prod . * _ mcode without weighting _",
    "( m - prod ) ( equation ( [ eq : oscore_prod ] ) * _ mcode with reliability weights _",
    "( m - rw ) ( equation ( [ eq : oscore ] ) ) * _ mcode with local reliability weights _ ( m - lrw ) ( equation ( [ eq : oscore_local ] ) )    to obtain data models in i - prod , m - prod , m - rw , and m - lrw , we use @xmath63-penalized logistic regression and choose their regularization parameters by cross validation . in lof and m - lrw , we set the number of neighbors @xmath64 .",
    "we use _ eight _ public datasets with multi - dimensional input and output .",
    "these are collected from various application domains , including sound recognition ( _ birds _ ) , biology ( _ yeast _ , _ genbase _ ) , text categorization ( _ yahoo _ datasets , _ bibtex _ , _ enron _ ) , and semantic video / image annotation ( _ mediamill _ ) . table [ table : datasets ] summarizes the characteristics of the datasets , such as dataset size , data domain , and short descriptions of the input and output variables .",
    "-.65in-.65 in +      for the purpose of our comparative evaluation , we simulate multivariate conditional outliers by perturbing the output space of data .",
    "there are two parameters in our simulation process : _ outlier ratio _ specifies how many outliers per simulation are injected .",
    "we set this parameter to @xmath65 throughout the experimental study .",
    "_ outlier dimensionality _ specifies how many output dimensions of an outlier to be perturbed .",
    "we vary this parameter relative to the dimensionality of the output by perturbing @xmath66 of outputs . to summarize , we simulate outliers as :    1 .",
    "in each dataset , select @xmath65 of instances uniformly at random 2 .",
    "for each of the selected instances , perturb the values of @xmath66 of the output dimensions ( _ i.e. _ , @xmath67 ) uniformly at random    we would like to stress that all methods ( including their model building and detection stages ) are always run on data with injected outliers .",
    "that is , we never learn a model on the original ( unperturbed ) data and detect outliers on the simulated ( perturbed ) data .",
    "such an design would be unrealistic since we do not know ahead of time what data instances to remove to learn a model .",
    "note that the simulated outliers can be analogous to the errors or mistakes in each application domain .",
    "for example , in semantic video / image annotation , perturbed output values can be perceived as inaccurate subject labels .",
    "we use _ true positive alert rate _ ( tpar ) as our evaluation metric : @xmath68 tpar ( or precision ) measures the percentage of instances with perturbation in the total number of instances detected by the methods .",
    "we assess tpar in two ways : we first evaluate tpar at different alert rate ( detection threshold ) and analyze the quality of outlier scores ( see figure [ fig : tpar_results ] ) .",
    "we also measure the _ averaged tpar _ ( atpar ) in @xmath69 $ ] range , which coincides with the outlier ratio in our experiment setting .",
    "for both tpar and atpar , higher is better .",
    "figure [ fig : tpar_results ] and table [ table : aucprec ] show the performance of the five compared methods .",
    "all results are obtained from _ ten _ repeats .",
    "figures [ fig : r_mediamill ] , [ fig : r_yahoo_arts ] , and [ fig : r_birds ] present the results on three datasets ( _ mediamill _ , _ yahoo - arts _ , and _ birds _ ) for different outlier dimensions .",
    "each figure illustrates the tpars of all methods ; x - axes show the alert rate , ranging between 0 and 0.04 ; y - axes show tpar .",
    "the vertical gray line at alert rate @xmath70 indicates where the alert rate is equal to the injected outlier ratio .    in general",
    ", tpars improve as the outlier dimensionality increases , because outliers with larger perturbations are easier to detect . comparing the conditional outlier detection approaches ( i - prod , m - prod , m - rw , and m - lrw ) with the unconditional approach ( lof ) ,",
    "the conditional approaches are clear winners as the conditional methods outperform lof in most cases .",
    "this shows the advantages of the conditional outlier detection approaches in addressing the problem .",
    "only exceptions are i - prod on _ mediamill _ when outlier dimensionality is low .",
    "this is because i - prod does not consider the dependence relations among the output variables .",
    "such advantages in modeling the inter - dependences of the outputs are consistently observed as m - prod outperforms i - prod in most experiments .    to show the benefits of our reliability weights , we analyze the performance of m - rw and m - lrw in comparison to that of m - prod .",
    "an interesting point is that m - rw and m - lrw not only improve the performance drastically , but also make tpars stable .",
    "this confirms that our reliability weighting methods can effectively estimate the quality of the models , and the resulting weights are useful in outlier scoring .",
    "lastly , although m - lrw does not show much improvement from m - rw compared to the other key components of mcode that we have discussed , the local weights seem to make m - rw even more stable as shown with _ mediamill _ and _ yahoo - arts_.    table [ table : aucprec ] summarizes the results on all eight datasets in terms of atpar at 0.01 .",
    "the table consists of four sections grouped by different values of outlier dimensionality ( @xmath66 ) .",
    "we do not report the results on the first four datasets ( _ birds _ , _ yeast _ , _ genbase _ , and _ yahoo - arts _ ) for outlier dimensionality @xmath71 ( for _ yeast _ , 2.5% and 5.0% ) because the output dimensionality ( @xmath1 ) is too small .",
    "the best performing methods on each experiment are shown in bold .",
    "the results confirms the conclusions that we have drawn with figure [ fig : tpar_results ] .",
    "one interesting point is that lof shows exceptionally high ( compared with its performance on other datasets ) atpar on _",
    "mediamill_. this is because the dataset has a similar number of input and output variables ; hence , as outlier dimensionality increases , the simulated outliers become like unconditional outliers .",
    "in this work , we introduced and tackled multivariate conditional outlier detection , a special type of the conditional outlier detection problem .",
    "we briefly reviewed existing research and motivated this new type of outlier detection problem .",
    "we presented our novel outlier detection framework that analyzes and detects abnormal input - output associations in data using a decomposable conditional probabilistic model that is learned from all data instances .",
    "we discussed how to obtain an efficient data representation and accurate conditional probability estimates of observed input - output pairs , using the probabilistic structured data modeling approach .",
    "motivated by the brier score , we developed present two outlier scoring methods by analyzing the reliability of probability estimates . through the experimental results , we demonstrated the ability of our framework to successfully identify multivariate conditional outliers .",
    "stephen  d. bay and mark schwabacher .",
    "mining distance - based outliers in near linear time with randomization and a simple pruning rule . in _ proceedings of the ninth acm sigkdd international conference on knowledge discovery and data mining _ , kdd 03 , pages 2938 , new york , ny , usa , 2003 .",
    "acm .",
    "krzysztof dembczynski , weiwei cheng , and eyke hllermeier .",
    "bayes optimal multilabel classification via probabilistic classifier chains . in _ proceedings of the 27th international conference on machine learning ( icml-10 ) _ , pages 279286 .",
    "omnipress , 2010 .",
    "milos hauskrecht , michal valko , branislav kveton , shyam visweswaram , and gregory cooper .",
    "evidence - based anomaly detection . in _ annual american medical informatics association symposium _ , pages 319324 , november 2007 .",
    "abhishek kumar , shankar vembu , aditya  krishna menon , and charles elkan .",
    "learning and inference in probabilistic classifier chains with beam search . in _ proceedings of the 2012 european conference on machine learning and knowledge discovery in databases_. springer - verlag , 2012 .",
    "spiros papadimitriou , hiroyuki kitagawa , phillip  b gibbons , and christos faloutsos .",
    "loci : fast outlier detection using the local correlation integral . in _ data engineering , 2003 .",
    "19th international conference on _ , pages 315326 .",
    "ieee , 2003 .",
    "jesse read , bernhard pfahringer , geoff holmes , and eibe frank .",
    "classifier chains for multi - label classification . in _ proceedings of the european conference on machine learning and knowledge discovery in databases_. springer - verlag , 2009 .",
    "shiguo wang .",
    "a comprehensive survey of data mining - based accounting - fraud detection research . in _",
    "intelligent computation technology and automation ( icicta ) , 2010 international conference on _ , volume  1 , pages 5053 , may 2010 .",
    "weng - keen wong , andrew moore , gregory cooper , and michael wagner .",
    "bayesian network anomaly pattern detection for disease outbreaks . in _ proceedings of the twentieth international conference on machine learning _ ,",
    "pages 808815 .",
    "aaai press , august 2003 .",
    "min - ling zhang and kun zhang .",
    "multi - label learning by exploiting label dependency . in _ proceedings of",
    "the 16th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 10 , pages 9991008 .",
    "acm , 2010 ."
  ],
  "abstract_text": [
    "<S> despite tremendous progress in outlier detection research in recent years , the majority of existing methods are designed only to detect _ unconditional _ outliers that correspond to unusual data patterns expressed in the joint space of all data attributes . </S>",
    "<S> such methods are not applicable when we seek to detect _ conditional _ outliers that reflect unusual responses associated with a given context or condition . </S>",
    "<S> this work focuses on _ multivariate conditional outlier detection _ </S>",
    "<S> , a special type of the conditional outlier detection problem , where data instances consist of multi - dimensional input ( context ) and output ( responses ) pairs . </S>",
    "<S> we present a novel outlier detection framework that identifies abnormal input - output associations in data with the help of a decomposable conditional probabilistic model that is learned from all data instances . since components of this model can vary in their quality , we combine them with the help of weights reflecting their reliability in assessment of outliers . </S>",
    "<S> we study two ways of calculating the component weights : global that relies on all data , and local that relies only on instances similar to the target instance . </S>",
    "<S> experimental results on data from various domains demonstrate the ability of our framework to successfully identify multivariate conditional outliers . </S>"
  ]
}