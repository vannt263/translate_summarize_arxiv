{
  "article_text": [
    "a tensor is a multi - dimensional or @xmath3-array .",
    "tensor computations are increasingly prevalent in a wide variety of applications  @xcite .",
    "alas , libraries for dense multi - linear algebra ( tensor computations ) are in their infancy .",
    "the target objects of this paper are symmetric tensors ; tensors whose entries are invariant under any permutation of indices.the aim of this paper is to explore how ideas from matrix computations can be extended to the domain of tensors . specifically , this paper focuses on exploring how exploiting symmetry in matrix computations extends to computations with symmetric tensors and exploring how block structures and algorithms extend to computations with symmetric tensors",
    ".    libraries for dense linear algebra ( matrix computations ) have long been part of the standard arsenal for computational science , including the basic linear algebra subprograms interface  @xcite , lapack  @xcite , and more recent libraries with similar functionality , like the blas - like interface software framework ( blis )  @xcite , and libflame  @xcite . for distributed memory architectures , the scalapack  @xcite , plapack  @xcite , and elemental  @xcite libraries",
    "provide most of the functionality of the blas and lapack .",
    "high - performance implementations of these libraries are available under open source licenses .    for tensor computations ,",
    "no high - performance general - purpose libraries exist .",
    "the matlab tensor toolbox  @xcite defines many commonly used operations that would be needed by a library for multilinear algebra , but does not have any high - performance kernels nor special computations or data structures for symmetric tensors .",
    "the pls toolbox  @xcite provides users with operations for analyzing data stored as tensors , but does not expose the underlying system for users to develop their own set of operations .",
    "the tensor contraction engine ( tce ) project  @xcite focuses on sequences of tensor contractions and uses compiler techniques to reduce workspace and operation counts .",
    "the cyclops tensor framework ( ctf )  @xcite focuses on exploiting symmetry in storage for distributed memory parallel computation with tensors , but at present does not include efforts to optimize computation within each computational node . in a talk at the eighteenth householder symposium meeting in tahoe city , ca ( 2011 ) , prof .",
    "charlie van loan stated `` in my opinion , blocking will eventually have the same impact in tensor computations as it does in matrix computations . ''",
    "the approach we take in this paper heavily borrows from the flame project  @xcite .",
    "we use the _ change - of - basis _ operation , also known as a ymmetric ensor imes ame atrix ( in all modes ) ( sttsm ) operations  @xcite , to motivate the issues and solutions . in the field of computational chemistry , this operation is sometimes referred to as an atomic integral transformation  @xcite when applied to order-@xmath4 tensors .",
    "this operation appears in other contexts as well , such as computing a low - rank tucker - type decomposition of a symmetric tensor  @xcite .",
    "we propose algorithms that require significantly less ( possibly minimal ) computation relative to an approach based on a series of successive matrix - matrix multiply operations by computing and storing temporaries .",
    "additionally , the tensors are stored by blocks , following similar solutions developed for matrices  @xcite .",
    "the algorithms are reformulated to operate with these blocks .",
    "in addition to many of the projects mentioned previously , other work , such as that by van loan and ragnarsson  @xcite suggest devising algorithms in terms of tensor blocks to aid in computation with both symmetric tensors and tensors in general .",
    "the algorithms are reformulated to operate with these blocks .",
    "since we need only store the unique blocks of a symmetric tensor , symmetry is exploited at the block level ( both for storage and computation ) while preserving regularity when computing within blocks .",
    "temporaries are formed to reduce the computational requirements , similar to work in the field of computational chemistry  @xcite . to further reduce computational and storage requirements , we exploit partial symmetry within temporaries .",
    "it should be noted that the symmetry being exploited in this article is different from the symmetry typically observed in chemistry fields .",
    "here , we take advantage of symmetry of the entries of the tensor as opposed to the physical symmetry of molecules which results in eliminated entries in the tensor  @xcite .    the contributions of this paper can be summarized as reducing storage and computational requirements of the sttsm operation for symmetric tensors as :    * utilizing temporaries to reduce computational costs of the sttsm operation for symmetric tensors . * providing a framework for given for exploiting symmetry in symmetric tensors ( and partial symmetry in temporaries ) . * using blocked algorithms and data structures to improve performance of computing environment .",
    "the paper analyzes the computational and storage costs demonstrating that the simplicity need not adversely impact the benefits derived from symmetry .",
    "an implementation shows that the insights can be made practical .",
    "the paper concludes by listing opportunities for future research .",
    "we start with some basic notation , definitions , and the motivating tensor operation .      in this discussion , we assume all indices and modes are numbered starting at zero .    the _ order _ of a tensor is the number of ways or modes . in this paper , we deal only with tensors where every mode has the same dimension . therefore , we define @xmath5}}}$ ] to be the set of real - valued order-@xmath3 tensors where each mode has dimension @xmath6 , i.e. , a tensor @xmath7}}}$ ] can be thought of as an @xmath3-dimensional cube with @xmath6 entries in each direction .",
    "as the order of a tensor corresponds to the number of ways or modes of a tensors , we also refer to an order-@xmath3 tensor as an @xmath3-way tensor .",
    "an element of @xmath8 is denoted as @xmath9 where @xmath10 for all @xmath11 .",
    "this also illustrates that , as a general rule , we use lower case greek letters for scalars ( @xmath12 ) , bold lower case roman letters for vectors ( @xmath13 ) , bold upper case roman letters for matrices ( @xmath14 ) , and upper case scripted letters for tensors ( @xmath15 ) .",
    "we denote the @xmath16th row of a matrix @xmath17 by @xmath18 .",
    "if we transpose this row , we denote it as @xmath19 .      for our forthcoming discussions ,",
    "it is to define the notion of a _ partitioning _ of a set @xmath20 .",
    "we say the sets @xmath21 form a _ partitioning _ of @xmath20 if @xmath22 @xmath23 and @xmath24      it is possible that a tensor @xmath8 may be symmetric in 2 or more modes , meaning that the entries are invariant to permutations of those modes .",
    "for instance , if @xmath8 is a 3-way tensor and symmetric in all modes , then @xmath25 it may also be that @xmath8 is only symmetric in a subset of the modes .",
    "for instance , suppose @xmath8 is a 4-way tensor that is symmetric in modes @xmath26 .",
    "then @xmath27 we define this formally below .",
    "let @xmath20 be a finite set .",
    "define @xmath28 to be the set of all permutations on the set @xmath20 where a permutation is viewed as a bijection from @xmath20 to @xmath20 . under this interpretation , for any @xmath29 , @xmath30 is the resulting element of applying @xmath31 to @xmath32 .",
    "let @xmath33 , and define @xmath34 to be the set of all permutations on @xmath20 as described above .",
    "we say an order-@xmath3 tensor @xmath8 is _ symmetric in the modes in @xmath20 _ if @xmath35 for any index vector @xmath36 defined by @xmath37    technically speaking , this definition applies even in the trivial case where @xmath20 is a singleton , which is useful for defining multiple symmetries .",
    "it is possible that a tensor may have multiple symmetries at once .",
    "for instance , suppose @xmath8 is a 4-way tensor that is symmetric in modes @xmath38 and also in modes @xmath39 .",
    "then    @xmath40 we define this formally below .",
    "let @xmath41 be a partitioning of @xmath42 .",
    "we say an order-@xmath3 tensor @xmath8 has symmetries defined by the mode partitioning @xmath43 if    @xmath35 for any index vector @xmath36 defined by @xmath44    technically , a tensor with no symmetries whatsoever still fits the definition above with @xmath45 and @xmath46 for @xmath47 . if @xmath48 and @xmath49 , then the tensor is _",
    "symmetric_. if @xmath50 , then the tensor is _ partially symmetric_. later , we look at partially symmetric tensors such that @xmath51 and the remaining @xmath52 s are singletons .",
    "the operation used in this paper to illustrate issues related to storage of , and computation with , symmetric tensors is the _ change - of - basis _ operation @xmath53,\\ ] ] where @xmath54 } } } $ ] is symmetric and @xmath55 is the change - of - basis matrix .",
    "this is equivalent to multiplying the tensor @xmath8 by the same matrix @xmath56 in every mode .",
    "the resulting tensor @xmath57}}}$ ] is defined elementwise as @xmath58 where @xmath59 for all @xmath60 .",
    "it can be observed that the resulting tensor @xmath61 is itself symmetric .",
    "we refer to this operation ( [ eq : cob ] ) as the operation .",
    "the sttsm operation is used in computing symmetric versions of tucker and cp ( notably the cp - opt ) decompositions for symmetric tensors  @xcite . in the cp decomposition , the matrix @xmath56 of the sttsm operation is a single vector .",
    "in the field of computational chemistry , the sttsm operation is used when transforming atomic integrals  @xcite .",
    "many fields utilize a closely - related operation to the sttsm operation , which can be viewed as the multiplication of a symmetric tensor in all modes * but one*. problems such as calculating nash - equlibria for symmetric games  @xcite utilize this related operation .",
    "we focus on the sttsm operation not only to improve methods that rely on this exact operation , but also to gain insight for tackling related problems of symmetry in related operations .",
    "we build intuition about the problem and its solutions by first looking at symmetric matrices ( order-2 symmetric tensors ) .      letting @xmath63 yields @xmath64 $ ] where @xmath65}}}$ ] is an @xmath66 symmetric matrix , @xmath67}}}$ ]",
    "is a @xmath68 symmetric matrix , and @xmath69 = { { \\bm { \\mathbf{\\makeuppercase{x } } } } } { { \\bm { \\mathbf{\\makeuppercase{a } } } } } { { \\bm { \\mathbf{\\makeuppercase{x}}}}}^t $ ] . for @xmath62 , ( [ eq : cob ] ) becomes @xmath70           we now give a few details about the particular implementation of bcss , and how this impacts storage requirements . notice that this is one choice for implementing this storage scheme in practice .",
    "one can envision other options that , at the expense of added complexity in the code , reduce the memory footprint .",
    "bcssviews tensors hierarchically . at the top level",
    ", there is a tensor where each element of that tensor is itself a tensor ( block ) . our way of implementing this stores a description ( meta - data ) for a block in each element of the top - level tensor .",
    "this meta - data adds to memory requirements . in our current implementation ,",
    "the top - level tensor of meta - data is itself a dense tensor .",
    "the meta - data in the upper hypertriangular tensor describes stored blocks . the meta - data in the rest of the top - level tensor reference the blocks that correspond to those in the upper hypertriangular tensor ( thus requiring an awareness of the permutation needed to take a stored block and transform it ) .",
    "this design choice greatly simplifies our implementation ( which we hope to describe in a future paper ) .",
    "we show that although the meta - data can potentially require considerable space , this can be easily mitigated .",
    "we use @xmath8 for example purposes .",
    "given @xmath7}}}$ ] stored with bcsswith block dimension @xmath71 , we must store meta - data for @xmath72 blocks where @xmath73 .",
    "this means that the total cost of storing @xmath8 with bcssis @xmath74 @xmath75 is a constant representing the amount of storage required for the meta - data associated with one block , in floats .",
    "obviously , this meta - data is of a different datatype , but floats will be our unit .      *",
    "if @xmath78 , then we only require a minimal amount of memory for meta - data , @xmath79 floats , but must store all entries of @xmath8 since there now is only one block , and that block uses dense storage",
    ". we thus store slightly more than we would if we stored the tensor without symmetry . * if @xmath80 , then @xmath81 and we must store meta - data for each element , meaning we store much more data than if we just used a dense storage scheme .    picking a block dimension somewhere between these two extremes results in a smaller footprint overall .",
    "for example , if we choose a block dimension @xmath82 , then @xmath83 and the total storage required to store @xmath8 with bcssis @xmath84 which , provided that @xmath85 , is significantly smaller than the storage required for the dense case ( @xmath86 ) .",
    "this discussion suggests that a point exists that requires less storage than the dense case ( showing that bcss is a feasible solution ) .    in figure",
    "[ fig : varybactualstorage ] , we illustrate that as long as we pick a block dimension that is greater than @xmath4 , we avoid incurring extra costs due to meta - data storage .",
    "it should be noted that changing the dimension of the tensors also has no effect on the minimum , however if they are too small , then the dense storage scheme may be the minimal storage scheme .",
    "additionally , adjusting the order of tensors has no real effect on the block dimension associated with minimal storage . however increasing the amount of storage allotted for meta - data slowly shifts the optimal block dimension choice towards the dense storage case ."
  ],
  "abstract_text": [
    "<S> symmetric tensor operations arise in a wide variety of computations . however , the benefits of exploiting symmetry in order to reduce storage and computation is in conflict with a desire to simplify memory access patterns . in this paper , we propose a blocked data structure ( blocked compact symmetric storage ) wherein we consider the tensor by blocks and store only the unique blocks of a symmetric tensor . </S>",
    "<S> we propose an algorithm - by - blocks , already shown of benefit for matrix computations , that exploits this storage format by utilizing a series of temporary tensors to avoid redundant computation . further </S>",
    "<S> , partial symmetry within temporaries is exploited to further avoid redundant storage and redundant computation . </S>",
    "<S> a detailed analysis shows that , relative to storing and computing with tensors without taking advantage of symmetry and partial symmetry , storage requirements are reduced by a factor of @xmath0 and computational requirements by a factor of @xmath1 , where @xmath2 is the order of the tensor . </S>",
    "<S> an implementation demonstrates that the complexity introduced by storing and computing with tensors by blocks is manageable and preliminary results demonstrate that computational time is also reduced . </S>",
    "<S> the paper concludes with a discussion of how the insights point to opportunities for generalizing recent advances for the domain of linear algebra libraries to the field of multi - linear computation . </S>"
  ]
}