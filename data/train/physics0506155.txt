{
  "article_text": [
    "the direct numerical simulation ( dns ) of the navier  stokes equations written for low - reynolds - number , incompressible turbulent flows in simple geometries is becoming an increasingly valuable tool for basic turbulence research @xcite .",
    "interesting wall - bounded flows span a number of simple geometries , either in cartesian ( plane channel flow , boundary layer over a flat plate ) or in cylindrical ( pipe flow , annular pipe flow , flow in curved channels ) coordinate systems .    for the cartesian case ,",
    "an effective formulation of the equations of motion was presented 15 years ago by kim , moin & moser in their pioneering and widely - referenced work on the dns of turbulent plane - channel flow @xcite .",
    "this formulation can be regarded today as a _ de facto _ standard ; it has since then been employed in many of the dns of turbulent wall flows in planar geometries .",
    "it consists in the replacement of the continuity and momentum equations written in primitive variables by two scalar equations , one ( second - order ) for the normal component of vorticity and one ( fourth - order ) for the normal component of velocity , much in the same way as the orr  sommerfeld and squire decomposition of linear stability problems .",
    "the main advantages of such an approach are that pressure is eliminated from the equations , and the two wall - parallel velocity components are recovered as the solution of a @xmath0 algebraic system ( a cheap procedure from a computational point of view ) , when a fourier expansion is adopted for the homogeneous directions",
    ". a high computational efficiency can thus be achieved .",
    "the same approach can be employed to write the equation in cylindrical coordinates , but it appears to be much less popular than the primitive - variable formulation of the navier ",
    "stokes equations .",
    "the formulation in terms of two scalar equations for radial velocity and radial vorticity can be found for example in @xcite .",
    "this optimally efficient formulation does not prescribe any particular discretization method for the differential operators in the wall - normal direction .",
    "many researchers , including kim _",
    "@xcite , used spectral methods ( typically chebyshev polynomials ) in this direction too , but other possibilities exist , finite differences and b - splines @xcite being the most popular ones .",
    "the use of finite differences has seen growing popularity @xcite , but mainly in the context of the primitive - variable formulation , and for the discretization of the derivatives in _ all _ three spatial directions ( see for example @xcite ) .",
    "the choice of spectral methods for the discretization of the wall - normal , inhomogeneous direction has a direct impact on the parallelization of a computer code , given the non - locality of the spectral differential operators . as a matter of fact , to our knowledge",
    "no fully spectral dns code has been able to date to run in parallel without a large amount of communication . as a consequence",
    ", high - performance parallel dns has been mostly restricted to large computing systems with a specially - designed communication infrastructure , a.k.a .",
    "supercomputers , even though the floating - point computing performance of the modern , mass - marketed cpus is comparable or better than those of supercomputers @xcite . in a recent paper @xcite jimnez",
    "draws an interesting picture of the future of the dns of turbulent flows in the next 30 years , and assumes that such simulations will be run on supercomputers . the work by karniadakis and coworkers @xcite makes no exception , in that it shows that dns of turbulent flows can be carried out with reasonably good performance on a cluster of pc , provided they are interconnected by a high - performance myrinet network . when a beowulf cluster of pc connected with standard fast ethernet cards is employed @xcite on a isotropic turbulence problem , even after extensive optimization of the code the parallel efficiency , i.e. the ratio between total time on one processor and @xmath1 times the computing time on @xmath1 processors , is as low as 0.5 already when two machines are used .    in this paper",
    "we present a numerical method for the dns of turbulent wall flows that has been designed to require a limited amount of communication , and thereby is well suited for running on commodity hardware .",
    "the method is based on the standard normal velocity - normal vorticity formulation and hence uses fourier discretization in the homogeneous directions , but high - order , compact finite differences schemes are chosen for the discretization of the wall - normal direction , instead of the classical expansion in terms of chebyshev polynomials .",
    "it can be used with either cartesian or cylindrical coordinates .",
    "the outline of the paper is as follows . in  [ sec : equations ] the cartesian form of the navier  stokes equations which is best suited for their numerical solution , and their fourier discretization with respect to the homogeneous directions are briefly recalled . in ",
    "[ sec : timescheme ] time discretization is discussed , with emphasis on our strategy for data storage , which allows us to achieve an important memory optimization . in  [ sec : cart - fd ] the finite - difference discretization of the wall - normal direction based on explicit compact schemes is introduced . ",
    "[ sec : parallel ] illustrates the parallel algorithm , which takes advantage of distributed - memory (  [ sec : distributed - mem ] ) as well as shared - memory (  [ sec : shared - mem ] ) machines ;  [ sec : personal - supercomputer ] describes how a specialized parallel computing system can be set up to achieve the highest efficiency based on commodity hardware . ",
    "[ sec : performance ] discusses the performance of the present parallel method , compared to the information available in the literature for similar codes .",
    "a comparison with a classical parallel strategy often employed in similar codes is used in  [ sec : transpose ] to assess the usefulness of the present method when used with low - bandwidth network connections .",
    "[ sec : conclusions ] is devoted to conclusions .",
    "here we describe only the main aspects of the numerical method .",
    "full details , including the extension to the cylindrical case , can be found in @xcite .",
    "our cartesian coordinate system is illustrated in figure [ fig : cart - comp - domain ] , where a sketch of a plane channel flow is shown : @xmath2 , @xmath3 and @xmath4 denote the streamwise , wall - normal and spanwise coordinates , and @xmath5 , @xmath6 and @xmath7 the respective components of the velocity vector .",
    "the flow is assumed to be periodic in the streamwise and spanwise directions .",
    "the reference length @xmath8 is taken to be one half of the channel height .",
    "the non - dimensional navier ",
    "stokes equations for an incompressible fluid in cartesian coordinates are rewritten , following @xcite , in terms of two scalar differential equations , one ( second order ) for the wall - normal component of vorticity @xmath9 and one ( fourth - order ) for the wall - normal component of velocity @xmath6 , and then fourier - transformed along the homogeneous directions ( fourier - transformed variables will be indicated with an hat sign ) . if the nonlinear terms are considered to be known , as is the case when such terms are treated explicitly in the time discretization , these equations ( supplemented by no - slip boundary conditions at the walls ) become uncoupled and can be solved separately to advance the solution in time by one step .",
    "computing the nonlinear terms and their spatial derivatives requires us first to compute @xmath10 and @xmath11 from @xmath12 and @xmath13 . by using the definition of @xmath13 and the continuity equation written in fourier space ,",
    "a @xmath0 algebraic system can be written and solved analytically for the unknowns @xmath10 and @xmath11 .",
    "its solution is available in analytical form only when the variables are fourier - transformed in the homogeneous directions .",
    "the present method therefore enjoys its computational efficiency only when a fourier discretization is employed for these directions , which means that either periodic boundary conditions are suitable for the physical problem under consideration or a fringe - region technique @xcite is adopted .",
    "the unknowns are represented in terms of truncated fourier series in the homogeneous directions . for example",
    "the wall - normal velocity component @xmath6 is represented as : @xmath14 where @xmath15 and @xmath16 are integer indices corresponding to the streamwise and spanwise direction respectively , and @xmath17 and @xmath18 are the corresponding fundamental wavenumbers , defining the streamwise and spanwise periods @xmath19 and @xmath20 of the computational domain .",
    "the numerical evaluation of the velocity products would require computationally expensive convolutions in wavenumber space , but can be carried out efficiently by transforming the three fourier components of velocity back into physical space , multiplying them in all six possible pair combinations , and eventually re - transforming the six results into wavenumber space .",
    "fast - fourier - transform ( fft ) algorithms are used in both directions .",
    "this technique is often considered `` pseudo - spectral '' , but it should be observed that , when de - aliasing is performed by expanding the number of collocation points by a factor of at least 3/2 before going from wavenumber space into physical space , the velocity products become exactly identical to the `` spectral '' ones that could have been obtained , at a much higher computational cost , through the actual evaluation of the convolution products .",
    "time integration of the equations is performed by a partially - implicit method , implemented in such a way as to reduce the memory requirements of the code to a minimum , by exploiting the finite - difference discretization of the wall - normal direction .",
    "the use of a partially - implicit scheme is a common approach in dns @xcite : the explicit part of the equations can benefit from a higher - accuracy scheme , while the stability - limiting viscous part is subjected to an implicit time advancement , thus relieving the stability constraint on the time - step size @xmath21 .",
    "we employ an explicit third - order , low - storage runge  kutta method , combined with an implicit second - order crank - nicolson scheme @xcite .",
    "the procedure to solve the discrete equations for @xmath22 and @xmath23 at the time level @xmath24 is made by two distinct steps . in the first step ,",
    "the rhss corresponding to the explicit part have to be assembled . in the representation ( [ eq : cart - discretization ] ) , at a given time the fourier coefficients of the variables are represented at different @xmath3 positions ; hence the velocity products can be computed through inverse / direct fft in wall - parallel planes .",
    "their spatial derivatives are then computed : spectral accuracy can be achieved for wall - parallel derivatives , whereas the finite - differences compact schemes described in  [ sec : cart - fd ] are used in the wall - normal direction .",
    "these spatial derivatives are eventually combined with values of the rhs at previous time levels . the whole @xmath3 range from one wall to the other",
    "must be considered .",
    "the second step involves , for each @xmath25 pair , the solution of a set of two odes , derived from the implicitly integrated viscous terms , for which the rhs is now known .",
    "a finite - differences discretization of the wall - normal differential operators produces two real banded matrices , in particular pentadiagonal matrices when a 5-point stencil is used .",
    "the solution of the resulting two linear systems gives @xmath26 and @xmath27 , and then the planar velocity components @xmath28 and @xmath29 can be computed . for each @xmath25 pair , the solution of the two odes requires the simultaneous knowledge of the rhs in all @xmath3 positions .",
    "the whole @xmath30 space must be considered . in the @xmath31 space",
    "the first step of this procedure proceeds per wall - parallel planes , while the second one proceeds per wall - normal lines .     +     to understand our memory - efficient implementation of the time integration procedure , let us consider the following differential equation for the one - dimensional vector @xmath32 : @xmath33 where @xmath34 denotes non - linear operations on @xmath35 , and @xmath36 is the coefficient matrix which describes the linear part .",
    "after time discretization of this generic equation , that has identical structure to both the @xmath13 and @xmath12 equations , the unknown at time level @xmath24 stems from the solution of the linear system : @xmath37 where @xmath38 is given by a linear combination ( with suitable coefficients which depend on the particular time integration scheme and , in the case of runge - kutta methods , on the particular sub - step too ) of @xmath35 , @xmath39 and @xmath40 evaluated at time level @xmath41 and at a number of previous time levels .",
    "the number of previous time levels depends on the chosen explicit scheme . for the present , low - storage runge - kutta scheme",
    ", only the additional level @xmath42 is required .",
    "the quantities @xmath35 , @xmath39 and @xmath43 can be stored in distinct arrays , thus resulting in a memory requirement of 7 variables per point for a two - levels time integration scheme .",
    "an obvious , generally adopted optimization is the incremental build into the same array of the linear combination of @xmath35 , @xmath39 and @xmath40 , as soon as the single addendum becomes available .",
    "the rhs can then be efficiently stored in the array @xmath35 directly , thus reducing the memory requirements down to 3 variables per point .",
    "the additional optimization we are able to enforce here relies on the finite - difference discretization of the wall - normal derivatives . referring to our simple example , the incremental build of the linear combination",
    "is performed contemporary to the computation of @xmath39 and @xmath40 , the result being stored into the same array which already contained @xmath35 .",
    "the finite - difference discretization ensures that , when dealing with a given @xmath3 level , only a little slice of values of @xmath35 , centered at the same @xmath3 level , is needed to compute @xmath39 .",
    "hence just a small additional memory space , of the same size of the finite - difference stencil , must be provided , and the global storage space reduces to two variables per point for the example equation ( [ eq : example - timescheme ] ) .    the structure of the time integration procedure implemented in our dns code is symbolically shown in the bottom chart of figure [ fig : timescheme ] , and compared with the standard approach , illustrated in the top chart . within the latter approach , in a main loop over the wall - parallel planes ( integer index @xmath44 ) the velocity products are computed pseudo - spectrally with planar fft , their spatial derivatives are taken and the result is eventually stored in the three - dimensional array @xmath45 . after the loop has completed , the linear combination of @xmath35 , @xmath45 and @xmath46 is assembled in a temporary two - dimensional array @xmath47 , then combined into the three - dimensional array @xmath35 with the contribution from the previous time step , and eventually stored in the three - dimensional array @xmath48 for later use .",
    "the rhs , which uses the storage space of the unknown itself , permits now to solve the linear system which yields the unknown at the future time step , and the procedure is over , requiring storage space for 3 three - dimensional arrays .",
    "the flow chart on the bottom of figure [ fig : timescheme ] illustrates the present approach . in the main loop over wall - parallel planes , not only the non - linear terms",
    "are computed , but the rhs of the linear system is assembled plane - by - plane and stored directly in the three - dimensional array @xmath35 , provided the value of the unknown in a small number of planes ( 5 when a 5-point finite - difference stencil is employed ) is conserved . as a whole , this procedure requires only 2 three - dimensional arrays for each scalar equation .",
    "the discretization of the first , second and fourth wall - normal derivatives @xmath49 , @xmath50 and @xmath51 , required for the numerical solution of the present problem is performed through finite - differences ( fd ) compact schemes @xcite .",
    "one important difference with @xcite is that our compact schemes are at the same time explicit and at fourth - order accuracy .",
    "the computational molecule is composed of five arbitrarily spaced ( with smooth stretching ) grid points on a mesh of @xmath52 points @xmath53 , with @xmath54 .",
    "we indicate here with @xmath55 the five coefficients discretizing the exact operator @xmath49 over five adjacent grid points centered at @xmath53 , i.e. : @xmath56 where @xmath53 is the @xmath3 position on the computational mesh where the derivative has to be evaluated .",
    "the coefficients @xmath57 change with the distance from the wall ( i.e. with the integer index @xmath44 ) when a non - uniform mesh is employed .",
    "compact schemes are also known as implicit finite - differences schemes , because they typically require the inversion of a linear system for the actual calculation of a derivative @xcite : this increases the complexity and the computational cost of such an approach . for the present problem we are able however to determine explicitly the coefficients for compact , fourth - order accurate schemes ,",
    "thanks to the absence of the @xmath58 operator from the present equations .",
    "this important simplification has been highlighted first in the original gauss - jackson - noumerov compact formulation exploited in his seminal work by thomas @xcite , concerning the numerical solution of the orr - sommerfeld equation .    to illustrate thomas method ,",
    "let us consider a 4th - order ordinary differential equation ( linear for simplicity ) for a function @xmath59 in the following conservation form : @xmath60 where the coefficients @xmath61 are arbitrary functions of the independent variable @xmath3 , and @xmath62 is the known rhs .",
    "let us moreover suppose that in frequency space a differential operator , for example @xmath51 , is approximated as the ratio of two polynomials , say @xmath63 and @xmath64 .",
    "polynomials like @xmath63 and @xmath64 have their counterpart in physical space , and @xmath65 and @xmath66 are the corresponding fd operators .",
    "the key point is to impose that _ all _ the differential operators appearing in the example equation ( [ eq : example - eq ] ) admit a representation such as the preceding one , in which the polynomial @xmath64 at the denominator remains _ the same_. eq .",
    "( [ eq : example - eq ] ) can thus be recast in the new , equivalent discretized form : @xmath67 and this allows us to use explicit fd schemes , provided the operator @xmath66 is applied to the rhs of the equation and to the terms not involving @xmath3 derivatives .",
    "the overhead related to the use of implicit finite difference schemes disappears , while the advantage of using compact schemes is retained .",
    "when compared to @xcite , the present approach is similar , but we decided to allow for variable coefficients @xmath68 inside the differential operators @xmath69 .",
    "thomas choice of considering differential operators of the form @xmath70 is equivalent in principle , but it would have required to solve for an auxiliary variable @xmath71 .",
    "the present choice moreover is better suited for a differential equation written in conservative form , where only 6 convolutions have to be evaluated .    in our implementation , to obtain a formal accuracy of order 4 we have used a computational stencil of five grid points . to compute the finite - difference coefficients ,",
    "we have followed a standard procedure in the theory of pad approximants @xcite . for each distance @xmath53 from the wall , a @xmath72 linear system can be set up and solved for the unknown coefficients .",
    "a mesh with variable size in the wall - normal direction is often desirable , in order to keep track of the increasingly smaller turbulence length scales when the wall is approached .",
    "the use of a non - uniform mesh together with compact schemes at high accuracy is known @xcite to require special care when the differential equation is used as an additional relation which can be differentiated to eliminate higher - order truncation errors . in the present approach the use of a non - uniform mesh in such a way as to still keep a fourth - order accuracy simply requires the procedure outlined above to be performed ( numerically ) again for each @xmath3 station , but only once at the beginning of the computations .",
    "the computer - based solution of these systems requires a negligible computing time .",
    "we end up with fd operators which are altogether fourth - order accurate ; the sole operator @xmath51 is discretized at sixth - order accuracy .",
    "as suggested in @xcite and @xcite , the use of all the degrees of freedom for achieving the highest formal accuracy might not always be the optimal choice .",
    "we have therefore attempted to discretize @xmath51 at fourth - order accuracy only , and to spend the remaining degree of freedom to improve the spectral characteristics of _ all _ the fd operators at the same time .",
    "our search has shown however that no significant advantage can be achieved : the maximum of the errors can be reduced only very slightly , and - more important - this reduction does not carry over to the entire frequency range .",
    "the boundaries obviously require non - standard schemes to be designed to properly compute derivatives at the wall .",
    "for the boundary points we use non - centered schemes , whose coefficients can be computed following the same approach as the interior points , thus preserving by construction the formal accuracy of the method .",
    "nevertheless the numerical error contributed by the boundary presumably carries a higher weight than interior points , albeit mitigated by the non - uniform discretization . a systematic study of this error contribution and of alternative more refined treatments of the boundary are ongoing work .",
    "if the calculations are to be executed in parallel by @xmath1 computing machines ( nodes ) , data necessarily reside on these nodes in a distributed manner , and communication between nodes will take place .",
    "our main design goal is to keep the required amount of communication to a minimum .",
    "when a fully spectral discretization is employed , a transposition of the whole dataset across the computing nodes is needed every time the numerical solution is advanced by one time ( sub)step when non - linear terms are evaluated .",
    "this is illustrated for example in the paper by pelz @xcite , where parallel fft algorithms are discussed in reference to the pseudo - spectral solution of the navier ",
    "stokes equations .",
    "pelz shows that there are basically two possibilities , i.e. using a distributed fft algorithm or actually transposing the data , and that they essentially require the same amount of communication .",
    "the two methods are found in @xcite to perform , when suitably optimized , in a comparable manner , with the distributed strategy running in slightly shorter times when a small number of processors is used , and the transpose - based method yielding an asymptotically faster behavior for large @xmath1 .",
    "the large amount of communication implies that very fast networking hardware is needed to achieve good parallel performance , and this restrict dns to be carried out on very expensive computers only .",
    "of course , when a fd discretization in the @xmath3 direction is chosen instead of a spectral one , it is conceivable to distribute the unknowns in wall - parallel slices and to carry out the two - dimensional inverse / direct ffts locally to each machine .",
    "moreover , thanks to the locality of the fd operators , the communication required to compute wall - normal spatial derivatives of velocity products is fairly small , since data transfer is needed only at the interface between contiguous slices .",
    "the reason why this strategy has not been used so far is simple : a transposition of the dataset seems just to have been delayed to the second half of the time step advancement procedure .",
    "indeed , the linear systems which stem from the discretization of the viscous terms require the inversion of banded matrices , whose principal dimension span the entire width of the channel , while data are stored in wall - parallel slices .     computing nodes . ]",
    "a transpose of the whole flow field can be avoided however when data are distributed in slices parallel to the walls , with fd schemes being used for wall - normal derivatives .",
    "the arrangement of the data across the machines is schematically shown in figure  [ fig : slices ] : each machine holds all the streamwise and spanwise wavenumbers for @xmath73 contiguous @xmath3 positions . as said , the planar ffts do not require communication at all .",
    "wall - normal derivatives needed for the evaluation of the rhss do require a small amount of communication at the interface between contiguous slices .",
    "however , this communication can be avoided at all if , when using a 5-point stencil , two boundary planes on each internal slice side are duplicated on the neighboring slice .",
    "this duplication is obviously a waste of computing time , and translates into an increase of the actual size of the computational problem .",
    "however , since the duplicated planes are @xmath74 , as long as @xmath75 this overhead is negligible .",
    "when @xmath1 becomes comparable to @xmath76 , an alternative procedure involving a small amount of communication becomes convenient .",
    "we will further discuss this point in  [ sec : performance ] .",
    "the critical part of the procedure lies in the second half of the time - step advancement , i.e. the solution of the set of two linear systems , one for each @xmath77 pair , and the recovery of the planar velocity components : the necessary data just happen to be spread over all the @xmath1 machines .",
    "it is relatively easy to avoid a global transpose , by solving each system in a _ serial _ way across the machines : adopting a lu decomposition of the pentadiagonal , distributed matrices , and a subsequent sweep of back - substitutions , only a few coefficients at the interface between two neighboring nodes must be transmitted .",
    "the global amount of communication remains very low and , at the same time , local between nearest neighbors only .",
    "the problem here is obtaining a reasonably high parallel efficiency : if a single system had to be solved , the computing machines would waste most of their time waiting for the others to complete their task . in other words , with the optimistic assumption of infinite communication speed , the total wall - clock time would be simply equal to the single - processor computing time .",
    "the key observation to obtain high parallel performance is that the number of linear systems to be solved at each time ( sub)step is very large , i.e. @xmath78 , which is at least @xmath79 and sometimes much larger in typical dns calculations @xcite .",
    "this allows the solution of the linear systems to be efficiently pipelined as follows .",
    "when the lu decomposition of the matrix of the system for a given @xmath80 pair is performed ( with a standard thomas algorithm adapted to pentadiagonal matrices ) , there is a first loop from the top row of the matrix down to the bottom row ( elimination of the unknowns ) , and then a second loop in the opposite direction ( back - substitution ) .",
    "the machine owning the first slice performs the elimination in the local part of the matrix , and then passes on the boundary coefficients to the neighboring machine , which starts its elimination . instead of waiting for the elimination in the @xmath80 system matrices to be completed across the machines , the first machine can now immediately start working on the elimination in the matrix of the following system , say @xmath81 , and so on .",
    "after the elimination in the first @xmath1 systems is started , all the computing machines work at full speed .",
    "a synchronization is needed only at the end of the elimination phase , and then the whole procedure can be repeated for the back - substitution phase .",
    "clearly this pipelined - linear - system ( pls ) strategy involves an inter - node communication made by frequent sends and receives of small data packets ( typically two lines of a pentadiagonal matrix , or two elements of the rhs array ) .",
    "while the global amount of data is very low , this poses a serious challenge to out - of - the - box communication libraries , which are known to have a significant overhead for very small data packets .",
    "in fact , as we will mention in  [ sec : performance ] , we have found unacceptably poor performance when using mpi - type libraries . on the other hand",
    "we have succeeded in developing an effective implementation of inter - node communication using only the standard i / o functions provided by the c library .",
    "details of this alternative implementation are illustrated in ",
    "[ sec : personal - supercomputer ] .",
    "the single computing node may be single - cpu or multi - cpu . in the latter case , it is possible to exploit an additional and complementary parallel strategy , which does not rely on message - passing communication anymore , and takes advantage of the fact that local cpus have direct access to the same , local memory space .",
    "we stress that this is different from using a message - passing strategy on a shared - memory machine , where the shared memory simply becomes a faster transmission medium .",
    "using multiple cpus on the same memory space may yield an additional gain in computing time , at the only cost of having the computing nodes equipped with more than one ( typically two ) cpus .",
    "for example the fft of a whole plane from physical to fourier - space and vice - versa can be easily parallelized this way , as well as the computing - intensive part of building up the rhs terms . with smp machines ,",
    "high parallel efficiencies can be obtained quite easily by `` forking '' new processes which read from and write to the same memory space ; the operating system itself then handles the assignment of tasks to different cpus , and only task synchronization is a concern at the programming level .",
    "while a computer program based on the numerical method described heretoforth can be easily used on a general - purpose cluster of machines , connected through a network and a switch , for maximum efficiency a dedicated computing system can be specifically designed and built on top of the parallel algorithm described above .    at the cpu level ,",
    "the mass - marketed cpus which are commonly found today in desktop systems are the perfect choice : their performance is comparable to the computing power of the single computing element of any supercomputer @xcite , at a fraction of the price .",
    "the single computing node can hence be a standard desktop computer ; smp mainboards with two cpus are very cheap and easily available .",
    "the present pls parallel strategy allows an important simplification in the connection topology of the machines .",
    "since the transposition of the whole dataset is avoided , communications are always of the _ point - to - point _ type ; moreover , each computing machine needs to exchange data with and only with two neighboring machines only .",
    "this can be exploited with a simple ring - like connection topology among the computing machines , sketched in figure  [ fig : cluster ] , which replicates the logical exchange of information and the data structure previously illustrated in figure  [ fig : slices ] : each machine is connected through two network cards only to the previous machine and to the next .",
    "the necessity of a switch ( with the implied additional latency in the network path ) is thus eliminated , in favor of simplicity , performance and cost - effectiveness .",
    "concerning the transmission protocol , the simplest choice is the standard , error - corrected tcp / ip protocol .",
    "we have estimated that on typical problem sizes the overall benefits from using a dedicated protocol ( for example the gamma protocol described in @xcite ) would be negligible : since the ratio between communication time and computing time is very low ( see  [ sec : performances - parallel ] and fig.[fig : tempi - itanium ] ) , the improvements by using such a protocol are almost negligible , and to be weighed against the increase in complexity and decrease in portability .",
    "the simplest and fastest strategy we have devised for the communication type is to rely directly on the standard networking services of the unix operating system , i.e. sockets ( after all , message - passing libraries are socket - based ) . at the programming level",
    ", this operation is very simple , since a socket is seen as a plain file to write into and to read from .",
    "using sockets allows us to take advantage easily and efficiently of the advanced buffering techniques incorporated in the management of the input / output streams by the operating system : after opening the socket once and for all , it is sufficient to write ( read ) data to ( from ) the socket whenever they are available ( needed ) , and the operating system itself manages flushing the socket when its associated buffer is full .",
    "we have found however that for best performances the buffer size had to be empirically adjusted : for fast ethernet hardware , the optimum has been found at the value of 800 bytes , significantly smaller than the usual value ( the linux operating system defaults at 8192 ) .    we have built a prototype of such a dedicated system , composed of 8 smp personal computers .",
    "each node is equipped with 2 pentium iii 733mhz cpu and 512 mb of 133mhz sdram .",
    "the nodes are connected to each other by two cheap 100mbits fast ethernet cards .",
    "we call such a machine a personal supercomputer .",
    "the performance of our numerical method used on this system will be shown in  [ sec : performance ] to be comparable to that of a supercomputer .",
    "such machines enjoy the advantages of a simple desktop personal computer : low cost and easy upgrades , unlimited availability even to a single user , low weight , noise and heat production , small requirements of floor space , etc .",
    "further details and instructions to build and configure such a machine can be found in @xcite .",
    "the performance of the present pipelined - linear - system ( pls ) method is assessed here in terms of memory requirements , single - processor cpu time and parallel speedup , both in absolute terms and by comparison with similar dns codes .",
    "we compare it also with an alternative version of the code , that we have written with a different distributed - memory parallel strategy .",
    "this alternative code employs the more traditional transpose fft method @xcite , so that the streamwise wavenumbers are distributed across the system while the wall - normal direction is local to each processor . both parallel algorithms have been preliminarly tested for correctness , and checked to give identical output between a single - processor run and a truly parallel execution .",
    "our tests have been mainly conducted on the computing system described in  [ sec : personal - supercomputer ] , either on a single pentium iii 733 mhz cpu for the single - processor tests , or by using multiple processors for the parallel tests .",
    "a few measurements are collected by using more recent dual - processor opteron machines , available at salerno university .",
    "each of the opteron machines is equipped with two 1.6 ghz amd cpus , and carries 1 gbyte of ram ; the machines are equipped with 3 gigabit ethernet cards each . they are thus connected both in a switched network via one card and the interposed hp 2724 switch , and in the ring topology by means of the other two cards .",
    "the performance of our codes on these machines can not easily be compared with performance figures of similar codes , since the information available in the literature is often partial and based on each time different computing machines . some data ( for example , ram requirements ) of course",
    "can be compared directly , given their independence of the particular computer architecture . for other data",
    "( typically , cpu time ) we report our own figures , and we try in addition to compare qualitatively such data with cpu time on different architectures , by using the performance database maintained and published monthly by dongarra @xcite .",
    "the results presented in what follows are computed with the parallel algorithm described in  [ sec : distributed - mem ] . as already pointed out ,",
    "this algorithm is especially well suited when a limited number of computing nodes is available .",
    "indeed , the duplication of four computing planes for each internal slice interface implies a cpu penalty , while allowing the bare minimum of inter - node communication .",
    "this penalty increases with the number of computing nodes and decreases with the number of discretization points in the @xmath3 direction .",
    "we define the speedup factor as the ratio of the actual wall - clock computing time @xmath82 obtained with @xmath1 nodes and the wall - clock time @xmath83 required by the same computation on a single node : @xmath84    the maximum or ideal speedup factor @xmath85 that we can expect with our pls algorithm , corresponding to the assumption of infinite communication speed , is less than linear , and can be estimated with the formula : @xmath86 where the factor @xmath87 accounts for the two wall - parallel planes duplicated at each side of interior slices .",
    "( [ eq : ideal - speedup ] ) reduces to a linear speedup when @xmath88 for a finite value of @xmath1 .",
    "a quantitative evaluation of the function ( [ eq : ideal - speedup ] ) for typical values of @xmath89 shows that the maximum achievable speedup is nearly linear as long as the number of nodes remains moderate , i.e. @xmath90 .",
    "we are presently considering a slightly different parallel implementation , still in development at the present time , which is better suited for use when @xmath91 .",
    "one fundamental requirement for a dns code is to save ram : jimnez in @xcite considers ram occupation and cpu time as performance monitors of equivalent importance .",
    "the amount of required ram is dictated by the number and the size of the three - dimensional arrays , and it is typically reported @xcite to be no less than @xmath92 floating - point variables .",
    "cases where ram requirements are significantly higher are not uncommon : for example in @xcite a channel flow simulation of @xmath93 reportedly required 1.2 gb of ram , suggesting a memory occupation approximately 18 times larger .    in our code",
    "all the traditional optimizations are employed : for example there is no reserved storage space for @xmath13 , which overwrites @xmath10 in certain sections of the time - integration procedure , and is overwritten by @xmath10 in other sections .",
    "an additional saving specific to the present method comes from the implementation of the time advancement procedure , discussed in ",
    "[ sec : timescheme ] , which takes advantage of the finite - difference discretization of the wall - normal derivatives .",
    "each of the two scalar equations for @xmath13 and @xmath12 requires two variables per point .",
    "in addition , solving the algebraic system for @xmath10 and @xmath11 raises the global memory requirement to 5 variables per point .",
    "thus our code requires a memory space of @xmath94 floating - point variables , plus workspace and two - dimensional arrays . for example",
    "a simulation with @xmath95 takes only 94 mbytes of ram ( using 64-bit floating - point variables ) .    in a parallel run",
    "the memory requirement can be subdivided among the computing machines . with @xmath96",
    "the same @xmath97 case runs with 53 mbytes of ram ( note that the amount of ram is slightly larger than one half of the @xmath98 case , due to the aforementioned duplication of boundary planes ) .",
    "the system as a whole therefore allows the simulation of turbulence problems of very large computational size even with a relatively small amount of ram deployed in each node .",
    "a problem with computational size of @xmath99 would easily fit into our 8 nodes equipped with 512 mb ram each .",
    "as far as cpu efficiency is concerned , without special optimization the @xmath97 test case mentioned above requires 42.8 cpu seconds for the computation of a full three - sub - steps runge - kutta temporal step on a single pentium iii 733mhz processor .",
    "unfortunately , we are not aware of papers where a similar code is clearly documented in terms of time required for running a problem of a specified size on these cpu types",
    ". one can however deduce from @xcite that a computational case of slightly smaller size , i.e. @xmath100 ( which takes 31 seconds on our machine ) runs on a single processor of the 256 nodes cray t3e of the national supercomputer center of linkping ( sweden ) in approximately 40 seconds , and on a single processor of the 152 nodes ibm sp2 machine , available at the center for parallel computers of kth university , in 8 seconds of cpu time .",
    "these timings are in a ratio which is not far from the ratio among the computing power of the different cpus , as deduced from the tables reported in @xcite , and indicate that the sp2 machine is the one which is able to achieve the higher percentage of its theoretical peak power .",
    "the present code hence is roughly equivalent ( in its serial version ) to that described in @xcite in terms of cpu efficiency .",
    "another paper which reports execution times for a @xmath97 problem is in ref .",
    "@xcite : their code for isotropic turbulence appears to run on one cpu of an ibm sp3 power3 nighthawk taking approximately 10 minutes per time step .",
    "the internal timings of our code show that the direct / inverse two - dimensional fft routines take the largest part of the cpu time , namely 56% .",
    "the calculation of the rhs of the two governing equations ( where wall - normal derivatives are evaluated ) takes 25% of the total cpu time , the solution of the linear systems arising from the implicit part around 12% , and the calculation of the planar velocity components 3% .",
    "the time - stepping scheme takes 3% and computing a few runtime statistics requires an additional 1% of the cpu time .         of computing nodes .",
    "thick lines are the ideal speedup @xmath85 from eq .",
    "( [ eq : ideal - speedup ] ) for @xmath101 ( continuous line ) and @xmath102 ( dashed line ) .",
    "gray circles are speedups inferred from @xcite for a case with @xmath103 and measured on two different supercomputers.,scaledwidth=90.0% ]    the parallel ( distributed - memory ) performance of the code is illustrated in figure  [ fig : tempi ] , where speedup ratios are reported as a function of the number of computing nodes .",
    "the maximum possible speedup @xmath85 is shown with thick lines .",
    "@xmath85 approaches the linear speedup for large @xmath76 , being reasonably high as long as @xmath1 remains small compared to @xmath76 : with @xmath104 it is 6.25 for @xmath101 and 7.125 for @xmath102 . notwithstanding the commodity networking hardware and the overhead implied by the error - corrected tcp protocol ,",
    "the actual performance compared to @xmath85 is extremely good , and improves with the size of the computational problem .",
    "the percentage of time @xmath105 spent for communication is estimated as follows : @xmath106    the case @xmath107 is hardly penalized by the time spent for communication , which is only 2% of the total computing time when @xmath104 .",
    "the communication time becomes 7% of the total computing time for the larger case of @xmath108 , @xmath102 and @xmath109 , and is @xmath110% for the worst ( i.e. smallest ) case of @xmath97 , which requires 7.7 seconds for one time step on our machine , with a speedup of 5.55 . in figure",
    "[ fig : tempi ] we report also ( with gray symbols ) speedup data from @xcite for his case with size @xmath111 : this case runs in approximately 2 seconds when 8 processors are used on the sp2 ( speedup 3.9 ) , while it requires 6 seconds with 8 processors of the t3e ( speedup 6.8 ) .",
    "it is worth mentioning again that our communication procedure can easily be implemented through a standard message - passing library , but shows in this case a noticeably degraded performance .",
    "in fact we have tested the present method on a @xmath97 case with @xmath96 and using the mpi library .",
    "a speedup of @xmath112 has been measured ( i.e. the wall clock is increased ) , to be compared with @xmath113 and a measured speedup of @xmath114 with the use of plain sockets .",
    "this result is explained by the large overhead implied by the mpi library , that is known to be inefficient in transmitting extremely small data packets .",
    "we would like to stress again that , from a programming point of view , plain sockets are definitely simple to use : once the socket is properly opened , the procedure of communication with another machine boils down to simply writing to or reading from a file .     of computing nodes .",
    "thick line is the ideal speedup from eq .",
    "( [ eq : ideal - speedup ] ) for @xmath102 .",
    "speedup measured when using gigabit ethernet cards ( circles ) , and the same cards run at the slower speed of 100mbit / s ( empty squares ) and 10 mbit / s ( filled squares).,scaledwidth=90.0% ]    figure [ fig : tempi - cfd ] illustrates the speedup achieved with the faster opteron machines connected via gigabit ethernet cards in the ring - topology layout , compared with @xmath85 .",
    "the test case has a size of @xmath115 .",
    "the cpus of this system are significantly faster than the pentium iii , and the network cards , while having 10 times larger bandwidth , have latency characteristics typical of fast ethernet cards .",
    "it is remarkable how well the measured speedup still approaches the ideal speedup , even at the largest tested value of @xmath1 .",
    "furthermore , we report also the measured speedup when the opteron machines are used with the gigabit cards set up to work at the lower speeds of 100 mbit / s and 10mbit / s .",
    "it is interesting to observe how slightly performance is degraded in the case at 100mbit / s , whose curve is nearly indistinguishable form that at 1gbit / s . even with the slowest 10mbit / s bandwidth connecting such fast processors , and with a problem of large computational size , it is noteworthy how the present method is capable to achieve a reasonable speedup for low @xmath1 and not to ever degrade below @xmath116 .",
    "this relative insensitivity to the available bandwidth can be ascribed to the limited amount of communication required by the present method .    the amount of data which has to be exchanged by each machine for the advancement of the solution by one time step made by 3 runge ",
    "kutta substeps can be quantified as follows .",
    "the number of bytes @xmath117 transmitted and received by each computing node for @xmath118 and in one complete time step is : @xmath119 where 3 is the number of temporal substeps , 8 accounts for 8-bytes variables , and 88 is the total number of scalar variables that are exchanged at the slice interfaces for each wavenumber pair ( during solution of the linear systems and of the algebraic system to compute @xmath10 and @xmath11 ) . for the @xmath97 case , @xmath120 mbit of network traffic , evenly subdivided between the two network cards .",
    "interestingly , the quantity @xmath117 is linear in the total number of fourier modes @xmath121 , and does not depend upon @xmath76 .",
    "moreover , the amount of traffic does not increase when @xmath1 increases .",
    "this has to be contrasted with the amount of communication required by other parallel methods ; this comparison will be discussed in the next subsection .",
    "as already mentioned , the parallel strategy described here targets only systems where the number @xmath1 of computing nodes is small compared to the number @xmath76 of points in the @xmath3 direction .",
    "increasing @xmath1 at fixed @xmath76 leads to gradually worse performance , since the actual size of the problem increases owing to the duplicated planes at the interface .",
    "nevertheless , figure [ fig : tempi - itanium ] shows that , even when a larger number of computing nodes is employed on a problem of a large size , the percentage of the computing time spent for communication , estimated according to eq .",
    "( [ eq : percentage - comm - time ] ) , remains very low .",
    "this figure reports the parallel performance measured on a cluster of smp machines equipped with 2 itanium ii processors , and connected with gigabit ethernet cards and a switch , available courtesy of the sharcnet computing centre at the university of western ontario .",
    "the test case has a size of @xmath122 , @xmath102 and @xmath123 .",
    "figure  [ fig : tempi - itanium ] suggests that the communication time remains limited up to relatively large values of @xmath1 , even when the networking hardware ( gigabit ethernet ) allows a bandwidth more than two orders of magnitude smaller than a typical supercomputer .     spent for communication as percentage of the total computing time , defined in formula ( [ eq : percentage - comm - time ] ) , for the itanium ii cluster with gigabit ethernet interconnects .",
    "problem size : @xmath122 , @xmath102 and @xmath123.,scaledwidth=90.0% ]      in order to be able to compare the present pls parallel strategy with a standard strategy which performs a block transpose before and after each fft , we have written a second version of our code , that adopts the same fd discretization for the wall - normal direction , but distributes the streamwise wavenumbers across the nodes , i.e. organizes data in slices parallel to the @xmath124-axis of figure [ fig : slices ] .",
    "the serial performance of the two computer codes is identical , since for @xmath98 they perform the same operations . even though we have obviously put less optimization effort into the transpose code , written _",
    "ad hoc _ for this test , compared the the pls code , we have to mention that the transpose code has been written with the basic optimizations in mind .",
    "in particular , we have taken care that communications are scheduled in such a way that the machines are always busy communicating , which is an essential requirement to achieve high performance with the transpose fft .",
    "the amount of data ( in bytes ) @xmath125 which has to be exchanged by each machine for the complete advancement by one time step with the transpose - based method is as follows : @xmath126    again , the factors 3 and 8 account for the number of temporal substeps and the 8-bytes variables respectively . in the whole process of computing non - linear terms",
    "9 scalars have to be sent and received ( 3 velocity components before ift and 6 velocity products after fft ) ; for each wall - parallel plane , each machine must exchange with each of the others @xmath127 nodes an amount of @xmath128 grid cells , and the factor @xmath129 corresponds to dealiasing in one horizontal direction ( the @xmath129 expansion , and the subsequent removal of higher - wavenumber modes , in the other horizontal direction can be performed after transmission ) .",
    "the ratio between the communication required by the transpose - based method and the pls method can thus be written as : @xmath130 which corresponds to the intuitive idea that the transpose method exchanges all the variables it stores locally , whereas the pls method only exchanges a ( small ) number of wall - parallel planes , independent on @xmath76 and @xmath1 .",
    "moreover the ratio @xmath131 , being proportional to @xmath76 for a given @xmath1 , is expected to increase with the reynolds number of the simulation , since so does the number of points needed to discretize the wall - normal direction , thus indicating an increase of pls efficiency relative to the transpose strategy .",
    "more important , when the transpose - based method is employed , the global amount of communication that has to be managed by the switch increases with the number of computing machines and is all - to - all rather than between nearest neighbors only , so that its performance is expected to degrade when a large @xmath1 is used .    for a case",
    "sized @xmath97 and @xmath96 , @xmath125 amounts to @xmath132 2700 mbit .",
    "this gives a lower bound for the communication time with the transpose - based method of 27 seconds with fast ethernet , to be compared with a single - processor execution time of @xmath133 seconds on a single pentium iii .",
    "indeed , when tested on the fast ethernet pentiums , the transpose - based method has not been found to yield any reduction of the wall - clock computing time : we have measured @xmath134 seconds .",
    "this is in line with the previous estimate : @xmath135 stems from a computing time @xmath136 plus a communication time of 29.8 seconds , which is near to the time estimate on the basis of @xmath125 and assuming network cards running at full speed .     of computing nodes .",
    "continuous line is the pls method , and dashed line is the transpose - based method.,scaledwidth=90.0% ]    with a faster network the performance of the transpose - based method improve .",
    "figure  [ fig : tempi - transpose ] reports comparative measurements between the pls and the transpose - based methods , run on 8 opteron boxes interconnected with gigabit ethernet .",
    "the pls method is run with the machines connected in a ring , while the transpose - based method is tested with machines linked through a switch .",
    "measurements show that @xmath137 can now be achieved with the transpose - based method . however , the transpose method performs best for the smallest problem size , while the pls shows the opposite behavior . for the @xmath115 case , which is a reasonable size for such machines ,",
    "the speedup from the transpose - based method is around one half of what can be obtained with pls .      in the present approach ,",
    "we exploit the availability of two cpus for each computing node by assigning to each of them the computation of a different fft , and then the rhs setup for one half of the wavenumber pairs .",
    "since this part of the code is local to each machine , there is no communication overhead associated with the use of the second cpu , therefore the gain is essentially independent of the number @xmath1 of computing machines .",
    "we have tested machines equipped with 2 cpus , and measured a 1.55 speedup on a 2-cpu pentium iii box .",
    "on the dual opteron system , which exploits a faster memory at 400 mhz and a proprietary memory and inter - processor bus , the speedup increases to 1.7 .",
    "the implementation of the shared - memory parallelism was not the main focus of the present work , so that for simplicity we have parallelized only the non - linear part of the time - stepping procedure , a portion of the serial code estimated around 80% .",
    "the additional smp gain is thus an interesting result , since it comes at a low cost .",
    "indeed , the cost of a dual - cpu node is only a fraction greater than the cost of a single - cpu computing node .",
    "systems with more than 2 cpu are today significantly more expensive and , although we have not tested any of them , may be suspected to perform quite inefficiently when used with the present application , owing to the increased memory - contention problems .",
    "hence we regard the use of a second cpu as an added bonus at a little extra cost .     of computing nodes , when 2 cpus per node are used .",
    "a continuous line denotes the pls method , and a dashed line the transpose - based method .",
    "open symbols refer to the single - cpu speedup.,scaledwidth=90.0% ]    the smp speedup is where the pls method shows another advantage compared with the transpose - based method .",
    "the second cpu can be exploited in the transpose - based method too , however in this case the global effectiveness degrades , since the speed of each node increases while the communication time remains the same .",
    "this can be appreciated in figure [ fig : tempi - transpose - smp ] , where performance for a case @xmath115 is observed to become unacceptable when @xmath138 and two cpu for each machine are used with the transpose method . with pls , on the other hand , the penalty associated with the larger amount of communication per unit of computing time is clearly seen to remain limited , and no significant decrease of the parallel efficiency",
    "is observed when the second cpu in each node is activated .",
    "in this paper we have described a numerical method suitable for the parallel direct numerical simulation of incompressible wall turbulence , capable of achieving high efficiency by using commodity hardware .",
    "the method can be used when the governing equations are written either in cartesian or in cylindrical coordinates .    the key point in its design is the choice of compact finite differences of fourth - order accuracy for the discretization of the wall - normal direction . the use of finite differences schemes , while retaining a large part of the accuracy enjoyed by spectral schemes , is crucial to the development of the parallel strategy , which exploits the locality of the fd operators to largely reduce the amount of inter - node communication .",
    "finite differences are also key to the implementation of a memory - efficient time integration procedure , which permits a minimal storage space of 5 variables per point , compared to the commonly reported minimum of 7 variables per point .",
    "this significant saving is available in the present case too , the use of compact schemes notwithstanding , since they can be written in explicit form , leveraging the missing third derivative in the governing equations .",
    "the parallel method described in this paper , based on the pipelined solution of the linear systems arising from the discretization of the viscous terms , achieves its best performance on systems where the number of computing nodes is significantly smaller than the number of points in the wall - normal direction .",
    "the global transpose of the data , which typically constrains dns codes to run on machines with very large networking bandwidth , is completely avoided .",
    "we have verified that a code based on transpose algorithms can not yield acceptable parallel speedups when fast ethernet network cards are employed . when the 10-times - faster gigabit ethernet is used , the transpose - based method yields positive speedups but can not compete , in absolute terms , with the present pls method , that is capable to guarantee high parallel efficiency also on state - of - the - art processors connected with relatively slow fast ethernet cards .",
    "as a result , the computing effort , as well as the required memory space , can be efficiently subdivided among a number of low - cost computing nodes . moreover",
    ", the distribution of data in wall - parallel slices allows us to exploit a particular , efficient and at the same time cost - effective connection topology , where the computing machines are connected to each other in a ring .",
    "getting rid of the switch is something that should not be underestimated .",
    "when the transpose - based code is run for a @xmath97 case on two opteron machines connected each other point - to - point without the hp switch , the parallel speedup increases significantly , from 1.13 to 1.53 . when a third machine is inserted between the two computing machines ,",
    "the parallel speedup becomes 1.40 . while we are not in the position to extrapolate the relevance of this result to higher - quality switches , removing the need for a switch altogether is certainly an improvement performance - wise .",
    "a dedicated system can be easily built , using commodity hardware and hence at low cost , to run a computer code based on the pls method .",
    "such a system grants high availability and throughput , as well as ease in expanding / upgrading .",
    "it is our opinion that this concept of personal supercomputer can be successful , since it is a specialized system , yet built with mass - market components , and can be fully dedicated to a single research group or even to a single researcher , rather than being shared among multiple users through a queueing system .",
    "moreover , the additional investment required to specialize towards the pls code a general - purpose cluster of pc is simply that required to acquire two additional network cards for each node , plus a few meters of network cable .",
    "this means that a cluster can be easily built in such a way that it works optimally both as a parallel computing server for the general public and a dns computer for a research group employing a pls - based code .    concerning the ( theoretical ) peak computing power , we have estimated in @xcite that the investment ( early 2003 ) required to obtain 200 gflop / s of peak power with a state - of - the - art supercomputer would be 50 - 100 times higher than that needed to build a personal supercomputer of the same power . the smaller investment , together with additional advantages like reduced power consumption and heat production , minimal floor space occupation , etc , allows the user to have dedicated access to the machine for unlimited time , thus achieving the highest throughput .",
    "as an example , in @xcite we have performed with the pentium iii - based machine a large number of turbulent channel flow simulations , whose global computational demand is estimated to be 300 - 400 times larger than the dns described in @xcite .",
    "even though our performance measurements have been partly carried out on relatively old computing hardware , we have demonstrated that very good parallel speedups can be obtained for problems whose computational size fits that of typical dns problems affordable with the given hardware .",
    "the sole significant difference performance - wise between such a system and a real supercomputer lies in the networking hardware , which offers significantly larger bandwidth and better latency characteristics in the latter case .",
    "however the negative effects of this difference are not felt when the present parallel algorithm is employed , since the need for a large amount of communication is removed _ a priori _ , thanks to the algorithm itself .",
    "the test of the pls method on the itanium ii cluster has been carried out at the sharcnet computing centre at the university of western ontario , canada .",
    "preliminary versions of this work have been presented by m.q . at the xv aimeta conference on theoretical mechanics @xcite and at the xi conference of the cfd society of canada @xcite .",
    "m.  quadrio , p.  luchini , the numerical solution of the incompressible navier ",
    "stokes equations in cartesian and cylindrical geometries on a low - cost , dedicated parallel computer . ,",
    "dip . ing .",
    "aerospaziale , politecnico di milano dia - sr 04 - 16 .",
    "http://pc - quadrio.aero.polimi.it / papers/2004-dia0416.pd% f[http://pc - quadrio.aero.polimi.it / papers/2004-dia0416.pd% f ]                                m.  quadrio , p.  luchini , j.  floryan , a parallel algorithm for the direct numerical simulation of turbulent channel flow , in : proc . of the xi conf . of the cfd society of canada , vancouver ( can ) ,",
    "may 28 - 30 , 2003 .",
    "m.  quadrio , p.  luchini , a 4th order accurate , parallel numerical method for the direct simulation of turbulence in cartesian and cylindrical geometries . , in : proc . of the xv aimeta conf . on theor .",
    "mech , 2001 ."
  ],
  "abstract_text": [
    "<S> a numerical method for the direct numerical simulation of incompressible wall turbulence in rectangular and cylindrical geometries is presented . </S>",
    "<S> the distinctive feature resides in its design being targeted towards an efficient distributed - memory parallel computing on commodity hardware . </S>",
    "<S> the adopted discretization is spectral in the two homogeneous directions ; fourth - order accurate , compact finite - difference schemes over a variable - spacing mesh in the wall - normal direction are key to our parallel implementation . </S>",
    "<S> the parallel algorithm is designed in such a way as to minimize data exchange among the computing machines , and in particular to avoid taking a global transpose of the data during the pseudo - spectral evaluation of the non - linear terms . </S>",
    "<S> the computing machines can then be connected to each other through low - cost network devices . </S>",
    "<S> the code is optimized for memory requirements , which can moreover be subdivided among the computing nodes . </S>",
    "<S> the layout of a simple , dedicated and optimized computing system based on commodity hardware is described . </S>",
    "<S> the performance of the numerical method on this computing system is evaluated and compared with that of other codes described in the literature , as well as with that of the same code implementing a commonly employed strategy for the pseudo - spectral calculation .    ,    navier  stokes equations , direct numerical simulation , parallel computing , turbulence , compact finite differences . </S>"
  ]
}