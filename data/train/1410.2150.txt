{
  "article_text": [
    "our era has witnessed the massive explosion of data and a dramatic improvement of technology in collecting and processing large data sets .",
    "we often encounter huge data sets that the number of features greatly surpasses the number of observations .",
    "it makes many traditional statistical analysis tools infeasible and poses great challenge on developing new tools .",
    "regularization methods have been widely used for the analysis of high - dimensional data .",
    "these methods penalize the least squares or the likelihood function with the @xmath0-penalty on the unknown parameters ( lasso , @xcite ) , or a folded concave penalty function such as the scad @xcite and the mcp@xcite .",
    "however , these penalized least - squares methods are sensitive to the tails of the error distributions , particularly for ultrahigh dimensional covariates , as the maximum spurious correlation between the covariates and the realized noises can be large in those cases . as a result",
    ", theoretical properties are often obtained under light - tailed error distributions @xcite .    to tackle the problem of heavy - tailed errors , robust regularization methods have been extensively studied .",
    "@xcite , @xcite and @xcite developed robust regularized estimators based on quantile regression for the case of fixed dimensionality .",
    "@xcite studied the @xmath0-penalized quantile regression in high dimensional sparse models .",
    "@xcite further considered an adaptively weighted @xmath0 penalty to alleviate the bias problem and showed the oracle property and asymptotic normality of the corresponding estimator .",
    "other robust estimators were developed based on least absolute deviation ( lad ) regression .",
    "@xcite studied the @xmath0-penalized lad regression and showed that the estimator achieves near oracle risk performance under the high dimensional setting .",
    "the above methods essentially estimate the conditional _ median ( or quantile ) _ regression , instead of the conditional _ mean _ regression function . in the applications where the mean regression is of interest , these methods are not feasible unless a strong assumption is made that the distribution of errors is symmetric around zero .",
    "a simple example is the heteroscedastic linear model with asymmetric noise distribution .",
    "another example is to estimate the conditional variance function such as arch model @xcite . in these cases ,",
    "the conditional mean and conditional median are very different .",
    "another important example is to estimate large covariance matrix without assuming light - tails .",
    "we will explain this more in details in section 5 .",
    "in addition , lad - based methods tend to penalize strongly on small errors .",
    "if only a small proportion of samples are outliers , they are expected to be less efficient than the least squares based method .",
    "a natural question is then how to conduct ultrahigh dimensional mean regression when the tails of errors are not light ? how to estimate the sample mean with very fast concentration when the distribution has only bounded second moment ?",
    "these simple questions have not been carefully studied .",
    "lad - based methods do not intend to answer these questions as they alter the problems of the study .",
    "this leads us to consider huber loss as another way of robustification .",
    "the huber loss @xcite is a hybrid of squared loss for relatively small errors and absolute loss for relatively large errors , where the degree of hybridization is controlled by one tuning parameter . unlike the traditional huber loss , we allow the regularization parameter to diverge ( or converge if its reciprocal is used ) in order to reduce the bias induced by the huber loss for estimating conditional mean regression function . in this paper ,",
    "we consider the regularized approximate quadratic ( ra - lasso ) estimator with an @xmath0 penalty and show that it admits the same @xmath1 error rate as the optimal error rate in the light - tail situation .",
    "in particular , if the distribution of errors is indeed symmetric around 0 ( where the median and mean agree ) , this rate is the same as the regularized lad estimator obtained in @xcite .",
    "therefore , the ra - lasso estimator does not lose efficiency in this special case . in practice , since the distribution of errors is unknown , ra - lasso is more flexible than the existing methods in terms of estimating the conditional mean regression function .",
    "a by - product of our method is that the ra - lasso estimator of the population mean has the exponential type of concentration even in presence of the finite second moment .",
    "@xcite studied this type of problem and proposed a class of losses to result in a robust @xmath2-estimator of mean with exponential type of concentration .",
    "we further extend his idea to the sparse linear regression setting .    as done in many other papers , estimators with nice sampling properties",
    "are typically defined through the optimization of a target function such as the penalized least - squares .",
    "the properties that are established are not the same as the ones that are computed . following the framework of @xcite , we propose the composite gradient descent algorithm for solving the ra - lasso estimator and develop the sampling properties by taking computational error into consideration .",
    "we show that the algorithm indeed produces a solution that admits the same optimal @xmath1 error rate as the theoretical estimator after sufficient number of iterations .",
    "this paper is organized as follows .",
    "first , in section [ sec2 ] , we introduce the ra - lasso estimator and show that it has the same @xmath1 error rate as the optimal rate under light - tails . in section [ sec3 ] , we study the property of the composite gradient descent algorithm for solving our problem and show that the algorithm produces a solution that performs as well as the solution in theory . in section [ sec4 ]",
    ", we show the connection between huber loss and catoni loss and establish an concentration inequality for robust estimation of mean .",
    "the estimation of the error s variance is investigated in section [ sec5 ] .",
    "numerical studies are given in section [ sec6 ] and [ sec7 ] to compare our method with two competitors .",
    "all technical proofs are presented in section [ sec : proofs ] .",
    "we consider the linear regression model @xmath3 where @xmath4 are independent and identically distributed ( i.i.d ) @xmath5-dimensional covariate vectors , @xmath6 are i.i.d errors , and @xmath7 is a @xmath5-dimensional regression coefficient vector .",
    "we consider the high - dimensional setting , where @xmath8 for some constant @xmath9 .",
    "we assume the distributions of @xmath10 and @xmath11 are independent and both have mean 0 . under this assumption , @xmath12 is the mean effect of @xmath13 conditioning on @xmath10 , which is assumed to be of interest .    to adapt for different magnitude of errors and robustify the estimation",
    ", we propose to use the huber loss @xcite : @xmath14 the huber loss is quadratic for small values of @xmath15 and linear for large values of @xmath15 .",
    "the parameter @xmath16 controls the blending of quadratic and linear penalization .",
    "the least squares and the lad can be regarded as two extremes of the huber loss for @xmath17 and @xmath18 , respectively . deviated from the traditional huber s estimator",
    ", the parameter @xmath16 converges to zero in order to reduce the biases of estimating the mean regression function when the conditional distribution of @xmath19 is not symmetric . on the other hand",
    ", @xmath16 can not shrink too fast in order to maintain the robustness . in this paper , we regard @xmath16 as a tuning parameter , whose optimal value will be discussed later in this section . in practice",
    ", @xmath16 needs to be tuned by some data - driven method . by letting @xmath16 vary , we call @xmath20 the robust approximate quadratic ( ra - quadratic ) loss .    to estimate @xmath12",
    ", we propose to solve the following convex optimization problem : @xmath21 to assess the performance of @xmath22 , we study the property of @xmath23 , where @xmath24 is the euclidean norm of a vector .",
    "when @xmath25 converges to zero sufficiently fast , @xmath22 is a natural @xmath2-estimator of @xmath26 , which is the population minimizer under the ra - quadratic loss and varies by @xmath16 . in general",
    ", @xmath27 differs from @xmath12 .",
    "but , since the ra - quadratic loss approximates the quadratic loss as @xmath16 tends to 0 , @xmath27 is expected to converge to @xmath12 .",
    "this property will be established in theorem  [ thm1 ] .",
    "therefore , we decompose the statistical error @xmath28 into the approximation error @xmath29 and the estimation error @xmath30 .",
    "the statistical error @xmath23 is then bounded by @xmath31 in the following , we give the rate of the approximation and estimation error , respectively .",
    "we show that @xmath23 admits the same rate as the optimal rate under light tails , as long as the two tuning parameters @xmath16 and @xmath25 are properly chosen .",
    "we first give the rate of the approximation error under some moment conditions on @xmath10 and @xmath11 .",
    "we assume both @xmath12 and @xmath27 are interior points of an @xmath1 ball with sufficiently large radius .",
    "[ thm1 ] it holds that @xmath32 , under the following conditions :    * @xmath33 , for some @xmath34 .",
    "* @xmath35)\\leq\\lambda_{\\max}(\\operatorname{e}[\\bx\\bx^t])\\leq \\ku<\\infty$ ] , * for any @xmath36 , @xmath37 is sub - gaussian with parameter at most @xmath38 , i.e. @xmath39 , for any @xmath40 .",
    "theorem [ thm1 ] reveals that the approximation error vanishes faster if higher moments of error distribution exists .",
    "we next give the rate of the estimation error @xmath41 .",
    "this part differs from the existing work regarding the estimation error of high dimensional regularized @xmath2-estimator @xcite as the population minimizer @xmath27 now varies with @xmath16 .",
    "however , we will show that the estimation error rate does not depend on @xmath16 , given a uniform sparsity condition .    in order to be solvable in the high - dimensional setting ,",
    "@xmath12 is usually assumed to be sparse or weakly sparse , i.e. many elements of @xmath12 are zero or small . by theorem [ thm1 ]",
    ", @xmath27 converges to @xmath12 as @xmath16 goes to 0 . in view of this fact ,",
    "we assume that @xmath27 is uniformly weakly sparse when @xmath16 is sufficiently small .",
    "in particular , we assume that there exists a small constant @xmath42 , such that @xmath27 belongs to an @xmath43-ball with a uniform radius @xmath44 that @xmath45 for all @xmath46 $ ] , and some @xmath47 $ ] . when the conditional distribution of @xmath19 is symmetric , @xmath48 for all @xmath16 and @xmath49 .",
    "therefore the condition reduces to that @xmath12 is in the @xmath43 ball . in a special case where @xmath50",
    ", it follows from theorem [ thm1 ] that if @xmath12 belongs to the @xmath0-ball with radius @xmath51 and @xmath52^{\\frac{1}{k-1}}$ ] , where @xmath53 is a generic constant , then @xmath27 belongs to the @xmath0-ball with radius @xmath54 for all @xmath46 $ ] . for a general @xmath55",
    ", we assume a uniform upper bound @xmath44 as in ( [ eq2.4 ] ) , which is allowed to diverge to infinity .    since the ra - quadratic loss is convex",
    ", we show that with high probability the estimation error @xmath56 belongs to a star - shaped set , which depends on @xmath16 and the threshold level @xmath57 of signals .",
    "[ lem1 ] under conditions ( c1 ) and ( c3 ) , by choosing @xmath58 and @xmath59 , where @xmath60 , @xmath61 and @xmath62 are some constants , with probability greater than @xmath63 , @xmath64 where @xmath65 , @xmath57 is a positive constant , @xmath66 and @xmath67 denotes the subvector of @xmath68 with indices in set @xmath69 .",
    "we further verify a restricted strong convexity ( rsc ) condition , which has been shown to be critical in the study of high dimensional regularized @xmath2-estimator @xcite .",
    "let @xmath70^t\\bdelta,\\ ] ] where @xmath71 , @xmath68 is a @xmath5-dimensional vector and @xmath72 is the gradient of @xmath73 at the point of @xmath74 .",
    "[ def : rsc ] the loss function @xmath73 satisfies rsc condition on a set @xmath75 with curvature @xmath76 and tolerance @xmath77 if @xmath78    next , we show that with high probability the ra - quadratic loss ( [ eq2.2 ] ) satisfies rsc for all @xmath79 with uniform constants @xmath80 and @xmath77 that do not depend on @xmath16 .",
    "lemma [ lem2 ] is a preliminary result , based on which rsc is checked in lemma [ lem3 ] .",
    "[ lem2 ] under conditions ( c1)-(c3 ) , for all @xmath81 , there exist uniform positive constants @xmath82 and @xmath83 , such that @xmath84 with probability at least @xmath85 for some positive constants @xmath86 and @xmath87 .",
    "[ lem3 ] suppose conditions ( c1)-(c3 ) hold and assume that @xmath88 with the choice @xmath89 , with probability at least @xmath85 , the rsc condition holds for @xmath90 and @xmath91 with @xmath92 and @xmath93 .",
    "lemma [ lem3 ] shows that , even though @xmath27 is unknown and the set @xmath94 depends on @xmath16 , rsc holds with uniform constants that do not depend on @xmath16 .",
    "this further gives the following upper bound of the estimation error @xmath41 , which also does not depend on @xmath16 .",
    "[ thm2 ] under conditions of lemma [ lem1 ] and , with probability at least @xmath95 , @xmath96^{1/2-q/4}).\\ ] ]    finally , theorem [ thm1 ] and [ thm2 ] together lead to the following main result , which gives the rate of the statistical error @xmath23 .",
    "[ thm3 ] under conditions of lemma [ lem1 ] and , with probability at least @xmath95 , @xmath97^{1/2-q/4}).\\ ] ]    next , we compare our result with the existing results regarding the robust estimation of high dimensional linear regression model .    1 .",
    "when the distribution of @xmath11 is symmetric around 0 , then @xmath98 for any @xmath16 , which has no approximation error .",
    "if @xmath11 has heavy tails in addition to being symmetric , we would like to choose @xmath16 sufficiently large to robustify the estimation .",
    "it then follows from theorem [ thm2 ] that @xmath99^{1/2-q/4})$ ] , where @xmath100 . the rate is the same as the minimax rate @xcite for weakly sparse model under the light tails . in a special case that @xmath101 , it gives @xmath102 , where @xmath103 is the number of nonzero elements in @xmath12",
    "this is the same rate as the regularized lad estimator in @xcite and the regularized quantile estimator in @xcite .",
    "it suggests that our method does not lose efficiency for symmetric heavy - tailed errors . 2 .",
    "if the distribution of @xmath11 is asymmetric around 0 , the quantile and lad based methods are inconsistent , since they estimate the median instead of the mean .",
    "theorem [ thm3 ] shows that our estimator still achieves the optimal rate given that @xmath104^{1-\\frac{q}{2}}\\}^{\\frac{1}{2(k-1)}})$ ] even though the @xmath105-th moment of @xmath106 is assumed . recall from conditions in lemma [ lem1 ] that we also need to choose @xmath107 for some constant @xmath108 .",
    "given the sparsity condition ( [ eq2.7 ] ) , @xmath16 can be chosen to meet the above two requirements . in terms of estimating the conditional mean effect , errors with heavy but asymmetric tails",
    "give the case where the ra - lasso has the biggest advantage over the other estimators .",
    "in practice , the distribution of errors is unknown .",
    "however , we proved that our method is no worse than the existing methods for any type of errors , as long as the tuning parameters are chosen properly .",
    "hence , our method is more flexible .",
    "the gradient descent algorithm @xcite is usually applied to solve the convex problem ( [ eq2.3 ] ) . for example , we can replace the ra - quadratic loss with its local quadratic approximation ( lqa ) and iteratively solve the following optimization problem : @xmath109^t ( \\bbeta-{\\widehat}{\\bbeta}^t )      + \\frac{\\gamma_{u}}{2 } { \\lvert\\bbeta-{\\widehat}{\\bbeta}^t\\rvert_2}^2+\\lambda_n { \\lvert\\bbeta\\rvert_1}\\right\\},\\ ] ] where @xmath110 is a fixed constant at each iteration , and the side constraint `` @xmath111 '' is introduced to guarantee good performance in the first few iterations and @xmath112 is allowed to be sufficiently large . to solve ( [ eq3.1 ] ) , the update can be computed by a two - step procedure .",
    "we first solve ( [ eq3.1 ] ) without the norm constraint by soft - thresholding the vector @xmath113 at level @xmath25 and call the solution @xmath114 .",
    "if @xmath115 , set @xmath116 . otherwise , @xmath117 is obtained by further project @xmath114 onto the @xmath0-ball @xmath118",
    ". the projection can be done @xcite by soft - thresholding @xmath114 at level @xmath119 , where @xmath119 is given by the following procedure : ( 1 ) sort @xmath120 into @xmath121 ; ( 2 ) find @xmath122 and let @xmath123 .",
    "@xcite considered the computational error of such first - order gradient descent method .",
    "they showed that , for a convex and differentiable loss functions @xmath124 and decomposable penalty function @xmath125 , the error @xmath126 has the same rate as @xmath23 for all sufficiently large @xmath127 , where @xmath128 , and @xmath129 . different from their setup , our population minimizer @xmath27 varies by @xmath16 . nevertheless ,",
    "as @xmath27 converges to the true effect @xmath12 , by a careful control of @xmath16 , we can still show that @xmath126 has the same rate as @xmath23 , where @xmath22 is the theoretical solution of ( [ eq2.3 ] ) and @xmath130 is as defined in ( [ eq3.1 ] ) .",
    "the key is that the ra - quadratic loss function @xmath73 satisfies the restricted strong convexity ( rsc ) condition and the restricted smoothness condition ( rsm ) with some uniform constants , namely @xmath131 as defined in ( [ eq2.5 ] ) satisfies the following conditions : @xmath132 for all @xmath74 and @xmath68 in some set of interest , with parameters @xmath133 , @xmath134 , @xmath135 and @xmath136 that do not depend on @xmath16 .",
    "we show that such conditions hold with high probability .",
    "[ lem4 ] under conditions ( c1)-(c3 ) , for all @xmath137 and @xmath138 , with probability greater than @xmath85 , ( [ eq3.2 ] ) and ( [ eq3.3 ] ) hold with @xmath139 , @xmath140 , @xmath141 , @xmath142 .",
    "we further show in theorem [ thm4 ] that , whenever @xmath143 , which is required for consistency of _ any method _ over the weak sparse @xmath43 ball by the known minimax results @xcite , it holds that @xmath144 for sufficiently many iterations with lower bound specified in theorem  [ thm4 ] .",
    "hence , @xmath145^{1/2-q/4}),\\end{aligned}\\ ] ] which has the same rate as @xmath23 .",
    "hence , from a statistical point of view , there is no need to iterate beyond @xmath127 steps .",
    "[ thm4 ] under conditions of theorem [ thm3 ] , suppose we choose @xmath25 as in lemma [ lem1 ] and also satisfying @xmath146\\frac{\\log p}{n},\\ ] ] where @xmath147 denotes the cardinality of set @xmath148 and @xmath149 , then with probability at least @xmath150 , we have @xmath151 \\right),\\ ] ] for all iterations @xmath152 where @xmath153 and @xmath154 is the initial value , @xmath155 is the tolerance level , @xmath156 and @xmath106 are some constants as will be defined in ( [ eq7.21 ] ) and ( [ eq7.22 ] ) , respectively .",
    "@xcite considered the estimation of the mean of heavy - tailed distribution with fast concentration .",
    "he proposed an @xmath2-estimator by solving @xmath157=0,\\ ] ] where the influence function @xmath158 is chosen such that @xmath159 .",
    "he showed that this @xmath2-estimator has the exponential type of concentration by only requiring the existence of the variance .",
    "it performed as well as the sample mean under the light - tail case . in section [ sec2 ]",
    ", we essentially showed the same type of concentration for the ra - quadratic loss under the linear regression setting .",
    "the estimation of mean can be regarded as a univariate linear regression where the covariate equals to 1 . in that special case , we have a more explicit concentration result for the ra - mean estimator , which is the estimator that minimizes the ra - quadratic loss .",
    "let @xmath160 be an i.i.d sample from some unknown distribution with @xmath161 and @xmath162 .",
    "the ra - mean estimator @xmath163 of @xmath164 is the solution of @xmath165=0,\\ ] ] for parameter @xmath166 , where the influence function @xmath167 if @xmath168 , @xmath169 , if @xmath170 and @xmath171 if @xmath172 .",
    "the following theorem gives the exponential type of concentration of @xmath163 around @xmath164 .",
    "[ thm5 ] assume @xmath173 and let @xmath174 where @xmath175 .",
    "then , @xmath176    the above result provides fast concentration of the mean estimation with only two moments assumption .",
    "this is very useful for last scale hypothesis testing @xcite and covariance matrix estimation @xcite , where uniform convergence is required . taking the estimation of large covariance matrix as an example , in order for the elements of the sample covariance matrix to converge uniformly , the aforementioned authors require the underlying multivariate distribution be sub - gaussian .",
    "this restrictive assumptions can be removed if we apply the robust estimation with concentration bound . regarding @xmath177 as the expected value of the random variable @xmath178 ( it is typically not the same as the median of @xmath179 ) , it can be estimated with accuracy @xmath180 where @xmath181 and @xmath182 is ra - mean estimator using data @xmath183 .",
    "since there are only @xmath184 elements , by taking @xmath185 and the union bound , we have @xmath186 when @xmath187 .",
    "this robustified covariance estimator requires much weaker condition than the sample covariance and has far wide applicability than the sample covariance .",
    "it can be regularized further in the same way as the sample covariance matrix .",
    "on the other hand , catoni s idea could also be extended to the linear regression setting .",
    "suppose we replace the ra - quadratic loss @xmath20 in ( [ eq2.3 ] ) with catoni loss @xmath188 where the influence function @xmath189 is given by @xmath190 let @xmath191 be the corresponding solution .",
    "then , @xmath191 has the same convergence rate as the ra - lasso , when the second or the third moment of errors exists .    [ thm6 ] suppose condition ( c1 ) holds for @xmath192 or 3 , ( c2 ) , ( c3 ) and ( [ eq2.7 ] ) hold , then with probability at least @xmath95 , @xmath193^{1/2-q/4}).\\ ] ]    unlike the ra - lasso , the order of bias of @xmath191 can not be further improved , even when higher moments of errors exist beyond the third order .",
    "the reason is that the catoni loss is not exactly the quadratic loss over any finite intervals .",
    "similar results regarding the computational error of @xmath191 could also be established as in theorem [ thm4 ] , since the rsc / rsm conditions also hold for catoni loss with uniform constants .",
    "we estimate @xmath194 based on the ra - lasso estimator and a cross - validation scheme . to ease the presentation",
    ", we assume the data set can be evenly divided into @xmath195 folds with @xmath196 observations in each fold .",
    "then , we estimate @xmath197 by @xmath198 where @xmath199 is the ra - lasso estimator obtained by using data points outside the @xmath105-th fold .",
    "we show that @xmath200 is asymptotically efficient .",
    "[ thm7 ] under conditions of theorem [ thm3 ] , if @xmath201 for @xmath202 , and @xmath203^{1-\\frac{q}{2}}\\}^{\\frac{1}{2(k-1)}}\\right)$ ] , then @xmath204",
    "in this section , we assess the finite sample performance of the ra - lasso and compare it with other methods through various models .",
    "we simulated data from the following high dimensional model @xmath205 where we generated @xmath206 observations and the number of parameters was chosen to be @xmath207 .",
    "we chose the true regression coefficient vector as @xmath208 where the first 20 elements are all equal to 3 and the rest are all equal to 0 . to involve various shapes of error distributions , we considered the following five scenarios :    1 .",
    "normal with mean 0 and variance 4 ( n(0,4 ) ) ; 2 .   two times",
    "the t - distribution with degrees of freedom 3 ( @xmath209 ) ; 3 .",
    "mixture of normal distribution(mixn ) : @xmath210 ; 4 .",
    "log - normal distribution ( lognormal ) : @xmath211 , where @xmath212 is standard normal .",
    "weibull distribution with shape parameter = 0.3 and scale parameter = 0.5 ( weibull ) .    in order to meet the model assumption ,",
    "the errors were standardized to have mean 0 .",
    "table [ tab1 ] categorizes the five scenarios according to the shapes and tails of the error distributions .",
    ".summary of the shapes and tails of five error distributions [ tab1 ] [ cols=\"^,^,^\",options=\"header \" , ]     ]",
    "let @xmath213 . since @xmath12 minimizes @xmath214 , it follows from condition(c2 ) that @xmath215=(\\betaa-\\betas)^t\\operatorname{e}(\\bx\\bx^t)(\\betaa-\\betas)\\geq      \\kappa_l   { \\lvert\\betaa-\\betas\\rvert_2}^2.\\ ] ] let @xmath216 . then , since @xmath217 is the minimizer of @xmath218 , we have @xmath219\\\\      & = & \\operatorname{e}[\\ell(y-\\bx^t\\betaa)-\\ell_{\\alpha}(y-\\bx^t\\betaa ) ]      + \\operatorname{e}[\\ell_{\\alpha}(y-\\bx^t\\betaa)-\\ell_{\\alpha}(y-\\bx^t\\betas)]\\\\      &   & \\hspace{3ex}+\\operatorname{e}[\\ell_{\\alpha}(y-\\bx^t\\betas)-\\ell(y-\\bx^t\\betas)]\\\\      & \\leq & \\operatorname{e}[g_{\\alpha}(y-\\bx^t\\betaa)]-\\operatorname{e}[g_{\\alpha}(y-\\bx^t\\betas ) ] .    \\end{aligned}\\ ] ] by taylor s expansion , we have @xmath220      \\leq   2\\operatorname{e } [ ( z-\\alpha^{-1 } ) i(z>\\alpha^{-1 } ) |\\bx^t(\\betaa-\\betas)|],\\ ] ] where @xmath221 is a vector lying between @xmath12 and @xmath27 and @xmath222 . with @xmath223 denoting the conditional expectation with respect to @xmath11 given @xmath10 , we have @xmath224 \\leq \\operatorname{e}_{\\epsilon } [ z i(z > \\alpha^{-1 } ) ] \\leq      \\alpha^{k-1}\\operatorname{e}_{\\epsilon } z^k .",
    "\\end{aligned}\\ ] ] therefore , @xmath225 $ ] is further bounded by @xmath226,\\ ] ] where the constant @xmath227 is defined in condition ( c1 ) .",
    "next , we show that @xmath228)=o(1)$ ] .",
    "let @xmath229 be a @xmath5-dimensional vector with @xmath230 . by the cauchy - schwartz inequality ,",
    "@xmath231\\leq [ \\operatorname{e}(m_k+|\\bx^t(\\tilde{\\bbeta}-\\betas)|^k)^4]^{1/2 }      [ \\operatorname{e}(\\bx^t\\bnu)^4]^{1/2}.\\ ] ] by ( c3 ) , @xmath37 is sub - gaussian with parameter @xmath232 . under the assumption that @xmath12 and @xmath27 are interior points of an @xmath1-ball with sufficiently large radius , @xmath233 is sub - gaussian with parameter @xmath234 , which is no larger than @xmath235 . using the moment results of sub - gaussian random variables @xcite , @xmath236 .",
    "similarly , @xmath237 .",
    "therefore , @xmath238=o(1)$ ] .",
    "hence , by definition , @xmath228)=o(1)$ ] . using this result and ( [ eq7.3 ] ) , @xmath239&\\leq      2(2\\alpha)^{k-1}[\\lambda_{\\max}(\\operatorname{e}[(m_k+|\\bx^t(\\tilde{\\bbeta}-\\betas)|^k)^2\\bx\\bx^t])]^{1/2 }      { \\lvert\\betaa-\\betas\\rvert_2}\\\\      & = o(\\alpha^{k-1 } { \\lvert\\betaa-\\betas\\rvert_2 } ) .",
    "\\end{aligned}\\ ] ] this together with ( [ eq7.1 ] ) completes the proof .",
    "first of all , it follows from lemma 1 of @xcite that @xmath240 on the event @xmath241 .",
    "hence , we need to show that the event @xmath241 holds with high probability .",
    "the latter will be established by using bernstein s inequality along with the union bound .",
    "the gradient of @xmath73 , @xmath242\\bx_i,\\ ] ] where @xmath167 , for @xmath168 ; @xmath169 , for @xmath170 ; and @xmath171 , for @xmath172 . using @xmath243 and assumption ( c3 ) , we have @xmath244x_{ij } \\}^2      & \\leq 4\\operatorname{e}\\{(y_i-\\bx_i^t\\betaa)^2x_{ij}^2 \\}\\\\      & \\leq 8\\operatorname{e}\\{(\\epsilon_i^2+|\\bx_i^t(\\betaa-\\betas)|^2)x_{ij}^2\\}\\\\      & \\leq v ,    \\end{aligned}\\ ] ] where @xmath61 is a constant depending on @xmath245 and @xmath246 and",
    "the last inequality follows from a similar argument as in the proof of theorem [ thm1 ] . by ( c3 ) and that @xmath247 , @xmath248x_{ij}$ ] is also sub - gaussian . for any @xmath249 , using the relation between the @xmath105th moment and the second moment of sub - gaussian random variables @xcite , @xmath250x_{ij}|^k\\leq \\frac{k!}{2}l^{k-2}\\operatorname{e}|\\psi[\\alpha(y_i-\\bx_i^t\\betaa)]x_{ij}|^2,\\ ] ] where @xmath62 is a constant depending on @xmath245 only .",
    "hence , @xmath251x_{ij}|^k",
    "\\leq \\frac{k!}{2}(2l/\\alpha)^{k-2 } v.\\ ] ] by bernstein inequality ( proposition 2.9 of @xcite ) and note that @xmath252\\bx_i)={\\ensuremath{\\boldsymbol{0}}}$ ] , we have @xmath253x_{ij}\\right| \\geq        \\sqrt{\\frac{2vt}{n}}+\\frac{lt}{\\alpha",
    "n } \\right)\\leq 2\\exp(-t).\\ ] ] let @xmath254 and observe that @xmath255 by the choice of @xmath25 and @xmath16 .",
    "we have @xmath253x_{ij}\\right| \\geq        \\frac{\\lambda_n}{2}\\right ) \\leq",
    "2\\exp \\left(-\\frac{n\\lambda_n^2}{32 v } \\right).\\ ] ] it then follows from union inequality that @xmath256\\bx_i \\right \\rvert_{\\infty}}>\\frac{\\lambda_n}{2 } \\right ) & \\leq 2 \\exp      \\left(-\\frac{n\\lambda_n^2}{32 v}+\\log p \\right)=2p^{-c_0 } ,    \\end{aligned}\\ ] ] where @xmath257 .",
    "this completes the proof .",
    "denote @xmath258 . applying a second - order taylor expansion to @xmath259 between @xmath27 and @xmath260 ,",
    "we conclude that for some @xmath261 $ ] , @xmath262(\\bx_i^t\\bdelta)^2 , \\label{eq7.5 }    \\end{aligned}\\ ] ] where @xmath263 for @xmath168 , and @xmath264 otherwise .",
    "note that each term in is nonnegative .",
    "however , the quadratic component in is not lipschitz continuous with a bounded lipschitz coefficient . in order to apply the contraction theorem of @xcite",
    ", we introduce a truncation function that is lipschitz and bound from below .",
    "let @xmath265 where @xmath266 is the indicator function .",
    "clearly , @xmath267 and satisfies the lipschitz condition with lipschitz coefficient bounded by @xmath268 .",
    "we first show @xmath269 for @xmath270 , where the thresholds @xmath271 and @xmath272 will be chosen as in ( [ eq7.10 ] ) .",
    "let @xmath273(\\bx_i^t\\bdelta)^2 $ ] .",
    "we need only to show that @xmath274 when @xmath275 or @xmath276 , the right hand side is zero and the inequality holds trivially .",
    "thus , we need only to consider the case @xmath277 and @xmath278 . in this case ,",
    "@xmath279 and hence @xmath280=1 $ ] . using this , @xmath281    using ( [ eq7.7 ] ) , to prove the lemma ,",
    "we need to show that , for any fixed @xmath282 $ ] , with high probability @xmath283 where constants @xmath82 and @xmath83 do not depend on @xmath16 and @xmath284 this is equivalent to proving ( [ eq7.8 ] ) for @xmath285 . indeed , from the definition ( [ eq7.6 ] ) , for any @xmath286 and @xmath287",
    ", we have @xmath288 .",
    "thus , the event @xmath289 is the same as the event @xmath290 which equals to the event with @xmath291 .    to establish ,",
    "let us consider its complementary event .",
    "define @xmath292 let @xmath293 be the unit sphere with @xmath1-radius one , and @xmath294 be the sphere of @xmath0-radius @xmath127 , which is to be chosen later .",
    "the complementary event of is given by @xmath295<\\kappa_1\\{1-\\kappa_2 \\sqlogpn { \\lvert\\bdelta\\rvert_1}\\ } , \\text { for some }      \\bdelta\\in \\bbs_2(1)\\big\\}.\\ ] ] our goal is to show that the probability of this event is very small , which is demonstrated through the following three steps .    * first",
    ", we show that with the following choice of truncation @xmath296 for any fixed @xmath297 , we have @xmath298\\geq \\kl/2.\\ ] ] * second , with @xmath299-\\operatorname{e}[\\gd(\\bx)]|$ ] , we prove the tail probability bound for @xmath300 is bounded by @xmath301 for each given @xmath127 . *",
    "finally , we use a standard peeling argument @xcite to establish @xmath302    the result ( c ) together with ( [ eq7.11 ] ) show that the probability of the complementary event of ( [ eq7.9 ] ) with @xmath303 and @xmath304 is bounded by @xmath305 , which completes the proof .",
    "we first prove ( [ eq7.11 ] ) .",
    "in fact , by condition ( c2 ) , for any @xmath306 , @xmath307\\geq \\kl { \\lvert\\bdelta\\rvert_2}^2=\\kl$ ] .",
    "so , it suffices to show that @xmath308\\leq \\kl/2 $ ] .",
    "note that , @xmath309 for all @xmath10 such that @xmath310 and @xmath311 .",
    "therefore , we have @xmath312\\leq \\operatorname{e}[(\\bx^t\\bdelta)^2i(|y-\\bx^t\\betaa|\\geq      t)]+\\operatorname{e}[(\\bx^t\\bdelta)^2i(|\\bx^t\\bdelta|\\geq \\tau/2)].\\ ] ] to bound the first term on the right hand side of ( [ eq7.13 ] ) , it follows from the cauchy - schwartz inequality that @xmath313 \\leq [ \\operatorname{e}(\\bx^t\\bdelta)^4]^{1/2 }      [ p(|y-\\bx^t\\betaa|\\geq t)]^{1/2}.\\ ] ] since @xmath314 is sub - gaussian with parameter at most @xmath315 by assumption ( c3 ) , we have @xmath316 .",
    "meanwhile , for any @xmath317 , it follows from the chebyshev inequality and theorem [ thm1 ] that @xmath318\\\\      & \\leq 2\\operatorname{e}\\epsilon^2 + 2\\operatorname{e}[\\bx^t(\\betas-\\betaa)]^2\\\\      & \\leq 2 { m_k}^{2/k}+o(\\alpha^{2k-2})\\\\      & \\leq 4 { m_k}^{2/k}.    \\end{aligned}\\ ] ] to bound the second term on the right hand side of ( [ eq7.13 ] ) , by the concentration inequality of sub - gaussian variables , we have @xmath319 then , by the choice of @xmath271 and @xmath272 in ( [ eq7.10 ] ) , @xmath313\\leq \\frac{\\kl}{4 } \\qquad \\text { and }",
    "\\qquad      \\operatorname{e}[(\\bx^t\\bdelta)^2i(|\\bx^t\\bdelta|\\geq \\tau/2 ) ] \\leq \\frac{\\kl}{4}.\\ ] ] hence , ( [ eq7.11 ] ) follows .",
    "next , we give the tail bound as in ( b ) . indeed , for any @xmath297 , we have @xmath320 .",
    "therefore , by massart concentration inequality ( theorem 14.2 of @xcite ) , for any @xmath321 , we have @xmath322 . by choosing @xmath323 , we have @xmath324    next , we bound @xmath325 .",
    "let @xmath326 be an i.i.d .",
    "sequence of rademacher variables .",
    "a symmetrization theorem ( theorem 14.3 of @xcite ) yields @xmath327\\leq 2\\operatorname{e}\\left[\\sidx |\\frac{1}{n } \\sum_{i=1}^n \\omega_i        \\gd(\\bx_i)|\\right]=2\\operatorname{e}\\left[\\sidx |\\frac{1}{n }        \\sum_{i=1}^n \\omega_i\\varphi_{\\tau}(\\fd(\\bx_i))|\\right].\\ ] ] by definition , the function @xmath328 is lipschitz with parameter at most @xmath329 and @xmath330 .",
    "therefore , by the ledoux - talagrand contraction theorem ( @xcite , p.112 ) , we have @xmath331&\\leq 8\\tau^2\\operatorname{e}\\left[\\sidx |\\frac{1}{n } \\sum_{i=1}^n \\omega_i \\fd(\\bx_i)|\\right ] \\\\      & = 8\\tau^2\\operatorname{e}\\left[\\sidx |\\frac{1}{n } \\sum_{i=1}^n \\omega_i\\bx_i^t\\bdelta        i(|y_i-\\bx_i^t\\betaa|\\leq t)|\\right]\\\\      & \\leq 8\\tau^2 t \\operatorname{e}{\\left \\lvert\\frac{1}{n } \\sum_{i=1}^n \\omega_i\\bx_ii(|y_i-\\bx_i^t\\betaa|\\leq t ) \\right \\rvert_{\\infty}}.    \\end{aligned}\\ ] ] since the variables @xmath332 are zero - mean i.i.d .",
    "sub - gaussian with parameter at most @xmath315 , so are @xmath333 . since @xmath334 is the maxima of @xmath5 such terms , known bounds on the expectation of sub - gaussian maxima ( e.g. see @xcite , p.79 ) yield @xmath335 hence , @xmath336\\leq 24\\tau^2\\ko t \\sqlogpn.\\ ] ] combining ( [ eq7.14 ] ) and ( [ eq7.15 ] ) , we have @xmath337 where constants @xmath338 and @xmath339 depends on @xmath340 and @xmath341 only .",
    "this result holds for each given @xmath127 .",
    "next , we furnish the peeling argument in ( c ) .",
    "let @xmath342 and @xmath343 .",
    "since @xmath344 , the set can be covered by partition @xmath345 with @xmath346 .",
    "thus , by union bound , @xmath347 since @xmath348 for @xmath349 . by letting @xmath350 as in ( [ eq7.12 ] ) and solving for @xmath127 , by , we obtain @xmath351 where the last inequality follows from sum of geometric series .",
    "note that , @xmath352 therefore , @xmath353 . let @xmath354 , we have @xmath355 hence , for any @xmath356 , we have @xmath357 by the cauchy - schwartz inequality and , we can bound further that @xmath358 it then follows from lemma [ lem2 ] that @xmath359 \\}\\\\      & = \\big(\\kappa_1 - 4\\kappa_1\\kappa_2\\rq^{1/2}\\eta^{-q/2}\\sqlogpn\\big )      { \\lvert\\bdelta\\rvert_2}^2 - 4\\kappa_2\\rq\\eta^{1-q}\\sqlogpn .",
    "\\end{aligned}\\ ] ] with @xmath360 and @xmath89 , it holds that @xmath361 which is no larger than @xmath362 under assumption ( [ eq2.7 ] ) . on the other hand",
    ", @xmath363 therefore , rsc holds with @xmath364 and @xmath365 .",
    "let @xmath366 and @xmath367 denote the events that lemma [ lem1 ] and lemma [ lem3 ] hold , respectively . by theorem 1 of @xcite , within @xmath368",
    ", it holds that @xmath369^{1-(q/2)}\\right ) ,    \\end{aligned}\\ ] ] where ( i ) follows from the choice of @xmath89 . on the other hand , by lemma [ lem1 ] and [ lem3 ] , @xmath370 .    from the proof of lemma [ lem2 ]",
    ", we can see that indeed holds for all @xmath74 and @xmath371 that @xmath372 using the fact that @xmath373 , we conclude that @xmath374 therefore , ( [ eq3.2 ] ) holds with @xmath139 and @xmath140 . meanwhile , since @xmath375 , it follows from ( [ eq7.5 ] ) that @xmath376 under the sub - gaussianity assumption ( c3 ) , it follows from some existing work ( e.g. page 18 of @xcite ) that , with probability great than @xmath85 , it holds that @xmath377 where @xmath86 and @xmath87 are some generic constants .",
    "hence , ( [ eq3.3 ] ) holds with @xmath378 and @xmath379 .",
    "\\(a ) we first show that , for any @xmath380 , @xmath381 , for all @xmath127 greater than the right hand side of ( [ eq7.20 ] ) , where @xmath382 is a contraction constant and @xmath106 is a tolerance parameter , which will be given in ( [ eq7.21 ] ) and ( [ eq7.22 ] ) , respectively .        _ [ theorem 2 of @xcite ] suppose for any data set @xmath385 , the loss function @xmath386 is convex and differentiable and the regularizer @xmath387 is a norm . consider the optimization problem of @xmath388 for a radius @xmath112 such that @xmath389 is feasible , where @xmath390 , and a regularization parameter @xmath25 satisfying bound @xmath391 where @xmath392 is the dual norm of the regularizer .",
    "in addition , suppose that the loss function @xmath73 satisfies the rsc / rsm condition with parameters ( @xmath133 , @xmath134 ) and ( @xmath135 , @xmath136 ) , respectively .",
    "let @xmath393 be any @xmath387-decomposable pair of subspaces such that @xmath394 where @xmath395 , @xmath396 , @xmath397 , and @xmath398 .",
    "denote @xmath399 , where @xmath400 is the projection of @xmath389 onto @xmath401 .",
    "then for any @xmath380 , we have @xmath402 for all @xmath403 where @xmath404 , @xmath405 is the solution by the gradient descent algorithm after @xmath406 iteration , and @xmath407 is the initial value of @xmath408 . _    in fact ,",
    "theorem 2 of @xcite is a deterministic statement for all choices of pairs @xmath393 . from lemma [ lem1 ] and lemma [ lem4 ]",
    ", we have shown that with our choice of @xmath25 , the ra - quadratic loss function satisfy ( [ eq7.18 ] ) and rsc / rsm with probability at least @xmath63 and @xmath85 , respectively .",
    "hence , theorem 2 of @xcite applies to our problem with high probability .",
    "we further choose the pair @xmath409 and give the explicit expression of constants for our problem as the follows : @xmath410 where @xmath411 , @xmath412 , and @xmath413 .",
    "it remains to check ( [ eq7.19 ] ) . by ( [ eq7.21 ] )",
    ", @xmath382 is equivalent to requiring @xmath414 with @xmath89 , it follows from ( [ eq7.16 ] ) that @xmath415 hence , ( [ eq7.23 ] ) holds when @xmath416 is sufficiently large .",
    "moreover , from we need @xmath146\\frac{\\log p}{n},\\ ] ] which is satisfied under the stated assumption .",
    "it then follows from theorem 2 of @xcite that , for any @xmath417 , @xmath381 , for all iterations @xmath127 greater than the right hand side of ( [ eq7.20 ] ) .    for step ( b )",
    ", it follows from rsc condition that @xmath418^t      ( { \\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta } ) \\geq \\frac{\\gamma_l}{2 }      { \\lvert{\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta}\\rvert_2}^2-\\tau_l      { \\lvert{\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta}\\rvert_1}^2.\\ ] ] then we have @xmath419^t({\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta})+      \\lambda_n({\\lvert{\\widehat}{\\bbeta}^t\\rvert_1}-{\\lvert{\\widehat}{\\bbeta}\\rvert_1})+\\frac{\\gamma_l}{2 }      { \\lvert{\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta}\\rvert_2}^2-\\tau_l { \\lvert{\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta}\\rvert_1}^2 .",
    "\\end{aligned}\\ ] ] since @xmath22 is the minimizer of @xmath420 , by the first - order condition , @xmath421^t({\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta})\\geq    0 $ ] .",
    "therefore , @xmath422^t({\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta})+",
    "\\lambda_n({\\lvert{\\widehat}{\\bbeta}^t\\rvert_1}-{\\lvert{\\widehat}{\\bbeta}\\rvert_1})+\\frac{\\gamma_l}{2 }      { \\lvert{\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta}\\rvert_2}^2-\\tau_l { \\lvert{\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta}\\rvert_1}^2.\\ ] ] by the convexity of the @xmath0-norm , @xmath423^t({\\widehat}{\\bbeta}^t-{\\widehat}{\\bbeta } ) \\geq 0 $ ] .",
    "hence , @xmath424 next , we bound @xmath425 .",
    "it follows from lemma 3 of @xcite that @xmath426 where @xmath427 is defined as in ( a ) .",
    "then , by the cauchy - schwartz inequality , @xmath428 equations ( [ eq7.24 ] ) and ( [ eq7.25 ] ) together with results in ( a ) imply that , @xmath429 letting @xmath430 , we have @xmath431 we now bound the second term in ( [ eq7.26 ] ) . by ( [ eq7.16 ] ) and ( [ eq7.17 ] ) , we have @xmath432 .      \\end{split}\\ ] ] meanwhile , from ( a ) we have @xmath433 since @xmath434 , @xmath435 , @xmath436 , @xmath437 , and @xmath438 , it follows from ( [ eq7.26 ] ) , ( [ eq7.27 ] ) and ( [ eq7.28 ] ) that @xmath439 \\right ) .\\ ] ]    the proof follows the same spirit of the proof of proposition 2.4 of @xcite .",
    "the influence function @xmath440 satisfies @xmath441 using this and independence , with @xmath442 $ ] , we have @xmath443 \\right\\ }      & \\leq \\bigl ( \\operatorname{e}\\{\\exp \\ { \\psi[\\alpha(y_i-\\theta ) ] \\ } \\big ) ^n \\\\      & \\leq \\{1+\\alpha(\\mu-\\theta)+\\alpha^2[\\sigma^2+(\\mu-\\theta)^2 ] \\}^n\\\\      & \\leq \\exp \\left\\{n\\alpha(\\mu-\\theta)+n\\alpha^2 [ v^2+(\\mu-\\theta)^2 ] \\right\\}.    \\end{aligned}\\ ] ] similarly , @xmath444",
    "\\right\\}\\leq \\exp    \\left\\{-n\\alpha(\\mu-\\theta)+n\\alpha^2 [ v^2+(\\mu-\\theta)^2 ] \\right\\}$ ]",
    ". define @xmath445+\\frac{\\log ( 1/\\delta)}{n\\alpha}\\\\      b_-(\\theta)&=\\mu-\\theta-\\alpha [ v^2+(\\mu-\\theta)^2]-\\frac{\\log ( 1/\\delta)}{n\\alpha }    \\end{aligned}\\ ] ] by chebyshev inequality , @xmath446 \\right\\ } }      { \\exp\\{\\alpha n(\\mu-\\theta)+n\\alpha^2 [ v^2+(\\mu-\\theta)^2]+\\log(1/\\delta ) \\}}\\leq \\delta\\ ] ] similarly , @xmath447 .",
    "let @xmath448 be the smallest solution of the quadratic equation @xmath449 and @xmath450 be the largest solution of the equation @xmath451 . under the assumption that @xmath452 and the choice of @xmath453 , we have @xmath454 .",
    "therefore , @xmath455 similarly , @xmath456 with @xmath457 , @xmath458 , @xmath459 .",
    "since the map @xmath460 is non - increasing , under event @xmath461 @xmath462 i.e. @xmath463 . meanwhile , @xmath464 .    first , we prove that the approximation error rate @xmath465 , where @xmath466 is the population minimizer under the catoni loss .",
    "let @xmath467 \\text{d}t$ ] .",
    "it follows from ( [ eq7.2 ] ) that @xmath468\\leq      \\operatorname{e}[|g_{\\alpha}'(y-\\bx^t\\tilde{\\bbeta})\\bx^t(\\betacs-\\betas)|],\\ ] ] where @xmath221 is a vector lying between @xmath12 and @xmath469 . since @xmath470 , by the second - order taylor expansion with an integral remainder , @xmath471 hence , by the cauchy - schwartz inequality , @xmath472&\\leq \\alpha^2      \\operatorname{e}[|y-\\bx^t\\tilde{\\bbeta}|^3|\\bx^t(\\bbeta_{\\alpha}^{c\\ast}-\\betas)| ] \\\\              & \\leq 4\\alpha^2\\operatorname{e}[(m_3+|\\bx^t(\\tilde{\\bbeta}-\\betas)|^3)|\\bx^t(\\bbeta_{\\alpha}^{c\\ast}-\\betas)|]\\\\      & \\leq 4\\alpha^2[\\lambda_{\\max}(\\operatorname{e}[(m_3+|\\bx^t(\\tilde{\\bbeta}-\\betas)|^3)^2      \\bx\\bx^t])]^{1/2 } { \\lvert\\bbeta_{\\alpha}^{c\\ast}-\\betas\\rvert_2 } ,    \\end{aligned}\\ ] ] which is of order @xmath473 , as @xmath474)=o(1)$ ] by an analogous argument as in the proof of theorem [ thm1 ] .",
    "similarly as in ( [ eq7.1 ] ) , @xmath475\\geq \\kappa_l      { \\lvert\\betaa-\\betas\\rvert_2}^2.\\ ] ] hence , @xmath476 .",
    "if @xmath11 only has the second moment exist , by a first - order taylor expansion of @xmath477 similarly as in ( [ eq7.29 ] ) , we have @xmath478 .",
    "next , since @xmath479 , by the same argument as in the proof of lemma [ lem3 ] , rsc holds for catoni s loss with probability no less than @xmath85 .",
    "hence , similarly as in theorem [ thm2 ] , @xmath480^{1/2-q/4})$ ] .",
    "this together with @xmath465 completes the proof .",
    "first of all , observe that @xmath481 given that @xmath482 exists , by central limit theorem , @xmath483 .",
    "let @xmath484 .",
    "we now need to prove that the last two terms are negligible .",
    "conditioning on data outside the @xmath105th fold , @xmath485 hence , @xmath486 , where the last equality follows from theorem [ thm3 ] . by an analogous argument",
    ", we have @xmath487^{1-q/2 } \\})=o(1/\\sqrt{n } ) .",
    "\\end{aligned}\\ ] ] this completes the proof of the theorem .",
    "duchi , j. , shalev - shwartz , s. , singer , y. , and chandra , t. ( 2008 ) .",
    "efficient projections onto the @xmath0-ball for learning in high dimensions .",
    "_ proceedings of the 25th international conference on machine learning _ , 272279 .",
    "huang , c.  c. , liu , k. , pope , r.  m. , du , p. , lin , s. , rajamannan , n.  m. , et  al .",
    "activated tlr signaling in atherosclerosis among women with lower framingham risk score : the multi - ethnic study of atherosclerosis . _",
    "plos one _ , * 6 * , e21067 .",
    "wainwright , m. j. ( 2013 ) . regularized @xmath2-estimators with nonconvexity : statistical and algorithmic theory for local optima .",
    "_ advances in neural information processing systems _ , 476484 .",
    "negahban , s. n. , ravikumar , p. , wainwright , m. j. , and yu , b. ( 2012 ) . a unified framework for high - dimensional analysis of m - estimators with decomposable regularizers .",
    "_ statistical science _ * 27 * , 538557 ."
  ],
  "abstract_text": [
    "<S> data subject to heavy - tailed errors are commonly encountered in various scientific fields , especially in the modern era with explosion of massive data . to address this problem , </S>",
    "<S> procedures based on quantile regression and least absolute deviation ( lad ) regression have been developed in recent years . </S>",
    "<S> these methods essentially estimate the conditional median ( or quantile ) function </S>",
    "<S> . they can be very different from the conditional mean functions when distributions are asymmetric and heteroscedastic . </S>",
    "<S> how can we efficiently estimate the mean regression functions in ultra - high dimensional setting with existence of only the second moment ? to solve this problem , we propose a penalized huber loss with diverging parameter to reduce biases created by the traditional huber loss . </S>",
    "<S> such a penalized robust approximate quadratic ( ra - quadratic ) loss will be called ra - lasso . in the ultra - high dimensional </S>",
    "<S> setting , where the dimensionality can grow exponentially with the sample size , our results reveal that the ra - lasso estimator produces a consistent estimator at the same rate as the optimal rate under the light - tail situation . </S>",
    "<S> we further study the computational convergence of ra - lasso and show that the composite gradient descent algorithm indeed produces a solution that admits the same optimal rate after sufficient iterations . as a byproduct </S>",
    "<S> , we also establish the concentration inequality for estimating population mean when there exists only the second moment . </S>",
    "<S> we compare ra - lasso with other regularized robust estimators based on quantile regression and lad regression . </S>",
    "<S> extensive simulation studies demonstrate the satisfactory finite - sample performance of ra - lasso .    _ </S>",
    "<S> key words _ : high dimension , huber loss , m - estimator , optimal rate , robust regularization . </S>"
  ]
}