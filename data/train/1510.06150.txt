{
  "article_text": [
    "[ [ section ] ]    [ [ section-1 ] ]    [ [ section-2 ] ]",
    "[ [ section-3 ] ]    [ [ section-4 ] ]      [ [ section-5 ] ]    [ [ section-6 ] ]    [ [ section-7 ] ]    [ [ section-8 ] ]      [ [ section-9 ] ]      [ [ section-10 ] ]    [ [ section-11 ] ]      [ [ section-12 ] ]    [ [ section-13 ] ]      [ [ section-14 ] ]    [ [ section-15 ] ]    [ [ validation ] ] validation + + + + + + + + + +    [ [ perplexity ] ] perplexity + + + + + + + + + +    [ [ section-16 ] ]    [ [ verification ] ] verification + + + + + + + + + + + +",
    "[ [ section-17 ] ]    [ [ section-18 ] ]    [ [ section-19 ] ]    * both types of vertices ( compute nodes and users ) arrive online with respect to some probability distribution , whereas traditional bipartite matching algorithms assume there exist one group of offline vertices and another group arrives online .",
    "* after a computing node finishes its assigned task , the node may become available again for matching . in traditional matching problems ,",
    "once a vertex is matched ( or exceeds its capacity / budget as in some variations of the matching problem , see @xcite ) it can not be matched again .",
    "* there are tradeoffs between strategyproofness and optimizing different metrics ( see section [ sec : evaluation - metrics ] ) .",
    "traditional matching algorithms usually aim to maximize the sum of weights across all matched edges .",
    "[ [ section-20 ] ]    * a computing node has edges connected to all users whose computing task can be computed by this node within reasonable amount of time * each task submitter is also a computing node * the distribution of computing power and the arriving time of computing nodes are known .",
    "[ [ section-21 ] ]    [ [ section-22 ] ]    [ [ section-23 ] ]    [ [ section-24 ] ]    * @xmath0 : computing devices that may join the system as sellers and buyers * @xmath1 : current active buyers waiting on query results * @xmath2 : current active sellers providing computation resources * @xmath3 : a query carrying a computation task submitted by a buyer * @xmath4 : a matching between a query @xmath5 and a seller tuple @xmath6 * @xmath7 : @xmath8for each device , the amount of time until the next query is made after the device s current query is completed    [ [ section-25 ] ]    * @xmath9 : the device that initiated query @xmath5 * @xmath10 : the gain for @xmath9 when query @xmath5 is matched with sellers @xmath11 * @xmath12 : the total wait time when @xmath5 is matched with sellers @xmath11 , and this matching at @xmath13-th position waiting for seller @xmath14 , and @xmath15-th position waiting for seller @xmath16 .",
    "the total wait time includes both the amount of time waiting for a matching to establish and the amount of time during which actual computation takes place . * @xmath17 : the amount of computation time for seller @xmath18 to compute query @xmath5 * @xmath19 : time until device @xmath18 finishes his current computation . if @xmath18 is not currently doing any computation , @xmath20 .",
    "* @xmath21 : the computation performance ( measured in tokens / second ) of a device @xmath18 is drawn from a distribution @xmath22 with parameters @xmath23 * @xmath24 : the size of the computing task associated with query @xmath5 ( measured in number of tokens ) drawn from a distribution @xmath25 with parameters @xmath26 . *",
    "@xmath27 : the number of servers .",
    "a server is defined as a computing device that is much faster than others , and which never submits tasks ( thus never generates queries ) .",
    "the inclusion of a small number of servers is helpful in improving the initial performance of the system when there are very few sellers .",
    "servers are also helpful for handling queries that have been waiting in the queue for too long .",
    "[ [ section-26 ] ]",
    "[ [ section-27 ] ]    * average wait time : the average amount of time for fulfilling each query .",
    "the objective function is @xmath28 where the other variables of @xmath29 are observed .",
    "this metric measures the overall throughput of our system .",
    "a lower average wait time means our system is efficient in maximizing the average rate at which queries are fulfilled and results returned to the user .",
    "* maximum wait time : the maximum amount of time taken to complete any query ,",
    "i.e : @xmath30 .",
    "this metric measures the worst case wait time for any participating user .",
    "a lower maximum wait time gives a guaranteed time bound for any query and often means that computational resources are more evenly shared across devices . * slow query ratio : the percent of queries that are fulfilled slower than if they were computed by the buyers themselves .",
    "this metric measures the overall incentives for the users to use the system as opposed to doing computation alone .",
    "lower slow query ratio is desirable in the sense that it means users are more likely to save computing time by participating in the system . in the event that slow query ration is less than @xmath31 for every user",
    ", we have that for any user submitting a query the most likely scenario is one in which the query is fulfilled faster than the user could compute the result on his own .",
    "this can also be used to identify the existence of a nash equilibrium in which all users benefit from staying in the system . *",
    "average amount of time saved / wasted : the average amount of time saved / wasted across all queries for each device .",
    "similar to slow query ratio , these two measurements give deeper insight into how much users gain by participating the system . *",
    "net gain / net loss per device , and the number of devices with net gain / loss : these are additional metrics for measuring incentives to join the system and the stability of the system . in the case of number of devices with net loss , a value of 0 means that no users received negative average utility for being a part of the system .",
    "in designing our heuristics , we pay special attention to two desirable properties of any matching system and optimization metric ",
    "nash equilibrium and strategyproofness . in the following discussion we let players represent devices in the system , where each player",
    "s goal is to minimize the expected amount of time he waits for his queries to be fulfilled .",
    "+   + suppose each player in the system has two options : stay in the network and allow the system to handle fulfillment of his queries , or leave the network and self - fulfill all queries . in a state of nash equilibrium",
    ", we have that the expected utility of remaining in the system is at least the expected utility of leaving the system . from our player goals",
    ", we have that for each player @xmath32 the following should hold : @xmath33 here , each @xmath34 represents a query made by @xmath32 , @xmath35 and @xmath36 represents matchings made by the system for a query , and @xmath37 and @xmath38 represent their respective orderings .",
    "thus , the expected amount of time it takes the matching system to compute a result for a player should be no more than the amount of time it would take @xmath32 to perform the computations himself . as such , in expectation it should be beneficial for each player to remain a part of the system in order to prevent a scenario in which the fastest devices continuously leave the system , causing a cascading effect .",
    "+   + we define the notion of strategyproofness as such : a matcher is strategyproof if no player can benefit from querying unnecessary products or misreporting his performance .",
    "the first of these follows directly from the fact that we require a player to wait until his last query is completed to make the next query .",
    "thus , in expectation submitting unnecessary queries will lead to an increase in wait time and decreased utility of the system for that player .",
    "+   + for the second , we must address two options .",
    "in the first , a player attempts to overreport his performance by completing computations faster than should be possible given his device hardware . from our verification system",
    ", a player who behaves in this way will in expectation have decreased credit , leading to the eventual removal from the system when the player is identified as misbehaving . on the other hand , it should be the case that a player can not benefit by underreporting his performance .",
    "in this situation , a player takes longer than is necessary to perform computations in order to improve his wait times . in order to avoid this ,",
    "several heuristics are examined that give priority to faster devices . in this case",
    ", it is optimal for a player to report the highest possible performance in order to increase his query priority ; overreporting will lead to the loss of credit , and thus the optimal strategy is to report one s true performance .",
    "we examined two primary types of matcher : immediate and scheduled . in an immediate matcher , a matching event is triggered each time a query arrives or a computation completes . with an immediate matcher , we have the capability of providing the fastest possible time - to - match , however we may miss more optimal matchings that could be obtained by waiting for new sellers to make queries and enter the marketplace .",
    "+   + this issue is explored using scheduled matchers , which instead perform matchings at a fixed rate without regard for any events occurring within the marketplace . by performing matchings at a fixed rate ,",
    "it is sometimes possible to obtain a better assignment of devices to queries by waiting for a small amount of time and accumulating a larger pool of sellers . from this , it is often possible to compute a better matching than would have been obtained by an immediate matcher .",
    "for example , consider a matcher that simply matches each incoming query , in order of arrival time , with the two fastest sellers available .",
    "suppose that we have two idle servers @xmath39 and @xmath40 such that @xmath41 for all user devices @xmath42 . for simplicity , assume all user devices have the same performance .",
    "if queries arrive in order from largest to smallest , our matcher will have to wait for the largest tasks to finish before starting the next smallest task , and so on .",
    "this results in substantial wait times that can be remedied by performing scheduled matchings , reordering events by task size , and fulfilling queries in ascending order .",
    "the order in which queries are processed in one of the key areas of improvement in designing heuristics .",
    "we wish to reorder and fulfill queries in such a way that optimize for each of our evaluation metrics .",
    "below , we briefly describe a few heuristics along with the goals of each .      in the fifo ordering , a matcher attempts to fulfill queries in the order in which they arrive . in this way , the fifo ordering defines a truly greedy ordering with respect to arrival time . in a real - time system",
    ", this allows matchings to be made extremely quickly .",
    "in addition , by ignoring the task size and performance of each seller when ordering queries we obtain a framework whereby a device can not misreport his performance in order to improve the priority of his tasks .",
    "unfortunately , the fifo ordering suffers from many pitfalls . as a key example , this ordering makes the system directly dependent on the ordering in which queries arrive , even within a small timeframe . as such , small variations in query arrival times can result in extremely high variance of wait times .",
    "each arriving query begins with a priority equal to its number of tokens .",
    "then , in each matching round , we order all unfilled queries in descending order of priority and attempt to fulfill queries in order . after each round of matching",
    ", the priorities of all remaining unmatched queries are doubled .",
    "thus , we attempt to reduce in the variance in the number of rounds a query remains unmatched while still giving priority to larger , harder to complete tasks .      in the hardest - to - fulfill heuristic , we order all unfilled queries in ascending order of hardness to fulfill , defined as the amount of time it would take the querying device to do the computation alone : @xmath43 in this way",
    ", we give priority to queries that are hardest to compute faster than the querying device could have done the computation . in this way",
    ", we hope to optimize the percent of queries that are returned faster than a device could obtained alone , thus moving towards a system in which having all devices as part of the system is a nash equilibrium .",
    "once a query is selected as the next - in - line to attempt to fulfill , we have two options .",
    "we can match the query with one or more available sellers , or we can defer the matching of the query until the next round . for each of the following",
    ", we have the option of each performing partial matchings ",
    "a single seller can begin fulfilling a query before the second matched seller is available  or enforcing complete matchings  both seller devices must be available for computation before beginning to fulfill a query .",
    "we also test probabilistic versions of several selectors , where each query has a random probability of being skipped during each matching round . by adding this random component ,",
    "the goal is to maintain a distribution of idle sellers that is more robust to sudden influxes of large tasks , resulting in better overall performance .      in the fifo heuristic ,",
    "we match each incoming with the first two sellers in our list .",
    "not surprisingly , this is the worst of our heuristics by most metrics .",
    "while a fifo selector is in itself strategyproof with respect to performance misreporting , ignoring the performance of sellers and the task size of queries results in the frequent situation in which large tasks are assigned to slow sellers . for each of the heuristics below",
    ", we attempt to avoid this situation .      in the greedy fastest selector",
    ", we match a query with the fastest available sellers . by doing so ,",
    "we attempt to maximize the throughput of the system by maximally utilizing the fastest available devices .",
    "interestingly , this heuristic actually leads to decreased utilization of fast devices . by attempting to quickly satisfy all queries , fast sellers quickly leave the computation pool and only slow sellers remain in times of high load .      in order to maximally utilize our fast sellers and minimize the probability that a query is fulfilled slower than the query device",
    "could have computed the result itself ( e.g. maximize the probability of `` on - time '' queries ) , we propose two heuristics : fastest on - time and slowest - on time .",
    "+   + in the fastest on - time heuristic , we match a query with the fastest devices such that both matched devices are fast enough to fulfill the query on - time . if no such sellers are available , we defer matching of the query until the next round . if the performance of a seller required to fulfill a query is faster than all sellers , we match the query with the fastest available sellers regardless of their performance .",
    "+   + in the slowest on - time heuristic , we match a query with the slowest devices such that both matched devices are fast enough to fulfill the query on - time . in the way , we still attempt to fulfill queries on - time while keeping faster sellers in the system longer and buffering against spikes of large tasks . in doing so ,",
    "we sacrifice average wait time in low load situations .",
    "however , in the event of high load , the large proportion of available fast sellers results in a net decrease in average wait time as we better avoid the situation in which large tasks are matched to slower - performing devices .",
    "+   + in a particular instance of the sp slowest on - time selector , paired with a immediate matcher and hardest - to - fulfill query reorderer , we empirically show that the result eventually demonstrates nash equilibrium with high probability , such that for each device the best option is to remain in the system to submit queries .",
    "in the subsections below , we derive a distributional model under a known query distribution to obtain a model that allows us to match current queries with future sellers . by doing so ,",
    "we minimize the expected wait time across all queries , taking into account the probability that any given seller will query a product and enter the marketplace .",
    "suppose for our optimization metric we choose to maximize the expected gain across all unmatched queries . in deriving a theoretically optimal algorithm , we assume the following :    1 .",
    "the query distribution @xmath44 is known and fixed .",
    "2 .   the performance of each device is known .",
    "3 .   computation time is a deterministic function of device performance and task size , namely @xmath45 for device @xmath18 and query @xmath5 .",
    "no partial matchings are allowed  a query must be matched to two sellers before either seller computation can begin .",
    "we have an infinite number of products , where each query is of a distinct product .    then for any matching",
    ", we have the following maximization problem : @xmath46\\end{gathered}\\ ] ] by linearity of expectation , the maximization problem above is equivalent to maximizing the sum of expected gains across all possible matchings . also , note that @xmath47 is a constant across all possible matchings , and for optimization purposes we can ignore it .",
    "thus we have : @xmath48\\end{gathered}\\ ] ] additionally , we can decompose the wait time for a query @xmath49 as follows : @xmath50 here , we define @xmath51 as the recursive function that represents the total time until device @xmath18 is idle for the @xmath52th person in line waiting on @xmath18 .",
    "let @xmath53 denote the query ahead of @xmath5 in line waiting for device @xmath18 , and @xmath54 denote the time until device @xmath18 becomes idle starting at the time when @xmath18 finishes computation for the @xmath52 queries ahead of @xmath5 in line . then : @xmath55 from this , we obtain the optimal matching @xmath56 : @xmath57\\\\ & = { \\operatornamewithlimits{argmin}}_{m \\in \\mathcal{m } } \\sum_{(q , p_1 , p_2 , k_1 , k_2 ) \\in m } e[\\max(\\psi(p_1 ) + \\phi(p_1 , q ) + \\tau(p_1 , k ) , \\psi(p_2 ) + \\phi(p_2 , q ) + \\tau(p_2 , k)]\\end{aligned}\\ ] ] +      suppose we assume that queries frequencies follow an exponential distribution with parameter @xmath58 such that @xmath59 .",
    "in order to make the above problem tractable , we approximate @xmath51 with @xmath60 and break ordering ties by assigning each incoming query a random priority and ordering conflicting matchings from highest to lowest priority .",
    "then , given the tasks that can be started after we obtain our optimal matching , we iteratively recompute the optimal matching on our remaining queries until convergence . by using the expectation possible future query times , we allow a query to match with future devices and wait for them to appear in the marketplace . +   + we now utilize our exponential distribution over query frequency to formulate our matching algorithm . substituting in our approximation for @xmath61",
    ", we obtain : @xmath62\\end{aligned}\\ ] ] we then have three cases for our expectation . in the first ,",
    "suppose both @xmath39 and @xmath40 are idle .",
    "then @xmath63 , and our expectation is simply the maximum of two constant valued expressions .",
    "+   + in the second case , suppose one of @xmath64 or @xmath65 is 0 .",
    "we then have the expected maximum over a constant - valued expression and a shifted exponential distribution . as an example",
    ", suppose that @xmath66 and @xmath67 .",
    "then : + @xmath68\\\\ = & \\psi(p_1 ) + \\phi(p_1 , q ) + e_{f(\\lambda;\\alpha)}[\\max(0 , \\psi(p_2 ) + \\phi(p_2 , q ) - \\psi(p_1 ) - \\phi(p_1 , q ) + \\lambda)]\\\\ = & \\psi(p_1 ) + \\phi(p_1 , q ) + \\alpha e^{-\\alpha ( \\psi(p_1 ) + \\phi(p_1 ,",
    "q ) -\\psi(p_2 ) - \\phi(p_2 , q))}\\\\\\end{aligned}\\ ] ] in the final case , suppose neither device is currently idle .",
    "furthermore , without loss of generality let @xmath69 .",
    "then we have : @xmath70\\\\ = & \\int_{\\lambda = 0}^{\\infty } \\alpha \\lambda ( e^{-\\alpha ( \\lambda - \\psi(p_1 ) - \\phi(p_1 , q ) ) } - 2 e^{-\\alpha ( 2 \\lambda - \\psi(p_1 ) - \\phi(p_1 , q ) - \\psi(p_2 ) - \\phi(p_2 , q ) ) } + e^{-\\alpha(\\lambda - \\psi(p_2 ) - \\phi(p_2 , q ) ) } ) d\\lambda\\\\ = & \\alpha^{-1 } + \\psi(p_2 ) + \\phi(p_2 , q ) + \\frac{1}{2 } e^{-\\alpha(\\psi(p_1 ) + \\phi(p_1 , q ) - \\psi(p_2 ) - \\phi(p_2 , q))}\\end{aligned}\\ ] ] from these expressions , we are able to compute the approximate optimal solution by selecting the pairing @xmath71 for each unfilled query @xmath5 that minimizes the expected waiting time , breaking ties using random priorities as described above .",
    "[ [ section-28 ] ]    [ [ instantspreversedimproved - maxnetlossmaxtimewastedaveragetimewastedinstantspimprovedmaxnetlossmaxtimewastedaveragetimewasted .- instantspimproved - instantsp.instantsp - scheduledspscheduledminvar ] ] instantspreversedimproved maxnetlossmaxtimewastedaveragetimewastedinstantspimprovedmaxnetlossmaxtimewastedaveragetimewasted . instantspimproved instantsp.instantsp scheduledspscheduledminvar + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    .[tab : performance - metrics - comparison ] performance metrics comparison between algorithms .",
    "the first three columns are taken from the statistics over the last ~250000 queries .",
    "the last column is taken from the cumulative statistics of the entire run .",
    "all units are in seconds or fractions . [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "to conclude , we have formulated a new problem space that extends to general situations in which a user wishes to repeatedly perform low - bandwidth , computationally - intensive tasks . while the chital system was designed with lda in mind , it is easy to see how the system could prove useful in any number of machine learning settings that require multiple restarts to mitigate the risk of bad local optima ( e.g. neural networks , clustering ) .",
    "we defined several evaluation metrics of matcher performance and described heuristics that optimize for these evaluation metrics while attempting to obtain nash equilibrium and enforce strategyproofness of performance reporting and querying . additionally , we showed how assuming a known distribution over incoming queries can theoretically improve matcher performance by allowing the matcher to perform probabilistically sound pairings between current unfilled queries and future sellers .",
    "+   + for future research , it would be interesting to explore the modeling of our optimization problem using physical systems .",
    "for example , we may choose to represent the matching problem as an electrical circuit in which each resistor represents the amount of time required to complete a task .",
    "additional work on improving the speed of the algorithm in [ sec : exponential - approximation ] may prove useful in scaling up the algorithm to larger scale , realtime systems .",
    "we also wish to formally prove the state of nash equilibrium in our sp matchers .",
    "while the immediate matching sp slowest on - time matcher appeared to _ eventually _ reach a state in which all users had net positive gain from the system under our tested situations , a formal proof on the conditions under which this state will be achieved would be useful in better characterizing the underlying properties of the matcher across a broader class of situations .",
    "10 aaron q li , multi - gpu distributed parallel bayesian differential topic model .",
    "australian national university ( thesis )    aaron q li , amr ahmed , mu li , vanja josifovski , alexander j smola , high performance latent variable models .",
    "www 2015 ( submitted ) .",
    "aaron q li , amr ahmed , sujith ravi , alexander j smola , reducing sampling complexity of topic models .",
    "acm sigkdd 2014 .",
    "aaron q li , joseph w robinson , yuntian deng , kublai jing , creating scalable and interactive web applications using high performance latent variable models .",
    "cmu 10 - 715 final project .",
    "andrew kachites mccallum , mallet : a machine learning for language toolkit . http://mallet.cs.umass.edu .",
    "david m blei , andrew y ng , michael i jordan , latent direchlet allocation . journal of machine learning research 3 .",
    "mu li , li zhou , zichao yang , aaron q li , fei xia , david g anderson , alexander j smola , parameter server for distributed machine learning .",
    "nips big learning workshop 2013 .",
    "jure leskovec and andrej krevl , snap large network dataset collection 2014 .",
    "http://snap.stanford.edu/data/web-amazon-links.html .",
    "aranyak mehta , online matching and ad allocation , foundations and trends in theoretical computer science 2013 .",
    "subhash khot and oded regev , vertex cover might be hard to approximate to within @xmath72 , ieee conference on computational complexity , 2003 .",
    "yajun wang and sam chiu - wai wong , online vertex cover and matching : beating the greedy algorithm , 2013"
  ],
  "abstract_text": [
    "<S> in this paper we describe matching mechanisms for a real - time computational resource exchange market , chital , that incentivizes participating clients to perform computation for their peers in exchange for overall improved performance . </S>",
    "<S> the system is designed to discourage dishonest behavior via a credit system , while simultaneously minimizing the use of dedicated computing servers and the number of verifications performed by the administrating servers . </S>",
    "<S> we describe the system in the context of a pre - existing system ( under development ) , vedalia @xcite , for analyzing and visualizing product reviews , by using machine learning such as topic models . </S>",
    "<S> we extend this context to general computing tasks , describe a list of matching algorithms , and evaluate their performance in a simulated environment . </S>",
    "<S> in addition , we design a matching algorithm that optimizes the amount of time a participant could save compared to computing a task on their own , and show empirically that this algorithm results in a situation in which it is almost always optimal for a user to join the exchange than do computation alone . </S>",
    "<S> lastly , we use a top - down approach to derive a theoretically near - optimal matching algorithm under certain distributional assumptions on query frequency . </S>"
  ]
}