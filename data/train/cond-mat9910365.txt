{
  "article_text": [
    "since the introduction of the ising spin model , the study of models with discrete degrees of freedom has become a core activity in statistical mechanics .",
    "when combined with disorder , such models often have interesting connections to problems of computational complexity , to learning theory or to open problems in statistics .",
    "discreteness and disorder introduce intrinsic difficulties , and exactly solvable models are rare .",
    "the main purpose of this paper is to present a discrete model with disorder which can be solved in full detail .",
    "the model is most naturally presented as an unsupervised learning problem , and we briefly review the connection with the existing literature .    the goal of unsupervised learning is finding structure in high - dimensional data . in one of the simplest parametric models introduced in the literature  ,",
    "@xmath3-dimensional independently drawn data vectors @xmath4 , @xmath5 , are uniformly distributed , except for a single symmetry - breaking direction @xmath6 . if we assume that all the relevant probability distributions are known , the aim of learning is to construct an estimate vector @xmath1 of the true direction @xmath6 .",
    "previous studies of this model focused on the case where @xmath6 is constrained to have a constant size , being otherwise equiprobably sampled from the @xmath3-sphere .",
    "this so - called _ spherical case _ is associated with a spherical _ prior distribution _ @xmath7 . the focus of the present paper , however , is on binary ( or ising ) vectors . in this case",
    ", @xmath6 is known to have binary components only , @xmath8 , @xmath9 .",
    "this extra knowledge is taken into account by assigning a binary prior distribution    @xmath10\\ ] ]    to the preferential direction .    in this framework",
    ", a gaussian scenario was introduced in  @xcite as a kind of minimal model , allowing the calculations to be much simplified and the spherical case to be solved exactly . in this model ,",
    "the components of @xmath11 perpendicular to @xmath6 are assumed to be independent gaussian distributed variables with zero mean and unit variance , i.e. @xmath12 , where @xmath13 .",
    "the distribution of the component @xmath14 parallel to @xmath6 , on the other hand , can be chosen at will , and in the gaussian scenario it is completely determined by the mean @xmath15 and variance @xmath16 :    @xmath17    where @xmath18^{-1 } % = \\exp[-b^{2}/(2(1-a))]/\\sqrt{1-a } $ ] is a normalization constant and @xmath19/\\sqrt{2\\pi}$ ] .    in comparison with the spherical case",
    ", the binary case presents several extra difficulties , which motivates the study of this simple model .",
    "the main question to be addressed in this work is : given the @xmath20 data vectors ( also called patterns ) and the knowledge of the probability distributions , what is the best estimate @xmath1 one can construct to approximate @xmath6 ?",
    "the answer , cast in the framework of bayesian inference , depends on whether @xmath1 is allowed to have continuous components or , conversely , is required to be a binary vector .",
    "we also address the problem of whether these upper bounds can be simply attained , by first obtaining a continuous vector via minimization of a potential and then transforming its components .",
    "the results of the replica calculation for this problem are briefly reviewed in section  [ general ] .",
    "section  [ gibbs ] discusses the special case of gibbs learning , for which simulations have been performed . in section  [ bayes ]",
    "we review the reasoning leading to the bayesian bound in the continuous as well as the binary space , with simulations compared to the theoretical results .",
    "a simple strategy which attempts to saturate these upper bounds is studied in section  [ transform ] , while our conclusions are presented in section  [ conclusions ] .",
    "in order to obtain a good candidate vector @xmath1 , we construct a cost function of the form @xmath21 , where @xmath22 . in the gaussian scenario , the _ potential _",
    "@xmath23 has a quadratic form ,    @xmath24    learning is defined as sampling @xmath1 from the boltzmann distribution with temperature @xmath25    @xmath26    where @xmath27 is the normalization constant and the measure @xmath28 is used to enforce either a binary ( @xmath29 ) or a spherical ( @xmath30 ) constraint on @xmath1 . while the spherical case has been dealt with in  @xcite , we focus now on the case where the candidate vectors have binary components",
    ". the thermodynamic properties of such a system can be read from the free energy @xmath31 . in the thermodynamic limit @xmath32 ,",
    "@xmath33 becomes self - averaging , @xmath34 , and can be calculated via the replica trick .",
    "this by now standard calculation will not be reproduced here , only the results are quoted : for a replica symmetric _ ansatz _ , the quadratic forms of the gaussian scenario allow the calculations to be performed exactly , and the free energy reads    @xmath35 \\nonumber \\\\   & \\ &   + \\alpha \\left [ \\frac { c [ q - r^{2}(a - b^{2 } ) ] - 2brd - d^{2}\\beta(1-q ) } { 2[1+c\\beta(1-q ) ] } \\right ] \\bigg\\}\\ ; .\\end{aligned}\\ ] ]    the above order parameters are also self - averaging quantities and can be interpreted as follows : @xmath36 measures the alignment between a typical ( binary ) sample of eq .",
    "[ pjgivend ] and the preferential direction , its absolute value as a function of @xmath37 being hereafter used to account for the performance of a given potential @xmath23 ; @xmath38 is the mutual overlap between two different samples of eq .",
    "[ pjgivend ] , while @xmath39 and @xmath40 are the associated conjugate parameters .",
    "the equilibrium values of the four variables are determined by the solution of the saddle point equations which arise from the extremum operator in eq .",
    "[ freebin ] .",
    "gibbs learning arises as a particular but very important case in this general framework . in order to define it properly",
    ", we first recall the bayes inversion formula    @xmath41    the _ posterior distribution _ @xmath42 expresses the knowledge about @xmath6 which is gained after the presentation of the data . replacing @xmath6 with @xmath1 in this formula gives the probability density that @xmath1 is the `` true '' direction @xmath6 , given the data vectors .",
    "note that the binary prior in eq .",
    "[ posterior ] constrains the acceptable candidates @xmath1 to the corners of the @xmath3-hypercube , i.e. @xmath43 . making use of eq .",
    "[ pofb ] , one rewrites    @xmath44    apart from a normalization constant .",
    "gibbs learning is defined as sampling from distribution  [ gibbslearning ] .    a comparison with eq .",
    "[ pjgivend ] shows that the thermodynamics of such a process is obtained by setting @xmath45  @xcite . upon substitution of @xmath46 , @xmath47 and @xmath48 in eq .",
    "[ freebin ] , one finds that the extremum of the corresponding free - energy is reached for @xmath49 and @xmath50 , where the subscript @xmath51 will hereafter be used to denote results from gibbs learning .",
    "the equalities reflect the symmetric role played by @xmath1 and @xmath6 in gibbs learning , a property which has been previously noted in several publications ( see e.g.  @xcite and  @xcite , among others ) .",
    "the four original saddle point equations are then effectively reduced to a single one :    @xmath52    where    @xmath53    is a function coming from the entropic term of the free energy , while    @xmath54\\ ; .\\ ] ]    the solution of eq .",
    "[ sprg ] also determines the value of the conjugate parameter @xmath55 :    @xmath56    in order to check that the replica symmetric _ ansatz _ is correct , we also study the entropy @xmath57 $ ] , which for gibbs learning reads    @xmath58 \\ ; .\\end{aligned}\\ ] ]    on physical grounds , this quantity should always remain positive . additionally , by relating @xmath59 with the mutual information @xmath60 per degree of freedom between the data @xmath61 and the preferential direction @xmath6 , herschkowitz and nadal  @xcite",
    "show that it can not decrease faster than linearly with @xmath37 . for the gaussian scenario ,",
    "the inequality reads @xmath62 $ ] .    before we proceed to study in detail the solution of eq .",
    "[ sprg ] , we turn to the analysis of the asymptotic behavior of the system .",
    "the asymptotics of the solution of eq .",
    "[ sprg ] can be immediately inferred by carrying out expansions of @xmath63 and @xmath64 . in the vicinity of @xmath65 , if we assume a smooth behavior for @xmath66 , the predictions for the gaussian scenario are :    @xmath67    where    @xmath68    we see that in the so called _ biased case _",
    "@xmath69 , it is much easier to learn .",
    "the _ unbiased case _ @xmath70 presents much more difficulties for information about vector @xmath6 to be extracted , due to the intrinsic symmetry @xmath71 . in this case , _ retarded learning _ occurs  @xcite , meaning that a non - zero macroscopic overlap @xmath72 will be obtained only after a critical number of examples @xmath73 is presented . for @xmath74 ,",
    "the entropy saturates its linear bound exactly  @xcite .",
    "this second order phase transition is identical to the one obtained in the spherical case  @xcite , revealing that the binary nature of the preferential direction plays no role in the poor performance regime .    in the limit @xmath75 ,",
    "on the other hand , the differences with respect to the spherical case become pronounced : @xmath72 approaches 1 exponentially ,    @xmath76\\ ; .\\ ] ]    as opposed to the power law observed for the spherical case .",
    "[ gl : asy : largealpha ] also implies an exponential decay to the entropy , @xmath77 .",
    "these qualitative asymptotic results can be shown to hold for general distributions @xmath78  @xcite . in the following ,",
    "we explore the gaussian scenario in more detail , studying the behavior of @xmath66 away from the asymptotic regimes .",
    "the first case to be addressed is @xmath79 with @xmath69 .",
    "the non - zero bias makes sure learning starts off as soon as @xmath80 , while @xmath79 eliminates the dependence of @xmath55 on @xmath72 ( see eqs .",
    "[ gl : gauss : sp : sp ] and  [ sprghat ] ) , much simplifying the saddle point equations , which can be solved exactly .",
    "the behavior of @xmath72 is seen to be simply determined by the rescaled variable    @xmath81    namely @xmath82 .",
    "this function can be seen in fig .",
    "[ figrrhat ] .",
    "it shows a linear increase for small @xmath83 and an exponential behavior for @xmath84 .",
    "the entropy saturates its linear bound only in the limit @xmath85 , approaching zero exponentially when @xmath84 but remaining otherwise strictly positive .",
    "( left axis ) as a function of @xmath83 for @xmath79 ( see eq .",
    "[ gl : gauss : bias : alphaprime ] ) : theory ( solid line ) and simulations with @xmath86 ( symbols ; error bars represent one standard deviation , see text for details ) .",
    "the dashed line represents the entropy ( right axis ) while the dotted line shows the linear bound ( right axis ) . ]    , @xmath87 and @xmath88 .",
    "histograms of @xmath89 ( thick lines ) and @xmath72 ( thin lines ) for @xmath86 ( dashed ) and @xmath90 ( solid ) ; the vertical line is the theoretical prediction .",
    "the upper inset shows the metropolis @xmath72-dynamics for two pattern sets ( same legend , see text for details ) .",
    "the lower inset presents the variance of the distribution for @xmath72 ( symbols ) as a function of @xmath91 for @xmath92 and @xmath93 ; the dotted line is a linear fit of the three leftmost points . ]    note that @xmath79 means that @xmath94 has unit variance .",
    "the patterns can thus be pictured as being distributed in an @xmath3-dimensional spherically symmetric cloud , whose displacement @xmath15 from the origin conveys the information about @xmath6 .",
    "[ [ simulations ] ] simulations + + + + + + + + + + +    binary disordered systems are known to be very hard to simulate due to the existence of very many local minima . a noisy dynamics with unity temperature and general cost function @xmath95 will typically get stuck in one of these minima , preventing a proper sampling of the posterior distribution  [ gibbslearning ] in an acceptable time .",
    "the gaussian scenario with @xmath79 provides an exception to this rule , allowing gibbs learning to be very easily implemented with a simple metropolis algorithm  @xcite . since @xmath79 implies a linear function @xmath96 , the changes in energy can be very quickly calculated because it depends only on @xmath97 .",
    "[ figrrhat ] shows the results for simulations with @xmath86 ( the smallest system size simulated ) and two values of @xmath15 , checking the relevance of the variable @xmath83 . for each pattern set @xmath61 ,",
    "10 samples of @xmath72 and @xmath89 were measured , after a random initialization of the system and a warming up of the dynamics ( see further details below ) .",
    "the whole procedure was repeated for 1000 pattern sets and the standard deviation was calculated over these 10000 samples .",
    "the measurement of @xmath89 during simulations is another tool to check both the property @xmath49 and the correctness of the rs _",
    "ansatz_. fig .",
    "[ figfancyplot ] focus on the second simulated point of fig .",
    "[ figrrhat ] ( @xmath98 ) .",
    "it shows histograms for both @xmath72 and @xmath89 ( measured between pairs of consecutive samples ) which are virtually indistinguishable on the scale of the figure , with a mean value in excellent agreement with the theoretical prediction .",
    "the upper inset gives a glimpse of the metropolis dynamics : the system is initialized randomly at @xmath99 and evolves up to @xmath100 monte carlo steps per site ( mcs / site ) , at which moment a different pattern set is drawn .",
    "the system reaches thermal equilibrium after @xmath101 mcs / site , which motivated the choice of safely waiting 100 mcs / site during the simulations before any measurement was made .",
    "the system was reinitialized after every measurement of the overlaps .",
    "note that some pattern sets yield time - averaged values of @xmath72 which deviate from theory ( notably the first one for @xmath86 and the second one for @xmath90 ) and only a second average over the pattern sets gives the right results .",
    "this reflects the property of self - averaging , which only holds in the thermodynamic limit ( note that deviations from theory are smaller for larger @xmath3 ) .",
    "the lower inset shows the typical scaling with @xmath102 of the width of the distribution of overlaps .",
    "( thick lines , left axis ) and @xmath59 ( thin lines , right axis ) as functions of @xmath37 for @xmath103 and @xmath70 .",
    "the thin dotted lines correspond to the linear bound , which is exactly saturated up to the second order phase transition .",
    "a small bias @xmath104 ( with @xmath105 ) breaks the symmetry and destroys the second order phase transition ( thick dotted line , left axis , entropy not shown ) . ]     of the saddle point equations  [ sprg ] and  [ gl : gauss : sp : sp ] as a function of @xmath37 for @xmath70 and three values of @xmath106 .",
    "@xmath59 is plotted with dashed lines and the thermodynamically stable solutions are plotted with thick lines . @xmath107",
    "( upper inset ) : @xmath72 ( left axis ) vs. @xmath37 ( bottom axis ) and @xmath59 ( right axis ) vs. @xmath37 ( top axis  note the different @xmath37 scale , which zooms in the first order phase transition ) ; @xmath108 ( lower inset ) : @xmath72 ( left axis ) and @xmath59 ( right axis ) vs. @xmath37 . ]    ) .",
    "second order phase transitions occur at @xmath109 ( solid line ) , while first order phase transitions take place at @xmath110 ( dashed line ) .",
    "see text for details . ]",
    "when @xmath70 , retarded learning is expected to occur , according to eq .",
    "[ gl : asy : smallalpha2 ] .",
    "[ figrhoalphaa06 ] shows the solution of the @xmath72 saddle point equation for two values of @xmath106 , namely 0.6 and @xmath111 . in both cases ,",
    "a second order phase transition occurs at the critical value @xmath112 predicted by eq .",
    "[ gl : gauss : asy : constants ] and the entropy saturates exactly the linear bound before the phase transition . based on the relation between @xmath59 and @xmath60  @xcite ,",
    "the retarded learning phase transition can be interpreted as follows : for @xmath74 , the system extracts maximal information from each pattern but is nonetheless unable to obtain a non - zero alignment @xmath72 with the preferential direction @xmath6 . only at @xmath113",
    "does @xmath72 depart from zero , which on its turn immediately gives an increasing degree of redundancy ( measured by the deviation of @xmath59 from its linear bound ) to the patterns coming thereafter .",
    "[ figrhoalphaa06 ] also shows the effect of a small bias @xmath104 in an otherwise symmetric distribution : for sufficiently large @xmath37 ( say , @xmath114 ) , the effect is negligible , but for small @xmath37 the broken symmetry destroys the second order phase transition .",
    "it is interesting to note in fig .",
    "[ figrhoalphaa06 ] that even though the phase transition for @xmath115 and @xmath105 occurs at the same critical value , the overlap increases much slower in the former case than in the latter . recalling the definition of @xmath106 ( eqs .",
    "[ pofb]-[ugauss ] ) , this means that prolate gaussian distributions ( @xmath3-dimensional `` cigars ''  @xcite ) convey less information about the preferential direction than oblate distributions ( @xmath3-dimensional `` pancakes ''  @xcite ) for the same absolute value of @xmath106 .    however , the second order phase transition at @xmath116 is not the only interesting phenomenon for this model .",
    "first order phase transitions are also possible , depending on the value of @xmath106 .",
    "they can occur in two situations : either for @xmath117 , in which case two consecutive phase transitions take place during learning ( a second - order one followed by a first - order one ) , or @xmath118 , in which case the asymptotic result eq .",
    "[ gl : asy : smallalpha2 ] is overridden .",
    "the first order phase transition appears when there is more than one solution to the saddle point equation . in such cases",
    "the solution with minimal free energy has maximal probability of occurrence , being thus the thermodynamically stable state .",
    "such first order phase transitions have been found for the spherical case with a two - peaked distribution  @xcite , but not with the gaussian scenario  @xcite , which shows that they are due to the discrete nature of the search space in this case .",
    "an overview of this phenomenology is presented in fig .",
    "[ figbequalzero ] .",
    "it shows the three typical behaviors that occur for @xmath70 . for comparison , the case @xmath105 plotted in fig .",
    "[ figbequalzero ] is shown again , as an example of a parameter region where there is only a second order phase transition ( at @xmath119 ) .",
    "for @xmath107 the second order phase transition at @xmath120 is followed by a first order phase transition at @xmath121 ( upper inset , lower axis ) , while for @xmath108 only a first order phase transition takes place at @xmath122 , overriding the second order phase transition at @xmath123 ( lower inset ) which was predicted on asymptotics and smoothness grounds .",
    "note that none of these first order phase transitions can be predicted by the asymptotic expansion , eq .",
    "[ gl : asy : smallalpha2 ] .",
    "it is also interesting to observe that some solutions of the saddle point equation may violate the linear bound and/or the positivity of the entropy ( notably @xmath108 in fig .",
    "[ figbequalzero ] ) .",
    "however , it turns out that these branches are always thermodynamically unstable , while the stable solutions satisfy all the requirements .    the whole phase diagram for @xmath70",
    "is shown in fig .",
    "[ figphasedia ] . for @xmath124 , a first order phase transition takes place at the line @xmath125 , after the second order one has already occurred . for increasing @xmath106",
    ", @xmath125 gets closer and closer to @xmath126 , until there is finally a collapse at @xmath127 . for larger values of @xmath106 , only the first order phase transition occurs .",
    "we now switch to the following question : given the @xmath20 data vectors and the prior information about @xmath6 , what is the _ best _ performance @xmath128 one could possibly attain with a vector @xmath1 ?",
    "watkin and nadal  @xcite answered this question in a bayesian framework by defining _ optimal learning _",
    "( see also  @xcite ) .",
    "we briefly review their reasoning here and extend it to take into account the binary nature of the vectors .",
    "we define the quality measure @xmath129 , which quantifies how well @xmath6 is approximated by any candidate vector @xmath1 satisfying @xmath130 . since @xmath6 is unknown , @xmath131 is formally inaccessible .",
    "but one can take its average with respect to the posterior distribution  [ posterior ] , leading to @xmath132 .",
    "@xmath133 is then a formally accessible _ bona fide _ quantity which can be used to measure the performance of @xmath1 .",
    "optimal learning is _ defined _ as constructing a vector @xmath134 which maximizes @xmath133 .",
    "the linearity of @xmath131 in @xmath1 immediately implies    @xmath135    leading on its turn to    @xmath136    where the @xmath137 factor guarantees the proper normalization of @xmath134 .",
    "this is the so - called bayesian vector , which is the center of mass of the gibbs ensemble . in the thermodynamic limit",
    ", its performance @xmath138 is shown to be simply related to that of gibbs learning  @xcite : @xmath139 .      up to now",
    "the reasoning is fairly general .",
    "the whole procedure can actually be carried out without explicitly mentioning what the prior distribution @xmath140 is . for clarity , in the following @xmath134",
    "will specifically denote the bayesian vector for a _ binary _ prior .",
    "note , however , that despite being the center of mass of the ensemble of _ binary _ vectors sampled from the posterior distribution , @xmath134 has real - valued components  @xcite , in general .",
    "one would therefore like to address the next question : what is the _",
    "best binary _ vector one can construct ?",
    "in other words , what is the binary vector  inferable from the data  that outperforms  on average  any other binary vector in approximating @xmath6 ?",
    "the answer is again straightforward  @xcite : the vector @xmath141 which maximizes @xmath133 _ among the binary vectors _ is simply obtained by the clipping prescription , namely @xmath142_{j } = \\mbox{sign}([\\bm{j}_{b}]_{j})$ ] , @xmath9 or , in shorthand notation ,    @xmath143    this can be easily checked by noting that the quantity to be maximized ( the r.h.s . of eq .",
    "[ qtilde ] ) is proportional to @xmath144_{j}$ ] . in what follows ,",
    "@xmath141 is called the _",
    "best binary _ vector .",
    "summarizing , if @xmath6 is known to be binary , @xmath134 is the best estimator one can provide .",
    "but if the estimator is required to be binary as well , then @xmath141 is the optimal choice .",
    "the proof that @xmath134 and @xmath141 are optimal estimators in their respective spaces , is relatively simple  @xcite .",
    "what we show below is that maximal @xmath133 implies maximal alignment @xmath128 with @xmath6 , in the thermodynamic limit . for the best binary ,",
    "one departs from the inequality @xmath145 , @xmath146 and takes the average with respect to the data distribution :    @xmath147 = \\nonumber \\\\ & = & \\int d\\bm{b}\\,p_{b}(\\bm{b } ) \\int dd\\ , p(d|\\bm{b } ) \\left[\\frac{\\bm{b}\\cdot\\bm{j}_{bb}}{n } - \\frac{\\bm{b}\\cdot\\bm{j}}{n}\\right ] \\geq 0\\ ; .\\end{aligned}\\ ] ]    if we now assume that @xmath148 and @xmath149 are self - averaging , the average over the data can be bypassed and one obtains    @xmath150 \\stackrel{n\\to\\infty}{\\geq } 0\\ ; , \\ ] ]    finally , one notices that @xmath151 is a uniform prior , making no distinction between any particular binary vector ( this is reflected , for instance , in the free energy  [ freebin ] being independent of the particular choice of @xmath6 ) : this allows the last average in eq .",
    "[ demonstration2 ] to be bypassed as well , leading to the stronger upper bound @xmath152 .",
    "the performance @xmath153 of the best binary vector can be explicitly calculated by extending previously obtained results for the clipping prescription  @xcite . in  @xcite ,",
    "schietse _ et al .",
    "_ study the effect of a general transformation @xmath154 on the components of a properly normalized continuous vector @xmath1 satisfying @xmath155 .",
    "if @xmath6 is binary , @xmath156 is odd and @xmath157 is self - averaging , then the following relation follows :    @xmath158^{1/2}}\\ ; , \\ ] ]    where the variable @xmath159 is expected to be distributed independently of the index , because of the permutation symmetry among the axes .    we are left then with the problem of calculating @xmath160 , after which eq .",
    "[ rtilde ] can be applied for @xmath161 , providing @xmath153 as a function of @xmath72 .",
    "if @xmath1 is uniformly distributed on the cone @xmath162 , then @xmath160 is just a gaussian with mean @xmath128 and variance @xmath163  @xcite .",
    "however this can hardly be expected to hold for the bayesian vector , since it is a sum of ising vectors .",
    "one would naively expect @xmath134 to be closer to the corners of the @xmath3-hypercube instead . to obtain the relevant @xmath164_{1})$ ] , we calculate the @xmath165-th quenched moment of @xmath166_{1}$ ] ,    @xmath167_{1})^{m}\\right>_{d } =   \\frac{1}{r_{g}^{m/2}}\\left <   z^{-m}\\left ( \\int d\\bm{j}\\,p_{b}(\\bm{j})\\ , e^{-\\beta\\sum_{\\mu}^{\\alpha   n}u(\\lambda_{\\mu})}j_{1 } \\right)^{m } \\right>_{d}\\ ; .\\ ] ]    the above expression can be evaluated again with the use of the replica trick .",
    "the replica symmetric result is    @xmath168_{1})^{m}\\right>_{d } = \\frac{1}{(r_{g}(\\alpha))^{m/2}}\\int { \\cal d}z\\ , \\left [ \\tanh\\left ( z\\sqrt{\\hat{r}_{g}(\\alpha)}+\\hat{r}_{g}(\\alpha)b_{1 } \\right ) \\right]^{m}\\ ; , \\ ] ]    or , equivalently ,    @xmath169^{m}\\ ; , \\ ] ]    where the values of @xmath170 and @xmath171 should be taken at the solution of the saddle point equations  [ sprg ] and  [ sprghat ] for gibbs learning .",
    "notice that the passage from eq .",
    "[ mthmoment : final1 ] to eq .",
    "[ mthmoment : final2 ] is valid _ only _ if @xmath6 is binary .",
    "from eq .",
    "[ mthmoment : final2 ] one immediately rewrites the probability distribution @xmath172 , by identifying a change of stochastic variables @xmath173 , with @xmath174 normally distributed :    @xmath175^{2}\\ ; .\\ ] ]    note that , since @xmath72 and @xmath55 are simply related to each other ( eqs .  [ sprg ] and  [ sprghat ] ) , @xmath172 can always be parametrized in function of @xmath72 only . in fig .",
    "[ fignewpy ] , the probability distribution of @xmath176 is plotted for different values of @xmath72 , illustrating the fact that @xmath177 .     for different values of @xmath72 . ]     and @xmath87 .",
    "probability distribution of @xmath178_{1}$ ] for @xmath179 , @xmath180 ( left plot ) and @xmath181 ( right plot ) or , correspondingly , @xmath182 , @xmath183 and @xmath184 .",
    "solid line : simulations .",
    "dashed line : theory ( eq .  [ px ] ) . ]",
    "[ px ] should be compared to the gaussian distribution obtained in  @xcite .",
    "it shows that @xmath134 is indeed closer to the corners of the @xmath3-hypercube , optimally incorporating the information that @xmath6 is binary .",
    "we have run simulations for @xmath79 and @xmath87 as described in section  [ biased ] . for a system size @xmath185 , the center of mass",
    "was constructed with @xmath186 samplers , being normalized afterwards .",
    "each component of @xmath6 and @xmath134 was used to measure @xmath187 , the procedure being repeated 100 times for each of the 100 pattern sets .",
    "a comparison between the resulting histogram and the theoretical prediction can be seen in fig .",
    "[ figsimpx ] .",
    "the good agreement shows that eq .",
    "[ px ] correctly describes the statistical properties of the bayesian vector .",
    "we can finally proceed to calculate the performance @xmath153 of the best binary vector . making use of eqs .",
    "[ rtilde ] and  [ px ] , we make a change of variables to obtain    @xmath188    where @xmath189 . making use of the relation between @xmath72 and @xmath55 , we finally write    @xmath190    where @xmath191 is the inverse of @xmath63 .",
    "@xmath153 , @xmath192 and @xmath193 : comparison between simulations and theory , for different values of @xmath37 .",
    "error bars represent one standard deviation .",
    "the diagonal is plotted for comparison . ]",
    "histograms of the overlaps @xmath72 , @xmath192 , @xmath153 and @xmath193 for @xmath194 .",
    "the vertical lines show the theoretical predictions . ]",
    "[ jbclip ] expresses an upper bound for binary candidate vectors @xmath1 in approximating @xmath6 , satisfying two obvious inequalities : @xmath195 .",
    "the asymptotic behavior of @xmath153 can always be written in terms of @xmath72 : in the poor performance regime ( @xmath196 ) , one recovers previous results for clipping a spherical vector  @xcite , @xmath197 , while in the large @xmath37 regime ( @xmath196 ) a faster exponential decay is achieved than with gibbs learning : @xmath198  @xcite .    as a spin - off of the calculation , we have also obtained the overlap between the bayesian vector and its clipped counterpart , @xmath199_{j}|$ ] .",
    "this quantity can be easily computed ,    @xmath200    if one notices the counterintuitive identity @xmath201 , @xmath202 , which is proved in the appendix .",
    "the simple result @xmath203 immediately implies the equality @xmath204 , for which we still have not found a deeper interpretation .    the curves",
    "@xmath153 , @xmath192 and @xmath193 as functions of @xmath72 are plotted on fig .  [ figoverlapssim ] , together with results for simulations with the same parameters as those of fig .",
    "[ figsimpx ] .",
    "the data is in excellent agreement with the theoretical results , errors generally remaining below the margin of one standard deviation .",
    "note that @xmath205 , with equality holding only for @xmath196 .",
    "this result is another confirmation of the picture that @xmath134 lies closer to the binary vectors , since @xmath206 is the overlap between a continuous vector isotropically sampled from the @xmath3-hypersphere and its clipped counterpart .",
    "[ fighistoverlapsalpha117 ] zooms in the fourth column of points of fig .",
    "[ figoverlapssim ] ( @xmath207 ) , showing the histograms of the overlaps .",
    "one observes that the distribution of @xmath193 is much sharper than the other ones , while the statistics of @xmath72 is better because it has @xmath186 times more samples .",
    "since sampling the binary gibbsian vectors is usually a very difficult task , the construction of the bayesian vector according to the center of mass recipe is not always possible , in practice .",
    "alternative methods should therefore be developed for approximating the @xmath192 and @xmath153 performances .",
    "one such method is the technique of transforming the components of a previously obtained _ spherical _ vector , as described in section  [ persim ] . in the following , we first derive general results ( for any function @xmath95 ) and subsequently look at the gaussian scenario in detail .",
    "a natural choice for the vector to be transformed is @xmath208 , which can be obtained by minimizing  in the @xmath3-hypersphere  an optimally constructed cost function  @xcite @xmath209 .",
    "it attains the bayes - optimal performance @xmath210 _ for the spherical case _ , which satisfies    @xmath211    where @xmath212 .",
    "note that @xmath208 saturates the performance of the center of mass of the gibbs ensemble _ for a spherical prior_. eq .",
    "[ ropts ] should be compared to the performance of @xmath134 , which obeys    @xmath213    while @xmath214 for small @xmath37 , the differences between @xmath134 and @xmath208 are clearly manifested in the asymptotic behavior for large @xmath37 , with @xmath215 approaching unity with a power law  @xcite instead of the exponential decay of eq .",
    "[ gl : asy : largealpha ] .     for two choices of parameters in the gaussian scenario : @xmath216 with @xmath217 ( upper curves ) and @xmath218 with @xmath219 ( lower curves ) . the upper bounds @xmath192 ( solid ) and @xmath153 ( dashed ) are depicted with thick lines , while the approximations @xmath220 ( solid ) and @xmath221 ( dashed ) are plotted with thin lines . ]     ( thick ) and @xmath222 ( thin ) according to eqs .",
    "[ px ] and eq .",
    "[ pxstar ] , respectively .",
    "the values @xmath223 ( solid ) and @xmath224 ( dashed ) refer to the gaussian scenario with @xmath225 and @xmath104 ( see fig .  [",
    "fig : compareopttransfspherical ] ) . ]",
    "we would like to depart from @xmath208 and obtain approximations to both @xmath141 and @xmath134 .",
    "the first one is obtained by clipping , @xmath226 .",
    "the second one relies on an optimal transformation  @xcite @xmath227 , which maximizes the transformed overlap becomes @xmath228 , that is , no improvement is possible . ] .",
    "the vector obtained by such a transformation on @xmath208 is denoted by @xmath229 , thus @xmath230_{j } = \\phi^{*}([\\bm{j}_{opt}^s]_{j})/r_{*}^{s}$ ] , @xmath9 , where @xmath231 .    since @xmath208 contains no information about the binary nature of @xmath6 , the results of schietse _ et al .",
    "_ can be directly applied to render @xmath232 and @xmath220 . in this case , @xmath160 is gaussian and one obtains  @xcite    @xmath233    we would like to compare eqs .",
    "[ jbclip ] and  [ rb ] with eqs .",
    "[ compareropts ] and  [ comparerstars ] , respectively .",
    "despite their resemblance in form , one notices that the former should be solved , while the latter just map the solution of eq .",
    "[ ropts ] . in order to compare the equations",
    ", one should first note that @xmath234 , @xmath235 . since @xmath236 , in general @xmath237 .",
    "this result in turn immediately implies the inequalities    @xmath238    _ with equality holding for both equations in the asymptotic limits @xmath75 and @xmath196_. this general behavior is confirmed in fig .",
    "[ fig : compareopttransfspherical ] , which shows the results for the gaussian scenario in the two relevant cases : zero and non - zero bias .    another available measure of the success of the optimal transformation @xmath239 in rendering a good approximation for @xmath134 , is the probability distribution @xmath222 , where @xmath240 . in order to obtain @xmath222 ,",
    "one just has to recall that @xmath160 is gaussian with mean @xmath241 and variance @xmath242 .",
    "the optimal transformation is then @xmath243 , and can be regarded as an attempt to attach some structure to the distribution of the transformed @xmath244 . with a simple change of variables , @xmath222",
    "is readily seen to be    @xmath245^{2 } \\right\\}\\ ; .\\ ] ]    a comparison with eq .",
    "[ px ] shows that the two equations are very similar , but not identical . some similarity in shape",
    "should indeed be expected , mainly because @xmath222 , just like @xmath172 , _ must be _ such that @xmath246 , in order to consistently prevent any further improvement by a similar transformation .",
    "one can verify in fig .",
    "[ fig : pxvspxprime ] that the resemblance between the probability distributions is closely associated with the success of @xmath220 in saturating the upper bound @xmath192 .",
    "the curves correspond to the gaussian scenario with @xmath225 and @xmath104 for two values of @xmath37 ( one can thus refer to the upper solid curves of fig .  [",
    "fig : compareopttransfspherical ] ) .",
    "note that for @xmath223 , the difference between @xmath192 and @xmath220 is very small in fig .",
    "[ fig : compareopttransfspherical ] , which is reflected in the solid curves of fig .",
    "[ fig : pxvspxprime ] being very close to each other .",
    "accordingly , the dashed curves in fig .  [ fig : pxvspxprime ]",
    "get further apart for @xmath247 as the mismatch between the overlaps increase in fig .",
    "[ fig : compareopttransfspherical ] .",
    "the simple biased case with @xmath79 and @xmath69 provides an interesting exception to the performances of @xmath229 and @xmath248 . the fact that @xmath249 is linear implies @xmath250 , as can be readily verified in eq .",
    "[ gl : gauss : sp : sp ] .",
    "this , on the other hand , implies the equalities in eqs .",
    "[ rcliprbb ] and  [ rstarsrb ] , that is ,    @xmath251    therefore the strategy described in the previous section is successful in attaining the opper bounds of section  [ bayes ] , and not only asymptotically . it should be noted that for a linear @xmath95 , the vector @xmath252 can be simply constructed with the hebbian rule , @xmath253 , @xmath254 .",
    "therefore the best binary performance is attainable by the clipped hebbian vector @xmath248 , in this case .",
    "the second equality , however , seems to us more remarkable , because it stablishes a result which we could not find elsewhere in the literature : _ the optimal transformation manages to completely incorporate the information about the binary nature of @xmath6 , leading to the bayes - optimal performance @xmath192 without the need of explicitly constructing the center of mass of the gibbs ensemble_. in other words , the technique of non - linearly transforming the components of the vectors , introduced in  @xcite and extended in  @xcite , is able to give a definitive answer to the problem it aims to solve .",
    "we have presented results on learning a binary preferential direction from disordered data .",
    "constraining the candidate vectors to the binary space as well , we first showed that gibbs learning presents not only an exponential asymptotic decay and second order `` retarded learning '' phase transitions , but also first order phase transitions for a simple gaussian scenario .    on the question of what is the optimal estimator , given the data and the knowledge that the preferential direction is binary , we have shown that the answer depends on which space the estimator @xmath1 is allowed to lie in .",
    "the best _ continuous _ estimator is the bayesian vector , which is the center of mass of _ binary _ gibbsian vectors . the best _ binary _ vector is obtained by clipping the bayesian vector .",
    "we have calculated its properties in detail , providing an upper bound to the performance of binary vectors .",
    "finally , we have also studied one possible way of constructing approximations to these two optimal estimators . by transforming the components of a previously obtained continuous vector ,",
    "we show that the upper bounds can not be saturated , in general .",
    "exceptions to this rule are the asymptotic limits ( both @xmath196 and @xmath75 ) and the special case of a linear function @xmath95 .",
    "interestingly , the linear case also seems to be the only one in which gibbs sampling can be performed without computational difficulties .",
    "we are therefore left with a situation where the approximations work perfectly only in the case where they are actually not needed .",
    "we believe this kind of result reinforces the need of investigating the connection between results in statistical mechanics and computational complexity theory .",
    "we would like to thank manfred opper for very fruitful discussions .",
    "we also acknowledge financial support from fwo vlaanderen and the belgian iuap program ( prime minister s office ) . this work was supported in part by the engineering research program of the office of basic energy sciences at the u.s .",
    "department of energy under grant no .",
    "de - fg03 - 86er13606 .",
    "in order to show that eq .  [ gamma ] is correct , one just has to show that the integral below vanishes identically , @xmath202 :    @xmath255 \\nonumber \\\\   & = & \\int{\\cal d}z\\ , \\mbox{sign}(az+a^{2})\\left[1   -\\tanh(az+a^{2})\\right ]   \\nonumber \\\\   & \\stackrel{z = y -a}{= } & \\int \\frac{dy}{\\sqrt{2\\pi}}e^{-(y - a)^{2}/2}\\ ; \\mbox{sign}(ay)\\ ; [ 1 - \\tanh(ay ) ] \\nonumber \\\\   & = & e^{-a^{2}/2}\\int { \\cal d}y\\ , \\mbox{sign}(ay)\\ , e^{ay } \\left [ \\frac{e^{-ay}}{\\cosh(ay ) } \\right ] \\nonumber \\\\   & = & 0\\ ; .\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> we study a model of unsupervised learning where the real - valued data vectors are isotropically distributed , except for a single symmetry breaking binary direction @xmath0 , onto which the projections have a gaussian distribution . </S>",
    "<S> we show that a candidate vector @xmath1 undergoing gibbs learning in this discrete space , approaches the perfect match @xmath2 exponentially . besides the second order `` retarded learning '' phase transition for unbiased distributions </S>",
    "<S> , we show that first order transitions can also occur . extending the known result that the center of mass of the gibbs ensemble has bayes - optimal performance , we show that taking the sign of the components of this vector ( clipping ) leads to the vector with optimal performance in the binary space . </S>",
    "<S> these upper bounds are shown generally not to be saturated with the technique of transforming the components of a special continuous vector , except in asymptotic limits and in a special linear case . </S>",
    "<S> simulations are presented which are in excellent agreement with the theoretical results .    </S>",
    "<S> pacs numbers : 64.60cn , 87.10sn , 02.50-r </S>"
  ]
}