{
  "article_text": [
    "the main goal of this paper is to design feasible error - correcting algorithms that approach ml decoding on the moderate lengths ranging from 100 to 1000 bits .",
    "the problem is practically important due to the void left on these lengths by the best algorithms known to date . in particular , exact ml decoding has huge decoding complexity even on the blocks of 100 bits . on the other hand , currently known  iterative ( message - passing )",
    "algorithms have been efficient only on the blocks of thousands of bits .",
    "to achieve near - ml performance with moderate complexity ,  we wish to use _ recursive _ techniques that repeatedly split an original  code into the shorter ones . for this reason",
    ", we consider reed - muller ( rm ) codes , which represent the most notable example of recursive constructions known to date .",
    "these codes - denoted below @xmath0 - have length @xmath1 and hamming distance @xmath2 they also admit a simple recursive structure based on the _ plotkin construction _ @xmath3 which splits the original rm code into the two shorter codes of length @xmath4 .",
    "this structure was efficiently used in recursive decoding algorithms of @xcite-@xcite , which derive the corrupted symbols of the shorter codes @xmath5 and @xmath6 from the received symbols .",
    "these recalculations are then repeated until the process reaches the repetition codes or full spaces , whereupon new information symbols can be retrieved by any powerful algorithm - say , ml decoding . as a result ,",
    "recursive algorithms achieve bounded distance decoding  with a low complexity order of @xmath7 , which improves upon the complexity of majority decoding @xcite .",
    "we also mention two list decoding algorithms of @xcite and @xcite , which substantially reduce the error rates at the expense of a higher complexity . in both algorithms , rm codes",
    "are represented as the generalized concatenated codes , which are repeatedly decomposed into the shorter blocks similarly to the plotkin construction . in all intermediate steps , the algorithm of @xcite tries to estimate the euclidean distance to the received vector and then retrieves the codewords with the smallest estimates . to do so",
    ", the algorithm chooses some number @xmath8 of codewords from both constituent codes @xmath5 and @xmath9 then the product list is constructed for the original code .",
    "these lists are recursively re - evaluated and updated in",
    "_ multiple runs_. the second technique @xcite is based on a novel sequential scheme that uses both the main stack and the complementary one .",
    "the idea here is to lower - bound the minimum distance between the received vector and the best code candidates that will be obtained in the _ future steps_. this `` look - ahead '' approach gives low error rates and reduces the decoding complexity of @xcite .",
    "recently , new recursive algorithms were considered in @xcite and @xcite . in particular , for long rm codes of fixed code rate @xmath10 recursive decoding of @xcite corrects most error patterns of weight ( @xmath11 instead of the former threshold of @xmath12 this is done without any increase in decoding complexity .",
    "however , the new decoding threshold is still inferior to that of a much more powerful ml decoding .    in the sequel",
    ", we advance the algorithm of @xcite , also applying list decoding techniques .",
    "this approach mostly follows @xcite and differs from the prior results in a few important aspects .",
    "first , we use exact posterior probabilities in our recursive recalculations instead of the distance approximations employed before .",
    "this allows us to design a tree - like recursive algorithm that can better sort out all plausible candidates in intermediate steps and avoid multiple decoding runs .",
    "second ,  we  shall see that the output error rate significantly varies for the different information symbols derived in the recursive process .",
    "therefore , we also consider subcodes of rm codes obtained by removing the least protected information bits .",
    "finally , decoding will be improved by applying a few permutations on code positions . as a result",
    ", we closely approach the performance of ml decoding on the lengths 256 and 512 , which was beyond the reach of the former techniques .",
    "the material is organized as follows .  in section 2",
    ", we briefly summarize some recursive properties of rm codes and their decoding procedures . in section 3",
    ", we describe our list decoding algorithm @xmath13 .",
    "finally , in section 4 we discuss the improvements obtained by eliminating the least protected information bits and using permutation techniques .",
    "the following description is detailed in @xcite .",
    "let any codeword @xmath14 of rm code @xmath0 be represented in the form @xmath15 where @xmath16 and @xmath17 .",
    "we say that @xmath14 is split onto two `` paths '' @xmath5 and @xmath6 . by splitting both paths ,  we obtain four paths that lead to rm codes of length @xmath18 and so on . in each step @xmath19 of our splitting , we assign the path value @xmath20 to a new @xmath6-component and @xmath21 to a new @xmath5-component .",
    "all paths end at the repetition codes @xmath22 or full spaces @xmath23 where @xmath24 thus , we can consider a specific binary path@xmath25 that leads from the origin @xmath0 to some left - end code @xmath22 .",
    "for any right - end node @xmath23 the same process gives a subpath @xmath26 of length @xmath27 @xmath28    a similar decomposition can be performed on the block @xmath29 of  @xmath30information bits  that encode the original vector @xmath31 in this way , any left - end path @xmath26 gives only one information bit associated with its end node @xmath32 any right - end path gives @xmath33 information bits associated with the end code @xmath34 we can also add an arbitrary binary suffix of length @xmath35 to the right - end paths , and obtain a one - to - one mapping between the extended paths @xmath26 and @xmath36 information bits @xmath37      let any binary symbol @xmath38 be mapped onto @xmath39 .",
    "then any codeword of rm code belongs to @xmath40 and has the form @xmath41 this codeword is transmitted over a memoryless channel @xmath42 the received block @xmath43 consists of the two halves @xmath44 and @xmath45 , which are the corrupted images of vectors @xmath5 and @xmath46 .",
    "the decoder first takes the symbols @xmath47 and @xmath48 for any position @xmath49  and finds the posterior probabilities of transmitted symbols @xmath50 and @xmath51 @xmath52 to simplify our notation , below we use the associated quantities @xmath53 note that @xmath54 is the _ difference _ between the two posterior probabilities @xmath55 and @xmath56 of @xmath57 and @xmath58 in position @xmath19 of the left half .",
    "similarly , @xmath59 is obtained on the right half .",
    "the following basic recursive algorithm is described in @xcite and section iv of @xcite in more detail",
    ".    * step 1 .",
    "* let @xmath60 be the posterior probability of any symbol @xmath61of  the codeword @xmath9 we find the corresponding quantity @xmath62 which is ( see formula ( 18 ) in @xcite ) @xmath63 symbols @xmath64 form the vector @xmath65 of length @xmath66 then we use some soft - decision decoder @xmath67 that gives a vector @xmath68 and its information block @xmath69    * step 2 . *",
    "now we assume that @xmath70step 1 gives _ correct _ _ vector _",
    "@xmath71 let @xmath72 be the posterior probability of a symbol @xmath73 then the corresponding quantity @xmath74 is ( see formula ( 19 ) in @xcite ) @xmath75 where @xmath76 the symbols @xmath77 form the vector @xmath78 of length @xmath66 we use some ( soft decision ) decoding algorithm @xmath79 to obtain a vector @xmath80 and its information block @xmath81 @xmath82    in a more general scheme @xmath83 , vectors @xmath65 and @xmath78 are not decoded but used as our new inputs @xmath84 these inputs are recalculated multiple times according to ( [ 1 ] ) and ( [ 2 ] ) . finally , we reach the end nodes @xmath22 and @xmath85 . here",
    "we perform maximum - likelihood ( ml ) decoding as follows .    at any node @xmath86 ,",
    "our input is a newly recalculated vector @xmath87 of length @xmath88 with the given differences @xmath89 between posterior probabilities of two symbols @xmath90 .",
    "rewriting definition ( [ pe ] ) , we assign the posterior probability @xmath91 to a symbol @xmath90 . in this way , we can find the posterior probability @xmath92 of any codeword @xmath93 , and choose the most probable codeword @xmath94 , where @xmath95 the decoded codeword  @xmath96 and the corresponding information block  @xmath97 are now obtained as follows ( here operations ( [ 1 ] ) and ( [ 2 ] ) are performed on vectors componentwise ) .",
    "@xmath98{l}\\text{algorithm } \\psi_{r}^{m}\\text { for an input vector } \\mathbf{y.}\\medskip\\\\ \\text{1 . if }",
    "0<r < m\\text { , execute the following.}\\medskip\\\\\\begin{array } [ c]{l}\\quad\\text{1.1.\\ calculate vector } \\mathbf{y}^{v}=\\mathbf{y}^{\\prime } \\mathbf{y}^{\\prime\\prime}\\text{.}\\smallskip\\text{\\smallskip}\\\\ \\quad\\text{decode } \\mathbf{y}^{v}\\text { into vector } \\mathbf{\\hat{v}=}\\psi_{\\text { } r-1}^{m-1}(\\mathbf{y}^{v}).\\smallskip\\text { } \\\\ \\quad\\text{pass } \\mathbf{\\hat{v}}\\text { and } \\hat{\\mathbf{a}}^{v}\\text { to step 1.2}\\medskip\\\\ \\quad\\text{1.2.\\ calculate vector } \\mathbf{y}^{u}=(\\mathbf{y}^{\\prime } + \\mathbf{\\hat{y}})/(1+\\mathbf{y}^{\\prime}\\mathbf{\\hat{y}})\\text{.\\smallskip } \\smallskip\\\\ \\quad\\text{decode } \\mathbf{y}^{u}\\text { into vector } \\mathbf{\\hat{u}=}\\psi _ { r}^{m-1}(\\mathbf{y}^{u}).\\smallskip\\ \\\\ \\quad\\text{output decoded components:}\\\\ \\quad\\hat{\\mathbf{a}}:=(\\hat{\\mathbf{a}}^{v}\\mid\\hat{\\mathbf{a}}^{u});\\quad\\mathbf{\\hat{c}}:=(\\mathbf{\\hat{u}}\\mid\\mathbf{\\hat{u}\\hat{v}}).\\medskip \\end{array } \\\\",
    "\\text{2 . if } r=0,\\text { use ml - decoding ( \\ref{ml2})\\ for \\ } \\left\\ { \\genfrac{}{}{0pt}{}{r}{0}\\right\\ }   .\\medskip\\\\ \\text{3 .",
    "if } r = m,\\text { use ml - decoding ( \\ref{ml2 } ) for } \\left\\ { \\genfrac{}{}{0pt}{}{r}{r}\\right\\ }   .\\smallskip \\end{array } $ } \\ ] ] note that this algorithm @xmath99 differs from the simplified algorithm @xmath100 of @xcite in three aspects .",
    "firstly , we use exact recalculations ( [ 2 ] ) instead of the former simplification@xmath101 secondly , we use ml decoding instead of the minimum distance decoding that chooses @xmath94 with the maximum inner product : @xmath102 thirdly , we employ a different rule and stop at the repetition codes @xmath103 instead of the biorthogonal codes used in @xcite .",
    "this last change will make it easier to use the list decoding described in the following section .",
    "finally , note that recalculations ( [ 1 ] ) require @xmath104 operations , while recalculations ( [ 2 ] ) can be done  in @xmath105 operations .",
    "therefore our decoding complexity satisfies recursion@xmath106 similarly to @xcite , this recursion gives decoding complexity @xmath107 thus , complexity @xmath108 has maximum order of @xmath109 which is twice the complexity @xmath110 of the algorithm @xmath100 of @xcite .",
    "to enhance algorithm @xmath83 , we shall use some lists of @xmath111 or fewer codewords obtained on any path @xmath26 .",
    "this algorithm  called @xmath112  increases the number of operations at most @xmath8 times and has the overall complexity order of @xmath113 given any integer parameter @xmath114 ,  we say that the list have size @xmath115 if decoding outputs either all available records or @xmath114 records , whichever is less@xmath116 this algorithm performs as follows .",
    "at any step @xmath117 of the algorithm @xmath118 , our input consists of @xmath119 records @xmath120 each record is formed by some information block @xmath121 its cost function @xmath122 and the corresponding input @xmath123 which is updated in the decoding process these three entries are defined below .",
    "_ starts at the root node @xmath124 here we set @xmath125 and take one record @xmath126 where @xmath87 is the input vector .",
    "decoding takes the first path ( denoted @xmath127 to the leftmost code @xmath128 and recalculates vector @xmath129 similarly to the algorithm @xmath130 however , now we take _ both _",
    "values @xmath131 of  the first information symbol and consider both codewords @xmath132 and @xmath133 of length @xmath134 in the repetition code @xmath135 .",
    "the posterior probabilities ( [ ml1 ] ) of these two vectors will also define the cost function of the new information block @xmath136 @xmath137 in our list decoding , we represent the two outcomes @xmath138 as the initial edges mapped with their cost functions @xmath139 then we proceed to the next code @xmath140 which corresponds to the subsequent path denoted @xmath141 given two different decoding results @xmath142our recursion ( [ 1 ] ) , ( [ 2 ] ) gives two different vectors @xmath129 arriving at this node .",
    "therefore , decoding is performed two times and gives the full tree of depth 2 .",
    "more generally , at any step @xmath143 , decoding is executed as follows .",
    "suppose that the first @xmath144 paths are already processed .",
    "this gives @xmath145 information blocks @xmath146 of length @xmath144 and the corresponding records @xmath147 each vector @xmath148 is then recalculated on the new path @xmath149 using formulas ( [ 1 ] ) and ( [ 2 ] ) , in the same way it was done in @xmath130 let this path end on some left - end code @xmath22 .",
    "decoding of the new information symbol @xmath150 yields @xmath151 extended blocks @xmath152 of depth @xmath153 marked by their cost functions@xmath154 step @xmath143 is completed by choosing @xmath119 blocks with the highest cost functions @xmath155    the decoding on the right - end nodes is similar .",
    "the only difference is that the full spaces @xmath156 include @xmath33 codewords defined by information blocks @xmath157 of length @xmath158 in this case , we can choose the two most probable vectors @xmath159 ( in essence , making bit - by - bit decisions ) and set @xmath160 in our cost calculations ( [ cost ] ) .",
    "another - more refined version of the algorithm - chooses four different vectors of the code @xmath156 whenever @xmath161 the best record is chosen at the last node @xmath162 more generally , the algorithm is executed as follows .",
    "@xmath98{l}\\text{algorithm } \\psi_{r}^{m}(l).\\text { input : } l^{\\ast}\\text { records}\\\\ \\text { } a=(\\mathbf{\\bar{a}},\\rho(\\mathbf{\\bar{a}}),\\mathbf{y(\\bar{a}})),\\text { counter } s=0.\\medskip\\\\ \\text{1 . if }",
    "0<r < m\\text { , for all vectors } \\mathbf{y(\\bar{a}}):\\medskip\\\\\\begin{array } [ c]{l}\\quad\\text{1.1 .",
    "\\quad\\text{perform decoding } \\psi_{\\text { } r-1}^{m-1}\\left (   \\mathbf{y(\\bar { a}})\\right )   .\\medskip\\\\ \\quad\\text{pass } l^{\\ast}\\text { new records } a\\text { to step 1.2}\\medskip\\\\ \\quad\\text{1.2.\\ set } \\mathbf{y(\\bar{a}}):=\\ds\\frac{\\mathbf{y}^{\\prime } \\mathbf{(\\bar{a}})+\\mathbf{\\hat{y}(\\bar{a}})}{1+\\mathbf{y}^{\\prime } \\mathbf{(\\bar{a}})\\mathbf{\\hat{y}(\\bar{a}})}\\text{.}\\medskip\\smallskip\\\\ \\quad\\text{perform decoding } \\psi_{r}^{m-1}\\left (   \\mathbf{y(\\bar{a}})\\right ) .\\smallskip\\ \\\\ \\quad\\text{output } l^{\\ast}\\text { new records } a\\text{.\\smallskip}\\smallskip \\end{array } \\\\ \\text{2 .",
    "if } r=0,\\text { take both values } a_{s}=0,1.\\\\ \\text{calculate costs ( \\ref{cost } ) for each } \\left (   \\mathbf{\\bar{a},}a_{s}\\right )   .\\smallskip\\\\ \\text{choose } l^{\\ast}\\text { best blocks } \\mathbf{\\bar{a}:=(\\bar{a},}a_{s}).\\\\ \\text{set } s:=s+1\\text { and return } l^{\\ast}\\text { records } a.\\medskip\\\\ \\text{3 . if } r = m,\\text { choose 4}^{\\ast}\\text { best blocks } a_{s}.\\\\ \\text{calculate costs ( \\ref{cost } ) for each } \\left (   \\mathbf{\\bar{a},}a_{s}\\right )   \\mathbf{.}\\smallskip\\\\ \\text{choose } l^{\\ast}\\text { best blocks } \\mathbf{\\bar{a}:=(\\bar{a},}a_{s}).\\\\ \\text{set } s:=s+|a_{s}|\\text { and return } l^{\\ast}\\text { records } a.\\medskip \\end{array } $ } \\ ] ]    _ discussion .",
    "_  consider the above algorithm on the awgn channel @xmath163 using the results of @xcite , it can be proven that on this channel , the @xmath6-component is decoded on the channel with the new noise power@xmath164 the first approximation is tight for very small @xmath165 ( though the channel is no longer gaussian ) , while the second one performs well on the `` bad '' channels with @xmath166 thus , the noise power always increases in the @xmath6-direction ; the more so the worse the original channel is .",
    "by contrast , the @xmath5-channel can be approximated by the smaller power @xmath167 .",
    "these observations also show that the first information symbol - which is obtained on the binary path @xmath168 - is protected the least , and then the decoding gradually improves on the subsequent paths .",
    "now we see that the algorithm @xmath118 with the list of size @xmath111 delays our decision on any information symbol by @xmath169 steps , making this decision better protected . in the particular case of a bad channel",
    ", it can be verified that the first symbol @xmath170 is now decoded when the noise power is reduced @xmath171 times .",
    "for this reason , this list decoding substantially reduces the output word error rates ( wer ) even for small size @xmath172    for @xmath173 the algorithm @xmath118 processes all the codewords of the first biorthogonal code @xmath174 and is similar to the algorithm @xmath100 of @xcite .  on the other  hand",
    ", algorithm @xmath118 updates all @xmath8 cost functions , while @xmath100 chooses one codeword on each end node .",
    "therefore @xmath118 can be considered as a generalization of @xmath100 that continuously updates decoding lists in all intermediate steps .",
    "the result is a more powerful decoding that comes along with a higher complexity .",
    "_ simulation results . _ below we present our simulation results for the awgn channels . here",
    "we also counted all the instances , when for a given output the decoded vector  was more probable than the transmitted one .",
    "obviously , these specific events also represent the errors of ml decoding .",
    "thus , the fraction of these events gives a lower bound on the ml decoding error probability .",
    "this lower bound is also depicted in the subsequent figures for all the codes tested .",
    "our simulation results show that for all rm codes of lengths 128 and 256 , the algorithm @xmath118 rapidly approaches ml performance as the list size @xmath8 grows .  for rm codes of length 128 and distance @xmath175 ,",
    "we summarize  these results in fig .",
    "[ fig : rm128 ] . for each rm code",
    ", we present _ tight _ lower bounds for the error probability of ml decoding . to measure the efficiency of the algorithm @xmath176",
    "we also exhibit the actual list size @xmath177 at which @xmath118 approaches the optimal ml decoding within a small margin of @xmath178 this performance loss @xmath179 is measured at the output word error rate ( wer ) @xmath180 however , we found little to no difference for all other wer tested in our simulation . in table 1 , we complement these lists sizes @xmath177 with the two other relevant parameters :    @xmath181 the signal - to - noise ratios ( snr per information bit ) at which algorithm @xmath118 gives the wer @xmath180",
    "@xmath181 the complexity estimates @xmath182 counted as the number of floating point operations .",
    "for which the algorithm @xmath118 performs withing @xmath183 db from these bounds.,width=336 ]    [ c]|c|c|c|c|rm code & @xmath184 & @xmath185 & @xmath186 + @xmath187 & 16 & 16 & 8 + @xmath188 & 21676 & 33618 & 18226 + @xmath189 & 3.47 & 3.71 & 4.85 +    table 1 .",
    "rm codes of length 128 : the list size @xmath190 decoding complexity , and the corresponding snr at which algorithm @xmath118 performs within @xmath183 db from ml decoding at wer @xmath191    for rm codes of length 256 , we skip most decoding results as these will be improved in the next section by the permutation techniques . in our single example in fig .",
    "[ fig : rm0308 ] , we present the results for the @xmath192 code @xmath193 this code gives the _ lowest _ rate of convergence to the ml decoding among all rm codes of length @xmath194 in other words , all other codes require the smaller lists to achieve the same performance loss @xmath195 this example and other simulation results show that the algorithm _ _ @xmath118 performs within 0.5 db from ml decoding on the lengths 128 and 256 using lists of small or moderate size .",
    ", @xmath196 ) rm code @xmath197 .",
    "wer for the algorithm @xmath118 with lists of size @xmath172,width=336 ]",
    "more detailed results also show that many codes of length @xmath198 require lists of large size @xmath199 to approach ml decoding within the small margin of 0.25 db . therefore for @xmath198",
    ", we also employ a different approach .",
    "namely , the decoding performance can be improved by eliminating those paths , where recursive decoding fails more often .",
    "here we use the results of @xcite , which show that the leftmost paths are the least protected .    recall that each left - end path @xmath26 corresponds to one information symbol . therefore ,",
    "decoding on these paths can be eliminated by _ setting the corresponding _",
    "_ information bits as zeros_. in this way , we employ the _ subcodes _ of the original code @xmath0 .",
    "note that our decoding algorithm @xmath118 runs virtually unchanged on subcodes .",
    "indeed , the single difference arises when some information block @xmath157 takes only one value 0 on the corresponding left node ( or less than @xmath33 values on the right node ) .",
    "therefore , on each step @xmath153 we can proceed as before , by taking only the actual blocks @xmath157 left at this node after expurgation .    in the algorithm @xmath118 ,",
    "this expurgation starts with the least protected information path @xmath168 that ends at the node @xmath200 it can be shown that for long rm codes of fixed order @xmath201 eliminating even the single weakest path @xmath168  increases the admissible noise power @xmath202 times@xmath116 thus , the lowest orders @xmath203 yield the biggest gain ( 10log@xmath204 db , which equals 0.75 db and 0.375 db , respectively .    to proceed further ,",
    "we eliminate the next weakest path @xmath205 however , the theoretical analysis becomes more complicated on the subsequent bits and it is unclear which bits should be eliminated first . for this reason , we optimized this pruning process in our simulation by making a few ad hoc trials and eliminating subsequent bits in different order .    , @xmath206)-subcode of the ( @xmath207 , @xmath196 ) rm code @xmath208 wer for the algorithm @xmath118 with lists of size @xmath172,width=336 ]",
    "the corresponding simulation results are presented in figure  [ fig : srm0308 ] for the @xmath209-code @xmath210 and its @xmath211-subcode .",
    "we see that pruning substantially improves code performance .",
    "it is also interesting to compare figures  [ fig : rm0308 ] and [ fig : srm0308 ] .",
    "we see that the subcode approaches the optimal ml performance much faster than the original code does .",
    "in particular , the same margin of @xmath183 db can be reached with only @xmath212 codewords instead of @xmath213 codewords needed on the code . in all other examples , the subcodes also demonstrated a much faster convergence , which leads to a lesser complexity .    in fig .",
    "[ fig : srm0309 ] , we present similar results for the @xmath214-subcode of the @xmath215-code @xmath216 here in table 2 , we also give a few list sizes @xmath217 the corresponding  snrs needed to reach the output wer @xmath218 and the complexity estimates @xmath182 counted by the number of floating point operations .",
    "similar results were also obtained for the subcodes of other rm codes of length 512 .     , @xmath219)-subcode of the ( @xmath220 , @xmath221 ) rm code @xmath222 wer for the algorithm @xmath118 with lists of size @xmath172,width=336 ]    [ c]|c|c|c|c|c|@xmath223 & 1 & 4 & 16 & 64 + @xmath224 & 7649 & 25059 & 92555 & 378022 + @xmath225 & 4.31 & 3 & 2.5 & 2.1 +    table 2 .",
    "( @xmath220 , @xmath219)-subcode of the ( @xmath220 , @xmath221 ) rm code @xmath222 list sizes @xmath217 the corresponding snrs@xmath226 and complexity estimates @xmath227 needed at wer @xmath191    these simulation results show that combining both techniques - eliminating the least protected bits and using small lists of codewords - gives a gain of 3 to 4 db on the lengths @xmath228 over the original non - list decoding algorithm @xmath130 for subcodes , we also approach ml decoding with the lists reduced 4 to 8 times relative to the original rm codes .      the second improvement to the algorithm @xmath118 utilizes the rich _ symmetry group _ @xmath229 of rm codes @xcite that includes @xmath230 permutations of @xmath231 positions @xmath232 to employ fewer permutations , we first _",
    "_ permute the @xmath233 indices @xmath234 of all @xmath231 positions @xmath232 thus , we first take a permutation@xmath235 of @xmath233 _ indices _ and consider the corresponding @xmath236 permutations @xmath237 of positions @xmath238 @xmath239    _ remark .",
    "_ note that the @xmath233 indices represent the different _ axes _ in @xmath240 thus , any permutation of indices is the permutation of axes of @xmath240 for example , the permutation @xmath241 of @xmath233 indices leaves unchanged the first and the fourth quarters of all positions @xmath242 but changes the order of the second and the third quarters .    given a permutation @xmath243 consider the subset of @xmath244 original indices  ( axes ) @xmath245 that were transformed into the first @xmath244 axes @xmath246 by the permutation @xmath247 .",
    "we say that two permutations @xmath247 and @xmath248 are equivalent if these images form the identical ( unordered ) subsets @xmath249 now consider any subset @xmath250 of permutations ( [ perm ] ) that includes exactly one permutation from each equivalent class .",
    "thus , @xmath250 includes @xmath251 permutations , each of which specifies a subset of the first  @xmath244 indices . recall that these @xmath244 indices correspond to the axes that are processed first on the subpath @xmath168 ( for example",
    ", we can start with the axis @xmath252 instead of @xmath253 in which case we first fold the adjacent quarters instead of the halves of the original block ) .",
    "thus , this subset @xmath250 specifies all possible ways of choosing @xmath244 unordered axes that will be processed first by the algorithm @xmath130    given some positive integer @xmath254 ( which is smaller than the former parameter @xmath255 we then incorporate these permutations @xmath237 into the list decoding @xmath256 namely ,  we form all permutations @xmath257 of the received vector @xmath87 and apply algorithm @xmath258 to each vector @xmath257 .",
    "however , at each step of the algorithm , we also _ combine different _ lists and leave only @xmath254 best candidates in the combined list , each counted once .",
    "note that this technique makes only marginal changes to our conventional list decoding @xmath256 indeed , the single vector @xmath87  in our original setting ( [ ini1 ] ) is replaced by @xmath259  permutations @xmath260 thus , we use parameter @xmath251 in our initial setting but keep parameter @xmath254 for all decoding steps .",
    "if @xmath261 then the number of records drops to @xmath254 almost immediately , after the first decoding is performed on the path @xmath262    also , information bits are now decoded in different orders depending on a specific permutation @xmath263 note that we may ( and often do ) get the same entries repeated many times . therefore , in steps 2 and 3 we must eliminate identical entries .",
    "this is done in all steps by applying inverse permutations and comparing the corresponding blocks @xmath264 .",
    "this permutation - based algorithm is called @xmath265 below and has complexity similar to @xmath266 for all the codes tested .",
    "the motivation for this algorithm is as follows .",
    "the specific order of our axes also defines the order  in which the decoding algorithm folds  the original block into the subblocks of lengths @xmath267 then @xmath268 and so on .",
    "now note that this folding procedure will likely accumulate the errors whenever erroneous positions substantially disagree on the two halves ( correspondingly , quarters , and so on ) .",
    "this can also happen if the errors are unevenly spread over the two halves of the original block . by using many permutations ,",
    "we make it more likely that the error positions are spread more evenly even if they get accumulated in the original setting @xmath269 or any other specific setting . in this way",
    ",  permutation techniques serve the same functions as interleaving does on the bursty channels .",
    "simulation results for the moderate lengths 256 and 512 show that the algorithm @xmath265 approaches the optimal ml performance even when the combined list of @xmath254 most probable candidates _ is reduced two to eight times _ relative to the original algorithm @xmath118 . for rm codes of length 256 , we summarize these results in fig .  [",
    "fig : rm256 ] . for each rm code",
    ", we first present the lower bounds for the ml decoding error probability . similarly to fig .",
    "[ fig : rm128 ] , we then find the minimum size @xmath270 that makes the algorithm @xmath265 perform only within @xmath183 db away from ml decoding@xmath116 these sizes and complexity estimates @xmath271 are also given in table 3@xmath116 note that both algorithms give smaller lists once this performance loss @xmath179  is slightly increased . in particular , the results in table 4 show that the lists are reduced two times for @xmath272 db .",
    "in summary , the permutation algorithm _ _ @xmath265 _ performs within 0.5 db from ml decoding on the length 256 , _ by processing  @xmath273 vectors for all rm codes .",
    "_ to date , _ _ both techniques - permutation decoding @xmath265  of complete rm codes and list decoding @xmath118 of their subcodes - yield the best trade - offs between near - ml performance and its complexity known on the lengths @xmath274     and @xmath177 for which the algorithms @xmath265 and @xmath118 perform within @xmath183 db from these bounds.,width=336 ]    [ c]|c|c|c|c|c|rm code & @xmath275 & @xmath197 & @xmath276 & @xmath277 + @xmath278 & 64 & 128 & 128 & 16 + @xmath279{c}\\text{complexity } \\\\",
    "\\left|   \\upsilon_{r}^{m}(l)\\right| \\end{array } } $ ] & 216752 & 655805 & 777909 & 94322 + @xmath225 & 2.91 & 2.65 & 3.38 & 5.2 +    table 3 .",
    "rm codes of length 256 :  the list sizes , complexities , and the corresponding snrs , at which the permutation algorithm @xmath265 performs within @xmath183 db from ml decoding at wer @xmath191    [ c]|c|c|c|c|c|rm code & @xmath275 & @xmath197 & @xmath276 & @xmath277 + @xmath278 & & 64 & 64 & 8 + @xmath279{c}\\text{complexity } \\\\ \\left|   \\upsilon_{r}^{m}(l)\\right| \\end{array } }",
    "$ ] & 116471 & 333506 & 389368 & 37756 + @xmath225 & 3.12 & 2.82 & 3.55 & 5.4 +    table 4 .",
    "rm codes of length 256 : the  list sizes , complexities , and the corresponding snrs , at which the permutation algorithm @xmath265 performs within  @xmath272 db from ml decoding at wer @xmath191    note , however , that the algorithm @xmath265 gives almost no advantage for the subcodes considered in the previous subsection .",
    "indeed , these subcodes are obtained by eliminating the leftmost ( least protected ) information bits .",
    "however , any new permutation @xmath237 assigns the new information bits to these leftmost nodes .",
    "thus , the new bits also become the least protected .",
    "another unsatisfactory observation is that increasing the size of the permutation set @xmath250 - say , to include all @xmath236 permutations of all @xmath233 indices - helps little in improving decoding performance . more generally , there are a number of important open problems related to these permutation techniques .",
    "we name a few :    @xmath181 find the best permutation set @xmath250 for the algorithm @xmath265 ;    @xmath181 analyze the algorithm @xmath265 analytically ;    @xmath181 modify the algorithm @xmath265 for subcodes .",
    "in this paper , we considered recursive decoding algorithms for rm codes that can provide near - maximum likelihood decoding with feasible complexity for rm codes or their subcodes on the moderate lengths @xmath228 .",
    "our study yet leaves many open problems .",
    "firstly , we need to tightly estimate the error probabilities @xmath280 on the different paths @xmath281 to optimize our pruning procedures for specific subcodes , it is important to find the order in which information bits should be removed from the original rm code .",
    "finally , it is yet an open problem to analytically estimate the performance of the algorithms @xmath13 and @xmath282        r. lucas , m. bossert , and a. dammann , `` improved soft - decision decoding of reed - muller codes as generalized multiple concatenated codes , '' _ proc .",
    "itg  conf .",
    "on source and channel coding , _ _ _ aahen , germany , 1998 , pp .",
    "137 - 141 .",
    "n. stolte and u. sorger , `` soft - decision stack decoding of binary reed - muller codes with `` look - ahead '' technique , '' _ _ proc .",
    "7__@xmath284 _  int . workshop `` algebr . and comb . coding theory '' , _ bansko , bulgaria , june 18 - 24 , 2000 , pp .",
    "293 - 298 .",
    "i. dumer and k. shabunov , `` recursive constructions and their maximum likelihood decoding , '' _ _ proc .",
    "38__@xmath284 _  allerton conf .",
    "on commun .",
    ", contr . , and computing , _ monticello , il , usa , 2000 , pp ."
  ],
  "abstract_text": [
    "<S> recursive list decoding is considered for reed - muller ( rm ) codes . </S>",
    "<S> the algorithm repeatedly relegates itself to the shorter rm codes by recalculating the posterior probabilities of their symbols . </S>",
    "<S> intermediate decodings are only performed when these recalculations reach the trivial rm codes . in turn , the updated lists of most plausible codewords are used in subsequent decodings . the algorithm </S>",
    "<S> is further improved by using permutation techniques on code positions and by eliminating the most error - prone information bits . </S>",
    "<S> simulation results show that for all rm codes of length 256 and many subcodes of length 512 , these algorithms approach maximum - likelihood ( ml ) performance within a margin of 0.1 db . as a result , we present tight experimental bounds on ml performance for these codes </S>",
    "<S> .    * index terms  * maximum - likelihood performance , plotkin construction , posterior probabilities , recursive lists , reed - muller codes . </S>"
  ]
}