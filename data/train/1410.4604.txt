{
  "article_text": [
    "one of the challenges in rl is the trade - off between exploration and exploitation . the agent must choose between taking an action known to give positive reward or to explore other possibilities hoping to receive a greater reward in the future . in this context ,",
    "a common strategy in unknown environments is to assume that unseen states are more promising than those states already seen .",
    "one such approach is optimistic initialization of values ( * ? ? ?",
    "* section  2.7 ) .",
    "several rl algorithms rely on estimates of expected values of states or expected values of actions in a given state @xcite .",
    "optimistic initialization consists in initializing such estimates with higher values than are likely to be the true value . to do so , we depend on prior knowledge of the expected scale of rewards .",
    "this paper circumvents such limitations presenting a different way to optimistically initialize value functions without additional domain knowledge or assumptions .    in the next section",
    "we formalize the problem setting as well as the rl framework .",
    "we then present our optimistic initialization approach .",
    "also , we present some experimental analysis of our method using the arcade learning environment @xcite as the testbed .",
    "consider a markov decision process , at time step @xmath0 the agent is in a state @xmath1 and it needs to take an action @xmath2 .",
    "once the action is taken , the agent observes a new state @xmath3 and a reward @xmath4 from a transition probability function @xmath5 .",
    "the agent s goal is to obtain a policy @xmath6 that maximizes the expected discounted return @xmath7 $ ] , where @xmath8 $ ] is the discount factor and @xmath9 is the action - value function for policy @xmath10 . sometimes it is not feasible to compute @xmath9 , we then approximate such values with linear function approximation : @xmath11 , where @xmath12 is a learned set of weights and @xmath13 is the feature vector .",
    "function approximation adds further difficulties for optimistic initialization , as one only indirectly specifies the value of state - action pairs through the choice of @xmath12 .",
    "an approach to circumvent the requirement of knowing the reward scale is to normalize all rewards ( @xmath14 ) by the first non - zero reward seen ( @xmath15 ) , _ i.e. _ : @xmath16 .",
    "then we can optimistically initialize @xmath17 as @xmath18 , representing the expectation that a reward the size of the first reward will be achieved on the next timestep to @xmath19 . for sparse reward domains , which is common in the arcade learning environment ,",
    "the mild form is often sufficient . ] . with function approximation",
    ", this means initializing the weights @xmath12 to ensure @xmath20 , _",
    "e.g. _ : @xmath21",
    ". however , this requires @xmath22 to be constant among all states and actions . if the feature vector is binary - valued then one approach for guaranteeing @xmath23 has a constant norm is to stack @xmath24 and @xmath25 , where @xmath26 is applied to each coordinate .",
    "while this achieves the goal , it has the cost of doubling the number of features .",
    "besides , it removes sparsity in the feature vector , which can often be exploited for more efficient algorithms .",
    "our approach is to shift the value function so that a zero function is in fact optimistic .",
    "we normalize by the first reward as described above .",
    "in addition , we shift the rewards downward by @xmath27 , so @xmath28 .",
    "thus , we have :    @xmath29\\\\     & = & \\underbrace{\\mathbb{e}_\\pi\\bigg[\\sum_{k = 0}^\\infty \\gamma^k \\frac{r_{t+k+1}}{|r_{1\\mbox{\\tiny{st}}}| }     \\bigg]}_{\\frac{q_\\pi(s_t , a_t)}{|r_{1\\mbox{\\tiny{st}}}| } } + \\underbrace{\\sum_{k = 0}^\\infty \\gamma^k ( \\gamma - 1)}_{-1}\\end{aligned}\\ ] ]    notice that since @xmath30 , initializing @xmath31 is the same as initializing @xmath32 .",
    "this shift alleviates us from knowing @xmath33 , since we do not have the requirement @xmath34 anymore .",
    "also , even though @xmath35 is defined in terms of @xmath15 , we only need to know @xmath15 once a non - zero reward is observed .    in episodic tasks this shift will encourage agents to terminate episodes as fast as possible to avoid negative rewards . to avoid this",
    "we provide a termination reward @xmath36 , where @xmath37 is the number of steps in the episode and @xmath38 is the maximum number of steps .",
    "this is equivalent to receiving a reward of @xmath39 for additional @xmath40 steps , and forces the agent to look for something better .",
    "we evaluated our approach in two different domains , with different reward scales and different number of active features .",
    "these domains were obtained from the arcade learning environment @xcite , a framework with dozens of atari 2600 games where the agent has access , at each time step , to the game screen or the ram data , besides an additional reward signal .",
    "we compare the learning curves of regular sarsa(@xmath41 ) @xcite and sarsa(@xmath41 ) with its q - values optimistically initialized .",
    "basic _ features with the same sarsa(@xmath41 ) parameters reported by @xcite .",
    "basic _ features divide the screen in to @xmath42 tiles and check , for each tile , if each of the 128 possible colours are active , totalling 28,672 features .",
    "the results are presented in figure  1 .",
    "we report results using two different learning rates @xmath43 , a low value ( @xmath44 ) and a high value ( @xmath45 ) , each point corresponds to the average after 30 runs .",
    "the game freeway consists in controlling a chicken that needs to cross a street , avoiding cars , to score a point ( @xmath46 reward ) .",
    "the episode lasts for 8195 steps and the agent s goal is to cross the street as many times as possible .",
    "this game poses an interesting exploration challenge for ramdom exploration because it requires the agent to cross the street acting randomly ( @xmath47 ) for dozens of time steps .",
    "this means frequently selecting the action `` go up '' while avoiding cars .",
    "looking at the results in figure  1 we can see that , as expected , optimistic initialization does help since it favours exploration , speeding up the process of learning that a positive reward is available in the game .",
    "we see this improvement over sarsa(@xmath41 ) for both learning rates , with best performance when @xmath44 .",
    "the game private eye is a very different domain . in this game the agent",
    "is supposed to move right for several screens ( much more than when crossing the street in the game freeway ) and it should avoid enemies to avoid negative rewards . along the path the agent can collect intermediate rewards ( @xmath48 ) but its ultimate goal is to get to the end and reach the goal , obtaining a much larger reward .",
    "we can see that the optimistic initialization is much more reckless in the sense that it takes much more time to realize a specific state is not good ( one of the main drawbacks of this approach ) , while sarsa(@xmath41 ) is more conservative .",
    "interestingly , we observe that exploration may have a huge benefit in this game as a larger learning rate guides the agent to see rewards in a scale that was not seen by sarsa(@xmath41 ) .",
    "0.47        0.47        0.47        0.47     thus , besides our formal analysis , we have shown here that our approach behaves as one would expect optimistically initialized algorithms to behave .",
    "it increased agents exploration with the trade off that sometimes the agent `` exploited '' a negative reward hoping to obtain a higher return .",
    "rl algorithms can be implemented without needing rigorous domain knowledge , but as far as we know , until this work , it was unfeasible to perform optimistic initialization in the same transparent way .",
    "besides not requiring adaptations for specific domains , our approach does not hinder algorithm performance .",
    "the authors would like to thak erik talvitie for his helpful input throughout this research .",
    "this research was supported by alberta innovates technology futures and the alberta innovates centre for machine learning and computing resources provided by compute canada through westgrid ."
  ],
  "abstract_text": [
    "<S> in reinforcement learning ( rl ) , it is common to use optimistic initialization of value functions to encourage exploration . </S>",
    "<S> however , such an approach generally depends on the domain , viz . , the scale of the rewards must be known , and the feature representation must have a constant norm . </S>",
    "<S> we present a simple approach that performs optimistic initialization with less dependence on the domain . </S>"
  ]
}