{
  "article_text": [
    "suppose the linear regression model is used to relate @xmath1 to the @xmath2 predictors @xmath3 , @xmath4 where @xmath5 is an unknown intercept parameter , @xmath6 is an @xmath7 vector of ones , @xmath8 is an @xmath9 design matrix , and @xmath10 is a @xmath11 vector of unknown regression coefficients .",
    "in the error term , @xmath12 is an unknown scalar and @xmath13 has a spherically symmetric distribution , @xmath14 where @xmath15 is the probability density , @xmath16={\\bm{0}}_n$ ] , and @xmath17=\\bm{i}_n$ ] .",
    "we assume that the columns of @xmath18 have been centered so that @xmath19 for @xmath20 .",
    "we also assume that @xmath21 and @xmath22 are linearly independent , which implies that @xmath23 the class of error distributions we study includes the class of ( spherical ) multivariate-@xmath0 distributions , probably the most important of the possible alternative error distributions .",
    "it is often felt in practice that the error distribution has heavier tails than the normal and the class of multivariate-@xmath0 distributions is a flexible class that allows for this possibility .",
    "they are also contained in the class of scale mixture of normal distributions and thus , by de finetti s theorem , represent exchangeable distributions regardless of the sample size @xmath24 .    in this paper",
    "we consider estimation of @xmath25 $ ] , the variance of each component of error term , under stein s loss ( see @xcite ) , @xmath26 hence the risk function @xmath27 is given by @xmath28 $ ] .",
    "the best equivariant estimator is the unbiased estimator given by @xmath29 where rss is residual sum of squares given by @xmath30 in the gaussian case , the stein effect in the variance estimation problem has been studied in many papers including @xcite .",
    "@xcite showed that @xmath31 dominates @xmath32 . for smooth ( generalized bayes ) estimators",
    ", @xcite gave the improved estimator @xmath33 where @xmath34 is a smooth increasing function given by @xmath35 and @xmath36 is the coefficient of determination given by @xmath37 @xcite proposed another class of improved generalized bayes estimators .",
    "the proofs in all of these papers seem to depend strongly on the normality assumption .",
    "so it seems then , that it may be difficult or impossible to extend the dominance results to the non - normal case .",
    "also many statisticians have thought that estimation of variance is more sensitive to the assumption of error distribution compared to estimation of the mean vector , where some robustness results have been derived by @xcite .",
    "note that we use the term `` robustness '' in this sense of distributional robustness over the class of spherically symmetric error distributions .",
    "we specifically are not using the term to indicate a high breakdown point .",
    "the use of the term `` robustness '' in our sense is however common ( if somewhat misleading ) in the context of insensitivity to the error distribution in the context of shrinkage literature .    in this paper",
    ", we derive a class of generalized bayes estimators relative to a class of separable priors of the form @xmath38 and show that the resulting generalized bayes estimator is independent of the form of the ( spherically symmetric ) sampling distribution .",
    "additionally , we show , for a particular subclass of these separable priors , @xmath39 , that the resulting robust generalized bayes estimator has the additional robustness property of being minimax and dominating the unbiased estimator @xmath40 simultaneously , for the entire class of scale mixture of gaussians .",
    "a similar ( but somewhat stronger ) robustness property has been studied in the context of estimation of the vector of regression parameters @xmath41 by @xcite .",
    "they gave separable priors of a form similar to priors in this paper for which the generalized bayes estimators are minimax for the entire class of spherically symmetric distributions ( and not just scale mixture of normals ) .",
    "we suspect that the distributional robustness property of the present paper also extends well beyond the class of scale mixture of normal distributions but have not been able to demonstrate just how much further it does extend .",
    "our class of improved estimators utilizes the coefficient of determination @xmath36 in making a ( smooth ) choice between @xmath40 ( when @xmath36 is large ) and @xmath42 ( when @xmath36 is small ) and reflects the relatively common knowledge among statisticians , that @xmath43 overestimates @xmath44 when @xmath36 is small .",
    "see remark [ rem : rsq ] for details .",
    "the organization of this paper is as follows . in section [ sec : gb ]",
    "we derive generalized bayes estimators under separable priors and demonstrate that the resulting estimator is independent of the ( spherically symmetric ) sampling density . in section [ sec : minimax ] we show that a certain subclass of estimators which are minimax under normality remains minimax for the entire class of scale mixture of normals .",
    "further , we show that certain generalized bayes estimators studied in section [ sec : gb ] have this ( double ) robustness property .",
    "some comments are given in section [ sec : cr ] and an appendix gives proofs of certain of the results .",
    "in this section , we show that the generalized bayes estimator of the variance with respect to a certain class of priors is independent of the particular sampling model under stein s loss . also we will give an exact form of this estimator for a particular subclass of `` ( super)harmonic '' priors that , we will later show , is minimax for a large subclass of spherically symmetric error distributions .",
    "[ thm : indep ] the generalized bayes estimator with respect to @xmath45 under stein s loss is independent of the particular spherically symmetric sampling model and hence is given by the generalized bayes estimator under the gaussian distribution .",
    "see appendix .",
    "now let @xmath46 and @xmath47 .",
    "this is related to a family of ( super)harmonic functions as follows .",
    "if , in the above joint prior for @xmath41 , we make the change of variables , @xmath48 , the joint prior of @xmath49 becomes @xmath50 the laplacian of @xmath51 is given by @xmath52 which is negative ( i.e.  super - harmonic ) for @xmath53 and is zero ( i.e.  harmonic ) for @xmath54 .",
    "[ thm : harmonic ] under the model with spherically symmetric error distribution and stein s loss , the generalized bayes estimator with respect to @xmath55 for @xmath56 is given by @xmath57 where @xmath58    see appendix .",
    "in this section , we demonstrate robustness of minimaxity under scale mixture of normals for a class of estimators which are minimax under normality .",
    "[ thm : main ] assume @xmath59 where @xmath60 is monotone nondecreasing , improves on the unbiased estimator , @xmath40 , under normality and stein s loss .",
    "then @xmath61 also improves on the unbiased estimator , @xmath40 , under scale mixture of normals and stein s loss .",
    "let @xmath62 be a scale mixture of normals where the scalar @xmath63 satisfies @xmath64=1 $ ] , that is , @xmath65 the risk difference between these estimators is given by @xmath66 .",
    "\\end{split}\\ ] ] for given @xmath67 , @xmath68 and @xmath69 are independently distributed as @xmath70 with @xmath71 and @xmath72 . since",
    "@xmath36 is given by @xmath73 , we have @xmath74 \\tau^2 |v \\right ] \\frac{v}{(n - p-1)e[\\tau^2]}\\right ] \\\\ & \\qquad + e\\left [ \\log \\phi(\\{1+v / u\\}^{-1})\\right ] .",
    "\\end{split}\\ ] ] now note that by the monotone likelihood ratio property of non - central @xmath75 , we have the following lemma .",
    "[ lem : non - chi ] @xmath76 $ ] with @xmath77 is decreasing in @xmath63 if @xmath78 is increasing .",
    "by lemma [ lem : non - chi ] and the covariance inequality , @xmath79 \\tau^2 |v \\right ] \\\\ & \\qquad \\qquad \\qquad \\geq   e[\\tau^2 ]   e\\left [ 1-\\phi(\\{1+v / u\\}^{-1})| v \\right ] .",
    "\\end{split}\\ ] ] hence we get @xmath80 \\\\ & = \\int_0^\\infty \\left\\{r_g\\left(\\{\\alpha,{\\bm{\\beta}},\\tau^2\\ } , \\delta_u\\right ) -r_g\\left(\\{\\alpha,{\\bm{\\beta}},\\tau^2\\ } , \\delta_\\phi \\right)\\right\\ } g(\\tau^2)d\\tau^2 \\\\ & \\geq 0 ,   \\end{split}\\ ] ] where @xmath81 is the risk function under the gaussian assumption .    under the normality assumption , @xcite showed that the estimator @xmath82 with nondecreasing @xmath83 dominates the unbiased estimator @xmath32 if @xmath84 , where @xmath85 is given by .",
    "@xcite demonstrated that the generalized bayes estimator of theorem [ thm : harmonic ] with @xmath54 satisfies this condition .",
    "hence our main result shows that the generalized bayes estimator of theorem [ thm : harmonic ] with @xmath54 , is minimax for the entire class of variance mixture of normal distributions .",
    "[ thm : main2 ] let @xmath86 . under stein",
    "s loss , the estimator given by @xmath87 where @xmath88 is minimax and generalized bayes with respect to the harmonic prior @xmath89 for the entire class of scale mixture of normals .",
    "[ rem : rsq ] the coefficient of determination is given by @xmath90 and @xmath91= \\sigma^2\\{\\xi+p\\ } , \\\\ & e\\left[\\|{\\bm{y}}-\\bar{y}{\\bm{1}}_n\\|^2\\right]=\\sigma^2\\{\\xi+n-1\\ } , \\end{split}\\ ] ] where @xmath92 .",
    "hence the smaller @xmath36 corresponds to the smaller @xmath93 .",
    "our class of improved estimators utilizes the coefficient of determination @xmath36 in making a ( smooth ) choice between @xmath40 ( when @xmath36 and @xmath94 are large ) and @xmath42 ( when @xmath36 and @xmath94 are small ) and reflects the relatively common knowledge among statisticians , that @xmath43 overestimates @xmath44 when @xmath94 is small .",
    "the estimator @xmath95 is not the only minimax generalized bayes estimator under scale mixture of normals . in theorem [ thm : harmonic ] , we also provided the generalized bayes estimator with respect to superharmonic prior given by @xmath55 . in @xcite , we show that for @xmath96 with @xmath97 is minimax in the normal case with a monotone @xmath98 . hence for @xmath99 in this range @xmath96 is also minimax and generalized bayes for the entire class of scale mixture of normals .",
    "the bound @xmath100 has a somewhat complicated form and we omit the details ( however , see @xcite for details ) .",
    "since @xmath101 and @xmath40 correspond @xmath54 to @xmath102 , respectively , we conjecture that @xmath96 with @xmath103 is minimax .    under the normality assumption",
    ", @xcite gave a subclass of minimax generalized bayes estimators with the particularly simple form @xmath104 for @xmath105 where @xmath106 has a slightly complicated form , which we omit ( see @xcite for details ) .",
    "under spherical symmetry , this estimator is not necessarily derived as generalized bayes ( see the following remark ) , but is still minimax under scale mixture of normals .",
    "interestingly , when @xmath107 , the generalized bayes estimator with respect to @xmath108 is given by @xmath109 for the entire class of spherically symmetric distributions ( see @xcite for the technical details ) .",
    "hence when @xmath110 @xmath111 is minimax and generalized bayes for the entire class of scale mixture of normals .",
    "unfortunately , numerical calculations indicate that , for @xmath24 in the range @xmath112 , the inequality is only satisfied for @xmath113 for @xmath24 odd and @xmath114 and @xmath115 for @xmath24 even .    for theorems [ thm : main ] and",
    "[ thm : main2 ] , the choice of the loss function is the key .",
    "many of the results introduced in section [ sec : intro ] are given under the quadratic loss function @xmath116 . under the gaussian assumption , the corresponding results can be obtained by replacing @xmath117 by @xmath24 . on the other hand ,",
    "the generalized bayes estimator with respect to @xmath45 depends on the particular sampling model and hence robustness results do not hold under non - gaussian assumption .",
    "in this paper , we have studied estimation of the error variance in a general linear model with a spherically symmetric error distribution . we have shown , under stein s loss , that separable priors of the form @xmath38 have associated generalized bayes estimators which are independent of the form of the ( spherically symmetric ) sampling distribution .",
    "we have further exhibited a subclass of `` superharmonic '' priors for which these generalized bayes estimators dominate the usual unbiased and best equivariant estimator , @xmath40 , for the entire class of scale mixture of normal error distributions .",
    "we have previously studied a very similar class of prior distributions in the problem of estimating the regression coefficients @xmath41 under quadratic loss ( see @xcite ) . in that study",
    "we demonstrated a similar double robustness property , to wit , that the generalized bayes estimators are independent of the form of the sampling distribution and that they are minimax over the entire class of spherically symmetric distributions .",
    "the main difference between the classes of priors in the two settings are a ) in the present study , the prior on @xmath44 is proportional to @xmath118 while it is proportional to @xmath119 in the earlier study ; and b ) in this paper , the prior on @xmath41 is also separable with @xmath5 being uniform on the real line and @xmath10 having the `` superharmonic '' form , while in the earlier paper @xmath41 jointly had the superharmonic form .",
    "the difference a ) is essential since a prior on @xmath44 proportional to @xmath118 gives the best equivariant and minimax estimator @xmath40 , while such a restriction is not necessary when estimating the regression parameters @xmath41 .",
    "the difference in b ) is inessential , and either form of priors on the regression parameters @xmath41 will give estimators with the double robustness properties in each of the problems studied .",
    "the form of the estimators , of course , will be somewhat different . in the case of the present paper",
    ", the main difference would be to replace @xmath120 by @xmath121 and to replace @xmath36 by @xmath122    as a consequence , the results in these papers suggest that separable priors , and in particular the `` harmonic '' prior given , are very worthy candidates as objective priors in regression problems .",
    "they produce generalized bayes minimax procedures dominating the classical unbiased , best equivariant estimators of both regression parameters and scale parameters simultaneously and uniformly over a broad class of spherically symmetric error distributions .",
    "the ( generalized ) bayes estimator with stein s loss is given by @xmath123\\}^{-1}$ ] . under the improper density @xmath124 ,",
    "the generalized bayes estimator is given by @xmath125 where @xmath126 for @xmath127 is the conditional marginal density of @xmath128 with respect to @xmath129 given @xmath5 and @xmath10 , @xmath130 further we have @xmath131 where @xmath132 hence the generalized bayes estimator is @xmath133 where @xmath134 since @xmath135 has a spherically symmetric density @xmath136 and @xmath16={\\bm{0}}_n$ ] and @xmath137=\\bm{i}_n$ ] , @xmath62 as well as @xmath138 satisfies @xmath139 and @xmath140 hence we have @xmath141 and hence the generalized bayes estimator is given by @xmath142 which is independent of @xmath62 .        by the simple relation @xmath148 where @xmath149 mean the mean of @xmath128 and @xmath150",
    ", we have the pythagorean relation , @xmath151 since @xmath18 has been already centered",
    ". then we have @xmath152 next we consider the integration with respect to @xmath10 .",
    "note the relation of completing squares with respect to @xmath10 @xmath153 where @xmath154 and @xmath155 is the coefficient of determination .",
    "hence we have @xmath156 next we consider integration with respect to @xmath44 . by , we have @xmath157 finally we consider integration with respect to @xmath147 . by we",
    "have @xmath158 the second equality follows from the change of variables @xmath159 . by using the relation @xmath160 , @xmath161 is written as ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the error variance in a general linear model when the error distribution is assumed to be spherically symmetric , but not necessary gaussian . </S>",
    "<S> in particular we study the case of a scale mixture of gaussians including the particularly important case of the multivariate-@xmath0 distribution . under stein s loss , we construct a class of estimators that improve on the usual best unbiased ( and best equivariant ) estimator . </S>",
    "<S> our class has the interesting double robustness property of being simultaneously generalized bayes ( for the same generalized prior ) and minimax over the entire class of scale mixture of gaussian distributions . </S>"
  ]
}