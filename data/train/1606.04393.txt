{
  "article_text": [
    "learning , especially deep neural networks  @xcite have shown considerable promise through tremendous results in recent years , significantly improving the accuracy of a variety of challenging problems when compared to other machine learning methods  @xcite .",
    "however , deep neural networks require high performance computing systems due to the tremendous quantity of computational layers they possess , leading to a massive quantity of parameters to learn and compute .",
    "this issue of architectural complexity has increased greatly in recent years  @xcite , driven by the demand for increasingly deeper and larger deep neural networks to boost modeling accuracy . as such ,",
    "it has become increasingly more difficult to take advantage of such complex deep neural networks in scenarios where computational and energy resources are scarce .    to enable the widespread use of deep learning",
    ", there has been a recent drive towards obtaining highly - efficient deep neural networks with strong modeling power .",
    "much of the work in obtaining efficient deep neural networks have focused on deterministically compressing trained deep neural networks  @xcite , using traditional lossless and lossy compression techniques such as quantization  @xcite , deterministic pruning  @xcite , huffman coding  @xcite , and hashing  @xcite . rather than attempting to take an existing deep neural network and compress it into a smaller representation heuristically , we instead consider the following idea : _ can deep neural networks * evolve * naturally over successive generations into highly efficient deep neural networks ? _ using an example of evolutionary progress towards efficiency from nature , a recent study by moran  @xcite proposed that the eyeless mexican cavefish evolved to lose its vision system over generations due to the high metabolic cost of vision . therefore , by evolving naturally over generations in a way where the cavefish lost its vision system , the amount of energy expended is significantly reduced and thus improves survivability in subterranean habitats where food availability is low . the ability to mimic the biological evolutionary process for the task of producing highly - efficient deep neural networks over successive generations can have considerable benefits .    in this study , we entertain a different notion for producing highly - efficient deep neural networks by introducing the evolutionary synthesis of deep neural networks over successive generations based on ancestor deep neural networks . while the idea of leveraging evolutionary computation concepts for training and generating deep neural networks have been previously explored in literature  @xcite , there are significant key differences between these previous studies and this study :    * while previous studies have focused on improving the accuracy and training of deep neural networks , to the best of the authors knowledge this study is the first to explore and focus on the notion of evolutionary synthesis of deep neural networks with high network architectural efficiency over successive generations . *",
    "while the evolutionary computational approaches leveraged by these previous studies are classical approaches such as genetic algorithms and evolutionary programming , this study introduces a new probabilistic framework where evolution mechanisms such as genetic encoding and environmental conditions are modeled via probability distributions , and the stochastic synthesis process leverages these probability models to produce deep neural networks at successive generations . to the best of the authors knowledge",
    ", this study is the first to leverage a probabilistic approach to evolutionary synthesis of deep neural networks .",
    "* to the best of the authors knowledge , the new approach introduced in this study is the first to achieve evolution and synthesis of deep neural networks with very deep , large neural network architectures that have been demonstrated to provide great performance in recent years  @xcite .",
    "previous studies have focused on deep neural networks with smaller and shallower network architectures , as the approaches used in such studies are more difficult to scale to very deep , large network architectures .",
    "the proposed evolutionary synthesis of deep neural networks is primarily inspired by real biological evolutionary mechanisms . in nature , traits that are passed down from generation to generation through dna may change over successive generations due to factors such as natural selection and random mutation , giving rise to diversity and enhanced traits in later generations .",
    "to realize the idea of evolutionary synthesis for producing deep neural networks , we introduce a number of computational constructs to mimic the following mechanisms of biological evolution : i ) * heredity * , ii ) * natural selection * , and iii ) * random mutation*.    * heredity . * here , we mimic the idea of heredity by encoding the architectural traits of deep neural networks in the form of synaptic probability models , which are used to pass down traits from generation to generation .",
    "one can view these synaptic probability models as the ` dna ' of the networks .",
    "let @xmath1 denote the possible architecture of a deep neural network , with @xmath2 denoting the set of possible neurons and @xmath3 denoting the set of possible synapses , with @xmath4 denoting a synapse between two neurons @xmath5 .",
    "one can encode the architectural traits of a deep neural network as @xmath6 , which denotes the conditional probability of the architecture of a network in generation @xmath7 ( denoted by @xmath8 ) , given the architecture of its ancestor network in generation @xmath9 ( denoted by @xmath10 ) .",
    "if we were to treat areas of strong synapses in an ancestor network in generation @xmath7 as desirable traits to be inherited by descendant networks at generation @xmath7 , where descendant networks have a higher probability of having similar areas of strong synapses as its ancestor network , one can instead encode the architectural traits of a deep neural network as the synaptic probability @xmath11 , where @xmath12 encodes the synaptic strength of each synapse @xmath13 . modeling @xmath11 as an exponential distribution , with the probability of each synapse in the network assumed to be independently distributed , one arrives at @xmath14 where @xmath15 is a normalization constant .",
    "* natural selection and random mutation . * the ideas of natural selection and random mutation are mimicked through the introduction of a network synthesis process for synthesizing descendant networks , which takes into account not only the synaptic probability model encoding the architectural traits of the ancestor network , but also an environmental factor model to mimic the environmental conditions that help drive natural selection , in a random manner that drives random mutation .",
    "more specifically , a synapse is synthesized randomly between two possible neurons in a descendant network based on @xmath11 and an environmental factor model @xmath16 , with the neurons in the descendant network synthesized subsequently based on the set of synthesized synapses . as such , the architecture of a descendant network at generation @xmath7",
    "can be synthesized randomly via synthesis probability @xmath17 , which can be expressed by @xmath18 the environmental factor model @xmath16 can be the combination of quantitative environmental conditions that are imposed upon the descendant networks that they must adapt to .    to have a better intuitive understanding ,",
    "let us examine an illustrative example of how one can impose environmental conditions using @xmath16 to promote the evolution of highly efficient deep neural networks .",
    "* efficiency - driven evolutionary synthesis .",
    "* one of the main environmental factors in encouraging energy efficiency during evolution is to restrict the resources available .",
    "for example , in a study by moran _",
    "et al . _",
    "@xcite , it was proposed that the eyeless mexican cavefish lost its vision system over generations due to the high energetic cost of neural tissue and low food availability in subterranean habitats .",
    "their study demonstrated that the cost of vision is about 15% of resting metabolism for a 1-g eyed phenotype , thus losing their vision system through evolution has significant energy savings and thus improves survivability .",
    "as such , we are inspired to computationally restrict resources available to descendant networks to encourage the evolution of highly - efficient deep neural networks .",
    "considering the aforementioned example , the descendant networks must take on network architectures with more efficient energy consumption than this original ancestor network to be able to survive .",
    "the main factor in energy consumption is the quantity of synapses and neurons in the network .",
    "therefore , to mimic environmental constraints that encourage the evolution of highly - efficient deep neural networks , we introduce an environmental constraint @xmath19 that probabilistically constrains the quantity of synapses that can be synthesized in the descendant network ( which in effect also constrains the quantity of neurons that can be synthesized ) , such that descendant networks are forced to evolve more efficient network architectures than their ancestor networks .    therefore , given @xmath11 and @xmath19 , the synthesis probability @xmath17 can be formulated as @xmath20 where @xmath21 is the highest percentage of synapses desired in the descendant network .",
    "the random element of the network synthesis process mimics the random mutation process and promotes network architectural diversity .",
    "given the probabilistic framework introduced above , the proposed evolutionary synthesis of highly - efficient deep neural networks can be described as follows ( see figure  [ fig : evonet ] ) .",
    "given an ancestor network at generation @xmath9 , a synaptic probability model @xmath22 is constructed according to eq .",
    "[ synaptiveprob ] . using @xmath22 and environmental constraint @xmath16 ,",
    "a synthesis probability @xmath17 is constructed according to eq .",
    "[ synthprob ] . to synthesize a descendant nework at generation @xmath7 , each synapse @xmath23 in the descendant network",
    "is synthesized randomly as follows : @xmath24 where @xmath25 is a uniformly distributed random number from a uniform distribution between 0 and 1 .",
    "the synthesized descendant networks at generation @xmath7 are then trained into fully - functional networks , like one would train a newborn , and the evolutionary synthesis process is repeated for producing successive generations of descendant networks .",
    "to investigate the efficacy of the proposed evolutionary synthesis of highly - efficient deep neural networks , experiments were performed using the msra - b  @xcite and hku - is datasets  @xcite for the task of visual saliency .",
    "this task was chosen given the importance for biological beings to detect objects of interest ( e.g. , prey , food , predators ) for survival in complex visual environments , and can provide interesting insights into the evolution of networks .",
    "three generations of descendant deep neural networks ( second , third , and fourth generations ) were synthesized within an artificially constrained environment beyond the original , first - generation ancestor network .",
    "the environmental constraint imposed during synthesis in this study is that the descendant networks should not have more than 40% of the total number of synapses that its direct ancestor network possesses ( i.e. , @xmath26 ) , thus encouraging the evolution of highly - efficient deep neural networks .",
    "the network architecture of the original , first generation ancestor network used in this study , and details on the tested datasets and performance metrics are as follow .    * network architecture . *",
    "the network architecture of the original , first generation ancestor network used in this study builds upon the vgg16 very deep convolutional neural network architecture  @xcite for the purpose of image segmentation as follows .",
    "the outputs of the c3 , c4 , and c5 stacks from the vgg16 architecture are fed into newly added c6 , c7 , c8 stacks , respectively . the output of the c7 and c8 stacks",
    "are then fed into d1 and d2 stacks .",
    "the concatenated outputs of the c6 , d1 , and d2 stacks are then fed into the c9 stack .",
    "the output of the c5 stack is fed into c10 and c11 stacks .",
    "finally , the combined output of the c9 , c10 and c11 stacks are fed into a softmax layer to produce final segmentation result .",
    "the details of different stacks are as follows : c1 : 2 convolutional layers of 64 , @xmath27 local receptive fields , c2 : 2 convolutional layers of 128 , @xmath28 local receptive fields , c3 : 3 convolutional layers of 256 , @xmath28 local receptive fields , c4 : 3 convolutional layers of 512 , @xmath28 local receptive fields , c5 : 3 convolutional layers of 512 , @xmath28 local receptive fields , c6 : 1 convolutional layers of 256 , @xmath28 local receptive fields , c7 and c8 : 1 convolutional layers of 512 , @xmath28 local receptive fields , c9 : 1 convolutional layers of 384 , @xmath29 local receptive fields , c10 and c11 : 2 convolutional layers of 512 , @xmath30 local receptive fields and 384 , @xmath29 local receptive fields , d1 and d2 are deconvolutional layers",
    ".    * datasets . *",
    "the msra - b dataset  @xcite consists of 5000 natural images and their corresponding ground truth maps where the salient objects in the images are segmented with pixel - wise annotation .",
    "the dataset is divided into training , validation and testing groups containing 2500 , 500 and 2000 images , respectively .",
    "figure  [ fig : msra - b ] the hku - is dataset  @xcite consists of 4447 natural images and their corresponding ground truth maps where the salient objects in the images are segmented with pixel - wise annotation .",
    "the entire dataset is used as a testing group for the descendant networks trained on the training group of the msra - b dataset .",
    "figure  [ fig : hku - is ] illustrates some of the example images from the dataset with their corresponding ground truths .    [ cols=\"^,^,^,^ \" , ]",
    "a.w . conceived the concept of evolutionary synthesis for deep learning proposed in this study .",
    "m.s . and a.w .",
    "formulated the evolutionary synthesis process proposed in this study .",
    "a.m. implemented the evolutionary synthesis process and performed all experiments in this study .",
    ", m.s . , and a.m. all participated in writing this paper ."
  ],
  "abstract_text": [
    "<S> taking inspiration from biological evolution , we explore the idea of `` _ _ can deep neural networks * evolve * naturally over successive generations into highly efficient deep neural networks ? _ _ '' by introducing the notion of synthesizing new highly efficient , yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks . </S>",
    "<S> the architectural traits of ancestor deep neural networks are encoded using synaptic probability models , which can be viewed as the ` dna ' of these networks . </S>",
    "<S> new descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models , in a random manner to mimic heredity , natural selection , and random mutation . </S>",
    "<S> these offspring networks are then trained into fully functional networks , like one would train a newborn , and have more efficient , more diverse network architectures than their ancestor networks , while achieving powerful modeling capabilities . </S>",
    "<S> experimental results for the task of visual saliency demonstrated that the synthesized ` evolved ' offspring networks can achieve state - of - the - art performance while having network architectures that are significantly more efficient ( with a staggering @xmath048-fold decrease in synapses by the fourth generation ) compared to the original ancestor network .    </S>",
    "<S> shell : bare demo of ieeetran.cls for computer society journals    deep neural network , evolutionary , evonet , deep learning , saliency detection , </S>"
  ]
}