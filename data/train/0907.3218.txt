{
  "article_text": [
    "automatic face recognition is a challenging problem with currently achievable levels of performance not adequate for universal practical application , and there remains a need for further work to improve performance and flexibility .",
    "numerous algorithms have been developed for face recognition since it has been proved that gabor - type receptive field could extract the maximum information from local image regions  @xcite and gabor filters function similarly to the visual neurons of the human visual system  @xcite .",
    "therefore , mathematical transforms using gabor wavelets ( gw ) play an increasingly important role in extracting robust features from face images for classification  @xcite .",
    "representing images by gw is difficult problem due to two reasons .",
    "first , since gw are not orthogonal , they can not be used as basis functions as the reconstruction coefficients will not be unique for each image .",
    "second , although exploiting the locality property of gw allows convolution of a gw at each location of the image to extract detailed local image information , the application of gw for all possible orientations and scales at every location in the image results in an enormous computational overhead .",
    "high computational cost can be avoided by reducing the feature dimensionality which requires optimization of the criterions for selecting gw .",
    "most existing research studies select gabor wavelets empirically , rather than optimally .",
    "the challenge that researchers are facing today is how best to exploit gw to maximize benefits in terms of object recognition performance .",
    "this paper aims to optimize the criterions for selecting gw by adaboost ( ab ) algorithm in a parallel manner and incorporating mutual information ( mi ) into the algorithm . after giving the theoretical background on gw in section",
    "[ sec : gw ] , ab and parallel adaboost ( pab ) algorithms are explained in section  [ sec : fext ] as means of feature extraction and selection .",
    "section  [ sec : mi ] explains mi based gw selection procedures with ab and pab algrotihms respectively .",
    "experimental results in section  [ sec : exp ] are followed by conclusion in section  [ sec : conc ] .",
    "in the spacial domain , the 2d gabor filter is a gaussian kernel modulated by a sinusoidal plane wave  @xcite @xmath0",
    "where @xmath1 , @xmath2 , @xmath3 is the central frequency of the sinusoidal plane wave , @xmath4 is the anti - clockwise rotation of the gaussian and the plane wave , @xmath5 is the sharpness of the gaussian along the major axis parallel to the wave , and @xmath6 is the sharpness of the gaussian minor axis perpendicular to the wave . @xmath7 and",
    "@xmath8 are defined to keep the ratio between frequency and sharpness constant .",
    "the gabor filters , like many other wavelets , can be generated from one mother wavelet by dilation and rotation .",
    "each filter is in the shape of plane waves with frequency @xmath3 , restricted by a gaussian envelope function with relative width @xmath5 and @xmath6 . to extract useful features from an image , normally a set of gabor filters with different frequencies and orientations",
    "are required @xcite , @xmath9 as shown in eq ( [ g1 ] ) and ( [ g2 ] ) , the following parameters need to be determined to design gabor filters for feature extraction : the highest peak frequency @xmath10 , the ratio between centre frequency and the sharpness of gaussian major axis : @xmath11 and minor axis : @xmath12 , the number of scales @xmath13 and orientations @xmath14 .",
    "see our previous studies  @xcite for further theoretical details on how to select these parameters .",
    "the aim is to use gw to extract unique features uniformly across all images so that these features can be compared for face recognition .",
    "a common approach is to convolve each image with the same set of gw .",
    "the number of gabor wavelets used for this varies with different applications , but usually 40 filters ( @xmath13=5 scales and @xmath14=8 orientations ) are chosen empirically for face recognition applications  @xcite .",
    "specifically , given a bank of 40 gw @xmath15 , image features at different locations , frequencies and orientations can be extracted by convolving the image @xmath16 , locally , with the gw @xmath17 .",
    "the feature set thus consists of the results of the local convolution of the image @xmath16 with all of the 40 gw @xmath18 a gabor feature vector can be obtained by concatenating the rows ( or columns ) of @xmath19 for all @xmath20 to represent the   where @xmath21 is the gabor feature extraction operation . as an example , taking an image of size 64 x 64 , the gabor feature vector will be of 64 x 64 x 5 x 8=163.840 dimensions , which is incredibly large .",
    "due to the large number of convolution operations , the computation cost is also necessarily high .    instead of performing a convolution operation at every image location , using all the 40 gw , it is more sensible to select only the relevant gw to perform convolution with the image at appropriate positions .",
    "two questions arise from this consideration : first , which wavelets should be used and , second , at which image locations . to fully appreciate the solution of these questions ,",
    "we have developed an approach using ab algorithm to select gw based on not only location parameter of gw , but also orientation and frequency parameters of gw in  @xcite . in this study , we improve this approach further by introducing the mi concept to the feature selection procedure and parallelizing the ab algorithm .      briefly , ab algorithm iteratively builds a trainable model @xmath22 using linear superposition of different realizations .",
    "the base model @xmath22 is re - trainable by using different weight combinations , @xmath23 , where @xmath24 is number of samples  @xcite . after each training step",
    ", the weights are updated according to classification performance of the previous step over the training data .",
    "the weights of misclassified points , @xmath25 , are increased and weights of correctly classified points , @xmath26 , are decreased accordingly  @xcite . therefore , at each step there is an associated model @xmath27 . the final hypothesis / model is the linear superposition of all these model instances .",
    "the ab algorithm is computationally expensive . in particular , for any  hard  point , the distribution of the associated weights appears to converge , as the number of the steps of the ab algorithm grows to infinity , to a definite , stable distrubition  @xcite .",
    "pab aims to decrease the computational cost by approximating these asymptotic distributions .",
    "it is shown that weight parameters can be modelled well by gamma distributions of suitable parameters  @xcite .",
    "using early estimates of weights , one can construct a distribution system from which ab weights can be selected instead of waiting for the sequential outputs of each steps .",
    "once weight distributions @xmath28 are modelled under the gamma distribution by @xmath29 then weights are updated independently and randomly from this distribution where values for @xmath5 and @xmath4 are obtained from the mean , @xmath30 , and the variance , @xmath31 , of the weights based on first @xmath32-step evolutions .",
    "the relationship of these variables is the following : @xmath33      given the data set @xmath34 ;    1 .",
    "initialize weights @xmath35 2 .",
    "run adaboost for @xmath32 steps and keep weights for each step , @xmath36 3 .   for @xmath37 , estimate the distribution @xmath28 from weights stored previously , @xmath38 .",
    "parallel computation starts here + for each value of @xmath39 : do the steps below in parallel 1 .   for i running on the data set , generate random and independent weights @xmath40 by sampling the corresponding @xmath28 ; 2 .",
    "train base model @xmath22 using weights @xmath40 , resultant model instance @xmath41 ; 3 .",
    "compute model error @xmath42 ; 4 .",
    "compute model weights @xmath43 : @xmath44ln@xmath45 5 .   compute the output hypothesis + @xmath46",
    "as easily seen that after the step 3 , new values to the weights could then be assigned not by following the standard ab algorithm , but by randomly and independently sampling the respective gamma distribution model .",
    "this leads dramatic reduction in computational cost without losing accuracy in classification performance due to correctly keeping dynamics of stochastic process .",
    "we simplify the task of selecting gw for feature extraction from a multi - class face recognition problem to a two - class problem : selecting gw that are effective for intra- and extra - person space discrimination .",
    "such selected gw should be robust for face recognition , as intra- and extra - person space discrimination is one of the major difficulties in face recognition .    the transition from a multi - class to a two - class problem",
    "is based on a method proposed in  @xcite , reformulating the face recognition problem as a two class problem .",
    "two spaces , intra- and extra - person spaces are defined , with intra - person space measuring respectively dissimilarities between faces of the same person and extra - person space dissimilarities between different people .",
    "we define intra- and extra - person spaces as @xmath47 where @xmath48 and @xmath49 are the facial images of persons @xmath50 and @xmath51 respectively .",
    "now it is seen that intra- and extra person space discrimination is a two - class problem and to use pab algorithm for selecting gw , the training set will be @xmath52 .",
    "samples in the intra - person space are regarded as positive examples whilst those from extra - person space are regarded as negative examples .",
    "each weak classifier can be defined on one gabor wavelet , such that the weak classifier determines the class of a vector based on a feature extracted from the vector using just this one gabor wavelet .",
    "selected weak classifiers ( and therefore the corresponding gw ) are therefore effective in discriminating intra- and extra - person classes , and should be used to extract features for face recognition .",
    "recall that each component of a vector in @xmath53 is associated with a gabor wavelet , i.e. , it is obtained by convolving an image with a gabor wavelet @xmath54 , therefore , a weak classifier can be defined as a simple threshold function on a component of the vector as @xmath55 where @xmath56 can be determined by the intra - person sample mean and extra - person sample mean @xmath57 where @xmath58 and @xmath59 are the numbers of intra- and extra - person samples , respectively .    in each of the pab and/or ab iterations , the space of all possible weak classifiers",
    "is searched exhaustively to find the best weak classifier that will produce the lowest classification error .",
    "the error is then used to update the weights such that the wrongly classified samples get more focus .",
    "the resulting strong classifier is a weighted linear combination of all the selected weak classifiers .",
    "the pab and/or ab algorithm select hundreds of features and weak classifiers to form the final strong classifier .",
    "the pab and ab algorithm select only features that perform  individually  best , and the redundancy among selected features is not considered . to eliminate redundancy ,",
    "mi can be used . before a new weak classifier",
    "is selected , the mi between the new classifier and those already selected is examined to make sure that the information carried by the new classifier has not been captured before . at stage @xmath60",
    "where @xmath61 weak classifiers @xmath62 are selected , the function to measure the mi between a candidate classifier @xmath63 and the selected classifiers can be defined as follows @xmath64    each weak classifier is now considered as a random variable .",
    "the estimation of mi between two such variables , e.g. @xmath65 and @xmath66 , requires information about the marginal distribution @xmath67 , @xmath68 and the joint probability distribution @xmath69 , where @xmath70 represents probability .",
    "though a gaussian distribution could be assumed , many of the features might not be gaussian . to reduce the complexity and computation cost of the feature selection process",
    ", we therefore focus on binary random variables only , i.e. @xmath71 , @xmath72 . for binary random variables ,",
    "the probabilities could be estimated by simply counting the number of possible cases and dividing that number by the total number of training samples .",
    "the value of @xmath73 can be directly used to determine whether the new classifier is redundant or not .",
    "the value is compared with a pre - defined threshold @xmath74 , if it is bigger than the @xmath74 , we can deduce that the information carried by the classifier has already been captured .",
    "besides mi , the classification error of the weak classifier is also taken into consideration , i.e. , only those classifiers with small classification errors are selected .",
    "the features thus selected are uncorrelated with each other and are therefore non - redundant .",
    "[ img : face ] shows the first and last six selected gw using mi enhanced pab algorithm .",
    "it is interesting to see that most of the selected gabor features are located around the prominent facial features such as eyebrows , eyes , nose and chin , which indicates that these regions are more robust against the variance of expression and illumination encountered within the database subset .",
    "this result is consistent with the fact that the eye and eyebrow regions remain relatively stable when a person s facial expression changes .",
    "recall that the selection criterion is the ability of the gw in discriminating intra- and extra - person classes .",
    "we use a subset of 600 images from the feret database to test the gabor feature selection algorithm using ab , pab and mi . two images of each subject are randomly chosen for training , and the remaining one is used for testing .",
    "the selected 400 face images ( 2 images for each subject ) are first used in boosting algorithms ( ab and pab ) training to select gw for intra- and extra - person space discrimination . as a result , 200 intra - person difference samples and 1,600 extra - person difference samples",
    "are randomly generated for training  @xcite .",
    "although the required training time is longer than using the original ab due to the use of mi , the computational cost is reduced by using pab algorithm so that required training time using mi with pab is always lower than that of ab with mi .",
    "if the computational cost of ab algorithm is @xmath75 , on the other hand , the cost of pab algorithm is @xmath76 , where number of serial iterations @xmath32 in pab is chosen smaller than total number of iterations @xmath60 in ab , @xmath77 .",
    "the normalized correlation distance measure and the nearest neighbor classifier are used .",
    "table  [ table : results ] shows the recognition performance on the 200 test images , where the highest accuracies achieved for the three algorithms are 93% , 95% and 96% for ab , ab+mi and pab+mi respectively .",
    "since the mi values for all of the first 60 features are quite small , the effect of mutual information on the selection process is not obvious initially .",
    "once the number of features increases , ab and pab start to pick up highly redundant features while the use of mutual information reduces the redundancy and improves recognition rate . in pab ,",
    "first 50 iterations are processed as ab , then the algorithm is parallelized in which weights in each iteration are selected randomly and independently from the model built using first 50 weights dynamics . to compare ab with pab ,",
    "not only computational cost is reduced dramatically , but also pab algorithm appears to converge quickly to the reference model .",
    ".face recognition rates ( % ) for various dimensions of feature set .",
    "ab : adaboost , ab+mi : adaboost with mutual information and pab+mi : parallel - adaboost with mutual information .",
    "[ table : results ] [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "the locality property of gw has both advantages and disadvantages .",
    "a positive aspect is that it allows the extraction of local features , while a more negative aspect is its computational complexity due to uncertainty in the parameter selection process . in this paper , we have discussed the effect of gw parameters on face recogniton performance and selection of gw for face recognition .",
    "we have introduced , step by step , the development process of gw selection method optimised for face recognition .",
    "these developments have demonstrated very encouraging results when investigated in a practical scenario as applied to the feret face database .",
    "work such as that reported here is important in demonstrating how pab and mi techniques can be used to improve the effectiveness , efficiency and reliability of face recognition in biometrics - related applications .",
    "this research is funded by the european commission fp6 marie curie action programme ( mest - ct-2005 - 021170 ) under the cmiag ( collaborative medical image analysis on grid ) project .",
    "k.  messer , j.  kittler , m.  sadeghi , m.  hamouz , a.  kostin , f.  cardinaux , s.  marcel , s.  bengio , c.  sanderson , n.  poh , y.  rondriguez , j.  czyz , l.  vandendorpe , c.  mccool , s.  lowther , s.  sridharan , v.  chandran , r.  p. palacios , e.  vidal , l.  bai , l.  shen , y.  wang , y.  h. chiang , h.  c. liu , y.  p. huang , a.  heinrichs , m.  miiller , a.  tewes , c.  v.  d. malsburg , r.  wiirtz , z.  g. wang , f.  xue , y.  ma , q.  yang , c.  fang , x.  q. ding , s.  lucey , r.  goss , and h.  schneiderman , `` face authentication test on the banca database , '' in _ proc . of international conference on pattern recognition _ , cambridge , uk , 2004 , pp . 523532 .",
    "laurenz wiskott , jean - marc fellous , norbert krger , and christoph von  der malsburg , `` face recognition by elastic bunch graph matching , '' in _ proc .",
    "7th intern .",
    "conf .  on computer analysis of images and patterns , caip97 , kiel _ , number 1296 .",
    "jonathon phillips , `` support vector machines applied to face recognition , '' in _ proceedings of the 1998 conference on advances in neural information processing systems ii _ , cambridge , ma , usa , 1999 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper , the problem of automatic gabor wavelet selection for face recognition is tackled by introducing an automatic algorithm based on parallel adaboosting method . incorporating mutual information into the algorithm leads to the selection procedure not only based on classification accuracy but also on efficiency . </S>",
    "<S> effective image features are selected by using properly chosen gabor wavelets optimised with parallel adaboost method and mutual information to get high recognition rates with low computational cost . </S>",
    "<S> experiments are conducted using the well - known feret face database . in proposed framework , </S>",
    "<S> memory and computation costs are reduced significantly and high classification accuracy is obtained . </S>"
  ]
}