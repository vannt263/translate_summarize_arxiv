{
  "article_text": [
    "recently there has been an increasing interest in the linear regression for interval - valued data .",
    "see diamond(1990 ) , krner and nther ( 1998 ) , gil et al .",
    "( 2002 , 2007 ) , manski and tamer ( 2002 ) , carvalho et al .",
    "( 2004 ) , billard ( 2007 ) , gonzlez - rodrguez et al .",
    "( 2007 ) , lima neto and de carvalho ( 2008 , 2010 ) , blanco - fernndez et al .",
    "( 2011 ) , cattaneo and wiencierz ( 2012 ) , for a partial list of references .",
    "existing models have been developed mainly in two directions . in the first direction ,",
    "separate point - valued linear regression models are fitted to the center and range ( or the lower and upper bounds ) , respectively , treating the intervals essentially as bivariate vectors .",
    "examples belonging to this category include the center method by billard and diday ( 2000 ) , the minmax method by billard and diday ( 2002 ) , the ( constrained ) center and range method by lima neto and de carvalho ( 2008 , 2010 ) , and the model m by blanco - fernndez et al .",
    "the second direction is to view the intervals as subsets in @xmath1 and study their linear relationship in the framework of random sets .",
    "investigations along this direction include diamond ( 1990 ) , gil et al .",
    "( 2001 , 2002 ) , gil et al . ( 2007 ) , gonzlez - rodrguez ( 2007 ) , and sun and li ( 2014 ) , among others . in this paper",
    ", we propose a new linear model for interval - valued data that aims at connecting the two directions and achieving improved flexibility .    to facilitate our presentation , let us give a brief introduction on the theoretical framework of random sets .",
    "let @xmath2 be a probability space .",
    "denote by @xmath3 or @xmath4 the collection of all non - empty compact subsets of @xmath5 . in the space @xmath4 , a linear structure is defined by minkowski addition and scalar multiplication , i.e. , @xmath6 @xmath7 and @xmath8 .",
    "a random compact set is a borel measurable function @xmath9 , @xmath4 being equipped with the borel @xmath10-algebra induced by the hausdorff metric . for each @xmath11 ,",
    "the function defined on the unit sphere @xmath12 : @xmath13 is called the support function of x. if @xmath14 is convex almost surely , then @xmath15 is called a random compact convex set .",
    "( see molchanov 2005 , p.21 , p.102 . )",
    "the collection of all compact convex subsets of @xmath5 is denoted by @xmath16 or @xmath17 . especially , when @xmath18 , @xmath19 contains all the non - empty bounded closed intervals in @xmath1 .",
    "a measurable function @xmath20 is called a random interval .",
    "much of the random sets theory has focused on compact convex sets ( see , e.g. , artstein and vitale ( 1975 ) , aumann ( 1965 ) , and lyashenko ( 1982 , 1983 ) ) . let @xmath21 be the space of support functions of all non - empty compact convex subsets in @xmath17 .",
    "then , @xmath21 is a banach space equipped with the @xmath22 metric @xmath23^{\\frac{1}{2}},\\ ] ] where @xmath24 is the normalized lebesgue measure on @xmath12 . according to various embedding theorems ( see rdstrm 1952 ; hrmander 1954 ) ,",
    "@xmath17 can be embedded isometrically into the banach space @xmath25 of continuous functions on @xmath12 , and @xmath21 is the image of @xmath26 into @xmath25 .",
    "therefore , @xmath27 , @xmath28 , defines an @xmath22 metric on @xmath26 .",
    "the central idea to constructing linear models in the random sets framework is to minimize the distance @xmath29 on the data , where @xmath30 are random intervals and @xmath31 is a linear function of @xmath32 in the sense of ( [ def : int - linear ] ) .",
    "such models have very nice mathematical interpretations , but the restriction to the space @xmath33 unfortunately results in a reduced flexibility from practical point of view .",
    "notice that @xmath34 this implies that the slope parameters of the corresponding linear model for the center ( c ) and range ( r ) must be the same in absolute value ( see , e.g. , gil et al .",
    "( 2002 ) and sun and li ( 2014 ) ) .",
    "such a restriction is usually relaxed in the models developed for the center and range separately .",
    "those models typically treat an interval as a vector in the euclidean space @xmath35 and minimize the euclidean distance @xmath36 on the data .",
    "however , this approach is slightly problematic in that once the intervals are represented by vectors in @xmath35 , they should be modeled as such , as opposed to being broken down to the centers and ranges separately . particularly , a linear model in @xmath35 in general takes on the form @xmath37 where @xmath15 is a @xmath38 coefficient matrix , @xmath39 is a @xmath40 intercept vector , and @xmath41 is a @xmath40 error vector .",
    "there is no reason to separate the two coordinates of @xmath32 by forcing @xmath15 to be diagonal .",
    "this problem makes the bivariate types of models hard to interpret both in @xmath33 and in @xmath35 .",
    "our main contribution in this paper is to generalize the bivariate types of models from the literature ( i.e. , models in the first research direction by the preceding discussion ) to the form ( [ model - origin ] ) , which is accomplished by embedding the space @xmath33 into @xmath35 , and more precisely , into the cone @xmath0 . as such",
    ", our proposed new linear model has generally improved flexibility over the existing models in both directions .",
    "it is also well interpretable in @xmath42 due to the embedding .",
    "we extend the univariate model to the multiple case and derive the matrix form of the general multivariate model .",
    "the least squares ( ls ) estimates for the model parameters are provided in matrix form , from which a series of properties are derived .",
    "furthermore , we give explicit analytical ls solutions for the positive parameters , which shed light on the behaviors of the ls estimators in connection with the positive restriction and the model validity .",
    "simulation studies are carried out that produce consistent results with our theoretical findings .",
    "finally , an application to a real data set is presented to demonstrate the applicability of our model .",
    "the rest of the paper is organized as follows .",
    "section 2 formally introduces our model and discusses the associated model properties .",
    "the ls estimators and their properties are presented in section 3 , followed by a rigorous discussion on the estimation of the positive parameters in section 4 .",
    "simulation studies are reported in section 5 , and the real data application is presented in section 6 .",
    "we give concluding remarks in section 7 .",
    "technical proofs are collected in the appendix .",
    "assume observing an i.i.d . random sample of paired intervals @xmath44 $ ] , @xmath45 $ ] , @xmath46 , where @xmath47 , @xmath48 and @xmath49 , @xmath50 are the lower and upper bounds of @xmath51 and @xmath52 , respectively .",
    "alternatively , the interval @xmath51 can also be represented by its center @xmath53 and range @xmath54 as @xmath55 and similarly for @xmath52 .",
    "the @xmath56-metric in the space @xmath43 is given by @xmath57 this suggests that the metric space @xmath58 can be embedded isometrically into the cone @xmath0 equipped with the euclidean metric .",
    "therefore , we consider each interval @xmath59\\in\\mathcal{k}_\\mathcal{c}(\\mathbb{r})$ ] to be represented by the point @xmath60 .    from the preceding discussion of embedding",
    ", we propose to construct a linear model in @xmath43 based on the affine operator in @xmath42 , i.e. , affine operator @xmath61 satisfying @xmath62 .",
    "obviously , such affine operators are represented by @xmath63 with @xmath64 and @xmath65 .",
    "this leads us to propose the following univariate linear model @xmath66 where @xmath67 are coefficients , and @xmath68 are i.i.d .",
    "zero mean random variables with variance @xmath69 , @xmath70 .",
    "the most important property of affine transformation is that it preserves collinearlity .",
    "this in the cone @xmath42 means that points lying on a ray are still on a ray after transformation .",
    "precisely , the operator @xmath71 maps the ray @xmath72 into another ray @xmath73t\\left(x\\right )      + \\gamma b+\\theta-\\frac{\\gamma\\left(a-1\\right)}{\\alpha+\\beta a}\\left(\\beta b+\\eta\\right),\\       t\\left(y\\right)\\geq t\\left(x\\right).\\ ] ] figure [ fig : affine ] gives an illustration of this effect .",
    "considering the equivalence of the point @xmath60 and the interval @xmath74\\in\\mathcal{k}_\\mathcal{c}(\\mathbb{r})$ ] , we define a collection of intervals @xmath75:i\\in i\\right\\}$ ] to be collinear if their representations @xmath76 in @xmath42 are on a ray .",
    "a collection of intervals @xmath75:i\\in i\\right\\}$ ] are said to be collinear if they satisfy the equation @xmath77 where @xmath78 if @xmath79 , @xmath80 if @xmath81 , and @xmath82 if @xmath83 .",
    "it is easily seen that equation ( [ def - collinear-1 ] ) can be equivalently expressed as @xmath84.\\ ] ] so , we can also define collinearity in terms of the center and range of the interval .",
    "the collinearity of a collection of intervals @xmath75:i\\in i\\right\\}$ ] is equivalently defined by @xmath85 where @xmath86 if @xmath87 , @xmath88 if @xmath89 , and @xmath90 if @xmath91 .    from these two definitions ,",
    "collinearity of intervals essentially means that the upper bound changes linearly with the lower bound , or equivalently , the range changes linearly with the center .",
    "for example , it is a common situation in practice that a larger center is associated with a wider range .",
    "when this relationship is linear , the corresponding intervals are considered collinear , and such a characteristic gets preserved under the operator @xmath71 .",
    "figure [ fig : collinearity ] provides a visualization of this property . in terms of modeling , if an interval - valued data @xmath92 $ ] , @xmath93 $ ] , @xmath46 , follows our model ( [ uni - mod-1])-([uni - mod-2 ] ) , then for @xmath94 s that are collinear , their associated @xmath95 s are also collinear .    , which is above the line @xmath96 in @xmath35 .",
    "the solid line is a ray @xmath97 in @xmath42 , and the dash - dotted line is its image by @xmath71 with the parameters @xmath98 . left : @xmath99 ; right : @xmath100.,title=\"fig:\",width=211,height=192 ] , which is above the line @xmath96 in @xmath35 . the solid line is a ray @xmath97 in @xmath42 , and the dash - dotted line is its image by @xmath71 with the parameters @xmath98 . left : @xmath99 ; right : @xmath100.,title=\"fig:\",width=211,height=192 ]    ) with ( top ) @xmath99 and ( bottom ) @xmath100 .",
    "the right two plots are their corresponding images by @xmath71 with parameters @xmath98.,title=\"fig:\",width=192,height=172 ] ) with ( top ) @xmath99 and ( bottom ) @xmath100 .",
    "the right two plots are their corresponding images by @xmath71 with parameters @xmath98.,title=\"fig:\",width=192,height=172 ] + ) with ( top ) @xmath99 and ( bottom ) @xmath100 .",
    "the right two plots are their corresponding images by @xmath71 with parameters @xmath98.,title=\"fig:\",width=192,height=172 ] ) with ( top ) @xmath99 and ( bottom ) @xmath100 .",
    "the right two plots are their corresponding images by @xmath71 with parameters @xmath98.,title=\"fig:\",width=192,height=172 ] +      as we mentioned in the introduction , our model has systematically improved flexibility over typical models in the literature . in this section ,",
    "we compare our univariate model ( [ uni - mod-1])-([uni - mod-2 ] ) to two popular models to gain more insight into this .",
    "consider the m model proposed by blanco - fernndez et al .",
    "( 2011 ) , and the constrained center and range method ( ccrm ) by lima neto and de carvalho ( 2010 ) .",
    "the m model is specified as @xmath101 where @xmath102 and @xmath103 are center and range of the interval - valued random error @xmath104 , respectively .",
    "@xmath102 is assume to be a centered random variable and @xmath103 is assumed to be a positive random variable . on the other hand , the model of ccrm",
    "is defined as @xmath105 where @xmath106 and @xmath107 are both centered random variables without any geometric interpretations .",
    "the two coefficients @xmath108 , @xmath109 in the range regression equation are both restricted to be positive to ensure the positiveness of @xmath110 .",
    "it is easy to see that these two models are essentially equivalent with @xmath111 , @xmath112 , and @xmath113 . rewriting equations ( [ ccrm-1])-([ccrm-2 ] ) in terms of the lower and upper bounds , the model of ccrm is equivalently represented as @xmath114 where @xmath115 , @xmath116 .",
    "this compared to our model ( [ uni - mod-1])-([uni - mod-2 ] ) is a reduced form with the restrictions @xmath117 .",
    "so our model has one extra degree of freedom , which will drastically expand the model flexibility .",
    "the ccrm is extended to the multiple case , from which the advantage of our general model introduced in the following gets multiplied .",
    "we will elaborate more on this in the simulation and real data application sections .",
    "consider the general case involving the outcome interval @xmath118 $ ] and @xmath119 interval - valued predictors @xmath120 $ ] , @xmath46 ; @xmath121 . to model @xmath52 by a linear transformation of @xmath122 , we extend the univariate model ( [ uni - mod-1])-([uni - mod-2 ] ) to the following form : @xmath123+\\eta+\\theta+\\epsilon_i^u,\\label{gen - mod-2}\\end{aligned}\\ ] ] with @xmath124 , @xmath125 , and @xmath126 , @xmath127 , for @xmath46 and @xmath121 .",
    "define @xmath128 and @xmath129 then equation ( [ gen - mod-1 ] ) is expressed as @xmath130 define @xmath131 and @xmath132 then equation ( [ gen - mod-2 ] ) is rewritten as @xmath133 to jointly express the model , define @xmath134 then , the general model ( [ gen - mod-1])-([gen - mod-2 ] ) , can be written in the matrix form @xmath135",
    "we define our least squares estimator @xmath136 of @xmath137 as the minimizer of the sum of squared lower and upper bound errors .",
    "namely , @xmath138\\right\\},\\ ] ] where @xmath139+\\eta+\\theta.\\label{pred_u}\\end{aligned}\\ ] ] this is equivalent to minimizing the sum of squared @xmath56-distance in the metric space @xmath140 .",
    "theorem [ thm : lse - matrix ] gives the explicit analytical expression of @xmath136 in matrix form .",
    "[ thm : lse - matrix ] consider the linear model ( [ gen - mod-1])-([gen - mod-2 ] ) , or equivalently , its matrix form ( [ mod - matrix ] ) .",
    "if the data matrix @xmath32 has full rank , then the least squares estimate @xmath136 defined in ( [ beta - lse ] ) is given by @xmath141    departing from its matrix form , a series of nice properties of @xmath136 follows immediately from the classical theory of linear models .",
    "( see , e.g. , seber ( 1997 ) . )",
    "we summarize them in the following corollaries .",
    "[ coro : unbias ] @xmath136 in theorem [ thm : lse - matrix ] is unbiased .",
    "[ coro : consis ] @xmath136 in theorem [ thm : lse - matrix ] is consistent .",
    "[ coro : cov ] the variance - covariance matrix of @xmath136 is @xmath142    [ coro : var - est ] an unbiased estimator of @xmath143 is given by @xmath144",
    "the model setting requires that @xmath124 , @xmath127 , @xmath121 .",
    "however , @xmath136 given in ( [ beta - matrix ] ) does not automatically guarantee these conditions . in this section ,",
    "we thoroughly discuss these positive restrictions for the least squares estimation and their implications on the model fitting .",
    "we begin by making a few notations and assumptions .",
    "denote by @xmath145 and @xmath146 the random variables from which @xmath147 and @xmath148 are samples , respectively , where @xmath149 and @xmath150 .",
    "denote by @xmath151 the sample covariance of @xmath152 and @xmath153 , @xmath154 .",
    "similarly , denote by @xmath155 the sample covariance of @xmath152 and @xmath156 , @xmath149 .",
    "[ assumption-1 ] the ranges of the predictors @xmath157 are mutually uncorrelated .",
    "[ assumption-2 ] the range of each predictor @xmath153 is empirically positively correlated with the range of the outcome @xmath156 , i.e. , @xmath158 for @xmath121 .    from the model specification ( [ gen - mod-1])-([gen - mod-2 ] ) , it is seen that @xmath159 where @xmath160 , @xmath70 .",
    "this immediately implies the following results , which give interpretations of the positive parameters @xmath161 and @xmath162 , @xmath121 .",
    "[ prop : gamma - theta ] assume model ( [ gen - mod-1])-([gen - mod-2 ] ) . then ,    1 .",
    "@xmath163 ; 2 .   @xmath164 .",
    "it can be shown that the positive parameters @xmath165 are indeed estimated independently from the rest of the parameters @xmath166 .",
    "we list their analytical ls solutions separately in the following theorem .",
    "[ thm : lse - positivity ] consider model ( [ gen - mod-1])-([gen - mod-2 ] ) .",
    "define the sample variance - covariance matrix of @xmath157 as @xmath167    _ { k , j=1}^{p}:=\\left[s_{k , j}\\right]_{k , j=1}^{p}.\\ ] ] additionally , denote by @xmath168 the vector that contains the sample covariances of @xmath152 and @xmath156 , @xmath149 , i.e. @xmath169_{k=1}^{p}:=\\left[s_k\\right]_{k=1}^{p}.\\ ] ] let @xmath170^t$ ] .",
    "then , the ls estimator @xmath171 is the solution of the linear system @xmath172 and the ls estimator of @xmath161 is @xmath173    from ( [ solu - gamma])-([solu - theta ] ) and in view of proposition [ prop : gamma - theta ] , we see that @xmath174 are essentially moment estimators of the underlying parameters , which are in fact strongly consistent .",
    "this also explains from another perspective the consistency shown in corollary [ coro : consis ] .",
    "in particular , an important interpretation of theorem [ thm : lse - positivity ] is that if at least one of the positive parameters is estimated to be negative for a large sample size , it indicates that the underlying true parameter is negative with a high probability , and forcing the parameter to be positive may result in possible biases .",
    "we give a simplified example in the following corollary to illustrate the implication of the positive restriction on @xmath175 .",
    "[ coro : model - bias ] consider the univariate model ( [ uni - mod-1])-([uni - mod-2 ] ) .",
    "let @xmath176 be the ls estimate and @xmath177 be any constrained ls estimate of @xmath178 such that @xmath179 .",
    "if @xmath180 , then @xmath181 where @xmath182 is the predicted value for @xmath110 based on the constrained ls estimates . the `` = '' holds if and only if @xmath183 .    for the univariate model ,",
    "if the ls estimate of @xmath178 is negative , forcing it to be positive will result in the model being worse than the constant model @xmath184 for the range .",
    "similar biases are expected for the multivariate cases too .",
    "therefore , it is not recommended that a constrained optimization algorithm always be used to ensure positive estimates , if some of the ls estimates @xmath185 are negative .",
    "at least a different model that accounts for the negative ls estimates should be considered as an alternative to the constrained linear model . in practice",
    ", it is often assumed that the predictors @xmath186 are independent .",
    "we provide a sufficient condition under which the ls estimates @xmath185 are positive with probability converging to one .",
    "[ coro : lse - positivity ] under assumption [ assumption-1 ] , @xmath187 with probability going to one if assumption [ assumption-2 ] is met",
    ".    intuitively , under the circumstance of independent predictors , model ( [ gen - mod-1])-([gen - mod-2 ] ) implies that @xmath188 consequently , for data that the model is appropriate for , the sample covariances @xmath189 are positive almost surely , which by theorem [ thm : lse - positivity ] is sufficient to ensure the positiveness of @xmath185 . otherwise , the @xmath190 s can be negative , but that is essentially because one or more of the predictors are negatively correlated with the outcome in range and hence the model is not appropriate .    from the preceding discussion ,",
    "if @xmath191 , it means that the model fits the linear structure of the data very well . at this point , if @xmath192 , it may not be worth forcing it to be positive using a constrained optimization , as that may bring unnecessary biases .",
    "the following theorem gives a guidance of judgment for such a situation .",
    "[ thm : r - positive ] assume model ( [ gen - mod-1])-([gen - mod-2 ] ) , or its equivalent matrix form ( [ mod - matrix ] ) .",
    "let @xmath193 be the model predicted value for @xmath110 .",
    "then , @xmath194    given a negative @xmath195 , it is possible to get negative predicts for @xmath156 .",
    "however , if the unexplained variance of @xmath156 is very small compared to the scale of @xmath196 , the chance to get a negative predict is tiny , and the rare cases of negative predict , if happened , can be rounded up to @xmath197 . in practice ,",
    "the unexplained variance of @xmath156 is estimated by @xmath198 , which is then compared to the scale of @xmath196 from the data to decide whether to stay with the negative unbiased ls estimate @xmath195 or resort to a constrained ls estimate .",
    "we present a simulation study to demonstrate the empirical performance of the ls estimates and compare our model to some peer models in the literature . in particular , we consider the following four model configurations : +    * i : p=1 , @xmath199 , @xmath200 , and @xmath201 with @xmath202 , @xmath46 ; * ii : p=1 , @xmath203 , @xmath200 , and @xmath201 with @xmath202 , @xmath46 ; * iii : p=3 , @xmath204 , @xmath205 , @xmath206 , and @xmath201 with @xmath202 , @xmath46 .",
    "the first two are univariate models , with positive and negative interval correlations between @xmath207 and @xmath208 , respectively . figure [ fig : sim - data-1 - 2 ] shows a plot of simulated data with @xmath209 observations from each of the two models .",
    "the third one is a 3-dimensional model , with @xmath207 and @xmath210 , @xmath206 , either positively or negatively correlated .",
    "a particular data with @xmath209 observations simulated from this model is visualized in figure [ fig : sim - data-3 ] , where it is seen that @xmath207 is positively correlated with both @xmath208 and @xmath211 , and negatively correlated with @xmath212 .",
    "+   +    to investigate the empirical performance of the ls estimation , we simulate 500 independent data from each of the three model configurations and calculate the ls estimates of the parameters for each simulated data .",
    "the results are summarized into table [ tab : sim ] . the mean relative error ( mre ) for the estimated coefficient matrix @xmath136 and variance of error @xmath213 , given a fixed sample size @xmath214 ,",
    "are defined as @xmath215 where @xmath216 denotes the euclidean norm , and @xmath217 respectively .",
    "we simulate observations for each @xmath51 independently , so assumption [ assumption-1 ] is automatically satisfied .",
    "assumption [ assumption-2 ] is checked before we compute the ls estimates for the parameters for each simulated data .",
    "if it is satisfied , then @xmath136 is calculated by ( [ beta - matrix ] ) , which according to corollary [ coro : lse - positivity ] produces positive @xmath218 , @xmath121 with probability going to one .",
    "if otherwise assumption [ assumption-2 ] is violated , a constrained optimization algorithm is employed to calculate @xmath136 , with the constraints that @xmath219 , @xmath121 and @xmath220 . for this paper , we have used the matlab function @xmath221 to compute the constrained ls estimates .",
    "consistent to our theorems , we see that the mre s for both @xmath136 and @xmath213 converge to @xmath197 as sample size increases . especially ,",
    "if the model really fits the data , which is the case for our simulation , the unconstrained ls estimate given in ( [ beta - matrix ] ) is sufficient , without the need of a constrained optimization algorithm , with probability going to one .",
    ".evaluation of the ls estimation for simulated data based on 500 independent repetitions . [ cols= \" < , > , > , > , > , > \" , ]     [ tab : real - data ]",
    "we have introduced a linear model for interval - valued data based on the affine operators in the cone @xmath0 . the new model is shown both theoretically and empirically to have improved flexibility over the existing models in the literature .",
    "we present the general model for multiple predictors in matrix form , from which the ls estimators of the model parameters are immediately derived with a series of nice properties from the classical theory of linear models .",
    "some parameters have positive constraints , which we show are closely related to the intrinsic structure of the model .",
    "therefore , it is not recommended to blindly force these parameters to be positive with a constrained optimization algorithm .",
    "instead , it is better to let the data speak for itself by the unconstrained ls estimates and decide later whether to employ a constrained optimization algorithm or resort to a different model , according to the guideline we have provided in the paper .",
    "differentiating @xmath224 $ ] with respect to @xmath225 , we obtain the system of equations @xmath226=0,\\label{eqn-5}\\\\    & & k=1,\\cdots , p.\\nonumber\\end{aligned}\\ ] ] equations ( [ eqn-1])-([eqn-2 ] ) yield @xmath227 meanwhile , equations ( [ eqn-3])-([eqn-4 ] ) yield @xmath228 plugging ( [ eqn-6 ] ) into ( [ eqn-7 ] ) , we obtain @xmath229\\nonumber\\\\    & = & \\frac{1}{n}\\sum_{i=1}^{n}x_{k , i}^ry_{k , i}^r-\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{k , i}^r\\right)\\left(\\frac{1}{n}\\sum_{}^{}y_i^r\\right),\\ \\",
    "k=1,\\cdots , p.\\label{eqn-8}\\end{aligned}\\ ] ] writing equations ( [ eqn-8 ] ) in matrix form yields ( [ solu - gamma ] ) .",
    "( [ solu - theta ] ) is obtained by plugging @xmath162 , @xmath121 in equation ( [ eqn-6 ] ) .      from theorem [ thm : lse - positivity ] ,",
    "@xmath230 , where @xmath231 and @xmath232 are the sample covariance of @xmath233 and @xmath156 , and the sample variance of @xmath233 , respectively .",
    "namely , @xmath234 let @xmath235 $ ] be the joint constrained ls estimates of @xmath236 $ ] such that @xmath237 .",
    "then , @xmath238 it follows that @xmath239 then the sum of squared errors for the prediction of @xmath156 based on the constrained ls estimates is calculated to be @xmath240 ^ 2\\\\    & = & \\sum_{i=1}^{n}\\left(y_i^r-\\overline{y^r}\\right)^2+\\tilde{\\gamma}^2\\sum_{i=1}^{n}\\left(x_i^r-\\overline{x^r}\\right)^2    -2\\tilde{\\gamma}\\sum_{i=1}^{n}\\left(y_i^r-\\overline{y^r}\\right)\\left(x_i^r-\\overline{x^r}\\right)\\\\    & = & \\sum_{i=1}^{n}\\left(y_i^r-\\overline{y^r}\\right)^2+\\tilde{\\gamma}^2ns_{1,1}-2\\tilde{\\gamma}ns_1.\\end{aligned}\\ ] ] therefore , in view of ( [ coro6:eqn-1])-([coro6:eqn-2 ] ) , @xmath241 and `` = '' holds if and only if @xmath183 .",
    "this completes the proof .      under assumption",
    "[ assumption-1 ] , @xmath242 and @xmath243^t\\ a.s .. \\ ] ] it follows that @xmath244^t\\ a.s .. \\ ] ] equations ( [ coro5:eqn-1 ] ) and ( [ coro5:eqn-2 ] ) together imply @xmath245 and therefore , @xmath246 hence , if @xmath158 , @xmath247      notice that @xmath248    & = & e\\left\\{e\\left[\\left(y_i^r-\\hat{y}_i^r\\right)\\hat{y}_i^r|x_i^r\\right]\\right\\}\\\\    & = & e\\left[\\hat{y}_i^re\\left(y_i^r-\\hat{y}_i^r|x_i^r\\right)\\right]\\\\    & = & 0.\\end{aligned}\\ ] ] therefore , @xmath249 this together with the fact that @xmath250 yields @xmath251 separately , @xmath252 by markov s inequality , we have @xmath253 ( [ thm3:eqn-3 ] ) together with ( [ thm3:eqn-1 ] ) and ( [ thm3:eqn-2 ] ) proves the desired result .          , diday , e. ( 2000 ) .",
    "regression analysis for interval - valued data . in _",
    "data analysis , classification and related methods _ , proceedings of the seventh conference of the international federation of classification societies ( ifcs00 ) ( pp .",
    "369374 ) .",
    "springer , belgium .    ,",
    "diday , e. ( 2002 ) .",
    "symbolic regression analysis . in _ classification , clustering and data analysis _ , proceedings of the eighth conference of the international federation of classification societies ( ifcs02 ) ( pp .",
    "281288 ) .",
    "springer , poland .",
    "blanco - fernndez , a. , corral , n. , gonzlez - rodrguez , g. ( 2011 ) .",
    "estimation of a flexible simple linear model for interval data based on set arithmetic .",
    "_ computational statistics and data analysis _",
    ", 55 , 25682578 .",
    "gil , m.a .",
    ", gonzlez - rodrguez , g. , colubi , a. , and montenegro , m. ( 2007 ) .",
    "testing linear independence in linear models with interval - valued data .",
    "_ computational statistics & data analysis _ , 51 , 30023015 .",
    "gonzlez - rodrguez , g. , blanco , a. , corral , n. , and colubi , a. ( 2007 ) .",
    "least squares estimation of linear regression models for convex compact random sets .",
    "_ advances in data analysis and classification _ , 1 , 6781 ."
  ],
  "abstract_text": [
    "<S> interval - valued linear regression has been investigated for some time . </S>",
    "<S> one of the critical issues is optimizing the balance between model flexibility and interpretability . </S>",
    "<S> this paper proposes a linear model for interval - valued data based on the affine operators in the cone @xmath0 . </S>",
    "<S> the resulting new model is shown to have improved flexibility over typical models in the literature , while maintaining a good interpretability . </S>",
    "<S> the least squares ( ls ) estimators of the model parameters are provided in a simple explicit form , which possesses a series of nice properties . </S>",
    "<S> further investigations into the ls estimators shed light on the positive restrictions of a subset of the parameters and their implications on the model validity . a simulation study is presented that supports the theoretical findings . </S>",
    "<S> an application to a real data set is also provided to demonstrate the applicability of our model . </S>"
  ]
}