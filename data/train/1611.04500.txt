{
  "article_text": [
    "the holy grail of representation learning is to discover and exploit the invariances of the data .",
    "performing this task within the deep learning framework  @xcite can benefit from its immense capacity and trainability .",
    "this has motivated many recent works on finding the invariances in data or defining models with known invariances .    in the following [ sec : main ] defines a minimal notion of structure and invariance of a function with respect to this structure .",
    "we then consider an abstract neuron as our function class of interest and derive some of the familiar convolution layers using this formulation .",
    "[ sec : composition ] studies composition of new structures using two definitions of graph product and pooling .",
    "[ sec : ordering ] defines a partial ordering on structures with the data and feature dimension at extremes of this ordering .",
    "this partial ordering also suggests an incremental approach for handling product structures .",
    "[ sec : acyclic ] briefly studies acyclic structures , where under some conditions , a forward pass in the deep network performs ancestral sampling in a deep directed graphical model , with recurrent networks as a special case .",
    "[ sec : set ] studies application of these ideas to the `` set '' structure , one of the most basic structures with a wide range of applications that have so far remained unexplored in the context of deep learning .",
    "in particular , we consider application of set - invariant layers in detecting set outliers , and performing point - cloud classification where particles are treated as set members .",
    "application of set - invariant layer in a semi - supervised setting is discussed in [ sec : semi ] .",
    "[ sec : discussion ] reviews the related work and puts our contribution in perspective .",
    "given a set @xmath0 , with @xmath1 , a ( binary ) relation @xmath2 is a collection of ordered pairs @xmath3 of members of @xmath4 .",
    "we define the structure @xmath5 , as a collection of non - overlapping relations on @xmath4 .",
    "we will drop the subscript @xmath4 whenever the set @xmath4 is evident from the context .",
    "we will use this minimal notion of structure to define simple rules of parameter - sharing that respect invariances of the structure .    for this purpose",
    "we need to define functions on the set @xmath4 .",
    "we use the subscript @xmath6 to denote dependencies of @xmath7 in @xmath8 .",
    "note that @xmath9 is a set and our abuse of notation is for simplicity .",
    "the structural dependencies of @xmath7 in @xmath10 are accordingly defined as the union of its relational dependencies @xmath11 .",
    "we are interested in the functions @xmath12 associated with @xmath13 .",
    "these functions are defined over the domain of each @xmath14  that is for each @xmath13 , @xmath15 . here",
    ", @xmath16 has no particular ordering information .",
    "let the tuple @xmath17 denote @xmath4 with some fixed ordering .",
    "sub - tuples @xmath18 and @xmath19 inherit the same ordering .",
    "@xmath20 denotes the symmetric group of all permutations of size @xmath21 .",
    "the action @xmath22 of each group member @xmath23 is a re - ordering of @xmath19 .",
    "each @xmath24 identifies a sub - group @xmath25 consisting of all permutations of elements in @xmath9 .",
    "we say a function @xmath12 is invariant to relation @xmath8 iff @xmath26 for @xmath27 and it is invariant to the structure @xmath10 if the same identity holds for @xmath28  _ i.e. _ ,  it is invariant to all its relations @xmath29 .",
    "since we often work with functions @xmath12 that are invariant to @xmath10 , in the rest of the paper we will use @xmath30 to denote @xmath31 .",
    "we consider a sub - class of functions in the following form @xmath32 where @xmath33 is an associative and commutative operation such as maximization or summation , @xmath34 are parameters shared within a relation and @xmath35 is for example a sigmoid .",
    "note that the total number of parameters required by functions in @xmath36 depends only on @xmath37 rather than @xmath38 .",
    "[ prop:1 ] @xmath39 is invariant to @xmath10 .",
    "our definition of structure @xmath10 can be visualized as an edge - colored directed graph ( di - graph ) , where each color identifies a relation @xmath8 over the vertex - set @xmath4 .",
    "we use this representation in our examples ; see [ fig : structures ] .    .",
    "_ _ ]    [ ex : conv2d ] ( a : left ) shows the colored di - graph representation of @xmath10 for a single input / output channel in a * 2d convolution * with @xmath40 kernel .",
    "@xmath10 consists of 9 relations corresponding to 9 styles in connections of [ fig : conv2d_set](a : left ) . here",
    ", @xmath9 for each variable contains a single member and @xmath41 is the collection of these 9 relations .",
    "if we wished our convolution operation to be invariant to up - down and left - right flip , we could share the corresponding parameters , in which case @xmath42 .",
    "[ ex : graph_conv ] in * graph convolution * all graph - edges are considered equivalent . here",
    ", the structure contains two types of relation relations @xmath43 , where @xmath44 is the edge - set .",
    "( a ) shows the structure for an undirected graph . using @xmath45 , the parameter sharing function of [ eq : f ] becomes @xmath46 .",
    "if we assume an ordering on @xmath4 to get @xmath17 , we can rewrite this expression for @xmath17 as @xmath47 where @xmath48 is the normalized binary adjacency matrix , @xmath49 is the identity matrix and @xmath50 . here",
    "@xmath12 and @xmath51 are applied element - wise .",
    "this is similar in form ( aside from normalization ) to the graph - convolution in @xcite , with single input / output channels . to see how multiple channels affect the structure ,",
    "we should first define composition of structures .",
    "deep learning models seldom work with a single structure .",
    "as we will see in this section , even the classic multi - layer perceptron assumes a composite structure .",
    "we consider special forms of composition , where different structures are defined on `` dimensions '' of the set @xmath4 , where the notion of dimension comes from the cartesian product .",
    "more specifically , we start with sets @xmath52 , each having their own structure @xmath53 and define a new structure on the product set @xmath54",
    ". examples of such compositions are when we have a data - set of feature - vectors , sequence of sets or a graph of graphs . here",
    ", we introduce two families of composition :    * cardinal composition . *",
    "let @xmath55 and @xmath56 be structures over the set @xmath57 with @xmath58 and @xmath59 with @xmath60 respectively .",
    "their cardinal composition @xmath61 defines a new structure over the cartesian product set @xmath62 . for this",
    "we first define the _ product of two relations _ as the tensor - product of their di - graph representation : @xmath63    we then define the cardinal ( or tensor ) product of two structures as the product of all pairs of their relations : @xmath64    * cartesian composition . *",
    "we first define the _ extension _ of relation @xmath65 to the cartesian product domain @xmath66 @xmath67 where we define new pairs for all the combinations of pairs in @xmath65 and members of the new domain @xmath59 .    given @xmath55 and @xmath56 , the cardinal composition @xmath68 is defined as the union of extension of their member relations @xmath69    a structure that is often present is the * dataset structure*. multiple instances in the dataset can be thought of as the product of a dataset structure , @xmath70 with other structures .",
    "it is easy to check that this product , be it cartesian or cardinal , simply replicates the rest of structure for each data - instance and the corresponding parameter - sharing scheme shares all the model parameters across the dataset , where the number of parameters does not grow with the size of the dataset .",
    "the opposite extreme of structure is the * unstructured data features * that assume @xmath71 , and the resulting parameter - sharing scheme , the fully connected layer , shares no parameter .    to highlight the difference between cartesian and cardinal composition in a familiar setting , let us proceed with the example of 2d convolution as the product of 1d convolutions . here , the structures @xmath72 and @xmath73 , each have 3 relations as shown in [ fig : conv2d_set](a )",
    "the figure then compares the resulting cartesian and cardinal product of @xmath55 and @xmath56 .",
    "it is evident that the cardinal product defines relations based on all combination of original relations , while the cartesian product , considers their union  therefore , it often reduces the total number of relations and increases parameter - sharing .",
    "the multiplicity of channels / feature - maps , when for example using convolution , can be studied at both layers below and above . from the lower layer s perspective",
    ", we are simply defining multiple functions that are invariant to the same structure . from the point of view of the layer above , however , multiple _ channels _ with the same structure are often considered unstructured , similar to the fully connected layer  _ i.e. _ ,  @xmath71 .",
    "this structure is then composed with the main structure using the cardinal product .",
    "\\(b ) shows the structure of the output of a set - invariant layer with set - size of 3 that uses 2 output channel .",
    "the resulting structure has @xmath74 parameters ( 2 parameters of the set - invariant layer is multiplied by the number of channels since we are using cardinal product ) .",
    "to denote layer @xmath75 , we use super - script in parentheses . using",
    "this notation @xmath76 represents the sets of variables at layer @xmath77 as a function of previous layer , where @xmath78 is the input .",
    "we define pooling as the application of a commutative and associative binary operation such as maximization , averaging , product or summation over @xmath79 : @xmath80    since @xmath33 is invariant to the order of its summands , for @xmath81 , both invariant to @xmath82 , @xmath83 is invariant to @xmath82 as well .",
    "it follows that _ pooling @xmath84 preserves invariances _ of @xmath85 with respect to the structure @xmath82 of that layer .",
    "the commonly used max - pooling in images and cluster - pooling ( a.k.a .",
    "coarsening ) for graph structure fit within this definition .",
    "a useful partitioning of pooling variables are the ones that are consistent with a product structure .",
    "let @xmath86 be the product of @xmath57 and @xmath59 .",
    "partitioning of @xmath4 wrt @xmath57 is defined as @xmath87  that is we have one set per each member of @xmath57 and the structure @xmath55 is preserved .",
    "this type of pooling is specially useful in semi - supervised learning , when the output of the network retains a structure @xmath88 , while pooling over other structures .     for more examples . _ ]",
    "in practical settings , it may be technically challenging to build network layers that perform weight - sharing for invariance wrt a complex product structure . ideally , we want to reuse weight - sharing layers that are invariant to individual elements of the composition , and at the time retain invariance to the product structure",
    ".    however , maintaining invariance is often not enough  that is we need the parameter - sharing scheme to be sensitive ( not invariant ) to permutations over @xmath17 that are not induced by the given structure @xmath10 .",
    "we call such functions ( or equivalently , parameter - sharing scheme ) _ minimally invariant _ to @xmath10",
    ". the functions of the form [ eq : f ] are minimally invariant to @xmath10 . we formalize this notion using a partial ordering of all structures .",
    "we define a partial ordering , such that if the function @xmath89 is invariant to @xmath90 , it is also invariant to @xmath91 .",
    "we say @xmath92 iff we can construct @xmath90 starting from @xmath10 and using arbitrary repetition of the following operations ( that strictly increase invariances of @xmath10 )    1 .",
    "merging of @xmath93 2 .   shrinking a relation @xmath94 , which removes members of some @xmath95 .    a simple inspection of [ fig : structures](c , d ) wrt the operations above shows that dataset structure @xmath96 and fully connected structure @xmath97 are the * greatest and the least elements of our partial ordering *  that is @xmath98 .",
    "therefore , in a sense our exploitation of structure in deep models appears in between two dimensions of data in a traditional multi - layer perceptron .",
    "now we extend ordering of structures to their product .",
    "[ claim:1 ] composition of structures preserves their partial ordering @xmath99      from our definition of partial ordering , it follows that a function @xmath12 is minimally invariant to @xmath10 if it is not invariant to any @xmath100 . due to `` partial '' ordering",
    ", @xmath12 could be minimally invariant to several distinct structures .",
    "a sensible approach to handling one structure at a time , is to maintain invariance in all layers , while ensuring that at least one layer is minimally invariant to each component of the product structure .",
    "the key to maintaining invariance ( which is not minimal ) is to use claim  [ claim:1 ] : since @xmath101 , from the claim  [ claim:1 ] it follows that any function @xmath12 that is invariant to @xmath102 is also invariant to @xmath103 .",
    "since @xmath104 , the implication is that we can treat a subset of structural dimensions the same way that we handle dataset , while maintaining invariance to the ignored structure .",
    "however , this comes at the cost , demonstrated in the following example .    given a decomposition of @xmath4 to @xmath66",
    ", the fully connected structure can be decomposed accordingly @xmath105 . by handling each of these structures at their own layer , we maintain the invariances that are provided by @xmath106 ( which is basically no invariance ) .",
    "however , our model does not capture the inter - dependence between the variables in @xmath57 and @xmath59 .",
    "this suggests caution when using this shortcut .",
    "our use of this trick in the experiments is minimal  _",
    "i.e. _ ,  we use this to handle the convolution / grid structure , without worrying about the `` set '' structure at first few layers of our model for face outlier detection .",
    "in this paper , we only focus our experimental results on the set data - structure .",
    "this simple structure alone is applicable in many settings including distribution regression and distribution classification which have become popular recently @xcite .",
    "they have proven to be very useful in many applied problems from computer vision @xcite via neuroscience @xcite and robotics @xcite to cosmology @xcite .",
    "our approach to ( semi-)supervised learning with sets also generalizes various settings in multi - instance learning  @xcite .    in the following , after introducing the set - invariant layer in [ sec : parameter_sharing ] , we explore several novel applications . in particular for the vision task we perform outlier detection on celeba face dataset in [ sec : celeb ]",
    ". studies an important application of sets in representing low - dimensional point - clouds .",
    "we show that deep networks can successfully classify objects using their point - cloud representation .",
    "studies application of semi - supervised learning with set structure in cosmology for prediction of galaxy red - shift using galaxy - clustering information .",
    "\\(b ) shows an example of a set data - structure @xmath107 . the corresponding weight - sharing scheme applied to [ eq : f ]",
    "has exactly two parameters can be different from @xmath33 in the set layer .",
    "this does not alter the invariance of @xmath12 to @xmath108 . ]",
    "@xmath109 here , @xmath110 and @xmath12 is associated with a particular member @xmath13 .",
    "the two parameters ( @xmath111 ) of @xmath12 account for two relations in @xmath108 .",
    "let us assume we have @xmath112 input channels , with a set of size @xmath113 and @xmath114 output channels . using the matrix form @xmath115 for the input ,",
    "@xmath116 as parameters , and assuming @xmath45 and @xmath117 , the output is @xmath118 , where @xmath119 is the unit column vector of size @xmath113 .",
    "we found using @xmath120 to be slightly advantageous in some settings . ignoring the @xmath121 in [ eq : set_func ] , this choice of operations gives @xmath122 where @xmath123 is the row vector of max - values over the rows of @xmath17 and @xmath124 is an additional bias parameter . in practice",
    "we can marginally increase generalization performance by factoring the parameters and using @xmath125 with multiple input / output channels , the complexity of this layer for each instance is @xmath126 . subtracting the mean or max over the set also reduces the internal covariate shift  @xcite and we observe that for deep networks ( even using tanh activation ) , batch - normalization is not required .",
    "the objective here is for the deep model to find the anomalous face in each set , simply by observing examples and without any access to the attribute values .",
    "celeba dataset  @xcite contains 202,599 face images , each annotated with 40 boolean attributes .",
    "we use @xmath127 stamps and using these attributes we build 18,000 sets , each containing @xmath128 images ( on the training set ) as follows : after randomly selecting two attributes , we draw 15 images where those attributes are present and a single image where both attributes are absent . using a similar procedure",
    "we build sets on the test images .",
    "no individual person s face appears in both train and test sets .",
    "our deep neural network consists of 9 2d - convolution and max - pooling layers followed by 3 set - invariant layers ( of the type given by [ eq : set_factored ] ) and a softmax layer that assigns a probability value to each set member ( note that one could identify arbitrary number of outliers using a sigmoid activation at the output . ) in the initial convolution and pooling layers we use the trick of [ sec : one_at_a_time ] , ignoring the set structure .",
    "our trained algorithm successfully finds the anomalous face in _ 75% of test sets_. visually inspecting these instances suggests that the task is non - trivial even for humans ; see [ fig : faces ]",
    ". for details of the network model and more identification examples see [ sec : details ] .",
    "a low - dimensional point - cloud is a set of low - dimensional vectors .",
    "this type of data is frequently encountered in various applications from robotics and vision to cosmology . in these applications point - cloud data",
    "is often converted to voxel or mesh representation at a preprocessing step  ( _ e.g. _ ,   * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "since the output of many range sensors such as lidar  which are extensively used in applications such as autonomous vehicles  is in the form of point - cloud , direct application of deep learning methods to point - cloud is highly desirable .",
    "moreover , when working with point - clouds rather than voxelized 3d objects , it is easy to apply transformations such as rotation and translation as differentiable layers , and achieve this type of continuous invariances , that our framework can not formulate .    here",
    ", we show that treating the point - cloud data as a set , we can use the set - invariant layer to classify point - cloud representation of a subset of shapenet objects  @xcite , called modelnet40  @xcite .",
    "this subset consists of 3d representation of 9,843 training and 2,468 test instances belonging to 40 classes of objects ; see  [ fig : modelnet_dataset ] .",
    "we produce point - clouds with 1000 particles each ( @xmath129-coordinates ) from the mesh representation of objects using the point - cloud - library s sampling routine  @xcite .",
    "each set is normalized by the initial layer of the deep network to have zero mean and unit variance .",
    "additionally we experiment with the k - nearest neighbor graph of each point - cloud and report the results using graph - convolution ( see [ sec : details ] for model details . )",
    "[ table : point_cloud ] compares our method against the competition .",
    "note that we achieve our accuracy using @xmath130 dimensional representation of each object , which is much smaller than most other methods .",
    "all other techniques use either voxelization or multiple view of the 3d object for classification .",
    "we see that reducing the number of particles to only 100 still produces comparatively good results .",
    "using graph - convolution is computationally more challenging and produces inferior results in this setting .",
    "the results using 5000 particles is also invariant to small changes in scale and rotation around the @xmath131-axis ( see [ sec : details ] for details ) .",
    "* features . *",
    "to visualize the set - invariant layers , we used adamax  @xcite to locate 1000 particle coordinates maximizing the activation of each unit . and @xmath132 respectively .",
    "we optimized the input in @xmath133 iterations .",
    "the results of [ fig : activations ] are limited to instances where tanh units were successfully activated . since the input at the first layer of our deep network is normalized to have a zero mean and unit standard deviation",
    ", we do not need to constrain the input while maximizing unit s activation . ] activating the tanh units beyond the second layer proved to be difficult .",
    "shows the particle - cloud - features learned at the first and second layers of our deep network .",
    "we observed that the first layer learns simple localized ( often cubic ) point - clouds at different @xmath134 locations , while the second layer learns more complex surfaces with different scales and orientations .",
    "several works have considered different approaches to deep - learning on graphs in the past  ( _ e.g. _ ,   * ? ? ? * ; * ? ? ?",
    "graph convolution using the spectrum of graph was initially proposed by @xcite and further developed in @xcite .",
    "our discussion of parameter - sharing across layers in acyclic structures ( in [ sec : acyclic ] ) is closely related to recursive neural networks ( _ e.g. _ ,   * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) that use back - propagation through structure  @xcite .",
    "@xcite apply similar techniques for structure classification , where a topological ordering in cyclic structures is used to construct a directed acyclic graph .",
    "several works have studied invariance ( and equivariance ) in deep models using harmonic analysis and group theory .",
    "scattering convolution networks  @xcite , group - convolution @xcite and symmetry networks  @xcite are some examples that provide a general treatment of the topic . however , to our knowledge , these techniques do not directly extend to discrete structures such as graphs ( unless the graph is a lattice ) .",
    "other works in this area explicitly address known invariances through application of transformations that do not vary the output ( _ e.g. _ ,   * ? ? ?",
    "* ; * ? ? ?",
    "contrasting our formulation with previous work on structure and symmetry in neural networks suggests several benefits and a drawback : our approach is minimal in its use of machinery , yet quite general in application , and the notion of structure as defined here , has enough capacity to accommodate complex composite models .",
    "moreover , any structure directly translates to a parameter - sharing scheme which has the benefit of being fast and easy to implement .",
    "however , this approach is also limited by the invariances achieveable through parameter - sharing .    in the future",
    ", we would like to apply parameter - sharing schemes discussed here to real - word composite structures .",
    "extending these ideas to deep generative models is also a direction we would like to explore in the future .",
    "of proposition  [ prop:1 ] + for this it is enough to show that for a @xmath135 in @xmath10 , @xmath136 , when @xmath137 .",
    "@xmath138    of claim  [ claim:1 ] + for both cardinal and cartesian products we show that give @xmath139 may be constructed from @xmath140 using the operations above , we can also produce the product structure @xmath141 or @xmath142 using the same two operations .",
    "let us start with the _ cardinal product _ :",
    "@xmath143 where @xmath144 is in common on both sides of the rhs of [ eq : order ] and therefore @xmath145 is in common as well . however , our assumption is that @xmath146 is constructed from @xmath147 using by merge and shrinkage .",
    "we replace the merge and shrink operations for relations to their product form    1 .",
    "merging of products @xmath148 2 .",
    "shrinking of a product of relations @xmath149 for some @xmath150 .    by replacing the merge and",
    "shrink operations that derive @xmath139 from @xmath151 , we are guaranteed to obtain @xmath152 starting from @xmath153 .    for _ cartesian product _",
    "@xmath154 note that the second term @xmath155 is identical in both @xmath142 and @xmath156 .",
    "therefore , we only need to show that @xmath157 recall the definition of extension : @xmath158 our assumption is that @xmath146 is constructed from @xmath147 using by merge and shrink operations . we replace the merge and shrink with those operations applied to the extension of relations wrt @xmath159 as follows    1 .",
    "merging of extensions @xmath160 2 .",
    "expansion of extensions @xmath161 .    by replacing the merge and",
    "shrink operations that derive @xmath139 from @xmath151 , we are guaranteed to obtain @xmath162 starting from @xmath163 .",
    "let @xmath164 be the directed graph associated with @xmath10 that merges all its relations .",
    "when @xmath165 has no directed cycles ( _ i.e. _ ,  it is a dag ) , we call the structure @xmath10 an acyclic structure .",
    "this structure consistently identifies the inter - dependence of variables .",
    "consistency is in the sense that the value of @xmath13 does not implicitly depend on itself through a loop , and starting from the leaves , one could sample the value of each node once all its children have been assigned a value . this procedure",
    "is called ancestral sampling . here ,",
    "we show that with a simple inter - layer parameter - sharing scheme , a deep model implements ancestral sampling in a deep directed graphical model .    if for a given @xmath10 :    1 .",
    "@xmath166 is a dag of depth @xmath167 2 .",
    "the number of layers @xmath168 ( in @xmath169 ) , is not smaller than @xmath167 3 .",
    "layers share the same structure @xmath10 ( up to an isomorphism of layers @xmath170 ) and parameters @xmath171    then a forward pass through the network is equivalent to ancestral sampling .    to see why this is true",
    "let us follow a forward pass through this network , where at each iteration @xmath172 , we calculate the output of functions at layer @xmath173 and update their ( deterministic ) sample value @xmath174 .",
    "since the leaf nodes do not depend on any other node their value does not change @xmath175 .",
    "similarly the parents of these leaf nodes are fixed at the second level _i.e .",
    "_ ,  for these nodes @xmath176 and so on . the value of the rood nodes is set at layer @xmath167 and does not change from here on .",
    "this also suggests that for saving computation , we could sample a node at level @xmath177 of the dag , only at layer @xmath178 .",
    "we call this computation scheme in a multi - layer perceptron that satisfies the conditions above , _ minimal ancestral sampling_. if the structure does not reduce to a dag , we could still use parameter - sharing between layers , to assimilate @xmath168 iterations of gibbs sampling .",
    "vanilla * recurrent neural network * , effectively performs minimal ancestral sampling , where the structure is the cardinal product of a directed sequence structure @xmath179 and a fully - connected structure @xmath180 . shows two different products of @xmath181 and @xmath182 , it is not hard to notice that the cartesian product structure is not an acyclic structure .",
    "in this section we treat galaxy clusters as sets and use set - invariant layers to produce estimates of the galaxy red - shift ( distance ) using the observation of u , g , r , i and z photometric bands  @xcite , measurements error bars , location in the sky and the probability of being the cluster center ( for a total of @xmath183 features per galaxy ) .",
    "this information is provided by redmapper galaxy cluster catalog  @xcite which contains photometric readings for 26,111 red galaxy clusters . each cluster in this catalog has between @xmath184 galaxies ; see [ fig : cluster_results](a ) for distribution of cluster sizes .",
    "having access to accurate spectroscopic red - shift estimates for a subset of these galaxies , we would like to use clustering information to produce better estimates of the red - shift using photometric data .",
    "[ fig : cluster_results](b ) reports the distribution of available spectroscopic red - shift estimates per cluster .",
    "we randomly split the data into 90% training and 10% test instances ( of clusters ) and define use the following architecture for semi - supervised learning using available spectroscopic redshift estimates and cluster information .    for this",
    "we use four set - invariant layers with 128 , 128 , 128 and 1 output channels respectively , where the output of the last layer is used as red - shift estimate .",
    "the squared loss of the prediction for available spectroscopic readings is minimized using mini - batches of size 128 with adam .",
    "all layers except for the last layer use tanh units and ( simultaneous ) dropout with 50% dropout rate .",
    "[ fig : cluster_results](c ) shows the agreement of predictions with spectroscopic readings on the test - set .",
    "the average absolute error of the red - shift estimates is @xmath185 in this setting .",
    "we repeat this experiment , replacing the set layers with fully connected layers and only using the individual galaxies with available spectroscopic estimate for training .",
    "this achieves a mean absolute error of @xmath186 , suggesting that using clustering information indeed improves the performance of redshift estimation .",
    "in the following , all our implementations use tensorflow  @xcite .",
    "our model has 9 convolution layers with 3x3 kernels .",
    "the model has convolution layers with @xmath187 feature - maps followed by max - pooling followed by 2d convolution layers with @xmath188 feature - maps followed by another max - pooling layer .",
    "the final set of convolution layers have @xmath189 feature - maps , followed by a max - pooling layer with pool - size of @xmath190 that reduces the output dimension to @xmath191 , where the set - size @xmath192 .",
    "this is then forwarded to three set - invariant layers with @xmath193 and @xmath194 output channels .",
    "the output of final layer is fed to the softmax , to identify the outlier .",
    "we use exponential linear units  @xcite , drop out with 20% dropout rate at convolutional layers and 50% dropout rate at the first two set layers . when applied to set layers , the selected feature ( channel )",
    "is simultaneously dropped in all the set members of that particular set .",
    "we use adam  @xcite for optimization and use batch - normalization only in the convolutional layers .",
    "we use mini - batches of @xmath195 sets , for a total of @xmath196 images per batch .",
    "* set convolution .",
    "* we use a network comprising of 3 set - invariant layers with 256 channels followed by max - pooling over the set structure . the resulting vector representation of the set",
    "is then fed to a fully connected layer with 256 units followed by a 40-way softmax unit .",
    "we use tanh activation at all layers and dropout on the layers after set - max - pooling ( _ i.e. _ ,  two dropout operations ) with 50% dropout rate . applying dropout do set - invariant layers for point - cloud data deteriorated the performance .",
    "we observed that using different types of set - invariant layers ( see [ sec : parameter_sharing ] ) and as few as 64 channels for set - invariant layers changes the result by less than @xmath197 in classification accuracy .    for",
    "the setting with 5000 particles , we increase the number of units to 512 in all layers and randomly rotate the input around the @xmath131-axis .",
    "we also randomly scale the point - cloud by @xmath198 . for this",
    "setting only , we use adamax  @xcite instead of adam and reduce learning rate from @xmath199 to @xmath200 .",
    "* graph convolution .",
    "* for each point - cloud instance with 1000 particles , we build a sparse k - nearest neighbor graph and use the three point coordinates as input features .",
    "we normalized all graphs at the preprocessing step . for direct comparison with set - invariant layer",
    ", we use the exact architecture of 3 graph - convolution layer followed by set - pooling ( global graph pooling ) and dense layer with 256 units .",
    "we use exponential linear activation function instead of tanh as it performs better for graphs . due to over - fitting",
    ", we use a heavy dropout of 50% after graph - convolution and dense layers .",
    "similar to dropout for sets , all the randomly selected features are simultaneously dropped across the graph nodes .",
    "the we use a mini - batch size of 64 and adam for optimization where the learning rate is .001 ( the same as that of set - invariant counter - part ) .    despite our efficient sparse implementation using tensorflow ,",
    "graph - convolution is significantly slower than set - convolution .",
    "this prevented a thorough search for hyper - parameters and it is quite possible that better hyper - parameter tuning would improve the results that we report here ."
  ],
  "abstract_text": [
    "<S> we study a simple notion of structural invariance that readily suggests a parameter - sharing scheme in deep neural networks . </S>",
    "<S> in particular , we define structure as a collection of relations , and derive graph convolution and recurrent neural networks as special cases . </S>",
    "<S> we study composition of basic structures in defining models that are invariant to more complex `` product '' structures such as graph of graphs , sets of images or sequence of sets . for demonstration , </S>",
    "<S> our experimental results are focused on the setting where the discrete structure of interest is a set . </S>",
    "<S> we present results on several novel and non - trivial problems on sets , including point - cloud classification , set outlier detection and semi - supervised learning using clustering information . </S>"
  ]
}