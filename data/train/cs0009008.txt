{
  "article_text": [
    "text chunking is a useful preprocessing step for parsing .",
    "there has been a large interest in recognizing non - overlapping noun phrases ( ramshaw and marcus and follow - up papers ) but relatively little has been written about identifying phrases of other syntactic categories .",
    "the conll-2000 shared task attempts to fill this gap .",
    "text chunking consists of dividing a text into phrases in such a way that syntactically related words become member of the same phrase . these phrases are non - overlapping which means that one word can only be a member of one chunk .",
    "here is an example sentence :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ @xmath0 he @xmath1 $ ] @xmath2 reckons @xmath1 $ ] @xmath0 the current account deficit @xmath1 $ ] @xmath2 will narrow @xmath1 $ ] + @xmath3 to @xmath1 $ ] @xmath0 only @xmath4 1.8 billion @xmath1 $ ] + @xmath3 in @xmath1 $ ] @xmath0 september @xmath1 $ ] . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    chunks have been represented as groups of words between square brackets .",
    "a tag next to the open bracket denotes the type of the chunk .",
    "as far as we know , there are no annotated corpora available which contain specific information about dividing sentences into chunks of words of arbitrary types .",
    "we have chosen to work with a corpus with parse information , the wall street journal ( wsj ) part of the penn treebank ii corpus @xcite , and to extract chunk information from the parse trees in this corpus .",
    "we will give a global description of the various chunk types in the next section .",
    "the chunk types are based on the syntactic category part ( i.e.  without function tag ) of the bracket label in the treebank ( cf .",
    "bies p.35 ) .",
    "roughly , a chunk contains everything to the left of and including the syntactic head of the constituent of the same name . some treebank constituents do not have related chunks .",
    "the head of s ( simple declarative clause ) for example is normally thought to be the verb , but as the verb is already part of the vp chunk , no s chunk exists in our example sentence .    besides the head",
    ", a chunk also contains premodifiers ( like determiners and adjectives in nps ) , but no postmodifiers or arguments .",
    "this is why the pp chunk only contains the preposition , and not the argument np , and the sbar chunk consists of only the complementizer .",
    "there are several difficulties when converting trees into chunks . in the most simple case ,",
    "a chunk is just a syntactic constituent without any further embedded constituents , like the nps in our examples . in some cases ,",
    "the chunk contains only what is left after other chunks have been removed from the constituent , cf . ``",
    "( vp loves ( np mary ) ) '' above , or adjps and pps below .",
    "we will discuss some special cases during the following description of the individual chunk types .",
    "[ [ np ] ] np ~~    our np chunks are very similar to the ones of ramshaw and marcus .",
    "specifically , possessive np constructions are split in front of the possessive marker ( e.g. @xmath0  eastern airlines  ] @xmath0   creditors  ] ) and the handling of coordinated nps follows the treebank annotators .",
    "however , as ramshaw and marcus do not describe the details of their conversion algorithm , results may differ in difficult cases , e.g. involving nac and nx .",
    "robin leigh - pemberton  ] , bank @xmath3  of  ] @xmath0  england  ] @xmath0  governor  ] whereas ramshaw and marcus state that ` `` governor '' is not included in any basenp chunk ' .",
    "]    an adjp constituent inside an np constituent becomes part of the np chunk :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( np the ( adjp most volatile ) form ) + @xmath5 @xmath0  the most volatile form  ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    [ [ vp ] ] vp ~~    in the treebank , verb phrases are highly embedded ; see e.g. the following sentence which contains four vp constituents .",
    "following ramshaw and marcus v - type chunks , this sentence will only contain one vp chunk :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( ( s ( np - sbj-3 mr .",
    "icahn ) ( vp may not ( vp want ( s ( np - sbj * -3 ) ( vp to ( vp sell ... ) ) ) ) ) . ) ) + @xmath5 @xmath0  mr . icahn  ] @xmath2  may not want to sell  ] ... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it is still possible however to have one vp chunk directly follow another : @xmath0  the impression  ] @xmath0  i  ] * @xmath2  have got  ] @xmath2  is  ] * @xmath0  they  ] @xmath2  d love to do  ] @xmath6  away  ] @xmath3  with  ] @xmath0  it  ] . in this case",
    "the two vp constituents did not overlap in the treebank .",
    "adverbs / adverbial phrases become part of the vp chunk ( as long as they are in front of the main verb ) :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( vp could ( advp very well ) ( vp show ... ) ) + @xmath5 @xmath2  could very well show  ] ... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in contrast to ramshaw and marcus , predicative adjectives of the verb are not part of the vp chunk , e.g. in `` @xmath0  they  ] @xmath2  are  ] @xmath7  unhappy  ] '' .    in inverted sentences ,",
    "the auxiliary verb is not part of any verb phrase in the treebank .",
    "consequently it does not belong to any vp chunk :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( ( s ( sinv ( conjp not only ) does ( np - sbj-1 your product ) ( vp have ( s ( np - sbj * -1 ) ( vp to ( vp be ( adjp - prd excellent ) ) ) ) ) ) , but ...",
    "+ @xmath5 @xmath8  not only  ] does @xmath0  your product  ] @xmath2  have to be  ] @xmath7  excellent  ] , but ...",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _      advp chunks mostly correspond to advp constituents in the treebank .",
    "however , advps inside adjps or inside vps if in front of the main verb are assimilated into the adjp respectively vp chunk . on the other hand , advps that contain an np make two chunks :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( advp - tmp ( np a year ) earlier ) + @xmath5 @xmath0  a year  ] @xmath9  earlier  ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    adjps inside nps are assimilated into the np . and parallel to advps , adjps that contain an np make two chunks :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( adjp - prd ( np 68 years ) old ) + @xmath5 @xmath0  68 years  ] @xmath7  old  ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it would be interesting to see how changing these decisions ( as can be done in the treebank - to - chunk conversion script ) influences the chunking task .",
    "most pp chunks just consist of one word ( the preposition ) with the part - of - speech tag in .",
    "this does not mean , though , that finding pp chunks is completely trivial .",
    "ins can also constitute an sbar chunk ( see below ) and some pp chunks contain more than one word .",
    "this is the case with fixed multi - word prepositions such as _ such as , because of , due to _ , with prepositions preceded by a modifier : _ well above , just after , even in , particularly among _ or with coordinated prepositions : _ inside and outside_. we think that pps behave sufficiently differently from nps in a sentence for not wanting to group them into one class ( as ramshaw and marcus did in their n - type chunks ) , and that on the other hand tagging all np chunks inside a pp as i - pp would only confuse the chunker .",
    "we therefore chose not to handle the recognition of true pps ( prep.+np ) during this first chunking step .",
    "sbar chunks mostly consist of one word ( the complementizer ) with the part - of - speech tag in , but like multi - word prepositions , there are also multi - word complementizers : _ even though , so that , just as , even if , as if , only if_.      conjunctions can consist of more than one word as well : _ as well as , instead of , rather than , not only , but also_. one - word conjunctions ( like _ and , or _ ) are not annotated as conjp in the treebank , and are consequently no conjp chunks in our data .    the treebank uses the prt constituent to annotate verb particles , and our prt chunk does the same .",
    "the only multi - word particle is _ on and off_. this chunk type should be easy to recognize as it should coincide with the part - of - speech tag rp , but through tagging errors it is sometimes also assigned in ( preposition ) or rb ( adverb ) .",
    "intj is an interjection phrase / chunk like _ no , oh , hello , alas , good grief!_. it is quite rare .",
    "the list marker lst is even rarer .",
    "examples are _ 1 . , 2 . , 3 .",
    ", first , second , a , b , c_. it might consist of two words : the number and the period .    the ucp chunk is reminiscent of the ucp ( unlike coordinated phrase ) constituent in the treebank . arguably , the conjunction is the head of the ucp , so most ucp chunks consist of conjunctions like _ and _ and _ or_. ucps are the rarest chunks and are probably not very useful for other nlp tasks",
    ".      tokens outside any chunk are mostly punctuation signs and the conjunctions in ordinary coordinated phrases .",
    "the word _ not _ may also be outside of any chunk .",
    "this happens in two cases : either _ not _ is not inside the vp constituent in the treebank annotation e.g. in    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ... ( vp have ( vp told ( np-1 clients ) ( s ( np - sbj * -1 ) not ( vp to ( vp ship ( np anything ) ) ) ) ) ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    or _ not _ is not followed by another verb ( because the main verb is a form of _ to be _ ) . as the right chunk boundary is defined by the chunk s head , i.e. the main verb in this case , _",
    "not _ is then in fact a postmodifier and as such not included in the chunk : `` ... @xmath10  that  ] @xmath0  there  ] @xmath2  were  ] nt @xmath0  any major problems  ] . ''",
    "all chunks were automatically extracted from the parsed version of the treebank , guided by the tree structure , the syntactic constituent labels , the part - of - speech tags and by knowledge about which tags can be heads of which constituents . however , some trees are very complex and some annotations are inconsistent . what to think about a vp in which the main verb is tagged as nn ( common noun ) ?",
    "either we allow nns as heads of vps ( not very elegant but which is what we did ) or we have a vp without a head .",
    "the first solution might also introduce errors elsewhere ... as ramshaw and marcus already noted : `` while this automatic derivation process introduced a small percentage of errors on its own , it was the only practical way both to provide the amount of training data required and to allow for fully - automatic testing . ''",
    "for the conll shared task , we have chosen to work with the same sections of the penn treebank as the widely used data set for base noun phrase recognition @xcite : wsj sections 1518 of the penn treebank as training material and section 20 as test material .",
    "the chunks in the data were selected to match the descriptions in the previous section .",
    "an overview of the chunk types in the training data can be found in table [ tab - types ] .",
    "de data sets contain tokens ( words and punctuation marks ) , information about the location of sentence boundaries and information about chunk boundaries .",
    "additionally , a part - of - speech ( pos ) tag was assigned to each token by a standard pos tagger ( brill trained on the penn treebank ) .",
    "we used these pos tags rather than the treebank ones in order to make sure that the performance rates obtained for this data are realistic estimates for data for which no treebank pos tags are available .    .",
    "number of chunks per phrase type in the training data ( 211727 tokens , 106978 chunks ) . [ cols=\">,>,<\",options=\"header \" , ]",
    "the eleven systems that have been applied to the conll-2000 shared task can be divided in four groups :    1 .   rule - based systems : villain and day ; johansson ; djean .",
    "memory - based systems : veenstra and van den bosch .",
    "3 .   statistical systems : pla , molina and prieto ; osborne ; koeling ; zhou , tey and su .",
    "combined systems : tjong kim sang ; van halteren ; kudoh and matsumoto .",
    "vilain and day approached the shared task in three different ways .",
    "the most successful was an application of the alembic parser which uses transformation - based rules .",
    "johansson uses context - sensitive and context - free rules for transforming part - of - speech ( pos ) tag sequences to chunk tag sequences .",
    "djean has applied the theory refinement system allis to the shared task . in order to obtain a system which could process xml formatted data while using context information , he has used three extra tools .",
    "veenstra and van den bosch examined different parameter settings of a memory - based learning algorithm .",
    "they found that modified value difference metric applied to pos information only worked best .    a large number of the systems applied to the conll-2000 shared task uses statistical methods .",
    "pla , molina and prieto use a finite - state version of markov models .",
    "they started with using pos information only and obtained a better performance when lexical information was used .",
    "zhou , tey and su implemented a chunk tagger based on hmms .",
    "the initial performance of the tagger was improved by a post - process correction method based on error driven learning and by incorporating chunk probabilities generated by a memory - based learning process .",
    "the two other statistical systems use maximum - entropy based methods .",
    "osborne trained ratnaparkhi s maximum - entropy pos tagger to output chunk tags .",
    "koeling used a standard maximum - entropy learner for generating chunk tags from words and pos tags .",
    "both have tested different feature combinations before finding an optimal one and their final results are close to each other .",
    "three systems use system combination .",
    "tjong kim sang trained and tested five memory - based learning systems to produce different representations of the chunk tags .",
    "a combination of the five by majority voting performed better than the individual parts .",
    "van halteren used weighted probability distribution voting ( wpdv ) for combining the results of four wpdv chunk taggers and a memory - based chunk tagger .",
    "again the combination outperformed the individual systems .",
    "kudoh and matsumoto created 231 support vector machine classifiers to predict the unique pairs of chunk tags .",
    "the results of the classifiers were combined by a dynamic programming algorithm .",
    "the performance of the systems can be found in table [ tab - results ] .",
    "a baseline performance was obtained by selecting the chunk tag most frequently associated with a pos tag .",
    "all systems outperform the baseline .",
    "the majority of the systems reached an f@xmath11 score between 91.50 and 92.50 .",
    "two approaches performed a lot better : the combination system wpdv used by van halteren and the support vector machines used by kudoh and matsumoto .",
    "in the early nineties , abney proposed to approach parsing by starting with finding related chunks of words . by then",
    ", church had already reported on recognition of base noun phrases with statistical methods .",
    "ramshaw and marcus approached chunking by using a machine learning method .",
    "their work has inspired many others to study the application of learning methods to noun phrase chunking .",
    "other chunk types have not received the same attention as np chunks .",
    "the most complete work is buchholz et al . , which presents results for np , vp , pp , adjp and advp chunks .",
    "veenstra works with np , vp and pp chunks .",
    "both he and buchholz et al .",
    "use data generated by the script that produced the conll-2000 shared task data sets .",
    "ratnaparkhi has recognized arbitrary chunks as part of a parsing task but did not report on the chunking performance .",
    "part of the sparkle project has concentrated on finding various sorts of chunks for the different languages @xcite .",
    "we have presented an introduction to the conll-2000 shared task : dividing text into syntactically related non - overlapping groups of words , so - called text chunking . for this task",
    "we have generated training and test data from the penn treebank .",
    "this data has been processed by eleven systems .",
    "the best performing system was a combination of support vector machines submitted by taku kudoh and yuji matsumoto .",
    "it obtained an f@xmath11 score of 93.48 on this task .",
    "we would like to thank the members of the cnts - language technology group in antwerp , belgium and the members of the ilk group in tilburg , the netherlands for valuable discussions and comments .",
    "tjong kim sang is funded by the european tmr network learning computational grammars .",
    "buchholz is supported by the netherlands organization for scientific research ( nwo ) .",
    "john carroll , ted briscoe , glenn carroll , marc light , dethleff prescher , mats rooth , stefano federici , simonetta montemagni , vito pirrelli , irina prodanof , and massimo vanocchi .",
    "sparkle work package 3 , deliverable d3.2 .",
    "ferran pla , antonio molina , and natividad prieto .",
    "2000 . improving chunking by means of lexical - contextual information in statistical language models . in _ proceedings of conll-2000 and lll-2000_. lisbon , portugal .",
    "lance  a. ramshaw and mitchell  p. marcus",
    "text chunking using transformation - based learning . in _ proceedings of the third acl workshop on very large corpora_. association for computational linguistics ."
  ],
  "abstract_text": [
    "<S> we describe the conll-2000 shared task : dividing text into syntactically related non - overlapping groups of words , so - called text chunking . </S>",
    "<S> we give background information on the data sets , present a general overview of the systems that have taken part in the shared task and briefly discuss their performance .    </S>",
    "<S> = 4.4 cm </S>"
  ]
}