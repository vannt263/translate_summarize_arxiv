{
  "article_text": [
    "dimensionality reduction and manifold learning have an important part to play in the understanding of data . in this work",
    "we introduce two fast constructive heuristics for dimensionality reduction called unsupervised k - nearest neighbor regression .",
    "meinicke @xcite proposed a general unsupervised regression framework for learning of low - dimensional manifolds .",
    "the idea is to reverse the regression formulation such that low - dimensional data samples in latent space optimally reconstruct high - dimensional output data .",
    "we take this framework as basis for an iterative approach that fits knn to this unsupervised setting in a combinatorial variant .",
    "the manifold problem we consider is a mapping @xmath0 corresponding to the dimensionality reduction for data points @xmath1 , and latent points @xmath2 with @xmath3 .",
    "the problem is a hard optimization problem as the latent variables @xmath4 are unknown .    in section [ sec : related ] we will review related dimensionality reduction approaches , and repeat knn regression .",
    "section [ sec : unn ] presents the concept of unn regression , and two iterative strategies that are based on fixed latent space topologies .",
    "conclusions are drawn in section [ sec : conclusions ] .",
    "many dimensionality reduction methods have been proposed , a very famous one is principal component analysis ( pca ) , which assumes linearity of the manifold @xcite . an extension for learning of non - linear manifolds is kernel pca @xcite that projects the data into a hilbert space .",
    "further famous approaches for manifold learning are isomap by tenenbaum , silva , and langford @xcite , locally linear embedding ( lle ) by roweis and saul @xcite , and principal curves by hastie and stuetzle @xcite .",
    "the work on unsupervised regression for dimensionality reduction starts with meinicke @xcite , who introduced the corresponding algorithmic framework for the first time . in this line of research",
    "early work concentrated on non - parametric kernel density regression , i.e. , the counterpart of the nadaraya - watson estimator @xcite denoted as unsupervised kernel regression ( ukr ) .",
    "klanke and ritter @xcite introduced an optimization scheme based on lle , pca , and leave - one - out cross - validation ( loo - cv ) for ukr .",
    "carreira - perpin and lu @xcite argue that training of non - parametric unsupervised regression approaches is quite expensive , i.e. , @xmath5 in time , and @xmath6 in memory .",
    "parametric methods can accelerate learning , e.g. , unsupervised regression based on radial basis function networks ( rbfs ) @xcite , gaussian processes @xcite , and neural networks @xcite .      in the following ,",
    "we give a short introduction to k - nearest neighbor regression that is basis of the unn approach .",
    "the problem in regression is to predict output values @xmath7 to given input values @xmath8 based on sets of @xmath9 input - output examples @xmath10 .",
    "the goal is to learn a function @xmath11 known as regression function .",
    "we assume that a data set consisting of observed pairs @xmath12 is given . for a novel pattern @xmath13",
    ", knn regression computes the mean of the function values of its k - nearest neighbors : @xmath14 with set @xmath15 containing the indices of the @xmath16-nearest neighbors of @xmath13 .",
    "the idea of knn is based on the assumption of locality in data space : in local neighborhoods of @xmath17 patterns are expected to have similar output values @xmath18 ( or class labels ) to @xmath19 .",
    "consequently , for an unknown @xmath13 the label must be similar to the labels of the closest patterns , which is modeled by the average of the output value of the @xmath16 nearest samples .",
    "knn has been proven well in various applications , e.g. , in detection of quasars in interstellar data sets @xcite .",
    "in this section we introduce two iterative strategies for unn regression based on minimization of the data space reconstruction error ( dsre ) @xcite .",
    "let @xmath20 with @xmath7 be the matrix of high - dimensional patterns in data space .",
    "we seek for a low - dimensional representation , i.e. , a matrix of latent points @xmath21 , such that a regression function @xmath22 applied to @xmath4 , , point - wise optimally reconstructs the pattern  ,",
    "i.e. , we search for an @xmath4 that minimizes @xmath23 @xmath24 is called data space reconstruction error ( dsre ) . latent points @xmath4 define the low - dimensional representation .",
    "the regression function applied to the latent points should optimally _ reconstruct _ the high - dimensional patterns .",
    "an unn regression manifold is defined by variables @xmath25 with the unsupervised formulation of an unn regression manifold @xmath26 matrix @xmath27 contains the latent points @xmath17 that define the manifold , i.e. , the low - dimensional representation of data @xmath28 .",
    "parameter @xmath29 is the location where the function is evaluated .",
    "an optimal unn regression manifold minimizes the dsre @xmath30 with frobenius norm @xmath31 in other words : an optimal unn manifold consists of low - dimensional points @xmath4 that minimize the reconstruction of the data points @xmath32 w.r.t . knn regression .",
    "regularization in unn regression may be not as important as regularization in other methods that fit into the unsupervised regression framework .",
    "for example , in ukr regularization means penalizing extension in latent space with @xmath33 , and weight @xmath34 @xcite . in knn regression moving the low - dimensional data samples infinitely apart from each other",
    "does not have the same effect as long as we can still determine the k - nearest neighbors , but extension can be penalized to avoid redundant solutions . for practical purposes ( limitation of size of numbers )",
    "it might be reasonable to restrict continuous knn latents spaces , e.g. , to @xmath35^q$ ] . in the following section fixed latent space topologies",
    "are used that do not require further regularization .      for knn",
    "not the absolute positions of data samples in latent space are relevant , but the relative positions that define the _ neighborhood relations_. this perspective reduces the problem to a combinatorial search for neighborhoods @xmath36 with @xmath37 that can be solved by testing all combinations of @xmath16-element subsets of @xmath9 elements , i.e. , all @xmath38 combinations .",
    "the problem is still difficult to solve , in particular for high dimensions . in the following ,",
    "we introduce a combinatorial approach to unn , and introduce two iterative local strategies .",
    "the idea of our first iterative strategy ( unn  1 ) is to iteratively assign the data samples to a position in an existing latent space topology that leads to the lowest dsre .",
    "we assume fixed neighborhood topologies with equidistant positions in latent space , and therefore restrict the optimization problem of equation ( [ eq : unn ] ) to a search in a subset of latent space .",
    "unn  1 : illustration of embedding of a low - dimensional point to a fixed latent space topology w.r.t .",
    "the dsre testing all @xmath39 positions . ]    as a simple variant we consider the linear case of the latent variables arranged equidistantly on a line @xmath40 . in this simplified case",
    "only the order of the elements is important .",
    "the first iterative strategy works as follows :    1 .",
    "choose one element @xmath41 , 2 .",
    "test all @xmath39 intermediate positions of the @xmath42 embedded elements in latent space , 3 .",
    "choose the latent position with @xmath43 , and embed  @xmath18 , 4 .",
    "remove @xmath18 from @xmath32 , and repeat from step 1 until all elements have been embedded .",
    "figure [ fig : local1 ] illustrates the @xmath39 possible embeddings of a data sample into an existing order of points in latent space ( yellow / bright circles ) . for example , the position of element @xmath44 results in a lower dsre with @xmath45 than the position of @xmath46 , as the mean of the two nearest neighbors of @xmath44 is closer to @xmath18 than the mean of the two nearest neighbors of @xmath46 .",
    "the complexity of unn  1 can be described as follows .",
    "each dsre evaluation takes @xmath47 computations .",
    "we assume that the @xmath16 nearest neighbors are saved in a list during the embedding for each latent point @xmath17 , so that the search for indices @xmath48 takes @xmath49 time .",
    "the dsre has to be computed for @xmath50 positions , which takes @xmath51 steps , i.e. , @xmath52 time .",
    "the iterative approach introduced in the last section tests all intermediate positions of previously embedded latent points .",
    "we propose a second iterative variant ( unn  2 ) that only tests the neighbored intermediate positions in latent space of the nearest embedded point @xmath53 in data space .",
    "the second iterative strategy works as follows :    1 .   choose one element @xmath41 , 2 .",
    "look for the nearest @xmath53 that has already been embedded ( w.r.t .",
    "distance measure like euclidean distance ) , 3 .",
    "choose the latent position next to @xmath54 with @xmath43 and embed @xmath18 , 4 .",
    "remove @xmath18 from @xmath32 , add @xmath18 to @xmath55 , and repeat from step 1 until all elements have been embedded .",
    "figure [ fig : local2 ] illustrates the embedding of a 2-dimensional point @xmath18 ( yellow ) left or right of the nearest point @xmath54 in data space .",
    "the position with the lowest dsre is chosen . in comparison to unn  1",
    ", @xmath42 distance comparisons in data space have to be computed , but only 2 positions have to be tested w.r.t .",
    "the data space reconstruction error .",
    "unn  2 : testing only the neighbored positions of the nearest point @xmath54 in data space . ]",
    "unn  2 computes the nearest embedded point  @xmath54 for each data point , which takes @xmath56 steps . only for the two neighbors the dsre has to be computed , resulting in an overall number of @xmath57 steps , i.e.",
    ", it takes @xmath52 time . because of the multiplicative constants , unn  2 is faster in practice .",
    "for example , for @xmath58 , @xmath59 , and @xmath60 , unn  1 takes @xmath61 steps , while unn  2 takes @xmath62 steps .",
    "testing all combinations takes @xmath63 steps , which is not computable in reasonable time .",
    "the following experimental section will answer the question , if this speedup of unn  2 has to be paid with worse dsres .",
    "this section shows the behavior of the iterative strategies on three test problems .",
    "we will compare the dsre of both strategies to the initial dsre at the end of this section .",
    "first , we compare unn  1 and unn  2 on a simple 2-dimensional data set , i.e. , the 2-dimensional noisy @xmath64 with @xmath65 ( 2d-@xmath64 ) . figure [ fig:2d ] shows the experimental results with @xmath66 nearest neighbors .",
    "similar colors correspond to neighbored latent    points .",
    "part  ( a ) shows an unn  1 embedding of the 2d-@xmath64 data set . part  ( b ) shows the embedding of the same data set with unn  2 . the colors of both embeddings show a satisfying topological sorting , although we can observe local optima .      in the following , we will test unn regression on a 3-dimensional @xmath64 data set ( 3d-@xmath64 ) .",
    "the variant without a hole consists of 500 data points , the variant with a hole in the middle consists of 400 points .",
    "figure [ fig:3d ] ( a ) shows the order of elements of the 3d-@xmath64 data set    without a hole at the beginning .",
    "the corresponding embedding with unn  1 and @xmath59 is shown in part  ( b ) of the figure . again",
    ", similar colors correspond to neighbored points in latent space .",
    "part  ( c ) of figure [ fig:3d ] shows the unn  2 embedding achieving similar results . also on the unn embedding of the @xmath64 data set with hole ,",
    "see part  ( d ) of the figure , a reasonable neighbored assignments can be observed .",
    "quantitative results for the dsre are reported in table [ tab : dsre ] .",
    "last , we experimentally test unn regression on test problems from the usps digits data set @xcite . for this sake",
    "we take 100 data samples of 256-dimensional ( 16 x 16 pixels ) pictures of handwritten digits of 2 s and 5 s .",
    "we embed a one - dimensional manifold , and show the    high - dimensional data that is assigned to every 14th latent point , i.e. , neighbored digits in the plot are neighbored in latent space .",
    "figure [ fig:7up ] shows the result of the unn  2-embedding for 2 s and 5 s with @xmath59 .",
    "we can observe that neighbored digits are similar to each other , while digits that are dissimilar are further away from each other in latent space .",
    "last , we compare the dsre achieved by both strategies with the initial dsre , and the dsre achieved by lle on all test problems .",
    ".comparison of dsre for initial data set , and after embedding with strategy unn  1 , and unn  2[tab : dsre ] .",
    "[ cols= \" < , > , > , > , > , > , > , > , > \" , ]     for the usps digits data set we choose the number @xmath67 .",
    "table [ tab : dsre ] shows the experimental results of three settings for the neighborhood size @xmath16 .",
    "the lowest dsre on each problem is highlighted with bold figures .",
    "after application of the iterative strategies the dsre is significantly lower than initially . increasing @xmath16 results in higher dsres .",
    "with exception of lle with @xmath59 on 2d-@xmath64 , the unn  1 strategy always achieves the best results .",
    "unn  1 achieves lower dsres than unn  2 , with exception of 2d-@xmath64 , and @xmath59 .",
    "the win in accuracy has to be paid with a constant runtime factor that may play an important role in case of large data sets , or high data space dimensions .",
    "with unn regression we have fitted a fast regression technique into the unsupervised setting for dimensionality reduction .",
    "the two iterative unn strategies are efficient methods to embed high - dimensional data into fixed one - dimensional latent space taking @xmath52 time .",
    "the speedup is achieved by restricting the number of possible solutions ( reduction of solution space ) , and applying fast iterative heuristics .",
    "both methods turned out to be performant on test problems in first experimental analyses .",
    "unn  1 achieves lower dsres , but unn  2 is slightly faster because of the multiplicative constants of unn  1 .",
    "our future work will concentrate on the analysis of local optima the unn strategies approximate , and how the approach can be extended to guarantee global optimal solutions .",
    "furthermore , the unn strategies can be extended to latent topologies with higher dimensionality . for @xmath68",
    "the insertion of intermediate solutions into a grid is more difficult : it results in shifting rows and columns of the grid , and thus changes the latent topology in parts that may not be desired .",
    "a simple stochastic search strategy can be employed that randomly swaps positions of latent points in the grid ."
  ],
  "abstract_text": [
    "<S> in many scientific disciplines structures in high - dimensional data have to be found , e.g. , in stellar spectra , in genome data , or in face recognition tasks . in this work we present a novel approach to non - linear dimensionality reduction . </S>",
    "<S> it is based on fitting k - nearest neighbor regression to the unsupervised regression framework for learning of low - dimensional manifolds . </S>",
    "<S> similar to related approaches that are mostly based on kernel methods , unsupervised k - nearest neighbor ( unn ) regression optimizes latent variables w.r.t . the data space reconstruction error employing the k - nearest neighbor heuristic . </S>",
    "<S> the problem of optimizing latent neighborhoods is difficult to solve , but the unn formulation allows the design of efficient strategies that iteratively embed latent points to fixed neighborhood topologies . </S>",
    "<S> unn is well appropriate for sorting of high - dimensional data . </S>",
    "<S> the iterative variants are analyzed experimentally . </S>"
  ]
}