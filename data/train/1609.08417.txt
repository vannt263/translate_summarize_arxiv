{
  "article_text": [
    "in the machine learning and data mining applications , the performance measures are used to evaluate the performance of the predictive models @xcite .",
    "the outputs of the predictive model over a set of test data points are compared to their ground truth labels , and the performance measures are used to produce the performance scores .",
    "the performance measures include the area under the receiver operating characteristic curve ( auc ) , the recall - precision break - even point ( rpb ) , the top @xmath0-rank precision ( top@xmath0 pre ) , and the positives at top ( pos@top ) @xcite .",
    "recently the performance of pos@top is being more and more popular in the machine learning community .",
    "this performance measures only counts the positive instances ranked before the first - ranked negative instance .",
    "the rate of these positive instances in all the positive instances is defined as the * pos@top*. in many machine learning applications , we observed that the top - ranked instances / classes plays critical roles in the performance evaluation , and the pos@top performance measure can give a good description about how the top - ranked instances / classes distribute @xcite .",
    "moreover , it is parameter free , and it use the top - ranked negative as the boundary of the recognized positive instance pool .    although this performance measure has been used in various applications , its usage is limited to the test process , but is ignored in the training process .",
    "this problem can result in a classifier not optimal for the maximization of the pos@top performance measure . to solve this problem , a few works were proposed to optimize the pos@top measure in the training process directly .",
    "@xcite proposed a highly efficient algorithm to maximize the pos@top over the training set by learning a linear classifier model , and named it as toppush .",
    "this algorithm has a linear time - complexity with regard to the number of the training instances .",
    "agarwal @xcite proposed a ranking algorithm which can maximize the pos@top performance measure over the training set .",
    "it is a support vector style model , but it has a different objective , and it is solved not as a quadratic programming problem , but as a @xmath1 constrained problem .",
    "boyd et al .",
    "@xcite proposed to maximize the convex surrogate of the loss function of pos@top , which is the number of positive data instances ranked behind the too - ranked negative instances .",
    "the objective is optimized as a set of convex optimization problems . among these three existing works of optimization of the pos@top ,",
    "all the predictive models are linear models , and they are designed to tickle the single - instance data .    to solve these problems ,",
    "we develop a convolutional neural network ( cnn ) model to optimize the pos@top performance measure .",
    "this model is designed to tickle the multiple instance sequence as input .",
    "it is composed convolutional layer , the activation layer , the max - pooling .",
    "moreover , the output of the cnn is used as a representation of the multi - instance data point , and we apply a linear classifier to predict the class label .",
    "we propose to learn the parameters of the cnn and classifier model , including the filters and the classifier parameter to maximize the pos@top . to this end , we argue that the loss function should be defined as the number of the positive instances sorted behind the top - ranked negative instance .",
    "we define a hinge loss function for this problem .",
    "we solve this problem by alternate optimization problem and develop an iterative algorithm .",
    "the following parts of the paper are organized as follows . in section [ sec : method ] , we introduce the proposed classification model and the learning method of the parameters of the model . in section [ sec : exp ] , we evaluate the proposed method over some benchmark data sets . in section",
    "[ sec : conclusion ] , the conclusion of this paper is summarized .",
    "the training set is composed of @xmath2 data points , and denoted as @xmath3 , where @xmath4 is the input data of the @xmath5-th data point , and @xmath6 is its binary class label .",
    "@xmath4 is a sequence of @xmath7 instances , denoted as a matrix @xmath8\\in \\mathbb{r}^{d\\times m_i}$ ] , where @xmath9 is the @xmath10-dimensional feature vector of the @xmath11-th instance . to represent the @xmath5-th data point",
    ", we use a cnn model , @xmath12 in this model , @xmath13\\in \\mathbb{r}^{d\\times m}$ ] is the matrix of @xmath14 filters , and @xmath15 is the @xmath0-th filter vector .",
    "@xmath16 is a element - wise non - linear activation function , defined as @xmath17 .",
    "@xmath18 is the row - wise maximization operator , and it selects the maximum element from each row of a matrix . to approximate the class label @xmath19 of a data point @xmath4 from its cnn representation @xmath20",
    ", we propose to use a linear classifier , @xmath21 @xmath22 is a parameter vector of the linear classifier . the overall framework of the proposed model is shown in figure [ fig : overall ] .",
    "+    the argued performance measure , pos@top , is defined as number of positive data points which are ranked before the top - ranked negative data point , @xmath23 . to maximize the pos@top",
    ", we argue a 0 - 1 loss function for each positive data point , @xmath24 to minimize this loss function , we argue that for any positive data point , its classification score should be larger than that of the top - ranked negative plus a margin amount , @xmath25 we further propose a hinge loss function as follows to give a loss when this condition does not hold , @xmath26 to learn the cnn classifier parameters @xmath27 and @xmath28 to maximize the pos@top , we should minimize the loss function of ( [ equ : loss1 ] ) over all the positive data points .",
    "mean while we also propose to regularize the filter parameters and the full connection weights to prevent the over - fitting problem , and the squared @xmath29 norms of @xmath28 and @xmath27 are minimized .",
    "the overall objective function of the learning problem is the combination of the losses measured by ( [ equ : loss1 ] ) over the positive data points , the regularization terms of @xmath28 and @xmath27 , and the minimization problem is given as follows ,    @xmath30    where @xmath31 and @xmath32 are the tradeoff parameters .",
    "we further define @xmath33 as the classification score of the top negative , and @xmath34 as the response of ( [ equ : loss1 ] ) ,    @xmath35    with the slack variables , we rewrite the problem in ( [ equ : objective1 ] ) as follows ,    @xmath36      the dual form of the optimization problem is as follows @xmath37 where @xmath38\\in\\{1,0\\}^n$ ] and @xmath39 if @xmath40 , and @xmath41 otherwise .",
    "@xmath42 is the lagrange multiplier variable for the constraint @xmath43 if @xmath44 , and that for the constraint @xmath45 otherwise .",
    "@xmath46^\\top$ ] , @xmath47^\\top$ ] .",
    "@xmath48 is defined as @xmath49 and it indicates the instance in a bag which gives the maximum response with regard to a filter .",
    "we propose to use an alternate optimization strategy to optimize this problem . in an iterative algorithm , @xmath50 and @xmath27",
    "are updated alternately .      to optimize @xmath50 , we fix @xmath27 and remove the irrelevant term to obtain the following optimization problem ,    @xmath51",
    "this is a linear constrained quadratic programming problem , and we can use an active set algorithm to solve it . after it is solved , we can recover @xmath28 as follows , @xmath52      to optimize @xmath27 , we fixe @xmath50 and remove the irrelevant terms to obtain the following problem ,    @xmath53    where @xmath54 it is clear that @xmath55 is a independent function of @xmath56 , thus we can update the filters one by one .",
    "when one filter is being updated , other filters are fixed .",
    "the updating of @xmath56 is conducted by gradient descent , @xmath57 and the gradient function of @xmath58 is calculated as follows , @xmath59      based on the optimization results of section [ sec : opt ] , we develop an iterative algorithm to learn both @xmath27 and @xmath28 .",
    "the algorithm is given as in algorithm [ alg : overall ] .",
    "in this algorithm , we repeat the iterations until convergence . in each iteration , we first update the indicator of the maximally responding the filters , then update the lagrange multipliers , and finally update the filters .",
    "the proposed algorithm is named as convolutional max pos@top classifier ( convmpt ) .",
    "* input * : @xmath60 .",
    "initialize @xmath27 randomly .",
    "update @xmath61 .",
    "update @xmath50 by solving the following quadratic programming problem ,    @xmath62    calculate the gradient function @xmath63 as in ( [ equ : gradient ] ) .",
    "update the @xmath0-th filter , @xmath64 .    calculate the classifier parameter @xmath65 .",
    "* output * : @xmath27 and @xmath28 .",
    "in this section of experiments , we evaluate the proposed algorithm over several multiple instance data set for the problem of maximization of pos@top .      in the experiment ,",
    "we use tree types of data set  image set , text set , and audio set .    *",
    "the image set used by us is the caltech-256 dataset @xcite . in this set",
    ", we have 30,607 images in total .",
    "these images belongs to 257 classes .",
    "each image is presented as a bag of multiple instances . to this end , each image is split into a group of small image patches , and each patch is an instance . * the text data set used in this experiment is the semeval-2010 task 8 data set @xcite .",
    "it contains 10,717 sentences , and these sentences belongs to 10 different classes of relations .",
    "each sentence is composed of several words , and thus it is natural a multiple instance data set .",
    "each word is represented as a feature vector of 100 dimensions using the word embedding algorithm proposed by sun et al .",
    "the audio data set used in this experiment is the spoken arabic digits ( sad ) @xcite . in this data",
    "set , there are 8,800 sequences of voice signal .",
    "these voice signal sequences belongs to 10 classes of digits . to represent each sequence , we split it to a group of voice signal frames .",
    "each frame is an instance , thus each sequence is a bag of multiple instances .",
    "the summary of the information of the data sets are listed in the table [ tab : dataset ] .",
    ".information of the data sets used in the experiments .",
    "[ cols=\"<,<,^,^\",options=\"header \" , ]    +   + * the algorithm is an iterative algorithm , so we also study how the algorithm performs with different numbers of iterations .",
    "we report the average of rate of pos@top with regard to different iterations in figure [ tab : iter ] . from this table",
    ", we can observe a trend of improving performance with growing number of iterations .",
    "but generally speaking , the algorithm is stable to the change of iteration numbers .",
    "in this paper , we propose a novel model to maximize the performance of pos@top .",
    "the proposed model has a structure of cnn .",
    "the parameter learning of cnn is to optimize the loss function of pos@top .",
    "we propose a novel iterative learning algorithm to solve this problem .",
    "meanwhile we also propose to minimize the squared @xmath29 norm of the filter matrix of the convolutional layer .",
    "the proposed algorithm is compared to the ordinary cnn and the existing pos@top minimization method , and the results show its advantage . in the future , we will apply the proposed method to other applications , such as medical imaging @xcite , computer vision @xcite , network security @xcite , et al .",
    "we will also consider learning bayesian network to maximize the pos@top performance measure @xcite .",
    "s.  agarwal .",
    "the infinite push : a new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list . in _ proceedings of the 11th siam international conference on data mining ,",
    "sdm 2011 _ , pages 839850 , 2011 .",
    "n.  s. al  madi and j.  i. khan .",
    "measuring learning performance and cognitive activity during multimodal comprehension . in _",
    "2016 7th international conference on information and communication systems ( icics ) _ , pages 5055 .",
    "ieee , 2016 .",
    "x.  fan , b.  malone , and c.  yuan .",
    "finding optimal bayesian network structures with constraints learned from data . in _",
    "proceed . of the 30th conf . on uncertainty in artificial intelligence ( uai-2014 )",
    ".      x.  fan , k.  tang , and t.  weise .",
    "margin - based over - sampling method for learning from imbalanced datasets . in _ proceedings of the 15th pacific - asia conference on knowledge discovery and data mining ( pakdd-2011 ) _ , pages 309320 .",
    "springer berlin heidelberg , 2011 .",
    "n.  hammami , m.  bedda , and n.  farah .",
    "spoken arabic digits recognition using mfcc based on gmm . in _ sustainable utilization and development in engineering and technology ( student ) , 2012 ieee conference on _ , pages 160163 .",
    "ieee , 2012 .",
    "i.  hendrickx , s.  n. kim , z.  kozareva , p.  nakov , d.    saghdha , s.  pad , m.  pennacchiotti , l.  romano , and s.  szpakowicz .",
    "semeval-2010 task 8 : multi - way classification of semantic relations between pairs of nominals . in _ proceedings of the workshop on semantic evaluations",
    ": recent achievements and future directions _ , pages 9499 . association for computational linguistics , 2009 .",
    "d.  r. king , w.  li , j.  j. squiers , r.  mohan , e.  sellke , w.  mo , x.  zhang , w.  fan , j.  m. dimaio , and j.  e. thatcher .",
    "surgical wound debridement sequentially characterized in a porcine burn model with multispectral imaging .",
    ", 41(7):14781487 , 2015 .",
    "w.  li , w.  mo , x.  zhang , y.  lu , j.  j. squiers , e.  w. sellke , w.  fan , j.  m. dimaio , and j.  e. thatcher .",
    "burn injury diagnostic imaging device s accuracy improved by outlier detection and removal . in _",
    "spie defense+ security _ ,",
    "pages 947206947206 .",
    "international society for optics and photonics , 2015 .",
    "w.  li , w.  mo , x.  zhang , j.  j. squiers , y.  lu , e.  w. sellke , w.  fan , j.  m. dimaio , and j.  e. thatcher .",
    "outlier detection and removal improves accuracy of machine learning approach to multispectral burn diagnostic imaging .",
    ", 20(12):121305121305 , 2015 .",
    "liang , l.  shi , h.  wang , j.  meng , j.  j .- y .",
    "wang , q.  sun , and y.  gu . optimizing top precision performance measure of content - based image retrieval by learning similarity function . in _ pattern recognition ( icpr ) , 2016 23st international conference on_. ieee , 2016 .",
    "liang , w.  xie , w.  li , h.  wang , j.  j .- y .",
    "wang , and l.  taylor .",
    "a novel transfer learning method based on common space mapping and weighted domain matching . in _",
    "tools with artificial intelligence ( ictai ) , 2016 ieee 28th international conference on_. ieee , 2016 .",
    "w.  mo , r.  mohan , w.  li , x.  zhang , e.  w. sellke , w.  fan , j.  m. dimaio , and j.  e. thatcher . the importance of illumination in a non - contact photoplethysmography imaging system for burn wound assessment . in _",
    "spie bios _ , pages 93030m93030 m .",
    "international society for optics and photonics , 2015 .",
    "j.  e. thatcher , w.  li , y.  rodriguez - vaqueiro , j.  j. squiers , w.  mo , y.  lu , k.  d. plant , e.  sellke , d.  r. king , w.  fan , et  al .",
    "multispectral and photoplethysmography optical imaging techniques identify important tissue characteristics in an animal model of tangential burn excision .",
    ", 37(1):3852 , 2016 .",
    "l.  xu , z.  zhan , s.  xu , and k.  ye . an evasion and counter - evasion study in malicious websites detection . in _ communications and network security ( cns ) , 2014 ieee conference on _ , pages 265273 .",
    "ieee , 2014 ."
  ],
  "abstract_text": [
    "<S> in the machine learning problems , the performance measure is used to evaluate the machine learning models . </S>",
    "<S> recently , the number positive data points ranked at the top positions ( pos@top ) has been a popular performance measure in the machine learning community . in this paper </S>",
    "<S> , we propose to learn a convolutional neural network ( cnn ) model to maximize the pos@top performance measure . </S>",
    "<S> the cnn model is used to represent the multi - instance data point , and a classifier function is used to predict the label from the its cnn representation . </S>",
    "<S> we propose to minimize the loss function of pos@top over a training set to learn the filters of cnn and the classifier parameter . </S>",
    "<S> the classifier parameter vector is solved by the lagrange multiplier method , and the filters are updated by the gradient descent method alternately in an iterative algorithm . </S>",
    "<S> experiments over benchmark data sets show that the proposed method outperforms the state - of - the - art pos@top maximization methods . </S>"
  ]
}