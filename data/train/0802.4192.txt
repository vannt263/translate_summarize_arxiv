{
  "article_text": [
    "the topic of this paper lies on the frontier between statistics and approximation theory .",
    "our goal is to characterize the functions well estimated by a special class of estimation procedures : the model selection rules .",
    "our purpose is not to build new model selection estimators but to determine thoroughly the functions for which well known model selection procedures achieve good performances .",
    "of course , approximation theory plays a crucial role in our setting but surprisingly its role is even more important than the one of statistical tools .",
    "this statement will be emphasized by the use of the _ maxiset approach _ , which illustrates the well known fact that `` well estimating is well approximating '' .",
    "more precisely we consider the classical gaussian white noise model @xmath0 where @xmath1 , @xmath2 is the unknown function , @xmath3 is the brownian motion in @xmath4 and @xmath5 .",
    "this model means that for any @xmath6 , @xmath7 is observable where @xmath8 is a centered gaussian process such that for all functions @xmath9 and @xmath10 , @xmath11= \\int_{{\\mathcal { d}}}u(t )   u'(t ) dt.\\ ] ] we take a noise level of the form @xmath12 to refer to the asymptotic equivalence between the gaussian white noise model and the classical regression model with @xmath13 equispaced observations ( see @xcite ) .",
    "two questions naturally arise : how to construct an estimator @xmath14 of @xmath2 based on the observation @xmath15 and how to measure its performance ?",
    "many estimators have been proposed in this setting ( wavelet thresholding , kernel rules , bayesian procedures ... ) . in this paper , we only focus on model selection techniques described accurately in the next paragraph .",
    "the model selection methodology consists in constructing an estimator by minimizing an empirical contrast @xmath16 over a given set , called a model .",
    "the pioneer work in model selection goes back in the 1970 s with mallows @xcite and akaike @xcite .",
    "birg and massart develop the whole modern theory of model selection in @xcite or @xcite for instance .",
    "estimation of a regression function with model selection estimators is considered by baraud in @xcite , while inverse problems are tackled by loubes and ludea @xcite .",
    "finally model selection techniques provide nowadays valuable tools in statistical learning ( see boucheron et al .",
    "@xcite ) .    in nonparametric",
    "estimation , performances of estimators are usually measured by using the quadratic norm , which gives rise to the following empirical quadratic contrast @xmath17 for any function @xmath9 , where @xmath18 denotes the norm associated to @xmath19 .",
    "we assume that we are given a dictionary of functions of @xmath20 , denoted by @xmath21 where @xmath22 is a countable set and we consider @xmath23 , a collection of models spanned by some functions of @xmath24 .",
    "for any @xmath25 , we denote by @xmath26 the subset of @xmath22 such that @xmath27 and @xmath28 the dimension of @xmath29 .",
    "let @xmath30 be the function that minimizes the quadratic empirical criterion @xmath31 with respect to @xmath32 .",
    "a straightforward computation shows that the estimator @xmath33 is the projection of the data onto the space @xmath29 .",
    "so , if @xmath34 is an orthonormal basis ( not necessarily related to @xmath24 ) of @xmath29 and @xmath35 then @xmath36 now , the issue is the selection of the best model @xmath37 from the data which gives rise to the _ model selection estimator _ @xmath38 . for this purpose ,",
    "a penalized rule is considered , which aims at selecting an estimator , close enough to the data , but still lying in a small space to avoid overfitting issues .",
    "let @xmath39 be a penalty function which increases when @xmath40 increases .",
    "the model @xmath41 is selected using the following penalized criterion @xmath42 the choice of the model collection and the associated penalty are then the key issues handled by model selection theory .",
    "we point out that the choices of both the model collection and the penalty function should depend on the noise level .",
    "this is emphasized by the subscript @xmath13 for @xmath43 and @xmath44 .",
    "the asymptotic behavior of model selection estimators has been studied by many authors .",
    "we refer to massart @xcite for general references and recall hereafter the main oracle type inequality .",
    "such an oracle inequality provides a non asymptotic control on the estimation error with respect to a bias term @xmath45 , where @xmath46 stands for the best approximation ( in the @xmath47 sense ) of the function @xmath2 by a function of @xmath48 in other words @xmath46 is the orthogonal projection of @xmath2 onto @xmath29 , defined by @xmath49    [ massart ] let @xmath50 be fixed and let @xmath51 be some family of positive numbers such that @xmath52 let @xmath53 and assume that @xmath54 then , almost surely , there exists some minimizer @xmath55 of the penalized least - squares criterion @xmath56 over @xmath57 .",
    "moreover , the corresponding penalized least - squares estimator @xmath58 is unique and the following inequality is valid : @xmath59\\leq   c\\left[\\inf_{m\\in    \\mathcal { m}_n}\\left\\{\\|s_m - s\\|^2+\\pen_n(m)\\right\\}+\\frac{1+\\sigma_n}{n}\\right],\\ ] ] where @xmath60 depends only on @xmath61 .    equation ( [ oracletypemas ] ) is the key result to establish optimality of penalized estimators under oracle or minimax points of view . in this paper , we focus on an alternative to these approaches : the maxiset point of view .      before describing the maxiset approach ,",
    "let us briefly recall that for a given procedure @xmath62 , the minimax study of @xmath63 consists in comparing the rate of convergence of @xmath63 achieved on a given functional space @xmath64 with the best possible rate achieved by any estimator .",
    "more precisely , let @xmath65 be the ball of radius @xmath66 associated with @xmath67 , the procedure @xmath62 achieves the rate @xmath68 on @xmath65 if @xmath69\\right\\}<\\infty.\\ ] ] to check that a procedure is optimal from the minimax point of view ( said to be minimax ) , it must be proved that its rate of convergence achieves the best rate among any procedure on each ball of the class .",
    "this minimax approach is extensively used and many methods cited above are proved to be minimax in different statistical frameworks .",
    "however , the choice of the function class is subjective and , in the minimax framework , statisticians have no idea whether there are other functions well estimated at the rate @xmath70 by their procedure .",
    "a different point of view is to consider the procedure @xmath63 as given and search all the functions @xmath2 that are well estimated at a given rate @xmath70 : this is the _ maxiset _ approach , which has been proposed by kerkyacharian and picard @xcite .",
    "the maximal space , or maxiset , of the procedure @xmath63 for this rate @xmath70 is defined as the set of all these functions .",
    "obviously , the larger the maxiset , the better the procedure .",
    "we set the following definition .",
    "let @xmath68 be a decreasing sequence of positive real numbers and let @xmath62 be an estimation procedure .",
    "the maxiset of @xmath63 associated with the rate @xmath70 is @xmath71\\right\\}<\\infty\\right\\},\\ ] ] the ball of radius @xmath72 of the maxiset is defined by @xmath73\\right\\}\\leq r^2\\right\\}.\\ ] ]    of course , there exist connections between maxiset and minimax points of view : @xmath63 achieves the rate @xmath70 on @xmath67 if and only if @xmath74 in the white noise setting , the maxiset theory has been investigated for a wide range of estimation procedures , including kernel , thresholding and lepski procedures , bayesian or linear rules .",
    "we refer to @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite for general results .",
    "maxisets have also been investigated for other statistical models , see @xcite and @xcite .",
    "the goal of this paper is to investigate maxisets of model selection procedures .",
    "following the classical model selection literature , we only use penalties proportional to the dimension @xmath40 of @xmath29 : @xmath75 with @xmath76 to be specified .",
    "our main result characterizes these maxisets in terms of approximation spaces .",
    "more precisely , we establish an equivalence between the statistical performance of @xmath58 and the approximation properties of the model collections @xmath23 . with @xmath77 for any @xmath78 , theorem  [ main ] , combined with theorem  [ massart ] proves that , for a given function @xmath2 , the quadratic risk @xmath79 $ ] decays at the rate @xmath80 if and only if the deterministic quantity @xmath81 decays at the rate @xmath80 as well .",
    "this result holds with mild assumptions on @xmath76 and under an embedding assumption on the model collections ( @xmath82 ) .",
    "once we impose additional structure on the model collections , the deterministic condition can be rephrased as a linear approximation property and a non linear one as stated in theorem  [ main2 ] .",
    "+ we illustrate these results for three different model collections based on wavelet bases . the first one deals with sieves in which all the models are embedded , the second one with the collection of all subspaces spanned by vectors of a given basis .",
    "for these examples , we handle the issue of calculability and give explicit characterizations of the maxisets . in the third example",
    ", we provide an intermediate choice of model collections and use the fact that the embedding condition on the model collections can be relaxed .",
    "finally performances of these estimators are compared and discussed .",
    "the paper is organized as follows .",
    "section  [ s : main ] describes the main general results established in this paper .",
    "more precisely , we specify results valid for general dictionaries in section [ dico ] . in section [ basis ]",
    ", we focus on the case where @xmath24 is an orthonormal family .",
    "section  [ part3 ] is devoted to the illustrations of these results for some model selection estimators associated with wavelet methods .",
    "in particular , a comparison of maxiset performances are provided and discussed .",
    "section  [ proofs ] gives the proofs of our results .",
    "as explained in the introduction , our goal is to investigate maxisets associated with model selection estimators @xmath38 where the penalty function is defined in ( [ eq : pen ] ) and with the rate @xmath83 where @xmath84 is specified in ( [ ratel ] ) . observe that @xmath84 depends on the choice of @xmath76 .",
    "it can be for instance polynomial , or can take the classical form @xmath85 so we wish to determine @xmath86\\right\\}<\\infty\\right\\}.\\ ] ] in the sequel , we use the following notation : if @xmath67 is a given space @xmath87 means that for any @xmath72 , there exists @xmath88 such that @xmath89 and for any @xmath88 , there exists @xmath72 such that @xmath90      in this section , we make no assumption on @xmath24 .",
    "theorem  [ massart ] is a non asymptotic result while maxisets results deal with rates of convergence ( with asymptotics in @xmath13 ) .",
    "therefore obtaining maxiset results for model selection estimators requires a structure on the sequence of model collections .",
    "we first focus on the case of nested model collections ( @xmath91 ) .",
    "note that this does not imply a strong structure on the model collection for a given @xmath13 .",
    "in particular , this does not imply that the models are nested . identifying the maxiset",
    "@xmath92 is a two - step procedure .",
    "we need to establish inclusion ( [ inc1 ] ) and inclusion ( [ inc2 ] ) .",
    "recall that we have introduced previously @xmath93 roughly speaking , theorem  [ massart ] established by massart proves that any function @xmath2 satisfying @xmath94 belongs to the maxiset @xmath92 and thus provides inclusion ( [ inc2 ] ) .",
    "the following theorem establishes inclusion ( [ inc1 ] ) and highlights that @xmath95 plays a capital role .",
    "[ main ] let @xmath96 be fixed .",
    "let us assume that the sequence of model collections satisfies for any @xmath13 @xmath97 and that the sequence of positive numbers @xmath98 is non - decreasing and satisfies @xmath99 and there exist @xmath100 and two constants @xmath101 and @xmath102 such that for @xmath103 , @xmath104 @xmath105 and @xmath106 where @xmath107 is a positive constant only depending on @xmath108 , @xmath109 and @xmath110 defined in equation ( [ erwan ] ) of section 4 .",
    "then , the penalized rule @xmath111 is such that for any @xmath112 $ ] , for any @xmath72 , there exists @xmath88 such that for @xmath113 , @xmath114\\right\\}\\leq r^2    \\rightarrow   \\sup_n\\left\\{\\rho_{n,{\\ensuremath{\\alpha}}}^{-2}q(s , n)\\right\\}\\leq ( r')^2.\\ ] ]    technical assumptions ( [ hyplambdalim ] ) , ( [ hyplambda ] ) , ( [ eq : kraftmain ] ) and ( [ hyplambda0 ] ) are very mild and could be partly relaxed while preserving the results .",
    "assumption ( [ hyplambdalim ] ) is necessary to deal with rates converging to 0 .",
    "note that the classical cases @xmath115 or @xmath116 satisfy ( [ hyplambdalim ] ) and ( [ hyplambda ] ) .",
    "furthermore , assumption ( [ hyplambda0 ] ) is always satisfied when @xmath117 or when @xmath118 with @xmath119 large enough .",
    "assumption ( [ eq : kraftmain ] ) is very close to assumptions  -([minpen ] ) .",
    "in particular , if there exist two constants @xmath120 and @xmath102 such that for any @xmath13 , @xmath121 then , since @xmath122 conditions ( [ kraft ] ) , ( [ minpen ] ) and ( [ eq : kraftmain ] ) are all satisfied .",
    "the assumption @xmath123 $ ] can be relaxed for particular model collections , which will be highlighted in proposition  [ nestedtheo ] of section [ nested ] .",
    "finally , assumption  ( [ hyplambdaembed ] ) can be removed for some special choice of model collection @xmath43 at the price of a slight overpenalization as it shall be shown in proposition  [ prop : approxmod ] and section  [ special ] .    combining theorems [ massart ] and [ main ] gives a first characterization of the maxiset of the model selection procedure @xmath124 :    [ corollaire ] let @xmath125 be fixed .",
    "assume that assumptions ( [ hyplambdaembed ] ) , ( [ hyplambdalim ] ) , ( [ hyplambda ] )  ( [ hyplambda0 ] ) and  ( [ eq : kraftmainbis ] ) are satisfied",
    ". then for any @xmath112 $ ] , @xmath126    the maxiset of @xmath58 is characterized by a deterministic approximation property of @xmath2 with respect to the models @xmath43 .",
    "it can be related to some classical approximation properties of @xmath2 in terms of approximation rates if the functions of @xmath24 are orthonormal .      from now on , @xmath127 is assumed to be an orthonormal basis ( for the @xmath128 scalar product ) .",
    "we also assume that the model collections @xmath43 are constructed through restrictions of a single model collection @xmath129 .",
    "namely , given a collection of models @xmath129 we introduce a sequence @xmath130 of increasing subsets of the indices set @xmath22 and we define the intermediate collection @xmath131 as @xmath132 the model collections @xmath131 do not necessarily satisfy the embedding condition  ( [ hyplambdaembed ] ) .",
    "thus , we define @xmath133 so @xmath134 .",
    "the assumptions on @xmath24 and on the model collections allow to give an explicit characterization of the maxisets .",
    "we denote @xmath135 .",
    "remark that without any further assumption @xmath136 can be a larger model collection than @xmath129 .",
    "now , let us denote by @xmath137 the sequence of approximation spaces defined by @xmath138 and consider the corresponding approximation space @xmath139 where @xmath140 is the projection of @xmath2 onto @xmath141 .",
    "define also another kind of approximation sets : @xmath142 the corresponding balls of radius @xmath72 are defined , as usual , by replacing @xmath143 by @xmath66 in the previous definitions .",
    "we have the following result .    [ main2 ]",
    "let @xmath125 be fixed .",
    "assume that ( [ hyplambdalim ] ) , ( [ hyplambda ] ) ,  ( [ hyplambda0 ] ) and  ( [ eq : kraftmainbis ] ) are satisfied .",
    "then , the penalized rule @xmath111 satisfies the following result : for any @xmath144 $ ] , @xmath145    the result pointed out in theorem [ main2 ] links the performance of the estimator to an approximation property for the estimated function .",
    "this approximation property is decomposed into a linear approximation measured by @xmath146 and a non linear approximation measured by @xmath147 .",
    "the linear condition is due to the use of the reduced model collection @xmath43 instead of @xmath129 , which is often necessary to ensure either the calculability of the estimator or condition  ( [ eq : kraftmainbis ] ) .",
    "it plays the role of a minimum regularity property that is easily satisfied .",
    "observe that if we have one model collection , that is for any @xmath148 and @xmath149 , @xmath150 , @xmath151 for any @xmath13 and thus @xmath152 .",
    "then @xmath153 and theorem [ main2 ] gives @xmath154 the spaces @xmath147 and @xmath155 highly depend on the models and the approximation space . at first glance , the best choice seems to be @xmath156 and@xmath157 since the infimum in the definition of @xmath158 becomes smaller when the collection is enriched .",
    "there is however a price to pay when enlarging the model collection : the penalty has to be larger to satisfy ( [ eq : kraftmainbis ] ) , which deteriorates the convergence rate .",
    "a second issue comes from the tractability of the minimization  ( [ eq : minimization ] ) itself which will further limit the size of the model collection . to avoid considering the union of @xmath159 , that can dramatically increase the number of models considered for a fixed @xmath13 , leading to large penalties",
    ", we can relax the assumption that the penalty is proportional to the dimension .",
    "namely , for any @xmath13 , for any @xmath160 , there exists @xmath161 such that @xmath162 then for any model @xmath163 , we replace the dimension @xmath40 by the larger dimension @xmath164 and we set @xmath165 the minimization of the corresponding penalized criterion over all model in @xmath166 leads to a result similar to theorem [ main2 ] . mimicking its proof , we can state the following proposition that will be used in section  [ special ] :    [ prop : approxmod ] let @xmath125 be fixed .",
    "assume ( [ hyplambdalim ] ) , ( [ hyplambda ] )  ( [ hyplambda0 ] ) and  ( [ eq : kraftmainbis ] ) are satisfied .",
    "then , the penalized estimator @xmath167 where @xmath168 satisfies the following result : for any @xmath144 $ ] , @xmath169    remark that @xmath23 , @xmath146 and @xmath170 can be defined in a similar fashion for any arbitrary dictionary @xmath24 .",
    "however , one can only obtain the inclusion @xmath171 in the general case .",
    "the aim of this section is twofold .",
    "firstly , we propose to illustrate our previous maxiset results to different model selection estimators built with wavelet methods by identifying precisely the spaces @xmath147 and @xmath146 .",
    "secondly , comparisons between the performances of these estimators are provided and discussed .",
    "we briefly recall the construction of periodic wavelets bases of the interval @xmath172 $ ] .",
    "let @xmath173 and @xmath174 be two compactly supported functions of @xmath175 and denote for all @xmath176 , all @xmath177 , @xmath178 and @xmath179 .",
    "those functions can be periodized in such a way that @xmath180 constitutes an orthonormal basis of @xmath181)$ ] .",
    "some popular examples of such bases are given in @xcite .",
    "the function @xmath173 is called the scaling function and @xmath174 the corresponding wavelet .",
    "any periodic function @xmath182)$ ] can be represented as : @xmath183 where @xmath184}s(t)\\phi_{00}(t)dt\\ ] ] and for any @xmath185 and for any @xmath186 @xmath187}s(t)\\psi_{jk}(t)dt.\\ ] ] finally , we recall the characterization of besov spaces using wavelets .",
    "such spaces will play an important role in the following . in this section",
    "we assume that the multiresolution analysis associated with the basis @xmath188 is @xmath189-regular with @xmath190 as defined in  @xcite . in this case , for any @xmath191 and any @xmath192",
    ", the periodic function @xmath2 belongs to the besov space @xmath193 if and only if @xmath194 and @xmath195 @xmath196 where @xmath197 .",
    "this characterization allows to recall the following embeddings : @xmath198 and @xmath199      we consider first a single model collection corresponding to a class of nested models @xmath200 for such a model collection , theorem  [ main2 ] could be applied with @xmath201 .",
    "one can even remove assumption   which imposes a minimum value on @xmath202 that depends on the rate @xmath203 :    [ nestedtheo ] let @xmath204 and let @xmath205 be the model selection estimator associated with the model collection @xmath206 . then , under assumptions  ,   and  , @xmath207    remark that it suffices to choose @xmath208 with @xmath119 , independent of @xmath209 , large enough to ensure condition  .",
    "it is important to notice that the estimator @xmath205 can not be computed in practice because to determine the best model @xmath210 one needs to consider an infinite number of models , which can not be done without computing an infinite number of wavelet coefficients . to overcome this issue",
    ", we specify a maximum resolution level @xmath211 for estimation where @xmath212 is non - decreasing .",
    "this modification is also in the scope of theorem  [ main2 ] : it corresponds to @xmath213 and the model collection @xmath214 defined as follows : @xmath215    for the specific choice @xmath216 we obtain : @xmath217 since @xmath218 reduces to @xmath219 , arguments of the proofs of theorem  [ main2 ] and proposition  [ nestedtheo ] give :    [ larms0 ] let @xmath204 and let @xmath220 be the model selection estimator associated with the model collection @xmath214 .",
    "then , under assumptions , and @xmath221    this tractable procedure is thus as efficient as the original one .",
    "we obtain the maxiset behavior of the non adaptive linear wavelet procedure pointed out in @xcite but here the procedure is completely data - driven .      in this paragraph",
    "we enlarge the model collections in order to obtain much larger maxisets .",
    "we start with the following model collection @xmath222 where @xmath223 and @xmath224 is the set of all subsets of @xmath22 .",
    "this model collection is so rich that whatever the sequence @xmath225 , condition   ( or even condition  ) is not satisfied . to reduce the cardinality of the collection , we restrict the maximum resolution level to the resolution level @xmath211 defined in ( [ j0trunc ] ) and consider the collections @xmath226 defined from @xmath227 by @xmath228 where @xmath229    remark that this corresponds to the same choice of @xmath141 as in the previous paragraph and the corresponding estimator fits perfectly within the framework of theorem  [ main2 ] .",
    "the classical logarithmic penalty @xmath230 which corresponds to @xmath231 , is sufficient to ensure condition ( [ eq : kraftmainbis ] ) as soon as @xmath119 is a constant large enough ( the choice @xmath118 is not sufficient ) .",
    "the identification of the corresponding maxiset focuses on the characterization of the space @xmath232 since , as previously , @xmath233 .",
    "we rely on sparsity properties of @xmath232 . in our context",
    ", sparsity means that there is a _ small _ proportion of _ large _ coefficients of a signal .",
    "let introduce for , for @xmath234 , the notation @xmath235 to represent the non - increasing rearrangement of the wavelet coefficient of a periodic signal @xmath2 : @xmath236 as the best model @xmath237 of prescribed dimension @xmath238 is obtained by choosing the subset of index corresponding to the @xmath238 largest wavelet coefficients , a simple identification of the space @xmath232 is @xmath239 theorem 2.1 of @xcite provides a characterization of this space as a weak besov space : @xmath240 with for any @xmath2410,2[$ ] , @xmath242 following their definitions , the larger @xmath243 , the smaller @xmath244 and the sparser the sequence @xmath245 .",
    "lemma 2.2 of @xcite shows that the spaces @xmath246 ( @xmath247 ) have other characterizations in terms of wavelet coefficients : @xmath248 we obtain thus the following proposition .",
    "[ larms1 ] let @xmath249 be fixed , let @xmath250 and let @xmath251 be the model selection estimator associated with the model collection @xmath214 . then , under assumptions , , and : @xmath252    observe that the estimator @xmath253 is easily tractable from a computational point of view as the minimization can be rewritten coefficientwise : @xmath254 the best subset @xmath255 is thus the set @xmath256 and @xmath257 corresponds to the well - known hard thresholding estimator , @xmath258 proposition [ larms1 ] corresponds thus to the maxiset result established by kerkyacharian and picard@xcite .",
    "we consider now the model collection proposed by massart  @xcite .",
    "this collection can be viewed as an hybrid collection between the collections of sections  [ nested ] and  [ strut ] .",
    "this strategy turns out to be minimax for all besov spaces @xmath259 when @xmath260 and @xmath261 .",
    "more precisely , for a chosen @xmath262 , define the model collection by @xmath263 where for any @xmath264 @xmath265 is the set of all subsets @xmath26 of @xmath22 that can be written @xmath266 with @xmath267 .    as remarked in @xcite , for any @xmath268 and any @xmath269",
    ", the dimension @xmath270 of the corresponding model @xmath29 depends only on @xmath271 and is such that @xmath272 we denote by @xmath273 this common dimension .",
    "note that the model collection @xmath274 does not vary with @xmath13 . using theorem  [ main2 ] with @xmath201",
    ", we have the following proposition .    [ massarttheo ]",
    "let @xmath249 be fixed , let @xmath250 and let @xmath275 be the model selection estimator associated with the model collection @xmath274 . then , under assumptions , , and : @xmath276 with @xmath277 where @xmath278 is the reordered sequence of coefficients @xmath279 : @xmath280    remark that , as in section [ nested ] , as soon as @xmath281 with @xmath119 large enough , condition   holds .",
    "this large set can not be characterized in terms of classical spaces .",
    "nevertheless it is undoubtedly a large functional space , since as proved in section  [ sec : space - embeddings ] , for every @xmath78 and every @xmath282 satisfying @xmath283 we get @xmath284    this new procedure is not computable since one needs an infinite number of wavelet coefficients to perform it .",
    "the problem of calculability can be solved by introducing , as previously , a maximum scale @xmath211 as defined in .",
    "we consider the class of collection models @xmath285 defined as follows : @xmath286 this model collection does not satisfy the embedding condition @xmath287 .",
    "nevertheless , we can use proposition  [ prop : approxmod ] with @xmath288 if @xmath29 is obtained from an index subset @xmath289 in @xmath265 .",
    "this slight over - penalization leads to the following result .",
    "[ massartcutheo ] let @xmath249 be fixed , let @xmath250 and let @xmath290 be the model selection estimator associated with the model collection @xmath291 . then , under assumptions , , and : @xmath292    modifying massart s strategy in order to obtain a practical estimator changes the maxiset performance .",
    "the previous set @xmath293 is intersected with the strong besov space @xmath294 .",
    "nevertheless , as it will be proved in section [ sec : space - embeddings ] , the maxiset @xmath295 is still a large functional space . indeed , for every @xmath78 and every @xmath109 satisfying @xmath296 @xmath297      in this paragraph",
    ", we compare the maxiset performances of the different model selection procedures described previously . for a chosen rate of convergence",
    "let us recall that the larger the maxiset , the better the estimator . to begin , we propose to focus on the model selection estimators which are tractable from the computational point of view .",
    "gathering propositions [ larms0 ] , [ larms1 ] and [ massartcutheo ] we obtain the following comparison .    [ vs1 ] let @xmath298    * if for every @xmath299 @xmath300 with @xmath119 large enough ,",
    "then @xmath301 * if for every @xmath299 @xmath118 with @xmath119 large enough , then @xmath302    it means the followings .    * if for every @xmath299 @xmath300 with @xmath119 large enough , then , according to the maxiset point of view , the estimator @xmath251 strictly outperforms the estimator @xmath290 which strictly outperforms the estimator @xmath220 . * if for every @xmath299 @xmath303 or @xmath300 with @xmath119 large enough",
    ", then , according to the maxiset point of view , the estimator @xmath290 strictly outperforms the estimator @xmath220 .",
    "the corresponding embeddings of functional spaces are proved in section [ sec : space - embeddings ] .",
    "the hard thresholding estimator @xmath251 appears as the best estimator when @xmath76 grows logarithmically while estimator @xmath290 is the best estimator when @xmath76 is constant . in both cases ,",
    "those estimators perform very well since their maxiset contains all the besov spaces @xmath304 with @xmath305 .",
    "we forget now the calculability issues and consider the maxiset of the original procedure proposed by massart .",
    "propositions  [ larms1 ] , [ massarttheo ] and  [ massartcutheo ] lead then to the following result .",
    "[ vs2 ] let @xmath204 .    * if for any @xmath299 @xmath300 with @xmath119 large enough then @xmath306 * if for any @xmath299 @xmath303 or @xmath307 with @xmath119 large enough then @xmath308    hence , within the maxiset framework ,",
    "the estimator @xmath275 strictly outperforms the estimator @xmath290 while the estimators @xmath275 and @xmath251 are not comparable .",
    "note that we did not consider the maxisets of the estimator @xmath205 in this section as they are identical to the ones of the tractable estimator @xmath220 .     and",
    "@xmath309.,width=491 ]     and @xmath309.,width=491 ]    we summarize all those embeddings in figure  [ fig : maxilog ] and figure  [ fig : maxicst ] : figure [ fig : maxilog ] represents these maxiset embeddings for the choice @xmath310 , while figure [ fig : maxicst ] represents these maxiset embeddings for the choice @xmath303 .",
    "for any functions @xmath9 and @xmath10 of @xmath19 , we denote by @xmath311 the @xmath128-scalar product between @xmath9 and @xmath10 : @xmath312 we denote by @xmath60 a constant whose value may change at each line .      without loss of generality , we assume that @xmath313 .",
    "we start by constructing a different representation of the white noise model . for any model @xmath29 , we define @xmath314 , the projection of the noise on @xmath29 by @xmath315 where @xmath316 is any orthonormal basis of @xmath29 . for any function @xmath317 , we have : @xmath318 the key observation is now that with high probability , @xmath319 can be controlled simultaneously over all models .",
    "more precisely , for any @xmath320 , we define the space @xmath321 as the space spanned by the functions of @xmath29 and @xmath322 and control the norm of @xmath323 .",
    "[ cirelson ] let @xmath13 be fixed and @xmath324 then , under assumption ( [ eq : kraftmain ] ) , we have @xmath325 .",
    "the cirelson - ibragimov - sudakov inequality ( see @xcite , page 10 ) implies that for any @xmath326 , any @xmath327 and any @xmath328 @xmath329+t\\right\\}\\leq e^{-\\frac{t^2}{2}}.\\ ] ] since @xmath330\\leq \\sqrt{{\\ensuremath{\\mathbb{e}}}\\left[\\|{\\mathbf{w}}_{m+m'}\\|^2\\right]}\\leq \\sqrt{d_m+d_{m'}},\\ ] ] with @xmath331 , we obtain @xmath332 assumption   implies thus that @xmath333    we define @xmath334 ( denoted @xmath335 when there is no ambiguity ) , the model that minimizes a quantity close to @xmath95 : @xmath336 where @xmath337 is an absolute constant larger than 1 specified later .",
    "the proof of the theorem begins by a bound on @xmath338 :    [ lem : lemmastart ] for any @xmath339 , @xmath340 + \\left ( \\frac { k(2\\gamma^{-1}+1 ) } { \\tilde k { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } } + \\frac{2k\\gamma\\lambda_n}{\\tilde k } \\right )   \\frac{d_{m_0}}{kn}\\end{aligned}\\ ] ] if the constant @xmath341 satisfies @xmath342 .    by definition , @xmath343",
    "thus , @xmath344 let @xmath339 .",
    "as @xmath345 is supported by the space @xmath346 spanned by the functions of @xmath347 and @xmath335 , we obtain with the previous definition @xmath348 we multiply now by @xmath349 to obtain @xmath350 and thus @xmath351 one obtains @xmath352 we derive now a bound on @xmath353 . by definition , @xmath354 and thus @xmath355 by multiplying by @xmath349 and plugging the bound  ( [ eq : bound1 ] ) , we have : @xmath356 and thus @xmath357 taking the expectation on both sides yields @xmath358",
    "\\\\ & \\qquad + \\left ( \\frac{\\frac{2}{\\gamma}+1}{k(1-\\gamma ) } +   \\frac{2\\gamma}{k(1-\\gamma ) } { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } \\lambda_n    \\right )     \\frac{d_{m_0}}{n } \\end{split}\\end{aligned}\\ ] ] and thus as soon as @xmath359 @xmath360\\\\ & \\qquad\\qquad + \\frac{\\frac{\\frac{2}{\\gamma}+1}{k(1-\\gamma ) } +   \\frac{2\\gamma}{k(1-\\gamma ) } { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } \\lambda_n    } { \\left(1 -    \\frac{\\frac{2}{\\gamma}+1}{k(1-\\gamma)}\\right){\\ensuremath{\\mathbb{p}}}\\{a_n\\ } }    \\frac{d_{m_0}}{n } \\end{split } \\\\",
    "\\begin{split } & \\leq \\frac{k(1-\\gamma)+\\frac{2}{\\gamma}-1}{\\left(k(1-\\gamma)-\\frac{2}{\\gamma}-1\\right){\\ensuremath{\\mathbb{p}}}\\{a_n\\ } } { \\ensuremath{\\mathbb{e}}}\\left[\\|\\hat s_{\\hat m}-s\\|^2\\right]\\\\ & \\qquad\\qquad + \\frac { \\frac{2}{\\gamma}+1 + 2\\gamma { \\ensuremath{\\mathbb{p}}}\\{a_n\\}\\lambda_n } { \\left(k(1-\\gamma)-\\frac{2}{\\gamma}-1\\right){\\ensuremath{\\mathbb{p}}}\\{a_n\\ } }   \\frac{d_{m_0}}{n } \\end{split}\\\\ & \\leq \\frac{\\tilde",
    "k + \\frac{4}{\\gamma}}{\\tilde k { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } } { \\ensuremath{\\mathbb{e}}}\\left[\\|\\hat s_{\\hat m}-s\\|^2\\right ] + \\frac { \\frac{2}{\\gamma}+1 + 2\\gamma { \\ensuremath{\\mathbb{p}}}\\{a_n\\}\\lambda_n } { \\tilde k { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } }   \\frac{d_{m_0}}{n}\\\\ \\intertext{which yields } \\|s_{m_0}-s\\|^2   & \\leq \\frac{\\tilde k + \\frac{4}{\\gamma}}{\\tilde k { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } } { \\ensuremath{\\mathbb{e}}}\\left[\\|\\hat s_{\\hat m}-s\\|^2\\right ] + \\left ( \\frac { k(\\frac{2}{\\gamma}+1 ) } { \\tilde k { \\ensuremath{\\mathbb{p}}}\\{a_n\\ } } + \\frac{2k\\gamma\\lambda_n}{\\tilde k } \\right )   \\frac{d_{m_0}}{kn}\\end{aligned}\\ ] ] with @xmath361 .",
    "now , let us specify the constants .",
    "we take @xmath362}\\inf_{x\\in \\left[\\frac{1}{2},1-\\delta\\right]}\\left\\{x^{\\frac{2{\\ensuremath{\\alpha}}}{2{\\ensuremath{\\alpha}}+1}}-x\\right\\}=(1-{\\ensuremath{\\delta}})^{\\frac{2{\\ensuremath{\\alpha}}_0}{2{\\ensuremath{\\alpha}}_0 + 1}}-1+{\\ensuremath{\\delta}}\\in ( 0,1).\\ ] ] then we take @xmath363 this implies @xmath364 and assumptions of the previous lemma are satisfied .",
    "we consider now the dependency of @xmath335 on @xmath13 and prove by induction the following lemma .",
    "[ induction ] if there exists @xmath365 such that for any @xmath13 , @xmath366 & \\leq c_1   \\left(\\frac{2\\lambda_{n/2}}{n}\\right)^{\\frac{2\\alpha}{2\\alpha+1}}\\end{aligned}\\ ] ] then , provided @xmath367 , where @xmath368 there exists a constant @xmath369 such that for any @xmath13 , @xmath370    by using @xmath371 and ( [ eq : bound2 ] ) , for any @xmath372 $ ] , if we denote @xmath373 we have @xmath374   + ( 1-\\beta ) \\|s_{m_0(n/2)}-s\\|^2 \\\\ & \\qquad + \\left ( \\beta   \\left(\\frac { k(\\frac{2}{\\gamma}+1 ) } { \\tilde k { \\ensuremath{\\mathbb{p}}}\\{a_{n/2}\\}\\lambda_{n/2 } } + \\frac{2k\\gamma}{\\tilde k } \\right ) + \\frac{\\lambda_n}{2\\lambda_{n/2 } } \\right )    \\frac{2 \\lambda_{n/2 } d_{m_0(n/2)}}{kn}. \\end{split}\\end{aligned}\\ ] ] as @xmath375 , there exists @xmath376 $ ] such that @xmath377 so that @xmath378\\\\ & \\qquad\\qquad   + ( 1-\\beta_n ) \\left ( \\|s_{m_0(n/2)}-s\\|^2 +     \\frac{2\\lambda_{n/2}d_{m_0(n/2)}}{kn}\\right ) . \\end{split}\\end{aligned}\\ ] ] the induction can now be started .",
    "we assume now that for all @xmath379 @xmath380 by assumption , @xmath381 \\leq c_1   \\left(\\frac{2\\lambda_{n/2}}{n}\\right)^{\\frac{2\\alpha}{2\\alpha+1}},\\ ] ] so that , @xmath382 so , we have to prove that @xmath383 or equivalently , @xmath384 this condition can be rewritten as @xmath385 or @xmath386^{-1}\\end{aligned}\\ ] ] provided the right member is positive . under the very mild assumption @xmath387 , it is sufficient to ensure that ( [ erwan ] ) is true .",
    "indeed , @xmath388 and using values of the constants we have @xmath389^{-1}\\\\ & \\leq&\\frac{2\\left(\\frac{2}{\\gamma}+1\\right)}{p } \\left[\\frac{g({\\ensuremath{\\delta}},{\\ensuremath{\\alpha}}_0)}{2 } -   \\frac{\\tilde k + \\frac{4}{\\gamma}}{\\tilde k p }   \\frac{c_1}{c_2 } \\right]^{-1}\\\\ & \\leq&\\frac{8\\left(\\frac{2}{\\gamma}+1\\right)}{p g({\\ensuremath{\\delta}},{\\ensuremath{\\alpha}}_0)}\\\\ & \\leq&\\frac{8}{p g({\\ensuremath{\\delta}},{\\ensuremath{\\alpha}}_0)}\\left(\\frac{16 } { g({\\ensuremath{\\delta}},{\\ensuremath{\\alpha}}_0)}+1\\right)\\end{aligned}\\ ] ] if @xmath390    finally , theorem [ main ] follows from the previous lemma that gives the following inequality : @xmath391      theorem  [ main ] implies that for any @xmath392 , @xmath393 or equivalently there exists @xmath394 such that for any @xmath13 , @xmath395 by definition of @xmath141 , any function @xmath46 with @xmath396 belongs to @xmath141 and thus inequality   implies @xmath397 that is @xmath398 . by definition",
    ", @xmath136 is a larger collection than @xmath43 and thus inequality   also implies that for any @xmath13 , @xmath399 which turns out to be a characterization of @xmath400 when @xmath401 as a consequence of the following lemma .",
    "under assumptions of theorem  [ main2 ] , @xmath402    we denote @xmath403 first , let us assume that for any @xmath13 @xmath404 where @xmath405 is a constant .",
    "then , @xmath406 using @xmath407 , for @xmath408 , as soon as @xmath409 , there exists @xmath234 such that @xmath410 then , @xmath411 conversely , assume that there exists @xmath412 satisfying @xmath413 then for any @xmath414 , @xmath415 where @xmath405 is a constant .",
    "we have proved so far that @xmath416 .",
    "it remains to prove the converse inclusion .",
    "corollary  [ corollaire ] and the previous lemma imply that it suffices to prove that inequalities   and imply inequality   ( possibly with a different constant @xmath60 ) .",
    "let @xmath417 . by inequality  , for every @xmath13",
    ", there exists a model @xmath418 such that @xmath419 by definition of @xmath136 , there exists @xmath148 such that @xmath420 .    if @xmath421 then @xmath422 and thus @xmath423    otherwise @xmath424 and let @xmath425 be the model such that @xmath426 as defined in section  [ basis ] .",
    "we define @xmath427 by its index set @xmath428 .",
    "remark that @xmath429 and @xmath430 , so @xmath431 theorem [ main2 ] is proved .",
    "the proof of proposition  [ prop : approxmod ] relies on the definition of @xmath432 .",
    "recall that for any model @xmath433 there is a model @xmath434 such that @xmath435 and that @xmath436 one deduces @xmath437 and thus @xmath438 mimicking the proof of theorem  [ main2 ] , one obtains proposition  [ prop : approxmod ] .      in the same spirit as in the proof of theorem  [ main ] , for any @xmath13 , we denote @xmath439 ( we have set @xmath440 ) and @xmath441 in the nested case , lemma  [ lem : lemmastart ] becomes the following much stronger lemma :    [ initnest ] for any @xmath13 , almost surely @xmath442    as the models are embedded , either @xmath443 or @xmath444 .    in the first case , @xmath445 and",
    "thus ( [ equanest ] ) holds .",
    "otherwise , by construction @xmath446 combining these two inequalities yields @xmath447 now , ( [ equanest ] ) holds as @xmath448    now we can conclude the proof of proposition  [ nestedtheo ] with an induction similar to the one used in the proof of lemma  [ induction ] . indeed , let @xmath449 @xmath450 where @xmath405 is a constant .",
    "it suffices thus to verify that @xmath451 which is the case as soon as @xmath452 .      in this paragraph",
    "we provide many embedding properties between the functional spaces considered in section [ part3 ] .",
    "let us recall the following definitions : @xmath453):\\quad \\sup_{j\\in \\mathbb{n}}2^{j(\\alpha-\\frac{1}{p}+\\frac{1}{2})p}\\sum_{k=0}^{2^j-1}|\\beta_{jk}|^p<\\infty\\right\\};\\\\ \\mathcal{b}^{\\frac{\\alpha}{1 + 2\\alpha}}_{2,\\infty}&=&\\left\\{s \\in \\mathbb{l}_2([0,1 ] ) : \\quad \\sup_{j\\in \\mathbb{n}}2^{\\frac{2 j \\alpha}{1 + 2\\alpha}}\\sum_{j\\geq j}\\sum_{k=0}^{2^j-1}\\beta_{jk}^2<\\infty\\right\\};\\\\ \\mathcal{a}^{\\alpha}_{_{{\\mathcal{m}^{(h)}}}}&=&\\left\\{s \\in \\mathbb{l}_2([0,1 ] ) : \\quad \\sup_{j \\in \\mathbb{n}}2^{2j \\alpha}\\sum_{j\\geq j } \\sum_{k= \\lfloor2^{j}(j - j+1)^{-\\theta}\\rfloor}^{2^j } |\\beta_{j}|_{(k)}^2<\\infty\\right\\};\\\\ \\mathcal{w}_{\\frac{2}{1 + 2\\alpha}}&=&\\left\\{s \\in \\mathbb{l}_2([0,1 ] ) : \\quad \\sup_{u>0}u^{\\frac{2}{1 + 2\\alpha}}\\sum_{j = 0}^\\infty\\sum_{k=0}^{2^j-1}\\mathbf{1}_{_{|\\beta_{jk}|>u}}<\\infty\\right\\}.\\\\ \\ ] ]      @xmath455    * proof of @xmath456 .",
    "* + let @xmath2 belong to @xmath457 with @xmath458 and @xmath459 and , for any scale @xmath460 , let us denote by @xmath461 the sequence of the non - decreasing reordered wavelet coefficients of any level @xmath462 . then there exists a non negative constant @xmath60 such that for any @xmath463 @xmath464    fix @xmath268 . if @xmath465 , according to lemma  4.16 of @xcite , for all @xmath462 larger than @xmath271 @xmath466 summing over the indices @xmath462 larger than @xmath271 yields @xmath467 so @xmath2 belongs to @xmath468 .",
    "+ for the case @xmath469 , @xmath470 thus @xmath471 so @xmath2 also belongs to @xmath468 .",
    "+ we conclude that for any @xmath458 satisfying @xmath459 , @xmath472 + let us now prove the strict inclusion by considering the function @xmath473 defined as follows : @xmath474    for any @xmath475 such that @xmath476 @xmath477 and thus goes to @xmath478 when @xmath462 goes to @xmath479 it implies that @xmath473 does not belong to @xmath259 for any @xmath480 .",
    "+ now for any @xmath268 , @xmath481 which implies @xmath482 and thus @xmath483 . hence @xmath456 is proved .",
    "@xmath484     + * proof of @xmath485 .",
    "* + there is no doubt that @xmath486 since @xmath487 the strict inclusion is a direct consequence of @xmath488 just below .",
    "@xmath484     +      @xmath490    * proof of @xmath491 . * + let @xmath78 and @xmath458 satisfying @xmath492 . using the classical besov embeddings @xmath493 , and , according to @xmath456 , we have @xmath494 . hence @xmath495 and @xmath491 is proved . @xmath484",
    "+ * proof of @xmath496 . * + we already know that @xmath497 the strict inclusion is a direct consequence of @xmath498 proved in the next subsection .",
    "@xmath484     +      @xmath499    * proof of @xmath500 . *",
    "+ let us consider the function @xmath501 defined in the proof of @xmath456 .",
    "we already know that it does not belong to @xmath502 for any @xmath475 satisfying @xmath503 as a consequence for the case @xmath504 where @xmath78 , we deduce that @xmath473 does not belong to @xmath505 + moreover , we immediately deduce that @xmath506.@xmath484     + * proof of @xmath498 . * + let @xmath507)$ ] whose wavelet expansion is given by @xmath508 we set @xmath509 we are going to prove that @xmath510 while @xmath511    summing at a given scale @xmath462 yields @xmath512 and thus @xmath513    let @xmath514 and @xmath515 the real number such that @xmath516 then    @xmath517    so @xmath518 and @xmath519 .",
    "+ let us now prove that @xmath520 does not belong to @xmath521 fix @xmath268 large enough .",
    "then @xmath522 let @xmath523 be the real number such that @xmath524 . + from @xmath525 one deduces thus @xmath526 , which implies @xmath527 , and finally @xmath528 so , @xmath529 so , @xmath530 this implies that @xmath531 . finally @xmath498 is proved .",
    "we warmly thanks the anonymous referees for their carefull reading and their remarks which allow us to improve the paper .",
    "10 akaike , h. _ information theory and an extension of the maximum likelihood principle .",
    "_ second international symposium on information theory ( tsahkadsor , 1971 ) , pp . 267281 .",
    "akadmiai kiad , budapest , 1973 .",
    "autin , f. _ maxiset for density estimation on @xmath532 .",
    "methods statist . *",
    "15 * , no .",
    "2 , 123145 , 2006 .",
    "autin , f. _ maxisets for @xmath533-thresholding rules_. 2008 . test , * 17 * , ( 2 ) , 332 - 349 , 2008 .      baraud , y. _ model selection for regression on a random design_. esaim probab .",
    "6 , 127 - 146 , 2002 .",
    "baraud , y. _ model selection for regression on a fixed design_. probab . theory related fields 117 , no . 4 , 467 - 493 , 2000 .",
    "cohen a. , devore r.a .",
    ", hochmuth , r. _ restricted nonlinear approximation .",
    "* 16 * , no .",
    "1 , 85113 , 2000 .",
    "cohen a. , devore r.a . ,",
    "kerkyacharian , g. and picard , d. _ maximal spaces with given rate of convergence for thresholding algorithms .",
    "11 * , no .",
    "2 , 167191 , 2001 . daubechies , i. _ ten lectures on wavelets _ , siam , philadelphia , 1992 .",
    "devore , r.a . and lorentz , g.g .",
    "_ constructive approximation . _ springer - verlag , berlin , 1993 .",
    "kerkyacharian , g. and picard , d. _ thresholding algorithms , maxisets and well - concentrated bases .",
    "_ test 9 , no .",
    "2 , 283344 , 2000 .",
    "loubes , j - m . and ludea , c. _ adaptive complexity regularization for linear inverse problems_. electron . j. stat .",
    "2 , 661677 . , 2008 .",
    "loubes , j - m . and ludea , c. _ penalized estimators for non linear inverse problems_. 2008 . to appear in esaim ps .",
    "mallows , c.l . _ some comments on @xmath534_. _ technometrics _ , * 15 * , 661 - 675 , 1973 .",
    "massart , p. _ concentration inequalities and model selection _ lectures on probability theory and statistics ( saint - flour , 2003 ) , lecture notes in math . , 1896 ,",
    "springer , berlin , 2007 .",
    "meyer , y. _ ondelettes et oprateurs .",
    "i. _ hermann , paris , 1990 .",
    "rivoirard , v. _ maxisets for linear procedures .",
    "_ statist .",
    ". lett . * 67 * , no .",
    "3 , 267275 , 2004 .",
    "rivoirard , v. _ bayesian modeling of sparse sequences and maxisets for bayes rules .",
    "methods statist . *",
    "14 * , no .",
    "3 , 346376 , 2005 .",
    "rivoirard , v. and tribouley , k. _ the maxiset point of view for estimating integrated quadratic functionals .",
    "_ 2007 . to appear in statistica sinica .",
    "nussbaum , m. _ asymptotic equivalence of density estimation and gaussian white noise _ , ann .",
    ", * 24 * , no . 6 , 23992430 , 1996 ."
  ],
  "abstract_text": [
    "<S> we address the statistical issue of determining the maximal spaces ( maxisets ) where model selection procedures attain a given rate of convergence . by considering first general dictionaries , then orthonormal bases , we characterize these maxisets in terms of approximation spaces . </S>",
    "<S> these results are illustrated by classical choices of wavelet model collections . </S>",
    "<S> for each of them , the maxisets are described in terms of functional spaces . </S>",
    "<S> we take a special care of the issue of calculability and measure the induced loss of performance in terms of maxisets .    </S>",
    "<S> * keywords : * approximations spaces , approximation theory , besov spaces , estimation , maxiset , model selection , rates of convergence .    </S>",
    "<S> * ams mos : * 62g05 , 62g20 , 41a25 , 42c40 . </S>"
  ]
}