{
  "article_text": [
    "the nonnegative matrix factorization ( nmf ) has been shown recently to be useful for many applications in environment , pattern recognition , multimedia , text mining , and dna gene expressions  @xcite .",
    "nmf can be traced back to 1970s ( notes from g. golub ) and has been studied extensively by paatero  @xcite .",
    "the work of lee and seung  @xcite brought much attention to nmf in machine learning and data mining fields .",
    "various extensions and variations of nmf have been proposed recently  @xcite .",
    "nmf , in its most general form , can be described by the following factorization @xmath0 where @xmath1 is the dimension of the data , @xmath2 is the number of data points ( usually more than @xmath1 ) and @xmath3 .",
    "generally , this factorization has been compared with data decomposition techniques . in this sense @xmath4",
    "is called the set of basis functions and the set @xmath5 is the data specific weights .",
    "it has been claimed by numerous researchers that such a decomposition has some favorable properties over other similar decompositions , such as pca etc .    in the vast amount of literature present in this area",
    ", the parameter @xmath6 largely goes unnoticed .",
    "we pose the question , what are the fundamental differences in the decomposition for the three cases @xmath3 , @xmath7 and @xmath8 . the nmf decomposition for @xmath3 can be imagined to be an energy compaction process and as such , only basis vectors with higher energy remain in the decomposition . for the case of @xmath7 , we can think @xmath4 as some sort of rotation in @xmath9dimensions and as such the locally linear attributes of the data are preserved , as can be verified by finding the indices of the nearest neighbors of each data point in @xmath10 as well as @xmath5 .    now",
    "the remaining question is what happens for @xmath8 .",
    "it is at this juncture that we want to concentrate our research and draw meaningful conclusions from experimental as well as empirical analysis . to develop a superficial motivation we look into the literature of sparse coding  @xcite .",
    "the basic idea which we borrow , from them is the fact that @xmath6 need not be limited by the dimensionality of the data .",
    "the similarity has been shown to be even greater if an additional sparseness constraint is introduced into the optimization framework  @xcite .",
    "we motivate our analysis from a classification point of view . in the actual application domain",
    "we would like to handle missing attributes .",
    "in this section we introduce the idea of additive nmf ( anmf ) which can be motivated by the following scenario .",
    "assume that given a non - negative matrix @xmath10 , we have run nmf algorithm for a long amount of time , but due to the inherent sub - optimal nature of the nmf algorithm , we have only converged to a local optimum .",
    "now , we can look at the residue matrix @xmath11 , and then perform the decomposition again such that we find @xmath12 . by coupling the sub - optimality conditions on the original and the second decomposition",
    ", we can claim that @xmath13 .",
    "this leads us to the generic anmf formulation @xmath14 this decomposition is inherently equivalent to the standard nmf for @xmath15 .",
    "given such a formulation we can write the update equations in one of the two ways      this scheme essentially means that we employ nmf updates for each value of @xmath16 for the residue obtained from all the previous values , namely 1 to @xmath17 . the error values for @xmath18 for the scurve data is shown in fig .",
    "[ fig : succ_err ] .      proceeding in a way similar to lee and seung  @xcite ,",
    "we can write an update scheme for the anmf scheme . writing the update equation for @xmath19 , we can write @xmath20\\ ] ] where we have dropped the index @xmath21 for simplicity . substituting @xmath22 leads to a simple multiplicative update for @xmath5 , and an analogous scheme for @xmath4 . @xmath23      convergence of the snmf scheme can be proved in the same manner as done by lee and seung  @xcite .",
    "the auxiliary function @xmath24 remains exactly the same for us in form , the only difference being the first order derivative which in our case is @xmath25 the minimizer for the auxiliary function can now be shown to be exactly similar to the update rules mentioned in eqn .",
    "[ eqn : h_update ] .",
    "the training data is used to learn @xmath4 , with @xmath8 .",
    "this can be viewed as developing an over - complete dictionary from the data .",
    "the hope is that this over - complete dictionary will encode enough information , to guess the values of missing attributes , which can be further used for classification .",
    "the similar procedure for @xmath3 has no guarantee to encode extra information , since the matrix @xmath4 will be rank limited by the dimension @xmath6 and hence removing a row from @xmath1 might eliminate a rank dimension .",
    "the basic idea is that since nmf results in a decomposition of _ feature dependent _ ( w ) and _ data dependent _ term ( h ) , we can remove the particular row from w for which we do not have information , and still generate a good estimate for the data dependent term h for the data point with missing attributes .",
    "a simple multiplication with the whole w then gives the approximation for the missing attributes .",
    "the generic data imputation based classification algorithm is as follows :    * training : assume labeled training data without missing attributes and find the decomposition @xmath26 * now keeping the same @xmath27 find the decomposition @xmath28 .",
    "the mask @xmath29 is placed to zero out the rows of @xmath27 corresponding to the missing attributes .",
    "finally , the joint estimate for the missing attributes can be obtained from @xmath30 . *",
    "learn a classifier for @xmath31 .",
    "generate the classification results for @xmath32 .",
    "some of the advantages of the decomposition is that the training data decomposition can be done offline once , and then the learned set of basis functions @xmath27 can be used for the test data transformation .",
    "also , the classification engine does not need to perform any additional task because we convert the data back to its original dimension .",
    "the optimization scheme for the observed part , @xmath37 , can now be written as @xmath38 the update equations can be obtained directly from the update rules of lee and seung  @xcite . once the iterations have converged we can find the missing attribute from the projection @xmath39 .",
    "the squared error @xmath40 is non - increasing under the following updates @xmath41 where @xmath42 and @xmath43 are as defined in eqn .  [ eqn : notations ] .",
    "the squared error can be written as @xmath44 writing the first order derivatives with respect to @xmath35 and @xmath45 and equating them to zero we get @xmath46 the update for @xmath35 is simply obtained from eqn .",
    "[ eqn : dfx ] .",
    "[ eqn : dfh ] suggests that the update for @xmath45 can now be obtained by solving the reduced system @xmath47 which is the same as eqn .",
    "[ eqn : nmf_3 ] , for which the optimum non - negative , non - increasing update has been shown to be the same as eqn .",
    "[ eqn : newupdateb ]  @xcite .    a similar extension can now be applied to the snmf scheme , which leads us to the following claim :    the squared error @xmath48 is non - increasing under the following updates @xmath49 where @xmath42 is as defined in eqn .  [ eqn : notations ] and @xmath50 s are defined analogous to @xmath43 .",
    "first we present results on manifold data as shown in fig .",
    "[ fig : prel_res ] . as can be seen from the results , similar color dots , which have one axis value artificially set to zero are pulled closer to same color data points on the true manifold .",
    "next we present results for the wdbc1 data from uci machine learning repository .",
    "the data is represented as 30 dimensional vectors , with 2 possible classes .",
    "there are total 569 data points .",
    "we randomly select about 80% of the data ar training data and the rest as testing data .",
    "the baseline performance denotes the classification accuracy with complete data .",
    "we introduce the missing attributes in the following way : for each test data point we generate a 30 dimensional random vector @xmath51 .",
    "all the indices in the vector @xmath52 having values less than a threshold @xmath53 are marked for deletion .",
    "all marked indices are subsequently replaced by zeros in the test data point .",
    "this process is repeated for the entire test data set ."
  ],
  "abstract_text": [
    "<S> non - negative matrix factorization ( nmf ) has previously been shown to be a useful decomposition for multivariate data . </S>",
    "<S> we interpret the factorization in a new way and use it to generate missing attributes from test data . </S>",
    "<S> we provide a joint optimization scheme for the missing attributes as well as the nmf factors . </S>",
    "<S> we prove the monotonic convergence of our algorithms . </S>",
    "<S> we present classification results for cases with missing attributes . </S>"
  ]
}