{
  "article_text": [
    "to test whether a random independent sample of size @xmath1 comes from a specific distribution can be done by dividing all possible outcomes of the corresponding random variable ( say @xmath2 ) into @xmath3 distinct regions ( called categories ) so that these have similar probabilities of happening .",
    "the sample of @xmath1 values of @xmath2 is then converted into the corresponding observed frequencies , one for each category ( we denote these @xmath4 ) , equivalent to sampling a multinomial distribution with probabilities @xmath5.(computed , for each category , based on the original distribution ) .",
    "the new random variables @xmath6 have expected values given by @xmath7 ( where @xmath8 goes from @xmath0 to @xmath3 ) and variance - covariance matrix given by@xmath9where @xmath10 is a column vector with @xmath3 elements ( the individual @xmath11 probabilities ) , and @xmath12 is similarly an @xmath3 @xmath13 @xmath3 _ diagonal _ matrix , with the same @xmath14 probabilities on its main diagonal .    the usual test statistic is@xmath15where@xmath16equivalent to ( in its vector form)@xmath17where @xmath18 is a column vector of the @xmath19 observations .",
    "the @xmath20 s have a mean of zero and their variance - covariance matrix is@xmath21where @xmath22 is the @xmath3 @xmath13 @xmath3 unit matrix and @xmath23 denotes a column vector with elements equal to @xmath24 .",
    "the matrix ( [ one ] ) is _ idempotent _ , since@xmath25and its _ trace _ is @xmath26 , since@xmath27 = \\text{tr}% \\left [ ( \\mathbf{p}^{1/2})^{t}\\mathbf{p}^{1/2}\\right ] = \\sum_{i=1}^{k}p_{i}=1.\\]]because the @xmath3-dimensional distribution of ( [ y ] ) tends ( as @xmath28 ) to a normal distribution with zero means and variance - covariance matrix of ( [ one ] ) , ( [ tstat ] ) must similarly converge to the @xmath29 distribution ( assuming that @xmath2 does have the hypothesized distribution ) .",
    "a substantial disagreement between the observed frequencies @xmath6 and their expected values @xmath7 will be reflected by the test statistic @xmath30 exceeding the ( right - hand - tail ) critical value of @xmath31 leading to a rejection of the null hypothesis .",
    "since the sample size is always finite , the critical value ( computed under the assumption that @xmath32 ) with have an error roughly proportional to @xmath33 to remove this error is an objective of this article .",
    "a small modification of the results of @xcite indicate that a substantially better approximation ( which removes the @xmath34 -proportional error ) to the probability density function ( pdf ) of the distribution of @xmath30 ( under the null hypothesis ) is@xmath35where @xmath36 is the pdf of the regular chi - square distribution and@xmath37@xmath38@xmath39where @xmath40 and @xmath41 , are cumulants of the ( multivariate ) @xmath42 distribution .",
    "they can be found easily , based on the logarithm of the joint moment generating function of ( [ yi ] ) , namely@xmath43by differentiating @xmath44 with respect to @xmath45 @xmath46 and @xmath47 to get @xmath40 ( and the extra @xmath48 to get @xmath40 ) , followed by setting all @xmath49 .    this yields@xmath50 and@xmath51    using these formulas , we can proceed to compute@xmath52where@xmath53and @xmath54 and @xmath55 are the first two elementary symmetric polynomials in @xmath56 i.e.@xmath57(note that @xmath58 ) . realizing that @xmath59 the expression for @xmath60 can be simplified to@xmath61when choosing the categories in a manner which makes all @xmath14 equal to @xmath62 , the last expression reduces to@xmath63    similarly,@xmath64where@xmath65note that@xmath66and that the final formula reduces to @xmath67 in the case of all categories being equally likely .",
    "the corresponding _ distribution function _ is given by@xmath68",
    "\\notag\\end{aligned}\\]]which can be used for a substantially more accurate computation of critical values of @xmath30 ( by setting @xmath69 and solving for @xmath70 ) .",
    "we investigate the improvement achieved by this correction by selecting ( rather arbitrarily ) the value of @xmath3 ( from the most common @xmath71 to @xmath72 range ) , the individual components of @xmath10 , and the sample size @xmath1 ( with a particular interest in small values ) .",
    "then we generate a million of such samples and , for each of these , compute the value of @xmath30 .",
    "the resulting empirical ( yet ` nearly exact ' ) distribution is summarized by a histogram , which is then compared with the @xmath29 approximation , first _ without _ and then _ with _ the proposed correction of ( [ corr ] ) .",
    "marginally we mention that , when @xmath73 for all @xmath8 ( the _ uniform _ case ) , the set of potential values of @xmath30 becomes rather small ( the values range from @xmath74 to @xmath75 in steps of @xmath76 ) .",
    "for large enough @xmath77 the shape of the exact distribution still follows the @xmath29 curve , but in a correspondingly ` discrete ' manner .",
    "our examples tend to avoid this complication by making the @xmath14 values sufficiently distinct from each other ; the exact @xmath30 distribution remains discrete , but the number of its possible values increases so dramatically that this is no longer an issue ( unless @xmath1 is extremely small , the distribution can be considered , for any practical purposes , to be continuous ) .",
    "the simulation reveals that , when @xmath78 the essential discreteness of the the @xmath30 distribution remains ` visible ' ( even with a _ non - uniform _ choice of @xmath14s ) unless @xmath1 is at least @xmath79 . such a relatively large value of @xmath1 ( an average of @xmath80 per category ) results in only a marginal improvement achieved by our correction ",
    "@xmath0 , with the blue curve being the basic @xmath29 approximation and the red one representing ( [ corr ] ) .",
    "[ fig1 ] figure 1 .    when @xmath81 and the @xmath10 values are reasonable ` diverse ' ( those of our example range from @xmath82 to @xmath83 ) , the discreteness of the exact @xmath30 distribution is less of a problem ( even though still showing  see fig .",
    "@xmath84 ) , even for @xmath1 as low as @xmath85 ( our choice ) .",
    "the new formula already proves to be a definite improvement over the basic approximation :     figure 2 .",
    "[ fig3 ]    finally , when @xmath86 the distribution becomes almost perfectly smooth ( eliminating all traces of discreteness  see fig .",
    "@xmath87 ) even for @xmath88 unfortunately , this sample size is now so small that it is our approximation itself which starts showing a visible error ( for this value of @xmath3 , this happens whenever the absolute value of either @xmath60 or @xmath89 exceeds @xmath90 ; in this example @xmath91 and @xmath92 ) .",
    "the general rule of thumb is that neither @xmath60 nor @xmath89 should exceed @xmath93 ( beyond that , the approximation may become increasingly nonsensical ) .",
    "[ fig3 ]    to demonstrate the true superiority of the new approximation , we now use @xmath94 and @xmath95 with the individual probabilities ranging from @xmath96 to @xmath97 ( fig .",
    "@xmath80 ) . since now @xmath98 and @xmath99 the new approximation ( unlike the old one , which is clearly off the mark ) represents a decent agreement with the ` exact ' answer .",
    "using the @xmath100 approximation to perform the usual goodness - of - fit test , the number of observations should be as large as possible ; when this becomes impractical ( e.g. each observation is very costly ) , one can still achieve good accuracy by :    1 .   increasing the number of categories",
    "( one should aim for the @xmath101 range ) ; this inevitably results in reducing the average number of observations per category  in spite of that , the test becomes more accurate , 2 .",
    "choosing categories in such a way that their individual probabilities are all distinct from each other ( avoiding the @xmath102 situation ) but , at the same time , not letting any one of them become too small ( this would increase , often dramatically , the value of each @xmath60 and @xmath89 of our correction  see the next item ) , 3 .   using the @xmath34 proportional correction of ( [ fd ] ) , but monitoring the values of @xmath60 and @xmath89 ( neither of them should be bigger , in absolute value , than @xmath93 ) ."
  ],
  "abstract_text": [
    "<S> it is well known that the approximate distribution of the usual test statistic of a goodness - of - fit test is chi - square , with degrees of freedom equal to the number of categories minus @xmath0 ( assuming that no parameters are to be estimated  something we do throughout this article ) . here </S>",
    "<S> we show how to improve this approximation by including two correction terms , each of them inversely proportional to the total number of observations . </S>"
  ]
}